# AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking

AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking

 Shiqi Sun\({}^{1}\), Yantao Lu\({}^{1}\), Ning Liu\({}^{2}\), Bo Jiang\({}^{3}\), Jinchao Chen\({}^{1}\), Ying Zhang\({}^{1}\)

\({}^{1}\) Department of Computer Science, Northwestern Polytechnical University

\({}^{2}\) Midea Group

\({}^{3}\) Didi Chuxing

{shiqisun, yantaolu, cjc, ying_zhang}@nwpu.edu.cn

ningliu1220@gmail.com

boj.horizon@gmail.com

Joint first authorship. Either author can be cited first.Corresponding author.

###### Abstract

Camera-LiDAR fusion models significantly enhance perception performance in autonomous driving. The fusion mechanism leverages the strengths of each modality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR fusion models utilize pre-trained backbones for efficient training. However, we argue that directly loading single-modal pre-trained camera and LiDAR backbones into camera-LiDAR fusion models introduces similar feature redundancy across modalities due to the nature of the fusion mechanism. Unfortunately, existing pruning methods are developed explicitly for single-modal models, and thus, they struggle to effectively identify these specific redundant parameters in camera-LiDAR fusion models. In this paper, to address the issue above on camera-LiDAR fusion models, we propose a novelty pruning framework **AIter**native **M**odality **M**asking **P**uning (AlterMOMA), which employs alternative masking on each modality and identifies the redundant parameters. Specifically, when one modality parameters are masked (deactivated), the absence of features from the masked backbone compels the model to _reactivate_ previous redundant features of the other modality backbone. Therefore, these redundant features and relevant redundant parameters can be identified via the reactivation process. The redundant parameters can be pruned by our proposed importance score evaluation function, **AIter**native **E**valuation (AlterEva), which is based on the observation of the loss changes when certain modality parameters are activated and deactivated. Extensive experiments on the nuScenes and KITTI datasets encompassing diverse tasks, baseline models, and pruning algorithms showcase that AlterMOMA outperforms existing pruning methods, attaining state-of-the-art performance.

## 1 Introduction

Camera-LiDAR fusion models are prevalent in autonomous driving, effectively leveraging the sensor properties, including the accurate geometric data from LiDAR point clouds and the rich semantic context from camera images , providing a more comprehensive understanding of the environment . However, the exponential increase in parameter counts due to fusion architectures introduces significant computational costs, especially when deploying these systems on resource constrained edge devices, which is a crucial challenge for autonomous driving . Network pruning is one of the most attractive methods for addressing the challenge above of identifying and eliminating redundancy in models. Existing pruning algorithms target single-modal models [6; 7; 8; 9; 10; 11; 12] or multi-modal models that merge distinct types of data [13; 14], such as visual and language inputs. However, it's important to note that directly applying these algorithms to camera-LiDAR fusion models can lead to significant performance degradation. The degradation can be reasoned for two main factors that existing pruning methods overlooked: 1) the key fusion mechanism specific to vision sensor inputs within models, and 2) the training scheme where models typically load single-modal pre-trained parameters onto each backbone [2; 15]. Specifically, since single-modality models lack the cross-modality fusion mechanism, existing pruning algorithms traditionally do not consider inter-modality interactions. Furthermore, because the pre-trained backbones (image or LiDAR) are trained separately, they are not optimized jointly, exacerbating the redundancy in features extracted from each backbone. Though leveraging pre-trained backbone improves the training efficiency compared with models training from scratch, we argue that _directly loading single-modal pre-trained camera and LiDAR backbones into camera-LiDAR fusion models introduces similar feature redundancy across modalities due to the nature of the fusion mechanism._

In detail, since backbones are independently pre-trained on single-modal datasets, they extract features comprehensively, which leads to similar feature extraction across modalities. Meanwhile, the fusion mechanism selectively leverages reliable features while minimizing weaker ones across modalities to enhance model performance. This selective utilization upon similar feature extraction across modalities introduces the additional redundancy: _Each backbone independently extracts similar features, which subsequent fusion modules will not potentially utilize_. For instance, both camera and LiDAR backbones extract geometric features to predict depth during pre-training. However, geometric features extracted from the LiDAR backbone are considered more reliable during fusion because LiDAR input data contain more accurate geometric information than the cameras, e.g., object distance, due to the physical properties of sensors. Consequently, this leads to the redundancy of geometric features of the camera backbone. In summary, similar feature extraction across modalities, coupled with the following selective utilization in fusion modules, leads to two counterparts of similar features across modalities: those utilized by fusion modules in one modality (i.e., fusion-contributed), and those that are redundant in the other modality (i.e., fusion-redundant). We also illustrate the fusion-redundant features in Figure 1.

To address the above challenge, we propose a novel pruning framework **AlterMOMA**, specifically designed for camera-LiDAR fusion models to identify and prune fusion-redundant parameters. AlterMOMA employs alternative masking on each modality, followed by observing loss changes when certain modality parameters are activated and deactivated. These observations serve as important indications to identify fusion-redundant parameters, which are integral to our importance scores evaluation function, **AlterEva**. Specifically, the camera and LiDAR backbones are alternatively masked. During this process, the absence of fusion-contributed features and relevant parameters in the masked (deactivated) backbone compels the fusion modules to reactivate their fusion-redundant

Figure 1: **Motivating example** of fusion-redundant features in the 3D object detection task. We employ backward propagation on camera-LiDAR fusion models with pre-trained backbones to observe the gradient difference (features utilization) between with camera backbone only and with both the camera and LiDAR backbone. Notably, certain pre-trained parameters in the camera backbone are redundant due to the amendment of LiDAR information. It reveals that similar feature extraction exists across modalities, which introduces additional redundancy when camera-LiDAR fusion models directly loads single-modal pre-trained backbones.

counterparts from the other backbone. Throughout this reactivation, changes in loss are observed as indicators for contributed and fusion-redundant parameters across modalities. These indicators are then combined in AlterEva to maximize the importance scores of contributed parameters while minimizing the scores of fusion-redundant parameters. Then, parameters with low importance scores will be pruned to reduce computational costs.

To validate the effectiveness of our proposed framework, extensive experiments are conducted on several popular 3D perception datasets with camera and LiDAR sensor data, including nuScenes  and KITTI . These datasets encompass a range of 3D autonomous driving tasks, including 3D object detection, tracking, and segmentation.

The contributions of this paper are as follows: 1) We propose a pruning framework **AlterMOMA** to effectively compress camera-LiDAR fusion models 2) we propose an importance score evaluation function **AlterEva**, which identifies fusion-redundant features and their relevant parameters across modalities 3) we validate the effectiveness of the proposed AlterMOMA on **nuScenes** and **KITTI** for 3D detection and segmentation tasks.

## 2 Related Work

**Camera-LiDAR Fusion.** With the advancement of autonomous driving technology, the efficient fusion of diverse sensors, particularly cameras and LiDARs, has become crucial [18; 19]. Fusion architectures can be categorized into three types based on the stage of fusion within the learning framework: early fusion [20; 21], late fusion , and intermediate fusion [2; 3; 15]. Current state-of-the-art (SOTA) fusion models evolve primarily within intermediate fusion and combine low-level machine-learned features from each modality to yield unified detection results, thus significantly enhancing perception performance compared with early or late fusion. Specifically, camera-LiDAR fusion models focus on aligning the camera and LiDAR features through dimension projection at various levels, including point , voxel , and proposal . Notably, the SOTA fusion paradigm aligns all data to the bird's eye view (BEV) [2; 4; 15; 25; 26], has gained traction as an effective approach to maximize the utilization of heterogeneous data types.

**Network Pruning.** Network pruning effectively compresses deep models by reducing redundant parameters and decreasing computational demands. Pruning algorithms have been well-explored for single-modal perception tasks [27; 28; 29; 30; 31; 32], focusing on evaluating importance scores to identify and remove redundant parameters or channels. These scores are based on data attributes [7; 33], weight norms [34; 35], or feature map ranks . However, single-modal pruning algorithms are not suited for the complexities of camera-LiDAR fusion models. While some multi-modal pruning algorithms exist [13; 14], they are mainly designed for models combining different data types like language and vision. Therefore, there is a pressing need for pruning algorithms specifically devised for camera-LiDAR fusion models. From the perspective of granularity, pruning algorithms can be divided into two primary categories: 1) structured pruning, which entails removing entire channels or rows from parameter matrices, and 2) unstructured pruning, which focuses on eliminating individual parameters. For practical applications, we have adapted our method to support both types of pruning.

## 3 Methodology

### Preliminaries

We firstly review some basic concepts including camera-LiDAR fusion models and pruning formulation. Camera-LiDAR fusion models consist of 1) a LiDAR feature extractor \(_{l}\) to extract features from point cloud inputs, 2) a camera feature extractor \(_{c}\) to extract features from image inputs, 3) the fusion module and following task heads \(_{f}\) to get the final task results. The parameters denote as \(\) = { \(_{l}\), \(_{c}\), \(_{f}\) } for LiDAR backbone, camera backbone, and fusion and task heads, respectively. Take camera backbone for instance, \(_{c}=\{_{c}^{1},_{c}^{2},...,_{c}^{N_{c}}\}\) denotes all weights in the camera backbone, where \(N_{c}\) represents the total number of parameters in camera backbone. Therefore, for the LiDAR input \(_{l}\) and camera input \(_{c}\), the training process of models could be denoted as

\[_{_{l,c,f}}(,_{f}(_{f}; _{l}(_{l};_{l}),_{c}(_{c}; _{c})),\] (1)

where \(\) denotes the ground truth, and \(\) represents the task-specific loss functions.

Importance-based pruning typically involves using metrics to evaluate the importance scores of parameters or channels. Subsequently, optimization methods are employed to prune the parameters with lower importance scores, that are nonessential within the model. For the camera-LiDAR fusion models, the optimization process can be formulated as follows:

\[_{_{ij}}_{i\{l,c,f\}}_{j=1}^{N_{i}}_{ij}_{i}^{j}}_{i\{l,c,f\}}_{j=1}^{N_{i}}_{ij}=k,\] (2)

where \(_{ij}\) is an indicator which is 1 if \(_{i}^{j}\) will be kept or 0 if \(_{i}^{j}\) is to be pruned. \(\) is designed to measure the importance scores for parameters, and \(k\) represents the kept parameter number, where \(k=(1-)_{i\{l,c,f\}}N_{i}\) with the pruning ratio \(\).

### Overview of Alternative Modality Masking Pruning

Similar feature extraction across modalities, coupled with the selective utilization of features in the following fusion modules introduce redundancy in camera-LiDAR fusion models. Therefore, similar features and their relevant parameters can be categorized into two counterparts across modalities: those that contribute to fusion and subsequent task heads (fusion-contributed), and those that are redundant (fusion-redundant). In this section, we propose the pruning framework AlterMOMA, which alternatively employs masking on camera and Lidar backbones to identify and remove the fusion-redundant parameters. AlterMOMA is developed based on a novel insight: _"The absence of fusion-contributed features will compel fusion modules to'reactivate' their fusion-redundant counterparts as supplementary, which, though less effective, are necessary to maintain functionality."_ For instance, if the LiDAR backbone is masked, the previously fusion-contributed geometric features it provided are absent. To fulfill the need for accurate position predictions, the model still needs to process geometric features. Consequently, the fusion module is compelled to utilize the geometric features from the unmasked camera backbone, which were previously fusion-redundant. We refer to this process as _Redundancy Reactivation_. By observing changes during this _Redundancy Reactivation_, fusion-redundant parameters can be identified. The overview of AlterMOMA is shown in Figure 2, and the detailed steps are in Algorithm 1 of Appendix D. The key steps are introduced as follows:

**Modality Masking.** Three binary masks are denoted as \(_{l}\), \(_{c}\), and \(_{f}\{0,1\}\), correspond to the parameters applied separately on the LiDAR backbone, the camera backbone, and the fusion and

Figure 2: **Overview of the AlterMOMA** : The framework begins with _Modality Masking_, where one of the backbones is initially masked. This step is followed by _Redundancy Reactivation_ and _Importance Evaluation_, where the parameter importance scores are initially calculated with AlterEva. Afterward, the models undergo _Reinitialization_ and _Alternative Masking_ of the other backbone, leading to another round of _Redundancy Reactivation_ and _Importance Evaluation_. When scores of all parameters in backbones are calculated fully with AlterEva (detailed in Section 3.3), models are pruned to remove parameters with low importance scores and then finetuned. Notably, we use _black_ lines to represent parameters of models and _red_ lines to represent reactivated fusion-redundant parameters. The thickness of these lines indicates the contribution of parameters.

tasks head. Our framework begins by masking either one of the camera backbones or the LiDAR backbone. Here we take masking the LiDAR backbone as the illustration. The masks are with \(_{l}=0\), \(_{c}=1\), \(_{f}=1\). The camera backbone will be masked alternatively.

**Redundancy Reactivation.** To allow masked models to reactivate fusion-redundant parameters, we train masked models with batches of data. Specifically, \(B\) batches of data \(_{i}\), \(i\{1,2,...,B\}\) are sampled from the multi-modal dataset \(\).

**Importance Evaluation.** After _Redundancy Reactivation_, the importance scores of parameters in the camera backbones are calculated with our proposed importance score evaluation function AlterEva detailed in Section 3.3. Since fusion modules need to consider the reactivation of both modalities, the importance scores of parameters in the fusion module and task heads will be updated once the importance scores of both the camera and Lidar backbones' parameters are calculated.

**Alternative Masking.** After _Importance Evaluation_ of camera modality, models will reload the initialized parameters, and then the other backbone will alternatively be masked, with \(_{l}=1\), \(_{c}=0\), \(_{f}=1\). Then the step _Redundancy Reactivation_ and _Importance Evaluation_ will be processed again to update the importance scores of parameters in the LiDAR backbone and the fusion module.

**Pruning with AlterEva.** After evaluating the importance scores using AlterEva, parameters with low importance scores are pruned with a global threshold determined by the pruning ratio. Once the pruning is finished, the model is fine-tuned with the task-specific loss, as indicated in Eqn. 1.

### Alternative Evaluation

In this section, we will detail the formulation of our proposed AlterEva, which consists of two distinct indicators to evaluate the parameter importance scores. As outlined in section 3.2, the importance scores are alternatively calculated with AlterEva in _Importance Evaluation_. Then, parameters with low importance scores are removed in the pruning process. The goal of AlterEva is to maximize the scores of parameters that contribute to task performance while minimizing the scores of fusion-redundant parameters. To achieve this, AlterEva incorporates two key indicators: 1) **Deactivated Contribution Indicator** (DeCI) evaluate the parameter contribution to the overall task performance of the fusion models, 2) **Reactivated Redundancy Indicator** (ReRI) identifies fusion-redundant parameters across both modalities. Since changes in loss can directly reflect the parameter contribution difference to task performance during alternative masking, both indicators are designed based on the observation of loss decrease or increase, when certain modality parameters are activated or deactivated. Specifically, take parameters in the camera backbone as an instance, DeCI observe the loss increases with masking camera backbone itself, while ReRI observe loss decrease with masking LiDAR backbone and reactivating camera backbone via _Redundancy Reactivation_. Formally, we formulate the loss for the fusion models with masks. With three binary masks and the dataset defined in Section 3.2, the loss is denoted as follows, by simplifying some of the extra notations used in Eqn. 1:

\[_{m}(_{c},_{l},_{f};)=(_{l} _{l},_{c}_{c},_{f}_{f};).\] (3)

For brevity, we assume \(_{c}=1\), \(_{l}=1\), and \(_{f}=1\), and we only specify in the formulation when a mask is zero. For example, \(_{m}(_{c}=0;)\) indicates that \(_{c}=0\), \(_{f}=1\) and \(_{l}=1\). Since the alternative masking is performed on both backbones, we illustrate our formulation by calculating two indicators for parameters in the camera backbone.

**Deactivated Contribution Indicator.** If a parameter is important and contributes to task performance, deactivating this parameter will lead to task performance degradation, which will be reflected in an increase in loss. Therefore, to derive the contribution of the \(i\)-th parameter \(_{c}^{i}\) of the camera backbone, we observe the changes in loss when this parameter is deactivating via masking, denoted as follows:

\[_{_{c}^{i}}=|_{m}(;)-_{m}( _{c}^{i}=0;)|,\] (4)

where \(_{c}^{i}\) represents the mask for \(_{c}^{i}\), and \(_{_{c}^{i}}\) denotes the indicator DeCI for \(_{c}^{i}\). However, the total number of parameters is enormous, deactivating and evaluating each parameter independently are computationally intractable. Therefore, we design an alternative efficient method to approximate the evaluation in Eqn. 4 by leveraging the Taylor first-order expansion inspired by . We first observe the loss changes \(|_{m}(;)-_{m}(_{c}=0;)|\) by deactivating the entire camera backbones. Then, the first-order approximation of evaluation in Eqn. 4 is calculated by expanding the loss change in each individual parameter \(_{c}^{i}\) with Taylor expansion, considering \(_{c}=\{_{c}^{1},...,_{c}^{N_{c}}\}\). This method allows us to estimate the contribution for each parameter, denoted as follows:

\[_{_{c}^{i}}=_{m}(;)+_{c}^{i} _{c}^{i}_{m}(;)}{ _{c}^{i}}-_{m}(_{c}=0;)-_{c}^{i} _{c}^{i}_{m}(_{c}=0;)}{ _{c}^{i}}.\] (5)

When \(_{c}\) is deactivating with \(_{c}=0\), \(_{c}^{i}=0\) for \(i\{1,...,N^{c}\}\), which means that the last term of Eqn. 5 is zero. Meanwhile, when considering importance scores on a global scale, the \(_{m}(;)\) and \(_{m}(_{c}=0;)\) can be treated as constant for all \(_{c}^{i}\). Thus the first term and the third term can be disregarded. Therefore, the final indicator of each parameter's contribution, represented by our DeCI, can be expressed as follows:

\[_{_{c}^{i}}=_{c}^{i}_{m}(;)}{_{c}^{i}}.\] (6)

This formulation enables tractable and efficient computation without _Modality Masking_ of the camera backbone itself, achieved by performing a single backward propagation in the _Importance Evaluation_ with initialized parameters.

**Reactivated Redundancy Indicator.** As discussed in Section 3.2, the identification of fusion-redundant parameters relies on our understanding of the fusion mechanism: when fusion-contributed features from the LiDAR backbone are absent due to masking, the previously fusion-redundant counterparts and their relevant parameters from the camera backbone will be reactivated during the _Redundancy Reactivation_. Therefore, to reactivate and identify fusion-redundant parameters in the camera backbone, the _Modality Masking_ of the LiDAR backbone (\(_{l}=0\)) and _Redundancy Reactivation_ are processed first. Throughout this process, the loss evolves from \(_{m}(_{l}=0;_{1})\) to \(_{m}(_{l}=0;_{B})\), and the parameters evolve from \(_{c,0}\) (i.e. \(_{c}\)) to \(_{c,B}\). Similar to the formulation of DeCI, we observe the decrease in loss during _Redundancy Reactivation_ and refer to this observation as our ReRI, denoted as follows:

\[_{_{c}}&=|_{m}(_{l}=0;)-_{m}(_{l}=0;_{1})+...+ _{m}(;_{B-1})-_{m}(_{l}=0;_{B}) |\\ &=|_{m}(_{l}=0;)-_{m}(_{l} =0;_{B})|.\] (7)

Specifically, this process is designed to identify parameters that contribute to the task performance of models with the masked LiDAR backbone, highlighting those that are fusion-redundant. Since we want to observe reactivation rather than parameters updating of this masked model across training batches, we apply the first-order Taylor expansion to the initial \(i\)-th parameters \(_{c,0}^{i}\), denoted as:

\[_{_{c,0}^{i}}=_ {m}(_{l}=0;)&+_{c}^{i}_{c,0}^{i} _{m}(_{l}=0;)}{_{c,0}^{i}}\\ &-_{m}(_{l}=0;_{B})-_{c}^{i} _{c,0}^{i}_{m}(_{l}=0; _{B})}{_{c,0}^{i}}.\] (8)

To derive the gradient on initial parameters \(_{c,0}^{i}\) of the last term, we could use the chain rule and write out based on the gradient of the last step,

\[_{m}(_{l}=0;_{B})}{_{c, 0}^{i}}_{c,0}^{i}=_{m}(_{l}=0;_{B})}{_{c,B}^{i}}_{j=1}^{B}^{i}}{ _{c,j-1}^{i}}_{c,0}^{i}_{m}(_{l}=0,_{B})}{_{c,B}^{i}}_{c,0}^{i}.\] (9)

According to the Proposition A in the Appendix A, this approximation is reached by dropping some small terms with sufficiently small learning rates . Since \(_{c}^{i}\) is activating with \(_{c}^{i}=1\), and the \(_{m}(_{l}=0;)\) and \(_{m}(_{c}=0;_{B})\) can be treated as constant for all \(_{c}^{i}\), we could denote our final formulation by simplifying Eqn. 8, and denoted as follow:

\[_{_{c}^{i}}=|_{c}^{i}_{m }(_{l}=0;)}{_{c}^{i}}-_{c}^{i}_{m}(_{l}=0;_{B})}{_{c,B}^{i}} ,\] (10)

where \(_{c}^{i}\) is the \(_{c,0}^{i}\) and \(_{_{c}^{i}}\) represent the ReRI for \(_{c}^{i}\).

To the goal of parameters with significant contributions maintaining high importance scores while those identified as fusion-redundant are assigned lower scores, AlterEva calculate the final importance scores by subtracting ReRI from DeCI. Since DeCI of parameters in the camera backbone could be calculated without masking the camera backbone itself, DeCI and ReRI of camera parameters could be calculated in the same alternative masking stage (with LiDAR backbone masking), which simplifies the process of our framework AlterMOMA. Therefore, with the combination Eqn. 6 and Eqn. 10, the AlterEva of the camera backbones could be presented with the normalization:

\[(_{c}^{i})=_{_{c}^{i}}}{_ {j=0}^{N_{c}}_{_{c}^{i}}}-_{ _{c}^{i}}}{_{j=0}^{N_{c}}_{_{c}^{i}}},\] (11)where \(\) and \(\) are the hyper parameters and \((_{c}^{i})\) represent the importance score evaluation function AlterEva for \(_{c}^{i}\). Similarly, the AlterEva of parameters in the LiDAR backbones (i.e. \(_{l}\)) and in the fusion modules (i.e. \(_{f}\) ) could be derived as:

\[(_{l}^{i})=_{_{l }^{i}}}{_{j=0}^{N_{l}}_{_{l}^{j}}}-_{_{l}^{i}}}{_{j=0}^{N_{l}}_{_{l}^{j}}},\] (12) \[(_{f}^{i})=_{_{ f}^{i}}}{_{j=0}^{N_{f}}_{_{f}^{j}}}- _{_{f}^{j}}(_{l}=0)}{_{j=0}^{N_{f}}_{_{f}^{j}}(_{l}=0)}-_{ _{f}^{j}}(_{c}=0)}{_{j=0}^{N_{f}}_{_{f}^{j}}(_{c} =0)},\] (13)

where \(_{_{l}^{i}}\) and \(_{_{f}^{j}}(_{c}=0)\) is calculated when camera backbone is masking, while \(_{_{f}^{i}}(_{l}=0)\) is calculated with LiDAR backbone masking. AlterEva can efficiently calculate importance scores with backward propagation, enhancing the tractability of AlterMOMA. For brevity, we omit the derivations related to parameters in the LiDAR backbone and fusion modules, but additional details are available in Appendix B and Appendix C.

## 4 Experimental Results

### Baseline Models and Datasets

To validate the efficacy of our proposed framework, empirical evaluations were conducted on several camera-LiDAR fusion models, including the two-stage detection models AVOD-FPN , as well as the end-to-end architecture based on BEV space, such as BEVfusion-mit  and BEVfusion-pku . For AVOD-FPN, the point cloud input is processed using a voxel grid representation, while all input views are extracted using a modified VGG-16 . Notably, the experiment on the AVOD-FPN demonstrates the efficiency of AlterMOMA on two-stage models, although this isn't the SOTA fusion architecture for recent 3D perception tasks. Current camera-LiDAR fusion models are moving towards a unified architecture that extracts camera and LiDAR features within a BEV space. Thus, our primary results focus on BEV-based unified architectures, specifically BEVfusion-mit  and BEVfusion-pku . We conducted tests using various backbones. For camera backbones, we included Swin-Transformer (Swin-T)  and ResNet . For LiDAR backbones, we used SECOND , VoxelNet  and PointPillars .

We perform our experiments for both 3D object detection and BEV segmentation tasks on the KITTI  and nuScenes , which are challenging large-scale outdoor datasets devised for autonomous driving tasks. The KITTI dataset contains 14,999 samples in total, including 7,481 training samples and 7,518 testing samples, with a comprehensive total of 80,256 annotated objects. To adhere to standard practice, we split the training samples into a training set and a validation set in approximately a 1:1 ratio and followed the difficulty classifications proposed by KITTI, involving _easy_, _medium_, and _hard_. NuScenes is characterized by its comprehensive annotation scenes, encompassing tasks including 3D object detection, tracking, and BEV map segmentation. Within this dataset, each of the annotated 40,157 samples presents an assemblage of six monocular camera images, adept at capturing a panoramic 360-degree field of view. This dataset is further enriched with the inclusion of a 32-beam LiDAR scan, amplifying its utility and enabling multifaceted data-driven investigations.

### Implementation Details

We conducted the 3D object detection and segmentation experiments with MMdetection3D  on NVIDIA RTX 3090 GPUs. To ensure fair comparisons, consistent configurations of hyperparameters were employed across different experimental groups. To train the 3D object detection baselines, we utilize Adam as the optimizer with a learning rate of 1e-4. We employ Cosine Annealing as the parameter scheduler and set the batch size to \(2\). For BEV segmentation tasks, we employ Adam as the optimizer with a learning rate of 1e-4. We utilize the one-cycle learning rate scheduler and set the batch size to 2. The hyperparameters \(\) and \(\) in Section 3.3 are both set with 1. The baseline pruning methods include IMP , SynFlow , SNIP , and ProsPr , with the hyperparameters specified in their papers respectively.

### Experimental Results on Unstructured Pruning

To evaluate the efficiency of AlterMOMA with unstructured pruning, we conduct experiments across multiple fusion architectures and datasets for 3D object detection and semantic segmentation. Specifically, to evaluate the efficiency of AlterMOMA on two-stage fusion architectures, we applied AlterMOMA to AVOD-FPN , using the KITTI dataset with the task of 3D detection. Besides, BEVfusion-mit  and BEVfusion-pku , as two representative camera-LiDAR fusion models with unified BEV-based architectures, are applied with AlterMOMA using the nuScenes dataset to validate the efficiency on both 3D detection and semantic segmentation tasks. Additionally, to validate the robustness of AlterMOMA with various backbones, we conducted experiments with alternative images and point backbone, including ResNet  and PointPillars .

**3D Object detection on nuScenes with BEV-based fusion Architectures.** The experimental results are presented in Table 1. Note that baseline models are BEVfusion-mit trained with SwinT and VoxelNet backbone. As reported in Table 1, single-modal pruning methods, including IMP, SynFlow, SNIP, and ProsPr, experience significant declines in accuracy performance. Even the ProsPr, considered the best-performing method among these single-modal pruning techniques, demonstrates the mAP decrease of 3.5% in accuracy at the 80% pruning ratio and 9.2% at the 90% pruning ratio on BEVfusion-mit. Conversely, the incorporation of our AlterMOMA yielded promising results. For example, comparing with the baseline pruning method ProsPr, AlterMOMA boosts the mAP of BEVfusion-mit by 3.0% (64.3% \(\) 67.3%), 3.6% (61.9% \(\) 65.5%), and 4.9% (58.6% \(\) 63.5%) for the three different pruning ratios. Similarly, AlterMOMA obtains much higher mAP and NDS than the other four pruning baselines with different pruning ratios on BEVFusion-mit and BEVFusion-pku.

**3D Object detection on KITTI with the two-stage fusion architecture.** To validate the efficiency of AlterMOMA on the two-stage detection fusion architecture, we conduct experiments with various pruning ratios on KITTI with AVOD-FPN architecture as the baseline. The experimental results are presented in Table 2. Specifically, Table 2 presents the results for the car class on the KITTI, detailing AP-3D and AP-BEV across various difficulty levels including _easy_, _moderate_, and _hard_. Existing pruning methods experience significant declines in performance on different metrics of different difficulties. Even the best-performing method among single-modal pruning methods, ProPr, shows a decrease in AP-3D of 3.5%, 2.6%, and 4.4% in the _easy_, _moderate_, and _hard_ difficulty levels, respectively, at the 80% pruning ratio. Conversely, the AlterMOMA has yielded

  
**Baseline Model** &  &  \\ 
**Sparsity** &  &  &  &  &  &  \\ 
**Metric** & mAP & NDS & mAP & NDS & mAP & NDS & mAP & NDS & mAP & NDS & mAP & NDS \\ 
**[No Pruning]** & 67.8 & 70.7 & - & - & - & - & 66.9 & 70.4 & - & - & - & - \\  IMP & 59.3 & 66.8 & 51.2 & 59.2 & 42.7 & 50.4 & 57.3 & 65.9 & 49.8 & 57.7 & 40.7 & 47.2 \\ SynFlow & 63.2 & 67.9 & 56.9 & 64.1 & 49.3 & 58.7 & 62.4 & 67.1 & 55.4 & 63.2 & 47.6 & 57.1 \\ SNIP & 62.2 & 67.5 & 56.4 & 63.6 & 50.2 & 58.8 & 61.8 & 67.5 & 54.7 & 62.9 & 47.8 & 57.3 \\ ProsPr & 64.3 & 69.6 & 61.9 & 66.1 & 58.6 & 62.5 & 63.6 & 68.4 & 59.9 & 66.1 & 56.7 & 62.7 \\
**AlterMOMA (Ours)** & **67.3** & **70.2** & **65.5** & **69.5** & **63.5** & **66.7** & **66.5** & **70.1** & **64.2** & **68.1** & **62.3** & **66.0** \\   

Table 1: **3D object detection performance comparison with the state-of-the-art pruning methods on the nuScenes validation dataset. We list the mAP and NDS of models pruned by different approaches within 80%, 85%, and 90% pruning ratios. The two baseline models are trained with SwinT and VoxelNet backbone.**

  
**Sparsity** &  &  \\ 
**Task** &  &  &  &  \\ 
**Difficulty** & Easy & Moderate & Hard & Easy & Moderate & Hard & Easy & Moderate & Hard & Easy & Moderate & Hard \\ 
**Car [No Pruning]** & 82.4 & 72.2 & 66.5 & 89.4 & 83.9 & 78.7 & - & - & - & - & - & - \\  IMP & 65.8 & 57.7 & 51.3 & 69.2 & 64.6 & 59.7 & 52.1 & 45.7 & 43.2 & 59.6 & 54.2 & 51.7 \\ SynFlow & 74.2 & 65.7 & 60.2 & 79.5 & 75.3 & 70.3 & 64.5 & 54.7 & 48.1 & 73.5 & 67.6 & 64.4 \\ SNIP & 73.5 & 64.9 & 59.8 & 79.1 & 75.8 & 69.6 & 62.7 & 52.3 & 45.8 & 72.4 & 66.9 & 63.7 \\ ProPr & 78.9 & 69.6 & 62.1 & 85.2 & 79.1 & 75.7 & 74.2 & 63.4 & 59.1 & 81.2 & 75.1 & 71.9 \\
**AlterMOMA (Ours)** & **80.5** & **70.2** & **63.2** & **87.2** & **81.5** & **77.9** & **77.4** & **68.2** & **62.3** & **85.3** & **79.9** & **75.8** \\   

Table 2: **3D object detection performance comparison with the state-of-the-art pruning methods on the KITTI Validation dataset on car class. We list the models pruned by different approaches within 80%, and 90% pruning ratios. The baseline model is AVOD-FPN architecture.**promising results. For instance, comparing with the pruning method ProsPr, AlterMOMA enhances both the AP-3D and AP-BEV on AVOP-FPN by 3.2% (74.2% \(\) 77.4%) and 3.9% (81.2% \(\) 85.3%) for _easy_ difficulties at the 90% pruning ratio. Furthermore, AlterMOMA consistently outperforms the other four pruning baselines across various difficulties and pruning ratios on AVOD-FPN. The comprehensive experimental results on 3D object detection validate the effectiveness of our AlterMOMA across different camera-LiDAR fusion architectures.

**3D Semantic Segmentation on nuScenes** To validate the robustness of our work, we extend our performance evaluation of AlterMOMA to the semantic-centric BEV map segmentation task. Note that baseline models are BEVfusion-mit trained with SwinT and VoxelNet backbone. We use the nuScenes dataset and utilize the BEV map segmentation validation set. The pivotal evaluation metric for this task is the mean Intersection over Union (mIoU). with the experimental configuration detailed in the work by , we perform our evaluation on the BEVfusion-mit, as shown in Table 3. We observed that existing pruning methods still meet a significant accuracy drop by 8.6% (IMP), 5.3% (SynFlow), 5.9% (SNIP), and 4.1% (ProsPr) for the 80% pruning ratio. Alternatively, our proposed approach yields significant advancements in performance. Specifically, compared with the ProsPr, AlterMOMA achieves a substantial enhancement by 3.0% (57.7% \(\) 60.7%), 3.0% (56.2% \(\) 59.2%), and 3.6% (54.1% \(\) 57.7%) for the three different pruning ratios. These results empirically prove the efficacy of our pruning algorithms when applied to the BEV map segmentation task.

**Results on Various Backbone Architectures** To comprehensively assess the efficacy of AlterMOMA, we conducted experiments with alternative images and point backbones which will influence fusion. Specifically, we replaced the original VoxelNet backbone with PointPillar and the SwinT backbone with ResNet101 on the architecture of BEVFusion-mit. As depicted in Table 4, the results obtained from these experiments consistently demonstrate state-of-the-art performance with various pruning ratios. Particularly noteworthy is the achievement of substantial 1.6%, 3.1%, and 4.2% improvement compared to the ProsPr baseline under the pruning ratio of 80%, 85% and 90%. This consistent improvement is observed across different pruning ratios, affirming the effectiveness of AlterMOMA with different backbones employed. These outcomes robustly demonstrate the general applicability of AlterMOMA to various backbone architectures.

    & 80\% & 85\% & 90\% \\  & mAP & mAP & mAP \\ 
[No Pruning] & 53.7 & - & - \\  IMP & 47.1 & 43.3 & 37.8 \\ SynFlow & 49.5 & 45.8 & 40.3 \\ SNIP & 49.7 & 45.5 & 41.2 \\ ProsPr & 50.1 & 47.5 & 44.1 \\
**AlterMOMA** & **51.7** & **50.6** & **48.3** \\   

Table 4: **3D object detection performance with various backbones on the nuScenes validation dataset.**

    & 80\% & 85\% & 90\% \\  & mIoU & mIoU & mIoU \\ 
**[No Pruning]** & 61.8 & - & - \\  IMP & 53.2 & 51.8 & 49.9 \\ SynFlow & 56.5 & 55.3 & 53.1 \\ SNIP & 55.9 & 54.9 & 53.2 \\ ProsPr & 57.7 & 56.2 & 54.1 \\
**AlterMOMA** & **60.7** & **59.2** & **57.7** \\   

Table 3: **BEV segmentation performance comparison on the nuScenes validation dataset.**

    &  \\  & mAP & NDS & GFLOPs(\(\)\%) \\ 
**[No Pruning]** & 64.6 & 69.4 & 610.66 \\  IMP-30\% & 60.8 & 67.2 & 428.7 (29.8) \\ ProsPr-30\% & 64.2 & 69.1 & 413.4 (32.3) \\
**AlterMOMA-30\%** & **65.3** & **69.9** & **420.13 (31.2)** \\ IMP-50\% & 57.6 & 65.2 & 297.39 (51.3) \\ ProsPr-50\% & 62.5 & 68.4 & 285.79 (53.2) \\
**AlterMOMA-50\%** & **64.5** & **69.5** & **264.42 (56.7)** \\   

Table 5: **3D object detection performance of structure pruning on the nuScenes validation dataset.**

### Structure Pruning Results on 3D object Detection

To assess the efficacy of our proposed pruning approach in structure pruning, we conducted experiments with BEVfusion-mit models with ResNet101 as the camera backbone and SECOND as the LiDAR backbones. Specifically, We measured the performance of the pruned networks with a similar amount of FLOP reductions and reported the number of FLOPs('GFLOPs'). As depicted in Table 5, the results obtained from these experiments consistently demonstrate state-of-the-art performance with various pruning sparsities. Our evaluations at 30% and 50% pruning sparsities reveal that AlterMOMA not only maintains a competitive mAP and NDS but also achieves a substantial reduction in computational overhead. Notably, with the 30% pruning sparsities, AlterMOMA achieves a 0.7% on mAP and 0.5% on NDS compared with the unpruned baseline models, which reveals that removing similar feature redundancy improves the efficiency of models. Specifically, compared with the ProsPr baseline, AlterMOMA achieves a substantial enhancement by 1.1% (64.6% \(\) 65.3%), and 2.0% (62.5% \(\) 64.5%), for the two different pruning sparsities.

## 5 Discussion and Conclusion

Although our approach identifies similar feature redundancy in camera-LiDAR fusion models, it is limited to the perception field. Extending it to other multi-modal models, such as vision-language models, requires further research. Fusion modules across various modalities exhibit different functionalities. In multi-sensor fusion models (camera, LiDAR, and Radar), the focus is on supplementing and spatially aligning data by leveraging the sensors' physical properties, fusing low-level features. However, in models with disparate data types like vision and language, fusion modules focus on matching high-level semantic contexts. Therefore, AlterMOMA primarily addresses redundancy from supplementary functionality in multi-sensor fusion perception architectures.

In this paper, we explore the computation reduction of camera-LiDAR fusion models. A pruning framework AlterMOMA is introduced to address redundancy in these models. AlterMOMA employs alternative masking on each modality and observes loss changes when certain modality parameters are activated and deactivated. These observations are integral to our importance scores evaluation function AlterEva. Through extensive evaluation, our proposed framework AlterMOMA achieves better performance, surpassing the baselines established by single-modal pruning methods.