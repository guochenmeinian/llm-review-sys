# TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables

Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang,

**Yong Liu, Yunzhong Qiu, Jianmin Wang, Mingsheng Long**

School of Software, BNRist, Tsinghua University, Beijing 100084, China

{wangyuxu22,whx20,djx20,qinguo24,zhang-hr24,liuyong21,qiuyz24}@mails.tsinghua.edu.cn

{jimwang,mingsheng}@tsinghua.edu.cn

Equal Contribution

###### Abstract

Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called _endogenous variables_, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the _exogenous variables_ can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, **TimeXer**, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: [https://github.com/thuml/TimeXer](https://github.com/thuml/TimeXer).

## 1 Introduction

Time series forecasting is of pressing demand in real-world scenarios and have been widely used in various application domains, such as meteorology , electricity , and transportation . Thereof, forecasting with exogenous variables is a prevalent and indispensable forecasting paradigm since the variations within time series data are often influenced by external factors, such as economic indicators, demographic changes, and societal events. For example, electricity prices are highly dependent on the supply and demand of the market, and it is intrinsically impossible to predict future prices solely based on historical data. Incorporating external factors in terms of exogenous variables, as illustrated in Figure 1 (Left), allows for a more comprehensive understanding of the correlations and causalities among various variables, leading to better performance and interpretability.

From the perspective of time series modeling, exogenous variables are introduced to the forecaster for informative purposes and do not need to be predicted. The distinction between endogenous and exogenous variables poses unique challenges compared to existing multivariate forecasting methods. First, there are always multiple external factors that are illuminating to the prediction of the target series, which requires models to reconcile the discrepancy and dependency among _endogenousand _exogenous_ variables. Regarding exogenous variables equally with endogenous ones will not only cause significant time and memory complexity but also involve unnecessary interactions from endogenous series to external information. Second, external factors may have a causal effect on endogenous series, so models are expected to reason about the systematic time lags among different variables. Moreover, as a practical forecasting paradigm applied extensively in real scenarios, it is essential for models to tackle irregular and heterogeneous exogenous series, including value missing, temporal misalignment, frequency mismatch, and length discrepancy as showcased in Figure 1(Left).

Despite the success of deep learning models in capturing intricate temporal dependencies in time series data, the incorporation of exogenous variables remains underexplored. A common practice to import them is adding or concatenating exogenous features to the endogenous ones. However, given the crucial role of exogenous variables in forecasting, it is imperative to incorporate them precisely and properly. Recent Transformers  have exhibited remarkable performance in time series forecasting due to their capability of capturing both temporal dependencies and multivariate correlations. Based on the working dimensions of the attention mechanism, existing Transformer-based works can be roughly divided into patch-oriented models and variate-oriented models. Patching is a basic module to preserve the semantic information underlying temporal variations. Therefore, the attention mechanism is applied over patches to unearth the intricate temporal patterns. Based on the channel independence assumption, PatchTST and follow-ups  are capable of capturing temporal dependencies but weak at capturing multivariate correlations. In contrast, variate-oriented models represented by iTransformer  successfully reason about interrelationships between variables by considering each variate of time series as a single token and applying attention over multiple variate tokens. Unfortunately, they lack the ability to capture internal temporal variations since the whole series is embedded into a coarse variate token by a temporal linear projection.

To enable accurate forecasting with exogenous variables in real-world scenarios, it is indispensable to capture both the intra-endogenous temporal dependencies and inter-series correlations between endogenous and exogenous variables. Based on the above observations, we believe that modeling the temporal-wise and variate-wise dependencies within time series data requires hierarchical representations at different levels. In this paper, we unleash the potential of the canonical Transformer without modifying any component, and propose a **Time** Series Transformer with e**X**ogenous variables (**TimeXer**). Technologically, we leverage representations and perform attention mechanisms at both patch and variate levels. First, the endogenous patch-level tokens are applied to capture temporal dependencies. Second, to tackle the arbitrarily irregular exogenous variables, TimeXer adopts their variate-level representations to seamlessly ingest the impact of external factors on endogenous ones. Third, inspired by Vision Transformers , we introduce learnable global tokens for each endogenous series to reflect the macroscopic information of the series, which interact simultaneously with patch-level endogenous tokens and variate-level exogenous tokens. Throughout this information pathway, the external information can be propagated effectively and selectively to corresponding endogenous patches. In summary, our contributions can be listed as follows.

* Motivated by the universality and importance of exogenous variables in time series forecasting, we empower the canonical Transformer to simultaneously modeling exogenous and endogenous variables without any architectural modifications.
* We propose a simple and general TimeXer model, which employs patch-level and variate-level representations respectively for endogenous and exogenous variables, with an en

Figure 1: Left: The forecasting with exogenous variables paradigm includes inputs from multiple external variables as auxiliary information without the need for forecasting. Right: Model performance comparison on existing electricity price forecasting with exogenous variables benchmarks.

-ogenous global token as a bridge in-between. With this design, TimeXer can capture intra-endogenous temporal dependencies and exogenous-to-endogenous correlations jointly.
* Extensive experiments on twelve datasets show that TimeXer can better utilize exogenous information to facilitate endogenous forecasting, in both univariate and multivariate settings.

## 2 Related Work

### Transformer-based Time Series Forecaster

Motivated by the great success in the field of natural language processing  and computer vision , Transformers have garnered significant interest in time series data due to their ability to capture long-term temporal dependencies and complex multivariate correlations. Categorized based on the granularity of representation used in the attention mechanism, Transformer-based models can be divided into point-wise, patch-wise, and variate-wise. Due to the serial nature of time series, most previous works use a point-wise representation of time series data and apply attention mechanisms to capture the correlations among different time points. Therefore, many efficient Transformers  were proposed to reduce the complexity caused by point-wise modeling. Informer  designs a ProbSparse self-attention to reduce the quadratic complexity in time and memory. Autoorrner  replaces canonical self-attention with Auto-correlation to discover the sub-series similarity within time series data. Pyraformer  develops a pyramidal attention module to capture both short- and long-temporal dependencies with linear time and space complexity.

Considering point-wise representations fall short in revealing local semantic information lies in the temporal variation, PatchTST  split time series data into subseries-level patches and then capture dependencies between patches. Pathformer  utilizes multi-scale patch representations and performs dual attention over these patches to capture global correlations and local details as temporal dependencies. Recent large-scale time series models  have widely included patch-level representation to learn the complex temporal patterns. Beyond capturing the patch-level temporal dependencies within one single series, recent approaches have endeavored to capture interdependencies among patches from different variables over time. Crossformer  introduces a Two-Stage Attention layer to efficiently capture the cross-time and cross-variate dependencies of each patch. Further expanding the receptive field, iTransformer  utilizes the global representation of the whole series and applies attention to these series-wise representations to capture multivariate correlations. Yet, as shown in Table 1, most of the existing Transformer-based approaches only focus on multivariate or univariate time series forecasting paradigms and do not conduct special designs for exogenous variables, which is different from the scenario we studied in this paper.

### Forecasting with Exogenous Variables

Time series forecasting with exogenous variables has been widely discussed in classical statistical methods. A vast majority of statistical methods have been extended to include exogenous variables as part of input. Extending the well-acknowledged ARIMA model, ARIMAX  and SARIMAX  incorporate the correlations between exogenous and endogenous variables along with the autoregression on endogenous variables. Although time series modeling methods have evolved from statistical to deep models, most of the existing deep models incorporating covariates, such as Temporal Fusion Transformer (TFT) , primarily focus on variable selection. Some approaches, including NBEATSx  and TiDE  contend that forecasting models are capable of accessing future values of exogenous variables during the prediction of endogenous variables. It is notable that previous models concatenate exogenous features with endogenous features at each time point and then map them to a latent space, necessitating the alignment of the endogenous and exogenous

   Methods & **TimeXer** & iTran.  & PatchTST  & Cross.  & Auto.  & TFT  & NBEATSx  & TiDE  \\  Univariate & ✓ & ✗ & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ Multivariate & ✓ & ✓ & \(\) & ✓ & \(\) & ✗ & ✗ & ✓ \\ Exogenous & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of related methods with its forecasting capability. The character “.” in the Transformers denotes the name of *former. The character \(\) indicates that the model can be applied to multivariate forecasting scenarios but not explicitly model the cross-variate dependency.

series in time. However, time series in real-world applications often suffer from problems such as missing value and uneven sampling, which leads to significant challenges in modeling the effects of exogenous variables on endogenous variables. In contrast, TimeXer introduces external information to Transformer architecture through a deftly designed embedding strategy, which can effectively introduce the external information into patch-wise representations of endogenous variables, thereby being able to adapt to time-lagged or data-missing records.

## 3 TimeXer

In forecasting with exogenous variables, the endogenous series is the target to be predicted, while the exogenous series are covariates that provide valuable information to boost endogenous predictability.

Problem SettingsIn forecasting with exogenous variables, we are given an endogenous time series \(_{1:T}=\{x_{1},x_{2},...,x_{T}\}^{T 1}\) and multiple exogenous series \(_{1:T_{}}=\{_{1:T_{}}^{(1)},_{ 1:T_{}}^{(2)},...,_{1:T_{}}^{(C)}\}^{T _{} C}\). Here \(x_{i}\) denotes the value at the \(i\)-th time point, \(_{1:T_{}}^{(i)}\) represents the \(i\)-th exogenous variable, and \(C\) is the number of exogenous variables. In addition, \(T\) and \(T_{}\) are the look-back window lengths of the endogenous and exogenous variables respectively. Noteworthily, any series that provides useful information for endogenous forecasting can be used as an exogenous variable, regardless of their look-back lengths, so we relax to the most flexible settings with \(T_{} T\). The goal of forecasting model \(_{}\) parameterized by \(\) is to predict the future \(S\) time steps \(}=\{x_{T+1},x_{T+2},...,x_{T+S}\}\) based on both historical observations \(_{1:T}\) and corresponding exogenous series \(_{1:T_{}}\):

\[}_{T+1:T+S}=_{}(_{1:T}, _{1:T_{}}). \]

Structure OverviewAs shown in Figure 2, the proposed TimeXer model repurposes the canonical Transformer without modifying any component, while endogenous and exogenous variables are manipulated by different embedding strategies. TimeXer adopts self-attention and cross-attention to capture temporal-wise and variate-wise dependencies respectively.

Endogenous EmbeddingMost of the existing Transformer-based forecasting models embed each time point or a segment of time series as a temporal token and apply self-attention to learn temporal dependencies. To finely capture temporal variations within the endogenous variable, TimeXer adopts patch-wise representations. Concretely, the endogenous series is split into non-overlapping patches, and each patch is projected to a temporal token. Given the distinct roles of endogenous and exogenous variables in the prediction, TimeXer embeds them at _different_ granularity. Therefore, directly combining endogenous tokens and exogenous tokens at different granularity will result in information misalignment. To address this, we introduce a learnable global token for each endogenous variable that serves as the macroscopic representation to interact with exogenous variables. This design helps bridge the causal information from the exogenous series to the endogenous temporal patches. The overall endogenous embedding is formally stated as:

\[\{_{1},_{2},...,_{N} \}&=(),\\ _{}&= (_{1},_{2},...,_{N}),\\ _{}&= (). \]

Denote by \(P\) the patch length, by \(N=\) the number of patches split from the endogenous series, and by \(_{i}\) the \(i\)-th patch. \(()\) maps each length-\(P\) patch, added by its position embedding, into a \(D\)-dimensional vector via a trainable linear projector. In all, \(N\) patch-level temporal token embeddings \(_{}\) and \(1\) series-level global token embedding \(_{}\) are fed into the Transformer encoder.

Exogenous EmbeddingThe primary use of exogenous variables is to facilitate accurate forecasting of endogenous variables. We will show in Appendix B.3 that the interactions of different variables can be captured more naturally by variate-level representations, which are adaptive to arbitrary irregularities such as missing values, misaligned timestamps, different frequencies, or discrepant look-back lengths. In contrast, patch-level representations are overly fine-grained for exogenous variables, introducing not only significant computational complexity but also unnecessary noise information. These insights lead to a design that each exogenous series is embedded in a series-wise variate token, which is formalized as:

\[_{,i}=(^{(i)} ),\;i\{1,,C\}. \]Here \(:^{T_{}}^{D}\) is a trainable linear projector, \(T_{}\) is the look-back length of exogenous series, and \(_{}=\{_{,i}\}_{i=1}^{C}\) is the set of representations for multiple exogenous series.

Endogenous Self-AttentionFor accurate time series forecasting, it is vital to discover intrinsic temporal dependencies within the endogenous variable, as well as the interactions with the variate-level representations from exogenous variables. In addition to self-attention over endogenous temporal tokens (Patch-to-Patch), the learnable global token builds a _bridge_ between endogenous and exogenous variables. Concretely, the global token plays an asymmetric role in cross-attention: (1) Patch-to-Global: the global token attends to temporal tokens for aggregating patch-level information across the entire series; (2) Global-to-Patch: each temporal token attends to the global token for receiving the variate-level correlations. This provides a comprehensive view of the temporal dependencies within the endogenous variable, as well as better interactions with the arbitrarily irregular exogenous variables. The attention mechanism can be formalized as follows:

\[&}_{}^{I,1}=(_{}^{I }+(_{}^{I})),\\ &}_{ }^{I,2}=(_{}^{I}+(_{}^{I},_{}^{I} )),\\ &}_{ }^{I}=(_{}^{I}+(_{}^{I},_{}^{I})).  \]

The overall process can be simplified into an endogenous self-attention computation:

\[}_{}^{I},}_{}^{I}= ([_{}^{I},_{}^{I}]+([_{}^{I}, _{}^{I}])). \]

where \(l\{0,,L-1\}\) denotes the \(l\)-th TimeXer block, and \(_{}^{0}=_{}\), \(_{}^{0}=_{}\). Here, \([,,]\) denotes the concatenation of the patch-wise tokens and global token of the endogenous variable along the sequence dimension. By adopting a self-attention layer over the concatenated tokens \([_{}^{I},_{}^{I}]\) of the endogenous series, TimeXer can capture temporal dependencies between patches and the relationships between each patch to the entire series simultaneously.

Exogenous-to-Endogenous Cross-AttentionCross-attention has been widely used in multi-modal learning  to capture the adaptive token-wise dependencies between different modalities. In TimeXer, the cross-attention layer takes the endogenous variable as query and the exogenous variable as key and value to build the connections between the two types of variables. Since the exogenous variables are embedded into variate-level tokens, we use the learned global token of the endogenous variable to aggregate information from exogenous variables. The above process can be formalized as

\[}_{}^{I}=(}_{}^{I}+( }_{}^{I},_{})). \]

Figure 2: The schematic of TimeXer, which empowers time series forecasting with exogenous variables. (a) The endogenous embedding module yields multiple temporal token embeddings and one global token embedding for the endogenous variable. (b) The exogenous embedding module yields a variate token embedding for each exogenous variable. (c) Self-attention is applied simultaneously over the endogenous temporal tokens and the global token to capture patch-wise dependencies. (d) Cross-attention is applied over endogenous and exogenous variables to integrate external information.

Finally, all temporal tokens and the learnable global token will be transformed by the feedforward layer, which is formally stated as:

\[_{}^{l+1}=(}_{ {en}}^{l}),_{}^{l+1}=( {}_{}^{l}), \]

where \(l\{1,,L\}\). We write each Transformer block as \(_{}^{l+1},_{}^{l+1}=( _{}^{l},_{}^{l})\).

Forecasting LossIn time series forecasting with exogenous variables, the exogenous variables do not need to be predicted. So we generate the forecast \(}\) by applying a linear projection on the endogenous output embeddings \([_{}^{L},_{}^{L}]\), a common practice in the encoder-only forecasters. We employ the squared loss (L2) to measure the discrepancy between the prediction and the ground truth:

\[=_{i=1}^{S}_{i}-}_{i}_{2}^{2},\ \ \ \ \ }=[_{}^{L},_{}^{L}]. \]

Parallel Multivariate ForecastingMultivariate forecasting can be viewed as predicting each variable in the multivariate data, with the other variables treated as exogenous ones. So for each variable, the other variables are leveraged by TimeXer to facilitate more accurate and causal prediction. Our key discovery is that forecasting with exogenous variables can be a unified forecasting paradigm that generalizes straightforwardly to multivariate forecasting. By employing the _channel independence_ mechanism, for each variable of the multivariate, it is treated as the endogenous one. Then TimeXer is applied in a parallel manner for all variables with shared self-attention and cross-attention layers.

## 4 Experiments

To verify the effectiveness and generality of TimeXer, we extensively experiment under two different time series paradigms, _i.e._ short-term forecasting with exogenous variables and long-term multivariate forecasting, on a diverse range of real-world time series datasets from different domains. We also conduct experiments on long-term forecasting with exogenous variables on the multivariate benchmark, which are presented in Appendix I.3.

DatasetsFor short-term forecasting tasks, we include short-term electricity price forecasting datasets (EPF) , which is a real-world forecasting with exogenous various benchmarks derived from five major power market data spanning six years each. Each dataset contains electricity price as an endogenous variable and two influential exogenous variables in practice. Meanwhile, we adopt seven well-established public long-term multivariate forecasting benchmarks  to evaluate the performance of TimeXer in multivariate forecasting.

BaselinesWe include nine state-of-the-art deep forecasting models, including Transformer-based models: iTransformer , PatchTST , Crossformer , Autotformer , CNN-based models: TimesNet , SCINet , and linear-based models: RLinear , DLinear , TiDE . Notably, TiDE is a recently developed advanced forecaster specifically designed for exogenous variables.

Implementation DetailsFor short-term electricity price prediction, we follow the standard protocol of NBEATSx , where the input series length and prediction length are respectively set as 168 and 24. In addition, we set the patch length as 24 without overlapping. For long-term forecasting datasets, we uniformly use the patch length 16 and fix the length of the look-back series at 96, while the prediction length varies across four lengths \(\{96,192,336,720\}\).

### Main Results

Comprehensive forecasting results for short-term and long-term forecasting are listed in Table 2 and Table 3. A lower MSE or MAE indicates better forecasting performance.

The short-term electricity price forecasting task is derived from real-world scenarios, and presents a unique challenge for the forecasting model for the endogenous variable has been shown to be highly correlated with two exogenous variables in the dataset. Since the interactions between different variables are crucial for this task, linear forecasters, including RLinear  and DLinear , fail to triumph over Transformer-based forecasters. Similar to TimeXer, Crossformer divides all input series into different segments and captures multivariate correlations over all segments; However,it fails to outperform other baselines which indicates that modeling all variables at a granular level introduces unnecessary noise into the forecasting. Also designed for capturing cross-variate dependency, iTransformer neglects the temporal-wise attention module, indicating that there are still limitations in capturing temporal dependencies solely through linear projection. By contrast, our proposed TimeXer effectively integrates information from exogenous variables while capturing temporal dependencies of endogenous series. As shown in Table 2, TimeXer achieves consistent state-of-the-art performance on all five datasets, outperforming various baseline models.

We also evaluate TimeXer on well-established public benchmarks for conventional multivariate long-term forecasting. As mentioned above, TimeXer has the ability to perform multivariate forecasting by employing the channel independence mechanism. We present the results averaged from all four prediction lengths in Table 3. It can be observed that TimeXer achieves consistent state-of-the-art performance on most of the datasets, highlighting its effectiveness and generality. In addition, since TimeXer is initially designed for exogenous variables, we also conduct vanilla forecasting with exogenous variables on these datasets by taking the last dimension of the multivariate data as endogenous series and others as exogenous variables. Detailed results are listed in Appendix I.3.

### Ablation Study

In TimeXer, three types of tokens are used to capture temporal-wise and variate-wise dependencies, including multiple patch-level temporal tokens, learnable global tokens of the endogenous variables, and multiple variate-level exogenous tokens. Besides, to incorporate the information from exogenous variables, TimeXer adopts a cross-attention layer to model the mutual relationship between different variables. To elaborate on the validity of TimeXer, we conducted detailed ablations covering both the embedding module and the inclusion of exogenous factors. Specifically, for the embedding design, we replace or remove existing components of the embedded vector from exogenous and endogenous variables respectively. Moreover, we keep the existing embedding design and replace the cross-attention by adding the variate token of exogenous variables to the variate token of endogenous variables or concatenating all the variate tokens and temporal tokens. As listed in Table 4, TimeXer exhibits superior performance compared to various architectural designs across all datasets.

   Model &  & iTransformer & RLinear & PatchTST & Crossformer & TiDE & TimesNet & DLinear & SCINet & Autorformer \\ Metric & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\  NP & **0.236 0.268** & 0.265 & 0.300 & 0.335 & 0.340 & 0.267 & 0.284 & 0.240 & 0.285 & 0.335 & 0.340 & 0.250 & 0.289 & 0.309 & 0.321 & 0.373 & 0.368 & 0.402 & 0.398 \\  PIM & **0.093 0.192** & 0.097 & 0.197 & 0.124 & 0.229 & 0.106 & 0.209 & 0.101 & 0.199 & 0.124 & 0.228 & 0.097 & 0.195 & 0.108 & 0.215 & 0.143 & 0.259 & 0.168 & 0.267 \\  BE & **0.379 0.243** & 0.394 & 0.270 & 0.520 & 0.337 & 0.400 & 0.262 & 0.420 & 0.290 & 0.523 & 0.336 & 0.419 & 0.288 & 0.463 & 0.313 & 0.731 & 0.412 & 0.500 & 0.333 \\  FR & **0.385 0.208** & 0.439 & 0.233 & 0.507 & 0.290 & 0.411 & 0.220 & 0.434 & 0.208 & 0.510 & 0.290 & 0.431 & 0.234 & 0.429 & 0.260 & 0.855 & 0.384 & 0.519 & 0.295 \\  DE & **0.440 0.415** & 0.479 & 0.443 & 0.574 & 0.498 & 0.461 & 0.432 & 0.574 & 0.430 & 0.568 & 0.496 & 0.502 & 0.446 & 0.520 & 0.463 & 0.565 & 0.497 & 0.674 & 0.544 \\  AVG & **0.307 0.265** & 0.335 & 0.289 & 0.412 & 0.339 & 0.330 & 0.282 & 0.354 & 0.284 & 0.412 & 0.338 & 0.340 & 0.290 & 0.366 & 0.314 & 0.533 & 0.384 & 0.453 & 0.368 \\   

Table 2: Full results of the short-term forecasting task on EPF dataset. We follow the standard protocol in short-term electricity price forecasting, where the input length and predict length are set to 168 and 24 respectively for all baselines. Avg means the average results from all five datasets.

    &  & iTransformer & RLinear & PatchTST & Crossformer & TiDE & TimesNet & DLinear & SCINet & Autorformer \\ Metric & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\  ECL & **0.171 0.270** & 0.178 & 0.270 & 0.219 & 0.298 & 0.205 & 0.290 & 0.244 & 0.334 & 0.251 & 0.244 & 0.192 & 0.295 & 0.212 & 0.300 & 0.268 & 0.365 & 0.22

### TimeXer Generality

#### 4.3.1 Practical Situations

Increasing Look-back LengthTheoretically, the forecasting performance of the model could potentially benefit from increasing the look-back length of time series, as a longer historical context encompasses more comprehensive information. However, the attention will be distracted when the look-back length becomes excessively long. In TimeXer, we use the variate-level representation of exogenous variables which allows for the misalignment between endogenous and exogenous variables. This is particularly valuable in real-world scenarios where the time series data may be collected from a newly introduced sensor that has limited historical data. Therefore, we conducted three different experimental settings to assess the generality of TimeXer by increasing the length of either the endogenous or exogenous series, which include "Fix Endogenous and increase Exogenous", "Increase Endogenous and Fix exogenous", and "Increase Endogenous and Exogenous". Results shown in Figure 3 reveal that TimeXer can be adapted to situations where the look-back of endogenous and exogenous are mismatched. Moreover, extending the look-back length indeed yields improvements in forecasting performance. Compared to enlarging the historical exogenous series, increasing the look-back length of the endogenous series brings greater benefits to the model, and the performance is further improved with both increases.

Missing ValuesTo further verify the generalizability of TimeXer in complex real-world scenarios, we conduct experiments in scenarios where the historical information of time series is missing. Specifically, for both exogenous and endogenous series, we adopt two strategies to evaluate TimeXer's adaptability to series with missing historical information: (1) **Zeros**: filling the whole series with the scalar value 0. (2) **Random**: substituting the whole series with random values from a uniform distribution on the interval \([0,1)\). As shown in Table 5, the forecasting results deteriorate when exogenous variables are replaced with meaningless noise, indicating that the model's performance benefits from the inclusion of informative exogenous variables. Interestingly, neither using zero-filled exogenous series nor employing exogenous series with random numbers results in a significant

    &  &  &  &  &  &  &  &  \\   & & & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & **Ours** & P+G & V & **0.236** & **0.268** & **0.093** & **0.192** & 0.379 & **0.243** & **0.385** & **0.208** & **0.440** & **0.415** & **0.307** & **0.265** \\   & Replace & P+G & P & 0.237 & 0.269 & 0.101 & 0.196 & **0.376** & 0.246 & 0.390 & 0.206 & 0.457 & 0.422 & 0.312 & 0.268 \\   & Remove & P & V & 0.239 & 0.273 & 0.106 & 0.200 & 0.381 & 0.260 & 0.393 & 0.208 & 0.468 & 0.425 & 0.316 & 0.273 \\  Add & P+G & V & 0.247 & 0.272 & 0.125 & 0.206 & 0.387 & 0.247 & 0.404 & 0.209 & 0.483 & 0.430 & 0.329 & 0.273 \\  Concatenate & P+G & V & 0.237 & 0.266 & 0.098 & 0.196 & 0.383 & 0.255 & 0.390 & 0.209 & 0.450 & 0.423 & 0.312 & 0.270 \\   

Table 4: Ablation Results. _Ex._ and _En._ are abbreviations for Exogenous variable and Endogenous variable. \(P\), \(G\) and \(V\) denote patch token, learnable global token, and variate token respectively.

Figure 3: Performance with the enlarged look-back length varying from \(\{96,192,336,512,720\}\). Different styles of lines represent different prediction lengths. In most cases, the forecasting performance benefits from enlarged look-back lengths of both endogenous and exogenous series.

decline in model performance. This robustness can be attributed to TimeXer's design, which uses two attention layers to model endogenous temporal dependencies and the multivariate correlations between endogenous and exogenous variables respectively. This architecture allows endogenous temporal representations to dominate the predictions, ensuring consistent performance even in the presence of uninformative exogenous data. Consequently, it can be observed that when the endogenous series is replaced with meaningless zeros or random values, rendering the time series unpredictable, there is a significant decline in model performance. This underscores that TimeXer's performance is closely tied to the quality of endogenous series, deteriorating markedly when the historical information is severely limited.

#### 4.3.2 Scalability

Since recent Transformer-based forecasters have demonstrated promising scalability, leading to the success of Large Time Series Models, we explore the scalability of TimeXer on large-scale time series data. Specifically, we build a large-scale weather dataset for forecasting with exogenous variables. The endogenous series is the hourly temperature of 3,850 stations worldwide, spanning from January 1, 2019, to December 31, 2020, which can be downloaded from the National Centers for Environmental Information (NCBI)  and has been well-processed by . Further, we utilize meteorological indicators of corresponding adjacent areas from ERA5  as exogenous variables, which is with a sampling interval of 3 hours. The adjacent area is defined as the 3x3 grid centered on the endogenous weather station, with four meteorological variables per grid cell, totaling 36 exogenous variables. We set the historical horizon of endogenous and exogenous to be 7 days to predict the endogenous variable for the next 3 days. Noteworthily, this is a complex forecasting scenario as we aforementioned where the frequencies of endogenous and exogenous are different. We choose existing state-of-the-art multivariate forecasters as baselines and use identical hidden dimensions and batch sizes for a fair comparison. Since baseline forecasters cannot handle mismatched series, we interpolate the exogenous series into hourly data using the nearest values. Figure 4 demonstrates that TimeXer surpasses other baselines, verifying its capability to handle large-scale forecasting tasks.

Figure 4: Forecasting performance on large-scale time series datasets. Left: Illustration of the forecasting scenario. The endogenous is the temperature collected from weather stations, and the exogenous variables are meteorological indicators from the surrounding 3x3 grids including the weather station. Each area contains four types of information, namely, temperature, pressure, u- and v- components of wind. Right: TimeXer outperforms other advanced forecasters.

    &  &  &  &  &  &  &  \\   & & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & Zeros & 2.954 & 1.396 & 0.188 & 0.288 & 0.930 & 0.664 & 0.781 & 0.534 & 0.774 & 0.559 & 1.125 & 0.688 \\   & Random & 3.140 & 1.450 & 0.233 & 0.325 & 0.926 & 0.667 & 0.761 & 0.527 & 0.692 & 0.533 & 1.150 & 0.701 \\   & Zeros & 0.257 & 0.278 & 0.108 & 0.210 & 0.400 & 0.254 & 0.416 & 0.214 & 0.471 & 0.430 & 0.330 & 0.277 \\   & Random & 0.258 & 0.280 & 0.110 & 0.212 & 0.399 & 0.253 & 0.424 & 0.221 & 0.475 & 0.432 & 0.333 & 0.280 \\   & **0.236** & **0.268** & **0.093** & **0.192** & 0.379 & **0.243** & **0.385** & **0.208** & **0.440** & **0.415** & **0.307** & **0.265** \\   

Table 5: Model performance under missing values. _Zeros_ and _Random_ represent the cases that the corresponding series is set as zeros or random values respectively.

### Model Analysis

Variate-wise CorrelationsTimeXer adopts cross-attention between the global endogenous token and variable-level exogenous tokens to capture the multivariate correlation, enhancing the interpretability of the learned attention map. To validate the rationale behind attention on variate tokens, we visualize the learned attention map alongside the time series of the highest and lowest attention scores. As illustrated in Figure 5 (Left), the case study on the Weather dataset reveals a notable distinction in the attention maps of endogenous variables with different exogenous variables. This demonstrates that TimeXer has the ability to distinguish between exogenous variables, allocating greater attention to those that are most informative for prediction, thereby resulting in a more focused and interpretable attention map. Additionally, it is observed that exogenous series exhibiting similar shapes to the endogenous series tend to receive more attention. This phenomenon may arise because time series with analogous shapes often share temporal features, leading to higher similarity scores. Consequently, the exogenous series most prominently highlighted by the attention mechanism may intuitively resemble the endogenous variable. Furthermore, physical interpretations for the visualized are provided. For the endogenous variable CO2-Concentration, there is indeed a strong correlation between it and Air Density, while the Maximum Wind Velocity has a relatively minor impact, which validates the effectiveness of TimeXer.

Model EfficiencyTo evaluate the efficiency of TimeXer, we evaluate the training time and memory footprint of TimeXer on forecasting with exogenous variables compared with six baseline models with the identical hidden dimension and batch size for a fair comparison. We present the results on the ECL dataset with 320 exogenous variables in Figure 5 (Right). It is notable that when faced with numerous variables TimeXer exhibits its advantage by outperforming iTransformer in terms of memory footprint. Notably, iTransformer embeds each variate series into one token and applies a self-attention mechanism among all variate tokens, whether endogenous or exogenous. Although this design can keep refining the learned variate token in multiple layers, it does cause more complexity. As for TimeXer, exogenous variables will be embedded to variate tokens at the beginning, which will be shared in all layers and interact with the endogenous global token by cross-attention. Thus, TimeXer omits the interaction among learned exogenous variate tokens, resulting in favorable efficiency. We provide a comprehensive theoretical analysis of the model efficiency in Appendix E.

## 5 Conclusion

Considering the prevalence of exogenous variables in real-world forecasting scenarios, we empower the canonical Transformer architecture with the ability to incorporate exogenous information without architectural modifications. Technologically, TimeXer revisits the attention mechanism in a per-patch-per-variate manner to capture both endogenous temporal dependencies and multivariate correlations between endogenous and exogenous variables. With a deftly designed global token, our proposed TimeXer is able to reconcile variables of different purposes. Experimental results demonstrate that our proposed TimeXer effectively ingests exogenous information to facilitate the prediction of endogenous series, in both univariate and multivariate settings. Besides, TimeXer has shown the potential scalability and promising abilities to address complex real-world forecasting scenarios, including challenges such as value missing, temporal misalignment, or series heterogeneity.

Figure 5: Model analysis of TimeXer. Left: Visualization of learned attention map and the endogenous time series and exogenous time series with highest and lowest attention scores. Right: Model efficiency comparison under the forecasting with exogenous variables paradigm on the ECL dataset.