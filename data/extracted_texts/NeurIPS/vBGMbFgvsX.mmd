# Going Beyond Heuristics by Imposing Policy Improvement as a Constraint

Chi-Chang Lee\({}^{1}\)

\({}^{*}\)indicates equal contribution. \({}^{1}\) National Taiwan University, Taiwan. \({}^{2}\) Improbable AI Lab, MIT, Cambridge, USA.

 Zhang-Wei Hong\({}^{2}\)

\({}^{*}\)indicates equal contribution. \({}^{1}\) National Taiwan University, Taiwan. \({}^{2}\) Improbable AI Lab, MIT, Cambridge, USA.

 Pulkit Agrawal\({}^{2}\)

\({}^{1}\)Institute of Informatics, Vienna University, Vienna, Austria.

###### Abstract

In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance. Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms. However, such hints may not be optimal, limiting the performance of learned policies. The currently established way of using heuristics is to modify the heuristic reward in a manner that ensures that the optimal policy learned with it remains the same as the optimal policy for the task reward (i.e., optimal policy invariance). However, these methods often fail in practical scenarios with limited training data. We found that while optimal policy invariance ensures convergence to the best policy based on task rewards, it doesn't guarantee better performance than policies trained with biased heuristics under a finite data regime, which is impractical. In this paper, we introduce a new principle tailored for finite data settings. Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy. As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward. Our experiments on robotic locomotion, helicopter control, and manipulation tasks demonstrate that our method consistently outperforms the heuristic policy, regardless of the heuristic rewards' quality. Code is available at https://github.com/Improbable-AI/hepo.

## 1 Introduction

Reinforcement learning (RL)  is a powerful framework for learning policies that can surpass human performance in complex tasks. However, training RL policies with sparse or delayed rewards is often ineffective. Instead of relying solely on sparse task rewards that indicate an agent's success or failure, it is common to augment the sparse reward with _heuristic_ reward terms that provide denser reward supervision to speed up and improve the performance of RL policies . Shining examples of the success and necessity of heuristic reward terms are complex robotic object manipulation  and locomotion  tasks. However, heuristics impose human assumptions that may limit the RL algorithm. For example, a heuristic reward function may encourage a robot to walk like a human, yet there could be faster walking policies that don't resemble human gait.

The key question is how to learn a policy \(\) that outperforms one trained solely on heuristics (i.e., heuristic policy \(_{H}\)). Practitioners tackle this problem by tuning the balance between the task objective \(J()\) and the heuristic objective \(H()\) in the augmented training objective \(J()+ H()\), where \(\) controls the balance between the two objectives for a policy \(\). However, it requires careful tuning of \(\) to make the policy \(\) outperform the heuristic policy; otherwise, the algorithm might prioritize heuristic rewards while neglecting the task objective.

Tuning \(\) to balance both objectives is time-consuming. We desire an algorithm that finds a policy that outperforms the heuristic policy without requiring such tuning for any given heuristic reward function. Classic methods [2; 9; 10; 11; 12; 13; 14] modify heuristic rewards to align the augmented objective's optimal policy with the one for the task objective (i.e., optimal policy invariance), theoretically ensuring that with infinite data the policy outperforms the heuristic. However, in practice, these modified heuristics often fall short on complex robotic tasks compared to policies trained solely on heuristic objectives, as demonstrated in our study (Section 4.1) and prior work .

In this paper, we challenge the prevailing paradigm by questioning whether optimal policy invariance is the appropriate objective to prevent heuristics from limiting RL agent's performance in the finite data regime. As optimal policy invariance ensures convergence to the optimal policy with infinite data it may not be practical in many real-world settings. We propose an alternative paradigm that, in every step of training, imposes the constraint of improving task performance beyond a policy trained solely on heuristic rewards (i.e., \(J() J(_{H})\)). This condition \(J() J(_{H})\) guarantees that the learned policy \(\) outperforms the heuristic policy \(_{H}\), effectively surpassing human-designed heuristics. Additionally, such policy improvements can be verified and achieved using many existing deep RL algorithms [15; 16; 17; 18] in finite data settings.

Therefore, we enforce the policy improvement condition \(J() J(_{H})\) as a constraint, preventing the policy from exploiting heuristic rewards during training. We propose the following constrained optimization objective:

\[_{}J()+H() J() J(_{H})\]

Optimizing this objective at each iteration allows learning a policy performing better than or equal to policies trained only on heuristic rewards. It prevents capitalizing on heuristic rewards at the expense of task rewards. Moreover, it enables adaptively balancing both rewards over time instead of using a fixed coefficient \(\). Our contribution is an add-on to existing deep RL algorithms to improve RL algorithms trained with heuristic rewards. We evaluated our method on robotic locomotion, helicopter, and manipulation tasks using the IsaacGym simulator . The results show that our method led to superior task rewards and higher task-completion success rates compared to the policies solely trained with heuristic rewards, even when heuristic rewards are ill-designed.

## 2 Preliminaries: Reinforcement Learning with Heuristic

Reinforcement Learning (RL):RL is a popular paradigm for solving sequential decision-making problems  where the problems are modeled as an interaction between an agent and an unknown environment . The agent aims to improve its performance through repeated interactions with the environment. At each round of interaction, the agent starts from the environment's initial state \(s_{0}\) and samples the corresponding trajectory. At each timestep \(t\) within that trajectory, the agent perceives the state \(s_{t}\), takes an action \(a_{t}(.|s_{t})\) according to its policy \(\), receives a _task_ reward \(r_{t}=r(s_{t},a_{t})\), and transitions to a next state \(s_{t+1}\) until reaching terminal states, after which a new trajectory is initialized from \(s_{0}\), and the cycle repeats. The agent's goal is to learn a policy \(\) that maximizes the expected return \(J()\) in a trajectory as below:

\[J()=_{}[_{t=0}^{}^{t}r(s_{t},a_{t})],\] (1)

where \(\) denotes a discount factor  and \(_{}[.]\) denotes taking expectation over the trajectories sampled by \(\). In the following, we term \(J\) as the true _task_ objective, as it indicates the performance of a policy on the task.

RL with Heuristic:In many tasks, learning a policy to maximize the true objective \(J\) is challenging because rewards may be sparse or delayed. This lack of feedback makes policy optimization difficult for RL algorithms. To address this, practitioners often use a heuristic reward function \(h\) with denser reward signals to facilitate optimization, aiming to learn a policy that performs better in \(J\). The policy trained to maximize the expected return of heuristic rewards is called the _heuristic_ policy \(_{H}\). The expected return of heuristic rewards, termed the _heuristic_ objective \(H\), is defined as:

\[H(_{H})=_{_{H}}[_{t=0}^{}^{t}h(s_{t},a_{t} )],\] (2)where \(h(s_{t},a_{t})\) is the heuristic reward at timestep \(t\) for state \(s_{t}\) and action \(a_{t}\).

## 3 Method: Improving Heuristic Policy via Constrained Optimization

Problem statement:Optimizing both task \(J\) and heuristic \(H\) objectives jointly could lead to better task performance than training solely with \(J\) or \(H\), but needs careful tuning on the weight coefficient \(\) among both objectives in \(_{}J()+ H()\). Without careful tuning, the policy \(\) may learn to exploit heuristic rewards \(H\) and compromise performance of \(J\). The goal of this paper is to mitigate the requirement of tuning this coefficient to balance them in order to improve task performance.

Key insight - Leveraging Heuristic with Constraint:We aim to use the heuristic objective \(H\) for training only when it improves task performance \(J\) and ignore it otherwise. Rather than manually tuning the weight coefficient \(\) to balance both rewards, we introduce a key insight: impose a _policy improvement_ constraint (i.e., \(J() J(_{H})\)) during training. This prevents RL algorithms from exploiting heuristic rewards \(H\) at the expense of task rewards \(J\). To achieve this goal, we introduce the following constrained optimization objective:

\[_{}J()+H()J() J(_{H}).\] (3)

This constrained objective (Equation 3) results in an improved policy \(\) over the heuristic policy \(_{H}\), leading us to call this framework _Heuristic-Enhanced Policy Optimization (HEPO)_. A practical algorithm to optimize this objective is presented in Section 3.1, and its implementation on a widely-used RL algorithm  in robotics is detailed in Section 3.2.

### Algorithm: Heuristic-Enhanced Policy Optimization (HEPO)

Finding feasible solutions for the constrained optimization problem in Equation 3 is challenging due to the nonlinearity of the objective function \(J\) with respect to \(\). One practical approach is to convert it into the following unconstrained min-max optimization problem using Lagrangian duality:

\[_{ 0}_{}(,),(,):=J()+H()+(J()-J(_{}) ),\] (4)

where the Lagrangian multiplier is \(^{+}\). We can optimize the policy \(\) and the multiplier \(\) for this min-max problem by a gradient descent-ascent strategy, alternating between optimizing \(\) and \(\).

Enhanced policy \(\):The optimization objective for the policy \(\) can be obtained by rearranging Equation 4 as follows:

\[&_{}\ (1+)J()+H(),\\ &(1+)J()+H()=_{}[_{t =0}^{}^{t}(1+)r(s_{t},a_{t})+h(s_{t},a_{t}) ].\] (5)

This represents an unconstrained regular RL objective with the modified reward at each step as \((1+)r(s_{t},a_{t})+h(s_{t},a_{t})\), which can be optimized using any off-the-shelf deep RL algorithm. In this modified reward, the task reward \(r(s_{t},a_{t})\) is weighted by the Lagrangian multiplier \(\), reflecting the potential variation in the task reward's importance during training as \(\) evolves. The interaction between the update of the Lagrangian multiplier and the policy will be elaborated upon next.

Lagrangian Multiplier \(\):The Lagrangian multiplier \(\) is optimized for Equation 4 by stochastic gradient descent, with the gradient defined as:

\[_{}(,)=J()-J(_{}).\] (6)

Notably, \(_{}(,)\) is exactly the performance gain of the policy \(\) over the heuristic policy \(_{H}\) on the task objective \(J\). By applying gradient descent with \(_{}(,)\), when \(J()>J(_{H})\) and thus \(_{}(,)>0\), the Lagrangian multiplier \(\) decreases. As \(\) represents the weight of the task reward in Equation 5, it indicates that when \(\) outperforms \(_{H}\), the importance of the task reward diminishes because \(\) already achieves superior performance compared to the heuristic policy \(_{H}\) regarding the task objective \(J\). Conversely, when \(J()<J(_{H})\), \(\) increases, thereby emphasizing the importance of task rewards in optimization. The update procedure for the Lagrangian multiplier \(\) offers an adaptive reconciliation between the heuristic reward \(h\) and the task reward \(r\).

### Implementation

We present a practical approach to optimize the min-max problem in Equation 4 using Proximal Policy Optimization (PPO) . We selected PPO because it is widely used in robotic applications involving heuristic rewards, although our HEPO framework is not restricted to PPO. The standard PPO implementation involves iterative stochastic gradient descent updates over numerous iterations, alternating between collecting trajectories with policies and updating those policies. We outline the optimization process for each iteration and provide a summary of our implementation in Algorithm 1.

**Training policies \(\) and \(_{H}\):** Instead of pre-training the heuristic policy \(_{H}\), which requires additional data and reduces data efficiency, we concurrently train both the enhanced policy \(\) and the heuristic policy \(_{H}\), allowing them to share data. For each iteration \(i\), we gather trajectories \(\) and \(_{H}\) using the enhanced policy \(^{}\) and the heuristic policy \(^{i}_{H}\), respectively. Following PPO's implementation, we compute the advantages \(A_{r}^{^{i}}(s_{t},a_{t})\), \(A_{r}^{^{i}_{H}}(s_{t},a_{t})\), \(A_{h}^{^{i}_{H}}(s_{t},a_{t})\), and \(A_{h}^{^{i}_{H}}(s_{t},a_{t})\) for the task reward \(r\) and heuristic reward \(h\) with respect to \(^{i}\) and \(^{i}_{H}\). We then weight the advantage with the action probability ratio between the new policies being optimized (i.e., \(^{i+1}\) and \(^{i+1}_{H}\)) and the policies collecting the trajectories (i.e., \(^{i}\) or \(^{i}_{H}\)). Finally, we optimize the policies at the next iteration \(i+1\) for the objectives in Equations 7 and 8:

\[^{i+1}*{arg\,max}_{} _{^{i}}[|s_{t})}{^{i} (a_{t}|s_{t})}((1+)A_{r}^{^{i}}(s_{t},a_{t})+A_{h}^{^{i}}(s_{ t},a_{t}))]+\] (7) \[_{_{H}^{i}_{H}}[|s_{t})} {^{i}_{H}(a_{t}|s_{t})}((1+)A_{r}^{^{i}_{H}}(s_{t},a_{t})+A_{ h}^{^{i}_{H}}(s_{t},a_{t})+)]\] (Enhanced policy) \[^{i+1}_{H}*{arg\,max}_{} _{_{H}^{i}_{H}}[|s_{t})}{^{i}_{H }(a_{t}|s_{t})}A_{h}^{^{i}}(s_{t},a_{t})]+\] (8) \[_{^{i}}[|s_{t})}{^{i} (a_{t}|s_{t})}A_{h}^{^{i}_{H}}(s_{t},a_{t})]\] (Heuristic policy).

Maximizing the advantages will result in a policy that maximizes the expected return for a chosen reward function, as demonstrated in PPO . This enables us to maximize the objective \(J\) and \(H\). We estimate the advantages \(A_{r}^{^{i}}(s_{t},a_{t})\) and \(A_{h}^{^{i}}(s_{t},a_{t})\) (or \(A_{r}^{^{i}_{H}}(s_{t},a_{t})\) and \(A_{h}^{^{i}_{H}}(s_{t},a_{t})\)) using the standard PPO implementation with different reward functions. Therefore, we omit the details of the advantage's clipped surrogate objective in PPO, and leave them in Appendix A.1.

Although PPO is an on-policy algorithm, the use of off-policy importance ratio correction (i.e., the action probability ratios between two policies) allows us to use states and actions generated by another policy. This enables us to train \(\) using data from \(_{H}\) and vice versa. Both policies \(\) and \(_{H}\) are trained using the same data but with different reward functions. Note that collecting trajectories from both policies does not require more data than the standard PPO implementation. We collect half the trajectories with each policy, \(\) and \(_{H}\), for a total of \(B\) trajectories (see Algorithm 1). Then, we update both \(\) and \(_{H}\) using all \(B\) trajectories.

**Optimizing the Lagrangian multiplier \(\):** To update the Lagrangian multiplier \(\), we need to compute the gradient in Equation 6, which corresponds to the performance gain of the enhanced policy \(\) over the heuristic policy \(_{H}\) on the task objective \(J\). Utilizing the performance difference lemma , we relate this improvement to the expected advantages over trajectories sampled by the enhanced policy \(\) as \(J()-J(_{H})=_{}[A^{_{H}}r(s_{t},a_{t})]\). However, this approach only utilizes half of the trajectories at each iteration since it exclusively relies on trajectories from the enhanced policy \(\). To leverage trajectories from both policies, we also consider the performance gain in the reverse direction as \(-(J(_{H})-J())=-_{_{H}}[A^{_{H}}r(s_{t},a_{t})]\). Consequently, we can estimate the gradient of \(\) using trajectories from both policies, as illustrated below:

\[_{}(,) =J()-J(_{H})=_{}[A_{r}^{_{H}}(s_{t},a_{t })]\] (9) \[=-(J(_{H})-J())=-_{_{H}}[A_{r}^{}(s_{t },a_{t})].\] (10)

At each iteration \(i\), we estimate the gradient of \(\) using the advantage \(A_{r}^{_{H}}(s_{t},a_{t})\) and \(A_{r}^{}(s_{t},a_{t})\) on the trajectories sampled from both \(^{i}\) and \(^{i}_{H}\), and update \(\) with stochastic gradient descent as follows:

\[-(_{^{i}}[A _{r}^{^{i}}(s_{t},a_{t})]-_{^{i}_{H}}[A_{r}^{ ^{i}}(s_{t},a_{t})]),\] (11)where \(^{+}\) is the step size. The expected advantage in Equation 11 are estimated using the generalized advantage estimator (GAE) .

```
1:Input: Number of trajectories per iteration \(B\)
2:Initialize the enhanced policy \(^{0}\), the heuristic policy \(^{0}_{H}\), and the Lagrangian multiplier \(\)
3:for\(i=0\)do\(\)\(i\) denotes iteration index
4: Rollout \(B/2\) trajectories \(\) by \(^{i}\)
5: Rollout \(B/2\) trajectories \(_{H}\) by \(^{i}_{H}\)
6:\(^{i+1}\) Train the policy \(^{i}\) for optimizing Equation 7 using both \(\) and \(_{H}\)
7:\(^{i+1}_{H}\) Train the policy \(^{i}_{H}\) for optimizing Equation 8 using both \(\) and \(_{H}\)
8:\(\) Update the Lagrangian multiplier \(\) by gradient descent (Equation 9) using \(\) and \(_{H}\)
9:endfor ```

**Algorithm 1** Heuristic-Enhanced Policy Optimization (HEPO)

### Connection to Extrinsic-Intrinsic Policy Optimization 

Closely related to our HEPO framework, Chen et al.  proposes Extrinsic-Intrinsic Policy Optimization (EIPO), which trains a policy to maximize both task rewards and exploration bonuses  subject to the constraint that the learned policy \(\) must outperform the _task_ policy \(_{J}\) trained solely on task rewards. HEPO and EIPO differ in their objective functions and implementation of the constrained optimization problem. Additional information can be found in the Appendix, covering the objective formulation (Appendix A.1), implementation tricks (Appendix A.2), and detailed pseudocode (Appendix A.3).

Exploration bonuses  can be viewed as heuristic rewards. The main difference between HEPO and EIPO's optimization objectives lies in constraint design. Both frameworks require the learned policy \(\) to outperform a reference policy \(_{}\) (i.e., \(J() J(_{})\)) but use a different reference policy. EIPO uses the task policy \(_{J}\) as the reference policy \(_{}\) because they aim for asymptotic optimality in task rewards. If the constraint is satisfied with \(_{J}\) being the optimal policy for task rewards, the learned policy \(\) will also be optimal for task rewards. In contrast, HEPO uses the heuristic policy \(_{H}\) trained solely on heuristic rewards since HEPO aims to improve upon it.

HEPO simplifies the implementation. Both HEPO and EIPO train two policies with shared data, but EIPO alternates the policy used for trajectory collection each iteration and has a complex switching rule, which introduces more hyperparameters. HEPO collects trajectories using both policies together at each iteration, simplifying implementation and avoiding extra hyperparameters.

## 4 Experiments

We evaluate whether HEPO enhances the performance of RL algorithms in maximizing task rewards while training with heuristic rewards. We conduct experiments on 9 tasks from IsaacGym (Isaac)  and 20 tasks from the Bidexterous Manipulation (Bi-Dex) benchmark . These tasks rely on heavily engineered reward functions for training RL algorithms. Each task has a task reward function \(r\) that defines the task objective \(J\) to be maximized, and a heuristic reward function \(h\) that defines the heuristic objective \(H\), provided in the benchmarks to facilitate the optimization of task objectives \(J\). We implement HEPO based on PPO  and compare it with the following baselines:

* **H-only (heuristic only)**: This is the standard PPO baseline provided in Isaac. The policy is trained solely using the heuristic reward: \(_{}H()\). The heuristic reward functions in Isaac and Bi-Dex are designed to help RL algorithms maximize the task objective \(J\). This baseline is crucial to determine if an algorithm can surpass a policy trained with highly engineered heuristic rewards.
* **J-only (task only)**: The policy is trained using only the task reward: \(_{}J()\). This baseline demonstrates the performance achievable without heuristics. Ideally, algorithms that incorporate heuristics should outperform this baseline.

* **J+H (mixture of task and heuristic)**: The policy is trained using a mixture of task and heuristic rewards: \(_{}J()+ H()\), with \(\) balancing the two rewards. As  shows, proper tuning of \(\) can enhance task performance by balancing both training objectives.
* **Potential-based Reward Shaping (PBRS) **: The policy is trained to maximize \(_{}[_{t=0}^{}^{t}r_{t}+ h_{t+1}-h_{t}]\), where \(r_{t}\) and \(h_{t}\) are the task and heuristic rewards at timestep \(t\). PBRS guarantees that the optimal policy is invariant to the task reward function. We include it as a baseline to examine if these theoretical guarantees hold in practice.
* **HuRL **: The policy is trained to maximize \(_{}[_{t=0}^{}^{t}r_{t}+(1-_{i}) h_{t+ 1}]\), where \(_{i}\) is a coefficient updated at each iteration to balance heuristic rewards during different training stages. The scheduling mechanism is detailed in  and our source code provided in the Supplementary Material.
* **EIPO **: The policy is trained using the constrained objective: \(_{}J()+H()\) s.t. \(J() J(_{J})\), where \(_{J}\) is the policy trained with task rewards only. EIPO is similar to HEPO but differs in formulation and implementation, as detailed in Section 4.3.

Each method is trained for \(5\) random seeds and implemented based on the open-sourced implementation , where the detailed training hyperparameters can be found in Appendix A.4.

**Metrics:** Based on the task success criteria in Isaac and Bi-Dex, we consider two types of task reward functions \(r\): (i) Progressing (for locomotion or helicopter robots) and (ii) Goal-reaching (for manipulation). In progressing tasks, robots aim to maximize their traveling distance or velocity from an initial point to a destination. Thus, movement progress is defined as the task reward. In goal-reaching tasks, robots aim to complete assigned goals by reaching specific goal states. Here, task rewards are binary, with a value of \(1\) indicating successful attainment of the goal and \(0\) otherwise. Detailed descriptions of our task objectives and total reward definitions are provided in Appendix C.

### Benchmark results

**Setup:** We aim to determine if HEPO achieves higher task returns and improves upon the policy trained with only heuristic rewards (H-only) in the majority of tasks. In this experiment, we use the heuristic reward functions from the Isaac and Bi-Dex benchmarks. To measure performance improvement over the heuristic policy, we normalize the return of each algorithm \(X\) using the formula \((J_{X}-J_{})/(J_{}-J_{})\), where \(J_{X}\), \(J_{}\), and \(J_{}\) denote the task returns of algorithm \(X\), the heuristic policy, and the random policy, respectively. In Figure 1, we present the interquartile mean (IQM) of the normalized return and the probability of improvement for each method across 29 tasks, following . IQM, also known as the 25% trimmed mean, is a robust estimate against outliers. It discards the bottom and top 25% of runs and calculates the mean score of the remaining 50%. The probability of improvement measures whether an algorithm performs better than another, regardless of the margin of improvement. Both approaches prevent outliers from dominating the performance estimate.

**Results:** The results in Figure 1 indicate that policies trained with task rewards only (J-only) generally perform worse than those trained with heuristics, both in terms of IQM of normalized return and probability of improvement. PBRS does not improve upon J-only, demonstrating that the optimal policy invariance guarantee rarely holds in practice. Both EIPO and HuRL outperform J-only but do not surpass H-only, demonstrating that neither approach can improve upon the heuristic policy. Policies trained with both task and heuristic rewards (J+H) perform slightly worse than those trained with heuristics only (H-only), possibly because the weight coefficient balancing both rewards is too task-sensitive to work across all tasks. HEPO, however, outperforms all other methods in both IQM of normalized returns and shows a probability of improvement over the heuristic policy greater than 50%, indicating statistically significant improvements as suggested by Agarwal et al. . Complete learning curves are presented in the Appendix B.1. Additional results on various benchmarks and RL algorithms are provided in Appendix B.3, demonstrating that HEPO is effective in hard-exploration tasks using exploration bonuses  and with RL algorithms beyond PPO.

### Can HEPO be robust to reward functions designed in the wild?

**Setup:** We envision to develop an RL algorithm that can effectively utilize heuristic reward functions, thereby reducing the time costs associated with reward design. We simulate reward design scenarios

[MISSING_PAGE_FAIL:7]

ized returns below \(1\), while HEPO achieves returns greater than or close to \(1\). Since returns are normalized using the performance of the PPO policy trained with the well-designed heuristic reward function in Isaac, a return below \(1.0\) indicates a performance drop for PPO (H-only) when using potentially ill-designed heuristic rewards. In contrast, HEPO can improve upon policies trained with carefully engineered heuristic reward functions, even when trained with possibly ill-designed heuristic reward functions.

**Qualitative observation:** We aim to understand why PPO's performance declines when trained with heuristic reward functions \(H2\), \(H5\), and \(H6\). These functions are similar to the original heuristic reward in _FrankaCabinet_, but with different weights for each term. For example, in \(H5\), the weight of action penalty is \(1\), whereas in the original heuristic reward function it is \(7.5\). This suggests that HEPO might handle poorly scaled heuristic reward terms better than PPO, which is sensitive to these weights. The heuristic reward functions \(H12\) and \(H9\) had an incorrect sign for the distance component, which caused the policy to be rewarded for moving away from the cabinet instead of toward it, making the learning task more challenging.

### Ablation Studies

Expanding on the discussion of relation to relevant work EIPO  in Section 3.3, our goal is to examine the implementation choices of HEPO and illustrate the efficacy of each modification in this section. HEPO differs from EIPO primarily in two aspects: (1) the selection of a reference policy \(_{}\) in the constraint \(J() J(_{})\), and (2) the strategy for utilizing policies to gather trajectories. Both studies are conducted on standard locomotion and manipulation tasks, such as _Ant_, _FrankaCabinet_, and _AllegroHand_. In addition, we provide further studies on the sensitivity to hyperparameters in Appendix B.2.

**Selection of reference policy in constraint:** HEPO and EIPO both enforce a performance improvement constraint \(J() J(_{})\) during training. HEPO uses a heuristic policy \(_{H}\) as the reference (\(_{}=\)), while EIPO uses a task-only policy (\(_{}=\)). However, relying solely on policies trained with task rewards as references may not suffice for complex robotic tasks, as they often perform much worse than those trained with heuristic rewards. We compared the performance of HEPO with different reference policies in Figure 2(a). The result shows that setting \(_{}=\) (EIPO) improves the performance over the task-only policy _J-only_ while notably degrading performance, sometimes even worse than _H-only_, suggesting it's insufficient for surpassing the heuristic policy.

Strategy of collecting trajectories:We use both the enhanced policy \(\) and the heuristic policy \(_{H}\) simultaneously to sample half of the environment's trajectories (referred to as _Joint_). Conversely, EIPO switches between \(\) and \(_{H}\) using a specified mechanism, where only one selected policy samples trajectories for updating both \(\) and \(_{H}\) within the same episode (referred to as _Alternating_). This study compares the performance of these two trajectory rollout methods. We modify HEPO to

Figure 2: Normalized task return of PPO (H-only) and HEPO that are trained with heuristic reward function \(H1\) to \(H12\) designed by human subjects in the real world reward design condition. HEPO achieves higher task return than PPO (H-only) in 9 out of 12 tasks. This shows HEPO is robust to possibly ill-designed heuristic reward functions and can leverage them to improve performance.

gather trajectories using the _Alternating_ strategy and present the results in Figure 2(b). The findings indicate that _Alternating_ results in a performance drop during mid-training and fails to match the performance of _HEPO(Joint)_. We hypothesize that this occurs because the batch of trajectories collected solely by one policy deviates significantly from those that another policy can generate (i.e., high off-policy error), leading to less effective PPO policy updates. In contrast, _Joint_ samples trajectories using both policies, preventing the collected trajectories from deviating too much from each other.

## 5 Related Works

**Reward shaping:** Reward shaping has been a significant area, including potential-based reward shaping (PBRS) [2; 27], bilevel optimization approaches [28; 29; 30] on reward model learning, and heuristic-guided methods (HuRL)  that schedule heuristic rewards. Our method differs as it is a policy optimization method agnostic to the heuristic reward function and can be applied to those shaped or learned rewards.

**Constrained policy optimization:** Recent work like Extrinsic-Intrinsic Policy Optimization (EIPO)  proposes constrained optimization by tuning exploration bonuses to prevent exploiting them at the cost of task rewards. Extensions  balance imitating a teacher model and reward maximization. Our work differs in balancing human-designed heuristic rewards and task rewards, improving upon policies trained with engineered heuristic rewards. We also propose implementation enhancements over EIPO  (Section 4.3).

## 6 Discussion & Limitation

**HEPO for RL practitioners:** In this paper, we showed that HEPO is robust to the possibly ill-designed heuristic reward function in Section 4.2 and also exhibit high-probability improvement over PPO when training with heavily engineered heuristic rewards in robotic tasks in Section 4.1. Moving forward, when users need to integrate heuristic reward functions into RL algorithms, HEPO can potentially be a useful tool to reduce users' time on designing rewards since it can improve performance even with under-engineered heuristic rewards.

**Limitations:** While HEPO shows high-probability performance improvement over heuristic policies trained with well-designed heuristic reward, one limitation is that HEPO does not have a guarantee

Figure 3: **(a) We show that using the policies trained with heuristic rewards (J-only) is better than using the policies trained with task rewards (J-only) when training HEPO. (b) _HEPO(Joint)_ that collects trajectories using both policies leads to better performance than _HEPO(Alternating)_ that alternates between two policies to collect trajectories. See Section 4.3 for details**

to converge to the optimal policy theoretically. One future work can be incorporating the insight in recent theoretical advances on reward engineering  to make a convergence guarantee.

## Author Contributions

* **Chi-Chang Lee:** Co-led the project, led the implementation of the proposed algorithms and baselines, and conducted the experiments.
* **Zhang-Wei Hong:** Co-led the project, led the writing of the paper, scaled up the experiment infrastructure, and conducted the experiments.
* **Pulkit Agrawal:** Played a key role in overseeing the project, editing the manuscript, and the presentation of the work.