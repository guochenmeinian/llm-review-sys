# LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios

Yazhe Niu\({}^{1,3}\) Yuan Pu\({}^{2}\) Zhenjie Yang\({}^{1}\) Xueyan Li\({}^{2}\) Tong Zhou\({}^{1}\)

Jiyuan Ren\({}^{2}\) Shuai Hu\({}^{1}\) Hongsheng Li\({}^{3,4}\) Yu Liu\({}^{1,2}\)

Corresponding Author

###### Abstract

Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as _Go_ and _Atari_. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce _LightZero_, the first unified benchmark for deploying _MCTS/MuZero_ in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, _Atari_, _MuJoCo_, _MiniGrid_ and _GoBigger_. Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence. The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.

## 1 Introduction

General decision intelligence needs to solve tasks in many distinct domains. Recent advances in reinforcement learning (RL) algorithms have addressed several challenging decision-making problems  and even surpassed top-level human experts in performance . However, these state-of-the-art RL agents often exhibits poor data efficiency and face significant challenges when handling a wide range of diverse problems. Different environments present specific learning requirements and difficulties that prompted currently various algorithms (e.g. DQN , PPO , R2D2 , SAC ) and system architectures such as IMPALA  and others . Designing a general and data-efficient decision solver needs to tackle various challenges, while ensuring that the proposed algorithm can be universally deployed anywhere without domain-specific knowledge requirements.

Monte Carlo Tree Search (MCTS) is a powerful approach that utilizes a search tree with simulation and backpropogation mechanisms to train agents with a small data budget . To model high-dimensional observation spaces and complex policy behaviour, AlphaGo  enhances MCTS with deep neural networks and designs the policy and value network that identify optimal actions and winning rates respectively, which was the first to defeat the strongest professional human player in Go. Despite the impressive results, MCTS-style algorithms rely on a series of necessary conditions, such as knowledge of game rules and simulators, discrete action space and deterministic state transition, which severely restrict the application scope of these methods. In recent years, several successorsto AlphaGo have attempted to extend its capabilities in various directions. MuZero  relaxes the requirements for prior knowledge of environments by training a set of neural networks to reconstruct reward, value and policy. Sampled MuZero  successfully applies MCTS to various complex action space with a novel planning mechanism based on sampled actions. [16; 17; 18] improve MuZero in terms of planning stochasticity, representation learning effectiveness and simulation efficiency respectively. These emerging algorithm insights and techniques have contributed to the development of more general MCTS algorithms and toolchains.

In this paper, we present a unified algorithm benchmark named _LightZero_ that first comprehensively integrates different MCTS/MuZero algorithm branches, including 9 algorithms and more than 20 decision environments with detailed evaluation. To better understand the potential of MCTS as an efficient general-purpose sequential decision solver, we revisit the development history of MCTS methods  and the diverse criterions of newly proposed RL environments [20; 21; 22]. As shown in Figure 2, we outline the six most challenging dimensions in developing LightZero as a general method, including multi-modal and high-dimensional observation space , complex action space, reliance on prior knowledge, inherent stochasticity, simulation cost, and hard exploration.

Furthermore, highly coupled algorithm and system architectures greatly increase the cost and barriers of migrating and improving MCTS-style methods. Some special mechanisms like tree search and data reanalyze  seriously hinder the simplification and parallel acceleration of code implementation. To overcome these difficulties, LightZero designs a modularly pipeline to enable distinct algorithm components as plug-ins. For example, the chance node planning for modelling stochasticity can also be used in continuous control or hybrid action environments. From the unified viewpoint provided by LightZero, we can systematically divide the whole training scheme of MCTS-style methods into four sub-modules: data collector, data arrangement, agent learner, and agent evaluator. LightZero's decoupled architecture empowers developers to focus intensively on the customization of environments and algorithms. Meanwhile, some techniques like off-policy correction and data throughput limiter can ensure the steady convergence of the algorithm while achieving runtime speedups.

Based on these supports, LightZero also explores the advantages of combining some novel insights from model-based RL with MCTS approaches. In particular, the misalignment problem  of state representation learning and dynamics learning can result in the problematic optimization for MuZero, thus a simple self-consistency loss can significantly speed up convergence without special tuning. Besides, intrinsic reward mechanism  can address the exploration deficiency of tree-search methods with hand-crafted noises. Subsequently, we evaluate the ability of LightZero as a general solver for various decision problems. Experiments on different types of environments demonstrate LightZero's rich application ranges and data efficiency regimes with few hyper-parameter adjustments. At last, we provide discussions on the future optimization directions of each sub-module.

In general, we summarize the three key contributions of this paper as follows:

* We present LightZero, the first general MCTS/MuZero algorithm benchmark that systematically evaluates related algorithms and system designs.
* We outline the most critical challenges of real-world decision applications. To address these issues, we decouple the algorithm and system design of MCTS methods and design a modular training pipeline, which can easily integrate novel insights for better scalability.
* We demonstrate the capability and future potential of LightZero as a general sequential decision solver, which can be trained and deployed across diverse domains.

## 2 Background

**Reinforcement Learning** models a decision-making problem as a Markov Decision Process (MDP) \(=(,,,,,_{0})\), where \(\) and \(\) denote the state space and action space, respectively. The transition function \(\) maps \(\) to \(\), while the expected reward function \(\) maps \(\) to \(\). The discount factor \([0,1)\) determines the importance of future rewards, and \(_{0}\) represents the initial state distribution. The goal of RL is to learn a policy \(:\) that maximizes the expected discounted return over the trajectory distribution \(J()=_{,_{0},,}[_{t=0}^{} ^{t}r_{t}]\).

**AlphaZero** is a generalized version of AlphaGo , eliminating the reliance on supervised learning from game records. It is trained entirely through unsupervised self-play and achieves superhuman performance in various board games, such as chess, shogi, and Go. This approach replaces the handcrafted features and heuristic priors commonly used in traditional intelligent programs. Specifically, AlphaZero employs a deep neural network parameterized by \(\), represented as \((,v)=f_{}(s)\). Given a board position \(s\), the network produces a action probability \(p_{a}=P_{r}(a|s)\) for each action \(a\) and a scalar value \(v\) to predict the expected return \(z\), i.e. \(v z\).

**MuZero** achieves superhuman performance in more complex domains with visual input , without knowledge of the environment's transition rules. It combines tree search with a learned model, using three networks: 1 Representation Network: \(s^{0}=h_{}(o_{1},,o_{t})\). This network represents the root node (at time \(t\)) as a latent state, obtained by processing past observations \(o_{1},,o_{t}\), 2 Dynamics Network: \(r^{k},s^{k}=g_{}(s^{k-1},a^{k})\). This network simulates the dynamics of the environment. Given a state and selected action, it outputs the transitioned next state and corresponding reward. 3 Prediction Network: \(^{k},v^{k}=f_{}(s^{k})\). Given a latent state, this network predicts the action probability and value. Notably, MuZero searches within the learned latent space. For the MCTS process in MuZero, assume the initial root node \(s_{0}\) is generated from the original board state through the representation network, each edge stores the following information: \(N(s,a),P(s,a),Q(s,a),R(s,a),S(s,a)\), respectively representing visit counts, policy, mean value, reward, and state transition. The MCTS process in the latent space can be divided into three phases:

* **Selection**: Actions are chosen according to the Upper Confidence Bound (UCB)  formula: \[a^{*}=*{arg\,max}_{a}Q(s,a)+P(s,a)N(s,b)}}{1+ N(s,a)}[c_{1}+(N(s,b)+c_{2}+1}{c_{2}})]\] where, \(N\) represents the visit count, \(Q\) is the estimated average value, and \(P\) is the policy's prior probability. \(c_{1}\) and \(c_{2}\) are constants that control the relative weight of \(P\) and \(Q\).
* **Expansion**: The selected action is executed in the learned model, continuing until a leaf node is encountered. At this point, a new state node \(s^{l}\) is generated, and its associated predicted reward \(r^{l}\) is determined. Utilizing the prediction function, we obtain the predicted values \(p^{l}\) and \(v^{l}\). Subsequently, this node is incorporated into the search tree.
* **Backup**: The estimated cumulative reward at step \(k\) is calculated based on \(v^{l}\), denoted as: \(G^{k}=_{=0}^{l-1-k}^{}r_{k+1+}+^{l-k}v^{l}\). Subsequently, \(Q\) and \(N\) are updated along the search path.

After the search is completed, the visit count set \(N(s,a)\) is returned at the root node \(s_{0}\). These visit counts are normalized to obtain the improved policy:

\[_{}(a|s)=N(s,a)^{1/T}/_{b}N(s,b)^{1/T}\]

Figure 1: Overview of LightZero. The left side depicts the development of MCTS, while the right side showcases various RL environments. LightZero incorporates and extends recent advances within the MCTS/MuZero sub-domain and effectively applies them across diverse environments.

where \(T\) is the temperature coefficient controlling the exploration degree. Finally, an action is sampled from this distribution for interaction with the environment or self-play. During the learning phase, MuZero perform end-to-end training with the following loss function, where \(l^{p}\), \(l^{v}\) and \(l^{v}\) are loss functions for policy, value and reward respectively, and the final term is weight decay.

\[l_{t}()=_{k=0}^{K}l^{p}(_{t+k},p_{t}^{k})+l^{v}(z_{t+k},v_{t}^{k}) +l^{r}(u_{t+k},r_{t}^{k})+c||||^{2}\]

## 3 LightZero

In this section, we will first introduce the overview of LightZero, followed by a comprehensive analysis of challenges in various decision environments. Additionally, we propose a specific training pipeline design for a modular and scalable MCTS toolchain. We will conclude this section with two algorithm insights inspired by the decoupled design of LightZero.

### Overview

As is shown in Figure 1, LightZero is the first benchmark that integrates almost all recent advances in the MCTS/MuZero sub-domain. Specifically, LightZero incorporates nine key algorithms derived from the original AlphaZero , establishing a standardized interface for training and deployment across diverse decision environments. Unlike the original versions of these derived algorithms, which focused on specific avenues of improvement, LightZero provides a unified viewpoint and interface. This unique feature enables exploration and comparison of all possible combinations of these techniques, offering an comprehensive baseline for reproducible and accessible research. The concrete experimental results are thoroughly described in Section 4 and Appendix B.

### How to Evaluate A General MCTS Algorithm: 6 Environment Challenges

The algorithm extensions integrated in LightZero have greatly relaxed the constraints and broadened the applicability of MCTS-style methods. In the following part, we hope to delve deeper into the key

Figure 2: Radar chart comparison of MCTS-style methods and model-free RL (e.g. PPO) on six environment challenges and another data efficiency dimensions. We categorize the critical capabilities of general decision solvers as follows: multi-modal observation space, complex action space, inherent stochasticity, reliance on prior knowledge, simulation cost, hard exploration and data efficiency. Each curve in the graph represents the score of an algorithm across these six categories. A score of 1 indicates that the algorithm perform poorly in this dimension and is only applicable to limited scenarios, while a higer score means a large application scope and better performance. In particular, model-free RL methods need no simulation and have little dependence on priors, so it achieves high score in corresponding dimensions. Please note that within this context, the term _LightZero_ refers to the special algorithm that embodies the optimal combination of techniques and hyperparameter settings within our framework. Details about qualitative score rules can be found in Appendix D.

issues in the design of general and efficient MCTS algorithms. In order to systematically complement this endeavor, we conducted an analysis of a set of classic and newly proposed RL environments to identify common characteristics. Based on this analysis, we have summarized six core challenging dimensions, which are presented in a radar plot depicted in Figure 2. Concretely, The intentions and goals of six types of environmental capabilities are: _1) Multi-modal observation spaces_ pose a challenge for agents as they must be able to extract different representation modalities (e.g., low-dimensional vectors, visual images, and complex relationships) while effectively fusing distinct embeddings. _2) Complex action space_ necessitates the agent's proficiency in generating diverse decision signals, encompassing discrete action selection, continuous control, and hybrid structured action space. _3) Reliance on prior knowledge_ is a major drawback of methods like AlphaZero. These approaches inherently require accessibility to a perfect simulator and specific rules of the environment. In contrast, MuZero and its derived methods address this limitation by learning an environment model to substitute the simulator and related priors. _4) Inherent stochasticity_ presents a fundamental challenge in tree-search-based planning methods. The uncertainty of environment dynamics and partially observable state spaces both can lead to misalignment of planning trajectories, resulting in a large number of useless or conflicting search results. _5) Simulation cost_ stands as the primary contributor to wall-time consumption for MCTS-style methods. At the same time, the algorithm performance will degrade a lot if the algorithm fails to visit all the necessary actions during the simulation process. _6) Hard exploration_ represents a crucial challenge that is often overlooked. While search trees can enhance efficiency by reducing the scope of exploration, MCTS-style methods are susceptible to difficulties in environments with numerous non-terminating cases, such as mazes.

### How to Simplify A General MCTS Algorithm: Decouple Pipeline into 4 Sub-Modules

The impressive performance of MCTS-style methods is often accompanied by a notable drawback: the complexity of implementations, which greatly restricts their applicability. In contrast to some classic model-free RL algorithms like DQN  and PPO , MCTS-style methods require multi-step simulations using search trees at each agent-environment interaction. Also, to improve the quality of training data, MuZero Unplugged  introduce a data reanalyze mechanism that uses the newly obtained model to compute improved training targets on old data. However, both of these techniques require multiple calls to simulators or neural networks, increasing the complexity across various aspects of the overall system, including code, distributed training, and communication topology.

Figure 3: Four core sub-modules of the training pipeline in LightZero. _Context Exchanger_ is responsible for transporting configurations, models and trajectories among different sub-modules.

Therefore, it is necessary to simplify the whole framework based on the integration of algorithms. Figure 3 presents a depiction of the complete pipeline of LightZero with four core sub-modules.

Firstly, LightZero offers support for both online and offline RL  training schemes. The distinction between them lies in the utilization of either an online interaction data collector or direct usage of an offline dataset. Secondly, LightZero restructures its components and organizes them into four main sub-modules, based on the principle of _high cohesion and low coupling_. **Data collector** is responsible for efficient action selection using policy network and search tree. It also contains various exploration strategies, data pre-processing and packaging operations. **Data arrangement** plays a unique role in MCTS by effectively storing and preparing valuable data for training purposes. This sub-module involves the data reanalyze technique  to correct off-policy and even offline data. Furthermore, the modified priority sampling  ensures training mini-batches have both sufficient variety and high learning potential. To balance these tricks with efficiency, the throughput limiter controls the ratio of adding and sampling data to ensure optimal data utilization within a fixed communication bandwith. **Agent learner** is responsible for training multiple networks. It can be enhanced through various optimization techniques, such as self-supervised representation learning [35; 36], model-based rollout [37; 38], distributional predicton  and normalization [40; 41]. These techniques contribute to the policy improvement and further enhance the overall performance of the agent. **Agent evaluator** periodically provides the diverse evaluation metrics  to monitor the training procedure and assess policy behaviour. It also integrates some inference-time tricks like beam search  to enhance test performance. We provide a detailed analysis of how these sub-modules are implemented in specific algorithms in Appendix F. Built upon these abstractions, LightZero serves as a valuable toolkit, enabling researchers and engineers to develop enhanced algorithms and optimize systems effectively. For example, the exploration strategies and ensuring the alignment of a learned model in MCTS is crucial, and this will be discussed in the subsequent sub-section. In addition, exploring parallel schemes for multiple vectorized environments and search trees can be an insightful topic for machine learning system. The associated dataflow and overhead analysis will be presented in the Appendix E.

### How to Improve A General MCTS Algorithm: 2 Examples

In this section, we present two algorithm improvement examples inspired by LightZero. The below dimensions pose necessary challenges in designing a comprehensive MCTS solver. LightZero addresses these challenges through various improvements, resulting in superior performance compared to individual algorithm variants across different domains (Section 4 and 5).

**Intrinsic Exploration** While tree-search-based methods perform well in board games with only eventual reward, they may encounter challenges or perform poorly in other environments with sparse rewards, such as _MiniGrid_. One crucial distinction between these two problems is that in the former, the search tree can always reach several deterministic final states, whereas in the latter, it may encounter various non-termination states due to the limitation of maximum episode length. To address this issue, LightZero incorporates the idea of intrinsic reward methods  and implement it efficiently within MuZero's learned models. Further details can be found in Section 5.1.

**Alignment in Environment Model Learning** MuZero employs a representation network to generate latent states and a dynamics network to predict next latent states. However, there is no explicit supervision guiding the desired properties of the latent space. Traditional self-supervised representation learning methods often fail to align these proxy tasks with RL objectives. The difference of rollouts between the perfect simulator and the learned model is also a problems that can not be ignored. Further exploration of misalignments across different environments are discussed in Section 5.2.

## 4 Experiments

In Section 4.1, we initially present some representative experiments of LightZero, with detailed experimental settings and more comprehensive results outlined in the Appendix B. Subsequently, in Section 4.2, we delve into key observations and reflections based on these benchmark results, introducing some critical insights. Particularly regarding the exploration and the alignment issues of environment model learning, we conduct an in-depth experimental analysis in Section 5.

### Benchmark Results

To benchmark the difference among distinct algorithms and the capability of LightZero as a general decision solver, we conduct an extensive comparisons across a diverse range of RL environments. The algorithm variants list contains AlphaZero , MuZero , EfficientZero , Sampled MuZero , Stochastic MuZero , Gumbel MuZero  and other improved versions of LightZero. For each scenario, we evaluate all the possible variants on corresponding environments. In Figure 4, we show some selected results as examples. For detailed settings, metrics, comprehensive benchmark results and related analysis, please refer to Appendix B.

### Key Observations and Insights

Building on the unified design of LightZero and the benchmark results, we have derived some key insights about the strengths and weaknesses of each algorithm, providing a comprehensive understanding of these algorithms' performance and potential applications.

**O1**: In board game environments, AlphaZero's sample efficiency greatly exceeds that of MuZero. This suggests that employing AlphaZero directly is advantageous when an environment simulator is available; however, MuZero can still achieve satisfactory results even in the absence of a simulator.

**O2**: Self-supervised loss substantially enhances performance in most Atari environments with image inputs. Figure 7 demonstrates that MuZero with SSL performs similarly to MuZero in _MsPacman_, while outperforming it in the other five environments. This result highlights the importance of SSL for aligning the model and accelerating the learning process in image input environments.

**O3**: Predicting _value_prefix_ instead of reward does not guarantee performance enhancement. For example, in Figure 7, EfficientZero outperforms MuZero with SSL only in the _MsPacman_ and _Breakout_ environments, while showing similar performance in the other environments. In certain specific scenarios, such as the sparse reward environments depicted in Figure 12, EfficientZero's performance is significantly inferior to that of MuZero with SSL. Therefore, we should prudently decide whether to predict _value_prefix_, taking into account the attributes of the environment.

**O4**: MuZero with SSL and EfficientZero demonstrate similar performance across most _Atari_ environments and in complex structured observation settings, such as _GoBigger_. This observation suggests

Figure 4: Comparisons of mean episode return for algorithm variants in LightZero across diverse environments: _Atari_ with discrete action and partial-observable state (_Qbert_, _Breakout_, _MsPacman_), _GoBigger_ with complex observation and multi-agent cooperation, continuous control with environment stochasticity (_Bipedalwalker_), and _Gomoku_ with varying accessibility to simulator.

that environments with complex structured observations can benefit from representation learning and contrastive learning techniques  to enhance sample efficiency and robustness.

**O5**: In discrete action spaces, Sampled EfficientZero's performance is correlated with action space dimensions. For instance, Sampled EfficientZero performs on par with EfficientZero in _Breakout_ (action space dimension of 4), but its performance decreases in _MsPacman_ (dimension of 9).

**O6**: Sampled EfficientZero with _Gaussian policy representation_ is more scalable in continuous action spaces. The Gaussian version performs well in traditional continuous control and _MuJoCo_ environments, while _factored discretization_ is limited to low-dimensional actions.

**O7**: Gumbel MuZero achieves notably better performance than MuZero when the number of simulations is limited, which exhibits its potential in designing low time-cost MCTS agent.

**O8**: In environments with stochastic state transitions or partial observable states (such as Atari without stacked frames), Stochastic MuZero can obtain slightly better performance than MuZero.

**O9**: The self-supervised loss proposed in , sampling-related techniques in Sampled MuZero , computational improvements in Gumbel MuZero  for utilizing MCTS searched information, and environment stochasticity modeling in Stochastic MuZero  are orthogonal to each other, exhibiting minimal interference. LightZero is exploring and developing ways to seamlessly integrate these characteristics to design a universal decision-making algorithm.

## 5 Two Algorithm Case Studies for LightZero

### Exploration Strategies in MCTS

**Motivation** Finding the optimal trade-off between exploration and exploitation is a fundamental challenge in RL. It is well-known that MCTS can reduce the policy search space and facilitate exploration. However, there exists limited research on the performance of MCTS algorithms in hard-exploration environments. Based on the above benchmark results, we conduct a detailed analysis of the algorithm behaviours between challenging sparse reward environments and board games, as well as insights behind the selection of exploration mechanisms in this section and Appendix C.1.

**Settings** We performed experiments in _MiniGrid_ environment, mainly on the KeyCorridorS3R3 and FourRooms scenarios. Expanding upon the naive setting (handcrafted temperature decay), we conducted a comprehensive investigation of six distinct exploration strategies in LightZero. A detailed description of each exploration mechanism is provided in Appendix C.1.

**Analysis** Figure 5 indicate that simply increasing search budgets does not yield improved performance in challenging exploration environments. Instead, implementing a larger temperature and

Figure 5: Performance of various MCTS exploration mechanisms in _MiniGrid_ environment (_Return_ during the collection phase). Under the naive setting, the agent fails due to inadequate exploration. Merely increasing search budgets with the _NaiveDoubleSimulation_ approach does not yield any significant improvement. _EpsGreedy_, _FixedTemperature_ and _PolicyEntropyRegularizatio-x_ display higher variance as they cannot guarantee enough exploration. _IntrinsicExploration_ effectively explores the state space by leveraging curiosity mechanisms, resulting in the highest sample efficiency.

incorporating policy entropy bonus can enhance action diversity during data collection, albeit at the cost of increased variance. However, theoretically, they cannot guarantee sufficient exploration, often resulting in mediocre performance and a higher likelihood of falling into local optima due to policy collapse. Epsilon-greedy exploration ensures a small probability of uniform sampling, which aids in exploring areas with potentially high returns. _EpsGreedy_ has varying effects in different environments in early stages, but theoretically, due to its ability to ensure sufficient exploration, it may achieve good results in the long run. A more effective strategy involves curiosity-driven techniques, such as RND , which assigns higher intrinsic rewards to novel state-action pairs, bolstering the efficiency of exploration. The performance of the _IntrinsicExploration_ method supports this assertion, and it can be integrated into MuZero with minimal overhead (Appendix C.1.3).

### Alignment in Environment Model Learning

**Motivation** Aligned and scalable  environment models are vital for MuZero-style algorithms, with factors such as model structure, objective functions, and optimization techniques contributing to their success. The consistency loss proposed in  could serve as an approach for aligning the latent state generated by the dynamics model with the state obtained from the observation. In this section, we investigate the impact of consistency loss on learning dynamic models and final performance in environments with diverse observations (vector, standard images, special checkerboard images).

**Settings** To study the impact of the consistency loss on various types of observation data, we employ the MuZero algorithm as our baseline. To ensure the reliability of our experimental results, we maintain the same configurations across other settings, with additional experimental details provided in Appendix C.2. In the experiments, we use _Pong_ as the environment for image input, _LunarLander_ for continuous vector input, and _TicTacToe_ for special image input (checkerboard) environments.

**Analysis** In Figure 6, consistency loss is critical for standard image input. Removing the consistency loss results in significant decline in performance, indicating the challenge of learning a dynamic model for high-dimensional inputs. For vector input environments like _LunarLander_, consistency loss provides a minor advantage, suggesting that learning a dynamic model is relatively easier on the compact vector observations. In special two dimension input environments like _TicTacToe_, the consistency loss remains large, highlighting the difficulty of achieving consistency between latent state outputs. Additionally, adding consistency loss with inappropriate hyper-parameters may lead to non-convergence (Appendix C.2). In conclusion, our experiments demonstrate that the effectiveness of the consistency loss depends on the special observation attributes. For board games, a future research direction involves investigating suitable loss functions to ensure alignment during training.

## 6 Related Work

**Sequential Decision-making Problems** In the domain of sequential decision-making problems, intelligent agents aim to make optimal decisions over time, taking into account observed states and prior actions . However, these problems are often compounded by the presence of continuous action spaces, dynamic transitions, and exploration difficulties. To address such problems, model-free RL methods [5; 7; 32] focus on learning expected state rewards, optimizing actions, or combining

Figure 6: Impact of self-supervised consistency loss across different environments with various types of observations. From left to right, performance comparisons involve standard image input, compact vector input, and unique board image input, considering cases with and without consistency loss. Experiments show that the consistency loss proves to be critical only for standard image input.

both strategies to achieve optimal policy learning. Conversely, model-based RL  incorporates the environment's transition into its optimization objective, aiming to maximize the expected return on trajectory distribution. MCTS  is a modeling approach derived from search planning algorithms such as minimax  and alpha-beta pruning . Unlike these algorithms, which recursively search decision paths and evaluate their returns, MCTS employs a heuristic search on prior-guided simulations, effectively addressing excessive search consumption in complex decision spaces.

**MCTS Algorithms and Toolkits** Despite the impressive performance and efficiency of the MCTS+RL approach, constructing the training system and dealing with intricate algorithmic details pose significant challenges when applying these algorithms to diverse decision intelligence domains. Recent research has made progress in this direction. MuZero Unplugged  introduced the Reanalyze technique, a simple and efficient enhancement that achieves good performance both online and offline. ROSMO  investigated potential issues with MuZero in offline RL and suggested a regularized one-step lookahead approach. The lack of comprehensive open-source implementations of various algorithms remains a challenge within the research community. For example, Sampled MuZero  lacks a public implementation. AlphaZero-General  and MuZero-General  each support only a single algorithm, and neither offers distributed implementations. Although EfficientZero , does support multi-GPU implementation, it is limited to the single algorithms. KataGo , while primarily focused on the AlphaZero and Go game, requires substantial computational resources during training, potentially posing hardware barriers for ordinary users. As a result, the research community continues to seek more efficient and enhanced open-source tools.

**Standardization and Reproducibility** In the realm of Deep RL, the quest for standardizing algorithm coupled with the creation of unified benchmarks has ascended as focal points of growing significance.  emphasize the critical necessity of not only replicating existing work but also accurately assessing the advancements brought about by new methodologies. However, the process of reproducing extant Deep RL methods is far from straightforward, largely due to the non-determinism inherent in environments and the variability innate to the methods themselves, which can render reported results challenging to interpret.  proposed a set of metrics for quantitatively measuring the reliability of RL algorithms. These metrics, focusing on variability and risk both during and after training, are intended to equip researchers and production users with tools to evaluate and enhance the reliability of RL algorithms. In , a large-scale empirical study was undertaken to identify the factors that significantly influence the performance of on-policy RL algorithms within continuous control tasks. The insights garnered from this research offer valuable, practical suggestions for the training of on-policy RL algorithms. Despite these advancements, there remains a noticeable dearth of work specifically investigating benchmarks and the details of reproducing studies in the domain of MCTS.

## 7 Conclusion

In this paper, we introduce LightZero, the first unified algorithm benchmark to modularly integrates various MCTS-style RL methods, systematically analyze and address the challenges and opportunities for deploying MCTS as a general and efficient decision solver. Through the incorporation of decoupled system design and novel algorithm insights, we conduct detailed benchmarks and demonstrate the potential of LightZero as scalable and efficient decision-making problem toolchains for the research community. Besides, based on this benchmark and related case studies, We also discuss existing limitations and valuable topics for future work in Appendix I.

## 8 Acknowledgements

This project is funded in part by National Key R/D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK. We thank several members of the SenseTime and Shanghai AI Laboratory for their help, support and feedback on this paper and related codebase. We especially thank Shenghan Zhang for informative and inspiring discussions at the beginning of this project. We are grateful to the assistance of Qingzi Zhu for many cute visual materials of LightZero.