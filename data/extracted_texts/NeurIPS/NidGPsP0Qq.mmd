# Provably Efficient Interaction-Grounded Learning

with Personalized Reward

 Mengxiao Zhang

University of Iowa

mengxiao-zhang@uiowa.edu

&Yuheng Zhang

University of Illinois Urbana-Champaign

yuhengz2@illinois.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

&Paul Mineiro

Microsoft Research

pmineiro@microsoft.com

Equal contribution.

###### Abstract

Interaction-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions. To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al.  study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees. In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability. Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects. Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances. Building on this estimator, we propose two algorithms, one based on explore-then-exploit and the other based on inverse-gap weighting. We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice. Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms.

## 1 Introduction

Traditional bandit problems [Auer et al., 2002] or reinforcement learning problems [Sutton and Barto, 2018] assume that the learner has access to the reward, which is her learning goal. However, such explicit reward information is usually difficult to obtain in many real-world scenarios, including human-computer interface applications [Pantic and Rothkrantz, 2003, Freeman et al., 2017] and recommender systems [Yi et al., 2014, Wu et al., 2017]. Recently, Xie et al.  study a new setting called Interaction-Grounded Learning (IGL), where the learner interacts with the environment and receives some feedback on her actions instead of explicit reward signals. The learner needs to discover the implicit information about the reward in the feedback and maximizes the reward.

From an information-theoretic perspective, IGL is intractable unless further assumptions are made. Xie et al.  introduce a conditional independence assumption which states that the feedback is conditionally independent of the action and context given the latent reward. However, this assumption is unrealistic in many scenarios. For example, different users interact with recommender systems in different ways even under the same latent reward [Maghakian et al., 2022]. This inspires us to study IGL with personalized rewards, a setting where the feedback depends on the context. AlthoughMaghakian et al. (2022) study this for recommender systems, they only provide empirical results of their approach, leaving the following question open:

_Can we design provably efficient algorithms for interaction-grounded learning when the feedback depends on the context given the latent reward?_

Contributions.In this paper, we answer the question in the positive and provide the first provably efficient algorithms with sublinear regret guarantee for IGL with personalized reward. To achieve this, in Section 3.1, we first propose a novel reward estimator via inverse kinematics. Specifically, using the samples collected by applying a uniform policy, we construct a _Lipschitz reward_, which underestimates the reward for all the policies but approximates the reward for the optimal policy well. With this reward estimator, we propose two algorithms, one based on Explore-then-Exploit (Section 3.2) and the other based on inverse-gap weighting (Section 3.3). Both algorithms achieve \((T^{})\) regret. To the best of our knowledge, we are the first to propose provable regret guarantees for IGL with personalized reward. In Section 4, we implement both algorithms and apply them to both an image classification dataset and a conversational dataset. The empirical performance showcases the effectiveness of our algorithm and the importance of using the Lipschitz reward estimator.

### Related Work

Interaction-Grounded Learning (IGL).Xie et al. (2021) is the first work studying IGL and assumes that the feedback is independent of the context and action conditioned on the latent reward. Xie et al. (2022) further relaxes the assumption to an action-inclusive version, where the feedback is independent of context conditioned on both the action and the reward.2Hu et al. (2024) follows the same setting as Xie et al. (2021) but proposes an information-theoretic approach to enforce the conditional independence assumption. However, as we mentioned before, context-dependent feedback is ubiquitous in real-word applications, so such conditional independence assumptions reduce the generality of the IGL framework. Maghakian et al. (2022) is the closest work to ours which also considers personalized rewards. Nonetheless, their work focuses on the empirical side and does not provide any theoretical guarantees.

Contextual online learning with partial feedback.Our work is closely related to the recent trend of designing efficient contextual learning algorithms with partial feedback, including contextual bandits (Langford and Zhang, 2007; Agarwal et al., 2012, 2014; Foster and Krishnamurthy, 2018; Foster et al., 2018; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2021), where the learner receives explicit reward signal of her taken action; contextual bandits with feedback graphs (Zhang et al., 2024, 2024), where the learner's observation of the reward is determined based on a feedback graph; and contextual partial monitoring (Bartok and Szepesvari, 2012; Kirschner et al., 2020), where the learner's observation is defined by a signal matrix or a linear observation operator. We adopt and generalize the ideas for designing contextual bandits algorithms (Foster and Rakhlin, 2020) to design algorithms for IGL.

## 2 Preliminary

Problem setup.Throughout the paper, we use \([N]\) to denote the set \(\{1,2,,N\}\) for a positive integer \(N\). The problem of online Iterative-Grounded Learning (IGL) with personalized reward we consider is defined as follows. The interaction between the learner and the environment lasts for \(T\) rounds. At each round \(t[T]\), the environment reveals a stochastic context \(x_{t}\) i.i.d. drawn from an unknown distribution \(\), and the learner decides an action \(a_{t}[K]\) from a finite action set of size \(K\) based on this context. Then, different from the classic contextual bandits in which the learner observes the binary reward of her chosen action \(r(x_{t},a_{t})\{0,1\}\), she receives feedback \(y_{t}\).

Feedback dependence assumption.We follow the assumption proposed in (Maghakian et al., 2022) that the feedback is conditionally independent of the action given the context and the realized reward.

**Assumption 1**.: _For arbitrary \((x,a,r,y)\) tuple where the reward \(r\) and the feedback \(y\) are generated based on the context \(x\) and action \(a\), we assume \(y\) is conditionally independent of action \(a\) given context \(x\) and reward \(r\). In other words, we assume that \(y a r,x\)._

As mentioned, compared to prior work (Xie et al., 2021, 2022), this better captures many real-world applications such as recommender systems where different users interact with them in different ways (Beel et al., 2013; Shin, 2020; Maghakian et al., 2022).

Realizability.We assume that the learner has access to two function classes \(([K])\) and \((\{0,1\})\), where \(\) characterizes the mean of the reward for a given context-action pair, and \(\) characterizes the realized reward given the context and the received feedback. A policy \(:(K)\) specifies the action probability conditioned on the context. For each \(f\), we use \(_{f}\) to denote the induced policy which takes action greedily according to \(f\), that is, \(_{f}(a|x)=\{a=*{argmax}_{a^{}[K]}f(x,a^{ })\}, a[K]\). We also use the shorthand \(^{*}\) for the optimal policy \(_{f^{*}}\). We then make the following realizability assumption following previous contextual bandits literature (Agarwal et al., 2012; Foster et al., 2018; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2021).

**Assumption 2** (Realizability).: _There exists a regression function \(f^{}\) such that \([r(x_{t},a)|x_{t}]=f^{}(x_{t},a)\) for all \(a[K]\) and \(t[T]\). Furthermore, there exists a feedback decoder \(^{}\) such that \(^{}(x_{t},y_{t})=r(x_{t},a_{t})\) for all \(t[T]\)._

For simplicity, we also assume that \(\) and \(\) are finite with cardinality \(||\) and \(||\). Our results can be further generalized to broader function classes, which will be discussed in later sections.

Identifiability.As mentioned in (Xie et al., 2021, 2022; Maghakian et al., 2022), it is impossible to learn if we do not break the symmetry between reward being \(1\) and being \(0\). Following (Maghakian et al., 2022), we make the following assumption:

**Assumption 3** (Identifiability).: _For any \(x\), \(f^{}\) defined in Assumption 2 satisfies that: 1) \(_{a=1}^{K}f^{}(x,a)\) for some \(0<<\); 2) \(_{a[K]}f^{}(x,a)\) where \(>\)._

The first condition says that the sum of the expected reward over actions is less than \(\) given any context \(x\). That is to say, the reward vector is sparse if \(f^{}(x,a)\{0,1\}\). The second condition says that for each context \(x\), there exists an action that achieves a large enough expected reward. These two conditions are indeed satisfied by many real-world applications, including the \(s\)-multi-label classification problem (with \(s<K/2\)) where \(=s\) and \(=1\).

Regret.The learner's performance is measured via the notion of regret, which is defined as the expected difference between the learner's total reward and the one received by the optimal policy:

\[_{}=[_{t=1}^{T}f^{}(x_{t},^{ }(x_{t}))-_{t=1}^{T}f^{}(x_{t},a_{t})],\]

Other notations.We denote the \((K-1)\)-dimensional simplex by \(_{K}\). Let \(\) be the all-one vector in an appropriate dimension and \(_{}=_{K}\). For a \(d\)-dimensional vector \(v^{d}\), we denote its \(i\)-th entry by \(v_{i}\). \(\{\}\) is the indicator function and \(e_{i}\) is the \(i\)-th standard basis vector in an appropriate dimension.

## 3 Methodology

In this section, we discuss our methodology. In Section3.1, we start from introducing how we construct a Lipschitz reward estimator based on uniform samples, which serves as the key for our algorithm construction. We prove that this estimator is an _underestimator_ of the reward, and more importantly matches the reward for the optimal policy. Based on this estimator, we design two algorithms for this problem in Section3.2 and Section3.3, with the first one based on explore-then-exploit and the second one based on inverse-gap weighting (IGW).

### Reward Estimator Construction via Uniform Exploration

We first show how we construct a reward estimator based on feedback collected from a uniform policy, which serves as the most important component in our two algorithms.

#### 3.1.1 Inverse Kinematics

When the reward for each context-action pair is _deterministic_ and binary, Maghakian et al. (2022) show that if the learner uniformly samples an action for any context and is able to accurately predict the posterior probability of her chosen action given the context and the feedback (inverse kinematics), then she is able to infer that the reward is \(1\) if that posterior probability is above certain threshold. Here, we generalize this thresholding reward estimator to the _randomized_ binary reward case, and further prove that this estimator correctly models the reward for the optimal policy. To see this, we first prove the following lemma showing the exact posterior distribution over actions if the learner selects an action uniformly randomly. This is also proven in (Maghakian et al., 2022, Eq.(2)) as well, and we include it here for completeness.

**Lemma 1**.: _For any context \(x\), suppose that the learner picks a uniformly random action \(a[K]\). Let \(r\) and \(y\) be its realized reward and the corresponding feedback. Then, under Assumption 1 and Assumption 2, the posterior distribution of a given the context \(x\) and feedback \(y\) equals to_

\[[a|x,y]=(x,a)^{}(x,y)}{_{a^{ }=1}^{K}f^{}(x,a^{})}+(x,a))(1-^{}(x,y))} {K-_{a^{}=1}^{K}f^{}(x,a^{})},\] (1)

_where \(f^{}\) and \(^{}\) are the true expected reward and feedback decoder defined in Assumption 2._

Now we show how to infer the true reward from this inverse kinematics. Specifically, we show:

**Lemma 2**.: _For any context \(x\) and action \(a[K]\), let \(r(x,a)\) be the realized reward and \(y\) be the feedback. Let \(h^{}(x,y)_{K}\) be such that for each \(a[K]\),_

\[h^{}_{a}(x,y)(x,a)^{}(x,y)}{_{ a^{}=1}^{K}f^{}(x,a^{})}+(x,a))(1-^{ }(x,y))}{K-_{a^{}=1}^{K}f^{}(x,a^{})}.\] (2)

_Then we have_

* \(r(x,a)=^{}(x,y)\{h^{}_{a}(x,y)\}\) _for all_ \(a[K]\)_;_
* \(r(x,a)=^{}(x,y)=\{h^{}_{a}(x,y)\}\) _for_ \(a=^{}(x)\)_._

Proof.: Note that since \(^{}(x,y)\{0,1\}\), only one term can be non-zero in Eq. (2). If \(h^{}_{a}(x,y)\), where \(\) and \(\) are defined in Assumption 3, then the reward \(^{}(x,y)\) has to be \(1\) since otherwise, we have \(^{}(x,y)=0\) and

\[ h^{}_{a}(x,y)=(x,a)}{K-_{ a^{}=1}^{K}f^{}(x,a^{})}<,\]

where the second and the third inequality are due to Assumption 3. This leads to a contradiction. Therefore, we know that the realized reward is \(1\) if \(h^{}_{a}(x,y)\) is no less than \(\). Therefore, \(\{h^{}_{a}(x,y)\}\) can be viewed as an underestimator of \(r(x,a)\). To prove the second property, consider the case in which \(a=^{}(x)\). Then, we know that when \(^{}(x,y)=1\), we must also have \(h^{}_{a}(x,y)\) since

\[h^{}_{a}(x,y)=(x,^{}(x))}{_{i=1}^{K}f^{}(x,a)}(x,^{}(x))}{},\]

where the first inequality uses the first property in Assumption 3 and the second inequality uses the second property in Assumption 3. 

Lemma 2 shows that the function \(h^{}(x,y)\) defined in Eq. (2) satisfies two important properties. First, \(\{h^{}_{a}(x,y)\}\) serves as a reward _underestimator_ for all the policies. Second, it matches the reward of the optimal policy. Note that this is different from (Maghakian et al., 2022, Eq.(3)), sincethey only consider the _deterministic_ binary reward for each context-action pair, and they do not show that the constructed reward estimator matches the reward of the optimal policy.

Based on these two properties, we know that if we have access to \(h^{}\), then the policy \(\) that maximizes the surrogate reward \([\{h^{}_{(x)}(x,y)\}]\) also maximizes the true expected reward.

#### 3.1.2 Learning the Posterior Distribution via ERM

Next, we show how to learn this \(h^{}\) via uniformly collected samples. Define the function class \(\) as:

\[=h:_ {K},h_{a}(x,y)=^{K}f(x,i)}+^{K}f(x,i)}\\ \ f,,a[K]}.\]

Note that \(||=||||\). Since \(h^{}\) models the posterior distribution over actions when the learner selects her action uniformly randomly, we collect \(N\) tuples of \((x_{t},a_{t},y_{t})\) by sampling \(a_{t}\) uniformly from \([K]\) and find the empirical risk minimizer (ERM) \(\) over these samples using squared loss. In the following lemma, we show that \(\) enjoys \((|}{N})\) excess risk with high probability.

**Lemma 3**.: _Let \(\{(x_{i},a_{i},y_{i})\}_{i=1}^{N}\) be \(N\) i.i.d. samples where \(x_{i}\), \(a_{}\), and \(y_{i}\) is the corresponding feedback. Let \(\) be the ERM with respect to the squared loss defined as follows:_

\[=*{argmin}_{h}\{_{i=1}^{N} \|h(x_{i},y_{i})-e_{a_{i}}\|_{2}^{2}\}.\] (3)

_Then, with probability at least \(1-\), we have_

\[_{x,a_{},y|x,a}[ \|(x,y)-e_{a}\|_{2}^{2}-\|h^{}(x,y)-e_{a}\|_{2}^{2}] (|}{}}{N}),\] (4) \[_{x,a^{}_{},y|x,a^{}}[\|(x,y)-h^{}(x,y)\|_{2}] (|}{}}{N}}).\] (5)

The full proof is deferred to Appendix A. Different from the classic one-dimensional squared loss, here we consider the \(_{2}\)-loss between two vectors in \(_{K}\). Directly applying the generalization bound for each entry leads to a \(K\)-factor worse bound. Instead, our proof is based on the observation that the loss function \(\|h(x,y)-e_{a}\|_{2}^{2}\), when seen as a function of \(h\), satisfies the so-called strong 1-central condition (Van Erven et al., 2015, Definition 7). Moreover, these results can be extended to function classes with infinite size and bounded covering number based on Theorem 7.7 of (Van Erven et al., 2015).

#### 3.1.3 Constructing Lipschitz Reward Estimators Based on \(\)

Now we show how to construct a reward estimator based on the ERM \(\). According to Section 3.1.1, an intuitive form of the reward (under)estimator is \(\{_{a}(x,y)\}\). However, since the indicator function is not Lipschitz, \(\{_{a}(x,y)\}\) can be very different from \(\{h^{}_{a}(x,y)\}\) even with the generalization bound proven in Lemma 3. To resolve this issue, we propose a Lipschitz variant of the indicator function (defined in Eq. (6)) and show that it also satisfies the two properties shown in Lemma 2.

**Lemma 4**.: _Define \(G(v,,)\) as_

\[G(v,,)=(v-)\{ v<+ \}+\{v+\}.\] (6)

_Then, for any context \(x\), action \(a[K]\), and feedback \(y\) generated via context \(x\) and the realized reward \(r(x,a)\), we have the following two properties with \((- )>0\)._* \(r(x,a)=^{}(x,y) G(h_{a}^{}(x,y),-,)\)_,_
* \(r(x,a)=^{}(x,y)=G(h_{a}^{}(x,y),-,)\) _if_ \(a=^{}(x)\)_,_

The proof shares a similar spirit to Lemma2 and is deferred to AppendixA. Notably, the function \(G(v,,)\) is \(\)-Lipschitz in \(v\), which is important in order to show concentration between the reward estimator with respect to the true posterior distribution \(G(h_{a}^{}(x,y),-,)\) and that constructed via the ERM function \(\): \(G(_{a}(x,y),-,)\). In the following, we will show how to use the reward estimator \(G(_{a}(x,y),-,)\) to design algorithms with provable guarantees.

### Off-Policy Algorithm

Built on the reward estimator in Section3.1, we present our off-policy algorithm, summarized in Algorithm1. Our algorithm follows the explore-then-exploit idea and consists of two phases. In the exploration phase, we perform uniform exploration and collect the dataset \(\{x_{i},a_{i},y_{i}\}_{i=1}^{2N}\). The first \(N\) samples are used to learn the reward estimator \(\) and the rest \(N\) samples are used to learn the policy \(\) with \(\). For the exploitation phase, we employ the learned policy \(\) in the remaining \(T-2N\) iterations. We present the regret bound of Algorithm1 in the following theorem.

**Theorem 1**.: _Under Assumptions1-3, Algorithm1 with \(N=T^{2/3}K^{2/3}^{-2/3}^{1/3}(||T)\) guarantees that \(_{}(T^{2/3}K^{2/3}^{-2/3} ^{1/3}(||T))\)._

We remark that this is the first provably efficient algorithm for IGL with personalized reward. The key of the proof is to show that the learned policy \(\) is near-optimal under the true reward. This is achieved by using the properties of function \(G\) in Lemma4 and proving that the reward decoder \(_{}\) is close to the ground-truth. The full proof is deferred to AppendixB. Besides the dependence on \(T\), \(K\) and \(||\), our regret bound also depends on \(^{-1}\), which measures how sparse the reward vector is and characterizes the difficulty of the problem. For example, \(^{-1}=(1)\) in the multi-class classification problem and \(^{-1}(s^{2})\) in the \(s\)-multi-label classification problem.

### On-Policy Algorithm

Since on-policy algorithms are more favorable in practice, we further introduce an on-policy algorithm based on the inverse-gap weighting strategy. Following (Foster and Rakhlin, 2020), we assume access to an online regression oracle \(\): at each round \(t[T]\), the oracle \(\) produces an estimator \(_{t}\) in the convex hull of \(\), then receives a context-action-reward tuple \((x_{t},a_{t},r_{t})\). The squared loss of the oracle for this round is defined as \((_{t}(x_{t},a_{t})-r_{t})^{2}\), which is on average assumed to be close to that of the best predictor in \(\).

**Assumption 4** (Bounded squared loss regret).: _For any sequence \(\{(x_{t},a_{t},r_{t})\}_{t=1}^{T}\), the regression oracle \(\) guarantees the following regret bound for some \(_{}\) that depends on \(T\), \(K\), and \(\):_

\[_{t=1}^{T}(_{t}(x_{t},a_{t})-r_{t})^{2}-_{f }_{t=1}^{T}(f(x_{t},a_{t})-r_{t})^{2}\,_{}.\]Based on the ground-truth inverse kinematics function \(h^{}\), we define a function \(^{}(x,a):=_{y|x,a}[G(h^{}_{a}(x,y),-,)]\), which is always a lower bound on \(f^{}(x,a)\) according to Lemma4. Since we only feed the surrogate reward to the oracle, we make another mild assumption that our regression function class \(\) also realizes \(^{}\).

**Assumption 5** (Lower Bound Realizability).: _We also assume that \(^{}\), where \(^{}\) is defined as \(^{}(x,a)=_{y|x,a}[G(h^{}_{a}(x,y), {}{}-,)]\)_

We now summarize our algorithm in Algorithm2. After obtaining the inverse kinematics predictor \(\) in the same manner as Algorithm1, instead of uniform exploring, we use the estimated reward from the oracle and an inverse-gap weighting strategy (Abe and Long, 1999; Foster and Rakhlin, 2020) defined in Eq.7. Different from the contextual bandit problem where the true reward is given and fed to the oracle, we feed the predicted reward \(G(_{a_{i}}(x_{t},y_{t}),-,)\) to the oracle.

One might wonder how such misspecification in rewards would affect the regret bound. Our key observation is that since we use the uniform policy to collect data used to train \(\), the generalization error of \(\) is small for any \(a\) due to good coverage of the dataset on each action. Based on this observation, we prove the following theorem for Algorithm2.

**Theorem 2**.: _Under Assumptions1-5, Algorithm2 with certain choice of \(N\) and \(\) guarantees that \(_{}=(_{ }}+^{-2/3}(KT)^{2/3}^{1/3}(||T))\)._

The proof mainly relies on the generalization bounds in Lemma3 and the property of \(G\) in Lemma4, and is deferred to AppendixC. We observe that Algorithm2 enjoys the same dependence on \(T\), \(K\), \(^{-1}\) as Algorithm1. For finite \(\), we can use Vovk's aggregation algorithm (Vovk, 1995) as the regression oracle and achieve \(_{}=(||)\), making the second term in the regret bound negligible.3 While in theory our on-policy algorithm does not seem to be more favorable than the off-policy algorithm (mostly because they both need to uniformly explore for a certain period in order to build \(\)), it can still perform better in practice, as shown in our experiments.

## 4 Experiments

In this section, we apply IGL to learning from image feedback and learning from text feedback. Specifically, we conduct experiments on the MNIST dataset and a conversational dataset to verify the effectiveness of our algorithms and the Lipschitz reward estimator constructed by Eq.6.

### Experiments on Learning from Image Feedback

Experiment SetupWe first conduct experiments on MNIST dataset and implement both the off-policy algorithm Algorithm 1 and the on-policy algorithm Algorithm 2. The setup is as follows: at each step \(t\), the learner receives an image \(x_{t}\) with ground-truth label \(l_{t}\) and picks action \(a_{t}\) from \(\{0,,9\}\) as the predicted label. If the prediction \(a_{t}\) is correct, the learner receives as feedback an image \(y_{t}\) with digit \((l_{t}+1) 10\); otherwise, the learner receives an image with digit \((l_{t}-1) 10\). Therefore, different from the experimental setup in (Xie et al., 2021), our feedback \(y_{t}\) does depend on both the context and the reward. Both function classes \(\) and \(\) are implemented as two-layer convolutional neural networks.

We use PyTorch framework (Paszke et al., 2019) and parameter-free optimizers (Orabona and Tommasi, 2017) to learn the reward estimator and the policy. For both algorithms, we set the number of exploration samples \(N=5000\) and pick the parameter \(\{0,0.05,0.1,0.15,0.2,0.25,0.3\}\) and \(\{+,+\}\). For the on-policy algorithm, we use a time-varying exploration parameter \(_{t}=\) as suggested by Foster and Krishnamurthy (2021). We run the experiments on one NVIDIA GeForce RTX 2080 Ti.

The interaction lasts for \(T=60000\) rounds. We use two metrics to evaluate the performance of the algorithm, including the average progressive reward during the interaction process and the test accuracy on a held-out test set containing \(10000\) samples. When evaluating the on-policy algorithm on the test set, we take actions greedily according to \(_{T}\). Since we use the uniform policy to collect data for learning the reward estimator, the progressive reward at that phase is counted as \(\).

ResultsWe run the experiments with \(4\) different random seeds and report the mean value and standard deviation in Table 1. The running averaged reward over the entire \(T\) rounds are shown in Figure 1. We can see that both algorithms achieve good performance, despite never observing any true labels. While the theoretical regret guarantees for both algorithms are of the same order, empirically, the on-policy Algorithm 2 performs better than Algorithm 1 with over \(90\%\) test accuracy. This is because the on-policy algorithm uses the inverse-gap weighting strategy to achieve a better trade-off between exploration and exploitation, while the off-policy algorithm learns the policy from uniform exploration. On the other hand, to demonstrate the effectiveness of the Lipschitz reward estimator, we compare the performances of the Lipschitz estimator Eq. (6) with a binary reward estimator \(_{}(x,y,a)=1\{_{a}(x,y)\}\), where the parameter \(\) is searched over the same space. The results in Table 1 show that the Lipschitz reward estimator improves over the binary one by a clear margin for both algorithms. This matches our theoretical analysis that highlights the vital role of the Lipschitzness of the reward estimator in obtaining good regret guarantees. We also plot the performance of Algorithm 2 under both true reward and constructed reward in the left figure of Figure 2.4 The figure shows that the constructed reward is indeed a lower bound of the true reward, and the policy can learn from the constructed reward effectively.

### Experiments on Learning From Text Feedback

We further consider an application of learning with text feedback. The IGL framework fits this problem well since this is a natural reward-free setting involving learning from user's text feedback, which is idiosyncratically related to the quality of the actions taken and the question asked. Specifically, given the input of a question, the learner needs to select one of the \(K\) possible answers. Instead of

  
**Algorithm** & **Reward Estimator** & **Average Progressive Reward** & **Test Accuracy** \\  Off-policy Algorithm 1 & Binary & 0.614 (0.012) & 72.6\% (1.5\%) \\  & Lipschitz & 0.638 (0.042) & 75.9\% (4.9\%) \\  On-policy Algorithm 2 & Binary & 0.718 (0.006) & 89.4\% (4.0\%) \\  & Lipschitz & 0.740 (0.022) & 90.3\% (3.7\%) \\   

Table 1: Performance of Algorithm 1 and Algorithm 2 on the MNIST dataset.

receiving whether the selected answer is correct, the learner only receives user's text feedback to the chosen answer. The goal of the learner is to choose the best answer only based on such text feedback.

Dataset ConstructionOur dataset is constructed as follows. Specifically, we construct our question set \(S=\{q_{i}\}_{i}\) from Chatbot Arena datasets (Zheng et al., 2024). Then, for each question \(q_{i} S\) we ask a larger language model \(\) with a high ELO score on the chatsys leaderboard (Chiang et al., 2024) to generate a "good" answer \(g_{i,0}\) with \(r_{i,0}=1\); and ask a (much) smaller language model \(\) with a (much) lower ELO score to generate \(4\) "bad" answers \(b_{i,j}\) with reward \(r_{i,j}=0\), \(j\{1,2,3,4\}\). Specifically, we pick \(\) to be "Qwen1.5-32B-Chat" with ELO score 1134 and \(\) to be "Qwen1.5-0.5B-Chat" with ELO score5 less than 804 (Bai et al., 2023).6 For each question-answer tuple \((q_{i},g_{i,0},b_{i,1},b_{i,2},b_{i,3},b_{i,4})\), we ask another large language model \(\) to simulate a user response \(f_{i,j}\), \(j\{0,1,2,3,4\}\) to the good (bad) answers under the instruction that the user is satisfied (unsatisfied) with the answer. We pick \(\) to be Qwen1.5-32B-Chat as well. This forms our final dataset \(S_{}=\{(q_{i},(g_{i,0},f_{i,0},r_{i,0}),\{(b_{i,j},f_{i,j},r_{i,j}) \}_{j})\}_{i}\). Again, the true reward is never revealed to the learner, and we only use this reward to measure the performance of our algorithm. The prompt we use is deferred to Appendix D.1. We generate our dataset using one A100 GPU for two weeks.

#### 4.2.1 Algorithm Configurations and Results

Given the superior performance of Algorithm 2 over Algorithm 1 on the MNIST dataset shown in Section 4.1, we only test Algorithm 2 on the conversational dataset. We use the first \(N=10000\) data points to learn \(\) and the remaining \(|S|-N=10000\) data points to learn the optimal policy based on the reward function constructed via \(\). We use the same parameter-free optimizer as the one in Section 4.1. Next, we introduce the construction of \(\) and the regression oracle:

Inverse kinematic model.The model class \(\) we consider is the pretrained language model Llama-3-8B-Instruct (AI@Meta, 2024) with an additional multi-class classification head.7 The language model is prompted with a question-answer-feedback tuple \((x,a,y)\); see Appendix D.2 for the prompt. To learn the inverse kinematic model, we use parameter efficient fine-tuning (Mangrulkar et al., 2024) with a rank-1 LORA adapter (Hu et al., 2021) and binary cross entropy loss, which is with respect to the indicator of whether-or-not the predicted action corresponds to the action selected in the tuple.

Figure 1: Running averaged reward of Algorithm 1 and Algorithm 2 on MNIST. Note that Algorithm 1 uniformly explores in the first \(2N=10000\) rounds, and thus its averaged reward at \(t=10000\) is about \(1/K=0.1\).

We successfully learn \(\) using one A100 GPU within 6 hours. After obtaining \(\), we construct the reward estimator \(G(_{a}(x,y),-,)\) with \(=0.1\), \(==\), and \(=1\).

Regression oracle.Similar to the inverse kinematic model, the reward prediction function class \(\) is again based on the pretrained Llama-3-8B-Instruct model but with an additional regression head. Specifically, the language model is prompted only with a question-answer pair \((x,a)\) using the prompt deferred to Appendix D.3 and predicts a score in \(\) for this question-answer pair. The regression oracle again applies parameter efficient fine-tuning with a different rank-1 LORA adapter on the regression loss, which measures the error of predicting the output of the reward predictor \(G(_{a}(x,y),-,)\). This process is done on one A100 GPU within 3 hours.

Results.The empirical results on the conversational dataset are shown in Figure 2. We show the running averaged true reward and the running average constructed reward received by Algorithm 2 after the first \(N=10000\) rounds of learning \(\). The \(x\)-axis is the time horizon and \(y\)-axis is the value of average reward. Similar to our experiment results in Section 4.1, the right figure in Figure 2 shows that our constructed reward estimator is a lower bound on the true reward, matching our theoretical results in Lemma 4, and Algorithm 2 is able to learn the reward effectively through the text feedback with the constructed reward estimator.