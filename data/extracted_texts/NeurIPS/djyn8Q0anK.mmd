# Scalable Transformer for PDE Surrogate Modeling

Zijie Li, Dule Shu, Amir Barati Farimani

Carnegie Mellon University

Mechanical Engineering Department

{zijieli, dules}@andrew.cmu.edu & barati@cmu.edu

###### Abstract

Transformer has shown state-of-the-art performance on various applications and has recently emerged as a promising tool for surrogate modeling of partial differential equations (PDEs). Despite the introduction of linear-complexity attention, applying Transformer to problems with a large number of grid points can be numerically unstable and computationally expensive. In this work, we propose Factorized Transformer (FactFormer), which is based on an axial factorized kernel integral. Concretely, we introduce a learnable projection operator that decomposes the input function into multiple sub-functions with one-dimensional domain. These sub-functions are then evaluated and used to compute the instance-based kernel with an axial factorized scheme. We showcase that the proposed model is able to simulate 2D Kolmogorov flow on a \(256 256\) grid and 3D smoke buoyancy on a \(64 64 64\) grid with good accuracy and efficiency. The proposed factorized scheme can serve as a computationally efficient low-rank surrogate for the full attention scheme when dealing with multi-dimensional problems.

## 1 Introduction

Various physics processes are modeled by partial differential equations (PDEs), from the interaction between atoms in molecular systems to large-scale cosmological phenomena. Solving PDEs advances the understanding of complex physical phenomena, enabling people to make accurate predictions, and make informed decisions across a wide range of scientific and engineering disciplines. Numerical solvers provide a practical way to simulate and predict PDEs since many PDEs are often difficult to solve analytically. Most numerical solvers divide the continuous domain into a discretized grid and reduce the continuous differential equations to algebraic equations via methods like finite difference/element/volume methods or spectral method. Despite the theoretical guarantees behind them, their practical realization of specific problems can pose challenges that require careful expertise to overcome, such as a sufficient understanding of the underlying physics, or a fine-tailored mesh that resolves the necessary spatio-temporal scales. The interest in developing user-friendly and efficient PDE solvers, along with the success of deep learning models in many other areas [13; 44; 51; 103], has facilitated the emergence of neural-network-based PDE solvers, where the neural network can be used to parameterize the solution function of the target equation , or to approximate the solution operator[69; 76]. Compared to many numerical solvers, neural PDE solvers appear to be more tolerant with coarse discretization , and can be applied without explicit meshing . In addition, knowing the underlying equations are not strictly necessary for neural PDE solvers, which gives them the potential to simplify and accelerate the process of physics simulation based on PDEs.

Among various neural network designs, attention-based models (Transformer)  have become state-of-the-art for a wide array of applications [13; 18; 27; 51], which gives rise to a recent surge of interest in applying the Transformer to PDE modeling [16; 29; 35; 41; 43; 54; 67; 73; 82; 86]. By viewing the input sequence as a function sampled on a discretization grid, attention can be interpreted as a learnable kernel integral [17; 35; 54; 60] or a learnable Galerkin projection , and the sequence-to-sequence Transformer  can be modified correspondingly to be better suited forPDE modeling [16; 35; 43; 54; 67; 74; 86]. In these works, attention is typically applied to every grid point in the domain to exploit both the local and non-local structure of the system, and therefore a linear-complexity variant of attention is usually necessary. As the number of grid points grows exponentially with respect to the number of dimensions, this results in a very large attention matrix that computes the interaction between every pair of the grid points (despite this attention matrix is not evaluated explicitly in linear attention). Consequently, cascading a deep stack of attention layers introduces instability and relatively high computational cost on high-resolution grid. To alleviate these issues and improve the scalability of Transformer in PDE modeling, we propose a modified attention mechanism. Our model is inspired by the kernel integral viewpoint of softmax-free attention, with a factorized integration scheme motivated by the inherent low-rank structure of dot-product kernel matrix. More specifically, we propose a multi-dimensional factorized kernel integral with each kernel function in the integral having only single-dimensional domains. To calculate these axial kernels, we propose a learnable integral operator that is able to project the input function with high-dimensional domain into a set of sub-functions with single-dimensional domain. The computation of each axial kernel is quadratic with respect to the number of grid points along that the corresponding axis but does not grow with the number of dimensions, which alleviates the curse of dimensionality in standard attention. With the modified attention mechanism, our proposed model can scale up to multi-dimensional problems with a large number of grid points and achieve competitive performance compared to state-of-the-art models. Moreover, we show that our factorized attention mechanism can reduce the computational cost and improve stability compared to softmax-free linear attention.1

## 2 Related works

Neural PDE solverBased on the emphases of model design, neural PDE solvers can generally be divided into the following groups. The first group of work focuses on using neural networks with mesh-specific architecture design (such as convolutional layers for uniform mesh, or a graph layer for irregular mesh) to learn the spatial and/or temporal correlation of the PDE data [10; 38; 48; 64; 65; 75; 88; 91; 92; 100; 104; 109; 114; 116]. With input-target data collected, the training process can be conducted without the knowledge of underlying PDEs. This can be appealing when the physics of the system is unknown or partially known, such as large-scale climate modeling [61; 83; 90; 97]. The second group of work, namely the Physics-Informed Neural Networks (PINNs)[15; 40; 42; 52; 77; 87; 96; 106; 127], treat neural networks as a parametrization of the underlying solution function. PINNs incorporate the knowledge of the governing equations into the construction of loss function, which includes the residual of the PDE, the consistency with given boundary condition and initial condition. Unlike the previous group of works, PINNs do not necessarily need input-target data and can be trained solely based on equation loss. The third group of works, often referred to as the neural operator, focuses on learning a mapping between the function spaces[5; 8; 9; 16; 36; 43; 50; 54; 60; 68; 70; 71; 76; 78; 86]. Neural operator has the generalization capability within a family of PDE and can potentially be adapted to different discretization without retraining. DeepONet  proposes a practical realization of the universal operator approximation theorem . Concurrent work graph neural operator  proposes a learnable kernel integral to approximate the solution operator of parametric PDEs and the follow-up work Fourier Neural Operator (FNO)  achieves excelling accuracy and efficiency on certain types of problems. Broadly speaking, the

Figure 1: Modelâ€™s prediction (pred.) and reference ground truth (ref.). **Left**: 2D Kolmogorov flow on \(256 256\) grid; **Right**: 3D smoke buoyancy on \(64 64 64\) grid (\(zOy\) cross-section is shown).

operator learning can be conducted upon different types of function bases, such as the Fourier bases [34; 58; 59; 68; 95; 110; 121], wavelet bases , learned bases in an attention layer [16; 67], or based on approximation of the Green's function [7; 108]. The training of neural operators can also be combined with the principle of PINNs to yield a more physically consistent prediction [72; 117]. Our model is closely related to the neural operator, as the major building blocks in our proposed model are a learnable projection operator and a learnable kernel integral operator.

In addition to direct surrogate modeling, neural networks can also be combined with numerical solvers to improve their accuracy and efficiency. For example, using a trained neural network to correct the error of the solver on the fly [3; 28; 56; 89; 113], or doing offline high-fidelity reconstruction [24; 30; 49; 66; 102].

Transformer for Physics SimulationThe Transformer model  have gained outstanding popularity in natural language modeling [13; 25], imagery data processing  and beyond . In the field of physics simulation, Transformer has drawn increasing research interest as a surrogate model for simulation, with its modeling capability demonstrated both as a neural PDE solver [16; 32; 35; 41; 43; 48; 54; 67; 82; 86] and as a pure data-driven model in the absence of a known governing PDE [14; 20; 31; 83]. The dot-product attention can be considered as an approximation of an integral transform with a non-symmetric learnable kernel function [16; 17; 35; 54; 60; 122], which relates Transformer to other popular operator learning models such the FNO . We will expand the discussion of Transformer under the kernel viewpoint in Section 3.

Efficient TransformerFollowing the introduction of Transformer , various works have investigated ways of reducing the computational cost of standard scaled-dot product attention. The first line of work seeks to remove the softmax and make use of matrix associativity to derive linear complexity attention [23; 53; 101], which has also been explored for PDE modeling[16; 43; 67]. The second line of work tries to approximate the dot product between query and key matrix by exploiting the low-rank structure of it [4; 22; 45; 55; 120; 123; 126]. Our work is related to the first group of works with a softmax-free design, but still calculates the dot product between query and key first. Among the second line of work, Axial Transformer is closely related to our work, as both works have explored conducting attention in an axial fashion. However, the derivation of attention matrix is different in the two works (see Section 3.2 for detailed comparison). More generally, the exploitation of the multi-dimensional tensor structure in our proposed model can be related to tensor factorization methods [57; 85] and their applications in various deep learning models [58; 63; 80; 84; 124].

## 3 Method

### Attention mechanism

Standard attentionGiven three sets of vectors, namely the queries \(\{_{i}\}_{i=1}^{N_{q}}\), keys \(\{_{i}\}_{i=1}^{N_{b}}\), and values \(\{_{i}\}_{i=1}^{N_{v}}\) (assuming \(N_{k}=N_{v}\)), attention mechanism [2; 33; 79; 115] dynamically computes a weighted average of the values: \(_{i}=_{j=1}^{N_{v}}h(_{i},_{j})_{j}\), where \(_{i},_{i},_{i}^{1 d}\), \(h()\) is the weight function that determines the contribution of a specific value to the final output. An example of \(h()\) is the scaled-dot product with softmax : \(h(_{i},_{j})=(_{i}_{j}^{T}/ )/_{s}(_{i}_{s}^{T}/)\), and \(\) is usually chosen as \(=\). The queries/keys/values are usually obtained from inputs via learnable projection. In self-attention, all of them are computed from the same source as follow:

\[_{i}=_{i}W_{q},_{i}=_{i}W_{k}, _{i}=_{i}W_{v},\] (1)

where \(_{i}^{1 d_{u}}\) is the input vector and \(\{W_{q},W_{k},W_{v}\}^{d_{u} d}\) are learnable projection matrices. In cross-attention, queries are derived from one input while keys and values are derived from another.

Attention as learnable integralUnder the hood of PDE modeling, the input sequence to the attention layer can be viewed as the sampling of input function on the discretization grid [16; 54; 60; 67]. Kovachki et al.  propose that the scaled-dot product attention  can be viewed as a special case of a Neural Operator , where the attention amounts to the Monte Carlo approximation of the learnable kernel integral. Cao  further proposes two interpretations of softmax-free attention. The first is to view attention as the Fredholm integral equation of the second kind with a learnable asymmetric dot-product kernel, and the second is to view it as a Peterov-Galerkin projection with learnable basis function. The softmax-free attention proposed by Cao  is later extended in OFormer , where Rotary Positional Encoding (RoPE)  is introduced to modulate the dot product and can be viewed as another special case of the kernel integral in Neural Operator style.

In this work, we continue on adopting the learnable kernel integral viewpoint of attention and view each channel of the hidden feature map as the sampling of a specific function on the discretization grid. Given query/key/value matrix \(\{Q,K,V\}^{N d}\), their row vectors: \(_{i}/_{i}/_{i}\), correspond to the sampling of a set of functions \(\{q_{l}(),k_{l}(),v_{l}()\}_{l=1}^{d}\) on grid point \(x_{i}\), where \(\{x_{i}\}_{i=1}^{N}\) discretizes the underlying domain. As a more concrete example, the \(l\)-th column (channel) of \(_{i}\), represents the sampling of function \(q_{l}()\) on a grid point, i.e. \((_{i})^{l}=q_{l}(x_{i})\). Furthermore, softmax-free attention is equivalent to the numerical quadrature of a kernel integral:

\[(_{i})^{l}=_{s=1}^{N}w_{s}(_{i} _{s})(_{s})^{l}_{}(x_{i}, )v_{l}()d,\] (2)

where \(_{i}\) is the output vector, \((x,)=_{l=1}^{d}q_{l}(x)k_{l}()\) is an instance-based kernel and \(w_{s}\) is the quadrature weight. Understanding attention from the perspective of the kernel has been an active topic of research . The theoretical approximation power of different kernel integrals has also been analyzed under the context of PDE learning .

Note that the above kernel does not explicitly depend on the spatial coordinates \((x_{i},)\). For this work, we opt for a modified kernel formulation proposed in OFormer , which modulates the dot product kernel with relative position. Assuming the underlying spatial domain is 1-D (which is sufficient for our proposed model, see next subsection), given query and key vectors \(_{i},_{j}\) and their corresponding spatial coordinates \(x_{i},x_{j}\), RoPE  (\(g(,):^{1 d}^{1  d}\)) is defined as:

\[g(_{i},x_{i})=_{i}(x_{i}),\ \ g(_{j},x_{j})=_{j} (x_{j})\] (3) \[(x_{i})=(R_{1}(x _{i}),,R_{d/2}(x_{i})),\ \ \ R_{l}=( x_{i}_{l})&- ( x_{i}_{l})\\ ( x_{i}_{l})&( x_{i}_{l} ),\]

and \(,_{l}\) are hyperparameters. \(_{l}\) is usually chosen as \(10000^{-2(l-1)/d},l\{1,2,,d/2\}\) following Vaswani et al.  and Su et al. . \(\) is a mesh-based weight that we heuristically set to \(64\) throughout most problems. The projection function \(():^{d}^{d}\) can explicitly modulate the dot product with relative position: \(g(_{i},x_{i})g(_{j},x_{j})^{T}= _{i}(x_{i}-x_{j})_{j}^{T}\), thanks to the following property of rotation matrix: \(R_{l}(x_{i})R_{l}(x_{j})^{T}=R_{l}(x_{i}-x_{j})\).

To summarize, we will adopt attention mechanism in the following form for the proposed model (with modification discussed in the next subsection):

\[Z=w^{T}V,\] (4)

where \(\) denotes a matrix whose row vectors are RoPE encoded as in (3), e.g., \(_{i}=g(_{i},x_{i})\), \(w\) is the quadrature weight with a typical choice of \(1/N\) for uniform quadrature rule, \(Z\) is the output matrix. The query/key/value matrix \(Q/K/V\) is derived from the input via learnable projections defined in (1). The matrix product \(^{T}\) evaluates the kernel function \((,)\) on the discretization grid \(\{x_{i}\}_{i=1}^{N}\).

### Multidimensional factorized attention

Compared to the standard scaled dot product attention that has quadratic complexity with respect to the length of the input sequence, the attention in (4) can enjoy a linear complexity by making use of the associativity of matrix multiplication (calculate \(^{T}V\) first). In PDE modeling, the length of the input sequence is equal to the number of points on the underlying discretization grid. Assuming the \(n\)-dimensional domain is discretized by \(S_{1} S_{2} S_{n}=N\) points, the softmax-free attention in (4) will compute the kernel in (2) with the dot product of \(Q\) and \(K\), which are \(N\) by \(d\) matrices with \(N\) usually much larger than \(d\). The kernel matrix computed is by design low-rank as it is the product of two tall and thin matrices. Meanwhile, attending a large number of grid points to each other can be unstable and the linear attention has a complexity that is quadratic to the channel dimension \(d\), which can limit the scalability of the model in terms of its width. To improve the numerical stability and reduce the computational cost of the aforementioned attention mechanism, we propose a simple yet efficient way to modify the kernel integral discussed in the previous section which is motivated by the low-rank structure of attention. Essentially, our model computes the kernel integral in an axial factorized manner instead of convolving over all the grid points in the domain.

For the following discussion, we will use tensor notation  to describe the operation. We assume the data is represented on a uniform Eulerian grid and can be treated as \(n\)_-way_ tensor \(U^{S_{1} S_{2} S_{n}}\)2. The product of it with a matrix \(W^{{}^{J S_{m}}}\) across the \(m\)-th mode will result in a tensor of shape \(S_{1} S_{m-1} J S_{m+1} S_{n}\), whose elements are defined as:

\[(U_{m}W)_{i_{1}i_{2} i_{m-1}ji_{m+1} i_{n}}=_{i_{m}=1}^{S _{m}}U_{i_{1}i_{2} i_{m} i_{n}}W_{ji_{m}}.\] (5)

Learnable projectionThe first major component of the proposed framework is a set of learnable integral operators \(\{^{(1)},^{(2)},,^{(n)}\}\) that projects the input function \(u:^{n}^{d}\) into a set of functions with one-dimensional domain \(\{^{(1)},^{(2)},,^{(n)}\}^{d}\), which is defined as:

\[^{(m)}(x_{i}^{(m)})=^{(m)}(u)(x_{i}^{(m)})\] (6) \[=h^{(m)}(w\!_{_{1}}\!\!_{_{n}}\! ^{(m)}(u(_{1},,_{m-1},x_{i}^{(m)},_{m+1},,_{n}))d_{1}\!\!d_{m-1}d_{m+1}\!\!d_{n} ),\]

where \(h^{(m)}():^{d}^{d}\) and \(^{(m)}():^{d}^{d}\) are pointwise learnable functions and \(w=1/(L_{1}L_{2} L_{m-1}L_{m+1} L_{n})\), with \(L_{m}\) being the size of domain \(_{m}\) discretized by \(\{x_{i}^{(m)}\}_{i=1}^{S_{m}}\). In practice, we implement \(h^{(m)}\) as a three-layer multi-layer perception (MLP) similar to the feedforward network in Transformer  and \(^{(m)}\) as a simple linear transformation. When the underlying grid is uniform, (6) simply amounts to first transforming the input with pointwise learnable functions \(^{(m)}()\), applying mean pooling over all but the \(m\)-th spatial dimension, and then applying another pointwise learnable function \(h^{(m)}\).

Factorized kernel integralEquipped with the above projection module, we now introduce our factorized kernel integral scheme. More specifically, we propose to use the following integral to replace the kernel integral in (2):

\[& z(x_{i_{1}}^{(1)},x_{i_{2}}^{(2)},,x_{ i_{n}}^{(n)})\\ &=_{_{1}}^{(1)}(x_{i_{1}}^{(1)},_{1})_{ _{2}}^{(2)}(x_{i_{2}}^{(2)},_{2})_{_{n}} ^{(n)}(x_{i_{n}}^{(n)},_{n})v(_{1},_{2},,_{n})d_{1}d_{ 2} d_{n},\] (7)

where kernels \(\{^{(1)},,^{(n)}\}: \) are computed based on projected single-dimensional function along each axis, \(v():^{n}^{d}\) is derived from the input function \(u\) via linear transformation (just as the value in standard attention). Next, we will discuss how the above kernel integral is implemented in practice. Using the learnable projection operator defined in (6), we can obtain \(\{^{(1)},,^{(n)}\}\) (\(^{(m)}^{S_{m}}\)) from the input \(U^{S_{1} S_{n} d}\), where the \(i_{m}\)-th row of \(\) is the evaluation of projected function \(^{(m)}()\) at \(x_{i_{m}}\): \(_{i_{n}}^{(m)}=^{(m)}(x_{i_{m}})\). Then we apply linear transformation on them to obtain the query/key matrix just as standard attention: \(Q^{(m)}=^{(m)}W_{q}^{(m)},K^{(m)}=^{(m)}W_{k}^{(m)}\), where \(\{W_{q}^{(m)},W_{k}^{(m)}\}^{d d}\) are learnable matrices. The query and key are used to compute the kernel matrix \(A^{(m)}^{S_{m} S_{m}}\):

\[A^{(m)}=w_{m}^{(m)}(^{(m)})^{T},\] (8)

Figure 2: Schematic of the factorized kernel attention. **Upper path**: the input is transformed into the _Value_ via a linear transformation. **Lower path**: the input is first projected into multiple sub-functions with a one-dimensional domain. These sub-functions are then used to derive the _Query_ and _Key_ on each axis, and their dot products form the kernel function of the corresponding axis. The _Value_ is iteratively updated by the kernel integral transform along each axis and finally sent to an MLP.

where \(w_{m}\) is the mesh weight, \(\) denotes the RoPE encoded matrix as discussed in (4), the \(i\)-th row and \(j\)-th column of \(A^{(m)}\) represents the kernel value \(^{(m)}(x_{i}^{(m)},x_{j}^{(m)})\). Despite (8) has a quadratic complexity with respect to the grid size \(S_{m}\), this is an affordable cost for most of the problems where the axial grid size \(S_{m}\) is mostly between \(64\) to \(512\). Meanwhile, the value \(V^{S_{1} S_{n} d}\) is derived from the input via a linear transformation (i.e. \((m+1)\)-th mode product): \(V=U_{n+1}W_{v}\), where \(W_{v}^{d d}\) is again a learnable matrix. The overall factorized kernel integral is numerically approximated with the following tensor-matrix product (Figure 2):

\[Z=(U)=V_{1}A^{(1)}_{2}A^{(2)}_{n}A^{( n)}.\] (9)

In (9), the computation of all kernel is of complexity \(O(S_{1}^{2}d+S_{2}^{2}d++S_{n}^{2}d)\), and the time complexity of a single tensor-matrix product \(V_{m}A^{(m)}\) is \(O(NS_{m}d)\). After evaluating the tensor-matrix product, the output tensor \(Z\) will be sent to a pointwise feedforward network \(f():^{d}^{d}\). To sum up, the update protocol of a single layer in our proposed factorized Transformer is defined as follows:

\[U f(((U)))+U,\] (10)

where \(()\) is the attention from (9), \(()\) is instance normalization  that normalizes each channel instance-wise.

It is worth pointing out that the axial factorized kernel proposed here shares some similarities with the Axial Transformer proposed in Ho et al. , but has two significant differences despite the connection. Firstly, Axial Transformer reduces the computational cost by constraining the context of attention along each axis (e.g. a pixel can only attend to other pixels on the same row), which amounts to moving all but one axis to the batch dimension. In this way computing the axial kernel matrix is of \(O(NS_{m}d)\) complexity (recall \(N=S_{1} S_{n}\)) instead of \(O(S_{m}^{2}d)\) as in our model. And its overall computation of attention is relatively more expensive due to the presence of softmax. Secondly, the decomposition in Axial Transformer is not layer-wise. For example, in the first layer, the attention is conducted in a row-wise manner and then the second block will conduct attention in a column-wise manner, whereas our model decomposes attention along all axes into a tensor-matrix product within every layer. We provide an illustrative example in Figure 27 of the Appendix.

### Training techniques

In this subsection, we will discuss several techniques used for training the model (including baselines) in our numerical experiments. In general, these techniques aim to alleviate the compounding error of autoregressive neural PDE solvers when applied to time-dependent PDEs.

Latent marchingIt is proposed in the Li et al.  that a simple pointwise learnable function \((,)^{d}_{>0}^{d}\) can be used to propagate dynamics in the latent space with a fixed time interval \( t\): \(z(x,t+ t)=z(x,t)+(z(x,t ),t)\), where \(z\) is the output of the final attention layer. In practice, \(\) is implemented as a pointwise MLP and is efficient to compute. Leveraging this technique, with one call to the neural solver, we can forward the state for multiple time steps (by marching in the latent space for \(k\) steps), thus reducing the total number of calls by a ratio of \(k\). This is in principle similar to the _Temporal Bundling_ technique proposed in Message-Passing Neural PDE solver (MP-PDE) , yet different in practical realization. In MP-PDE, the multi-timestep prediction is implemented as first predicting the difference in time \(\{d_{1},d_{2},,d_{k}\}\) and then adding them to the input \(u_{0}\) by a forward Euler scheme in the physical space: \(_{k}=u_{0}+d_{k} t\). In this work, we opt for the latent marching to predict multi-timesteps as the forward Euler scheme (in the physical space) is less stable for fluid problems with relatively large time step sizes.

PushforwardNeural PDE solvers are observed to be unstable for time-dependent problems. A small error or perturbation that occurs at the beginning of neural PDE solvers' prediction, can easily result in an unbounded rollout error. While there is hardly a universal method for guaranteeing their stability, a wide array of techniques have been proposed to improve the stability of neural PDE solvers, such as adding physics constraints [72; 118; 119], rollout training  or adding random-walk noise [91; 100; 104]. For this work, we adopt the _pushforward_ technique from MP-PDE, which amounts to rolling out the model for two steps during training and then letting the gradient only flows through the last step. This allows training the model on error-corrupted samples and promotes the stability of the model. From a practical perspective, this is straightforward to implement and also computationally much cheaper than standard rollout training.

Experiment

In this section, we will investigate our proposed model numerically on several challenging problems. Furthermore, we compare our model against softmax-free attention [16; 67]. The baseline models we compared against are Fourier Neural Operator (FNO) , Factorized Fourier Neural Operator (F-FNO)  and Dilated ResNet (Dil-ResNet) [44; 125]. FNO has been shown to have good accuracy on a wide range of PDE problems and is computationally very efficient owing to the Fast Fourier Transformation (FFT). F-FNO factorizes the spectral convolution in FNO into separate spectral convolution along different axes and adopt an improved residual connection formulation like Transformer . Dil-ResNet is recently introduced by Stachenfeld et al.  to learn the coarse-grained dynamics of turbulent flow and has demonstrated state-of-the-art performance across several problems. We adopt the implementation of Dil-ResNet with group normalization from PDEArena . On 2D steady-state problem where linear attention's computational cost is affordable, we also include the result from Galerkin Transformer , which uses CNN to project the function onto a coarse grid and applies linear attention on the coarse grid. The implementation details of the proposed model and baselines are available in Section A, B of the Appendix.

### Benchmark problems

We first apply our model to three fluid-like systems, where the underlying physics patterns are sensitive to the spatiotemporal scale that discretization can resolve, and typically require fine discretization for classical numerical solvers. In these problems, the neural PDE solver is trained to predict the next frame (or multiple frames if using latent marching) given a context of previous frames. The number of context frames of Kolmogorov flow and isotropic turbulence is set to \(10\) following [68; 72], and \(4\) for smoke buoyancy similar to . We also consider a well-known steady-state problem-2D Darcy flow, which has been studied in many of the previous works. Below we provide a brief description of each problem we studied. More details can be found in Section E of the Appendix.

2D Kolmogorov flowThe first example is 2D Kolmogorov flow governed by incompressible Navier-Stokes equation with a periodic boundary condition. The Reynolds number _Re_ determines how turbulent the system will be. We adopt the setting of forced turbulence following Kochkov et al.  and generate the data by using the pseudo-spectral method to simulate fluid flow with Reynolds number \(=1000\). The objective is to predict the vorticity \(\) of the flow field within an interval \([t_{0},t_{0}+T]\), where \(T=1\)s and \(t_{0}\) is a random starting point in the sequence. We use a spatial grid of \(256 256\) and temporal discretization of \( t=0.0625\)s (therefore \(1\)s corresponds to 16 frames) to train and evaluate the model.

3D isotropic turbulenceThe second example is 3D isotropic turbulence governed by incompressible Navier-Stokes equation with a periodic boundary condition. The major difference from the first example is that the vortex stretching term is non-zero for three-dimensional flow. We use the 3D spectral simulator from Mortensen and Langtangen , which simulates the forced turbulence described in Lamorgese et al. . For generating the dataset, we simulate a system of Taylor Reynolds number \(_{}=84\). The objective is to predict the pressure \(p\) and velocity \(\) from \(t=0.5\) to \(t=1\)s (10 frames). The model is trained and evaluated on a \(60 60 60\) spatial grid with \( t=0.05\)s.

3D smoke buoyancyThe third example is 3D buoyancy-driven flow, which depicts smoke volume rising in a closed domain. A similar system in 2D formulation has been studied in several previous works [11; 113]. The underlying governing equation is the incompressible Navier-Stokes equation coupled with an advection equation. The boundary condition for the smoke field is Dirichlet while the boundary condition for the flow field is Neumann. The advection equation describes the motion of smoke, which is transported along the flow field. We modify the solver from  that is implemented in _phiflow_ to generate the data, with buoyancy factor set to \(0.5\) and viscosity \(=0.003\). The objective is to predict the scalar density field of smoke \(d\) and velocity of flow \(\) from \(t=3\) to \(t=15\)s (16 frames). The model is trained and evaluated on a \(64 64 64\) spatial grid with \( t=0.75\)s. To account for non-periodic boundary conditions, we pad the domain for FNO variants and DilResNet following the original works. For FactFormer, we append a simple CNN block after the model, which comprises \(3\)-by-\(3\) convolutional layers with zero padding.

2D Darcy flowIn addition to the above time-depedendent systems, the fourth example is 2D steady-state problem from Li et al. . Given the diffusion coefficient, the model predicts the steady-state flow field. The boundary condition is also Dirichlet so we adopt settings for all models similar to the 3D smoke problem.

### Results and discussion

For all the models, we study two protocols of training. The first is Latent Marching with Pushforward (denote as **LM**). The second is simply Autoregressive (denote as **AR**), where the model is rolled out for two steps during training. For LM models, each call to the model will output \(k\) future steps. On 2D Kolmogorov flow/3D smoke buoyancy, \(k\) is set to \(4\), and \(2\) for 3D isotropic turbulence. We interleave pushforward training with standard per-step training for LM models. The relative \(L^{2}\) norm is used to train and measure the error of each model following Li et al. . The sequence-wise averaged error and the frame-wise error at the end frame are reported in Table 1, 2, 3. We also report the time cost of simulating a sequence and the number of parameters for each model. The frame-wise error trends are shown in Figure 6, 7, 8, 9, 10 in the Appendix. The visualization of predicted samples are provided in the Section F of Appendix.

We observe that Dil-ResNet has a slightly better per-frame fitting capability compared to the other models on 3D flow problems. As shown in the loss trend plots, it starts at a lower error compared to other models. This coincides with the observation in Stachenfeld et al.  where Dil-ResNet's performance is strong on 3D fluid problems. On 2D flow, F-FNO has the best accuracy compared to other models. Interestingly, FactFormer can catch up with Dil-ResNet on 2D Kolmogorov flow and 3D smoke buoyancy when the time duration becomes longer. Yet for shorter-term prediction - 3D isotropic turbulence, Dil-ResNet still has the best final accuracy. This suggests that the accuracy of long-term prediction can potentially benefit from exploiting the global structure that lies in the input. Nonetheless, compared to Dil-ResNet, FactFormer offers superior efficiency as indicated by the inference time (time cost of simulating a sequence). Since the training time is roughly proportional to the model forward time, on 3D problems Dil-ResNet generally takes 3-4 times longer to train. In terms of different training strategies, we find that AR models are less stable than multi-step training (LM) and computationally more expensive as it requires more calls to the neural solver. Despite the average error varies case by case, LM models' error generally accumulates slower on the problems we studied, whereas AR models quickly blow up in some cases.

Lastly, while Dil-ResNet has shown good accuracy for 3D flow problems, its performance is highly dependent on the training discretizations. As shown in the Figure 3, without changing model architecture, its evaluation errors increases significantly when the resolution increases, while Transformer-based models and FNO models' performance are roughly invariant to the resolution. This highlights a major difference between CNN-based models and neural operators.

    &  &  &  &  \\   & AR\({}^{*}\) & LM & AR\({}^{*}\) & LM & AR & LM & AR & LM \\  \(\) avg. error & 0.3177 & 0.2978 & **0.1486** & 0.2453 & 0.8156 & 0.1655 & 0.8835 & 0.1734 \\ \(\) final error & 0.4423 & 0.4567 & **0.2811** & 0.3861 & 1.1692 & 0.3051 & 1.0963 & 0.3017 \\  Inf. time (s) & 0.73 & 0.81 & 0.86 & 1.01 & 4.69 & 1.78 & 3.14 & 1.38 \\ \# params (M) & 85.1 & & 3.7 & & 2.4 & & 3.5 \\   

Table 1: Evaluation results of 2D Kolmogorov flow. A batch size of 10 is used for inference. LM models predict 4 steps with each call to the model. Total prediction length is 16 steps. **AR\({}^{*}\)**: Since for 2D problem FNO variants can afford to rollout more steps during training, AR FNO rollout for 12 steps, AR F-FNO rollout for 6 steps, whereas other AR models rollout for 2 steps during training. For model that has complex parameters, each cfloat parameter count as two paramaters.

    &  &  &  &  \\   & AR & LM & AR & LM & AR & LM & AR & LM \\  \(p\) avg. error & 0.8080 & 0.4634 & 0.3151 & 0.3264 & **0.1725** & 0.1778 & 0.2989 & 0.2545 \\ \(p\) final error & 1.1285 & 0.6522 & 0.4250 & 0.4159 & 0.2573 & **0.2448** & 0.4407 & 0.3431 \\  \(\) avg. error & 0.3967 & 0.3382 & 0.2298 & 0.2303 & **0.1143** & 0.1250 & 0.1775 & 0.1670 \\ \(\) final error & 0.6561 & 0.4735 & 0.2799 & 0.2850 & 0.1675 & **0.1671** & 0.2594 & 0.2218 \\  Inf. time (s) & 1.01 & 0.91 & 2.77 & 1.37 & 12.67 & 6.89 & 2.68 & 1.31 \\ \# params (M) & 509.8 & & 3.0 & & 6.9 & & 5.1 \\   

Table 2: Evaluation results of 3D isotropic turbulence. A batch size of 4 is used for inference. LM models predict 2 steps with each call to the model. Total prediction length is 10 steps.

### Comparison against full attention

In this subsection, we will present an ablation study of the proposed factorized attention mechanism with softmax-free attention (denoted as "linear attention") previously applied to PDE modeling. More specifically, we employ the attention from Li et al.  (in the form of (4)) to replace factorized attention in (10), with \(,V\) normalized column-wise via instance normalization, e.g. \(||V_{,j}||_{2}=1\). To accommodate for the memory cost of linear attention, we further downsample the 2D Kolmogorov flow discussed in the last subsection to a \(128 128\) grid and train both linear and factorized attention models on it (with latent marching and pushforward trick).

Comparison of performanceWe compare the accuracy and computational cost of the two attention mechanisms in Figure 4 and Table 4. While in principle full attention could have better approximation capacity than factorized attention, in practice we find that it performs worse than factorized attention on this problem we studied. Specifically, its rollout is less stable and results in a degraded accuracy. We hypothesize that this is due to the instability of iteratively calculating the attention matrix of a large size, as rolling out the prediction requires recursively calling the model multiple times. In

    & \) Norm} &  &  \\   & & Enc. time (s) & Prop. time (s) & Time (s) & Mem. (MB) \\  Factorized attention & 0.1529 & 0.0202 & & 0.0954 & 5217 \\ Linear attention (matmul) & 0.5853 & 0.1370 & 0.0112 & 0.3013 & 12029 \\ Linear attention (einsum) & 0.1333 & & 0.2938 & 12029 \\   

Table 4: Comparison between factorized and linear attention on their forward/backward computational cost, Mem. denotes the peak memory usage. The benchmark is carried out using PyTorch 1.8.2 on an RTX 3090, with a batch size of 4. **Enc. time**: the time spent on obtaining the latent encoding, primarily includes attention layers and feedforward layers after each attention layer; **Prop. time**: the time used to propagate dynamics in the latent space with a 3-layer MLP.

addition to the accuracy improvement, the benchmark on computation empirically demonstrates the computational efficiency improvement of factorized attention over linear attention. We provide more detailed comparison between factorized attention and linear attention in the Section D of Appendix, where we observed consistent efficiency improvement with different grid sizes and model sizes.

Pattern of attention matricesWe also investigate the structure of different attention matrices. By construction, when using softmax-free attention to compute the kernel integral in (2), the kernel matrix \(A=QK^{T}\) is going to have a low-rank structure since \((A)((Q),(K))\), \(Q,K^{N d}\) and usually\(N>>d\). After training, we compute the attention matrices based on \(100\) samples and conduct singular value decomposition (SVD) on them. We define the total energy of the spectrum as the sum of singular values \(E=_{i}_{i}\), where \(_{i}\) is the \(i\)-th singular value and report the normalized cumulative energy histogram \(b_{k}=_{i=1}^{k}_{i}/_{i}_{i}\) in Figure 4(a), 4(b), 4(c). For each layer, the histogram is averaged across the attention matrices of all heads. Observe that for linear attention, its rank is relatively low as less than 5% of the singular values capture over 90% of the total energy, which is similar to the trend observed from previous works studying the rank of standard softmax-attention [6; 26; 120]. Note that the spectrum of linear attention is based on a truncated SVD and therefore its rank will be even lower if a full SVD is performed. The highly low-rank structure of the full attention matrix hints the potential to approximate with or decomposed into smaller and more compact matrices, and our proposed factorized scheme is one example.

## 5 Conclusion

In this work, we propose an end-to-end Transformer for PDE modeling, which features a learnable projection operator and a factorized kernel integral. We demonstrate that the proposed model balances efficiency and accuracy well, making it a promising and scalable solution for PDE surrogate modeling. However, the proposed attention mechanism is still not free from the curse of dimensionality. The computation of the factorized kernel integral requires evaluating the function on all \(S_{1} S_{2} S_{m}\) grid points. A future direction could be extending the factorization scheme to a more efficient tensor decomposition format like tensor-train. The proposed model currently exploits the uniform structure of the underlying grids and use mean pooling when doing projection, but non-uniform quadrature weight will be necessary when applying to non-uniform grids. It is also observed that the proposed model and other neural PDE solvers can be unstable due to the error accumulation when solving time-dependent systems.