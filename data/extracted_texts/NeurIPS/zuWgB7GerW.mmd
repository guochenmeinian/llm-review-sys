# How DNNs break the Curse of Dimensionality:

Compositionality and Symmetry Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We show that deep neural networks (DNNs) can efficiently learn any composition of functions with bounded \(F_{1}\)-norm, which allows DNNs to break the curse of dimensionality in ways that shallow networks cannot. More specifically, we derive a generalization bound that combines a covering number argument for compositionality, and the \(F_{1}\)-norm (or the related Barron norm) for large width adaptivity. We show that the global minimizer of the regularized loss of DNNs can fit for example the composition of two functions \(f^{*}=h g\) from a small number of observations, assuming \(g\) is smooth/regular and reduces the dimensionality (e.g. \(g\) could be the modulo map of the symmetries of \(f^{*}\)), so that \(h\) can be learned in spite of its low regularity. The measures of regularity we consider is the Sobolev norm with different levels of differentiability, which is well adapted to the \(F_{1}\) norm. We compute scaling laws empirically and observe phase transitions depending on whether \(g\) or \(h\) is harder to learn, as predicted by our theory.

## 1 Introduction

One of the fundamental features of DNNs is their ability to generalize even when the number of neurons (and of parameters) is so large that the network could fit almost any function . Actually DNNs have been observed to generalize best when the number of neurons is infinite [8; 21; 20]. The now quite generally accepted explanation to this phenomenon is that DNNs have an implicit bias coming from the training dynamic where properties of the training algorithm lead to networks that generalize well. This implicit bias is quite well understood in shallow networks [11; 36], in linear networks [24; 30], or in the NTK regime , but it remains ill-understood in the general deep nonlinear case.

In both shallow networks and linear networks, one observes a bias towards small parameter norm (either implicit  or explicit in the presence of weight decay ). Thanks to tools such as the \(F_{1}\)-norm , or the related Barron norm , or more generally the representation cost , it is possible to describe the family of functions that can be represented by shallow networks or linear networks with a finite parameter norm. This was then leveraged to prove uniform generalization bounds (based on Rademacher complexity) over these sets , which depend only on the parameter norm, but not on the number of neurons or parameters.

Similar bounds have been proposed for DNNs [7; 6; 39; 33; 25; 40], relying on different types of norms on the parameters of the network. But it seems pretty clear that we have not yet identified the 'right' complexity measure for deep networks, as there remains many issues: these bounds are typically orders of magnitude too large [29; 23], and they tend to explode as the depth \(L\) grows .

Two families of bounds are particularly relevant to our analysis: bounds based on covering numbers which rely on the fact that one can obtain a covering of the composition of two function classes fromcovering of the individual classes [7; 25], and path-norm bounds which extend the techniques behind the \(F_{1}\)-norm bound from shallow networks to the deep case [32; 6; 23].

Another issue is the lack of approximation results to accompany these generalization bounds: many different complexity measures \(R()\) on the parameters \(\) of DNNs have been proposed along with guarantees that the generalization gap will be small as long as \(R()\) is bounded, but there are often little to no result describing families of functions that can be approximated with a bounded \(R()\) norm. The situation is much clearer in shallow networks, where we know that certain Sobolev spaces can be approximated with bounded \(F_{1}\)-norm .

We will focus on approximating composition of Sobolev functions, and obtaining close to optimal rates. This is quite similar to the family of tasks considered , though the complexity measure we consider is quite different, and does not require sparsity of the parameters.

### Contribution

We consider Accordion Networks (AccNets), which are the composition of multiple shallow networks \(f_{L:1}=f_{L} f_{1}\), we prove a uniform generalization bound \((f_{L:1})-}_{N}(f_{L:1}) R(f_{1},,f_ {L})}\), for a complexity measure

\[R(f_{1},,f_{L})=_{=1}^{L}Lip(f_{})_{=1}^{L}\|_{F_{1}}}{Lip(f_{})}+d_{-1}}\]

that depends on the \(F_{1}\)-norms \(\|f_{}\|_{F_{1}}\) and Lipschitz constants \(Lip(f_{})\) of the subnetworks, and the intermediate dimensions \(d_{0},,d_{L}\). This use of the \(F_{1}\)-norms makes this bound independent of the widths \(w_{1},,w_{L}\) of the subnetworks, though it does depend on the depth \(L\) (it typically grows linearly in \(L\) which is still better than the exponential growth often observed).

Any traditional DNN can be mapped to an AccNet (and vice versa), by spliting the middle weight matrices \(W_{}\) with SVD \(USV^{T}\) into two matrices \(U\) and \(V^{T}\) to obtain an AccNet with dimensions \(d_{}=W_{}\), so that the bound can be applied to traditional DNNs with bounded rank.

We then show an approximation result: any composition of Sobolev functions \(f^{*}=f_{L^{*}}^{*} f_{1}^{*}\) can be approximated with a network with either a bounded complexity \(R()\) or a slowly growing one. Thus under certain assumptions one can show that DNNs can learn general compositions of Sobolev functions. This ability can be interpreted as DNNs being able to learn symmetries, allowing them to avoid the curse of dimensionality in settings where kernel methods or even shallow networks suffer heavily from it.

Empirically, we observe a good match between the scaling laws of learning and our theory, as well as qualitative features such as transitions between regimes depending on whether it is harder to learn the symmetries of a task, or to learn the task given its symmetries.

## 2 Accordion Neural Networks and ResNets

Our analysis is most natural for a slight variation on the traditional fully-connected neural networks (FCNNs), which we call Accordion Networks, which we define here. Nevertheless, all of our results can easily be adapted to FCNNs.

Accordion Networks (AccNets) are simply the composition of \(L\) shallow networks, that is \(f_{L:1}=f_{L} f_{1}\) where \(f_{}(z)=W_{}(V_{}z+b_{})\) for the nonlinearity \(:\), the \(d_{} w_{}\) matrix \(W_{}\), \(w_{} d_{-1}\) matrix \(V_{}\), and \(w_{}\)-dim. vector \(b_{}\), and for the widths \(w_{1},,w_{L}\) and dimensions \(d_{0},,d_{L}\). We will focus on the ReLU \((x)=\{0,x\}\) for the nonlinearity. The parameters \(\) are made up the concatenation of all \((W_{},V_{},b_{})\). More generally, we denote \(f_{_{2}:_{1}}=f_{_{2}} f_{_{1}}\) for any \(1_{1}_{2} L\).

We will typically be interested in settings where the widths \(w_{}\) is large (or even infinitely large), while the dimensions \(d_{}\) remain finite or much smaller in comparison, hence the name accordion.

If we add residual connections, i.e. \(f_{1:L}^{res}=(f_{L}+id)(f_{1}+id)\) for the same shallow nets \(f_{1},,f_{L}\) we recover the typical ResNets.

_Remark_.: The only difference between AccNets and FCNNs is that each weight matrix \(M_{}\) of the FCNN is replaced by a product of two matrices \(M_{}=V_{}W_{-1}\) in the middle of the network (such a structure has already been proposed ). Given an AccNet one can recover an equivalent FCNN by choosing \(M_{}=V_{}W_{-1}\), \(M_{0}=V_{0}\) and \(M_{L+1}=W_{L}\). In the other direction there could be multiple ways to split \(M_{}\) into the product of two matrices, but we will focus on taking \(V_{}=U\) and \(W_{-1}=V^{T}\) for the SVD decomposition \(M_{}=USV^{T}\), along with the choice \(d_{}=M_{}\).

### Learning Setup

We consider a traditional learning setup, where we want to find a function \(f:^{d_{in}}^{d_{out}}\) that minimizes the population loss \((f)=_{x}[(x,f(x))]\) for an input distribution \(\) and a \(\)-Lipschitz and \(\)-bounded loss function \((x,y)[0,B]\). Given a training set \(x_{1},,x_{N}\) of size \(N\) we approximate the population loss by the empirical loss \(}_{N}(f)=_{i=1}^{N}(x_{i},f(x_{i}))\) that can be minimized.

To ensure that the empirical loss remains representative of the population loss, we will prove high probability bounds on the generalization gap \(}_{N}(f)-(f)\) uniformly over certain functions families \(f\).

For **regression tasks**, we assume the existence of a true function \(f^{*}\) and try to minimize the distance \((x,y)=\|f^{*}(x)-y\|^{p}\) for \(p 1\). If we assume that \(f^{*}(x)\) and \(y\) are uniformly bounded then one can easily show that \((x,y)\) is bounded and Lipschitz. We are particularly interested in the cases \(p\{1,2\}\), with \(p=2\) representing the classical MSE, and \(p=1\) representing a \(L_{1}\) distance. The \(p=2\) case is amenable to 'fast rates' which take advantage of the fact that the loss increases very slowly around the optimal solution \(f^{*}\), We do not prove such fast rates (even though it might be possible) so we focus on the \(p=1\) case.

For **classification tasks** on \(k\) classes, we assume the existence of a 'true class' function \(f^{*}:\{1,,k\}\) and want to learn a function \(f:^{k}\) such that the largest entry of \(f(x)\) is the \(f^{*}(k)\)-th entry. One can consider the hinge cost \((x,y)=\{0,1-(y_{f^{*}(k)}-_{i f^{*}(x)}y_{i})\}\), which is zero whenever the margin \(y_{f^{*}(k)}-_{i f^{*}(x)}y_{i}\) is larger than \(1\) and otherwise equals \(1\) minus the margin. The hinge loss is Lipschitz and bounded if we assume bounded outputs \(y=f(x)\). The cross-entropy loss also fits our setup.

## 3 Generalization Bound for DNNs

The reason we focus on accordion networks is that there exists generalization bounds for shallow networks [5; 44], that are (to our knowledge) widely considered to be tight, which is in contrast to the deep case, where many bounds exist but no clear optimal bound has been identified. Our strategy is to extend the results for shallow nets to the composition of multiple shallow nets, i.e. AccNets. Roughly speaking, we will show that the complexity of an AccNet \(f_{}\) is bounded by the sum of the complexities of the shallow nets \(f_{1},,f_{L}\) it is made of.

We will therefore first review (and slightly adapt) the existing generalization bounds for shallow networks in terms of their so-called \(F_{1}\)-norm , and then prove a generalization bound for deep AccNets.

### Shallow Networks

The complexity of a shallow net \(f(x)=W(Vx+b)\), with weights \(W^{w d_{out}}\) and \(V^{d_{in} w}\), can be bounded in terms of the quantity \(C=_{i=1}^{w}\|W_{ i}\|\|^{2} +b_{i}^{2}}\). First note that the rescaled function \(f\) can be written as a convex combination \(f(x)=_{i=1}^{w}\|\|^{2}+b_{i}^{2}}}{C}_{ i}(_{i}x+_{i})\) for \(_{ i}=}{\|W_{ i}\|}\), \(_{i}.=}{\|^{2}+b_{i}^{2}}}\), and \(_{i}=}{\|^{2}+b_{i}^{2}}}\), since the coefficients \(\|\|^{2}+b_{i}^{2}} }{C}\) are positive and sum up to 1. Thus \(f\) belongs to \(C\) times the convex hull

\[B_{F_{1}}=\{x w(v^{T}x+b):\|w\|^{2} =\|v\|^{2}+b^{2}=1\}.\]We call this the \(F_{1}\)-ball since it can be thought of as the unit ball w.r.t. the \(F_{1}\)-norm \(\|f\|_{F_{1}}\) which we define as the smallest positive scalar \(s\) such that \(f B_{F_{1}}\). For more details in the single output case, see .

The generalization gap over any \(F_{1}\)-ball can be uniformly bounded with high probability:

**Theorem 1**.: _For any input distribution \(\) supported on the \(L_{2}\) ball \(B(0,b)\) with radius \(b\), we have with probability \(1-\), over the training samples \(x_{1},,x_{N}\), that for all \(f B_{F_{1}}(0,R)=R B_{F_{1}}\)_

\[(f)-}_{N}(f) bR+d_{out}} }+c_{0}}\]

This theorem is a slight variation of the one found in : we simply generalize it to multiple outputs, and also prove it using a covering number argument instead of a direct computation of the Rademacher complexity, which will be key to obtaining a generalization bound for the deep case. But due to this change of strategy we pay a \( N\) cost here and in our later results. We know that the \( N\) term can be removed in Theorem 1 by switching to a Rademacher argument, but we do not know whether it can be removed in deep nets.

Notice how this bound does not depend on the width \(w\), because the \(F_{1}\)-norm (and the \(F_{1}\)-ball) themselves do not depend on the width. This matches with empirical evidence that shows that increasing the width does not hurt generalization [8; 21; 20].

To use Theorem 1 effectively we need to be able to guarantee that the learned function will have a small enough \(F_{1}\)-norm. The \(F_{1}\)-norm is hard to compute exactly, but it is bounded by the parameter norm: if \(f(x)=W(Vx+b)\), then \(\|f\|_{F_{1}}(\|W\|_{F}^{2}+\| V\|_{F}^{2}+\|b\|^{2})\), and this bound is tight if the width \(w\) is large enough and the parameters are chosen optimally. Adding weight decay/\(L_{2}\)-regularization to the cost then leads to bias towards learning with small \(F_{1}\) norm.

### Deep Networks

Since an AccNet is simply the composition of multiple shallow nets, the functions represented by an AccNet is included in the set of composition of \(F_{1}\) balls. More precisely, if \(\|W_{}\|^{2}+\|V_{}\|^{2}+\|b_{} \|^{2} 2R_{}\) then \(f_{L:1}\) belongs to the set \(\{g_{L} g_{1}:g_{} B_{F_{1}}(0,R_{})\}\) for some \(R_{}\), which is width agnostic.

As already noticed in , the covering number number is well-behaved under composition, this allows us to bound the complexity of AccNets in terms of the individual shallow nets it is made up of:

**Theorem 2**.: _Consider an accordion net of depth \(L\) and widths \(d_{L},,d_{0}\), with corresponding set of functions \(=\{f_{L:1}:\|f_{}\|_{F_{1}} R_{},( f_{})_{}\}\). With probability \(1-\) over the sampling of the training set \(X\) from the distribution \(\) supported in \(B(0,b)\), we have for all \(f\)_

\[(f)-}_{N}(f) C b_{L:1}_{=1}^ {L}}{_{}}+d_{-1}}}(1+o(1))+c_{0}}.\]

Theorem 2 can be extended to ResNets \((f_{L}+id)(f_{1}+id)\) by simply replacing the Lipschitz constant \(Lip(f_{})\) by \(Lip(f_{}+id)\).

The Lipschitz constants \(Lip(f_{})\) are difficult to compute exactly, so it is easiest to simply bound it by the product of the operator norms \(Lip(f_{})\|W_{}\|_{op}\|V_{}\|_{op}\), but this bound can often be quite loose. The fact that our bound depends on the Lipschitz constants rather than the operator norms \(\|W_{}\|_{op},\|V_{}\|_{op}\) is thus a significant advantage.

This bound can be applied to a FCNNs with weight matrices \(M_{1},,M_{L+1}\), by replacing the middle \(M_{}\) with SVD decomposition \(USV^{T}\) in the middle by two matrices \(W_{-1}=V^{T}\) and \(V_{}=U\), so that the dimensions can be chosen as the rank \(d_{}=M_{+1}\). The Frobenius norm of the new matrices equal the nuclear norm of the original one \(\|W_{-1}\|_{F}^{2}=\|V_{}\|_{F}^{2}=\|M_{ }\|_{*}\). Some boundsassuming rank sparsity of the weight matrices also appear in . And several recent results have shown that weight-decay leads to a low-rank bias on the weight matrices of the network [27; 26; 19] and replacing the Frobenius norm regularization with a nuclear norm regularization (according to the above mentioned equivalence) will only increase this low-rank bias.

We compute in Figure 1 the upper bound of Theorem 2 for both AccNets and DNNs, and even though we observe a very large gap (roughly of order \(10^{3}\)), we do observe that it captures rate/scaling of the test error (the log-log slope) well. So this generalization bound could be well adapted to predicting rates, which is what we will do in the next section.

_Remark_. Note that if one wants to compute this upper bound in practical setting, it is important to train with \(L_{2}\) regularization until the parameter norm also converges (this often happens after the train and test loss have converged). The intuition is that at initialization, the weights are initialized randomly, and they contribute a lot to the parameter norm, and thus lead to a larger generalization bound. During training with weight decay, these random initial weights slowly vanish, thus leading to a smaller parameter norm and better generalization bound. It might be possible to improve our generalization bounds to take into account the randomness at initialization to obtain better bounds throughout training, but we leave this to future work.

## 4 Breaking the Curse of Dimensionality with Compositionality

In this section we study a large family of functions spaces, obtained by taking compositions of Sobolev balls. We focus on this family of tasks because they are well adapted to the complexity measure we have identified, and because kernel methods and even shallow networks do suffer from the curse of dimensionality on such tasks, whereas deep networks avoid it (e.g. Figure 1).

More precisely, we will show that these sets of functions can be approximated by a AccNets with bounded (or in some cases slowly growing) complexity measure

\[R(f_{1},,f_{L})=_{=1}^{L}Lip(f_{})_{=1}^{L}\|_{F_{1}}}{Lip(f_{})}+d_{-1}}.\]

This will then allow us show that AccNets can (assuming global convergence) avoid the curse of dimensionality, even in settings that should suffer from the curse of dimensionality, when the input dimension is large and the function is not very smooth (only a few times differentiable).

Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs) achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks, in agreement with our theory. We also see that our new generalization bounds approximately recover the right saling laws (even though they are orders of magnitude too large overall). We consider a compositional true function \(f^{*}=h g\) where \(g\) maps from dimension 15 to 3 while in maps from 3 to 20, and we denote \(_{g},_{h}\) for the number of times \(g,h\) are differentiable. In the first plot \(_{g}=8,_{h}=1\) so that \(g\) is easy to learn while \(h\) is hard, whereas in the second plot \(_{g}=9,_{h}=9\), so both \(g\) and \(h\) are relatively easier. The third plot presents the decay in test error and generalization bounds for networks evaluated using the real-world dataset, WESAD .

### Composition of Sobolev Balls

The family of Sobolev norms capture some notion of regularity of a function, as it measures the size of its derivatives. The Sobolev norm of a function \(f:^{d_{in}}\) is defined in terms of its derivatives \(_{x}^{}f\) for some \(d_{in}\)-multi-index \(\), namely the \(W^{,p}()\)-Sobolev norm with integer \(\) and \(p 1\) is defined as

\[\|f\|_{W^{,p}()}^{p}=_{||}\|_{x}^{}f\|_{L_ {p}()}^{p}\,.\]

Note that the derivative \(_{x}^{}f\) only needs to be defined in the 'weak' sense, which means that even non-differentiable functions such as the ReLU functions can actually have finite Sobolev norm.

The Sobolev balls \(B_{W^{,p}()}(0,R)=\{f:\|f\|_{W^{,p}()} R\}\) are a family of function spaces with a range of regularity (the larger \(\), the more regular). This regularity makes these spaces of functions learnable purely from the fact that they enforce the function \(f\) to vary slowly as the input changes. Indeed we can prove the following generalization bound:

**Proposition 3**.: _Given a distribution \(\) with support the \(L_{2}\) ball with radius b, we have that with probability \(1-\) for all functions \(f=\{f:\|f\|_{W^{,2}} R,\|f\|_{} R\}\)_

\[(f)-}_{N}(f) 2 C_{1}RE_{}{{d}}}(N)+c_{0}}.\]

_where \(E_{r}(N)=N^{-}\) if \(r>\), \(E_{r}(N)=N^{-} N\) if \(r=\), and \(E_{r}(N)=N^{-r}\) if \(r<\)._

But this result also illustrates the **curse of dimensionality:** the differentiability \(\) needs to scale with the input dimension \(d_{in}\) to obtain a reasonable rate. If instead \(\) is constant and \(d_{in}\) grows, then the number of datapoints \(N\) needed to guarantee a generalization gap of at most \(\) scales exponentially in \(d_{in}\), i.e. \(N^{-}{}}\). One way to interpret this issue is that regularity becomes less and less useful the larger the dimension: knowing that similar inputs have similar outputs is useless in high dimension where the closest training point \(x_{i}\) to a test point \(x\) is typically very far away.

#### 4.1.1 Breaking the Curse of Dimensionality with Compositionality

To break the curse of dimensionality, we need to assume some additional structure on the data or task which introduces an 'intrinsic dimension' that can be much lower than the input dimension \(d_{in}\):

**Manifold hypothesis**: If the input distribution lies on a \(d_{surf}\)-dimensional manifold, the error rates typically depends on \(d_{surf}\) instead of \(d_{in}\).

Figure 2: A comparison of empirical and theoretical error rates. The first plot illustrates the log decay rate of the test error with respect to the dataset size \(N\) based on our empirical simulations. The second plot depicts the theoretical decay rate of the test error as discussed in Section 4.1, \(-\{,}{d_{in}},}{d_{mid}}\}\). The final plot on the right displays the difference between the two. The lower left region represents the area where \(g\) is easier to learn than \(h\), the upper right where \(h\) is easier to learn than \(g\), and the lower right region where both \(f\) and \(g\) are easy.

**Known Symmetries:** If \(f^{*}(g x)=f^{*}(x)\) for a group action \(\) w.r.t. a group \(G\), then \(f^{*}\) can be written as the composition of a modulo map \(g^{*}:^{d_{in}}^{d_{in}}/G\) which maps pairs of inputs which are equivalent up to symmetries to the same value (pairs \(x,y\) s.t. \(y=g x\) for some \(g G\)), and then a second function \(h^{*}:^{d_{in}}/_{G}^{d_{out}}\), then the complexity of the task will depend on the dimension of the modulo space \(^{d_{in}}/_{G}\) which can be much lower. If the symmetry is known, then one can for example fix \(g^{*}\) and only learn \(h^{*}\) (though other techniques exist, such as designing kernels or features that respect the same symmetries) .

**Symmetry Learning:** However if the symmetry is not known then both \(g^{*}\) and \(h^{*}\) have to be learned, and this is where we require feature learning and/or compositionality. Shallow networks are able to learn translation symmetries, since they can learn so-called low-index functions which satisfy \(f^{*}(x)=f^{*}(Px)\) for some projection \(P\) (with a statistical complexity that depends on the dimension of the space one projects into, not the full dimension ). Low-index functions correspond exactly to the set of functions that are invariant under translation along the kernel \( P\). To learn general symmetries, one needs to learn both \(h^{*}\) and the modulo map \(g^{*}\) simultaneously, hence the importance of feature learning.

For \(g^{*}\) to be learnable efficiently, it needs to be regular enough to not suffer from the curse of dimensionality, but many traditional symmetries actually have smooth modulo maps, for example the modulo map \(g^{*}(x)= x^{2}\) for rotation invariance. This can be understood as a special case of composition of Sobolev functions, whose generalization gap can be bounded:

**Theorem 4**.: _Consider the function set \(=_{L}_{1}\) where \(_{}=\{f_{}:^{d_{-1}}^{d_{}}\|f_{}\|_{W^{r_{L}}2} f_{},\|f_{}\|_{ } b_{},Lip(f_{})_{}\}\), and let \(r_{min}=_{}r_{}\) for \(r_{}=}{d_{-1}}\), then with probability \(1-\) we have for all \(f\)_

\[(f)-}_{N}(f) C_{0}(_{=1}^{L }(C_{}_{L:+1}R_{})^{+1}})^{r _{min}+1}E_{r_{min}}(N)+c_{0}},\]

_where \(C_{}\) depends only on \(d_{-1},d_{},_{},b_{-1}\)._

We see that only the smallest ratio \(r_{min}\) matters when it comes to the rate of learning. And actually the above result could be slightly improved to show that the sum over all layers could be replaced by a sum over only the layers where the ratio \(r_{}\) leads to the worst rate \(E_{r_{}}(N)=E_{r_{min}}(N)\) (and the other layers contribute an asymptotically subdominant amount).

Coming back to the symmetry learning example, we see that the hardness of learning a function of the type \(f^{*}=h g\) with inner dimension \(d_{mid}\) and regularities \(_{g}\) and \(_{h}\), the error rate will be (up to log terms) \(N^{-\{,}{d_{in}},}{d_{mid}}\}}\). This suggests the existence of three regimes depending on which term attains the minimum: a regime where both \(g\) and \(h\) are easy to learn and we have \(N^{-}\) learning, a regime \(g\) is hard, and a regime where \(h\) is hard. The last two regimes differentiate between tasks

Figure 3: Comparing error rates for shallow and AccNets: shallow nets vs. AccNets, and kernel methods vs. AccNets. The left two graphs shows the empirical decay rate of test error with respect to dataset size (N) for both shallow nets and kernel methods. In contrast to our earlier empirical findings for AccNets, both shallow nets and kernel methods exhibit a slower decay rate in test error. The right two graphs present the differences in log decay rates between shallow nets and AccNets, as well as between kernel methods and AccNets. AccNets almost always obtain better rates, with a particularly large advantage at the bottom and middle-left.

where learning the symmetry is hard and those where learning the function knowing its symmetries is hard.

In contrast, without taking advantage of the compositional structure, we expect \(f^{*}\) to be only \(\{_{g},_{h}\}\) times differentiable, so trying to learn it as a single Sobolev function would lead to an error rate of \(N^{-\{,,_{h}\}}{d_{in}}\}}=N^{-\{,}{d_{in}},}{d_{in}}\}}\) which is no better than the compositional rate, and is strictly worse whenever \(_{h}<_{g}\) and \(}{d_{in}}<\) (we can always assume \(d_{mid} d_{in}\) since one could always choose \(d=id\)).

Furthermore, since multiple compositions are possible, one can imagine a hierarchy of symmetries that slowly reduce the dimensionality with less and less regular modulo maps. For example one could imagine a composition \(f_{L} f_{1}\) with dimensions \(d_{}=d_{0}2^{-}\) and regularities \(_{}=d_{0}2^{-}\) so that the ratios remain constant \(r_{}=2^{-}}{d_{0}2^{-+1}}=\), leading to an almost parametric rate of \(N^{-} N\) even though the function may only be \(d_{0}2^{-L}\) times differentiable. Without compositionality, the rate would only be \(N^{-2^{-L}}\).

_Remark_.: In the case of a single Sobolev function, one can show that the rate \(E_{}{{d}}}(N)\) is in some sense optimal, by giving an information theoretic lower bound with matching rate. A naive argument suggests that the rate of \(E_{\{r_{1},,r_{L}\}}(N)\) should similarly be optimal: assume that the minimum \(r_{}\) is attained at a layer \(\), then one can consider the subset of functions such that the image \(f_{-1:1}(B(0,r))\) contains a ball \(B(z,r^{})^{d_{-1}}\) and that the function \(f_{L:+1}\) is \(\)-non-contracting \(\|f_{L:+1}(x)-f_{L:+1}(y)\|\|x-y\|\), then learning \(f_{L:1}\) should be as hard as learning \(f_{}\) over the ball \(B(z,r^{})\) (more rigorously this could be argued from the fact that any \(\)-covering of \(f_{L:1}\) can be mapped to an \(}{{}}\)-covering of \(f_{}\)), thus forcing a rate of at least \(E_{r_{}}(N)=E_{\{r_{1},,r_{L}\}}(N)\).

An analysis of minimax rates in a similar setting has been done in .

### Breaking the Curse of Dimensionality with AccNets

Now that we know that composition of Sobolev functions can be easily learnable, even in settings where the curse of dimensionality should make it hard to learn them, we need to find a model that can achieve those rates. Though many models are possible 2, we focus on DNNs, in particular AccNets. Assuming convergence to a global minimum of the loss of sufficiently wide AccNets with two types of regularization, one can guarantee close to optimal rates:

**Theorem 5**.: _Given a true function \(f^{*}_{L:1}=f^{*}_{L^{*}} f^{*}_{1}\) going through the dimensions \(d^{*}_{0},,d^{*}_{L^{*}}\), along with a continuous input distribution \(_{0}\) supported in \(B(0,b_{0})\), such that the distributions \(_{}\) of \(f^{*}_{}(x)\) (for \(x_{0}\)) are continuous too and supported inside \(B(0,b_{})^{d^{*}_{}}\). Further assume that there are differentiabilities \(_{}\) and radii \(R_{}\) such that \(\|f^{*}_{}\|_{W^{_{}}:2^{(}B(0,b_{}))} R_{}\), and \(_{}\) such that \(Lip(f^{*}_{})_{}\). For an infinite width AccNet with \(L L^{*}\) and dimensions \(d_{} d^{*}_{1},,d^{*}_{L^{*}-1}\), we have for the ratios \(_{}=}{d^{*}_{}+3}\):

* _At a global minimizer_ \(_{L:1}\) _of the regularized loss_ \(f_{1},,f_{L}}_{N}(f_{L:1})+_{=1 }^{L}Lip(f_{})_{=1}^{L}\|_{F_{1}}}{Lip(f_{})} +d_{}}\)_, we have_ \((_{L:1})=(N^{-\{,_{1}, ,_{L^{*}}\}})\)_._
* _At a global minimizer_ \(_{L:1}\) _of the regularized loss_ \(f_{1},,f_{L}}_{N}(f_{L:1})+_{=1 }^{L}\|f_{}\|_{F_{1}}\)_, we have_ \((_{L:1})=(N^{-+_{=1}^{L^{*}}\{ 0,_{}-\}})\)_._

There are a number of limitations to this result. First we assume that one is able to recover the global minimizer of the regularized loss, which should be hard in general3 (we already know from  that this is NP-hard for shallow networks and a simple \(F_{1}\)-regularization). Note that it is sufficient to recover a network \(f_{L:1}\) whose regularized loss is within a constant of the global minimum, which might be easier to guarantee, but should still be hard in general. The typical method of training with GD on the regularized loss is a greedy approach, which might fail in general but could recover almost optimal parameters under the right conditions (some results suggest that training relies on first order correlations to guide the network in the right direction ).

We propose two regularizations because they offer a tradeoff:

**First regularization:** The first regularization term leads to almost optimal rates, up to the change from \(r_{}=}{d_{}^{2}}\) to \(r_{}=}{d_{}^{2}+3}\) which is negligible for large dimensions \(d_{}\) and differentiabilities \(_{}\). The first problem is that it requires an infinite width at the moment, because we were not able to prove that a function with bounded \(F_{1}\)-norm and Lipschitz constant can be approximated by a sufficiently wide shallow networks with the same (or close) \(F_{1}\)-norm and Lipschitz constant (we know from  that it is possible without preserving the Lipschitzness). We are quite hopeful that this condition might be removed in future work.

The second and more significant problem is that the Lipschitz constants \(Lip(f_{})\) are difficult to optimize over. For finite width networks it is in theory possible to take the max over all linear regions, but the complexity might be unreasonable. It might be more reasonable to leverage an implicit bias instead, such as a large learning rate, because a large Lipschitz constant implies that the network is sensible to small changes in its parameters, so GD with a large learning rate should only converge to minima with a small Lipschitz constant (such a bias is described in ). It might also be possible to replace the Lipschitz constant in our generalization bounds, possibly along the lines of .

**Second regularization:** The second regularization term actually does not require an infinite width, only a sufficiently large one. Also its regularization term is equivalent to \((\|W_{}\|^{2}+\|V_{}\|^{2}+\|b_{}\|^{2})\) which is much closer to the traditional \(L_{2}\)-regularization (and actually one could prove the same or very similar rates for \(L_{2}\)-regularization). The issue is that it lead to rates that could be far from optimal depending on the ratios \(_{}\): it recovers the same rate as the first regularization term if no more than one ratio \(_{}\) is smaller than \(\), but if many of these ratios are above \(\), it can be arbitrarily smaller.

In Figure 2, we compare the empirical rates (by doing a linear fit on a log-log plot of test error as a function of \(N\)) and the predicted optimal rates \(\{,}{d_{}},}{d_{}}\}\) and observe a pretty good match. Though surprisingly, it appears the the empirical rates tend to be slightly better than the theoretical ones.

_Remark_.: As can be seen in the proof of Theorem5, when the depth \(L\) is strictly larger than the true depth \(L^{*}\), one needs to add identity layers, leading to a so-called Bottleneck structure, which was proven to be optimal and observed empirically in . These identity layers add a term that scales linearly in the additional depth \()d_{min}^{*}}{}\) to the first regularization, and an exponential prefactor \((2d_{min}^{*})^{L-L^{*}}\) to the second. It might be possible to remove these factors by leveraging the bottleneck structure, or simply by switching to ResNets.

## 5 Conclusion

We have given a generalization bound for Accordion Networks and as an extension Fully-Connected networks. It depends on \(F_{1}\)-norms and Lipschitz constants of its shallow subnetworks. This allows us to prove under certain assumptions that AccNets can learn general compositions of Sobolev functions efficiently, making them able to break the curse of dimensionality in certain settings, such as in the presence of unknown symmetries.