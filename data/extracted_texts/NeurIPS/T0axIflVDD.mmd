# Frequency Adaptive Normalization For

Non-stationary Time Series Forecasting

 Weiwei Ye\({}^{1}\) Songgaojun Deng\({}^{2}\) Qiaosha Zou\({}^{3}\) Ning Gui\({}^{1}\)

\({}^{1}\)Central South University \({}^{2}\)University of Amsterdam \({}^{3}\)Zhejiang Lab

wyve155@gmail.com, s.deng@uva.nl, qiaoshazou@zhejianglab.org, ninggui@gmail.com

Corresponding author

###### Abstract

Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns. To address the non-stationarity, reversible instance normalization has been recently proposed to alleviate impacts from the trend with certain statistical measures, e.g., mean and variance. Although they demonstrate improved predictive accuracy, they are limited to expressing basic trends and are incapable of handling seasonal patterns. To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns. Specifically, we employ the Fourier transform to identify instance-wise predominant frequent components that cover most non-stationary factors. Furthermore, the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model. FAN is a model-agnostic method that can be applied to arbitrary predictive backbones. We instantiate FAN on four widely used forecasting models as the backbone and evaluate their prediction performance improvements on eight benchmark datasets. FAN demonstrates significant performance advancement, achieving 7.76%\(\)37.90% average improvements in MSE. Our code is publicly available2.

## 1 Introduction

Time series forecasting plays a key role in various fields such as traffic , finance  and infectious disease , etc. Recent research focuses on deep learning-based methods, as they demonstrate promising capabilities to capture complex dependencies between variables . However, time series with trends and seasonality, also called non-stationary time series , create covariate pattern shifts across different time steps. These dynamics pose significant challenges in forecasting.

To mitigate non-stationarity issues, reversible normalization has been proposed  which first removes non-stationary information from the input and returns the information back to rebuild the output. Current work focuses on removing non-stationary signals with statistical measures, e.g., mean and variance in the time domain . However, while these methods have improved prediction accuracy, these statistical measures are only capable of extracting the most prominent component, i.e., the trend, leaving substantial room for improvement. They, we argued, can hardly measure the characteristics of seasonal patterns, which are quite common in many time series. This significantly limits their capability to handle the non-stationarity, especially the seasonal patterns.

We illustrate an example featuring one of the simplest non-stationary signals in Fig. 1. This graph shows a time-variant signal with a gradually damping frequency, which is widely seen in many passive systems, e.g., spring-mass damper systems. As depicted in Fig. 1, the three input stages(highlighted in different background colors) exhibit the same mean and variance but differ in Fourier frequencies. Previous methods that model non-stationary information using means and variances can hardly distinguish this type of change in the time domain. In comparison, changes in periodic signals can be easily identified with the instance-wise Fourier transform (\(f_{1} f_{2} f_{3}\)). Thus, in this context, the principal Fourier components provide a more effective representation of non-stationarity compared to statistical values such as mean and variance. This simple example also shows that many existing frequency-based solutions, e.g., TimesNet , Koopa , which assume that the principal frequencies of the input signal are constant, can not identify the evolving principal frequencies.

With this inspiration, we introduce a novel instance-based normalization method, named **F**requency **A**daptive **N**ormalization (FAN). Rather than normalizing temporal statistical measures, FAN mitigates the impacts from the non-stationarity by filtering top \(K\) dominant components in the Fourier domain for each input instance, this approach can handle unified non-stationary fact composed of both trend and seasonal patterns. Furthermore, as those removed patterns might evolve from inputs to outputs, we employ a pattern adaptation module to forecast future non-stationary information rather than assuming these patterns remain unchanged.

In summary, our main contributions are: 1) We illustrate the limitations of reversible instance normalization methods in using temporal distribution statistics to remove impacts from non-stationarity. To address this limitation, we introduce a novel reversible normalization method, named FAN, which adeptly addresses both trend and seasonal non-stationary patterns within time series data. 2) We explicitly address pattern evolvement with a simple MLP that predicts the top \(K\) frequency signals of the horizon series and applies these predictions to reconstruct the output. 3) We apply FAN to four general backbones for time series forecasting across eight real-world popular benchmarks. The results demonstrate that FAN significantly improves their predictive effectiveness. Furthermore, a comparative analysis between FAN and state-of-the-art normalization techniques underscores the superiority of our proposed solution.

## 2 Related Work

Time series forecasting has been a hot topic of study for many years. This section provides discussions on related work from the following three perspectives.

### Time Series Forecasting

Traditional statistical methods, such as ARIMA , assume the stationarity of the time series and dependencies between temporal steps. Although these methods provide theoretical guarantees, they typically require data with ideal properties, which is often inconsistent with real-world scenarios . Besides, they can only handle a limited amount of data and features. In recent years, the field has witnessed a significant proliferation in the application of deep learning techniques for multivariate time series forecasting, a development ascribed to their ability in handling high-dimensional datasets. Consequently, various methods have been proposed to model time series data. Work based on recurrent neural networks [36; 4] preserves the current state and models the evolution of time series as a recurrent process. However, they generally suffer from a limited receptive field, which restricts their ability to capture long temporal dependencies . Inspired by their successes in Computer Vision (CV) and Natural Language Processing (NLP), convolutional neural networks and the self-attention mechanism have been extensively utilized in time series forecasting [22; 19; 40; 25]. Nevertheless, those works still face difficulties in handling non-stationary data with covariate pattern shifts. Making an accurate prediction for non-stationary time series remains challenging.

Figure 1: A sinusoidal signal with linearly varying frequency which is a common example of a non-stationary time series. In the lower-left corner, we plot the Fourier spectrum for three segments of the signal.

### Non-stationary Time Series Forecasting

To address non-stationarity, many methods directly model non-stationary phenomena with different modeling techniques. Li et al.  utilize a domain-adaptation paradigm to predict data distributions. Du et al.  propose an adaptive RNN to alleviate the impact of non-stationary factors through distribution matching. Liu et al.  introduce a non-stationary Transformer with de-stationary attention that incorporates non-stationary factors in self-attention mechanisms. To model non-linear dynamic systems, several models based on Koopman theory [21; 29; 37; 44] have been proposed with Fourier transform. To learn different patterns at different scales, Wang et al.  employs global and local Koopman operators. Liu et al.  model non-stationarity identified with Fourier transform and use Koopman operators to learn those components. However, these solutions typically select fixed frequency components based on the whole sequence rather than frequencies based on inputs. This time-invariant assumption can hardly be true in real-world scenarios.

### Instance-wise Normalization against Non-stationarity

To mitigate the time-variant property of non-stationary time series, a set of instance-wise normalization methods have been proposed to remove the impacts from non-stationarity. To reflect instance-wise changes, Ogasawara et al.  propose the usage of normalization based on local properties rather than global statistics. Passalis et al.  introduce an adaptive and learnable approach to this instance-wise normalization paradigm. However, although these methods effectively remove non-stationary components from inputs, they still need to predict the non-stationary time series in the output series, which remains challenging. In response, reversible instance normalization  is introduced by reintegrating the removed non-stationary components back to reconstruct the output. However, it still assumes unchanged trends between inputs and outputs. Kim et al.  developed RevIN, which mainly addresses evolving trends between input sequences. Recent works [10; 28] explore trends at a finer granularity, e.g., at the sliced level. However, these approaches still model non-stationarity with temporal statistical distribution parameters and fail to account for evolving seasonality, which is a crucial aspect of non-stationarity [35; 11; 45].

## 3 Proposed Method: FAN

Given a multivariate time series \(^{N D}\), where \(N\) is the total time steps and \(D\) denotes the number of feature dimensions. We use inputs series with length \(L\) to predict outputs series within length \(H\). The forecast task can be formulated as: \(_{t-L:t}_{t+1:t+H}\), where \(_{t-L:t}^{L D}\) and \(_{t+1:t+H}^{H D}\). For a clearer notation, we denote the input and output series as \(_{t}^{L D}\) and \(_{t}^{H D}\) respectively.

Our proposed method, FAN, consists of symmetrically structured instance-wise normalization and denormalization layers, illustrated in Fig. 2. The normalization process removes the impacts of non-stationary signals through frequency domain decomposition (upper left part of Fig. 2), while the denormalization process, supported by a prediction module, addresses potential shifts in frequency components between the input and output (lower part of Fig. 2).

### Frequency-based Normalization

First, FAN removes the top \(K\) dominant components in the frequency domain for each input instance, so the forecasting backbone can concentrate on the stationary aspects of the input. We term this process as _Frequency Residual Learning_ (FRL). The input at time \(t\), \(_{t}\), is multivariate with \(D\) dimension, and each dimension might have different frequency patterns; thus, we apply the FRL to each dimension in a channel independence setting . Here, the FRL is realized by the 1-dim Discrete Fourier Transform (DFT) with \(()\) towards each input \(_{t}\):

\[_{t}=(_{t})_{t }=((_{t}))_{t }^{non}=((_{t},_{t}))\] (1)

Equ. 1 shows that \(()\) transforms an input into Fourier components in complex values, denoted \(_{t}^{T D}\). Then, \(()\) selects the frequency set with the top \(K\) largest amplitude, which are calculated with \(()\) function. \(\) is the operation to filter out the \(_{t}\) frequency from \(_{t}\). To mitigate the impact of non-stationary signals, FRL restores the top \(K\) components into time domain components \(_{t}^{non}\) with \(()\).

With \(_{t}^{non}\), we can easily normalize the inputs and get the stationary components by removing \(_{t}^{non}\) from \(_{t}\), which is

\[_{t}^{res}=_{t}-_{t}^{non}\] (2)

Here, \(()\) and \(()\) can be performed using Fast Fourier Transform (FFT)  with a computational complexity of \(O(L L)\). And the \(\) and \(\) operations both exhibit complexity of \(O(L+K)\). It is important to note that all these operations are GPU-friendly and can be fully paralleled. Thus, the impact of applying these operations independently on each dimension can be largely mitigated. GPU-friendly PyTorch pseudocode is in Appendix A.2. After the normalization step, the normalized sequences \(_{t}^{res}\) can be more stationary and have a more consistent covariate distribution, the theoretical proof is provided in Appendix C.

### Forecast & Denormalization

As a result, the normalization layer allows the forecast backbone model \(g_{}\) to focus more on the dynamics within the inputs. Here, following the reversible instance normalization paradigm, the forecast backbone \(g_{}\) receives the transformed data \(_{t}^{res}\) as input and forecasts only the stationary part \(_{t}^{res}\) of the outputs \(_{t}\). This design makes it easier for the model to forecast non-stationary time series. Then, we apply the removed non-stationary information back to the output. We define this process as:

\[}_{t}^{res}=g_{}(_{t}^{res})\] (3)

where \(}_{t}^{non}\) is the reconstruct signal of \(_{t}^{non}\). We illustrate \(}^{non}\) as follow:

**Non-stationary shift forecasting.** For reverse instance normalization, we need to estimate \(}_{t}^{non}\) in the outputs. As an input and its corresponding output are rather close, RevIN  directly adds \(_{t}^{non}\) back by assuming \(_{t}^{non}\) with exactly the same trend as \(_{t}^{non}\). However, this assumption can hardly be true as the non-stationary information between the input and output may evolve. Furthermore, although SAN  and Dish-TS  predict statistics to address the discrepancy between the input and output, these statistics can only represent the most salient trend patterns.

To this end, rather than predicting statistics [10; 28], we use a simple MLP model \(q_{}\) to directly predict future values of the composite top \(K\) frequency components for \(D\) dimensions, defined as:

\[}_{t}^{non}=q_{}(_{t}^{non},_{t})= _{3}(_{2}( (_{1}_{t}^{non}),_{t}))\] (4)

where \(_{1}\), \(_{2}\), \(_{3}\) are all learnable parameters. Here, since \(_{t}^{non}\) only contains top \(K\) frequency information, it is difficult to capture variations in other frequencies solely relying on \(_{t}^{non}\). Therefore,

Figure 2: An overview of FAN which consists of normalization, frequency residual learning, denormalization steps, and incorporates a prior loss for non-stationary patterns.

we concatenate the top \(K\) components with the original input \(_{t}\) to handle potential frequency variations.

**Loss Functions.** To help with the residual learning process, we incorporate a prior guidance loss for the prediction of principal frequency components, the final loss is defined in Eq. 5. The forecast with prior guidance can be considered a multi-task optimization problem , where \(_{}^{nonstat}\) ensures \(q_{}\) accurately predict the non-stationary principal frequency component and \(_{,}^{forecast}\) guarantees that both model optimizes along the overall forecast accuracy.

\[,=*{arg\,min}_{,}_{t} _{}^{nonstat}(_{t}^{non},}_{t}^{non})+_{,}^{forecast}(_{t},}_{t})\] (5)

Here, the mean square error loss is used for both loss functions.

## 4 Experiments

### Experiment Setup

**Datasets.** We use eight popular datasets in multivariate time series forecasting as benchmarks, including: (1-4) **ETT** (Electricity Transformer Temperature) 3 records the oil temperature and load of the electricity transformers from July 2016 to July 2018. Four subsets are included in this dataset, where ETThs are sampled per hour and ETims per 15 minutes. (5) **Electricity**4 contains the electricity consumption of 321 clients from July 2016 to July 2019 per 15 minutes. (6) **ExchangeRate**5 contains the daily exchange rates of 8 countries from 1990 to 2016. (7) **Traffic**6 includes the hourly traffic load of San Francisco freeways recorded by 862 sensors from 2015 to 2016. (8) **Weather**7 is made up of 21 indicators of weather, including air temperature and humidity collected every 10 minutes in 2021.

For preprocessing, we apply z-score normalization  on all datasets to scale different variables to the same scale. Note that z-score normalization is unable to handle non-stationary time series since the statistics remain unchanged for different input instances . The split ratio for training, validation, and test sets is set to 7:2:1 for all the datasets. We report datasets properties in Table 1, including (1) Trend Variation: Differences in the means across different sections of the dataset. (2) Seasonality Variation: We report the average variance over the Fourier spectrum to examine the presence of evolving seasonality. Other dataset details can be found at Appendix B.

**Evaluation.** We set the prediction length \(H\{96,168,336,720\}\), covering both short- and long-term prediction . A fixed input window length \(L=96\) is used for all datasets. We evaluate the performance of baselines using mean squared error (MSE) and mean absolute error (MAE). The MSE and MAE are computed on z-score normalized data to measure different variables on the same scale. We report the final results on the test set for the model that performed optimally on the validation set.

**Backbone Models.** FAN is model-agnostic and could be applied to any prediction backbones. To validate its effectiveness, four state-of-the-art time-series forecasting model are used: MLP-based DLinear , Transformer-based Informer  and FEDformer, and convolutional neural network-based SCINet . Notably, FEDformer also employs the Fourier transform for analyzing

   Datasets & ETTh1 & ETTh2 & ETTm1 & ETTm2 & ExchangeRate & Electricity & Traffic & Weather \\  Trend Variation & 3.839 & 0.154 & 0.030 & 0.196 & 0.249 & 0.242 & 0.068 & 0.028 \\ Seasonality Variation & 3.690 & 1.013 & 3.330 & 1.648 & 0.435 & 2.645 & 14.225 & 0.387 \\ K & 4 & 3 & 11 & 5 & 2 & 3 & 30 & 2 \\   

Table 1: Properties of datasets and used hyperparameter \(K\) of each dataset.

seasonality. Results later show that FAN continues to make considerable improvements over these frequency-based solutions like FEDformer.

**Implementation and Settings.** For the non-stationary prediction module \(q_{}\) in FAN, the MLP model has hidden sizes . All the experiments are implemented by PyTorch  and are conducted for five runs with fixed seeds \(\{1,2,3,4,5\}\) on NVIDIA RTX 4090 GPU (24GB). For the different baselines, we follow the implementation and settings provided in their official code repository. ADAM  as the default optimizer across all the experiments. More experiment details, including training details and hyperparameter, can be found in Appendix A.1.

**Sections of Hyperparameter \(K\).** FAN allows for \(K\) to be any integer number less than \(L\). Regarding the selection of \(K\), we analyze these benchmarks and found that the main variation frequencies of these datasets are within 10% of the maximum amplitude. Therefore, we select the value of \(K\) based on the average maximum amplitude within 10% in the training set, the selected \(K\) is shown in Table 1. More evidence of this selection strategy is at Sec. 4.4, and we provide a detailed hyperparameter sensitivity analysis at Appendix D.1.

### Main Results

We report MAE/MSE forecasting errors of the baselines and FAN in Table 2. Since the performance in ETT datasets shows similar results, only results of ETTm2 are reported. The full results of the ETT benchmarks and further discussion are in Appendix E.2.

As shown in Table 2, our proposed FAN effectively enhances the performance of all four backbone models, by a large margin, achieving state-of-the-art performance on five datasets. Specifically, on the ETTm2, Electricity, Exchange, Traffic, and Weather datasets, the average MSE performance improvements are rather significant: 10.81%, 21.49%, 51.27%, 21.97%, and 21.55% respectively. It clearly shows that frequency residual learning of FAN effectively mitigates the impacts of evolving seasonal and trend patterns and enhances the stationarity that simplifies the prediction for backbones.

FAN demonstrates increasing performance improvements as the prediction length extends in the Informer backbone, with MSE improvements of 9.87%, 18.87%, 36.91%, 16.26%, and 20.05%, from 96 steps to 720 steps. We believe this can be attributed to the fact that the periodicity contained in the prediction series increases with step length, and the FAN's pattern prediction module helps uncover periodicity in longer step lengths, thereby enhancing long-term prediction effectiveness. It is important to note that even in the models that utilize FFT to analyze seasonal patterns, like FEDformer, we still observe significant performance improvements (19.81%). This conclusion underscores our model's advantage in handling non-stationary aspects by directly extracting non-stationary seasonality patterns rather than learning these patterns.

   Methods &  &  &  &  &  &  &  &  \\ Metrics & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\    & 96 & 0.203 & 0.080 & **0.198** & **0.078** & 0.208 & 0.082 & **0.194** & **0.074** & 0.226 & 0.091 & **0.198** & **0.077** & 0.206 & 0.079 & **0.198** & **0.078** \\  & 168 & 0.220 & 0.093 & **0.219** & **0.033** & 0.249 & 0.116 & **0.220** & **0.093** & 0.251 & 0.112 & **0.219** & **0.092** & 0.226 & 0.094 & **0.218** & **0.093** \\  & 336 & 0.245 & 0.114 & **0.241** & **0.113** & 0.282 & 0.143 & **0.272** & **0.131** & 0.283 & 0.140 & **0.245** & **0.114** & 0.262 & 0.122 & **0.241** & **0.113** \\  & 720 & 0.270 & 0.142 & **0.264** & **0.139** & 0.308 & 0.174 & **0.275** & **0.145** & 0.347 & 0.212 & **0.287** & **0.154** & 0.297 & 0.153 & **0.264** & **0.139** \\   & 96 & 0.277 & 0.195 & **0.269** & **0.184** & 0.298 & 0.183 & **0.243** & **0.148** & 0.376 & 0.277 & **0.250** & **0.153** & 0.296 & 0.188 & **0.261** & **0.168** \\  & 168 & 0.227 & 0.183 & **0.268** & **0.178** & 0.305 & 0.191 & **0.251** & **0.154** & 0.371 & 0.269 & **0.257** & **0.156** & 0.306 & 0.196 & **0.258** & **0.163** \\  & 336 & 0.294 & 0.197 & **0.289** & 0.192 & 0.312 & 0.194 & **0.272** & **0.167** & 0.377 & 0.273 & **0.273** & **0.273** & **0.167** & 0.330 & 0.214 & **0.278** & **0.175** \\  & 720 & 0.333 & 0.233 & **0.325** & **0.227** & 0.330 & 0.213 & **0.300** & **0.189** & 0.401 & 0.311 & **0.306** & **0.194** & 0.352 & 0.240 & **0.312** & **0.204** \\   & 96 & **0.164** & **0.052** & 0.167 & 0.053 & 0.260 & 0.112 & **0.186** & **0.062** & 0.532 & 0.412 & **0.189** & **0.066** & 0.218 & 0.085 & **0.169** & **0.055** \\  & 168 & 0.219 & 0.090 & **0.217** & **0.088** & 0.312 & 0.163 & **0.222** & **0.090** & 0.582 & 0.491 & **0.257** & **0.128** & 0.266 & 0.126 & **0.221** & **0.093** \\   & 336 & **0.288** & **0.155** & 0.297 & 0.162 & 0.456 & 0.338 & **0.338** & **0.198** & 0.721 & 0.847 & **0.333** & **0.191** & 0.337 & 0.203 & **0.303** & **0.167** \\   & 720 & 0.453 & 0.352 & **0.406** & **0.292** & 0.669 & 0.661 & **0.436** & **0.329** & 0.889 & 1.210 & **0.513** & **0.474** & 0.502 & 0.430 & **0.439** & **0.345** \\   & 96 & 0.387 & 0.504 & **0.334** & **0.403** & 0.348 & 0.383 & **0.3326** & **0.371** & 0.350 & 0.428 & **0.314** & **0.364** & 0.399 & 0.471 & **0.344** & **0.393** \\   & 168 & 0.588 & 0.804 & **0.334** & **0.414** & 0.366 & 0.422 & **0.336** & **0.391** & 0.366 & 0.457 & **0.324** & **0.383** & 0.377 & 0.443 & **0.348** & **0.403** \\   & 336 & 0.380 & 0.504 & **0.346** & **0.437** & 0.383 & 0.452 & **0.348** & **0.414** & 0.414 & 0.455 & 0.536 & **0.364** & 0.427 & 0.384 & 0.459 & **0.360** & **0.426** \\   & 720 & 0.407 & 0.532 & **0.372** & **0.472** & 0.391 & 0.465 & **0.372** & **0.454** & 0.656 & 1.002 & **0.397** & **0.482** & 0.401 & 0.409 & **0.377** & **0.454** \\   & 96 & 0.249 & 0.180

### Comparison With Reversible Instance Normalization Methods

In this section, we compare FAN with three state-of-the-art normalization methods for non-stationary time series forecasting: SAN , Dish-TS , and RevIN , with the same experimental setup as Sect. 4.2. We report the average MSE over all the forecasting lengths of all backbones for all datasets in Table 3. It is evident that FAN generally outperforms the baseline models, except for a few cases with a close margin. Here, SAN generally ranks second as it slices the whole sequence into sub-series which can make seasonal patterns into evolving trends that could be partially predicted with its statistics prediction module. In comparison, RevIN and Dish-TS have much worse performance. Detailed results of all cases and further discussions are provided in E.4.

**Show Cases.** Fig. 3 illustrates the forecasting results with DLinear backbone in Traffic to show why FAN has performance advantages. This data has very clear evolving seasonality with daily waveform patterns. FAN can extract trends and seasonal patterns especially the seasonal patterns during weekends while Dish-TS and RevIN only focus on trends statistics. Furthermore, FAN can adaptively adjust frequency pattern forecasting results based on the input main frequency signals, capturing the evolving patterns between the input and horizon. Fig. 3(a) clears shows FAN can identify the seasonal patterns with increasing amplitudes from hour 100\(\)150.

**Stationarity Analysis.** To verify our model's effectiveness against non-stationarity, we use the ADF test  to examine the stationarity of the data after normalization. The results are shown in Fig. 4(a), smaller value (further from the center) indicates higher stationarity. Compared to previous normalization methods, our model achieves greater stationarity across all datasets, particularly in cases with larger seasonal patterns (Traffic, ETTh1, ETTm1). In some datasets, e.g., Weather, Exchange, despite having less apparent seasonality, our model still enhances stationarity. We attribute this to our method's ability to adaptively capture the low-frequency trend signals, such as mixed-linear changes, while other methods assume consistent distribution over a period and remove estimated statistics, due to which they might fail to capture these intricate trend patterns.

**Model Efficiency.** We compare the performance, training time per iteration, and number of parameters with SAN on Traffic with \(D=862,H=720\). DLinear is used for both as the backbone. The results are shown in Fig. 4(b). FAN and SAN have similar training iteration times, but FAN has \(29.79\%\) less parameters. Moreover, FAN achieves a \(15.56\%\) improvement in MSE and a \(15.30\%\) improvement in MAE. This highlights our model's effectiveness and efficiency.

**Various Input Length.** In time series learning, the non-stationarity of inputs varies with the choice and change of the time window , which in turn impacts the performance of deep learning models . Therefore, we compare the performance changes under different input lengths on

   Models &  &  &  &  \\ Methods & FAN & SAN & Dish-TS & RevIN & FAN & SAN & Dish-TS & RevIN & FAN & SAN & Dish-TS & RevIN & FAN & SAN & Dish-TS & RevIN \\  ETTh1 & **0.441** & 0.454 & 0.465 & 0.477 & **0.443** & 0.530 & 0.565 & 0.591 & **0.465** & 0.624 & 0.714 & 0.688 & **0.442** & 0.454 & 0.489 & 0.472 \\ ETT2 & 0.135 & **0.134** & 0.136 & 0.149 & 0.149 & **0.148** & 0.217 & 0.183 & **0.164** & 0.201 & 0.259 & 0.199 & **0.136** & 0.139 & 0.160 & 0.149 \\ ETTm1 & 0.395 & **0.390** & 0.405 & 0.419 & **0.400** & 0.416 & 0.489 & **0.491** & **0.397** & 0.427 & 0.504 & 0.485 & 0.395 & **0.393** & 0.424 & 0.443 \\ ETTm2 & **0.105** & 0.106 & 0.108 & 0.113 & 0.111 & **0.106** & 0.125 & 0.121 & **0.106** & 0.114 & 0.153 & 0.130 & **0.105** & **0.105** & 0.122 & 0.112 \\ Electricity & **0.193** & 0.200 & 0.201 & 0.207 & **0.164** & 0.169 & 0.181 & 0.180 & **0.167** & 0.191 & 0.219 & 0.190 & 0.177 & 0.175 & 0.207 & **0.164** \\ Exchange & **0.149** & 0.172 & 0.265 & 0.190 & **0.170** & 0.192 & 0.333 & 0.267 & **0.168** & 0.265 & 0.472 & 0.238 & **0.162** & 0.174 & 0.281 & 0.183 \\ Traffic & **0.432** & 0.514 & 0.591 & 0.652 & 0.408 & **0.395** & 0.433 & 0.424 & **0.400** & 0.515 & 0.446 & 0.894 & **0.419** & 0.431 & 0.489 & 0.442 \\ Weather & **0.249** & 0.250 & 0.269 & 0.272 & 0.295 & **0.272** & 0.562 & 0.280 & **0.254** & 0.256 & 0.322 & 0.275 & **0.242** & **0.242** & 0.250 & 0.251 \\   

Table 3: The MSE performance averaged across all steps. Bold values indicate the best performance.

Figure 3: Visualization of long-term 168 steps forecasting results of a test sample in Traffic dataset, using DLinear enhanced with different normalization methods.

ETTm2 dataset and DLinear as the backbone. Fig. 4(c) shows FAN exhibits the best performance across all windowed data. As can be seen, compared to other models, as the input length increases, among these normalizations, the enhancement of increases the most. The MSE enhancement with previous SOTAs increases from 0.49% in short inputs \(L=48\) to 4.37% in long inputs with \(L=336\). This demonstrates that the instance-wise DFT is capable of extracting more seasonal patterns from the longer input windows.

### TopK vs. Frequency Distributions

As different datasets might have different non-stationary patterns, it is crucial to select appropriate K frequency components from inputs. We study the relations between the selected TopK and the frequency distribution on the ExchangeRate and Traffic datasets.

Fig. 5 plots the frequency amplitude distribution for frequency 0\(\)32 by performing DFT towards the different input instances with \(L=96\) of the whole training sequences. Here, we can see the clear relation between the selection of K and the frequency amplitude distributions. As we can see, the Traffic dataset contains rich seasonal signals ranging from 0\(\)32 while the ExchangeRate dataset only has mainly one principal frequency component with frequency 0 (trend) in the inputs. Thus, the prediction on the ExchangeRate dataset might not benefit from a bigger K while a bigger K indeed helps for the Traffic dataset. Results for more datasets are in Appendix B.

### Ablation Studies

**Main Components.** This section aims to evaluate the effectiveness of various FAN's designs. Three variants are studied: "w/o predict" denotes the removal of the non-stationary pattern prediction module and directly reconstructing \(}^{non}\) with \(^{non}\). "pure backbone" refers to the omission of the reconstruction in the output or "w/o backbone" is the omission of the stationary part. We evaluate their performance on two non-stationary datasets, ETTh1 and Weather. The experimental settings are consistent with those described in Section 4.2. The evaluation results, along with the standard deviations, are presented in Table 4. The results show that FAN achieves best performance across all metrics in all variants. _FAN w/o backbone_ ranks second as the learning model of FAN already learns the principle changes. In comparison, the results from _pure backbone_ is the weakest, as it cannot handle nonstationary signals. _FAN w/o predict_ also has poor performance. Those results clearly

Figure 4: Comparison with other normalization methods. (a) ADF test after normalization, the smaller the value, the higher the stationarity. (b) Model efficiency comparison with SAN, including MSE/MAE, parameters (in millions), and training time per iteration (ms/100). (c) Performance in MSE vs. input length on the ETTm2 dataset.

Figure 5: Frequency distributions vs. forecast error in MSE with different K and output length \(H\).

show that trends and seasonal patterns do evolve and that our proposed residual frequency learning is crucial in dealing with these changes.

**Instance-wise vs. Global Fourier Analysis.** This section investigates the effect from instance-wise Fourier Analysis of FAN. Previous Fourier-based methods select predominant Fourier signals based on fixed frequencies [41; 27; 43; 9]. However, as shown in Fig. 6, on the Traffic and Electricity datasets, the predominant components from the input-wise view are not fixed but exhibit distinct distribution characteristics and vary across the inputs. The assumption of fixed spectrum and the reality of changing frequency limits their performance, supported with two additional experiments on Fourier-based backbones at Appendix E.3.

Here, we compare the performance of FAN with fixed frequencies computed using global sequences and original FAN with instance-specific frequencies.The results are shown in Table 5.

As shown in Table 5, by selecting instance-wise predominant frequencies, FAN achieves an average improvement of 18.50% and 10.29% on the Electricity and Traffic datasets respectively. This highlights instance-wise frequency selection rather than assuming fixed frequency patterns.

## 5 Conclusion

In this paper, we study the problem of non-stationary time series prediction. We identify the fact that traditional statistical measurement-based instance-wise normalization can not effectively recover the evolving seasonal patterns. We propose FAN to perform instance normalization for each input window. The Fourier transform is used to remove the main frequency components in the inputs and reconstruct the Fourier basis through denormalization. To address the evolving trend and seasonal patterns between inputs and outputs, we utilize a simple MLP model to predict the changes in the extracted non-stationary pattern. The effectiveness of FAN is verified with a set of experiments on eight widely used benchmark datasets. Compared to other state-of-the-art normalization baselines, FAN significantly improves the prediction performance and outperforms state-of-the-art normalization methods. One potential avenue for improvement involves the autonomous determination of an optimal K for the selection of principal frequency components.

   Variations & Steps &  &  &  &  \\ Datasets & Metrics & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\   & 96 & **0.42740.000** & **0.36240.001** & 0.5174.001 & 0.58240.001 & 0.52740.022 & 0.54940.071 & 0.45740.001 & 0.39240.001 \\  & 168 & **0.45440.003** & **0.39540.002** & 0.53640.000 & 0.61040.001 & 0.61440.031 & 0.60340.007 & 0.49440.002 & 0.43684.003 \\  & 336 & **0.48740.004** & **0.43940.003** & 0.55740.001 & 0.65440.001 & 0.62140.022 & 0.62240.079 & 0.52440.002 & 0.47440.004 \\  & 720 & **0.5710.004** & **0.57240.003** & 0.63240.003 & 0.78940.007 & 0.63340.013 & 0.6360.005 & 0.61640.009 & 0.61820.013 \\   & 96 & **0.21540.002** & **0.17040.001** & 0.29340.002 & 0.27140.001 & 0.33540.007 & 0.33240.008 & 0.24640.000 & 0.19540.000 \\  & 168 & **0.25340.001** & **0.20640.001** & 0.32540.003 & 0.31040.005 & 0.34740.007 & 0.34640.013 & 0.28400.001 & 0.23240.001 \\   & 336 & **0.29990.001** & **0.26840.002** & 0.36840.002 & 0.37640.003 & 0.37640.010 & 0.37040.005 & 0.32940.001 & 0.29680.001 \\   & 720 & **0.33990.003** & **0.32240.002** & 0.41140.004 & 0.44140.004 & 0.41140.008 & 0.40140.010 & 0.36740.002 & 0.34140.003 \\   

Table 4: Forecasting errors under the multivariant setting with respect to variations of FAN with SCINet backbone. The best performances are highlighted in bold.

    \\  Steps & 96 & 168 & 336 & 720 & Avg.Imp. \\  FAN & **0.162** & **0.165** & **0.173** & **0.194** & 18.50\% \\ Fixed & 0.176 & 0.192 & 0.231 & 0.265 & - \\   \\  Steps & 96 & 168 & 336 & 720 & Avg.Imp. \\  FAN & **0.393** & **0.403** & **0.426** & **0.454** & 10.29\% \\ Fixed & 0.446 & 0.457 & 0.469 & 0.496 & - \\   

Table 5: MSE Performance between instance-wise (FAN) and global selection (Fixed) on SCINet backbone.

Figure 6: Top 10 selection propablity density on Traffic and Electricity datasets.