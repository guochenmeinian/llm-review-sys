# Is RLHF More Difficult than Standard RL?

A Theoretical Perspective

 Yuanhao Wang, Qinghua Liu, Chi Jin

Princeton University

{yuanhao,qinghual,chij}@princeton.edu

###### Abstract

Reinforcement learning from Human Feedback (RLHF) learns from _preference_ signals, while standard Reinforcement Learning (RL) directly learns from _reward_ signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduced to adversarial MDP when preferences only depend on the final state. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation. We further provide guarantees when K-wise comparisons are available.

## 1 Introduction

Reinforcement learning (RL) is a control-theoretic problem in which agents take a sequence of actions, receive _reward feedback_ from the environment, and aim to find good policies that maximize the cumulative rewards. The reward labels can be objective measures of success (winning in a game of Go (Silver et al., 2017)) or more often hand-designed measures of progress (gaining gold in DOTA2 (Berner et al., 2019)). The empirical success of RL in various domains (Mnih et al., 2013; Vinyals et al., 2019; Todorov et al., 2012) crucially relies on the availability and quality of reward signals. However, this also presents a limitation for applying standard reinforcement learning when designing a good reward function is difficult.

An important approach that addresses this challenge is reinforcement learning from Human feedback (RLHF), where RL agents learn from _preference feedback_ provided by humans. Preference feedback is arguably more intuitive to human users, more aligned with human values and easier to solicit in applications such as recommendation systems and image generation (Pereira et al., 2019; Lee et al., 2023). Empirically, RLHF is a key ingredient empowering the successes in tasks ranging from robotics (Jain et al., 2013) to large language models (Ouyang et al., 2022).

A simple observation about preference feedback is that preferences can always be reconstructed from reward signals. In other words, preference feedback contains arguably less information than scalar rewards, which may render the RLHF problem more challenging. A natural question to ask is:

**Is preference-based RL more difficult than reward-based RL?**Existing research on preference-based RL (Chen et al., 2022; Pacchiano et al., 2021; Novoseller et al., 2020; Xu et al., 2020; Zhu et al., 2023) has established efficient guarantees for learning a near-optimal policy from preference feedback. These works typically develop specialized algorithms and analysis in a white-box fashion, instead of building on existing techniques in standard RL. This leaves it open whether it is necessary to develop new theoretical foundation for preference-based RL in parallel to standard reward-based RL.

This work presents a comprehensive set of results on provably efficient RLHF under a wide range of preference models. We develop new simple reduction-based approaches that solve preference-based RL by reducing it to existing frameworks in reward-based RL, with little to no additional cost:

* For utility-based preferences--those drawn from a reward-based probabilistic model (see Section 2.1), we prove a reduction from preference-based RL to reward-based RL with robustness guarantees via a new Preference-to-Reward Interface (P2R, Algorithm 1). Our approach incurs no sample complexity overhead and the human query complexity 1 does not scale with the sample complexity of the RL algorithm. We instantiate our framework for comparisons based on (1) immediate reward of the current state-action pair or (2) cumulative reward of the trajectory, and apply existing reward-based RL algorithms to directly find near-optimal policies for RLHF in a large class of models including tabular MDPs, linear MDPs, and MDPs with low Bellman-Eluder dimension, etc. We further provide complexity guarantees when K-wise comparisons are available. * For general (arbitrary) preferences, we consider the objective of the _von Neumann winner_ [see, e.g., Dudik et al., 2015; Kreweras, 1965], a solution concept that always exists and extends the Condorcet winner. We reduce this problem to multiagent reward-based RL which finds Nash equilibria for a special class of factored two-player Markov games under a restricted set of policies. When preferences only depend on the final state, we prove that such factored Markov games can be solved by both players running Adversarial Markov Decision Processes (AMDP) algorithms independently. For preferences that depend on entire trajectory, we develop an adapted version of optimistic Maximum Likelihood Estimation (OMLE) algorithm (Liu et al., 2022), which handles this factored Markov games under general function approximation.

Notably, our algorithmic solutions are either reductions to standard reward-based RL problems or adaptations of existing algorithms (OMLE). This suggests that technically, preference feedback is not difficult to address given the existing knowledge of RL with reward feedback. Nevertheless, our impossibility results for utility-based preference (Lemma 2, 3) and reduction for general preferences also highlight several important conceptual differences between RLHF and standard RL.

### Related work

**Dueling bandits.** Dueling bandits (Yue et al., 2012; Benggs et al., 2021) can be seen as a special case of preference based RL with \(H=1\). Many assumptions later applied to preference based RL, such as an underlying utility model with a link function (Yue and Joachims, 2009), the Plackett-Luce model (Saha and Gopalan, 2019), and the Condorcet winner (Zoghi et al., 2014) can be traced back to literature on dueling bandits. A reduction from preference feedback to reward feedback for bandits is proposed by Ailon et al. (2014). The concept of the von Neumann winner, which we employ for general preferences, has been considered in Dudik et al. (2015) for contextual dueling bandits.

**RL from human feedback.** Using human preferences in RL has been studied for at least a decade (Jain et al., 2013; Busa-Fekete et al., 2014) and is later incorporated with Deep RL (Christiano et al., 2017). It has found empirical success in robotics (Jain et al., 2013; Abramson et al., 2022; Ding et al., 2023), game playing (Ibarz et al., 2018) and fine-tuning large language models (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). RL with other forms of human feedback, such as demonstrations and scalar ratings, has also been considered in previous research (Finn et al., 2016; Warnell et al., 2018; Arakawa et al., 2018) but falls beyond the scope of this paper.

**Theory of Preference-based RL.** For utility-based preferences, Novoseller et al. (2020); Pacchiano et al. (2021) and Zhan et al. (2023) consider _tabular_ or _linear_ MDPs, and assume that the per trajectory reward is linear in trajectory features. Xu et al. (2020) also considers the tabular setting. However, instead of assuming an explicit link function, several structural properties of the preference is assumed which guarantee a Condorcet winner and ensure small regret of a black-box dueling bandit algorithm. Finally, recent work of Zhu et al. (2023); Zhu et al. (2023) consider utility-based preferences in the offline setting, assuming the algorithm is provided with a pre-collected human preference (and transition) dataset with good coverage. Compared to the above works, this paper considers the online setting and derives results for utility-based preferences with general function approximation, which are significantly more general than those for tabular or linear MDPs.

For general preferences, Chen et al. (2022) also develops sample-efficient algorithms for finding the von Neumann winner 2. Their algorithm is computationally inefficient in general even when restricted to the tabular setting. Compared to this result, our AMDP-based reduction algorithm (Section 4.2) is computationally efficient in the tabular or linear setting when the comparison only depends on the final states. For general trajectory-based comparison, our results apply to a richer class of RL problems including POMDPs.

Finally, we remark that all prior results develop specialized algorithms and analysis for preference-based RL in a white-box fashion. In contrast, we develop reduction-based algorithms which can directly utilize state-of-the-art results in reward-based RL for preference-based RL. This reduction approach enables the significant generality of the results in this paper compared to prior works.

## 2 Preliminaries

We consider reinforcement learning in episodic MDPs, specified by a tuple \((H,,,)\). Here \(\) is the state space, \(\) is the action space, and \(H\) is the length of each episode. \(\) is the transition probability function; for each \(h[H]\) and \(s,a\), \(_{h}(|s,a)\) specifies the distribution of the next state. A _trajectory_\(()^{H}\) is a sequence of interactions \((s_{1},a_{1},,s_{H},a_{H})\) with the MDP. A Markov policy \(=\{_{h}:\ _{}\}_{h[H]}\) specifies an action distribution based on the current state, while a general policy \(=\{_{h}:()^{h-1} _{}\}_{h[H]}\) can choose a random action based on the whole history up to timestep \(h\).

In RLHF, an algorithm interacts with a reward-less MDP environment and may query comparison oracle (human evaluators) for preference information. We consider two types of preferences: utility-based ones and general ones.

### Utility-based preferences

For utility-based comparison, we assume there exists an underlying reward function \(r^{}:()^{H}\). Given a reward function, the value of a policy can be defined as \(_{}[_{h=1}^{H}r^{}(s_{h},a_{h})]\), _i.e._ the expected cumulative reward obtained by executing the policy. An optimal policy is one that maximizes \(_{}[_{h=1}^{H}r^{}(s_{h},a_{h})]\). We say that \(\) is \(\)-optimal if \(_{}[_{h=1}^{H}r^{}(s_{h},a_{h})]_{^{}} _{^{}}[_{h=1}^{H}r^{}(s_{h},a_{h})]-\). We also consider a setting where the utility is only available for a whole trajectory. In this case, we assume that there is an underlying trajectory reward function \(r^{}:()^{H}[0,H]\), which assigns a scalar utility to each trajectory. In this case the value of a policy can be defined similarly as \(_{}[r()]\).

In preference based RL, the reward is assumed to be unobservable, but is respected by the comparison oracle which models human evaluators.

**Definition 1** (Comparison oracle).: _A comparison oracle takes in two trajectories \(_{1}\), \(_{2}\) and returns_

\[o((r^{}(_{1})-r^{}(_{2}))),\]

_where \(()\) is a link function, and \(r^{}()\) is the underlying reward function._

Here \((p)\) denotes a Bernoulli distribution with mean \(p\). The comparison outcome \(o=1\) indicates \(_{1}_{2}\), and vice versa. We additionally require that the inputs \(_{1}\), \(_{2}\) to the comparison oracle are _feasible_ in the sense that they should be actual trajectories generated by the algorithm and cannot be synthesized artificially. This is motivated by the potential difficulty of asking human evaluators to compare out-of-distribution samples (e.g. random pixels).

In this work, we also consider \(\)**preferences**, where \([(s_{1},a_{1})(s_{2},a_{2})]=(r(s_{1},a_{1})-r(s_{2},a_{2}))\). For notational simplicity, we will use \(\) to denote a state-action pair \((s,a)\) (which can be thought as an incomplete trajectory) so that \((s,a)\) preferences can be seen as a special case of the comparison oracle (Definition 1). See more details in Remark 1.

### General preferences

For general preferences, we assume that for every trajectory pair \(,^{}\), the probability that a human evaluator prefers \(s\) over \(s^{}\) is

\[M[,^{}]=[^{}].\] (1)

A general preference may not be realizable by a utility model, so we cannot define the optimal policy in the usual sense. Instead, we follow Dudik et al. (2015) and consider an alternative solution concept, the _von Neumann winner_ (see Definition 5).

### Function approximation

We first introduce the concept of eluder dimension, which has been widely used in RL to measure the difficulty of function approximation.

**Definition 2** (eluder dimension).: _For any function class \(()\), its Eluder dimension \(_{}(,)\) is defined as the length of the longest sequence \(\{x_{1},x_{2},,x_{n}\}\) such that there exists \(^{}\) so that for all \(i[n]\), \(x_{i}\) is \(^{}\)-independent of its prefix sequence \(\{x_{1},,x_{i-1}\}\), in the sense that there exists some \(f_{i},g_{i}\) such that_

\[^{i-1}((f_{i}-g_{i})(x_{j}))^{2}}^{ }\ \ \ \ \ \ |(f_{i}-g_{i})(x_{i})|^{}.\]

Intuitively, eluder dimension measures the number of worst-case mistakes one has to make in order to identify an unknown function from the class \(\). It is often used as a sufficient condition to prove sample efficiency guarantees for optimism-based algorithms.

As in many works on function approximation, we assume knowledge a class of reward functions \(\) and realizability.

**Assumption 1** (Realizability).: \(r^{}\)_._

We further use \(}:=\{r+c|c[-H,0],r\}\) to denote the reward function class augmented with a bias term. The inclusion of an additional bias is due to the assumption that preference feedback is based on reward _differences_, so we could only learn \(r^{}\) up to a constant. We note that for the \(d\)-dimension linear reward class \(_{}\), \(_{}(}_{})(d)\), and that for a general reward class \(\), \(_{}(},)(H_{ }(,/2)^{1.5}/)\). The details can be found in Appendix B.

## 3 Utility-based RLHF

Given access to comparison oracles instead of scalar rewards, a natural idea is to convert preferences back to scalar reward signals, so that standard RL algorithms can be trained on top of them. In this section, we introduce an efficient reduction from RLHF to standard RL through a preference-to-reward interface. On a high level, the interface provides approximate reward labels for standard RL training, and only queries the comparison oracle when uncertainty is large. The reduction incurs small sample complexity overhead, and the number of queries to human evaluators does not scale with the sample complexity of the RL algorithm using it. Moreover, it solicits feedback from the comparison oracle in a limited number of batches, simplifying the training schedule.

### A preference-to-reward interface

The interaction protocol of an RL algorithm with the Preference-to-Reward (P2R) Interface is shown in Fig. 1. P2R maintains a confidence set of rewards \(B_{r}\). When the RL algorithm wishes to learn the reward label of a trajectory \(\), P2R checks whether \(B_{r}\) approximately agrees on the reward of \(\). If so, it can return a reward label with no queries to the comparison oracle; if not, it will query the comparison oracle on \(\) and a fixed trajectory \(_{0}\), and update the confidence set of reward functions. The details of P2R are presented in Algorithm 1.

The performance of P2R depends on the RL algorithm running on top of it. In particular, we define sample complexity of a standard reward-based RL algorithm \(\) as follows.

**Definition 3**.: _An RL algorithm \(\) is \(g()\)-robust and has sample complexity \((,)\) if it can output an \(\)-optimal policy using \((,)\) samples with probability at least \(1-\), even if the reward of each trajectory \(\) is perturbed by \(()\) with \(\|()\|_{} g()\)._

We would like to note that the requirement of being \(g()\)-robust is typically not restrictive. In fact, any tabular RL algorithm with sample complexity would be \(O(/H)\)-robust with the same sample complexity, while many algorithms with linear function approximation are \(O(/(d,H))\)-robust (Jin et al., 2020; Zanette et al., 2020) (again with the same sample complexity). We can further show that one can effortlessly convert any standard RL algorithms with sample complexity \(\) into an \(O(1/)\)-robust one by using the procedure described in Lemma B.3.

**Remark 1** (Trajectory vs. \((s,a)\) preferences).: So far, we presented the comparison oracle (Definition 1) and the P2R (Algorithm 1) using trajectory rewards. This would require the RL algorithm using P2R to be one that learns from once-per-trajectory scalar reward. To be compatible with standard RL algorithms, we can formally set \(\) as an "incomplete trajectory" \((s,a)\) in both Definition 1 and Algorithm 1. This would not change the theoretical results regarding the P2R reduction.

```
1:\(_{r}\), \(\{\}\), \(_{}\{\}\)
2: Execute the random policy to collect \(_{0}\)
3:Upon query of trajectory \(\):
4:if\((,)_{}\)then
5: Return \(\)
6:if\(_{r,r^{}_{r}}(r()-r(_{0}))-(r^{}()-r ^{}(_{0}))<2_{0}\)then
7:\( r()-r(_{0})\) for an arbitrary \(r_{r}\)
8:\(_{}_{}(,)\)
9:else
10: Query comparison oracle \(m\) times on \(\) and \(_{0}\); compute average comparison result \(\)
11:\(*{argmin}_{x[-H,H]}|(x)-|\), \((,)\), \(_{}_{}(,)\)
12: Update \(_{r}\) \[_{r}\{r_{r}:_{(,) }(r()-r(_{0})-)^{2}\}\]
13: Return \(\) ```

**Algorithm 1** Preference-to-Reward (P2R) Interface

Theoretical analysis of P2RWe assume that \(()\) is known and satisfies the following regularity assumption.

**Assumption 2**.: \((0)=\); for \(x[-H,H]\), \(^{}(x)>0\).

Assumption 2 is common in bandits literature (Li et al., 2017) and is satisfied by the popular Bradley-Terry model, where \(\) is the logistic function. It is further motivated by Lemma 2 and Lemma 3: when \(\) is unknown or has no gradient lower bounds, the optimal policy can be impossible to identify.

**Lemma 2**.: _If \(\) is unknown, even if there are only two possible candidates, the optimal policy is indeterminate._

Figure 1: Interaction protocol with the reward learning interface.

**Lemma 3**.: _If \(^{}()\) is not lower bounded, for instance when \((x)=(1+(x))\), the optimal policy is indeterminate._

P2R enjoys the following theoretical guarantee when we choose \(_{0}=g()/2\), \(=^{2}}{4}\), \(d_{}}=_{}(},_ {0})\) and \(m=(}}(d_{}}/ )}{_{0}^{2}^{2}})\).

**Theorem 4**.: _Suppose that Assumption 1 and 2 hold, and that \(\) is an \(g()\)-robust RL algorithm with sample complexity \((,)\). By running \(\) with the interface in Algorithm 1, we can learn an \(\)-optimal policy using \((,)\) samples and \(}(}}^{2}}{^{ 2}g()^{2}})\) queries to the comparison oracle with probability \(1-2\)._

The full proof of Theorem 4 is deferred to Appendix B.3. The key idea of the analysis is to use the fact that \(r^{}_{r}\) and the condition in Line 6 to show that the returned reward labels \(\) are approximate accurate, and to use properties of eluder dimension to bound the number of samples for which the comparison oracle is called.

Theorem 4 shows that P2R is a provably efficient reduction: any standard RL algorithm \(\) combined with P2R induces a provably efficient algorithm that learns an approximately optimal policy from preference feedback. The number of required interactions with the MDP environment is identical to that of the standard RL algorithm, and the query complexity only scales with the complexity of learning the reward function.

Comparison with other approachesA more straightforward reduction would be extensively querying the comparison oracle for every sample generated by the RL algorithm. While this direct reduction would not suffer from increased sample complexity, it encounters two other drawbacks: (1) the oracle complexity, or the total workload for human evaluators, increases proportionally with the sample complexity of the RL algorithm, which can be prohibitive; (2) the RL training would need to pause and wait for human feedback at every iteration, creating substantial scheduling overhead.

Another method to construct reward feedback is to learn a full reward function directly before running the RL algorithm, as in Ouyang et al. (2022). However, without pre-existing high-quality offline datasets, collecting the samples for reward learning would require solving an exploration problem at least as hard as RL itself (Jin et al., 2020), resulting in significant sample complexity overhead. In P2R, the exploration problem is solved by the RL algorithm using it.

Compared to the two alternative approaches, our reduction achieves the best of both worlds by avoiding sample complexity overhead with a query complexity that does not scale with the sample complexity.

### Instantiations of P2R

When combined with existing sample complexity results, Theorem 4 directly implies concrete sample and query complexity bounds for preference-based RL in many settings, with no statistical and small computational overhead.

\((s,a)\) preferencesWe first consider the comparison that is based on the immediate reward of the current state-action pair. Here we give tabular MDPs and MDPs with low Bellman-Eluder dimension (Jin et al., 2021) as two examples.

**Example 1** (Tabular MDPs).: Our first example is tabular MDPs whose state and action spaces are finite and small. In this case \(d_{}}=}(||||)\). The UCBVI-BF algorithm, proposed by Azar et al. (2017), is a model-based tabular RL algorithm which uses upper-confidence bound value iteration with Bernstein-Freedman bonuses. UCBVI-BF has sample complexity \((,)=(H^{3}||| |/^{2})\) and is \((/H)\) robust due to Lemma B.4.

**Proposition 5**.: _Algorithm 1 with UCBVI-BF learns an \(\)-optimal policy of a tabular MDP from preference feedback using \(}||||}{^ {2}}\) episodes of interaction with the environment and \(}||^{2}||^{2}}{ ^{2}^{2}}\) queries to the comparison oracle. The algorithm is computationally efficient._

**Example 2** (Low Bellman-eluder dimension).: Bellman-eluder dimension (Jin et al., 2021) is a complexity measure for function approximation RL with a Q-value function class \(\). It can beshown that a large class of RL problems, including tabular MDPs, linear MDPs, reactive POMDPs and low Bellman rank problems, all have small Bellman-eluder dimension. Furthermore, Jin et al. (2021) designed an algorithm, named GOLF, which (i) first constructs a confidence set for the optimal Q-functions by including all the candidates with small temporal difference loss, (ii) then optimistically picks Q-estimate from the confidence set and executes its greedy policy, and (iii) repeats (i) and (ii) using the newly collected data. Assuming that \(\) satisfies realizability and completeness property, GOLF is \(g()=(}H^{2}}})\)-robust with sample complexity \((,)=}(}H^{4}||}{^{2}})\) where \(d_{}\) is the Bellman-eluder dimension of the problem. By applying Theorem 4, we immediately have the following result.

**Proposition 6**.: _Algorithm 1 with GOLF (Jin et al., 2021) learns an \(\)-optimal policy of RL problems with Bellman-Eluder dimension \(d_{}\) in \(}}H^{4}||}{ ^{2}}\) episodes of interaction with the environment and queries to the comparison oracle._

Trajectory-based preferencesWhen the reward function is trajectory-based, we can instantiate P2R with the OMLE algorithm (Liu et al., 2022) to solve any model-based RLHF of low generalized eluder dimension. In brief, OMLE is an optimism-based algorithm that maintains a model confidence set. This set comprises model candidates that demonstrate high log-likelihood on previously collected data. And in each iteration, the algorithm chooses a model estimate optimistically and executes its greedy policy to collect new data.

**Example 3** (Low generalized eluder dimension).: Generalized eluder dimension (Liu et al., 2022) is a complexity measure for function approximation RL with a transition function class \(\). In Appendix D.1, we show that a simple adaptation of OMLE is \(g()=(}}})\)-robust with sample complexity \((,)=}(d_{ }\|_{}|^{2}||}{^{2}}+ {Hd_{}\|_{}|}{}),\) where \(d_{}\) denotes the generalized eluder dimension of \(\), \(|_{}|\) is a parameter in the OMLE algorithm, and \(d_{}=_{}(,)\). Plugging it back into Theorem 4, we obtain the following result.

**Proposition 7**.: _Algorithm 1 with OMLE learns an \(\)-optimal policy of RL problems with low generalized eluder dimension in \(}(d_{}\|_{}|^{2}||}{^{2}}+}\|_{}|}{})\) episodes of interaction with the environment and queries to the comparison oracle._

Liu et al. (2022) prove that a wide range of model-based reinforcement learning problems have a low generalized eluder dimension \(d_{}\) and only require a mild \(|_{}|\) to run the OMLE algorithm. Examples of such problems include tabular MDPs, factored MDPs, observable POMDPs, and decodable POMDPs. For a formal definition of generalized eluder dimension and more details on the aforementioned bound and examples, we refer interested readers to Appendix D.1 or Liu et al. (2022). Finally, we remark that it is possible to apply the P2R framework in other settings with different complexity measures, such as DEC (Foster et al., 2021) or GEC (Zhong et al., 2022), by making some minor modifications to the corresponding algorithms to ensure robustness.

### Extension to \(K\)-wise comparison

In this subsection, we briefly discuss how our results can be extended to \(K\)-wise comparison assuming the following Plackett-Luce (PL) model (Luce, 1959; Plackett, 1975) of \(K\) item preferences.

**Definition 4** (Plackett-Luce model).: _The oracle takes in \(K\) trajectories \(_{1},,_{K}\) and outputs a permutation \(:[K][K]\) with probability \(()=_{k=1}^{K}(k)}) )}{_{t=k}^{K}((_{^{-1}(t)}))}.\)_

Note that when \(K=2\), the above PL model reduces to a pair-wise trajectory-type comparison oracle (Definition 1) with \((x)=( x)/(( x)+1)\) which satisfies Assumption 2 with \(=((- H))\). The PL model satisfies the following useful property, which is a corollary of its internal consistency (Hunter, 2004).

**Property 8** (Hunter (2004, p396)).: _For any disjoint sets \(\{i_{1},j_{1}\},,\{i_{k},j_{k}\}[K]\), the following pair-wise comparisons are mutually independent: \(((i_{1})>(j_{1})),,((i_{k})>(j_{k}))\) where \(\) is a permutation sampled from PL\((_{1},,_{K})\). Moreover, \(((i_{m})<(j_{m}))((r(_{i_{m}})-r( _{j_{m}}))),\) where \((x)=( x)/(( x)+1)\)._This property enables "batch querying" the preferences on \( K/2\) pairs of \((_{1},_{2})\) in parallel, which returns \( K/2\) independent pairwise comparisons outcomes. This would enable us to reduce the number of queries by a factor of \((K)\) for small \(K\) in both Algorithm 1 and 3.

**Theorem 9** (P2R with \(K\)-wise comparison).: _Suppose \(\) is an \(g()\)-robust RL algorithm with sample complexity \((,)\), and assume the same conditions and the same choice of parameters as in Theorem 4. By running \(\) with the interface in Algorithm 1, we can learn an \(\)-optimal policy using \((,)\) samples and \(}(}^{2}}{^{2}g()^ {2}\{K,m\}})\) queries to the \(K\)-wise comparison oracle with probability \(1-2\)._

Theorem 9 is a direct consequence of Theorem 4: If \(K 2m\), we can obtain \(m\) independent comparisons between two trajectories by a single query to the \(K\)-wise comparison oracle and therefore reduce the overall query complexity in Theorem 4 by a factor of \(m\); otherwise, we can get \(m\) independent comparisons by making \((m/K)\) queries to the \(K\)-wise comparison oracle, which reduces the overall query complexity by a factor of \(K\).

However, the above parallelization benefits of using \(K\)-wise comparison might be an artifact of the PL model: it seems improbable that the same human evaluator would independently rank \( K/2\) copies of item \(A\) and item \(B\). It remains an interesting problem to develop \(K\)-wise comparison models more suitable to RLHF.

## 4 RLHF From General Preferences

The utility-based approach imposes strong assumptions on human preferences. Not only is the matrix \(M[,^{}]\) in (1) assumed to be exactly realizable by \((r()-r(^{}))\), but also \(\) is assumed to be known and have a gradient lower bound. Moreover, the utility-based approach assumes that _transitivity_: if \([_{1}_{2}] 0.5\), \([_{2}_{3}] 0.5\), then \([_{1}_{3}] 0.5\). However, experiments have shown that human preferences can be intransitive . These limitations of the utility-based approach motivates us to consider general preferences.

A general preference may not be realizable by a utility model, so we cannot define the optimal policy in the usual sense. Instead, we follow Dudik et al. (2015) and consider an alternative solution concept, the _von Neumann winner_.

**Definition 5**.: \(^{}\) _is the von Neumann winner policy if (\(^{},^{}\)) is a symmetric Nash equilibrium of the constant-sum game: \(_{}_{^{}}_{,^{}^{ }}M[,^{}]\)._

The duality gap of the game is defined as

\[(_{1},_{2}):=_{}_{,^{ }_{2}}M[,^{}]-_{}_{_{ 1},^{}}M[,^{}].\]

We say that \(\) is an \(\)-approximate von Neumann winner if the duality gap of \((,)\) is at most \(\). The von Neumann winner has been studied under the name of _maximal lotteries_ in the context of social choice theory (Kreweras, 1965; Fishburn, 1984). The von Neumann winner is a natural generalization of the optimal policy concept for non-utility based preferences. It is known that

* Intuitively, the von Neumann winner \(^{}\) is a randomized policy that "beats" any other policy \(^{}\) in the sense that \(_{^{},^{}^{}}M[,^{ }] 1/2\);
* If the utility-based preference model holds and the transitions are deterministic, the von Neumann winner is the optimal policy;
* The von Neumann winner is the only solution concept that satisfies population-consistency and composition-consistency in social choice theory (Brandl et al., 2016).

Finding the von Neumann winner seems prima facie quite different from standard RL tasks. However, in this section we will show how finding the von Neumann winner can be reduced to finding the restricted Nash equilibrium in a type of Markov games. For preferences based on the final state, we can further reduce the problem to RL in adversarial MDP.

### A reduction to Markov games

Factorized and independent Markov games (FI-MG).Consider a two-player zero-sum Markov games with state space \(=^{(1)}^{(2)}\), action space \(^{(1)}\) and \(^{(2)}\) for each player respectively,transition kernel \(\{_{h}\}_{h[H]}\) and reward function \(r\). We say the Markov game is factorized and independent if the transition kernel is factorized:

\[_{h}(s_{h+1} s_{h},a_{h})=_{h}(s_{h+1}^{(1)} s_{h}^{( 1)},a_{h}^{(1)})_{h}(s_{h+1}^{(2)} s_{h}^{(2)},a_{h}^{(2)}),\]

where \(s_{h}=(s_{h}^{(1)},s_{h}^{(2)})\), \(s_{h+1}=(s_{h+1}^{(1)},s_{h+1}^{(2)})\), \(a_{h}=(a_{h}^{(1)},a_{h}^{(2)})^{(1)}^{(2)}\).

The above definition implies that the Markov game can be partitioned into two MDPs, where the transition dynamics are controlled separately by each player, and are completely independent of each other. The only source of correlation between the two MDPs arises from the reward function, which is permitted to depend on the joint trajectory from both MDPs. Building on the above factorization structure, we define _partial trajectory_\(_{i,h}:=(s_{1}^{(i)},a_{1}^{(i)},,s_{h}^{(i)})\) that consists of states of the \(i\)-th MDP factor and actions of the \(i\)-th player. Furthermore, we define a _restricted policy class_\(_{i}\) that contains all policies that map a partial trajectory to a distribution in \(_{_{i}}\), i.e.,

\[_{i}:=\{_{h}\}_{h[H]}:\;_{h}(^{(i)} _{i})^{h-1}^{(i)}_{_{i}} }i.\]

And the goal is to learn a restricted Nash equilibrium \((^{},^{})_{1}_{2}\) such that

\[^{}_{_{1}}_{,^{} ^{}}[r(,^{})]^{}_{_{2}} _{^{},^{}}[r(,^{})].\]

Finding von Neumann winner via learning restricted Nash.We claim that finding an approximate von Neumann winner can be reduced to learning an approximate restricted Nash equilibrium in a FI-MG. The reduction is straightforward: we simply create a Markov game that consists of two independent copies of the original MDP and control the dynamics in the \(i\)-th copy by the \(i\)-th player's actions. Such construction is clearly factorized and independent. Moreover, the restricted policy class \(_{i}\) is equivalent to the universal policy class in the original MDP. We further define the reward function as \(r(,^{})=M[,^{}]\) where \(M\) is the general preference function. By definition 5, we immediately obtain the following equivalence relation.

**Proposition 10**.: _If \((^{},^{})\) is a restricted Nash equilibrium of the above FI-MG, then both \(^{}\) and \(^{}\) are von Neumann winner in the original problem._

The problem we are faced with now is how to learn restricted Nash equilibria in FI-MG. In the following sections, we present two approaches that leverage existing RL algorithms to solve this problem: (i) when the preference function depends solely on the final states of the two input trajectories, each player can independently execute an adversarial MDP algorithm; (ii) for general preference functions, a straightforward adaptation of the OMLE algorithm is sufficient under certain eluder-type conditions.

### Learning from final-state-based preferences via adversarial MDPs

In this section, we consider a special case where the preference depends solely on the final states of the two input trajectories, i.e., \(M(,^{})=M(s_{H},s_{H}^{})\). Given the previous equivalence relation between von Neumann winner and restricted Nash in FI-MG, one natural idea is to apply no-regret learning algorithms, as it is well-known that running two copies of no-regret online learning algorithms against each other can be used to compute Nash equilibria in zero-sum normal-form games. Since this paper focuses on sequential decision making, we need no-regret learning algorithms for adversarial MDPs, which we define below.

Adversarial MDPs.In the adversarial MDP problem, the algorithm interacts with a series of MDPs with the same unknown transition but adversarially chosen rewards for each episode. Formally, there exists an unknown groundtruth transition function \(=\{_{h}\}_{h=1}^{H}\). At the beginning of the \(k\)-th episode, the algorithm chooses a policy \(^{k}\) and then the adversary picks a reward function \(r^{k}=\{r_{h}^{k}\}_{h=1}^{H}\). After that, the algorithm observes a trajectory \(^{k}=(s_{h}^{k},a_{1}^{k},y_{1}^{k},,s_{H}^{k},a_{H}^{k},y_{H}^{k})\) sampled from executing policy \(^{k}\) in the MDP parameterized by \(\) and \(r^{k}\), where \([y_{h}^{k} s_{h}^{k},a_{h}^{k}]=r_{h}^{k}(s_{h}^{k},a_{h}^{k})\). We define the _regret_ of an adversarial MDP algorithm \(\) to be the gap between the algorithm's expected payoff and the best payoff achievable by the best fixed Markov policy:

\[_{K}():=_{_{}}_{k=1 }^{K}_{}[_{h=1}^{H}r_{h}^{k}(s_{h},a_{h})]-_{k= 1}^{K}_{_{k}}[_{h=1}^{H}r_{h}^{k}(s_{h},a_{h})].\]

Now we explain how to learn a von Neumann winner via running adversarial MDP algorithms. We simply create two copies of the original MDP and instantiate two adversarial MDP algorithms \(_{1}\) and \(_{2}\) to control each of them separately. To execute \(_{1}\) and \(_{2}\), we need to provide reward feedback to them in each \(k\)-th episode. Denote by \(s_{H}^{k,(1)}\) and \(s_{H}^{k,(2)}\) the final states \(_{1}\) and \(_{2}\) observe in the \(k\)-th episode. We will feed \(y^{k}(M(s_{H}^{k,(1)},s_{H}^{k,(2)}))\) into \(_{1}\) and \(1-y^{k}\) into \(_{2}\) as their reward at step \(H-1\), respectively. And all other steps have zero reward feedback.The formal pseudocode is provided in Algorithm 4 (Appendix E). The following theorem states that as long as the invoked adversarial MDP algorithm has sublinear regret, the above scheme learns an approximate von Neumann winner in a sample-efficient manner.

**Theorem 11**.: _Suppose \(_{K}() K^{1-c}\) with probability at least \(1-\) for some \(c(0,1)\). Then Algorithm 4 with \(K=(4/)^{1/c}\) outputs an \(\)-approximate von Neumann winner with probability at least \(1-2\)._

In order to demonstrate the applicability of Theorem 11, we offer two examples where sublinear regret can be achieved in adversarial MDPs via computationally efficient algorithms. The first one is adversarial tabular MDPs where the number of states and actions are finite, i.e., \(||,||+\).

**Example 4** (adversarial tabular MDPs).: Jin et al. (2019) proposed an algorithm \(\) with \(_{K}()}(|^{2}||H^{3}K})\). Plugging it into Theorem 11 leads to \(K=}(||^{2}||H^{3}/^{2})\) sample complexity and query complexity for learning \(\)-approximate von Neumann winner.

The second example is adversarial linear MDPs where the number of states and actions can be infinitely large while the transition and reward functions admit special linear structure. See Sherman et al. (2023) for the precise formulation of the adversarial linear MDP problem.

**Example 5** (adversarial linear MDPs).: Sherman et al. (2023) proposed an algorithm \(\) with \(_{K}()}(dH^{2}K^{6 /7})\) for online learning in adversarial linear MDPs.3 Combining it with Theorem 11 leads to \(K=}(d^{7}H^{14}/^{7})\) sample complexity and query complexity for learning \(\)-approximate restricted Nash equilibria.

### Learning from trajectory-based preferences via OMLE_equilibrium

In this section, we consider the more general case where the preference \(M[,^{}]\) is allowed to depend arbitrarily on the two input trajectories. Similar to the utility-based setting, we assume that the learner is provided with a preference class \((()^{H}( )^{H})\) and transition function class \(\) a priori, which contains the groundtruth preference and transition we are interacting with. Previously, we have established the reduction from learning the von Neumann winner to learning restricted Nash in FI-MG. In addition, learning restricted Nash in FI-MG is in fact a special case of learning Nash equilibrium in partially observable Markov games (POMGs). As a result, we can directly adapt the existing OMLE algorithm for learning Nash in POMGs (Liu et al., 2022) to our setting, with only minor modifications required to learn the von Neumann winner. We defer the algorithmic details for this approach (Algorithm 5) to Appendix F, and present only the theoretical guarantee here.

**Theorem 12**.: _Suppose Assumption 3 holds. There exist absolute constant \(c_{1}\) and \(c_{2}\) such that for any \((T,)(0,1]\) if we choose \(_{}=c_{1}(||T/)\) and \(_{}=c_{1}(||T/)\) in Algorithm 5, then with probability at least \(1-\), the duality gap of the output policy of Algorithm 5 is at most_

\[},T,c_{2}_{},|_{}|)}{ T}+c_{2}}_{}}{T}},\]

_where \(d_{}=_{}(,1/T)\)._

It has been proven that a wide range of RL problems admit a regret formulation of \(=}(}|_{}|T})\) with mild \(d_{}\) and \(|_{}|\)(Liu et al., 2022). These problems include, but are not limited to, tabular MDPs, factored MDPs, linear kernel MDPs, observable POMDPs, and decodable POMDPs. For more details, please refer to Appendix D.1 or Liu et al. (2022). For problems that satisfy \(=}(}_{}|_{ }|T})\), Theorem D.1 implies a sample complexity of

\[}(}|_{}|| |}{^{2}}+}||}{ ^{2}}).\]

The sample complexity for specific tractable problems can be derived by plugging their precise formulation of \(\) (provided in Appendix D.1) into the above bound.