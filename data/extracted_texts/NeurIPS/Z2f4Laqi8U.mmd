# An efficient search-and-score algorithm for ancestral graphs using multivariate information scores

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a greedy search-and-score algorithm for ancestral graphs, which include directed as well as bidirected edges, originating from unobserved latent variables. The normalized likelihood score of ancestral graphs is estimated in terms of multivariate information over relevant subsets of vertices, \(\), that are connected through collider paths confined to the ancestor set of \(\). For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational strategy is shown to outperform state-of-the-art causal discovery methods on challenging benchmark datasets.

## 1 Introduction

The likelihood function plays a central role in the selection of a graphical model \(\) based on observational data \(\). Given \(N\) independent samples from \(\), the likelihood \(_{|}\) that they might have been generated by the graphical model \(\) is given by ,

\[_{|}=,}}\, (-NH(p,q))\] (1)

where \(H(p,q)=-_{}p() q()\) is the cross-entropy between the empirical probability distribution \(p()\) of the observed data \(\) and the theoretical probability distribution \(q()\) of the model \(\) and \(Z_{,}\) a data- and model-dependent factor ensuring proper normalization condition for finite dataset. In short, Eq.1 results from the asymptotic probability that the \(N\) independent samples, \(^{(1)},,^{(N)}\), are drawn from the model distribution, \(q()\), _i.e._\(_{|} q(^{(1)},,^{(N)})= _{i}q(^{(i)})\), rather than the empirical distribution, \(p()\). This leads to, \(_{|}=_{i} q(^{(i)})\), which converges towards \(N_{}p() q()=-N\,H(p,q)\) in the large sample size limit, \(N\), with \( Z_{,}=( N)\).

The structural constraints of the model \(\) translate into the factorization form of the theoretical probability distribution, \(q()\)[2; 3; 4; 5; 6]. In particular, the probability distribution of Bayesian networks (BN) factorizes in terms of conditional probabilities of each variable given its parents, as \(q_{{}_{}}()=_{i}q(x_{i}|_{X_{i}})\), where \(_{X_{i}}\) denote the values of the parents of node \(X_{i}\) in \(\), \(_{X_{i}}\). For Bayesian networks, the factors of the model distribution, \(q(x_{i}|_{X_{i}})\), can be directly estimated with the empirical conditional probabilities of each node given its parents as, \(q(x_{i}|_{X_{i}}) p(x_{i}|_{X_{ i}})\), leading to the well known estimation of the likelihood function in terms of conditional entropies \(H(X_{i}|_{X_{i}})=-_{}p(x_{i},_ {X_{i}}) p(x_{i}|_{X_{i}})\),

\[_{|_{}}=, _{}}}\,(-N}{} }{H(X_{i}|_{X_{i}})})\] (2)This paper concerns the experimental setting for which some variables of the underlying Bayesian model are not observed. This frequently occurs in practice for many applications. We derive an explicit likelihood function for the class of ancestral graphs, which include directed as well as bidirected edges, arising from the presence of unobserved latent variables. Tian and Pearl 2002  showed that the probability distribution of such graphs factorizes into c-components including subsets of variables connected through bidirected paths (_i.e._ containing only bidirected edges). Richardson 2009  later proposed a refined factorization of the model distribution of the broader class of acyclic directed mixed graphs in terms of conditional probabilities over "head" and "tail" subsets of variables within each ancestrally closed subsets of vertices. However, unlike with Bayesian networks, the contributions of c-components or head-and-tail factors to the likelihood function cannot simply be estimated in terms of empirical distribution \(p()\), as shown below. This leaves the likelihood function of ancestral graphs difficult to estimate from empirical data, in general, although iterative methods have been developped when the data is normally distributed [8; 9; 10; 11; 12; 13].

The present paper provides an explicit decomposition of the likelihood function of ancestral graphs in terms of multivariate cross-information over relevant '\(ac\)-connected' subsets of variables, Figs. 1, which do not rely on the head-and-tail factorization but coincide with the parametrizing sets  derived from the head-and-tail factorization. It suggests a natural estimation of these revelant contributions to the likelihood function in terms of empirical distribution \(p()\). This result extends the likelihood expression of Bayesian Networks (Eq. 2) to include the effect of unobserved latent variables and enables the implementation of a greedy search-and-score algorithm for ancestral graphs. For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational strategy is shown to outperform state-of-the-art causal discovery methods on challenging benchmark datasets.

## 2 Theoretical results

### Multivariate cross-entropy and cross-information

The theoretical result of the paper (Theorem 1) is expressed in terms of multivariate cross-information derived from multivariate cross-entropies through the Inclusion-Exclusion Principle. The same expressions can be written between multivariate information and multivariate entropies by simply substituting \(q(\{x_{i}\})\) with \(p(\{x_{i}\})\) in the equations below and will be used to estimate the likelihood function of ancestral graphs (Proposition 3).

As recalled above, the cross-entropy between \(m\) variables, \(=\{X_{1},,X_{m}\}\), is defined as,

\[H()\;=\;-\!\!_{\{x_{i}\}}p(x_{1},,x_{m}) q(x_{1},,x _{m})\] (3)

where \(p(\{x_{i}\})\) is the empirical joint probability distribution of the variables \(\{X_{i}\}\) and \(q(\{x_{i}\})\) the joint probability distribution of the model. Bayes formula, \(q(\{x_{i}\},\{y_{j}\})=q(\{x_{i}\}|\{y_{j}\})\)\(q(\{y_{j}\})\), directly translates into the definition of conditional cross-entropy through the decomposition,

\[H(\{X_{i}\},\{Y_{j}\})=H(\{X_{i}\}|\{Y_{j}\})+H(\{Y_{j}\})\] (4)

Multivariate (cross) information, \(I() I(X_{1};;X_{m})\), are defined from multivariate (cross) entropies through Inclusion-Exclusion formulas over all subsets of variables [15; 16; 17; 18] as,

\[I(X) = H(X)\] \[I(X;Y) = H(X)+H(Y)-H(X,Y)\] \[I(X;Y;Z) = H(X)+H(Y)+H(Z)-H(X,Y)-H(X,Z)-H(Y,Z)+H(X,Y,Z)\] \[I() = -_{}(-1)^{||}H()\] (5)

where the semicolon separators are needed to distinguish multipoint (cross) information from joint variables as \(\{X,Z\}\) in \(I(\{X,Z\};Y)=I(X;Y)+I(Z;Y)-I(X;Y;Z)\). Below, implicit separators between non-conditioning variables in multivariate (cross) information will always correspond to semicolons, _e.g._ as in \(I()\) in Eq. 5. Unlike multivariate (cross) entropies, which are always positive,

[MISSING_PAGE_FAIL:3]

#### 2.2.2 _ac_-connecting paths and _ac_-connected subsets

Let us now define **ancestor collider connecting paths** or _ac_-connecting paths**, which entail simpler path connecting criterion than the traditional **m-connecting criterion**, discussed in the Appendix A. Yet, _ac_-connecting paths** and _ac_-connected subsets** will turn out to be directly relevant to characterize the likelihood decomposition and Markov equivalent classes of ancestral graphs.

**Definition 2**. [_ac_-connecting path] An \(ac\)-connecting path between \(X\) and \(Y\) given a subset of variables \(\) (possibly including \(X\) and \(Y\)) is a collider path, \(X\!\! Z_{1} Z_{K} \!\!Y\), with all \(Z_{i}_{}(\{X,Y\})\), that is, with \(Z_{i}\) in \(\) or connected to \(\{X,Y\}\) by an ancestor path, _i.e._\(Z_{i} T\) with \(T\{X,Y\}\).

**Definition 3**. [_ac_-connected subset] A subset \(\) is said to be \(ac\)-connected if \( X,Y\), \(X\) and \(Y\) are connected (through any type of edge) or there is an \(ac\)-connecting path between \(X\) and \(Y\) given \(\).

### Likelihood decomposition of ancestral graphs

**Theorem 1**.: **[likelihood of ancestral graphs]** _The cross-entropy \(H(p,q)\) and likelihood \(_{|}\) of an ancestral graph \(\) is decomposable in terms of multivariate cross-information, \(I()\), summed over all \(ac\)-connected subsets of variables, \(\) (Definition 3),_

\[H(p,q) = -_{}^{-} (-1)^{||}I()\] \[_{|} = ,}}(N_{ }^{-}(-1)^{||}I())\] (12)

_where \(N\) is the number of iid samples in the dataset \(\) and \(Z_{,}\) a data- and model-dependent normalization constant._

The proof of Theorem 1 is left to the Appendix B. It is based on a partition of the cross-entropy (Eq. 6) into cross-information contributions from \(ac\)-connected and non-\(ac\)-connected subsets of variables, which do not rely on head-and-tail factorizations. Hu and Evans  proposed an equivalent result (Proposition 3.3 in ) with a proof using head-and-tail decomposition to define parametrizing sets, which happen to coincide with the \(ac\)-connected sets defined here (Definition 3). Theorem 1 characterizes in particular the Markov equivalence class of ancestral graphs  as,

**Corollary 2**.: _Two ancestral graphs are Markov equivalent if and only if they have the same \(ac\)-connected subsets of vertices._

Note, in particular, that Eq. 12 holds for _maximal ancestral graphs_ (MAG), for which all pairs of \(ac\)-connected variables are connected by an edge, and their Markov equivalent representatives, the _partial ancestral graphs_ (PAG) .

**Proposition 3**.: The likelihood decomposition of ancestral graphs (Eq. 12, Theorem 1) can be estimated by replacing the model distribution \(q\) by the empirical distribution \(p\) in the retained multivariate cross-information terms \(I()\) corresponding to all _ac_-connected subsets of variables, \(\).

Hence, Proposition 3 amounts to estimating all relevant cross-information terms in the likelihood function with the corresponding multivariate information terms computed from the available data, while assuming by construction that the model distribution obeys all local and global conditional independences entailed by the ancestral graph. The corresponding factorization of the model distribution can be expressed in terms of empirical distribution, assuming positive distributions, see Appendix C.

Fig. 1 illustrates the cross-entropy decomposition for a few graphical models in terms of cross-information contributions from their \(ac\)-connected subsets of vertices. In particular, an unshielded non-collider (_e.g._\(X Z W\), Fig. 1A), is less likely (_i.e._ higher cross-entropy) than an unshielded collider or 'v-structure' (_e.g._\(X Z W\), Fig. 1B), if the corresponding three-point information term is negative, \(I(X;Z;W)<0\), in agreement with earlier results . However, this early approach, exploiting the sign and magnitude of three-point information to orient v-structures, does not include higher order terms involving multiple v-structures, which can lead to orientation conflicts between unshielded triples, in practice. Resolving such orientation conflicts requires to include

[MISSING_PAGE_EMPTY:5]

collider path, Fig. 1D. By contrast, the cross-entropy based on the head-and-tail factorization of the same two-collider path, _i.e._\(q(x,z,y,w)=q(z,y|x,w)q(x)q(w)\), is found to be equivalent to the cross-entropy of a Bayesian graph without bidirected edge, Fig. 1E, when estimated with the empirical distribution \(p(.)\), see Appendix C. This observation illustrates the difficulty to estimate the likelihood functions of ancestral graphs using head-and-tail factorization.

Further examples of graphical models, Figs. 1F-I, show the relative simplicity of the decomposition with only few (non-trivial) \(ac\)-connected contributing subsets \(\) with \(|| 3\), as compared to the much larger number of non-\(ac\)-connected non-contributing subsets, that cancel each other by construction due to conditional independence constraints of the underlying model. Note, in particular, that most contributing multivariate information \(I()\) only concern direct connections or collider paths within a single component subgraph induced by \(\) (solid line edges in Fig. 1). However, occasionally, collider paths extending beyond \(\) into \(_{}()\) (marked with wiggly edges) with corresponding ancestor path(s) (marked with dashed edges) do occur, as shown in Fig. 1G.

In addition, the present information-theoretic decomposition of the likelihood of ancestral graphs can readily distinguish their Markov equivalence classes according to Corollary 2. For instance, the ancestral graphs of Fig. 1F and Fig. 1G, despite sharing the same edges and the same unshielded collider (\(X Z T\)), turn out not to be Markov equivalent, as discussed in . Indeed, their cross-entropy decompositions differ by two \(ac\)-connected contributing terms: a three-point cross information \(I(X;Y;T)\) with a collider path not confined in \(\) (_i.e._\(X Z T Y\) and corresponding ancestor path \(Z Y\)) and a four-point information term \(I(X;Y;Z;T)\) due to the two-collider path (\(X Z T Y\)). More quantitatively, it shows that the graph of Fig. 1G with a two-collider path is more likely than the graph of Fig. 1F whenever \(I(X;Y;T)-I(X;Y;Z;T)=I(X;Y;T|Z)=I(X;Y|Z)-I(X;Y|Z,T)\!<\!0\). Finally, the Markov equivalent graphs of Fig. 1H and Fig. 2, also due to , illustrate the fact that the actual ancestor collider path between unconnected pairs does not need to be unique nor conserved between Markov equivalent graphs (as long as their cross-entropies share the same multivariate cross-information decomposition).

## 3 Efficient search-and-score causal discovery using local information scores

The likelihood estimation of ancestral graphs (Theorem 1 and Proposition 3) enables the implementation of a search-and-score algorithm for this broad class of graphs, which has attracted a number of contributions recently [11; 12; 13; 30; 31; 32]. Our specific objective is not to develop an exact method limited to simple graphical models with a few nodes and small datasets but to implement an efficient and reliable heuristic method applicable to more challenging graphical models and large datasets.

Indeed, search-and-score structure learning methods need to rely on heuristic rather than exhaustive search, in general, given that the number of ancestral graphs grows super-exponentially as the number of vertices increases. This can be implemented for instance with a Monte Carlo algorithmic scheme with random restarts, which efficiently probes relevant graphical models. Here, we opt, instead, to use the prediction of an efficient hybrid causal discovery method, MMIC [29; 33; 34], as starting point for a subsequent search-and-score approach based on the proposed likelihood estimation of ancestral graphs (Eq. 12 and Proposition 3).

Moreover, while the likelihood decomposition of ancestral graphs may involve extended \(ac\)-connected subsets of variables, as illustrated in Fig. 1, we aim to implement a computationally efficient search-and-score causal discovery method based on approximate local scores limited to the close surrounding vertices of each node and edge. Yet, while MMIC only relies on unshielded triple scores, the novel search-and-score extension, MMIC_search&score, uses also higher-order local information scores to compare alternative subgraphs, as detailed below.

The proposed method is shown to outperform MMIC and other state-of-the-art causal discovery methods on challenging datasets including latent variables.

### MMIC, an hybrid causal discovery method based on unshielded triple scores

MIIC is an hybrid causal discovery method combining constraint-based and information-theoretic frameworks [29; 35]. Unlike traditional constraint-based methods [4; 5], MMIC does not directly attempt to uncover conditional independences but, instead, iteratively substracts the most significant three-point (conditional) information contributions of successive contributors, \(A_{1}\), \(A_{2}\),..., \(A_{n}\), from the mutual information between each pair of variables, \(I(X;Y)\), as,

\[I(X;Y)-I(X;Y;A_{1})-I(X;Y;A_{2}|A_{1})--I(X;Y;A_{n}|\{A_{i}\}_{n-1})=I(X;Y| \{A_{i}\}_{n})\] (13)

where \(I(X;Y;A_{k}|\{A_{i}\}_{k-1})>0\) is the _positive_ information contribution from \(A_{k}\) to \(I(X;Y)\)[28; 36]. Conditional independence is eventually established when the residual conditional mutual information on the right hand side of Eq. 13, \(I(X;Y|\{A_{i}\}_{n})\), becomes smaller than a complexity term, _i.e._\(k_{X;Y|\{A_{i}\}}() I(X;Y|\{A_{i}\}_{n}) 0\), which dependents on the considered variables and sample size \(N\).

This leads to an undirected skeleton, which MMIC then (partially) orients based on the sign and amplitude of the regularized conditional 3-point information terms [28; 29]. In particular, negative conditional 3-point information terms, \(I(X;Y;Z|\{A_{\}})\!<\!0\), correspond to the signature of causality in observational data  and lead to the prediction of a v-structure, \(X Z Y\), if \(X\) and \(Y\) are not connected in the skeleton. By contrast, a positive conditional 3-point information term, \(I(X;Y;Z|\{A_{\}})\!>\!0\), implies the absence of a v-structure and suggests to propagate the orientation of a previously directed edge \(X Z-Y\) as \(X Z Y\).

In practice, MMIC's strategy to circumvent spurious conditional independences significantly improves recall, that is, the fraction of correctly recovered edges, compared to traditional constraint-based methods [28; 29]. Yet, MMIC only relies on unshielded triple scores to reliably uncover significant contributors and orient v-structures, as outlined above. MMIC has been recently improved to ensure the consistency of the separating set in terms of indirect paths in the final skeleton or (partially) oriented graphs [37; 34] and to improve the reliably of predicted orientations [33; 34].

The predictions of this recent version of MMIC, which include three type of edges (directed, bidirected and undirected), have been used as starting point for the subsequent local search-and-score method implemented in the present paper.

### New search-and-score method based on higher-order local information scores

Starting from the structure predicted by MMIC, as detailed above, MMIC_search&score method proceeds in two steps.

#### 3.2.1 Step 1: Node scores for edge orientation priming and edge removal

The first step consists in minimizing a node score corresponding to the local normalized log likelihood of each node w.r.t. its possible parents or spouses amongst the connected nodes predicted by MMIC. To this end, the node score assesses the conditional entropy of each node w.r.t. a selection of parents, spouses or neighbors, \(^{}_{x_{i}}_{x_{i}}\!_{x_{ i}}\!_{x_{i}}\!\), and a factorized Normalized Maximum Likelihood (fNML) regularization , see Appendix D for details,

\[_{}(X_{i})=H(X_{i}|^{}_{x_{i}})+ _{j}^{q_{x_{i}}}^{r_{x_{i}}}_{n_{j}}\] (14)

where \(q_{x_{i}}\) corresponds to the combination of levels of \(^{}_{x_{i}}\), while \(r_{x_{i}}\) is the number of levels of \(X_{i}\), and \(n_{j}\) the number of samples corresponding to a particular combination of levels \(j\) in each summand, with \(_{j}n_{j}=N\), the total number of samples. \(^{r_{x_{i}}}_{n_{j}}\) is the fNML regulatization cost summed over all combinations of levels, \(q_{x_{i}}\), [38; 39], see Appendix D.

This first algorithm is looped over each node, priming the orientations of their surrounding edges (as directed, bidirected or undirected), until convergence. Edges without orientation priming at either extremity are removed at the end of Step 1.

#### 3.2.2 Step 2: Edge orientation scores

The second step consists in minimizing an edge orientation score corresponding to the local normalized log likelihood of each edge w.r.t. its nodes' parents and spouses inferred in Step 1. To this end, the edge score assesses the conditional information and a fNML complexity cost with respect to the type of orientation, given three sets of parents and spouses of \(X\) and \(Y\), _i.e._\(^{}_{x_{i}\!\!Y}=_{x_{i}}\!_{x _{i}}\!\!Y\), \(^{}_{\!\!X}=_{x_{i}}\!_{x_{i} }\!\!X\) and \(^{}_{x_{i}\!\!Y}=^{}_{x_{i}\!\!Y} ^{}_{\!\!X}\) with their corresponding combinations of levels, \(q_{_{x}}\), \(q_{_{y}}\) and \(q_{_{x}}\). These orientation scores, listed in Table 1, include symmetrized fNML complexity terms to enforce Markov equivalence, if \(X\) and \(Y\) share the same parents or spouses (excluding \(X\) and \(Y\)), see Appendix D. Indeed, all three scores become equals if \(^{}_{_{Y}}=^{}_{_{Y}}=^{}_{_{Y}}\) implying also the same combinations of parent and spouse levels, \(q_{_{x}}=q_{_{y}}=q_{xy}\).

This second algorithm is looped over each edge to compute an orientation score decrement, given the orientations of its surrounding edges. The orientation change corresponding to the largest orientation score decrement is then chosen at each iteration until convergence or until a limit cycle is reached and stopped at the lowest sum of local orientation scores.

## 4 Experimental results

We first tested whether MIL_search&score orientation scores (Table 1) effectively predicts bidirected orientations on three simple ancestral models, Fig. 3, when the end nodes do not share the same parents (Fig. 3, Model 1), share some parents (Fig. 3, Model 2) or when the bidirected edge is part of a longer than two-collider paths (Fig. 3, Model 3). The prediction of the edge orientation scores are summarized in Table 3, Appendix E, and show good predictions for large enough datasets.

Beyond these simple examples, focussing on the discovery of bidirected edges in small toy models of ancestral graphs, we also analyzed more challenging benchmarks from the bnlearn repository , Fig. 2. They concern ancestral graphs obtained by hiding up to 20% of variables in Bayesian Networks of increasing complexity (number of nodes and parameters), such as Alarm (37 nodes, 46 links, 509 parameters), Insurance (27 nodes, 52 links, 984 parameters), and Barley (48 nodes, 84 links, 114,005 parameters). We then assessed causal discovery performance in terms of _Precision_, \(TP/(TP+FP)\), and _Recall_, \(TP/(TP+FN)\), relative to the theoretical PAGs, while counting as false positive (\(FP\)), all correctly predicted edges but without or with a different orientation as the directed or bidirected edges of the PAG.

Fig. 2 compares MIL_search&score performance to MILC results used as starting point for MILC_search&score and to FCI . MILC and MILC_search&score settings were set as described in section 3 above. The open-source MILC R package (v1.5.2, GPL-3.0 license) was obtained at https://github.com/miicTeam/miic_R_package. FCI from the python causal-learn package (v0.1.3.8, MIT license)  was obtained at https://github.com/py-why/causal-learn and run with G\({}^{2}\)-conditional independence test and default parameter \(=0.05\).

Overall, MILC_search&score is found to outperform MILC in terms of edge precision with little to no decrease in edge recall, Fig. 2, demonstrating the benefit of MILC_search&score's rationale to improve MILC predictions by extending MILC information scores from unshielded triples to higher-order information contributions. These originate from \(ac\)-connected subsets including nodes with more than two parents or spouses, or \(ac\)-connected subsets including two-collider paths. MILC_search&score is also found to outperform FCI on complex ancestral benchmark networks with many parameters, such as Barley (114,005 parameters), Fig. 2. However, FCI is found to reach similar or better precision scores on easier benchmarks with fewer parameters (_i.e._ Alarm and Insurance), although its recall remains usually lower than MILC_search&score, especially at small sample size, as expected for a purely constraint-based causal discovery approach.

Importantly, the benchmark PAGs used to score the causal discovery results with increasing proportions of latent variables, Fig. 2, include not only bidirected edges originating from hidden common causes but also additional directed or undirected edges arising, in particular, from indirect effects of

  Edge & Information & Symmetrized fNML complexity (Markov equivalent) \\ \(X Y\) & \(-I(X;Y|^{}_{_{Y}})\) & \(_{j}^{q_{_{y}}q_{_{y}}}^{r_{x}}_ {n_{j}}-_{j}^{q_{_{y}}}^{r_{x}}_{n_{j}}+_{j}^{q_{ _{x}}r_{x}}^{r_{y}}_{n_{j}}-_{j}^{q_{_{x}}} ^{r_{y}}_{n_{j}}\) \\ \(X Y\) & \(-I(X;Y|^{}_{_{Y}})\) & \(_{j}^{q_{_{y}}q_{_{y}}}^{r_{x}}_ {n_{j}}-_{j}^{q_{_{y}}}^{r_{x}}_{n_{j}}+_{j}^{q_{ _{x}}r_{x}}^{r_{y}}_{n_{j}}-_{j}^{q_{_{y}}} ^{r_{y}}_{n_{j}}\) \\ \(X Y\) & \(-I(X;Y|^{}_{_{Y}})\) & \(_{j}^{q_{_{y}}r_{y}}^{r_{x}}_{n_{j}}- _{j}^{q_{_{x}}r_{y}}^{r_{x}}_{n_{j}}+_{j}^{q_{_ {x}}r_{x}}^{r_{y}}_{n_{j}}-_{j}^{q_{_{x}}}^{r_{y}}_{n_{j}}\) \\  

Table 1: Local scores for the orientation of a single directed or bidirected edge.

hidden variables with observed parents. Irrespective of their orientations, all these additional edges originating from indirect effects of hidden variables generally correspond to weaker effects (_i.e._ lower mutual information of indirect effects due to the Data Processing Inequality) and are more difficult to uncover than the edges of the original graphical model without hidden variables.

## 5 Limitations

The main limitation of the paper concerns the local scores used in the search-and-score algorithm, which are limited to \(ac\)-connected subsets of vertices with a maximum of two-collider paths.

While this approach could be extended to higher-order information contributions including three- or more collider paths, it allows for a simple two-step search-and-score scheme at the level of individual nodes (step 1) and edges (step 2), as detailed in section 3. This already shows a significant improvement in causal discovery performance (_i.e._ combing good precision and good recall on challenging benchmarks) as compared to existing state-of-the-art methods.

Figure 2: **Benchmark results on ancestral graphs of increasing complexity. Benchmark results on ancestral graphs obtained by hiding 0%, 5%, 10% or 20% of variables in Bayesian Networks of increasing complexity (see main text): Alarm (lhs), Insurance (middle), and Barley (rhs). MIL_search&score results are compared to MILC results used as starting point for MILC_search&score and FCI . Causal discovery performance is assessed in terms of _Precision_ and _Recall_ relative to the theoretical PAGs, while counting as false positive all correctly predicted edges but without or with a different orientation as the directed or bidirected edges of the PAG. Error bars (\(\)): standard deviations.**