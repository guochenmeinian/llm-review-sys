# DVSOD: RGB-D Video Salient Object Detection

Jingjing Li\({}^{1}\), Wei Ji\({}^{1,2}\)*, Size Wang\({}^{1}\), Wenbo Li\({}^{2}\), Li Cheng\({}^{1}\)

\({}^{1}\)University of Alberta, Canada

\({}^{2}\)Samsung Research America AI Center, USA

{jingjin1, wji3, size1, lcheng5}@ualberta.ca, wenbo.li1@samsung.com

###### Abstract

Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. To bridge this gap, we in this work introduce the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (**DVSOD**). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, _our dataset and benchmark results are publicly accessible at: https://dvsod.github.io/_.

## 1 Introduction

Salient object detection (SOD) is a fascinating field of study that aims at identifying and distinguishing the most eye-catching components in static images  or dynamic videos . This intriguing concept hails from cognitive science research into human visual attention behavior - our incredible ability to swiftly direct attention to the most valuable portions of visual scenes . As such, SOD assumes an indispensable role in a myriad of real-world applications , such as image retrieval, video editing/analysis, medical diagnosis, object detection, and target tracking.

This field has witnessed significant advances in recent years, particularly in RGB-image based SOD . However, it has been observed that performance often degrades when objects and their surroundings share similar appearances or when backgrounds are heavily cluttered. With the growing ubiquity of 3D imaging sensors in depth cameras, such as Kinect and Intel RealSense, as well as in mobile devices like iPhone 13 Pro, Huawei Mate 50, and Samsung Galaxy S21, the value of fully leveraging RGB-D information for SOD has gained considerable research interest . As visualized in Fig. 1, paired depth maps contribute valuable 3D geometric information that helps the discernment of object silhouettes against backgrounds. Consequently, augmenting RGB images with depth maps as additional input dramatically enhances the accuracy in localizing salient objects in complex scenes, _i.e._, '_w_/ RGB' _vs. '_w_/ RGBD' in Fig. 1.

On another front, considering the dynamic nature of real-world scenes, video salient object detection (VSOD) has been a prominent focus of research . In contrast to static images, dynamic video scenario presents considerable difficulties due to the diversity of motion patterns, occlusions, blurring, large object deformations, and the inherent complexity of human visual attention behavior.

By leveraging temporal contexts within a video sequence, VSOD methods [19; 63] have demonstrated improved capture of moving foreground objects and modeling of visual attention shifts in dynamic environments.

While there have been significant strides in both RGB-D SOD and VSOD individually, there remains a paucity of research into combining multimodal and temporal contexts, both of which are vital for accurately detecting the salient objects in a scene. This lack is primarily due to the absence of suitable benchmark datasets. As an alternative, Lu _et al._ have made the initial attempt to amalgamate existing VSOD datasets with estimated depth maps. However, these estimated depth maps often fall short of accurately reflecting the real world, and are prone to errors introduced by depth estimation methods, compared to data captured by actual depth sensors.

To bridge the existing gap and prosper the progress in RGB-D video SOD (_i.e._, DVSOD), we introduce the first real-world RGB-**D**V**ideo **S**aliency detection dataset, abbreviated as DViSal, in this study. Specifically, the DViSal dataset comprises 237 RGB-D videos at a frame rate of 25 ft/s, including 175,442 RGB-D pairs in total and 7,117 annotated frames. Besides, the videos were carefully selected to encompass a broad diversity of real scenarios and motion patterns. Another distinguishing characteristic of the DViSal dataset is the variety of annotation formats provided. In addition to the conventional object-level annotations used in SOD tasks, the dataset offers more comprehensive instance-level annotations, as well as weak annotations consisting of bounding boxes and scribbles. These diverse annotation sets may serve as valuable resources for a wider range of potential research directions, such as instance-level DVSOD, weakly-supervised DVSOD, and more.

We further benchmark the DVSOD by implementing 11 cutting-edge SOD models on the newly-proposed DViSal dataset, including _RGB-image-based_ SOD models, _RGB-D_ SOD models, and _VSOD_ models. Additionally, we have assembled a straightforward DVSOD model to dissect the specific contributions offered by depth maps and video contexts. The empirical results underscore the utility of both the multimodal RGB-D cues and temporal video contexts in accurately identifying salient objects. In particular, the DVSOD model is shown to significantly enhance the performance of the base RGB network , as evidenced by a noteworthy increase in the F-measure (_i.e._, 0.548 to 0.610). Lastly, we contemplate and highlight promising avenues for future research. We observe that the field of DVSOD is far from fully explored, with substantial room for improvement and innovation. This research presents the community with an exciting opportunity to delve deeper into this burgeoning field.

## 2 Related Works

In the era of deep learning, benchmark datasets have become the foundational infrastructure for advancing the state-of-the-art in computer vision research. Prominent publicly available benchmarks, such as DUTS , SOC , and PASCAL-S , have greatly contributed to the remarkable progress in RGB-image-based salient object detection (SOD) [73; 65; 72] over the past decade. Despite their significant contributions, these benchmarks primarily center their attention on RGB images, overlooking the valuable depth data that can enhance the discrimination of object silhouettes from backgrounds.

With the rising popularity of 3D imaging sensors, acquiring RGB-D data has become easier with modern equipments (_e.g._, Kinect, Samsung Galaxy S21 and iPhone 13 Pro). This development has

Figure 1: Visual illustration of the advantages of employing RGBD videos for salient object detection. The last three columns exhibit the segmentation results achieved using different input modalities.

s of the proposed DVISal, spurred the creation of several RGB-D datasets aimed at addressing the segmentation of salient objects in complex scenarios. For instance, the STERE dataset , which serves as the first collection of stereoscopic photos in this domain, boasts of 797 samples. The NJU2K dataset , on the other hand, houses 1,985 paired images in its most recent version, sourced from various platforms such as the internet, 3D movies, and images captured by a Fuji W3 stereo camera. Large-scale benchmarks such as ReDWeb-S  and COME  have been curated to validate the scalability of diverse models, thereby fostering advancement in this domain. Many novel RGB-D models [28; 34; 23; 16; 85; 86; 45; 38; 39] have been developed, showing that depth data, which includes spatial structure and 3D layout cues within a scene, can assist segmentation in challenging scenarios. For example,  proposed a prototype sampling network to adaptively weight RGB and depth superpixels according to a reliance selection module.  explored visual transformer to capture global contexts, achieving promising results.

It is worth noting that the existing RGB-D SOD datasets rely on single static images. The lack of a mechanism to account for the temporal contexts might impede their effectiveness when dealing with video inputs that depict dynamic scenes, which are omnipresent in our daily lives [87; 83; 40; 26]. As depicted in Table 1, the current SOD datasets provide as input either single pairs of RGB and depth images (_e.g._, STERE  and NJU2K ), or RGB only video sequences (_e.g._, VOS  and DAVSOD ). There unfortunately lacks a suitable dataset to provide both types of critical information - spatial layout cues derived from depth data and temporal context derived from dynamic video sequences, in addition to the RGB images.

In this work, we focus on a systematic exploration of RGB-D video salient object detection to bridge this existing gap. We introduce a carefully curated benchmark dataset named DViSal. Table 1

  Dataset & Year & Depth & Video & Obj-GT & Ins-GT & Weak-GT & \#Video & \#GT & \#Ins \\  MSRA-B  & 2007 & ✗ & ✗ & ✓ & ✗ & ✓ & - & 5,000 & - \\ SEDL2  & 2007 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 200 & - \\ ASD  & 2009 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 1,000 & - \\ SOD  & 2010 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 300 & - \\ M10K  & 2011 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 10,000 & - \\ DUT-O  & 2013 & ✗ & ✗ & ✓ & ✗ & ✓ & - & 5,168 & - \\ ECSSD  & 2013 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 1,000 & - \\ PASCAL-S  & 2014 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 850 & - \\ HKU-IS  & 2015 & ✗ & ✗ & ✓ & ✗ & ✗ & - & 4,447 & - \\ SOS  & 2015 & ✗ & ✗ & ✗ & ✗ & ✓ & - & 6,900 & - \\ MSO  & 2015 & ✗ & ✗ & ✗ & ✗ & ✓ & - & 1,224 & - \\ DUTS  & 2017 & ✗ & ✗ & ✓ & ✗ & ✓ & - & 15,572 & - \\ SOC  & 2022 & ✗ & ✗ & ✓ & ✓ & ✓ & - & 6,000 & 5,776 \\  STERE  & 2012 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 1,000 & - \\ NJU2K  & 2014 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 1,985 & - \\ NLPR  & 2014 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 1,000 & - \\ DES  & 2014 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 135 & - \\ LFSD  & 2014 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 100 & - \\ SSD  & 2017 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 80 & - \\ DUT-D  & 2019 & ✓ & ✗ & ✓ & ✗ & ✗ & - & 1,200 & - \\ SIP  & 2020 & ✓ & ✗ & ✓ & ✓ & ✗ & - & 929 & 1,471 \\ ReDWeb-S  & 2021 & ✓ & ✓ & ✓ & ✗ & ✗ & - & 3,179 & - \\ COME  & 2021 & ✓ & ✗ & ✓ & ✓ & ✓ & - & 15,625 & 26,862 \\  SegV2  & 2013 & ✗ & ✓ & ✓ & ✗ & ✗ & 14 & 1,065 & - \\ FBMS  & 2014 & ✗ & ✓ & ✓ & ✗ & ✗ & 59 & 720 & - \\ MCL  & 2015 & ✗ & ✓ & ✓ & ✗ & ✗ & 9 & 463 & - \\ ViSal  & 2015 & ✗ & ✓ & ✓ & ✗ & ✗ & 17 & 193 & - \\ DAVIS  & 2016 & ✗ & ✓ & ✓ & ✗ & ✗ & 50 & 3,455 & - \\ UVSD  & 2017 & ✗ & ✓ & ✓ & ✗ & ✗ & 18 & 3,262 & - \\ VOS  & 2018 & ✗ & ✓ & ✓ & ✗ & ✗ & 200 & 7,467 & - \\ DAVSOD  & 2019 & ✗ & ✓ & ✓ & ✓ & ✗ & 226 & 23,938 & 39,498 \\ 
**Our DVISal** & - & ✓ & ✓ & ✓ & ✓ & ✓ & 237 & 7,117 & 20,226 \\  

Table 1: Overview of existing RGB SOD, RGB-D SOD, VSOD datasets, and our proposed DVISal dataset. The table shows whether each dataset provides depth maps/video sequences (Depth/Video), object-level ground truths (Obj-GT), instance-level annotations (Ins-GT), and weak supervision signals (Weak-GT) such as bounding boxes or scribbles. The table also lists the number of videos (#Video), annotations (#GT), and instances (#Ins) each dataset includes.

presents the statistics of our newly launched DViSal alongside its counterparts. In addition, we undertake extensive benchmarking experiments that verify the advantages of utilizing RGB-D videos for saliency detection.

## 3 The DVSOD Dataset

In this section, we introduce the new RGB-**D** Video **Sali**iency detection dataset, _i.e._, _DViSal_. We first describe the construction process of the DViSal dataset, and then analyze its statistical results.

### Dataset Construction

**Dataset Collection.** The main principle of data acquisition is to provide a comprehensive collection of calibrated RGB and depth video sequences, covering a broad spectrum of scenarios. Toward this objective, we initially gathered over 703 RGB-D videos, sourced from multiple repositories, including CDTB , People , PTB , Scene , DET , Tracklam , and Track3D . The collected video samples exhibit diverse locations (_e.g._, parks, campuses, streets and indoor scenes), and are recorded under a wide range of challenging conditions (_e.g._, cluttered backgrounds, low-light conditions, and reflective environments). We are committed to ensuring the quality of our dataset. To this end, we meticulously remove unqualified videos or frames - ones that are blurry, misaligned, or feature ambiguous salient objects. After this selection process, we consolidate the DViSal dataset, which consists of 237 high-quality RGB-D videos, with 175,442 paired frames in total. Some visual examples from our dataset are illustrated in Fig. 2.

**Dataset Annotation.** Diverging from most established SOD datasets that primarily offer object-level saliency masks, our proposed dataset aims to deliver more comprehensive annotations. We employ the Labelme toolkit to annotate the collated RGB-D videos. The acquisition of the ground-truth saliency adheres to the annotation principle utilized in widely-recognized SOD datasets [62; 15; 41]. Initially, five annotators select candidate salient objects based on their initial instinct. Subsequently, a majority voting strategy is deployed to finalize the salient objects. Nonetheless, annotating a large-scale DViSal dataset presents additional challenges compared to labeling an RGB image dataset. First, our DViSal dataset includes many challenging scenes recorded under diverse conditions, _e.g._, multiple objects, transparent objects, similar foreground and background, and cluttered environment. These complexities make it more difficult to identify whole objects and distinguish their silhouettes. Second, for video sequences, it becomes necessary to account for the attention shift  of salient objects within a single video. For example, an object labeled as "background" in one frame may be categorized as "salient" in another if the camera view changes. To overcome these challenges, we put into action several proactive measures. We carry out a visualization assistance process that overlays depth heatmaps onto corresponding RGB images, facilitating easier verification of salient

Figure 2: Examples of the DViSal dataset. We provide diverse annotations, including fully-supervised object-/instance-level markings, as well as weakly-supervised scribbles and bounding boxes.

objects in complex scenarios by the annotators. Moreover, for videos showcasing saliency shifting, two additional inspectors are required to review the initial annotations on an item-by-item basis, identifying any mislabeled samples and sending them back to the annotators for corrections and re-verification. Through these efforts, we successfully obtain a large-scale DViSal dataset comprising 7,117 high-quality annotations. These include object-level saliency ground truths, instance-level IDs, and weak supervision signals, such as bounding boxes and scribbles.

**Dataset Splits.** The entire dataset is partitioned into training, validation and test sets, which consist of 103, 26, and 108 videos, respectively, with 3,560, 200, and 3,357 annotated frames each. To be specific, the training set includes the entire CDTB  dataset, which contains 71 video sequences, and an additional 32 randomly selected videos from the PTB , Tracklam  and Track3D  datasets. The validation set is made up of 26 videos sourced from PTB, serving the purpose of model performance assessment during training. The remaining 108 videos, extracted from Tracklam, Track3D, DET , People , and Scene  constitute the test set, designed to evaluate the performance of the models. The videos derived from the DET, People, and Scene datasets are fully preserved within the test set, facilitating a comprehensive evaluation of various SOD models and corroborating their ability to generalize across different scenarios. As presented in Table 2, we report the benchmarking results for each subset and provide an overall measurement.

### Dataset Statistics

**Comparison with Existing Datasets.** Table 1 presents a comparative overview of popular SOD datasets and our proposed DViSal dataset. As observed, our DViSal dataset offers several notable advantages over existing datasets. First, DViSal integrates more rich data types in both RGB and depth views and video sequences. This stands in contrast to current datasets such as COME  and DAVSOD , which either provide single pairs of RGB and depth images or solely RGB video sequences as input. Second, the DViSal furnishes a more comprehensive set of annotations. Compared to typical SOD datasets that often involve only one type of object-level annotation [31; 36; 60; 62; 74], the diverse annotation sets in our DViSal dataset enable the exploration of additional domains such as instance-level or weakly-supervised DVSOD. This expanded capacity for research could foster a wider spectrum of research opportunities and catalyze the development of more sophisticated algorithms. Additionally, our dataset covers a wide range of scenarios and contains many challenging cases, such as cluttered background in Fig. 1 and multi-object scenario in Fig. 2. The diversity will be a valuable asset in assessing the scalability of saliency models.

**Statistical Analysis.** Fig. 3 provides the statistical results of our DViSal dataset from several dimensions. (1) _Size of salient objects_: The sizes of salient objects in the dataset significantly impact the performance of saliency detection, as smaller objects are generally more challenging to detect. In our analysis, we define the object size (OS) as the proportion of salient pixels to the total number of pixels in the annotated frames. The results in Fig. 3 (a) reveal that the DViSal dataset exhibits a relatively small proportion of images with large objects, and the size of salient objects varies widely

Figure 3: Statistics of the proposed DViSal dataset in terms of (a) salient object size, (b) salient object numbers at image level, as well as (c) the number of annotated frames and (d) instances at video level.

within a broad range of 0.02% to 54.55%. Moreover, the mean OS score is 14.65%, which is notably appealing compared to the score of 23.16% in current popular RGB-D datasets as reported in . (2) _Number of salient objects_: Fig. 3 (b) analyzes the distribution of the number of salient objects within the images. It is observed that images with a single salient object constitute only 8% of our dataset, whereas these with multiple salient objects (_i.e._, the number of salient objects \( 3\)) constitute a significant 45.26%, surpassing the majority of existing RGB-D SOD and VSOD datasets as reported in [29; 15]. These findings highlight not only the diversity and challenge presented in our dataset but also underscore its potential for advancing saliency detection research. (3) _Video-level statistics_: we give video-level analysis on the distribution of labeled frames and the number of salient instances in the videos, with results depicted in Figs. 3 (c) and (d), respectively. These results further showcase the versatility of the proposed DViSal dataset.

## 4 The DVSOD Benchmark

### DVSOD Baseline

To date, various network architectures [28; 83] have been developed for the tasks of RGB-D SOD and VSOD. In the former task, advanced feature fusion techniques are adopted to fuse features extracted from RGB-D images based on two-stream encoders. The latter task focuses more on exploiting temporal associations in video sequence. Based on existing techniques in RGB-D SOD  and VSOD , we assemble a straightforward DVSOD baseline, aiming at examining the advantages of utilizing depth and temporal information in improving detection accuracy & robustness.

Fig. 4 shows an overview of the proposed DVSOD baseline, which consists of four steps: 1) feature extraction; 2) fusion of RGB-D multimodal information; 3) video temporal aggregation; and 4) prediction of salient objects. Specifically, the input is a RGB-D video clip that contains a Query pair of RGB and depth images at current frame \(t\), and \(L\) Memory pairs at past frames. We first feed these image pairs into two-stream encoders to extract RGB and depth features, respectively. Then, a multimodal fusion module, _i.e._, cross reference modules (CRM) , is adopted to unify the information from two modalities at each network layer. The features across multiple layers are then decoded by the segmentation decoder , resulting in a series of fused multimodal features. We represent them as \(\{_{d}^{H W D}\}_{d U}\), where \(H W\) represents the spatial size, \(D\) is the channel dimension, and \(d\) represents the time subscript of a certain frame in the set of \(U\!=\!\{t\!-\!L,,t\!-\!1,t\}\). To incorporate the temporal cues, we exploit a basic temporal fusion module, _i.e._, the space-time memory read block , to furnish the Query features by engaging the rich features of Memory frames. Finally, we apply a 1\(\)1 convolutional layer on the memory-augmented feature to predict the saliency map, which is supervised by the ground-truth saliency map using the conventional binary cross entropy (BCE) loss. More details to DVSOD baseline are described in the supplementary materials.

**Implementation Details.** The framework is implemented with PyTorch and trained using a NVIDIA RTX A6000 GPU. We utilize the CPD decoder  in conjunction with the ResNet-50 encoder  as the base network. For the depth stream, we generate 3-channel depth maps by repeating the 1-channel depth maps. Each image is uniformly resized to \(320 320\), and we perform random horizontal flipping and cropping to avoid potential over-fitting. During model training, the learning rate is set to 1e-4, and Adam optimizer is adopted with mini-batch of 2. In the inference stage, the proposed baseline predicts saliency maps in an end-to-end manner and no post-processing procedure (_e.g._, CRF ) is applied in this work.

Figure 4: An overview of the DVSOD baseline.

### Evaluation Metrics

We adopt four widely-used metrics to evaluate the performance of saliency models, including mean absolute error (MAE or \(M\)) , F-measure (\(F_{}\)) , S-measure (\(S_{}\))  and E-measure (\(E_{}\)) . _The lower the MAE, the better. For other metrics, the higher score is better_. Concretely, F-measure is an overall performance measurement and is computed by the weighted harmonic mean of the precision and recall:

\[F_{}=) Precision Recall}{^{2} Precision +Recall},\] (1)

where \(^{2}\) is set to 0.3 as suggested in . \(M\) represents the average absolute difference between the saliency map and ground truth. It is used to calculate how similar a normalized saliency maps \([0,1]^{W H}\) is compared to the ground truth \(\{0,1\}^{W H}\):

\[M=_{x=1}^{W}_{y=1}^{H}|(x,y)-(x,y)|,\] (2)

where \(W\) and \(H\) denote the width and height of \(\), respectively. Structural measure (S-measure) evaluates the structural similarity between the predicted saliency maps and the binary ground truths. S-measure (denoted as \(S_{}\)) contains two terms, \(S_{o}\) and \(S_{r}\), referring to object-aware and region-aware structural similarities, respectively:

\[S_{}=*S_{o}+(1-)*S_{r}\] (3)

where \(\) is the balance parameter and is set to 0.5 as in . Enhanced-alignment measure (\(E_{}\)) considers the global means of the image and local pixel matching simultaneously.

\[E_{}=_{i=1}^{W}_{j=1}^{H}_{s}(i,j),\] (4)

where \(_{s}()\) is the enhanced alignment matrix, which reflects the correlation between \(\) and \(\) after subtracting their global means, respectively.

### Benchmark Results

In this section, we benchmark the DVSOD task by conducting a range of experiments using the DViSal dataset with 11 popular state-of-the-art SOD methods. These methods include four _RGB-image-based SOD models_ (PoolNet , BASNet , CPD , and F3Net ), five _RGB-D SOD models_ (DMRA , CoNet , BBSNet , RD3D  and SPNet ), two _VSOD models_ (PCSA  and UGPL ), and our _DVSOD baseline_. We obtained results from these methods on the new dataset by reproducing them with their publicly available codes and default setups.

**Quantitative Results.** In DVSOD, one important expectation compared to the RGB-based model is whether properly utilizing RGB-D and temporal features improves the per-frame segmentation accuracy. To verify it, we first conduct three comparative experiments (last three columns of Table 2) to evaluate the influence of each data type. The basic model, CPD , which is trained with just RGB images, achieves an overall Mean Absolute Error (MAE) score of 13.2%. When we incorporate a depth component (CPD + Dep), the error significantly decreases by 1.1%. As we employ multimodal RGB-D data and temporal video data jointly (CPD + DepVid), the performance further improves to 11.3%. These results consistently highlight the advantages of using 3D layout data and temporal contexts to locate salient objects. Considering that DVSOD is relatively new and its development is still at an early stage, we additionally present the segmentation results of related SOD/RGB-D SOD/VSOD models in Table 2, to provide observational reference for our readers.

**Qualitative Results.** Fig. 5 visualizes the saliency predictions from different saliency models on the DViSal dataset. We find that the model trained with both RGB-D data and video data generates saliency maps that closely match the ground truths. For example, the DVSOD baseline effectively identifies entire objects even in challenging low-light situations as displayed in Fig. 5 where other methods struggle. We attribute this success to the combined benefits of multimodal and temporal contexts. In addition, our DViSal dataset's diverse scenarios in Figs. 1-2&5 demonstrate its versatility to provide a sufficiently realistic benchmark in this field.

[MISSING_PAGE_FAIL:8]

**(1) _Accuracy_**: The research of DVSOD is still in its initial stage and DVSOD research has room for improvement, particularly regarding the accuracy of the DVSOD models. There's potential to enhance the accuracy of DVSOD models by incorporating ideas from the established fields of RGB, RGB-D, and Video SODs. For instance, we could integrate the multi-scale learning techniques  into cross-modal and cross-frame fusion to improve the model's contextual representation. Moreover, we could introduce extra edge signals  to aid the model in capturing object boundary details. In addition, it could be beneficial to explore more sophisticated fusion techniques  to promote effective interaction between multimodal and temporal information.

**(2) _Efficiency_**: Although the engagement of RGB-D videos brings significant improvement, it introduces additional model parameters. More lightweight strategies can be explored to improve efficiency. For instance, we can develop lightweight operations such as depthwise separable convolution  or neural architecture search techniques , to advance feature extractors. On the other hand, we could explore knowledge distillation schemes  to transfer depth structure knowledge to the RGB stream, thereby reducing the heavy overhead of the depth encoder. To evaluate the impact of lightweight strategy, we conduct an exploratory experiment by replacing the conventional convolution in our network encoder with the more efficient depth-wise separable convolution. Our experiments show a reduction in network parameters from 97.3M to 58.1M and memory usage from 17.15G to 16.21G, along with an increase in inference speed from 16.7 FPS to 21.5 FPS. However, this change leads to a drop in performance, with the F-measure falling from 0.610 to 0.575. These findings underscore the potential of efficiency-enhancing strategies, and also highlight the need to balance accuracy with efficiency in saliency model design.

**(3) _Temporal Modeling_**: In this work, we conduct a preliminary investigation on the benefits of RGB-D videos for saliency detection by incorporating an well-designed temporal aggregation scheme  that focuses on modeling long-term dependencies. As a potential extension, we could consider introducing optical flow to capture short-term temporal cues. However, it's important to note that directly integrating optical flow could increase training complexity and potentially introduce noise, especially in complex scenes. Meanwhile, it is worth exploring the saliency change problem (_e.g._, the salient object changing from a person to a table) when referring to temporal information of previous frames. In the DVSOD baseline, we attempt to address it through engaging the attention mechanism within , which selectively discounts less relevant information and fetches the most relevant features. We encourage further investigation on this matter, and believe there is a significant opportunity for the development of more effective temporal modeling schemes that can address these challenges.

**(4) _Dataset_**: As the first dataset proposed for DVSOD, there is still considerable room for improving the various aspects of the dataset such as quality, quantity, and diversity by involving a broader range of scenarios of extreme lighting conditions, as well as various types of occlusions. Furthermore, annotating such large-scale RGB-D video dataset at pixel-level is very costly in terms of both time and effort. We may explore the recent advances of photorealistic rendering techniques  to simulate data or supplement annotations to help address challenges within the dataset.

**(5) _Evaluation Metrics_**: Evaluation metrics are crucial for model training, testing, and benchmarking. However, the saliency detection community largely relies on classic metrics such as MAE and F-measure, which are not designed to assess sequential DVSOD tasks specifically. Meanwhile, the Temporal Coherence (TC) metric, frequently used in evaluating RGB video, may not accurately

Figure 5: Visual results of different SOD models. The ‘\(^{+}\)’ stands for the DVSOD baseline in Sec. 4.1 that integrates both multimodal RGB-D cues and temporal contexts.

depict DVSOD model performance due to the complex scenes within the DViSal benchmark, such as low light or cluttered backgrounds. Thus, how to design appropriate metrics specifically tailored for DVSOD remains an open challenge.

**(6)**_Instance-level Extension_**: As revealed in , instance-level extension is capable of recognizing individual instances of salient objects. It is more challenging as it requires detailed parsing within the detected salient regions, and holds significant importance for practical applications. Hence exploring the research on RGB-D video salient instance detection emerges as a promising future direction.

**(7)**_Weakly-supervised Learning_**: The acquirement of precise per-pixel labels is laborious and time-consuming. As an alternative, training DVSOD models with weak supervisions  (_e.g._, bounding box, scribble) is an appealing research topic, which can avoid heavy annotation costs.

## 6 Conclusion

In this work, we present the DViSal dataset to spur research in RGB-D video salient object detection (DVSOD), a field yet to be well explored. This dataset, composed of 237 diverse RGB-D videos and comprehensive annotations, will significantly expand research avenues. Empirical benchmarking experiments demonstrate the effectiveness of multimodal video input in enhancing salient object detection. We highlight some promising future research directions and make the dataset and results publicly available to accelerate progress in this field.

**Acknowledgements.** This work was done jointly by UA and SRA. This research was supported by the CFI-JELF, Mitacs, University of Alberta Start-up grant, NSERC Discovery (RGPIN-2019-04575) grants.