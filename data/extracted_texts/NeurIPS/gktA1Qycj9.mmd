# CigTime: Corrective Instruction Generation Through Inverse Motion Editing

Qihang Fang

The University of Hong Kong

Chengcheng Tang

Meta Reality Labs

Bugra Tekin

Meta Reality Labs

Yanchao Yang

The University of Hong Kong

###### Abstract

Recent advancements in models linking natural language with human motions have shown significant promise in motion generation and editing based on instructional text. Motivated by applications in sports coaching and motor skill learning, we investigate the inverse problem: generating corrective instructional text, leveraging motion editing and generation models. We introduce a novel approach that, given a user's current motion (source) and the desired motion (target), generates text instructions to guide the user towards achieving the target motion. We leverage large language models to generate corrective texts and utilize existing motion generation and editing frameworks to compile datasets of triplets (source motion, target motion, and corrective text). Using this data, we propose a new motion-language model for generating corrective instructions. We present both qualitative and quantitative results across a diverse range of applications that largely improve upon baselines. Our approach demonstrates its effectiveness in instructional scenarios, offering text-based guidance to correct and enhance user performance.

## 1 Introduction

Corrective instructions are crucial for learning motor skills, such as sports. Without feedback, people are at risk of developing improper, suboptimal, and injury-prone moves that hinder long-term progress and health. With the growing popularity and immersion of motion-sensing sports games, the increasing accuracy and accessibility of 3D pose estimation techniques, and the advancement of fitness equipment and trackers with versatile sensing technologies, the need for intelligent coaching systems that provide corrective feedback on user motion is becoming increasingly important.

In this work, we study the task of _Motion Corrective Instruction Generation_, which aims to create text-based guidance to help users correct and improve their physical movements. This task has significant applications in sports coaching, rehabilitation, and general motor skill learning, providing users with precise and actionable instructions to enhance their performance. By leveraging advancements in human motion generation and editing, this task addresses the need for personalized and adaptive feedback in various instructional scenarios.

Recent research in text-conditioned human motion generation has shown impressive progress. Methods like MotionCLIP  and TEMOS  have utilized neural networks and transformer-based models to align text and motion into a joint embedding space, producing diverse and high-quality motion sequences. These models, however, focus primarily on generating motions from text rather than generating corrective instructions from motion pairs. Therefore they are not directly suitable for analyzing and improving user movements based on a comparison of motion sequences.

Research specifically focused on corrective instruction generation is still in its early stages. Traditional methods often rely on building statistical models for specific action categories, which require expert experience and are difficult to scale and generalize to various actions. For example, Pose Trainer  and AIFit  employ neural networks and statistical models to provide feedback on specific exercises, but these methods have significant drawbacks: (1) They often require large amounts of annotated data for each specific action class, making them hard to generalize across different types of motions. However, unlike text-to-motion or human pose correction (which can be annotated through simple pipelines ), human motion sequences involve temporal changes. Annotating the differences between these temporal changes is challenging. (2) Many of these methods are limited to analyzing static poses or images rather than dynamic sequences of motion, reducing their applicability to real-world scenarios where movement dynamics are crucial.

LLMs, such as Llama , have shown potential in generating corrective instructions using few-shot or zero-shot learning. However, without proper fine-tuning and additional modalities, LLMs struggle to understand the spatial and temporal context of poses and motions, limiting their effectiveness in specialized fields like coaching or corrective instruction generation.

To address these limitations, we propose a novel approach, _CigTime_, for generating motion corrective instructions. Our method leverages existing motion editing pipelines to create datasets of motion triplets (source, target, and instruction). The key components of our approach include: **Motion-Editing-Based Data Collection**: We develop a pipeline that uses motion editing techniques to generate large datasets of motion pairs and corresponding corrective instructions. This process involves using a pre-trained motion editor to modify source motions according to generated instructions, resulting in target motions that reflect the desired corrections. **Fine-Tuning Large Language Models**: We fine-tune a large language model (LLM) on the generated datasets to enable it to produce precise and actionable corrective instructions. By training the LLM on a diverse set of motion sequences and corrections, we enhance its ability to understand and generate contextually relevant feedback.

In summary, our contributions include:

* We introduce a motion-editing-based pipeline to efficiently generate large datasets of corrective instructions, reducing the dependency on extensive manual annotations.
* We propose a general motion corrective instruction generation method which utilizes a large language model to translate motion discrepancies into precise and actionable instructional text, addressing the relationship between language and dynamic motions.
* Through comprehensive evaluations, we show that our method significantly outperforms existing models in generating high-quality corrective instructions, providing better guidance for users in various real-world scenarios.

## 2 Related work

### Text Conditioned Human Motion Generation.

Conditional motion generation aims to synthesize diverse and realistic motion conditioning on different control signals, such as music [25; 26; 27; 28], action categories [2; 15; 21], physical signals [10; 16; 33]. Recent years have seen significant progress in text conditioned human motion generation [1; 2; 12; 13; 14; 20; 24; 34; 39; 40; 44; 45; 47]. Some methods [1; 12; 39] align the texts and motions into a joint embedding space for generation. Benefiting by aligning motion latent to the CLIP  embedding space, MotionCLIP  could generate out-of-distribution motions. Several works utilize other mechanisms to increase the diversity and quality of generated motions. TEMOS  and TEACH  employ transformer-based VAEs to generate motion sequences based on texts. Guo _et al._ propose an auto-regressive conditional VAE to generate human motion sequences. Inspired by the achievements in image generation, the diffusion models, such as MotionDiffuse , MDM  and FLAME , have also been applied to motion generation. Some follow-up works [37; 43] attempt to improve the controllability of the diffusion model. Recently, the Vector Quantized Variational Autoencoder (VQ-VAE) has gained significant traction in being used to convert 3D human motion into motion tokens which are subsequently employed alongside language models. TM2T  proposes using these quantized tokens to facilitate the mapping between text and motion. T2M-GPT  employs an auto-regressive method to predict the next-index token. Further, MotionGPT [20; 47] utilizes large language models (LLMs) to simultaneously handle different motion-relatedtasks. Recently, AvatarGPT  extends the generation models to unify high-level and low-level motion-related tasks, which supports human motions generation, prediction and understanding.

### Motion Editing

Motion editing enables users to interactively refine generated motions to suit their expectations. PoseFix  utilize neural networks to edit 3D poses. Holden _et al._ employs an autoencoder to optimize the trajectory constraints. MDM , MotionDiffuse  and FLAME  involve processing by masks that designate parts for editing through reverse diffusion. GMD  and PriorMDM  are designed to edit motion sequences conditioned on joint trajectories. OmniControl  incorporates control signals that encourage motions to conform to the spatial constraints while being realistic. Recently, FineMoGen  tackles fine-grained motion editing which allows for editing the motion of individual body parts, however its heavy reliance on specific-fine grained format limits the smooth coordination among movements of different body parts.

### Corrective Instruction Generation

Traditional methods [6; 11] focus on specific action categories by building statistical models that require expert experience. These methods struggle to scale and generalize to various actions. Pose Tutor  uses neural networks to learn statistical models but requires large amounts of data for each action and can only analyze static images or poses. FixMyPose  creates a dataset with human-annotated corrective instructions on synthetic 2D images. PoseFix  designs an automatic annotation system and a conditioned auto-regressive model for corrective instruction generation, but it is limited to static poses. Recently, Large Language Models (LLMs)  have made significant advances in text generation. With appropriate prompting, LLMs can generate pose corrective instructions with few-shot or zero-shot example data. However, LLMs' access to text makes them less aware of a variety of possible motions that people could perform and links them with languages.

Our key insight for corrective instruction generation is to regard this task as a close yet inverse problem to text-conditioned motion generation and editing, allowing us to bring the progress in that fast-growing space to this understudied problem: We first propose a novel corrective instruction data collection pipeline based on motion editing. Subsequently, we design a model that leverage large language models to provide corrective instructions on spatial form and temporal dynamics.

## 3 Method

### Overview

We present an overview of our approach in Fig. 1. Given a source motion sequence, \(x^{I}^{T D}\), where \(T\) is the number of frames and \(D\) is the dimensionality of the motion representation, and a target motion sequence, \(x^{O}^{T D}\), as input, our goal is to learn a function \(\) which maps \(x^{I}\) and \(x^{O}\) to the corrective text instruction \(L\), i.e., \((x^{I},x^{O})=L\).

To achieve this, we employ a pre-trained motion editor, which takes as input the source motion sequence and ground-truth corrective text, to output target motion sequences. Next, we quantize the source and target motion sequences into discrete tokens using a VQ-VAE-based network. Finally, we organize these tokens with a predefined template to fine-tune an LLM on the triplets that contain source motion sequence \(x^{I}\), target motion sequence \(X^{O}\), and corrective instruction \(L\) for generating instructions that can efficiently modify the source to the target motion sequence.

### Motion-Editing-Based Data Collection

The task of generating corrective instructions requires triplet data consisting of the source motion, the target motion, and the corrective instruction. Collecting such a dataset through human annotation is costly and inefficient. We aim to leverage existing pre-trained models to streamline the data collection process. However, there isn't an existing model that generates such triplets.

Our fundamental insight is to treat corrective instruction generation as an inverse process of motion editing, which uses a given text to guide an agent in editing its initial motion. We utilize the motion editing process to gather required triplets: we collect a set of source motions and employ a pre-trainedmotion editor to edit the source motion based on a corrective instruction, resulting in the target motion.

Motion EditingIn this work, we utilize the motion diffusion model (MDM)  as the motion editor. Given the input motion sequence, \(x\), and generation condition, \(c\), MDM uses probabilistic diffusion models for motion generation. It comprises a forward process, which is a Markov chain involving the sequential addition of Gaussian noise to the data, and a reverse process that progressively denoises the data to get the edited motion. The forward process of MDM is formulated as,

\[q(x_{1:T}|x_{0})=_{t 1}q(x_{t}|x_{t-1}),\] (1)

\[q(x_{t}|x_{t-1})=(}x_{t-1},(1-_{t})),\] (2)

where \(_{t}(0,1)\) are constant hyper-parameters. Further, the reverse process is formulated as,

\[p_{}(x_{0:T})=p(x_{T})_{t 1}p_{}(x_{t-1}|x_{t}),\] (3)

\[p_{}(x_{t-1}|x_{t})=(x_{t-1};_{}(x_{t},t),_{t }^{2}),\] (4)

where \(\) is the learnable parameters of the diffusion model, which gradually anneals the noise from a Gaussian distribution to the data distribution. We train MDM as a conditional generator \((x_{t},c,t)\) that outputs \(x_{0}\), where \(c\) is the text condition, to maximize \(p_{}(x_{0:T})\).

In inference, MDM takes noise \(n\) as \(x_{T}\) and applies the reverse process to denoise the input based on the text condition, \(c\), generating the motion sequences, \(x_{0}\), corresponding to \(c\). For the motion editing task, we utilize the corrective instruction, \(L\), as the generation condition, \(c\), to generate the corresponding corrective motion sequence, \(x^{L}\). We then calculate the target motion sequence, \(x^{O}\), by combining the source motion sequence, \(x^{I}\), and the corrective motion sequence, \(x^{L}\),

\[x^{O}=m x^{L}+(1-m) x^{I},\] (5)

where \(m\) is the joint mask for the body part \(P\), and \(\) is the element-wise multiplication for masking operation. Through the above process, we are able to collect a large amount of \( x^{I},x^{O},L\) triplets. We use this dataset to fine-tune a large language model (LLM) for the corrective instruction generation task as in the following.

Figure 1: **Overview of CigTime. Left: We leverage source motion tokens and corrective instructions as input to a motion editor to produce target motion tokens. Right: We then employ a language model to generate precise corrective instructions based on a given source and target motion. We demonstrate in the example generating corrective instructions for lifting weights with the upper body.**

### Fine-tuning LLMs for Corrective Instruction Generation

With the prepared dataset of triplets from the motion editing process, we learn the inverse process of motion editing, a function, \(\), that maps source and target motion sequence pairs to corrective instructions. We first learn an encoder based on VQ-VAE  to tokenize the motion sequences into discrete tokens and organize the discrete tokens based on a pre-defined template. Then, we fine-tuned an LLM to generate the corrective instruction, \(L\), based on the tokens of the source and target motion sequence, \(x^{I}\) and \(x^{O}\).

Tokenizer Pre-trainingCompared to directly feeding the original data to the LLMs, the discrete representation has been proven to be more suitable for fine-tuning LLMs with human-motion-related tasks . Inspired by these works, we initialize a VQ-VAE-based network, which contains an encoder \(\), a codebook \(C\), and a decoder \(\). The encoder \(\) takes motion sequence, \(x\), as input and maps, \(x\), into discrete features, \(f\), where \(H\) is the dimensionality of the frame feature.

The codebook, \(C^{K H}\), represents different codes, where \(K\) is a predefined number of different discrete codes and \(c_{k}^{H}\) is the k-th code. VQ-VAE quantizes the discontinuous feature \(f\) to the discrete latent codes, \(z^{T H}\), through codebook, \(c\), by projecting each per-frame feature \(f_{i}\) to its nearest code:

\[z_{i}=Q(f_{i})=c_{k},k=_{j} \|f_{i}-c_{j}\|_{2}^{2},\] (6)

where \(Q\) represents the quantization operation. The decoder \(\) takes the code, \(z\), as input, and reconstructs the motion sequence, \(x^{O^{}}\). We use the index \(k\) as the token of each discrete code, \(c_{k}\), as the token representation of the frame feature \(f_{i}\). We apply the L2 loss for the training of the tokenizer,

\[^{recon}=||x^{O}-x^{O^{}}||_{2}^{2}.\] (7)

Considering that the quantization operation disrupts gradient backpropagation, we employ an exponential moving average (EMA)  for the codebook update and stabilize the training process. Besides, we apply the commitment loss  to update the tokenizer encoder,

\[^{com}=_{i=1}^{T}\|f_{i}-sg(z_{i})\|_{2}^{2},\] (8)

where \(sg()\) is the stop gradient operation that helps stabilize the training process.

Fine-tuning LLMInstruction Tuning is a widely used technique to enable LLMs to handle specific tasks. In this work, we employ this technique to fine-tune our LLM. Specifically, given an LLM, \(\), a source discrete token set, \(I^{s}=I_{0}^{s},I_{1}^{s},...,I_{n^{s}}^{s}\), and a target discrete token set, \(I^{t}=I_{0}^{t},I_{1}^{t},...,I_{n^{t}}^{t}\), we

Figure 2: **Template for LLM fine-tuning. The LLM is required to output the corrective instructions given token lists for the source and target motion sequences (i.e., Action 1 and Action 2) as well as instructions on the expected output.**organize the input of \(\) to follow the template as shown in Fig. 2. This input is then tokenized into text tokens \(U^{I}=u_{0}^{I},u_{1}^{I},...,u_{n^{U}}^{I}\). Additionally, we tokenize the ground-truth corrective instruction, \(L\), into text tokens, \(U^{O}=u_{0}^{O},u_{1}^{O},...,u_{n^{U^{O}}}^{O}\).

The LLM processes the input tokens, \(U^{I}\), and auto-regressively predicts the probability distribution of the next tokens \(p_{}(u|U^{I})=_{j}p_{}(u_{j}^{O}|u_{0:j-1}^{O},U^{I})\). During training, we maximize the log-likelihood of the data distribution by applying cross-entropy loss:

\[^{LLM}=-_{j=0}^{U^{O}} p_{}(u_{j}^{O}|u_{0:j-1}^ {O},U^{I}).\] (9)

By using a structured input template and optimizing the cross-entropy loss, we enable the LLM to generate accurate and contextually relevant corrective instructions. This approach ensures that the model effectively learns to convert discrepancies between the source and target motions into precise and actionable instructional text.

Learning Representation for Motion TokensPrevious methods for training text-to-motion models involve either using an existing vocabulary for motion tokens  or assigning new learnable embeddings [20; 48], followed by fine-tuning with techniques like LoRA. We tried both approaches, but the results of utilizing one of them alone were not satisfying. There are two main reasons: First, using a fixed vocabulary and embeddings prevents capturing the correlation of motion differences and corrective instructions, as the weights are trained on tasks with a large domain gap. Second, while new embeddings can be learned with LoRA, the distribution of the original vocabulary's embeddings imposes constraints, making the learned embeddings suboptimal, especially given the smaller scale of training data for corrective instructions.

To address these challenges, we integrate the goods of both. We use existing vocabulary tokens for their rich semantics and fine-tune all embeddings to maximize performance and reduce the domain gap. We also introduce an anchor loss to prevent the embeddings from diverging:

\[^{Anchor}=\|W-W_{0}\|_{2}^{2},\] (10)

where \(\) is a regularization coefficient that controls the influence of loss, \(W_{0}\) represents the network weights before training, \(W\) represents the network weights after training.

## 4 Evaluation

### Experiment Setup

**Datasets** We obtain the source motion sequences from HumanML3D , a dataset containing 3D human motions and associated language descriptions. We make use of the entire dataset for the collection of source motions. We then generate triplets based on pre-trained motion editor with instructions and target motions. We split HumanML3D following the original setting and for each motion sequence in HumanML3D, we randomly select one instruction from the corresponding split for editing the sequence. We subsequently edit the source motion sequences with MDM  conditioned on the corrective instructions to obtain the target sequences.

**Implementation Details** We fine-tune a pre-trained Llama-3-8B  using full-parameter fine-tuning for corrective instruction generation. The model is optimized using the Adam optimizer with an initial learning rate of \(10^{-5}\). We use a batch size of 512 and train on four NVIDIA Tesla A100 GPUs for eight epochs, which takes approximately 5 hours to complete. Following HumanML3D , the dimensionality, \(D\), of the motion sequences is set to \(263\) for our experiments.

**Evaluation Metrics** We evaluate the generated corrective instruction with two types of metrics.

(1) Corrective instruction quality: BLEU , ROUGE , and METEOR  are commonly employed metrics that assess various n-gram overlaps between the ground-truth text and the generated text. Although these metrics focus on structural text similarity, they tend to disregard semantic meaning. Consequently, we also utilize the cosine similarity of text CLIP embeddings as an evaluation metric to better compare semantic similarity.

(2) Reconstruction accuracy: To evaluate the quality, we use the generated corrective instruction as an editing condition to modify the source motion sequences and obtain the generated target motion. We then compare this with the ground-truth target motion. Specifically, we employ Mean Per Joint Position Error (MPJPE) to measure the average Euclidean distance between the generated and ground-truth 3D joint positions for all joints. Additionally, we calculate the Frechet Inception Distance (FID) using a feature extractor  to evaluate the distance between the feature distributions of the generated and ground-truth target motions. Ideally, the generated motion sequences should closely resemble the target motion sequences.

**Comparison Baselines** To the best of our knowledge, we are the first to generate corrective instruction for general motion pairs. Thus, we adopt two different kinds of methods designed for general text-based tasks and motion captioning.

(1) Llama3 , Owen  and Mistral  are all large language models designed for general text-based tasks. They can be applied to unseen tasks with just a few-shot data. We utilize the in-context learning technique  to generate correction instructions by giving them examples of the source-target-instruction triplets. We present the detailed prompts in the supplemental material. In addition to the baselines that use in-context learning with LLMs, we ablate different fine-tuning techniques. To do so, we compare our approach, which uses full-parameter LLM tuning to a variant, which utilizes the LoRA adapter  to fine-tune the Llama 3 8B and Mistral 7B models.

(2) MotionGPT . Although MotionGPT isn't trained with corrective instruction data, it has been proven to have the ability to generalize across different motion-based tasks by utilizing specific input templates for different tasks. Thus, we adopt this method for corrective instruction generation by utilizing the template mentioned in Section. 3.3. In addition, as generating corrective instructions is not a target for MotionGPT, we create yet another baseline called MotionGPT-M2T that employs MotionGPT to generate captions corresponding for the target motions.

### Quantitative Results

Our quantitative results are presented in Table. 1. We further discuss below the quality of the corrective instructions and the reconstruction accuracy of target motion after editing.

**Corrective Instruction Quality** Our method demonstrated superior performance across most metrics when compared to baseline methods, as presented in Table. 1. Specifically, our method achieved the highest BLEU-4, ROUGE-2 and METERO scores of 0.24, 0.35, and 0.52, significantly surpassing the baseline methods. This indicates that our method generates text with higher precision.

Furthermore, our method achieved the highest CLIP Score of 0.82, outperforming other baselines. The CLIP Score indicates the semantic alignment of the generated text with visual content, and a higher score demonstrates better performance in maintaining this alignment.

We find that the two baselines adopted from MotionGPT both present inferior performances, which can be attributed to its training on a text-motion dataset, which lacks the capability to compare

    &  &  \\   & **BLEU**\(\) & **ROUGE\(\)** & **METERO**\(\) & **CLIPScore**\(\) & **MPJPE**\(\) & **FID**\(\) \\  Ground-Truth & 1.00 & 1.00 & 1.00 & 1.00 & 0.00 & 0.00 \\  Llama-3-8B & 0.15 & 0.29 & 0.45 & 0.77 & 0.21 & 3.04 \\ Llama-3-8B-LoRA & 0.10 & 0.19 & 0.36 & 0.77 & 0.24 & 2.09 \\ Mistral-7B & 0.16 & 0.30 & 0.46 & 0.80 & 0.22 & 5.03 \\ Mistral-7B-LoRA & 0.08 & 0.19 & 0.27 & 0.79 & 0.75 & 1.84 \\ MotionGPT & 0.02 & 0.10 & 0.11 & 0.76 & 0.80 & 8.84 \\ MotionGPT-M2T & 0.02 & 0.13 & 0.12 & 0.76 & 1.05 & 7.96 \\  Ours & **0.24** & **0.35** & **0.52** & **0.82** & **0.13** & **1.44** \\   

Table 1: **Comparison to the Existing Work.** We compare our approach against large language (Llama-3-8B, Llama-3-8B-LoRA, Qwen-7B, Mistral-7B) and motion-language (MotionGPT, MotionGPT-M2T) models. We demonstrate that our approach, _CigTime_ outperforms all the baselines by a large margin for corrective instruction generation for human motion.

two motion sequences and identify specific differences. Besides, although MotionGPT excels at generating captions for motion sequences, it's still difficult to reconstruct the original target motion sequence from the generated descriptions. This is because describing the differences and similarities between two motion sequences can help us accurately depict the target motion with fewer statements, which MotionGPT does not possess.

This evidenced that simply fine-tuning Llama-3 using the generated data would not result in a satisfactory corrective instruction generation, e.g., due to overfitting or catastrophic forgetting. Although the outputs can induce similar target motion sequences compared to the ground truth, the increased variance in the text can lead to a decrease in the overall NLP metrics such as BLEU, ROUGE, and METERO.

Overall, these results highlight the effectiveness of our method in generating high-quality corrective instructions, with significant improvements in precision, similarity, and visual-semantic consistency over the baseline methods.

Reconstruction AccuracyThe evaluation of reconstruction accuracy highlights the superior performance of our method in distinguishing between source and target motions. As shown in Table 1, our method achieved the lowest MPJPE of 0.1330, indicating the highest accuracy in pose reconstruction. Furthermore, our method also attained the lowest FID - Target score of 1.4442, demonstrating its effectiveness in generating data that closely matches the target motion. Similarly, MotionGPT's inferior performance in these metrics is a result of its limited ability to analyze differences between motion pairs, as evidenced by its MPJPE of 0.8011 and FID score of 8.8350.

Additionally, although LLM models like Llama-3-8B can maintain text consistency via in-context learning, they are unable to grasp the intricate connections between motion sequences and language, leading to inferior overall performance compared to our approach. Even when benefiting from fine-tuning through LoRA, these models still cannot generate high-quality corrective instructions.

Overall, these results underline the effectiveness of our method in accurately distinguishing and reconstructing the differences between source and target motions, outperforming the baseline methods in both MPJPE and FID metrics.

Ablation study with different network structureTo validate that our token embedding training method is superior to the extended token embedding approach used in previous algorithms, we

    &  &  \\   & **BLEU**\(\) & **ROUGE**\(\) & **METERO**\(\) & **CLIPScore**\(\) & **MPJPE**\(\) & **FID**\(\) \\  Llama-3-8B-Extended & 0.12 & 0.23 & 0.44 & 0.80 & 0.27 & 5.43 \\ Mistral-7B-Extended & 0.18 & 0.27 & 0.42 & 0.81 & 0.19 & 1.45 \\ Ours-Extended & **0.24** & **0.37** & **0.55** & **0.84** & 0.16 & 1.50 \\  Ours-Continuous & 0.12 & 0.24 & 0.47 & 0.78 & 0.20 & 2.56 \\ Ours-T5 & 0.14 & 0.25 & 0.46 & 0.80 & 0.33 & 5.03 \\ Ours & **0.24** & 0.35 & 0.52 & 0.82 & **0.13** & **1.44** \\   

Table 2: **Ablation study with different network structure**. We extend the LLMs’ vocabularies with new learnable embeddings for the motion tokens and update the corresponding embeddings during fine-tuning as baselines. We also compare variants that utilizes T5 as the backbone (ours-T5), and continous representaion (Ours-Continuous).

    &  &  &  \\   & **MPJPE**\(\) & **FID**\(\) & **MPJPE**\(\) & **FID**\(\) & **MPJPE**\(\) & **FID**\(\) \\  Ground-Truth & 0.00 & 0.00 & 0.22 & 2.97 & 0.25 & 5.22 \\  Llama-3-8B-LoRA & 0.24 & 2.09 & 0.27 & 3.08 & 0.37 & 7.08 \\ MotionGPT & 0.80 & 8.84 & 0.80 & 9.80 & 0.77 & 19.07 \\ MotionGPT-M2T & 1.05 & 7.96 & 0.80 & 8.48 & 0.74 & 28.95 \\  Ours & **0.13** & **1.44** & **0.22** & **3.02** & **0.26** & **5.34** \\   

Table 3: **Ablation study with different motion editors.** We assess the reconstruction accuracy of various methods employing different motion editors for evaluation.

conducted a comparison of LLMs trained using token embeddings, as shown in Table 2. Although fine-tuning with extended vocabulary can enhance the text-based metrics, these instructions cause a decline in the motion editing performance, resulting in a reduction in MPJPE and FID. From the perspective of the task definition, we require a model that prioritizes high reconstruction quality over instruction quality. Therefore, extending vocabulary is more detrimental than beneficial for our task.

Besides we fine-tune T5-770M , as in Motion-GPT  and AvatarGPT  to validate the impact of different LLM frameworks on the results. The experimental results show that the T5 framework does not offer an advantage over larger language models  in the Motion Corrective Instruction Generation task. We also compared our method with its variant based on continuous representations, as implemented by MotionLim . As observed, our method still outperforms the continuous baseline across all the reconstruction accuracy metrics.

Evaluation with Different Motion EditorsDifferent people may perform various actions in response to the same instruction. Our goal is for our model to produce instructions that are as accurate and widely accepted as possible. Therefore, we evaluate our methods and baselines using different motion editors. In addition to MDM, which we used to generate the ground-truth dataset, we also assess the methods with two different versions of PriorMDM as shown in Table 3.

Our proposed method consistently outperforms other models across different motion editors, demonstrating the lowest MPJPE and FID values, close to the ground truth. This highlights its effectiveness in generating accurate and visually similar corrective motions. In contrast, models like MotionGPT and its variant exhibit significantly higher errors, indicating limitations in their generation capabilities.

### Visual Results

To further analyze the performance of different methods, we present visual comparisons in 3. As shown in the results, our algorithm largely maintains similar semantics and achieves reconstruction results that are closely aligned with the ground truth. This demonstrates the accuracy of our algorithm in generating corrective instructions. In contrast, Llama3-8B, despite achieving favorable numerical results, may incorrectly identify the joint parts involved in motion editing. This highlights our approach's superiority in providing accurate and contextually appropriate motion corrections.

## 5 Conclusion

We introduced a new task and a framework for generating corrective instructions that translate a source motion into a target motion. Our key insight is to leverage the fast growing field of text-conditioned

Figure 3: Visualization of corrective instructions and reconstructed motions for different methods.

motion-editing for this related yet understudied inverse problem. To create a dataset for this task, we proposed a motion editing pipeline that minimizes the need for extensive manual annotations. We demonstrated the utility of our approach which largely outperforms existing related models.

While our work provides a strong foundation for corrective instruction generation in human motion, there are limitations to our framework.

1. First, the curated dataset captures differences between source and target motions, but it lacks targeted feedback on form and dynamics that are specific to actions and sports, which require more detailed and subtle instructions for learning particular skills.
2. Second, due to the limitation of the pretrained motion editor , we can only handle source and target motion pairs with the same sequence length, without context or scenes.
3. Third, the corrective instruction generation method may be misused to generate instructions for insulting or inappropriate motions.

We aim to address these limitations in future research, along with further advances of text-conditioned motion-editing frameworks, which share our challenges, limitations, and potential solutions.