# E.T. Bench: Towards Open-Ended

Event-Level Video-Language Understanding

 Ye Liu\({}^{1,2}\), Zongyang Ma\({}^{2,3}\), Zhongang Qi\({}^{4}\)\({}^{*}\), Yang Wu\({}^{5}\), Ying Shan\({}^{2}\), Chang Wen Chen\({}^{1}\)

\({}^{1}\) The Hong Kong Polytechnic University \({}^{2}\) ARC Lab, Tencent PCG

\({}^{3}\) Chinese Academy of Sciences \({}^{4}\)Huawei Noah's Ark Lab \({}^{5}\) Tencent AI Lab

 coco.ye.liu@connect.polyu.hk

https://polyu-chenlab.github.io/etbench/

Work done at ARC Lab, Tencent PCGCorresponding authors

###### Abstract

Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce **E.T. Bench** (**E**vent-Level & **T**ime-Sensitive Video Understanding **B**enchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, _e.g._, grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, **E.T. Chat**, together with an instruction-tuning dataset **E.T. Instruct 164K** tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.

## 1 Introduction

The recent advent of Multi-modal Large Language Models (MLLMs)  has led to a substantial paradigm shift in visual-language understanding, moving away from designing task-specific models and collecting domain-specific data, towards developing general-purpose task-solvers for open-ended scenarios. By integrating LLMs with visual encoders, these models jointly benefit from perception abilities and powerful reasoning skills, showing remarkable performance in even unseened applications, demonstrating great potential for such scheme.

To effectively evaluate the capabilities of these models, a number of benchmarks  have been introduced to study their feasibilities in different scenarios. Yet, most of the benchmarks are focusing on image or short (seconds-long) video understanding, which require strong static scene understanding abilities but overlooking the fine-grained temporal information. Some recent works  tend to evaluate MLLMs on longer videos, but they still leverage multiple-choice question-answering (MCQ) as their main task, lacking flexibilities for open-ended tasks. Nevertheless, none of the existing benchmarks are designed for multi-event or time-sensitive scenarios, thus theysuffer from severe single-frame biases, as can be seen in the comparable performances between Image- and Video-LLMs in these benchmarks .

To address these issues and better understand the open-ended capabilities of these models, we propose **E.T. Bench**, a comprehensive benchmark for event-level and time-sensitive video understanding. As shown in Figure 1 and compared in Table 1, our benchmark significantly diverse from previous ones that it focus on time-sensitive understanding on long and multi-event videos. Our motivation is that a well-performing Video-LLM should possess the capability to precisely refer to and localize any events that align with user interests. Based on this assumption, we build our task taxonomy by summarizing four essential capabilities required for time-sensitive video understanding: _referring_, _grounding_, _dense captioning_, and _complex understanding_. Then, each capability is delineated with carefully designed tasks. The diversity of scenarios is ensured by meticulously collecting videos from 15 datasets covering 8 domains. A comprehensive data cleaning, annotation repurposing, instruction design, manual verification, and sampling pipeline is leveraged to generate 7.8K high-quality annotations.

We extensively evaluate 20 models, including 7 open-source Image-LLMs, 9 open-source Video-LLMs, and 4 commercial MLLMs, on E.T. Bench. The results reveal that state-of-the-art models on existing VideoQA benchmarks  struggle on our E.T. Bench, especially on _grounding_, _dense captioning_, and _complex understanding_ tasks. We attribute it to two key limitations of existing development pipelines for MLLMs. First, the discrete next-token prediction paradigm has natural drawbacks in numerical calculations , limiting timestamp understanding and generation. Second, most existing video instruction-tuning datasets  predominantly comprise short videos with coarse-level annotations, bringing significant gap between training and real-world applications. To tackle these problems, we propose **E.T. Chat**, a novel time-sensitive Video-LLM that reformulates timestamp prediction as an embedding matching problem, serving as a

Figure 1: **Task definitions in E.T. Bench.** The 12 tasks derives from 4 essential capabilities for time-sensitive video understanding: _referring_, _grounding_, _dense captioning_, and _complex understanding_.

strong baseline on E.T. Bench. As for data, we also curate **E.T. Instruct 164K**, an instruction-tuning dataset tailored for multi-event and time-sensitive scenarios. Extensive comparisons demonstrate the effectiveness of the proposed model and dataset. We hope that the proposed benchmark, model, and dataset can inspire future research on Video-LLMs.

## 2 Event-Level & Time-Sensitive Video Understanding Benchmark

In this section, we illustrate the detailed pipeline employed to develop our E.T. Bench. As shown in Figure 2 (right), the pipeline begins with the definition of four essential capabilities for event-level and time-sensitive video understanding, _i.e._, _referring_, _grounding_, _dense captioning_, and _complex understanding_, arranged in increasing order of difficulty. For each capability, we design a series of tasks specifically for effective assessment of the respective capability. For each task, we meticulously select existing datasets with timestamp annotations provided by human annotators, and rewrite them into instruction-following formats according to task formulation, ensuring high quality and verisimilitude. The diversity of E.T. Bench is guaranteed by carefully choosing variable-length videos from different domains. Finally, a thorough manual check, filtering, and sampling process is conducted to eliminate unsatisfactory samples. Details for each step are introduced as follows.

### Hierarchical Task Taxonomy

To evaluate the open-ended Video-LLMs from various perspectives, we design a three-level task taxonomy depicted in Figure 2 (left). Definitions for capabilities and tasks are as follows.

**Referring** means the ability to comprehend time information from user inputs. For example, given a question _"What is the person doing from 23s to 35s?"_, the model has to understand which part of the video is the user referring to, and provide response with more consideration on that segment. For better quantifying the model performances, we formulate all the _referring_ tasks as multiple-choice question-answerings (MCQs), including 1) [RAR]_Referred Action Recognition_: Identify the action given a coarse timestamp hint (_e.g._, _"around 12s"_). The model has to determine the actual reference according to both the video and options. 2) [ECA]_Event-Caption Alignment_: Select the correct temporal boundary for a given caption. The model need to understand and distinguish multiple timestamps in the options. 3) [RVQ]_Referred Video Question-Answering_: Answer the question conditioning on a given segment. Each question is supplied with four candidates and an "unable to answer" option, denoting the case when it cannot be answered with given the segment.

**Grounding** indicates the ability to localize event- or moment-of-interests with accurate timestamps. It diverse from previous works [48; 101] that only consider coarse-level grounding, _e.g._, _"at the beginning/middle/end of the video"_. Outputs for _grounding_ tasks are open-ended, processed by rule-based parsers and evaluated with continuous metrics. The definitions of tasks are 1) [TVG]_Temporal Video Grounding_: Determine the temporal boundary of a single event according to the text description. 2) [EPM]_Episodic Memory_: Localize the event that can answer the given question in egocentric scenarios, _e.g._, _"Where is my backpack?"_. 3) [TAL]_Temporal Action Localization_: Detect and localize a series of segments containing the given action, _e.g._, finding all "golf swing" segments in a long video. 4) [EVS]_Extractive Video Summarization_: Provide a list of segments that can be merged to form a compact video summary (with around 15% of the total duration). 5) [VHD]_Video Highlight Detection_: Cherry-pick a single timestamp (_e.g._, _"15s"_) that can best reflect the highlight

   } & } & } & } & } & } &  **Avg./Max.** \\ **Duration** \\  }} &  **Long** \\ **Video** \\  }} &  **Event** \\ **Level** \\  }} &  **Time** \\ **Sensitive** \\  }} &  **Answer** \\ **Type** \\  }} &  **Evaluation** \\ **Method** \\  }} \\  SEED-Bench  & Action & LLM & 3 & 3,757 & 3,757 & 8 frames & ✗ & ✗ & ✗ & MCQ & Likelihood \\ EggSchema  & Egocentric & LLM & 1 & 5,031 & 5,031 & 180s/180s & ✗ & ✗ & MCQ & Likelihood \\ AutoSelf-ValVideo  & Open & Human & 9 & 327 & 158/101s & ✗ & ✗ & Open & GPT-4 \\ Video-Bench  & Open & LLM & 1 & 17,054 & 5,917 & 56s/3.599s & ✗ & ✗ & MCQ & Mixed\({}^{}\) \\ TempCompsas  & Open & LLM & 4 & 7,540 & 410 & 12s/355 & ✗ & ✗ & ✗ & MCQ/Open & Rule/GPT \\ MVBench  & Open & Human\({}^{}\) & 1 & 4,000 & 3,673 & 15s/116s & ✗ & ✗ & ✗ & MCQ & Rule \\ 
**E.T. Bench** (Ours) & Open & Human\({}^{}\) & 12 & 7,289 & 7,002 & 129s/795s & ✓ & ✓ & ✓ & MCQ/Open & Rule \\    \({}^{}\) Repurposed from existing datasets \({}^{}\) Including next-token likelihood, T5 sentence similarity, and GPT-3.5 assisted evaluation

Table 1: Quantitative comparison between E.T. Bench and existing Video-LLM benchmarks.

moment corresponding to a query. Note that the formulations of [TAL], [EVS], and [VHD] have been modified (compared with previous works [83; 87; 44]) to fit the nature of LLMs.

**Dense Captioning** is a more complicated ability that requires jointly localize key-events and generate descriptions/summaries for each segment. This is more practical for storytelling or key-information extraction from long videos in comparison with video-level captioning on trimmed clips [103; 12]. We define two _dense captioning_ tasks according to different goals: 1) [DVC] _Dense Video Captioning_: Comprehensively describe all the events happened in the video. This is a general case with the goal of covering as much events as possible. 2) [SLC] _Step Localization and Captioning_: Identify and describe only the key-steps in instructional videos. In this case, the segments are shorter & disjoint and the step descriptions are more precise compared with [DVC].

**Complex Understanding** refers to the versatile integration of the aforementioned time comprehension and event localization, requiring the model to demonstrate proficient event-level and time-sensitive understanding. The two tasks are: 1) [TEM] _Temporal Event Matching_: Find and locate a similar event in the same video conditioning on the given segment. This involves a two-stage reasoning process that first identify the event in the given timestamps, then localize another segment with the most similar content. 2) [GVQ] _Grounded Video Question-Answering_: Answer the given multiple-choice question by selecting an option and localizing a segment that supports the answer. This is also a complex scenario requiring both understanding and localization abilities. Validating the localization results can help diagnose the reasoning process of Video-LLMs.

### Data Collection and Annotation

The key challenge of data collection is how to obtain videos with precise temporal boundary annotations. Previous works either prompt LLMs with frame-level information extracted from collections of experts [100; 76; 101; 34] or transform human-annotated moment tags (_e.g._, 5.2s) to boundaries (_e.g._, 3.2s to 7.2s) using pre-defined rules [56; 78]. These solutions can only generate temporal boundaries with substantial noise, which are not suitable for accurate model evaluation. Therefore, we meticulously curate multi-event videos from existing datasets with high-quality human-annotated timestamps, and repurpose the annotations by transforming them according to our task formulations. To ensure the diversity of scenarios, we carefully select 15 datasets from 8 domains, _i.e._, _indoor activities_, _tabletop_, _sports_, _egocentric_, _cooking_, _news & vlogs_, _how-to_, and _open_. All the videos are collected from val or test splits to prevent potential data leakage. For annotation generation, we develop for each task a thorough process containing pre-filtering, manual annotation repurposing, and instruction template design, presented in Figure 2 (right). Please refer to Section A for detailed task-specific pre-filtering criteria, repurposing methods, and instruction templates.

After annotation generation, we conduct a careful manual review on the samples, focusing on content suitability, annotation correctness, instruction clarity, scene diversity, video & event length, and task difficultly. Feedback from this review helped us actively optimize the generation process. Finally, to

Figure 2: **Left: Task taxonomy and sample distribution. Right: Generation pipeline for E.T. Bench. We conduct a thorough process of pre-filtering, annotation repurposing, instruction writing, manual check, and sampling to obtain high-quality fine-grained annotations. Details discussed in Section 2.**

balance the quality and efficiency of evaluation, we randomly sample up to 500 samples for each sub-task (task-source combination). In most cases, each video would only be sampled once.

### Benchmark Analysis

We present some statistics of the generated benchmark. Detailed comparisons are shown in Table 1. Overall, the proposed E.T. Bench contains 7,289 samples under a taxonomy of 4 capabilities, 12 tasks, and 20 sub-tasks. There are in total 7,002 unique videos originate from 15 datasets, covering 8 domains. The average duration of videos is 129.3s, with the minimum and maximum values of 6.2s and 795.0s, respectively. This differs from most existing benchmarks that have averaged durations only 10 \(\) 20 seconds. We also have diverse answer types for different tasks, including both MCQ and open-ended styles. The evaluation process is purely rule-based without human or LLM integration, ensuring satisfied objectivity. Below we introduce more detailed analysis on the benchmark.

**Task and Sample Distribution.** Figure 2 (left) shows the distribution of tasks, sub-tasks, and samples in E.T. Bench. Here, a sub-task is defined as a task-source combination, _e.g._, [TVG] contains two sub-tasks from two source datasets. A large proportion of samples are in the _grounding_ category, as we emphasize the moment localization ability of modern Video-LLMs.

**Text Queries.** Figure 3 (left) shows the word cloud of text queries in E.T. Bench. Thanks to the wide range of video domains, the queries are also diverse in terms of both nouns and verbs. Most queries are human-centric, describing human activities or human-object interactions. Please refer to Section A.4 for distributions of nouns and verbs.

**Video Durations.** Figure 3 (right) shows the distribution of averaged video durations across tasks. Our videos have a wide spectrum of durations, where _referring_ tasks have relatively shorter videos, and _grounding_ and _captioning_ have longer ones. Our experimental results in Table 3 show that the duration of videos have significant influence on model performance.

## 3 Our Method

Extensive evaluations (in Table 3) reveal that even the state-of-the-art Video-LLMs cannot perform well on E.T. Bench, especially on the more complicated _grounding_, _dense captioning_, and _complex understanding_ tasks. We attribute this phenomenon to two key limitations of existing development pipelines for Video-LLMs: 1) **Model**: Existing models fall short in numerical modeling [21; 23], which are essential capabilities for arithmetic calculations - timestamps processing in our case; 2) **Data**: Both pre-training and instruction-tuning are conducted on short & single-event videos, leading to weak general understanding abilities for multi-event videos. To address these limitations, we propose **E.T. Chat**, a novel Video-LLM that reformulates timestamp prediction as an embedding matching problem, serving as a strong baseline on E.T. Bench. we also curate **E.T. Instruct 164K**, an instruction-tuning dataset tailored for multi-event and time-sensitive video understanding.

### Model

Figure 4 presents the overall architecture of E.T. Chat. Given a video frame \(_{t}^{H W 3}\) sampled at time \(t T\), where \(H\) and \(W\) are the height and width, we first leverage a frozen visual encoder

Figure 3: **Left: Word cloud of text queries shows a considerable degree of diversity. Right: Distribution of averaged video durations (in seconds) across 12 tasks.**

\(E_{v}\) to convert it into patch embeddings \(_{t}^{K C}\), where \(K\) and \(C\) are the number of patches and feature dimension. To preserve high temporal resolution while reducing redundant compute, we adopt a frame compressor \(E_{c}\) to merge and project patch embeddings to a single token \(_{v}^{t}^{1 D}\), where \(D\) is the embedding dimension of LLM. The compressed frame tokens \(\{_{v}^{t}\}_{t=1}^{T}\) are then concatenated with text tokens \(\{_{q}^{n}\}_{n=1}^{N}\) and sent into LLM for response generation.

**Frame Compression.** As illustrated in Figure 5, the frame compression \(E_{c}\) arises from  and consists of a Q-Former \(E_{q}\) with \(M\) learnable queries, a context aggregator \(E_{a}\), and a projector \(E_{p}\). For each time step, \(E_{q}\) accepts patch embeddings \(_{t}\) and the text prompt \(\) as inputs, and resamples them into learnable queries \(_{t}^{M C}\). Then, \(E_{a}\) merges \(_{t}\) with \(_{t}\) and compresses them into a single token. \(E_{p}\) finally projects it to the same embedding space as LLM. In particular, the context aggregator \(E_{a}\) is built upon cross attention module , formulated as follows:

\[=(_{q}_{t})^{} (_{k}_{t})}{})\] (1)

\[_{t}=(_{t}+_{t})\] (2)

Here, \(_{t}^{1 C}\) is the compressed frame token for time \(t\), containing text-conditioned visual information. This process can be parallelized across all frames. The projected video token sequence \(_{v}\) is then concatenated with text tokens \(_{q}\) to form the inputs with shape \((T+N) D\) for LLM.

**Timestamp Prediction as Embedding Matching.** Our key insight focuses on the design of timestamp processing. As discussed in Section B.1, we claim that directly generating continuous signals (_i.e.,_ timestamps in our case) via discrete next-token prediction is sub-optimal. Motivated by the characteristic of Transformers that they are naturally good at selective copying rather than numerical calculations [27; 38], we propose to reformulate timestamp prediction as an _embedding matching_ problem. That is, we train the model to generate/copy embeddings of video frames that it would like to refer to, and obtain timestamps by matching these embeddings back to the video.

Specifically, we define a special token <vid> used to stimulate the matching process. When <vid> is generated during inference, _e.g._, the model outputs _"the event happens around <vid>"_, this token is utilized to match a video frame token, such that the desired timestamp can be easily obtained from the matched frame index. For example, for a video sampled to 1 FPS, if <vid> is matched to the \(i\)-th frame, then <vid> means the \(i\)-th second of the video. The matching process is designed to be simple and efficient. We denote the \(l\)-th layer hidden states of <vid> token and video frame tokens as \(_{vid}^{l}^{1 D}\) and \(_{frm}^{l}^{T D}\), respectively. During matching, two MLPs \(E_{vid}\) and \(E_{frm}\) are first leveraged to project the hidden states to the alignment space \(g\):

\[_{vid}=E_{vid}(_{vid}^{L-1}),_{frm}=E_{frm }(_{v}^{L})\] (3)

Figure 4: **Overall architecture of E.T. Chat.** We reformulate timestamp prediction as an embedding matching problem. See Section 3 for details.

Figure 5: **Detailed illustration of frame compressor.** It accepts video patch embeddings \(_{t}\) and the text prompt \(\) as inputs, and compress video frame features into a single token.

Here, \(L\) refers to the total number of LLM layers. We extract <vid>'s hidden states from the second-last layer to preserve a larger feature range . Subsequently, we compute the cosine similarities between \(_{vid}\) and all \(\{_{frm}^{t}\}_{t=1}^{T}\) to obtain the matched frame index \(t_{match}\):

\[=_{vid}_{frm}}{||_{vid}||_ {2}||_{frm}||_{2}}^{1 T}, t_{match}= *{argmax}()\] (4)

The frame index \(t_{match}\) is then multiplied with frame rate \(r\) to generate the real timestamp in seconds. Through this operation, the direct prediction of timestamps is replaced by embedding matching, which is easier to learn as a selective copying problem by Transformer-based models. For the case when <vid> occurs in inputs, the input features of <vid> are added with the corresponding frame features. During training, an extra binary matching loss is utilized:

\[_{matching}=-_{t=1}^{T}_{t}( _{t})\] (5)

Here, \(_{t}\) denotes the binary label indicating whether \(t\) is the ground truth frame. \(_{matching}\) is added with the original language modeling loss \(_{language}\) to jointly optimize the model.

**Numerical Continuity.** The matching process above still cannot preserve numerical continuities among them, as the hidden states of adjacent frames may be far away from each other. We introduce two modifications to effectively alleviate this problem. First, we observe that the causal self-attentions in LLM block out the bi-directional information flow. This is reasonable for text but limits the ability of video understanding. Thus, we allow bi-directional attentions among video tokens. Second, we introduce a smoothed label \(|}}\) to replace the binary label \(_{t}\) in Eq. 5, where \(\) is a hyper-parameter controlling the extent of smoothing, and \(t_{gt}\) refers to the ground truth frame index.

### Instruction-Tuning Dataset

The proposed E.T. Instruct 164K contains multi-event understanding samples generated from 14 source datasets, illustrated in Table 2. It covers a wide range of event-level understanding tasks, including temporal grounding, summarization, highlight detection, dense captioning, and question-answering. More details about the generation process are presented in Section C.

## 4 Experiments

### Evaluation Settings

As different tasks in E.T. Bench are under different settings with diverse output formats. A single metric (_e.g._, accuracy) like existing benchmarks is not sufficient. To balance the quantity of metrics and the ease of ranking, we unify the metrics within each capability and leverage accuracy for _referring_ tasks, F1 score for _grounding_ tasks, F1 score and sentence similarity for _dense captioning_ tasks, and recall for _complex understanding_ tasks. Detailed metrics are introduced in Section D.1.

### Main Results

We extensively evaluate 7 open-source Image-LLMs, 9 open-source Video-LLMs, and 4 commercial MLLMs on E.T. Bench. Details of each model are introduced in Section D.2. For Image-LLMs, we uniformly sample 8 frames and add an extra prompt indicating the video duration as a hint for timestamps. For Video-LLMs, we use their default number of frames as inputs. Commercial MLLMs are evaluated by calling APIs on a subset with 470 samples. The inputs for GPT-4V and GPT-4o are aligned with Image-LLMs. For Gemini-1.5 models, raw videos are directly uploaded.

The evaluation results are presented in Table 3. We report the metrics averaged among sub-tasks due to space limit. The _Random_ in the first row refers to random guessing. We also provide comparisons and ranking in Figure 6. Below we summarize our key findings from the results.

**Performance gap between Image- and Video-LLMs.** We observe that on _referring_ tasks, most Image- and Video-LLMs perform at the same level. Some Image-LLMs such as XComposer and Qwen-VL-Chat can even beat most Video-LLMs. This is because the videos for _referring_ are generally short, as compared in Figure 3 (right), such that the sampled 8 frames cover most information. The gap becomes larger on tasks with longer videos such as [TVG] and [TAL], demonstrating the importance of temporal modeling on E.T. Bench compared with other benchmarks.

**Strong Video-LLMs on existing benchmarks struggle on E.T. Bench.** Some state-of-the-art Video-LLMs on existing benchmarks, _e.g._, Video-LLaMA-2 and PLLaVA, are less effective on our E.T. Bench, especially on _grounding_ and _dense captioning_ tasks. We attribute this to the single-frame bias caused by both model architecture and training data. It also motivate us to consider the balance between spatial and temporal modeling in Video-LLMs.

**Some Video-LLMs fail to follow instructions.** During evaluation, we also notice that some models, _e.g._, Video-LLaVA and Video-LLaMA-2, fail to generate outputs in desired formats for some tasks even with carefully designed instructions and examples. For instance, Video-LLaVA can only generate repeated text outputs without any timestamps for [EVS], while Video-LLaMA-2 faces the similar problem on almost all _grounding_ tasks. we claim that this is due to the severe overfitting on their instruction-tuning data, which do not contain any timestamps outputs.

**Some Image-LLMs performs exceptionally well.** Qwen-VL-Chat and Bunny-Llama-3-8B-V are two models producing relatively good results compared with other Image-LLMs and even some Video-LLMs. On [TVG] and [DVC], Qwen-VL-Chat achieves 15.6 and 21.3 F1 scores, respectively, surpassing a number of Video-LLMs. But there is still a significant gap when compared with best-performing Video-LLMs such as TimeChat and LITA.

**Time-sensitive Video-LLMs are the first-class models.** VTimeLLM, TimeChat, and LITA are three Video-LLMs with explicit optimizations for timestamps modeling, such that they can persistently follow instructions and generate considerable responses.

**Commercial MLLMs are still competitive.** Even with 8-frame inputs, GPT-4V and GPT-4o show their significance compared with open-source models on some tasks such as [TVG], [TAL], and [DVC]. By supporting direct video inputs, Gemini-1.5 series achieve the strongest performance on a number of tasks in E.T. Bench, including [RAR], [EVC], [RVQ], [TVG], [TAL], [VHD], and [TEM].

    &  &  &  &  \\   & _{}w}\) EVC\({}_{}}\) RVQ\({}_{}}\)} & _{}}\)} & _{}}\)} & _{}}\) EVS\({}_{}}\)} & _{}}\)} & _{}}\)} & _{}}\)} & _{}}\) SLC\({}_{}}\)} & _{}}\)} & _{}}\) GVQ\({}_{}}\)} \\  _Random_ & 25.0 & 25.0 & 20.0 & – & – & – & – & – & – & – & – & – & – & – & – & – \\  _Open-source image-LLMs: All models are 8 uniformly sampled frames as inputs._ & _Pramps_ & _here been added with hints about timestamps._ & & & & & & & & & & & & \\ 
1LAVA-1.5  & 34.2 & 27.4 & 26.2 & 6.1 & 1.9 & 7.8 & 2.4 & 30.9 & 14.5 & 11.5 & 0.9 & 9.5 & 7.7 & 0.0 \\
1LAVA-InternalM2  & 34.0 & 34.8 & 37.0 & 2.7 & 0.1 & 0.3 & 0.2 & 32.3 & 16.9 & 8.5 & 0.1 & 4.7 & 7.2 & 1.5 \\ mPLUG-Owl2  & 37.8 & 26.4 & 34.6 & 1.1 & 0.2 & 3.0 & 4.1 & 36.8 & 0.1 & 8.1 & 0.1 & 7.7 & 6.2 & 0.0 \\ XCompeser  & 33.0 & 19.6 & 40.2 & 49.9 & 1.5 & 9.9 & 2.8 & 28.9 & 5.4 & 5.9 & 2.7 & 9.0 & 10.5 & 0.0 \\ Bunny-Llama-V  & 33.2 & 27.4 & 26.6 & 7.0 & 0.1 & 5.1 & 0.4 & 30.6 & 13.5 & 8.8 & 0.1 & 7.6 & 7.2 & 0.0 \\ MinCPM-V-2.5  & 37.6 & 28.0 & 37.6 & 2.0 & 0.1 & 4.4 & 13.4 & 18.7 & 6.2 & 11.8 & 1.4 & 9.7 & 0.7 & 0.0 \\ Qwen-VL-Chat  & 33.4 & 32.2 & 33.6 & 16.2 & 4.0 & 10.7 & 16.3 & 34.4 & 17.4 & 13.8 & 6.2 & 13.1 & 3.2 & 1.5 \\  _Open-source Video-LLMs: All models are their default numbers of frames as inputs._ & & & & & & & & & & & & & \\  Video-ChaMP(F)  & 22.6 & 24.2 & 23.0 & 7.0 & 1.3 & 15.1 & 8.4 & 28.8 & 8.8 & 11.3 & 5.7 & 10.2 & 15.9 & 0.0 \\ Video-LLaVA  & 33.6 & 33.0 & 22.6 & 7.0 & 1.9 & 15.0 & 0.3 & 28.9 & 28.0 & 15.0 & 0.9 & 8.3 & 7.5 & 0.1 \\ LLaMA-VID  & 30.4 & 38.4 & 28.8 & 5.5 & 1.2 & 8.0 & 1.4 & 30.0 & 27.1 & 12.6 & 5.2 & 11.1 & 7.0 & 0.9 \\ Video-LLaMA-2  & 28.8 & 27.4 & 28.0 & 0.1 & 0.0 & 0.0 & 1.5 & 0.6 & 14.5 & 0.0 & 15.2 & 0.0 & 0.1 \\ PLLAVA  & 33.8 & 22.6 & 31.8 & 6.9 & 1.1 & 5.7 & 0.3 & 28.9 & 13.3 & 10.6 & 9.7 & 11.8 &

**E.T. Chat fills the gap between open-source and commercial MLLMs.** Benefit from the novel timestamps processing design and the multi-event instruction-tuning data, E.T. Chat achieves state-of-the-art performance among open-source MLLMs on most tasks, and obtain comparable results as commercial MLLMs. Notably, significant improvements can be viewed on [EPM], [VHD], [TEM], and [GVQ]. Implementation details and ablation studies can be viewed in Section B.2 and Section D.4, respectively.

## 5 Related Work

**Video Large Language Models.** Video-LLMs [66; 53; 51; 110; 86; 82; 34] represent a class of intelligent chatbots capable of understanding videos and perform various open-ended tasks. Generally, a Video-LLM comprises a visual encoder [77; 109] for perception, a projector [37; 46] for feature alignemnt, and a LLM [94; 94; 18; 11] for reasoning and response generation. VideoChat  and Video-ChatGPT  are two earliest attempts in this direction. Following works tend to provide better solutions via adding audio modality , joint training on images and videos [53; 40], or performing alignment before projection . A recent trend [33; 82; 34; 76] involves the integration of Video-LLMs with time-sensitive understanding capabilities, while their solutions remain sub-optimal. Therefore, we propose to reframe timestamp generation as an embedding matching problem.

**Benchmarks for Video-LLMs.** The increasing number of Video-LLMs motivate the development of benchmarks [45; 16; 69; 67; 48; 63]. Among the earliest is SEED-Bench , a MLLM benchmark that supports both Image-LLMs and Video-LLMs and offers three evaluation dimensions in the realm of temporal modeling. AutoEval-Video  and Video-Bench  are designed specifically for videos. They employ LLMs for either QA generation or model evaluation. MVbench  provides a novel scheme to repurpose existing datasets for Video-LLM evaluation. Recent benchmarks also expand their scope and consider the ability of understanding extremely long videos [67; 86] or comprehending fine-grained temporal order information . Nevertheless, none of the benchmarks have been designed for multi-event and time-sensitive understanding. In response to this gap, we introduce E.T. Bench, the first benchmark providing comprehensive evaluations on these scenarios.

## 6 Conclusion

In this work, we introduce **E.T. Bench**, a large-scale and comprehensive benchmark for multi-event & time-sensitive video-language understanding. Our benchmark encompasses a wide range of tasks on diverse video domains, evaluating multiple capabilities of Video-LLMs. Our experimental results reveal that current model designs and instruction-tuning data for Video-LLMs exhibit limitations in their capacity for timestamp representation and fine-grained multi-event modeling. To address these challenges, we further develop a novel model **E.T. Chat**, in conjunction with a multi-event instruction-tuning dataset, **E.T. Instruct 164K**, which serves as a robust baseline solution for such scenarios. We hope that the proposed benchmark, model, and instruction-tuning dataset will inspire future research on developing Video-LLMs.

Figure 6: **Left: Performance comparison between E.T. Chat and representative models. Right: Ranking of MLLMs on E.T. Bench, where red means higher ranks and blue represents lower ranks.**

## Appendix

In the appendix, we provide more details about the proposed benchmark, model, and instruction-tuning dataset to complement the main paper. Additional analysis, ablation studies, visualizations, and discussions are also incorporated. Below is the table of content.

* Pre-filtering Criteria
* Annotation Repurposing and Cleaning
* Instruction Templates
* Design Space for Timestamp Processing
* Implementation Details
* Instruction-tuning Dataset
* Task Selection
* Data Collection and Instruction Generation
* Evaluation Metrics
* More Benchmark Results
* Ablation Studies
* Qualitative Results
* Limitations & Future Work

## Appendix A Benchmark

### Pre-filtering Criteria

Table 4 presents the pre-filtering criteria for each source dataset when generating E.T. Bench.

### Annotation Repurposing and Cleaning

We summarize the detailed annotation repurposing and cleaning process for each task as follows.

[RAR] **Referred Action Recognition.** We adopt the _action localization_ subset of Perception Test  for this task. We first select videos with at least three different actions, in which one action is sampled as ground truth. We then sample two other actions from the same video as intra-video distracters, and one action from other videos as inter-video distracter. The coarse timestamp hint is sampled from the segment containing only the ground truth action.

[ECA] **Event Caption Alignment.** We utilize Charades-STA  as data source. For each event-query pair, we randomly generate distracters (temporal boundaries) with 0.5\(\) to 2\(\) lengths compared with the ground truth, and ensure the temporal IoUs between any two options are no more than 0.5.

[RVQ] **Referred Video Question-Answering.** We leverage the high quality QA pairs from _interaction_ and _sequence_ question types of STAR . Since the original annotations only contain question-relevant temporal boundaries, we randomly pick 20% of the QA pairs and modify their boundaries to have no overlap with the original ones, in order to synthesis the case when the question cannot be answered within the given boundary. An extra "unable to answer" option is added to all QA pairs.

[MISSING_PAGE_FAIL:11]

[TVG]**Temporal Video Grounding.** Two datasets, _i.e._, Charades-STA  and QVHighlights  are chosen. We filter out the samples with event duration shorter than 2s or longer than 50s, as these are generally noisy samples. On QVHighlights, only the samples with single events are chosen to align with our formulation.

[EPM]**Episodic Memory.** We employ Ego4D-NLQ  and conduct a thorough data cleaning & verification process. First, we perform a rule-based fix for noisy questions. Some common cases are: 1) Question starts with an additional "Query Text:" string. 2) Typos such as "I" \(\) "i" or "I". 3) Unclear references such as "person x". We also found that some questions are ambiguous in the context of long videos, so we randomly crop the all the videos to 300-second long.

[TAL]**Temporal Action Localization.** We adopt Perception Test , THUMOS'14  (test split), and THUMOS'15  (val split). Videos with ambiguous action classes, _e.g._, _moving object(s) around_ and _other_ in Perception Test, and _ambiguous_ in THUMOS, are discarded. To reduce the difficulty, samples with more than 10 ground truth moments are filtered out as well.

[EVS]**Extractive Video Summarization.** We repurpose TVSum  and SumMe  to generate samples. In conventional video summarization, each frame is annotated with a probability of being the summary, which is incompatible with Video-LLMs that cannot strictly produce temporally-aligned frame-level scores, as can be seen in the near-random results in . Therefore, we reformulate it to predicting a set of temporal boundaries that compose the summary. Ground truths are obtained by sorting the frame-level scores and generate boundaries for consecutive frames with top-15% scores. When a video is annotated by multiple annotators (_i.e._, in TVSum), we simply average the scores. This reformulation helps models persistently generate reasonable results.

[VHD]**Video Highlight Detection.** Samples are generated from QVHighlights  and YouTube Highlights  using a similar method as [EVS], but for highlights we only consider the frames with the highest scores. That means, ground truths are the frames with the highest highlight saliencies. During inference, a prediction is considered correct if the timestamp falls into any of the temporal boundaries. We directly utilize the text queries in QVHighlights as highlight queries. For YouTube Highlights, the domains are used, and videos with highlights covering more than 90% are discarded.

[DVC]**Dense Video Captioning.** We utilize YouCook2  and HiREST . Although HiREST only contains instructional videos, it is still considered as [DVC] rather than [SLC] because of the large event coverage. We also trimmed out the opening and closing scenes of HiREST videos.

[SLC]**Step Localization and Captioning.** We select CrossTask  and HT-Step  for this task. For CrossTask, we filter out the samples with wrong annotations, _e.g._, repeated steps, and select only the videos with all steps longer than 2s to avoid ambiguous annotations. For HT-Step, we keep only the videos with at least 2 steps and remove the samples with incorrect temporal orders.

[TEM]**Temporal Event Matching.** We repurpose Perception Test  and QVHighlights  for this novel task, where the former focus on actions and the latter is for general events. We select videos with actions (excluding the _other_ category) occurs multiple times from Perception Test, and sample one temporal boundary as the input reference. Other boundaries are used as ground truths. For QVHighlights, samples with one query referring to multiple disjoint moments are used.

[GVQ]**Grounded Video Question-Answering.** We adopt QAEgo4D  which naturally contains both QA pairs and corresponding timestamps derived from Ego4D-NLQ . To control the task difficulty, we randomly crop all the videos to 150-second long. Typos in QA pairs are fixed.

### Instruction Templates

The instruction templates for different tasks are shown in Table 5. To ensure the models give responses in desired formats, each instruction starts with a sentence introducing the domain/title of the video, followed by detailed requirements about the task. We also add an explicit statement about the format of response and an example as guidance. The model outputs are passed through carefully designed rule-based parsers for answer extraction before evaluation.

### Distribution of Queries

We visualize the frequency distribution of verbs and nouns in E.T. Bench in Figure 7 and Figure 8, respectively. The distribution histograms demonstrate the diversity of queries in E.T. Bench.

|p{142.3pt}|p{142.3pt}|}  
**Type** & **Task** & **Instruction Template** & **Example Response** \\   &  & You are given a video about indoor activities. Watch the video carefully and identify the action around “timney” by choosing from a set of options. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (B)”. Now I give you the options: (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice. & **Best Option: (C)** \\   &  & You are given a video about indoor activities. Watch the video carefully and select the moment that can be used described by the sentence “equery”. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (A) “. Now I give you the options: (A) “timney - (B) “timney - (C) “timney - (D) “timney - (C) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - (D) “timney - (C) “timney - & **FORD** \\   &  & You are given a video about indoor activities. Watch the video carefully and a multiple choice question solely based on the event in “timney - (Timney). The format of your response should be: “Best Option: (Your choice)”. For example: “Best Option: (C)”. You may select “mable to answer” if the question can not be answered based on the provided moment. Now I give you the question: “question: (E) “option: (B) “option: Please provide your choice. & **Best Option: (D)** \\   &  & You are given a video about daily activities / indoor activities. Watch the video carefully and find a visual event described by the sentence: “equery”. The format of your response should be: “The event happens in “timney - (Timney)”. You must represent start and end times in seconds. For example: “The event happens in 10.2 - 12.8 seconds”. & **Best Option: (Your choice)** \\   &  & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question: (Your choice)”. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The event happens in “timney - (Timney)** \\   &  & You are given a video containing a series of actions. Watch the video carefully and find the visual events belonging to the action category: “action”. The format of your response should be: “The action happens in “start time - - (end time), and “start time - (end time) - (end time). You must represent start and end times in seconds. For example: “The action happens in 4.2 - 6.8, 7.5 - 10.3, 15.1 - 18.6, and 23.4 - 27.5 seconds”. & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities. Watch the video carefully and find a highlight moment according to the sentence / its domain: “equery”. The format of your response should be: “The highlight moment happens at “timney”. You must represent time in seconds. For example: “The highlight moment happens at 26.8 seconds”. & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about “equery”. Watch the video carefully and densely describe all the events in it. For each event, you need to determine the start and end times and provide a concise description. The format of your response should be: “cstart time” - send time>, \(\) description”. For example: “90 - 102 seconds, spend margarine on two slices of white bread. 114 - 127 seconds, place a slice of the bread.” & **The highlight moment happens at “timney.** \\   &  & You are given a video about “classo”. Watch the video carefully and identify all the key steps in it. For each step, you need to determine the start and ends times and provide a concise description using a few words. The format of your response should be: “cstart time” - send time>, \(\) description”. You must represent start and end times in seconds. For example: “24.8 - 30.2 seconds, cut apple. 35.6 - 40.4 seconds, wash dishes.”. & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities / containing a series of actions. Watch the video carefully and identify the event in “timney - (Timney), then localize a different moment that contains the most similar event. The format of your response should be: “The similar event happens in “start time” - send time>. You must represent start and end times in seconds. For example: “The similar event happens in 16.8 - 20.4 seconds”. & **The same memory locates in “timeney - (Timney)** \\   &  & You are given an egocentric video about daily activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice. & **Best Option: (C)** \\   &  & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (D)** \\   &  & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question”. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **Best Option: (D)** \\   &  & You are given an egocentric video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question””. The format of your response should be: “The event happens in “timney - (Timney). You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities. Watch the video carefully and find a visual event that can answer the question: “question””. The format of your response should be: “The event happens in “static time - send time>. You must represent start and end times in seconds. For example: “The event happens in “timney - (Timney). & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities. Watch the video carefully and identify the action around “timney - (Timney). You choose from a set of options. The format of your response should be: “Best Option: (your choice)”. For example: “Best Option: (B) “option: (C) “option: (D) “option: (D) “timney - (Timney). You must represent start and end times in seconds. For example: “The similar event happens in “static time - send time>. You must represent time in seconds. For example: “The highlight moment happens at 26.8 seconds”. & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities. Watch the video carefully and describe all the events in it. For each event, you need to determine the start and end times and provide a concise description. The format of your response should be: “cstart time” - send time>, \(\) description”. For example: “90 - 102 seconds, spend margarine on two slices of white bread. 114 - 127 seconds, place a slice of the bread.” & **The similar event happens in “timney - (Timney)** \\   &  & You are given a video about “classo”. Watch the video carefully and identify all the key steps in it. For each step, you need to determine the start and ends times and provide a concise description using a few words. The format of your response should be: “cstart time” - send time>, \(\) description”. You must represent start and end times in seconds. For example: “24.8 - 30.2 seconds, cut apple. 35.6 - 40.4 seconds, wash dishes.”. & **The same memory locates in “timney - (Timney)** \\   &  & You are given a video about daily activities / containing a series of actions. Watch the video carefully and identify the event in “timney - (Timney), then localize a different moment that contains the most similar event. The format of your response should be: “The similar event happens in “static time - send time”. You must represent start and end times in seconds. For example: “The similar event happens in 16.8 - 20.4 seconds”. & **The same memory locates in “timeney - (Timney)** \\   &  & You are given an egocentric video about daily activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice), the relevant event happens in “static time - send time>. Now I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (C)** \\   &  & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be: “Best Option: (your choice)”. The relevant event happens in “static time - send time>. You I give you the question: “question”. The options are (A) “option: (B) “option: (C) “option: (D) “option: Please provide your choice and the relevant moment. & **Best Option: (D)** \\   &  & You are given a video about indoor activities. Watch the video carefully and answer an multiple choice question. Your answer should contain a choice of the best option and a relevant moment that supports your answer. The format of your response should be:

## Appendix B Method

### Design Space for Timestamp Processing

As illustrated in Figure 9, existing MLLMs typically handle timestamps (for videos) or coordinates (for images) in three ways: 1) **Numerical Expressions**: Representing numbers directly in the form of text. This straightforward strategy loses continuity among numbers . A wrong prediction could be extremely far away from the ground truth. 2) **Special Tokens**: Defining a set of special tokens to quantize time/position into a fixed number (typically 100 to 300) of bins. This solution inevitably brings severe quantization loss, and it is not flexible for videos with variable lengths. Moreover, introducing too many new tokens into the vocabulary would break the pre-trained distribution of LLMs, making them hard to optimize without post pre-training.

3) **External Modules**: Leveraging pre-trained external models (_e.g._, SAM ) for grounding. This would introduce extra parameters and latency to LLMs. It is not directly compatible with videos as well, as existing temporal grounding models  are domain-specific and hard to generalize to all scenarios like SAM. Therefore, we propose to reformulate timestamp prediction as an _embedding matching_ problem.

Figure 8: **Frequency distribution of nouns in E.T. Bench.** We only visualize the top 25 out of 3,090 nouns for clarity. The x- and y-axes denote the nouns and their frequencies, respectively.

Figure 7: **Frequency distribution of verbs in E.T. Bench.** We only visualize the top 25 out of 461 verbs for clarity. The x- and y-axes denote the verbs and their frequencies, respectively.

### Implementation Details

We adopt the pre-trained ViT-G/14 from EVA-CLIP  as visual encoder. The architecture of frame compressor and LLM are based on Q-Former  and Phi-3-Mini-3.8B , respectively. We first pre-train the model following the stage-1 and stage-2 recipes in , then activate the MLP projectors \(E_{vid}\) & \(E_{frm}\) and fine-tune them on E.T. Instruct 164K. \(E_{vid}\) and \(E_{frm}\) are randomly initialized, and the embeddings for <vid> token are initialized from the averaged embeddings of all existing tokens. During fine-tuning, we freeze the visual encoder and the FFN layers in Q-Former, and introduce LoRA  adapters on the LLM. Therefore, only the attention layers and projectors in frame compressor \(E_{c}\), LoRA adapters, and matching projectors (\(E_{vid}\) & \(E_{frm}\)) are learnable. We train the model with mixed precision (FP16) on a compute node with 8 \(\) NVIDIA V100 GPUs. The training process costs around 20 hours. More detailed hyper-parameters are listed in Table 6.

## Appendix C Instruction-Tuning Dataset

To fill the gap of lacking multi-event and time-sensitive training data for MLLMs, we introduce E.T. Instruct 164K, a large-scale instruction-tuning dataset with fine-grained timestamp annotations. Statistics about the dataset are shown in Table 7.

### Task Selection

E.T. Instruct 164K covers 9 event-level understanding tasks, including [RvC] referred video captioning, [TVG] temporal video grounding, [TAL] temporal action localization, [EVS] extractive video summarization, [VHD] video highlight detection, [DVC] dense video captioning, [TVC] tagged video captioning, [SLC] step localization and captioning, and [GVQ] grounded video question-answering. Most tasks are aligned with E.T. Bench but with different source datasets. The only exception are [RvC] and [TVC], where the former requires the model to generate captions for the the given temporal boundary, and the latter is similar to [DVC] but with starting timestamps only. Note that [RAR], [ECA], [RvQ], [EPM], and [TEM] are in E.T. Bench only, which can be regarded as held-out tasks during evaluation.

  
**Hyper-parameter** & **Value** \\  _Visual Encoder_\(E_{v}\) & \\  Frame Sampling Rate & 1 FPS \\ Preprocessing & Center Crop \\ Input Resolution & 224 \(\) 224 \\ Patch Size & 14 \(\) 14 \\  _Frame Compressor_\(E_{c}\) & \\  Number of Learnable Queries \(M\) & 32 \\ Number of Layers & 12 \\ Hidden Size & 768 \\  _MLP Project_\(E_{vid}\)\(\&\)\(E_{frm}\) & \\  Number of Layers & 2 \\ Hidden Size & 1536 \\ Output Size & 3072 \\  _Large Language Model_ & \\  LoRA \(r\) & 128 \\ LoRA \(\) & 256 \\ LoRA Dropout Rate & 0.05 \\ LoRA Modules & QKVO Layers \\  _Model Training_ & \\  Max Number of Tokens & 2048 \\ Number of Epochs & 1 \\ Batch Size & 32 \\ Learning Rate for LoRA & 5e-5 \\ Learning Rate for Other Parameters & 2e-5 \\ Weight Decay & 0.0 \\ Warmup Ratio & 0.03 \\ LR Schedeller Type & Cosine \\ Optimizer & AdamW  \\ AdamW \(_{1},_{2}\) & (0.9, 0.999) \\   

Table 6: **Hyper-parameters for fine-tuning.**

Figure 9: **Design space for timestamp processing. Existing MLLMs handle timestamps in videos or coordinates in images via a) numerical expressions, b) special tokens, or c) external modules. Details are discussed in Section B.1.**

### Data Collection and Instruction Generation

We meticulously sample videos and annotations from 14 datasets, including HowToCaption , DiDeMo , QueryD , TACoS , NaQ , ActivityNet , HACS , VideoXum , Mr. HiSum , ActivityNet Captions , ViTT , COIN , HowToStep , and EgoTimeQA . During sampling, we ensure that the videos have no overlap with E.T. Bench. Different from E.T. Bench in which all the samples are manually labeled, more than 40% of the samples in E.T. Instruct 164K are with automatically generated annotations. A similar filtering and rule-based cleaning process as E.T. Bench generation is conducted. Note that videos from EgoTimeQA are randomly cropped to 150-second long to reduce ambiguity during training.

We then convert the original annotations into instruction-following formats. Following previous works , for each task, we carefully write a well-designed instruction, then prompt GPT-4  to extend it to multiple diverse expressions. For some tasks having overlap with previous work , existing instructions are also taken into consideration. We manually select and refine 6 expressions to serve as the instruction templates for each task. To obtain ground truth responses, we convert the original annotations into natural language styles using manually designed templates. The generated instruction templates and response formats are shown in Table 8 & 9.

## Appendix D Experiments

### Evaluation Metrics

**Referring.** All the tasks are formulated as MCQs, thus we adopt accuracy as the main metric.

**Grounding.** We compute F1 scores averaged among IoU thresholds \(_{IoU}\) at four levels (0.1, 0.3, 0.5, and 0.7). For [TVG] and [EPM], only the first predicted temporal boundary is accepted. This aligns with conventional settings that use Recall@1 as metrics. For [TAL], all the predicted boundaries are used. In [EVS], F1 scores are computed at clip level, that is, each video is divided into 1-second long clips, and precision/recall is defined as the percentage of true positive clips with respect to all the predicted/ground truth clips. For [VHD], a prediction (single timestamp) is regarded as a true positive when it falls within any of the ground truth boundaries.

**Dense Captioning.** Similar to _grounding_, we utilize F1 score at the same four levels of \(_{IoU}\) for boundary predictions in [DVC] and [SLC]. This also aligns with previous works in these areas . To measure the correctness of descriptions, previous works leverage traditional metrics  for machine translation, which cannot handle ambiguity in open-ended scenarios. Therefore, we instead perform evaluation at semantic level and employ sentence similarity  to

  
**Task** & **Source** & **Manual Label** & **Avg. Duration** & **\#Samples** & **Ratio** \\    &  & \)} & 176.9s & 16,907 & 10.3\% \\   & DiDeMo  & ✓ & 49.0s & 33,000 & 20.1\% \\  & QueryD  & ✗ & 173.7s & 4.267 & 2.6\% \\  & TACoS  & ✓ & 151.9s & 6,693 & 4.1\% \\  & NaQ  & ✗ & 296.5s & 10,546 & 6.4\% \\   & ActivityNet  & ✓ & 118.8s & 9,807 & 6.0\% \\  & HACS  & ✓ & 159.5s & 15,218 & 9.3\% \\    & VideoXum  & ✓ & 123.5s & 7,989 & 4.9\% \\    &  &  & 196.4s & 9,056 & 5.5\% \\    & ActivityNet Captions  & ✓ & 118.9s & 9,830 & 6.0\% \\    & ViTT  & ✗ & 210.0s & 2908 & 1.8\% \\   & COIN  & ✓ & 138.9s & 7,659 & 4.7\% \\  & HowToStep  & ✗ & 189.8s & 20,000 & 12.2\% \\    & EgoTimeQA  & ✗ & 150.0s & 10,000 & 6.1\% \\   &  & 163,880 & 100\% \\   

Table 7: **Task and sample distribution in E.T. Instruct 164K.**

[MISSING_PAGE_EMPTY:17]

measure the distances between model outputs and ground truths. Following previous practices , the all-MiniLM-L6-v2 model in Sentence Transformers1 library is used as the embedding model.

**Complex Understanding.** We adopt Recall@1 as the metric for both [TEM] and [GVQ]. The IoU thresholds are aligned with _grounding_ and _dense captioning_. For [TEM], only the first predicted temporal boundary is accepted, and it is regarded as a true positive when it has the maximum IoU among all ground truths larger than the threshold. For [GVQ], aside from boundary prediction, the MCQ answer should also be correct for a successful recall.

With the unified evaluation metrics, we are able to average them and measure a model's general performance under each capability. To achieve this, we further define 5 averaged metrics: 1) Acc\({}_{ref}\): Averaged accuracy on _referring_ tasks; 2) F1\({}_{gnd}\): Averaged F1 score on _grounding_ tasks; 3) F1\({}_{cap}\): Averaged F1 score on _dense captioning_ tasks; 4) Sim\({}_{cap}\): Averaged sentence similarity on _dense captioning_ tasks; 5) Rec\({}_{com}\): Averaged recall on _complex understanding_ tasks. These metrics serve as indicators for general performance on event-level and time-sensitive video understanding.

### Baselines

We extensively evaluate 20 representative MLLMs on E.T. Bench, including 7 open-source Image-LLMs (LLaVA-1.5 , LLaVA-InternLM2 , mPLUG-Owl2 , InternLM-XComposer , Bunny-LIama3-V , MiniCPM-LIama3-V-2.5 , and Qwen-VL-Chat ), 9 open-source Video-LLMs (Video-ChatGPT , Video-LLaVA , LLaMA-VID , Video-LLaMA-2 , PLLaVA , VTimeLLM , VTG-LLM , TimeChat , and LITA ), and 4 commercial MLLMs (GPT-4V , GPT-4o , Gemini-1.5-Flash , and Gemini-1.5-Pro ). Note that the video interface for GPT-4o is not publicly available, hence we treat it as an Image-LLM instead.

  
We compare the architectures of open-source MLLMs in Table 10. Optional visual encoders for these models are CLIP , EVA , SigLIP , OpenCLIP , and LanguageBind , while the LLM backbones include LLaMA , Llama-2 , Llama-3 , Vicuna , InternLM , InternLM2 , Qwen , and Phi-3-Mini .

### More Benchmark Results

In Table 19 and Table 20, we provide performance breakdown across source datasets, where we observe that the ranking of models differs across source datasets. Table 21\(\) 25 present detailed comparisons under different IoU thresholds \(_{IoU}\) and more metrics (_e.g._, METEOR , Rouge-L , and CIDEr ) on [TVG], [EPM], [TAL], [DVC], [SLC], [TEM], and [GVQ] tasks.

### Ablation Studies

**Effect of architectural designs.** We verify the effectiveness of <vid> token, bi-directional attention across video tokens, and label smoothing during training. The results are compared in Table 11. Without introducing the <vid> token (first row), our model falls back to the _numerical expression_ variant discussed in Section B.1, which struggles in timestamp prediction even with instruction-tuning on E.T. Instruct 164K. By reformulating timestamp prediction as embedding matching (second row), our method significantly works better on all tasks on E.T. Bench. Extra modifications, _i.e._, bi-directional attention (third row) and label smoothing (fourth row), further enhance the model to achieve better localization abilities, demonstrated by the substantial increase in F1\({}_{gnd}\).

  
**Model** & **Size** & **Frame Resolution** & **Sampled Frames** & **Visual Encoder** & **LLM** \\  _Image-LLMs_ & & & & & \\  LLaVA-1.5  & 7B & 336 \(\) 336 & 8 & CLIP-ViT-L/14 & Vicuna-1.5 \\ LLaVA-InternalM2  & 7B & 336 \(\) 336 & 8 & CLIP-ViT-L/14 & InternLM2 \\ mPLUG-Owl2  & 7B & 448 \(\) 448 & 8 & CLIP-ViT-L/14 & Llama-2 \\ InternalM-X-Compster  & 7B & 224 \(\) 224 & 8 & EVA-ViT-G/14 & InternLM M \\ Bumy-Llama-3  & 8B & 384 \(\) 384 & 8 & SigLIP-ViT-L/14 & Llama-3 \\ MiniCPM-Llama3-V-2.5  & 8B & 980 \(\) 980 & 8 & SigLIP-ViT-L/14 & Llama-3 \\ Qwen-VL-Chat  & 7B & 448 \(\) 448 & 8 & CLIP-ViT-bigG/14 & Qwen \\  _Video-LLMs_ & & & & & \\  Video-ChaMPGT  & 7B & 224 \(\) 224 & 100 & CLIP-ViT-L/14 & LLaMA \\ Video-LLAVA  & 7B & 224 \(\) 224 & 8 & LanguageBind-ViT-L/14 & Vicuna-1.5 \\ LLaMA-VID  & 7B & 224 \(\) 224 & 1 FPS & EVA-ViT-G/14 & Vicuna-1.5 \\ Video-LLAMA  & 7B & 224 \(\) 224 & 8 & EVA-ViT-G/14 & Llama-2 -Chat \\ PLLAVA  & 7B & 672 \(\) 672 & 16 & CLIP-ViT-L/14 & Vicuna-1.5 \\ VTimeLLM  & 7B & 224 \(\) 224 & 100 & CLIP-ViT-L/14 & Vicuna-1.5 \\ VTG-LLM  & 7B & 224 \(\) 224 & 96 & EVA-ViT-G/14 & Llama-2 \\ TimeChat  & 7B & 224 \(\) 224 & 96 & EVA-ViT-G/14 & Llama-2 \\ LITA  & 13B & 224 \(\) 224 & 100 & CLIP-ViT-L/14 & Vicuna-1.3 \\  E.T. Chat (Ours) & 3.8B & 224 \(\) 224 & 1 FPS & EVA-ViT-G/14 & Phi-3-Mini \\   

Table 10: **Model architectures of MLLMs evaluated on E.T. Bench. Size means the LLM size.**

  
**<vid> Token** & **Bi-directional** & **Smoothing** & **Acc\({}_{grd}\)** & **F1\({}_{grd}\)** & **F1\({}_{vg}\)** & **Sim\({}_{vg}\)** & **Rec\({}_{cav}\)** \\   & & & 25.0 & 17.5 & 21.2 & 12.3 & 8.6 \\  ✓ & & & 34.0 & 25.2 & 26.4 & 15.4 & 9.2 \\ ✓ & ✓ & & 33.7 & 30.5 & 27.5 & 15.8 & 9.8 \\ ✓ & ✓ & ✓ & 34.5 & 25.8 & 26.4 & 13.6 & 9.5 \\  ✓ & ✓ & ✓ & **38.4** & **33.5** & **31.4** & **17.1** & **10.1** \\   

Table 11: **Comparison on architectural designs.**

  
**Method** & **Acc\({}_{grd}\)** & **F1\({}_{grd}\)** & **F1\({}_{cap}\)** & **Sim\({}_{vg}\)** & **Rec\({}_{cav}\)** \\  Pooling & 29.6 & 25.5 & 21.1 & 11.3 & 9.1 \\ Q-Former & **38.4** & **33.5** & **31.4** & **17.1** & **10.1** \\   

Table 12: **Choices of frame compressor.**

**Choice of frame compressor.** Table 12 compares the performance of two design choices for frame compressor \(E_{c}\), _i.e._, a naive spatial pooling among frame patches \(_{t}\) and a query-guided compression based on Q-Former . The results confirm that employing Q-Former for frame compression proves to be a superior alternative.

**Choice of LLM layer for matching.** As articulated in the main paper, during matching, we utilize the second-last layer's hidden states for the <vid> token, while the final-layer's hidden states for frame tokens. This strategy take into consideration the small feature range of final-layer hidden states for the <vid> token . We further verify its effectiveness in Table 13. While the results are essentially similar, current setting exhibits a marginally superior overall performance.

**Learnable modules.** In Table 14, we justify the training strategy for instruction-tuning. Updating only the context aggregator \(E_{a}\) and projector \(E_{p}\) (first row) makes the training hard to converge with new tokens, and fine-tuning the LLM with LoRA (second row) brings better performance. We contend that the pre-trained Q-Former is suitable only for short & single event videos due to the constraints of pre-training data. Serving as the frame compressor, such limitation would hinder the model's performance. Line 3 \(\) 5 corroborate our hypothesis, as updating Q-Former on E.T. Instruct 164K brings notable performance improvements. Furthermore, we observe that fine-tuning the whole Q-Former makes the model slightly overfit to dense captioning tasks, and freezing its FFN layers could strike the balance between adapting to new data and retaining pre-trained capabilities.

**Effect of \(\) for label smoothing.** We ablate the effect of different \(\) values for label smoothing in Table 15. Smaller \(\) values make the optimization goal of matching scores smoother. Generally, setting \(\) to around 2.0 brings considerable results.

**Joint effect of model and instruction-tuning dataset.** We compare in Table 16 the joint effect of model design and instruction-tuning dataset collection. We choose two representative models (LLaMA-VID  and TimeChat ) as baselines and train them on E.T. Instruct 164K. Our E.T. Chat is also trained on TimeIT dataset  for in-depth comparison. The comparison results between line 1 & 2, 3 & 4, and 5 & 6 demonstrate the effectiveness of E.T. Instruct 164K. Results in line 2, 4, and 6 verify the significance of our model design.

**Effect of instruction-tuning tasks.** To study the effect of each task during instruction tuning, we provide detailed comparisons in Table 17. We observe that adding more tasks for instruction-tuning

   \)**} &  &  &  & _{off}\)**} & _{adv}\)**} & _{adv}\)**} & _{vop}\)**} & _{conv}\)**} \\    & & & & & & & & \\   &  &  &  &  &  & 36.1 & 29.2 & 27.1 & 14.1 & 8.7 \\  & & & & & & 37.3 & 30.5 & 28.3 & 15.0 & 9.6 \\    & & & & & 37.9 & 31.8 & 29.6 & 15.7 & 9.5 \\    & & & & & **38.4** & **33.5** & 31.4 & 17.1 & **10.1** \\    & & & & & 37.5 & 29.1 & **32.2** & **17.5** & 9.3 \\   

Table 14: **Comparison on learnable modules.** ATTN and FFN represent the attention and feed-forward layers in Q-Former, respectively.

   _{vid}\)**} & _{gm}\)**} & _{off}\)**} & _{adv}\)**} & _{adv}\)**} & _{vop}\)**} & _{conv}\)**} \\  –1 & –1 & 38.2 & 32.2 & 31.0 & **17.2** & 9.6 \\ –1 & –2 & 37.8 & 32.5 & 30.8 & 16.5 & 9.9 \\ –2 & –1 & **38.4** & 33.5 & **31.4** & 17.1 & **10.1** \\ –2 & –2 & 37.6 & **33.8** & 31.2 & 16.7 & 9.7 \\   

Table 13: **Choices of layers for matching.** Layer\({}_{vid}\) and Layer\({}_{fm}\) are the index of LLM layer utilized for matching, _e.g._, “–1” means using the final-layer hidden states.

    & _{off}\)**} & _{adv}\)**} & _{adv}\)**} & _{vop}\)**} & _{conv}\)**} \\ 
1.0 & 37.2 & 30.8 & 26.3 & 14.8 & 9.6 \\
1.5 & 37.9 & 31.6 & 28.6 & 15.4 & **10.3** \\
2.0 & **38.4** & 33.5 & **31.4** & **17.1** & 10.1 \\
2.5 & 38.1 & 33.0 & 29.8 & 16.8 & 9.2 \\
3.0 & 37.5 & **33.7** & 28.4 & 16.0 & 9.8 \\   

Table 15: **Effect of \(\) for label smoothing.**might slightly affect the performance on original tasks. This can be alleviated by carefully balancing the number of samples per task.

### Qualitative Results

Figure 10\(\) 15 present task-specific qualitative comparisons among 5 representative open-source MLLMs, _i.e._, LLaVA-1.5 , Video-ChatGPT , LLaMA-VID , TimeChat , and E.T. Chat. The correct model responses are marked green. We observe that the unsatisfactory performance of existing methods comes from 1) weak instruction-following abilities, 2) low temporal resolution, 3) lack of event-level and time-sensitive designs, and 4) lack of multi-event instruction-tuning data.

## Appendix E Limitations and Future Work

Currently, the proposed E.T. Bench is based on val or test split of existing datasets, whose training split might be included for MLLM training. This could potentially result in data leakage, thereby compromising the integrity of the zero-shot evaluation framework and leading to unfair comparisons. Therefore, our next step would be self-collecting new videos and provide manual annotations under each carefully designed task. More flexible input-output formats shall also be incorporated to complement the existing benchmark.

For E.T. Chat, even with advanced frame compression strategies, the low spatial resolution (1 token per frame) limits the model's ability to understand spatial details. Modern Image-LLMs are becoming to support extra-high-resolution image inputs, but this is not directly compatible to videos due to the large compute resource consumption. Our future work will focus on the balance between spatial and temporal resolution for Video-LLMs.

## Appendix F Licenses

The annotations of E.T. Bench are provided to the public under CC BY-NC-SA 4.0 license. A copy can be obtained at https://creativecommons.org/licenses/by-nc-sa/4.0/. By downloading our dataset from our website or other sources, the user agree to adhere to the terms of CC BY-NC-SA 4.0 and licenses of the source datasets. Licenses of the source datasets are listed in Table 18.

  
**Model** & **IT Dataset** & **Acc\({}_{eef}\)** & **F1\({}_{sut}\)** & **F1\({}_{sut}\)** & **Sim\({}_{sup}\)** & **Rec\({}_{cnn}\)** \\  LLaMA-VID  & 723K Corpus  & 32.5 & 9.2 & 16.2 & 11.9 & 4.0 \\ LLaMA-VID  & E.T. Instruct 164K (Ours) & 31.3 & 16.0 & 19.8 & 14.9 & 7.8 \\  TimeChat  & TimeIT  & 27.7 & 21.9 & 11.1 & 10.8 & 9.7 \\ TimeChat  & E.T. Instruct 164K (Ours) & 29.5 & 24.3 & 21.5 & 11.5 & **11.4** \\  E.T. Chat (Ours) & TimeIT  & 34.9 & 22.1 & 20.1 & 13.4 & 6.9 \\ E.T. Chat (Ours) & E.T. Instruct 164K (Ours) & **38.4** & **33.5** & **31.4** & **17.1** & 10.1 \\   

Table 16: **Joint effect of model and instruction-tuning (IT) dataset.**

  
**RVC** & **TVG** & **TAL** & **EVS** & **VHD** & **DVC** & **SLC** & **GVQ** & **Acc\({}_{eef}\)** & **F1\({}_{sut}\)** & **Sim\({}_{sup}\)** & **Rec\({}_{cnn}\)** \\  ✓ & & & & & & & & 36.5 & 9.8 & 0.4 & 10.3 & 0.5 \\ ✓ & ✓ & & & & & & & 36.0 & 12.8 & 3.7 & 12.5 & 5.1 \\ ✓ & ✓ & ✓ & & & & & 35.4 & 31.9 & 10.3 & 11.5 & 9.9 \\ ✓ & ✓ & ✓ & ✓ & & & & 35.6 & 32.5 & 9.5 & 11.8 & 9.5 \\ ✓ & ✓ & ✓ & ✓ & ✓ & & & & 35.9 & 33.6 & 15.1 & 10.8 & 9.7 \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & & 37.4 & 34.8 & 18.2 & 12.5 & 9.4 \\ ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & & 34.2 & **33.7** & 28.5 & 14.3 & 9.2 \\  ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & **38.4** & 33.5 & **31.4** & **17.1** & **10.1** \\   

Table 17: **Ablation study on instruction-tuning tasks.**

## Appendix A

Figure 11: **Qualitative comparison on [RavQ] (left) and [TVG] (right).**

Figure 10: **Qualitative comparison on [RAR] (left) and [ECA] (right).**Figure 12: **Qualitative comparison on [EPM] (left) and [TAL] (right).**

Figure 13: **Qualitative comparison on [EVS] (left) and [VHD] (right).**

Figure 14: **Qualitative comparison on [DVC] (left) and [SLC] (right).**

Figure 15: **Qualitative comparison on [TEM] (left) and [GVQ] (right).**

    &  &  &  &  \\   & [WI]\({}_{FT}\) & [HI]\({}_{SN0}\) & [YC]\({}_{FI}\) & [YC]\({}_{SW0}\) & [CT]\({}_{FI}\) & [CT]\({}_{SN0}\) & [BS]\({}_{FI}\) & [HS]\({}_{SN0}\) & [PT]\({}_{RTc}\) & [QT]\({}_{RTc}\) & [QE]\({}_{RC}\) \\    \\  LLLAVA-1.5  & 20.6 & 12.2 & 8.3 & 10.9 & 0.6 & 10.1 & 1.3 & 8.9 & 13.9 & 1.6 & 0.0 \\ LLVA-InternalM2  & 16.4 & 9.4 & 17.5 & 7.6 & 0.1 & 5.1 & 0.0 & 4.3 & 13.0 & 1.3 & 1.5 \\ mPLUG-Ow12  & 0.0 & 8.5 & 0.1 & 7.7 & 0.1 & 8.0 & 0.0 & 7.3 & 9.4 & 3.0 & 0.0 \\ XComposer  & 2.0 & 2.2 & 8.8 & 9.6 & 3.4 & 9.6 & 2.0 & 8.5 & 18.1 & 2.9 & 0.0 \\ Bumy-Llama-3-V  & 11.6 & 8.8 & 15.4 & 8.9 & 0.1 & 7.4 & 0.0 & 7.7 & 11.8 & 2.6 & 0.0 \\ MinCPM-V-2.5  & 9.1 & 12.3 & 3.4 & 11.3 & 1.6 & 9.9 & 1.1 & 9.5 & 1.1 & 0.3 & 0.0 \\ Qwen-VL-Chu  & 14.9 & 12.2 & 19.8 & 15.4 & 9.5 & 11.7 & 3.0 & 14.4 & 4.2 & 2.3 & 1.5 \\    \\  Video-LLAVA-1.5  & 20.6 & 12.2 & 8.3 & 10.9 & 0.6 & 10.1 & 1.3 & 8.9 & 13.9 & 1.6 & 0.0 \\ LLVA-InternalM2  & 16.4 & 9.4 & 17.5 & 7.6 & 0.1 & 5.1 & 0.0 & 4.3 & 13.0 & 1.3 & 1.5 \\ mPLUG-Ow12  & 0.0 & 8.5 & 0.1 & 7.7 & 0.1 & 8.0 & 0.0 & 7.3 & 9.4 & 3.0 & 0.0 \\ XComposer  & 2.0 & 2.2 & 8.8 & 9.6 & 3.4 & 9.6 & 2.0 & 8.5 & 18.1 & 2.9 & 0.0 \\ Bumy-Llama-3-V  & 11.6 & 8.8 & 15.4 & 8.9 & 0.1 & 7.4 & 0.0 & 7.7 & 11.8 & 2.6 & 0.0 \\ MinCPM-V-2.5  & 9.1 & 12.3 & 3.4 & 11.3 & 1.6 & 9.9 & 1.1 & 9.5 & 1.1 & 0.3 & 0.0 \\ Qwen-VL-Chu  & 14.9 & 12.2 & 19.8 & 15.4 & 9.5 & 11.7 & 3.0 & 14.4 & 4.2 & 2.3 & 1.5 \\    \\  Video-ChatGPT  & 6.1 & 10.3 & 11.5 & 12.2 & 6.3 & 8.9 & 5.1 & 11.6 & 26.8 & 5.0 & 0.0 \\ Video-LLAVA  & 42.5 & 16.2 & 13.4 & 13.9 & 1.8 & 10.0 & 0.0 & 6.5 & 10.2 & 4.7 & 0.1 \\ LLLAVA-VD  & 41.5 & 12.4 & 12.7 & 12.9 & 6.5 & 10.0 & 4.0 & 12.3 & 11.4 & 2.6 & 0.9 \\ Video-LLALA-2  & 1.2 & 9.2 & 0.1 & **19.8** & 0.0 & 14.4 & 0.1 & **16.1** & 0.0 & 0.0 & 0.1 \\ PLLAVA  & 5.6 & 10.0 & 21.0 & 11.1 & 9.3 & 10.1 & 10.2 & 13.4 & 6.9 & 1.3 & 1.2 \\ VTimeLLM  & 14.4 & 13.0 & 10.4 & 13.1 & 10.5 & 5.8 & 7.0 & 7.0 & 4.2 & 9.4 & 1.9 \\ VTG-LLM  & 45.4 & 19.3 & **35.0** & 18.0 & 20.3 & 14.1 & 21.3 & 14.7 & 14.1 & 3.7 & 1.4 \\ TimeChat  & 14.2 & 12.8 & 19.0 & 12.3 & 8.0 & 9.1 & 3.3 & 9.2 & 24.5 & 11.4 & 1.5 \\ LITA  & **47.0** & 15.9 & 32.4 & 18.5 & 21.3 & 12.1 & 20.6 & 12.3 & 20.3 & **11.7** & 2.2 \\   

Table 20: **Performance breakdown across source datasets on _dense captioning_ and _complex understanding_ tasks. Abbreviations: [HI] HiREST, [YC] YouCook2, [CT] CrossTask, [HS] HT-Step, [PT] Perception Test, [QT] QVHighlights, [QE] QA-Ego4D.**

    &  &  &  &  \\   & [WI]\({}_{FT}\) & [HI]\({}_{SN0}\) & [YC]\({}_{FI}\) & [YC]\({}_{SW0}\) & [CT]\({}_{FI}\) & [CT]\({}_{SN0}\) & [BS]\({}_{FI}\) & [HS]\({}_{SN0}\) & [PT]\({}_{RTc}\) & [QT]\({}_{RTc}\) & [QT]\({}_{RTc}\) \\    \\  LLLAVA-1.5  & 3.0 & 9.2 & 1.9 & 7.6 & 7.7 & 8.0 & 3.1 & 1.7 & 33.6 & 28.2 \\ LLVA-InternalM2  & 0.2 & 5.2 & 0.1 & 0.4 & 0.2 & 0.1 & 0.1 & 0.3 & 24.4 & 40.1 \\ mPLUG-Ow12  & 0.2 & 2.0 & 0.2 & 1.0 & 3.8 & 4.3 & 7.5 & 0.7 & 25.2 & 48.3 \\ XComposer  & 1.4 & 8.4 & 1.5 & 16.8 & 6.3 & 6.6 & 4.1 & 1.4 & 26.2 & 31.6 \\ Bumy-Llama-3-V  & 10.2 & 3.8 & 0.1 & 2.6 & 6.3 & 6.4 & 0.3 & 0.4 & 20.0 & 41.2 \\ MinCPM-V-2.5  & 0.9 & 2.2 & 0.2 & 6.0 & 3.7 & 3.5 & 17.7 & 9.1 & 14.0 & 23.4 \\ Qwen-VL-Chu  & 6.8 & 25.8 & 4.0 & 14.2 & 9.2 & 8.6 & 25.0 & 7.7 & 25.6 & 43.2 \\    \\  Video-ChatGPT  & 2.8 & 11.1 & 1.3 & 24.0 & 10.2 & 11.0 & 12.9 & 4.0 & 25.4 & 32.2 \\ Video-LLAVA  & 3.0 & 11.1 & 1.9 & 23.9 & 10.2 & 11.0

  
**Method** & **F1@@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\  _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & & & \\  LLaVA-1.5  & 29.6 & 17.0 & 7.9 & 3.3 & 14.5 & 0.9 & 1.8 & 2.1 & 11.5 \\ LLaVA-InternetLM2  & 36.1 & 20.4 & 8.3 & 3.0 & 16.9 & 1.0 & 1.6 & 1.6 & 8.5 \\ mPLUG-Owl2  & 0.2 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 8.1 \\ XCompressor  & 12.8 & 5.8 & 2.5 & 0.6 & 5.4 & 0.6 & 0.9 & 0.1 & 5.9 \\ Banny-Llamlam-V  & 32.1 & 15.2 & 5.1 & 1.8 & 13.5 & 0.9 & 1.6 & 2.2 & 8.8 \\ MinCPM-V-2.5  & 15.7 & 6.2 & 2.4 & 0.7 & 6.2 & 1.0 & 1.4 & 0.2 & 11.8 \\ Owen-VL-Chat  & 39.6 & 20.4 & 6.9 & 2.6 & 17.4 & 1.3 & 2.2 & 2.7 & 13.8 \\    & & & & & & \\  Video-LLAM: 8 & 1.2 & 0.4 & 0.2 & 1.9 & \\ Video-LLAVA  & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 & \\ LLaVA-VID  & 30. & 1.2 & 0.4 & 0.2 & 1.2 & \\ Video-LLAM-2  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLAVA  & 3.2 & 1.0 & 0.2 & 0.0 & 1.1 \\ VTimeLMM  & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ VTG-LLM  & 9.2 & 4.6 & 0.6 & 0.4 & 3.7 \\ TimeTent  & 7.8 & 4.6 & 2.2 & 0.8 & 3.8 \\ LITA  & 12.6 & 4.4 & 1.0 & 0.4 & 4.6 \\    & **21.6** & **12.4** & **5.2** & **1.6** & **10.2** \\   

Table 23: Performance under more metrics on [DVC].

  
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\  _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & \\  LLaVA-1.5  & 29.1 & 17.0 & 7.9 & 3.3 & 14.5 & 0.9 & 1.8 & 2.1 & 11.5 \\ LLaVA-InternetLM2  & 36.1 & 20.4 & 8.3 & 3.0 & 16.9 & 1.0 & 1.6 & 1.6 & 8.5 \\ mPLUG-Owl2  & 0.2 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 8.1 \\ XCompressor  & 12.8 & 5.8 & 2.5 & 0.6 & 5.4 & 0.6 & 0.9 & 0.1 & 5.9 \\ Banny-Llamlam-V  & 32.1 & 15.2 & 5.1 & 1.8 & 13.5 & 0.9 & 1.6 & 2.2 & 8.8 \\ MinCPM-V-2.5  & 15.7 & 6.2 & 2.4 & 0.7 & 6.2 & 1.0 & 1.4 & 0.2 & 11.8 \\ Owen-VL-Chat  & 39.6 & 20.4 & 6.9 & 2.6 & 17.4 & 1.3 & 2.2 & 2.7 & 13.8 \\    & & & & \\  Video-LLAM: 8 & 1.9 & 10.8 & 4.3 & 1.2 & 8.8 & 1.1 & 2.0 & 2.6 & 11.3 \\ Video-LLAVA  & 54.6 & 33.0 & 16.6 & 7.7 & 28.0 & 1.4 & 2.7 & 2.1 & 15.0 \\ ILaVA-VID  & 50.8 & 32.6 & 16.8 & 8.2 & 27.1 & 0.9 & 1.9 & 1.2 & 12.6 \\ Video-LLAM-2  & 1.2 & 0.7 & 0.6 & 0.0 & 0.6 & 0.0 & 0.0 & 0.0 & 14.5 \\ PLLAVA  & 29.3 & 15.9 & 6.1 & 2.0 & 13.3 & 1.3 & 2.4 & 3.7 & 10.6 \\ VTimeLMM  & 28.1 & 13.7 & 6.1 & 1.8 & 12.4 & 1.5 & 2.9 & 2.4 & 13.1 \\ VTG-LLM  & **81.0** & **51.6** & 22.0 & 6.2 & **40.2** & 2.8 & 5.2 & 8.5 & 18.6 \\ TimeTent  & 41.3 & 17.2 & 6.1 & 1.9 & 16.6 & 1.7 & 3.3 & 3.5 & 12.5 \\ LITA  & 78.5 & 49.2 & 23.5 & 7.6 & 39.7 & 3.3 & 5.2 & 7.6 & 17.2 \\    & 73.3 & 46.8 & **23.8** & **9.8** & 38.4 & **3.3** & **5.7** & **10.4** & **19.7** \\   

Table 21: Performance under different IoU thresholds on [T].

  
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** \\  _Image-LLAMs: 8 uniformly sampled frames as inputs_ & & & & & \\  LLaVA-1.5  & 29.1 & 1.4 & 0.7 & 0.5 & 0.0 & 6.1 \\ LLaVA-InternetLM2  & 7.4 & 3.0 & 0.4 & 0.0 & 2.7 \\ mPLUG-Owl2  & 3.0 & 1.0 & 0.4 & 0.0 & 1.1 \\ XCompressor  & 17.7 & 1.8 & 0.0 & 0.0 & 4.9 \\ Banny-Llamlamlam-V  & 24.3 & 0.2 & 0.5 & 0.0 & 7.0 \\ MiniCPM-V-2.5  & 4.5 & 2.3 & 0.9 & 0.3 & 2.0 \\ Owen-VL-Chat  & 31.4 & 19.1 & 10.4 & 4.1 & 16.2 \\    & & & \\  Video-LLAM: 8 & 1.2 & 0.2 & 0.0 & 1.3 \\ Video-LLAVA  & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ Video-LLAVA  & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ Video-LLAVA-VID  & 3.0 & 1.2 & 0.4 & 0.2 & 1.2 \\ Video-LLAM-2  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLAVA  & 3.2 & 1.0 & 0.2 & 0.0 & 1.1 \\ VTimeLMM  & 5.8 & 1.2 & 0.4 & 0.2 & 1.9 \\ VTG-LLM  & 9.2 & 4.6 & 0.6 & 0.4 & 3.7 \\ TimeTent  & 7.8 & 4.6 & 2.2 & 0.8 & 3.8 \\ LITA  & 12.6 & 4.4 & 1.0 & 0.4 & 4.6 \\    & **21.6** & **12.4** & **5.2** & **1.6** & **10.2** \\    
  
**Method** & **R@@0.1** & **R@0.3** & **R@0.5** & **R@0.7** & **Rec** \\   \\  LLaVA-1.5  & 16.0 & 9.5 & 4.2 & 1.1 & 7.7 \\ LLaVA-InternRLM2  & 14.1 & 8.9 & 4.6 & 1.1 & 7.2 \\ mPLUG-Owl2  & 12.4 & 7.3 & 3.8 & 1.1 & 6.2 \\ XCompser  & 21.9 & 13.0 & 5.4 & 1.6 & 10.5 \\ Bunny-Llama-V  & 13.8 & 9.1 & 4.5 & 1.3 & 7.2 \\ MinCPM-V-2.5  & 1.5 & 0.7 & 0.5 & 0.2 & 0.7 \\ Owen-VL-Chat  & 7.7 & 3.5 & 1.3 & 0.4 & 3.2 \\   \\  Video-Cha4GPT  & 32.1 & 18.9 & **9.6** & **2.7** & 15.9 \\ Video-LLaVA  & 16.5 & 8.6 & 3.9 & 1.0 & 7.5 \\ LLaMA-VID  & 14.7 & 8.5 & 3.7 & 1.1 & 7.0 \\ Video-LLAMa-2  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ PLLAVA  & 8.4 & 5.1 & 2.4 & 0.7 & 4.1 \\ VTimeLLM  & 16.1 & 7.5 & 2.7 & 0.9 & 6.8 \\ VTG-LLM  & 17.4 & 10.9 & 5.2 & 2.1 & 8.9 \\ TimeChat  & 38.6 & **21.7** & 8.9 & 2.7 & **18.0** \\ LTTA  & **40.4** & 15.8 & 6.2 & 1.8 & 16.0 \\   & 36.9 & 20.2 & 6.7 & 2.0 & 16.5 \\   

Table 25: **Performance under different IoU thresholds on [TEM] (left) and [GVQ] (right).**

  
**Method** & **F1@0.1** & **F1@0.3** & **F1@0.5** & **F1@0.7** & **F1** & **METEOR** & **Rouge-L** & **CIDEr** & **Sim** \\   \\  LLaVA-1.5  & 2.1 & 1.1 & 0.5 & 0.1 & 0.9 & 0.1 & 0.1 & 0.2 & 9.5 \\ LLaVA-InternRLM2  & 0.3 & 0.0 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 4.7 \\ mPLUG-Owl2  & 0.1 & 0.1 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 7.7 \\ XCompser  & 5.9 & 3.4 & 1.1 & 0.4 & 2.7 & 0.3 & 0.3 & 0.0 & 9.0 \\ Bunny-Llama-V  & 0.2 & 0.1 & 0.0 & 0.0 & 0.1 & 0.0 & 0.0 & 0.0 & 7.6 \\ MinCPM-V-2.5  & 4.3 & 0.8 & 0.2 & 0.2 & 1.4 & 0.2 & 0.2 & 0.1 & 9.7 \\ Owen-VL-Chat  & 13.7 & 7.5 & 3.0 & 0.8 & 6.2 & 0.3 & 0.4 & 0.7 & 13.1 \\   \\  Video-Cha4GPT  & 12.2 & 7.1 & 2.5 & 0.9 & 5.7 & 0.4 & 0.7 & 1.2 & 10.2 \\ Video-LLaVA  & 1.8 & 1.1 & 0.5 & 0.2 & 0.9 & 0.0 & 0.0 & 0.1 & 8.3 \\ LLaMA-VID  & 14.0 & 4.6 & 1.7 & 0.6 & 5.2 & 0.2 & 0.3 & 0.3 & 11.1 \\ Video-LLaMA-2  & 0.2 & 0.0 & 0.0 & 0.0 & 0.0 & 0.2 & 0.5 & 0.4 & **15.2** \\ PLLVAVA  & 22.2 & 11.3 & 4.3 & 1.1 & 9.7 & 0.7 & 1.1 & 2.5 & 11.8 \\ VTG-LLM  & 19.1 & 10.4 & 4.1 & 1.4 & 8.7 & 0.4 & 0.6 & 0.9 & 6.4 \\ VTG-LLM  & **50.1** & 22.3 & 8.5 & 2.3 & 20.8 & 1.5 & 2.4 & 4.3 & 14.4 \\ TimeChat  & 15.9 & 4.7 & 1.5 & 0.4 & 5.6 & 0.6 & 1.0 & 1.2 & 9.2 \\ LTTA  & 48.9 & 23.2 & 9.1 & 2.8 & 21.0 & 1.4 & 1.8 & 2.3 & 12.2 \\   & 45.8 & **28.8** & **15.8** & **7.2** & **24.4** & **2.4** & **3.2** & **6.2** & 14.6 \\   

Table 24: **Performance under more metrics on [SLC].**