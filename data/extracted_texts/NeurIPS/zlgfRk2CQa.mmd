# Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints

Jay Bear Adam Prugel-Bennett Jonathan Hare

The University of Southampton, Southampton, UK

{jdhig19,abp1,jsh2}@soton.ac.uk

###### Abstract

Iterative algorithms solve problems by taking steps until a solution is reached. Models in the form of Deep Thinking (DT) networks have been demonstrated to learn iterative algorithms in a way that can scale to different sized problems at inference time using recurrent computation and convolutions. However, they are often unstable during training, and have no guarantees of convergence/termination at the solution. This paper addresses the problem of instability by analyzing the growth in intermediate representations, allowing us to build models (referred to as Deep Thinking with Lipschitz Constraints (DT-L)) with many fewer parameters and providing more reliable solutions. Additionally our DT-L formulation provides guarantees of convergence of the learned iterative procedure to a unique solution at inference time. We demonstrate DT-L is capable of robustly learning algorithms which extrapolate to harder problems than in the training set. We benchmark on the traveling salesperson problem to evaluate the capabilities of the modified system in an NP-hard problem where DT fails to learn.

## 1 Introduction

Iteration is a key ingredient in a vast number of important algorithms. Incremental progress towards a solution is demonstrated in many of these. Well-known examples include insertion sort, gradient descent, and the simplex method. This paper explores models that learn iterative algorithms. We ask questions including: Can deep learning models be used to learn the complex steps of algorithms similar to these? Is there a way to guarantee a solution or approximation is reached? Can we learn algorithms that extrapolate to larger or harder instances than we train on? In addressing these questions we propose a model called Deep Thinking with Lipschitz Constraints (DT-L).

Some work towards answering the above questions has occurred in recent years. A key approach has been to utilize various forms of a learned recurrent function to implement a'step' in an iterative algorithm that computes a solution to a problem. Typically these recurrent functions are combined

Figure 1: Recurrent-based model architectures for learning algorithms with input \(\) and output \(\). \(\), \(\) and \(\) are convolutional networks that work on any size input. A scratchpad \(\) serves as the working memory during computation. As described in Section 2 the original DT model didnâ€™t include _recall_, denoted by the dotted line. The improved DT-R and our DT-L model include this connection.

with learned functions that pre-process the initial input, and post-process the output of the last iteration into the final solution form. Examples of such approaches are the Deep Thinking (DT) networks  and Deep Thinking with Recall (DT-R) networks  that we build upon in this work (see Figure 1).

There are two key features to these models: firstly the use of an iteratively updated _scratchpad_, \(\) serving as a working memory; and secondly, the construction of the model from convolutional layers to enable it to scale to arbitrary sized problems. The latter is important because in principle it allows training on smaller problems, and then extrapolation at inference time to larger problems [2; 18].

Many challenges still remain with the existing DT and DT-R approaches however, and we aim to address these in this work. Firstly, DT-style networks are quite difficult to train and can be very unstable both during training and inference. The DT-style networks previously demonstrated in the literature are massively overparameterized -- we show in Section 3 that model width is inversely related to training stability in existing models, and later demonstrate how this can be addressed (see Section 4). Secondly, existing approaches have no guarantees on the convergence of the learned algorithm; this is important because if a solution to a problem exists, one would hope that an algorithm to solve it would terminate or reach a stable state once the solution has been reached. Section 4 details a design approach to guarantee convergence within the limits of floating point precision by considering the network in terms of a discrete dynamical system. Our contributions are:

* We systematically analyze DT networks and explore why they can be unstable;
* we introduce techniques grounded in theory into the DT framework which improve the stability of learning and guarantee convergence at run time. This allows the use of much smaller models to tackle the same problems, and improves extrapolation performance;
* we propose our Deep Thinking with Lipschitz Constraints (DT-L) model and perform a comprehensive evaluation and ablation study; and,
* we show that the approach can be applied to learn to find low-cost tours for a range of TSPs, including non-Euclidean and asymmetric instances.

## 2 Related work

Deep Thinking (DT) networks  were designed to learn algorithms by using recurrence to induce iterative behavior in such a way they could be trained on smaller and simpler examples before extrapolating to larger tests. The networks were used to solve 'Easy-to-Hard' problems (consisting of prefix sums, mazes, and chess puzzles ) and they could be trained with few iterations on easy problems in such a way that they can solve harder problems by increasing the number of iterations.

DT networks were further improved by Bansal et al.  by means of adding'recall' (which we refer to as DT-R networks) -- a mechanism for the recurrent component to have continuous access to the original input. Recall successfully allowed DT-R networks to solve much larger tests than DT networks without recall, while also mitigating _overthinking_, where if after too many iterations at inference time the predicted solution becomes progressively worse. In addition to recall, Bansal et al. introduced _incremental progress training_ (IPT), a training method which disables gradient tracking for a random number of initial iterations. This prevents the model from learning behaviors based strictly on the number of iterations and instead promotes incremental modification to the internal states.

There exist alternative approaches to producing machines which can learn algorithms. One set of models are Neural Turing Machines (NTMs), which use attention in a recurrent system for reading to, and writing from, some external memory . NTMs showed success in learning cell-based copying and repeating tasks which could be applied to unseen inputs. Differentiable Neural Computers (DNCs) operate similarly to NTMs, differing primarily in their method of external memory access . In the NTM, like most RNNs, the amount of compute is directly tied to the input sequence length. Graves  proposed a method to adaptively select the compute budget in RNNs. The models studied here are different in the sense that the objective is to extrapolate beyond the training data to harder problem instances as the compute budget is increased, and the input and output is not a sequence.

A second class of models exist where a specific base iterative algorithm is defined, and the parameters of the algorithm, or the problem, are learned. Examples include the reverse diffusion process learned in diffusion models , and models which involve optimization using gradient descent iterationswithin the forward pass to generate an output with certain properties that are difficult to produce directly with a neural network, such as permutation equivariance . Our work does not constrain the class of algorithm used directly, other than enforcing that we learn an iterative one.

## 3 Analysis of Deep Thinking Networks

The main architecture used in pre-existing Deep Thinking networks [2; 18], and our own network, is shown in Figure 1. It takes an input problem instance \(\) and generates a solution \(\). The model consists of three convolutional networks \(\), \(\) and \(\). The function \(\) is responsible for initially pre-processing the input \(\) into the initial state \(^{(0)}\); function \(\) is the recurrent function that takes the current state, \(^{(m)}\) (plus the original input \(\) in the case of DT-R and our models), to produce the next state, \(^{(m+1)}\). The final state produced by \(\), after \(M\) iterations, is denoted \(^{(M)}\). The function \(\) takes \(^{(M)}\) and produces the predicted output \(\). Formally

\[^{(0)} =()\,\] (1) \[^{(m+1)} =^{(m)}, m \{0,,M-1\}\,\] (2) \[ =^{(M)}\.\] (3)

Architecturally these networks are recurrent neural networks. Unlike more commonly used recurrent networks such as LSTMs they are not used to tackle problems with sequential data, but rather the recurrent part is used to find a solution through an iterative process. The _algorithm_ that the recurrent network uses, as well as the pre- and post- processing functions are learned through supervised training on example input-solution pairs for a particular problem.

As the networks \(\), \(\) and \(\) are convolutional neural networks they can work with an arbitrary size input. The solution \(\) is typically the same size as the input \(\). The feature of DT and DT-R that excited interest was that, not only could they solve unseen problem instances of the same size as they were trained on (which we call _interpolation_), but when trained on small problem instances (with relatively small \(M\)), they were able to find low cost solutions on much larger problem instances (which we refer to as _extrapolation_) by increasing \(M\) at inference time. Both DT and DT-R suffer from poor stability both in training but particularly at inference time when extrapolating to larger problems. This often lead to overflow errors. This was particularly seen if the number of channels in network \(\) is reduced, then DT and DT-R are hard to train with many runs failing to find a solution. We show the reason for this is that there is no mechanism to control the change in size of the scratchpad representation, \(\|^{(m)}\|/\|^{(m-1)}\|\), leading to this either overflowing or vanishing during learning.

### Training Stability

In this section we focus on DT-R as this is more stable than the original DT network. We use the _spectral norm_ of the reshaped weights of a convolution1 to capture the expansion or shrinkage in magnitude of the output relative to the input of a sub-network, with a value of 1 meaning that the

Figure 2: Distribution of spectral norms of reshaped weight matrices for the different convolutional layers in the recurrent part of DT-R. 30 prefix-sum-solving models with width \(w=32\) were sampled.

magnitude is constant. In Figure 2, we use violin plots to show the distribution of spectral norms for the four convolutional layers that arise after training DT-R on the prefix-sum problem (see Section 5 for a description of the problem). We observe that the spectral norms are typically greater than one, which can lead to the norm of \(^{(m)}\) growing with each iteration. In turn this can cause the model to overflow when applying the model to a larger input (the extrapolation scenario), where we increase the iteration number \(M\) in order to solve the larger instance size. The DT models as described and implemented by Bansal et al.  have training behavior which becomes increasingly unpredictable as the width \(w\) (number of channels in the scratchpad) is reduced. This can be seen in Figure 3, where the variation in training loss at the end of training over different runs is larger in models of smaller width. It is also worth noting that increasing the width can result in explosive behavior in loss, including not-a-number (NaN) results, which can be seen in Figure D4.

### Extrapolation Performance

The property of the DT networks that excited our interest was their ability to solve large instances than they were trained (that is, their extrapolative ability). This generalization to out-of-distribution examples is a key challenge for many machine learning algorithms. However, although DT-R can find models that extrapolate, very often the models that are trained extrapolate poorly. We observe models trained on few recurrences may have explosive behavior which only becomes a problem with extrapolative tasks requiring more recurrences.

Both DT and DT-R network used no bias terms in their model [2; 18]. In experimenting with these networks, we found that adding bias made these models very unstable. Unfortunately, having no biases exacerbates the 'dead cell problem', where some of the convolutional filters would have a zero response to all training inputs.

## 4 Refinement of the Architecture: Deep Thinking with Lipschitz Constraints

In the previous section we have identified a problem faced by DT-R, namely that there is instability in reaching a solution through the iteration in Equation (2). To overcome this difficulty we use a well known property of contraction maps. If \(c:\) is a mapping of objects, \(\) and \(\), in a normed vector space \(\), that is contractive in the sense that

\[\|c()-c()\|<\|-\|\,\] (4)

implying \(c()\) is a Lipschitz function with Lipschitz constant \(K<1\), then the iterations

\[^{(m)}=c(^{(m-1)})\] (5)

will converge to a unique solution as a consequence of the Banach fixed-point theorem [1; 3]. We can use this to refine DT-R by engineering the network \((,)\) to be a contraction mapping. Noting that in Equation (2) the input \(\) is held constant throughout the iteration, we construct \((,)\) so that it has an _approximate_ Lipschitz constant \(K\) less than 1.

Figure 3: Mean training (cross-entropy) loss at each epoch for prefix-sums-solving models of varying width \(w\). For small \(w\) training is stable, but not all models converge; larger \(w\) has a higher chance of models reaching a small loss, but the training process has very large spikes in the loss which causes some models to explode. Each curve is measured from a different random initialization of the model throughout training, for 10 models of each width

Although any Lipschitz constant less than 1 would guarantee convergence, the nature of the problem solving mechanism we seek to learn intuitively means that we do not want fast convergence. This is because the network \(\) performs local convolutions with a limited receptive field. However, finding good quality solutions often requires a global knowledge. To accumulate such knowledge requires long distance information to accumulate in the scratchpad vector \(^{(m)}\) over multiple iterations. To allow this to happen we choose the approximate Lipschitz constant associated with \(\) to be less than, but close to 1.

### Constraining the Lipschitz Constant

The primary tool we use to control the Lipschitz constant of \((,)\) is spectral normalization . This is an extension of spectral norm regularization, introduced by Yoshida and Miyato , but it sets the spectral norm of an operator to a fixed value rather than penalizing the norm of an operator that differs from a predefined value. The method uses a power iteration to compute an approximation of the spectral norm of an operator, updated after each gradient step. Computation of the spectral norm is a costly operation, but it only has to be done at the beginning of the iterative process.

As network \(\) consists of convolutions we divide each convolution kernel weight by the spectral norm plus a small constant \(\) to ensure the spectral norm is less than one; there is one exception in that the convolution applied to the recall connection \(\) does not need normalizing -- please see Appendix B for full details. In addition to using the spectral norm we need to make sure all the other transformations carried out by \(\) are at most 1-Lipschitz. However, to avoid applying constraints to the constant recall connection, we replace DT-R's method of applying a single convolution on the concatenation \([^{(m)},]\) with two separate convolutional layers on \(^{(m)}\) and \(\) respectively whose output is combined by element-wise addition. See Appendix A for a rigorous discussion on how the Lipschitz behaviour we require is ensured.

Constraining Activation Functions.To guarantee convergence under the constraints provided, any activation function used in \(\) must be 1-Lipschitz. This includes Rectified Linear Units (ReLUs), Exponential Linear Units (ELUs) , and \(\), but excludes Gaussian Error Linear Units (GELUs) , which have an absolute gradient greater than 1 around \(x=\). Results in the body of this paper use the ELU activation for DT-L networks and the ReLU for DT-R as originally defined. Additional results using ReLU for DT-L and ELU for DT-R can be found in Appendix D.2.

Constraining Residual Skip Connections.DT and DT-R use element-wise addition for residual connections in recurrent blocks. The sum of two functions \(:X Y\) and \(:X Y\) applied to the same input, with Lipschitz constants \(K_{}\) and \(K_{}\) respectively results in the upper bound,

\[\|((_{1})+(_{1}))-((_{2})+ (_{2}))\|(K_{}+K_{})\|_{1}-_{2}\|\] (6)

for all \(_{1},_{2} X\). In the case of a standard residual connection, one function is the identity (\(()=\)) and the other is the block of layers \(\) contained in the span of the residual connection. Under our constraints, the identity is 1-Lipschitz and the block of layers is \(K_{}\)-Lipschitz, where \(K_{}[0,1)\). This results in a Lipschitz upper bound for the output of \(1+K_{}\).

As a result, residual connections as addition between the identity and a block of layers can increase the Lipschitz constant of the recurrent part even if the layers themselves are 1-Lipschitz. A solution to this, which also allows more expression in the model, is to make each residual connection a parametric linear interpolation between the identity output and the block output

\[(1-)\ ()+\,(),\.\] (7)

DT-L applies this interpolation to each channel, \(c\), individually; an unconstrained learnable parameter \(_{c}\) exists for each channel in each residual connection, then the residual parameters are set to \(_{c}=(_{c})\) where \(\) is the logistic function to ensure that \(0_{c} 1\).

### Additional Modifications

The above changes leads to a more stable network, allowing us to make additional modifications and still obtain convergent behaviour with improved performance. Without explicitly controlling the Lipschitz constant, these changes often lead to complete failure of the network to solve the problem. In our final model design we apply three extra modifications that lead to consistent improvements:1. Batch normalization layers  are added for the input and after each convolution, except for layers in the recurrent block and the final output layer. Empirically this improves performance.
2. A bias term is added to the recall convolution (without the Lipschitz constraint) in \(\). The bias term is added to the recall convolution to mitigate the dead cell problem. Since this is a constant addition (along with recall) it does not disrupt the convergence constraints we have put in place (see Appendix A). A bias term is also added to the final (output) layer and, when batch normalization is not used, to \(\).
3. Exponential Linear Unit (ELU) activations  are used instead of rectified linear unit (ReLU) activations. This choice is influenced by the desire to promote activations staying close to zero  throughout iteration, as well as mitigating the effect of the dying ReLU problem  where 'dead' activations increase with depth . Artificially deep models like DT are therefore likely to encounter this problem. Using ReLU can also result in a rapid rate of convergence, preventing the model from learning complex algorithms for harder tasks (see Appendix D.2).

Our reasoning for not applying batch normalization in the recurrent block follows from Jastrzebski et al.  where unsharing batch normalization statistics was necessary to avoid activations exploding. In an architecture where the maximum number of iterations is unknown it appears infeasible to unshare batch normalization statistics for every iteration.

Using these modifications -- creating a model we call Deep Thinking with Lipschitz Constraints (DT-L) -- greatly improves the stability of training and using these networks. As we will see in Section 5, the new DT-L network allow us to solve the same problems explored by DT and DT-R, but using, in some cases, three orders of magnitude less parameters while achieving high performance and a more consistent success rate during training. The increased reliability offered by DT-L allows us to explore many other modifications enabling us to tackle more sophisticated problems. We illustrate this by attempting to find good solutions to one of the most notoriously difficult problems, namely, the traveling salesperson problem (TSP).

## 5 Results on Easy-to-Hard Problems

In this section, we compare our model (DT-L) against DT-R. Note that DT-R is an improvement of DT by the same authors and has already been shown to have better performance . We test on the three problem classes used by Bansal et al.  to evaluate DT-R, namely a **prefix sum problem**, a **maze problem** and a **ches problem**.

We have trained and evaluated the models on a range of different Nvidia GPU accelerators from RTX2080Tis to A100s, as well as on M3-series Apple Silicon. Memory usage is insignificant compared to available GPU memory. Training time is of the order of 30 minutes for the \(w=32\) model on the prefix-sum problem using a single RTX8000. More details can be found in Appendix E, and code for the experiments can be found at https://github.com/Jay-Bear/rethinking-deep-thinking.

The accuracy given in this section measures the proportion of instances where the network produces the exact correct solution. Note that even if a predicted solution differs from the target by one element (e.g. 1 bit in prefix sums, 1 pixel in mazes) it is considered a failure.

Prefix Sums.The prefix sum problem involves translating a string of ones and zero to a new string that counts sequences of ones (details are described by Bansal et al. ). The problem is simpler than the the maze and chess problem, and considerably faster to run. As a consequence we choose this as the main problem to perform a comparative study.

In our experiments we compare the DT-R model to our DT-L model. Both were trained on instances consisting of 32 bits for a maximum of 30 iterations, using IPT with \(=0.5\). Both models have a width (number of channels) of \(w=32\) (more details about architecture can be found in Appendix B.1). We found that for this problem there is little change in performance above \(w=32\) for DT-L. In Figure 4 we show the solution accuracy for 30 randomly-initialized models of both DT-R and DT-L on the 512-bit test dataset versus the number of iterations (\(M\)) to generate those solutions. We measured the performance on 10 000 randomly generated instances of each problem. The error in 

[MISSING_PAGE_FAIL:7]

The solution vector \(\) is a binary matrix with 1s corresponding to the edges that are used in the tour. We constructed the network so that the solution vector would correspond to a feasible solution. Unfortunately, in doing so involves an intermediate representation where the evaluation of the cost is non-trivial, which made the problem of learning an iterative solution very challenging. To overcome this, we increased the expressiveness of the scratchpad vectors \(^{(m)}\) to learn orthogonal transforms that it applied to a part of the scratchpad vector. This considerably improved the performance of the network. Details of the modification used are given in Appendix B.1.

### TSP Results

In Table 1 we give the mean tour length for randomly generated instances from two class of problems: symmetric random tours and asymmetric random tours. The DT-L models were trained for each problem class on tours of size \(n=15\) with \(M=45\). We show both interpolative results (using a new set of tours on the same size problems) and extrapolative results where we test of problems of size \(n=30\) with \(M=120\). For comparison we provide the length of random tours, the length of tours generated by the greedy nearest neighbor (NN) algorithm, and the length of tours from a modified version of the NN algorithm which selects the lowest-cost NN tour out of all starting points instead of starting from a random point (BNN).

The results in Table 1 show DT-L's ability to learn non-trivial algorithms by performing considerably better than random tours. That the results are worse than the nearest neighbor methods should not be surprising, as in the asymmetric non-Euclidean case these are amongst the best known algorithms.

The models take approximately 8 hours to train on an RTX2080Ti GPU for 80,000 batches of 64 randomly generated TSPs for both symmetric and asymmetric settings with \(n=15\) and \(M=45\)

Figure 5: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the mazes problem for small models. Two left plots show the solution accuracy of inference-time runs on \(33 33\) mazes for 14 different models each. The right plot shows the mean of all 14 for each. Models have a channel width of \(w=32\). Shaded areas show 95% confidence intervals.

Figure 6: Comparison between Deep Thinking with Recall and Deep Thinking with Lipschitz Constraints on the chess puzzles problem on models of two different widths, showing the mean accuracy of inference-time runs on problems ranked in increasing difficulty between 1,000,000 and 1,100,000. Aggregate of six models, each trained on the easiest 600,000 problems, where shaded areas show 95% confidence intervals.

Testing was performed on M3 Apple Silicon with MPS acceleration and takes 3.59ms per \(n=15\) problem instance for \(M=45\) and 23ms per \(n=30\) problem with \(M=120\).

## 7 Discussion

Deep Thinking-style architectures provide a new paradigm for using machine learning for general problem solving. The key idea is to use a recurrent architecture to find a solution through multiple iterations. This paper has addressed a major drawback of the published models which is the instability that frequently arises in the iteration steps. Having addressed these problems not only are we able to obtain networks that much more reliably solve new problems, but we show that we can run much smaller networks with similar and often better performance. To illustrate the potential of this approach we have tackled a notoriously difficult problem, namely TSP.

Deep Thinking approaches are attractive because they learn a general problem solving strategy that can be used to solve considerably larger instances of the problem than they were trained on. Extrapolation to problems outside the training dataset is an area where traditional machine learning struggles. Clearly, the problems these models extrapolate to are in the same problem class to the training data, but it is noteworthy that the strategies learned for the problem classes we have investigated scale in this way. Another interesting feature is that the network learns the problem solving strategy through examples (in the case of TSP it is not even shown any low cost solutions). Admittedly, for TSP we needed to construct a network that outputted tours and to get the quality of results we did we introduced a mechanism for learning orthogonal transformation which we then applied to part of the scratchpad vector. However, we did not build in any explicit rules for solving the TSP -- so much so that we do not fully understand the algorithm DT-L uses to find low cost tours, particularly in the case of asymmetric non-Euclidean TSP.

Broader impact.Given the ubiquity of iterative algorithms in solving problems, having a mechanism to learn such algorithms opens up a lot of possibilities. The advantage of using a recurrent mechanism over a feed-forward architecture is that to solve a larger problem we simply needed to run the recurrent loop more often; this is often desirable in the real world because it might be impossible to train on sufficient data that you can guarantee not to need to extrapolate at inference time. Clearly, models of this kind have very many potential applications - both in developing _new_ (and potentially improved -- could we learn an algorithm that is significantly faster or more energy efficient than anything currently existing?) algorithms for existing problem classes where we already have solutions, as well as in all areas of predictive modelling where we believe the input-output relationship is best captured through a potentially-deep recurrent function. Inevitably, some of the potential uses also have the potential for misuse.

Limitations.This work shows theoretically and empirically that it is possible to modify an architecture to learn recurrent functions with convergence guarantees. The problems we solve are relatively simple however. The performance we obtain, for example, on TSP is far from state-of-the-art, but we believe that the contribution is significant as we are able to find a heuristic algorithm running in \(M=O(n\,(n))\) steps through trial and error, without any explicit inductive bias towards finding what we might consider to be a sensible algorithm. There are also clearly other potential issues with the approach; learnability is significantly improved over previous attempts, but there are still cases

  Problem & DT-L & Random Tours & NN Tours & BNN Tours \\  Symmetric \(n=15\) & \(3.99 0.006\) & \(7.5 0.01\) & \(2.85 0.005\) & \(2.31 0.005\) \\ Symmetric \(n=30\) & \(6.02 0.007\) & \(15.0 0.01\) & \(3.50 0.005\) & \(2.72 0.004\) \\ Asymmetric \(n=15\) & \(4.66 0.008\) & \(7.5 0.01\) & \(2.82 0.006\) & \(2.09 0.004\) \\ Asymmetric \(n=30\) & \(7.7 0.01\) & \(15.0 0.01\) & \(3.50 0.006\) & \(2.53 0.004\) \\  

Table 1: Results for TSP runs on symmetric and asymmetric instances. The results are the mean for 12,800 random instances. For DT-L, \(M=45\) was used for problem size \(n=15\) and \(M=120\) for \(n=30\), where tour lengths are shown in column 2. Column 3 (Random Tours) gives the tour length for random tours, while columns 4 and 5 give the tour lengths for greedy nearest neighbor tours starting from a random point (NN) or choosing the lowest NN tour from all starting points (BNN) respectively.

where the model fails to learn. The use of convolutions means that the only way to capture long-term (or large distance) dependencies in the data is through iteration; this is possibly a benefit, but other architectures with different structural biases might work better on certain problems. We believe, in terms of comparison to NN and BNN heuristics, this local view of the distance matrix may contribute to the disappointing appearance of the results.

Outlook.This work leaves many open directions for future research. It would be interesting to explore different architectures, such as Transformers, and more diverse problem settings. It would also be fascinating to better understand what algorithms are learned by the model under different settings and whether for example the complexity of the learned algorithm can be controlled.