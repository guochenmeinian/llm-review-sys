# Scalable Membership Inference Attacks

via Quantile Regression

 Martin Bertran \({}^{}\)

Amazon AWS AI/ML

&Shuai Tang \({}^{}\)

Amazon AWS AI/ML

&Michael Kearns

University of Pennsylvania

Amazon AWS AI/ML

&Jamie Morgenstern

University of Washington

Amazon AWS AI/ML

&Aaron Roth

University of Pennsylvania

Amazon AWS AI/ML

&Zhiwei Steven Wu

Carnegie Mellon University

Amazon AWS AI/ML

###### Abstract

Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many _shadow models_--i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.

We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly "black-box". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures. Our code is available at github.com/amazon-science/quantile-mia.

0

## 1 Introduction

The basic goal of privacy-preserving machine learning is to find models that are predictive on some underlying data distribution, without being disclosive of the particular data points on which they were trained. The simplest kind of attack that can be launched on a trained model--falsifying privacy guarantees--is a membership inference attack. A membership inference attack, informally, is a statistical test that is able to reliably determine whether a particular data point was included in the training set used to train the model or not.

Almost all membership inference attacks are based on the observation that models tend to overfit their training sets in different ways. In particular, they tend to systematically predict higher confidence in the true labels of data points from their training set, compared to points drawn from the same distribution not in their training set. The confidence that a model places on the true label of a data-point is thus a natural test statistic to build a membership-inference hypothesis test around. Avariety of recent methods (Shokri et al., 2017; Long et al., 2020; Sablayrolles et al., 2019; Song and Mittal, 2021; Carlini et al., 2022) are based around this idea, and aim to estimate the distribution of the test statistic (the confidence assigned to the true label of a datapoint) over the distribution of datapoints that were _not used in training_ (and sometimes, also over the distribution of datapoints that were used in training) for the purpose of designing tests that can reject the null hypothesis--that a data point under attack was not used in training--with the desired level of confidence.

The efficacy of this class of attacks depends in large part on the granularity to which the distribution of the test statistic can be estimated. The simplest (and most computationally efficient) approach, originally proposed by Yeom et al. (2018), is to estimate this distribution _marginally_ -- i.e. without conditioning on the covariates \(x\) of the example being attacked. This reduces the problem to a simple one-dimensional estimation problem, and--under mild assumptions--the optimal hypothesis test (by the Neyman-Pearson Lemma) is simply a fixed threshold \(\) on the test statistic--examples are declared to have been used in training if the confidence the model places on their true label exceeds \(\), and are declared to have been not used in training otherwise. More sophisticated methods attempt to estimate the distribution of the test statistic conditional on the inclusion of a target point \(x\) in the training data (over the randomness of the selection of the other points used in training). Our method and others follow this approach, where the confidence score produced by each example \(x\) by the target model is compared to a sample-dependent threshold \((x)\)-- points \(x\) with scores exceeding this threshold are declared to be used in training. The most common method under this approach is to train _shadow models_(Shokri et al., 2017; Long et al., 2020; Sablayrolles et al., 2019; Song and Mittal, 2021; Carlini et al., 2022). Informally, shadow models are trained with the same architecture as the model being attacked, using random subsets of training data, that either include or do not include the target point \(x\). As a result, each shadow model gives a sample of the test statistic conditional on \(x\)'s inclusion (or non-inclusion) in the training set, where the randomness is over the _other examples_ in the training set and any randomness involved in training. Because many samples from this distribution on the test statistic are needed to estimate it, membership inference attacks based on shadow models generally require training many shadow models of the same architecture as the model under attack; between 64 and 256 shadow models were used in Carlini et al. (2022) (Figure 1 compares the receiver operating characteristic (ROC) of the attack on ImageNet against our proposed approach). Especially for large models, this makes shadow model attacks prohibitive, for at least two reasons:

1. **Training Cost**: Widely used commercial models, on which membership inference attacks would be most damaging, are extremely large and expensive to train. An attacker launching a membership inference attack based on shadow models must train many (dozens to hundreds) models of the same architecture. Thus the computational costs can be hundreds of times larger than the costs of training the model under attack, which for commercial models is prohibitive for attackers without enormous resources.
2. **Knowledge of the Model Under Attack:** As argued in Carlini et al. (2022), and also shown in Appendix A, when the shadow model is of the same complexity as or more complex than the target model, the LiRA attack performs well, and when less complex shadow models are deployed, the success rate of the attack drops precipitously. Hence the success of the attack depends on knowledge of the model under attack. But many aspects of the architecture and

Figure 1: Comparing the true positive rate vs. false positive rate of our membership inference attack with the marginal baseline proposed in Yeom et al. (2018) and the state-of-the-art LiRA proposed in Carlini et al. (2022) evaluated at 2, 4, 6, and 8 shadow models. We also provide a visual readout of their \(64\) shadow model results, as reported in their paper (we did not have the compute necessary to reproduce this). We faithfully replicated LIRAâ€™s attack setup and produced better results than their reported values. Our single-model quantile regression attack can reliably identify training samples on a ResNet-50 ImageNet target model (\(67.5\%\) test accuracy) without knowledge of the target architecture.

training process for large commercial models are not publicly known, making this style of attack less effective in realistic settings.

### Our Results

We introduce a new class of membership inference attacks, based on quantile regression, that is able to mitigate these issues:

1. Like attacks based on shadow models, it is a _conditional_ attack, subjecting different examples \(x\) to different thresholds \((x)\). However,
2. It only requires training a single model, and
3. The architecture of the model used in the attack need not be related to the architecture of the model under attack, and so no knowledge of the model architecture or training algorithm used to train the model under attack is needed.

**Our quantile regression attack**. Given a model \(f\) that we intend to attack, we collect a dataset of labelled examples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) from the underlying data distribution, known to not have been used in training.1 For each example \((x_{i},y_{i})\) in our dataset, we evaluate the model \(f\) on example \(x_{i}\), and record the (real-valued) confidence score \(s(x_{i},y_{i})\) that the model places on the correct label \(y\). We then train a quantile regression model \(q\) on the dataset \(\{(x_{i},s(x_{i},y_{i}))\}\) consisting of examples \(x\) labeled with their confidence scores \(s(x,y)\). Informally, the quantile regression model is trained to predict \(q(x)\), a target \(1-\) quantile of the conditional distribution on \(s(x_{i},y_{i})\), given \(x_{i}\).2 Intuitively, a score \(s(x_{i},y_{i})\) larger than the \(1-\) quantile \(q(x_{i})\) indicates that \(f\) assigns a confidence on the true label that is higher than a \(1-\) fraction of the examples not used in training -- giving us evidence that the example in question was part of the training set. Thus, given a new target point \((x,y)\), we _reject the null hypothesis_ that \(x\) was not used in training (i.e. we declare \(x\) to have been used in training) whenever \(s(x,y)\) exceeds \(q(x)\) -- i.e. when \(s(x,y) q(x)\). Similarly, whenever \(s(x,y)<q(x)\), we do not reject the null hypothesis, and declare the point \((x,y)\) to have not been used in training.

This attack by design has a false positive rate of \(\)-- the probability that it incorrectly declares a randomly selected point \((x,y)\) that was not used in training to have been used in training is \(\). The ability of \(q\) to correctly label those examples used in training as such, measured by its true positive rate or precision or related statistics, will vary with \(\) (the higher \(\), the larger the number of positive labels our test will assign). So, in varying our target \(\), we can sweep out our test's tradeoff between false positive and true positive rates.

The primary strength of our attack is that we need only a _single_ quantile regression model \(q\), rather than a large number of shadow models. Furthermore, because the success of our attack depends only on how well \(q\) predicts the quantiles of the confidence score distribution of \(f\) (rather than producing confidence scores drawn from the same distribution as \(f\)), \(q\) need not have any relationship to the architecture of \(f\) or any knowledge of it-- the only access to \(f\) that is needed is the ability to evaluate confidence scores \(s(x,y)\) produced by \(f\) given examples \(x\). Our attack is, therefore, more "black-box" than those which use shadow models of the same architecture as \(f\).

We derive a basic theory for our approach based on quantile regression, which trains a model to predict quantiles by minimizing _pinball loss_. We run an extensive series of experiments and find that our quantile regression approach is competitive with (and sometimes more effective than) much more computationally expensive shadow model approaches. The relative effectiveness of our approach appears to grow the more complex the classification task and model under attack are. For example, when attacking a ResNet-50 model trained on ImageNet-1k, our attack (which trains only a single model) outperforms shadow model approaches trained much more expensively at every false positive rate. On simpler and less data rich tasks (like CIFAR-10), the accuracy of our approach dominates the marginal baseline of Yeom et al. (2018), but falls short of shadow model approaches. Thought provokingly, however, we find that when this occurs, it is because the shadow model approach hasfound thresholds that correspond to a quantile model with lower pinball loss than our trained quantile regression model. This suggests that our fundamental approach of pinball loss minimization is sound, and that our attempts to directly optimize for it are less successful when data is less plentiful. Across all experiments, we find that the best quantile regression method (as measured by pinball loss) is uniformly the best membership inference attack.

### Additional Related Work

Starting with the seminal work of Homer et al. (2008), membership inference has become one of the most widely studied classes of privacy attacks. Most approaches for membership inference determine whether an example is part of the training set via some score function, which can be loss Yeom et al. (2018); Sablayrolles et al. (2019), confidence Salem et al. (2018), entropy Song and Mittal (2021), or difficulty calibration Watson et al. (2021) among others. Another common approach is to query the model on similar or related examples to the target point Wen et al. (2023); Jayaraman et al. (2020); Long et al. (2018); Li and Zhang (2020).

We focus the remainder of our discussion on related work on shadow model approaches to membership inference since they are our main benchmarks. Most work on shadow models considers a setup where there is a private dataset \(D^{}\) (unknown to the attacker) drawn from a distribution \(Q\), and an algorithm \(\) for training a model \(f=(D^{})\). The attacker has access to a set of data \(D^{}\) drawn from the same distribution \(Q\) and partial query access to the model \(f\) from which the attacker can compute scores \(s(x,y)\) given target examples \((x,y)\). The attacker aims to predict, for a data point \((x,y)\), whether \((x,y) D^{}\).

Likelihood Ratio Attacks.Membership inference attacks are fundamentally hypotheses tests between two competing hypotheses (\(H_{0}:(x,y) D^{}\), \(H_{1}:(x,y) D^{}\)). By the Neyman-Pearson lemma Neyman and Pearson (1933), the optimal hypothesis test based on a test statistic \(s(x,y)\) computes the likelihood ratio of the score under the null and alternative hypothesis, and subjects the likelihood ratio to a threshold \(\). The choice of the threshold \(\) determines the trade-off between precision (the fraction of examples labeled as belonging to the private dataset which did belong to the dataset) and recall (or true positive rate) of the resulting classifier. We call a membership inference attack carried out with this classifier a likelihood ratio attack (LiRA), introduced by Carlini et al. (2022). LiRA was designed to achieve very high precision (very few false positives relative to the number of positive predictions), as they noted high precision corresponds to a high degree of confidence that the data points accused of being part of the training set were, in fact, part of the training set. Prior work had looked at global notions of inference attack quality, at possibly much lower degrees of precision Ye et al. (2021); Jayaraman et al. (2021).

The main difficulty with implementing LiRA directly is that the density functions of the score under the null and alternative hypothesis are unknown. Instead, the literature aims to estimate these density functions, primarily by training a collection of _shadow_ models Shokri et al. (2017); Long et al. (2020); Sablayrolles et al. (2019); Song and Mittal (2021); Carlini et al. (2022). Shadow model attacks split the attacker's dataset \(D^{}\) into several pairs of shadow public/private datasets \(D^{}_{i},D^{}\), and for each of these shadow datasets, a shadow model \(f_{i}\) is trained on \(D^{}_{i}\). The shadow model \(f_{i}\), and corresponding datasets \(D^{}_{i},D^{}_{i}\) are used to generate private and public score samples \(s^{}_{i},s^{}_{i}\) from which to estimate the likelihood ratio function given parametric assumptions. Carlini et al. (2022) used a large number of shadow models to achieve high precision. This approach works well--but it is computationally demanding because it requires training many shadow models.

## 2 Preliminaries

We study attacks on models \(f\) that solve a supervised learning problem defined over a distribution \(()\) of labeled examples \((x,y)\), consisting of _feature vectors_\(x\) and labels \(y\). We make no assumptions about \(\) or \(\) (e.g. \(\) could be a discrete set in a multi-class classification problem, or we could have \(=\) in a regression problem). We assume that the model \(f\) outputs a _confidence score_ in \(\) for each possible label \(\): in other words, \(f:^{}\), and for each \(\), we write \(f_{}(x)\) for the confidence score that \(f\) assigns to label \(\) given input \(x\). Such models are often used to make point predictions by predicting the label \(=_{y}f_{y}(x)\) on input \(x\) -- but we will interact with such models \(f\) only at the level of confidence score predictions.

A model \(f\) is derived from a _training process_ that did not have direct access to \(\), but rather to a finite sample \(D^{}\) called the training set. The training process correlates \(f\) with \(D^{}\). A membership inference attack is a hypothesis test that must use a test statistic derived only from \(f\) that aims to determine whether a labeled example \((x,y)\) is a member of the training set \(D^{}\) or not. Formally, we model this as a hypothesis test that aims to solve the following simple hypothesis testing problem:

\[H_{0}:(x,y) H_{1}:(x,y) D^{}\]

Here, \((x,y) D^{}\) denotes sampling a point \((x,y)\) uniformly at random from \(D^{}\). Observe that since we derive the test statistic from \(f\), even if \(D^{}\) was itself sampled i.i.d. from \(\), \(H_{0}\) and \(H_{1}\) are distinct hypotheses since the training process has correlated \(f\) with \(D^{}\). In particular, we will base our attack on the presumption that \(f\) will tend to be over-confident on examples \((x,y) D^{}\). Towards this, we choose as our test statistic \(s(x,y)=z_{y}(x)-_{y^{} y}z_{y^{}}(x)\) the logit difference between the true label and its most likely alternative, where \(z(x)\) denotes the logits (unnormalized features before the softmax nonlinearity) of the model. This choice follows the scoring rule used in Carlini et al. (2022) and will be useful for experimental comparisons. However, the remainder of our theoretical treatment will be agnostic as to this choice.

In this paper we restrict attention to membership inference attacks (hypothesis tests) that apply a threshold function to \(s(x,y)\), with a threshold that may depend on \(x\). Given a function \(q:\) that maps examples \(x\) to thresholds, the corresponding membership inference attack is given by:

\[_{q}(x,y)=&(x)s(x,y)<q(x)\\ &(x D^{})s(x,y) q(x).\]

Here \(\) is shorthand for "We reject the null hypothesis that \((x,y)\) (and thus declare \((x,y)\) of being in the training set)", and \(\) is shorthand for "we do not reject the null hypothesis (and thus do not accuse \((x,y)\) of being in the training set)".

A natural baseline is to set \(q(x,y)=\) to be a constant. This is the attack proposed by Yeom et al. (2018), and we write this baseline as \(_{}\). If \(\) is set to be a \(1-\) quantile of the marginal distribution on \(s(x,y)\) when \((x,y)\), then this attack can easily be seen to have _false positive rate_\(\). Below we define quantiles assuming (for simplicity) that the distribution in question is continuous--but it is also possible to define quantiles without this assumption.

**Definition 1**.: _Fix a continuous distribution \(\). A number \(\) is a \((1-)\)-quantile of \(\) if:_

\[_{s}[s]=1-\]

We can evaluate the performance of a membership inference attack by evaluating its false positive rate, true positive rates, and precision3:

**Definition 2**.: _Fix an arbitrary membership inference attack \(:\{,\}\). We define the following performance metrics_

\[()=_{(x,y)}[(x,y)=], ()=_{(x,y) D^{}}[(x, y)=],\]

\[()=()}{()+()}.\]

It is immediate that the baseline membership inference attack achieves its target false positive rate; the true positive rate and precision of the attack can be evaluated empirically:

**Lemma 1**.: _Let \(\) be a \(1-\) quantile of \(\), the distribution on confidence scores \(s(x,y)\) that results from sampling \((x,y)\). Then the baseline membership inference attack \(_{}\) has \((_{})=\)._

Proof.: This follows from the definitions: \((_{})=_{(x,y)}[s(x,y)]=\).

## 3 Our Attack

Our attack is \(_{q}(x,y)\), where \(q\) is derived from a _quantile regression_ model trained to predict quantiles of our test statistic \(s(x,y)\) on a dataset of points \((x,y)\) drawn from our null hypothesis distribution \((x,y)\). A popular non-parametric quantile regression method is to minimize _pinball loss_, which elicits _quantiles_ (just as squared loss elicits means):

**Definition 3**.: _The pinball loss function defined for a \(1-\) quantile is:_

\[_{1-}(,y)=\{(-y),(1-)(y-)\}\]

Pinball loss is a useful objective function because it elicits quantiles:

**Lemma 2**.: _Fix any distribution \(\). Let:_

\[_{}*{}_{y }[_{1-}(,y)]\]

_Then \(\) is a \((1-)\)-quantile of \(\)._

Viewed through this lens, the baseline attack can be thought of as the end result of the following simple pipeline:

1. Select a target false positive rate \(\),
2. Choose a threshold \(\) by solving the minimization problem \[_{^{}}*{}_{(x,y) }[_{1-}(^{},s(x,y))]\]
3. Instantiate the baseline membership inference attack \(_{}\).

Our attack departs from this baseline attack by training a model \(q:\) on feature/confidence score pairs to optimize pinball loss, rather than a single threshold \(\):

1. Select a target false positive rate \(\) and a class of model architectures \(\) consisting of models \(q:\).
2. Train a model \(q\) by solving the following risk minimization problem: \[q_{q^{}}*{}_{(x,y) }[_{1-}(q(x),s(x,y))]\] (1)
3. Instantiate the membership inference attack \(_{q}\)

We train our quantile regression model on a dataset consisting of points \((x,y)\) drawn from the underlying distribution (of points _not_ used in training), labeled by the confidence scores \(s(x,y)\) derived from the model. Thus our attack assumes only that we have API access to the model under attack \(f\), and are able to query it on a finite set of points to obtain confidence scores.

We now establish some basic properties of our attack. The first is that, like the baseline attack, it actually achieves its target false positive rate. Unlike the baseline attack, this is no longer immediate, but can be derived from properties of the pinball loss:

**Theorem 1**.: _Fix a distribution \(()\) over labeled examples and a model \(f\). Suppose that the marginal distribution over \(s(x,y)\) for \((x,y)\) is continuous. Let \(\) be any class of models that is closed under additive shifts -- i.e. such that for each \(q\) and \(\), then we also have \(q^{}\) for \(q^{}(x)=q(x)+\). Then for the membership inference attack \(_{q}\) produced by our method, \((_{q})=\)._

We defer the proof to Appendix C.1. Thus by varying \(\), we can use our attack to sweep out a curve trading off our target false positive rate with our (empirically measured) true positive rate, just as we can for the baseline attack \(_{}\).

Is this guarantee stronger than the baseline attack, and if so, in what sense? To give one perspective on this, it will be helpful to define group conditional quantile consistency, which is related to multicalibration, a concept originating from the fairness in machine learning literature (Hebert-Johnson et al., 2018; Gupta et al., 2022; Bastani et al., 2022; Jung et al., 2023; Noarov and Roth, 2023).

**Definition 4**.: _Fix a collection \(\) of group indicator functions \(g:\{0,1\}\) and a model \(q:\). \(q\) satisfies group conditional quantile consistency with respect to a distribution \(()\), a target quantile \(1-\), and the collection of groups \(\) if for every \(g\):_

\[_{(x,s)}[q(x) s|g(x)=1]=1-\]

Group conditional quantile consistency asks that our quantile predictions be correct not just marginally over the data, but also (simultaneously) conditionally on membership in a large number of potentially intersecting groups. If we optimize pinball loss over a richer set of models (that are closed under shifts by a class of group indicator functions, rather than just constant functions), then our attack will achieve its target false positive rate even when conditioning on membership in each of the groups specified by the functions \(g\), rather than just marginally. This is a stronger guarantee, as a marginal false positive rate on its own need not hold subject to additional conditioning events.

**Theorem 2**.: _Fix a distribution \(()\) over labeled examples and a model \(f\). Fix a collection of group indicator functions \(\) and a class of models \(\) such that:_

1. \(\) _is closed under shifts from_ \(\)_: for every_ \(h\)_,_ \(g\)_, and_ \(\)_, the function_ \(h^{}(x)=h(x)+ g(x)\) _is such that_ \(h^{}\)_._
2. _The conditional distribution over_ \(s(x,y)\) _for_ \((x,y)\)_, conditional on_ \(g(x)=1\) _is continuous for all_ \(g\)_._

_Then for the membership inference attack \(_{q}\) produced by our method, its false positive rate is \(1-\) conditional on membership in each group \(g\): \(_{(x,y)}[_{q}(x,y)=|g(x)=1]=1-\)._

We defer the proof to Appendix C.2.

## 4 Experiments

We present two sets of experiments on two different data domains, including images and tabular data. Here, we mainly focus on attacking widely-used classification models in these two domains, however, our theoretical claims generalizes to other data domains as well.

### Image Classification Experiments

We evaluate the effectiveness of our proposed approach on four image classification datasets: CIFAR-10 [Krizhevsky et al., 2009], a standard image classification dataset with 10 target classes; CIFAR-100 [Krizhevsky et al., 2009] another image classification dataset with 100 target classes; ImageNet-1k [Russakovsky et al., 2015], a substantially larger image classification task with 1000 target classes; and CINIC-10 [Darlow et al., 2018], an extension of CIFAR-10 that additionally uses images from ImageNet-1k corresponding to the original 10 target classes. To provide a realistic evaluation, we ensure our base models use common, well-performing architectures and follow standard guidelines for hyperparameter selection [He et al., 2015], including data augmentation, learning rate schedule, and l2 regularization(weight decay). For CIFAR-10 and CIFAR-100, target (classification) models include ResNet-10, ResNet-18, ResNet-34, and ResNet-50 [He et al., 2015]. For ImageNet-1k and CINIC-10, the target model is a ResNet-50. In all experiments, 50% of the dataset is used for training the target model, and, following the common standards, the resolution of the target model is 32x32 for CIFAR and CINIC datasets, and 224x224 for the ImageNet-1k dataset. The accuracy of each target model is presented in Appendix B.

To perform our membership inference attack, we train a single quantile regression model following our proposal in Eq.(1). One of the advantages of our attack is that it is model-agnostic: since it does not require the knowledge of the model architecture of the target or the knowledge of the training algorithm, we use the same model architecture for our quantile regression model in all settings : a pretrained ConvNext-Tiny model [Liu et al., 2022]. On CINIC-10 we additionally experimented with a ResNet-50 model as our quantile model architecture.

For the scoring function, we use the hinge score proposed in Carlini et al. \(s_{hinge}(x,y)=z_{y}(x)-_{y^{} y}z_{y^{}}(x)\). This scoring rule is closely related to the logit function of the model's confidence \(s(x,y)=(f_{y}(x))-(1-f_{y}(x))\) for models with high label confidence, and hasbeen empirically shown to be approximately normally distributed. On the largest dataset, ImageNet-1k, we directly train a quantile regression model to predict (log-spaced) quantiles at \([0.9996,1]\) An alternative is to learn a parameterized model so that, for each sample, the model predicts the mean \((x)\) and the log of the standard deviation \((x)\), and the quantiles of a sample can be generated from the Gaussian distribution \(((x),(e^{(x)})^{2})\). Due to the fact that CIFAR and CINIC datasets are much smaller (\(25000\) samples available for training on CIFAR), rather than directly learning quantiles, we opt to learn the parametrized models with Gaussian parameters on these datasets.

All images are processed at 224x224 resolution by the quantile model, which is trained on the remaining 50% of the training samples that were not used to train the target model. Since there is a smaller body of literature on stable hyperparameters for regression models, we use Ray Tune (Liaw et al., 2018) for hyperparameter tuning (tuning is used to minimize validation pinball loss in a held out dataset). The compute budget for our attack was approximately 30 GPU minutes per quantile regression attack (4 hours including hyperparameter optimization) on CIFAR-10/CIFAR-100, 18 minutes (4 hours 40 minutes including hyperparameter optimization) on CINIC-10, and 16 hours (128 hours including hyperparameter optimization) on ImageNet-1k. Final hyperparameters were found to be consistent across all tasks sharing an architecture; more information is provided in Appendix D

Figures 1 and 2 shows ROC curves of our proposed approach on ImageNet-1k and CINIC-10 respectively; FPR is computed on a held-out dataset that was not used to train the target or the quantile regression model. Both the marginal quantile approach from Yeom et al. (2018) and the shadow model approach LIRA from Carlini et al. (2022) are also shown for reference. In these experiments, our quantile regression approach dominates the shadow model approach at all comparison points for ImageNet-1k, and is roughly comparable to 4 shadow models on CINIC-10. Appendix E shows the same comparison against all CIFAR-10/100 target models (ResNet-10, ResNet18, ResNet34, ResNet-50). In these latter experiments, our attack outperforms the marginal baseline but falls short of the shadow model approach. We note that in this case, the shadow models actually produce thresholds that have lower pinball loss than our quantile regression algorithm, suggesting that on the smaller CIFAR datasets, our optimization heuristic was unable to sufficiently minimize test pinball loss.We find this observation to be consistent across experiments, that the attack with smaller pinball loss on public data is generally the attack with the best TPR at that FPR level \(\)

Figure 2: Comparing the true positive rate vs. false positive rate of our membership inference attack (with parametric Gaussian loss) on the CINIC-10 dataset (an extension of the CIFAR-10 dataset with 270,000 images, 4.5 times that of CIFAR-10) with the marginal baseline proposed by Yeom et al. (2018) and the state-of-the-art LiRA proposed in Carlini et al. (2022) evaluated at 2, 4, 6, and 8 shadow models. Both the model under attack and our quantile regression model use a ResNet-50 architecture. Both LiRA and our method are able to reliably identify train samples at very low false positive rates.

Table 1 shows precision of the proposed membership inference attack at 1% and 0.1% false positive rate on ResNet-50 target networks (additional results shown in Appendix E). Our attack robustly predicts membership in the private dataset with high precision even at low FPR. The proposed approach works on all target architectures and datasets, but works particularly well on the more complex and data intensive ImageNet-1k task.

Since LIRA produces an explicit score distribution \((s(x,y);(x,y),(x,y))\) based on the shadow model's predictions, we can compare all \(3\) methods (Ours, LIRA, marginal baseline) in terms of pinball loss on public data (\(x\)). **We find that the attack with the smallest pinball loss on public data is the better membership inference attack across all datasets.** This shows that a strong quantile predictor on public data is a strong membership inference attack; which validates the core premise of our approach in Section 3.

A possible explanation for the relative lack of success of directly learning a quantile regression model on CIFAR datasets could be the relatively low number of available samples (25000). In such low data scenarios, training shadow models is also computationally affordable. The opposite holds true for the much larger ImageNet1k dataset, on which the computational cost of training a single target model, much less multiple shadow models far exceeds the cost of a single quantile regression model.

### Tabular Classification Experiments

In addition to experiments on image datasets, we here demonstrate the effectiveness of our membership inference attack on tabular datasets, including large datasets from derived from the US Census' American Community Survey (ACS) (Ding et al., 2021) and small ones from OpenML 4(Grinsztajn et al., 2022). Gradient boosting with decision trees is widely-used for classification tasks with tabular data, so in our experiments, to achieve a reasonable performance, we train a gradient boosting model with 5-fold cross validation for hyperparameter tuning on the private portion of the data. For our attack model, gradient boosting with regression trees is applied, but now with regression targets as mentioned above. Hyperparameters for regression tasks are also tuned using a public portion of the data to avoid overfitting. In our experiments, catboost is used for model training, and Optuna (Akiba et al., 2019) is used for hyperparameter tuning.

Table 2 shows that our attack, which involves learning a single regression model, performs on par with the LiRA attack, which requires learning at least 16 models on some tasks and more on other tasks. Since each model, including our regression model and a shadow model in LiRA, has the same latency in terms of hyperparameter tuning and model training, our attack requires significantly less compute (equivalent to a single shadow model), and it reduces a successful attack from training many models to only one model. Figure 3 shows ROC curves of our proposed approach on OpenML.

## 5 Discussion

We have introduced a new family of membership inference attacks that are competitive with the state of the art (and in our ImageNet experiments, substantially and uniformly better), while requiring

   &  &  \\  Method & C-10 & C-100 & CIN10 & IN-1k & C-10 & C-100 & CIN10 & IN-1k \\  Marginal & 48.56\% & 58.81\% & 49.02\% & 47.62\% & 60.94\% & 65.75\% & 45.76\% & 46.81\% \\ LIRA (n=2) & 78.55\% & 95.21\% & 55.43\% & 62.70\% & 83.18\% & 98.65\% & 54.07\% & 56.04\% \\ LIRA (n=4) & 80.52\% & 95.87\% & 78.71\% & 89.11\% & 91.48\% & 98.94\% & 86.99\% & 95.18\% \\ LIRA (n=6) & 83.19\% & 96.20\% & 88.75\% & 93.74\% & 93.17\% & 99.02\% & 96.40\% & 98.38\% \\ LIRA (n=8) & 83.00\% & 96.07\% & 91.86\% & 94.57\% & 93.70\% & 98.98\% & 97.94\% & 98.73\% \\  Ours & 62.95\% & 79.57\% & 76.67\% & 97.45\% & 64.48\% & 85.41\% & 85.46\% & 99.64\% \\  

Table 1: Precision of all membership inference attack at 1% and 0.1% false positive rates on ResNet-50 architectures. Our attack consistently dominates the marginal baseline and produces excellent results on ImageNet-1k compared to shadow model approaches. We do not beat the shadow model approach on the smaller CIFAR datasets, potentially owing to their small dataset size. On CINIC-10, a larger dataset than CIFAR, we obtain comparable results to 4 shadow models. Additional results for the remaining architectures are presented in Appendix E substantially fewer computational resources and less knowledge of the target model. Moreover, we have identified pinball loss as a key target objective: uniformly across all of our experiments, the methods that produce thresholds minimizing pinball loss are the most effective attacks. Together, this brings membership inference closer to practicality on large commercial models. This serves to highlight a growing risk to privacy--but also provides a more efficient means to audit models by subjecting them to our attacks. We hope that our methods encourage and enable a more widespread practice of auditing models for privacy violations by subjecting them to membership inference attacks before deployment.