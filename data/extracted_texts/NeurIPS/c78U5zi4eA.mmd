# How Does Message Passing Improve

Collaborative Filtering?

Clark Mingxuan Ju\({}^{1,2}\)1, William Shiao\({}^{3}\), Zhichun Guo\({}^{2}\), Yanfang Ye\({}^{2}\),

**Yozen Liu\({}^{1}\)**, **Neil Shah\({}^{1}\)**, **Tong Zhao\({}^{1}\)1

\({}^{1}\)Snap Inc., \({}^{2}\)University of Notre Dame, \({}^{3}\)University of California, Riverside

\({}^{1}\){mju,yliu2,nshah,tong}@snap.com, \({}^{2}\){mju2,zguo5,yye7}@nd.edu, \({}^{3}\)wshia002@ucr.edu

###### Abstract

Collaborative filtering (CF) has exhibited prominent results for recommender systems and been broadly utilized for real-world applications. A branch of research enhances CF methods by message passing (MP) used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF. They assume that MP helps CF methods in a manner akin to its benefits for graph-based learning tasks in general (e.g., node classification). However, even though MP empirically improves CF, whether or not this assumption is correct still needs verification. To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate. With our curated ablation studies and theoretical analyses, we discover that _(i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and **(ii) MP** usually helps low-degree nodes more than high-degree nodes._ Utilizing these novel findings, we present **T**est-time **A**ggregation for **C**ollaborative **F**iltering, namely **TAG-CF**, a test-time augmentation framework that only conducts MP once at inference time. The key novelty of TAG-CF is that it effectively utilizes graph knowledge while circumventing most of notorious computational overheads of MP. Besides, TAG-CF is extremely versatile can be used as a plug-and-play module to enhance representations trained by different CF supervision signals. Evaluated on six datasets (i.e., five academic benchmarks and one real-world industrial dataset), TAG-CF consistently improves the recommendation performance of CF methods without graph by up to **39.2%** on cold users and **31.7%** on all users, with little to no extra computational overheads. Furthermore, compared with trending graph-enhanced CF methods, TAG-CF delivers comparable or even better performance _with less than **1%** of their total training times_. Our code is publicly available at [https://github.com/snap-research/Test-time-Aggregation-for-CF](https://github.com/snap-research/Test-time-Aggregation-for-CF).

## 1 Introduction

Recommender systems are essential in improving users' experiences on web services, such as product recommendations , video recommendations , friend suggestions , etc. In particular, recommender systems based on collaborative filtering (CF) have shown superior performance . CF methods use preferences for items by users to predict additional topicsor products a user might like . These methods typically learn a unique representation for each user/item and an item is recommended to a user according to their representation similarities .

One popular line of research explores Graph Neural Networks (GNNs) for CF, exhibiting improved results compared with CF frameworks without the utilization of graphs . The key mechanism behind GNNs is message passing (MP), where each node aggregates information from its neighbors in the graph, and information from neighbors that are multiple hops away can be acquired by stacked MP layers . During the model training, traditional CF methods directly fetch user/item representations of an observed interaction (e.g., purchase, friending, click, etc.) and enforce their pair-wise similarity . Graph-enhanced CF methods extend this scheme by conducting stacked MP layers over the user-item bipartite graph, and harnessing the resulting user and item representations to calculate a pair-wise affinity.

A recent study  shows that removing several key components of the MP layer (e.g., learnable transformation parameters) greatly enhances GNNs' performance for CF. Its proposed method (LightGCN) achieves promising performance by linearly aggregating neighbor representations and has been used as the de facto backbone model for later works due to its simple and effective design . However, this observation contradicts GNN architectures for classic graph learning tasks, where GNNs without these components severely under-perform . Additionally, existing research  assumes that the contribution of MP for CF is similar to that for graph learning tasks in general (e.g., node classification or link prediction) - they posit that node representations are progressively refined by their neighbor information and the performance gain is positively proportional to the neighborhood density as measured in node degrees . However, according to our empirical studies in Section3.2, MP in CF improves low-degree nodes more than high-degree nodes, which also contradicts GNNs' behaviors for classic tasks . In light of these inconsistencies, we ask:

_What role does message passing really play for collaborative filtering?_

In this work, we investigate contributions brought by MP for CF from two perspectives. Firstly, we unroll the formulation of MP layer and show that its performance improvement could either come from additional representations passed from neighbors during the forward pass or accompanying gradient updates to neighbor representations during the back-propagation. With rigorously designed ablation studies, we empirically demonstrate that gains brought by the forward pass dominate those by the back-propagation. Furthermore, we analyze the performance distribution w.r.t. the user degree (i.e., the number of interactions per user) with or without message passing and discover that the message passing in CF improves low-degree users more compared to high-degree users. For the first time, we connect this phenomenon to Laplacian matrix learning , and theoretically show that popular supervision signals  for CF inadvertently conduct message passing in the back-propagation even without treating the input data as a graph. Hence, when message passing is applied, high-degree users demonstrate limited improvement, as the benefit of message passing for high degree nodes has already been captured by the supervision signal.

With the above takeaways, we present **T**est-time **A**ggregation for **C**ollaborative **F**iltering, namely **TAG-CF**. Specifically, unlike other graph CF methods, TAG-CF _does not require any message passing during training_. Instead, it is a test-time augmentation framework that only conducts a _single message-passing step at inference time_, and effectively enhances representations inferred from different CF supervision signals. The test-time design is inspired by our first perspective that, within total performance gains brought by message passing, gains from the forward pass dominate those brought by the backward pass. Applying message passing only at test time avoids repetitive queries (i.e., once per node and epoch) for representations of surrounding nodes, which grow exponentially as the number of layers increases. Moreover, following our second perspective that message passing helps low-degree nodes more in CF, we further offload the cost of TAG-CF by applying the one-time message passing only to low-degree nodes. We summarize our contributions as:

* This is the first work that formally investigates why message passing helps collaborative filtering. We demonstrate that message passing in CF improves the performance primarily by additional representations passed from neighbors during the forward pass instead of accompanying gradient updates to neighbors during the back-propagation, and prove that message passing helps low-degree nodes more than high-degree nodes.
* Given our findings, we propose TAG-CF, an efficient yet effective test-time aggregation framework to enhance representations inferred by different CF supervision signals such as BPR and DirectAU.

Evaluated on six datasets, TAG-CF consistently improves the performance of CF methods without graph by up to **39.2%** on cold users and **31.7**% on all users, with little to no extra computational overheads. Furthermore, compared with trending graph-enhanced CF methods, TAG-CF delivers comparable or even better performance _with less than 1% of their total training time_.
* Beside promising cost-effectiveness, we show that test-time aggregation in TAG-CF improves the recommendation performance in similar ways as the training-time aggregation does, further demonstrating the legitimacy of our findings.

## 2 Preliminaries and Related Work

**Collaborative Filtering**. Given a set of users, a set of items, and interactions between users and items, collaborative filtering (CF) methods aim at learning a unique representation for each user and item, such that user and item representations can reconstruct all observable interactions [45; 57; 32]. CF methods based on matrix factorization directly utilize the inner product between a pair of user and item representations to infer the existence of their interaction [32; 45]. CF methods based on neural predictors use multi-layer feed-forward neural networks that take user and item representations as inputs and output prediction results [21; 77]. Let \(\) and \(\) denote the user set and item set respectively, with user \(u_{i}\) associated with an embedding \(_{i}^{d}\) and item \(i_{i}\) associated with \(_{i}^{d}\), the similarity \(s_{ij}\) between user \(u_{i}\) and item \(i_{j}\) is formulated as \(s_{ij}=_{i}^{}}_{j}}\).

**Graph Neural Networks**. Graph neural networks (GNNs) are powerful learning frameworks to extract representative information from graphs [30; 56; 18; 70; 11; 26], with numerous applications in large-scale ranking and forecasting tasks [53; 52; 8; 49]. They aim to map each input node into low-dimensional vectors, which can be utilized to conduct either graph-level  or node-level tasks . Most GNNs explore layer-wise message passing , where each node iteratively extracts information from its first-order neighbors, and information from multi-hop neighbors can be captured by stacked layers. Given a graph \(=(,)\) and node features \(^{|| d}\), graph convolution in GCN  at \(k\)-th layer is formulated as:

\[_{i}^{(k+1)}=(_{j(i) i}}_{j}^{(k)}^{(k)}), \]

where \(_{i}^{0}=_{i}\), \((i)\) refers to the set of direct neighbors of node \(i\), and \(^{(k)}^{d^{k} d^{(k+1)}}\) refers to parameters at the \(k\)-th layer transforming the node representation from \(d^{k}\) to \(d^{(k+1)}\) dimension.

Recent works [37; 38] have shown that GNNs make predictions based on the distribution of node neighborhoods. Moreover, GNNs' performance improvement for high-degree nodes is typically better than for low-degree nodes [54; 23; 28; 17; 63]. They posit that node representations are progressively refined by their neighbor information and the performance gain is positively proportional to the neighborhood density as measured in node degrees. As we explore test-time augmentation in this work, it is worth noting that there also exist a group of relevant works that explore data augmentation techniques to enhance the GNN performance [28; 79; 78; 25; 24].

**Message Passing for Collaborative Filtering**. Recent research tends to apply the message passing scheme in GNNs to CF [20; 60; 42; 12; 50; 68; 31; 34]. In CF, they mostly conduct message passing between user-item bipartite graphs and utilize the resultant representations to calculate user-item similarities. For instance, NGCF  directly migrates the message passing scheme in GNNs (similar to Equation (1)) and applies it to bipartite graphs in CF. LightGCN  simplifies NGCF  by removing certain components (i.e., the self-loop, learning parameters for graph convolution, and activation functions) and further improves the recommendation performance compared with NGCF. The simplified parameter-less message passing in LightGCN can be expressed as:

\[_{i}^{(k)}=_{i_{j} N(u_{i})})|})|}}_{j}^{(k-1)},_{i}^{(k)}=_{u_{j} N(i_{i} )})|})|}}_{j}^{(k-1)}, \]

where \(N()\) refers to the set of items or users that the input interacts with, \(_{i}^{(0)}=_{i}\), and \(_{i}^{(0)}=_{i}\). With \(K\) layers, the final user/item representations and their similarities are constructed as:

\[}_{i}=_{k=0}^{K}_{i}^{(k)},\;\; }_{i}=_{k=0}^{K}_{i}^{(k)},\;\;s_{ij} =_{i}^{}}_{j}}. \]According to results reported in LightGCN and NGCF [20; 60; 2; 13] and empirical studies we provide in this work (i.e., Table 2 and Table 5), incorporating message passing to CF methods without graphs (i.e., matrix factorization methods [45; 21]) can improve the recommendation performance by up to 20%. Utilizing LightGCN as the backbone model, later works try to further improve the performance by incorporating self-supervised learning signals [35; 74; 1; 73; 64; 27]. Graph-based CF methods assume that the contribution of message passing for CF is similar to that for graph learning tasks in general (e.g., node classification or link prediction). However, whether or not this assumption is correct still needs verification, even though message passing empirically improves CF. There also exists a branch of research that aims at accelerating or simplifying message passing in CF by adding graph-based regularization terms during the training [48; 39; 44; 67]. While promising, they still repetitively query representations of adjacent nodes during the training.

**Efficient Efforts in Matrix Factorization**. A branch of research specifically focuses on improving the efficiency of matrix factorization [48; 44; 22; 43; 6]. For instance, GFCF  and Turbo-CF  explore graph signal processing to linearly convolve the interaction matrix and use the resulted matrix directly for recommendation without training. Furthermore, SVD-GCN  and SVD-AE  utilize a low rank version of the interaction matrix to further accelerate the convolution efficiency and yet remain the promising performance. Besides, BSPM  studies using diffusion process to gradually reconstruct the interaction matrix and achieves promising performance with fast processing. In parallel with these existing efforts, we propose to enhance any existing matrix factorization method through test-time augmentation that harnesses graph-based heuristics.

## 3 How Does Message Passing Improve Collaborative Filtering?

In this section, we demonstrate why message passing (MP) helps collaborative filtering from two major perspectives: Firstly, we focus on inductive biases brought by the MP explored in LightGCN, the de facto backbone model for graph-based CF methods. Secondly, we consider the performance improvement on different node subgroups w.r.t. the node degree with and without MP.

### Neighbor Information vs. Accompanying Gradients from Message Passing

Following the definition in Equation (2), given a one-layer LightGCN2, we unroll the calculation of the similarity \(s_{ij}\) between any user \(u_{i}\) and item \(i_{j}\) as the following:

\[s_{ij} =_{i}+_{i_{n} N(u_{i})})|})|}}_{n}^{} _{j}+_{u_{n} N(i_{j})})|})|}}_{n}\] \[=\ _{i}^{}_{j}+_{u_{n} N (i_{j})})|})|}}_{i}^{}_{n}_{i_{n} N(u_{i})})|})|}}_{n}^{}_{j}\] \[+_{i_{n} N(u_{i})}_{u_{n} N(i_{j})})|})|})|})|}}_{n}^ {}_{n}. \]

With derived similarities between user-item pairs, their corresponding representations can be updated by objectives (e.g., BPR  and DirectAU ) that enforce the pair-wise similarity between representations of user-item pairs in the training data.

CF methods without the utilization of graphs directly calculate the similarity between a user and an item with their own representations (i.e., \(s_{ij}=_{i}^{}_{j}\)), which aligns with the first term in Equation (4). Compared to the formulation in Equation (4), we can see that three additional similarity terms are introduced as inductive biases: similarities between users who purchase the same item (i.e., \(_{i}^{}_{n}\)), between items that share the same buyer (i.e., \(_{n}^{}_{j}\)), and between neighbors of an observed interaction (i.e., \(_{n}^{}_{n}\)). With these three additional terms from MP, we reason that the performance improvement brought by MP to CF methods without graph could come from **(i)** additional neighbor representations during the forward pass (i.e., numerical values of three extra terms in Equation (4)), or **(ii)** accompanying gradient updates to neighbors during the back-propagation.

To investigate the origin of the performance improvement brought by MP, we designed two variants of LightGCN. The first one (LightGCNw/o neigh. info) shares the same forward and backward procedures as LightGCN during the training but does not conduct MP during the test time. In this variant, additional gradients brought by MP are maintained as part of the resulting model, but information from neighbors are ablated. In the second variant (LightGCNw/o grad.), the model shares the same forward pass but drops gradients from these three additional terms during the backward propagation. Besides these two variants, we also experiment on LightGCN without MP, denoted as LightGCNw/o both, a matrix factorization model with the same supervision signal (i.e., BPR loss). Implementation details w.r.t. this experiment are in Appendix C.

From Table 1, we observe that the performance of all variants is downgraded compared with LightGCN, with the most significant degradation on LightGCNw/o neigh. info. This phenomenon indicates that **(i)** both additional representations passed from neighbors during the forward pass and accompanying gradient updates to neighbors during the back-propagation help the recommendation performance, and **(ii)** within total performance gains brought by MP, gains from the forward pass dominate those brought by the back-propagation. Comparing LightGCN with LightGCNw/o grad., we notice that the incorporation of gradient updates brought by MP is relatively incremental (i.e., \(\)2%). However, to facilitate these additional gradient updates for slightly better performance, LightGCN is required to conduct MP at each batch, which brings tremendous additional overheads.

### Message Passing in CF Helps Low-degree Users More Compared with High-degrees

Both empirical and theoretical evidence have demonstrated that GNNs usually perform satisfactorily on high-degree nodes with rich neighbor information but not as well on low-degree nodes [54; 23]. While designing graph-based model architectures for CF, most existing methods directly borrow this line of observations [60; 20] and assume that the contribution of message passing for CF is similar to that for graph learning tasks in general. However, whether or not these observations still transfer to message passing in CF remains questionable, as there exist architectural and philosophical gaps between message passing for CF and its counterparts for GNNs, as discussed in Section 2. To validate these hypotheses, we conduct experiments over representative methods (i.e., LightGCN and matrix factorization (MF) trained with BPR) and show their performance w.r.t. the node degree in Figure 1.

We observe that, overall both MF and LightGCN perform better on high-degree users than low-degree users. According to the upper two figures in Figure 1, MF behaves similarly to LightGCN, even without treating the input data as graphs, where the overall performance for high-degree user is stronger than that for low-degree users. However, the performance improvement of LightGCN from MF on low-degree users is larger than that for high-degree users (i.e., lower two figures in Figure 1). According to literature in general graph learning tasks [23; 36; 54], the performance improvement should be positively proportional to the node degree - the gain for high-degree users should be higher than that for low-degree users. This discrepancy indicates that it might not be appropriate to accredit contributions of message passing in CF directly through ideologies designed for classic graph learning tasks (e.g., node classification and link prediction). To bridge this gap, we connect supervision signals (i.e., BPR and DirectAU) commonly adopted by CF methods to Laplacian matrix learning. The formulation of BPR  and DirectAU  without the incorporation of graphs can be written as:

\[_{}=-_{(i,j)}_{(i,k) }(s_{ij}-s_{ik})=-_{(i,j)}_{( i,k)}(_{i}^{}_{j}- _{i}^{}_{k}),\] \[_{}=_{(i,j)}|| _{i}-_{j}||^{2}+_{u,u^{}} e^{ -2||-^{}||^{2}}+_{i,i^{}}  e^{-2||-^{}||^{2}}, \]

where \(\) refers to the set of observed interactions at the training phase and \(^{}\) and \(^{}\) refers to any random user/item. According to works on Laplacian matrix learning [82; 10; 38], learning node representations over graphs can be decoupled into Laplacian quadratic form, a weighted summation of two sub-goals:

\[_{}\{||-||^{2}+(^{ })\}, \]

   Method & Yelpâ€“2018 & Gowalla & Amazon-book \\   \\  LightGCN & 6.36 & 9.88 & 8.13 \\ w/o grad. & 6.16 (3.1\%) & 9.87 (0.1\%) & 7.80 (4.1\%) \\ w/o neigh. info. & 4.71 (5.9\%) & 6.95 (29.7\%) & 6.95 (14.5\%) \\ w/o both & 6.09 (4.2\%) & 9.83 (0.5\%) & 7.75 (4.7\%) \\   \\  LightGCN & 11.21 & 18.53 & 12.97 \\ w/o grad. & 10.87 (3.0\%) & 18.51 (0.1\%) & 12.81 (1.2\%) \\ w/o neigh. info & 8.44 (24.7\%) & 13.06 (29.5\%) & 11.25 (13.3\%) \\ w/o both & 10.71 (4.5\%) & 18.42 (0.6\%) & 12.57 (3.1\%) \\   

Table 1: Performance of LightGCN variants.

where \(\) refers to the node representation matrix after the message passing, \(\) refers to the input feature matrix, and \(\) refers to the Laplacian matrix. The first term regularizes the latent representation such that it does not diverge too much from the input feature; whereas the second term promotes the similarity between latent representations of adjacent nodes, which can be re-written as: \((^{})=_{(i,j) }||_{i}-_{j}||^{2}\) in CF bipartite graphs.  show that \(K\) layers of linear message passing exactly optimizes the second term in Equation (6). Given this theoretical foundation, we derive the following theorem w.r.t. relations between BPR, DirectAU, and message passing in CF:

**Theorem 1**.: _Assuming that \(||_{i}||^{2}=||_{j}||^{2}=1\) for any \(u_{i}\) and \(I_{j}\), objectives of BPR and DirectAU are strictly upper-bounded by the objective of message passing (i.e., \(_{}_{(i,j)}||_{i}-_{j}||^{2}\) and \(_{}_{(i,j)}||_{i}- _{j}||^{2}\))._

Proof of Theorem 1 can be found in Appendix A. According to Theorem 1, both BPR and DirectAU optimize the objective of message passing (i.e., \(_{(i,j)}||_{i}-_{j}||^{2}\)) with some additional regularization (i.e., dissimilarity between non-existing user/item pairs for BPR, and representation uniformity for DirectAU). Hence, directly optimizing these two objectives partially fulfills the effects brought by message passing during the back-propagation.

Combining this theory with the aforementioned empirical observations, we show that these two supervision signals could inadvertently conduct message passing in the backward step, even without explicitly treating interaction data as graphs. Since this inadvertent message passing happens during the back-propagation, its performance is positively correlated to the amount of training signals a user/item can get. In the case of CF, the amount of training signals for a user is directly proportional to the node degree. High-degree active users naturally benefit more from the inadvertent message passing from objective functions, because they acquire more training signals. Hence, when explicit message passing is applied to CF methods, the performance gain for high-degree users is less significant than that for low-degree users. Because the contribution of the message passing over high-degree nodes has been mostly fulfilled by the inadvertent message passing during the training.

To quantitatively prove this theory, we incrementally upsample low-degree training users and observe the performance improvement that TAG-CF could introduce at each upsampling rate. If our line of theory is correct, then we should expect less performance improvement on low-degree users for a larger upsampling rate. The results are shown in Appendix E with supporting evidence.

## 4 Test-time Aggregation for Collaborative Filtering

In Section 3, we demonstrate why message passing helps CF from two perspectives. Firstly, w.r.t. the formulation of LightGCN, we observe that the performance gain brought by neighbor information dominates that brought by additional gradients. Secondly, w.r.t. the improvement on user subgroups, we learn that message passing helps low-degree users more, compared with high-degree users.

In light of these two takeaways, we present **Test**-time **Agg**regation for **C**ollaborative **F**iltering, namely **TAG-CF**, a test-time augmentation framework that only conducts message passing once at inference time and is effective at enhancing matrix factorization methods trained by different CF supervision signals. Given a set of well-trained user/item representations, TAG-CF simply aggregates neighboring item (user) representations for a given user (item) at test time. Despite its simplicity,

Figure 1: Performances of LightGCN and Matrix Factorization w.r.t. the user degree across datasets. The performance improvement brought by message passing decreases as the user degree goes up.

we show that our proposal can be used as a plug-and-play module and is effective at enhancing representations trained by different CF supervision signals.

The test-time aggregation is inspired by our first perspective that, within total performance gains brought by message passing, gains from additional neighbor representations during the forward pass dominate those brought by accompanying gradient updates to neighbors during the back-propagation. Applying message passing only at test time avoids repetitive training-time queries (i.e., once per node and epoch) of surrounding neighbors, which grow exponentially as the number of layers increases by the neighbor explosion phenomenon [16; 76; 75]. Specifically, given a set of well-trained user and item representations \(^{|| d}\) and \(^{|| d}\), TAG-CF augments representations for user \(u_{i}\) and item \(i_{i}\) as:

\[_{i}^{*}=_{i}+_{i_{j} N(u_{i})}|N(u_{i})|^{m}|N(i_{ j})|^{n}_{j},_{i}^{*}=_{i}+_{u_{j} N(i_{ i})}|N(i_{i})|^{m}|N(u_{j})|^{n}_{j}, \]

where \(m\) and \(n\) are two hyper-parameters that control the normalization of message passing. With \(m=n=-\), Equation (7) becomes the exact formulation of one-layer LightGCN (i.e., Equation (2)). Empirically, we observe that the setup with \(m=n=-\) for TAG-CF does not always work for all datasets. This setup is directly migrated from message passing for homogeneous graphs , which might not be applicable for bipartite graphs where all neighbors are heterogeneous . Unlike LightGCN which can fill this gap by adaptively tuning all representations during the training, TAG-CF cannot update any parameter since it is applied at test time, and hence requires tune-able normalization hyper-parameters.

Moreover, following our second perspective that message passing helps low-degree nodes more in CF, we further derive TAG-CF\({}^{+}\), which reduces the cost of TAG-CF by applying the one-time message passing only to low-degree nodes. Focusing on only low-degree nodes has two benefits: **(i)** it reduces the number of nodes that TAG-CF\({}^{+}\) needs to attend to, and **(ii)** message passing for low-degree nodes is naturally cheaper than for high-degree nodes given the surrounding neighborhoods are sparser (mitigating neighbor explosion). The degree threshold that determines which nodes to apply TAG-CF\({}^{+}\) is selected by the validation performance, with details in Appendix C.

TAG-CF can effectively enhance MF methods by conducting message passing only once at test time. TAG-CF effectively utilizes graphs while circumventing most of notorious computational overheads of message passing. It is extremely flexible, simple to implement, and enjoys the performance benefits of graph-based CF method while paying the lowest overall scalability.

## 5 Experiments

We conduct extensive experiments to demonstrate the effectiveness and efficiency of TAG-CF. We aim to answer the following research questions: **RQ (1)**: how effective is TAG-CF at improving MF methods without using graphs, **RQ (2)**: how much overheads does TAG-CF introduce, **RQ (3)**: can TAG-CF effectively enhance MF methods trained by different objectives, **RQ (4)**: how effective is TAG-CF\({}^{+}\) w.r.t. different degree cutoffs, and **RQ (5)**: do behaviors of TAG-CF align with our findings in Section 3?

### Experimental Settings

**Datasets**. We conduct comprehensive experiments on five commonly used academic benchmark datasets, including Amazon-book, Anime, Gowalla, Yelp2018, and MovieLens-1M. Additionally, we also evaluate on a large-scale real-world industrial user-item recommendation dataset Internal. Descriptions of these datasets are provided in Appendix B.

**Baselines**. We compare TAG-CF with two branches of methods: (1) CF methods that do not explicitly utilize graphs, including vanilla matrix factorization (MF) methods trained by BPR and DirectAU [45; 57], Efficient Neural Matrix Factorization  (denoted as ENMF), and UltraGCN . (2) Graph-based CF methods, including LightGCN  and NGCF . Besides, we also compare with recent graph-based CF methods that extend LightGCN by adding additional self-supervised signals for better performance, including LightGCL , SimGCL , and SGL . For the coherence of reading, we include comprehensive discussions about evaluation protocols across all methods, tuning for hyper-parameters, and other implementation details in Appendix C.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

When setting the cutoff to a user degree of around 100, on Amazon-Book, Gowalla, and Yelp-2018, TAG-CF\({}^{+}\) can further improve TAG-CF by 125%, 17%, and 11%, respectively, with efficiency improvement of 7%, 4%, and 8%. In these cases, TAG-CF\({}^{+}\) not only significantly improves the performance but also effectively reduces computational overheads.

However, on these three datasets, after the cutoff bypasses a degree of 100, the performance improvement eventually decreases to the performance of TAG-CF (i.e., 100%), indicating that test-time aggregation jeopardizes the performance on high-degree nodes. On Anime, though no downgrade on high-degree users, the performance improvement of TAG-CF\({}^{+}\) to TAG-CF is incremental. These phenomenons not only demonstrate the effectiveness and efficiency of TAG-CF\({}^{+}\), but also verify our findings in Section 3.2 that message passing in CF helps low-degree users more than high-degree users.

## 6 Conclusion

In this study, we investigate how message passing improves collaborative filtering. Through a series of ablations, we demonstrate that the performance gain from neighbor contents dominates that from accompanying gradients brought by message passing in CF. Moreover, for the first time, we show that message passing in CF improves low-degree users more than high-degree users. We theoretically demonstrate that CF supervision signals inadvertently conduct message passing in the backward step, even without treating the data as a graph. In light of these novel takeaways, we propose TAG-CF, a test-time aggregation framework effective at enhancing representations trained by different CF supervision signals. Evaluated on five datasets, TAG-CF performs at par with SoTA methods with only a fraction of computational overhead (i.e., less than 1.0% of the total training time).

## 7 Limitation and Broader Impact

One limitation of our proposal could be the utilization of graphs in large-scale machine learning pipeline. TAG-CF conducts a single-time aggregation of neighbors, which could be equivalently achieved by existing technologies such as SQL, BigQuery, etc. Furthermore, we observe no ethical concern entailed by our proposal, but we note that both ethical or unethical applications based on collaborative filtering may benefit from the effectiveness of our work. Care should be taken to ensure socially positive and beneficial results of machine learning algorithms.

## 8 Acknowledgments

This work was mostly conducted during the internship of Clark, William, and Zhichun at Snap Inc. We would like to thank Xin Chen and his colleagues from Snap Inc. for their help on pre-processing the internal dataset. This work was partially supported by the NSF under grants IIS-2321504, IIS-2334193, IIS-2203262, IIS-2217239, CNS-2426514, CNS-2203261, and CMMI-2146076. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.

We sincerely appreciate constructive feedback from all reviewers during the paper review phase.

Figure 2: The performance and efficiency improvement of TAG-CF\({}^{+}\) w.r.t. different cutoffs. TAG-CF\({}^{+}\) further improves TAG-CF with less computational overheads. 100% is the original performance/efficiency of vanilla TAG-CF.