# On the Identifiability of Sparse ICA without

Assuming Non-Gaussianity

 Ignavier Ng\({}^{*1}\), Yujia Zheng\({}^{*1}\), Xinshuai Dong\({}^{1}\), Kun Zhang\({}^{1,2}\)

\({}^{1}\) Carnegie Mellon University

\({}^{2}\) Mohamed bin Zayed University of Artificial Intelligence

{ignavierng, yujiazh, dongxinshuai, kunz1}@cmu.edu

Equal contribution.

###### Abstract

Independent component analysis (ICA) is a fundamental statistical tool used to reveal hidden generative processes from observed data. However, traditional ICA approaches struggle with the rotational invariance inherent in Gaussian distributions, often necessitating the assumption of non-Gaussianity in the underlying sources. This may limit their applicability in broader contexts. To accommodate Gaussian sources, we develop an identifiability theory that relies on second-order statistics without imposing further preconditions on the distribution of sources, by introducing novel assumptions on the connective structure from sources to observed variables. Different from recent work that focuses on potentially restrictive connective structures, our proposed assumption of structural variability is both considerably less restrictive and provably necessary. Furthermore, we propose two estimation methods based on second-order statistics and sparsity constraint. Experimental results are provided to validate our identifiability theory and estimation methods.

## 1 Introduction

Independent component analysis (ICA)  has emerged as an essential statistical tool in the scientific community, with application in various disciplines including neuroscience , biology , and Earth science . It aims to uncover the hidden generative processes that govern the observed data and separate mixed signals into independent sources. However, it is known that the traditional approaches of ICA struggle with Gaussian sources, which may limit their applicability in a wide variety of contexts. For instance, several biological traits and measurements such as height, blood pressure, and measurement errors in genetics, as well as the thermal noises in electronic circuits, may often be normally distributed . Classical identifiability results in ICA rely on higher-order statistics (e.g., Kurtois) , and cannot provide desired theoretical guarantees when there is more than one Gaussian source. Identifiability based on second-order statistic is thus essential in these scenarios.

The primary hurdle in applying ICA to Gaussian sources lies in the rotational invariance of Gaussian distribution . To address this issue, earlier studies  incorporated additional information by assuming that the sources are nonstationary. However, this extra information may not always be readily available, limiting the generalizability of these approaches. Thus, recent research  has started to delve into the connective structure between the sources and the observed variables, as opposed to solely focusing on distributional assumptions (e.g., non-Gaussianity and nonstationarity). This shift of focus is motivated by a key observation: despite the rotational invariance in the distribution of Gaussian sources, the sparsity of mixing matrix undergoes noticeable changes, i.e., it may be denser after rotation . Building on this insight, Zheng et al.  introduced two assumptions on the support of the mixing matrix to achieve identifiability of Gaussian sources, leading to a novel perspective for tackling this long-standing challenge in the field of ICA. On the other hand, Abrahamsen and Rigollet  assumed that the mixing matrix is generated from a sparse Bernoulli-Gaussian ensemble.

Although the rotational invariance of Gaussian distribution can be resolved by the structural assumptions on the mixing matrix proposed by Zheng et al. , they may be deemed overly restrictive. For instance, both of their structural assumptions cannot deal with the case where the set of observed variables influenced by one source is a subset of those affected by another source, which may not be uncommon in practice. This may limit the applicability of ICA in complex real-world scenarios, thus underscoring the need for a weaker and more flexible structural assumption that is capable of addressing the rotational invariance in a more universally applicable manner.

To enhance the applicability with Gaussian sources, we develop an identifiability theory of ICA from second-order statistics under more flexible structural constraints. We introduce a novel assumption, namely _structural variability_, that is considerably weaker than existing ones. Notably, this assumption is proved to be among the necessary conditions for identifying Gaussian sources by focusing on the connective structure (i.e., the support of the mixing matrix). Moreover, we propose two estimation methods grounded in sparsity regularization and continuous constrained optimization. The efficacy of our proposed methods has been validated through experiments, which also reaffirm the validity of our theoretical result. Lastly, as a matter of independent interest, we establish the connection between our identifiability result of ICA with causal discovery from second-order statistics; our finding further bridges the gap between these two fields and provides insights into the interpretation of our result.

## 2 Problem Setting

We consider the ICA setup given by \(=}\), where \(^{n}\) denotes the observed random vector, \(^{n}\) is the latent random vector representing the independent components, also called sources, and \(}=[}_{1}||}_{n}] ^{n n}\) denotes the unknown mixing matrix. We assume here that \(}\) has full rank and all sources are standardized. For the estimated mixing matrix \(}\) and the ground-truth one \(}\), we write \(}}\) if they differ only in column permutations and sign changes of columns, and \(}}\) vice versa. The goal is then to estimate \(}\) such that \(}}\); in this case, one could identify a one-on-one mapping between the ground truth sources and the estimated ones, i.e., the unknown mixing process has been demixed during estimation. Since the support of a mixing matrix \(\) essentially represents the connective structure between sources and observed variables, we have the following definition for the ease of reference.

**Definition 1** (Connective Structure).: _Given a mixing matrix \(\), we define its connective structure as a directed bipartite graph \(_{}=(_{},_{})\) from sources \(\) to observed variables \(\), where the nodes and edges are defined as \(_{}\{s_{i}\}_{i=1}^{n}\{x_{i}\}_{i=1}^{n}\) and \(_{}\{(s_{j},x_{i}):a_{ij} 0\}\), respectively._

Notations.We use bold capital letters (e.g., \(\)), bold lowercase letters (e.g., \(\)), and italic letters (e.g., \(a\)) to denote matrices, vectors, and scalar quantities, respectively. For any matrix \(\), we denote its \(j\)-th column by \(_{j}\), \(i\)-th row by \(_{i,:}\), and \((j,j)\)-th entry by \(a_{i,j}\). We also denote by \(_{}\) the submatrix of \(\) by obtaining the columns indexed by set \(\). For any vector \(\), we denote its \(i\)-th entry by \(a_{i}\). We define the support set of matrix \(\) as \(()\{(i,j):a_{i,j} 0\}\), and its support matrix as \(_{}\) which is of the same size as \(\), where \((_{})_{i,j}=\) if \(a_{i,j} 0\) and \((_{})_{i,j}=0\) otherwise. The notations of support set and support matrix are similarly defined for vector \(\). We denote by \(\|\|_{0}\) the number of nonzero entries in \(\), and we have \(\|\|_{0}=|()|=\|_{ }\|_{0}\). Furthermore, we denote the \(n n\) identity matrix and \(m n\) zero matrix by \(_{n}\) and \(_{m n}\), respectively; to lighten the notation, we drop their subscripts when the context is clear. We also use \([n]\) to denote \(\{1,2,,n\}\).

## 3 Identifiability Result without Assuming Non-Gaussianity

By exploiting the non-Gaussianity of the sources, such as fourth-order cumulant, existing approaches are able to estimate the true mixing matrix \(}\) up to signed column permutation when there is at most one Gaussian source [23; 2]. However, these approaches typically fail in the presence of more than one Gaussian source, because higher-order statistics cannot be utilized for full identifiability. The primary challenge of achieving identifiability for Gaussian sources lies in the rotational invariance of the Gaussian distribution. More specifically, the second-order statistics (or, more specifically, population-level covariance matrix) \(}=}}^{}\) remains unchanged if one replaces \(}\) with \(}\) for any orthogonal matrix \(\). Therefore, considering only second-order statistics, the true mixing matrix \(}\) is, generally speaking, only identifiable up to right orthogonal transformation without further assumptions. In this section, we adopt a different perspective that departs from traditional distributional assumptions (i.e., non-Gaussianity and fourth-order cumulant), and instead introduces novel and precise assumptions on the connective structure, specifically the support of the mixing matrix. These assumptions enable identification of the true mixing matrix \(}\) up to signed column permutation using second-order statistics and sparsity constraint. Roughly speaking, with the population-level covariance matrix \(}\), we consider the following formulation:

\[_{^{n n}}\|\|_{0}^{}=}=} }^{}.\] (1)

Note that we start with the assumption of \(^{}=}\) in the formulation above and our identifiability result (e.g., in Theorem 1); such an assumption can be obtained, e.g., from the equality of Gaussian likelihoods in the large sample limit. This also inspires our sparsity-regularized likelihood-based estimation method that will be described in Section 4.2, which is more inline with, e.g., model selection approaches based on sparsity-regularized likelihood [36; 18].

In Section 3.1, we first examine various types of constraints arising from second-order statistics. We then present the main identifiability result in Section 3.2. We show that our assumptions are strictly weaker than existing sparsity assumptions in Section 3.3, and establish the connection between our identifiability theory with causal discovery in Section 3.4. All proofs are given in Appendices C and D.

### Semialgebraic Constraints Arising from Second-Order Statistics

We first discuss various notions related to constraints arising from covariance matrices of observed variables \(\), which serve as a fundamental basis for introducing our assumptions and identifiability result of ICA in Section 3.2. These notions have been studied in the field of algebraic statistics , factor analysis , graphical models [19; 40], and causality [14; 20].

We begin with the following definition on the set of covariance matrices entailed by \(\) for different values of the free parameters in \(\).

**Definition 2** (Covariance Set).: _The covariance set of support matrix \(\) is defined as_

\[()\{^{}: ^{n n},() (),\,\}.\]

The support \(\) of mixing matrix \(\) imposes certain constraints on the entries of the covariance matrix \(=^{}\), which, by Tarski-Seidenberg theorem (see ), correspond to _semialgebraic constraints_, i.e., equality and inequality constraints. The covariance set \(()\) is then said to be a _semialgebraic set_, i.e., a set that can be described with a finite number of polynomial equations and inequalities . Clearly, if a covariance matrix \(\) belongs to the covariance set \(()\), then \(\) satisfies the semialgebraic constraints imposed by \(\).

For an equality constraint, the set of values satisfying the constraint has zero Lebesgue measure over the parameter space involved. Given a support matrix \(\), we denote by \(H()\) the set of equality constraints it imposes on the corresponding covariance matrices. On the other hand, the set of values satisfying an inequality constraint has nonzero Lebesgue measure. To illustrate these constraints, we provide a three-variable example below. Note that the example only serves as illustrations of the constraints; our estimation methods (in Section 4) do not require deriving them in practice.

**Example 1** (Semialgebraic Constraints).: _Consider support matrices_

\[_{1}=&0&0\\ &&0\\ &0&_{2}= &0&\\ &&0\\ 0&&.\]

_The equality constraints imposed by \(_{1}\) include_

\[_{1,1}_{2,3}-_{1,2}_{1,3}=0,\]

_while the inequality constraints imposed by \(_{2}\) include_

\[(_{1,1}_{2,2}_{3,3}+_{1,1}_{2,3}^{2}-_{2,2 }_{1,3}^{2}-_{3,3}_{1,2}^{2})^{2}-4(_{1,1}_{2,2}- _{1,2}^{2})(_{1,1}_{3,3}_{2,3}^{2}-_{1,3}^{2} _{2,3}^{2}) 0.\]

The detailed derivation can be found in Appendix D.1 and provides insights into how such constraints arise from the corresponding support matrices. In the example above, the covariance matrix \(\) generated by any mixing matrix \(\) with support \(_{1}\) must satisfy the corresponding equality constraint; similarly, the covariance matrix \(\) generated by any mixing matrix \(\) with support \(_{2}\) must satisfy the above inequality constraint. These constraints serve as footprints of the mixing matrix on the covariance matrix, and can be exploited for its identifiability, which we explain in the next section.

### Identifiability Result from Second-Order Statistics

In this section, we present our identifiability result of ICA from second-order statistics. The core idea is to introduce precise and mild assumptions on the connective structure from sources to observed variables. These assumptions facilitate the identification of the mixing matrix through the application of a sparsity constraint, formulated in Problem (1). To begin, we describe our primary assumption concerning the connective structure as follows.

**Assumption 1** (Structural Variability).: _Every pair of the columns in the support matrix of \(\) differ in more than one entry. That is, for every \(i,j[n]\) and \(i j\), we have_

\[|(_{i})(_{j})| -|(_{i})(_{j})| >1.\]

The assumption above implies that every pair of sources should influence more than one different observed variable. Notably, in the field of nonlinear ICA with auxiliary variable, Hyvarinen and Morioka , Hyvarinen et al.  have adopted the assumption of _sufficient variability_ which requires that the auxiliary variable has a sufficiently diverse effect on the distributions of sources; specifically, the conditional distributions of the sources given the auxiliary variable must vary sufficiently. In contrast, our assumption of _structural variability_ requires that every pair of sources influence sufficiently diverse sets of observed variables, facilitating the disentanglement of each source.

We provide several examples in Appendix E.1 to illustrate the broad applicability of the assumption above. Furthermore, the following proposition justifies such an assumption because it is a necessary condition for identifiability via second-order statistics and sparsity. The intuition is that if Assumption 1 is violated, there exists a rotation that maps matrix \(}\) to another matrix \(}\) which has equal or smaller number of nonzero entries and is not a column permutation of \(}\).

**Proposition 1**.: _If the true mixing matrix \(}\) does not satisfy Assumption 1, then there exists a solution \(}\) to Problem (1) such that \(}}\)._

**Remark 1** (Necessary Condition).: _Assumption 1 is a necessary condition for identifiability of ICA via second-order statistics and under sparsity constraint._

We also adopt the following assumption on the mixing matrix for the identifiability of ICA.

**Assumption 2** (Permutations to Lower Triangular Matrix).: _The matrix \(\) can be permuted by independent row and column permutations to be lower triangular. That is, there exist permutation matrices \(_{1}\) and \(_{2}\) such that \(_{1}^{}_{2}\) is lower triangular._

As we show in the proof of identifiability result in Theorem 1, Assumption 2, loosely speaking, ensures that the resulting covariance matrix does not contain "nontrivial" inequality constraints. In Example 1, support matrix \(_{1}\) satisfies Assumption 2 and leads to an equality constraint, while matrix \(_{2}\) fails to meet this assumption, resulting in an inequality constraint. The Lebesgue measure of the parameters leading to such inequality constraint is not zero, thus requiring additional assumptions to handle such cases. Therefore, we adopt Assumption 2 in this work and focus on equality constraints.

A key ingredient of our identifiability result based on sparsity is the dimension of the covariance set \(()\). It may be natural to expect that the dimension of \(()\), denoted as \((())\), equals the number of parameters used to specify the mixing matrices, i.e., \(\|\|_{0}\). This is not the case for general mixing matrices, but we show that such property holds under Assumption 2.

**Proposition 2** (Dimension of Covariance Set).: _Let \(\) be a support matrix that satisfies Assumption 2. Then, its covariance set has a dimension of \(\|\|_{0}\), i.e., \((())=\|\|_{0}\)._

Note that Assumption 2 allows independent row and column permutations, which thus may be rather mild especially for sparse mixing matrix. Below we provide an example of the connective structure that satisfies this assumption. We also introduce an efficient approach to verify whether a mixing matrix satisfies Assumption 2 in Appendix E.2.

**Example 2**.: _If the connective structure \(_{}\) of mixing matrix \(\) is a polytree, then matrix \(\) satisfies Assumption 2._

Finally, the following assumption is needed to ensure that the equality constraints arising from the covariance matrix are entailed by the true mixing matrix, rather than accidental parameter cancellations. This establishes a correspondence between equality constraints in the covariance matrix and those imposed by the support of the mixing matrix. Similar assumption has been employed in various tasks such as causal discovery , as discussed in Section 3.4.

[MISSING_PAGE_FAIL:5]

### Connection with Causal Discovery

ICA has emerged as a useful tool for causal discovery over the past two decades . In particular, Shimizu et al.  demonstrated that the identifiability of ICA based on non-Gaussianity can be leveraged to discover the complete structure of a linear non-Gaussian structural equation model (SEM). In this section, we establish the connection and provide an analogy between ICA and causal discovery from second-order statistics. This connection further bridges the gap between these two fields, and provides insights into the interpretation of our identifiability result.

Let \(_{}^{n n}\) be the set of matrices whose diagonal entries are zero, and \((_{>0}^{n})\) be the set of positive diagonal matrices. Consider the linear SEM \(=}^{}+\), where \(\) denotes the random vector, \(}_{}^{n n}\) denotes the weighted adjacency matrix representing a directed graph without self-loop, and \(\) is the independent noise vector with covariance matrix \(}(_{>0}^{n})\). The graph is often assumed to be a directed acyclic graph (DAG); in this case, two DAGs are said to be _Markov equivalent_ if they share the same skeleton and v-structures , resulting in the same set of conditional independencies. Also, the inverse covariance matrix of \(\) is given by \(}=(-})}^{-1}(-})^{}\). We refer readers to Spirtes et al. , Glymour et al.  for more details and a review of causal discovery.

Score-based method is a major class of causal discovery methods that optimizes a goodness-of-fit measure under a sparsity constraint , e.g., BIC . In essence, score-based causal discovery from second-order statistics can often be formulated in the large sample limit as the following optimization problem (the commonly used acyclicity constraint is omitted here and will be clarified subsequently):

\[_{_{}^{n n}, \\ (_{>0}^{n})}\| \|_{0}(-)^{-1}(-)^{}=}=(-})}^{-1}(-})^{}.\] (4)

By substituting \(:=(-)^{-}\) into the above formulation, we obtain the ICA formulation with second-order statistics and sparsity constraint introduced in Problem (1). To establish a precise connection between formulations (4) and (1), we present the following theorem which indicates that these formulations can be translated into each other.

**Theorem 4** (**Equivalent Formulations)**.: _Suppose \(}=(-})}^{- }\). Then, we have:_

1. _Let_ \((},})\) _be a solution to Problem (_4_). Then,_ \(}(-})}^{- }\) _is a solution to Problem (_1_)._
2. _Let_ \(}\) _be a solution to Problem (_1_). Then, there exist matrices_ \(}_{}^{n n}\) _and_ \(}(_{>0}^{n})\) _such that_ \(}(-})}^{- }\)_, and_ \((},})\) _is a solution to Problem (_4_)._

Thus, the formulations of causal discovery and ICA via second-order statistics and sparsity constraint share inherent similarities. The key difference lies in their respective goals-the former aims to estimate the support of \(}\) up to a Markov equivalence class , while the latter aims to estimate \(}\) up to signed column permutation. The other difference is that \((-})}^{-1}(- {})^{}\) represents the inverse covariance matrix \(}\) of \(\) in causal discovery, while \(}}^{}\) represents the covariance matrix \(}\) of \(\) in ICA.

In addition to establishing the connection between formulations (4) and (1), we show that the assumptions we employ for identifiability of ICA, namely Assumptions 1, 2, and 3, are inherently related to causal discovery. Notably, Assumption 3 has been used in causal discovery  to ensure that the conditional independencies in the distribution are entailed by the true directed graph. We now present a result that establishes the connection of Assumptions 1 and 2 with causal discovery.

**Theorem 5**.: _Suppose \((-)^{-}\) for matrices \(^{n n}\), \(_{}^{n n}\), and \((_{>0}^{n})\). Then, \(\) satisfies Assumptions 1 and 2 if and only if \(\) represents a DAG whose Markov equivalence class is a singleton._

In causal discovery, it is rather common to assume that the true directed graph is acyclic and accordingly incorporate an acyclicity constraint to formulation (4). As indicated in Theorem 5 (and Proposition 10 in Appendix D.8), this acyclicity assumption corresponds to Assumption 2 in the context of ICA. Therefore, Theorem 4 can be straightforwardly extended to show the equivalence between formulations (2) and (4) with an additional acyclicity constraint on matrix \(\). Furthermore, it is worth noting that the mapping from mixing matrix \(\) satisfying Assumption 2 to a DAG is unique, which is straightforwardly implied by Shimizu et al. [38, Appendix A].

**Proposition 4** (Shimizu et al. ).: _Suppose matrix \(\) is non-singular and satisfies Assumption 2. Then, there exist unique matrices \(_{}^{n n}\) and \((_{>0}^{n})\) such that \((-)^{-}\). Furthermore, matrix \(\) represents a DAG._

Moreover, as indicated in Theorem 5 (and Proposition 11 in Appendix D.8), Assumption 1 implies that the Markov equivalence class of \(\) is a singleton; in this case, the true DAG can be completely identified. In particular, the Markov equivalence class of DAG is a singleton when all edges are either part of a v-structure or required to be oriented to avoid forming new v-structures or cycles [31; 4].

## 4 Estimation Methods with Second-Order Statistics

Building upon the identifiability result provided in Section 3, we propose two estimation methods that leverage second-order statistics and sparsity. These methods involve solving a continuous constrained optimization problem, which we discuss in detail in this section. First, in Section 4.1, we introduce a novel approach to formulate the search space in Problem (2) that enables the application of continuous optimization techniques. We then describe the proposed estimation methods in Section 4.2. All proofs are provided in Appendix D.

### Characterization of Search Space

The key to achieving the identifiability result presented in Theorem (1) lies in the optimization problem (2), where the search space involves the matrices \(\) that satisfy Assumption 2. Consequently, a crucial question arises: is there an efficient approach for exploring the space of matrices \(\) that satisfy Assumption 2? Inspired by Zheng et al. , Wei et al. , Zhang et al. , we introduce the following function to characterize the search space:

\[g()=(_{k=2}^{n}(() ())^{k}),\ \ \ \ (())_{i,j}=0,&i=j,\\ a_{i,j},&.\]

Here, symbol \(\) denotes the Hadamard product. We then provide the following lemma that establishes the relationship between function \(g()\) and a specific type of permutation, namely simultaneous equal row and column permutation.

**Lemma 1**.: _For any matrix \(\), \(g()=0\) if and only if it can be permuted via simultaneous equal row and column permutations to be lower triangular._

Intuitively speaking, if we interpret matrix \(\) as a weighted adjacency matrix of a directed graph, say \(\), then \(((()())^{k})\) counts the number of length-\(k\) weighted closed walks in \(\) excluding the self-loops. Therefore, \(g()\) counts the total number of weighted closed walks in \(\) without including self-loops. \(g()=0\) then implies that \(\) does not contain any cycle longer than one (i.e., it may contain self-loops). It is known that a directed graph is acyclic if and only if its weighted adjacency matrix can be permuted via simultaneous equal row and column permutations to be _strictly_ lower triangular. In our case, \(\) may contain self-loops, and thus can be permuted to a lower triangular form. This distinction elucidates the difference between our characterization and that introduced by Zheng et al. , Zhang et al. , i.e., their characterization focuses on matrices that are strictly lower triangular, while ours focuses on lower triangular matrices.

The following proposition sheds light on the connection between \(g()\) and Assumption 2.

**Proposition 5**.: _The matrix \(\) satisfies Assumption 2 if and only if there is a matrix \(}\) such that it is a column permutation of \(\) and that \(g(})=0\)._

The above proposition indicates that the search for matrices \(\) satisfying Assumption 2 can be effectively conducted by considering the constraint \(g()=0\). Accordingly, we establish an alternative formulation of the identifiability result presented in Section 3.2. In the following section, we will introduce efficient approaches for solving Problem (5).

**Theorem 6** (Alternative Formulation of Identifiability).: _Suppose that the true mixing matrix \(}\) satisfies Assumptions 1, 2, and 3. Let \(}\) be a solution of the following problem:_

\[_{^{n n}}\|\|_{0}\ ^{}=}}^{ } g()=0.\] (5)

_Then, we have \(}}\)._```
0: initial penalty coefficient \(c_{1}>0\); multiplicative factor \(>1\); maximum number of iterations \(k_{}>0\); tolerance \(_{1},_{2}>0\); initial solution \(_{0}\); empirical covariance matrix \(}\)
1:for\(k=1,2,,k_{}\)do
2: Solve \(_{k}_{^{n n}}( )+}{2}\|^{}- }\|_{F}^{2}+}{2}g()^{2}\) initialized at \(_{k-1}\)
3:if\(\|_{k}_{k}^{}-}\|_{F}^{2}< _{1}\) and \(g(_{k})<_{2}\)then break
4: Update penalty coefficient \(c_{k+1} c_{k}\)
5:Output solution \(_{k}\) ```

**Algorithm 1** Decomposition-Based Method

### Estimation Methods

Based on the identifiability results of Theorems 1 and 6, we propose two estimation methods, called _SparseICA_, to perform ICA from second-order statistics that leverage sparsity regularization and continuous constrained optimization. To proceed, we define \(}\) as the empirical covariance matrix of observed variables \(\) and \(T\) as the sample size.

Decomposition-based method.Given the formulation in Eq. (5), we consider the following constrained optimization problem

\[_{^{n n}}()^{}-}=  g()=0,\] (6)

where \(()\) is a suitable sparsity regularizer, often expressible as \(()=_{i,j}(a_{i,j})\). Formulation (5) indicates that one should apply the \(_{0}\) regularizer \(()=\|\|_{0}\). Alternatively, other possible choices include the \(_{1}\) regularizer \(()=\|\|_{1}\) that supports continuous optimization. Further details regarding our specific choice of sparsity regularizer will be elaborated later in this section. On the other hand, we simply use the empirical covariance matrix \(}\) as an estimate of the true covariance matrix \(}\), which is found to work well across different sample sizes in our experiments. One may also adopt a regularized estimator of the form \(}+\) with a proper choice of \(\), which may have notable advantage in certain cases [13; 28; 37].

Likelihood-based method.In addition to the decomposition-based method above, we introduce a likelihood-based estimation method formulated by the following constrained optimization problem:

\[_{^{n n}}L(;})+() g( )=0,\] (7) \[ L(;})=((^{})^{-1}})+ T||\]

is the negative Gaussian log-likelihood function and \(()\) is a sparsity regularizer. The following result establishes the theoretical guarantee of this likelihood-based method.

**Theorem 7** (Likelihood-Based Method).: _Suppose that the true mixing matrix \(}\) satisfies Assumptions 1, 2, and 3. Let \(}\) be a solution of Problem (7) with sparsity regularizer \(()=0.5\|\|_{0} T\). Then, we have \(}}\) in the large sample limit._

Implementation.Based on Theorems 6 and 7, ideally one should adopt the \(_{0}\) regularizer \(()=\|\|_{0}\) and develop an exact discrete search procedure over the support space of matrix \(\). However, such approach may pose computational challenges in practice. Since the functions \(L(;})\) and \(g()\) are differentiable, in this work we develop an estimation procedure that leverages efficient continuous optimization techniques. Therefore, some possible choices for \(()\) are \(_{1}\), smoothly clipped absolute deviation (SCAD) , and minimax concave penalty (MCP)  regularizers. The \(_{1}\) regularizer has been shown to exhibit bias during estimation [17; 10], especially for large coefficients. Here, we adopt the MCP regularizer that is less susceptible to such issue, given by

\[(a_{i,j})=|a_{i,j}|-^{2}}{2},& {if }|a_{i,j}|,\\ }{2},&,\]

where \(\) and \(\) are hyperparameters.

To solve Eqs. (6) and (7), standard constrained optimization methods can be used, such as quadratic penalty method, augmented Lagrangian method, and barrier method [7; 8; 33]. In this work, we adopt the quadratic penalty method that converts each constrained problem into a sequence of unconstrained optimization problems where the constraint violations are increasingly penalized. We describe the full procedure of the decomposition-based and likelihood-based methods based on quadratic penalty method in Algorithms 1 and 2, respectively. The unconstrained problem in each iteration can be solved using different continuous optimization solvers, including first-order methods such as gradient descent and steepest descent, as well as second-order methods such as quasi-Newton methods. In our experiments presented in the subsequent section, we employ L-BFGS , a quasi-Newton method, to solve the unconstrained optimization problem.

It is worth noting that the formulations in Eqs. (6) and (7) involve solving nonconvex optimization problems; in practice, the optimization procedure may return stationary points that correspond to suboptimal local solutions. Therefore, we run the method for a number of times and choose the final mixing matrix via model selection. Further details regarding the optimization procedure and implementation are provided in Appendix F.

## 5 Experiments

To empirically validate our proposed identifiability results, we carry out experiments under various settings. We also conduct ablation studies to verify the necessity of the proposed assumptions and include _FastICA_ as a representative baseline. Specifically, we consider the following methods:

* _SparseICA_: Decomposition-based (Eq. (6)) or likelihood-based (Eq. (7)) method on data where both Assumptions 1 and 2 hold;
* _Vanilla_: Decomposition-based (Eq. (6)) or likelihood-based (Eq. (7)) method without the constraint \(g()=0\), on data where neither Assumption 1 nor Assumption 2 holds;
* _FastICA-D_: FastICA on data where both Assumptions 1 and 2 hold;
* _FastICA_: FastICA on data where neither Assumption 1 nor Assumption 2 holds.

For all experiments, we simulate \(10\) sources, and generate the supports of the true mixing matrices \(}\) according to the assumptions required by each method above. The nonzero entries of \(}\) are sampled uniformly at random from \([-0.8,-0.2][0.2,0.8]\). We use mean correlation coefficient (MCC) and Amari distance  as evaluation metrics, where all results are reported for 10 random trials.

**Different sample sizes.** We first consider entirely Gaussian sources and different sample sizes. The empirical results of MCC are shown in Figure 1, while those of Amari distance are given in Figure 4 in Appendix G. By comparing _SparseICA_ with _Vanilla_ and _FastICA_, it is evident that the identification performance is much better across different sample sizes when the required assumptions on the connective structure are satisfied, as validated by Wilcoxon signed-rank test at \(5\%\) significance level. Furthermore, the unsatisfactory results of _FastICA-D_ indicate that our estimation methods are also essential for ensuring the quality of the identification, which further validates the proposed identifiability theory. Since _FastICA-D_ performs similarly to _FastICA_, it suggests that the data-generating process, while meeting our assumption, may not be inherently simpler to recover without considering specific procedure to handle Gaussian sources. In addition, as expected, the performance of SparseICA improves in terms of both MCC and Amari distance as the sample size increases.

**Different ratios of Gaussian sources.** We now conduct empirical study to investigate the performance in the presence of Gaussian and non-Gaussian sources. Here, the non-Gaussian sources follow exponential distributions. We consider different ratios of Gaussian sources, which are specifically \(0\), \(0.2\), \(0.4\), \(0.6\), \(0.8\), and \(1\). For instance, ratio of \(0.6\) indicates that there are \(6\) Gaussian sourcesand \(4\) non-Gaussian sources. The empirical results of MCC based on \(1000\) samples are depicted in Figure 1, while those of Amari distance are provided in Figure 4 in Appendix G. One observes that the identification performance _SparseICA_ is rather stable across different ratios of Gaussian sources, which may not be surprising as it leverages only second-order statistics. On the other hand, the performance of _FastICA-D_ and _FastICA_ deteriorates as the ratio of Gaussian sources increases, because it relies on non-Gaussianity of the sources. It is also observed that, in the presence of Gaussian sources, _FastICA-D_ and _FastICA_ may perform well, provided that the ratio (or number) of Gaussian sources is not large. This suggests a potential future direction to integrate our method based on second-order statistics with existing methods that rely on non-Gaussianity, which may better handle both Gaussian and non-Gaussian sources.

## 6 Conclusion

We develop an identifiability theory of ICA from second-order statistics without relying on non-Gaussianity. Specifically, we introduce novel and precise assumptions on the connective structure from sources and observed variables, and show that our proposed assumption of structural variability is strictly weaker than the previous ones. Importantly, we prove that this assumption is one of the necessary conditions for achieving identifiability in the investigated setting. We further propose two estimation methods based on second-order statistics that leverage sparsity regularization. Moreover, we establish a precise connection between our identifiability result of ICA and causal discovery from second-order statistics, which may open up avenues for exploring the interplay between ICA and causal discovery with linear Gaussian SEM. Our theoretical claims have also been empirically validated across different settings. The limitations include the lack of finite sample analysis and broader application of our theory in more real-world tasks, which are worth exploring in future work.

Figure 1: Empirical results of MCC across different sample sizes. Error bars indicate the standard errors calculated based on \(10\) random trials.

Figure 2: Empirical results of MCC across different ratios of Gaussian sources. Error bars indicate the standard errors calculated based on \(10\) random trials.