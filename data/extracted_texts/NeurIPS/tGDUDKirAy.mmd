# Verified Safe Reinforcement Learning for Neural Network Dynamic Models

Junlin Wu

Computer Science & Engineering

Washington University in St. Louis

junlin.wu@wustl.edu &Huan Zhang

Electrical & Computer Engineering

University of Illinois Urbana-Champaign

huan@huan-zhang.com &Yevgeniy Vorobeychik

Computer Science & Engineering

Washington University in St. Louis

yvorobeychik@wustl.edu

###### Abstract

Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at https://github.com/jlwu002/VSRL.

## 1 Introduction

The ability to synthesize safe control policies is one of the core challenges in autonomous systems. This problem has been explored from numerous directions across multiple disciplines, including control theory and AI . While considerable progress has been made, particularly when dynamics are linear , the ability to synthesize controllers that can be successfully _verified_ to be safe while maintaining high performance in nonlinear dynamical systems remains a major open problem. Indeed, even the subproblem of safety verification in nonlinear systems is viewed in itself as a major challenge and is an active area of research, particularly for neural network controllers . State-of-the-art approaches for safe control synthesis, including most that leverage reinforcement learning , typically only offer empirical evaluation of safety, and rely on safety proofs that hold either asymptotically (rather than for concrete problems)  or under idealized assumptions which do not hold in practice .

Two common properties are typically leveraged in safety verification: _forward invariance_ and _reachability_. The former aims to identify a set of starting subsets of safe states under which one-step(forward) dynamics remain in this (forward invariant) set. The latter computes the set of states that can possibly be reached after \(K\) steps of the dynamics for a given control policy, and checks whether it intersects with the unsafe set. Approaches for synthesizing (including those that do so using learning) safe policies almost exclusively aim to achieve verified safety through forward invariance. However, this has proved extremely challenging to employ beyond the simplest dynamics.

We propose the first (to our knowledge) approach for learning \(K\)-step verified safe neural network controllers that also aim to maximize efficiency in systems with neural dynamics. While neural dynamics are clearly not universal, they can capture or effectively approximate a broad range of practical dynamical systems (Nagabandi et al., 2018), and have consequently been the focus of much prior work in safe control and verification (Dai et al., 2021). For example, consider the scenario of a drone navigating through a series of obstacles to reach a designated goal, requiring \(K=50\) steps to safely maneuver through the obstacles. We aim to train a controller that can reach the goal as fast as possible, while guaranteeing safety for the initial \(50\) steps, ensuring 1) the drone does not collide with any obstacles and 2) its angle remains within a predefined safe range.

Our approach combines deep reinforcement learning with state-of-the-art differentiable tools for efficient reachability bound computation, and contains two key novel ingredients. The first is a novel curriculum learning scheme for learning a verified safe controller. This scheme takes advantage of the structure of the \(K\)-reachability problem at the root of our safety verification by creating a curriculum sequence with respect to increasing \(K\). An important insight that is specific to the verification setting is that verification must work not merely for a fixed \(K\), but for all steps prior, an issue we address by memorizing subsets of states who either violate, or nearly violate, safety throughout the entire \(K\)-step curriculum learning process. Additionally, to maintain _both_ strong empirical and verified performance, we propose a novel loss function that integrates overall reward, as well as _both_ traditional (empirical) safety loss along with the \(K\)-reachability bound. Our second innovation is to learn a _collection_ of controllers that depend on the initial state, in contrast to typical approaches that focus on learning a single "universal" controller. The ability to allow for learning multiple controllers makes the verified learning problem considerably easier, as we can "save" controllers that work on a subset of initial states, and simply try learning a new controller for the rest, guaranteeing incremental improvement through the learning process. We further improve performance through incremental verification, which leverages information obtained in previous learning iterations.

We evaluate the proposed approach in five control settings. The first two are lane following and obstacle avoidance, both pertaining to autonomous driving. The last three involve drone control with obstacle avoidance. Two of these consider fixed obstacles, while the third aims to avoid even moving obstacles (with known dynamics). We show that the proposed approach outperforms five state-of-the-art safe control baselines in the ability to achieve verified safety without significantly compromising overall reward (efficiency). In particular, our approach learns controllers that can verify \(K\)-step safety for \(K\) up to an order of magnitude larger than the prior art and maintains a perfect safety record for \(K\) far above what we verify, something no baseline can achieve.

In summary, we make the following contributions:

1. A framework for safe optimal control that combines both finite-horizon verified (worst-case) and empirical (average-case) safety constraints.
2. A novel curriculum learning approach that leverages memorization, forward reachability analysis, and differentiable reachability overapproximation for efficiently learning verified safe policies.
3. An approach for learning a _collection_ of control policies that depend on the initial state which enables significant improvements in verified safety horizon over large initial state sets \(S_{0}\).
4. An incremental verification approach that leverages small changes in gradient-based learning to improve verification efficiency during learning.
5. An extensive experimental evaluation that demonstrates the efficacy of the proposed approach in comparison with five state-of-the-art safe RL baselines.

Related Work:Safe reinforcement learning has been extensively studied through the lens of constrained Markov decision process (CMDP)-based approaches, which represent cost functions as constraints and aim to maximize reward while bounding cost, using approaches such as Lagrangian and penalty methods, and constrained policy optimization (Achiam et al., 2017; Stooke et al., 2020; Ma et al., 2022; Jayant and Bhatnagar, 2022; Yu et al., 2022; So and Fan, 2023; Ganai et al., 2024).

An alternative control-theoretic perspective aims to ensure stability or safety using Lyapunov and control barrier functions. For example, Dawson et al. (2022) used a learning-based approach to find robust control Lyapunov barrier functions; Chow et al. (2018) constructed Lyapunov functions to solve CMDPs; (Wang et al., 2023) proposed soft barrier functions for unknown and stochastic environments; and Alshiekh et al. (2018) created safety shielding for safe RL agents. These approaches, however, provide no practical formal safety guarantee for neural network controllers. In addition, some work on provably safe RL focuses on the probabilistic setting (Berkenkamp et al., 2017; Jansen et al., 2020; Xiong et al., 2024) and required statistical assumptions, whereas our work aims for strict deterministic safety guarantees over a finite horizon.

Among existing works focusing on safe RL with formal guarantees, Fulton and Platzer (2018) apply a theorem prover for differential dynamic logic to guarantee safety during runtime. Noren et al. (2021) and Wei et al. (2022) consider forward safety invariance for systems with uncertainty. Kochdumper et al. (2023) propose to project actions to safe subspace using zonotope abstraction and mixed-integer programming (MIP). However, these approaches do not readily apply to neural network controllers. For systems involving neural networks, Wei and Liu (2022) applied integer programming formulation for neural networks to solve an MIP problem to find safe control actions satisfying forward invariance; Bastani et al. (2018) extracted decision-tree-based policies for RL to reduce verification complexity; and Ivanov et al. (2019) used hybrid system verification tools to model deep neural networks. Our work differs from these and similar approaches because we consider forward reachability guarantees for neural network controllers in neural nonlinear systems.

We make extensive use of neural network verification tools. Early work in this vein used SMT (Katz et al., 2017; Huang et al., 2017) or MIP-based (Tjeng et al., 2019) approaches to solve this problem, but their scalability is extremely limited. Significant progress has been made in developing techniques to formally verify the properties of large neural networks through overapproximation, such as bound propagation (Zhang et al., 2018; Gowal et al., 2018; Xu et al., 2021), optimization (Qin et al., 2019; Dvijotham et al., 2018, 2020), and abstract interpretation (Gehr et al., 2018; Singh et al., 2019; Katz et al., 2019; Lopez et al., 2023). Recently, most verifiers have adopted branch-and-bound based approaches to further enhance their performance (Wang et al., 2021; Kouvaros and Lomuscio, 2021; Ferrari et al., 2022; Zhang et al., 2022). Our approach makes use of differentiable overapproximation methods known collectively as \(\),\(\)-CROWN (Wang et al., 2021; Zhang et al., 2022) (implemented with the auto_LiRPA package), and takes advantage of the particular structure of these verification approaches in applying incremental verification to significantly speed up safe controller learning.

## 2 Preliminaries

**Constrained Markov Decision Process (CMDP):** We consider a deterministic Constrained Markov Decision Process (CMDP) defined by the tuple \((,,F,R,,C_{1},C_{2},,C_{m},d_{1},d_{2}, ,d_{m})\), where: \(\) is a set of states, \(\) is a set of actions, \(F:\) is the deterministic state transition function, \(R:\) is the reward function, \(C_{i}:\) is the cost function for the \(i\)-th constraint, \(d_{i}\) is the cost limit for the \(i\)-th constraint, and \([0,1)\) is the discount factor. A policy \(:\) is a mapping from states to actions. A trajectory is a sequence of states and actions generated by following a policy \(\) from some initial state \(s_{0}_{0}\), which can be represented as a sequence \(=(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},)\) where \(s_{t}\), \(a_{t}=(s_{t})\) for all \(t\), \(s_{t+1}=F(s_{t},a_{t})\), a reward \(r_{t}=R(s_{t},a_{t})\) and a cost \(c_{t}=_{i[m]}C_{i}(s_{t},a_{t})\) are received after each action.

We denote \(_{}\) as the policy that is parameterized by the parameter \(\). A common goal for CMDP is to learn a policy \(_{}\) that maximizes a discounted sum of rewards \((_{})\) while ensuring that expected discounted costs \(_{C_{i}}(_{})\) do not exceed the cost limit \(d_{i}\), \( i[m]\). Formally, CMDP is to solve the below optimization problem:

\[_{}(_{})\ _{C_{i}}(_{ }) d_{i}, i[m],\] (1)

where \((_{})=_{}[_{t=0}^{} ^{t}R(s_{t},a_{t})]\) and \(_{C_{i}}(_{})=_{}[_{t=0}^{ }^{t}C_{i}(s_{t},a_{t})]\).

**Verified Safe CMDP:** We define the state space as the union of predefined safe and unsafe states, denoted as \(=_{}_{}\). We assume that the transition function \(F\) is represented by a ReLU neural network, and is known for verification purposes. This assumption is very general, as many known dynamical systems can be represented exactly or approximately using ReLU neural networks (Gillespie et al., 2018; Pfrommer et al., 2021; Dai et al., 2021; Liu et al., 2024). Our

[MISSING_PAGE_FAIL:4]

typical structure of safety constraints that only pertain to a small subset of state variables. For instance, in drone control for obstacle avoidance, we prioritize splitting the location and angle axes. Next, we design a cost function \(C_{R}\) for regions where \(C_{R}(S)=0\) if \(S_{}=,C_{R}(S)>0\) otherwise. A positive \(C_{R}\) means region \(S\) intersects with \(_{}\), while \(C_{R}=0\) indicates \(S\) is safe. For example, if the task is to avoid the region \([a,b]\), and the output bounds are given by \(x_{B}=[x_{lb},x_{ub}]\), we can define \(C_{R}(x_{B})=(x_{ub}-a,0)(b-x_{lb},0)\). We then calculate the gradient \( C_{R}(F^{t,_{}}_{}(_{0}))/ r\) for a chosen value of \(t\) and proceed to split along the dimensions with the largest gradient values, as a larger gradient indicates a higher likelihood of reducing the cost \(C_{R}\). We continue this process, keeping the total number of grid splits within a predetermined budget, and stop splitting once the budget is reached.

For each training phase \(k\), we monitor the training rounds (\(n_{}\)) as well as the \(k\)-step forward reachable regions returned by the verifier that are identified as unsafe (\(S_{uc}\)). Each phase is conducted for a maximum of \(n_{}\) rounds or until verified \(k\)-step safety is achieved, that is, when \(S_{uc}=\) (Line 8). At the end of each training phase, we also filter out regions \(S_{k}_{0}\) where \(F^{k,_{}}_{}(S_{k})\) are within \(\) distance to the unsafe regions. These regions are then stored in the buffer \(B\) (Line 13). We include these critical regions in the training set for each reinforcement learning update to enhance verified safety across the entire horizon. During this process, we optionally use the Branch-and-Bound algorithm (Everett et al., 2020; Wang et al., 2021) to refine \(_{0}\) up to a predetermined branching limit, which helps achieve tighter bounds.

For each RL update, we use a loss function that integrates the standard safe RL loss with a \(k\)-phase loss for bounds (Line 9), where

\[(x) =_{}(x)+_{ }(S_{uc} B)\] (3a) \[_{}(S_{uc} B) =C_{R}(F^{k,_{}}_{}(S_{uc}))+_{(S_{i},i)  B}C_{R}(F^{i,_{}}_{}(S_{i})).\] (3b)

Here, \(_{}\) is the standard safety RL loss, and \(_{}\) denotes the loss that incentivizes ensuring the output bounds returned by the verifier remain within the safe region. If both \(S_{uc}\) is \(k\)-step safe and \((S_{i},i) B\), \(S_{i}\) is \(i\)-step safe, then \(_{}(S_{uc} B)=0\), otherwise, \(_{}(S_{uc} B)>0\). In practice, we clip \(_{}(S_{uc} B)\) to ensure it remains within a reasonable range for training stability. The regularization parameter \(\) is calculated based on the magnitude of \(_{}\) and \(_{}\), with \(=(_{},a_{r}_{}/ _{})\), where \(_{}\) and \(a_{r}\) are hyperparamters. This approach helps maintain the effectiveness of bound training, especially when \(_{}\) is small. Furthermore, we cluster elements in \(B\) into categories so that we do not need to construct a computational graph for all \(i<k\). Specifically, we merge all \(S_{i}\) for \(i_{1} i i_{2}\) into the \(i_{2}\) category, meaning the elements in \(B\) are now \((_{i_{1} i i_{2}}S_{i},i_{1},i_{2})\) instead of \((S_{i},i)\).

It is important to note that while our training scheme targets \(K\)-step verified safety, the policy returned by Algorithm 1 does not necessarily guarantee it. We address this issue by learning initial-state-dependent controllers as described below. Furthermore, the computation of \(_{}\) is computationally intensive. Its backpropagation requires constructing computational graphs for the \(k\)-th step forward NN \(F_{}^{k,_{}}\), as well as for all \(i\)-th step forward NNs corresponding to each \((S_{i},i) B\). These NNs become increasingly deep as \(k\) grows, causing the computational graphs to consume memory beyond the typical GPU memory limits. We will address this next.

**Incremental Verification:** Above we discussed the challenge presented by the backpropagation of \(_{}\), which is GPU-memory intensive and does not scale efficiently as the target \(K\)-step horizon increases. To mitigate these issues, we propose the use of incremental verification to enhance computational efficiency and reduce memory consumption. While incremental verification is well-explored in the verification literature Wang et al. (2023); Althoff (2015), to our knowledge, we are the first to apply it in _training_ provably safe controllers.

At a high level, to calculate the reachable region for a \(k_{}\) step, we decompose the verification into multiple phases. We begin by splitting the \(k_{}\) horizon into intervals defined by \(0<k_{1}<k_{2}<<k_{n}=k_{}\). We first calculate the reachability region for the \(k_{i}\) step and then use its output bounds as input to calculate the reachable region for the \(k_{i+1}\) step. This approach ensures that the computational graph is only built for the \((k_{i+1}-k_{i})\) step horizon when using \(\),\(\)-CROWN.

Unlike traditional incremental verification, which typically calculates the reachable region from \(k\) to \(k+1\), we incrementally verify and backpropagate several steps ahead in a single training iteration (i.e., from \(k_{i}\) to \(k_{i+1}\), where \(k_{i+1}-k_{i}>1\)). This generalized version of incremental verification is essential for training, as it significantly accelerates the process and reduces the likelihood of becoming trapped in "local optima," where inertia from the policy obtained for \(k\) prevents successful verification for \(k+1\) (e.g., due to proximity to the unsafe region with velocity directed toward it).

For the bounds used in neural network training, we effectively build the computational graph and perform backpropagation using a neural network sized for \((k_{n}-k_{n-1})\) steps' reachability, which is independent of \(k_{}\). This significantly reduces GPU memory usage. Since \(F^{k,_{}}\) is an iterative composition of \(F\) under the same policy \(_{}\), the bound for \(k_{n-1}\) steps tends to be tight. Moreover, when training \(_{}\) to tighten these bounds, the overall bound for the entire \(k_{}\) horizon becomes increasingly tight.

**Initial-State-Dependent Controller:** While curriculum learning above includes verification steps, it does not guarantee verified safety for the controller over the entire \(K\)-step horizon. In this section, we propose using an initial-state-dependent controller to address this issue. For example, in a vehicle avoidance scenario, different initial conditions, such as varying speeds and positions, may correspond to different control strategies. We introduce a mapping function \(h:_{0}\), which maps each initial state \(s_{0}\) to a specific policy \(_{h(s_{0})}\). The underlying idea is that training a verifiable safe policy \(_{}\) over the entire set of initial states \(_{0}\) is inherently challenging. However, by mapping each initial state to a specific set of parameters, we can significantly enhance the expressivity of the policy. This approach is particularly effective in addressing and eliminating corner cases in unverifiable regions.

At a high level, the mapping and parameter set \(\) are obtained by first performing comprehensive verification for the controller output from Algorithm 1 over the entire \(K\)-step horizon. We then filter unverified regions, cluster them, and fine-tune the controller parameters \(\) for each cluster. We store these fine-tuned parameters in the parameter set \(\). This iterative refinement process continues until for every \(s_{0}_{0}\), there exists a \(\) such that \(_{}\) is verified safe for the entire \(K\)-step horizon. The detailed algorithm is presented in Algorithm 2.

The algorithm starts with verifying the policy \(_{}\) obtained from Algorithm 1. The function \((_{},_{0},K)\) (Line 4) performs verification of policy \(_{}\) for initial states \(_{0}\) for the entire horizon \(K\). This verification process identifies and categorizes regions into verified safe areas, \(S^{V}_{}\), and areas identified as unsafe, \(S^{V}_{}\). Notably, the union of these regions covers all initial states, meaning \(S^{V}_{} S^{V}_{}=_{0}\). After verifying that any state \(s_{0} S^{V}_{}\) is guaranteed to be safe under policy \(_{}\), we record \((S^{V}_{},_{})\) in the mapping dictionary \(H\) (Line 5).

Next, we address the unsafe regions \(S^{V}_{}\) that lack a corresponding verified safe controller. We first cluster them based on the type of safety violation (Line 7). The reason for clustering is that regions with similar safety violations are more likely to be effectively verified safe by the same controller. For instance, in a scenario involving navigation around two obstacles, we could potentially identify up to three clusters: the first corresponding to grids that can lead to collisions with obstacle 1, the second includes grids associated with collisions with obstacle 2, and the third is the set of grids that may lead to collisions with both. Given the finite number of safety constraints, the number of possible clusters is also finite. Although the theoretical maximum number of clusters grows exponentially with the number of safety constraints, in practice, this number is significantly smaller. This is due to the fact that the controller, being pretrained, is less likely to violate multiple or all constraints simultaneously. We then fine-tune the controller for the initial states in each cluster using Algorithm 1. This fine-tuning process is typically fast, as the initial policy is already well-trained. We store each initial state region and its corresponding verified safe policy in the mapping dictionary \(H\). This clustering and fine-tuning process continues until a verified safe policy exists for every \(s_{0}_{0}\).

At decision time, given an initial state \(s_{0}\), we first identify the pair \((S_{}^{V},_{})\) in the mapping dictionary \(H\) where \(s_{0} S_{}^{V}\), then use the corresponding verified safe controller \(_{}\). Note that the soundness of the algorithm directly follows from our use of the sound verification tool \(\),\(\)-CROWN.

```
1:Input: target safety horizon \(K\), policy \(_{}\)
2:Output: mapping dictionary \(H\), which includes the mapping \(h\) and parameter set \(\)
3:Initialize \(H=\{\}\)
4:\((S_{}^{V},S_{}^{V})( _{},_{0},K)\)
5:Store \((S_{}^{V},)\) in mapping dictionary \(H\)
6:while\(S_{}^{V}\)do
7:\(\{S_{1},S_{2},,S_{I}\}(S_{}^{V})\) // cluster based on safety violation
8:for\(i=1,2,,I\)do
9:\(_{^{}}(_{},S_{i},K)\)
10:\((S_{,i}^{V},S_{,i}^{V})( _{^{}},S_{i},K)\)
11: Store \((S_{,i}^{V},^{})\) in mapping dictionary \(H\)
12:endfor
13:\(S_{}^{V}_{i}S_{,i}^{V}\)
14:endwhile ```

**Algorithm 2** Initial-State-Dependent Controller

## 4 Experiments

### Experiment Setup

We evaluate our proposed approach in five control settings: Lane Following, Vehicle Avoidance, 2D Quadrotor (with both fixed and moving obstacles), and 3D Quadrotor (Kong et al., 2015; Dai et al., 2021). The dynamics of these environments are approximated using NN with ReLU activations. We use a continuous action space for those discrete-time systems. In each experiment, we specify the initial region \(_{0}\) for which we wish to achieve verified safety. We then aim to achieve the maximum \(K\) for which safety can be verified. We evaluate the approaches using four metrics: 1) _Verified-\(K\)_: the percentage of regions in \(_{0}\) that can be verified for safety over \(K\) steps; 2) _Verified-Max_: the maximum number of steps for which all states in \(_{0}\) can be verified as safe; 3) _Emp-k_: the percentage of regions in \(_{0}\) that are empirically safe for \(k\) steps, obtained by sampling \(10^{7}\) datapoints from the initial state \(_{0}\). This is evaluated for both \(k=K\) (the number of steps we are able to verify safety for) and \(k=T\) (total episode length); 4) _Avg Reward_: the average reward over 10 episodes, with both mean and standard deviations reported. Note that the average reward is computed over the entire episode horizon for each environment, independently of the verification horizon, as in conventional reinforcement learning.

We compare the proposed _verified safe RL (VSRL)_ approach to six baselines: 1) PPO-Lag, which utilizes constrained PPO with the standard Lagrangian penalty (Achiam et al., 2017); 2) PPO-PID, which employs constrained PPO with PID Lagrangian methods (Stooke et al., 2020); 3) CAP, which adopts model-based safe RL with an adaptive penalty (Ma et al., 2022); 4) MBPPO, which applies model-based safe RL with constrained PPO (Jayant and Bhatnagar, 2022); 5) CBF-RL, which is a Control Barrier Function (CBF)-based safe reinforcement learning approach (Emam et al., 2022); and 6) RESPO, which implements safe RL using iterative reachability estimation (Ganai et al., 2024).

Next, we describe the four autonomous system environments in which we run our experiments. Further experimental setup details are provided in Appendix A.2.

    &  \\  & Verified-80(\(\)) & Verified-Max(\(\)) & Emp-80(\(\)) & Emp-500(\(\)) & Avg Reward(\(\)) \\  PPO-Lag & \(98.6\) & \(7\) & \(99.9\) & \(99.9\) & \(326 6\) \\ PPO-PID & \(88.5\) & \(8\) & \(99.9\) & \(99.9\) & \(327 6\) \\ CAP & \(99.5\) & \(7\) & \(99.9\) & \(99.9\) & \(357 4\) \\ MBPPO & \(99.7\) & \(8\) & \(99.9\) & \(99.9\) & \(382 5\) \\ CBF-RL & \(98.7\) & \(7\) & \(99.9\) & \(99.9\) & \(331 7\) \\ RESPO & \(99.8\) & \(7\) & \(99.9\) & \(99.9\) & \(\) \\  VSRL & \(\) & \(\) & \(\) & \(\) & \(214 5\) \\    \\  & Verified-50(\(\)) & Verified-Max(\(\)) & Emp-50(\(\)) & Emp-500(\(\)) & Avg Reward(\(\)) \\  PPO-Lag & \(72.8\) & \(6\) & \(87.8\) & \(87.8\) & \(303 12\) \\ PPO-PID & \(72.0\) & \(6\) & \(89.4\) & \(89.4\) & \(287 22\) \\ CAP & \(73.3\) & \(13\) & \(89.5\) & \(89.5\) & \(393 35\) \\ MBPPO & \(82.6\) & \(6\) & \(94.2\) & \(94.2\) & \(375 10\) \\ CBF-RL & \(73.0\) & \(6\) & \(89.3\) & \(89.3\) & \(301 15\) \\ RESPO & \(74.5\) & \(9\) & \(89.6\) & \(89.6\) & \(391 20\) \\  VSRL & \(\) & \(\) & \(\) & \(\) & \(\) \\    \\  & Verified-50(\(\)) & Verified-Max(\(\)) & Emp-50(\(\)) & Emp-500(\(\)) & Avg Reward(\(\)) \\  PPO-Lag & \(0.0\) & \(5\) & \(83.4\) & \(83.4\) & \(405 30\) \\ PPO-PID & \(0.0\) & \(4\) & \(99.3\) & \(97.5\) & \(\) \\ CAP & \(0.0\) & \(3\) & \(99.5\) & \(99.5\) & \(393 12\) \\ MBPPO & \(58.9\) & \(9\) & \(99.9\) & \(84.5\) & \(399 11\) \\ CBF-RL & \(0.0\) & \(5\) & \(89.9\) & \(89.7\) & \(408 17\) \\ RESPO & \(60.4\) & \(14\) & \(99.9\) & \(99.9\) & \(339 19\) \\  VSRL & \(\) & \(\) & \(\) & \(\) & \(401 20\) \\    \\  & Verified-50(\(\)) & Verified-Max(\(\)) & Emp-50(\(\)) & Emp-500(\(\)) & Avg Reward(\(\)) \\  PPO-Lag & \(0.0\) & \(3\) & \(99.7\) & \(99.7\) & \(371 7\) \\ PPO-PID & \(0.0\) & \(2\) & \(99.7\) & \(99.7\) & \(371 5\) \\ CAP & \(57.1\) & \(8\) & \(99.2\) & \(99.2\) & \(362 3\) \\ MBPPO & \(0.0\) & \(4\) & \(99.3\) & \(99.3\) & \(\) \\ CBF-RL & \(0.0\) & \(4\) & \(99.3\) & \(99.3\) & \(369 6\) \\ RESPO & \(0.0\) & \(6\) & \(99.1\) & \(99.1\) & \(373 6\) \\  VSRL & \(\) & \(\) & \(\) & \(\) & \(364 4\) \\    \\  & Verified-15(\(\)) & Verified-Max(\(\)) & Emp-15(\(\)) & Emp-500(\(\)) & Avg Reward(\(\)) \\  PPO-Lag & \(0.0\) & \(3\) & \(85.2\) & \(81.2\) & \(132 11\) \\ PPO-PID & \(0.0\) & \(3\) & \(89.4\) & \(88.3\) & \(\) \\ CAP & \(0.0\) & \(4\) & \(63.6\) & \(59.2\) & \(141 11\) \\ MBPPO & \(41.1\) & \(1\) & \(75.4\) & \(73.1\) & \(132 9\) \\ CBF-RL & \(0.0\) & \(2\) & \(82.3\) & \(79.2\) & \(140 10\) \\ RESPO & \(0.0\) & \(1\) & \(65.7\) & \(21.3\) & \(79 8\) \\  VSRL & \(\) & \(\) & \(\) & \(\) & \(122 14\) \\   

Table 1: Results for verified safety, empirical safety and average reward. The percentage results are truncated instead of rounded, to prevent missing unsafe violations.

**Lane Following:** Our lane following environment follows the discrete-time bicycle model [Kong et al., 2015]. The model inputs are 3-dimensional \((x,,v)\), where \(x\) is the lateral distance to the center of the lane, \(\) is the angle relative to the center of the lane, and \(v\) represents the speed. The objective is to maintain a constant speed while following the lane, meaning the system equilibrium point is \((x,,v)=(0,0,v_{})\). The safety constraints are 1) \(x\) stays within a maximum distance from the lane center (\(\|x\| d_{}\)), 2) \(\) remains within a predefined range \((\|\|_{})\), and 3) \(v\) does not exceed the maximum threshold \((v v_{})\).

**Vehicle Avoidance:** Our vehicle avoidance environment features a vehicle moving on an \(x\)-\(y\) plane, with 4-dimensional inputs \((x,y,,v)\). Here, \((x,y)\) represents the location of the vehicle on the plane, \(\) is the angle relative to the \(y\)-axis, and \(v\) is the speed. In this setting, we have five moving obstacles, each moving from one point to another at constant speed. Each obstacle is represented as a square. Additionally, safety constraints are set for the speed \((v v_{})\) and angle \((\|\|_{})\). The task is to navigate the vehicle to a designated location while following safety constraints.

**2D Quadrotor:** For the 2D quadrotor environment, we follow the settings in Dai et al. . The input is 6-dimensional \((y,z,,,,)\), where \((y,z)\) represents the position of the quadrotor on the \(y\)-\(z\) plane, and \(\) represents the angle. The action space is 2-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Our safety criteria include an angle constraint (\(\|\|_{}\)) and a minimum height constraint to prevent collision with the ground \((y y_{})\). We consider two scenarios for obstacles: fixed and moving. For fixed obstacles, there are five rectangular obstacles positioned in the \(y\)-\(z\) plane. For moving obstacles, there are five obstacles that moves from one point to another at constant speed, each represented as a square.

**3D Quadrotor:** Our 3D quadrotor environment features a 12-dimensional input space, represented as \((x,y,z,,,,,,,_{x},_{y},_{ z})\). The action space is 4-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Here, \((x,y,z)\) denotes the location of the quadrotor in space, \(\) is the roll angle, \(\) is the pitch angle, and \(\) is the yaw angle, \(_{x},_{y},_{z}\) represent the angular velocity around the \(x\), \(y\), and \(z\) axes, respectively. The task is to navigating towards the goal while adhering to safety constraints, which include avoiding five obstacles represented as 3D rectangles. The details for the environment settings are deferred to the Appendix.

### Results

As shown in Table 1, our approach significantly outperforms all baselines in terms of verified safety, as well as empirical safety over the entire episode horizon. Furthermore, the only environment in which VSRL exhibits a significant decrease in reward compared to baselines is lane following; for the rest, it achieves reward comparable to, or better than the baselines.

Specifically, in the _lane following_ environment, the proposed VSRL approach achieves verified \(80\)-step safety using a single controller (i.e., \(||=1\)). This is an order of magnitude higher \(K\) than all baselines (which only achieve \(K 8\)). While all baselines obtain a safety record of over 99.9% over the entire episode (\(K=500\)), our approach empirically achieves perfect safety.

For vehicle avoidance, we achieve verified \(50\)-step safety using two controllers (i.e., \(||=2\)); in contrast, the best baseline yields only \(K=13\). We also observe considerable improvements in both verified and empirical safety over the baseline approaches: for example, the best verified baseline (CAP) violates safety over 10% of the time over the full episode length, whereas VSRL maintains a perfect safety record. In this case, VSRL also achieves the highest reward.

For the 2D Quadrotor environment with fixed and moving obstacles, we are able to achieve verified \(50\)-step safety using four and two controllers, respectively. The best baseline achieves only \(K=14\) in the case of fixed and \(K=8\) in the case of moving obstacles (notably, different baselines are best in these cases).

Finally, in the most complex 3D Quadrotor environment, we achieve verified safety for \(K=15\), but empirically maintain a perfect safety record for the entire episode direction. The best baseline achieves verified safety for only \(K=4\), but is empirically unsafe over 40% of the time during an episode. Even the best safety record of any baseline is unsafe nearly 12% of the time, and we can only verify its safety over a horizon \(K=3\).

**Ablation Study:** We evaluate the importance of both incremental verification and using multiple initial-state-dependent controllers as part of VSRL. As shown in the Appendix (Section A.1), theformer significantly reduces average verification time during training, whereas the latter enables us to greatly boost the size of the initial state region \(_{0}\) for which we are able to achieve verify safety.

## 5 Conclusion

We present an approach for learning neural network control policies for nonlinear neural dynamical systems. In contrast to conventional methods for safe control synthesis which rely on forward invariance-based proofs, we opt instead for the more pragmatic finite-step reachability verification. This enables us to make use of state-of-the-art differentiable neural network overapproximation tools that we combine with three key innovations. The first is a novel curriculum learning approach for maximizing safety horizon. The second is to learn multiple initial-state-dependent controllers. The third is to leverage small changes in iterative gradient-based learning to enable incremental verification. We show that the proposed approach significantly outperforms state of the art safe RL baselines on several dynamical system environments, accounting for both fixed and moving obstacles. A key limitation of our approach is the clearly weaker safety guarantees it provides compared to forward invariance. Nevertheless, our results demonstrate that finite-step reachability provides a more pragmatic way of achieving verified safety that effectively achieves safety over the entire episode horizon _in practice_, providing an alternative direction for advances in verified safe RL to the more typical forward-invariance-based synthesis.