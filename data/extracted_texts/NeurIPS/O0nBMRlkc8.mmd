# System

Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration

 Junyang Wang\({}^{1}\) Haiyang Xu\({}^{2}\) Haitao Jia\({}^{1}\) Xi Zhang\({}^{2}\) Ming Yan\({}^{2}\)

Weizhou Shen\({}^{2}\) Ji Zhang\({}^{2}\) Fei Huang\({}^{2}\) Jitao Sang\({}^{1}\)

{junyangwang, 23120356, jtsang}@bjtu.edu.cn

{shuofeng.xhy, zx443053, ym119608, shenweizhou.swz, zj122146, f.huang}@alibaba-inc.com

\({}^{1}\)Beijing Jiaotong University \({}^{2}\)Alibaba Group

Work done during internship at Alibaba Group.Corresponding author

###### Abstract

Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks -- task progress navigation and focus content navigation -- are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.

## 1 Introduction

Multi-modal Large Language Models (MLLMs), represented by GPT-4v OpenAI (2023), have demonstrated outstanding capabilities in various domains Bai et al. (2023); Liu et al. (2023c,b); Dai et al. (2023); Zhu et al. (2023); Chen et al. (2023); Ye et al. (2023a,b); Wang et al. (2023c); Hu et al. (2023, 2024); Zhang et al. (2024b). With the rapid development of agents based on Large Language Models (LLMs) Zhao et al. (2024); Liu et al. (2023f); Talebriad and Nadiri (2023); Zhang et al. (2023b); Wu et al. (2023); Shen et al. (2024); Li et al. (2023a), MLLM-based agents, which can overcome the limitations of MLLMs in specific application scenarios by various visual perception tools, have become a focal point of research attention Liu et al. (2023d).

Automated operations on mobile devices, as a practical multi-modal application scenario, are emerging as a major technological revolution in AI smartphone development Yao et al. (2022); Deng et al. (2023); Gur et al. (2024); Zheng et al. (2024); Zhang et al. (2023a); Wang et al. (2024); Chen and Li (2024a,b,c); Zhang et al. (2024a); Wen et al. (2023); Cheng et al. (2024). However, due to the limited screen recognition, operation, and location capabilities, existing MLLMs face challenges in this scenario. To address this, existing work leverages MLLM-based agent architecture to endow MLLMs with various capabilities for perceiving and operating mobile device UI. AppAgent Zhang et al. (2023a) tackles the limitation of MLLMs in localization by extracting clickable positions from device XML files. However, the reliance on UI files limits the applicability of this method to other platforms and devices. To eliminate the dependency on underlying UI files, Mobile-Agent Wang et al. (2024) proposes a solution for localization through visual perception tools. It perceives the screen through an MLLM and generates operations, locating their positions by visual perception tools.

Mobile device operation tasks involve multi-step sequential processing. The operator needs to perform a series of continuous operations on the device starting from the initial screen until the instructions are fully executed. There are two main challenges in this process. First, to plan the operation intent, the operator needs to navigate the current task progress from the history operations. Second, some operations may require task-relevant information in the history screens, for example, writing sports news in Figure 1 requires using the match results queried earlier. We refer to this important information as the focus content. The focus content also needs to be navigated out from the history screens. However, as the task progresses, the lengthy history of interleaved image and text history operations and screens as input can significantly reduce the effectiveness of navigation in a single-agent architecture, as shown in Figure 1.

In this paper, we propose Mobile-Agent-v2, a mobile device operation assistant with effective navigation via multi-agent collaboration. Mobile-Agent-v2 has three specialized agent roles: **planning agent**, **decision agent**, and **reflection agent**. The planning agent needs to generate a task progress based on the history operations. To save the focus content from the history screens, we design a **memory unit** to record task-related focus content. This unit will be observed by the decision agent when generating an operation, simultaneously checking if there is any focus content on the screen and updating it to the memory. Since the decision agent cannot observe the previous screen to reflect, we design the reflection agent to observe the changes in the screen before and after the decision agent's operation and determine whether the operation meets the expectations. If it finds that the operation does not meet expectations, it will take appropriate measures to re-execute the operation. The entire

Figure 1: Mobile device operation tasks require navigating focus content and task progress from history operation sequences, where the focus content comes from previous screens. As the number of operations increases, the length of the input sequences grows, making it extremely challenging for a single-agent architecture to manage these two types of navigation effectively.

process is illustrated in Figure 3. The three agent roles work respectively in the progress, decision, and reflection stages, collaborating to alleviate the difficulty of navigating.

Our summarized contributions are as follows:

* We propose a multi-agent architecture Mobile-Agent-v2 to alleviate various navigating difficulties inherent in the single-agent framework for mobile device operation tasks. We design a planning agent to generate task progress based on the history operations, ensuring effective operation generation by the decision agent.
* To avoid the loss of focus content navigating and reflection capability, we design both a memory unit and a reflection agent. The memory unit is updated by the decision agent with focus content. The reflection agent assesses whether the decision agent's operation meets expectations and generates appropriate remedial measures if expectations are not met.
* We conducted dynamic evaluations of Mobile-Agent-v2 across various operating systems, language environments, and applications. Experimental results demonstrate that Mobile-Agent-v2 achieves significant performance improvements. Furthermore, we empirically validated that the performance of Mobile-Agent-v2 can be further enhanced by manual operation knowledge injection.

## 2 Related Work

### Muiti-agent Application

The powerful comprehension and reasoning capabilities of Large Language Models (LLMs) enable LLM-based agents to demonstrate the ability to independently execute tasks Brown et al. (2020); Achiam et al. (2023); Touvron et al. (2023, 2023); Bai et al. (2023). Inspired by human-team collaboration, the multi-agent framework has been proposed. Park et al. (2023) constructs Smallville consisting of 25 agents in a sandbox environment. Li et al. (2023) proposes a role-playing-based multi-agent collaborative framework to enable two agents playing different roles to autonomously collaborate. Chen et al. (2024) innovatively propose an effective multi-agent framework for coordinating the collaboration of multiple expert agents. Hong et al. (2024) presents a groundbreaking meta-programming multi-agent collaboration framework. Wu et al. (2024) proposes a generic multi-agent framework that allows users to configure the number of agents, interaction modes, and toolsets. Chan et al. (2024); Subramaniam et al. (2024); Tao et al. (2024) investigate the implementation of a multi-agent debating framework, aiming to evaluate the quality of different texts or generated content. Abdelnabi et al. (2024); Xu et al. (2024); Mukobi et al. (2024) integrate multi-agent interaction with game theoretic strategies, aiming to enhance both the cooperative and decision abilities.

### LLM-based UI Operation Agent

Webpages, as a classic application scenario for UI agents, have attracted widespread attention to research on web agents. Yao et al. (2022) and Deng et al. (2023) aim to enhance the performance of agents on real-world webpage tasks by constructing high-quality website task datasets. Gur et al. (2024) utilizes pre-trained LLMs and self-experience learning to automate task processing on real-world websites. Zheng et al. (2024) leverages GPT-4V for visual understanding and webpage manipulation. Simultaneously, research on LLM-based UI agents for mobile platforms has also drawn significant attention. Wen et al. (2023) converts Graphical User Interface (GUI) information into HTML representations and then leverages LLM in conjunction with application-specific domain knowledge. Yan et al. (2023) proposes a multi-modal intelligent mobile agent based on GPT-4V, exploring the direct utilization of GPT-4V to perceive screen screenshots with annotations. Unlike the former approach that operates on screens with digital labels, Zhang et al. (2023) combines the application's XML files for localization operations, mimicking human spatial autonomy in operating mobile applications. Wang et al. (2024) eliminates the dependency on the application's XML files and leverages visual module tools for localization operations. Additionally, Hong et al. (2023) designed a GUI agent based on pre-trained vision-language models. Chen and Li (2024, 2024) propose small-scale client-side models for deployment on actual devices. Zhang et al. (2024) proposed a UI multi-agent framework tailored for the Windows operating system. Despite the significant performance improvements achieved by multi-agent architectures in many tasks, currently, there is no work that employs multi-agent architectures in mobile device operation tasks. To address the challenges of long-context navigation in mobile device operation tasks, in this paper, we introduce the multi-agent architecture Mobile-Agent-v2.

## 3 Mobile-Agent-v2

In this section, we will provide a detailed overview of the architecture of Mobile-Agent-v2. The operation of Mobile-Agent-v2 is iterative, and its process is depicted in Figure 2. Mobile-Agent-v2 has three specialized agent roles: planning agent, decision agent, and reflection agent. We also design the visual perception module and memory unit to enhance the agent's screen recognition capability and the capability to navigate focus content from history. Firstly, the planning agent updates the task progress, allowing the decision agent to navigate the progress of the current task. The decision agent then operates based on the current task progress, current screen state, and the reflection (if the last operation is erroneous). Subsequently, the reflection agent observes the screens before and after the operation to determine if the operation meets expectations.

### Visual Perception Module

Screen recognition remains challenging even for state-of-the-art MLLMs when processed end-to-end. Therefore, we have incorporated a visual perception module to enhance the screen recognition capability. In this module, we utilize three tools: text recognition tool, icon recognition tool, and icon description. Inputing a screenshot into this module will ultimately yield the text and icon information present on the screen, along with their respective coordinates. This process is represented by the following formula:

\[P_{t}=VPM(S_{t})\] (1)

where \(P_{t}\) represents the perception result of the screen in the \(t\)-th iteration.

### Memory Unit

Due to the task progress generated by the planning agent being in textual form, the navigation of focus content from history screens is still challenging. To address this issue, we design a memory unit to store the focus content related to the current task from history screens. The memory unit serves as a short-term memory module that is updated as the task progresses. The memory unit is crucial for scenarios involving multiple apps. For instance, as shown in Figure 3, weather information observed by the decision agent will be utilized in subsequent operations. At this point, the information related to the weather app's page will be updated in the memory unit.

### Planning Agent

We aim to reduce the reliance on lengthy history operations during decision-making by employing a separate agent. We observe that although each round of operation occurs on different pages and is different, often the goals of multiple operations are the same. For example, in the example illustrated

Figure 2: Illustration of the overall framework of Mobile-Agent-v2.

in Figure 1, the first four operations are all about searching for match results. Therefore, we design a planning agent to summarize the history operations and track task progress.

We define the operation generated by the decision agent at the \(t\)-th iteration as \(O_{t}\). Before the decision agent makes a decision, the planning agent observes the decision agent's operation \(O_{t-1}\) from the last iteration and updates the task progress \(TP_{t-1}\) to \(TP_{t}\). The task progress includes the sub-tasks that have already been completed. After generating the task progress, the planning agent passes it to the decision agent. This aids the decision agent in considering the content of tasks that have not yet been completed, thereby facilitating the generation of the next operation. As shown in Figure 3, the planning agent's inputs consist of four parts: user instruction \(Ins\), the focus content \(FC_{t}\) in memory unit, the previous operation \(O_{t-1}\), and the previous task progress \(TP_{t-1}\). Based on above information, the planning agent generates the \(TP_{t}\). This process is represented by the following formula:

\[TP_{t}=PA(Ins,O_{t-1},TP_{t-1},FC_{t-1})\] (2)

where the \(PA\) represents the LLM of the planning agent.

### Decision Agent

The decision agent operates during the decision stage, generating operations \(O\) and implementing them on the device, while also being responsible for updating the focus content \(FC\) in memory unit. This process is illustrated in the Decision Stage shown in Figure 3 and represented by the following formula:

\[O_{t}=DA(Ins,TP_{t-1},FC_{t-1},R_{t-1},S_{t},P_{t})\] (3)

where the \(DA\) represents the MLLM of the decision agent and the \(R_{t}\) represents the reflection result from reflection agent.

**Operation Space.** To reduce the complexity of operations, we design an operation space and restricted the decision agent to selecting operations only from within this space. For operations with higher degrees of freedom, such as tapping and swiping, we incorporate an additional parameter space to locate or handle specific content. Below is a detailed description of the operation space:

* Open app (_app name_). If the current page is the home page, this operation can be used to open the app named "_app name_".
* Tap (_x_, _y_). This operation is used to tap on the position with coordinates (_x_, _y_).
* Swipe (_x1_, _y1_), (_x2_, _y2_). This operation is used to swipe from the position with coordinates (_x1_, _y1_) to the position with coordinates (_x2_, _y2_).
* Type (_text_). If the current keyboard is in an active state, this operation can be used to input the content of "_text_" in the input box.

Figure 3: Illustration of the operation process and interaction of agent roles in Mobile-Agent-v2.

* Home. This operation is used to return to the home page from any page.
* Stop. If the decision agent thinks that all requirements have been fulfilled, it can use this operation to terminate the entire operation process.

Memory Unit Update.As each operation made by the decision agent is highly task-relevant and based on the visual perception results of the current page, it is well-suited to observe task-related focus content within the screen pages. Accordingly, we have endowed the decision agent with the ability to update the memory unit. When making decisions, the decision agent is prompted to observe whether there is task-related focus content within the current screen page. If such information is observed, the decision agent updates it in memory for reference in subsequent decisions. This process is represented by the following formula:

\[FC_{t}=DA(Ins,FC_{t-1},S_{t},P_{t})\] (4)

### Reflection Agent

Even with the visual perception module, Mobile-Agent-v2 may still generate unexpected operations. In some specific scenarios, MLLMs may even produce severe hallucinations Liu et al. (2023); Li et al. (2023); Gunjal et al. (2024); Wang et al. (2023); Zhou et al. (2023), even the most advanced MLLM GPT-4V Cui et al. (2023); Wang et al. (2023). Therefore, we design the reflection agent to observe the screen state before and after a decision agent's operation to determine whether the current operation meets expectations. This process is represented by the following formula:

\[R_{t}=RA(Ins,FC_{t},O_{t},S_{t},P_{t},S_{t+1},P_{t+1})\] (5)

where the \(RA\) represents the MLLM of the reflection agent.

As shown in Figure 3, the reflection agent generates three types of reflection results after operation execution: erroneous operation, ineffective operation, and correct operation. The following will describe these three reflection results:

* Erroneous operation refers to an operation that leads the device to enter a page unrelated to the task. For example, the agent intends to chat with contact \(A\) in a messaging app, but it accidentally opens the chat page of contact \(B\) instead.
* Ineffective operation refers to an operation that does not result in any changes to the current page. For example, the agent intends to tap on an icon, but it taps on the blank space next to the icon instead.
* Correct operation refers to an operation that meets the decision agent's expectations and serves as a step towards fulfilling the requirements of the user instruction.

If erroneous operation, the page will revert to the state before the operation. If ineffective operation, the page will remain in its current state. Neither erroneous nor ineffective operations are recorded in the operation history to prevent the agent from following these operations. If correct operation, the operation will be updated in the operation history, and the page will be updated to the current state.

## 4 Experiments

### Model

Visual Perception Module.For the text recognition tool, we use the document OCR recognition model ConvNextViT-document from ModelScope3. For the icon recognition tool, we employ GroundingDINO Liu et al. (2023), a detection model capable of detecting objects based on natural language prompts. For the icon description tool, we utilize the Qwen-VL-Int4.

**MLLMs.** For the planning agent, as it does not require screen perception, we utilize the text-only GPT-4 OpenAI (2023). For the decision agent and reflection agent, we employ GPT-4V OpenAI (2023). All calls are made through the official API method provided by the developers.

### Evaluation

**Evaluation Method.** To evaluate the performance of Mobile-Agent-v2 on real mobile devices, we employed a dynamic evaluation method. This evaluation method requires the operation tool to implement the agent's operations in real time on an actual device. We use two mobile operation systems, Harmony OS and Android OS, to assess capabilities in non-English and English scenarios, respectively. We used Android Debug Bridge (ADB) as the tool for operating mobile devices. ADB can simulate all operations of Mobile-Agent-v2 in the operation space. In both scenarios, we select 5 system apps and 5 popular external apps for evaluation. For each app, we devise two basic instructions and two advanced instructions. Basic instructions are relatively simple operations with clear instructions within the app interface, while advanced instructions require a certain level of experience with app operations to complete. Additionally, to evaluate multi-app operation capability, we design two basic instructions and two advanced instructions involving multiple apps. In total, there were 88 instructions for non-English and English scenarios, comprising 40 instructions for system apps, 40 instructions for external apps, and 8 instructions for multi-app operations.5 The apps and instructions used for evaluation in non-English and English scenarios are presented in the appendix.

**Metrics.** We design the following four metrics for dynamic evaluation:

* Success Rate (SR): When all the requirements of a user instruction are fulfilled, the agent is considered to have successfully executed this instruction. The success rate refers to the proportion of user instructions that are successfully executed.
* Completion Rate (CR): Although some challenging instructions may not be successfully executed, the correct operations performed by the agent are still noteworthy. The completion rate refers to the proportion of correct steps out of the ground truth operations.
* Decision Accuracy (DA): This metric reflects the accuracy of the decision by the decision agent. It is the proportion of correct decisions out of all decisions.
* Reflection Accuracy (RA): This metric reflects the accuracy of reflection by the reflection agent. It is the proportion of correct reflections out of all reflections.

**Implementation Details.** We use Mobile-Agent as the baseline. Mobile-Agent is a single-agent architecture based on GPT-4V end-to-end screen recognition. We fix the seed for GPT-4V invocation and set the temperature to 0 to avoid randomness. In addition to Mobile-Agent-v2, we further introduce the scenario of knowledge injection. This involves providing the agent with some operation hints in addition to the user instructions to aid the agent. It's worth noting that we only injected knowledge for instructions that Mobile-Agent-v2 couldn't accomplish. For instructions that could be completed without additional assistance, we keep the input unchanged.

### Results

#### 4.3.1 Evaluation

**Evaluation on Task Completion.** Table 1 and 2 respectively illustrate the performance of Mobile-Agent-v2 in non-English and English scenarios. Compared to Mobile-Agent, Mobile-Agent-v2 exhibits significant improvements in both basic and advanced instructions. With the multi-agent architecture, even in highly challenging advanced instructions, the success rate can still reach 55%, compared to only 20% with Mobile-Agent. Even in the English scenario, Mobile-Agent-v2 still achieves a significant performance improvement. Although Mobile-Agent performs better in English scenarios compared to Chinese scenarios, Mobile-Agent-v2 still achieves an average improvement of 27% in success rate.

**Evaluation on Reflection Capability.** In the case of knowledge injection, even though the decision accuracy does not reach 100%, the completion rate can still reach 100%. This indicates that even with knowledge injection, Mobile-Agent-v2 still makes erroneous decisions. Errors in decision-making are difficult to avoid, even for humans. Therefore, the importance of the reflection agent is highlighted.

**Evaluation on App Type.** From all metrics, it can be observed that the performance of all methods on system apps exceeds that of external apps. From the results of multiple apps, it can be seen that Mobile-Agent-v2 achieves improvements of 37.5% and 44.2% in SR and CR, respectively, compared to Mobile-Agent. Compared to single-app tasks, multi-app tasks rely more on the retrieval of history operations and focus content. The significant performance improvement indicates that the multi-agent architecture and memory unit of Mobile-Agent-v2 play an important role.

**Evaluation on Operation Knowledge Injection.** From the results of knowledge injection in Table 1 and 2, it can be observed that operation knowledge can effectively enhance the performance of Mobile-Agent-v2, which suggests that manually injected operation knowledge can mitigate the limitations of an agent's operation capability. This finding implies that knowledge injection can broaden the application scenarios of Mobile-Agent-v2 because even complex tasks can be guided by manually written operation tutorials to instruct agents. This finding may offer new insights for automated script testing on mobile devices and suggests that to enhance the operation capabilities of MLLMs to their limits, automating the generation of high-quality operation knowledge can further improve the performance of Mobile-Agent-v2. Moreover, the success brought about by knowledge injection also opens up new avenues for future mobile app testing. Existing mobile app testing solutions are still limited to manual script writing, which restricts the universality of testing and raises

    &  &  \\   & SR & CR & DA & RA & SR & CR & DA & RA \\   &  \\  Mobile-Agent & 5/10 & 41.2 & 37.6 & - & 3/10 & 37.3 & 32.9 & - \\ Mobile-Agent-v2 & 9/10 & 86.8 & 82.5 & 93.3 & 6/10 & 82.7 & 78.2 & 84.4 \\ Mobile-Agent-v2 + _Know._ & 10/10 & 100 & 98.2 & 98.9 & 8/10 & 88.9 & 87.2 & 91.4 \\   &  \\  Mobile-Agent & 2/10 & 38.3 & 35.4 & - & 1/10 & 29.2 & 27.0 & - \\ Mobile-Agent-v2 & 8/10 & 97.9 & 94.0 & 92.5 & 5/10 & 77.9 & 74.1 & 78.8 \\ Mobile-Agent-v2 + _Know._ & 10/10 & 100 & 95.6 & 97.3 & 8/10 & 87.8 & 83.0 & 85.9 \\   &  \\  Mobile-Agent & 1/2 & 52.8 & 50.0 & - & 0/2 & 33.3 & 31.4 & - \\ Mobile-Agent-v2 & 2/2 & 100 & 92.9 & 91.6 & 2/2 & 100 & 93.8 & 92.9 \\ Mobile-Agent-v2 + _Know._ & - & - & - & - & - & - & - & - \\   

Table 1: Dynamic evaluation results on non-English scenario, where the _Know._ represents manually injected operation knowledge.

    &  &  \\   & SR & CR & DA & RA & SR & CR & DA & RA \\   &  \\  Mobile-Agent & 9/10 & 92.5 & 89.7 & - & 4/10 & 62.0 & 71.3 & - \\ Mobile-Agent-v2 & 9/10 & 95.0 & 92.9 & 96.5 & 6/10 & 76.0 & 77.6 & 88.4 \\ Mobile-Agent-v2 + _Know._ & 10/10 & 100 & 96.2 & 98.7 & 8/10 & 85.3 & 87.9 & 92.0 \\   &  \\  Mobile-Agent & 7/10 & 79.7 & 72.0 & - & 3/10 & 45.3 & 38.7 & - \\ Mobile-Agent-v2 & 9/10 & 97.1 & 93.8 & 96.2 & 7/10 & 89.7 & 91.0 & 93.4 \\ Mobile-Agent-v2 + _Know._ & 10/10 & 100 & 98.2 & 97.4 & 9/10 & 97.1 & 94.2 & 98.5 \\   &  \\  Mobile-Agent & 2/2 & 100 & 91.2 & - & 1/2 & 86.7 & 92.9 & - \\ Mobile-Agent-v2 & 2/2 & 100 & 97.4 & 100 & 1/2 & 93.3 & 93.3 & 80.0 \\ Mobile-Agent-v2 + _Know._ & - & - & - & - & 2/2 & 100 & 100 & 100 \\   

Table 2: Dynamic evaluation results on English scenario, where the _Know._ represents manually injected operation knowledge.

the threshold for users. To address the aforementioned issues, one can inject natural language testing procedures into Mobile-Agent-v2. After injecting accurate testing procedures, the system can operate normally regardless of changes in the size or color of the mobile interface. Additionally, language descriptions eliminate the need for a knowledge base in script writing.

**Evaluation on MLLMs.** In Table 3, we evaluate the performance of different MLLMs within the Mobile-Agent-v2 framework. Since some models are not well-suited for handling sequential inputs, we selected specific instructions and modified each step to function as a single-step task. Therefore, we only evaluate the SR (which is the same as DA). We also evaluate the direct use of GPT-4V, bypassing the agent architecture for end-to-end operation. The results indicate that using GPT-4V directly as a mobile device operation assistant is almost infeasible. GPT-4V combined with the agent architecture remains the most effective configuration for operational capabilities.

#### 4.3.2 Ablation Study

We conduct ablation studies on Mobile-Agent-v2, including the planning agent, reflection agent, and memory unit. From the results in Table 4, it is evident that the impact of the planning agent on the overall framework is the most significant. This further demonstrates the challenging nature of navigation in long sequences for current MLLMs. Additionally, performance declines are observed after removing the reflection agent and memory unit. The reflection agent is essential for correcting erroneous operations. It enables the decision agent to avoid operating on incorrect pages or getting stuck in loops of invalid operations. The memory unit is crucial for successful execution in multi-app scenarios. Even in scenarios involving multiple sub-tasks, the memory can sometimes record the positions of critical UI elements, aiding better localization for the next sub-task execution.

#### 4.3.3 Analysis of Operation Sequence Length

As shown in Figure 4, we analyze the positions of erroneous or ineffective operations within failed instructions in English scenarios, dividing the relative sequence positions into three equal parts. The results indicate that in Mobile-Agent, such errors or ineffective operations predominantly occur in the later stages of the tasks. In contrast, Mobile-Agent-v2 does not exhibit any obvious pattern. This indicates that the multi-agent architecture is better equipped to handle the challenges posed by long sequences in UI operation tasks.

    &  &  \\  Planning Agent & Reflection Agent & Memory Unit & SR & CR & DA & SR & CR & DA \\   & ✓ & ✓ & 59.1 & 63.7 & 58.9 & 29.5 & 43.8 & 42.6 \\ ✓ & ✓ & & 77.3 & 83.6 & 84.0 & 45.5 & 72.3 & 69.8 \\ ✓ & & ✓ & 86.4 & 89.2 & 85.7 & 54.5 & 75.9 & 72.4 \\  ✓ & ✓ & ✓ & **88.6** & **93.9** & **89.4** & **61.4** & **82.1** & **80.3** \\   

Table 4: The results of the ablation study on planning agent, reflection agent, and memory unit.

    & Basic & Advanced \\   & SR\&DA & SR\&DA \\  GPT-4V w/o agent & 2.7 & 0.9 \\ Gemini-1.5-Pro & 38.2 & 29.8 \\ Owen-VL-Max & 42.1 & 33.6 \\  GPT-4V & **92.7** & **83.5** \\   

Table 3: Performance results of Mobile-Agent-v2 with different MLLMs. To better illustrate the differences, we converted all instructions to single-step forms and evaluated the success rate (which is the same as decision accuracy) of each single-step task.

#### 4.3.4 Case Study

Figure 5 (a) illustrates a complete operational process of Mobile-Agent-v2. Under the planning of the planning agent, the decision agent can navigate the task progress correctly in the case of single-image input. Meanwhile, the memory unit accurately stores the chat content needed for the task and, when required for search purposes, the decision agent can effectively navigate to it. Figure 5 (b) illustrates an example of correcting ineffective operations through reflection. After the failure of the previous operation, the reflection agent promptly detects the error and communicates the reflection result to the decision agent. Based on this, the decision agent rethinks and implements the correct operation.

## 5 Conclusion

Existing single-agent architectures for mobile device operation assistants experience a significant reduction in navigation effectiveness when dealing with long sequences of interleaved text and images, thereby limiting their performance. To address this issue, in this paper, we propose Mobile-Agent-v2, a mobile device operation assistant with efficient navigation via multi-agent collaboration. We address the aforementioned navigating challenges through the planning agent and memory unit, respectively. Additionally, we design reflectors to ensure the smooth progress of tasks. Experimental results demonstrate that Mobile-Agent-v2 achieves significant performance improvements compared to the single-agent Mobile-Agent. Furthermore, we find that performance can be further enhanced through the injection of manual operation knowledge, providing new directions for future work.

Figure 4: The relative positions of erroneous or ineffective operations in the operation sequence.

Figure 5: The cases of the complete operation process and reflection of Mobile-Agent-v2.

## 6 Acknowledgements

This work is supported by the National Key R&D Program of China (No. 2023YFC3310700).