# A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation

Jer Pelhan, Alan Lukezic, Vitjan Zavrtanik, Matej Kristan

Faculty of Computer and Information Science, University of Ljubljana

jer.pelhan@fri.uni-lj.si

###### Abstract

Low-shot object counters estimate the number of objects in an image using few or no annotated exemplars. Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation. Due to potentially diverse object appearances, the existing approaches often lead to overgeneralization and false positive detections. Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center. This loss is sensitive to annotation error, hyperparameters and does not directly optimize the detection task, leading to suboptimal counts. We introduce GeCo, a novel low-shot counter that achieves accurate object detection, segmentation, and count estimation in a unified architecture. GeCo robustly generalizes the prototypes across objects appearances through a novel dense object query formulation. In addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss. GeCo surpasses the leading few-shot detection-based counters by \(\)25% in the total count MAE, achieves superior detection accuracy and sets a new solid state-of-the-art result across all low-shot counting setups. The code is available on GitHub.

## 1 Introduction

Low-shot object counting considers estimating the number of objects of previously unobserved category in the image, given only a few annotated exemplars (few-shot) or without any supervision (zero-shot) . The current state-of-the-art methods are predominantly based on density estimation [4; 14; 32; 26; 22; 31; 7; 31]. These methods predict a density map over the image and estimate the total count by summing the density.

While being remarkably robust for global count estimation, density outputs lack explainability such as object location and size, which is crucial for many practical applications [33; 30]. This recently gave rise to detection-based low-shot counters [20; 19; 35], which predict the object bounding boxes and estimate the total count as the number of detections. Nevertheless, detection-based counting falls behind the density-based methods in total count estimation, leaving a performance gap.

In detection-based counters, a dominant approach to identify locations of the objects in the image involves construction of object prototypes from few (e.g., three) annotated exemplar bounding boxes and correlating them with image features [20; 35; 19]. The exemplar construction process is trained to account for potentially large diversity of object appearances in the image, often leading to overgeneralization, which achieves a high recall, but is also prone to false positive detection. Post-hoc detection verification methods have been considered [20; 35] to address the issue, but their multi-stage formulation prevents exploiting the benefits of end-to-end training.

Currently, the best detection counters [20; 35] predict object locations based on the local maxima in the correlation map. During training, the map prediction is supervised by a unit Gaussian placed oneach object center. However, the resulting surrogate loss is susceptible to the center annotation noise, requires nontrivial heuristic choice of the Gaussian kernel size and in practice leads to detection preference of compact blob-like structures (see Figure 1, column 1&2). Recently, DETR  inspired counter was proposed to avoid this issue , however, it fails in densely populated regions even though it applies a very large number of detection queries in a regular grid (see Figure 1, column 3&4).

We address the aforementioned challenges by proposing a new single-stage low-shot counter GeCo, which is implemented as an add-on network for SAM  backbone. A single architecture is thus trained for both few-shot and zero-shot setup, it enables counting by detection and provides segmentation masks for each of the detected objects. Our first contribution is a dense object query formulation, which applies a non-parametric model for image-wide prototype generalization (hence GeCo) in the encoder, and decodes the queries into highly dense predictions. The formulation simultaneously enables reliable detection in densely-populated regions (Figure 1, column 3&4) and prevents prototype over-generalization, leading to an improved detection precision at a high recall. Our second contribution is a new loss function for dense detection training that avoids the ad-hoc surrogate loss with unit Gaussians, it directly optimizes the detection task, and leads to improved detection not biased towards blob-like regions (Figure 1, column 1&2).

GeCo outperforms all detection-based counters on challenging benchmarks by 24% MAE and the density-based long-standing winner  by 27% MAE, while delivering superior detection accuracy. The method shows substantial robustness to the number of exemplars. In one-shot scenario, GeCo outperforms the best detection method in 5% AP50, 45% MAE and by 14% in a zero-shot scenario. GeCo is the first detection-based counter that outperforms density based counters in all measures by using the number of detections as the estimator, and thus sets a milestone in low-shot detection-based counting.

## 2 Related works

Traditional counting methods focus on predefined categories like vehicles, cells , people, and polyps,  requiring extensive annotated training data and lacking generalization to other categories, necessitating retraining or conceptual changes. Low-shot counting methods address this limitation by estimating counts for arbitrary categories with minimal or no annotations, enabling test-time adaptation.

With the proposal of the FSC147 dataset  low-shot counting methods emerged, which predict global counts by summing over a predicted density maps. The first method  proposed an

Figure 1: DAVE  predicts object centers (red dots) biased towards blob-like structures, leading to incorrect partial detections of ants (bottom left), while GeCo(ours) addresses this with the new loss (top left). CDETR  fails in densely populated regions (bottom right), while GeCo addresses this with the new dense query formulation by prototype generalization (top right). Exploiting the SAM backbone, GeCo delivers segmentations as well. Exemplars are denoted in blue.

adaptation of a tracking backbone for density map regression. BMNet+  tackled learning representation and similarity metric, while SAFECount  introduced a new feature enhancement module, improving appearance generalization. CounTR  utilized a vision transformer for image feature extraction and a convolutional network for encoding the exemplar features. LOCA  argued that exemplar shape information should be considered along with the appearance, and proposed an iterative object prototype extraction module. This led to a simplified counter architecture that remains a top-performer among density-based counters.

To improve explainability of the estimated counts and estimate object locations as well, detection-based methods emerged. The first few-shot detection-based counter  was an extended transformer-based object detector  with the ability to detect objects specified by the exemplars. Current state-of-the-art DAVE  proposed a two-stage detect-and-verify paradigm for low-shot counting and detection, wherein the first stage it generates object proposals with a high recall, but low precision, which is improved by a subsequent verification step. PSECO  proposed a three-stage approach called point-segment-and-count, which employs more involved proposal generation with better detection accuracy and also applies a verification step to improve precision. Both DAVE and PSECO are multi-stage methods that train a network for the surrogate task of predicting density maps for object centers, from which the bounding boxes are predicted. Although detection-based counters offer additional applicability, they fall behind the best density-based counters in global count estimation.

## 3 Single-stage low-shot object counting by detection and segmentation

Given an input image \(I^{H_{0} W_{0} 3}\) and a set of \(k\) exemplar bounding boxes \(^{}=\{_{i}\}_{i=1:k}\) specifying the target category, the task is to predict bounding boxes \(^{P}=\{_{j}\}_{j=1:N}\) for all target category objects in \(I\), with the object count estimated as \(N=|^{P}|\).

The proposed detection-based counter GeCo pipeline proceeds as follows (see Figure 2). The image is encoded by a SAM  backbone into \(^{I}^{h w d}\), where \(h=H_{0}/r\), \(w=W_{0}/r\) and \(d\) is number of feature channels. In the few-shot setup, two kinds of prototypes (appearance and shape) are extracted from each annotated object exemplar. The appearance prototypes \(^{A}^{k d}\) are extracted by RoI-pooling  features \(^{I}\) from the exemplar bounding boxes. Following , shape prototypes \(^{S}^{k d}\) are extracted as well, by \(^{S}_{i}=([W_{_{i}},H_{_{i}}])\), where \(W_{_{i}}\) and \(H_{_{i}}\) are the width and height of the \(i\)-th exemplar bounding box, and \(()\) is a small MLP network. The concatenation of \(^{A}\) and \(^{S}\) yields \(^{2k d}\) prototypes.

Note, however, that in a zero-shot setup, exemplars are not provided and the task is to count the majority-class objects in the image. In this setup, a single zero-shot prototype is constructed by attending a pretrained objectness prototype \(^{Z}\) to the image features, i.e., \(=(^{Z},^{I},^{I})\), where \((a,b,c)\) is cross-attention  followed by a skip connection, with \(a\), \(b\) and \(c\) as attention query, key and value, respectively.

The prototypes \(\) (either from few-shot or zero-shot setup) are then generalized across the image, and dense object detection queries are constructed by the Dense query encoder (DQE, Section 3.1). These are decoded into dense detections by the Dense query decoder (DQD, Section 3.2). The final detections are extracted and refined by a post-processing step (Section 3.3). The aforementioned modules are detailed in the following sections.

Figure 2: The architecture of the proposed single-stage low-shot counter GeCo.

### Dense object query encoder (DQE)

To account for the variation of the object appearances in the image, the current state-of-the-art [4; 20; 35] aims at constructing a small number of prototypes (e.g., three) that compactly encode the object appearance variation in the image, often leading to overgeneralization and false detections. We deviate from this paradigm by considering image-wide prototype generalization with a non-parametric model that constructs \(w h\) location-specific prototypes \(_{N_{P}}^{w h d}\). Let \(_{0}=^{I}\) be the initial dense generalized prototypes (i.e., one for each location). The final dense generalized prototypes \(_{N_{P}}\) are calculated by the following iterative adaptation via cross-attention

\[_{i}=(_{i-1},,),\] (1)

where \(i\{1,...,N_{P}\}\). Note that spatial encoding is not applied, to enable spatially-unbiased information flow from the prototypes \(\) to all locations.

Next, dense object queries are constructed from the generalized prototypes by the following iterations

\[_{j}=((^{I}),_{j-1},_{j-1}),\] (2)

where \(j\{1,...,N_{Q}\}\), \(_{0}=_{N_{P}}\), and \(()\) is a self-attention followed by a skip connection to adapt the input features to the current queries. In both cross- and self-attentions, positional encoding is applied to enable location-dependent query construction. In the remainder of the paper, the dense object queries \(_{N_{Q}}\) are denoted as \(\) for clarity

### Dense object query decoder (DQD)

The dense queries \(\) from Section 3.1 are decoded into object detections by a dense object query decoder (DQD). Note that the spatial reduction of image by the SAM backbone may lead to encoding several small objects into the same query in \(\). To address this, the object queries are first _spatially unpacked_ into high-resolution dense object queries i.e., \(^{HR}^{H W d}\), where \(H=H_{0}/2\), \(W=W_{0}/2\) and \(d\) is the number of feature channels. The unpacking process consists of three convolutional upsampling stages, with each stage composed of a \(3 3\) convolution, a Leaky ReLU and a \(2\) bilinear upsampling. To facilitate unpacking of small objects, the features after the second stage are concatenated by the SAM-HQ features \(^{HQ}\) before feeding into the final stage.

Finally, the objectness score \(^{}^{H W 1}\) is calculated by a simple transform, i.e., \(^{}=(_{O}^{HR})\), where \(_{O}\) is a learned projection matrix and \(()\) is a Leaky ReLU. Each query is also decoded into the object pose by a three-layer MLP, i.e., \(^{}=((^{HR}))\), where \(()\) is a sigmoid function and \(^{}^{H W 4}\) are bounding box parameters in the _ltrb_ format .

### Detections extraction and refinement

The final detections are extracted from \(^{}\) and \(^{}\) as follows. Bounding box parameters are read out from \(^{}\) at locations of local maxima on a thresholded \(^{}\) (using a \(3 3\) nonmaxima suppression, NMS). The bounding boxes are refined by feeding them as prompts into a SAM decoder  on the already computed backbone features \(^{I}\). The boxes are refitted to the masks by min-max operation and finally non-maxima suppression with \(=0.5\) is applied to remove duplicate detections. This process thus yields the predicted bounding boxes \(^{P}\) and their corresponding masks \(^{P}\).

### A novel loss for dense detection training

GeCotraining requires supervision on the dense objectness scores \(^{O}\) and the bounding box parameters \(^{BB}\). Ideally, a network should learn to predict points on objects that can be reliably detected by a NMS, and also from which the bounding box parameters can be reliably predicted. We thus propose a new dense object detection loss that pursues this property.

Following the detection step (Section 3.2) in the forward pass, a set of local maxima \(\{i\}_{i=1:N_{}}\) is identified by applying a NMS on \(^{O}\) and keeping all maxima higher than the median response, to ensure detection redundancy. The maxima are then labelled as _true positives_ (TP) and _false positives_ (FP) by applying Hungarian matching  between their bounding box parameters \(\{^{BB}_{i}\}_{i=1:N_{}}\) and the ground truth bounding boxes \(\{^{GT}_{j}\}_{j=1:N_{}}\). To account for missed detections, centers ofthe non-matched ground truth bounding boxes are added to the list of local maxima and labeled as _false negatives_ (FN). The new training loss is thus defined as

\[=-_{i}(_{i}^{BB},_{(i)}^{GT})+_{i}(_{i}^{}-1)^{2}+_{i }(_{i}^{}-0)^{2},\] (3)

where \((,)\) is the generalized IoU , and \((i)\) is the ground truth index matched with the \(i\)-th predicted bounding box. Note that the new loss simultaneously optimizes the bounding box prediction quality, promotes locations with better box prediction capacity that can be easily detected by a NMS, and enables automatic hard-negative mining in the objectness score via FP identification.

## 4 Experiments

### Implementation details.

Using the SAM  backbone, GeCo reduces the input image by a factor \(r=16\), and projects the features into \(d=256\) channels (Section 3). In DQE (Section 3.1), \(N_{P}=3\) iterations are applied in prototype generalization (1) and \(N_{Q}=2\) iterations in dense object query construction (2). Following the established test-time practice [20; 19; 26], the input image is scaled to fit \(W_{0}=H_{0}=1536\) if the average of the exemplars widths and heights is below 25 pixels, otherwise it is downscaled to fit the average of the exemplar width and height to 80 pixels and zero-padded to \(W_{0}=H_{0}=1024\). As in , the zero-shot GeCo is run twice, first to estimate the objects size and then again on the resized image.

**Training details.** With the SAM backbone frozen, GeCo is pretrained with the classical loss  for initialization and is then trained for 200 epochs with the proposed dense detection loss (3) using a mini-batch size of 8, AdamW  optimizer, with initial learning rate set to \(10^{-4}\), and weight decay of \(10^{-4}\). The training is done on 2 A100s GPUs with standard scale augmentation [20; 4] and zero-padding images to 1024\(\)1024 resolution. For the zero-shot setup, the few-shot GeCo is frozen and only the zero-shot prototype extension is trained for 10 epochs. Thus _the same trained network_ is used in all low-shot setups.

**Evaluation metrics and datasets.** Standard datasets are used. The FSCD147  is a detection-oriented extension of the FSC147 , which contains 6135 images of 147 object classes, split into 3659 training, 1286 validation, and 1190 test images. The splits are disjoint such that target object categories in test set are not observed in training. The objects are manually annotated by bounding boxes in the test set , while in the train set, the bounding boxes are obtained from point estimates by SAM . For each image, three exemplars are provided. The second dataset is FSCD-LVIS , derived from LVIS  and contains 377 categories. Specifically, the unseen-split is used (3959 training and 2242 test images), which ensures that test-time object categories are not observed during training.

The standard evaluation protocol [24; 26; 32] with Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) is followed to evaluate the counting accuracy. Following , Average Precision (AP) and Average Precision at IoU=50 (AP50) is used on the same output to evaluate the detection accuracy.

### Experimental Results

**Few-shot counting and detection.** GeCo is compared with state-of-the-art density-based counters (which only estimate the total count) LOCA , CounTR , SAFECount , BMNet+ , VCN , CFOCNet , MAML , FamNet  and CFOCNet , and with detection-based counters C-DETR , SAM-C , PSECO , and DAVE , which also provide object locations by bounding boxes. Results are summarized in Table 1.

GeCo outperforms both recent state-of-the-art detection-based counters DAVE  and PSECO  by a 24% and 39% MAE, and a remarkable 27% and 51% RMSE on the test split, setting a new state-of-the-art in detection-based counting. Notably, GeCo outperforms all single-stage density-based counters (top part of Table 1) by a large margin, which makes it the first detection-based counter that outperforms the longstanding total count estimation winner LOCA  by a remarkable 27% MAE and 4% RMSE on test split. In this respect, GeCo closes the performance gap that has been present for several years between state-of-the-art density-, and detection-based counters.

[MISSING_PAGE_FAIL:6]

4% AP and 5% AP50, and by significant 45% MAE and 49% RMSE on test split. These results show that GeCo features remarkable robustness to the number of exemplars since a single network (without re-training or fine-tuning) is used in both three- and one-shot setups. In particular, the performance drops by only 2%/11% of MAE/RMSE and 1%/1% AP/AP50 on the test split between both setups. In a _one-shot_ setting, GeCo surpasses state-of-the-art _three-shot_ models. Specifically, one-shot GeCo achieves 22% and 20% lower MAE and RMSE, respectively, compared to three-shot DAVE, and outperforms three-shot PSECO by 38% and 46% on the FSCD147 test set. These results highlight the robustness of GeCo to the number of exemplars, demonstrating its ability to handle inputs with lowered visual diversity.

**Zero-shot counting and detection.** Table 4 reports the results of the zero-shot GeCo compared with best zero-shot variants of the density-based counters, LOCA , CounTR , RepRPN-C , RCC  and with the zero-shot variant of the best detection-based counter DAVE . GeCo outperforms DAVE  by a significant margin of 14% MAE and 6% RMSE on the test set. Furthermore, it outperforms all density-based methods and sets a new state-of-the-art result on FSC  benchmark, by outperforming the top-performer CountTR  by impressive 6% MAE on the test set. Since the zero-shot variant of the recent detection-based counter PSECO  does not exist, we include its prompt-based variant for complete evaluation (i.e., target object class is specified by a text prompt). Even in this setup, the zero-shot GeCo outperforms the prompt-based PSECO by 20% MAE 16% RMSE, and 2% AP50 demonstrating great robustness to different counting and detection scenarios.

**Multiclass images.** To further verify the robustness of the proposed method, we validate it on a subset of FSCD147, that contain images with multiple object classes (FSCD147\({}_{}\)) . Results in Table 5 indicate that most state-of-the-art methods non-discriminatively count all objects in an image due to prototype over-generalization. GeCo outperforms all single-stage density-, and detection-based counters on multiclass images by at least 60%/67% in MAE/RMSE. This further verifies the

Figure 3: Compared with state-of-the-art few-shot detection-based counters DAVE , PSECO , and C-DETR , GeCo delivers more accurate detections with less false positives and better global counts. Exemplars are delineated with blue color, while segmentations are not shown for clarity.

robustness of the proposed architecture, which benefits from the hard-negative mining in the proposed loss function, leads to more discriminative prototype construction and false positive reduction.

### Ablation study

**Dense object detection loss.** To analyze the contribution of the new dense detection loss from Section 3.4, we trained GeCo using the standard loss [20; 35] that forms the ground truth objectness score by placing unit Gaussians on object centers - this variant is denoted by GeCoGauss. Table 6 shows that this leads to a substantial drop in total count estimation (38% RMSE, and 34% MAE) as well as in object detection (6% AP, and 3% AP50). Qualitative results are provided in Figure 4. As observed in columns 3 and 5, the classical unit-Gaussian-based loss [20; 35] forces the network to predict object locations from the object centers, which are not necessarily optimal for bounding box prediction. In contrast, the proposed dense detection loss enables the network to learn optimal point prediction, which more accurately aggregates information of the object pose. Columns 1 and 2 indicate that the new loss leads to superior detection of objects composed of blob-like structures avoiding false detections on individual object parts. Furthermore, the hard-negative mining integrated in the new loss design leads to better discriminative power of the detections and subsequent reduction of false positives (column 4).

    &  &  \\  Method & MAE (\(\)) & RMSE(\(\)) & AP(\(\)) & AP50(\(\)) & MAE(\(\)) & RMSE(\(\)) & AP(\(\)) & AP50(\(\)) \\  RepRPN-C  ACCV22 & 29.24 & 98.11 & - & - & 26.66 & 129.11 & - & - \\ RCC -AscV22 & 17.49 & 58.81 & - & - & 17.12 & 104.5 & - & - \\ CounTR  BAVC22 & 17.40 & 70.33 & - & - & 14.12 & 108.01 & - & - \\ LOCA  ICCV22 & 17.43 & 54.96 & - & - & 16.22 & 103.96 & - & - \\  PSECO * CVPR24 & 23.903\(\) & 100.33\(\) & - & - & 16.58\(\) & 129.77\(\) & 41.14\(\) & 69.03\(\) \\ DAVE  CVPR24 & 15.71\(\) & **60.34\(\)** & 16.31\(\) & 46.87\(\) & 15.51\(\) & 116.54\(\) & 18.55\(\) & 50.08\(\) \\ GeCo (ours) & **14.81\(\)** & 64.95\(\)** & **31.04\(\)** & **58.30\(\)** & **13.30\(\)** & **108.72\(\)** & **41.27\(\)** & **70.09\(\)** \\   

Table 4: Zero-shot density-based methods (top part), and detection-based methods (bottom part) on the FSCD147 . The symbol \(*\) denotes methods that also use text prompts as input.

    &  &  \\  Method & MAE (\(\)) & RMSE(\(\)) & AP(\(\)) & AP50(\(\)) & MAE(\(\)) & RMSE(\(\)) & AP(\(\)) & AP50(\(\)) \\  GMN  ACCV18 & 29.66 & 89.81 & - & - & 26.52 & 124.57 & - & - \\ CFOCNet  BAVC21 & 27.82 & 71.99 & - & - & 28.60 & 123.96 & - & - \\ FamNet  CVPR21 & 26.55 & 77.01 & - & - & 26.76 & 110.95 & - & - \\ BMNet+  CVPR22 & 17.89 & 61.12 & - & - & 16.89 & 96.65 & - & - \\ CountTR  BAVC22 & 13.15 & 49.72 & - & - & 12.06 & 90.01 & - & - \\ LOCA-1-366  ICCV23 & 11.36 & 38.04 & - & - & 12.53 & 75.32 & - & - \\  PSECO  CVPR24 & 18.31\(\) & 80.73\(\) & 31.47\(\) & 58.53\(\) & 14.86\(\) & 118.64\(\) & 41.63\(\) & 70.87\(\) \\ DAVE1+bot  CVPR24 & 10.98\(\) & 43.26\(\) & 18.00\(\) & 52.37\(\) & 11.54\(\) & 86.62\(\) & 19.46\(\) & 55.27\(\) \\ GeCo (ours) & **9.97\(\)** & **37.85\(\)** & **32.82\(\)** & **61.31\(\)** & **8.10\(\)** & **60.16\(\)** & **43.11\(\)** & **74.31\(\)** \\   

Table 3: One-shot density-based methods (top) and detection-based methods (bottom) on the FSCD147 .

    &  & _{}\)} \\  Method & MAE(\(\)) & RMSE(\(\)) & MAE(\(\)) & RMSE(\(\)) \\  C-DETR  ECCV22 & 16.79 & 123.56 & 23.09 & 30.09 \\ PSECO  CVPR24 & 13.05 & 112.86 & 25.73 & 44.95 \\ LOCA  ICCV23 & 10.79\(\) & 56.97\(\) & 21.28 & 43.67 \\ CounTR  BAVC22 & 11.95 & 91.23 & 14.56\(\) & 27.41\(\) \\ DAVE  CVPR24 & 10.45\(\) & 74.51\(\) & **3.09\(\)** & **5.28\(\)** \\ GeCo (ours) & **7.91\(\)** & **54.28\(\)** & 5.88\(\) & 9.17\(\) \\   

Table 5: Performance on FSCD147  test split, and its multiclass subset FSCD147\({}_{}\).

**Architecture.** To evaluate the impact of concatenating the SAM-HQ  features in the query _unpacking_ process in the DQD module (Section 3.2), we remove these features in \(_{}}\). Table 6 shows a counting performance drop 5% MAE and 10% RMSE. To validate the importance of modeling exemplar shapes, i.e., width and height, with prototypes \(^{S}\), we omit them in \(_{^{S}}}\). We observe a substantial performance decrease of 5% MAE, and 9% RMSE. Finally, we remove bounding box refinement in the detection refinement module (Section 3.3), and denote the variant as \(_{}}\). While this does not affect the global count estimation accuracy, we observe a 26% decrease in AP and 2% decrease in AP50. It is worth noting, that bounding box refinement improves the accuracy of predicted bounding boxes, however it does not enhance object presence detection.

To verify the importance of the DQE module (Section 3.1), we replace the dense object queries \(\) construction step (2) with a standard self-attention, i.e., \(=()_{3}\). This leads to a 8% MAE and 5% RMSE performance drop, verifying the proposed approach. To evaluate the importance of using image features as queries in (2), we change the object query construction to \(_{j}=((_{j-1}),^{I},}^{I})\) to follow a standard DETR -like approach, and denote it as \(_{}\). We observe a 20% MAE and 22% RMSE decrease in counting performance.

## 5 Conclusion

We proposed GeCo, a novel single-stage low-shot counter that integrates accurate detection, segmentation, and count prediction within a unified architecture, and covers all low-shot scenarios with a single trained model. GeCo features remarkables dense object query formulation, and prototype generalization across the image, rather than just into a few prototypes. It employs a novel loss function specifically designed for detection tasks, avoiding the biases of traditional Gaussian-based losses. The loss optimizes detection accuracy directly, leading to more precise detection and counting.

    &  &  \\  Method & MAE(\(\)) & RMSE(\(\)) & AP(\(\)) & AP50(\(\)) \\  GeCo & **9.52** & **43.00** & **33.51** & **62.51** \\ \(_{}\) & 12.79 & 59.33 & 31.43 & 60.73 \\ \(_{}}\) & 10.04 & 47.11 & 33.08 & 62.50 \\ \(_{}^{T}}\) & 9.97 & 46.93 & 32.56 & 61.19 \\ \(_{}}\) & 10.26 & 43.33 & 24.63 & 61.57 \\ \(_{}}\) & 10.32 & 45.14 & 33.01 & 61.68 \\ \(_{}\) & 11.45 & 52.46 & 32.24 & 61.60 \\   

Table 6: Ablation study on the FSCD147  validation split.

Figure 4: Response maps (in yellow), and locations for bounding box predictions (red dots) when using the proposed (first row) and the standard [20; 4; 35] (second row) training loss.

The main limitation of the presented method is that it cannot process arbitrarily large images, due to memory constraints, since it, as all current methods, operates globally. In future work, we will explore local counting, incremental image-wide count aggregation, optimizing inference speed utilizing a faster backbone .

Extensive analysis showcases that GeCo surpasses the best detection-based counters by approximately 25% in total count MAE, achieving state-of-the-art performance in a few-shot counting setup and demonstrating superior detection capabilities. GeCo showcases remarkable robustness to the number of provided exemplars, and sets a new state-of-the-art in one-shot as well as zero-shot counting.

**Acknowledgements.** This work was supported by Slovenian research agency program P2-0214 and projects J2-2506, L2-3169, Z2-4459, J2-60054, and by supercomputing network SLING (ARNES, EuroHPC Vega - IZUM).