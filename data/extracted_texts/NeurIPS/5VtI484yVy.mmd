# A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts

Samuele Bortolotti\({}^{1}\)1 Emanuele Marconato\({}^{2,1}\)1 Tommaso Carraro\({}^{3,6}\) Paolo Morettin\({}^{1}\)

**Emile van Krieken\({}^{4}\) Antonio Vergari\({}^{4}\) Stefano Teso\({}^{5,1}\) Andrea Passerini\({}^{1}\)**

\({}^{1}\) DISI, University of Trento \({}^{2}\)DI, University of Pisa \({}^{3}\)Fondazione Bruno Kessler

\({}^{4}\)University of Edinburgh \({}^{5}\)CIMeC, University of Trento

\({}^{6}\)University of Padova

{name.surname}@unitn.it

tcarraro@fbk.eu

{Emile.van.Krieken, avergari}@ed.ac.uk

Equal contribution.

###### Abstract

The advent of powerful neural classifiers has increased interest in problems that require both learning and reasoning. These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from _reasoning shortcuts_ (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the high-dimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal verification procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench.

## 1 Introduction

Although the field of deep learning has made significant progress in developing accurate neural classifiers, end-to-end neural networks struggle with tasks that also require symbolic reasoning on low-level inputs like visual objects [1; 2]. Instead, Neuro-symbolic (NeSy) AI [2; 3; 4; 5] promises to improve the trustworthiness of AI systems by integrating perception with symbolic reasoning [6; 7]. This involves extracting high-level _concepts_ from the input and reasoning over them with some prior knowledge, _e.g._, safety constraints, to obtain a prediction. This setup can encourage [8; 9; 10; 11; 12; 13] or even ensure [14; 15; 16] the output complies with the knowledge.

Recent evidence suggests that, in some problems, NeSy models can achieve high accuracy on the reasoning task by _learning concepts with incorrect semantics_. Such _reasoning shortcuts_ (RSs)  occur when the knowledge, which acts as a bridge between the given output labels and the concepts , allows for inferring the right label using unintended concepts. This can seriously undermine the original purpose of NeSy AI systems, especially in high-stakes scenarios. For instance, in the BDD-OIA dataset , a model is given a set of traffic laws, and must predict what actions an autonomous vehicle is allowed to perform (_e.g._, "go" or "stop"). It will believe it obeys these laws by confusing pedestrians for red lights, as both entail the correct action ("stop"). Yet, if - when usedin an out-of-distribution (OOD) task - the vehicle is allowed to cross over red lights in case of an emergency, its preexisting confusion can lead to unfortunate scenarios . RSs impact learnability , interpretability of the learned concepts [21; 22; 23; 24], and reliability in down-stream tasks [25; 26; 27; 28; 20]. At the same time, they can affect most NeSy architectures, regardless of how they are implemented, including approaches based on probabilistic logic [12; 13; 14; 16; 29; 30; 31; 32; 33], fuzzy logic [8; 34], reasoning in embedding space , and abduction [36; 37]. Given their impact, researchers have proposed several mitigation strategies [17; 25; 20; 38; 39; 40; 41; 42; 43], yet how to deal with RSs remains an open problem.

Unfortunately, suitable data sets with known RSs are scarce and scattered throughout the literature, hindering research on this challenging problem. Current benchmark suites for learning and reasoning neglect RSs altogether  and lack OOD data suitable for investigating their impact, while others are restricted to larger models [45; 46; 47]. Simultaneously, available data sets annotated with concept supervision (_e.g._, CUB200 ), which is essential for evaluating concept quality, do not require logical reasoning and do not supply prior knowledge.

**Contributions**. We fill this gap by introducing rsbench, an integrated benchmark suite providing all the ingredients needed for systematic evaluation of the impact of RSs and the efficacy of mitigation strategies. rsbench comprises: 1) A curated collection of _tasks_ that require learning and reasoning that are provably affected by RSs. rsbench comprises entirely new and already established tasks with different flavors - arithmetical, logical, and high-stakes - along with associated _data sets_ and _data generators_ for evaluating OOD scenarios.2 2) Python implementations of _quality metrics_ useful for assessing the impact of RSs on NeSy models and more generally the reliability of concepts learned (explicitly or implicitly) by other concept-based architectures and end-to-end neural networks. 3) a novel algorithm, countrss, that exploits automated reasoning techniques  to _verify a priori_ whether a task is affected by RSs and to count them. We showcase rsbench by assessing the impact of RSs on the quality of concepts acquired by several deep learning architectures, illustrated in Fig. 1.

## 2 Reasoning Shortcuts: Causes, Consequences, and Scope

We study tasks where models require both learning and reasoning (in short: _L&R tasks_) to accurately predict a (vector) output \(\) from low-level inputs \(\). First, we assume there is a set of \(k\) high-level concepts \(^{*}\) associated to the inputs \(\). Then, we assume the concepts \(^{*}\) and prior knowledge \(\) together infer the correct output \(^{*}\). The prior knowledge can encode known structural  or safety constraints  in some formal language (_e.g._, logical connectives).

**Example 1**.: _The_ SDD-OIA _dataset (detailed in Section 3.3) is a L&R task that contains images \(\) of 3D traffic scenes, and the goal is to predict one or more allowed actions \(\{,,,\}\). We assume the correct output depends on binary concepts \(c_{},c_{},c_{}\) encoding whether green lights, red lights, and pedestrians are visible, respectively. The knowledge specifies that if the latter are detected, the vehicle must stop: \(=(c_{} c_{} y_{})\)._

Figure 1: **Role of concepts in deep learning models.** (a) NeSy architectures like DeepProbLog (DPL) and Logic Tensor Networks (LTN) map the input \(\) to concepts \(\) and reason over these according to prior knowledge to obtain a label \(\). (b) CBMs are similar, except the prediction is computed by a learned linear layer, making it easy to obtain concept-level explanations of all predictions. (c) Black-box neural networks infer a label \(\) directly from the input \(\); concepts \(\) can be extracted from their latent representation by applying techniques like TCAV . Lighting bolts indicate what variables are usually supervised.

[MISSING_PAGE_FAIL:3]

## 3 The rsbench Benchmark Suite

In the following, we outline the L&R tasks and metrics provided by rsbench. By construction, RSs do not compromise in-distribution performance, and their worst effects are seen on OOD data. rsbench facilitates _constructing novel OOD data sets_ by providing a configurable _data generator_ for each of its tasks (except BDD-OIA, cf. Section 3.3). These enable fine-grained control over all details of the training, validation and test splits (like number of examples and percentage allocated to each split, in addition to task-specific settings discussed in the relevant subsection) and the creation of OOD splits, all through a simple YAML configuration file. All tasks are available as Python classes and their knowledge K is supplied in the widely used DIMACS CNF format , to support interoperability with model implementations and reasoning packages. In Sections 3.1 to 3.3, for each task, we illustrate a possible reasoning shortcut and its impact on an OOD input.

Table 1 provides an overview of the rsbench L&R tasks, breaking them down into relevant properties, namely whether they: include a _data generator_ (Gen); allow users to create () or provide ready-made () _out-of-distribution_ splits (OOD); allow users to create () or provide ready-made () data suitable for _continual learning_ (ConL) ; have complex inputs, making it difficult to extract concepts (, of different objects) separately (Cplx x); require complex reasoning when using the default knowledge (Cplx K); by default use knowledge that is intrinsically _ambiguous_, _i.e._, it yields RSs even if the training set contains all possible combinations of concepts and labels (Amb K). A task involves complex inputs (Cplx x) when it requires processing semi-realistic visual scenes with multiple objects for concept extraction (,, and ). It involves complex reasoning (Cplx K) when inference requires handling interrelated concepts or multi-step reasoning. For instance, BDD-OIA and SDD-OIA require inferring \(4\) actions from \(20\) interrelated concepts (, traffic lights of different colors, presence of pedestrians), where some concepts are mutually exclusive (, traffic lights can't be green and red simultaneously).

Fig. 2 illustrates how a NeSy architecture (DPL) operates on a rsbench L&R task (BDD-OIA).

### Arithmetical Tasks and Data Sets

MNAdd is the quintessential benchmark for evaluating reasoning in NeSy AI [7; 12; 58; 59; 60; 61; 62]. The goal is to infer the sum of \(k 2\) MNIST  digits, provided knowledge encoding the rule of summation, that is,._.g._, the model should predict \(y=7\). Despite its simplicity, MNAdd highlights a clear performance gap between pure neural baselines and NeSy architectures [14; 38]. RSs arise when we can infer the correct sum using the wrong digits. This can occur due to commutativity (, and both sum to \(4\)) or incomplete training data (, in absence of other training examples, knowing that sum to \(5\) is insufficient to discriminate

Figure 2: This figure illustrates inference and training in regular NeSy architectures for one BDD-OIA example . The input \(\) is a dashcam image. The model first extracts concepts from the image using a neural backbone (NN) and then uses a (differentiable) reasoning layer to infer a vector label \(=(y_{},y_{},y_{},y_{})\). While the model includes a neural component, the labels depend solely on the extracted concepts. The reasoning layer is aware of prior knowledge K, which encodes constraints like “if a pedestrian or a red light is detected, the prediction must be stop.”

[MISSING_PAGE_FAIL:5]

**Task:**Kand-Logic is a task - inspired by Wassily Kandinsky's paintings and  - that requires simple (but non-trivial) perceptual processing and relatively complex reasoning. In the simplest case, each input \(x=(x_{1},x_{2})\) consists of two \(64 64\) images, each depicting three geometric primitives with different shapes (\(\), \(\), \(\)) and colors (**red**, **blue**, **yellow**). The goal is to predict whether \(x_{1}\) and \(x_{2}\) fit the same predefined _logical pattern_ or not. Let each \(x_{i}\) contain three primitives \(x_{i,1}\), \(x_{i,2}\), \(x_{i,3}\) with two concepts each: shape \((x_{ij})\) and color \((x_{ij})\). The pattern is built out of predicates like "all primitives in the image have a different color", "all primitives have the same color", and "exactly two primitives have the same shape", formally:

\[(x_{i})=_{j j^{}}((x_{ij}) (x_{ij^{}})),(x_{i})=_{j j ^{}}((x_{ij})=(x_{ij^{}})),\]

and \((x_{i})=(x_{i})(x_{i})\). For instance, the default pattern \((x_{i})\) is "all images include either the same number of primitives with the same color, or the same number of primitives with the same shape", or equivalently:

\[((x_{1})(x_{2}))( (x_{1})(x_{2}))((x_ {1})(x_{2}))\] \[((x_{1})(x_{2}))( (x_{1})(x_{2}))((x_{1}) (x_{2}))\]

An input \(x\) is positive if and only if it satisfies the pattern, _i.e._, \(=(Y(x_{1},x_{2}))\). Unlike MNLogic, in Kand-Logic each primitive has multiple attributes that cannot easily be processed separately. This means that RSs can easily, _e.g._, confuse shape with color when either is sufficient to entail the right prediction, as in the example above. rsbench provides the data set used in  (\(3\) images per input with \(3\) primitives each) and a generator that allows configuring the number of images and primitives per input and the pattern itself.

**Task:**CLE4EVR focuses on logical reasoning over three-dimensional scenes, inspired by CLEVR and CLEVR-HANS. Among these, CLEVR is tailored for visual-question answering and CLEVR-HANS to contain confounding factors at the input level, to make _shortcuts_ arise . Both of them typically provide models with exhaustive concept-level supervision, obscuring whether RSs are present without it. Our CLE4EVR constitutes a simplified version where RSs can be easily determined. Each input image \(x\), of size \(240 320\), contains a variable number of objects differing in size (\(3\) possible values), shape (\(10\)), color (\(10\)), material (\(2\)), position (real), and rotation (real), and the goal is to determine whether the objects satisfy a pre-specified condition \(\) that depends on all discrete attributes of the objects in the scene. The default knowledge \(\) is designed to induce RSs : it asserts that an image \(x\) is positive iff at least two objects \(x_{i}\) and \(x_{j}\) have the same color and shape, _i.e._, \( i j\,.\,((x_{i})=(x_{j}))((x_{i})=(x_{j}))\). When all possible colors and shapes are observed, the only RSs CLE4EVR is affected by are those in which the attributes of first object are attributed to the second one. However, if the training set includes only _some_ combinations - _e.g._, pink rings and gray spheres are never observed together - the model can collapse different shapes and colors . Hence, even when objects are processed separately (_e.g._, via Faster-RCNN embeddings), the model can confuse colors and shapes with one another, _e.g._, it can mistake blue cones for red pyramids and vice versa. Occlusion further complicates the picture, complicating both perception and reasoning. As above, the generator allows to customize the number of objects per image, the knowledge, and whether occlusion is allowed.

### High-stakes Tasks and Data Sets

**Task:**BDD-OIA is a multi-label autonomous driving task for studying RSs in real-world, _high-stakes_ scenarios. The goal is to infer what actions out of \(\{,,,\}\) are safedepending on what objects (_e.g._, cars, traffic signs) are present in an input dashcam image. The knowledge \(\) establishes that, _e.g._, it is not safe to move forward if there are pedestrians on the road, based on a set of \(21\) binary concepts indicating the presence of different obstacles on the road. The constraints specify conditions for being able to proceed (\(\)), stop (\( \)), and for turning left and right, as well as relationships between actions (_e.g._, \(\)). Input images, of size \(720 1280\), come with concept-level annotations, making it possible to assess the quality of the learned concepts. The dataset comprises \(16,082\) training examples, \(2,270\) validation examples and \(4,572\) test examples. Common RSs allow to, _e.g._, confuse pedestrians with red_lights, as they both imply the correct (\(\)) action for all training examples .

**Task: SDD-OIA. BDD-OIA** is not suitable for systematically evaluating RSs out-of-distribution, where they show the highest impact. With rsbench, we fill this gap by introducing SDD-OIA, a synthetic replacement for BDD-OIA that comes with a fully configurable _data generator_, enabling fine-grained control over what labels, concepts, and images are observed and the creation of OOD splits. In short, SDD-OIA shares the same classes, concepts and (by default) knowledge as BDD-OIA, but the images are 3D traffic scenes modelled and rendered using Blender  as \(469 387\) RGB images. Images are generated by first sampling a desired label \(\), then picking concepts \(\) that yield that label, and then rendering an image \(\) displaying those concepts. This allows to easily control what concepts and labels should appear in all data splits, which in turn determine what kinds of RSs can be learned. The complete data generation process is described in Appendix C.11.

In Section 4, we showcase SDD-OIA by implementing an OOD autonomous ambulance scenario  in which the vehicle is allowed to cross red lights in case of an emergency. Formally, this requires altering the prior knowledge by introducing a new emergency variable that conditions the traffic rules, that is, (\(\)emergency\(\) original rule for \(\)) \(\) (\(\)emergency\(\) alternative rule for \(\)), and similarly for turn_left and turn_right. We specifically test this scenario in Section 4. Naturally, other challenging OOD scenarios can be created.

### Metrics for Reasoning Shortcuts

**Model-level metrics.** rsbench facilitates assessing learned models by implementing several metrics for label and concept predictions - including accuracy and \(F_{1}\) score - as well as metrics for RSs. First, rsbench provides concept-level confusion matrices, which show how well the predicted concepts \(=(c_{1},,c_{k})\) recover the annotations \(^{*}=(c_{1}^{*},,c_{k}^{*})\) and are essential for visualizing and spotting RSs, as can be seen in Table 3. Second, it implements _concept collapse_\((C)\), which measures to what extent the learned concepts mix distinct ground-truth concepts. Given a concept confusion matrix \(C^{m m}\), where \(m\) is the size of the confusion matrix (_e.g._, \(m=2^{k}\) when all ground-truth concepts are observed), it is defined as \((C)=1-p/m\), where \(p=_{j=1}^{m}1\{ i\.\ C_{ij}>0\}\). High collapse shows that the model tends to use fewer concepts to solve the task, making it useful for diagnostics. Vice versa, a lower concept collapse may indicate that the RS is densely activating all concepts. Concept collapse is not trivial to implement because not all \(2^{k}\) ground-truth concept combinations may appear in the test set (especially when \(k\) is large, see Appendix A for details), so rsbench provides ready-made implementations for all its tasks.

**Task-level metrics.** RSs arise due to a complex interaction between prior knowledge and training data (cf. Section 2) making it difficult to assess _a priori_ which L&R tasks they affect. Fortunately, it is possible to count how many optimal (deterministic) RSs affect a L&R task [20; 25], as long as this satisfies two technical assumptions.3 They also provide a closed-form expression for the count that works only when the training set is _exhaustive_ (that is, comprises all possible combinations of concepts, like MNAdd) and the concepts are extracted jointly. This makes it possible to _formally verify_ whether a L&R task can be solved via RSs by checking that the count is larger than \(1\). This is crucial for anticipating the occurrence of RSs in novel tasks and for iteratively improving task specifications in the design stage. In practice, however, the training set is seldom exhaustive and concepts are often processed separately. While the former issue can be overcome - and in fact, we provide a closed-form solution in Appendix A.3 - the latter is more challenging.

rsbench addresses it by implementing a practical counting algorithm, named countrss, that leverages _automated reasoning_ techniques. In a nutshell, each optimal RS can be viewed as a linear mapping \(=A^{*}\) that maps ground-truth to predicted concepts under the _constraint_ that these yield the correct label \(^{*}\) for all training examples. The problem thus boils down to counting how many matrices \(A\{0,1\}^{k k}\), where \(k\) is the number of concepts,4 satisfy this constraint. Given the exponential (in \(k\)) number of candidates, countrss relies on state-of-the-art _model counting_ solvers  for efficiency. That is, we encode the above _constraint_ as a propositional logic formula (see Appendix A for the exact encoding), such that each model (solution) represents a distinct RS. countrss, based on PyEda, works for all L&R tasks that satisfy the necessary technical assumptions, including all those in rsbench except BDD-OIA, and supports both exact  and, for the more complex tasks, approximate counting .

We showcase countrss by evaluating the impact of the amount of training examples on the RS count for two instances of MMLogic: AND (with \(=(y c_{1} c_{2} c_{2})\)) and XOR (\(=(y c_{1} c_{2} c_{3})\)). When the training set is exhaustive, AND admits \(6\) RSs and XOR \(24\), proving that symmetries in the XOR function make it latter more ambiguous in this case, as stated in Section 3.2. The number of RSs grows drastically when we only provide a single example, as it becomes easier to predict all labels correctly while confusing concepts, with the XOR presenting \(192\) RSs. For the AND, the count depends on whether the single ground truth label is positive or negative, the number of RSs growing to \(48\) and \(336\), respectively. This highlights how even simple formulas can be affected by RSs and that these depend crucially on the available data, as expected, and that countrss can anticipate the occurrence of RSs in L&R tasks without the need for training any model.

## 4 Evaluating RSs and Concept Quality with rsbench

rsbench is meant to be a general framework for evaluating the impact of RSs and concept quality in _any_ machine learning model. We showcase this by evaluating _five_ different architectures on three L&R tasks, one per "flavor", namely MlAdd-EvenOdd, Kand-Logic, and SDD-OIA.

We consider two state-of-the-art NeSy models: DeepProbLog (DPL)  and Logic Tensor Networks (LTN) . Both comprise a neural network module to extract concepts \(\) for every input \(\), which are later used to predict labels \(\) according to the knowledge \(\) (Fig. 1(a)). These predictions are done according to a probabilistic logic semantic in DPL and by using fuzzy logic in LTN. As stated in Section 2, we also experiment with purely neural models, evaluating the quality of the concepts they learn on rsbench. Specifically, we employ CBMs (Fig. 1(b)), black-box NNs and CLIP (Fig. 1(c)). In our analysis, we investigate directly the bottleneck layer for CBM, where concepts are expected to be learned, and adopt TCAV for NN and CLIP.

    & \(_{1}(Y)()\) & \(_{1}(C)()\) & \((C)()\) \\  DPL & \(0.87 0.15\) & \(0.25 0.09\) & \(0.69 0.04\) \\ LTN & \(0.77 0.09\) & \(0.35 0.04\) & \(0.00 0.01\) \\ CMN\({}^{}\) & \(0.36 0.04\) & \(0.59 0.01\) & \(0.00 0.01\) \\ NN & \(0.72 0.08\) & \(0.33 0.01\) & \(0.17 0.01\) \\ CLP\({}^{}\) & \(0.99 0.01\) & \(0.32 0.01\) & \(0.00 0.01\) \\   

Table 4: Results on Kand-Logic

    & \(_{1}(Y)()\) & \(_{1}(C)()\) & \((C)()\) \\  DPL & \(0.94 0.04\) & \(0.06 0.08\) & \(0.61 0.08\) \\ LTN & \(0.66 0.10\) & \(0.05 0.06\) & \(0.70 0.01\) \\ \(^{}\) & \(0.89 0.13\) & \(0.44 0.07\) & \(0.09 0.09\) \\ NM\({}^{}\) & \(0.57 0.38\) & \(0.07 0.03\) & \(0.29 0.34\) \\ CLP\({}^{}\) & \(0.62 0.14\) & \(0.04 0.01\) & \(0.48 0.17\) \\   

Table 2: Results on MNAdd-EvenOdd

    & \(_{1}(Y)()\) & \(_{1}(C)()\) & \(_{1}(C)()\) & \(\)-\(_{1}(Y)()\) \\  DPL & \(0.80 0.01\) & \(0.49 0.03\) & \(0.86 0.04\) & \(0.62 0.09\) \\ LTN & \(0.82 0.04\) & \(0.46 0.04\) & \(0.81 0.02\) & \(0.72 0.06\) \\ CRN\({}^{}\) & \(0.60 0.12\) & \(0.61 0.04\) & \(0.68 0.06\) & \(0.45 0.05\) \\ NN\({}^{}\) & \(0.93 0.18\) & \(0.44 0.02\) & \(0.43 0.28\) & \(0.47 0.19\) \\ CLIP\({}^{}\) & \(0.90 0.09\) & \(0.43 0.04\) & \(0.23 0.02\) & \(0.81 0.06\) \\   

Table 5: Performance on SDD-OIAWe evaluate macro \(F_{1}\) for predicted labels and concepts, denoted as \(_{1}(Y)\) and \(_{1}(C)\), respectively, and concept collapse \((C)\) (see Appendix A for the exact definition). We report the mean and standard deviation over \(10\) random seeds. We train all models by maximum likelihood on the labels, as customary in NeSy and for neural baselines. Notice that, CBM without annotated concepts would be equivalent to NN, therefore we supervise a handful of concepts, as customary [55; 22; 73; 23]. Specifically, we supervise a total of \(100\) examples for MNAdd-EvenOdd (only for the digits \(3\), \(4\), \(8\) and \(9\)); \(20\) examples for Kand-Logic (for red, \(\), and \(\)); and \( 700\) examples for SDD-OIA on the high-stakes concepts red_light, green_light, car, person, rider, other_obstacle, stop_sign, right_green_light, and left_green_light. All details about the losses, architectures, metrics, and model selection procedure we use are reported in Appendix B.

**All tasks succeed in inducing RSs across all models**. The results in Tables 2, 4 and 5 show that all models attain medium-to-high \(_{1}(Y)\) on the three benchmarks, meaning the labels _can_ be predicted accurately, with the following exceptions: On MNAdd-EvenOdd LTN, NN, and CLIP show medium-to-high variance (\(10\%\), \(38\%\), and \(14\%\), respectively); On Kand-Logic, LTN, CBM, and NN reach suboptimal performance on (around \(77\%\), \(36\%\), and \(72\%\), respectively); On SDD-OIA, CBM scores only \(60\%\)\(_{1}(Y)\). We attribute the subbar \(_{1}(Y)\) score of CBM to the fact that their top linear layer is not expressive enough to accurately infer the label from the concept bottleneck in more complex tasks like Kand-Logic and SDD-OIA. Despite these variations in prediction performance, _all models show overall low concept quality_, as measured by \(_{1}(C)\). Even CBM, despite receiving concept supervision, fare below \(60\%\).

**Understanding concept quality with rsbench.** The high rate of \((C)\) in all tasks (except for LTN in Kand-Logic) suggests that NeSy models tend mix concepts together . The left-most confusion matrix in Table 3 shows that in MNAdd-EvenOdd, DPL uses roughly half of the available digits to solve the task with high \(_{1}(Y)\). In contrast, CBM experiences less collapse due to concept supervision. NN and CLIP also yield overall low collapse (an exception being CLIP in MNAdd-EvenOdd), and in fact the right-most confusion matrix in Table 3 shows that NN in MNAdd-EvenOdd activates densely most concepts. We point out that, however, lower collapse for INI and CLIP stems also from TCAV, which can introduce noise in the extracted concepts. Due to space constraints, we report a detailed analysis of this phenomenon in the supplementary material.

**Generators enable measuring the OOD impact of RSs.** For SDD-OIA we leverage the generator to evaluate an ood setting where the same concepts are used with a different knowledge \(_{000}\) (reported in Section 3.3). _We observe that all models suffer a visible drop in OOD \(_{1}(Y)\) performance_, as expected: DPL drops by \(18\%\), LTN by \(10\%\), and CBM by \( 15\%\). NN is the most affected, with average \(46\%\) difference, while CLIP is the most resilient with only \(9\%\) drop.

## 5 Discussion and Conclusion

We introduced rsbench, an integrated benchmark suite for systematic evaluation of RSs and concept quality in tasks requiring learning and reasoning. While existing benchmark suites  neglect RSs altogether, rsbench supplies datasets for various RS-heavy tasks and corresponding ready-made data generators for evaluating OOD and continual learning scenarios. At the same time, rsbench provides formal verification and evaluation routines for assessing how much RSs and concept quality affect each task. Our experiments showcase how rsbench enables practitioners to easily investigate the impact of RSs on several existing or future deep learning architectures.

RSs are also connected to the more general problem of learning high-level concepts from data, _aka_ symbol grounding [74; 75]. Interpretable concepts play an increasingly central role as a _lingua franca_ in explainable AI (XAI) [21; 76] for both _post-hoc_[77; 50; 78; 79] and _ante-hoc_[55; 73; 22; 80; 81; 24] explanations of model decisions. As with RSs, a central question is whether the concepts encode the intended semantics [82; 83]. rsbench can be used to benchmark precisely whether learned concepts satisfy this condition. Furthermore, it can also benefit new research in mechanistic interpretability [84; 85; 86; 87; 88], specifically for studying challenging scenarios in which deriving a high-level explanation of neural networks behavior is complicated by poor concept semantics.

Another related topic is _identifiability_ of latent concepts, which is studied in independent component analysis  and causal representation learning [90; 53; 91]. rsbench can be readily used to empirically assess identification of latent concepts with only label supervision [92; 93; 94].

**Broader Impact.** Concepts confused by RSs can lead to poor down-stream decision making. With rsbench, we hope to enable research on mitigation strategies for RSs to avoid such consequences. At the same time, rsbench might be used to design adversarial attack that exploit or promote RSs and therefore undermine the trustworthyness of ML systems.

**Limitations and Future Work.** While rsbench already provides a variety tasks offering different learning and reasoning challenges, we plan to extend it to include variants of other popular reasoning datasets such as Visual Sudoku , Raven matrices , KANDY , CLEVR-HANS , and ROAD-R  which in their current status do not allow a systematic study of RSs. Implementations of NeSy architectures make use of distinct formisms and file formats, making it especially challenging to ensure data interoperability. rsbench partially addresses this by supplying both Python APIs and CNF specifications - the standard file format for logic formulas in formal verification - for all L&R tasks. In the future, we will build on initiatives like ULLER , which promise to provide a unified interface for NeSy architectures. We also plan to improve the scalability of the formal verification algorithm to larger L&R tasks and to leverage as a guide for active learning-based mitigation strategies.