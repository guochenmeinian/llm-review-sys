# Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling

Skyler Wu

Booz Allen Hamilton

Harvard University

Stanford University

wu_skyler@bah.com &Fred Lu

Booz Allen Hamilton

University of Maryland, Baltimore County

lu_fred@bah.com &Edward Raff

Booz Allen Hamilton

University of Maryland, Baltimore County

raff_edward@bah.com &James Holt

Laboratory for Physical Sciences

holt@lps.umd.edu

###### Abstract

Online learning methods, like the seminal Passive-Aggressive (PA) classifier, are still highly effective for high-dimensional streaming data, out-of-core processing, and other throughput-sensitive applications. Many such algorithms rely on fast adaptation to individual errors as a key to their convergence. While such algorithms enjoy low theoretical regret, in real-world deployment they can be sensitive to individual outliers that cause the algorithm to over-correct. When such outliers occur at the end of the data stream, this can cause the final solution to have unexpectedly low accuracy. We design a weighted reservoir sampling (WRS) approach to obtain a stable ensemble model from the sequence of solutions without requiring additional passes over the data, hold-out sets, or a growing amount of memory. Our key insight is that good solutions tend to be error-free for more iterations than bad solutions, and thus, the number of passive rounds provides an estimate of a solution's relative quality. Our reservoir thus contains \(K\) previous intermediate weight vectors with high survival times. We demonstrate our WRS approach on the Passive-Aggressive Classifier (PAC) and First-Order Sparse Online Learning (FSOL), where our method consistently and significantly outperforms the unmodified approach. We show that the risk of the ensemble classifier is bounded with respect to the regret of the underlying online learning method.

## 1 Introduction

Online learning algorithms are especially attractive when working with high-volume and high-dimensional streaming data, out-of-core processing, and other throughput-sensitive applications . For example, the seminal Vowpal Wabbit uses importance-weighted online learning algorithms  to reach high quality solutions quickly, with an optional second pass using LBFGS to refine the solution. The MOA library still uses the Pegasos algorithm as its linear classifier . Most relevantly, online learning algorithms are particularly appealing for binary classification tasks, such as web spam classification . Such algorithms often enjoy fast theoretical convergence rates due to their fast adaptation to errors on individual data points, as opposed to batch or offline learning.

However, in real-world deployment, **online algorithms can be very sensitive to noisy observations in the data stream and over-correct, resulting in out-of-sample performance dropping precipitously between timesteps.** Indeed, in many cases (see Figure 1), an online learning algorithm mightachieve over \(90\%\) test accuracy after a given timestep, but then see its test accuracy drop by \(20-30\%\) after over-correcting on the next observation in the data stream. In many real-world settings, it may be infeasible computationally or memory-wise to maintain a hold-out evaluation set to select the highest-performance solutions learned by our online algorithm. It may also be practically infeasible to train our classifier over multiple passes of a given dataset or when online algorithms are used for "any-time" ready predictions.

In this paper, we introduce a weighted reservoir sampling (WRS)  based approach that dramatically mitigates the aforementioned accuracy fluctuations. Our proposed method, WRS-Augmented Training (WAT), neither requires a hold-out evaluation set nor additional passes over the training data. Most importantly, WAT can be used to stabilize _any_ passive-aggressive online learning algorithm. We demonstrate the promise of our WAT method on the Passive Aggressive Classifier (PAC)  and First-Order Sparse Online Learning (FSOL)  methods -- creating two new methods PAC-WRS and FSOL-WRS. **Strikingly, across 16 benchmark datasets, WAT is able to mitigate test accuracy fluctuations in all 16 datasets on FSOL and 14 datasets on PAC.** To analyze the theoretical effectiveness of our method, we situate our approach in the online-to-batch conversion literature, enabling us to obtain generalization bounds on i.i.d. data streams.

## 2 Review of related work

Our work is motivated in part by a real-world need in malware detection , in which large datasets make online methods particularly attractive [7; 8], and a naturally noisy labeling process inhibits standard passive-aggressive methods [9; 10; 11; 12]. We show results for the EMBER malware benchmark in Appendix C.

**Online learning.** In online learning for linear binary classification, one maintains a solution vector \(_{t}^{D}\), where \(D\) is the dimensionality of our data points (usually quite large), and \(t\) represents the timestep. At each timestep \(t\), we observe a single observation \((_{t},y_{t})\) from a high-throughput data stream, with feature vector \(_{t}^{D}\) and class label \(y_{t}\{+1,-1\}\). Given \((_{t},y_{t})\), an online learning algorithm will make a minor (potentially no effect) update to \(_{t}\) to output \(_{t+1}\), before receiving the next \((_{t+1},y_{t+1})\) in the data stream. The classification rule using \(_{t}\) on any test point \(^{*}\) is simply \(^{*}=(_{t}^{}^{*})\). The goal is that as \(t\), the sequence of \(_{t}\) will enjoy

Figure 1: Test accuracies (\(y\)-axis) over timestep (\(x\)-axis) for PAC-WRS and FSOL-WRS on Avazu (App) and News20. **Light grey lines:** test accuracies of the baseline methods — PAC or FSOL — at each timestep. **Solid black lines:** test accuracies of the “oracle” models, computed as the cumulative maximum of the baselines. **Solid blue lines:** test accuracies of WRS-enhanced models. Note massive fluctuations of grey lines and stability of blue lines. All variants shown are using standard sampling weights for WRS, with simple-averaging.

low cumulative loss. Towards this end, many online learning algorithms with various update rules have been proposed in the literature, including the Passive-Aggressive Classifier (PAC) , the Adaptive Regularization of Weight Vectors (AROW) methods  and the Adaptive Subgradient method (ADAGRAD) . Some online learning algorithms have also been designed to specifically learn sparse solutions for \(_{t}\) (proportion of zero entries), such as Truncated Gradient method , Stochastic MIrror Descent Algorithm made Sparse method (SMIDAS) , Regularized Dual Averaging (RDA) method , and First-Order and Second-Order Sparse Online Learning methods (FSOL and SSOL) . In general, these sparsity-inducing methods are powered by some combination of solution truncation and L1 norm minimization.

**Passive-aggressive online learning.** Within the family of online learning algorithms, a _passive-aggressive_ algorithm is one whose update rule _makes no update_ to \(_{t}\) if \((_{t},y_{t})\) is classified correctly with sufficient margin. That is, we _passively_ leave \(_{t+1}=_{t}\). If a margin error occurs, we _aggressively_ update \(_{t+1}\) such that the error is fully correct (though a regularization penalty \(C\) will tamper the degree of aggressiveness). Usually, correct classification with sufficient margin is defined using the _hinge loss_\(\) -- the algorithm remains passive at timestep \(t\) if \((_{t};(_{t},y_{t}))=(1-y_{t}_{t}^{}_{t},0)=0\). Next, we introduce the two passive-aggressive algorithms that we will use to test our WRS-Augmented Training method.

_Passive-Aggressive Classifier (PAC)._ Introduced by Crammer et al., PAC is actually a family of three algorithms: PA, PA-I, and PA-II . The base PA algorithm update rule seeks to solve the following constrained optimization problem:

\[_{t+1}=*{arg\,min}_{^{D}}\|-_{t}\|^{2}\;\;\;\;(;(_{t},y_{t}))=0.\]

Because of the hard constraint of forcing \(_{t+1}\) to satisfy \((;(_{t},y_{t}))=0\), the optimization is particularly vulnerable to noisy data. As such, Crammer et al. introduce a new constrained optimization function with a slackness hyperparameter \(C_{err}\) to allow for some residual hinge loss and induce less aggressive, but presumably more stable updates:

\[_{t+1}=*{arg\,min}_{^{D}}\|-_{t}\|^{2}+C_{err}^{m}\;\;\;\; (;(_{t},y_{t}))\;\;\;\; 0,\]

where setting \(m=1\) corresponds to PA-I and \(m=2\) to PA-II. From initial testing, PA-I and PA-II performed very similarly, with a slight edge to PA-II. As such, for this paper, we will focus on PA-II, which performs the following closed-form update when in aggressive mode : \(_{t+1}=_{t}+_{t};(_{t},y_{ t}))}{\|_{t}\|^{2}+_{err}}}y_{t}_{t}\). For the remainder of this paper, "PAC" will refer to PA-II.

_First-Order Sparse Online Learning (FSOL)._ Introduced by Zhao et al. , FSOL is a passive-aggressive algorithm which attempts to find sparse solutions for \(_{t}\). Governed by a learning rate \(\) and a sparsity parameter \(\), FSOL keeps track of two vectors \(_{t},_{t}^{D}\) and performs the following update rules when in aggressive mode :

\[_{t+1}=_{t}+ y_{t}_{t};\;\;_{t+1}= (_{t+1})[|_{t+1}|-_{t}]_{+},\]

where \(_{t}=\) and \([]_{+}\) takes the maximum of each element in \(\) and \(0\). Zhao et al. note that the above update rules are identical to that of Xiao's RDA method with soft \(1\)-norm regularization .

**Weighted Reservoir Sampling (WRS).** Suppose we have a collection of items \(V=\{_{1},,_{T}\}\), with corresponding nonnegative weights \(w_{1},,w_{T}\). Our goal is to collect a size-\(K\) weighted random sample from \(V\)_in one pass_ (imagine this process is indexed by time), where the population size \(T=|V|\) is potentially unknown. Introduced by Efraimidis and Spirakis , _weighted random sampling with a reservoir_, which we shorten to _weighted reservoir sampling_ (WRS), is an algorithm that allows us to collect such a size-\(K\) weighted random sample under the aforementioned conditions. Specifically, as we are making our one pass through the items in \(V\), at each timestep \(t\), we maintain and update a _reservoir_ -- a temporary storage unit with a maximum capacity of \(K\) items, with each item in the reservoir a potential candidate for our final size-\(K\) sample. At time \(T\), the \(K\) items that are currently in the reservoir will constitute our sample of size-\(K\). We invite the interested reader to look at Algorithm A in  for specific details.

**Online-to-batch conversion.** Online learning algorithms such as PAC generally do not impose any restrictions on the distribution of the training data sequence. Their _regret_ bounds aim to control the cumulative loss \(M_{T}\) of the algorithm over any sequence of data, compared with a minimal-loss fixed model \(}\): \(_{T}_{t=1}^{T}(_{t};z_{t})-_{t=1}^{T }(};z_{t})=M_{T}-_{t=1}^{T}(};z_{t})\). When using a fixed model to classify unseen data, we need to impose an i.i.d. assumption on the data stream in order to measure the _risk_, or expected generalization error, of the model. Note that the distribution \(\) itself is arbitrary and can still permit outliers or mixtures. Then the population _risk_ is defined as \(R_{}()_{z}[(;z)]\).

To theoretically describe our algorithm, we leverage work on online-to-batch conversion, which takes an online algorithm with known regret bounds on an i.i.d. sequence of data and extracts a stable final model with low risk. For example, in the online perceptron algorithm, earlier work studied the _pocket_ approach, which selects the longest-surviving model in the sequence as the final model [18; 19]. Other well-known approaches use the uniform average of the whole model sequence or the best-performing model over a validation set .

As will be seen, our method generalizes these approaches to utilize multiple long-survival models as an ensemble model. Furthermore, we will introduce novel improvements, including a limited-size reservoir with probabilistic sampling. The risk bounds for our WAT model leverage improved techniques from [21; 22]. Our experimental results also demonstrate that our novel conversion technique outperforms prior baselines (see Appendix D).

## 3 Our method: WRS-Augmented Training (WAT)

In one extreme, if a candidate solution \(_{t}\) from a passive-aggressive algorithm had _perfect classification with sufficient margin_ on any given data point, then the _subsequent number of passive steps_ taken after time \(t\) (i.e., number of timesteps that our algorithm is in passive mode before going aggressive again) would be infinite. In the other extreme, if a candidate solution \(_{t}\) had extremely low performance, then our passive-aggressive algorithm is likely to go aggressive very soon after time \(t\), implying a very small subsequent number of passive steps after time \(t\). In short, our key insight is that high-performing solutions \(_{t}\) tend to be error-free for more iterations than low-performing solutions. As such, the subsequent number of passive steps taken after the formation of \(_{t}\) provides an estimate of \(_{t}\)'s relative quality (i.e., test accuracy).

However, we do not want to take the intermediate solution \(_{t}\) that had the most passive updates as this, too, can be noise (and luck) sensitive. Ideally, we would like to sample from the merging distribution of \(_{t}\) as they occur, and take an average of those solutions to obtain a singular, highly robust, solution vector that performs well with little variance. But, we do not wish to store all \(_{t}\) due to intractability.

Putting these thoughts together, our WRS-Augmented Training (WAT) method functions as follows. Given a base passive-aggressive algorithm (e.g., PAC or FSOL), we will run said algorithm through our data stream \(\{(_{t},y_{t})\}_{t=1}^{}\), as normal, but keep a size-\(K\) reservoir of promising candidate solutions. The reservoir approach allows us to run through our data stream and collect a weighted random sample of candidate solutions of size-\(K\), _weighted by their subsequent number of passive steps_ and without storing all intermediate solutions.

Naturally, this setup is suited for Efraimidis and Spirakis's WRS algorithm. Procedurally, every timestep that our algorithm goes aggressive, we obtain a new active candidate solution. Right before we apply our aggressive mode update rule, we will add the outgoing candidate solution to our size-\(K\) reservoir (and remove a current resident of the reservoir, if necessary) following the steps of the WRS algorithm. At any timestep \(t\), we can form an ensemble solution \(_{}\) by taking an average of the candidate solutions currently in our reservoir. Hopefully, at any timestep \(t\), \(_{}\) will have more stable test performance than the current active candidate solution \(_{t}\).

### WAT variants

We will experiment with a few additional variants of the WAT method. First, instead of weighting using the subsequent number of passive steps (which we denote as _standard weights_), what if we weight using the exponentiated subsequent number of passive steps (which we denote as _exponential weights_)? The idea with exponential weights is that it is closer in practice to deterministically picking the candidate solutions with the largest number of subsequent passive steps, while still maintaining some stochasticity -- i.e., a "greedier" policy. Second, when constructing our ensemble solution \(_{}\), should we take a _simple average_ of the residents in our reservoir or a _weighted average_? Third, there are reasonable concerns that constructing \(_{}\) via averaging might negate the sparsity advantages of a method like FSOL, due to different candidates in the reservoir containing \(0\)s in different entries. However, if the majority of candidate solutions in the reservoir contain \(0\)s at a given entry, what if we tried zeroing out said entry in \(_{}\), as it is likely to be uninformative? We denote this add-on as _voting-based zeroing_.

```
1:\(\) - initial solution vector, \(\{(_{t},y_{t})\}_{t=1}^{T}\) - data stream, \(K\) - reservoir size, WS - weighting scheme ("Standard" or "Exponential"), AS - averaging scheme ("Simple Average" or "Weighted Average"), VZ - voting-based zeroing (True or False), and other base-method-specific hyperparameters \(\) (e.g., for PAC or FSOL)
2:WRS-stabilized solution vector \(_{}\).
3:functionWAT(\(\), \(\{(_{t},y_{t})\}_{t=1}^{T};K,,,,\))
4:# Initializing intermediate data structures
5:\(s 0\)\(\) Counter for subsequent passive steps of current solution candidate
6:\([]\)\(\) Size-\(K\) reservoir for storing promising solutions, as array
7:\(,[],[]\)\(\) Size-\(K\) arrays for weights \(b_{r}\) and auxiliary \(k_{r}\) values for solutions in \(\)
8:# At each timestep, we observe \((_{t},y_{t})\)
9:for\(t 1\)to\(\)do
10:if\((;_{t},y_{t})>0\)then\(\) If made error, in aggressive mode
11:# Terminate current solution, probabilistically add to reservoir using WRS 
12: Draw \(u^{*}(0,1)\)
13:ifWS == "Standard" then
14:\(b^{*} s\)
15:elseifWS == "Exponential" then
16:\(b^{*}(s)\)
17:\(k^{*}(u^{*})^{}}\)
18:\(_{j 1, K}[j]\); \(i_{j 1,,K}[j]\)
19:if\(k^{*}>\) or \(\) is not full with \(K\) solutions then
20:\([i],[i] b^{*},[i]  k^{*}\)
21:# Base method update rule
22:\(+g()\)\(\)\(g()\) specific to base algorithm (e.g., PAC or FSOL)
23:\(s 0\)\(\) Reset number of subsequent passive steps
24:else\(\) correctly-classified, still in passive mode
25:\(s s+1\)\(\) Increment number of subsequent passive steps
26:\(\)
27:ifAS == "Simple Average" then
28:\(_{}_{j=1}^{K}[j]\)\(\) Simple average of solutions in reservoir
29:elseifAS == "Weighted Average" then
30:\(_{}_{j=1}^{K}[j][j]/( _{j=1}^{K}[j])\)\(\) Weighted avg. of solutions in reservoir
31:\(\)
32:ifVZ is True then
33:Zero out entries in \(_{}\) where the majority of solutions \([j]\) contain zeroes.
34:Return\(_{}\).
```

**Algorithm 1** WRS-Augmented Training (WAT)

We present the WAT method in full detail in Algorithm 1. To clarify, the elements in the data structures \(\), \(\), and \(\) (all of which are size-\(K\) arrays) are paired with each other, so that when we add/remove an element in \(\), the corresponding elements in \(\) and \(\) are removed as well. To be fully clear, \(\) contains candidate solution vectors \(,,[K]\), with each \([k]^{D}\). The vector \(\) contains scalar values \(,,[K]\), and likewise for the vector \(\).

### Theoretical Analysis

One of the goals of our method is to choose a set of effective models as the algorithm runs live, without the need for expensive evaluation on a validation set. In this section we first provide _validityarguments for using observed survival as a proxy to select high-accuracy models. These are finite-sample bounds based on reasoning about the reservoir \(\). Note that we are interested in studying generalization to unseen data, a complementary setting to prior work which shows convergence in terms of training set error for the perceptron model . After establishing validity, we will turn to learning bounds for our ensemble model risk in the i.i.d. setting. Proofs are deferred to Appendix A.

**Validity analysis.** We assume a given dataset \(z_{1}^{T}=\{z_{t}\}_{t=1}^{T}\) is an i.i.d. sequence sampled from a generating distribution \(\). We first suppose that \((;z)\) is the 0-1 loss, that is \((;(x,y))=( y)\). Then the risk \(R_{}()\) is the probability that a random \(z\) is misclassified by \(\). At any time \(t\), we have an online model \(_{t}\), for which we define the _survival time_\(s_{_{t}}\) to be the number of subsequent correct classifications, stopping when \(_{t}\) misclassifies a sample. As \(_{t}\) does not change until an error occurs, at any finite time we only collect \(K\) updated models \(\{^{(j)}\}_{j=1}^{K}\) into \(\).

Our first result bounds the difference in risk among two models in \(\): (1) the minimal-risk hypothesis \(}= R_{}(^{(j)})\) which we do not know, and (2) the longest-surviving hypothesis \(_{s}= s_{^{(j)}}\) which we observe. We also denote \(s_{}}\) and \(s_{_{s}}\) to be their respective survival times.

**Theorem 1**.: _Let \(^{(1)},,^{(k)}\) be the updated outputs of an online PA algorithm on inputs \(Z_{1}^{T}\). Also define \(r_{m}\) as the minimal achievable risk of any model, such that \(r_{m} R_{}(})\) almost surely. Then_

\[P(R_{}(_{s})>R_{}(})+ )K}{2r_{m}+-r_{m}(r_{m}+ )},e^{-r_{m}^{K}(r_{m}+)}}\]

This immediately leads to

**Corollary 1**.: _With prob. \(1-\), \(R_{}(_{s}) R_{}(})+ ^{K+1}}{r_{m}^{K}},(K-(2-r_{m}) )}{(1-r_{m})}}\)._

While this bound explains the use of top-1 survival in the worst case, we further justify the use of an ensemble (or reservoir) of top-surviving models. Specifically, given the use of top-\(B\) models, there is always a certain probability that the top-\((B+1)\) can lower the risk. Since \(B=1\) is a base case, this implies that any value of \(B\) is worthy of consideration, until this probability decays to 0.

To simplify the notation, we sort the models in \(\) by decreasing survival time \(s_{1}^{},,s_{k}^{}\), with corresponding re-indexed weights \(w_{1}^{},,w_{k}^{}\) and risks \(R_{1}^{},,R_{k}^{}\). We also add an assumption, that there is a partition of \(=_{g}_{b}\) such that \(_{g}=\{:r_{m} R_{}() r_{m}+\}\) and \(_{b}=\{:r_{m}+<R_{}( ) r_{m}+3\}\), and that \(|_{b}| B\). That is, there is a set of _good_ and _bad_ models in terms of risk. In the Appendix we show that the assumption is readily satisfied.

**Theorem 2**.: _Let \(_{}(_{B})\) be the averaged risk of the top-\(B\) surviving models, and let \(_{}(_{B+1})\) be the averaged risk including the next highest survival model. Then \(_{}(_{B+1})_{}(_{ B})\) with probability at least \(|_{g}|{{}_{b})}{B}}r_{m}^{2}(r_{m}+3 )^{|_{b}|-B}(1-r_{m}-3)^{B}}\)._

Finally note that we can define an averaged model (weighted or unweighted) \(}_{B}\). For any convex \(_{h}(;z)\) with risk \(R_{}^{h}\), such as the hinge loss, Jensen's inequality gives \(_{}^{h}(}_{B+1})_{}( _{B+1})\).

**Learning bounds.** Now we turn to generalization bounds of our method, when run to a fixed stopping time \(T\). We assume more generally that \((;z)\) is convex, such as the hinge loss. Furthermore we suppose that the loss of any point in the training set is bounded by \(C\). This is a safe assumption for many passive-aggressive algorithms, where the input data is normalized and the update steps are not too large.

**Theorem 3**.: _Suppose \(\) is convex and bounded from above by \(C\). By time \(T\), suppose the reservoir contains \(K_{T}\) models, with survival time of each at least \(s_{T}\). Let \(M_{}}}}}}}}}}}}}\) be the cumulative loss of the WAT reservoir sequence, with \(_{t}\) formally defined in the proof. Then w.p. \(1-\),_

\[R_{}(_{}}}}}}}}}}}}}}}}}}}})} }}}}}}}}}}}}}}}}}}{K_{T}s_{T}}+ )M_{}}}}}}}}}}}}}}}}} )}}}}{(K_{T}s_{T})^{2}}}+ )}{K_{T}s_{T}}\]

The result shows that the risk of the ensemble model is stochastically bounded by the cumulative loss of the online procedure. By applying known regret bounds for the underlying online algorithms (e.g. PAC or FSOL), we can further bound the risk in terms of the original learner, and subsequently the optimal risk. In particular, we can "abstract out" the actual online learning method in the proof, as a generic regret bound of form \(}{T}_{t=1}^{T}(;z_{t})+\), for any \(\). Depending on the algorithm often \(r(T)=O()\) or even \(O(1)\).

With this in hand, we can show that the risk actually approaches the minimal risk. One additional assumption is required: \(M_{wrs}/_{t=1}^{T}_{t}_{t}(_{t-1};z_{t })=}{T}\) (the WRS sequence has lower cumulative loss than the original learner). By definition, the reservoir contains the models which have longest survivals, and hence lowest regret density. So this statement is readily satisfied, as long as the distributions of non-zero losses are not different among the two sequences. In fact, under certain conditions, the inequality is likely strict which further improves the bound. Altogether we obtain:

**Theorem 4**.: _Given a PA algorithm, let \(M_{T}\) be its cumulative loss, and \(r(T)\) be the algorithm-specific excess regret. Then with probability \(1-\), the deviation in risk of our WRS model \(_{wrs}\) from the optimal model \(^{*}\) is bounded as_

\[R_{}(_{wrs}) R_{}(^{*})++)M_{T}}{TK_{T}s_{T}}}+ )}{K_{T}s_{T}}+C)}{2T}}\]

## 4 Numerical experiments

We combine WAT with base PAC and FSOL, forming two new methods PAC-WRS and FSOL-WRS1. We evaluate their performances across 16 binary classification datasets listed in Table 1. Please see Appendix B for more dataset details. We are interested in three metrics: **1) Final test accuracy:** proportion of test set data points correctly-classified by solution obtained after making one pass through the training data. **2) Final sparsity:** proportion of zeroes in our classification solution obtained after making one pass through the training data. **3) Relative oracle performance (ROP):** let \(p_{t,}\) be the test accuracy of our base model (either PAC or FSOL) at time \(t\) and \(p_{t}\) be the test accuracy of our model of interest (either base PAC, base FSOL, PAC-WRS, or FSOL-WRS). Then, define \(p_{t,}=_{}p_{t,}\) to be the cumulative maximum test accuracy of the base method at time \(t\). In other words, \(p_{t,}\) represents the highest performance we could obtain if we had an oracle telling us which candidate solution vector we encountered was best. Then, define \(=_{t=1}^{t}(p_{t,}-p_{t})\). Intuitively, if a method has very stable test accuracy over time with minimal fluctuations, then ROP should be close to \(0\), or negative (i.e. achieving a higher test accuracy than the oracle). In contrast, a large, positive ROP suggests fluctuations in test accuracy.

Because we are interested in the stability of WAT's test-set accuracy over the entirety of the training run, it is most illustrative to look at figures when possible. Representative figures will be shown in the

  
**Dataset** & **D** & **N** & \(}\) & \(}\) & **Sparsity** \\  Avazu (App)  (via LIBSVM) & 1000000 & 14596137 & 10217295 & 4378842 & 0.999985 \\ Avazu (Site)  (via LIBSVM) & 1000000 & 25832830 & 18082981 & 7749849 & 0.999985 \\ Critco (via LIBSVM) & 1000000 & 51882752 & 36317926 & 15564826 & 0.999961 \\ Dexter  & 20000 & 600 & 420 & 180 & 0.995319 \\ Dorothea  & 100000 & 1150 & 805 & 345 & 0.990009 \\ KDD2010 (Algebra)  (via LIBSVM) & 20216830 & 8918054 & 6242637 & 2675417 & 0.999998 \\ MNIST8 (4+9)  (via LIBSVM) & 784 & 1591785 & 1114249 & 477536 & 0.757170 \\ News20  (via LIBSVM) & 1355191 & 19954 & 13967 & 5987 & 0.999664 \\ Newsgroups (Binary, CS)  (via sklearn) & 101631 & 18311 & 12817 & 5494 & 0.999049 \\ PCMAC  & 3289 & 1943 & 1360 & 583 & 0.985418 \\ RCV1  (via LIBSVM) & 47236 & 697641 & 488348 & 209293 & 0.998451 \\ Real-Sim (McCallum via LIBSVM) & 20958 & 72201 & 50540 & 21661 & 0.997549 \\ SST-2  & 13757 & 67337 & 47135 & 20202 & 0.999421 \\ URI  (via LIBSVM) & 3231961 & 2396130 & 1677291 & 718839 & 0.999964 \\ W8A  (via LIBSVM) & 300 & 59245 & 41471 & 17774 & 0.957585 \\ Webspam  (via LIBSVM) & 254 & 350000 & 244999 & 105001 & 0.664833 \\   

Table 1: Sizes, dimensions, and sparsities of all datasets used for numerical experiments.

main paper, with all figures for all datasets in the appendix. When running any algorithm (PAC, FSOL, PAC-WRS, or FSOL-WRS) on any dataset, we perform a random 70/30 train-test split. We begin by tuning the \(C_{err}\), \(\), and \(\) hyperparameters for base PAC and FSOL, with details located in Appendix B. For PAC-WRS and FSOL-WRS, we use the hyperparameters for the corresponding base models, but try all possible WAT variants of weighting scheme (standard or exponential), averaging scheme (simple vs. weighted), voting-based zeroing (True or False), and reservoir size \(K\{1,4,16,64\}\). We perform five trials for each PAC-WRS and FSOL-WRS variant with randomized shuffles of the training and test data, running through the training data only once for each trial. All candidate solutions are initialized as the \(\) vector. Please see more experimental details in Appendix B.

### Stabilizing test performance

Figure 1 shows visually how PAC-WRS and FSOL-WRS are highly-effective at stabilizing base PAC and FSOL's wildly-fluctuating test accuracy on Avazu (App) and News20. Corresponding figures for all 16 datasets and PAC-WRS/FSOL-WRS variants can be found in Appendix G.2. From Figure 2, we see that PAC-WRS is highly-effective at reducing ROP compared to base PAC, and that **the larger the reservoir size \(K\)**, the more stable the resultant test accuracy for wws becomes.** Furthermore, looking more carefully at MNIST8 (4+9), we observe that many PAC-WRS variants were able to achieve _negative_ ROP values, suggesting that **w**wrs could achieve _higher test accuracies_ than even the oracle. Corresponding figures for all 16 datasets can be found in Appendix G.1.

From Table 2, we observe that FSOL-WRS with standard weights and simple averaging **successfully stabilized test accuracy in all 16 tested datasets compared to base FSOL**, as measured via ROP.

PAC-WRS with exponential weights and simple averaging successfully stabilized test accuracy in 14 of 16 tested datasets compared to base PAC. Following the best practices in [38; 39], we perform Wilcoxon signed-rank tests for statistical significance on the differences in ROP between PAC/FSOL-WRS versus base PAC and FSOL, taking into account performance on all 16 datasets. At a significance level of \(=0.05\), we find that both PAC/FSOL-WRS achieve statistically-significant reductions in ROP compared to their corresponding base models when equipped with standard weights (\(p<0.0386\) in each case, see Table 6 in Appendix F).

These results also suggest that standard weights are preferable to exponential weights. This makes sense because using exponential weights may be too "greedy," causing the algorithm to overly trust in the number of passive steps as an indicator of test performance, polluting the reservoir with poor "lucky" candidate solutions, and refusing to remove them later.

  &  &  \\   & Standard & Exp. & Standard & Exponential \\ 
**PAC** & 13 & 14 & 13 & 11 \\
**FSOL** & 16 & 13 & 16 & 11 \\  

Table 2: Numbers of datasets out of 16 where each PAC-WRS or FSOL-WRS variant with \(K=64\) outperformed its corresponding base method (PAC or FSOL), as measured by ROP averaged across 5 randomized trials.

Figure 2: Relative oracle performances (\(y\)-axis) of base PAC and PAC-WRS using standard weights over reservoir sizes \(K\) (\(x\)-axis) on 3 representative datasets. Error bars represent the minimum and maximum values achieved across 5 randomized trials. **Blue:** WRS-augmented variants via simple average of reservoir members. **Red:** WRS-augmented variants via weighted average of reservoir members. **Dotted lines:** indicates voting-based zeroing was performed for additional sparsity. **Lower values indicate more stable performance.**

**A note on final test accuracy.** From Figure 3, we see that the final test accuracies achieved by FSOL-WRS are not only higher on average than base FSOL, but also have significantly lower variance. On these datasets, there is no significant difference between FSOL-WRS variants with or without voting-based zeroing. As expected, final test accuracy seems to be higher for larger values of reservoir size \(K\). However, Wilcoxon signed-rank tests to compare PAC-WRS and FSOL-WRS's improvements in final test accuracy over base PAC and FSOL indicate that FSOL-WRS with standard weights yields a statistically-significant improvement in final test accuracy compared to base FSOL, but PAC-WRS does not compared to base PAC (see Table 7 in Appendix F). One hypothesis for this discrepancy is that it is relatively unlikely for any given training data point to cause a massive drop in test accuracy. Furthermore, subsequent data points will usually help the base model self-correct (see Appendix G.2). Thus, this explains why the base method's mean final test accuracy after a particular fixed timestep (e.g., \(N_{}\)) will not differ significantly from that of the WRS-augmented method, especially not across only 5 trials.

Nonetheless, there are enough data points in the training stream that these massive fluctuations in test accuracy could still happen thousands of times throughout the training process, corroborating what we saw in Figure 1. If such a drop in test accuracy were to occur at an unlucky timestep when the model training is stopped, the consequences could be unacceptable. Furthermore, in continuously updated "any-time" environments, it is hard to assess if one of these drops has occurred. As such, WAT is valuable as a simple and effective way of preventing such fluctuations in test accuracy.

**Preservation of sparsity.** Finally, we aim to retain the favorable weight sparsity from FSOL. From Figure 4, we observe that even with reservoir size \(K=64\), the final sparsities achieved by FSOL-WRS are quite similar to that of base FSOL. Furthermore, we observe that with voting-based zeroing, FSOL-WRS often achieves even higher sparsity than the base model. Please see Appendix G.1 for additional

Figure 4: Final sparsities (\(y\)-axis) of base FSOL and FSOL-WRS using standard weights over reservoir sizes \(K\) (\(x\)-axis) on 3 representative datasets. Error bars represent the minimum and maximum values achieved across 5 randomized trials. See Figure 2 for legend description.

   & Simple Average & Weighted Average \\ 
**PAC** & 10 & 12 \\
**FSOL** & 15 & 15 \\  

Table 3: Numbers of datasets out of 16 where each top-\(64\) PAC or FSOL variant outperformed its corresponding base method (PAC or FSOL), as measured by relative oracle performance averaged across 5 trials.

Figure 3: Final test accuracies (\(y\)-axis) of base FSOL and FSOL-WRS using standard weights over reservoir sizes \(K\) (\(x\)-axis) on 3 representative datasets. Error bars represent the minimum and maximum values achieved across 5 randomized trials. See Figure 2 for legend description.

errorbar-type plots and Appendix G.2 for sparsity over timestep. One explanation for why even FSOL-WRS variants _without_ voting-based zeroing can achieve similar sparsity to base FSOL is that the final reservoir will likely contain candidate solutions from significantly earlier timesteps, when weights were sparser due to fewer aggressive updates. Thus, WAT can maintain sparsity despite the use of weight averaging.

### Ablations and extensions

**Top-\(K\) ablation.** A natural question that one might ask is -- instead of probabilistically sampling candidate solutions, what if we deterministically picked the top-\(K\) candidate solutions with the largest subsequent numbers of passive steps and took their simple and/or weighted average? Setting \(K=64\), we see from Table 3 that the best top-\(64\) PAC/FSOL variants are not as effective as stabilizing test accuracy as the best PAC/FSOL-WRS variants, as shown in Table 2 and measured via ROP. Furthermore, Wilcoxon signed-rank test \(p\)-values from Table 4 also suggest that this top-\(64\) ablation cannot produce as statistically-significant increases in test accuracy stability compared to WAT, as measured via ROP. Like WAT, top-\(K\) is also ineffective at producing statistically-significant increases in final test accuracy on PAC. On FSOL, top-\(K\) can produce statistically significant increases in final test accuracy, but these \(p\)-values are over an order of magnitude larger than FSOL-WRS's (see Table 7 in Appendix F). As such, WAT is still the best for stabilizing test accuracy.

**Additional ablations and comparisons.** For the interested reader, in Appendix D we include empirical performance comparisons of WRS-Augmented Training against two traditional ensembling mechanisms: moving average (e.g., averaging the most-recently-observed \(K=64\) weight vectors at each timestep) and exponential average (e.g., forming an ensemble vector \(}_{t}=_{t}+(1-)}_{t-1}\) at each timestep), where \(_{t}\) is the base algorithm's candidate solution at timestep \(t\). In short, especially in more real-world, large-scale settings where evaluation and checking are prohibitively expensive, WRS-Augmented Training is the fastest, most accurate, and most reliable method compared to all the aforementioned baselines.

**Modifying WAT for non-passive aggressive methods.** While the main theoretical and empirical results in this paper were primarily oriented towards passive-aggressive base models, in Appendix E, we include empirical simulations of applying a modified form of WRS-Augmented Training on top of three non-passive-aggressive online learning methods: Stochastic Gradient Descent with Momentum , ADAGRAD , and Truncated Gradient Descent . In general, our modified WRS-Augmented Training effectively mitigates test accuracy when it exists, and does minimal harm when it does not.

## 5 Conclusion, limitations, and future work

In this paper, we introduced WRS-Augmented Training (WAT), a procedure that can be used to stabilize any passive-aggressive online learning algorithm, neither requiring a hold-out evaluation set nor additional passes over the training data. We applied WAT to base PAC and FSOL, demonstrating across 16 datasets that WAT is highly effective at mitigating the massive fluctuations in test accuracy between timesteps that affect many online learning algorithms. WAT runs at minimal cost, with only a fixed \(K\) multiple on memory for multiple weight vectors to be saved.

One limitation of this work is that WAT implicitly assumes that the training and test data come from fixed distributions that do not change over time. However, some data distributions will evolve over time. As such, a candidate solution that entered the reservoir early on due to having a large number of subsequent passive steps might not actually retain its performance as the data distribution evolves over time. While they may eventually be replaced, non-IID adaptions may be useful in the future.

  &  &  \\   & Simple Average & Weighted Average & Simple Average & Weighted Average \\ 
**PAC** & 0.162 & 0.0298 & 0.214 & 0.255 \\
**FSOL** & 0.00270 & 0.00270 & 0.0130 & 0.0130 \\  

Table 4: Wilcoxon signed-rank test \(p\)-values testing whether differences in **relative oracle performance** and **final test accuracy** between top-\(64\) PAC/FSOL variants and base PAC/FSOL are statistically significant.