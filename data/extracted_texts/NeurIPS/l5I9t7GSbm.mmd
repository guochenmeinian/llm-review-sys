# CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities. This paper studies the following question: _Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations?_ Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated web-scale image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.

## 1 Introduction

Foundation Models (FMs) are revolutionizing different domains of artificial intelligence and machine learning, including computer vision [31; 14; 17] and natural language processing [7; 2; 41]. FMs can be trained on web crawled data without relying on crowd or expert annotations, and yet they demonstrate strong generalization capabilities [15; 36].

CLIP, one of the most prominent methods for FM training in vision, uses contrastive learning to align image and text representations [31; 15]. In addition to robustness to data distribution shifts, CLIP offers impressive zero-shot and cross-modal retrieval capabilities on unseen datasets. Nevertheless, computer vision encompasses a broad range of tasks that require the ability to comprehend spatial relationships, semantic content, object localization, and 3D structures. In spite of CLIP's impressive zero-shot open-vocabulary classification accuracy, it exhibits poor localization capabilities and often struggles in associating text with objects in an image [40; 12; 32]. Consequently, in practice, many vision tasks (e.g., detection and segmentation), rely on CLIP through fine-tuning the entire model to compensate for these localization deficiencies.

In this work, we seek to answer the following question: _Can we augment pretrained CLIP models with task-specific vision models from model zoos to improve its visual representations?_ That is, we seek to (1) use open-source task-specific vision models to generate _hard_ pseudo-labels on a web-scale noisy image-text dataset and, (2) train CLIP on image-text pairs along with pseudo-labels with multiple objectives. An overview of our approach, which we call **CLIP** Training with **eX**perts (CLIPTeX ), is shown in Fig. 1. We show that CLIPTeX enhances the visual representations of CLIP and yields up to 16.3% enhancement in probing accuracy across a diverse set of vision tasks and datasets while preserving the existing capabilities of CLIP models, including prompting for zero-shot classification.

## 2 CLIPTeX

ModelCLIPTeX extends CLIP with pseudo-supervision from publicly available task experts specializing in localization, depth, and surface normal estimation. Our approach enhances CLIP's representations _without any labeled data collection_ (Fig. 1). Similar to CLIP, CLIPTeX uses two encoders: (1) an image encoder that takes an RGB image and produces an image embedding and (2) a text encoder that takes the text caption and produces a text embedding.

In addition to contrastive training, we would like to train CLIPTeX using pseudo-labels. Towards that end, we incorporate task-specific heads that take the output of image encoder as input and generate predictions for the respective task (see Fig. 1b). Previous work have shown that multi-scale representations provides significant benefit in tasks requiring localization and fine-grained visual understanding . However, some image encoders (e.g., ViT) do not inherently possess these capabilities. To ensure CLIPTeX can learn better visual representations independent of the image backbone, we include a single shared multi-scale module  between image encoder and task-specific heads. We feed the output of the image encoder through a multi-scale module , which in turn feeds into the lightweight task-specific classification or regression heads. In our implementation, we use independent point-wise convolution as the head for each task. As the task's head output dimensions should match input dimensions in dense prediction tasks, we perform nearest neighbour interpolation on head's output if necessary.

Training objectiveTo train CLIPTeX with pseudo-supervision on \(n\) tasks, we generate _hard_ pseudo-labels offline using publicly available task-specific experts on an uncurated web-scale dataset. We then train CLIPTeX with a weighted sum of contrastive loss and task-specific losses: \(=_{}_{}+_{t=1}^{n }_{}^{t}_{}^{t}\) where \(_{}^{t}\) is the loss of the \(t\)-th task and \(_{}\) is the contrastive loss. Here, \(_{}^{t}\) and \(_{}\) are the loss coefficients of \(t\)-th task and the standard CLIP loss, respectively.

## 3 Experimental Setup

Probing, a standard method to study the representations learnt by neural networks , is used to investigate whether pseudo-supervision in CLIPTeX can improve CLIP's image backbone.

Task-specific expertsWe train CLIPTeX with hard pseudo-labels generated from following experts: (1) _Semantic segmentation._ We use Mask-RCNN  with ViT backbone , trained on the COCO  with RangeAugment , to produce pseudo-labels for segmentation. (2) _Monocular depth estimation:_ We use DPT , trained on MIX-6 dataset , to generate monocular depth map pseudo-labels. (3) _Surface normal estimation:_ We use _NLL-AngMF_ as our surface normal expert, which is trained on ScanNet dataset .

BaselinesWe compare with following baselines: (1) _CLIP_. We use CLIP model  pretrained on 1.2 billion images with a variable resolution and batch sampler whose base input image's spatial

Figure 1: **Training CLIP with pseudo-labels improves its visual representations. (a) shows the standard CLIP training. (b) shows CLIPTeX that trains CLIP with pseudo-labels from experts. Note that the main purpose of task heads is to improve CLIPâ€™s image encoder with expert knowledge, and the heads can be discarded after training. (c) shows the relative improvement that CLIPTeX obtains over CLIP-FT. Here, SSeg, OD, ISeg, SNE, and DE refer to semantic segmentation, object detection, instance segmentation, surface normal estimation, and depth estimation respectively.**resolution is \(224 224\). (2) _CLIP-FT_. Many dense prediction tasks (e.g., segmentation) benefit from using high-resolution input images. To have a fairer baseline trained on the same resolution as CLIPTeX, we finetune CLIP with contrastive loss on CC3M. The training is done with variable resolution using a batch sampler whose base input image resolution is \(512 512\). Any improvements over this baseline signify a pure transfer of knowledge from pseudo-supervision.

To show the generality of CLIPTeX, we experiment with three image encoder backbones: ViT-B/16, ViT-H/16, and ResNet-50. Also note that we finetune CLIPTeX on CC3M's image and text pairs along with pseudo-labels using the same settings as CLIP-FT. We use cross-entropy loss to train on segmentation pseudo-labels, and L1 loss to train on depth and surface normal pseudo-labels.

Evaluation downstream tasks and datasetsWe evaluate the models using classifier and regressor probes on the following tasks: (1) _Semantic segmentation_. We use PASCAL VOC  with 20 classes. We report mean intersection over union (mIoU) on the validation set. (2) _Object detection and instance segmentation_. The models are evaluated on COCO dataset for detection and instance segmentation. Importantly, during training with pseudo-labels, we do not use the bounding boxes. Instead, the instance masks are converted to semantic segmentation pseudo-labels. This allows us to evaluate baselines on both instance segmentation and object detection, which are considered to be more challenging tasks than semantic segmentation. Following standard convention, we evaluate the accuracy on COCO's validation set in terms of mean average precision (mAP). (3) _Monocular depth estimation_. We use NYU-V2  dataset as our depth estimation benchmark. Note that DPT, the expert used for depth pseudo-supervision, is trained on a different dataset, i.e., ScanNet. We use absolute relative error as a metric for evaluation on the validation set. (4) _Surface normal estimation_. We use NYU-V2 for surface normal estimation. We train on the training set used by Bae et al.  and Qi et al. , and evaluate on the official test set of NYU-V2. Following , we use \(a{<}30\) as the metric for evaluation. (5) _Image classification_. We evaluate on ImageNet 1K  classification dataset and top-1 accuracy on the validation set is reported as the evaluation metric.

Classifier and regressor probes for evaluationTo study the visual representations of different _frozen_ pre-trained models, our experiments involve both classification and regression tasks across different datasets. For dense prediction tasks, such as semantic segmentation, depth, and surface normal estimation we probe frozen image encoders with two types of probes: (1) Linear which is a point-wise convolutional layer. (2) PSPNet , a standard non-linear head for dense prediction tasks. For image classification a fully-connected layer is used as the linear probe. For object detection and instance segmentation, Mask R-CNN  and SSD heads are used. Additional probing results with different heads (e.g. DeepLabV3) and tasks (e.g. ADE20k) are included in Appendix C.

## 4 Results

Pseudo-supervision improves visual representationsProbing results for all tasks are given in Table 1. In semantic segmentation, CLIPTeX shows consistent improvements over the baselines. Particularly noteworthy is the linear probing accuracy of CLIPTeX with ViT-B/16 and ViT-H/16 backbones on the PASCAL VOC dataset, which is about 10% and 16.3% better than CLIP-FT.

    &  &  &  &  &  \\   & **Linear** & **PSPNet** & **Mask-RCNN** & **SSD** & **Linear** & **PSPNet** & **Linear** & **PSPNet** & **Linear** \\  ViT-B/16 & & & & & & & & & \\ CLIP & 18.66 & 45.53 & 15.20 & 5.33 & 0.235 & 0.168 & 28.49 & 47.29 & **80.24** \\ CLIP-FT & 62.47 & 78.22 & 27.21 & 16.46 & 0.215 & 0.139 & 29.06 & 47.91 & 79.94 \\ CLIPTeX (**Ours**) & **73.43** & **80.71** & **28.89** & **17.50** & **0.159** & **0.128** & **39.96** & **50.80** & 79.64 \\  ViT-B/16 & & & & & & & & & \\ CLIP & 56.18 & 75.37 & 26.65 & 11.07 & 0.212 & 0.132 & 29.09 & 49.78 & **84.85** \\ CLIP-FT & 62.95 & 82.94 & 33.93 & 20.24 & 0.213 & 0.125 & 29.21 & 50.48 & 84.1 \\ CLIPTeX(**Ours**) & **79.30** & **84.31** & **34.50** & **21.55** & **0.138** & **0.117** & **43.22** & **53.89** & 83.2 \\  ResNet-50 & & & & & & & & & \\ CLIP & **46.96** & 70.92 & 29.49 & 20.32 & **0.212** & **0.147** & **33.67** & 47.28 & 78.35 \\ CLIP-FT & 34.78 & 74.17 & 38.13 & **30.28** & 0.239 & 0.155 & 28.72 & 48.66 & 78.92 \\ CLIPTeX(**Ours**) & 40.31 & **75.58** & **38.23** & 28.62 & 0.220 & 0.150 & 31.56 & **49.44** & **78.95** \\   

Table 1: **Probing results for different vision tasks.** Pseudo-labeling in CLIPTeX significantly improves the visual representations in the image encoder of CLIP.

For object detection with ViT-B/16 as the frozen backbone and Mask-RCNN as the probing head, CLIPTeX delivers 13.69% and 1.68% better bounding box mAP over CLIP and CLIP-FT respectively. We observe similar gains when CLIPTeX is probed with SSD.

For depth estimation, CLIPTeX obtains lower error rate, while for surface normal estimation, CLIPTeX obtains higher value of \(a{<}30\) compared to CLIP and CLIP-FT baselines. These results indicate a positive transfer of distance and surface orientation knowledge to CLIPTeX's image backbone, contributing to the better performance.

Unlike other dense prediction tasks, CLIP achieves similar or slightly better accuracy compared to CLIP-FT and CLIPTeX. This outcome can be attributed to the characteristics of image classification tasks as it primarily focuses on recognizing objects without requiring detailed information about spatial relationships or 3D structure of the scene.

Zero-shot capabilities are preserved in CLIPTeXOne of the important and powerful characteristics of CLIP is _prompting_, which enables zero-shot transfer to new datasets. Pseudo-supervision with experts can potentially lead to catastrophic forgetting of previously learned knowledge, which may in turn affect model's zero-shot generalization capabilities. Table 2 compares the zero-shot capabilities of CLIP-FT and CLIPTeX in classification on ImageNet-1k  and retrieval on Flickr-30k  tasks respectively. CLIPTeX's zero-shot performance is on par with that of CLIP-FT, indicating that enhanced representations do not result in catastrophic forgetting.

Ablation on the importance of pseudo-labels in CLIPTeX.Incorporating pseudo-supervision from task-specific experts, even from a single expert during training, results in substantial improvements in performance. These improvements are observed when evaluating models on various downstream tasks with different probes (see R1 vs. rest; Table 3). Overall, our findings indicate that incorporating knowledge from all experts contributes to learning better visual representations. Therefore, we use all experts for pseudo-supervision while training CLIPTeX.

## 5 Conclusion

As the field of machine learning research embraces openness, a growing number of specialized expert models become publicly available. Our study showcased the potential of leveraging these publicly available expert models to enhance CLIP's visual representations, all without the necessity of collecting task-specific data. Our experiments revealed that CLIPTeX yields improvements across a wide range of tasks, highlighting its versatility and effectiveness.

Table 2: **CLIPâ€™s zero-shot knowledge is preserved after training with experts. (a) report zero-shot top-1 accuracy for ImageNet-1k dataset and (b) reports recall@1/5/10 for Flickr-30k dataset.**

Table 3: **Role of pseudo-labels from each experts in CLIPTeX training.**