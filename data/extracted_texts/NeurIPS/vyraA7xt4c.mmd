# Mercury: A Code Efficiency Benchmark for Code Large Language Models

Mingzhe Du\({}^{1,2}\), Luu Anh Tuan\({}^{1}\), Bin Ji\({}^{2}\), Qian Liu\({}^{3}\), See-Kiong Ng\({}^{2}\)

\({}^{1}\)Nanyang Technological University

\({}^{2}\)National University of Singapore

\({}^{3}\)Sea AI Lab

{mingzhe001, anhtuan.luu}@ntu.edu.sg, {jibin, seekiong}@nus.edu.sg, liuqian@sea.com

###### Abstract

Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present _Mercury_, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On _Mercury_, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. 1

## 1 Introduction

The domain of code generation, which aims to empower computers to autonomously generate code based on natural language task descriptions (NL2Code), has long been considered a promising way to facilitate interaction between humans and computers . The recent emergence of Large Language Models (LLMs) has spurred a new wave of NL2Code models , which leverage the impressive language understanding and generative capabilities of LLMs to drive forward the ambitious goal of synthesizing high-quality code from natural language instructions.

To measure the quality of code, recent code generation benchmarks mainly focus on evaluating their functional correctness via test case fuzzing . This approach assesses the outcome congruence between the LLM-generated and canonical solutions by executing bespoken test cases. For instance, HumanEval  and MBPP  collected a small but fine set of handcrafted tasks with test cases. EvalPlus  further consolidates these two above benchmarks by augmenting the case scope. On the contrary, APPS  widely gathered over 5,000 public coding tasks from online platforms. Despite these strides, there is a discernible oversight in current code generation benchmarks concerning the code efficiency evaluation, although that is critical in software development . Moreover, handcrafting diverse solutions and test cases to cover all scenarios is infeasible . In light of these findings, we highlight vital limitations inherent in the existing code generation benchmarks:1. **Absence of Code Efficiency Evaluation.** Existing code generation benchmarks focus on assessing functional correctness while overlooking the evaluation of code efficiency [9; 3; 18]. As illustrated in Figure 1, despite both code snippets can handle the sorting task functionally, the _right_ efficient solution (_121 ms_) is nearly 50 times faster than the _left_ inefficient solution (_5,714 ms_). This striking runtime differentiation underscores the necessity of incorporating code efficiency assessments within code generation benchmarks, encouraging Code LLMs to produce not only correct but also efficient code.
2. **Insufficient Test Case Coverage.** As shown in Table 1, most code generation benchmarks manually build a small number of test cases or extract the accompanying test cases from existing resources, potentially overlooking edge cases and nuanced code behaviors [9; 3]. For example, Figure 8 displays that _HumanEval #55_ contains only 3 test cases, testing up to the 12th Fibonacci number . Its given canonical solution will quickly reach the recursion depth limitation when computing a larger Fibonacci number (the recursion limitation depends on the environment). Therefore, notwithstanding the generated code satisfies all test cases, such success does not necessarily equate to assurance of functional correctness and much less to code efficiency.
3. **Lack of Task Diversity.** Another noticeable deficit of existing code generation benchmarks is the insufficient diversity and complexity in their tasks [9; 3; 31]. Since most benchmarks only consist of elementary-level programming tasks, recent Code LLMs can effortlessly tackle most tasks regardless of their actual capacities . This flaw results in these benchmarks failing to pose a substantial challenge to Code LLMs and truly reflect their underlying potential.

**Code Efficiency.**_Code efficiency refers to the performance measure of time and space complexity to accomplish a specific task._ Efficient code can improve user experience, save energy, and make applications more sustainable and cost-effective. Compared with the scalable memory space, execution time is the performance bottleneck of most codes. Consequently, this work focuses on the time dimension of code efficiency.

**Our Benchmark.** In this work, we introduce _Mercury_, a novel code generation benchmark designed to assess and improve the code efficiency of Code LLMs. As depicted in Figure 2, _Mercury_ comprises 1,889 Python programming tasks with three difficulty stratification, which is divided into two datasets for model evaluation and fine-tuning separately. For each evaluation task, we assign a test case generator to remedy the shortfall of test case coverage. In measuring code efficiency, the primary challenge stems from normalizing the absolute runtime across tasks that have diverse runtime ranges. Thus, we collect and locally execute numerous historical solutions for each task to form a runtime distribution and leverage the runtime percentile of LLM-generated code on the distribution instead of the absolute runtime to evaluate code efficiency. Furthermore, to mitigate performance discrepancies attributed to irrelevant processes and diverse hardware configurations, we set up an isolated sandbox environment for task execution to establish local runtime distributions.

Figure 1: Executing these two LLM-generated codes on 100 test cases. While both codes successfully follow the task instruction and pass all test cases, the _right_ snippet notably excels in code efficiency, completing in a mere 121 ms compared to the 5,714 ms consumed by the _left_ snippet. As Code LLMs become widely used in the real world, code efficiency determines factual productivity, where Mercury can gauge the vital metric.

**Contribution.** Our work aimed to fill the code efficiency evaluation gap in code generation benchmarks with the following key contributions:

* **Dataset.** We collect a novel code generation dataset _Mercury_ designed to assess and improve Code LLM code efficiency in Section 2, accompanied by an extensible open-source data collection framework for enriching _Mercury_ with more tasks and programming languages.
* **Metric.** We propose the first efficiency-focused code generation metric Beyond and establish a benchmark to evaluate leading Code LLMs using this metric in Section 3.
* **Baselines.** In Section 4, we detail our extensive analysis of two baselines to enhance code efficiency while maintaining functional correctness. Experiment results reveal that despite Code LLMs excelling in functional correctness, there is still considerable potential to elevate efficiency.

## 2 Mercury Datasets

We initiate this work by collecting public programming tasks on Leetcode . Subjecting these questions to a series of filters, we distilled them down to 1,889 high-quality tasks. A difficulty-balanced subset of 256 tasks was randomly selected to form the **Mercury-eval** benchmark, which obtains an average of 18.4 solutions for each problem. The remaining tasks have been designated as the **Mercury-train** dataset for baseline training (detailed data distribution is listed in Appendix Table 6). To enhance clarity within this paper, we employ _Mercury_ to denote **Mercury-eval** unless otherwise specified.

  
**Benchmarks** & **Tasks** & **Sources** & **Cases** & **Solutions** & **Difficulty** & **Efficiency** \\  HumanEval & 164 & Crowd Source & 8.08 & 1 & 1 & ✗ \\ MBPP & 257 & Crowd Source & 3.01 & 1 & 1 & ✗ \\ APPS & 5,000 & Online & 21.2 & 23.4 & 3 & ✗ \\
**Mercury** & 256 & Online + Filters & +\(\) & 18.4 * & 3 & ✓ \\   

Table 1: A comparison of _Mercury_ to existing NL2Code benchmarks. _Mercury_ distinguishes itself by including a set of distilled high-quality solutions and a dedicated test case generator for each task. * signifies that the solution number can be further expanded by the data collection framework.

Figure 2: An overview of _Mercury_ dataset. Each _Mercury_ task has a task description, a test case generator, a prompt & entry point, and corresponding solutions. To evaluate code efficiency, we introduce the Beyond metric, which signifies the runtime percentile of the LLM-generated code on the runtime distribution supported by corresponding solutions. In this example, the LLM-generated code executes in 521 ms, outpacing 86.18% of collected solutions on the runtime distribution. Consequently, the Beyond metric in this case is 86.18%.

**Data Schema.** As illustrated in Figure 2, _Mercury_ offers a unified data schema to streamline the evaluation procedure and bolster further development endeavors. The data scheme encompasses these principal components: (1) **Task Description** contains the task instruction interpreted into a plain text format, along with illustrative examples and constraints of inputs and outputs. (2) **Test Case Generator** refers to a Python code snippet designed to automatically produce a comprehensive set of test cases in accordance with the specifications laid out in the task description. (3) **Solutions** are sampled from Leetcode historical submissions. Each solution within Mercury has undergone rigorous testing, and Locality-Sensitive Hashing  is employed to prevent the inclusion of any identical solutions. (4) **Prompts and Entry Points** where prompts act as the initiating prefixes for LLM code generation and entry points denote the start point for code execution. We delineate the definition of _Mercury_ fields in the Appendix Table 5.

**Task Filters.**_Mercury_ tasks originate from public programming problems on Leetcode. To assure the quality and uniformity of the dataset, we distilled gathered tasks based on the following conditions:

1. **Number of Solutions.** To establish a solution runtime distribution for each task, we filtered out tasks having less than two associated solutions. After excluding these tasks, _Mercury_ tasks possess an average of 18.4 unique solutions.
2. **Restricted Data Structure.** Above the inherent Python data types, _Mercury_ also incorporates two custom data types: Binary Tree and Linked List (the specific structure definitions can be found in Appendix Figure 4), which increases _Mercury's_ diversity and escalates its difficulty level. Tasks that contain other data structures will be removed.
3. **Unique Outputs.** Certain Leetcode tasks may permit non-unique answers. For example, a result list can be returned in any order. Evaluating all possible answers can drastically complicate the test case verification process. To eliminate this problem, we harness the corresponding test case generator to generate \(N\) test cases \(T_{i}=Input_{i},Output_{i})\ s.t.\ i\{0,1,,N\}\) and execute \(T\) on different solutions \(S_{m}\ s.t.\ m\{0,1,,M\}\) to observe if all \(Output_{i}=S_{m}(Input_{i})\ s.t.\ i\{0,1,,N\}\) remain identical. Any tasks that potentially yield non-unique answers were subsequently excluded.

**Task Difficulty.** Most existing NL2Code benchmarks predominantly comprise simplistic tasks, leading to a situation where LLMs of varied capabilities address most tasks effortlessly and yield indistinguishable high scores [52; 22]. To alleviate this issue, _Mercury_ inherits the difficulty categorization from Leetcode, _i.e._, Easy, Medium, and Hard. The stratification aims to probe the upper bounds of Code LLM capabilities, delivering a more evident distinction between various Code LLMs.

**Test Case Generator.** Manual creation of test cases can be a laborious process. To gather sufficient test cases to conduct an exhaustive assessment, we assign a test case generator for each evaluation task, which can produce a full range of test cases to thoroughly evaluate the functional correctness and code efficiency of given solutions. Specifically, We feed \(pretty\_content\) into GPT-4  to generate an initial test case generator snippet. To confirm the effectiveness of the initial generator, we subsequently create 24 test cases by the generator and submit these cases to the Leetcode Online Judge (OJ) system. Should any of the generated test cases not pass the LeetCode OJ validation, we manually revise the generator until all generated cases can be successfully validated.

## 3 Code Efficiency Metric

In the domain of software development, code efficiency can be defined as the absolute code runtime for executing a given test case set . Nonetheless, a primary obstacle in benchmarking code efficiency is normalizing runtime measurements across disparate environments. For instance, a sub-optimal solution might have a faster absolute runtime on high-performance hardware than an optimal solution on low-performance hardware. Moreover, different operation systems and code interpreters may also fluctuate the code runtime. Therefore, absolute runtime fails as a consistent and reliable code efficiency benchmark metric. To address this issue, an intuitive approach involves modeling a devoted runtime distribution for each task and calculating the average runtime percentiles of LLM solution samples over the runtime distribution. With this idea in mind, we proposed a normalized code efficiency metric Beyond:

\[p_{k}^{n}=)-clip(r_{k}^{n},min(R^{n}),max(R^{n}))}{max(R^{n})- min(R^{n})}, Beyond=^{n=0,k=0}p_{k}^{n}}{N K}.\] (1)Where \(N\) is the total number of tasks, and \(K\) denotes the size of LLM solution samples. For a specific task \(n N\), \(R^{n}\) is the runtime array corresponding to the collected historical solutions, and \(r^{n}_{k}\)\(s.t.\)\(k K\) denotes the runtime for the \(k\)-th LLM solution. \(clip\) is a function to constraint the value \(r^{n}_{k}\) in the range \([min(R^{n}),max(R^{n})]\). _Runtime_ is defined as the period from the solution instantiation to the evaluation across all test cases, culminating with a successful termination (More engineering details can be found in Appendix Section A.3). Since any case failure of the \(k\)-th solution results in \(r^{n}_{k}+\) and then \(p^{n}_{k}=0\), Beyond can reflect functional correctness as well.

**Untrusted Code Execution.** Since most Code LLMs are trained on an extensive code corpus from unverified sources, there is an intrinsic risk that these models may produce malicious code when driven by specific metocluous prompts . The direct execution of synthesized code raises significant security concerns. To alleviate the risk of running untrusted code, we engage a robust sandbox to execute code in an isolated environment. Sandbox details are deliberated in Appendix A.3

**Environment-agnostic Evaluation.** To ensure fair comparison across diverse configurations, we run each task \(n\) with corresponding test cases locally and aggregate their runtimes into the runtime array \(R^{n}\). Appendix Figure 10 illustrates the Beyond score of two LLMs ('deepsek-coder-33b' and 'deepsek-coder-6.7b') over three distinct hardware specifications: the micro-tier (0.25 CPU cores), the small-tier (0.5 CPU cores), and the standard-tier (1 CPU core). The results demonstrate that _Beyond_ remains consistent over different hardware configurations.

## 4 Experiments

In this section, we present a series of baseline experiments to improve code efficiency by training on _Mercury-train_ dataset and assessing on the _Mercury-eval_ dataset. Our empirical study encompasses 10 open-source LLMs with a broad parameter spectrum from 1.3 to 34 billion. For each LLM, we compare the performance of the original model and two optimization strategies, Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), for their potential to optimize LLM generating functionally correct and computationally efficient code. Finally, we analyzed the underlying factors contributing to the failure of LLMs on the _Mercury-eval_ dataset.

### Baselines

**Supervised Fine-Tuning (SFT).** Within the SFT  method, an LLM undergoes additional training on a small dataset, which aims to specialize the LLM to perform better on certain tasks correlated to the training dataset. To optimize the code efficiency performance of Code LLMs, the most intuitive strategy is to fine-tune the Code LLM using optimal runtime solutions. In our experimental setup, we apply a unified prompt template for each Code LLM to ensure a fair comparison. The "pretty_content" attribute fills the **<task_content>** placeholder, the "prompt" attribute fills the **<code_starter>** placeholder, and the **<code_completion>** placeholder is completed with the fastest solutions. To steer Code LLMs towards generating the intended code completion format, we prepend a one-shot example to the prompt template. Appendix Figure 9 presents the generation template.

**Direct Preference Optimization (DPO).** Although SFT exemplifies a straightforward approach, it is susceptible to the pitfall of catastrophic forgetting . To enable LLMs to align with human preferences while preserving their functional capabilities, existing methodologies employ reinforcement learning with human preference feedback (RLHF). However, RLHF introduces additional model complexities and potential instabilities, necessitating significant computing resources and extra reward model training [54; 5; 45]. DPO  bypasses these challenges by explicitly mapping reward functions and the optimal objective. This connection demonstrates that maximizing rewards under specific constraints can be effectively addressed through a singular training phase based on data reflecting human preferences. The DPO training procedure is elaborated in Appendix Section A.4.

### Functional Correctness Benchmarks

**HumanEval** assesses the functional correctness of synthesized code derived from docstrings. It contains 164 distinct Python tasks that cover several programming areas, such as language comprehension, algorithm development, and simple mathematics . **MBPP** has a sanitized collection of 257 entry-level Python programming problems. Each problem in this dataset consists of three components: a task description, an associated code solution, and three automated test cases to validate the code functionality . Both HumanEval and MBPP harness the metric Pass to measure the Code LLMs' functional correctness, where a task is considered solved if the given solution passes all test cases, and the total fraction of solved tasks is reported as \(Pass=N_{solved}/N_{total}\).

### Experimental Setups

**Configuration.** We employ LoRA  for both SFT and DPO experiments. We set \(lora\_alpha=16\), \(lora\_dropout=0.05\), and \(lora\_r=8\). The optimizer is _Adamw_, and the learning rate is 1e-4 and 5e-5 for SFT and DPO, respectively. For SFT experiments, we train each model in 200 steps. For DPO experiments, we set \(=0.1\) and \(training\_step=500\). For code generation, we set the temperature as 0.2. For the \(Beyond\) metric calculation, we set \(K=5\). All experiments are conducted on two A100-80G GPUs. We employed Accelerate  for distributed training, DeepSpeed  for gradient partitioning, and BitsandBytes  for model quantization.

**Training Data.** We use _Mercury-train_ for model training. As for the SFT process, we nominate the fastest solution as the supervised label, then format the training data as \( pretty\_content,prompt,solution\_optimal\). Regarding the DPO procedure, we select the top 5 pairs of solutions that exhibit the most significant discrepancy in runtime. The training date format is \( pretty\_content,prompt,solution\_fast,solution\_slow\).

### Empirical Results

Functional correctness is the prerequisite for evaluating code efficiency for the code generation task. Our primary objective is to enhance the code efficiency without compromising the functional correctness. To this end, we first introduce the existing metric Pass to gauge the functional correctness  and then leverage Beyond to provide a holistic evaluation, encompassing both code efficiency and functional correctness. Finally, we measure the Gap between Beyond and Pass to reflect the baseline ability of improving efficiency while preserving correctness. These experiments aim to investigate the innate capabilities of cutting-edge Code LLMs and their potential after baseline fine-tuning. Therefore, extensive parameter optimization and prompt engineering were not pursued for the Pareto front. To deliver a comprehensive evaluation, we have further integrated the HumanEval and MBPP benchmarks as supplementary measures for appraising functional correctness [9; 3].

**Functional Correctness.** Table 2 lists Pass scores over various Code LLMs, showing that larger models tend to provide better functional correctness. Except for the smallest model "deepseek-coder-1.3b-base", DPO invariably enhances the overall Pass scores across most Code LLMs, while SFT diminishes functional correctness on the largest two Code LLMs. These findings suggest that smaller models may struggle to integrate new knowledge while preserving their original functionality, and SFT may induce catastrophic forgetting in the pursuit of heightened code efficiency. Moreover, it is evident on _Mercury_ that Pass scores of each model consistently decline as the difficulty level increases, indicating that the _Mercury_ difficulty stratification is effective at probing the upper limitation of each Code LLM compared to the auxiliary benchmarks.

Figure 3: The horizontal axis represents the score for functional correctness, while the vertical axis indicates the score for code efficiency. The diagonal line represents perfect efficiency to the corresponding correctness. Points closer to this line indicate better efficiency improvements relative to their correctness. The _left_ figure illustrates the performance of the baseline model, whereas the _right_ one depicts the performance after DPO tuning.

**Code Efficiency.** Regarding the NL2Code task, once functional correctness has been assured, attention naturally pivots to enhancing code efficiency. As depicted in Table 3, we investigate code efficiency metrics across a spectrum of Code LLMs. Experiments demonstrate that DPO yields a stable enhancement in code efficiency from models exceeding 6.7B parameters. Notably, "deepseek-coder-33b-base" achieves the highest Beyond score of 66.47, marking a significant improvement of 17.94 over the vanilla model. In contrast, SFT detracts most Beyond scores from original models, suggesting that the plain SFT may not be a feasible strategy for enhancing code efficiency.

To investigate whether prompt engineering could offer a straightforward efficiency boost, we conducted an additional experiment using a specific prompt: _You are a coding expert. You can generate correct and fast code_. As outlined in Appendix Table 8, pre-trained code LLMs struggled to interpret the instruction, leading to decreased performance. Conversely, the instruction-tuned model "deepseek-coder-33b-instruct" demonstrated significant performance improvement, likely due to its training on Leetcode-style tasks, which enables it to effectively interpret the given instructions. By employing this simple prompt engineering technique, the Gap score is reduced from 10.6 to 8.8.

**Gap between Correctness and Efficiency.** Further analysis compares the Gap between Beyond and Pass. Since the ideal Beyond should be aligned with Pass (where the LLM-generated solution is correct and faster than all historical solutions), it shows how much the baseline method shrinks the gap between functional correctness and code efficiency. Our findings indicate that DPO substantially narrows Gap in models larger than 15B parameters. However, Gap tends to widen in smaller models under the same configuration. This implies that larger models possess a greater capacity to assimilate the nuanced knowledge to make strides in efficiency while retaining their functional correctness.

   } &  &  &  &  &  \\ 
**deepseek-coder-1.3b-base** & 28.7 & 55.4 & 60.7 & 52.8 & 23.2 & 38.1 \\ + SFT & 24.2 & 46.2 & 58.9 & 53.6 & 25.3 & 36.2 (-1.9) \\ + DPO & 29.1 & 50.2 & 61.4 & 53.6 & 20.0 & 35.9 (-2.2) \\ 
**starcoder2-3b** & 31.7 & 57.4 & 56.1 & 52.1 & 21.6 & 37.8 \\ + SFT & 29.0 & 47.2 & 60.7 & 58.8 & 25.3 & 38.8 (+1.0) \\ + DPO & 33.5 & 59.6 & 62.5 & 61.0 & 23.4 & 41.1 (+3.3) \\ 
**deepseek-coder-6.7b-base** & 47.6 & 70.2 & 69.3 & 68.9 & 56.1 & 61.0 \\ + SFT & 56.1 & 59.6 & 69.1 & 71.4 & 57.7 & 62.2 (+1.2) \\ + DPO & 54.3 & 72.8 & 74.1 & 72.6 & 58.9 & 65.4 (+4.4) \\ 
**starcoder2-7b** & 35.2 & 54.4 & 63.6 & 61.7 & 29.2 & 44.3 \\ + SFT & 42.9 & 57.2 & 64.8 & 58.5 & 31.3 & 47.5 (+3.2) \\ + DPO & 55.4 & 61.4 & 74.8 & 66.9 & 32.6 & 53.6 (+9.3) \\ 
**CodelLama-7b-hf** & 33.5 & 52.0 & 55.7 & 41.7 & 12.9 & 29.6 \\ + SFT & 29.5 & 47.6 & 58.9 & 38.5 & 16.1 & 31.3 (+1.7) \\ + DPO & 38.7 & 49.2 & 67.5 & 45.7 & 17.9 & 36.1 (+6.5) \\ 
**CodeQwen1.5-7B** & 51.8 & 72.2 & 70.0 & 70.1 & 49.7 & 61.1 \\ + SFT & 54.3 & 74.8 & 70.9 & 67.9 & 49.7 & 61.9 (+0.8) \\ + DPO & 55.5 & 75.4 & 72.5 & 66.9 & 45.7 & 61.1 (+0) \\ 
**starcoder2-15b** & 46.3 & 66.2 & 69.5 & 65.4 & 50.3 & 58.0 \\ + SFT & 51.6 & 69.2 & 72.0 & 68.9 & 51.7 & 61.3 (+3.3) \\ + DPO & 57.0 & 72.8 & 78.0 & 73.8 & 54.7 & 66.7 (+8.7) \\ 
**CodelLama-13b-hf** & 37.8 & 62.4 & 76.8 & 60.5 & 18.4 & 39.6 \\ + SFT & 39.5 & 59.8 & 65.5 & 54.8 & 19.5 & 39.5 (-0.1) \\ + DPO & 49.1 & 64.4 & 78.6 & 60.0 & 29.0 & 50.1 (+10.6) \\ 
**deepseek-coder-33b-base** & 54.3 & 73.2 & 70.9 & 67.9 & 62.3 & 65.0 \\ + SFT & 58.1 & 74.8 & 61.8 & 58.0 & 47.1 & 58.7 (-6.3) \\ + DPO & **72.9** & **80.6** & 78.9 & **76.5** & 61.6 & **73.4** (+8.4) \\ 
**CodelLama-34b-hf** & 48.2 & 65.4 & 77.7 & 63.7 & 32.4 & 52.4 \\ + SFT & 52.8 & 68.2 & 61.8 & 58.0 & 26.2 & 47.5 (-4.9) \\ + DPO & 65.9 & 75.2 & **83.9** & 68.4 & **63.2** & 70.6 (+18.2) \\   

Table 2: Functional correctness (Pass) evaluation results. The underlined values denote the top-performed approaches among the original model and baselines. **The bolded values** denote the best performance on each benchmark. We sample one solution for each task to calculate _pass_ score.

### Failure Analysis

Table 4 provides the error breakdown of where Code LLMs misstep during the _Mercury_ evaluation:

(1) **Generation Errors** arise from syntactical issues. The common manifestations include _improper indentation_, _mismatched parentheses_, or _unexpected truncation_. Fine-tuning introduces additional knowledge for Code LLMs to adapt the Mercury convention, emphasizing standard indentation, concise code, and minimal comments. Therefore, both SFT and DPO generally reduced these errors, while they may lead to catastrophic forgetting in relatively smaller models, such as "deepseek-coder-1.3b-base".

(2) **Execution Errors** differ from Generation Errors because they occur after the code has been successfully loaded. These errors emerge as exceptions, which could stem from various issues, such as flawed code logic, execution timeouts, memory leakage, or sandbox interruption. We observe that SFT tends to aggravate these errors on most models, whereas DPO mitigates these errors successfully.

(3) **Test Case Errors** are the most prevalent errors where the code is executed without exceptions, but the output fails to align with the expectation. DPO demonstrates the suppression of these errors, especially in relatively large models, while SFT tends to increase the occurrence of these errors across nearly all models. This suggests that direct SFT may lead to catastrophic forgetting in vanilla models, diminishing their ability to generate functionally correct code. In contrast, DPO not only enhances code efficiency but also more reliably preserves functional correctness.

 
**Model name** & **Easy** & **Medium** & **Hard** & **Overall** & **Gap** \\ 
**deepseek-coder-1.3b-base** & 47.97 & 39.77 & 19.26 & 35.62 & 9.85 \\ + SFT & 42.58 & 38.12 & 18.67 & 33.04 (-2.58) & 12.74 (+2.89) \\ + DPO & 46.91 & 42.27 & 16.78 & 35.21 (-0.41) & 9.64 (-0.21) \\ 
**starcoder2-3b** & 43.55 & 41.91 & 15.21 & 33.40 & 9.72 \\ + SFT & 44.64 & 42.10 & 15.72 & 34.01 (+0.61) & 14.04 (+4.31) \\ + DPO & 43.70 & 41.02 & 12.99 & 32.42 (-0.99) & 16.33 (+6.61) \\ 
**deepseek-coder-6.7b-base** & 48.80 & 51.16 & 45.11 & 48.29 & 16.40 \\ + SFT & 51.37 & 52.71 & 44.28 & 49.39 (+1.09) & 16.55 (+0.16) \\ + DPO & 56.25 & 52.35 & 40.62 & 49.70 (+1.41) & 18.73 (+2.34) \\ 
**starcoder2-7b** & 50.23 & 51.29 & 20.25 & 40.37 & 10.95 \\ + SFT & 42.21 & 44.02 & 21.09 & 35.61 (-4.77) & 15.80 (+4.84) \\ + DPO & 53.52 & 51.41 & 17.35 & 40.56 (+0.18) & 17.41 (+6.46) \\ 
**CodeLlama-7b-hf** & 42.55 & 30.99 & 8.88 & 27.45 & 9.27 \\ + SFT & 39.75 & 26.89 & 9.55 & 25.41 (-2.04) & 12.48 (+3.21) \\ + DPO & 54.14 & 34.48 & 11.10 & 33.29 (+5.84) & 10.46 (1.19) \\ 
**CodeQwen1.5-7B** & 51.11 & 53.56 & 39.03 & 47.78 & 15.35 \\ + SFT & 54.16 & 51.43 & 38.05 & 47.82 (0.04) & 14.91 (-0.43) \\ + DPO & 56.07 & 51.55 & 38.05 & 48.52 (0.74) & 13.12 (-2.22) \\ 
**starcoder2-15b** & 58.18 & 52.09 & 37.34 & 49.17 & 12.55 \\ + SFT & 53.54 & 52.77 & 37.73 & 47.92 (-1.25) & 16.22 (+3.67) \\ + DPO & 68.29 & 59.54 & 48.97 & 58.95 (9.78) & 10.81 (-1.74) \\ 
**CodeLlama-13b-hf** & 57.00 & 44.25 & 12.99 & 38.01 & 13.79 \\ + SFT & 44.95 & 39.96 & 13.55 & 32.70 (-5.31) & 13.78 (-0.01) \\ + DPO & 67.09 & 55.72 & 19.72 & 47.39 (9.38) & 8.47 (-5.32) \\ 
**deepseek-coder-33b-base** & 51.26 & 48.90 & 45.43 & 48.53 & 18.50 \\ + SFT & 40.33 & 37.75 & 36.82 & 38.32 (-10.21) & 17.30 (-1.20) \\ + DPO & 74.59 & 68.91 & **55.98** & **66.47** (+17.94) & **5.79** (-12.70) \\ 
**CodeLlama-34b-hf** & 56.28 & 48.21 & 22.96 & 42.40 & 15.49 \\ + SFT & 45.49 & 44.96 & 20.73 & 36.91 (-5.50) & 11.61 (-3.88) \\ + DPO & **78.55** & **60.95** & 51.94 & 63.94 (+21.54) & 8.01 (-7.47) \\  

Table 3: Code efficiency (Beyond) evaluation results across three difficulty levels. **The bolded value** indicates the top performance for each metric, while the underlined values denote the most effective approaches among the original model and the baselines. In our experiment, we sample 5 solutions for each task to calculate _Beyond_ score.

## 5 Related Work

**NL2Code Generation** is the task of generating a computer program that satisfies given specifications. Initial approaches to converting natural language to code relied on rigid methods like probabilistic grammars and domain-specific languages, having limited flexibility and scalability [23; 13]. The advent of statistical models, such as n-grams and Hidden Markov models, attempted to overcome these limitations but struggled with modeling complexity and dependencies [35; 46]. The transformational impact of the Transformer model  and its subsequent application to NL2Code  led to the development of LLMs like Codex, which significantly improved the task's feasibility by utilizing extensive unlabelled data sets . Follow-up LLMs such as AlphaCode , CodeGen , PaLM-Coder , and StarCoder  continued to advance this research field, exhibiting emergent abilities in coding and debugging that mirrored human programmers.

**NL2Code Correctness Evaluation** currently focuses on gauging the functional correctness of generated code. As a pioneer, CodeBLEU  adapts the BLEU  metric into code generation. However, given the abstract nature of programming languages, distinct code can express the equivalent semantics, prompting subsequent benchmarks to harness test case fuzzing instead of the similarity measurement. For example, HumanEval  and MBPP  consist of hand-written Python programming tasks and corresponding test cases. EvalPlus  enhances HumanEval by incorporating extensive auto-generated test cases, constructing a more rigorous benchmark HumanEval+ to evaluate the functional correctness of LLM synthesized code. On the note of enhancing language inclusiveness, ODEX  integrates multiple natural languages, while MBXP  extends the benchmarks to cater

   } &  &  &  &  \\    & E & M & H & E & M & H & E & M & H & E & M & H \\ 
**deepseek-coder-1.3b-base** & 82 & 85 & 59 & 17 & 33 & 95 & 74 & 73 & 180 & 267 & 214 & 101 \\ + SFT & 106 & 104 & 37 & 16 & 8 & 63 & 59 & 76 & 225 & 259 & 217 & 110 \\ + DPO & 90 & 108 & 40 & 17 & 5 & 49 & 63 & 75 & 259 & 270 & 217 & 87 \\ 
**starcoder2-3b** & 107 & 102 & 33 & 35 & 26 & 107 & 51 & 66 & 201 & 247 & 211 & 94 \\ + SFT & 97 & 93 & 24 & 29 & 13 & 90 & 47 & 61 & 211 & 267 & 238 & 110 \\ + DPO & 79 & 75 & 11 & 30 & 14 & 87 & 56 & 69 & 235 & 275 & 247 & 102 \\ 
**deepseek-coder-6.7b-base** & 107 & 101 & 30 & 16 & 5 & 56 & 12 & 20 & 105 & 305 & 279 & 244 \\ + SFT & 105 & 100 & 25 & 17 & 6 & 61 & 14 & 10 & 98 & 304 & 289 & 251 \\ + DPO & 87 & 82 & 23 & 12 & 6 & 58 & 15 & 23 & 98 & 326 & 294 & 256 \\ 
**starcoder2-7b** & 107 & 101 & 22 & 21 & 9 & 74 & 32 & 45 & 212 & 280 & 250 & 127 \\ + SFT & 105 & 100 & 22 & 18 & 13 & 72 & 32 & 55 & 205 & 285 & 237 & 136 \\ + DPO & 90 & 90 & 21 & 10 & 11 & 61 & 11 & 33 & 211 & 329 & 271 & 142 \\ 
**CodeLlama-7b-hf** & 23 & 28 & 23 & 41 & 69 & 122 & 131 & 139 & 234 & 245 & 169 & 56 \\ + SFT & 11 & 9 & 17 & 44 & 72 & 112 & 126 & 168 & 236 & 259 & 156 & 70 \\ + DPO & 9 & 10 & 12 & 23 & 56 & 117 & 111 & 154 & 228 & 297 & 185 & 78 \\ 
**CodeOwen1.5-7B** & 105 & 100 & 18 & 1 & 4 & 44 & 26 & 17 & 157 & 308 & 284 & 216 \\ + SFT & 105 & 101 & 16 & 4 & 3 & 35 & 19 & 26 & 168 & 312 & 275 & 216 \\ + DPO & 98 & 96 & 26 & 5 & 8 & 35 & 18 & 30 & 175 & 319 & 271 & 199 \\ 
**starcoder2-15b** & 105 & 100 & 20 & 4 & 7 & 49 & 25 & 33 & 147 & 306 & 265 & 219 \\ + SFT & 104 & 100 & 18 & 3 & 3 & 56 & 16 & 23 & 136 & 317 & 279 & 225 \\ + DPO & 83 & 64 & 10 & 1 & 1 & 33 & 13 & 41 & 141 & 343 & 299 & 251 \\ 
**CodeLlama-13b-hf** & 10 & 14 & 28 & 19 & 41 & 99 & 73 & 105 & 228 & 338 & 245 & 80 \\ + SFT & 46 & 52 & 32 & 29 & 19 & 111 & 77 & 112 & 207 & 288 & 222 & 85 \\ + DPO & 24 & 9 & 24 & 10 & 12 & 100 & 60 & 141 & 185 & 346 & 243 & 126 \\ 
**deepseek-coder-33b-base** & 105 & 103 & 26 & 11 & 11 & 47 & 12 & 16 & 91 & 312 & 275 & 271 \\ + SFT & 69 & 78 & 27 & 27 & 26 & 65 & 72 & 66 & 138 & 272 & 235 & 205 \\ + DPO & 56 & 75 & 15 & 9 & 7 & 86 & 28 & 13 & 66 & 347 & 310 & 268 \\ 
**CodeLlama-34b-hf** & 22 & 35 & 50 & 28 & 55 & 84 & 48 & 57 & 160 & 342 & 258 & 141 \\ + SFT & 35 & 97 & 50 & 37 & 19 & 56 & 96 & 54 & 215 & 272 & 235 & 114 \\ + DPO & 4 & 12 & 10 & 26 & 76 & 30 & 41 & 40 & 120 & 369 & 277 & 275 \\   

Table 4: The distribution of failure cases across _Code Generation_, _Code Execution_, and _Test Case_ errors. E/M/H indicates Easy/Medium/Hard levels, respectively. We sample 5 solutions for each task, so there are \(256*5=1280\) solutions in total for each model.

to a variety of programming languages, promoting polyglot code generation evaluation. Recent benchmarks have also begun to consider more aspects beyond functional correctness. For instance, the benchmark DS-100  dives deeply into the data analysis scenarios, and CodeGen  contributes a benchmark for multi-turn code generation. For security-oriented code generation, SecurityEval  offers a concentrating benchmark on mining the vulnerability of generated code. BigCodeBench  introduces more sophisticated instructions and diverse function calls to gauge the true programming capabilities of LLMs in realistic scenarios. LiveCodeBench  continuously updates its problem set, ensuring contamination-free functional correctness evaluations. One more work may be related to our work. AlphaCode  employs language models to generate solutions for competitive programming problems, but they do not focus on optimizing the solution performance.

**NL2Code Efficiency Evaluation** Evaluating code efficiency has long been a crucial topic in software engineering. With the advent of code generation models, it is gaining even more attention in the code LLM evaluation. As a pioneering effort, DeepPERF  employs a fine-tuned transformer to generate performance-enhancing patches for C# programs, evaluating the similarity between these generated patches and those created by developers. PIE  provides a benchmark suite for deterministically assessing the performance of C++ code within the Gem5  environment. More recently, EFFIBENCH  constructed an efficiency benchmark using 1,000 Python problems from LeetCode. SUPERSONIC  introduces a compact sequence-to-sequence model designed to iteratively optimize code performance. All these studies employ the relative speedup metric to evaluate code efficiency gains.

## 6 Limitations

In this work, we measure code efficiency under the assumption that the code runtime is uniformly distributed. The simplification streamlines code efficiency evaluation via limited solution samples. However, the distribution of code runtime in real-world scenarios is more intricate, which may call for more solution samples to support more precise modeling. Additionally, the presence of data contamination during the model training phase compromises the precision of the Mercury benchmark to reflect the performance of tainted models . To mitigate this issue, we will update our benchmark via our open-sourced data collection framework to import new tasks dynamically, thus laying the groundwork for more detailed investigations in subsequent studies.

## 7 Conclusion

In this work, we introduced Mercury, the first code efficiency benchmark for NL2Code evaluation. Unlike prior work that focused on functional correctness, our benchmark highlights the importance of code efficiency. By crafting dedicated test case generators and sampling ground-truth solutions across all difficulty levels from Leetcode, we have developed a comprehensive and rigorous Code LLM evaluation frame. We evaluated leading Code LLMs against benchmarks and found that even though these models are proficient in generating functionally correct code, there is still considerable space for code efficiency improvement. As Code LLMs become more widely used, code efficiency determines factual productivity, where Mercury can gauge the vital metric. As a commitment to ongoing research and to foster further innovation in this area, we have open-sourced the Mercury dataset collection framework, laying the groundwork for future advancements in the field.