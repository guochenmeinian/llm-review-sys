# MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks

Allen Nie\({}^{1*}\) Yuhui Zhang\({}^{1}\) Atharva Amdekar\({}^{2}\)

Chris Piech\({}^{1}\) Tatsunori Hashimoto\({}^{1}\) Tobias Gerstenberg\({}^{3*}\)

\({}^{1}\)Computer Science \({}^{2}\)ICME \({}^{3}\)Psychology

Stanford University

*{anie,gerstenberg}@stanford.edu

###### Abstract

Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.

## 1 Introduction

Humans rely on their intuition to understand the world. This intuition helps us to understand not only physical events (e.g., one ball caused the other to move) but also complex social situations (e.g., the collapse of Sam Bankman-Fried's FTX caused unprecedented turmoil in the cryptocurrency market). Given a complex set of events, even with a great amount of ambiguity, we can answer questions such as "What or who caused it?" Our answers to this question reflect how we intuitively understand events, people, and the world around us (Sloman & Lagnado, 2015; Pearl & Mackenzie, 2018). How do humans handle this complexity?

Cognitive scientists have proposed that we do so by organizing our understanding of the world into intuitive theories (Gerstenberg & Tenenbaum, 2017; Wellman & Gelman, 1992). Accordingly, people have intuitive theories of the physical and social world with which we reason about how objects and agents interact with one another (Battaglia et al., 2013; Ullman et al., 2017; Gerstenberg et al., 2021; Baker et al., 2017; Davis & Marcus, 2015; Lake et al., 2017). Concepts related to causality and morality form key ingredients of people's physical and social theories. Given a story, humans can readily make causal and moral judgments about the objects and agents involved in the story.

Studying these human **intuitions** or systematic **tendencies** when making decisions or judgments is the central focus of psychological experimentations. What are these tendencies? How do they influence our judgment? Over the last several decades, using text-based vignettes, psychologistshave disentangled what factors influence people's causal and moral judgments. These factors can be understood as the building blocks of our thought processes for making causal and moral judgments.

Over the last few years, large language models (LLMs) have become increasingly successful in emulating certain aspects of human commonsense reasoning ranging from tasks such as physical reasoning Tsividis et al. (2021), visual reasoning Buch et al. (2022), moral reasoning Hendrycks et al. (2020), and text comprehension Brown et al. (2020); Liu et al. (2019); Bommasani et al. (2021). Here, we investigate to what extent pretrained LLMs align with human intuitions about the role of objects and agents in text-based scenarios, using judgments about causality and morality as two case studies. We show how carefully constructed text scenarios from cognitive science can help shed light on the extent to which humans and LLMs align in a way that goes beyond mere agreement in aggregate.

Prior work on alignment between LLMs and human intuitions usually collected evaluation datasets in two stages. In the first phase, participants write stories with open-ended instructions. In the second phase, another group of participants labels these participant-generated stories (e.g. Hendrycks et al., 2020). The upside of this approach is the ease of obtaining a large number of examples in a short period of time. The downside of this approach is that the crowd-sourced stories are often not carefully written, and that they lack experimental control. Here, we take a different approach. Instead of relying on participant-generated scenarios, we collected two datasets from the existing literature in cognitive science: one on causal judgments and another on moral judgments. These scenarios were carefully written by researchers with the intention of systematically manipulating one or a few factors that have been theorized to influence people's judgments. These latent factors naturally bring structures to the otherwise plain text stories, which aim to explain human judgments (see Figure 1). Using these scenarios, we can design a series of experiments that aim to measure the LLMs' alignment with human intuition and use the scientific framework around human judgments to analyze where the differences occur. We have released the code and dataset here: https://github.com/cicl-stanford/moca

**Our Contributions**

1. [label=(C0)]
2. We summarize the main experimental findings of 24 cognitive science papers into factors that have been shown to influence participants' judgments on moral and causal stories (see short version in Table 2a, full version in Table A1). From these papers, we create a causal and moral judgment challenge set in which only a few words are changed, yet they lead to big differences in people's judgments. We collected 5150 human responses to our stories and annotated the latent factors with experts (see Figure 1).
3. We evaluate how a wide range of LLMs align with human judgments. This includes models of different sizes, models fine-tuned via reinforcement learning using human feedback (**RLHF**), and those distilled from instruction-following, like **Alpaca-7B**. Our results show that larger models and RLHF techniques lead to better alignment.
4. We compute the Average Marginal Component Effect (AMCE) to reveal the implicit tendencies for each factor from human and LLM judgments. Our analysis reveals a number of implicit tendency differences between models trained within the same company or trained with the same technique. Our analysis highlights the importance of using a carefully constructed dataset to understand tendency alignment.

## 2 Related Work

For causal reasoning, there is an active line of research at the intersection of natural language processing (NLP) and commonsense reasoning that involves extracting and representing causal relationships among entities in text. In some cases, these relationships are based on commonsense knowledge of how objects behave in the world Sap et al. (2019); Bosselut et al. (2019); Talmor et al. (2019). In other cases, models identify scenario-specific causal relations and outcomes. Sometimes, the causal relationship between entities is explicitly stated (e.g., _those cancers were caused by radiation_) Hendrickx et al. (2010), while at other times the relationship is left implicit and needs to be inferred Mostafazadeh et al. (2016). Causal reasoning is also included in broad benchmarks for language understanding, such as the Choice of Plausible Alternatives (COPA) in SuperGLUE Roemmele et al. (2011); Wang et al. (2019).

For moral reasoning, tasks have focused on evaluations of agents in narrative-like text. These tasks and datasets vary in the amount of structure they provide, ranging from pairs of free-form anecdotes and judgment labels Lourie et al. (2021); Hendrycks et al. (2020), to inputs with components separatedout into norms, intention, actions, and consequences (Emelin et al., 2021). Prior work has also looked into moral dilemmas, particularly around everyday scenarios (Hendrycks et al., 2020; Jiang et al., 2021). In this work, the moral scenarios were generated through crowd-sourcing (Emelin et al., 2021; Ziems et al., 2022), and there is general agreement about what's the morally right thing to do in a scenario (i.e. these scenarios weren't moral dilemmas). Alternatively, scenarios have been scraped from online communities such as Reddit (Forbes et al., 2020; Lourie et al., 2021; Roemmele et al., 2011). While these scenarios have greater external validity, they lack the experimental control of scenarios from cognitive science research. How LLMs align with people's intuitions across a range of moral dilemmas hasn't systematically been studied before. Kiciman et al. (2023) proposed modularized tests to understand whether LLMs can perform different types of causal analysis. Our work investigates whether these LLMs exhibit implicit tendencies similar to those of humans. Scherrer et al. (2023) evaluated LLMs on moral scenarios but focused on moral rule violation along the axis of moral "ambiguity". There are also discussions of moral behaviors with reinforcement learning agents in game-like environments (Hendrycks et al., 2021; Reinecke et al., 2023; Pan et al., 2023) and with respect to legal reasoning (Almeida et al., 2023).

Our work falls under a broad range of research on commonsense reasoning, where models are encouraged to produce outputs that match human intuitions (Gabriel, 2020; Kenton et al., 2021). In some cases, such as physical commonsense reasoning, alignment with human behavior is straightforward, while in the case of social commonsense, the subjectivity and diversity of human intuitions make alignment more challenging (Davani et al., 2022). Prior analysis of language models' reasoning abilities includes measuring their behavior in zero-shot settings, after task fine-tuning (Jiang et al., 2021; Hendrycks et al., 2020), or with human-curated support, such as chain-of-thought prompt engineering (Wei et al., 2022; Wang et al., 2022). Evaluating LLMs with causal hypotheses has also been explored by Kosoy et al. (2022). While they focused on the influence of causal structure, we investigate a variety of factors (including causal structure). Concurrently, Jin et al. (2022) curated a challenge dataset to examine and verify LLMs' ability to conform to three categories of social norms. We look at five factors that have been shown to influence people's moral judgments and analyze whether LLMs respond to these factors similarly to humans.

## 3 Judgment Tasks

We study the alignment between LLMs and human participants in two case studies: (1) a causal judgment task about whether someone or something caused the outcome of interest, and (2) a moral judgment task where the question is whether what a protagonist did (or failed to do) was morally permissible. Figure 1 shows an example of each task.

Figure 1: Two examples from our collected dataset. (a) shows a causal judgment story, and (b) shows a moral judgment story. In (a), a conjunction of two events was required, an abnormal event occurred, and Jane violated a prescriptive norm (scenario taken from Knobe and Fraser, 2008). In (b), the teenager’s death was inevitable; his death is a necessary means to save others, and bringing about his death requires the use of personal force (scenario taken from Christensen et al., 2014).

### Causal Judgment Task

Deciding what "the" cause of an outcome was can be challenging because there are often multiple events that contributed to an outcome. Here is an example story:

_A merry-go-round horse can only support one child. Suzy is on the horse. Billy is not allowed to get on, but he climbs on anyway. The horse broke due to the two children's weight. Did Billy cause the horse to break?_

Even though both children were involved in breaking the horse, when participants are asked to assess whether Billy (who wasn't allowed to get on) caused it to break, they tend to answer with "yes". Formal models of causal selection have been developed that capture choice patterns in scenarios like this one (Kominsky et al., 2015; Icard et al., 2017; Gerstenberg and Icard, 2020; Quillien and Lucas, 2023). In this example, Billy violated a prescriptive norm, whereas Suzy didn't. People often select norm-violating events as the cause of an outcome (Knobe and Fraser, 2008; Hitchcock and Knobe, 2009; Alicke et al., 2011; Hilton and Sluposki, 1986; Hart and Honore, 1959/1985). It is worth noting that causality is closely related to responsibility and blame. In fact, formal models of responsibility and blame have been built on top of a causal selection model (Halpern, 2015; Lagnado et al., 2013).

In addition to norm violations, prior work has identified a number of factors that systematically influence people's causal judgments. Here, we focus on the following six factors: causal structure, agent awareness, norm type, event normality, action/omission, and time. Table 1(a) provides brief definitions of the different factors.

### Moral Permissibility Task

Philosophers and cognitive scientists have used moral dilemmas to develop and evaluate normative theories of ethical behavior, and to investigate what moral intuitions people have. A popular moral dilemma is the trolley problem (Foot, 1967; Thomson, 1976), where the question is whether it's morally permissible for the protagonist to re-route a trolley from the main track (saving some) to a side track (killing others). Some LLMs already show a certain degree of alignment with human responses in various versions of the trolley dilemma (Hendrycks et al., 2020; Jiang et al., 2021). However, only a few factors varied in these trolley dilemmas, such as the number of people on each track or personal attributes (age, social status, etc.; Awad et al., 2018).

A large number of factors have been shown to influence people's moral intuitions. For example, in Figure 0(b), people may consider factors such as whether personal force was required to bring about the effect (Greene et al., 2009), whether there was a risk for the protagonist themselves (Bloomfield, 2007), whether the harm was a side effect or a means for bringing about the less bad outcome (Moore et al., 2008), and whether the harm was inevitable (Hauser, 2006; Mikhail, 2007). This work has shown that people's moral judgments are sensitive to a variety of factors that haven't been considered in existing work on the alignment between LLMs and people's moral intuitions (Waldmann and Dieterich, 2007; Liao et al., 2012; Christensen et al., 2014; Kleiman-Weiner et al., 2015). Table 1(b) provides brief definitions of these factors.

### Dataset

For each task, we transcribe stories from a number of papers. For the Causal Judgment Task, we transcribed 144 stories from a selection of 20 papers, and for the Moral Permissibility Task, we transcribed 62 stories from 4 papers that covered a wide range of moral dilemmas. High-level descriptions of the dataset are presented in Table 1. We select these papers relying on expert domain knowledge - as these papers contain robust scientific findings and cover different kinds of scenarios.

   Dataset & \# Stories & Yes (\(p>0.6\)) & No (\(p<0.4\)) & Ambiguous & \# words per & \# words per & \\  & & & & & story & translated story \\  Causal & 144 & 48 & 50 & 46 & 162 & 82.9 \\ Moral & 62 & 23 & 10 & 29 & 72.5 & 53.5 \\  Total & 206 & 71 & 60 & 75 & 135 & 74.1 \\   

Table 1: **Dataset**: We report dataset statistics on the label distribution, average length of each story, and inter-rater agreement between two annotators on the factors and the sentences they highlight. Additionally, we collect a binary response for each story from 25 people.

We collect 25 yes/no answers for each story from a crowd-sourcing platform with IRB approval for our data collection process. We describe the experimental design and data collection details in Appendix A.15. We additionally recruited two domain experts to annotate what factors are present in each story. We report the inter-rater agreement between the two annotators and describe their annotation process in Appendix A.14.

## 4 Experiment and Result

### Do LLMs make the same judgments as people on these stories?

Setup.We first directly compare the responses of a set of LLMs to those of human participants across the set of stories in the original cognitive science experiments. We choose a wide range of language models that have achieved good performance with fine-tuning on other natural language understanding tasks (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Clark et al., 2020). We conduct a 3-class comparison to compute accuracy: a response of "Yes", "No', and an additional response of "ambiguous" when the agreement between the average human responses or the probability of model output is 50% \(\) 10%. We normalize the model output probability to compute \(P(|)\) and \(P(|)\). For chat-based APIs, we prompt the model to output these labels directly. See Appendix A.3 for details about how we computed the probability for each model. Results should not be compared between completion and chat models.

We report discrete agreement (Agg) between LLMs and human participants. The agreement is computed in the same way as accuracy, but since we don't want to imply that higher agreement is necessarily better, we use the term "agreement" instead of "accuracy". We also report additional measures of agreement, such as the area under the receiver operating characteristic curve (AUC), the absolute-mean-squared error (MAE), and cross-entropy (CE) on the probability of the matched label.

Zero-shot AlignmentWe report the result in Table 3. We notice that there is no model that perfectly aligns with human judgments on these metrics. We first find that larger models (GPT-3, GPT-3.5, GPT-4, and Claude) generally align with humans better than smaller models (GPT-2, RoBERTa, etc.). This is most apparent when one compares the performance of GPT3-curie-v1 (6.7B) to GPT3-davinci-v1 (175B). Both models are trained using the same setup, but on both the Moral Permissibility Task and the Causal Judgment Task, GPT3.5-davinci-v2 aligns better than GPT3-curie-v1. While existing literature has shown improved performances of LLMs on general deterministic tasks (Wei et al., 2022), we show here that LLM alignment also improves on ambiguous tasks.

Recently, techniques such as supervised fine-tuning on human instructions (GPT3.5-davinci-v2; Ouyang et al., 2022), using reinforcement learning to update LLM weights from human feedback (RLHF; GPT3.5-davinci-v3, GPT3.5-turbo, Anthropic-claude-v1; Stiennon et al., 2020; Bai et al., 2022), and multi-modal joint training on both language and images (GPT-4; OpenAI, 2023) have improved LLMs on various tasks. Perhaps surprisingly, unlike the monotonic improvement we find by increased model size, these techniques impact these models' aggregate-level alignment differently. Comparing the result between GPT3.5-davinci-v2 and GPT3.5-davinci-v3, RLHF seems to increase aggregate alignment on causal judgment but decrease alignment on moral judgment. Comparing the

Table 2: **Factors**: Factors that influence causal selection judgments (top) and moral permissibility judgments (bottom). We provide definitions for each factor in Appendix A.1 and Appendix A.2. See the full version in Table A.1.

results between Anthropic-claude-v1 and GPT3.5-davinci-v3, it seems that even though both models are fine-tuned with RLHF, on aggregate, their alignment with human judgments differs. In the next section, we will design a method to examine how different training methods cause subtle shifts in the model's implicit tendency.

Social SimulacraPark et al. (2022) proposed a framework to elicit diverse social behaviors from LLMs by prompting them with different personas. For example, adding the persona, "Jane Smith is a liberal activist." before the question. The human responses we collect on each story are diverse (see Figure A4 for the distribution of P(Yes)). We shouldn't expect LLMs to generate a single response that matches the diversity of human perspectives. Therefore, we designed an experiment where we sample 5 personas from 5 simulacra defined by Park et al. (2022) (25 in total). We elicit \(P(|)\) and P("No"|) from the model.

We report three ways of incorporating personas: Persona (average), where we average \(P\) over all personas. Persona (best), where we find the persona that gave the most human-aligned responses. Persona (worst), where we use \(P\) from the persona that gave the least human-aligned responses. See how we designed this experiment in Appendix A.4 and our sampled personas in Table A3. We chose the GPT3.5-davinci-v2 and GPT-3.5-davinci-v3 for this experiment because their API endpoints explicitly provide the probability of Yes or No answers, and they align with humans best.

Personas elicit very different _moral permissibility_ responses - the most and least-aligned persona have close to 18.6% difference in discrete agreement. However, _causal judgments_ do not vary much based on different personas. We show the most and least aligned persona in Appendix A.4.

Table 3: **Original Story**: We ran our experiments and compute the 95% bootstrapped confidence interval for the result. We report discrete agreement (Agg), the area under the curve for the unambiguous stories (AUC, higher is better matched, 0.5 is chance, and below 0.5 is worse matched than chance), mean absolute error (MAE, lower is better matched), and cross-entropy (CE, lower is better matched). In Table 3, we only report results from the best model. Red shows the alignment decreased compared to before. Blue shows the alignment increased. See Table A2 for a full comparison.

Automatic Prompt OptimizationWe additionally explore whether we can use a black box method to automatically identify a prompt that is maximally aligned with collected human responses. Zhou et al. (2022) proposed Automatic Prompt Engineer (APE), a method that uses LLMs to generate prompts and then use the log-probability of a few training examples to select the best prompts. Note that APE does not currently have a workflow defined for chat-based models. Our experiment used five stories as training examples to select prompts. We describe how we searched the optimal prompt in Appendix A.5. We see that APE is still limited by the generative capability of LLMs - we are unable to find a better prompt for the Causal Judgment Task for GPT3.5-davinci-v3. However, for the Moral Permissibility Task, we found a prompt that substantially improved GPT3.5-davinci-v2's discrete agreement by 8.1%. The discovered prompt is "James is a consequentialist." On average, we found that prompts for the Moral Permissibility Task with words like "consequentialist" or "utilitarian" lead to the highest alignment with humans. However, this observation is nuanced, and we provide an extended discussion in Appendix A.6. For causality, good prompts are with the words "a good student", "a good researcher", or "a very logical person". We report all the generated prompts in Appendix A.5.

### Do LLMs have the same implicit tendencies as people on these stories?

In the previous section, we focus on analyzing aggregate metrics such as agreement over all stories. Such analysis often provides no information beyond comparing highly complicated systems with a single number. Since each of our stories is a combination of factors with corresponding attributes (see Figure 1), we can leverage conjoint analysis and compute the Average Marginal Component Effect (AMCE) for each factor attribute (Hainmueller et al., 2014; Awad et al., 2018), where AMCE reveals the implicit tendency of the underlying system when a particular attribute is present.

#### 4.2.1 Method

Average Marginal Component EffectWe provide an overview of how AMCE is computed for each factor that reveals the implicit tendency of the system. For \(N\) responses, \(K\) stories, and \(J\) factors. Each factor has 2 attributes/levels. For 3-level factors such as "Time", we only pick "Early" and "Late" attributes. Each response is \(Y_{jnk}\{0,1\}\), denoting \(n\)-th response, \(j\)-th factor, and \(k\)-th story. For LLMs, \(Y_{jnk}=P(|)\). We also define a factor profile matrix \(T\{0,1\}^{J K}\), where \(T_{jk}\) denotes the attribute level of the \(j\)-th factor in \(k\)-the story. Now, we can use a non-parametric difference-in-means estimator to compute AMCE:

\[(j)=^{J}_{n=1}^{N}_{k=1}^{K}\{T_{kj}= 1\}Y_{jnk}}{_{j=1}^{J}_{n=1}^{N}_{k=1}^{K}\{T_{kj}=1\}} -^{J}_{n=1}^{N}_{k=1}^{K}\{T_{kj}=0\}}{ _{j=1}^{J}_{k=1}^{K}_{n=1}^{N}\{T_{kj}=0\}}\] (1)

Note that if there is no preference between the two attributes of the same factor, \((j)=0\). Here is an example of how this estimator works. If we want to know if a system has a stronger tendency to find causing harm to be more morally permissible when they are "Inevitable" than "Avoidable" for the factor "Counterfactual Evitability", we can find all stories where "Counterfactual Evitability" has the attribute of "Inevitable", compute the average of all the binary responses \(Y\) for all respondents, then subtract the average responses \(Y\) for all stories with attribute "Avoidable". We additionally compute the 95% confidence interval on ACME using Bootstrap and show these results in Figure A2 and Figure A3. We only plot the average \((j)\) in Figure 2 and Figure 3. More details in Appendix A.7.

Zero-shot Implicit TendencyWe use the standard radar chart to capture the multi-factor tendency of each LLM and human. We put the attributes of the same factor (e.g., Abnormal and Normal) on opposite sides - since the leaning is over one attribute or the other: \((j)[-1,1]\). Each concentric circle is marked with a probability (e.g., 0.1), which represents the change in \(P()\) if the factor attribute had been present in the story. The implicit tendency is defined as the expected change in the probability of the system outputting "Yes" if this attribute had been present in the story. Intuitively, we can say a model has an implicit tendency for one attribute when it's more likely to judge an event to be the cause, or the impending harm to be more morally permissible, if this attribute is present.

#### 4.2.2 Results

Causality: Sensitivity to AbnormalityPeople tend to cite abnormal events as the cause of the outcome. In our stories, the normality of events is clearly stated. However, a model still needs to first, identify the norm specified in the story and then, determine which event violated the norm. We find that text-davinci-003 exhibits a strong tendency for citing abnormal events as causal compared to any other models (and to humans). We also find that smaller models (text-babbage-001, text-curie-001, and Alpaca-7B) don't have a tendency for abnormal over normal events, which might indicate an inability to perform the required chained reasoning.

Causality: Statistical or Prescriptive NormStatistical norms refer to commonly and naturally occurring events, such as "David usually comes home around 6 pm." or "When you flip the switch, lights usually turn on." Prescriptive norms refer to human-made rules, such as "You are not allowed to run in the hallway." or "The speed limit is 60 mph." People tend to find events that violate prescriptive norms to be the cause of the outcome. Surprisingly, although GPT-4, Claude-v1, text-davinci-002, and smaller models, such as text-curie-001 and Alpaca-7B, have similar tendencies to humans, text-davinci-003 and its counterpart ChatGPT exhibit a strong tendency for statistical norms. This shows that LLM implicit tendencies can change for different training methods.

Morality: Benefit Self or OthersThe stories in the Moral Permissibility Task often involve scenarios where participants must make a choice: they are either safe from danger, and their decision only affects others, or they need to save their own lives along with other people. Humans are more likely to save themselves (self-beneficial). Interestingly, smaller models such as text-babbage-001 and text-curie-001 find actions more morally permissible when they are self-beneficial, while larger models do not. RLHF makes the models more other-beneficial. For example, while Claude-v1 seems to prefer more self-beneficial actions, text-davinci-003, ChatGPT, and GPT-4, all prefer to be other-beneficial. Human and model tendencies do not align on this factor, but since models are lifeless systems, we may want models to be more other-beneficial. See Figure A2 for a clearer comparison.

Morality: Inevitable or Avoidable HarmSome stories involve situations where harm would inevitably happen to a group of people regardless of the participant's choice. Harm directed towards inevitable consequences is often considered less bad. This requires the model to comprehend the information described in the story about the counterfactual scenario. We find that ChatGPT, Claude-v1, and GPT-4 find harmful actions toward inevitable consequences much more morally permissible than any other model.

Figure 2: **Causal Implicit Tendencies**. Each concentric circle marks the implicit tendency – the change in probability of responding “Yes” if the attribute had been present in the story. For example, if “Abnormal” is present in the story, humans will have a 25% higher probability to respond “Yes” on average than “Normal” being present. The left figure compares models of different sizes. The right figure compares models trained or finetuned with different methods.

Morality: Intervening on Agent or Patient of HarmA hijacked airplane full of passengers is about to hit a building full of people. The people in the hijacked airplane are referred to as agents of harm, and the people in the building are the patients of harm. Since we compute the confidence interval on AMCE, although some models (such as GPT-4) seem to have a tendency to intervene on the agent of harm, the upper/lower bound of the confidence interval shows there might not be a clear tendency (see Figure A2).

Morality: Personal or Impersonal ForceThis factor captures whether or not the outcome was brought about by using personal force (e.g., pushing a person). Generally, people are less likely to find actions morally permissible that involve personal force. We find that only two models, ChatGPT and GPT-4, align with human intuitions about personal force.

Morality: Means or Side EffectAssessing an agent's causal role is important for moral judgments. Generally, people find actions more morally permissible when they lead to harm as side effects rather than when the harm is necessary to bring out the outcome. This can be illustrated as a diagram in Figure A1. We find that no model exhibits a clear preference for harm as a side effect.

We additionally saw the following larger trends across models:

1. [leftmargin=*]
2. **Non-monotonicity**: The alignment to human biases does not necessarily increase with model size. We speculate that alignment is an area where the inverse scaling law applies (McKenzie et al., 2023).
3. **Heterogeneity**: Interestingly, but perhaps not surprisingly, models that used the same training method and fine-tuned for human preferences do not have the same implicit tendencies - we highlight the difference between Claude-v1 and GPT3.5-turbo.
4. **Self-Tendency vs. Other-Preference**: Humans are ego-centric and often make self-beneficial decisions. However, when asked to judge the behaviors of others, we prefer others to be altruistic. For example, in scenarios involving self-driving cars, Kallioinen et al. (2019) reported a stark contrast between what we tend to do versus what we want other people to do. This difference will make models trained on human preferences through RLHF different from how humans would tend to act in a given situation. As it is more prevalent to use LLMs as proxy humans for data labeling or experiments (Dillion et al., 2023; Gilardi et al., 2023), it is increasingly important to understand and measure these implicit differences.

#### 4.2.3 Other Analyses

Same Model with Different Prompting MethodsWe also investigate how different prompting methods shift a model's implicit tendencies. Prompting methods shifted the model's implicit tendency significantly for some factors (e.g., abnormal sensitivity, benefit self or others) but not much for others (e.g., inevitable or avoidable harm). We report this in Figure A3.

Figure 3: **Moral Implicit Tendencies**. Note that the Left and right figures have different scales.

Predicting Factor-Attribute as Few-shot LearningWe asked LLMs to predict which factor attribute each story contains as a few-shot natural language understanding task. Across all prediction tasks, text-davinci-v2 achieved 85.5% accuracy on causal factors and 70.9% on moral factors. We report more details in Appendix A.10. This shows that even though LLMs and humans do not align, they are very capable of following examples and instructions to annotate stories with factors that are relevant to human judgments.

HallucinationWe conduct a small-scale analysis to investigate whether the tendency difference is due to actual tendency difference or model hallucinations. We annotated 80 examples by sampling 10 examples where 4 models made mistakes across 2 tasks. We prompt the model to explain why they made the choice for the original story. We use the explanation to check if the model hallucinates and has an accurate grasp of the facts represented in the story. A model hallucinates when they re-state the core story in a way that's inconsistent with the facts provided in the story. For example, if an action is not performed by character A, and the model thinks it is performed by character A, we count this as a hallucination. In Table 4, we can clearly see that when a model makes a mistake, smaller models tend to hallucinate more.

We read the model explanations on examples where no hallucination is found, and we conclude that larger models indeed have a different tendency compared to humans in these stories. On moral stories, Claude-v1, GPT3.5-turbo, and GPT-4 all seem to be bound by pre-entered moral principles and choose the same action regardless of the story circumstances (potentially due to the influence of AI constitution; Bai et al., 2022b), while humans tend to consider a few more factors (i.e., nuances in the story). For example, when faced with intricate situations, language models tend to take a very passive stance. This is true even if the action could save a greater number of people by putting a smaller number of people in danger. However, when self-sacrific is in question, the language model is inclined to metaphorically "sacrifice" itself. Our observation is based on a limited set of examples. We leave more extended evaluation to future work.

## 5 Conclusion

We summarized the main findings of 24 cognitive science papers around human intuitions on causal and moral judgments. We collected a dataset of causal and moral judgment tasks and used the scientific findings to enrich the stories with structural annotations. Using conjoint analysis, we compute the average marginal component effect (AMCE) for people's and the models' implicit tendency on the factors important to making causal and moral judgments. We find subtle differences in what humans and models prefer. Our work illuminates the importance of using a richly annotated dataset to understand a model's systematic tendencies, an important step to understanding where models and humans align or misalign.