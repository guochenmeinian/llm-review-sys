# Nitzan Bitton-Guetta\({}^{1}\)** **Aviv Slobodkin\({}^{2}\)** **Aviya Maimon\({}^{2}\)** **Eliya Habba\({}^{3}\)

## Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models**Royi Rassin\({}^{2}\)** **Yonatan Bitton\({}^{4}\)** **Idan Szpektor\({}^{4}\)** **Amir Globerson\({}^{4,5}\)** **Yuval Elovici\({}^{1}\)**

\({}^{1}\)Ben Gurion University \({}^{2}\)Bar-Ilan University

\({}^{3}\)The Hebrew University of Jerusalem \({}^{4}\)Google Research \({}^{5}\)Tel Aviv University

nitzangu@post.bgu.ac.il; yonatanbitton@google.com

https://visual-riddles.github.io/

## Abstract

Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person's discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, ground-truth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models' capabilities in interpreting complex visual scenarios.

Figure 1: Introducing Visual Riddles, designed to test models on their ability to use commonsense, world knowledge, hints, attributions, and factuality in interpreting complex visual cues. This resource aims to enhance models capability to handle nuanced and factual visual scenarios.

Introduction

Humans intuitively utilize commonsense reasoning to interpret complex elements in visual scenes, a skill that current vision and language models frequently lack. For instance, the simple act of a person scratching their arm gains added context when a mosquito is present on a nearby night-stand, as depicted in Fig. 1. While humans easily recognize such contextual nuances, existing image-understanding models struggle to integrate visual cues with world knowledge stemming from cultural aspects, life-experiences, and physical or social knowledge . This gap has spurred the development of various benchmarks like VisIT-Bench  and Encyclopedic VQA , which rely on pre-existing images to formulate challenging questions. While effective, this approach risks using images seen during pre-training of large vision-language models and restricts the scope of scenarios to those already captured, potentially limiting creativity and challenge variety.

To address these shortcoming, we introduce _Visual Riddles_, a benchmark comprising 400 visual riddles, each featuring a question and a synthetic image generated specifically for the challenge. The process of creating a riddle involves designing a scenario with embedded clues that appear as natural elements, then translating this scenario into both an image and a question (SS3). For example, as shown in the top of Fig. 1, a seemingly inconspicuous mosquito becomes a key clue to explain the person's discomfort. This method, going from scenario to image, and not the other way around, not only enhances creative liberty but also broadens the range of everyday situations explored, challenging both humans and models in their interpretative and commonsense reasoning abilities. The benchmark also incorporates textual hints and attributions (Fig. 1) to direct focus to clues and provide external sources of verification for answers, thus expanding the challenge's scope and depth.

Our benchmark's main task (SS4) involves solving riddles in an _open-ended_ visual question answering (VQA) format, which takes as input an image and a question, and expects a free-text answer. This setup evaluates the ability to detect subtle visual cues and apply commonsense reasoning or world knowledge to formulate answers. Additionally, we investigate the impact of including hints and attributions in the input to enhance comprehension.

Yet, as in every QA benchmark, evaluation is a key challenge in this setting. To facilitate scalable research, we introduce two more tasks. The first is a multiple-choice version of the main task, including for the hint- and attribution-assisted variants, allowing for easy accuracy-based automatic scoring. The second task assesses the ability of models to determine the accuracy of open-ended responses in two settings: _reference-free_, where models evaluate responses based solely on the image and the question, and _reference-based_, where the correct answer is also given. This task suggests auto-raters to evaluate our riddles, aiming to advance research on such automatic evaluation methods.

Experimental results (SS5) reveal a significant gap in performance between humans and state-of-the-art vision language models, with the top-performing model, Gemini-Pro-1.5 , only achieving 40% accuracy versus humans' 82%. Surprisingly, the multiple-choice format proved nearly as challenging, yielding only slightly better results than the open-ended task; however, performance showed a significant improvement when auxiliary data, such as hints and attributions, was provided. Automated tests, both reference-free and reference-based, show that Gemini-Pro-1.5, the top auto-rater, matches human judgment 87% of the time in reference-based evaluations, thereby proposing a suitable method for automatically evaluating open-ended answers. We also find that some models fail to respond to visual clues accurately when tasked with evaluating answers' validity (SS6.1). Finally, attempts to reproduce the benchmark's images, rich in nuanced visual clues, using the same prompts and various text-to-image models (SS6.2), result in a mere 15% success rate, showcasing the unique challenges visual riddles present to current generative models.

Overall, the findings suggest that the Visual Riddles poses a significant challenge even to state-of-the-art vision-and-language models, emphasizing the critical need for further developments in commonsense reasoning and world knowledge integration to improve model performance on complex visual riddles. To support future research and model evaluation, we make the Visual Riddles dataset, code, and leaderboard publicly available at https://visual-riddles.github.io/.

## 2 Related Work

Our research is closely linked to commonsense reasoning in multimodal models and the evaluation of factuality in both language-only and multimodal settings, with our benchmark uniquely focusing on fine-grained image understanding that combines commonsense and world knowledge, making direct comparisons challenging.

Commonsense reasoning in multimodal models.Recent progress in vision-language models including models like BLIP2 , Flamingo , LLaVA , GPT4 , and Gemini-Pro . These developments have sparked interest in commonsense reasoning , leading to complex visual reasoning challenges within the vision-and-language domain. These include specialized tests for understanding associations and analogies , interpreting unusual images in WHOOPS! , visual abductive reasoning tasks (e.g., Sherlock; ), multi-modal humor understanding tasks , and world-instructed image-editing tasks requiring commonsense reasoning (e.g., EditWorld; ). Notable benchmarks include VisIT-Bench  where the authors took existing images and generated challenging questions about them, OK-VQA  where questions require world knowledge, and others . However, Visual Riddles distinguishes itself by allowing annotators more creative freedom to devise challenging scenarios and generate corresponding images and questions, rather than restricting them exclusively to what appears in natural images. This also ensures the images have not been previously seen during pretraining.

Factuality evaluation.Evaluating factuality has been a focal point in language-only models, particularly for tasks where outputs rely on grounding texts like summarization or machine translation. This domain utilizes reference-based metrics (e.g., Rouge , BLEU , Bertscore , COMET ) that compare outputs to a reference, alongside reference-free metrics (e.g., SummaC , \(Q^{2}\), TRUE , FiC ) that evaluate outputs against the input texts. Parallel advancements in vision-language tasks have introduced metrics such as Clipscore , AVIBench , and Tifa , with benchmarks like MileBench , SeeTRUE , BLINK , and Vibe-Eval  enhancing the

Figure 2: Overview of the Visual Riddles tasks: (1) Main Task: Solve open-ended questions. (2) Utilizing Hints: Use textual aids to identify key visual clues in riddles. (3) Employing Attributions: Apply web-sourced attributions to improve world-knowledge. (4) Multiple Choice: Select the correct answer to the riddle from five options. (5) Automatic Evaluation: Evaluate open-ended answers in two scenarios— Reference-Free, assessing the correctness of a candidate answer (CA) based only on the visual riddle, and Reference-Based, comparing CAs to the ground truth answer (GTA).

rigor of factuality assessments. Our dataset, Visual Riddles, extends these challenges into the realm of visual commonsense, requiring both deep world knowledge and nuanced commonsense reasoning to interpret complex visual cues. This requirement marks a significant step beyond traditional benchmarks, like Encyclopedic VQA , which predominantly tests factual knowledge against curated text sources. By demanding high sensitivity to visual subtleties alongside robust commonsense analysis, Visual Riddles offers a uniquely stringent test of multimodal model capabilities.

## 3 Data Collection

The Visual Riddles Challenge uses visual clues in images to test common-sense reasoning in vision-and-language models. The goal is to develop a dataset with images that represent ambiguous and culturally rich scenarios. To solve the riddles, models must detect subtle visual clues and engage in reasoning that integrates commonsense and world knowledge. The Visual Riddles benchmark was hand-curated by seven designers, experienced in generating images with text-to-image models. The design process was guided by comprehensive instructions to create visual riddles that were complex enough to challenge models yet solvable by humans. Each riddle consisted of a synthetic image paired with a question and a corresponding ground truth answer (see examples in Fig. 1). To generate high-quality images, the designers had access to advanced text-to-image models, including Midjourney, Ideogram, Canva, Gemini , SDXL , DALL-E 3 , and Stable-Diffusion . The generated images included subtle clues crucial for solving the riddles, and designers were encouraged to embed cultural nuances and their own world knowledge into the riddles. The images were not only photo-realistic but also contained all necessary elements to lead to the correct answer. Designers provided additional hints to the visual clues that guided where to look in the image when trying to solve the riddle (top example in Fig. 1) and, for riddles requiring world knowledge, included attributions to relevant sources (bottom example in Fig. 1). After creation, each riddle was peer-reviewed by at least three other designers to ensure the hint's clarity and the riddle's solvability. Upon approval, the designer drafted a detailed answer that logically explained the solution based on the visual clues. Further elaboration on the image generation process and the instructions for riddle creation are in A.2. The Visual Riddles benchmark is licensed under Apache License 2.0.

Commonsense and World Knowledge Categorization and Difficulty AnalysisFinally, we categorize the instances based on the type of commonsense reasoning and knowledge required, including safety-related knowledge, biological knowledge, cultural knowledge, and many more (see A.3). Each riddle is assigned to one of 16 distinct categories, labeled with the single category that best fits it. Additionally, each riddle is assigned a Difficulty Level Index, which quantifies its complexity, ranging from simple, straightforward clues to obscure ones requiring specialized knowledge.

## 4 Visual Riddles Benchmark

This study introduces three vision-and-language tasks within the Visual Riddles benchmark to evaluate model capabilities. The tasks are: solving open-ended visual riddles, choosing the correct answer from multiple options, and conducting automated evaluations on open-ended riddles. These tasks may incorporate auxiliary information such as hints or attributions to enhance assessment accuracy.

Open-ended VQAIn this task, models are evaluated on their ability to solve visual riddles comprising of an image and a question, requiring not only correct solutions but also detailed explanations. Questions may be binary, requiring a 'yes' or 'no' answer with justification, or _open-ended_, inviting freely-formulated responses to queries like 'why', 'where', or 'how'. These questions require both locating visual clues in the images and integrating these clues with commonsense reasoning and world knowledge to formulate coherent and correct answers. For example, in Fig. 2.1, a good answer to the question _"What is probably the gender of the cat?"_ should reference both the _visual clue_ (the cat's fur color) and the _relevant world knowledge_ (Calico cats are predominantly female).

Multiple-choice VQAWe propose this task as an alternative to the open-ended VQA, which can be evaluated automatically using a simple accuracy metric. It involves selecting the correct answer from five options for a visual riddle comprising an image and a question, as shown in Fig. 2.4. Details on how these candidate answers were collected are provided in SS5.3.

Open-ended VQA Automatic EvaluationIn this task, models are being assessed for their ability to evaluate the accuracy of open-ended responses to visual riddles. As outlined in Fig. 2.5, it has two categories: **(a) Reference-Free**, where the model assesses a candidate answer based only on the image and question; and **(b) Reference-Based**, where it also considers the ground-truth answer to evaluate the candidate's response. This framework aims to identify the best auto-rater for open-ended tasks, supporting automated leaderboard creation. Designed for scalable evaluation of open-ended responses, this task uses vision and language models to judge both Reference-Free and Reference-Based responses. The Reference-Free baseline is included despite available ground truths, as it sometimes outperforms the Reference-Based approach , enhancing our insights into model performance across different contexts.

Auxiliary InformationWe utilized auxiliary information within tasks of our benchmark to assess its impact on models performances. Specifically, we incorporate textual hints and source attributions to enhance the model's ability to solve visual riddles by providing targeted contextual cues. Textual hints are concise directives that focus the model's attention on specific visual clues within the image. For instance, a hint such as _"Look at the colors of the fur"_ in the calico cat example (see Fig. 2.2) effectively directs the model's focus towards the type of the cat. Additionally, attributions provide essential world knowledge through a webpage URL from which we extract all the text content, offering the models the necessary information to solve the visual riddle. For example, providing a text stating that calico cats are _"almost exclusively female"_ (see Fig. 2.3) assists models in more effectively inferring a cat's gender from its fur color.

## 5 Experiments

We evaluate several state-of-the-art publicly accessible vision-and-language models on each of the tasks in Visual Riddles, which we overview in SS4. Then, we describe the experimental setup and results of each of the benchmark tasks, including open-ended VQA (SS5.2), multiple-choice VQA (SS5.3) and open-ended VQA automatic evaluation (SS5.4). For riddles evaluated with auxiliary data, which may have lengthy prompts, we only test models capable of accommodating such lengths.

### Models

We evaluate LLaVA-1.5-7B , LLaVA-1.6-34B , InstructBLIP-7B , GPT4-turbo-preview , Gemini-Pro-Vision , and Gemini-Pro-1.5 . We used the most up-to-date versions of each model, as of May 2024. After submission, we included four additional models: GPT4o , Claude 3.5 Sonnet , Owen-VL-Max , and Molmo-7B , all updated to the end of October 2024. Each experiment required prompts of varying token lengths; notably, the prompts for attribution tasks were particularly lengthy, as they also included the attributing texts. Models selection is

Figure 3: Amazon Mechanical Turk interface for selecting answers to open-ended riddles. Annotators are presented with an image, a question and several candidate answers, including both human-curated and model-generated predictions, and are tasked with identifying the correct responses.

therefore based on their capacity to accommodate the full prompt required for each task, ensuring that inputs are processed fully without truncation. For further details, see A.4.

### Open-ended VQA

We start our investigation by assessing the performance of humans and models on the primary _open-ended_ VQA task, which requires solving riddles based solely on the accompanying images. To that end, we collect human responses to the riddles through crowdsourcing, as well as prompting several vision-language models, and evaluate all responses using human judgement.

Human AnswersWe utilized Amazon Mechanical Turk to collect human open-ended answers for the benchmark's riddles. Qualification test ensured ten reliable workers, annotated each of the 400 riddles (three workers per riddle). During annotation, we instructed annotators to not only answer the question but also provide justifications. Annotators were also encouraged to utilize tools like Google Lens and different search engines to research clues and ensure accurate identification of objects. For example, in Fig. 2.4, annotators which are unfamiliar with the Kakapo bird were advised to use Google Lens to identify it. For more details on the annotation process, including full guidelines and UI screenshot examples, see A.5.

Model AnswersWe extract open-ended answers from the models in SS5.1 (latest configurations). Two baselines are evaluated: large vision and language models (LVLM) generating answers from image-question prompts, and Caption\(\) LLM. For the latter, we extract image captions using Gemini-Pro-1.5 and humans (pre-collected), and generate answers for the riddles from caption-question prompts using the best-performing LVLM model. Further details are available in A.4.

Human Rating of ResponsesWe assess the correctness of human and model answers using three annotators in a multiple-choice selection format. They select responses that match the ground truth, without any hallucinations, based on provided images, questions, and ground-truth answers. The final rating is determined by a majority vote. Annotator agreement, measured by Krippendorff's alpha , reached 79%. An example of human annotation from Visual Riddles is shown in Fig. 3. Further details on annotation guidelines and results can be found in A.6.

ResultsThe results, displayed in Table 1, show that models face significant challenges solving the riddles in the Visual Riddles benchmark. In the LVLM category, Gemini-Pro-1.5 is the best performing model with a score of 40%, followed by GPT-4 at 34% and Gemini-Pro-Vision at 30%, while other models perform below 15%. Human performance, by contrast, reaches 82%, underscoring that **these riddles remain a significant challenge for current models**. In the \(Caption LLM\) category the best model, Human (Oracle)\(\)Gemini-Pro-1.5, shows a 10% gain over the LVLM, yet it still falls short of human performance. This indicates a recognition gap, with Gemini-Pro-1.5 (23%) providing inadequate captions for images requiring subtle clues, unlike human-generated captions (50%). Further, this shortcoming persists as Gemini-Pro-1.5, even with ground truth captions, improves only to 50%, lagging behind the human rate of 82%. Further results on various categories and difficulty index levels are available in A.3.

   &  \\   & Gemini Pro 1.5 & **40** \\  & Gemini Pro Vision & 30 \\ LVLM & GPT4 & 34 \\  & LLaVA-1.6-34B & 15 \\  & LLaVA-1.5-7B & 13 \\  & InstructBlip & 13 \\  Caption\(\) LLM & Gemini Pro 1.5 \(\) Gemini Pro 1.5 & 23 \\  & Human (Oracle) \(\) Gemini Pro 1.5 & **50** \\   & Humans & **82** \\  

Table 1: Human ratings for large vision and language models, caption generation to large language models pipelines, and human evaluators on 400 riddles in the Visual Riddles benchmark.

Caption Quality and Model GapsTo understand the impact of caption quality on model performance in the Caption\(\)LLM setup, we analyzed 200 riddles (50% of the dataset), comparing human-generated (ground truth) captions with model-generated ones. Human captions, provided by riddle creators to generate the images, typically contained the key elements needed to solve the riddles. For example, Fig. 4 shows a riddle involving a 1000-piece Dora the Explorer jigsaw puzzle. The model-generated caption failed to mention crucial details such as the number of pieces, small size, or complexity, making the question "Should I buy this for my toddler?" unanswerable. Our findings show that 97% of human-generated captions included the necessary information, while only 57% of model-generated captions did. This discrepancy highlights that models often miss important details when captioning, limiting their ability to answer riddles effectively. Even when provided with human-generated Oracle captions, models still struggled with reasoning, achieving only a 50% success rate compared to 23% with model-generated captions, as presented in Table 1. This indicates a 27% "visual recognition gap", where models could improve by better recognizing visual cues, and a 32% "reasoning gap", showing the models' difficulty in reasoning even with perfect visual input. These findings underscore the importance of both accurate captioning and improved reasoning capabilities to close the gap between model and human performance.

Tool Usage and Model GapsWe analyzed all data with attribution, comprising 164 riddles (41%) where external tools, such as Google Search or Lens, could potentially aid in solving. The remaining 59% did not necessitate these tools. Model performance was assessed across both categories, revealing a slight difference: models reached 21% accuracy on riddles where external tools might have been useful and 26% on those not requiring such assistance. These findings suggest that the main difficulties for models lie in reasoning and interpreting visual clues rather than depending on external knowledge.

### Multiple-choice VQA

We next evaluate the performance of large vision and language models on the multiple-choice VQA task of the Visual Riddles benchmark, shifting from a generative to a classification task. Each riddle includes an image and a question, along with five answer choices: the correct one from the designer, three incorrect options derived from the human-judgment evaluations of model or human responses to the open-ended riddles (SS5.2), and a "cannot determine" option for ambiguous cases. For the 12% instances where fewer incorrect responses were available (i.e., when most responses by humans and models were correct), we used GPT-4 to generate additional distractors, using two in-context examples of visual riddles with incorrect answers. Finally, accuracy is calculated as the proportion of riddles correctly solved by the models, with random guessing yielding a baseline success rate of 20%.

Figure 4: Comparison of model-generated and human-generated captions that were used in the Caption\(\)LLM setup. ’X’ marks captions where critical details are missing in the model-generated version, while ’V’ marks captions where these details are present.

ResultsThe results in Table 2, indicate that this multiple-choice version is comparably challenging to the open-ended visual riddles, with GPT-4, Gemini-Pro-Vision and Gemini-Pro-1.5 showing the highest accuracies of 45%, 41% and 38%, respectively. Overall, model performance on this task slightly exceeds that on the open-ended task as assessed by human evaluators. Excluding instances where models selected the "cannot determine" option enhances these figures, elevating GPT-4 to 52% and Gemini-Pro-1.5 to 48%. This trend, detailed further in A.7, reveals that models often default to "cannot determine" in the absence of sufficient information. This tendency is mitigated by providing hints and attributions, making GPT-4's accuracy significantly increase to 69% and 82%, respectively. Post-submission results further highlight this trend, with GPT4o leading the models at 55% accuracy overall and reaching 83% with hints.

### Open-ended VQA Automatic Evaluation

The automatic evaluation experiments are designed to assess the capability of vision and language models to accurately judge the correctness of open-ended answers to visual riddles. This evaluation is critical for identifying the most effective approach for automated evaluation, supporting scalable future work on our benchmark, and ensuring integration with the leaderboard intended for widespread use by the research community.

Comparing Auto-Raters to Human RatingsWe evaluate the models in two scenarios: _reference-free_ and _reference-based_, in the first scenario, models independently assess the correctness of an answer based solely on the image and its associated question, and in the second, models are additionally provided with the ground-truth answer along with the candidate answer, challenging them to validate the provided answer against the correct one. Each auto-rater candidate was evaluated on a subset of two responses provided by other models and humans, excluding its own responses from this evaluation. This subset consists of balanced responses, including one correct and one incorrect. If one of these options was not available, two responses from the same category were selected. We tested the candidate auto-rater models on the visual riddles annotated in SS5.2 to determine which model's performance most closely aligns with human judgments. This alignment is crucial for selecting models suitable for evaluative roles in automated systems.

    & **\% Accuracy**\(\) & **\% Cannot** & **\% Accuracy w/o** & **+ Hint**\(\) & **+ Attribution**\(\) \\  & & **Determine** & **Cannot Determine** & & \\  Gemini Pro 1.5 & 38 & 20 & 48 & 66 & 72 \\ Gemini Pro Vision & 41 & 3 & 42 & 62 & - \\ GPT4 & **45** & 12 & 52 & **69** & **82** \\ LLaVA-1.6-34B & 24 & 8 & 26 & 30 & - \\ LLaVA-1.5-7B & 17 & 0 & 17 & 29 & - \\  Claude 3.5 Sonnet & 46 & 4 & 48 & 45 & - \\ GPT4o & **55** & 17 & 67 & **83** & - \\ Qwen-VL-Max & 35 & 3 & 37 & 53 & - \\ Molmo-7B & 34 & 1 & 35 & 42 & - \\   

Table 2: Three types of multiple choice evaluations: overall accuracy, accuracy excluding instances where the model selected “cannot determine”, and accuracy with auxiliary data (hints or attributions).

    & **Judge** & 
 **Accuracy of Judge Prediction** \\ **Compared to Human Rating**\(\) \\  \\  Reference-Based & Gemini Pro 1.5 & **87** \\  & Gemini Pro Vision & 75 \\  & GPT4 & 86 \\  & LLaVA -1.6-34b & 76 \\  & LLaVA -1.5-7b & 70 \\  Reference-Free & Gemini Pro 1.5 & **52** \\  & Gemini Pro Vision & 38 \\  & GPT4 & 51 \\  & LLaVA -1.6-34b & 43 \\  & LLaVA -1.5-7b & 35 \\   

Table 3: Candidate auto-raters performances compared to human rating.

ResultsTable 3 presents the accuracy of the models under two scenarios, with Gemini-Pro-1.5 achieving the highest performance--52% in the reference-free context and 87% in the reference-based context, as measured against human ratings. These results suggest that the reference-free scenario may be less suitable for auto-evaluation, as evidenced by the top score of 52%, indicating a moderate correlation with human judgment. However, the reference-based scenario shows a stronger correlation, with the top two models achieving 87% and 86%. These findings indicate the need for an appropriate judge to assess open-ended answers.

Utilizing the Optimal Auto-Rater to Evaluate Visual RiddlesHaving established Gemini-Pro-1.5 as the best auto-rater, we utilize it in a reference-based setting to assess all models on the three open-ended settings detailed in SS5.2, i.e., the main task, and its hint- and attribution-assisted variants. Table 4 presents the auto-rating accuracy of various models on the open-ended task, with Gemini-Pro-1.5 as the evaluator. Gemini-Pro-1.5 achieves the highest performance, scoring 53% (40% with human rating) on open-ended questions, 62% with hints, and 29% with attributions. These results suggest that models perform better when provided with hints but not with attributions. Hints improve model performance by directing attention to visual clues, but they do not notably enhance reasoning capabilities. Conversely, when attributions are provided, models struggle to filter through irrelevant details, indicating significant challenges in solving open-ended visual riddles compared to human levels, even with auxiliary information. This highlights ongoing difficulties in improving models' visual reasoning capabilities and bridging the gap between human and machine understanding in visual interpretation tasks.

## 6 Analysis

This section explores two key aspects of the Visual Riddles challenge: assessing models' use of visual cues through comparisons between original and modified images, and examining generative models' efficacy in replicating images aligned with our riddle prompts for an automatic generation pipeline.

### Assessing Visual Aspects

of Riddles: Ablation Study with Modified Images

To assess models' preference for specific answers, we selected 72 images from our benchmark and created modified versions, altering visual cues to invalidate ground truth answers. This study explores whether models base their answers solely on text or consider visual clues. We conducted two rounds of testing: one with original images and one with modified versions. In both instances, we presented the model with the question, the original ground truth answer, and asked,"Is the answer correct?" (Fig. 5). Post-image modification, known ground truth answers became incorrect, make the evaluation to a binary assessment. For each model, we calculated the percentage of correctness rates on each type of image and the gap. A high gap indicates that the model identified the answer as correct for

   &  &  &  \\   & **\% Human** & **\% Auto-Rater** & **\% Auto-Rater** & **\% Auto-Rater** \\  & **Rating**\(\) & **Rating**\(\) & **Rating**\(\) & **Rating**\(\) \\  Gemini Pro 1.5 & **40** & **53** & **62** & **29** \\ Gemini Pro Vision & 30 & 34 & 47 & - \\ GPT4 & 34 & 38 & 61 & 25 \\ LLaVA -1.6-34b & 15 & 21 & 16 & - \\ LLaVA -1.5-7b & 13 & 19 & 30 & - \\ Instruct4Blip & 13 & 20 & 28 & - \\  Claude 3.5 Sonnet & - & 39 & - & - \\ GPT4o & - & 50 & - & - \\ Qwen-VL-Max & - & 26 & - & - \\ Molmo-7B & - & 36 & - & - \\  Humans & **82** & **78** & - & - \\  

Table 4: Automatic evaluation of open-ended answers by Gemini-Pro-1.5.

   & **Original** & **Modified** & **Gap\(\)** \\  & **Images\(\)** & **Images\(\)** & **Gap\(\)** \\  Gemini Pro 1.5 & 74 & 14 & **60** \\ Gemini Pro Vision & 86 & 51 & 35 \\ GPT4 & 68 & 15 & 53 \\ Lava-1.6-34b & 93 & 53 & 40 \\ Lava-1.5-7b & 54 & 38 & 16 \\  

Table 5: Percentage of correctness rates of ground truth answers for original and altered images. The ‘Gap’ column quantifies the reduction in performance due to image modifications, illustrating the challenge of context changes for model accuracy.

the original image and incorrect for modified one. Table 5 reveals models' preference for specific answers despite changes in critical image elements and their struggle to identify correct answers, especially with modified images. Gemini-Pro-1.5 excels with a 60% gap, while others below 53%.

### The Visual Riddles Prompt Set: A Challenge for Text-to-Image Models

This section shifts our focus from evaluating VLMs and LLMs to showcasing the unique challenge posed by the Visual Riddles prompts to text-to-image models. Designing images for the Visual Riddles dataset, with their specific and often subtle visual clues, proved surprisingly difficult. To quantify this difficulty, we attempted to reproduce 100 visual riddles using five popular open-source diffusion models: SDXL , SDXL-Turbo , LCM-SDXL , SD-1.4, and SD-2.1 . Each model generated 100 images based on these prompts, amounting to 500 images in total. However, only 61 (12%) successfully matched the prompts. The most successful model was SDXL-Turbo, achieving a 15% success rate. A complete breakdown of the models' performance is available in A.8. Fig. 6 showcasing several models struggling to capture the nuances embedded in the prompt. This low overall success rate underscores the unique challenge presented by the Visual Riddles prompts, establishing them as a valuable resource for evaluating and advancing text-to-image generation, especially for tasks that demand high precision and the ability to render subtle visual features.

## 7 Conclusions and Limitations

Our analysis of the Visual Riddles Challenge shows that state-of-the-art vision-language models face difficulties in interpreting complex visual scenarios that require commonsense reasoning. With an average success rate of 40%, models significantly lag behind human performance, which stands at 82%. This performance gap underscores the ongoing challenge of bridging human and machine understanding in complex visual interpretation tasks. Although Visual Riddles is smaller than other benchmarks, it follows a "quality over quantity" approach, seen in recent benchmarks[51; 14; 16; 5; 52; 53], which emphasize high-quality challenges. Our dataset's size allows for meticulous handcuration to ensure diverse commonsense challenges, focusing on evaluation over training. While we utilized a variety of generative tools to reduce potential biases, this reliance may introduce inherent limitations, and despite efforts to exclude offensive content, some individuals may still find certain riddles inappropriate. Future work could explore automated dataset generation, creating a dedicated training set, and validating Visual Riddles as a robust test set. Generating multiple images for each prompt, coupled with repeated evaluations, would also allow more robust assessments of models' visual reasoning capabilities.

Figure 5: Modified images ablation study: a demonstration of the process where the model evaluates an answer’s validity using two scenarios: one with the original image and another with a modified image that alters the visual clue, affecting the correctness of the original ground truth answer.

Figure 6: Reproducing visual riddles: all models fail to capture the nuance in the description.