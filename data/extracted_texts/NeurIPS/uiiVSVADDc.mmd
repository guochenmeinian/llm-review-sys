# _Annotator_: A Generic Active Learning Baseline for LiDAR Semantic Segmentation

Binhui Xie

Beijing Institute of Technology

binhuixie@bit.edu.cn

&Shuang Li

Beijing Institute of Technology

shuangli@bit.edu.cn

&Qingju Guo

Beijing Institute of Technology

qingjuguo@bit.edu.cn

Chi Harold Liu

Beijing Institute of Technology

chiliu@bit.edu.cn

&Xinjing Cheng

Tsinghua University & Inceptio Technology

cnorbot@gmail.com

###### Abstract

Active learning, a label-efficient paradigm, empowers models to interactively query an oracle for labeling new data. In the realm of LiDAR semantic segmentation, the challenges stem from the sheer volume of point clouds, rendering annotation labor-intensive and cost-prohibitive. This paper presents _Annotator_, a general and efficient active learning baseline, in which a voxel-centric online selection strategy is tailored to efficiently probe and annotate the salient and exemplar voxel girds within each LiDAR scan, even under distribution shift. Concretely, we first execute an in-depth analysis of several common selection strategies such as Random, Entropy, Margin, and then develop voxel confusion degree (VCD) to exploit the local topology relations and structures of point clouds. _Annotator_ excels in diverse settings, with a particular focus on active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). It consistently delivers exceptional performance across LiDAR semantic segmentation benchmarks, spanning both simulation-to-real and real-to-real scenarios. Surprisingly, _Annotator_ exhibits remarkable efficiency, requiring significantly fewer annotations, e.g., just labeling five voxels per scan in the SynLiDAR \(\) SemanticKITTI task. This results in impressive performance, achieving 87.8% fully-supervised performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that _Annotator_ will offer a simple, general, and efficient solution for label-efficient 3D applications.

## 1 Introduction

3D perception and understanding have become indispensable for machines to effectively interact with the real world. LiDAR (Light Detection And Ranging)  is a widely-used methodology for capturing precise geometric information about the environment, spurring significant advancements in areas like autonomous vehicles and robotics . However, semantic segmentation of LiDAR presents an enormous challenge. The high-speed collection of millions of point clouds per second by on-board sensors sharply contrasts with the laborious and cost-prohibitive nature of annotating them. Consider, for instance, the vast number of outdoor scenes an autopilot can encounter, which is practically limitless. Yet, acquiring annotations for these large-scale point clouds entails intensive human labor. This underscores the urgency of establishing a label-efficient learning mechanism capable of boosting performance in the low-data regime  or facilitating the adaptation of models to new domains .

Extensive solutions encompass semi-supervised , weakly-supervised  or self-supervised  learning. Semi- and weakly-supervised learning methods aim to alleviate the annotation burden by harnessing partially labeled or weakly labeled data. In contrast, self-supervised ones learn representations from point clouds via pretext tasks and then transfer to downstream tasks for weight initialization. Although these works offer scalability and practicality for real-world utility, they also confront new challenges, such as variations in LiDAR configurations, sensor biases, and environmental conditions. That is, the majority of prior works has endeavored to in-distribution scenarios, with limited consideration for label-efficient paradigms in out-of-distribution scenarios, especially for sparse outdoor point clouds. Recent efforts turn to large-scale auxiliary datasets  and delve into domain adaptation (DA) algorithms  to significantly reduce the annotation workload under a domain shift. Nevertheless, the performance of these methods still lags behind the fully-supervised approaches. In Figure 1, we provide an intuitive comparison of results across various paradigms. It becomes evident that there is ample room for improvement in the performance of these methods.

To surmount these obstacles and promote performance in the domain of interest, active learning (AL) is being an optimal paradigm . Given the limited annotation budget, a common scenario is that only an unlabeled target domain of large amounts of point clouds is available with the goal to interactively select a minimal subset of data to be annotated to maximally improve the segmentation performance. In reality, this setting faces a significant hurdle known as _cold start problem_: the lack of prior information to guide the initial selection of annotated data. A recent work has explored the impact of seeding strategies on the performance of AL methods . Differently, we put forward a new path to access an auxiliary model via pre-training on the open-access auxiliary (source) dataset. This auxiliary model serves as a warm-up stage, allowing for smart target data selection for initial annotation. We formulate this new setting as active source-free domain adaptation, termed ASFDA. Take a further step, drawing inspiration from recent trends in 2D images , we delve into the third setting, active domain adaptation (ADA) for semantic segmentation of 3D point clouds. In this setting, a labeled auxiliary dataset is available, and the objective is to select target instances for annotating and learn a model with higher segmentation performance on the target test set.

Overall, in this work, we benchmark three distinct active learning settings for LiDAR semantic segmentation and deliver a simple and general baseline, _Annotator_, as illustrated in Figure 2. Borrowing the idea of modeling and computational techniques in geometry processing, we introduce a voxel-centric selection strategy dedicated to point clouds. Specifically, an input LiDAR scene is first voxelized into voxel grids, with a large voxel size to expand the local areas during the selection process. After obtaining final network predictions, importance estimation is carried out for each voxel grid using several common strategies such as Random, the softmax entropy (Entropy), and the margin between highest softmax scores (Margin). But considering only uncertainty for selection would be suboptimal . Therefore, we introduce the concept of voxel confusion degree (VCD), which takes into account nearby predictions, capturing diversity and redundancy within a voxel grid. VCD enables the exploitation of local topology relations and point cloud structures. As a result, VCD can represent both uncertainty and diversity of a voxel grid in the LiDAR scene. In each active round, we query the top one voxel grid within each scan for annotation until the budget is exhausted. Despite the simplicity of our _Annotator_, it achieves performance on par with the fully-supervised counterpart requiring 1000\(\) fewer annotations and significantly outperforms all prevailing acquisition strategies.

Figure 1: **Performance_vs._ annotated proportion** on SemanticKITTI val of existing label-efficient LiDAR segmentation paradigms including domain adaptation (\(\)) , weakly- (\(\))  and semi-supervised (\(\))  learning. As a reference, fully supervised counterpart (\(\)) is reported as well. _Annotator_ (\(\)) attains excellent balance between performance and annotation cost.

The contribution of this paper can be summarized in three aspects. _First_, we present a voxel-centric active learning baseline that significantly reduces the labeling cost and effectively facilitates learning with a limited budget, achieving near performance to that of fully-supervised methods with 1000\(\) fewer annotations. _Second_, we introduce a label acquisition strategy, the voxel confusion degree (VCD), which is more robust and diverse to select point clouds under a domain shift. _Third_, _Annotator_ is generally applicable for various network architectures (voxel-, range- and bev-views), settings (in-distribution and out-of-distribution), and scenarios (simulation-to-real and real-to-real) with consistent gains. We hope this work could lay a solid foundation for label-efficient 3D applications.

## 2 A Generic Baseline

### Preliminaries and overview

**Problem setup.** In the context of LiDAR semantic segmentation, a LiDAR scan is made of a set of point clouds and let \(X^{N 4}\,,Y^{N}\) respectively denote \(N\) points and the corresponding labels. \(\) is a predefined semantic class vocabulary \(=\{1\,,...\,,K\}\) of \(K\) categorical labels. Each point \(x_{i}\) in \(X\) is a \(1 4\) vector with a 3D Cartesian coordinate relative to the scanner \((a_{i}\,,b_{i}\,,c_{i})\) and an intensity value of returning laser beam. Our baseline works in the following settings: active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). First, we are given an unlabeled target domain \(^{t}=\{X^{t} X^{a}\}\), where \(X^{t}\) denotes unlabeled target point clouds and \(X^{a}\) denotes the selected points to be annotated and is initialized as empty set, i.e., \(X^{a}=\). Next, for ASFDA and ADA, a labeled source domain \(^{s}=\{X^{s},Y^{s}\}\) can be utilized only in pre-training stage and anytime respectively. Ultimately, given a limited budget, our goal is to iteratively select a subset of data points from \(^{t}\) to annotate until the budget is exhausted, all the while catching up with the performance of the fully-supervised model.

**Overview.** Figure 2 displays an overview of _Annotator_, which is a label-efficient baseline for LiDAR semantic segmentation. It is composed of two parts: 1) a generalist _Annotator_ which contains a voxelization process to get voxel grids and an active function with online selection for picking the most valuable voxel grid of each input scan in each active round; 2) the pipelines of distinct active

Figure 2: **An illustration of _Annotator_. _Annotator_ is a new active learning baseline with broad applicability, capable of interactively querying a tiny subset of the most informative new (target) data points based on available inputs without task-specific designs. This includes (i) only unlabeled new (target) data being available (active learning, AL); (ii) access to an auxiliary (source) pre-trained model (active source-free domain adaptation, ASFDA); and (iii) availability of labeled source data and unlabeled target data (active domain adaptation, ADA). Remarkably, _Annotator_ attains excellent results not only in in-domain settings but also manifests adaptive transfer to out-of-domain settings.**

learning settings are described. For AL, we interactively select a subset of voxels from the current scan to be annotated and train the network with these sparse annotated voxel grids. In the case of ASFDA, we begin by pre-training a network on the source domain through standard supervised learning. This warm-up network then serves as a strong initialization to aid the initial selection. As for ADA, except for the pre-training stage, we also make use of annotated source domain to promote the selection in each round and facilitate domain alignment. In the following, we will detail why we select salient and exemplar data points from a voxel-centric perspective and how to address _cold start problem_ via an auxiliary model. After that, overall objectives for all three settings are elaborated.

### A generalist _Annotator_

In this section, we proposed a general active learning baseline called _Annotator_. The core idea is to select salient and exemplar voxel grids from each LiDAR scan. It's important to note that previous researches have proposed frame-based [13; 94], region-based , and point-based  selection strategies. The first two usually require an offline stage, which may be infeasible at large scales. The last one is costly due to the sparsity of outdoor point clouds. By contrast, our voxel-centric selection focuses on querying salient and exemplar areas and annotating all points within those areas. This approach is more efficient and flexible. Moreover, it can be seamlessly applied to various network architectures, including voxel-, range- and bev-views, as demonstrated in the experiment section.

To implement it, we begin with the voxelization process as introduced in [9; 102]. Each input LiDAR scan \(X\) is transformed into a 3D voxel grid set \(V\). This process involves sampling the continuous 3D input space into discrete voxel grids, where points falling into the same grid are merged. Each voxel grid serves as a selection unit. Mathematically, for a point \(x_{i} X\), the corresponding voxel grid coordinate is \((a_{i}^{v},b_{i}^{v},c_{i}^{v})=(a_{i}\,,b_{i}\,,c_{i})/\), with \(\) denoting predefined voxel size. In our experiments, we have found that using a large voxel grid is more robust against noise and sparsity. Unless otherwise specified, we use \(_{1}=0.05\) for training and \(_{2}=0.25\) for the selection process.

**Selection strategies.** For each voxel grid \(v_{j} V\), we assess its importance and select the best voxel grid per LiDAR scan in each active round. Initially, we employ a Random selection strategy. Subsequently, we explore softmax entropy (Entropy) and the margin between highest softmax scores (Margin). It's essential to note that while these common selection strategies are not technical contributions, they are necessary to build our baseline. Detailed calculations are provided below.

* **Random**: randomly select a target voxel grid \(v_{j}\) from \(V\) to be annotated in each round.
* **Entropy**: first calculate the softmax entropy of each point \(x_{i} v_{j}\) and then adopt the maximum value as the Entropy score of this grid, i.e., \((v_{j})=_{x_{i} v_{j}}-p_{i} p_{i}\), where \(p_{i}\) is the softmax score of point \(x_{i}\). The voxel grid with the highest Entropy score is selected in each scan.
* **Margin**: first calculate the margin between highest softmax score of each point \(x_{i} v_{j}\) and then adopt the maximum value as the Margin score of this grid, i.e., \((v_{j})=_{x_{i} v_{j}}\,((p_{i})- 2(p_{i}))\), where \( 2()\) is the second-largest value operator. In each scan, the voxel grid with the lowest Margin score is chosen.

**The VCD strategy.** Our voxel confusion degree (VCD) is motivated by an important observation: the previously mentioned selection strategies become less effective when models are applied in new domains due to mis-calibrated uncertainty estimation. Therefore, the VCD is designed to estimate category diversity within a voxel grid rather than uncertainty, making it more robust under domain shift. Here's how it works: we begin by obtaining pseudo label \(_{i}\) for each point \(x_{i}\). Next, we divide points within \(v_{j}\) into \(K\) clusters: \(v_{j}^{<k>}=\{x_{i}^{<k>}|x_{i} v_{j}\,,_{i}=k\}\). This allows us to collect statistical information about the categories present in the voxel grid. With this information, we calculate VCD to assess the significance of voxel grids as follows:

\[(v_{j})=-_{k=1}^{K}^{<k>}|}{|v_{j}|}^{<k>}|}{|v_{j}|}\,,\]

where \(||\) denotes the number of points in a set. Finally, voxel grid with the highest VCD score is selected in each scan. The insight is that a higher score indicates a greater category diversity within a voxel, which would be beneficial for model training once being annotated. In all experiments, _Annotator_ is equipped with VCD by default, and the results indicate the superiority of VCD strategy.

**Making a good first impression.** To avoid the _cold start problem_ mentioned before, we introduce a warm start mechanism that pre-trains an auxiliary model with an auxiliary (source) dataset, and then it is used to select voxel grids in the first round. This warm start stage is applied in ASFDA and ADA.

_Discussion: balancing annotation cost and computation cost._ Our primary focus is on reducing annotation cost while maintaining performance comparable to fully-supervised approaches. Let's consider simulation-to-real tasks as an example. The simplest setup involves active learning within the real dataset. However, this setup yields less satisfactory results due to the _cold-start problem_: the lack of prior information for selecting an initial annotated set. To address this, we utilize a synthetic dataset to train an auxiliary model in a brief warm-up stage, enabling smarter data selection in the first round. Importantly, this warm-up process is short, conducted only once, and results in minimal costs (both annotation and computation). For a detailed analysis, please refer to Appendix B.1.

### Optimization

The overall loss function is the standard cross-entropy loss, which is defined as:

\[_{ce}(X)=_{x_{i} X}_{k=1}^{K}-y_{i}^{k} p _{i}^{k}\,\]

where \(K\) is the number of categories, \(y_{i}\) is the one-hot label of point \(x_{i}\) and \(p_{i}^{k}\) is the predicted probability of point \(x_{i}\) belonging to category \(k\). Hereafter, for AL, the objective is \(_{}_{ce}(X^{a})\); for ASFDA, the objective is \(_{_{s}}_{ce}(X^{a})\); for ADA, the objective is \(_{_{s}}_{ce}(X^{s})+_{ce}(X^{a})\). Here, \(\) and \(_{s}\) denote training from scratch and training from the source pre-trained model, respectively.

## 3 Experiments

In this section, we conduct extensive experiments on several public benchmarks under three active learning scenarios: (i) AL setting where all available data points are from unlabeled target domain; (ii) ASFDA setting where we can only access a pre-trained model from the source domain; (iii) ADA setting where all data points from source domain can be utilized and a portion of unlabeled target data is selected to be annotated. We first introduce the dataset used in this work and experimental setup and then present experimental results of baseline methods and extensive analyses of _Annotator_.

### Experiment setup

**Datasets.** We build all benchmarks upon SynLiDAR , SemanticKITTI , SemanticPOSS , and nuScenes , constructing two simulation-to-real and two real-to-real adaptation scenarios. SynLiDAR  is a large-scale synthetic dataset, which has 198,396 LiDAR scans with point-level segmentation annotations over 32 semantic classes. Following , we use 19,840 point clouds as the training data. SemanticKITTI (KITTI)  is a popular LiDAR segmentation dataset, including 2,9130 training scans and 6,019 validation scans with 19 categories. SemanticPOSS (POSS)  consists of 2,988 real-world scans with point-level annotations over 14 semantic classes. As suggested in , we use the sequence \(03\) for validation and the remaining sequences for training. nuScenes  contains 19,130 training scans and 4,071 validation scans with 16 object classes.

**Class mapping.** To ensure compatibility between source and target labels across datasets, we perform class mapping. Specifically, we map SynLiDAR labels into 19 common categories for SynLiDAR \(\) KITTI and 13 classes for SynLiDAR \(\) POSS. Similarly, we map labels into 7 classes for KITTI \(\) nuScenes and nuScenes \(\) KITTI. We refer readers to Appendix A.1 for detailed class mappings.

**Implementation details.** We primarily adopt MinkNet  and SPVCNN  as the segmentation backbones. Note that, all experiments share the same backbones and are within the same codebase, which are implemented using PyTorch  on a single NVIDIA Tesla A100 GPU. We use the SGD optimizer and adopt a cosine learning rate decay schedule with initial learning rate of \(0.01\). And the batch size for both source and target data is \(16\). For additional details, please consult Appendix A.2. Finally, we evaluate the segmentation performance before and after adaptation, following the typical evaluation protocol  in LiDAR domain adaptive semantic segmentation [29; 31; 55; 82; 83].

### Experimental results

Quantitative results summary.We initially evaluate _Annotator_ on four benchmarks and two backbones while adhering to a fixed budget of selecting and annotating five voxel grids in each scan.

The results in Table 1 and Table 2 paint a clear picture overall: all baseline methods achieve significant improvements over the Source-Only model, especially for _Annotator_ with VCD strategy, underscoring the success of the proposed voxel-centric online selection strategy. In particular, _Annotator_ achieves the best results across all simulation-to-real and real-to-real tasks. For SynLiDAR \(\) KITTI task, _Annotator_ achieves 87.8% / 88.5% / 94.4% fully-supervised performance under AL / ASFDA / ADA settings respectively. For SynLiDAR \(\) POSS task, they are 79.0% / 85.0% / 91.7% respectively. On the task of KITTI \(\) nuScenes, they are 85.3% / 87.8% / 92.0% respectively. And on the task of nuScenes \(\) KITTI, they are 92.2% / 90.3% / 98.2%, respectively. It is also clear that domain shift between simulation and real-world is more significant than those between real-world datasets. Therefore, simulation-to-real tasks show poorer performance. Further, we compare _Annotator_ with additional AL algorithms and extend it to indoor semantic segmentation in Appendix B.2 and B.3.

Per-class performance.To sufficiently realize the capacity of our _Annotator_, we also provide the class-wise IoU scores on two simulation-to-real tasks (Table 3 and Table 4) for different algorithms and comparison results with state-of-the-art DA methods . Other results of the remainder

  &  &  \\ Method & SynLiDAR \(\) & KITTI SynLiDAR \(\) & POSS & KITTI \(\) & nuScenes nuScenes \(\) KITTI \\ Source-/Target-Only & 22.0 / 61.1 & 30.4 / 56.7 & 28.4 / 82.5 & 34.6 / 83.3 \\  Random & 35.3 / 36.3 / 45.3 & 27.4 / 30.9 / 43.4 & 66.0 / 67.5 / 71.9 & 70.9 / 69.7 / 74.7 \\ Entropy  & 39.8 / 49.6 / 50.1 & 42.8 / 45.5 / 49.9 & 59.7 / 60.3 / 73.1 & 70.7 / 69.1 / 74.0 \\ Margin  & 46.9 / 44.3 / 49.0 & 41.6 / 44.1 / 46.9 & 60.2 / 59.2 / 71.4 & 73.1 / 70.3 / 76.7 \\ _Annotator_ & **53.7 / 54.1 / 57.7** & **44.9 / 48.2 / 52.0** & **70.4 / 72.4 / 75.9** & **76.8 / 75.3 / 81.8** \\ 

Table 1: Quantitative summary of all baselines’ performance based on MinkNet  over various LiDAR semantic segmentation benchmarks using only 5 voxel grids. Source-/Target-Only correspond to the model trained on the annotated source/target dataset which are considered as lower/upper bound. Note that results are reported following the order of AL / ASFDA / ADA in each cell.

 Model &  Model \\ Source-Only \\  }} &  \\ 50.4 \\  }} &  \\ 50.4 \\  }} &  \\ 50.5 \\  }} &  \\ 50.

tasks and backbones are listed in Appendix B.4. It is noteworthy that _Annotator_ under any active learning settings significantly outperform DA methods with respect to some specific categories such as "traf.", "pole", "garb." and "cone" etc. These results also showcase the class-balanced selection of the proposed _Annotator_, which is testified in Figure 6 as well.

Results with varying budgets.We investigate the impact of varying budgets and compare the performance with baseline methods, as illustrated in Figure 3, Figure 4 and Figure 5. A consistent

  & Model & car & bike & pers. & rider & grou. & buil. & fence & plants & trunk & pole & traf. & garb. & cone & mIoU \\  & Source-Only & 44.7 & 1.9 & 33.5 & 38.3 & 77.0 & 54.2 & 30.3 & 63.8 & 22.0 & 12.9 & 0.4 & 11.2 & 4.7 & 30.4 \\   & CRST  & 22.0 & 6.8 & 22.5 & 31.8 & 60.3 & 58.2 & 9.1 & 63.2 & 18.9 & 41.6 & 1.9 & 13.5 & 1.0 & 27.1 \\  & ST-PCT  & 27.8 & 6.6 & 28.9 & 34.8 & 63.9 & 64.1 & 12.1 & 63.7 & 18.6 & 41.0 & 4.9 & 16.6 & 1.6 & 29.6 \\  & CoSMix  & 36.2 & 10.6 & 55.8 & 51.4 & 78.7 & 66.2 & 24.9 & 71.3 & 23.5 & 34.2 & 22.5 & 28.9 & 20.4 & 40.4 \\  & PolarMix  & 25.0 & 10.7 & 32.6 & 39.1 & 79.0 & 44.8 & 23.8 & 64.2 & 11.9 & 29.6 & 5.8 & 15.3 & 13.3 & 30.4 \\   & Random & 24.0 & 47.8 & 28.9 & 0.1 & **79.3** & **66.7** & 27.7 & **76.4** & 0.1 & 5.5 & 0.2 & 0.0 & 0.0 & 27.4 \\  & Entropy  & 37.8 & 39.6 & **58.9** & 45.2 & 75.2 & 56.3 & 38.6 & 69.7 & **39.3** & 23.7 & **36.1** & 1.6 & **34.2** & 42.8 \\  & Margin  & 30.1 & 44.2 & 55.3 & 46.8 & 79.2 & 63.7 & 44.0 & 74.3 & 34.1 & 21.3 & 34.3 & 9.7 & 1.2 & 41.6 \\  & _Annotator_ & **41.0** & **50.1** & 49.3 & **52.0** & 78.5 & 66.4 & **56.4** & 73.4 & 31.1 & **29.6** & 34.5 & **15.7** & 6.2 & **44.9** \\   & Random & 32.2 & 46.4 & 37.9 & 0.4 & 79.2 & **69.8** & 33.0 & **77.7** & 17.4 & 5.6 & 2.8 & 0.0 & 0.0 & 30.9 \\  & Entropy  & 32.1 & 46.5 & **65.7** & 58.0 & 74.4 & 62.9 & 45.5 & 69.6 & **41.5** & **34.5** & 33.7 & 13.2 & **14.3** & 45.5 \\  & Margin  & 30.2 & 47.6 & 59.5 & 44.5 & 79.7 & 66.8 & 51.7 & 73.5 & 28.6 & 30.1 & **35.2** & **25.2** & 0.1 & 44.1 \\  & _Annotator_ & **56.2** & **54.2** & 63.6 & **58.7** & **80.9** & 64.6 & **58.1** & 78.34 & 37.8 & 26.3 & 34.0 & 6.3 & **11.9** & **48.2** \\   & Random & 65.0 & 10.9 & 99.3 & 54.3 & 58.6 & 70.0 & **54.2** & 63.9 & 39.6 & **39.8** & 20.8 & 27.8 & 0.0 & 43.4 \\  & Entropy  & 53.3 & **29.1** & 62.9 & 52.7 & **80.3** & **71.9** & 48.2 & **72.3** & 38.9 & 30.0 & 27.6 & 44.2 & 37.8 & 49.9 \\  & Margin  & 61.3 & 25.2 & 60.6 & **56.2** & 79.6 & 54.2 & 46.6 & 66.7 & 38.1 & 29.2 & 30.8 & 40.8 & 20.4 & 46.9 \\  & _Annotator_ & **67.4** & 18.0 & **64.0** & 52.0 & 78.5 & 61.5 & 1.5 & 68.6 & **48.5** & 23.7 & **37.9** & **50.8** & **43.8** & **52.0** \\   & Target-Only & 73.7 & 60.4 & 68.6 & 62.2 & 81.7 & 79.2 & 60.8 & 78.9 & 36.5 & 31.2 & 44.1 & 12.9 & 46.6 & 56.7 \\ 

Table 4: Per-class results on task of SynLiDAR 

Figure 4: Active source-free domain adaptation results on various benchmarks varying active budget.

Figure 5: Active domain adaptation results on various benchmarks varying active budget.

[MISSING_PAGE_FAIL:8]

frequencies of the SemanticPOSS train in Figure 6. As expected, we clearly see that the true distribution is exactly a long-tail distribution while _Annotator_ is able to pick out more voxels that contain rare classes. Particularly, it asks labels for more annotations of "rider", "pole", "trunk", "traf.", "grab." and "cone". This, along with the apparent gains in these classes in Table 4, confirms the diversity and balance of voxel grids selected by _Annotator_.

Qualitative results.Figure 7 visualizes segmentation results for Source-/Target-Only, our _Annotator_ under AL, ASFDA, and ADA approaches on SemanticKITTI val. The illustration demonstrates the ability of _Annotator_ to enhance predictions not only in distant regions but also to effectively eliminate false predictions across a wide range of directions around the center. By employing voxel-centric selection, _Annotator_ successes in enhancing segmentation accuracy even when faced with extremely limited annotation availability. More qualitative results are shown in Appendix B.5.

### Limitations

Currently, _Annotator_ has two main limitations. First, high annotation cost and potential biases. _Annotator_ has made substantial strides in enhancing LiDAR semantic segmentation with human involvement. Nonetheless, the annotation cost remains a challenge. It's imperative to acknowledge the existence of label and sensor biases, which can be a safety concern in real-world deployments. Second, expansion beyond semantic segmentation. _Annotator_ current focus on LiDAR semantic segmentation represents a significant limitation in fully realizing its potential. In the future work, we plan to extend _Annotator_ to other 3D tasks, such as LiDAR object detection. This may involve two key changes: i) shifting to frame-level selection; ii) reformulating the VCD strategy to consider the diversity for each box annotation.

## 4 Related Work

LiDAR perception.Deep learning has made LiDAR perception tasks such as classification [17; 49; 70; 98] and detection [7; 30; 65; 93] easy to solve, allowing deployment in outdoor scenarios. Differently, LiDAR semantic segmentation [3; 23; 32; 33; 40; 45; 46], receiving a class label for each point, is an indispensable technology to understand a scene that is beyond the scope of modern object detectors . There exist various techniques to segment the 3D LiDAR point clouds, e.g., point [32; 45; 79], voxel [18; 39; 103], range [80; 81], bird's eye , and multiple view [68; 89; 96] methods. As the best approaches for LiDAR perception are typically trained under full supervision, which can be costly more than capturing data itself, several methods resort to more frugal learning techniques , such as semi- [29; 31], weak- [22; 34] and self-supervision [73; 101], zero-shot  and few-shot  learning and, as studied here, active learning  and domain adaptation .

Active learning for LiDAR point clouds.To avoid the burden of complete point cloud annotation, these methods iteratively select and request the most exemplar scans , regions , points , or

Figure 7: **Visualization of segmentation results for the task SynLiDAR \(\) KITTI using MinkNet . Each row shows results of Ground-Truth, Target-Only, Source-Only, our _Annotator_ under AL, ASFDA, and ADA scenarios one by one. Best viewed in color.**

boxes  to be labeled during the network training. Most selection strategies lean on uncertainty  or diversity  criteria. Uncertainty sampling can be measured over each point prediction scores of the model, e.g., softmax entropy  or the margin between the two highest scores , to select the most confusing of the current model. For example, Hu _et al._ estimate the inconsistency across frames to exploit the inter-frame uncertainty embedded in LiDAR sequences. On the other side, diversity sampling has been ensured by selecting core sets . Leveraging the unique geometric structure of LiDAR point clouds, Liu _et al._ partition the point could into a collection of components then annotate a few points for each component. Recently, the need for an initially annotated fraction of the data to bootstrap an active learning method has been investigated , which is termed as cold start problem. In this work, we show that a smart selection of the first set of data with the aid of an auxiliary model can boost all baseline methods drastically.

Domain adaptation for LiDAR point clouds.To tackle the sensor-bias problem encountered in LiDAR deployment, a large body of literature on domain adaptation (DA)  has been developed. These methods aim to overcome the challenges posed by variations in data collection, sensor characteristics, and environmental conditions, enabling machines to perceive the real world more accurately and reliably. To name a few, Kong _et al._ explore cross-city adaptation for uni-modal LiDAR segmentation. Rochan _et al._ propose a self-supervised adaptation technique with gated adapters. Saltori _et al._ mitigate the domain shift by creating two new intermediate domains via sample mixing. Similarly, with the intermediate domain, Ding _et al._ propose a data-oriented framework with a pretraining and a self-training stage for 3D indoor scenes. Despite the significant progress made in DA, the label scarcity of target domain severely handicaps its utility as the performance of such models often lags far behind the supervised learning counterparts. With this consideration, given an acceptable annotation budget, we explore a simple annotating strategy to assist adaptation process and significantly boost the performance of target domain.

Up to now, active learning coupled with domain adaptation has great practical significance . Nevertheless, rather little work has been done to consider the problem in 3D domains. A recent effort, UniDA3D , effectively tackles domain adaptation and active domain adaptation tasks for 3D semantic segmentation. UniDA3D employs a unified multi-modal sampling strategy, selecting informative pairs of 2D-3D data from both source and target domains through a domain discriminator, primarily for ADA tasks. The primary distinction is that our _Annotator_ serves as a benchmark for active learning, active source-free domain adaptation, and active domain adaptation tasks, delivering a simple and general AL algorithm for LiDAR point clouds. _Annotator_ focuses on enabling AL in both in-distribution and out-of-distribution scenarios. In contrast, UniDA3D places a greater emphasis on adaptation tasks. On the other hand, _Annotator_ minimizes human labor in a new domain, regardless of the availability of samples from an auxiliary domain. Methodically, _Annotator_ adopts a voxel-centric representation for structured LiDAR data, which is different from the scan-based representation in UniDA3D. Furthermore, _Annotator_ is more efficient than UniDA3D in terms of both computation and annotation cost.

## 5 Conclusion

In this work, we present _Annotator_, a generalist active learning baseline, to tackle LiDAR semantic segmentation under three distinct label-efficient settings: active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). _Annotator_ harnesses the power of a purpose-designed voxel confusion degree selection strategy, enabling it to make optimal use of limited budgets while achieving efficient selection and effective performance. Experiments conducted on widely-used simulation-to-real and real-to-real LiDAR semantic segmentation benchmarks demonstrate a substantial performance improvement. Looking forward, we believe the effectiveness and simplicity of _Annotator_ has the potential to serve as a powerful tool for label-efficient 3D applications.