# BOLD: Boolean Logic Deep Learning

Van Minh Nguyen

Van Minh Nguyen1

Cristian Ocampo

Aymen Askri

Louis Leconte

Ba-Hien Tran

Mathematical and Algorithmic Sciences Laboratory,

Huawei Paris Research Center, France

vanminh.nguyen@huawei.com

###### Abstract

Computational intensiveness of deep learning has motivated low-precision arithmetic designs. However, the current quantized/binarized training approaches are limited by: (1) significant performance loss due to arbitrary approximations of the latent weight gradient through its discretization/binarization function, and (2) training computational intensiveness due to the reliance on full-precision latent weights. This paper proposes a novel mathematical principle by introducing the notion of Boolean variation such that neurons made of Boolean weights and/or activations can be trained --for the first time-- natively in Boolean domain instead of latent-weight gradient descent and real arithmetic. We explore its convergence, conduct extensively experimental benchmarking, and provide consistent complexity evaluation by considering chip architecture, memory hierarchy, dataflow, and arithmetic precision. Our approach achieves baseline full-precision accuracy in ImageNet classification and surpasses state-of-the-art results in semantic segmentation, with notable performance in image super-resolution, and natural language understanding with transformer-based models. Moreover, it significantly reduces energy consumption during both training and inference.

## 1 Introduction

Deep learning  has become the _de facto solution_ to a wide range of tasks. However, running deep learning models for _inference_ demands significant computational resources, yet it is just the tip of the iceberg. _Training_ deep models is even much more intense. The extensive literature on this issue can be summarized into four approaches addressing different sources of complexity. These include: _(1)_ model compression, pruning  and network design  for large model dimensions; _(2)_ arithmetic approximation  for intensive multiplication; _(3)_ quantization techniques like post-training , quantization-aware training , and quantized training to reduce precision ; and _(4)_ hardware design  to overcome computing bottleneck by moving computation closer to or in memory.

Aside from hardware and dataflow design, deep learning designs have primarily focused on the number of compute operations (ops), such as flops or bops, as a complexity measure  rather than the consumed energy or memory, and particularly in inference tasks. However, it has been demonstrated that ops alone are inadequate and even detrimental as a measure of system complexity. Instead, energy consumption provides a more consistent and efficient metric for computing hardware[94; 95; 108; 92]. Data movement, especially, dominates energy consumption and is closely linked to system architecture, memory hierarchy, and dataflow [57; 89; 110; 19]. Therefore, efforts aimed solely at reducing ops are inefficient.

Quantization-aware training, notably binarized neural networks (bnns) [24; 47], have garnered significant investigation [see, e.g. 83; 34], and references therein]. bnns typically binarize weights and activations, forming principal computation blocks in binary. They learn binary weights, \(_{}\), through _full-precision (fp) latent weights_, \(_{}\), leading to no memory or computation savings during training. For example, a binarized linear layer is operated as \(s=_{}^{}_{}\), where \(s\) is the output and \(\) is a fp scaling factor, \(_{}=(_{})\), and \(_{}=(_{})\) is the binarized inputs. The weights are updated via common gradient descent backpropagation, i.e. \(_{}=(_{}- _{_{}})\) with a learning rate \(\), and fp gradient signal \(_{_{}}\). Gradient approximation of binarized variables often employs a differentiable proxy of the binarization function \(\), commonly the identity proxy. Various approaches treat bnn training as a constrained optimization problem [7; 2; 3; 65], exploring methods to derive binary weights from real-valued latent ones. bnns commonly suffers notable accuracy drops due to reduced network capacity and the use of proxy fp optimizers  instead of operating directly in binary domain [83; 77; 36]. Recent works mitigate this by incorporating multiple fp components in the network, retaining only a few binary dataflows . Thus, while binarization aids in reducing inference complexity, it increases network training complexity and memory usage.

In contrast to binarizing fp models like bnns, designing native binary models not relying on fp latent weight has been explored. For example, Expectation Backpropagation , although operating on full-precision training, was proposed for this purpose. Statistical physics-inspired [8; 9] and Belief Propagation  algorithms utilize integer latent weights, mainly applied to single perceptrons, with unclear applicability to deep models. Evolutionary algorithms [75; 50] are also an alternative but encounter performance and scalability challenges.

**Summary:** No scalable and efficient algorithm currently exists for _natively_ training deep models in binary. The challenge of significantly reducing the training complexity while maintaining high performance of deep learning models remains open.

**Contributions.** For the aforementioned challenge, we propose a novel framework -- _Boolean Logic Deep Learning_ (b\(\)ld) -- which relies on Boolean notions to define models and training:

* We introduce the notion of variation to the Boolean logic and develop a new mathematical framework of function variation (see SS 3.2). One of the noticeable properties is that Boolean variation has the chain rule (see Theorem 3.12) similar to the continuous gradient.
* Based on the proposed framework, we develop a novel Boolean backpropagation and optimization method allowing for a deep model to support native Boolean components operated solely with Boolean logic and trained directly in Boolean domain, eliminating the need for gradient descent and fp latent weights (see SS 3.3). This drastically cuts down memory footprint and energy consumption during _both training and inference_ (see, e.g., Fig. 1).
* We provide a theoretical analysis of the convergence of our training algorithm (see Theorem 3.17).
* We conduct an extensive experimental campaign using modern network architectures such as convolutional neural networks (cnns) and Transformers  on a wide range of challenging tasks including image classification, segmentation, super-resolution and natural language understanding (see SS 4). We rigorously evaluate analytically the complexity of b\(\)ld and bnns. We demonstrate the superior performance of our method in terms of both accuracy and complexity compared to the state-of-the-art (see, e.g., Table 4, Table 5, Table 7).

## 2 Are Current Binarized Neural Networks Really Efficient?

Our work is closely related with the line of research on binarized neural networks (bnns). The concept of bnns traces back to early efforts to reduce the complexity of deep learning models. binaryconnect  is one of the pioneering works that introduced the idea of binarizing fp weights during training, effectively reducing memory footprint and computational cost. Similarly, binarnet, xnor-net extended this approach to binarize both weights and activations, further enhancing the efficiency of neural network inference. However, these early bnns struggled with maintaining accuracy comparable to their full-precision counterparts. To address this issue,

[MISSING_PAGE_FAIL:3]

Summary.Table 1 shows key characteristics of sota bnn methods. These methods will be considered in our experiments. Notice that all these techniques indeed have to involve operations on fp latent weights during training, whereas our proposed method works directly on native Boolean weights. In addition, most of bnn methods incorporate fp data and modules as mandatory components. As a result, existing bnnns consume much more training energy compared to our b\(\)ld method. An example is shown in Fig. 1, where we consider the vgg-small architecture  on cifar10 dataset. In SS 4 we will consider much larger datasets and networks on more challenging tasks. We can see that our method achieves \(36\) and more than \(15\) energy reduction compared to the fp baseline and binarynet, respectively, while yielding better accuracy than bnns. Furthermore, bnns are commonly tied to specialized network architecture and have to employ costly multi-stage or kd training. Meanwhile, our Boolean framework is completely orthogonal to these bnn methods. It is generic and applicable for a wide range of network architectures, and its training procedure purely relies on Boolean logic from scratch. Nevertheless, we stress that it is not obligatory to use all Boolean components in our proposed framework as it is flexible and can be extended to architectures comprised of a mix of Boolean and fp modules. This feature further improves the superior performance of our method as can be seen in Fig. 1, where we integrate batch normalization (bn) into our Boolean model, and we will demonstrate extensively in our experiments.

## 3 Proposed Method

### Neuron Design

Boolean Neuron.For the sake of simplicity, we consider a linear layer for presenting the design. Let \(w_{0},\,(w_{1},,w_{m})\), and \((x_{1},,x_{m})\) be the bias, weights, and inputs of a neuron of input size \(m 1\). In _the core use case_ of our interest, these variables are all Boolean numbers. Let \(\) be a logic gate such as and, or, xor, xnor. The neuron's pre-activation output is given as follows:

\[s=w_{0}+_{i=1}^{m}(w_{i},x_{i}),\] (1)

where the summation is understood as the counting of trues.

Mixed Boolean-Real Neuron.To allow for flexible use and co-existence of this Boolean design with real-valued parts of a deep model, two cases of mixed-type data are considered including Boolean weights with real-valued inputs, and real-valued weights with Boolean inputs. These two cases can be addressed by the following extension of Boolean logic to mixed-type data. To this end, we first introduce the essential notations and definitions. Specifically, we denote \(:=\{,\}\) equipped with the Boolean logic. Here, \(\) and \(\) indicate true and false, respectively.

_Definition 3.1_ (Three-valued logic).: _Define \(}}{{=}}\{0\}\) with logic connectives defined according to those of Boolean logic as follows. First, the negation is: \(=\), \(=\), and \( 0=0\). Second, let \(\) be a logic connective, denote by \(_{}\) and \(_{}\) when it is in \(\) and in \(\), respectively, then \(_{}(a,b)=_{}(a,b)\) for \(a,b\) and \(_{}(a,b)=0\) otherwise._

_Notation 3.2_.: Denote by \(\) a logic set (e.g., \(\) or \(\)), \(\) the real set, \(\) the set of integers, \(\) a numeric set (e.g., \(\) or \(\)), and \(\) a certain set of \(\) or \(\).

_Definition 3.3_.: _For \(x\), its logic value denoted by \(x_{}\) is given as \(x_{}= x>0\), \(x_{}= x<0\), and \(x_{}=0 x=0\)._

_Definition 3.4_.: _The magnitude of a variable \(x\), denoted \(|x|\), is defined as its usual absolute value if \(x\). And for \(x\): \(|x|=0\) if \(x=0\), and \(|x|=1\) otherwise._

_Definition 3.5_ (Mixed-type logic).: _For \(\) a logic connective of \(\) and variables \(a\), \(b\), operation \(c=(a,b)\) is defined such that \(|c|=|a||b|\) and \(c_{}=(a_{},b_{})\)._

Using Definition 3.5, neuron formulation Eq. 1 directly applies to the mixed Boolean-real neurons.

Forward Activation.It is clear that there can only be one unique family of binary activation functions, which is the threshold function. Let \(\) be a scalar, which can be fixed or learned, the forward Boolean activation is given as: \(y=\) if \(s\) and \(y=\) otherwise where \(s\) is the preactivation. The backpropagation throughout this activation will be described in Appendix C.3.

### Mathematical Foundation

In this section we describe the mathematical foundation for our method to train Boolean weights directly in the Boolean domain without relying on fp latent weights. Due to the space limitation, essential notions necessary for presenting the main results are presented here while a comprehensive treatment is provided in Appendix A.

**Definition 3.6**.: _Order relations '\(<\)' and '\(>\)' in \(\) are defined as follows: \(<\), and \(>\)._

**Definition 3.7**.: _For \(a,b\), the variation from \(a\) to \(b\), denoted \((a b)\), is defined as: \((a b)}}{{=}}\) if \(b>a\), \(}}{{=}}0\) if \(b=a\), and \(}}{{=}}\) if \(b<a\)._

Throughout the paper, \((,)\) denotes the set of all functions from source \(\) to image \(\).

**Definition 3.8**.: _For \(f(,)\), \( x\), write \( f(x x):=(f(x) f( x))\). The variation of \(f\) w.r.t. \(x\), denoted \(f^{}(x)\), is defined as: \(f^{}(x)}}{{=}}((x  x), f(x x))\)._

_Remark 3.9_.: The usual notation of continuous derivative \(f^{}\) is intentionally adopted here for Boolean variation for convenience and notation unification. Its underlying meaning, i.e., continuous derivative or Boolean variation, can be understood directly from the context where function \(f\) is defined.

Intuitively, the variation of \(f\) w.r.t. \(x\) is \(\) if \(f\) varies in the same direction with \(x\).

_Example 3.10_.: Let \(a\), \(f(x)=(x,a)\) for \(x\), the variation of \(f\) w.r.t. \(x\) can be derived by establishing a truth table (see Table 8 in Appendix A.1) from which we obtain \(f^{}(x)= a\).

For \(f(,)\), its derivative, also known in terms of _finite differences_, has been defined in the literature as \(f^{}(x)=f(x+1)-f(x)\), see e.g. . With the logic variation as introduced above, we can make this definition more generic as follows.

**Definition 3.11**.: _For \(f(,)\), the variation of \(f\) w.r.t. \(x\) is defined as \(f^{}(x)}}{{=}} f(x x+1)\), where \( f\) is in the sense of the variation defined in \(\)._

**Theorem 3.12**.: _The following properties hold:_

1. _For_ \(f(,)\)_:_ \(( f)^{}(x)= f^{}(x)\)_,_ \( x\)_._
2. _For_ \(f(,)\)_,_ \(\)_:_ \(( f)^{}(x)= f^{}(x)\)_,_ \( x\)_._
3. _For_ \(f,g(,)\)_:_ \((f+g)^{}(x)=f^{}(x)+g^{}(x)\)_,_ \( x\)_._
4. _For_ \(}{{}}}{{}}\)_:_ \((g f)^{}(x)=(g^{}(f(x)),f^{}(x))\)_,_ \( x\)_._
5. _For_ \(}{{}}}{{}}\)_,_ \(x\)_, if_ \(|f^{}(x)| 1\) _and_ \(g^{}(f(x))=g^{}(f(x)-1)\)_, then:_ \[(g f)^{}(x)=(g^{}(f(x)),f^{}(x)).\]

The proof is provided in Appendix A.1. These results are extended to the multivariate case in a straightforward manner. For instance, for multivariate Boolean functions it is as follows.

**Definition 3.13**.: _For \(=(x_{1},,x_{n})^{n}\), denote \(_{ i}:=(x_{1},,x_{i-1}, x_{i},x_{i+1},,x_{n})\) for \(n 1\) and \(1 i n\). For \(f(^{n},)\), the (partial) variation of \(f\) w.r.t. \(x_{i}\), denoted \(f^{}_{i}()\) or \( f()/ x_{i}\), is defined as: \(f^{}_{i}() f()/ x_{i}}}{{=}}((x_{i} x_{i}), f( _{ i}))\)._

**Proposition 3.14**.: _Let \(f(^{n},)\), \(n 1\), and \(g(,)\). For \(1 i n\):_

\[(g f)^{}_{i}()=(g^{}(f(x)),f^{ }_{i}()),^{n}.\] (2)

_Example 3.15_.: From Example 3.10, we have \((x,a)/ x= a\) for \(a,x\). Using Theorem 3.12-(1) we have: \((x,a)/ x=a\) since \((x,a)=(x,a)\).

_Example 3.16_.: Apply Theorem 3.12-(3) to \(s\) from Eq. 1: \( s/ w_{i}=(w_{i},x_{i})/ w_{i}\) and \( s/ x_{i}=(w_{i},x_{i})/ x_{i}\). Then, for \(=\) as an example, we have: \( s/ w_{i}=x_{i}\) and \( s/ x_{i}=w_{i}\).

### BackPropagation

With the notions introduced in SS 3.2, we can write signals involved in the backpropagation process as shown in Fig. 2. Therein, layer \(l\) is a Boolean layer of consideration. For the sake of presentationsimplicity, layer \(l\) is assumed a fully-connected layer, and:

\[x_{k,j}^{l+1}=w_{0,j}^{l}+_{i=1}^{m}x_{k,i}^{l},w_{i,j}^{l} , 1 j n,\] (3)

where \(\) is the utilized Boolean logic, \(k\) denotes sample index in the batch, \(m\) and \(n\) are the usual layer input and output sizes. Layer \(l\) is connected to layer \(l+1\) that can be an activation layer, a batch normalization, an arithmetic layer, or any others. The nature of \(/ x_{k,j}^{l+1}\) depends on the property of layer \(l+1\). It can be the usual gradient if layer \(l+1\) is a real-valued input layer, or a Boolean variation if layer \(l+1\) is a Boolean-input layer. Given \(/ x_{k,j}^{l+1}\), layer \(l\) needs to optimize its Boolean weights and compute signal \(/ x_{k,i}^{l}\) for the upstream. Hereafter, we consider \(=\) when showing concrete illustrations of the method.

Atomic Variation.First, using Theorem3.12 and its extension to the multivariate case by Proposition3.14 in the same manner as shown in Example3.16, we have:

\[^{l+1}}{ w_{i,j}^{l}}=(x_{k,i} ^{l},w_{i,j}^{l})}{ w_{i,j}^{l}}= }}{{=}}x_{k,i}^{l},^{l+1}}{ x_{k,i }^{l}}=(x_{k,i}^{l},w_{i,j}^{l})}{ x_{k,i}^{l}} =}}{{=}}w_{i,j}^{l}.\] (4)

Using the chain rules given by Theorem3.12-(4 & 5), we have:

\[q_{i,j,k}^{l} :=}{ w_{i,j}^{l}}|_{k}=(}{ x_{k,j}^{l+1}},^{l+1}} { w_{i,j}^{l}})=}}{{=}} (}{ x_{k,j}^{l+1}},x_{k,i}^{l}),\] (5) \[g_{k,i,j}^{l} :=}{ x_{k,i}^{l}}|_{j}=(}{ x_{k,j}^{l+1}},^{l+1}} { x_{k,i}^{l}})=}}{{=}} (}{ x_{k,j}^{l+1}},w_{i,j}^{l}).\] (6)

Aggregation.Atomic variation \(q_{i,j,k}^{l}\) is aggregated over batch dimension \(k\) while \(g_{k,i,j}^{l}\) is aggregated over output dimension \(j\). Let \(()\) be the indicator function. For \(b\) and variable \(x\), define: \((x=b)=1\) if \(x_{}=b\) and \((x=b)=0\) otherwise. Atomic variations are aggregated as:

\[q_{i,j}^{l} :=}{ w_{i,j}^{l}}=_{k}q_{i,j,k}^{l}=|q_{i,j,k}^{l}|-_{k} q_{i,j,k}^{l}=|q_{i,j,k}^{l}|,\] (7) \[g_{k,i}^{l} :=}{ x_{k,i}^{l}}=_{j}g_{k,i,j}^{l}=|g_{k,i,j}^{l}|-_{j} g_{k,i,j}^{l}=|g_{k,i,j}^{l}|.\] (8)

Boolean Optimizer.With \(q_{i,j}^{l}\) obtained in Eq.7, the rule for optimizing \(w_{i,j}^{l}\) subjected to making the loss decreased is simply given according to its definition as:

\[^{l}= w_{i,j}^{l}q_{i,j}^{l},w_{i,j}^{l}=.}\] (9)

Eq.9 is the core optimization logic based on which more sophisticated forms of optimizer can be developed in the same manner as different methods such as Adam, Adaptive Adam, etc. have been developed from the basic gradient descent principle. For instance, the following is an optimizer that accumulates \(q_{i,j}^{l}\) over training iterations. Denote by \(q_{i,j}^{l,t}\) the optimization signal at iteration \(t\), and by \(m_{i,j}^{l,t}\) its accumulator with \(m_{i,j}^{l,0}:=0\) and:

\[m_{i,j}^{l,t+1}=^{t}m_{i,j}^{l,t}+^{t}q_{i,j}^{l,t+1},\] (10)

Figure 2: Illustration of backpropagation signals with a Boolean linear layer. Notice that the subsequent layer can be any fp/Boolean layers or activation functions.

where \(^{t}\) is an accumulation factor that can be tuned as a hyper-parameter, and \(^{t}\) is an auto-regularizing factor that expresses the system's state at time \(t\). Its usage is linked to brain plasticity  and Hebbian theory  forcing weights to adapt to their neighborhood. For the chosen weight's neighborhood, for instance, neuron, layer, or network level, \(^{t}\) is given as:

\[^{t}=t}{}.\] (11)

In the experiments presented later, \(^{t}\) is set to per-layer basis. Finally, the learning process is as described in Algorithm1. We encourage the readers to check the detailed implementations, practical considerations, and example codes of our proposed method, available in Appendix B and Appendix C.

Convergence Analysis.The following result describes how the iterative logic optimization based on Eq.9 minimizes a predefined loss \(f\), under the standard non-convex assumption. The technical assumptions and the proof are given in Appendix A.2.

**Theorem 3.17**.: _Under the specified assumptions, Boolean optimization logic converges at:_

\[_{t=0}^{T-1}\| f(w_{t})\|^{2 }}{T}+B^{*}+C^{*}^{2}+_{d},\] (12)

_where \(A^{*}=2(f(w_{0})-f_{*})\) with \(f_{*}\) being uniform lower bound assumed exists, \(B^{*}=2L^{2}\), \(C^{*}=4L^{2}^{2}}\) in which \(L\) is the assumed Lipschitz constant, and \(r_{d}=}{{2}}\)._

The bound in Theorem3.17 contains four terms. The first is typical for a general non-convex target and expresses how initialization affects the convergence. The second and third terms depend on the fluctuation of the minibatch gradients. There is an "error bound" of \(2Ld\) independent of \(T\). This error bound is the cost of using discrete weights as part of the optimization algorithm. Previous work with quantized models also includes such error bounds [61; 62].

Regularization.Exploding and vanishing gradients are two well-known issues when it comes to train deep neural networks. During Boolean Logic training, our preliminary experiments indicated that the backpropagation signal also experiences similar phenomenon, resulting in unstable training. Our idea is to scale the backpropagation signals so as to match their variance. Thanks to the Boolean structure, assumptions on the involved signals can be made so that the scaling factor can be analytically derived in closed-form without need of learning or batch statistics computation. Precisely, through linear layers of output size \(m\), the backpropagation signal is scaled with \(}{{m}}}\), and for convolutional layers of output size \(c_{}\), stride \(v\) and kernel sizes \(k_{x},k_{y}\), the backpropagation signal is scaled with \(2}{{c_{}}}k_{x}k_{y}}\) if maxpooling is applied, or \(}{{c_{}}}k_{x}k_{y}}\) otherwise. The full detail is presented in Appendix C.

``` Input : Learning rate \(\), nb iterations \(T\); Initialize : \(m_{i,j}^{t,0}=0\); \(^{0}=1\); for\(t=0,,T-1\)do /* 1. Forward */  Compute \(x^{t+1,t}\) following Eq.3; /* 2. Backward */ Receive \(}}{ x^{(t+1),t}_{x,j}}\) from downstream layer; /* 2.1 Backpropagation */  Compute and backpropagate \(g^{l,t}\) of Eq.8; /* 2.2 Weight update process */ \(N_{}:=0\), \(N_{}:=0\); foreach\(w^{t}_{i,j}\) Compute \(q^{l,t+1}_{i,j}\) following Eq.7; Update \(m_{i,j}^{l,t+1}=^{t}m_{i,j}^{l,t}+^{t}q_{i,j}^{l,t+1}\); \(N_{} N_{}+1\); for\(}(m_{i,j}^{l,t+1},w_{i,j}^{l,t})=\)then \(w_{i,j}^{l,t+1}:= w_{i,j}^{l,t}\)/* invert */ \(m_{i,j}^{l,t+1}=0\); else \(w_{i,j}^{l,t+1}=w_{i,j}^{l,t}\) ; /* keep */ \(N_{} N_{}+1\); Update \(^{t+1}\), \(^{t+1}=N_{}/N_{}\) ; ```

**Algorithm 1**Illustration with a FC layer.

## 4 Experiments

Our B\(\)LD method achieves extreme compression by using both Boolean activations and weights. We rigorously evaluate its performance on challenging precision-demanding tasks, including _image classification_ on cifar10 and imagenet, as well as _super-resolution_ on five popular datasets. Furthermore, recognizing the necessity of deploying efficient lightweight models for edge computing, we delve into three fine-tuning scenarios, showcasing the adaptability of our approach. Specifically, we investigate fine-tuning Boolean models for image classification on cifar10 and cifar100 using vgg-small. For _segmentation tasks_, we study deeplabv3 fine-tuned on cityscapes  and pascal voc 2012  datasets. The backbone for such a model is our Boolean resnet18  network trained from scratch on imagenet. Finally, we consider an evaluation in the domain of _natural language understanding_, fine-tuning bert, a transformer-based  language model, on the glue benchmark .

Experimental Setup.To construct our b\(\)ld models, we introduce Boolean weights and activations and substitute full-precision (fp) arithmetic layers with Boolean equivalents. Throughout all benchmarks, we maintain the general network design of the chosen fp baseline, while excluding fp-specific components like ReLU, PReLU activations, or batch normalization (bn) , unless specified otherwise. Consistent with the common setup in existing literature [see, e.g., 86, 21, 11], only the first and last layers remain in fp and are optimized using an Adam optimizer . Comprehensive experiment details for reproducibility are provided in Appendix D.

Complexity Evaluation.It has been demonstrated that relying solely on flops and bops for complexity assessment is inadequate . In fact, these metrics fail to capture the actual load caused by propagating fp data through the bnn. Instead, energy consumption serves as a crucial indicator of efficiency. Given the absence of native Boolean accelerators, we estimate analytically energy consumption by analyzing the arithmetic operations, data movements within storage/processing units, and the energy cost of each operation. This approach is implemented for the Nvidia GPU (Tesla V100) and Ascend  architectures. Further details are available in Appendix E.

### Image Classification

Our b\(\)ld method is tested on two network configurations: _small & compact_ and _large & deep_. In the former scenario, we utilize the vgg-small baseline trained on cifar10. Evaluation of our Boolean architecture is conducted both without bn, and with bn including activation from . These designs achieve \(90.29 0.09\)% (estimated over six repetitions) and \(92.37 0.01\)% (estimated over five repetitions) accuracy, respectively (see Table 2). Notably, without bn, our results align closely with binaryconnect, which employs \(32\)-bit activations during both inference and training. Furthermore, bn brings the accuracy within 1 point of the fp baseline. Additional results are provided in the supplementary material for vgg-small models ending with 1 fc layer.

Our method requires much less energy than the fp baseline. In particular, it consumes less than 5% of energy for our designs with and without bn respectively. These results highlight the remarkable energy efficiency of our b\(\)ld method in both inference and training, surpassing latent-weight based training methods  reliant on fp weights. Notably, despite a slight increase in energy consumption, utilizing bn yields superior accuracy. Even with bn, our approach maintains superior efficiency compared to alternative methods, further emphasizing the flexibility of our approach in training networks with a blend of Boolean and fp components.

In the _large & deep_ case, we consider the resnet18 baseline trained from scratch on imagenet. We compare our approach to methods employing the same baseline, larger architectures, and additional training strategies such as kd with a resnet34 teacher or fp-based shortcuts . Our method consistently achieves the highest accuracy across all categories, ranging from the standard model (\(51.8\)% accuracy) to larger configurations (\(70.0\)% accuracy), as shown in Table 5. Additionally, our b\(\)ld method exhibits the smallest energy consumption in most categories, with a remarkable \(24.45\%\) for our large architecture with and without kd. Notably, our method outperforms the fp baseline when using \(4\) filter enlargement (base 256), providing significant energy reduction (\(24.45\%\)). Furthermore, it surpasses the sota pokebnn, utilizing resnet50 as a teacher.

For completeness, we also implemented neural gradient quantization, utilizing int4 quantization with a logarithmic round-to-nearest approach  and statistics-aware weight binning . Our experiments on imagenet confirm that 4-bit quantization is sufficient to achieve standard fp performances, reaching \(67.53\)% accuracy in \(100\) epochs (further details provided in Appendix D.1.4).

### Image Super-resolution

Next, we evaluate the efficacy of our b\(\)ld method to synthesize data. We use a compact edsr network  as our baseline, referred to as small edsr, comprising eight residual blocks. Ourb\(\)ld model employs Boolean residual blocks without bn. Results, presented in Table 3, based on the official implementation and benchmark3, reveal remarkable similarity to the fp reference at each scale. Particularly noteworthy are the prominent results achieved on set14 and bsd100 datasets. Our method consistently delivers high PSNR for high-resolution images, such as div2k, and even higher for low-resolution ones, like set5. However, akin to edsr, our approach exhibits a moderate performance reduction at scale \(4\). These findings highlight the capability of our method to perform adequately on detail-demanding tasks while exhibiting considerable robustness across image resolutions.

### Adaptability on New Data

Image classification fine-tuning.We aim to assess the adaptability of our method to similar problems but different datasets, a common scenario for edge inference tasks. We employ the vgg-small architecture without bn under two training configurations. Firstly, the b\(\)ld model is trained from scratch with random initialization on cifar10 (ref. c) and cifar100 (ref. d). Secondly, we fine-tune the trained networks on cifar100 (ref. f) and cifar10 (ref. h), respectively. Notably, in Table 6, fine-tuning our trained model on cifar100 (ref. f) results in a model almost identical to the model trained entirely from scratch (ref. d). Additionally, a noteworthy result is observed with our model (ref. h), which achieves higher accuracy than the model trained from scratch (ref. c).

Image segmentation fine-tuning.Next, we expand the scope of the aforementioned fine-tuning experiment to encompass a larger network and a different task. The baseline is the deeplabv3 network for semantic segmentation. It consists of our Boolean resnet18 (without bn) as the backbone, followed by the Boolean atrous pyramid pooling (aspp) module . We refrain from utilizing auxiliary loss or knowledge distillation techniques, as these methods introduce additional computational burdens, which are contrary to our objective of efficient on-device training. As demonstrated in Table 4, our method achieves a notable \(67.4\%\) mIoU on cityscapes (see Fig. 3 for prediction examples). This result surpasses the sota, binary dad-net, and approaches

  
**Ref.** & **Method** & **Model** & **Train/FT** & **Bilwidth** & **Acc.** \\
**Modity** & **Init.** & **Dataset** & **W/A/G** & **(\%)** \\    & FP Baseline & Random & cifar10 & \(32/32/32\) & 95.27 \\  & RP Baseline & Random & cifar10 & \(32/32/32\) & 77.27 \\  & RP Baseline & Random & cifar10 & \(1/116\) & 60.29 \\  & DW \(\)ld & \(\)ld & Random & cifar100 & \(1/116\) & 68.43 \\    & E & RP Baseline & A & cifar100 & \(32/32/32\) & 76.74 \\  & F & BD\(\)ld & C & cifar100 & \(1/116\) & 68.37 \\    & FP Baseline & B & cifar10 & \(32/32/32\) & 95.77 \\    & H & B\(\)ld & D & cifar10 & \(1/116\) & 92.09 \\   

Table 6: Results with vgg-small baseline fine-tuned on cifar10 and cifar100.

  
**Dataset** & **Model** & **mIoU (\%)** & **(\%)** \\    &  & fp Baseline & 70.7 \\  & & binary dad-net  & 58.1 \\  & & **(\#) bld [Ours]** & **67.4** \\   & fp Baseline & 72.1 \\  & & **(\#) bld [Ours]** & 67.3 \\   

Table 4: Image segmentation results.

  
**Method** & **W/A** & **Acc.(\%)** & **Cons.(\%)** & **Cons.(\%)** \\  & & & **Acc.(\%)** & **Total V100** \\   Full-precision  & 32/32 & 93.00 & 100.00 & 100.00 \\
**\(\)**histochastic & 1/32 & 90.10 & 38.59 & 48.49 \\
**\(\)**box-set & 1/1 & 89.83 & 24.21 & 45.68 \\
**\(\)**histochastic & 1/1 & 89.85 & 32.60 & 43.61 \\
**\(\)**histochastic & 1/1 & 89.20 & 3.64 & 2.78 \\
**\(\)**bld+lib with BN [Ours]** & 1/1 & **92.37** & 4.87 & 3.71 \\   

Table 2: Results with vgg-small on cifar10. ‘Cons.’ is the energy consumption w.r.t. the fp baseline, evaluated on 1 training iteration.

  
**Task** & **Method** & **sets** & **set14** & **bsd100** & **valvalvalval00** & **bvl2k** \\    & Pull down (fp) & 38.11 & 33.92 & 32.32 & 32.33 & 35.03 \\  & Small loss (FP) & 38.01 & 33.63 & 32.19 & 31.60 & 34.67 \\  & **98.000 [Ours]** & 37.42 & 31.00 & 31.75 & 30.26 & 33.53 \\   & Pull down (fp) & 34.65 & 30.52 & 29.25 & \(-\) & 31.26 \\  & Small loss (FP) & 34.35 & 30.24 & 29.10 & \(-\) & 30.93 \\  & **98.000 [Ours]** & 35.36 & 29.70 & 28.72 & \(-\) & 30.22 \\   & Pull down (fp) & 32.46 & 28.00 & 27.71 & 28.64 & 29.25 \\  & Small loss (FP) & 32.17 & 28.53 & 27.62 & 26.14 & 29.04 \\   & **98.000 [Ours]** & 31.23 & 27.97 & 27.24 & 25.12 & 28.36 \\   

Table 3: Super-resolution results measured in PSNR (dB) (\(\)), using the edsr baseline .

the performance of the fp baseline. Likewise, on pascal voc 2012, our methodology nears the performance of the fp baseline. Importantly, these improvements are attained without the intermediate use of fp parameters during training, highlighting the efficiency and effectiveness of our approach. This shows that our method not only preserves the inherent lightweight advantages of highly quantized neural networks but also significantly enhances performance in complex segmentation tasks.

BERT fine-tuning for NLU tasks.Finally, we consider fine-tuning bert, a transformer-based model , on the glue benchmark . We follow the standard experimental protocol as in [26; 6; 82]. Our model and the chosen baselines are employed with 1-bit bitwidth for both weights and activations. Our Boolean bert model is inspired by bit for binarizing activations and incorporating kd during training, where the fp teacher guides the student in a layer-wise manner. We follow the experimental setup of bit, including using the same method for binarizing activations and backpropagation for softmax and attention in the bert model. As shown in Table 7, all methods suffer from performance drop compared to the fp model as extreme binarization of transformer-based model is not trivial. Nevertheless, our method yields results comparable to bit, the sota method on this task, outperforming binarybert and bibert on average. This is remarkable as our method natively uses Boolean weights during the training, whereas the baselines heavily rely on fp latent weights. These findings indicate potential for energy-efficient large language models (llms) using our method for both training and inference.

## 5 Conclusions

We introduced the notion of Boolean variation and developed a first framework of its calculus. This novel mathematical principle enabled the development of Boolean logic backpropagation and Boolean optimization replacing gradient backpropagation and gradient descent for binary deep learning. Deep models can be built with native Boolean weights and/or Boolean activations, and trained in Boolean natively by this principled exact Boolean optimization. That brings a key advantage to the existing popular quantized/binarized training approach that suffers from critical bottlenecks - _(i)_ performance loss due to an arbitrary approximation of the latent weight gradient through its discretization/binarization function, _(ii)_ training computational intensiveness due to full-precision latent weights. We have extensively explored its capabilities, highlighting: _(i)_ both training and inference are now possible in binary; _(iv)_ deep training complexity can be drastically reduced to unprecedented levels. _(iii)_ Boolean models can handle finer tasks beyond classification, contrary to common belief; _(ii)_ in some applications, suitably enlarging Boolean model can recover fp performance while still gaining significant complexity reduction.

Limitations.Due to current computing accelerators, such as GPUs, primarily designed for real arithmetic, our method could not be assessed on native Boolean accelerator. Nevertheless, its considerable potential may inspire the development of new logic circuits and architectures utilizing Boolean logic processing. It also remains as an open question the approximation capacity of Boolean neural networks. A mathematical result equivent to the existing universal approximation theory of real-valued neural networks would provide a solid guarantee.

    &  \\   & MNLIQ opt & MNLI & SST-2 & OOLA & SST-2 & OOLA & RTE & **Avg.** \\   fpbert & 84.9 & 91.4 & 92.1 & 93.2 & 59.7 & 90.1 & 86.3 & 72.2 & 83.9 \\  binarybert & 35.6 & 66.2 & 51.5 & 53.2 & 0.0 & 6.1 & 68.3 & 52.7 & 41.0 \\ bibert & 66.1 & 84.8 & 72.6 & 88.7 & 25.4 & 33.6 & 72.5 & 74.7 & 63.2 \\ bit & 77.1 & 82.9 & 85.7 & 87.7 & 25.1 & 71.1 & 79.7 & 58.8 & 71.0 \\ bit (Reprod.) & 76.8 & 87.2 & 85.6 & 87.5 & 24.1 & 70.5 & 78.9 & 58.8 & 69.7 \\ (\# b\(\)D) & 75.6 & 85.9 & 84.1 & 88.7 & 27.1 & 68.7 & 78.4 & 58.8 & 70.9 \\   

Table 7: bert models results. \({}^{}\)Source code .

Figure 3: An example of cityscapes.