# The Implicit Bias of Adam on Separable Data

Chenyang Zhang

Department of Statistics and Actuarial Science

School of Computing and Data Science

The University of Hong Kong

chyzhang@connect.hku.hk

&Difan Zou

Department of Computer Science

School of Computing and Data Science

& Institute of Data Science

The University of Hong Kong

dzou@cs.hku.hk

&Yuan Cao

Department of Statistics and Actuarial Science

School of Computing and Data Science

& Department of Mathematics

The University of Hong Kong

yuancao@hku.hk

###### Abstract

Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum \(_{}\)-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective.

## 1 Introduction

Adam  is one of the most widely used optimization algorithms in deep learning. By entry-wisely adjusting the learning rate based on the magnitude of historical gradients, Adam has proven to be highly efficient in solving optimization tasks in machine learning. However, despite the remarkable empirical success of Adam, current theoretical understandings of Adam cannot fully explain its fundamental difference compared with other optimization algorithms.

It has been recently pointed out that the _implicit bias_ of an optimization algorithm is essential in understanding the performance of the algorithm in machine learning. In over-parameterized learning tasks where the training objective function may have infinitely many solutions, the implicit bias of an optimization algorithm characterizes how the algorithm prioritizes converging towards a specific optimum with particular structures and properties. Several recent works studied the implicit bias of Adam and other adaptive gradient methods. Specifically,  studied the implicit bias of AdaGrad, and showed that AdaGrad converges to a direction that can be characterized as the solution of a quadratic optimization problem related to the limit of preconditioners. However, their results cannot be extended to Adam.  showed that gradient descent with momentum (GDM) and its adaptive variants have the same implicit bias with gradient descent. This result is extended to the setting of training homogeneous models in . However, the results in  reply on a nonnegligible stability constant - when the gradient entries are minimized below the stability constant (which is by default \(10^{-8}\) in Adam), adaptive gradient methods will essentially behave like gradientdescent. Therefore, it remains an open question how Adam will behave under the more practical regime where stability constant is negligible.

We note that there exist several recent works studying variants of Adam without the stability constant. First of all, sign gradient descent, which can be regarded as Adam without momentum or stability constant, is usually considered a proxy of Adam in theoretical studies due to its ease of analysis.  studied the implicit bias of steepest descent with respect to different potentials and norms covering a variant of sign gradient descent, and demonstrated that sign gradient descent converges to the maximum \(_{}\)-margin solution. However, this result for sign gradient descent cannot cover Adam, as the momentum terms in the update of Adam are crucial. Besides, a more recent work  studied the implicit bias of AdamW without considering the stability constant. They showed that, if the iterates of AdamW converge, then the limiting point must be a KKT point of an optimization problem with \(_{}\) constraints. However, the analysis of AdamW in  relies on a non-zero regularization parameter, and therefore cannot be extended to the study of Adam.

In this paper, we investigate the implicit bias of Adam. Specifically, let \(\{(_{i},y_{i})\}_{i=1}^{n}^{d}\{ 1\}\) be a training data set of a binary classification problem. We consider using Adam to train a linear model to minimize the empirical logistic loss (or exponential loss). Then our main results can be summarized as the following informal theorem:

**Theorem 1.1** (Simplified version of Theorem 4.5).: Let \(\{_{t}\}_{t=0}^{},\{_{t}\}_{t=0}^{}\) be the sequence of learning rates and iterates of Adam respectively. Suppose that the data set \(\{(_{i},y_{i})\}_{i=1}^{n}\) is linearly separable, and that \(_{t}_{t}=0\), \(_{t=0}^{}_{t}=\). Then under certain conditions, it holds that

\[|_{i[n]}_{t},y_{i}_{i}}{\|_{t}\|_{}}-|  O^{t-1}_{}^{} +_{=0}^{t-1}_{}e^{-_{=0}^{-1} _{^{}}}}{_{=0}^{t-1}_{}},\] (for logistic loss) \[|_{i[n]}_{t},y_{i} _{i}}{\|_{t}\|_{}}-|  O^{t-1}_{}^{ }}{_{=0}^{t-1}_{}},\] (for exponential loss)

where \(:=_{\|\|_{} 1}_{i[n]},y_{i} _{i}\) is the maximum \(_{}\)-margin on the training data set.

Theorem 1.1 shows that, for a general class of learning rate schedules, Adam will eventually achieve the maximum \(_{}\)-margin on the training data set.

* We demonstrate the implicit bias of Adam for solving linear logistic regression with linearly separable data. Specifically, we prove that Adam has an implicit bias towards a maximum \(_{}\)-margin solution when solving linear classification problems. Our result distinguishes Adam from (stochastic) gradient descent with/without momentum, whose implicit bias is towards the maximum \(_{2}\)-margin solution.
* Our analysis of Adam covers a broad range of diminishing learning rate schedules. For \(_{t}=(t^{-a})\) with \(a(0,1]\), our result demonstrates the following convergence rate: \[|_{i[n]}_{t},y_{i}_{i} }{\|_{t}\|_{}}-| Ot^{-a/2},&a<2/3;\\ Ot^{-1/3} t,&a=2/3;\\ Ot^{1-a},&2/3<a<1;\\ O1/ t,&a=1.\] Notably, when \(a<1\), the above rates indicate that the convergence towards the maximum \(_{}\)-margin occurs in polynomial time. This further differentiates Adam from (stochastic) gradient descent with/without momentum in terms of the convergence speed.
* Our result focuses on a particularly challenging setting where we ignore the "stability constant \(\)" in the Adam algorithm. In practice, the stability constant is by default set as \(=10^{-8}\), which is almost negligible throughout the optimization process. Therefore, by covering the setting without the stability constant, our theory matches the practical setting better. We demonstrate by simulation that our theory can also correctly characterize the implicit bias of Adam with the stability constant.

**Notation.** Given two sequences \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if there exist some absolute constant \(C_{1}>0\) and \(N>0\) such that \(|x_{n}| C_{1}|y_{n}|\) for all \(n N\). Similarly, we denote \(x_{n}=(y_{n})\)if there exist \(C_{2}>0\) and \(N>0\) such that \(|x_{n}| C_{2}|y_{n}|\) for all \(n>N\). We say \(x_{n}=(y_{n})\) if \(x_{n}=O(y_{n})\) and \(x_{n}=(y_{n})\) both holds. We use \(()\), \(()\), and \(()\) to hide logarithmic factors in these notations respectively. Moreover, we denote \(x_{n}=(y_{n})\) if \(x_{n}=O(y_{n}^{D})\) for some positive constant \(D\), and \(x_{n}=(y_{n})\) if \(x_{n}=((y_{n}))\). For two scalars \(a\) and \(b\), we denote \(a b=\{a,b\}\) and \(a b=\{a,b\}\). For any \(n_{+}\), we use \([n]\) to denote the set \(\{1,2,,n\}\). For any scalar \(c\), \( c\) denotes the smallest integer larger or equal to \(c\) and \( c\) denotes the largest integer smaller or equal to \(c\). For a vector \(^{d}\), \([k]\) denote its \(k\)-th entry. Finally, \(_{i}^{n}\) denotes the \(i\)-th basis vector in \(^{n}\).

## 2 Additional Related Work

**Theoretical analyses of Adam and its variants.** There has been a line of works studying the properties of Adam and its variants from different aspects.  pointed out that there exists simple convex objective functions which Adam may fail to minimize, and proposed a new variant of Adam, the AMSGrad algorithm, which enjoys convergence guarantees in convex optimization. [54; 10; 17; 33; 53; 18] established optimization guarantees of Adam and its variants in non-convex optimization. [29; 19] implemented variance reduction techniques in Adam and proposed new variants of Adam accordingly. [47; 52; 55; 56] studied the generalization performance of Adam and compared it with GD under different learning tasks. [27; 4; 6; 3; 5] tried to explain the performance of Adam by studying the connections between Adam and sign gradient descent.  explored the optimization trajectories of Adam from the \(_{}\) geometry.

**Implicit bias.** Classic results [40; 21] demonstrated the iterates of GD will converge to the maximum \(_{2}\)-margin solution in direction on linear logistic regression with linear separable datasets.  extended this result under stochastic settings.  explored the implicit bias of a general class of optimization methods, containing mirror descent and steepest descent.  proposed a primal-dual analysis and derived a faster convergence rate with a larger learning rate compared to [40; 21].  explored the implicit bias of gradient descent at the 'edge of stability' regime, where the learning rate can be an arbitrarily large constant. [30; 22] showed that \(q\)-homogeneous neural network trained by GD will converge to a KKT point of maximum \(_{2}\)-margin optimization problem.  established an implicit bias type result for the Lion  algorithm in its continuous-time form. There also exist numerous works studying the implicit bias for different problem setting, including matrix factorization models [16; 28; 2; 36], squared loss models [38; 1; 24], weight normalization and batch normalization [49; 7], deep linear neural networks [15; 20], two-layer neural networks [11; 34; 13; 42; 43; 26].

## 3 Problem Settings

We consider binary linear classification problems. Specifically, given \(n\) training data points \(\{(_{i},y_{i})\}_{i=1}^{n}\) where \(_{i}^{d}\) and \(y_{i}\{+1,-1\}\), we aim to find a coefficient vector \(\) which minimizes the following empirical loss

\[()=_{i=1}^{n}( ,y_{i}_{i}),\] (3.1)

where \((,y_{i}_{i})\) is the loss function value on the data point \((_{i},y_{i})\). In this paper, we consider \(\{_{},_{}\}\), where \(_{}(z)=(1+e^{-z})\) is the logistic loss function and \(_{}(z)=e^{-z}\) is the exponential loss function. We consider using Adam to minimize (3.1). Denoting \(_{-1}=_{-1}=^{d}\) and starting with initialization \(_{0}\), Adam applies the following iterative formulas:

\[_{t}=_{1}_{t-1}+(1-_{1}) (_{t}),\] (3.2) \[_{t}=_{2}_{t-1}+(1-_{2}) (_{t})^{2},\] (3.3) \[_{t+1}=_{t}-_{t}_{t}}{ _{t}}},\] (3.4)

where \(_{1},_{2}[0,1)\) are the hyperparameters of Adam, and the square \(()^{2}\), square root \(()\) and division \(()\) above all denote entry-wise calculations.

Note that in practice, it is common to consider the variant \(_{t+1}=_{t}-_{t}}{_{t}+ }}\), where an additional term \( 10^{-8}\) is added in (3.4) to improve stability. However, in our analysis, we do not consider such a term \(\). This is because in practice, one seldom run Adam until \(_{t}\) is around the same level as \(\). However, by the nature of implicit bias, the result needs to cover infinitely many iterations, and the additional term \(\) will eventually significantly affect the result. In fact, a recent work  showed that when one considers such an additional \(\) term, Adam will be asymptotically equivalent to gradient descent. In comparison, in this paper, we will show that when ignoring \(\), Adam has a unique implicit bias that is different from gradient descent.

## 4 Main Results

In this section, we present our main result on the implicit bias of Adam in linear classification problems. We first introduce several assumptions.

**Assumption 4.1**.: There exists \(^{d}\) such that \(,y_{i}_{i}>0\) for all \(i[n]\).

Assumption 4.1 is a standard assumption in the study of implicit bias of linear models [40; 14; 31; 21; 23; 45; 48]. It can be easily satisfied in the over-parameterized setting where \(d n\). With the linear separability assumption, we can further define the maximum \(_{}\)-margin:

\[=_{\|\|_{} 1}_{i[n]},y_{ i}_{i}.\] (4.1)

We also make the following assumption on the initialization \(_{0}\).

**Assumption 4.2**.: The initialization \(_{0}\) of Adam satisfies that for all \(k[d]\), \((_{0})[k]^{2}\).

Assumption 4.2 ensures that at every finite iteration, the entries of \(_{t}\) are strictly positive. We remark that this is a mild assumption: if \(_{i}\), \(i[n]\) are generated from a continuous, non-degenerate distribution, then regardless of the choice of \(_{0}\), \((_{0})[k] 0\) with probability \(1\). Moreover, \(\) will only appear in our results in the form of \((1/)\), and therefore, even if \(\) is small, it will not significantly hurt the convergence rates. A similar assumption has also been considered in .

**Assumption 4.3**.: \(\{_{t}\}_{t=1}^{}\) are decreasing in \(t\), and satisfy \(_{t=0}^{}_{t}=\), \(_{t}_{t}=0\).

Assumption 4.3 is a mild and standard assumption of the learning rates \(\{_{t}\}_{t=0}^{}\) that is commonly considered in the general optimization literature. It has also been considered in recent studies of Adam and its variants [12; 19; 50].

**Assumption 4.4**.: For all \((0,1)\) and \(c_{1}>0\), there exist \(t_{1}_{+}\) and \(c_{2}>0\) that only depend on \(,c_{1}\), such that \(_{=0}^{t}^{}e^{c_{1}_{^{}=1}^{}_{ t-^{}}}-1 c_{2}_{t}\) for all \(t t_{1}\).

Although Assumption 4.4 seems non-trivial, we claim it is a fairly mild assumption. In fact, for both small fixed learning rate \(_{t}=\), and decay learning rate \(_{t}=(t+2)^{-a}\) with \(a(0,1]\), Assumption 4.4 always hold. We formally prove this result in Lemma C.1 in the appendix.

Now, we state our main theorem about the implicit bias about Adam as follows.

**Theorem 4.5**.: Let \(\{_{t}\}_{t=0}^{}\) be the iterates of Adam in (3.2)-(3.4) with \(_{1}_{2}\). In addition, let \(\) be defined in (4.1) and \(B:=_{i[n]}\|_{i}\|_{1}\). Then under Assumptions 4.1, 4.2, 4.3 and 4.4, there exists \(t_{0}=t_{0}(n,d,_{1},_{2},,B,,_{0})\) such that

* If \(=_{ exp}\), then for all \(t t_{0}\), \[(_{t}) e^{-_{ =t_{0}}^{t-1}_{}},\ \ _{i[n]}_{t},y_{i} _{i}}{\|_{t}\|_{}}- O ^{t_{0}-1}_{}+d_{=t_{0}}^{t-1} _{}^{}}{_{=0}^{t-1}_{}}.\]
* If \(=_{ log}\), then for all \(t t_{0}\), \[(_{t}) e^{-_{ =t_{0}}^{t-1}_{}},\] and \[_{i[n]}_{t},y_{i}_{i} }{\|_{t}\|_{}}- O^{t_{0}-1}_{}+d_{=t_{0}}^{t-1}_{}^{ }+_{=t_{0}}^{t-1}_{}e^{-_{^{}=t_{0} }^{-1}_{^{}}}}{_{=0}^{t-1}_{}},\]where we use \(O()\) to omit factors that only depend on \(_{1},_{2},,B\).

Theorem 4.5 implies that Adam can minimize the loss function to zero, and that the normalized \(_{}\)-margin achieved by Adam will eventually converge to the maximum \(_{}\)-margin of the training data set. To address general learning rate schedules, we do not specify a particular convergence rate for either the loss or the margin, nor do we provide an exact formula for \(t_{0}\). However, it can be easily verified that \((_{t}) Oe^{- t^{1-a}/4(1-a)}\) when \(_{t}=(t+2)^{-a}\) with \(a<1\). This loss convergence rate of Adam is much faster than that of (stochastic) gradient descent (with momentum) given a fixed small learning rate, which is of order \(O(1/t)\). In addition, we have \(t_{0}=[n,d,(1-_{1})^{-1},(1-_{2})^{-1},^{-1},B, (1/),(_{0})]\) when \(_{t}=(t+2)^{-a}\) with \(a<1\), and we defer the derivation details to Appendix B.2. Regarding margin convergence, we will give a set of detailed convergence rate results for different learning rate schedules in Corollary 4.7.

According to Theorem 4.5, the nature of Adam is vastly different from (stochastic) gradient descent from the perspective of implicit bias: Adam maximizes the \(_{}\)-margin, while existing works have demonstrated that (stochastic) gradient descent maximizes the \(_{2}\)-margin . Compared with existing works on the implicit bias of adaptive gradient methods , our result demonstrates a novel type of implicit bias with accurate convergence rates, which can not been covered in the previous results. Notably,  showed that, if a stability constant \(\) is added, i.e., (3.4) is replaced by \(_{t+1}=_{t}-_{t}_{t}}{_{t}+}}\), then Adam will eventually be equivalent to gradient descent and will converge to the maximum \(_{2}\)-margin solution. However, the analysis in  relies on a positive \(\): their proof is based the fact that after a large number of iterations, the entries of \(_{t}\) will eventually be much smaller than \(\), and the update of Adam will be similar to gradient descent with momentum. In our analysis, we are able to cover the setting where \(=0\), and our result demonstrates that studying the setting without \(\) is essential, as the implicit bias is completely different. In Section 5, we will demonstrate by experiments that our setting matches the practical observations better.

As we have discussed, Theorem 4.5 implies the convergence of the normalized \(_{}\)-margin of Adam iterates towards the maximum \(_{}\)-margin. Since the results cover very general learning rates, the convergence rates are presented in rather complicated formats. However, based on the assumption that \(_{t=0}^{}_{t}=\), \(_{t}_{t}=0\), we can immediately conclude the following simplified result by the Stolz-Cesaro theorem (see Theorem C.8 in the appendix).

**Corollary 4.6**.: Under the same conditions in Theorem 4.5, it holds that

\[_{t}_{i[n]}_{t},y_{i} _{i}}{\|_{t}\|_{}}=_{\| \|_{} 1}_{i[n]},y_{i}_{i}.\]

If there exists a unique maximum \(_{}\)-margin solution \(^{*}=_{\|\|_{} 1}_{i[n]} ,_{i}\), then we have \(_{t}_{t}}{\|_{t}\|_{}}= ^{*}\).

We can also investigate the convergence rates of the \(_{}\)-margin with specific learning rates. The results are summarized in the following Corollary.

**Corollary 4.7**.: Consider \(_{t}=(t+2)^{-a}\) with \(a(0,1]\). Denote by \(_{t}^{}\) and \(_{t}^{}\) the iterates of Adam for \(=_{}\) and \(=_{}\) respectively. Suppose that \(_{1}_{2}\) and Adam starts with initialization \(_{0}\). Let \(B:=_{i[n]}\|_{i}\|_{1}\). Then under Assumptions 4.1 and 4.2, there exists \(t_{0}=t_{0}(n,d,_{1},_{2},,B,,_{0})\) such that for all \(t t_{0}\), the following results hold:

* If \(a<\), \[_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-,_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-  O}.\]
* If \(a=\), \[_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-   O(_{0} )+[(1/)]^{1/3}}{t^{1/3}}.\]* If \(<a<1\), \[|_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-| O (_{0})+[(1/)]^{1-a}}{t^ {1-a}},\] \[|_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-|  O}+n(_{0})+[(1/)]^{1-a}}{t^{1-a}}.\]
* If \(a=1\), \[|_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-|  O(_{0})+ (1/)}{ t},\] \[|_{i[n]}_{t}^{},y_{i} _{i}}{\|_{t}^{}\|_{}}-|  O(_{0})+ (1/)}{ t}.\]

Corollary 4.7 comprehensively presents the convergence rate of the \(_{}\)-margin for different learning rates. It also indicates that the margin convergence rates for \(_{}\) and \(_{}\) are of the same order of \(t\). Notably, for \(a<1\), the normalized \(_{}\)-margin converges in polynomial time. This clearly distinguishes Adam from (stochastic) gradient descent with/without momentum, for which the normalized \(_{2}\)-margin converges at a speed \(O(1/ t)\)[40; 45; 20]. We note that a recent work  proposed a novel algorithm named progressive rescaling gradient descent that can maximize the margin at an exponential rate. Here our focus is different from : our purpose is not to propose new algorithms to achieve better convergence rates, but is to theoretically study the properties of the classic Adam algorithm. We would also like to remark that, although Corollary 4.7 seemingly indicates that \(_{t}=(t+2)^{-2/3}\) is the learning rate schedule with the fastest convergence rate, it does not mean that \(_{t}=(t+2)^{-2/3}\) always converge faster than the other learning rate schedules in all learning tasks. The bounds in Corollary 4.7 are derived under the worst cases, and in practice, we can frequently observe that the margins all converge faster than the bounds in the corollary.

## 5 Experiments

In this section, we conduct numerical experiments to verify our theoretical conclusions. We set the sample size \(n=50\), and dimension \(d=50\). Then the data set \(\{(_{i},y_{i})\}\) are generated as follows:

1. \(_{i}\), \(i[n]\) are independently generated from \(N(,)\).
2. \(y_{i}\), \(i[n]\) are independently generated from as \(+1\) or \(-1\) with equal probability.

Note that for data sets generated following the procedure above, Assumption 4.1 almost surely holds. We can also apply standard convex optimization to calculate the maximum \(_{}\)-margin \(\). In order to make a clearer comparison between Adam and GD, we generate \(10\) independent sets of data, and we select the dataset with the most significant difference in the directions of the maximum \(_{2}\)-margin solution and maximum \(_{}\)-margin solution. We then run the experiments on this selected data set. Throughout our experiments, for gradient descent with momentum, we set the momentum parameter as \(_{1}=0.9\), and for Adam, we set \(_{1}=0.9\), \(_{2}=0.99\). All these hyper-parameter setups are common in practice. All optimization algorithms are initialized with standard Gaussian distribution, and are run for \(10^{6}\) iterations.

We first run GD, GDM, Adam without the stability constant, and Adam with stability constant \(=10^{-8}\) to train a linear model minimizing the logistic loss, and compare their normalized \(_{}\)-margin and normalized \(_{2}\)-margin. The results are given in Figure 1. We can see that the normalized \(_{}\)-margins of Adam, both with and without \(\), converge to the maximum \(_{}\)-margin, whereas the normalized \(_{}\)-margins of GD and GDM do not. In contrast, the normalized \(_{2}\)-margins of GD and GDM converge to the maximum \(_{2}\)-margin, while the \(_{2}\)-margins of Adam, both with and without \(\), do not. By comparing the curves of Adam with and without \(\), we find that they behave similarly and their convergence remains highly stable. This justifies our theoretical setting where we ignore the stability constant in Adam, and demonstrate that our maximum \(_{}\)-margin implicit bias result derived without \(\) characterizes the practical behaviour of Adam more accurately compared with the maximum \(_{2}\)-margin result for Adam with \(\) in .

We also run a set of experiments to demonstrate the polynomial time convergence rate of the \(_{}\)-margin. We run experiments on Adam with learning rates \(_{t}=(t^{-a})\) for \(a\{0.3,0.5,0.7,1\}\), and report the log-log plots in Figure 2, where we perform the experiments for Adam with/without the stability constant separately. In the log-log plot, we observe that after a certain number of iterations, curves for \(a<1\) almost appear as straight lines, suggesting that the normalized \(_{}\)-margin converges in polynomial time for \(a<1\), while the curve for \(a=1\) exhibits logarithmic behavior, indicating the normalized \(_{}\)-margin converges logarithmically in \(t\) for \(a=1\). Similarly to the previous observations, there is still no significant distinction between Adam with and without \(\), further demonstrating that our theoretical setting, which disregards \(\), is reasonable. We also note that in Figure 2, the margin achieved by Adam with \(_{t}=(t^{-0.3})\) converges the fastest. However, as we have commented in Section 4, different learning rate schedules may perform differently on different data sets, and it is not necessarily true that \(_{t}=(t^{-0.3})\) is always the best learning rate schedule.

## 6 Proof Sketch for Theorem 4.5

In this section, we explain how we establish the convergence of the \(_{}\)-margin of linear models trained by Adam, and provide the sketch proof of Theorem 4.5. For simplicity, here we focus on the case \(=_{ exp}\). The proof for \(=_{ log}\) is almost the same.

Figure 1: Normalized \(_{}\)-margins and \(_{2}\)-margins achieved by GD, GDM, and Adam with/without the stability constant \(\) during training. (a) gives the results of normalized \(_{}\)-margins, while (b) shows the results of normalized \(_{2}\)-margins.

Figure 2: Log-log plots of the normalized \(_{}\)-margin gaps \(|_{i[n]}_{t},y_{i}_{i}/\| _{t}\|_{}-|\) versus training iterations. (a) presents the results for Adam with the stability constant \(\), and (b) presents the results for Adam without the stability constant \(\).

We first introduce several notations. Define

\[()=-_{i=1}^{n}^{}(, y_{i}_{i}).\]

Then for \(\{_{},_{}\}\), it is clear that \(()>0\) for all \(^{d}\). In the following, we will show that \(()\) plays a key role in the convergence and implicit bias analysis.

**Step 1. Accurate characterizations of the first and second moments.** Adam algorithm is defined based on the first and second moments \(_{t}\) and \(_{t}\), which are calculated as exponential moving averages of the historical gradients and squared gradients respectively. A key challenge in studying Adam is to accurately characterize each entry of \(_{t}\) and \(_{t}\) throughout training. We present the following lemma.

**Lemma 6.1**.: Under the same condition in Theorem 4.5, there exists \(t_{1}=t_{1}(_{1},_{2},B)\) such that

\[_{t}[k]-(1-_{1}^{t+1})(_{t})[k] c_{m}_{t}(_{t}),\] \[_{t}[k]}-^{t+1}} (_{t})[k] c_{v}}(_{t})\]

for all \(t>t_{1}\) and \(k[d]\), where \(c_{m}\) and \(c_{v}\) are constants that only depend on \(_{1}\), \(_{2}\) and \(B\).

Since \(_{t}\), \(_{1}^{t+1}\) and \(_{2}^{t+1}\) all decrease to zero as \(t\) increases, Lemma 6.1 implies that after a sufficient number of iterations, the entries of \(_{t}\) and \(_{t}\) will be close to the corresponding entries of \((_{t})\) and \(|(_{t})|\) respectively. Notably, the term \((_{t})\) also appears in the bounds. In fact, deriving such bounds with the factor \((_{t})\) is essential to enable our implicit bias analysis: when the algorithm converges, by definition, \((_{t})\) will also decrease to zero, which implies that the bounds with the factor \((_{t})\) are strictly tighter than the bounds without \((_{t})\). Lemma 6.1 is one of our key technical contributions.

**Step 2. \((_{t})\) starts to decrease after a fixed number of iterations.** Based on Lemma 6.1, we can analyze the convergence of \((_{t})\). Specifically, we can show that, after a fixed number of iterations, the training loss function will start to decrease. This result is summarized in the following lemma.

**Lemma 6.2**.: Under the same condition in Theorem 4.5, there exist \(t_{1}=t_{1}(_{1},_{2},B)\) such that for all \(t>t_{1}\), it holds that

\[(_{t+1})(_{t})- _{t}1-C_{1}_{1}^{t/2}-C_{2}d_{t}^{ }+_{t}(_{t}),\]

where \(C_{1},C_{2}\) only depend on \(_{1},_{2},B\).

Note that by definition, \(()>0\) for all \(^{d}\). Therefore, Lemma 6.2 implies that \((_{t})\) starts to decrease after a fixed number of iterations, and gives a bound on the decreasing speed. We remark that the proof of Lemma 6.2 is highly non-trivial. Although we have related \(_{t}\) and \(_{t}\) to the loss gradient \((_{t})\) in Lemma 6.1, the fact that \(_{t+1}\) is updated according to the entry-wise ratio \(_{t}/_{t}}\) still introduces challenges: under our problem setting, it is entirely possible that at a certain iteration, a certain entry of \((_{t})\) will exactly equal zero. In this case, the results in Lemma 6.1 can not directly lead to any conclusions about the ratio \(_{t}/_{t}}\). In our proof, we implement a careful inequality that also takes the historical values of \((_{t})\) into consideration.

**Step 3. Lower bound for un-normalized margin.** The proof of the implicit bias towards maximum \(_{}\)-margin also relies on a tight analysis on the un-normalized margin \(_{i[n]}_{t},y_{i}_{i}\) during training. We have the following lemma providing a lower bound on the un-normalized margin.

**Lemma 6.3**.: Under the same condition in Theorem 4.5, if there exists \(t_{0}\) such that \((w_{t})\) for all \(t t_{0}\), then it holds that

\[_{i[n]}_{t},y_{i}_{i} _{=t_{0}}^{t-1}_{}(_{})}{ (_{})}-C_{3}d_{=t_{0}}^{t-1}_{ }^{}+_{=t_{0}}^{t-1}_{}^{2}-C_{4}\]

for all \(t t_{0}\), where \(C_{3},C_{4}\) only depend on \(_{1},_{2},B\).

Note that this lower bound contains a negative term \(-C_{3}d_{=t_{0}}^{t-1}_{}^{3/2}+_{=t_{0}}^{t-1} _{}^{2}\). Under our (mild) assumptions on the learning rates, it is entirely possible that \(+\) and thus the negative term in the lower bound may go to \(-\). However, we can show that \(_{t}(_{t})/(_{t})=1\) (in fact, for exponential loss, it is obvious that \((_{t})/(_{t})=1\)). Therefore, after a fixed number of iterations, the positive term in the lower bound will dominate, and Lemma 6.3 gives a non-trivial bound. The strength of this lemma lies in its applicability to very general learning rates \((_{t})_{t=1}^{}\).

**Step 4. An upper bound of \(\|_{t}\|_{}\).**

In Lemma 6.3, we have obtained a lower bound of the un-normalized margin. However, to show the convergence of the \(_{}\)-normalized margin, we also need to establish a tight upper bound of \(\|_{t}\|_{}\). We present this result in the following lemma, which is inspired by Lemma 4.2 in .

**Lemma 6.4**.: Suppose that the same conditions in Theorem 4.5 hold. There exist \(C_{5},C_{6}\) that only depend on \(_{1},_{2},B\), such that the following result hold: if there exists \(t_{0}>(1/)\) such that \((w_{t})+C_{5}_{0}}}\) for all \(t t_{0}\), then \(\|_{t}\|_{}_{=t_{0}}^{t-1}_{}+C_{6}_{ =0}^{t_{0}-1}_{}\) for all \(t>t_{0}\).

Lemma 6.4 gives an upper bound of \(\|_{t}\|_{}\) which mainly depends on \(_{=t_{0}}^{t-1}_{}\). Note that Lemma 6.3 also gives a lower bound of the un-normalized margin which mainly depends on \(_{=t_{0}}^{t-1}_{}(_{})/( _{})\). These two lemmas will be combined to derive the convergence of the normalized margin.

**Step 5. Finalizing the proof.** Finally, based on the lemmas established in the previous steps, we can prove Theorem 4.5. We also need the following utility lemma provided by .

**Lemma 6.5** (Lemma A.2 in ).: For Adam iterations defined in (3.2)-(3.4) with \(_{1}_{2}\) and let \(=(1-_{1})^{2}}{(1-_{2})(_{2}-_ {1}^{2})}}\), then \(_{t}[k]_{t}[k]}\) for all \(k[d]\).

We are now ready to prove Theorem 4.5 for the case \(=_{}\).

Proof of Theorem 4.5.: By Lemma 6.2, there exists \(t_{2}=t_{2}(d,_{1},_{2},,B)\) such that

\[(_{t+1})(_{t})-}{2}(_{t})\] (6.1)

for all \(t t_{2}\). Note that for \(=_{}\), by definition we have \((_{t})=_{i=1}^{n}(-_{ t},y_{i}_{i})=(_{t})\). Therefore, for all \(t>t_{2}\), (6.1) can be re-written as

\[(_{t+1})1-}{2} (_{t})(_{t}) e^{- }{2}}(_{t_{2}}) e^{-}^{t}_{}}{2}}.\]

Although \(_{}\) is not Lipschitz continuous over \(\), we have \((_{t_{2}})(_{0}) e^{ B _{=0}^{t_{2}-1}_{}}\) by Lemma 6.5 and triangle inequality. Letting \(_{0}=\{,+C_{5}_{0}}}\}\) and \(t_{0}=t_{0}(n,d,_{1},_{2},,B,,_{0})\) be the first time such that \(_{=t_{2}}^{t_{0}-1}_{}_{= 0}^{t_{2}-1}_{}+(_{0})-2_{0}}{}\) and \(t_{0}-\). By such definition of \(t_{0}\), we can derive that for all \(t t_{0}\),

\[(_{t})(_{t_{2}}) e^{-}^{t}_{}}{2}} e^{-}^{t_{0}-1}_{}}{2}}_{0} e^{-}^{t_{0}-1}_{}}{2}},\]

which proves the bound on \((_{t})\). Since \(t_{0}\) satisfies all the conditions in Lemmas 6.3 and 6.4, by Lemmas 6.3, 6.4 and the fact that \((_{})=(_{})\) for exponential loss, we have

\[_{t},y_{i}_{i}}{\|_{t}\| _{}}}^{t-1}_{}-C_{3}d_{ =t_{0}}^{t-1}_{}^{}+_{=t_{0}}^{t-1}_{} ^{2}-C_{4}}{_{=t_{0}}^{t-1}_{}+C_{6}_{=0}^{t_{0 }-1}_{}}\]

for all \(i[n]\), where \(C_{3},C_{4}\) and \(C_{6}\) are constants solely depending on \(_{1},_{2}\) and \(B\). Now by definition, we have \(_{i[n]}_{t},y_{i}_{i} /\|_{t}\|_{}\). Therefore, we have

\[|_{i[n]}_{t},y_{i}_{i}}{\|_{t}\|_{}}-| _{=0}^{t_{0}-1}_{}+C_{3}d _{=t_{0}}^{t-1}_{}^{}+_{=t_{0}}^{t-1}_{ }^{2}+C_{4}}{_{=t_{0}}^{t-1}_{}+C_{6}_{=0}^{t_{0 }-1}_{}}\] \[ O^{t_{0}-1}_{}+d_{ =t_{0}}^{t-1}_{}^{}}{_{=0}^{t-1}_{}} ,\]

where the second inequality follows by the assumption that \(_{t} 0\). This finishes the proof.

Conclusion and Future Work

In this paper, we study the implicit bias of Adam under a challenging but insightful setting where the "stability constant \(^{*}\) is negligible and set to zero. We demonstrate that Adam has an implicit bias converging towards the maximum \(_{}\)-margin solution, and such convergence occurs in polynomials of time for a general class of learning rates. This result further helps to understand the distinctions between Adam and (stochastic) gradient descent with/without momentum, whose iterates will eventually converge to the maximum \(_{2}\)-margin solution with an \(O(1/ t)\) convergence rate. This finding aligns with the implicit bias of Adam observed in experiments, for both cases the stability constant \(\) is zero and \(10^{-8}\). We predict that similar result can be extended to homogeneous neural networks, and we believe that this is a good future work direction. Moreover, since this paper focuses on full-batch Adam, another feasible future work is to investigate the implicit bias of stochastic Adam based on our results. In addition, establishing matching lower bounds for the loss and margin convergence rates for Adam is also an interesting future work direction.