# A Sober Look at the Robustness of CLIPs

to Spurious Features

Qizhou Wang\({}^{1}\) Yong Lin\({}^{2}\) Yongqiang Chen\({}^{3}\)

\({}^{1}\)TMLR Group, Department of Computer Science, Hong Kong Baptist University

\({}^{2}\)The Hong Kong University of Science and Technology

\({}^{3}\)The Chinese University of Hong Kong

\({}^{4}\)University of Washington

\({}^{5}\)University of Illinois Urbana-Champaign

Equal contributions.Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).

###### Abstract

Large vision language models, such as CLIP, demonstrate impressive robustness to spurious features than single-modal models trained on ImageNet. However, existing test datasets are typically curated based on ImageNet-trained models, which aim to capture the spurious features inherited in ImageNet. Benchmarking CLIP models based on the ImageNet-oriented spurious features may not be sufficient to reflect the extent to which CLIP models are robust to spurious correlations within CLIP training data, e.g., LAION. To this end, we craft a new challenging dataset named CounterAnimal designed to reveal the reliance of CLIP models on realistic spurious features. Specifically, we split animal photos into groups according to the backgrounds, and then identify a pair of groups for each class where a CLIP model shows high-performance drops across the two groups. Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models. We provide theoretical insights that the CLIP objective cannot offer additional robustness. Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data. We find that they still help mitigate the spurious features, providing a promising path for future developments.

## 1 Introduction

Large vision language models (LVLMs) have demonstrated huge success across a wide range of vision and multi-modal tasks, surpassing conventional ImageNet (-trained) models by a remarkably large margin . LVLMs are typically trained with or based on Contrastive Language Image Pre-training (CLIP)  on an unprecedented scale of real-world vision and language data such as LAION , which are significantly larger than ImageNet. The huge success of CLIP has presented a paradigm shift for modern vision and vision-language models to conduct the pre-training from ImageNet benchmarks to web-scale multi-modal datasets .

A key signature of CLIP models is the impressive robustness against various ImageNet-oriented distribution shifts , which is shown to be prohibitive to ImageNet models . The performanceboosts over ImageNet models seem to suggest that CLIP resolves distribution shifts, thereby sparking a rich discussion about its rationale [6; 7; 8; 9; 10]. However, _the elephant in the room_ is that adopted testsets (i.e., ImageNet variants) to evaluate the robustness of CLIPs are primarily designed for ImageNet-based models [5; 11]. These datasets may not correctly reflect the exact robustness of CLIP, given that CLIP models are trained on a large amount of data that may include, and possibly extend beyond those ImageNet variants during pre-training . In this paper, we investigate the robustness of CLIP to distribution shifts caused by the presence of spurious features. These features are highly correlated with labels, but this correlation may break down under distributional shifts [12; 13; 14; 15; 16; 17; 18; 19; 20]. We raise a challenging research question in the following:

_Is there a benchmark that reflects the exact reliance on spurious features of CLIP?_

Sadly, most of the existing benchmarks [21; 22; 23; 24] are tailored primarily for ImageNet models, which are unsuitable for CLIP. To fill this gap, we introduce a new testset, named CounterAnimal, specifically designed for assessing the robustness of CLIP models against real-world spurious features. Figure 1 presents several examples of CounterAnimal where data are divided into two groups, a) the easy group: animals in commonly appeared backgrounds that the CLIP models make correct predictions, and b) the hard group: animals in less commonly yet still plausible backgrounds, where the CLIP models are likely to misclassify them. Intuitively, the easy part captures some real-world biases that the web-scale data may naturally inherit. Hence, by comparing the performances of the two groups, one can quantify to what extent the model relies on spurious features.

More specifically, the CounterAnimal dataset is curated based on raw photos collected from iNaturalist3. The construction pipeline consists of 4 steps. a) Data collection: querying iNaturalist with each animal class, where we select some of the animal names from the ImageNet-1K dataset . b) Data curation: manually cleansing low-quality photos that potentially contain ambiguity and corruption. c) Background labeling: manually annotating photos with their respective backgrounds, selected from the label space of the candidate backgrounds. d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot performance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds. The resulting CounterAnimal dataset covers a total of 45 animal classes, and ends up with 7,174 easy photos and 5,926 hard photos, aligning with the standard size as an evaluation dataset, such as [26; 27]. Moreover, CLIP-LAION400M-ViT-B/32 is used as the proxy CLIP model in spurious discovering (cf., Appendix C.5 for the model naming rules).

We evaluate the CLIP models on our CounterAnimal with various backbones, e.g., ViT , along with different pre-train datasets, e.g., LAION . We also consider more advanced LVLMs like MiniGPT4  and LLaVA . We employ two evaluation setups crafted for different families of models (cf., Appendix C): a) **1 vs. 1000 setup**: using the full ImageNet-1K class names as the candidate label space and b) **1 vs. 20 setup**: using the top-20 most confusing classes regarding CLIP-LAION400M-ViT-B/32 as the candidate label space. We provide some of results in Table 1 and Figure 2, highlighting the key observations in the following:

    &  &  &  \\    & bkg & accu & bkg & accu & \\  ice bear & snow & 97.62 & grass & 70.91 & 26.71 \\ black swam & water & 93.63 & earth & 68.87 & 24.76 \\ flaming & water & 79.70 & sky & 55.45 & 24.25 \\ valture & sky & 87.76 & tree & 41.84 & 45.92 \\ dung beetle & earth & 56.92 & hand & 17.02 & 39.90 \\   

Table 1: 1 vs. 1000 results of exemplary animal classes within the CounterAnimal dataset for CLIP-LAION400M-ViT-B/32. “bkg” denotes the background label, “accu” (%) denotes the zero-shot accuracy, and “drop” (%) denotes the drop in accuracy between easy and hard groups.

Figure 1: We showcase CounterAnimal examples from the class of ice bear, separated into easy and hard groups with different backgrounds (i.e., snow and grass). The zero-shot performance of CLIP-LAION400M-ViT-B/32 drops from 97.62% (easy) to 70.91% (hard).

CounterAnimal captures general spurious correlations within CLIP.As exemplified in Table 1, we observe a significant drop of CLIP-LAION400M-ViT-B/32 in zero-shot accuracy from the easy to hard groups for each class. Furthermore, the observed biases in CLIP-LAION400M-ViT-B/32 also generalize to other CLIP configurations, with non-trivial performance drop from the easy to hard groups across various backbones and pre-train datasets as in Figure 2. It implies that CounterAnimal characterizes some general spuriousness common in large-scale multi-modal datasets.

**ImageNet models are more robust to spurious correlations captured by** CounterAnimal. Figure 2 also illustrates the performance changes of ImageNet models (colored in red). Compared with CLIP models (colored in blue), we find that ImageNet models exhibit stronger robustness to spurious correlations captured by CounterAnimal. Our findings contrast with previous studies that assess the ImageNet variants , highlighting that CLIP models do not always generalize better than ImageNet models. It underscores the necessity of choosing appropriate benchmarks to comprehensively assess the robustness of different models and training schemes.

**Larger CLIP models are more robust.** Shown also in Figure 2, we use the sizes and the color shades of the markers to indicate the scales of backbones and the pre-train datasets, respectively. Overall, larger CLIP backbone models (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance model performance against spurious features. In contrast, increasing the scale of the pre-train dataset (i.e., darker markers) does not yield the same improvement, implying that collecting more data alone cannot rectify much bias, which provides some new understanding in addition to the data-centric perspective [6; 10].

**CLIP models trained on high-quality data are more robust.** We categorize CLIP models into two distinct groups according to the pre-train data quality: a) CLIP-DC using DataComp  and CLIP-DFN employing Data Filtering Networks , as well as b) those pre-trained on datasets that lack stringent curation, labeled simply as CLIP. The results indicate that CLIP models pre-trained on high-quality datasets demonstrate enhanced robustness in general. It suggests that enhancing data quality remains a promising strategy for mitigating the spurious features.

**The CLIP objective may not offer additional robustness.** Complementary to our empirical observations, we also provide theoretical explanations for the reasons why CLIP learns spurious features. We further conduct confirmatory experiments that fine-tune CLIP models onto datasets with synthetic spurious features. The results align with our observations on CounterAnimal that the CLIP objective can not offer additional robustness over standard single-modal supervised training.

**Comparison with previous results.** Our work presents a new benchmark to effectively and systematically evaluate the robustness of CLIP models, which complements the literature in understanding the generalizability of CLIP models and LVLMs. More specifically,  reports that CLIP models

Figure 2: The easy vs. hard performance (%) for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., \(y=x\), where the models will not learn any bias.

may wrongly align co-occurred objects with their texts.  reports similar failure modes for more sophisticated LVLMs such as MiniGPT4 or LLaVA.  finds that CLIP misaligned samples will further cause the hallucination of LVLMs. Complementary to these works, our study explicitly characterizes the spurious features captured by CLIP and explains the existence of the reported failure cases. Our study provides interesting empirical and theoretical counterexamples to the previous beliefs for the substantial improvements in robustness for CLIP models, especially for those results observed on ImageNet variants [7; 8; 9; 35]. Based on the newly collected CounterAnimal dataset, we suggest that distribution shifts remain an open problem for CLIP models. Also, we need to be cautious about the test setups when evaluating new models pre-trained on datasets that differ significantly in scales and distributions from traditional ImageNet models.

**Comparison with previous benchmarks.** There are many other datasets to study distribution shifts, e.g., ImageNet variants [21; 26; 36; 37; 38; 39; 40], DomainBed , and Wilds . However, these datasets have biases when assessing the OOD robustness of CLIP models, as they may fail to represent the true OOD scenarios during CLIP training. Moreover, numerous recently released datasets, such as [22; 23; 43; 44; 45], have also explored distribution shifts. However, these studies primarily focus on synthetic distribution shifts, which may not fully represent real-world cases. In fact, it has been shown that previous OOD benchmarks are contained in CLIP training , making it hard to ablate ID/OOD cases for data in these benchmarks. Consequently, CLIP models have shown to be more robust than ImageNet models on these contaminated datasets .

## 2 Dataset and Evaluation Setups

To begin with, we describe the basic experimental setups, including the pipelines in constructing CounterAnimal, its key characteristics, as well as the adopted evaluation settings.

### Construction of CounterAnimal

We introduce the curation pipeline of our new dataset CounterAnimal, tailored for CLIP to investigate spurious correlations. The pipeline consists of 4 steps as follows:

**Data Collection.** We query animal names listed in the ImageNet-1K dataset and collect raw data via the search interface of iNaturalist, a global biodiversity data-sharing platform. We retrieve the latest 300-800 photos per animal class, organizing them based on the queried labels.

**Data Curation.** The collected raw samples are susceptible to noise and ambiguities. Therefore, we manually cleanse the low-quality data that fall into any one of the following 4 situations: label noise, feature noise, obscurity, and clarity. Label noise refers to cases where photos do not belong to the queried classes; feature noise refers to cases where some pixels are disrupted or missing; obscurity occurs when photos belong to more than one object class; clarity issues refer to cases where animal objects are largely occluded by the backgrounds or other irrelevant objects. It also includes the cases where animal objects do not occupy the majority of the space in photos.

**Background labeling.** We consider a typical form of spurious features where the backgrounds of photos can be biased . To identify such data for CLIP models, we manually label the backgrounds for the curated data. The considered class space of backgrounds is defined as follows: ground, water, earth, sand, snow, grass, human, sky, road, rock, shrub, indoor, tree, and outdoor. Note that the class space of backgrounds as above is not entirely orthogonal due to the inherent ambiguity: Some backgrounds may be ambiguous and some photos may contain more than one background. Nevertheless, we try our best to determine the assigned background labels for each animal class and exclude those photos challenging to be labeled.

**Spurious Discovery.** For each class, we quantify the impacts of spurious correlations to CLIP models by comparing the performances on the associated samples across different backgrounds. We take those classes as containing spurious features on which we observe a relatively obvious decrease in accuracy when changing backgrounds. In realization, we adopt the checkpoint of CLIP-LAION400M-ViT-B/32 for evaluation, where the prompt for its text encoder is "A photo of <object label>.", and the space of <object label> is the ImageNet-1K class names, i.e., we follow an 1 vs. 1000 setup. Then, we consider the classes where the zero-shot accuracy varies by more than 5% when changing backgrounds as the cases where CLIP model has learned the spurious features. The data with the preserved classes and backgrounds are used to create our final CounterAnimal dataset. Photos with the highest CLIP accuracy are assigned to the easy group, and those with the lowest CLIP accuracy are assigned to the hard group. We further refine the collected data to remove any mistake that the labelers may made during data curation and background labeling.

Our objective in developing CounterAnimal is to reflect the spurious correlations learned by CLIP. Therefore, we need to employ the CLIP models for dataset curation and thus ensure the construction is effectively biased towards CLIP configurations . In Appendix E, we further show that our data curation pipeline is general and reliable to characterize the spurious features within the considered models. Moreover, our experimental results later in Section 3 will corroborate that the spurious features captured by our CounterAnimal dataset are general across different CLIP setups and may not be so influential for ImageNet benchmarks. These findings will justify that our crafted testset satisfies our primary objective in characterizing the spuriousness for CLIP specifically.

### Characteristics of CounterAnimal

We depict the data layout in Figure 3 and visualize the zero-shot gaps for each animal class in Figure 4, where we use CLIP-LAION400M-ViT-B/32 as our referred model. Please refer to the detailed object/background names concerning the easy and hard groups in Appendix B. Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the easy group yet is compromised when coming to the hard group. Accordingly, Figure 4 implies a reliance for the CLIP models on the backgrounds.

### Evaluation Setups

We evaluate a series of CLIP models on the CounterAnimal dataset for their zero-shot performance. For each class, we use the pre-defined prompt of "A photo of <object label>." as in our data collection procedure and the similarity between image and text embeddings in classification. By default, we use the label space of the ImageNet-1K dataset and report the top-1 accuracy, i.e., the 1 vs. 1000 setup. Moreover, when involving more advanced LVLMs, we adopt the 1 vs. 20 setup where we employ the top-20 most confusing classes regarding CLIP-LAION400M-ViT-B/32 as the candidate label space. For re-productivity, we adopt the pre-trained CLIP checkpoints from OpenCLIP  and ImageNet model checkpoints from the PyTorch repository. The model naming rules are in Appendix C.5 and the evaluation details are discussed in Appendix C.

## 3 Experimental Analysis

Our experiments center on the evaluation and the analysis of our CounterAnimal dataset. In Section 3.1, we examine the generality of the captured spurious correlations. In Section 3.2, we explore the potential facets that affect the robustness of CLIP models. In Section 3.3, we extend the evaluation to a broader family of models with different training paradigms.

Figure 4: The 1 vs. 1000 performance drop (%) with CLIP-LAION400M-ViT-B/32. The horizontal axis denotes the class IDs and the vertical axis denotes the percentage points of decline.

Figure 3: The data layout across various animal classes. The horizontal axis denotes the class IDs and the vertical axis denotes the number of photos for the easy and hard groups, respectively.

[MISSING_PAGE_FAIL:6]

data from OpenAI, along with backbones of increasing scales. We observe a clear trend indicating that larger models exhibit better performance against spurious correlations. It may tell us that larger models possess stronger robustness, making them less prone to the shortcuts of spurious features.

**Data Quality Matters.** Moreover, we observe that the results obtained with DataComp- and DFN-trained CLIPs exhibit better performance and smaller drops across backbones, Figure 8 offers their comparisons. We notice that these datasets have been stringently filtered and thus possess high-quality data. It indicates that enhancing data quality is still a promising way to improve OOD generalization.

Our analysis focuses on absolute performance drop. In Appendix F, we strengthen our conclusions by incorporating the analysis based on effective robustness , where our findings still hold.

### Evaluations for other Learning Paradigms

We extend our evaluations to broader families of models, including ImageNet-1K supervised models and more advanced LVLMs, such as MiniGPT4 and LLaVA.

**ImageNet Models.** We first extend our evaluations to include ImageNet models. The main results are summarized in Table 3. Moreover, Figure 9 further illustrates the accuracy drops of various CLIP models, in comparison to ImageNet models. Surprisingly, we find that ImageNet models are more robust to spurious features in CounterAnimal. This finding indicates that our CounterAnimal specifically characterizes the spurious features that are unique to CLIP configurations. Additionally, it indicates that spurious correlations in large-scale multi-modal data are distinct from that of the ImageNet scenarios which are widely used in conventional single-modal supervised learning. It further highlights the importance of our proposed dataset, which is especially suitable to study the spurious correlations for vision-language pre-training.

**Advanced LVLMs.** We further evaluate for more advanced LVLMs, which align CLIP visual encoders with advanced large language models like Vicuna . To reduce inference costs, our evaluation follows the 1 vs. 20 setup. We summarize their results in Table 4, along with the 1 vs. 20 results for several CLIP models (cf., Appendix F for more results). We further depict the full results in Figure 2(b). As we can see, these advanced LVLMs have lower performance yet smaller drops, but the spurious features in CounterAnimal still impact them.

## 4 Understanding Why CLIPs Rely on Spurious Features

To better understand the observed phenomena in Section 3, we present a theoretical analysis of why the CLIP models rely on spurious features. We begin by establishing the setup for analyzing multi-modal contrastive learning following .

**Definition 1** (Multi-modal Dataset).: _Consider \(n\) image-text pairs \(\{(_{I}^{i},_{T}^{i})\}_{i=1}^{n}\), both image \(_{I}^{i}\) and text \(_{T}^{i}\) are generated from the latent factor \(_{i}\), where \(=[z_{inv},z_{spu}]^{2}\) is composed of an invariant feature \(z_{inv}(_{inv}y,_{inv}^{2})\) and a spurious feature \(z_{spu}(_{spu}a,_{spu}^{2})\) with \((a=y)=p_{spu}\) otherwise \(a=-y\). \(y\) is the label uniformly drawn from \(\{-1,1\}\). The training data \(^{}\) is drawn with \( p_{spu} 1\) and OOD data \(^{*}\) is drawn with a \(p_{spu}=\)._

We employ two linear encoders: \(g_{I}:^{d_{I}}^{h}\) for the image modality and \(g_{T}:^{d_{T}}^{h}\) for the text modality, implemented as \(g_{I}(_{I})=_{I}_{I}\) and \(g_{T}(_{T})=_{T}_{T}\) with \(_{I}^{h d_{I}}\) and \(_{T}^{h d_{T}}\). The encoders are trained through the linearized contrastive loss [9; 50] that mimics the CLIP dynamics:

\[_{}=_{i}_{j i}(s_{ij}-s_{ii })+_{i}_{j i}(s_{ji}-s_{ii})+||_{I}^{T}_{T}||_{F}^{2},\] (1)

where \(s_{ij}=g_{I}(_{I}^{i})^{T}g_{T}(_{T}^{j})\) is the similarity with respect to the \(i\)-th image and \(j\)-th text representations. Once the CLIP \((g_{I},g_{T})\) has been trained, the performance will be measured in a zero-shot manner by matching the most similar caption with the corresponding object name filled in, such as "a photo of <object label>" . Intuitively, once the model focuses more on invariant features, it will have a better zero-shot classification accuracy across different distributions. Nevertheless, in the following theorem, we justify that CLIP remains to learn to use spurious features, aligning with our experimental observations on the CounterAnimal dataset.

**Theorem 1**.: _Given a multi-modal dataset (Def. 1) with suitable variance in the features \(_{inv}=(1)>_{spu}\), and spurious features with a large spurious correlation \(p_{spu}=1-o(1)\), an overparameterized CLIP model where \(n=(1),d_{M}=(n)\) and \(d_{T}=(n)\), if the spurious features (e.g., backgrounds of the image) takes up a relatively large amount of the image \(_{spu}^{2}+2}{2}_{inv}=1\), then with a high probability of at least \(1-O()=1-o(1)\), the CLIP model achieves a large error in zero-shot accuracy in the OOD test data where \(a y\):_

\[(g_{I},g_{T}) 1-(_{1})-o(1),\]

_and a small error in the OOD test data where \(a=y\):_

\[(g_{I},g_{T}) 1-(_{2})-o(1),\]

_where \(_{1}=^{2}+2-2_{spu}p_{spu}}{^ {2})^{2}_{inv}^{2}+(2_{spu}p_{spu}-1)^{2}_{spu}^{2}}}\), \(_{2}=p_{spu}-_{inv}^{2}}{^{2} )^{2}_{inv}^{2}+(2_{spu}p_{spu}-1)^{2}_{spu}^{2}}}\) and \(\) denotes the CDF of a standard normal distribution._

   LVLMs & easy & hard & drop \\  MiniGPTA-Viceuma7B & 47.99 & 39.73 & 8.26 \\ LiaVA1.5-7B & 40.06 & 30.09 & 9.97 \\  CLIP-LATION4000-ViT-L/14 & 80.90 & 63.31 & 17.39 \\ CLIP-OpenAI-ViT-L/14 & 85.38 & 70.28 & 15.10 \\ CLIP-Datacomb1B-ViT-L/14 & 89.29 & 79.90 & 9.39 \\ CLIP-LATION2B-ViT-L/14 & 82.23 & 66.27 & 15.96 \\ CLIP-DFR2B-ViT-L/14 & 90.77 & 80.55 & 10.22 \\   

Table 4: The 1 vs. 20 results of CounterAnimal for advanced LVLMs and several CLIP models. More results of CLIP models and ImageNet models can be found in Appendix F.

We leave more theoretical details as well as the proof to Appendix D due to space limit. Intuitively, Theorem 1 implies that once there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features, with object captions. Although our theory discusses a simplistic case of one invariant and one spurious feature, there could exist more features describing the objects and even more features describing the backgrounds. CLIP will fail to robustly align the visual features of objects to its captions, once there exists a spurious correlation between any of the background features with the object caption. Our theory is the first to provably demonstrate the drawbacks of CLIPs in OOD generalization, providing the foundation for future developments tackling the issue.

To verify our theory, we construct multi-modal datasets named ColoredCOCO following . It contains \(9\) classes and the spurious correlation in the training part is \(80\%\), i.e., each class has a correlation of \(80\%\) to a specific biased color and \(20\%\) uniformly correlates to \(10\) different randomly chosen colors, cf., Figure 10. The OOD datasets are built with classes randomly correlating to other \(8\) biased colors. We consider two prompts with different descriptiveness: a) obj: "a photo of <object label>" and b) objbkg: "a photo of <object label> in <color label> background", with either objects or both objects and backgrounds.

We tune the pre-trained CLIP models using the CLIP objective, which has been shown to be most robust to distribution shifts . In addition, we also incorporate the baseline of full fine-tuning with a new MLP onto the image encoder using the ERM objective. As shown in Figure 11, fine-tuning with CLIP objective based on neither of the prompts provides any non-trivial robustness against the vanilla full fine-tuning. The results further verify our theory. Nevertheless, the degraded robustness of CLIP could also be caused by the weak language understanding capability of the BERT encoder in the CLIP. To this end, we also conduct additional experiments with a perfect language encoder setting. The results are given in Appendix D.4. Nevertheless, we find that CLIP still performs similarly to ERM and is prone to distribution shifts even with perfect captions.

## 5 Conclusion

In this paper, we highlight biases in previous evaluations for assessing the robustness of CLIP models, primarily relying on ImageNet variants. Such improper benchmarking would cause illusions that CLIP models seem to resolve spurious correlations, particularly in comparison with ImageNet models. It motivates us to craft the new testset, named CounterAnimal, which is specifically designed to probe the natural spurious correlations between animal and their backgrounds. The spuriousness captured by CounterAnimal is general across different CLIP setups and exerts relatively small impacts on the ImageNet benchmarks, thereby specifically capturing the spurious correlations within CLIP setups. Our experiments on CounterAnimal show that many conventional strategies, e.g., increasing backbone scales and improving pre-train data quality, remain effective in enhancing the robustness of CLIP models. Moreover, we present a theoretical analysis for the reasons of the CLIP objective to learn biases. Overall, we provide a platform for future developments of more advanced and robust CLIP and vision-language models, and we hope our presented experiments can offer a sober look at the robustness of CLIP models to spurious correlations.