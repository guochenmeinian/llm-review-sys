# Probabilistic Emulation of a Global Climate Model

with Spherical DYffusion

Salva Ruhling Cachay

UC San Diego

&Brian Henn

Allen Institute for AI

&Oliver Watt-Meyer

Allen Institute for AI

Christopher S. Bretherton

Allen Institute for AI

&Rose Yu

UC San Diego

###### Abstract

Data-driven deep learning models are transforming global weather forecasting. It is an open question if this success can extend to climate modeling, where the complexity of the data and long inference rollouts pose significant challenges. Here, we present the first conditional generative model that produces accurate and physically consistent global climate ensemble simulations by emulating a coarse version of the United States' primary operational global forecast model, FV3GFS. Our model integrates the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead compared to single-step deterministic baselines. The model achieves near gold-standard performance for climate model emulation, outperforming existing approaches and demonstrating promising ensemble skill. This work represents a significant advance towards efficient, data-driven climate simulations that can enhance our understanding of the climate system and inform adaptation strategies.1

## 1 Introduction

Climate models are foundational tools used to understand how the Earth system evolves over long time periods and how it may change as a response to possible greenhouse gas emission scenarios. Such climate simulations are currently very expensive to generate due to the computational complexity of the underlying physics-based climate models, which must be run on supercomputers. As a result, scientists and policymakers are limited to exploring only a small subset of possibilities for different mitigation and adaptation strategies .

Training relatively cheap-to-run data-driven surrogates to emulate global climate models could provide a compelling alternative . Although recent deep learning models are on the verge of transforming the conceptually similar field of medium-range weather forecasting , these advances do not directly transfer to long-term climate projections . Indeed, most such models only report forecasts up to two weeks into the future and may diverge or become physically inconsistent over longer simulations. In contrast, climate projections demand accurate and stable simulations of the global Earth system spanning decades or centuries, requiring reliable reproduction of long-term statistics.

Figure 1: Weather performance (x-axis) is not a strong indicator of climate performance (y-axis). Each dot corresponds to a distinct sample or checkpoint epoch.

In Figure 1 we quantitatively show this divergence between the medium-range weather forecasting skill of ML models (measured as the average RMSE on 5-day forecasts) and their performance on longer climate time scales (measured as the RMSE of the 10-year time-mean). We have verified that this finding holds regardless of the analyzed variable and the proxy used for weather performance, which we discuss in more detail in Appendix E.3. Heuristically, optimizing weather skill ensures that a climate model takes a locally accurate path around the climate 'attractor', but it does not guarantee that small but systematic errors may not build up to distort that simulated attractor to have biased long-term climate statistics. While this is a little-discussed observation in the ML community, the climate modeling community has documented it for physics-based models .

A recent breakthrough is a deterministic surrogate called ACE (Ai2 Climate Emulator) , which remains remarkably stable and physically consistent over 10-year simulations at 6-hourly time steps, forced by time-varying specified sea-surface temperature and sea-ice. Its success can be attributed to careful data processing, problem design, and the Spherical Fourier Neural Operator (SFNO)  architecture. ACE is trained to emulate the United States' primary operational global forecast model, the physics-based FV3GFS , which is operationally used at the US National Weather Service and US National Centers for Environmental Prediction. ACE produces encouragingly small ten-year mean climate biases (i.e. biased long-term averages), but they are still significantly larger than the theoretical minimum imposed by internal variability of the reference physics-based model.

ACE's deterministic nature restricts its ability to model the full distribution of climate states or to facilitate ensemble simulations, which involve drawing multiple samples from the same model. These capabilities are crucial for climate modeling, as they enable better uncertainty quantification, more robust and physically consistent predictions, and a deeper understanding of potential future climate scenarios and associated risks . While it is possible to ensemble a deterministic model by perturbing its inputs, this approach often leads to under-dispersed (i.e. overly confident) ensembles compared to generative or physics-based approaches . Even then, the problem remains that due to optimizing them on MSE-based loss functions, the deterministic predictions may degrade to a mean prediction for longer forecast time scales and underestimate unlikely events .

A generative modeling approach, particularly the use of diffusion models , appears to be a promising solution to these challenges. However, standard diffusion models are computationally intensive to train and sample from. This complexity poses significant problems for climate modeling because: 1) atmospheric data is extremely high-dimensional, making the use of video diffusion models  prohibitive, even more so as this class of models still struggle with videos longer than a few seconds; and 2) the sampling speed of standard diffusion models is particularly problematic for long, sequential inference rollouts. For instance, generating a single 10-year-long simulation, as in our experiments, with a standard autoregressive diffusion model  that uses \(N\) diffusion steps would require \(14600 N\) neural network forward passes. If a second-order solver for sampling is used , this number doubles. Even with \(N\) as small as 30, this results in half a million forward passes to generate a single sample trajectory, severely limiting the potential of data-driven models to serve as fast surrogates for expensive physics-based models.

As a solution to this computational problem, we build upon the dynamics-informed diffusion model framework, DYffusion, from Ruhling Cachay et al. , which caps the computational overhead at inference time (as measured by the number of neural net forward passes) to less than \(3\) as much as for a deterministic next-step forecasting model such as SFNO or ACE. Unfortunately, the original DYffusion method relies on an UNet-based architecture designed for Euclidean data rather than physical fields on a sphere. As we show in Figure 1, this mismatch of inductive biases becomes more problematic at the long climate time scales that we focus on in this paper.

We address these limitations by carefully integrating the DYffusion framework with the SFNO architecture from Bonev et al. , and the data and evaluation procedure from Watt-Meyer et al. . To achieve this integration, we extend SFNO with time conditioning and inference stochasticity modules. Our proposed framework, Spherical DYffusion, achieves strong results: On average, across all 34 predicted fields, our model reduces climate biases to within \(50\%\) of the reference model, which is more than \(2\) and \(4\) lower than the best baselines. For critical fields, such as the derived total water path quantity, our method achieves results within \(20\%\) of the reference model, representing a \(5\) improvement over the next best baseline (see Fig. 2). Additionally, our method proves effective for ensemble climate simulations, reproducing climate variability consistent with the reference model and further reducing climate biases towards the theoretical minimum through ensemble-averaging.

Our generative model is a leap forward toward purely ML-based large ensemble climate projections that are both efficient and accurate. Our main contributions are:

1. We present the first conditional generative model for probabilistic emulation of a realistic climate model, with minimal computational overhead over deterministic baselines.
2. We carefully integrate two distinct frameworks, ACE and DYffusion, including additional modifications to the SFNO architecture such as time-conditioning modules.
3. We show that our integrated method performs considerably better than relevant baselines in terms of reduced climate biases, ensemble-based climate modeling, and consistent variability of the climate predictions.
4. We show that short-term weather performance does not necessarily translate to accurate reproduction of long-term climate statistics.

## 2 Related Work

ML for weather and climate modeling.There are fundamental differences in weather and climate modeling. Climate refers to the average weather over long periods of time2. While weather forecasting focuses on short time scales in the order of days or weeks, climate modeling simulates longer periods of decades to centuries. Weather forecasting is primarily an initial-value problem, for which it is important to analyze short-term time-specific predictions. Climate modeling is primarily a boundary-condition (or forcing-driven) problem , characterized by long-term averages and distributions.

Deep learning-based models have emerged as a much more computationally efficient alternative to traditional physics-based numerical weather prediction (NWP) models, showing impressive skill for deterministic medium-range weather forecasting . This success has been more recently extended to ensemble-based probabilistic weather forecasting . An alternative approach is hybrid modeling, where a physics-based component is complemented by ML-based parameterizations or corrections . At longer lead times, when weather becomes chaotic and less predictable, the ensemble mean prediction of a physics-based or probabilistic ML-based ensemble improves deterministic metrics such as root mean squared error (RMSE) over non-ensembled methods .

However, advances in weather forecasting hardly transfer to long-term climate projections. Fully data-driven models fail to maintain stability beyond two-week-ahead forecasts, as errors accumulate over their autoregressive rollouts. Weyn et al.  and Bonev et al.  showed stable forecasts for horizons of up to six weeks and one year, respectively. Only recently, Watt-Meyer et al.  notably achieved stable and accurate 10-year simulations, followed by another deterministic SFNO-based climate emulator showing promising results using four prognostic variables . Easier, but less flexible and

Figure 2: RMSE of 10-year time-means for a subset of important fields. The leftmost bar in the first two subplots shows the reference noise floor, determined by comparing ten independent 10-year reference FV3GFS simulations with the validation simulation. The scores computed using the mean over these ten simulations (a proxy for an ”ensemble prediction”) are shown in light shade. The subsequent bars show the corresponding scores for our method and the deep-learning baselines, using a 25-member ensemble for the probabilistic methods (all except ACE, which only reports scores for its single deterministic prediction). Scores computed using the ensemble-mean prediction are shown in light shade. The dark shaded bar on top indicates the performance drop when using a single member’s prediction only, with error bars representing the standard deviation over the 25 different member choices. The rightmost subplot displays the average time-mean RMSE of the ML-based emulators relative to the reference across all 34 variables. On average, our method’s time-mean RMSEs are \(50\%\) higher than the noise floor, which is less than half the average RMSE of the next best method, ACE. When using the 25-member ensemble mean prediction, this reduces to \(29.28\%\).

informative, alternatives to full-scale temporal modeling of atmospheric dynamics, include emulation of annual means given an emission scenario [66; 30; 46; 42], temporal super-resolution of monthly means , or debiasing climate model output [3; 7; 45].

Diffusion models.Diffusion models [25; 59; 60; 61] have demonstrated significant success in generating data such as natural images and videos. While traditionally formulated for finite-dimensional spaces, these models have been extended to function spaces . Their direct applications to autoregressive forecasting [35; 51] and downscaling [64; 43; 20] of physical data have shown promising results. However, these approaches inherit the computational complexity associated with training and sampling from standard diffusion models. This is particularly prohibitive for autoregressive predictions on climate time scales, as the total number of neural network forward passes increases proportionally with the number of sampling steps, typically ranging from 20 to 1000. Consequently, recent research that leverages insights from diffusion models to balance predictive performance and sampling speed appears more promising for assessing their viability in climate simulations [57; 41]. While diffusion models traditionally rely on U-Net architectures [55; 13], vision transformers have shown promising results in image synthesis [50; 29; 24]. Our work explores a different, neural operator-based, architecture for Earth data.

## 3 Background

We first define the problem and then introduce the key components in our framework, namely DYffusion and SFNO. We abbreviate a time series of tensors \(_{0},,_{t}\) with \(_{0:t}\).

### Problem Setting

Our goal is to learn the probability distribution \(P(_{1:H}\,|\,_{0},_{0:H})\) over a horizon of \(H\) time steps, conditional on initial conditions \(_{0}\) and a scenario of forcing variables \(_{0:H}\) (i.e. time-varying boundary conditions). In our paper, these forcings correspond to prescribed sea surface temperatures and incoming solar radiation (see Section 5.1), leaving it to future work to force based on greenhouse gas emission scenarios explicitly. Each \(_{t}^{D}\) represents the state of the atmosphere at a given timestep, \(t\), consisting of two- and three-dimensional surface and atmospheric variables across a latitude-longitude grid. These variables, which serve as both input and output, are referred to as _prognostic_ variables. We assume a constant time interval between successive time steps \(t\) and \(t+1\). To make training feasible, it is necessary to train on a much shorter horizon \(h\), i.e. learn the distribution \(P(_{t+1:t+h}\,|\,_{t},_{t:t+h})\), and apply the model autoregressively. This process begins with \(P(_{1:h}\,|\,_{0},_{0:h})\) and continues until reaching time step \(H\) at inference time.

### Diffusion Models and DYffusion

Diffusion models can be seen as a general paradigm to learn the target distribution \(p(^{(0)})\), by iterating over \(N\) diffusion steps of a forward or reverse process. We denote the states of each diffusion step with \(^{(n)}\), using a superscript \(n\) to clearly distinguish them from the physical time steps of the data \(_{t}\). Standard diffusion models [59; 25; 31], initialize the reverse process from a simple isotropic Gaussian distribution \(^{(N)}(,)\) so that as \(n 0\) the intermediate states \(^{(n)}\) are gradually denoised towards a real data sample \(^{(0)}\).

In Cold Diffusion , this paradigm is extended to more general data corruption processes such as blurring. Ruhling Cachay et al.  propose DYffusion, by adapting cold diffusion models to forecasting problems. The key idea is to make the forward and reverse processes dynamics-informed by directly coupling them to the physical time steps of the data. That is, the reverse process is initialized with \(^{(N)}=_{0}\) and iteratively evolves jointly with the dynamics of the data \(_{1},,_{h-1}\) to reach the data at some target time step, \(^{(0)}=_{h}\).

In DYffusion, the forward and reverse processes are informed by temporal dynamics in the data and do not rely on data corruption. Their only source of stochasticity comes from using a stochastic neural network as an operator for the forward process and is implemented by using Monte Carlo (MC) dropout . This forward process essentially corresponds to a temporal interpolator network, while the reverse process is represented by a multi-step forecasting network. Thus, compared to standard diffusion models, DYffusion requires training one more neural network, which they propose doing in separate stages, beginning with the interpolator model. Due to its dynamics-informednature, DYffusion was shown to be faster at sampling time and more memory-efficient than standard diffusion models, while matching or outperforming their accuracy.

### Spherical Fourier Neural Operator (SFNO)

The SFNO architecture  extends the FNO framework from Li et al.  to spherical data and symmetries such as the Earth. FNOs efficiently model long-range interactions in the Fourier space, but because the underlying Fast Fourier Transform is defined on a Euclidean domain, this can lead to modeling artifacts. SFNOs overcome this issue by using the spherical harmonic transform (SHT) , a generalization of the Fourier transform, instead. The SFNO model achieves higher long-term stability of autoregressive rollouts than the FNO model, showing stable forecasts of Earth's atmospheric dynamics for up to 1-year-long rollouts at six-hourly time steps. The ACE model from Watt-Meyer et al.  is based on the SFNO architecture, modifying some of the hyperparameters and the grid used for the first and last SHT of the SFNO. We use the SFNO configuration from ACE in our experiments.

## 4 Spherical DYffusion

SFNO and ACE are deterministic models that cannot be readily used for uncertainty quantification or ensemble-based climate modeling. DYffusion introduces an efficient diffusion-based approach specifically for forecasting problems but only for Euclidean data. Thus, we propose Spherical DYffusion, a deep generative model for data-driven probabilistic climate simulations that carefully integrates SFNO and DYffusion into an unified framework.

DYffusion requires two neural networks that are used for temporal interpolation and direct multi-step forecasts. In the original framework, these are UNet-like networks. For our approach, we propose to replace them with modified versions of the SFNO architecture, which we denote by SFNO\({}_{}\) and SFNO\({}_{}\), respectively.

Training.We follow the original training procedure from DYffusion, complementing it with the use of the input-only forcing variables. That is, for a specified training horizon \(h\), these networks are

Figure 4: Diagram of one of the blocks of the modified SFNO architecture for our proposed method. The full architecture consists of a sequence of 8 such blocks. Our newly introduced time-conditioning modules correspond to the Time Embedding, followed by the MLP on the right, and the scale-shift operation. Our method relies on dropout, which is part of the two-layer MLP on the top. SFNO-based baselines use the same architecture and hyperparameters without the time embedding module.

trained in two stages such that for sequences of prognostic data \(_{t:t+h}\) and forcings \(_{t:t+h}\)

\[_{}(_{t},_{t+h},_{t},i) _{t+i}\] \[_{}(_{}(_{t},_{t +h},_{t},j),_{t+j},j) _{t+h},\]

where \(i\{1,,h-1\}\) and we use \(j\{0,1,,h-1\}\), defining \(_{}(_{t},,,0)=_{t}\). In our experiments, we use \(h=6\). Here, \(\) refers to the random variable representing the interpolator network's inference stochasticity. We discuss its implementation further below. The forecaster network, \(_{}\), is deterministic. The full training scheme is defined in Algorithm 1.

Inference.At inference time, we follow the DYffusion sampling scheme based on cold sampling . Essentially, we start with the initial conditions \(_{0}\) to generate a first forecast of time step \(h\) through a forward pass of the forecaster network, i.e. \(}_{h}=_{}(_{0},_{0},0)\). Given this prediction, we can now use the interpolator network to interpolate \(}_{1}=_{}(_{0},}_{h},_{0 },1)\). In practice, cold sampling applies a correction term to this estimate. The prior forecast of \(_{h}\) can now be refined with \(}_{h}=_{}(}_{1},_{1},1)\). The alternation between forecasting and interpolation continues until \(_{}\) predicts \(}_{h-1}\) and the forecaster network performs a last refinement forecast of time step \(h\), conditioned on the time \(j=h-1\) and interpolated sample \(}_{h-1}\). After this final forecast of \(_{h}\), the process is repeated autoregressively, starting with \(_{h}\) as the new initial condition. This slightly simplified sampling process is illustrated in Figure 3 and fully described in Algorithm 2. Repeating this sampling process multiple times using the same initial conditions will lead to an ensemble of samples, thanks to the interpolator network being stochastic.

SFNO time-conditioning.To use SFNO as described above, it is necessary to implement time-conditioning modules that allow the interpolator and forecaster networks to be conditioned on the time \(i\) and \(j\), respectively, given that the original SFNO architecture does not support this. We follow the same approach taken by standard diffusion models , which consists of transforming the time condition into a vector of sine/cosine Fourier features at 32 frequencies with base period 16, then pass them through a 2-layer MLP to obtain 128-dimensional time encodings that are mapped by a linear layer into the learnable scale and offset parameters. We scale and shift the neural representations of every SFNO block directly following the normalization layer and preceding the application of the SFNO spectral filter, as shown in Figure 4.

SFNO inference stochasticity.A stochastic interpolator network, made explicitly through the random variable \(\) above, was shown to be a key design choice in the original DYffusion framework. However, to the best of our knowledge, the SFNO model has been only used for deterministic modeling. We overcome this issue through MC dropout , i.e. enabling dropout modules  at inference time. Following the original SFNO implementation (of training-time-only dropout), we propose to use a dropout module inside the MLP of each SFNO block. In addition, we enable stochastic depth -also known as drop path-at inference time at a rate of \(0.1\). Stochastic depth randomly skips a whole SFNO block. When this happens the whole block reduces to the identity function, since only the residual connection is enabled. To the best of our knowledge, this has not been explored before as a source of inference stochasticity.

## 5 Experiments

### Dataset and Experimental Setup

To compare our proposed method against ACE , we use the same dataset, training and evaluation setup. The dataset consists of 11 distinct 10-year-long simulations from the state-of-the-art global atmospheric model FV3GFS , saved every 6 hours. The forcings consist of annually repeating climatological sea surface temperature (1982-2012 average) and incoming solar radiation. Greenhouse gas and aerosol concentrations are kept fixed. The data was regridded conservatively from the cubed-sphere geometry of FV3GFS to a \(1^{}\) Gaussian grid, and filtered with a spherical harmonic transform round-trip to remove artifacts in the high latitudes. We train on 100 years of simulated data from FV3GFS, and evaluate the models on how well they can emulate a distinct 10-year-long validation simulation (i.e. \(H=14600=10 365 4\)). The 11 simulations form an initial-condition ensemble, where each simulation is independent of the other-after some discarded spinup time-due to the chaoticity of the atmosphere . For more details, see Appendix B.

### Baselines

We compare with the following baselines for climate projection.

* **ACE** applied the SFNO architecture to the FV3GFS dataset described above.
* **ACE-STO**: We re-train ACE but use MC dropout, in the same way how it is applied in SFNO\({}_{}\) for our method, to generate stochastic predictions.
* **DYffusion**: We train DYffusion using the original UNet-based architecture as its interpolator and forecaster neural networks.
* **Reference**: physics-based FV3GFS climate model simulations. We use the ten training simulations to create a 10-member reference ensemble that we use to more robustly estimate the 'noise floor' introduced in  and to compare the variability of the reference ensemble with sample simulations from our method. Note that this reference is _not_ appropriate for weather forecasts given that it is initialized from different initial conditions.

It is worth noting that ACE also compared their results against a physics-based baseline called C48, which corresponds to running FV3GFS at half the original spatial resolution. This makes C48 around \(8\) less computationally costly to run compared to the reference simulations but was shown to underperform ACE, which our method is shown to outperform in the experiments below.

For ACE, we directly use the pre-trained model from the original paper. ACE was trained on a next-step forecasting objective based on a MSE loss. For ACE-STO, we re-train ACE from scratch with the only difference being that we use a dropout rate of \(10\%\) for the MLP in the SFNO architecture. We use the same dropout rate for the interpolator model, SFNO\({}_{}\), in our method. For both DYffusion and our approach, we choose \(h=6\). That is, these models are trained to forecast up to 36 hours into the future. We use the same training and sampling procedures for both, the only difference being the underlying neural architectures.

Runtime analysis.In Table 1, we report the computational complexity in terms of the number of neural function evaluations (NFEs) needed to predict \(h\) time steps, and the wall clock runtime for simulating one complete validation trajectory of 10 years. For our method, NFEs is not \(3h\) because in the first and last iteration we do not need to actually run line 8 and lines 7 & 8 in Algorithm 2, respectively. Our runtime analysis confirms that the computational overhead at inference time for using our method, is less than \(3\) as much as for a deterministic next-step forecasting model such as SFNO or ACE. This enables our method to provide significant \(25\) speed-ups and associated energy savings over using the emulated physics-based model, FV3GFS.

All models were trained on A6000 GPUs using distributed training on 2 up to 8 GPUs, ensuring that the effective batch size remains the same (see Figure 8). For a fair inference runtime comparison measuring the wall clock time needed to simulate 10 years (i.e. one full validation rollout), we run all deep-learning baselines on one A100 GPU. We also include the runtime for the physics-based FV3GFS climate model which was run on 96 cores (24 cores for the \(2\) coarser version) of AMD EPYC 7H12 processors. The deep learning methods are not only much faster, but also much more energy-efficient than FV3GFS.

For illustrative purposes, we also report the complexity of a standard autoregressive diffusion model  approach in terms of the number of neural function evaluations (NFEs) needed to predict \(h\) time steps, totaling to \(Nh\) where \(N\) is the number of sampling steps required to reverse the diffusion process. \(N\) usually ranges from between 20 to 1000. This makes the use of such an approach less attractive for climate emulation since the resulting inference runtime would not offer as significant speed-ups over the physics-based reference model.

   Method & NFE & Runtime \\  ACE / SFNO & \(h\) & 01:08 \\ Standard diffusion & \(Nh\) & N/A \\ Ours & \(3(h-1)\) & 02:56 \\  Physics-based FV3GFS & N/A & 78:04 \\ FV3GFS (\(2\) coarser) & N/A & 45:38 \\   

Table 1: Computational complexity of the different deep learning methods in terms of: 1) the number of neural function evaluations (NFEs) needed to predict \(h\) time steps. and 2) Total inference runtime (simulating 10 years), including the time needed to compute metrics (in hours:minutes). \(N\) refers to the number of diffusion steps which usually ranges between \(20\) to \(1000\).

### Climate Biases

Metrics.The most crucial quality of an ML-based climate model is its ability to reproduce the climatology of the emulated reference system, i.e. the long-term average ("time-mean") of weather states. The time-mean of the validation simulation is defined as \(_{t=1}^{H}_{t}\). The time-mean for each model is defined as \(_{t=1}^{H}}_{t}\), where \(}_{t}\) is the model's prediction for time step \(t\). These two quantities are then compared against each other using the bias, i.e. prediction - target, and root mean squared error (RMSE) as key metrics of interest for analyzing climate biases. For the probabilistic methods, i.e. ours, DYffusion, and ACE-STO, we generate simulation ensembles by sampling from the model multiple times using the same initial conditions. Unless specified otherwise, all ensemble results are based on \(E=25\) ensemble members. We evaluate the ensemble performance using two metrics: the RMSE of the ensemble-mean prediction (\(_{e=1}^{E}_{t=1}^{H}}_{t,e}\)) and the RMSE of member-wise time-means (\(_{t=1}^{H}}_{t,e}\)), where \(e\) indexes individual ensemble members. For the latter, standard deviations are computed over the member-wise errors. The corresponding "optimal noise floor" for the ML-based emulators is estimated by comparing the validation simulation with the 10-member reference ensemble. All metrics, which are fully defined in Appendix D, are weighted by grid cell area. It is important to acknowledge the potential for improving the estimate of the "noise floor" based on statistical significance testing and improved metrics .

Quantitative analysis.Our method and all baselines consistently produce stable long-term climate simulations without diverging. In Figure 2, we compare the RMSE of the time-means of the reference, our method, and all baselines.

Our method significantly reduces climate biases compared to baseline methods across most fields, with errors often closer to the reference simulation's noise floor than to the next best baseline. The performance of ACE is notably degraded when made stochastic through MC dropout. Similarly, a direct application of DYffusion fails to accurately reproduce long-term climate statistics. Both these baselines are unable to outperform or even match the scores of the deterministic ACE baseline. Only our proposed careful integration of these two paradigms leads to a skillful climate model emulator: On average, our method's time-mean RMSEs are only \(49.36\%\) higher than the noise floor, which is less than half the average RMSE (\(110.47\%\)) of the next best method, ACE.

Ensemble averaging significantly enhances our method's performance, reducing climate biases by \(29.28\%\) on average across all variables. As shown by the light shading in Fig. 2, the ensemble-mean predictions consistently achieve lower time-mean RMSEs compared to single-member predictions (dark shading). This ensemble-based improvement distinguishes our approach from ACE-STO and DYffusion, where ensemble averaging proves less effective, and from ACE, where initial-condition perturbations would be required for ensembling. Additional results for more fields are available in Figure 9 of the Appendix. Our comprehensive evaluation in Table 4 includes ensemble metrics such as the Continuous Ranked Probability Score (CRPS) and spread-skill ratio. The results demonstrate that our method outperforms alternatives in emulating the 10-year time-mean climatology of the reference model for most variables and metrics. However, some challenges remain, particularly in matching the reference ensemble's performance for stratospheric (level 0) variables and in achieving better ensemble scores.

Figure 5: Global maps of the 10-year time-mean biases of a single sample from the reference noise floor simulation, our model, and the ACE baseline for the total water path field. Each subplot reports the global mean RMSE and bias of the respective bias map. Our model reproduces biases of similar location and magnitude to the reference noise floor, suggesting they are mainly due to internal climate variability rather than model bias, while the baseline exhibits larger climate biases.

Qualitative analysis.In Figure 5 we show the corresponding global maps of the time-mean biases for the total water path (TWP) field. Our model reproduces small biases of remarkably similar location and magnitude to the "perfect-model" reference simulation, with spatial pattern RMSEs of approximately 1% of the global-time-mean TWP. The perfect-model bias is due to unforced random decadal variability in the mean climate of the reference model - each 10-year period has randomly different weather, leading to a slight difference in 10-year time-mean averages across this weather. The reference bias is due to comparing one such decade simulated with the reference model with other simulated decades; its spatial pattern depends strongly on which decade is used for computing the reference model climatology. That our model (trained on 100 years of output) reproduces this pattern suggests that it emulates the long-term (e.g. century-long) time-mean statistics of the reference model even more accurately than a 10-year-mean RMSE can reliably resolve. On the other hand, the baseline ACE model exhibits somewhat larger climate biases, indicative of an actual, albeit small model deficiency that is already evident with a single 10-year estimate of climatology.

In Appendix E.4, we visualize two sample 10-year trajectories simulated by Spherical DYffusion as well as the corresponding validation simulation from FV3GFS. Supplementary videos demonstrate the full temporal evolution of key derived variables: near-surface wind speed3 and total water path4. The emulated fields demonstrate high realism, closely mimicking the patterns and variability observed in actual climate model outputs. This showcases Spherical DYffusion's capability to generate plausible and physically consistent climate scenarios over decadal timescales.

Climate variability.Above, we have verified that sampling 10-year-long trajectories from our model produces encouragingly low ensemble mean and member-wise time-mean biases. An important feature of climate is its natural variability on time scales of years, decades, or even centuries even when external forcings (e.g. sunlight or greenhouse gas concentrations) remain unchanged. For instance, multi-decadal periods of relative drought have stressed many past human civilizations. The present simulations are more constrained than natural climate variability because they employ a repeating cycle of sea-surface temperature and thus do not allow for feedbacks between the atmosphere, ocean, vegetation, and cryosphere. Nevertheless, an important quality of an ML emulator of the global atmosphere suitable for climate studies is that it simulates a similar level of low-frequency climate variability as the reference model.

Here, we verify that our time-mean ensemble passes this challenging test, measured using the intra-ensemble variability of time-mean averages of a few important climate statistics simulated by 25-member ensembles of the emulators vs. the ten reference simulations. We measure this variability by computing the area-weighted average of the standard deviation of time-means across the ensemble dimension. In Table 2 we show that the resulting global mean variability of the ensemble of time-means of our method is within 10-20% of those of the reference simulations for all tabulated variables (and other predicted fields). DYffusion achieves similarly accurate ensemble variability, while ACE-STO In Appendix E.1.2 we show that the corresponding global maps of the time-mean variability reveal similar spatial patterns. That is, our method generates ensemble climate simulations with decadal variability consistent with the underlying climate model.

100-year-long simulation.We evaluate the long-term stability of Spherical DYffusion through a 100-year simulation, a critical timescale for many climate modeling applications. Figure 6 demonstrates the model's robustness through time series of key global mean variables from a single (random) simulation, which completed in approximately 26 hours of wall-clock time. The model generates physically consistent temporal patterns in response to annually repeating forcings. Notably, Spherical

 Model & \(p_{s}\) & TWP & \(T_{7}\) & \(u_{7}\) & \(v_{7}\) \\  Reference & \(19.96\) & \(0.199\) & \(0.090\) & \(0.142\) & \(0.110\) \\ Ours & \(23.52\) & \(0.214\) & \(0.094\) & \(0.167\) & \(0.121\) \\ DYffusion & \(24.75\) & \(0.223\) & \(0.082\) & \(0.169\) & \(0.127\) \\ ACE-STO & \(30.32\) & \(0.256\) & \(0.135\) & \(0.192\) & \(0.131\) \\  

Table 2: Global area-weighted mean of the spread of an ensemble of 10-yr time-mean’s for surface pressure, total water path, air temperature, zonal wind, and meridional wind (the last three at the near-surface level). The climate variability of our method is consistent with the reference model.

DYffusion exhibits improved variability patterns compared to the baseline ACE model, which suffers from unrealistic annual fluctuations (e.g. see surface pressure).

## 6 Conclusion

We introduce Spherical DYffusion, a novel approach that combines efficient diffusion modeling with a spherical-aware neural architecture to probabilistically emulate complex global climate dynamics across decadal to centennial timescales. Our model achieves lower climate biases than relevant deterministic and probabilistic baselines, getting significantly closer to the optimal performance provided by the emulated climate model. For climate model emulation problems, our approach presents a unique solution for balancing generative modeling, computational efficiency, and low climate biases. This opens up the ability to perform fully data-driven ensemble climate simulations.

Limitations.To achieve real-world impact, the dataset will need to be expanded so that ML emulators can be evaluated (and trained) on climate change scenarios/simulations. This will require using time-varying climate change forcings such as greenhouse gas and aerosol concentrations. Although our use of the state-of-the-art FV3GFS atmospheric model enables generation of such training data, any emulator will inherently reflect biases present in the base model. Additionally, we only considered emulating the atmosphere, but to achieve a full Earth System Model (ESM) we also need to emulate (or couple to a physics-based model of) other components such as ocean, land, sea-ice, etc. It is important to stress that while our method is more than \(25\) faster than the reference physics-based climate model, it is still slower than deterministic emulators such as ACE. Though our method characterizes model uncertainty through its generative design, extending it to incorporate initial condition uncertainty--a key component of traditional ensemble physics-based models--could further enhance its capabilities. The method also needs extension to handle output-only variables like precipitation, either through dedicated prediction heads or modifications to the DYffusion framework.

Figure 6: Comparison of 100-year global mean simulations between Spherical DYffusion and ACE. From top to bottom: near-surface air temperature (\(T_{7}\)), total water path (TWP), and surface pressure (\(p_{s}\)). Both models are driven by identical annually repeating forcings. Spherical DYffusion demonstrates more stable trajectories, particularly evident in the surface pressure predictions, while maintaining physically realistic variability patterns. The consistent behavior across all variables indicates the model’s robustness for long-term climate simulations.