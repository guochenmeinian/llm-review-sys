# Grounded Answers for Multi-agent Decision-making Problem through Generative World Model

Zeyang Liu

zeyang.liu@stu.xjtu.edu.cn

&Xinrui Yang

xinrui.yang@stu.xjtu.edu.cn

&Shiguang Sun

ssg2019@stu.xjtu.edu.cn

&Long Qian

qianlongym@stu.xjtu.edu.cn

&Lipeng Wan

wanlipeng77@xjtu.edu.cn

&Xingyu Chen

chenxingyu_1990@xjtu.edu.cn

&Xuguang Lan

xglan@mail.xjtu.edu.cn

&National Key Laboratory of Human-Machine Hybrid Augmented Intelligence

National Engineering Research Center for Visual Information and Application

Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China

Corresponding authors.

###### Abstract

Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.

## 1 Introduction

Recent progress in generative artificial intelligence with models capable of generating creative content has shown attractive prospects for real-world applications, such as image generation (Takagi & Nishimoto, 2023), embodied agents (Brohan et al., 2023), and chatbots (Kopf et al., 2024). Most generative models attempt to directly obtain the answer by training on natural language or image datasets and inserting decomposed reasoning steps in few-shot demonstrations. However, these methods do not experience firsthand the situations described by the language and the image. Theycannot find the correct answers through trial and error as humans, which is necessary to ground reasoning on complicated problems and transfer learned knowledge to unfamiliar domains. For example, as shown in Figure 1, when asked a complex multi-agent decision problem, one of the most widely-used large language models, GPT4 - though achieving superhuman performance in many reasoning tasks - will generate sketchy and misleading answers.

To tackle this problem, we can utilize the generative models to understand the properties of the task that the user describes and simulate the effects of the actions. We can derive the answer with a highly realistic simulator by experiment-reasoning or training any machine intelligence from simulated experience. The origin of this idea can be traced back to Dyna architecture (Sutton, 1990) and has spawned a series of model-based reinforcement learning (MBRL) theories and methods (Janner et al., 2019; Kaiser et al., 2019; Lai et al., 2020). Inspired by this, Mind's Eye (Liu et al., 2022) enables language models to perform reasoning conditioned on the simulation results by running the corresponding experiment on a computational physics engine named MuJoCo (Todorov et al., 2012). Mind's Eye can boost reasoning performance in zero-shot and few-shot settings by infusing such physical knowledge into language models. However, it is particularly designed for physical reasoning rather than decision-making problems.

In contrast, UniSim (Yang et al., 2024) formulates the action-in-video-out framework as an observation prediction diffusion model conditioned on finite history. It shows that the simulator learned from broad data can generalize to the real world and bridge the sim-to-real gap. Genie (Bruce et al., 2024) enables users to act in the generated environments on a frame-by-frame basis, opening the path for training generalist agents of the future. Notably, most of the existing breakthroughs on learning in the imagined experience have been focusing on single-agent scenarios and leave the world model largely unstudied for multi-agent reinforcement learning (MARL) tasks - it is common in real-world applications that multiple agents are required to solve a task in a coordinated fashion.

The roadblocks to building a simulator for MARL tasks are twofold. First, MARL tasks involve multiple entities' attributes, e.g., positions and roles, making it difficult to describe a state using only text. The text and image information can be brought together to enrich the inputs for the simulator, but such a dataset is limited in quantity. Second, the dynamics and reward models of the MARL environment are more intricate than the single-agent setting. Current approaches assume the reward is known in the dataset (Meng et al., 2023) or can be easily deduced by the frame information (Yang et al., 2024), which could be challenging for MARL methods due to the abundance of agents' tactics and the compositional nature of their functionalities.

This work explores a paradigm that adds language-guided simulation to the MARL pipeline to make policy learning grounded within the learned world model. First, we propose new offline MARL datasets to provide paired state-image information for the StarCraft Multi-Agent Challenge (SMAC) environment by transforming the state in the trajectory to the corresponding image. We also designed a parser to convert each trajectory to a task description using natural language. Then, we pre-train a vector quantized variational autoencoder (VQ-VAE) (Van Den Oord et al., 2017) to generate discrete representations for each frame. The world model is formulated as an interactive simulator that consists

Figure 1: Complex decision problems that require a good understanding of the environment’s dynamics and the objective are still challenging for current vision-language models, e.g., the answer elicited by GPT-4 is sketchy and misleading. Instead, Learning before Interaction (LBI) enables grounded reasoning by simulating the task in the given question. LBI utilizes the simulator to train a MARL policy and generate the answer by running the converged policy on the simulator.

of a dynamics and a reward model. The dynamics model comprises an image tokenizer and a causal transformer to generate interaction transitions autoregressively. The reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under the task description.

Given a decision-making problem by the user and an image from the environment, we store the simulated interaction trajectories into a replay buffer by running an off-policy MARL algorithm on the generated dynamics model. Then, we utilize the generated reward model to label the reward for each state-action pair based on the whole trajectory. We update the policy network according to the reward with a behavior-regularization term, which serves as the conservatism for out-of-distribution state-action pairs. We use the image sequence generated by the interaction of the dynamics model and the converged policy model as the answer to the decision-making problem.

We summarize the main contributions of this paper in three folds: (1) It proposes novel MARL datasets for SMAC, where a parser automatically generates the ground-truth image of a given state and task description. (2) It introduces Learning before Interaction (LBI), an interactive simulator that generates trial-and-error experiences and improves the answers for multi-agent decision-making problems. (3) The empirical results show that LBI outperforms various offline learning methods on training and unseen MARL tasks. The visualization also indicates that LBI can produce consistent imagined trajectories and explainable rewards for interaction states.

## 2 Background

**Decentralized Partially Observable Markov Decision Process.** A fully cooperative multi-agent task in the partially observable setting can be formulated as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), consisting of a tuple \(G= A,S,,O,U,P,r,\), where \(a A\{1,,n\}\) is a set of agents, \(S\) is a set of states, and \(\) is a set of joint observations. At each time step, each agent obtains its observation \(o D\) based on the observation function \(O(s,a):S A\), and an action-observation history \(_{a} T( U)^{*}\). Each agent \(a\) chooses an action \(u_{a} U\) by a stochastic policy \(_{a}(u_{a}|_{a}):T U\), which forms a joint action \(\). It results in a joint reward \(r(s,)\) and a transit to the next state \(s^{} P(|s,)\). The formal objective function is to find the joint policy \(\) that maximizes a joint action-value function \(Q^{}(s_{t},_{t})=r(s_{t},_{t})+ _{s^{}}[V^{}(s^{})]\), where \(V^{}(s)=[_{t=0}^{}^{t}r_{t}|s_ {0}=s,]\), and \([0,1)\) is a discounted factor.

**Inverse Reinforcement Learning.** Suppose we do not have access to the ground truth reward function but have demonstrations \(\) provided by an expert policy \(_{E}\). Imitation learning aims to directly learn policies that behave similarly to these demonstrations, whereas inverse reinforcement learning (IRL) seeks to infer the underlying reward functions which induce the expert policies. The MaxEnt IRL framework (Ziebart et al., 2008) aims to recover a reward function that rationalizes the expert behaviors with the least commitment, denoted as \((_{E})\):

\[(_{E})=_{r}_{_{E}}[r(s,u)]- (r)\] (1) \[(r)=_{}()+_{}[r(s,u)]\]

where \(()=_{}[-(u|s)]\) is the entropy of current policy \(\). It looks for a reward function that assigns a high reward to the expert policy and a low reward to the current policy \(\) while searching for the best policy for the reward function in the inner loop.

## 3 Methodology

We formulate an interaction simulator as a transition prediction model that, given some state of the world and descriptions of the task, can take some actions as input and produce the consequence of the actions in the form of images, states, and rewards. In this paper, we consider building such simulators for a multi-agent decision-making environment named StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019), known for its rich environments and high control complexity. More information about SMAC can be found in Appendix C.

### VisionSMAC

The SMAC benchmark saves a replay of an episode as a SC2REPLAY file rather than providing the image feature during exploration. It is computationally expensive to construct datasets of images by watching such replay within the StarCraft II client and then subsampling a frame that captures meaningful actions. To solve this problem, we introduce VisionSMAC to convert the state into images and languages through a parser \(f\), decoupled from StarCraft, making it easy to create new content.

First, we collect offline datasets across ten training maps in the SMAC benchmark by running multi-agent exploration methods named EMC (Zheng et al., 2021) and IIE (Liu et al., 2024). Each dataset contains a large number of interaction trajectories:

\[:=\{s_{t},\{o^{a}_{t}\}_{a=1}^{n},\{u^{a}_{t}\}_{a=1}^{n},\{d^{a}_{t}\}_{a=1 }^{n}\}_{t=0}^{T}\] (2)

where \(s_{t}\) denotes the state, \(\{o^{a}_{t}\}_{a=1}^{n}\) is the observation of each agent, \(\{u^{a}_{t}\}_{a=1}^{n}\) is the joint action, and the done signal \(d^{a}_{t}=1\) when the agent \(a\) is killed at timestep \(t\), \(n\) and \(T\) denote the number of agents and the length of the episode, respectively. We further collect the element images that appear in the game and affect the state, such as the units and the background terrain of training maps.

Given a multi-agent system and its interaction trajectory, the parser \(f\) reads predefined map information, such as the number and races of agents and enemies. Then, the parser converts the original state information into structured information, reading agents' and enemies' positions and health points. It will generate the corresponding interaction scenes by placing each unit image and its health bar at their positions with the corresponding background terrain. The image generated by the parser can resemble the frame subsampled from the original replay by running a SC2REPLAY file within the StarCraft II client, where the comparisons can be found in Appendix B. We also perform data augmentation to enable better feature abstraction by changing the background to different terrains.

Finally, we define a task description \(L\) to specify the environment and the task. The task description can be the terminated state, a slice of a trajectory, or any other representation of the episode. In this paper, we use the terrain information, the number and unit types of agents and enemies, and the sum of enemies' remaining health points at the terminated state as the task description. To this end, the parser reads the last state of the trajectory and extracts the remaining health points of both sides. We can obtain the practical task description by filling in predefined description templates (e.g., "_Consider that we control [number of agents] [agent races] on the left._") and adding connecting words (e.g., "_What plan should we use_"). The detailed description of the datasets and the parser \(f\) can be found in Appendix B.

### Training An Interactive Simulator

With trajectories with corresponding images and language guidance from different scenarios, we can formulate the interactions with StarCraft II as interacting with an interactive simulator. The simulator contains three key components: (1) an image tokenizer that converts raw video frame into discrete tokens, (2) a dynamics model that predicts the next frame and state given past frame and state tokens, (3) a reward model that infers the reward of a state-action pair given a trajectory. The idea behind decomposing the world model into a dynamics model and a reward model is to reuse the dynamics model for different tasks by combining it with any reward function.

Figure 2: Datasets construction and VQ-VAE training.

Image TokenizerWe compress images into discrete tokens to reduce dimensionality and enable higher-quality image generation. We make use of vector quantized variational autoencoder (VQ-VAE) (Van Den Oord et al., 2017), which takes every single image of the state as input, generating discrete representations. The tokenizer is trained using a standard VQ-VAE objective.

Dynamics ModelThe dynamics model is a causal transformer \(q\) parameterized by \(\), where the target sequence has the following form \(x=\{...,L,z_{t},s_{t},o_{t}^{1},...,o_{t}^{n},u_{t}^{1},...,u_{t}^{n},z_{t+1}, s_{t+1},...\}\), where \(z_{t}\) is the image representation generated by the fixed image tokenizer. We utilize the task description \(L\) to specify the dynamics of the environment, remaining consistent in one sequence. An embedding for each timestep is learned and added to each token. The dynamics model processes all past tokens and predicts future tokens via autoregressive modeling.

Then, we use the prediction heads to decode the predicted tokens to the corresponding element in the sequence and train them by minimizing the cross-entropy loss for actions and mean-squared error for others. The actions would serve as the reference policy for the learning with the simulated trajectories described in Section 3.3. In particular, we use a dynamics residual term to improve the accuracy and the stability of the generation by changing the target from \(s_{t+1}\) to \( s_{t+1}=s_{t+1}-s_{t}\) for the state prediction head. We also apply this term to predict image representations. In addition, since the observation is only related to the current state and the vision range of the agents, we filter out the historical memories and use \(s_{t}\) as the input for the observation prediction.

Reward ModelWe resemble the training pipeline of inverse reinforcement learning - maximizing the likelihood of trajectories in the expert demonstrations while minimizing the likelihood of trajectories collected from an inner-loop policy. We introduce a reward function \(\), which receives the entire trajectory as inputs to perform credit assignment under deterministic dynamics within the trajectory. We remake this formulation as a generalized version of the conventional reward design; if the reward function is Markovian, the temporal dependencies on other state-action pairs should always be zero.

To this end, we model the reward function as a bidirectional transformer model parameterized by \(\), where the sequence is \(=\{...,s_{t},L,u_{t}^{1},...,u_{t}^{n},_{t},s_{t+1},...\}\), and \(_{t}=\{_{t}^{}\}_{s=1}^{N}\) is a set of individual rewards for the agents. Again, we utilize the task description \(L\) to perform hindsight relabeling, which converts imperfect trajectories into possible solutions for reaching the last state \(s_{T}\) of the episode, generating the expert demonstration \(\). We optimize the reward function by minimizing the following loss:

\[_{}=-_{}[_{i}^{N} _{t}^{t}_{}_{t}^{a}(;)]+_{ ^{}}[_{i}^{N}_{t}^{t}_ {}_{t}^{a}(;)],\] (3)

where \(^{}=\{^{a}(u^{a}|s;)\}_{s=1}^{N}\) is the inner-loop MA-SAC policy parameterized by \(\), and we use the reward constraint by imposing an L2 penalization of the predicted rewards over all possible actions, which can be viewed as a conservative update for out-of-distribution state-action pairs. In particular, we alternate between \(k\)-step of policy update and \(k\)-step of reward update to avoid completely solving

Figure 3: The overview of Learning before Interaction.

the policy optimization subproblem before updating the reward parameters, where \(k\) is a given update iteration.

### Inference: Learning Policy in the Simulator

We now describe how to generate grounded answers for multi-agent decision-making problems via LBI. Given an image of the initial state and a task description from the user, the agents interact with the dynamics model using a randomly initialized off-policy MARL algorithm to collect reward-free trajectories in an autoregressive manner. Then, the reward model predicts the immediate reward for each transition pair in the simulation trajectories. These relabeled trajectories are added to the replay buffer, serving as the training data for the policy network.

In practice, we construct the MARL problem in the simulator as a behavior-regularized MDP by imposing a behavior-regularization term:

\[_{}}_{t=0}^{ }(_{a=1}^{n}(r_{t}^{a}(;)- (_{i}(u_{t}^{a}|o_{t}^{a};)}{q(u_{t}^{a}|x_{<u_{t}^{a}}; )}))),\] (4)

where \(}=\{_{i}(u_{t}^{a}|o_{t}^{a},)\}_{a=1}^{n}\) is the joint policy, and \(q(u_{t}^{a}|x_{<u_{t}^{a}};)\) is the reference policy provided by the dynamics model. The last term enables in-sample learning and further mitigate the impact of exploring OOD regions in the state-action space. We use independent \(Q\)-learning to train the parameter-sharing agent network.

Since it is possible for specific agents to become inactive before the game terminates, we mark the terminated timestep for each agent and enemy once its predicted health is less than zero and then use zero vectors as the subsequent actions and observations. It can mitigate the hallucinating unrealistic outcomes - a dead agent performs a "moving" action. We also mask the predicted reward after the terminated timestep for the inactive agent to get a more accurate value estimate.

## 4 Related Work

**World Models.** There is a long-standing history of learning predictive models of the world. We list three categories of model-based reinforcement learning (MBRL) according to the type of model usage. The first category applies planning methods with world model simulation. AlphaGo (Silver et al., 2016) and MuZero (Schrittwieser et al., 2020) learn a transition model and apply Monte Carlo Tree Search to search for an action sequence with the highest accumulated rewards. By contrast, MBMF (Nagabandi et al., 2018), PETS (Chua et al., 2018), and PlaNet (Hafner et al., 2019) integrate model predictive control (MPC) into the learned world model and sample high-reward action sequences. TD-MPC (Hansen et al., 2022) and TD-MPC2 (Hansen et al., 2022) utilize value functions to bootstrap the trajectories used for MPC planning.

The second category models a differentiable world model and utilizes the internal structure of the model to facilitate policy learning. GPS (Levine and Koltun, 2013) and GDP (Srinivas et al., 2018) perform differential planning and obtain the analytic form of the optimal policy. SVG (Heess et al., 2015) re-parameterizes the policy and the world model, then computes the policy gradient estimate by backpropagation via the world model. MAAC (Clavera et al., 2019), Dreamer (Hafner et al., 2019) and its subsequent variants (Hafner et al., 2020, 2023) use the recurrent state-space model in PlaNet to learn the world model in a compact latent space and learn the policy entirely within this space.

The last one utilizes the learned world model to generate more experiences and then trains a policy on the dataset augmented by the model, also known as Dyna-style methods (Sutton, 1990). MVE (Feinberg et al., 2018) and STEVE (Buckman et al., 2018) depict a learned world model to calculate multi-step temporal-difference prediction for better value estimation. In contrast, SLBO (Luo et al., 2018), MBPO (Janner et al., 2019), and BMPO (Lai et al., 2020) theoretically analyze this learning framework and prove that the policy performance will improve monotonically in a world model with certain model bias and rollout length. To further increase the rollout length and avoid compounding errors, M2AC (Pan et al., 2020) and COPlanner (Wang et al., 2023) compute the uncertainty of each rollout step and perform conservative model rollouts by discarding the samples with high uncertainty or adding a penalty term into total reward. In practice, GAIA-1 (Hu et al., 2023), UniSim (Yang et al., 2024), and Genie (Bruce et al., 2024) show that the learned world model can enable the control policy to generalize to the real world when trained purely in simulation and bridge the sim-to-realgap. These methods have impressive performance and theoretical bounds, attracting much research interest in the MBRL community. However, they focus on generating videos or trajectories that only involve one single agent instead of building a multi-agent simulator that can be used to further improve decision-making performance on coordination tasks in our work.

**Imitation Learning.** Imitation Learning (Bain and Sammut, 1995) formulates imitating an expert as a supervised learning problem, which has been widely adopted in various domains due to its simplicity and effectiveness (Silver et al., 2016; Swamy et al., 2020). GAIL (Ho and Ermon, 2016) and its extensions (Song et al., 2018; Ghasemipour et al., 2020) stand as a cornerstone approach, which trains a generator policy to imitate expert behaviors and a discriminator to distinguish between the expert and the learner's state-action pair distributions. In light of the recent interest in foundational models, the conditional diffusion model is used to represent and learn an imitation learning policy, which produces a predicted action conditioning on a state and a sampled noise vector Pearce et al. (2022); Chi et al. (2023). These methods achieve encouraging results in modeling stochastic and multimodal behaviors from human experts or play data. DT-style methods (Chen et al., 2021; Wu et al., 2024) formulate the trajectory generation as a sequence modeling problem, which generates states, actions, and rewards by conditioning on a return-to-go token in an autoregressive manner.

In contrast, inverse reinforcement learning (IRL) is designed to infer the reward function that underlies the expert demonstrations, taking into account the temporal structure and showing better generalization than direct Behavioral Cloning (Ng and Russell, 2000; Ross et al., 2011; Barde et al., 2020). A main class of algorithms, Maximum entropy (MaxEnt) IRL (Haarnoja et al., 2017) and its extensions (Liu et al., 2021; Rolland et al., 2022), learn a stationary reward by minimizing divergence between the agent and expert distribution. Since the learned reward function can solve downstream tasks and transfer behavior across different dynamics, IRL is also helpful in several broader applications, e.g., IRL with natural language goals (Fu et al., 2018; Zhou and Small, 2021; Xu et al., 2022), and RL with human feedback (Ziegler et al., 2019; Zhu et al., 2023; Wu et al., 2023), and dynamics learning (Luo et al., 2023). Furthermore, a series of sample-efficient algorithms are proposed to solve the MaxEnt IRL formulation (Fu et al., 2018; Zeng et al., 2022, 2024). To side-step the expensive online environmental interactions in classic IRL, some work aims to learn a reward function from a static dataset by a variational Bayesian framework (Chan and van der Schaar, 2021), representing reward function via a learned soft \(Q\)-function (Garg et al., 2021), or incorporating conservatism into the estimated reward like offline \(Q\)-learning (Yue et al., 2022). The major bottleneck for these methods includes a lack of knowledge of the dynamics information and the reward overestimation for out-of-distribution state-action pairs. We formulate the reward model as a bidirectional transformer to receive the whole trajectory as the input, making it possible to solve non-Markovian rewards. In addition, we leverage the reward constraint and the behavior regularization to perform in-sample learning to avoid reward overestimation. The amount of expert demonstrations in these existing studies is also limited, as they do not treat hindsight relabeling via the textual description as an expert trajectory like in our work.

**Offline \(Q\)-Learning.** Offline \(Q\)-learning learns a policy from a fixed dataset where the reward is provided for each transition sample. Most off-policy reinforcement learning (RL) algorithms are applicable in offline \(Q\)-learning. However, they typically suffer from the overestimation problem of out-of-distribution (OOD) actions due to the distribution shift between the action distribution in the training dataset and that induced by the learned policy (Fujimoto et al., 2019). Several constraint methods are proposed to restrict the learned policy from producing OOD actions by leveraging importance sampling (Sutton et al., 2016; Nachum et al., 2019), incorporating explicit policy constraints (Kostrikov et al., 2021; Fakoor et al., 2021; Fujimoto and Gu, 2021; Tarasov et al., 2024), penalizing value estimates (Kumar et al., 2020; An et al., 2021; Shao et al., 2024), and uncertainty quantification (Wu et al., 2021; Zanette et al., 2021). Another branch resorts to learning without querying OOD actions and thus constrain the learning process within the support of the dataset (Bai et al., 2021; Lyu et al., 2022).

## 5 Experiments

In this section, we conduct empirical experiments to answer the following questions: (1) Is Learning before Interaction (LBI) better than the existing multi-agent reinforcement learning (MARL) methods in complex cooperative scenarios? (2) Can LBI generate long-horizon trajectories and reasonable reward functions at critical states? (3) Does LBI have the zero-shot ability to generalize to unseen tasks? Then, we investigate the contribution of each component in the dynamics and the reward model. We provide the information of training datasets and experimental settings in Appendix B and D. We also discuss this paper's broader impacts and limitations in Appendix A.1 and A.2.

### Performance Comparison

Reward-free Offline LearningWe compare LBI with the following imitation learning baselines: (1) BC: behavior cloning that imitates the whole datasets, (2) MA-AIRL (Yu et al., 2019): using adversarial learning to perform policy imitation, (3) MADT (Meng et al., 2023): utilizing the Decision Transformer (Chen et al., 2021) to perform sequence modeling, (4) MA-TREX: infering the reward according to ranked demonstrations, the multi-agent version of TREX (Brown et al., 2019), (5) MAPT (Zhu et al., 2024): infering the team rewards according to the preference return from a well-trained scripted teacher.

As shown in Table 1, LBI outperforms the baselines by a significant margin on various maps with different difficulty levels, indicating the importance and effectiveness of learning reward functions via the proposed world model. In contrast, BC and MA-AIRL fail to achieve success rates in most tasks because they imitate all past interaction sequences and cannot generalize and avoid sub-optimal solutions. MA-TREX and MAPT have plateaued in performance because they use the accumulated rewards and the preference deduced by the scripted teacher to specify the quality of the training data, respectively. MADT performs better than other baselines because Decision Transformer can be thought of as performing imitation learning on a subset of the data with a certain return.

Offline MARLWe also compare LBI with the existing offline MARL methods with ground-truth rewards from the StarCraft Multi-Agent Challenge (SMAC), including the multi-agent version of BCQ (Fujimoto et al., 2019) and CQL (Kumar et al., 2020) (namely BCQ-MA and CQL-MA), ICQ (Yang et al., 2021), OMAR (Pan et al., 2022), and OMIGA (Wang et al., 2024). Table 2 shows that the performance of these offline MARL methods degrades dramatically with an increasing number of agents and is much lower than that of LBI. We hypothesize that the reasons for this gap are: (1) It is challenging and unnecessary to recover the \(Q\)-value based on the reward functions provided by SMAC (the hit-point damage dealt) because such reward design is inefficient for learning optimal policy. (2) These methods may introduce too much conservatism and affect the learning of the optimal policy, as the conservative update of the out-of-distribution (OOD) suboptimal policy that consists of some agents taking non-optimal actions and others taking optimal will inhibit the learning of the agents that take the optimal actions.

### Generalization on Unseen Tasks

Since zero-shot generalization ability is crucial for generating grounded answers for multi-agent decision-making problems, we also test LBI's ability to generalize to extensive unseen scenarios

  
**Map Name** & **BCQ-MA** & **CQL-MA** & **ICQ** & **OMAR** & **OMIGA** & **LBI** \\ 
5m\_vs\_6m & 9.13\(\) 0.21 & 10.15\(\) 0.15 & 9.47\(\) 0.45 & 8.76\(\) 0.52 & 10.38\(\) 0.50 & 18.96\(\) 0.56 \\
2e\_vs\_64zg & 18.86\(\) 0.35 & 19.20\(\) 1.25 & 18.47\(\) 0.25 & 17.10\(\) 0.94 & 19.25\(\) 0.38 & 20.45\(\) 0.25 \\
6h\_vs\_8z & 11.91\(\) 0.44 & 9.95\(\) 0.32 & 11.55\(\) 0.15 & 9.74\(\) 0.28 & 12.74\(\) 0.21 & 18.97\(\) 0.28 \\ corridor & 16.42\(\) 1.55 & 6.64\(\) 0.90 & 16.74\(\) 1.78 & 8.15\(\) 0.89 & 17.10\(\) 1.33 & 19.50\(\) 0.73 \\   

Table 2: Test return and standard deviations compared with offline reinforcement learning methods.

  
**Map Name** & **BCQ** & **MA-AIRL** & **MADT** & **MAPT** & **MA-TREX** & **LBI** \\ 
1c3s5z & 16.44\(\) 1.35 & 7.88\(\) 2.49 & 61.35\(\) 7.26 & 74.77\(\) 5.15 & 64.76\(\) 11.62 & 94.59\(\) 3.41 \\
10m\_vs\_11m & 26.19\(\) 4.42 & 41.69\(\) 7.12 & 82.76\(\) 4.41 & 66.85\(\) 9.28 & 48.78\(\) 11.28 & 90.45\(\) 6.99 \\
2e\_vs\_64zg & 17.37\(\) 1.012 & 24.75\(\) 10.83 & 61.90\(\) 5.74 & 58.28\(\) 7.84 & 22.45\(\) 7.74 & 71.44\(\) 8.83 \\
3s\_vs\_5z & 0.00\(\) 0.00 & 0.05\(\) 0.03 & 80.90\(\) 0.45 & 72.33\(\) 3.93 & 55.38\(\) 18.03 & 92.82\(\) 6.25 \\
5m\_vs\_6m & 13.78\(\) 2.15 & 11.59\(\) 6.75 & 79.78\(\) 4.98 & 56.01\(\) 3.17 & 50.01\(\) 14.87 & 87.98\(\) 5.10 \\
6h\_vs\_8z & 9.28\(\) 5.06 & 16.47\(\) 8.08 & 30.94\(\) 2.54 & 37.16\(\) 6.27 & 28.38\(\) 5.31 & 66.61\(\) 4.57 \\
3s5z\_vs\_36z & 0.00\(\) 0.00 & 0.00\(\) 0.00 & 27.44\(\) 9.49 & 34.90\(\) 6.84 & 36.16\(\) 3.68 & 83.34\(\) 4.27 \\ corridor & 0.00\(\) 0.00 & 0.76\(\) 0.15 & 69.85\(\) 1.54 & 45.91\(\) 15.47 & 30.59\(\) 9.86 & 87.45 \(\) 2.94 \\ MMM2 & 0.00\(\) 0.00 & 0.00\(\) 0.00 & 54.34\(\) 12.83 & 19.21\(\) 5.59 & 21.52\(\) 6.58 & 95.96\(\) 4.65 \\   

Table 1: Test win rates (%) and standard deviations compared with imitation learning methods.

without retraining. Specifically, we evaluate our LBI and MADT on the ten unseen testing maps, varying agent numbers, action spaces, and levels of environment complexity. Table 3 shows that LBI consistently outperforms MADT in unseen scenarios by a large margin, successfully transferring knowledge to new tasks without requiring additional fine-tuning. It highlights that learning a reward function has better zero-shot generalization performance than simple policy adaptation.

### Ablation Studies

In this section, we conduct ablation studies to analyze the contributions of each component in the dynamics model and the reward model across five evaluation runs on four training maps (6h_vs_8z, 3s5z_vs_3s6z, corridor, and MMM2) and four unseen maps (3s5z_vs_3s7z, 1c3s7z, 3s4z, 1c_vs_32rg). We show the results of the dynamics model in Table 4. Using the dynamics residual term is necessary to reduce the prediction error of the subsequent states and obtain good performance across all training and unseen tasks. The image reference is not so effective, even if we use ground-truth images as the reference. However, since images are more powerful in representing some situations than language or state information, we believe that the image serves as another modality to correct the prediction of the state. We would leave it for future work.

We demonstrate the ablation results of the reward model in Table 5. Compared with LBI-wo-RC&BR, the reward constraint and behavior regularization term can improve the overall performance on the training tasks. However, LBI-wo-BR performs better than LBI-wo-RC on unseen tasks, suggesting that the conservatism for reward is more important than the policy when OOD state-action pairs exist. The poor performance of LBI-w-GTR indicates that learning rewards from conditioned demonstrations may be more accessible and valuable for policy updates than reconstructing the pre-defined rewards by human experts.

### Visualization

This section evaluates the dynamics model as a long-horizon policy-conditioned predictive model. Figure 4 showcases examples of length-40 image trajectories generated by the dynamics model, including MMM2, 3s_vs_5z, and 5m_vs_6m. We do not observe conspicuous compounding errors as the single-step prediction model does, highlighting that LBI has consistency and long-horizon generation ability. In the case of 5m_vs_6m, we present the following frames after taking one of the possible actions, showing that LBI can also perform action-controllable generation.

We also investigate the reward prediction at a critical junction in the state-action space that can transit to various states and significantly influence the success rate on the 5m_vs_6m task. At the moment, the agents have to learn to micromanage leapfrogging to achieve good coordination. Specifically, Agent 1 has a low health point and must move backward to avoid the enemies focusing fire on it; otherwise, the enemies will eliminate Agent 1 immediately and weaken our scarce forces. In Figure 4,

  
**Algorithm** & **Return (Training)** & **Return (unseen)** \\  LBI & 19.47\(\) 0.77 & 18.54 \(\) 1.49 \\ LBI-GTR & 16.68 \(\) 1.55 & 14.07 \(\) 2.79 \\ LBI-wo-RC & 17.85 \(\) 0.59 & 14.75 \(\) 1.67 \\ LBI-wo-RR & 18.82 \(\) 1.28 & 17.46 \(\) 2.01 \\ LBI-wo-RC\&BR & 12.35 \(\) 2.38 & 9.83 \(\) 1.46 \\   

Table 4: The ablation results for the dynamics model without residual term (wo-RT), image reference (wo-IR), and using ground-truth image (GTI) as the reference for state prediction.

  
**Unseen Task** & **MADT** & **MA-TREX** & **LBI** & **Unseen Task** & **MADT** & **MA-TREX** & **LBI** \\ 
1c3s & 16.21\(\) 5.38 & 23.53 \(\) 8.83 & 56.47\(\) 5.63 & 1c2s7z & 6.16\(\) 3.09 & 5.69\(\)3.81 & 28.26\(\) 6.41 \\
6m & 49.28 \(\) 4.06 & 37.12\(\)2.59 & 97.85\(\) 2.15 & 6m\_vs\_7m & 73.45\(\) 7.22 & 32.88\(\)4.47 & 81.07\(\) 5.17 \\
1c_vs_32rg & 2.08\(\) 1.51 & 11.41\(\)3.41 & 58.33\(\)6.44 & 334z & 90.21\(\) 1.82 & 79.71\(\)3.56 & 87.55\(\) 1.76 \\
3s2z_vs_2s3z & 0.00\(\) 0.00 & 9.16\(\)5.62 & 18.22\(\) 2.46 & 35z_vs_3s7z & 10.21\(\) 3.66 & 15.88\(\)4.34 & 22.08\(\) 7.63 \\
1c3s6z & 16.41\(\) 6.44 & 58.09\(\)3.41 & 65.38\(\) 5.12 & 9m\_vs\_11m & 76.44\(\) 4.17 & 70.91\(\)6.95 & 75.05\(\) 2.16 \\   

Table 3: Test win rates (%) and standard deviations on unseen tasks.

  
**Algorithm** & **Return (Training)** & **Return (unseen)** \\  LBI & 19.47\(\) 0.77 & 18.54 \(\) 1.49 \\ LBI-GTR & 16.68 \(\) 1.55 & 14.07 \(\) 2.79 \\ LBI-w-RC & 17.85 \(\) 0.59 & 14.75 \(\) 1.67 \\ LBI-wo-BR & 18.82 \(\) 1.28 & 17.46 \(\) 2.01 \\ LBI-wo-RC\&BR & 12.35 \(\) 2.38 & 9.83 \(\) 1.46 \\   

Table 5: The ablation results for the reward model without reward constraint (wo-RC), behavior regularization (wo-BR), and using ground-truth rewards (w-GTR) provided by the SMAC benchmark.

we visualize the learned reward function of Agent 1, where the action space is no-operation, stopping, moving in cardinal directions, and selecting an enemy's identity to attack. The learned reward for moving to the left is much higher than the other actions, allowing one to learn the optimal joint policy quickly. The rewards provided by the SMAC benchmark do not show this property, where multiple Monte Carlo samples are required to find the correct policy by estimating the expected return.

## 6 Conclusion and Future Work

We proposed Learning before Interaction (LBI), a novel paradigm that enables generative models to ground their answers for multi-agent decision-making problems with simulations between the world and the multi-agent system. We formulate an interactive simulator consisting of dynamics and reward models, given some states of the world and the task descriptions, generating the consequence of the actions in the form of images, states, and rewards. We hope the idea of including simulations in the reasoning will instigate broad interest in applying generative models to aid machine intelligence and decision-making.

## 7 Acknowledgements

This work was supported in part by NSFC under grant No.62125305, No.62088102, No. U23A20339, No. 62203348.

Figure 4: Visualization of the prediction from dynamics and reward model, where “np-op” and “s” denote no-operation and stopping, respectively.