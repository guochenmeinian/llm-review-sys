# Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning

Xinran Li Ling Pan Jun Zhang

Department of Electronic and Computer Engineering

The Hong Kong University of Science and Technology

xinran.li@connect.ust.hk, lingpan@ust.hk, eejzhang@ust.hk

###### Abstract

In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce _Kaleidoscope_, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations. Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at https://github.com/LXXXXR/Kaleidoscope.

## 1 Introduction

Cooperative multi-agent reinforcement learning (MARL) has demonstrated remarkable effectiveness in solving complex real-world decision-making problems across various domains, such as resource allocation (Ying and Dayong, 2005), package delivery (Seuken and Zilberstein, 2007), autonomous driving (Zhou et al., 2021), and robot control (Swamy et al., 2020). To mitigate the challenges posed by the non-stationary and partially observable environments typical of MARL (Yuan et al., 2023), the centralized training with decentralized execution (CTDE) paradigm (Foerster et al., 2016) has become prevalent, inspiring many influential MARL algorithms such as MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2018), MATD3 (Ackermann et al., 2019), QMIX (Rashid et al., 2020), and MAPPO (Yu et al., 2022).

Under the CTDE paradigm, parameter sharing among agents is a commonly adopted practice to improve sample efficiency. However, identical network parameters across agents often lead to homogeneous policies, restricting diversity in behaviors and the overall joint policy representational capacity. This limitation can result in undesired outcomes in certain situations (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023), as shown in Figure 1, impeding further performancegains. An alternative approach is the non-parameter sharing scheme, where each agent possesses its own unique parameters. Nevertheless, while this method naturally supports heterogeneous policies, it suffers from reduced sample efficiency, leading to significant training costs. This is particularly problematic given the current trend towards increasingly large model sizes, with some scaling to trillions of parameters (Zhao et al., 2023; Achiam et al., 2023). Therefore, it is imperative to develop a parameter sharing strategy that enjoys both high sample efficiency and broad policy representational capacity, potentially achieving significantly enhanced performance. While several efforts (Christianos et al., 2021; Kim and Sung, 2023) have explored partial parameter sharing initiated at the start of training, such initializations can be challenging to design without detailed knowledge of agent-specific environmental transitions or reward functions (Christianos et al., 2021).

In this work, we build upon insights from previous studies (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023) and introduce _Kaleidoscope_, a novel adaptive partial parameter sharing scheme. It maintains a single set of policy parameters and employs multiple learnable masks to designate the shared parameters. Unlike earlier methods that depend on fixed initializations, Kaleidoscope dynamically learns these masks alongside MARL parameters throughout the training process. This end-to-end training approach inherently integrates environmental information, and its adaptive nature enables Kaleidoscope to dynamically adjust the level of parameter sharing based on the demands of the environment and the learning progress of the agents. The learnable masks facilitate a dynamic balance between full parameter sharing and non-parameter sharing, offering a flexible trade-off between sample efficiency and policy representational capacity through enhanced heterogeneity. Initially, we build Kaleidoscope upon agent networks, where it achieves diverse policies. Following this success, we extend it to multi-agent actor-critic algorithms to encourage heterogeneity among the central critic ensembles for further performance enhancement.

Just like a _kaleidoscope_ uses the reflective properties of rotating mirrors to transform simple shapes into beautiful patterns, our proposed method leverages learnable masks to map a single set of parameters into diverse policies, thereby enhancing task performance.

We summarize our contributions as follows:

* To enable policy heterogeneity among agents for better training flexibility, we adapt the soft threshold reparameterization (STR) technique to learn distinct masks for different agent networks while only maintaining one set of common parameters, effectively balancing between full parameter sharing and non-parameter sharing mechanisms.
* To enhance policy diversity among agents, we introduce a novel regularization term that encourages the pairwise discrepancy between masks. Additionally, we design resetting mechanisms that recycle masked parameters to preserve the representational capacity of the joint networks.
* Through extensive experiments on MARL benchmarks, including multi-agent particle environment (MPE) (Lowe et al., 2017), multi-agent MuJoCo (MAMuJoCo) (Peng et al., 2021) and StarCraft multi-agent challenge v2 (SMACv2) (Ellis et al., 2024), we demonstrate the superior performance of Kaleidoscope over existing parameter sharing approaches.

## 2 Background

Multi-agent reinforcement learning (MARL)In MARL, a fully cooperative partially observable multi-agent task is typically formulated as a decentralized partially observable Markov decision process (dec-POMDP) (Oliehoek and Amato, 2016), represented by a tuple \(=,A,P,R,,O,N,\). Here, \(N\) denotes the number of agents, and \((0,1]\) represents the discount factor. At each timestep \(t\), with the environment state as \(s^{t}\), agent \(i\) receives a local observation \(o_{i}^{t}\) drawn from the observation function \(O(s^{t},i)\) and then follows its local policy

Figure 1: Full parameter sharing confines the policies to be homogeneous. In this example, all predators pursue the same prey, neglecting another prey in the game World. Further game details are in Appendix A.2.

\(_{i}\) to select an action \(a^{t}_{i} A\). Individual actions form a joint action \(^{t} A^{N}\), leading to a state transition to the next state \(s^{t+1} P(s^{t+1}|s^{t},^{t})\) and inducing a global reward \(r^{t}=R(s^{t},^{t})\). The overall team objective is to learn the joint policies \(=_{1},,_{N}\) such that the expectation of discounted accumulated reward \(G^{t}=_{t}^{t}r^{t}\) is maximized.

To learn such policies \(}\), various MARL algorithms (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2020; Yu et al., 2022) have been developed. For instance, the off-policy actor-critic algorithm MATD3 (Ackermann et al., 2019) serves as an example method. Specifically, the critic networks are updated by minimizing the temporal difference (TD) error loss

\[_{c}()=_{(s^{t},^{t},^{t},r^{t},s^{ t+1},^{t+1})}[(y^{t}-Q(s^{t},^{t}; ))^{2}],\] (1)

with

\[y^{t}=r^{t}+_{j=1,2}Q(s^{t+1},_{1}(o_{1}^{t+1};_{1}^{ })+,,_{N}(o_{N}^{t+1};_{N}^{})+;_{j}),\] (2)

where \(\) are the parameters for critics, \(\) are the parameters for actor policies and \(^{}\) are the parameters for target actor policies. And \(\) is the clipped Gaussian noise, given as \(((0,),-c,c)\).

The policy is updated by the deterministic policy gradient algorithm (Silver et al., 2014)

\[(_{i})=_{(s^{t},^{t},^{t},r ^{t},s^{t+1},^{t+1})}[_{_{i}}_{ i}(o_{i}^{t};_{i})_{a_{i}}Q(s^{t},a_{1},,a_{N}|_{a_{i}= _{i}(o_{i}^{t};_{i})};_{1})].\] (3)

Soft threshold reparameterization (STR)Originally introduced in the context of model sparsification, STR (Kusupati et al., 2020) is an unstructured pruning method that achieves notable performance without requiring a predetermined sparsity level. Specifically, STR applies a transformation to the original parameters \(W\) as follows

\[_{g}(W,s)=(W)(|W|-g(s)),\] (4)

where \(s\) is a learnable parameter, \(=g(s)\) serves as the pruning threshold, and \(()=(,0)\). The original supervised learning problem modeled by

\[_{}(;)\] (5)

with \(\) as the data is now transferred to

\[_{,}(_{g}(,);).\] (6)

Overall, this approach optimizes the learnable pruning threshold alongside the model parameters, facilitating dynamic adjustment to the sparsity level during training.

## 3 Learnable Masks for Heterogenous MARL

In this section, we propose using learnable masks as a low-cost method to enable network heterogeneity in MARL. The core concept, illustrated in Figure 2, is to learn a single set of shared parameters complemented by multiple masks for distinct agents, specifying which parameters to share.

Specifically, in Section 3.1, we first adapt STR into a dynamic partially parameter sharing method, unlocking the joint policy network's capability to represent diverse policies among agents. In Section 3.2, we actively foster policy heterogeneity through a novel regularization term based on the masks. Given that the masking technique could excessively sparsify the network, potentially diminishing its representational capacity, in Section 3.3, we propose a straightforward remedy to periodically reset the parameters based on the outcomes of masking, which additionally mitigates primacy bias. Finally, in Section 3.4, we explore how to further extend this approach within the critic components of actor-critic algorithms to improve value estimations in MARL and further boost performance.

For the sake of clarity, we integrate the proposed Kaleidoscope with the MATD3 (Ackermann et al., 2019) algorithm to demonstrate the concept within this section. Nevertheless, as a versatile partial parameter-sharing technique, our method can readily be adapted to other MARL algorithms. We defer its integration with other MARL frameworks to Appendix A.1.2 and will evaluate them empirically in Section 4.

### Adaptive partial parameter sharing \(\)

The core idea of this work is to learn distinct binary masks \(_{i}\) for different agents to facilitate differentiated policies, ultimately aiming to improve MARL performance. To achieve this, we apply the STR (Kusupati et al., 2020) technique to the policy parameters with different thresholds dedicated to each agent:

\[_{i}=_{0}_{i},\] (7)

where \(_{i}\) parameterizes the policy for agent \(i\), \(_{0}\) is the set of learnable parameters shared by all agents, and \(_{i}\) is the learnable mask for agent \(i\). Specifically, assume \(_{0}=[_{0}^{(1)},,_{0}^{(N_{a})}]\), \(_{i}=[_{i}^{(1)},,_{i}^{(N_{a})}]\) and \(_{i}=[m_{i}^{(1)},,m_{i}^{(N_{a})}]\) with \(N_{a}\) being the total parameter count of an agent's network. In line with STR, we compute each element \(m_{i}^{(k)}\) of \(_{i}\) as \(m_{i}^{(k)}=[|_{0}^{(k)}|>(s_{i}^{(k)})]\), where \(()\) denotes the Sigmoid function.

The benefits of such a combination are summarized as follows:

* **Preservation of original MARL learning objectives:** Unlike most of the methods in pruning literature, which primarily aim to minimize the discrepancies between pruned and unpruned networks in terms of weights, loss, or activations (Hoefler et al., 2021; Menghani, 2023; Deng et al., 2020), STR maintains the original goal of minimizing task-specific loss, aligning directly with our objectives to enhance MARL performance.
* **Flexibility in sparsity:** Many classical pruning methods require predefined per-layer sparsity levels (Evci et al., 2020; Ramanujan et al., 2020). Such requirements can complicate our design, with the goal not to gain extreme sparsity but rather to promote heterogeneity through masking. The STR technique is ideal in our case as it does not require predefining sparsity levels, allowing for adaptive learning of the masks.
* **Enhanced network representational capacity:** Utilizing learnable masks for adaptive partial parameter sharing enhances the network's representational capacity beyond traditional full parameter sharing. In full parameter sharing, agents' joint policies are parameterized as \(^{}}}(|_{0})=_{1}(| _{0}),,_{N}(|_{0})\). In contrast, our proposed adaptive partial parameter sharing mechanism parameterizes the joint policies as \(^{}(|_{0},)=_{1}(| _{0}_{1}),,_{n}(|_{0}_{N})\). In the extreme case where all the values in \(_{i}\) are \(1\)s, the function set represented by \(^{}(|_{0},)\) degrades to that of \(^{}}}(|_{0})\). In other scenarios, it is a superset of that represented by \(^{}}}(|_{0})\).

Figure 2: Overall network architecture of Kaleidoscope. It maintains one set of parameters \(_{0}\) with \(N\) sets of masks \([_{i}]_{i=1}^{N}\) for actor networks, and one set of parameters \(_{0}\) with \(K\) sets of masks \([_{j}^{c}]_{j=1}^{K}\) for critic ensemble networks, where \(N\) is the number of agents, \(K\) is the number of ensembles, and \(\) denotes the Hadamard product.

### Policy diversity regularization \(\)

While independently learned masks enable agents to develop distinct policies, without a specific incentive, these policies may still converge to being homogeneous. To this end, we propose to explicitly encourage agent policy heterogeneity by introducing a diversity regularization term maximizing the weighted pairwise distance between network masks, which is defined as

\[^{}()=_{i=1,,n}_{j=1, ,n\\ j i}_{0}(_{i}-_{j})_{1}.\] (8)

This term is inherently non-differentiable due to the indicator function \([]\) inside \(\). To overcome this difficulty, following established practices in the literature (Bengio et al., 2013; Alizadeh et al., 2018), we utilize a surrogate function for gradient approximation:

\[^{}}{ g(_{i})}=- [^{}}{_{i}}].\] (9)

We formally provide the overall training objective for actors in Appendix A.1.1.

### Periodically reset \(\)

As the training with masks proceeds, we observe an increasing sparsity in each agent's network, potentially reducing the overall network capacity. To remedy the issue, we propose a simple approach to periodically reset the parameters that are consistently masked across all \(_{i}\) with a certain probability \(\), which is illustrated in Figure 2(a). At intervals defined by \(t==0\), if the parameter index \(k\) satisfies \( i,m_{i}^{(k)}==0\), we apply the following resetting rule

\[_{0}^{(k)},s_{1}^{(k)},,s_{N}^{(k)}[_{0}^{(k)},s_{1}^{(k)},,s_{N}^{(k)}]&\\ _{0}^{(k)},s_{1}^{(k)},,s_{N}^{(k)}&1- .\] (10)

This resetting mechanism recycles the weights masked as zeros by all the masks, preventing the networks from becoming overly sparse. A side benefit of this resetting mechanism is the enhancement of neural plasticity (Lyle et al., 2023; Nikishin et al., 2024), which helps alleviate the primacy bias (Nikishin et al., 2022) in reinforcement learning. Unlike methods that reinitialize entire layers resulting in abrupt performance drops (Nikishin et al., 2022), our resetting approach selectively targets weights as indicated by the learnable masks, thus avoiding significant performance disruptions, as shown in Section 4.

### Critic ensembles with learnable masks

In actor-critic algorithm frameworks, we further apply Kaleidoscope to central critics as an efficient way to implement ensemble-like critics. By facilitating dynamic partial parameter sharing, Kaleidoscope enables heterogeneity among critic ensembles. Furthermore, by regularizing the diversity

Figure 3: Illustration on resetting mechanisms.

among critic functions, we can control ensemble variances. This approach is elaborated in subsequent paragraphs.

\(\) **Adaptive partial parameter sharing for critic ensembles** In the standard MATD3 algorithm (Ackermann et al., 2019), two critics with independent parameters are maintained to mitigate overestimation risks. However, using separate parameters typically results in a low update-to-data (UTD) ratio (Hiraoka et al., 2022). To address this issue, we propose to enhance the UTD ratio by employing Kaleidoscope parameter sharing among ensembles of critics. Specifically, we maintain a single set of parameters \(_{0}\) and \(K\) masks \([_{j}^{c}]_{j=1}^{K}\) to distinguish the critic functions, resulting in \(K\) ensembles \([Q(;_{j})]_{j=1}^{K}\) with \(_{j}=_{0}_{j}^{c}\).

To be specific, we update the critic networks by minimizing the temporal difference (TD) error loss

\[_{c}(_{j})=_{(s^{t},^{t},s_{t+1})}[(y^{t}-Q(s^{t},^{t};_{j}))^{2}],\] (11)

with

\[y^{t}=r^{t}+_{j=1,,K}Q(s^{t+1},_{1}(o_{1}^{t+1};_{1}^ {})+,,_{n}(o_{N}^{t+1};_{N}^{})+; _{j}).\] (12)

And the policies are updated by the mean estimation of the ensembles as

\[(_{i})=_{s^{t}}[_ {_{i}}_{i}(o_{i}^{t};_{i})_{a_{i}}_{j=1}^{ K}[Q(s^{t},a_{1},,a_{N}|_{a_{i}=_{i}(o_{i}^{t};_{i})};_{j}) ]].\] (13)

\(\) **Critic ensembles diversity regularization** As in Section 3.2, we also apply diversity regularization to critic masks to prevent critics functions from collapsing to identical ones. The diversity regularization to maximize for the critic ensembles is expressed as

\[_{c}^{}(^{c})=_{i=1,,K}_{ j=1,,K\\ j i}_{0}(_{i}^{c}-_{j}^{c}) _{1}.\] (14)

Intuitively, as training progresses, this term encourages divergence among the critic masks, leading to increased model estimation uncertainty. This process fosters a gradual shift from overestimation to underestimation. As discussed in prior research (Hiraoka et al., 2022; Lan et al., 2020; Chen et al., 2021; Wang et al., 2021b), overestimation can encourage exploration, beneficial in early training stages, whereas underestimation alleviates error accumulation (Fujimoto et al., 2018), which is preferred in the late training stage. We formally provide the overall training objective for critic ensembles in Appendix A.1.1.

\(\) **Periodically reset** To further promote diversity among critic ensembles and counteract the reduction in network capacity caused by masking, we implement a resetting mechanism similar to that described in Section 3.3. In particular, we sequentially reinitialize the masks \(_{j}^{c}\) following a cyclic pattern, as illustrated in Figure 2(b). In this way, each critic function's mask is trained on distinct data segments, leading to different biases.

In summary, by adopting Kaleidoscope parameter sharing with learnable masks, we establish a cost-effective implementation for critic ensembles that enjoy a high UTD ratio. Through enforcing distinctiveness among the masks, we subtly control the differences among critic functions, thereby improving the value estimations in MARL.

## 4 Experimental Results

In this section, we integrate Kaleidoscope with the value-based MARL algorithm QMIX and the actor-critic MARL algorithm MATD3, and evaluate them across eleven scenarios in three benchmark tasks.

### Experimental Setups

Environment descriptionsWe test our proposed Kaleidoscope on three benchmark tasks: MPE (Lowe et al., 2017), MaMuJoCo (Peng et al., 2021) and SMACv2 (Ellis et al., 2024). For the discrete tasks MPE and SMACv2, we integrate Kaleidoscope and baselines with QMIX (Rashid et al., 2020) and assess the performance. For the continuous task MaMuJoCo, we employ MATD3 (Ackermann et al., 2019). We use five random seeds for MPE and MaMuJoCo and three random seeds for SMACv2, reporting averaged results and displaying the \(95\%\) confidence interval with shaded areas. The chosen benchmark tasks reflect a mix of discrete and continuous action spaces and both homogeneous and heterogeneous agent types, detailed further in Appendix A.2.

BaselinesIn the following, we compare our proposed Kaleidoscope with baselines (Christianos et al., 2021; Kim and Sung, 2023), as listed in Table 1. For both Kaleidoscope and the baselines, in scenarios with fixed agent types (MPE and MaMuJoCo), we assign one mask per agent. For SMACv2, where agent types vary, we assign one mask per agent type. We use official implementations of the baselines where available; otherwise, we closely follow the descriptions from their respective papers, integrating them into QMIX or MATD3. Hyperparameters and further details are provided in Appendix A.1.3.

### Results

PerformanceWe present the comparative performance of Kaleidoscope and baselines in Figure 4 and Figure 5. Overall, Kaleidoscope demonstrates superior performance, attributable to the flexibility of the learnable masks and the effectiveness of diversity regularization. Additionally, we observe that FuPS + ID generally outperforms NoPS, except for the Ant-v2-4x2 scenario (Figure 3(c)). This advantage is largely due to FuPS's higher sample efficiency; a single transition data sample updates the model parameters \(N\) times in FuPS + ID, once for each agent, compared to just once in NoPS. Consequently, FuPS + ID models learn faster from the same number of transitions. Similarly, Kaleidoscope benefits from this mechanism as it shares weights among agents, allowing a single transition to update the model parameters multiple times. Furthermore, by integrating policy heterogeneity through learnable masks, Kaleidoscope enables diverse agent behaviors, as illustrated in the visualization results in Figure 8. Ultimately, Kaleidoscope effectively balances parameter sharing and diversity, outperforming both full parameter sharing and non-parameter sharing approaches.

Cost analysisDespite its superior performance, Kaleidoscope does not increase computational complexity at test time compared to the baselines. We report the test time averaged FLOPs comparison of Kaleidoscope and baselines in Table 2. We see that due to the masking technique, Kaleidoscope has lower FLOPs compared to baselines, thereby enjoying a faster inference speed when being deployed.

   Methods & Paradigm & Sharing level & Adaptive & Descriptions \\  NoPS & No sharing & - & No & Agents have distinct parameters \\ FuPS & Full sharing & Networks & No & Agents share all the parameters \\ FuPS + ID & Full sharing & Networks & No & Agents share all the parameters \\ SePS & Partial sharing & Networks & No & Agents are clustered to share parameters within each cluster \\ MultiH & Partial sharing & Layers & No & Agents share all the parameters except for distinct action heads \\ SNP & Partial sharing & Neurons & No & Agents share specific neurons \\ Kaleidoscope & Partial sharing & Weights & Yes & Agents share parameters based on distinct, learnable masks \\   

Table 1: Methods compared in the experiments. Here, “adaptive” indicates whether the sharing scheme evolves during training.

Ablation studiesWe conduct ablation studies to assess the impact of key components in Kaleidoscope, with results presented in Figure 6. Specifically, we compare Kaleidoscope with three ablations: 1) _Kaleidoscope w/o reg_, which lacks the regularization term in Equation (8) that encourages the masks to be distinct. 2) _Kaleidoscope w/o reset_, which does not reset parameters. 3) _Kaleidoscope w/o ce_, which does not use Kaleidoscope parameter sharing in critic ensembles and instead maintains two independent sets of parameters for critics. From the results, we observe that diversity regularization contributes the most to the performance of Kaleidoscope. Without it, masking degrades the performance due to the reduced number of parameters in each policy network. Resetting primarily aids learning in the late stages of training when needed, which aligns with the observation made by Nikishin et al. (2022). Notably, even with resetting, the performance does not experience abrupt drops thanks to the guidance provided by the masks on where to reset. When ablating the critic ensembles with Kaleidoscope parameter sharing, we observe inferior performance from the beginning of the training. This is because the critic ensembles with Kaleidoscope parameter sharing enable a higher UTD ratio of the critics, as discussed in Section 3.4.

Furthermore, we conduct experiments to study the impact of mask designs. The results are shown in Figure Figure 7. Specifically, we compare original Kaleidoscope with two alternative mask design choices: 1) _Kaleidoscope w/ neuron masks_, where adaptive masking techniques are applied to neurons rather than weights. 2) _Kaleidoscope w/ fixed masks_, where the masks are initialized at the beginning of training and kept fixed throughout the learning process. The results show that performance drops with either alternative design choice, demonstrating that Kaleidoscope's superior performance originates from the flexibility of the learnable masks on weights.

More results on hyperparameter analysis are included in Appendix B.2.

Figure 4: Performance comparison with baselines on MPE and MaMuJoCo benchmarks.

Figure 5: Performance comparison with baselines on SMACv2 benchmarks.

VisualizationWe visualize the trained policies of Kaleidoscope on World, as shown in Figure 7(a). The agents exhibit cooperative divide-and-conquer strategies (four red agents divide into two teams and surround the preys), contrasting with the homogeneous policies depicted in Figure 1. We further examine the distinctions in the agents' masks and present the results in Figure 7(b). First, we observe that by the end of the training, each agent has developed a unique mask, revealing that distinct masks facilitate diverse policies by selectively activating different segments of the neural network weights. Second, throughout the training process, we note that the differences among the agents' masks evolve dynamically. This observation confirms that Kaleidoscope effectively enables dynamic parameter sharing among the agents based on the learning progress, empowered by the adaptability of the learnable masks. More visualization results are provided in Appendix B.3.

## 5 Related Work

Parameter sharingFirst introduced by Tan (1993), parameter sharing has been widely adopted in MARL algorithms (Foerster et al., 2018; Rashid et al., 2020; Yu et al., 2022), due to its simplicity and high sample efficiency (Grammel et al., 2020). However, schemes without parameter sharing typically offer greater flexibility for policy representation. To balance sample efficiency with policy representational capacity, some research efforts aim to find effective partial parameter sharing schemes. Notably, SePS (Christianos et al., 2021) first clusters agents based on their transitions at the start of training and restricts parameter sharing within these clusters. Subsequently, SNP (Kim and Sung, 2023) enables partial parameter sharing by utilizing the lottery ticket hypothesis (Su et al., 2020) to initialize heterogeneous network structures. Concurrent to our work, AdaPS (Li et al., 2024) combines SNP and SePS by proposing a cluster-based partial parameter sharing scheme. While these methods have shown promise in certain domains, their performance potential is often limited by the static nature of the parameter sharing schemes set early in training. Our proposed Kaleidoscope distinguishes itself by dynamically learning specific parameter sharing configurations alongside the development of MARL policies, thereby offering enhanced training flexibility.

Agent heterogeneity in MARLTo incorporate agent heterogeneity in MARL and enable diverse behaviors among agents, previous methods have explored concepts such as diversity and roles. Specifically, diversity-based approaches aim to enhance pairwise distinguishability among agents based on identities (Jiang and Lu, 2021), trajectories (Li et al., 2021), or credits assignment (Liu et al., 2023; Hu et al., 2023) through contrastive learning techniques. Concurrently, role-based strategies, sometimes referred to as skills (Yang et al., 2020) or subtasks (Yuan et al., 2022), employ conditional policies to differentiate agents by assigning them to various conditions. These conditions may be based on agent identities (Yang et al., 2022), local observations (Yang et al., 2020), local

   Methods & NoPS & FuPS & FuPS +ID & SePS & MultiH & SNP & Kaleidoscope \\  MPE & 1.0x & 0.992x & 1.0x & 1.0x & 1.0x & 0.988x & **0.901x** \\ MaMuJoCo & 1.0x & 0.985x & 1.0x & 1.0x & 1.0x & 0.900x & **0.680x** \\ SMACv2 & 1.0x & 0.992x & 1.0x & 1.0x & 1.0x & 0.988x & **0.890x** \\   

Table 2: Averaged FLOPs (with calculation methods detailed in Appendix A.3) across different methods. Results are first normalized with respect to the FuPS + ID model for each scenario and then averaged across scenarios within each environment (detailed results in Appendix B.1). The lowest costs are highlighted in **bold**.

Figure 6: Ablation studies. Figure 7: Comparison on mask designs.

histories (Wang et al., 2020, 2021a; Yuan et al., 2022) or joint histories (Liu et al., 2021; Iqbal et al., 2022; Zeng et al., 2023). This line of researches mainly focus on module design and operate separately from parameter-level adjustments, making them orthogonal to our approach. Nevertheless, integrating these methods with our work could potentially enhance performance further.

Sparse networks in deep reinforcement learning (RL)Although relatively few, there are some noteworthy recent attempts to find sparse networks for deep RL. In particular, PoPS (Livne and Cohen, 2020) prunes the dense networks post-training, achieving significantly reduced execution time complexity. Additionally, (Yu et al., 2020) validate the lottery ticket hypothesis within the RL domain, producing high-performance models even under extreme pruning rates. Subsequent efforts, including DST (Sokar et al., 2022), TE-RL* (Graesser et al., 2022) and RLx2 (Tan et al., 2023) employ topology evolution (TE) techniques to further decrease the training costs. While these developments utilize sparse training techniques, which are similar to the methods we employ, their primary focus is on reducing training and execution costs in single-agent settings. In contrast, our work leverages sparse network strategies as a means to enhance parameter sharing techniques, aiming to improve MARL performance.

## 6 Conclusions and Future Work

In this work, we introduced _Kaleidoscope_, a novel adaptive partial parameter sharing mechanism for MARL. It leverages distinct learnable masks to facilitate network heterogeneity, applicable to both agent policies and critic ensembles. Specifically, Kaleidoscope is built on three technical components: STR-empowered learnable masks, network diversity regularization, and a periodic resetting mechanism. When applied to agent policy networks, Kaleidoscope balances sample efficiency and network representational capacities. In the context of critic ensembles, it improves value estimations. By combining our proposed Kaleidoscope with QMIX and MATD3, we have empirically demonstrated its effectiveness across various MARL benchmarks. This study shows great promises in developing adaptive partial parameter sharing mechanisms to enhance the performance of MARL. For future work, it is interesting to further extend Kaleidoscope to other domains such as offline MARL or meta-RL.