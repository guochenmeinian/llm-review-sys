# Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration

Longlin Yu\({}^{1,*}\), Tianyu Xie\({}^{1,*}\), Yu Zhu\({}^{3,4,*}\), Tong Yang\({}^{5}\), Xiangyu Zhang\({}^{5}\), Cheng Zhang\({}^{1,2,}\)

\({}^{1}\) School of Mathematical Sciences, Peking University

\({}^{2}\) Center for Statistical Science, Peking University

\({}^{3}\) Institute of Automation, Chinese Academy of Sciences

\({}^{4}\) Beijing Academy of Artificial Intelligence

\({}^{5}\) Megvii Technology Inc.

{llyu, tianyuxie}@pku.edu.cn, zhuyu2022@ia.ac.cn,

{yangtong, zhangxiangyu}@megvii.com, chengzhang@math.pku.edu.cn

Equal contribution. This work was done during an internship at Megvii Technology Inc.Corresponding Author.

###### Abstract

Semi-implicit variational inference (SIVI) has been introduced to expand the analytical variational families by defining expressive semi-implicit distributions in a hierarchical manner. However, the single-layer architecture commonly used in current SIVI methods can be insufficient when the target posterior has complicated structures. In this paper, we propose hierarchical semi-implicit variational inference, called HSIVI, which generalizes SIVI to allow more expressive multi-layer construction of semi-implicit distributions. By introducing auxiliary distributions that interpolate between a simple base distribution and the target distribution, the conditional layers can be trained by progressively matching these auxiliary distributions one layer after another. Moreover, given pre-trained score networks, HSIVI can be used to accelerate the sampling process of diffusion models with the score matching objective. We show that HSIVI significantly enhances the expressiveness of SIVI on several Bayesian inference problems with complicated target distributions. When used for diffusion model acceleration, we show that HSIVI can produce high quality samples comparable to or better than the existing fast diffusion model based samplers with a small number of function evaluations on various datasets.

## 1 Introduction

Variational inference (VI) is an approximate Bayesian inference method that is gaining in popularity, where one tries to find an approximation to the target posterior distribution using an optimization approach (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2016). To do that, it first posits a family of variational distributions and then seeks the closest member from this family that minimizes some statistical distance to the target posterior, usually the Kullback-Leibler (KL) divergence. As the posterior is not analytically available, an equivalent formulation is often adopted in practice where one maximizes the evidence lower bound (ELBO) instead (Jordan et al., 1999).

One classical VI method is mean-field VI, which assumes a factorizable structure of the variational distributions over the parameters or latent variables (Bishop & Tipping, 2000). This often leads to closed-form coordinate-ascent update rules when certain conditional conjugacy conditions are satisfied. In practice, the conditional conjugacy may not hold and the true posterior could be muchmore complicated than what a factorized variational distribution can accurately approximate. In recent years, several attempts have been made in VI that alleviate these constraints by designing more flexible variational families (Jaakkola and Jordan, 1998; Saul and Jordan, 1996; Giordano et al., 2015; Tran et al., 2015; Rezende and Mohamed, 2015; Dinh et al., 2017; Kingma et al., 2016; Papamakarios et al., 2019), together with generic training algorithms via Monte Carlo gradient estimators (Nott et al., 2012; Paisley et al., 2012; Ranganath et al., 2014; Rezende et al., 2014; Kingma and Welling, 2014). While successful, these approaches all assume tractable densities of variational distributions. To further expand the capacity of variational families, one approach is to incorporate the implicit models that have intractable densities but are easy to sample from (Huszar, 2017; Tran et al., 2017; Mescheder et al., 2017; Shi et al., 2018, 2018; Song et al., 2019). However, as the densities are intractable for implicit models, one often resorts to density ratio estimation for ELBO evaluation during training, which is known to be difficult in high dimensional settings (Sugiyama et al., 2012). To avoid density ratio estimation, semi-implicit variational inference (SIVI) has been proposed where the variational distributions are formed through a semi-implicit hierarchical construction, and various training criteria have been employed (Yin and Zhou, 2018; Moens et al., 2021; Titsias and Ruiz, 2019; Yu and Zhang, 2023).

While striking a good balance between approximation flexibility and training efficiency, current SIVI methods often use a single conditional layer which can be insufficient when the target posterior possesses complicated structures (e.g., multimodality, see an example in Section 5.1). To enhance the expressiveness of single-layer models, an intuitive but effective approach is to extend them to multi-layer hierarchical models (Vahdat and Kautz, 2020; Ranganath et al., 2016; Sobolev and Vetrov, 2019). In this paper, we propose hierarchical semi-implicit variational inference (HISVI), which is a generalization of SIVI that allows multiple conditional layers. Instead of training the hierarchical semi-implicit model end to end, we introduce auxiliary distributions that interpolate between a simple base distribution and the target distribution to guide the intermediate semi-implicit distributions toward the target distribution. The conditional layers are then trained sequentially to match these auxiliary bridging distributions given the fitted semi-implicit distributions from the previous layers (Figure 1), using different criteria from before. This way, HISVI allows progressive learning of the target distribution that significantly reduces the burden of each conditional layer. Moreover, HISVI with the score matching objective can also be used to accelerate the sampling process of diffusion models where the pre-trained score networks corresponding to different noise levels provide a natural sequence of bridging distributions. In experiments, we demonstrate the effectiveness of HISVI on both Bayesian inference tasks with complicated target distributions and diffusion model acceleration.

## 2 Background on semi-implicit variational inference

The semi-implicit variational family (Yin and Zhou, 2018; Titsias and Ruiz, 2019) is defined as

\[q_{}()= q_{}(|)q(), \]

where \(\) are the variational parameters, \(q_{}(|)\) is called the conditional layer, and \(q()\) is called the mixing layer. This variational family is said to be semi-implicit as \(q_{}(|)\) is required to be explicit and \(q()\) is often implicit. The semi-implicit variational family is capable of capturing more complicated dependencies between variables (Yin and Zhou, 2018; Titsias and Ruiz, 2019; Yu and Zhang, 2023) than explicit variational families without the hierarchical structure. Given the observed data \(D\), the classical VI methods often use the evidence lower bound (ELBO) for training, which is defined as \(:=_{q_{}()}[ p(D,)- q_{}( )]\). However, as \(q_{}()\) is no longer tractable in SIVI, alternative training objectives have been introduced.

ELBO related objectivesYin and Zhou (2018) considered a sequence of lower bounds of the ELBO

\[_{}(p(|D)\|q_{}()):=_{ q(), q_{}(,)}_{\{^{(i)} \}_{i=1}^{K}^{-}q()})}{ (q_{}(|)+_{k=1}^{K}q_{}(|^{(k)}) )}. \]

It is an asymptotically exact surrogate in the sense that \(_{K}_{}=\). Titsias and Ruiz (2019) proposed unbiased implicit variational inference (UIVI) which uses samples from the inverse conditional distribution \(q_{}(|)\) (from an MCMC run, e.g. Hamiltonian Monte Carlo (Neal, 2011)) to provide an unbiased gradient estimator of the exact ELBO. See more details of UIVI in Appendix B.

Score matching objectiveBesides the ELBO, score based distance measures have also been used for variational inference where the score function \(():=_{} p(|D)=_{} p(D,)\) is assumed to be tractable (Liu et al., 2016; Zhang et al., 2018; Hu et al., 2018). Yu & Zhang (2023) considered the following Fisher divergence between the target distribution and the semi-implicit variational distribution

\[_{}(p(|D)\|q_{}()):=_{ q_{}()}\|()-_{} q_{}()\| _{2}^{2}. \]

By reformulating \(_{}\) as the maximum of the following optimization problem

\[_{}(p(|D)\|q_{}())=_{( )}[2()^{T}(()-_{} q_{}())-\|()\|_{2}^{2}],\]

and using a similar trick as in denoising score matching (Vincent, 2011; Song & Ermon, 2019), one can transform the minimization of \(_{}\) into the following minimax problem which is tractable

\[_{}_{}\ _{}(p(|D)\|q_{}( )):=_{ q(), q_{}(|)} [2_{}()^{T}[()-_{} q_{}( |)]-\|_{}()\|_{2}^{2}]. \]

In practice, \(_{}()\) is parametrized using neural networks. The above minimax optimization problem can be efficiently solved by optimizing \(\) and \(\) alternately.

## 3 Hierarchical semi-implicit variational inference

The semi-implicit variational family \(q_{}()\) in equation (1) is indeed a single-layer model in the sense that it contains only one conditional layer. Our main idea is to expand this single-layer semi-implicit variational family into its multi-layer variants and introduce a sequence of auxiliary distributions to guide the semi-implicit distributions toward the target distribution. This leads to a new SIVI method which we call hierarchical semi-implicit variational inference (HISVI). We start with the following definition which is motivated by equation (1).

**Definition 1** (Hierarchical Semi-Implicit Distribution).: _Let \(_{T} q_{T}(_{T})\) for some \(T^{}\), where \(q_{T}(_{T})\) is called the variational prior. Let \(q_{t}(_{t}|_{t+1};_{t})\) be the \(t\)-th conditional layer for \(0 t T-1\). Denote \(\{_{k}\}_{k=t}^{T-1}\) by \(_{ t}\). The \(t\)-th layer hierarchical semi-implicit distribution \(q_{t}(_{t};_{ t})\) is defined recursively from \(T-1\) to \(0\) by_

\[q_{t}(_{t};_{ t})= q_{t}(_{t}|_{t+1};_{t})q_ {t+1}(_{t+1};_{ t+1})_{t+1}, 0 t T -1, \]

_where \(q_{T}(_{T};_{ T}):=q_{T}(_{T})\). Here, the \(t\)-th conditional layer \(q_{t}(_{t}|_{t+1};_{t})\) is required to be explicit and reparametrizable with a tractable score function \(_{_{t}} q_{t}(_{t}|_{t+1};_{t})\)._

Compared to the single-layer semi-implicit variational family (1), the family of hierarchical semi-implicit distributions provides a principled way to construct more expressive mixing layers using multi-layer architectures. Also, unlike the hierarchical variational models (Ranganath et al., 2016) which require an extra reverse model and explicit variational prior, hierarchical semi-implicit distributions inherit the advantage of SIVI that allows \(q_{t}(_{t};_{ t})\) to be implicit, and as shown next, they do not require a reverse model and can be progressively trained using the simple algorithms of SIVI for each conditional layer, from \(t=T-1\) to \(t=0\).

Figure 1: An example for 4-layer HISVI. The target distribution \(p_{0}()\) is a Gaussian mixture and the auxiliary distributions \(\{p_{i}()\}_{i=0}^{3}\) are constructed using the diffusion bridge. The auxiliary distributions are plotted in the squares, where the blue heatmap describes the probability density and the arrows represent the score functions of the auxiliary distributions.

### Progressive approximation with the auxiliary bridge

In this section, we introduce a bridging technique for progressively approximating the target distribution \(p()\) using hierarchical semi-implicit distributions. Rather than approximating \(p()\) with \(q_{0}(;_{ 0})\) directly, we construct a sequence of intermediate auxiliary distributions \(\{p_{t}()\}_{t=0}^{T-1}\) as a bridge between the target distribution \(p_{0}():=p()\) and an easy-to-approximate distribution \(p_{T-1}()\), to amortize the difficulty of one-pass fitting. A typical example of an auxiliary bridge is the geometric interpolation as described below.

**Example 1** (Geometric Interpolation).: _Let \(():= p()\) be the score function of target distribution \(p()\) and \(_{}():= p_{}()\) be the score function of a base distribution \(p_{}()\). In geometric interpolation (Neal, 2001; Bernton et al., 2019), each auxiliary distribution \(p_{t}()\) for \(0 t T-1\) has the following probability density function (pdf) and score function_

\[p_{t}() p_{}()^{1-_{t}}p()^{ _{t}},\;_{t}():=_{} p_{t}()=(1-_{t}) _{}()+_{t}(), \]

_where \(\{_{t}\}_{t=0}^{T-1}\) is a non-negative decreasing sequence satisfying \(_{0}=1\)._

Intuitively, we expect the distance between two neighboring distributions \(p_{t}()\) and \(p_{t+1}()\) to be not too large so that it would be easy to construct a conditional distribution \(q_{t}(_{t}|_{t+1})\) such that \(p_{t}(_{t}) q_{t}(_{t}|_{t+1})p_{t+1}(_{t+1 })_{t+1}\). Note that the auxiliary bridge \(\{p_{t}()\}_{t=0}^{T-1}\) does not necessarily need to have analytical pdfs (up to a constant). In fact, it suffices if they have tractable score functions \(\{_{t}()\}_{t=0}^{T-1}\) which lead to another type of auxiliary bridge (Example 2 in Section 4).

### Sequential training of HSIVI

Given the auxiliary distributions \(\{p_{t}(_{t})\}_{t=0}^{T-1}\), a natural approach is to progressively train the hierarchical semi-implicit distribution \(q_{t}(_{t};_{ t})\) to match \(p_{t}(_{t})\) from \(t=T-1\) to \(t=0\). Let the parameters \(_{t}\) in the \(t\)-th conditional layer be independent across different \(t\)s. We first train \(q_{T-1}(_{T-1};_{T-1})\) to match \(p_{T-1}(_{T-1})\) by optimizing \(_{T-1}\) w.r.t. the single-layer SIVI objective \(_{f}(p_{T-1}(_{T-1})|q_{T-1}(_{ T-1};_{T-1}))\). For \(t=T-2,,0\), given the trained semi-implicit distribution \(q_{t+1}(_{t+1};_{ t+1})\), we can fix it as the mixing layer and train the \(t\)-th conditional layer \(q_{t}(_{t}|_{t+1};_{t})\) by optimizing \(_{t}\) w.r.t. the single-layer SIVI objective \(_{f}(p_{t}(_{t})|q_{t}(_{t}; _{ t}))\) as well. Note this is fine as the mixing layer can be implicit in SIVI. Here, \(f\) is some distance criterion, e.g. \(_{}\) in equation (2) or \(_{}\) in equation (4). In this article, we mainly focus on \(_{}\) and \(_{}\), while other distance criteria can also be applied. We summarize this sequential training procedure in Algorithm 1.

```
Input: Auxiliary bridge \(\{p_{t}()\}_{t=0}^{T-1}\); initial value of parameters \(^{(0)}=\{_{t}^{(0)}\}_{t=0}^{T-1}\). Output: The optimal parameters \(^{*}\).  Initialization: \(^{(0)}\). for\(t=T-1\)to 0do while not converge do  Sample a minibatch \(\{_{T}^{(k)}\}_{k=1}^{K}\) from the variational prior \(q_{T}(_{T})\). if\(t<T-1\)then  Sequentially sample \(\{_{t+1}^{(k)}\}_{k=1}^{K}\) through \(q(_{t}|_{t+1};_{i})\) from \(i=T-1\) to \(i=t+1\).  Detach the computation graphs from \(\{_{t+1}^{(k)}\}_{k=1}^{K}\). endif  Update \(_{t}\) by optimizing the \(_{f}(p_{t}(_{t})|q_{t}(_{t}; _{ t}))\) based on the minibatch \(\{_{t+1}^{(k)}\}_{k=1}^{K}\). endwhile \(_{t}^{*}_{t}\). endfor \(^{*}\{_{t}^{*}\}_{t=0}^{T-1}\).
```

**Algorithm 1** Hierarchical semi-implicit variational inference (sequential training)

Score based trainingIn addition to the common assumption that \(p_{t}()\) is known up to a constant, it is worth noting that \(_{}\) is also applicable when only the score functions \(\{_{t}()\}_{t=0}^{T-1}\) are available which is important for the diffusion bridge construction of auxiliary distributions in Example 2. Concretely, assume \(q_{t}(_{t}|_{t+1};_{t})\) is induced by a parametrized transform \(_{t}=_{t}(_{t+1},;_{t})\)where \( p_{}()\) is a random noise. The only term in \(_{}(p_{t}(_{t})\|q_{t}(_{t};_{ t }))\) containing \(p_{t}(_{t})\) is \(_{q_{}(_{t};_{ t})} p_{t}(_{t})\) (see equation (2)) whose gradient takes the form

\[_{_{t}}_{q_{}(_{t};_{ t})} p _{t}(_{t})=_{q_{t+1}(_{t+1};_{ t+1})p_{}()}_{t}(_{t}(_{t+1},;_{t}))_{_{t}}_{t}(_{t+1},;_{t}). \]

In the training of HSIVI-SM, each term \(_{}(p_{t}(_{t})\|q_{t}(_{t};_{  t}))\) involves a nested optimization of \(_{t}(_{t};_{t})\). When the score functions are computationally expensive, we find that an alternative parametrization \(_{t}(_{t};_{t}):=_{t}(_{t})-_{t}(_{t}; _{t})\) is useful to avoid the time-consuming evaluation of \(_{t}(_{t})\) when optimizing \(_{t}\) in equation (4). The reason for this lies in Proposition 1. See Appendix C.2 for the proof of Proposition 1.

**Proposition 1**.: _Let \(q_{t}(_{t},_{t+1};_{ t})=q_{t}(_{t}|_{t+1}; _{t})q_{t+1}(_{t+1};_{ t+1})\). The minimax optimization of \(_{}(p_{t}(_{t})\|q_{t}(_{t};_{ t }))\) is equivalent to_

\[_{_{t}} _{q_{t}(_{t},_{t+1};_{ t})}[ _{t}(_{t})-_{t}(_{t};_{t})]^{T}[_ {t}(_{t})+_{t}(_{t};_{t})-2_{_{t}} q_{t }(_{t}|_{t+1};_{t})],\] \[_{_{t}} _{q_{t}(_{t},_{t+1};_{ t})}\| _{t}(_{t};_{t})-_{_{t}} q_{t}(_{t}|_{t+1 };_{t})\|_{2}^{2}.\]

Marginal approximation v.s. joint approximationPrevious works (Bernton et al., 2019; Bao et al., 2022) often construct a joint distribution \(p(_{0:T})\) and minimize \((p(_{0:T-1})\|q(_{0:T-1}))\) where \(q(_{0:T-1})\) is a variational distribution. In HSIVI, we directly approximate \(p_{t}(_{t})\) using the semi-implicit variational distributions. When \(p(_{0:T-1})\) is complex and \(T\) is small, the variational distribution \(q(_{0:T-1})\) may be insufficient to fully capture the joint distribution \(p(_{0:T-1})\). For example, the optimal fit of the joint distribution for diffusion models established by Analytic-DPM (Bao et al., 2022) does not guarantee that the marginal distributions would be approximated well (see Table 2 for comparison).

## 4 Application to diffusion model acceleration

### Review of diffusion models

Recently, diffusion models have shown great success on many generative modeling benchmarks, including image generation (Ho et al., 2020; Song et al., 2020, 2020, 2020), graph generation (Niu et al., 2020), and text generation (Austin et al., 2021). Diffusion models work by adding noise to the training data in the forward process and then removing the noise to recover the data in the backward process, which can be integrated into a general stochastic differential equation (SDE) framework. The forward process \(\{_{s}\}_{s[0,L]}\) is usually described by

\[_{s}=(_{s},s)s+g(s)_{s}, _{0} p_{0}(), \]

where \(p_{0}()\) is the data distribution, \(_{s}\) is a standard Brownian motion, and \((_{s},s)\) and \(g(s)\) are the drift and diffusion coefficients respectively. To generate samples from the data distribution, one can run the following backward process

\[_{s}=[(_{s},s)-g^{2}(s)_{_{s}} p_{ s}(_{s})]s+g(s)}_{s},_{L}  p_{L}(), \]

where \(p_{s}()\) is the pdf of \(_{s}\) and \(}_{s}\) is a standard Brownian motion when time flows from \(L\) to \(0\). As the score function \(_{_{s}} p_{s}(_{s})\) is intractable, we need to estimate it by denoising score matching (Vincent, 2011; Song et al., 2020). See more details of diffusion models and the training objectives in Appendix A.

### Diffusion model acceleration via HSIVI

While diffusion models prove effective for generative modeling, it often takes a large number of discretization steps in the backward process (9) to produce high quality samples, which caps their potential for real time applications. Note that the forward process (8) naturally provides another type of auxiliary bridge, which combined with HSIVI, can be used to accelerate the sampling process of diffusion models.

**Example 2** (Diffusion Bridge).: _Consider the forward process \(\{_{s}\}_{s[0,L]}\) with \(L>0\) (defined in equation (8)) in diffusion models. We choose \(T\) discrete time steps \(0 s_{0}<<s_{T-1} L\) and _let \(_{t}:=_{s_{t}}\) with probability density function \(p_{t}()\). Assume each auxiliary distributions \(p_{t}()\) for \(0 t T-1\) admits a score function as_

\[_{t}():=_{} p_{t}()^{*}(,s_ {t}),\ 0 t T-1,\]

_where \(^{*}(,s)\) is a pre-trained score model with the denoising score matching loss (equation (13) in Appendix A). Let us denote \(^{*}(,s_{t})\) by \(^{*}_{t}()\) for short. With sufficient samples from the data distribution \(p_{0}()\) and model capacity, the approximation \(^{*}_{t}()\) can be reasonably accurate for almost all \(\) and \(t\)(Song et al., 2020b)._

As the pre-trained score model provides a diffusion bridge from the simple distribution \(p_{T-1}\) (e.g., standard Gaussian) to the data distribution, we can train the hierarchical semi-implicit distributions to approximate the diffusion bridge within the HSIVI framework. Given the expressiveness of hierarchical semi-implicit distributions, we may expect an accurate approximation of the data distribution with a small number \(T\) and hence acceleration can be achieved.

However, the memory usage during the sequential training process for HSIVI might be large because of the necessity for independent parameters. Therefore, we may employ a parameter sharing scheme which is commonly assumed in diffusion models (Song and Ermon, 2019; Ho et al., 2020) such that different conditional layers share the same parameters \(\). Note that sequential training is not suitable in this setting. Therefore, we propose a joint training procedure that minimizes a weighted sum of the SIVI objectives

\[_{f}()=_{t=0}^{T-1}(t)_{f}(p_{t}(_{t})\|q_{t}(_{t};)), \]

where \((t):\{0,,T-1\}_{+}\) is a positive weighting function and \(f\) is some distance criterion. See Algorithm 2 in Appendix C.3 for more details of joint training.

More specifically, in this work, we mainly focus on building the diffusion bridge with variance preserving SDE (VP-SDE) (Song et al., 2020b) such that \(_{s}|_{0}(_{0},(1-(s)) )\) with a decreasing function \((s)\) of \(s\). We use \(_{}\) in equation (10) for training and set the weighting function \((t)=1-(s_{t})\) as recommended in Song et al. (2020b), which tends to train layers that are far from \(t=0\) first during the training, resembling the sequential training. Another popular formulation of diffusion models is to fit a noise model \(^{*}(,s)\) that predicts the noise added to a noisy sample \(\) at time \(s\)(Ho et al., 2020). HSIVI-SM also generalizes to the case where a pre-trained noise model is available. The pre-trained noise model forms a (generalized) diffusion bridge by letting \(^{*}_{t}()=^{*}(,s_{t})\), and we call the corresponding training method "\(\)-training". We provide a reparametrized objective function \(}_{}\) for \(\)-training in Appendix C.4.

Several efforts have been made to accelerate the sampling process of diffusion models, including faster numerical ordinary differential equation (ODE) solvers (Song et al., 2020a; Zhang and Chen, 2022; Lu et al., 2022) and distillation techniques (Luhman and Luhman, 2021; Salimans and Ho, 2022; Zheng et al., 2022). Our approach is different from these previous efforts in that we accelerate the stochastic diffusion model directly (hence would provide more diverse samples (Figure 6)) and do not require sampling datasets from the diffusion models prior to distillation which is computationally expensive. From a Bayesian perspective, HSIVI is related to Song and Ermon (2019), where the authors used the annealed Langevin dynamics guided by a pre-trained score model to sample from the data distribution. By solving this problem using a variational inference approach, HSIVI enjoys faster sampling speed and scales better to high-dimensional data.

## 5 Experiments

In this section, we first compare HSIVI to its single-layer counterpart, SIVI, on two inference tasks. We use the sequential training method where each conditional layer in the hierarchical semi-implicit variational distributions has independent parameters. We then apply HSIVI-SM to diffusion model acceleration on various datasets. As the memory consumption for generative models is large, we use the joint training method where the conditional layers in hierarchical semi-implicit distributions have shared parameters across different \(t\)s. For all experiments, each conditional layer is modeled as a Gaussian distribution with parametrized mean and variance. More details of the model architectures and hyper-parameters are included in Appendix E. The code is available at [https://github.com/longinYu/HSIVI](https://github.com/longinYu/HSIVI).

### Target distribution approximation

Gaussian mixture modelWe first evaluate HSIVI and SIVI on a two-dimensional Gaussian mixture model. The target distribution \(p()\) takes the form \(p()=_{i=1}^{8}1/8(;_{i},^{2})\) where \(_{i}=[10(),10()]^{T}\), \(=1\). For HSIVI, we construct an auxiliary bridge of \(T=5\) with geometric interpolation in Example 1, where \(p_{}()=(;,)\) and \(_{t}=1-t/5\). The results are presented in Figure 2. Note that the modes in this Gaussian mixture model are far apart from each other, and both SIVI-LB and SIVI-SM are trapped in local modes. In contrast, both HSIVI-LB and HSIVI-SM discover all modes and provide an accurate approximation of the target distribution with HSIVI-SM being better for recovering the right scale of variance.

High-dimensional conditioned diffusionThe second example is a high-dimensional Bayesian inference problem arising from the following Langevin SDE

\[x_{s}=10x_{s}(1-x_{s}^{2})s+w_{s}, \]

where \(x_{0}=0\) and \(w_{s}\) is a one-dimensional standard Brownian motion. This system describes the motion of a particle with negligible mass trapped in an energy potential with thermal fluctuations represented by the Brownian forcing (Cui et al., 2016). Using an Euler-Maruyama scheme with step size \( s=0.01\) on a time interval \(\), we discretize the SDE (11) into \(=(x_{d_{1}},,x_{d_{000}})\) where \(d_{i}=0.01i\), which gives the prior distribution \(p_{}()\) of the 300-dimensional variable \(\). The noisy observations \(\) is obtained by \(=+\), where \((,^{2})\) with \(=0.1\). Our goal is to infer the posterior distribution of the latent states \(p(|) p_{}()p(|)\). The ground truth is formed by running 100,000 independent stochastic gradient Langevin dynamics (SGLD) chains with a step size of 0.0001 and collecting the results after 10,000 iterations.

For HSIVI, we form the auxiliary bridge using geometric interpolation with \(p_{}()=(;,^{2})\) and \(_{t}=1-t/(T-1)\) for \(t=0,,T-1\). Figure 3 shows the estimated posteriors obtained by different methods. We see that SIVI-SM severely underestimates the variance. With \(T=5\) layers, HSIVI-SM fits the variance better and hence provides more accurate posterior estimates. For both HSIVI-SM and HSIVI-LB, the estimated covariance matrix becomes more accurate as \(T\) increases (Table 4 in Appendix D.2), demonstrating the effectiveness of hierarchical models for fitting complicated distributions.

Figure 3: The posterior estimates obtained by different methods. For each method, we collect 100,000 samples to calculate the sample mean and confidence interval.

Figure 2: Comparison of 10,000 generated samples from SIVI and 5-layer HSIVI on a two-dimensional Gaussian mixture model (blue).

### Diffusion model acceleration

2D toy examplesIn this toy model example, we test four synthetic 2D datasets: Checkerboard, Circles, Moons, and Swissroll (Pedregosa et al., 2011). We first pre-train the score model \(^{*}(,s)\) for \(s\) with quadratic noise schedule \(1-(s)=s^{2}\). For constructing the \(T\)-layer diffusion bridge, we select \(\{s_{t}\}_{t=0}^{T-1}\) so that \(1-(s_{t})=[0.01+(-0.01)t/T]^{2}\). Figure 4 shows the sample trajectories (\(_{9}\), \(_{7}\), \(_{5}\) and \(_{0}\)) progressively generated from 10-layer HSIVI-SM. We see clearly how the semi-implicit distributions are guided towards the target distribution and all modes are discovered. We also report the Jensen-Shannon (JS) divergence between the target distributions and the estimated distributions in Table 1. We see that HSIVI-SM significantly improves upon DDIM and DDPM in both cases with 5 and 10 steps. Also, 10-layer HSIVI-SM is comparable to DDPM with 1000 full steps. See Figure 10 in Appendix D.3 for visualization of samples from different methods.

MnistOn MNIST, we use the noise model \(^{*}(,s)\) instead of the score model and use \(\)-training to train HSIVI-SM. The structure of \(^{*}(,s)\) follows the UNet in Ho et al. (2020) by reducing the number of input and output channels to one. With the same noise schedule employed in Song et al. (2020), we first pre-train the noise model \(^{*}(,s)\) with 1000 discretization steps and then form the \(T\)-layer diffusion bridge for HSIVI-SM by selecting \(T\) discrete time steps. Figure 5 shows the samples from DDPM, DDIM, and HSIVI-SM with \(T=5\) steps. We see that the samples produced by HSIVI-SM are much cleaner and more recognizable than those produced by DDPM and DDIM.

CIFAR-10, CelebA & ImageNetOn both CIFAR-10 and CelebA, the structure of our pre-trained noise model \(^{*}(,s)\) follows the UNet structure(Ronneberger et al., 2015) employed by Ho et al. (2020), instead of the huge VP deep continuous-time model (Song et al., 2020) that has more channels and layers. We also provide additional results on ImageNet (64\(\)64) with more powerful pre-trained score nets in (Nichol and Dhariwal, 2021)(bigger models with more parameters). Since this generative modeling has been formulated as a score-based VI problem, we do not have to use any training data for training HSIVI-SM.

Following the noise schedule employed in Song et al. (2020), we first pre-train the noise model \(^{*}(,s)\) with 1000 discretization steps and then form the \(T\)-layer diffusion bridge for HSIVI-SM

    &  &  &  \\   & DDPM & DDIM & HSIVI-SM & DDPM & DDIM & HSIVI-SM & DDPM \\  Checkerboard & 0.891 & 0.591 & **0.068\(\)0.006** & 0.521 & 0.373 & **0.030\(\)0.005** & 0.058 \\ Swissroll & 1.037 & 0.332 & **0.126\(\)0.006** & 0.334 & 0.164 & **0.082\(\)0.003** & 0.042 \\ Circles & 0.907 & 0.397 & **0.083\(\)0.015** & 0.364 & 0.201 & **0.073\(\)0.005** & 0.032 \\ Moons & 0.961 & 0.355 & **0.096\(\)0.013** & 0.352 & 0.137 & **0.059\(\)0.007** & 0.036 \\   

Table 1: JS divergences between the target distribution and the variational approximation on the four toy datasets. The results of HSIVI-SM are averaged by 5 independent runs with standard deviation in the subscripts. JS divergences are calculated by the ITE package (Szabo, 2014) with 10,000 samples.

Figure 4: Sample trajectories generated from 10-layer HSIVI-SM on four 2D toy examples. The arrows represent the estimated score function in HSIVI-SM. The sample size is 10,000.

by selecting \(T\) discrete time steps as before. For HSIVI-SM with \(\)-training, the conditional layer \(q_{t}(|_{t+1};)\) is modeled as a Gaussian distribution with mean \(_{t}(_{t+1};^{})\) and diagonal variance matrix \(_{t}(^{})\) where \(\{^{},^{}\}=\) are the variational parameters. In our implementations, both \(_{t}(_{t+1};^{})\) and \(_{t}(_{t};)\) use the same architecture as \(^{*}(,s)\). The number of layers, which is also the number of function evaluations (NFE), is set to be \(T=5,10,15\) in our experiments. We train HSIVI-SM with the same setting for \(T=10,15\). The 5-layer HSIVI-SM is trained by further fine-tuning the well-trained 15-layer HSIVI-SM and we find this strategy leads to better results. During each nested training loop of \(_{t}(_{t};)\), we update \(\) 20 times before each update of \(\), since we find \(_{t}(_{t};)\) needs more training empirically to provide reliable guidance.

For each method, we draw 50,000 samples and use the Frechet inception distance (FID) score (Karras et al., 2022) to evaluate the sample quality (Table 2). We find that HSIVI-SM performs on par or better than the other baselines on both CIFAR-10 and CelebA, and the advantage is evident when the NFE is small. The sampling trajectories of 10-layer HSIVI-SM on CelebA with the same starting point but different random seeds are shown in Figure 6. We see that HSIVI-SM is capable of producing more diverse samples due to its stochastic nature, which is different from existing ODE based fast diffusion model samplers.

### Additional Study

Ablation of layers numberIn Figure 7, We provide a failure case on fitting the checkerboard target with diffusion bridge, demonstrating that the HSIVI-SM algorithm fails when the layer number \(T\) is small (the distances of auxiliary distributions at successive time steps are large) on a checkerboard

   Dataset &  &  &  \\  NFE & 5 & 10 & 15 & 5 & 10 & 15 & 5 & 10 & 15 \\  DDPM (Ho et al., 2020) & 320.16 & 278.65 & 198.00 & 366.10 & 309.95 & 206.92 & 402.68 & 358.80 & 284.00 \\ DDIM (Song et al., 2020a) & 41.53 & 13.73 & 8.78 & 27.38 & 10.89 & 7.78 & 147.03 & 42.31 & 24.85 \\ FastDPM (Kong \& Ping, 2021) & 67.64 & 9.85 & 6.16 & 27.63 & 15.44 & 12.05 & N/A & N/A & N/A \\ Analytic-DDFM (Bao et al., 2022) & 93.16 & 34.54 & 20.03 & 50.92 & 28.93 & 21.84 & N/A & 60.65 & 45.98 \\ Analytic-DDM (Bao et al., 2022) & 51.86 & 14.08 & 8.65 & 29.40 & 15.74 & 12.25 & N/A & 70.62 & 41.56 \\ DPM-Solver-fast (Lu et al., 2022) & 329.13 & 10.89 & 4.67 & 355.96 & 6.76 & 2.98 & 402.43 & 28.96 & 20.03 \\
**HSIVI-SM (ours)** & **6.27** & **4.31** & **4.17** & **6.22** & **3.09** & **2.23** & **40.43** & **17.67** & **15.49** \\   

Table 2: Sample quality measured by FID (\(\)) on CIFAR-10, CelebA and ImageNet with a varying number of function evaluations (NFE). Results of baselines are calculated by running their official codes, where the architectures of score model (or noise model) are the UNet employed in Ho et al. (2020) for CIFAR-10 and CelebA and (Nichol & Dhariwal, 2021) in ImageNet.

Figure 5: Comparison of the quality of uncurated samples generated by DDPM, DDIM, and HSIVI-SM with 5 discrete time steps on MNIST.

Figure 6: Sample trajectories of 10-layer HSIVI-SM with the same starting point \(_{10}\) on CelebA.

distribution. In fact, the score function on the checkerboard target is sharp on the boundaries but vanishes elsewhere. Therefore, fitting this target distribution is somewhat challenging.

Ablation of the variational familyTo validate the improvement of HSIVI-SM on diffusion models, we train HSIVI-SM with isotropic conditional layers in consistency with denoising-diffusion sampling, like DDPM and DDIM. We report the results of FID on the CIFAR-10 dataset in Table 3. These results provide further evidence for the statement outlined in Section 3.2. HSIVI-SM matches the marginal distributions \(q_{t}(_{t})\) and \(p_{t}(_{t})\) directly via score matching and would ensure a better fit for \(p_{0}(_{0})\). The enhancement of HSIVI-SM over DDPM stems not only from its more expressive variational distribution but also from the direct alignment of the marginal distributions.

## 6 Conclusions

We introduced HSIVI, a hierarchical semi-implicit variational inference method that enables more expressive multi-layer construction of semi-implicit distributions. Given appropriate auxiliary distributions that interpolate between a simple base distribution and the target distribution, the conditional layers in hierarchical semi-implicit distributions can be progressively trained one layer after another. In experiments, we showed that HSIVI outperforms previous single-layer SIVI methods on several Bayesian inference tasks with complicated posteriors. HSIVI can also be used to accelerate the sampling process of diffusion models, where pre-trained score networks serve as a natural sequence of bridging distributions, which allows for direct acceleration of the stochastic diffusion model and does not require expensive sampling from the diffusion models during training. We showed that HSIVI can produce high quality samples comparable to or better than existing fast diffusion model samplers with few function evaluations on various datasets. Limitations are discussed in Appendix F.