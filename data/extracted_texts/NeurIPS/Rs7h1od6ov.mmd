# Privacy-Preserving Large Language Model Inference via GPU-Accelerated Fully Homomorphic Encryption

Leo de Castro\({}^{2,3,}\) Daniel Escudero\({}^{1,2}\) Antigoni Polychroniadou\({}^{1,2}\)

\({}^{1}\)J.P. Morgan AI Research \({}^{2}\)J.P. Morgan AlgoCRYPT Center of Excellence

\({}^{3}\) J.P. Morgan Chase Cybersecurity & Technology Controls

{leo.decastro, daniel.escudero, antigoni.polychroniadou}@jpmorgan.com

This work was done while the author was affiliated with the Massachusetts Institute of Technology.

###### Abstract

As large language models (LLMs) become more ubiquitous, security concerns regarding sensitive queries grow. Due to the complexities of deploying these models, LLM evaluation is often outsourced to a third-party cloud, which leaks the clients' queries to this external provider. These queries could contain sensitive information such as intellectual property, medical information, and proprietary data. Protecting this data while maintaining the LLM's functionality is a major privacy challenge. Fully homomorphic encryption (FHE) presents a natural solution to this problem: simply encrypt the query and evaluate the LLM homomorphically on the cloud machine. The result remains encrypted and can only be learned by the client who holds the secret key. There are two barriers to this solution: (1) FHE operations do not easily support the LLM activation functions and (2) FHE implementations remain too slow to evaluate an LLM in a reasonable time.

In this work, we address both of these barriers to present a fully encrypted version of GPT-2 with forward pass times over \(150\) faster than the CPU baseline. This result builds on two main technical contributions. First, we present the first open-sourced implementation of GPU-accelerated FHE as an extension to the popular OpenFHE library, achieving roughly \(200\) performance improvement for many critical functions including bootstrapping. Second, we present novel and extensive experimental analysis of approximations of LLM activation functions to maintain accuracy while achieving this performance. We run extensive benchmarks using the HellaSwag, LAMBADA and ARC datasets, and our results show that the accuracy/perplexity degradation with respect to "out-of-the-box" GPT-2 is minimal.

## 1 Introduction

Large language models (LLMs) have proven to be groundbreaking artificial intelligence tools that are set to change the way humans interact with software. By training on massive amounts of data and using an incredibly large amount of trainable parameters, LLMs are able to provide unprecedented inference results. The tasks that LLMs excel at include natural language generation, question-answering, summarization, translation, code generation, among several others. Models like GPT-3 (https://openai.com/index/gpt-3-apps/) or Claude (https://www.anthropic.co m/news/claude-2) can produce coherent and contextually appropriate text on a wide range of topics. However, these models require massive amounts of resources to be trained, and are often not publicly available as this constitutes the provider's intellectual property. This leads to a "inference-as-a-service" scenario, where clients send their queries to external providers who locally run an LLM to return a result to the client. Furthermore, even open source LLMs such as Llama 2(https://llama.meta.com/llama2/) are very expensive to run in commodity hardware and still require in most cases delegating inference to a third party provider.

Unfortunately, delegating inference is undesirable in many settings where the client wants to preserve the privacy of their input. Furthermore, as mentioned above, there are multiple contexts in which the model owner also wants to retain privacy of the model itself, for example when the model involves massive monetary resources to be trained, or when it incorporates sensitive data (_e.g._ a bank servicing a credit score model trained on internal data). This is particularly relevant as LLMs become more pervasive and find more use-cases that permeate all areas of society. This tension between privacy and utility heavily limits the applicability of LLMs, rendering them useless in contexts where data cannot be outsourced due to privacy constraints.

Towards resolving this tension, fully homomorphic encryption (FHE) is a promising tool that enables computing on data without revealing it, only outputting the final result (_cf_ for a survey). Using FHE, a client can _encrypt_ their query to the server, who can locally apply their model to this encrypted data, making use of the homomorphic properties of the scheme to obtain an encrypted result, which is sent back to the client for decryption. See Fig. 1 for a pictorial representation of this interaction pattern. Advances in the last decade on all fronts including algorithms, software and hardware, have made FHE practical for several tasks that were not within reach before. However, LLMs are in an entirely different regime: their computation is already very expensive in the clear, up to the point in which specialized software such as high-end GPUs, coupled with several architectural optimizations, are needed in order to provide a reasonable inference latency. Any computation that is ran under FHE becomes _much_ slower, which is going to be a major blocker when porting LLMs to FHE. However, the question remains:

_How practical is FHE-based privacy-preserving LLM evaluation?_

To address this question, a good starting point is the CKKS scheme by Cheon et al. , which enables approximate additions and multiplications over real (in fact, complex) numbers. We provide detail background on FHE and CKKS in Sections 2.2 and 5, respectively. The literature in improving the efficiency of this scheme is vast and fruitful , and this has enabled several applications in contexts such as logistic regression  and secure password search .

Only the recent work of Zhang et al.  has explored large language model inference via CKKS, reporting an implementation of the transformer architecture in C++, using the SEAL library for FHE (https://github.com/Microsoft/SEAL). Their experiments report minor accuracy degradation due to polynomial approximations needed in FHE, and performance in Intel CPUs seems promising, as it is accelerated via HEXL . We discuss this work further in section 1.2. Although promising given the massive overheads involved in both LLMs and FHE, this is still far from practical for real-world usage, even for applications that are not latency sensitive such as text summarization or content generation (in contrast to chatbots or Q/A tasks, which are more demanding in terms of responsiveness).

### Our contributions

We approach the problem of improving the efficiency of FHE-based privacy-preserving LLM inference, by providing a _GPU-based_ implementation of the transformer architecture using CKKS. Prior work  has shown GPUs to help in improving the efficiency of CKKS. However, to the best of our knowledge, there is currently no available implementation of such works to deploy and test these ideas. In contrast, there are popular open-source _CPU-based_ frameworks that aim at making FHE techniques more accessible by providing high level programming interfaces, and access to multiple FHE schemes, like CKKS. One such framework is _OpenFHE_, which has gained traction as one of the most comprehensive and widely used FHE implementations available. Unfortunately, OpenFHE is limited to CPUs, and hence its performance in tasks such as LLM inference would be rather poor.

In this work we extend the capabilities of OpenFHE by enabling a GPU-based workflow, which leads to direct efficiency improvements across many FHE applications that build on this framework--not only LLMs. This requires a deep understanding of the internal CKKS operations to replicate them on the GPU. We provide benchmarks for our GPU-accelerated CKKS bootstrapping in appendix E. We have open-sourced the code of our OpenFHE+GPU extension2.

With our GPU-based implementation in place, we set out to benchmark the performance of large language models under FHE. We focus specifically on the GPT-2 architecture by OpenAI, which is fully open-source and shares common features with many of the more powerful industry-grade models. One first obstacle we face is that FHE techniques do not support all operations available to a common CPU/GPU and instead only supports additions and multiplications. As usual in the FHE literature, we use off-the-shelf polynomial approximations to replicate as faithfully as possible the transformer architecture, while adapting for FHE use. Our approximations are discussed in Appendix D. Note that these modifications have the potential of negatively affecting the accuracy of the model, which is far from ideal. To address this, we modify the GPT-2 implementation from HuggingFace's transformers library (https://github.com/huggingface/transformers) so that it includes these FHE-friendly modifications, and thoroughly benchmark the resulting accuracy using the LM evaluation harness library (https://github.com/EleutherAI/lm-evaluation-harness) on a selection of tasks. This allows us to select optimal parameters for the approximations that strike the right balance between efficient FHE runtimes and model accuracy. Furthermore, for reproducibility we also open source our modified HuggingFace GPT-2 implementation.

Our results given in section 3 show that a GPU-accelerated FHE implementation provides a roughly \(200\) speedup in the GPT-2 forward pass, reducing the time from several hours to just a few minutes. This brings the forward pass time down to a range where non-real-time applications become more practical, such as document summarization and fine-tuning models on private data.

### Related Work

There is a long line of works studying secure inference for protecting the privacy of both a client owning a query, and a server holding a trained model. At a high level, we can divide these techniques into two groups: highly interactive approaches based on MPC, and less communication-demanding but more computationally-heavy paradigms based on FHE. We focus this section on FHE-based approaches, leaving the discussion on MPC-based techniques to Appendix A.

Figure 1: On the left: communication pattern between the two parties. On the right: GPT-2 architecture, which corresponds to the local computation by the server.

FHE-based LLM inference.FHE-based secure inference has the notable advantage that it preserves the same communication pattern of non-private inference: the client sends the query to the server, who performs certain (presumably heavy) computation and sends back the result. This is applicable to real-world settings where client and servers may not be well connected, and the server is considerably more powerful than the client. In this context, the most relevant work in secure LLM inference with FHE is . This work makes use of several polynomial approximations from the literature, some of which we borrow as well (see Appendix D). Importantly, their implementation is limited to CPU, which caps their performance substantially. Rather than comparing to this work, we instead compare directly to the out-of-the-box OpenFHE CPU implementations of the FHE functions. This allows us to account for variations in the approximations and the placement of the bootstrapping functions.

The work of  studies HE-friendly approximation of the transformer architecture, but it is not applicable to our case since this require re-training. Primer  and THE-X  also employ FHE for LLM evaluation (Primer in fact mixes FHE and MPC), but these works also make substantial modifications to the underlying model. THE-X even reveals intermediate values of the computation.

Privacy-preserving ML for other models.Finally, we mention that there are several other works that have studied FHE-based inference of other machine learning models, such as convolutional neural networks (_cf._). These are not applicable to transformers directly as they do not support all of the operations involved in this architecture, and additionally the scale of the models they consider is much more reduced.

### Setting and Threat Model

We consider a client who holds as input a text sequence, and a server who holds a large language model. The goal is for the client to learn the evaluation of their query on the model without leaking the input to the server, and while protecting the privacy of the model towards the client. See Fig. 0(a) for a pictorial representation of the task and the communication flow. The server does not learn any information about the client's input, but we provide no correctness guarantees regarding the result the server returns to the client--a corrupt server can return an incorrect answer, or no answer at all. This is consistent with prior works, and it is strictly better than the guarantees provided by MPC-based solutions, which may leak information towards a corrupt server that deviates from the protocol specification.

We assume the client has access to the _tokenizer_ of the model (see Section B.1), so that the client can locally transform their text into a sequence of real-valued vectors, which are then encrypted towards the server. We do not provide any guarantees on the plaintexts underlying the ciphertexts that the client sends. In particular, a corrupt client may send a sequence of vectors that does not correspond to valid token embeddings, and will be able to learn the LLM evaluation on this input. This is in par with previous privacy-preserving ML works based on FHE.

## 2 Preliminaries

In what follows we provide background on large language models and fully homomorphic encryption.

Some general notation we will use throughout the paper is the following. Vectors are denoted by bold letters, like \(\), and indexing the \(i\)-th entry is denoted by \([i]\). Given a positive integer \(n\), we let \([n]\) denote the set \(\{1,,n\}\).

### Large Language Models

A large language model (LLM) is a type of machine learning (ML) model that is characterized by its ability to predict _language_, with the "large" term emphasizing their comparatively gigantic sizes and computational demands. Vaswani et al.  introduced the transformer architecture, which is the basis for several LLMs that came right after. Among LLMs, an interesting and relevant family are generative pretrained transformers (GPTs), which are used in natural language processing contexts. This family, developed by OpenAI, has been widely influential and has spawned a series of follow-ups. In this work we focus specifically on the **GPT-2** model, which is trained on WebText:40 GB of text, 8 million documents, from 45 million webpages upvoted on Reddit. We chose this model as (1) it is fully _open source_, (2) it follows the transformer architecture shared by other more powerful LLMs, and (3) this is already challenging in terms of efficiency for current FHE approaches. We note however that our findings carry out to several other LLMs that follow this paradigm, such as the larger models like GPT-3 or GPT-4 or other transformer-based LLMs like Llama and Llama 2. In what follows, we describe the GPT-2 architecture in detail. There are four variants of GPT-2 which vary in size and performance: S, M, L and XL, and we discuss below the points where these differ.

LLMs use deep learning to analyze and generate human-like text. The transformer architecture by Vaswani et al.  receives as input a piece of text, which is split into numerical representations referred to as _tokens_. Transformers are comprised of an encoder and a decoder section, which are very similar in structure. However, generative LLMs such as GPT are _decoder-only_, and so for the sake of this work we will focus on the decoder component of the transformer architecture; we note that encoders follow a similar structure and our findings apply to encoder-decoder or encoder-only architectures as well.

The model is trained to predict the best next word given a sequence of words. For example, it may receive as an input "Today is a good", and then predict "day" as the next word. The resulting concatenated sentence "today is a good day" can be fed into the model again to obtain as the next word, perhaps, "for". This way a sequence like "today is a good day for running outside" can be generated.

An overview of the GPT-2 architecture, highlighting the blocks that are most relevant for FHE, is given in fig. 0(b); see appendix B for additional details. Throughout this work, we use the "small" variant of GPT-2 with embedding dimension \(d=768\).

### Fully Homomorphic Encryption

A fully homomorphic encryption (FHE) scheme ,  is an encryption scheme that allows computations to be performed over the data while the data remains encrypted. More formally, an FHE scheme is defined by the following tuple of algorithms.

* \((,,)(1^{})\). This is the key generation algorithm. The input is the security parameter \(\) and the output is three keys. The secret key \(\) is used for decryption, the public key \(\) is used for encryption, and the evaluation key \(\) is used to homomorphically compute over encrypted data.
* \((,m)\). This is the encryption algorithm. It takes in a message \(m\) and a public key \(\) and outputs a ciphertext \(\).
* \(m^{}(,^{})\). This is the decryption algorithm. It takes in a ciphertext \(^{}\) and a secret key \(\) and outputs a message \(m^{}\).
* \(_{f}(,,f)\). This is the homomorphic evaluation algorithm. It takes in as input an evaluation key \(\), a ciphertext \(\), and a function \(f\). Let \(m\) be the message encrypted by \(\) (i.e. \(m(,)\)). The output of \(\) is the ciphertext \(_{f}\) that encrypts \(f(m)\).

FHE must satisfy the same security level as a regular encryption scheme, which dictates that a party without access to the secret key cannot distinguish between encryptions of any two messages, even if the messages are adversarially chosen.

## 3 Experimental Results

In this section, we present the full LLM runtimes under FHE. These evaluations are run entirely on the server, and at no point can the server view the underlying query or any intermediate value. Furthermore, the output of the LLM forward pass can be fed directly back into the model to compute the next token without any interaction with the client. This powerful technique allows an arbitrary number of forward passes to be executed on the client's encrypted query. This method extends to other operations that require the forward-pass as a subroutine, such as fine-tuning on private data.

As we mentioned in the introduction, we focus on GPT-2 (small) due to its accessibility as well as the similarity in the architecture of larger GPT models. Our performance benchmarks can be extended to models with many more parameters by linearly scaling the transformer architecture.

### Accuracy of the Approximate Model

In order to make our LLM compatible with FHE, we replace each non-linear function with the corresponding approximation described in section 2. We evaluate this variant of GPT-2 on standard accuracy benchmarks to ensure that these approximations do not compromise the model's performance. We achieve this by forking the GPT-2 implementation in the HuggingFace transformers library (https://github.com/huggingface/transformers), and making the following modifications in order to reflect the changes that FHE imposes:

* The GeLU activation is replaced by the approximation from Section D.2. We use degree \(2\) for the \(f\) and \(g\) polynomials in the comparison from Section D.1, and we compose them \(2\) times each.
* LayerNorm is approximated as in Section D.3. We use \(16\) Newton iterations
* SoftMax is approximated as in Section D.4. For the approximation of \(\) we use \(r=7\), and for Goldschmidt algorithm--used for the division--we use \(7\) iterations.

Performing these modifications is intricate as the transformers library is not intended to support changes such as replacing the SoftMax, for instance, which is rather uncommon in machine learning contexts. Once our modified model is loaded in HuggingFace's "format", we are able to leverage the Language Model Evaluation Harness library (https://github.com/EleutherAI/lm-evaluat ion-harness), which includes multiple benchmarks to evaluate LLM performance. Our accuracy benchmarks appear in table 1, where we measure the performance of our modifications with respect to the baseline GPT-2 (small) on three datasets: Lambda, HellaSwag and ARC. The Lambda dataset is a collection of passages and sentences used for evaluating the ability of language models to understand context and perform coherent text continuation or next word prediction. HellaSwag tests LLM's ability to capture commonsense reasoning about situations described in natural language. ARC (AI2 Reasoning Challenge) is a dataset created by the Allen Institute for Artificial Intelligence (AI2) to evaluate question answering systems' ability to perform multi-step reasoning. We refer the reader to the evaluation harness library for details on these tasks.

Overall, we observe that our modifications incur in little accuracy degradation with respect to the baseline model. This reflects the robustness of large language model to slight deviations, highly exploited in the quantization literature (_cf_), and is crucial for enabling privacy-preserving inference. Note that these approximations are also useful for MPC-based approaches.

    &  &  & ARC (Easy) \\   & Perplexity & Accuracy & Accuracy & Accuracy \\  Baseline & 40.0554 & 0.3256 & 0.2892 & 0.4381 \\ Approximate & 41.8580 & 0.3013 & 0.2918 & 0.4327 \\   

Table 1: Performance of GPT-2 (small) with our different approximations vs. the unaltered baseline. We use polynomials of degree 4—each composed twice—for the comparison approximations, (see Section D.1). We use \(16\) Newton iterations for the inverse square root (see Section D.3). We use \(7\) iterations of Goldschmidt algorithm for the Softmax division, and we use \(r=7\) for the approximation of \(\) in Softmax (see Section D.4).

   Function & SoftMax & LayerNorm & GeLU & Argmax \\  depth & 133 & 13 & 17 & 272 \\ number of ciphertexts & \(0.25\) & \(1.5\) & \(6\) & \(1\) \\   

Table 2: Depths of our approximate activation functions. The approximations (described in appendix D) have the same parameters as the plaintext circuits benchmarked in table 1. The softmax input size is \(128 128\) values, which requires a in a depth-7 comparison tree to compute the max of all sets of \(128\) values in parallel. The number of slots in each ciphertext is \(n=2^{16}\). Non-integer ciphertexts indicate that not all slots are filled and batched evaluation is available in this layer.

### Runtimes of LLM Inference in FHE

We now present the end-to-end runtime of a GPT-2 forward pass using our GPU-accelerated FHE. Note that, as illustrated in Fig. 0(b), the complexity of a GPT forward pass is dependent on the position of the token being generated in the output, given that the dimension for the softmax in each decoder block depends on the token position. Furthermore, as we discuss in Remark 2, all tokens of the input sequence have to be processed _once_ by the decoder blocks before any new token can be generated. Throughout this section, we benchmark generating a token at position 128, assuming that the previous input tokens have been processed. The cost of processing the input is amortized away as more tokens are produced, which is also consistent with prior works.

We note a few important optimizations that are incorporated into this benchmark:

_Input & Output Sizes._ We give the depth of each approximation in table 2. Recall from the high-level GPT architecture that SoftMax and GeLU are run once per block and LayerNorm is run twice per block. The GPT-2 model consists of 12 blocks, and the final ArgMax function is run at the end of the forward pass. The dimension of one token embedding is \(768\), and the inputs and outputs of both LayerNorm operations is \(128 768\). The GeLU input consists of 24 channels of the typical \(128 768\), resulting in a total input of \(3072 768\). By contrast, the SoftMax input is the result of many inner-product operations with the context embeddings, resulting in an input and output size of \(128 128\). With \(2^{16}\) slots in each ciphertext, this gives the values in the second row of Table 2.

_Batched Evaluation._ When a function is evaluated over an input that does not use all available slots in a ciphertext, additional performance can be gained by evaluating another input to that function and using the additional unused slots. This batched evaluation maximizes the available parallelism in the CKKS scheme. For example, the LayerNorm function only requires \(1.5\) ciphertexts to store the input and output. If only one LayerNorm function is being evaluated, then we must perform the operation over two ciphertexts even though the second is half empty. However, if we have the option of running a second LayerNorm function over an independent input, we can evaluate both LayerNorm functions using only three ciphertexts, which doubles our throughput with only a \(50\%\) increase in latency. This is an important optimization for tasks such as training or fine-tuning, where the model is evaluated on batches of samples from the training set. We also present the "unbatched" single-input evaluation for comparison.

Benchmarks.We present our benchmarks in Figure 2 and Figure 3. Both figures display the forward pass time of our encrypted GPT-2 at position 128. All individual layer benchmarks include the internal bootstrapping time, which is interleaved within the function as needed. All benchmarks were run on the same machine as the bootstrapping benchmarks in appendix E. This machine has an Intel Xeon chip running at 2.4 GHz and 2 TB of RAM as well as an NVIDIA A100 80GB PCIe.

In Figure 2, we demonstrate the speedup of our GPU-accelerated FHE library when applied to the task of a GPT-2 forward pass. This figure measures our GPU implementation against the out-of-the-box OpenFHE functions running on a CPU.

In the unbatched forward pass, the SoftMax function is one of the most expensive operations primarily due to the low utilization of the ciphertext. When switching to batched evaluation, the overhead of the SoftMax drops significantly (\(4\)) as well as the LayerNorm function discussed above. The GeLU function has full utilization of the ciphertexts, so the overhead with batching remains the same. The batching speedups translate into the benchmarks for the full model. Recall that the full forward pass consists of 12 blocks and an ArgMax. We do not batch the ArgMax evaluation since only a small portion of the ciphertext is left unused.

We provide benchmarks at two different security levels depending on the application requirements. Setting the security parameter \(=128\) is standard for encryption schemes, although many applications allow a slightly weaker \(=80\). Concretely, setting \(=128\) gives us a bootstrapping routine that refreshes \(20\) ciphertext levels in roughly \(550\) milliseconds, while relaxing to \(=80\) allows a bootstrapping routine that refreshes \(45\) levels in under \(1\) second. This increase in the bootstrapping throughput is the main source of speedup.

### Limitations

We briefly discuss the limitations of our results. Our benchmarks are based on the accuracy of the GPT-2 model with the activation functions replaced with polynomial approximations. The degree of these polynomials has a major impact on the performance of the encrypted forward pass, since a higher degree directly translates into deeper circuits that require more bootstrapping operations. While many LLM models seems to remain accurate with low precision, many other AI models such as image recognition models require higher precision during evaluation to maintain accuracy. If a model requires a higher precision than GPT-2, the polynomial approximations would need to be increased. When the required precision increases beyond roughly \(16\) bits, the complexity of the bootstrapping itself must be increased, since internal to the bootstrapping is an approximation of a modular reduction function. The relatively low precision required by these transformer models is crucial to our results.