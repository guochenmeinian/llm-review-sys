# Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games

Youbang Sun

Northeastern University

sun.youb@northeastern.edu

&Tao Liu

Texas A&M University

tliu@tamu.edu

&Ruida Zhou

Texas A&M University

ruida@tamu.edu

&P. R. Kumar

Texas A&M University

prk@tamu.edu

&Shahin Shahrampour

Northeastern University

s.shahrampour@northeastern.edu

The first two authors contributed equally.

###### Abstract

This work studies an independent natural policy gradient (NPG) algorithm for the multi-agent reinforcement learning problem in Markov potential games. It is shown that, under mild technical assumptions and the introduction of the _suboptimality gap_, the independent NPG method with an oracle providing exact policy evaluation asymptotically reaches an \(\)-Nash Equilibrium (NE) within \((1/)\) iterations. This improves upon the previous best result of \((1/^{2})\) iterations and is of the same order, \((1/)\), that is achievable for the single-agent case. Empirical results for a synthetic potential game and a congestion game are presented to verify the theoretical bounds.

## 1 Introduction

Reinforcement learning (RL) is often impacted by the presence and interactions of several agents in a multi-agent system. This challenge has motivated recent studies of multi-agent reinforcement learning (MARL) in stochastic games . Applications of MARL include robotics , modern production systems , economic decision making , and autonomous driving . Among the various types of stochastic games, we focus on a commonly studied model for MARL, known as Markov Potential Games (MPGs). MPGs are seen as a generalization of canonical Markov Decision Processes (MDPs) in the multi-agent setting. In MPGs, there exists a potential function that can track the value changes of all agents. Unlike single-agent systems, where the goal is to find the optimal policy, the objective in this paper is to find a global policy, formed by the joint product of a set of local policies, that leads the system to reach a Nash equilibrium (NE) , which is precisely defined in Section 2.

A major challenge in the analysis of multi-agent systems is the restriction on joint policies of agents. For single-agent RL, policy updates are designed to increase the probability of selecting the action with the highest reward. However, in multi-agent systems, the global policy is constructed by taking the product of local agents' policies, which makes MARL algorithms suffer a greater risk of being trapped near undesirable stationary points. Consequently, finding a NE in MARL is more challenging than finding the global optimum in the single-agent case, and it is therefore difficult for MPGs to recover the convergence rates of single-agent Markov decision processes (MDPs).

Additionally, the global action space in MPGs scales exponentially with the number of agents within the system, making it crucial to find an algorithm that scales well for a large number of agents. Recentstudies [8; 14] addressed the issue by an approach called independent learning, where each agent performs policy update based on local information without regard to policies of the other agents. Independent learning algorithms scale only linearly with respect to the number of agents and are therefore preferred for large-scale multi-agent problems.

Using algorithms such as policy gradient (PG) and natural policy gradient (NPG), single-agent RL can provably converge to the global optimal policy . However, extending these algorithms from single-agent to multi-agent settings presents natural challenges as discussed above. Multiple recent works have analyzed PG and NPG in multi-agent systems. However, due to the unique geometry of the problem and complex relationships among the agents, the theoretical understanding of MARL is still limited, with most works showing slower convergence rates when compared to their single-agent counterparts (see Table 1).

ContributionsWe study the independent NPG algorithm in multi-agent systems and provide a novel technical analysis that guarantees a provably fast convergence rate. We start our analysis with potential games in Section 3.1 and then generalize the findings and provide a convergence guarantee for Markov potential games in Section 3.2. We show that under mild assumptions, the ergodic (i.e., temporal average of) NE-gap converges with iteration complexity of \((1/)\) after a finite threshold (Theorem 3.6). This result provides a substantial improvement over the best known rate of \(1/^{2}\) in . Our main theorem also reveals mild or improved dependence on multiple critical factors, including the number of agents \(n\), the initialization dependent factor \(c\), the distribution mismatch coefficient \(M\), and the discount factor \(\), discussed in Section 3. We dedicate Section 3.3 to discuss the impact of the asymptotic suboptimality gap \(^{*}\), which is a new factor in this work.

In addition to our theoretical results, two numerical experiments are also conducted in Section 4 for verification of the analysis. We consider a synthetic potential game similar to  and a congestion game from . The omitted proofs of our theoretical results can be found in the appendix.

### Related Literature

Markov Potential GamesSince many properties of single-agent RL do not hold in MARL, the analysis of MARL presents several challenges. Various settings have been addressed for MARL in recent works. A major distinction between these works stems from whether the agents are competitive or cooperative . In this paper we consider MPGs introduced in stochastic control . MPGs are a generalized formulation of identical-reward cooperative games. Markov cooperative games have been studied in the early work of , and more recently by [23; 24; 32; 10]. The work 

   Algorithm & Iteration Complexity 2  \\  PG + direct[39; 19] & \(^{n}|_{i}|M^{2}}{(1-)^{4} ^{2}}\) \\ PG + softmax & \(|_{i}|M^{2}}{(1-)^{4} ^{2}}\) \\ NPG + softmax & \(|_{i}|M^{2}}{(1-)^{4} ^{2}}\) \\ NPG + softmax + log-barrier reg. & \(|_{i}|M^{2}}{(1-)^{4} ^{2}}\) \\ NPG + softmax + log-barrier reg. & \(|_{i}|M^{2}}{(1-)^{4} ^{2}}\) \\ Projected Q ascent & \(_{i}|_{i}|M^{2}}{(1-)^{7} ^{4}}\) or \(|_{i}|M^{4}}{(1-)^{2} ^{2}}\) \\ Projected Q ascent (fully coop)  & \(|_{i}|M}{(1-)^{3}^{4} }\) \\  (Ours) NPG + softmax & \(}{(1-)^{2}^{3}^{4}} \)3  \\   

Table 1: Convergence rate results for policy gradient-based methods in Markov potential games. Some results have been modified to ensure better comparison.

also offered extensive empirical results for cooperative games using multi-agent proximal policy optimization. The work by  established polynomial convergence for MPGs under both Q-update as well as actor-critic update.  studied an independent Q-update in MPGs with perturbation, which converges to a stationary point with probability one. Conversely, [17; 13] studied potential games, a special static case of simplified MPGs with no state transition.

Policy Gradient in GamesPolicy gradient methods for centralized MDPs have drawn much attention thanks to recent advancements in RL theory [1; 26]. The extension of PG methods to multi-agent settings is quite natural. [6; 35; 5] studied two-player zero-sum competitive games. The general-sum linear-quadratic game was studied in .  studied general-sum Markov games and provided convergence of V-learning in two-player zero-sum games.

Of particular relevance to our work are the works [19; 39; 38; 8] which focus on the MPG setting and propose adaptations of PG and NPG-based methods from single-agent problems to the MARL setting. Table 1 provides a detailed comparison between these works. The previous theoretical results in multi-agent systems have provided convergence rates dependent on different parameters of the system. However, the best-known iteration complexity to reach an \(\)-NE in MARL is \(1/^{2}\). Therefore, there still exists a rate discrepancy between MARL methods where \((1/)\) complexity has been established in centralized RL algorithms . Our main contribution is to close this gap by establishing an iteration complexity of \((1/)\) in this work.

## 2 Problem Formulation

We consider a stochastic game \(=(n,,,P,\{r_{i}\}_{i[n]},,)\) consisting of \(n\) agents denoted by a set \([n]=\{1,...,n\}\). The global action space \(=_{1}..._{n}\) is the product of individual action spaces, with the global action defined as \(:=(a_{1},...,a_{n})\). The global state space is represented by \(\), and the system transition model is captured by \(P:()\). Furthermore, each agent is equipped with an individual reward function \(r_{i}:\). We use \((0,1)\) to denote the discount factor and \(()\) to denote the initial state distribution.

The system policy is denoted by \(:(_{1})( _{n})()\), where \(()\) is the probability simplex over the global action space. In the multi-agent setting, all agents make decisions independently given the observed state, often referred to as a _decentralized_ stochastic policy . Under this setup, we have \((|s)=_{i[n]}_{i}(a_{i}|s)\), where \(_{i}:(_{i})\) is the local policy for agent \(i\). For the ease of notation, we denote the joint policy over the set \([n]\{i\}\) by \(_{-i}=_{j[n]\{i\}}_{j}\) and use the notation \(a_{-i}\) analogously.

We define the state value function \(V_{i}^{}(s)\) with respect to the reward \(r_{i}(s,)\) as \(V_{i}^{}(s):=^{}[_{t=0}^{}^{t}r_{i}(s^{t}, {a}^{t})|s^{0}=s]\), where \((s^{t},^{t})\) denotes the global state-action pair at time \(t\), and we denote the expected value of the state value function over the initial state distribution \(\) as \(V_{i}^{}():=_{s}[V_{i}^{}(s)]\). We can similarly define the state visitation distribution under \(\) as \(d_{}^{}(s):=(1-)^{}[_{t=0}^{}^ {t}(s_{t}=s)|s_{0}]\), where \(\) is the indicator function. The state-action value function and advantage function are, respectively, given by

\[Q_{i}^{}(s,)=^{}[_{t=0}^{}^{t}r_{i}(s^{ t},^{t})|s^{0}=s,^{0}=],\ \ A_{i}^{}(s,)=Q_{i}^{}(s,)-V_{i}^{}(s).\] (1)

For the sake of analysis, we further define the marginalized Q-function and advantage function \(_{i}:_{i}\) and \(_{i}:_{i}\) as:

\[_{i}^{}(s,a_{i}):=_{a_{-i}}_{-i}(a_{-i}|s)Q_{i}^{}(s,a_{i}, a_{-i}),\ _{i}^{}(s,a_{i}):=_{a_{-i}}_{-i}(a_{-i}|s)A_{i}^{}(s,a_{i},a_{- i}).\] (2)

**Definition 2.1** ().: _The stochastic game \(\) is a Markov potential game if there exists a bounded potential function \(:\) such that for any agent \(i\), initial state \(s\) and any set of policies \(_{i},_{i}^{},_{-i}\):_

\[V_{i}^{_{i}^{},_{-i}}(s)-V_{i}^{_{i},_{-i}}(s)=^{_{i}^ {},_{-i}}(s)-^{_{i},_{-i}}(s),\]

_where \(^{}(s):=^{}[_{k=0}^{}^{k}(s^{k},^{k})|s^{0}=s]\)._We assume that an upper bound exists for the potential function, i.e., \(0(s,)_{max}, s,,\) and consequently, \(^{}(s)}{1-}.\)

It is common in policy optimization to parameterize the policy for easier computations. In this paper, we focus on the widely used softmax parameterization [1; 26], where a global policy \((|s)=_{i[n]}_{i}(a_{i}|s)\) is parameterized by a set of parameters \(\{_{1},...,_{n}\},_{i}^{||| _{i}|}\) in the following form

\[_{i}(a_{i}|s)=]_{s,a_{i}}\}}{_{a_{j}_{i}}\{[_{i}]_{s,a_{j}}\}},\;(s,a_{i}) _{i}.\]

### Optimality Conditions

In the MPG setting, there may exist multiple stationary points, a set of policies that has zero policy gradients, for the same problem; therefore, we need to introduce notions of solutions to evaluate policies. The term _Nash equilibrium_ is used to define a measure of "stationarity" in strategic games.

**Definition 2.2**.: _A joint policy \(^{*}\) is called a Nash equilibrium if for all \(i[n]\), we have_

\[V_{i}^{_{i}^{*},_{-i}^{*}}() V_{i}^{_{i}^{},_{-i}^{* }}()\;\;\;^{}$}.\]

For any given joint policy that does not necessarily satisfy the definition of NE, we provide the definition of NE-gap as follows :

\[():=_{i[n],_{i}^{}(_{i})} [V_{i}^{_{i}^{},_{-i}}()-V_{i}^{_{i},_{-i}}() ].\]

Furthermore, we refer to a joint policy \(\) as \(\)-NE when its NE-gap\(()\). The NE-gap satisfies the following inequalities based on the performance difference lemma [15; 38],

\[()_{i,s,a_{i}}d_{}^{_{i}^{*},_{-i}}(s)_{i}^{*}(a_{i}|s)_{i}^{}(s,a_{i})_{i,s}d_{}^{_{i}^{*},_{-i}}(s)_{a_{i}}_{i}^{ }(s,a_{i}).\]

In the tabular single-agent RL, most works consider the optimality gap as the difference between the expectations of the value functions of the current policy and the optimal policy, defined as \(V^{^{k}}()-V^{^{*}}()\). However, this notion does not extend to multi-agent systems. Even in a fully cooperative MPG where all agents share the same reward, the optimal policy of one agent is dependent on the joint policies of other agents. As a result, it is common for the system to have multiple "best" policy combinations (or stationary points), which all constitute Nash equilibria. Additionally, it has also been addressed by previous works that any NE point in an MPG is first order stable . Given that this work addresses a MARL problem, we focus our analysis on the NE-gap.

## 3 Main Results

### Warm-Up: Potential Games

In this section, we first consider the instructive case of static potential games, where the state does not change with time. Potential games are an important class of games that admit a potential function \(\) to capture differences in each agent's reward function caused by unilateral bias [28; 13], which is defined as

\[r_{i}(a_{i},a_{-i})-r_{i}(a_{i}^{},a_{-i})=(a_{i},a_{-i})-(a_{i }^{},a_{-i}), a_{i},a_{i}^{},a_{-i}.\] (3)

Algorithm UpdateIn the potential games setting, the policy update using natural policy gradient is :

\[_{i}^{k+1}(a_{i})_{i}^{k}(a_{i})_{i}^{k}( a_{i}),\] (4)

where the exact independent gradient over policy \(_{i}\), also referred to as oracle, is captured by the marginalized reward \(_{i}(a_{i})=_{a_{-i}_{-i}}[r_{i}(a_{i},a_{-i})].\) By definition, the NE-gap for potential gamesis calculated as \(_{i[n]}_{i}^{*k}-_{i}^{k},_{i}^{k}\), where \(_{i}^{*k}_{_{i}}V_{i}^{_{i},_{-i}^{k}}\) is the optimal solution for agent \(i\) when the rest of the agents use the joint policy \(_{-i}^{k}\).

The local marginalized reward \(_{i}(a_{i})\) is calculated based on other agents' policies; hence, for any two sets of policies, the difference in marginalized reward can be bounded using the total variation distance of the two probability measures . Using this property, we can also show that there is a "smooth" relationship between the marginalized rewards and their respective policies. We note that this relationship holds for stochastic games in general. It does not depend on the nature of the policy update or the potential game assumption.

We now introduce a lemma that is specific to the potential game formulation and the NPG update:

**Lemma 3.1**.: _Given policy \(^{k}\) and marginalized reward \(_{i}^{k}(a_{i})\), for \(^{k+1}\) generated using an NPG update in (4), we have the following inequality for any \(<}\),_

\[(^{k+1})-(^{k})(1-)_{i=1}^{n} _{i}^{k+1}-_{i}^{k},_{i}^{k}.\]

Lemma 3.1 provides a lower bound on the difference of potential functions between two consecutive steps. This implies that at each time step, the potential function value is guaranteed to be monotonically increasing, as long as the learning rate satisfies \(<}\).

Note that the lower bound of Lemma 3.1 involves \(_{i}^{k+1}-_{i}^{k},_{i}^{k}\), which resembles the form of NE-gap \(_{i}^{*k}-_{i}^{k},_{i}^{k}\). Assuming we can establish a lower bound for the right-hand side of Lemma 3.1 using NE-gap, the next step is to show that the sum of all NE-gap iterations is upper bounded by a telescoping sum of the potential function, thus obtaining an upper bound on the NE-gap.

We start by introducing a function \(f^{k}:\) defined as follows,

\[f^{k}()= _{i=1}^{n}_{i,}-_{i}^{k},_{i}^{k} {, where }_{i,}()_{i}^{k}()_{i}^{k }()}.\] (5)

It is obvious that \(f^{k}(0)=0\), \(f^{k}()=_{i}_{i}^{k+1}-_{i}^{k},_{i}^{k} 0\), and \(_{}f^{k}()=_{i}_{i}^{*k}-_{ i}^{k},_{i}^{k}\).

Without loss of generality, for agent \(i\) at iteration \(k\), define \(a_{i_{p}}^{k}_{a_{j}_{i}}_{i}^{k}(a_{j})=: _{i_{p}}^{k}\) and \(a_{i_{q}}^{k}_{a_{j}_{i}_{i_{p}}^ {k}}_{i}^{k}(a_{j})\), where \(_{i_{p}}^{k}\) denotes the set of the best possible actions for agent \(i\) at iteration \(k\). Similar to , we define

\[c^{k}:=_{i[n]}_{a_{j}_{i_{p}}^{k}}_{ i}^{k}(a_{j})(0,1),^{k}:=_{i[n]}[_{i}^{k}(a_{i_{p}}^{k })-_{i}^{k}(a_{i_{q}}^{k})](0,1).\] (6)

Additionally, we denote \(c_{K}:=_{k[K]}c^{k};c:=_{K}c_{K}>0;_{K}:=_{k[K]} ^{k}\).

We provide the following lemma on the relationship between \(f^{k}()\) and \(f^{k}():=_{}f^{k}()\), which lays the foundation to obtain sharper results than those in the existing work.

**Lemma 3.2**.: _For function \(f^{k}()\) defined in (5) and any \(>0\), we have the following inequality._

\[f^{k}() f^{k}()[1-)-1)+1}].\]

We refer to \(_{K}\) as the minimal suboptimality gap of the system for the first \(K\) iterations, the effect of which will be discussed later in Section 3.3. Using the two lemmas above and the definitions of \(c\) and \(_{K}\), we establish the following theorem on the convergence of the NPG algorithm in potential games.

**Theorem 3.3**.: _Consider a potential game with NPG update using (4). For any \(K 1\), choosing \(=}\), we have_

\[_{k=0}^{K-1}(^{k})}{K}(1+}{c_{K}}).\]Proof.: Choose \(==}\) in Lemma 3.2,

\[(^{k}) _{}f^{k}())-1)+1}}f^{k}()+1 }}f^{k}()\] \[)}{c_{K}+1 }}[(^{k+1})-(^{k})]=2[(^{k+1})-(^{k})](1+}{c_{K}}).\]

Then, we have \(_{k=0}^{K-1}(^{k}))- (^{0}))}{K}(1+}{c_{K}}) }}{K}(1+}{c_{K}})\). 

One challenge in MARL is that the NE-gap is not monotonic, so we must seek ergodic convergence results, which characterize the behavior of the temporal average of the NE-gap. Theorem 3.3 shows that for potential games with NPG policy update, the ergodic NE-gap of the system converges to zero with a rate \((1/(K_{K}))\). When \(_{K}\) is uniformly lower bounded, Theorem 3.3 provides a significant speed up compared to previous convergence results. Apart from the iteration complexity, the NE-gap is also dependent linearly on \(1/c\) and \(1/_{K}\). We address the effect of \(c\) in the analysis here and defer the discussion of \(_{K}\) to Section 3.3. Under some mild assumptions, we can show that the system converges with a rate of \((1/K)\).

The Effect of \(c\)The convergence rate given by Theorem 3.3 scales with \(1/c\), where \(c\) might potentially be arbitrarily small. A small value for \(c\) generally describes a policy that is stuck at some regions far from a NE, yet the policy gradient is small. It has been shown in  that these ill-conditioned problems could take exponential time to solve even in single-agent settings for policy gradient methods. The same issue also occurs to NPG in MARL, since the local Fisher information matrix can not cancel the occupancy measure and action probability of other agents. A similar problem has also been reported in the analysis of  for the MPG setting.  proposed the addition of a log-barrier regularization to mitigate this issue. However, that comes at the cost of an \((1/( K))\) convergence rate to a \(\)-neighborhood solution, which is only reduced to the exact convergence rate of \((1/)\) when \(=1/\). Therefore, this limitation may not be effectively avoided without impacting the convergence rate.

### General Markov Potential Games

We now extend our analysis to MPGs. The analysis mainly follows a similar framework as potential games. However, the introduction of state transitions and the discount factor \(\) add an additional layer of complexity to the problem, making it far from trivial. As pointed out in , we can construct MDPs that are potential games for every state, yet the entire system is not a MPG. Thus, the analysis of potential games does not directly apply to MPGs.

We first provide the independent NPG update for MPGs.

Algorithm UpdateFor MPGs at iteration \(k\), the independent NPG updates the policy as follows :

\[_{i}^{k+1}(a_{i}|s)_{i}^{k}(a_{i}|s)\!(_{i}^{^{k}}(s,a_{i})}{1-}),\] (7)

where \(_{i}\) is defined in (2).

Different agents in a MPG do not share reward functions in general, which makes it difficult to compare evaluations of gradients across agents. However, with the introduction of Lemma 3.4, we find that MPGs have similar properties as fully cooperative games with a shared reward function. This enables us to establish relationships between policy updates of all agents. We first define \(h_{i}(s,):=r_{i}(s,)-(s,)\), which implies \(V_{i}^{}(s)=^{}(s)+V_{h_{i}}^{}(s)\).

**Lemma 3.4**.: _Define \(_{h_{i}}^{}(s,a_{i})\) with respect to \(h_{i}\) similar to (2). We then have_

\[_{a_{i}}(_{i}^{}(a_{i}|s)-_{i}(a_{i}|s))_{h_{i}}^{} (s,a_{i})=0, s,i[n],_{i}^{},_{i} (_{i}).\]Lemma 3.4 shows a unique property of function \(h_{i}\), where the expectation of the marginalized advantage function over every local policy \(^{}_{i}\) yields the same effect. This property is directly associated with the MPG problem structure and is later used in Lemma 3.5. Next, we introduce the following assumption on the state visitation distribution, which is crucial and standard for studying the Markov dynamics of the system.

**Assumption 3.1** ().: _The Markov potential game \(\) satisfies: \(_{}_{s}d^{}_{}(s)>0\)._

Similar to potential games, when the potential function \(\) of a MPG is bounded, the marginalized advantage function \(_{i}\) for two policies can be bounded by the total variation between the policies.

Additionally, similar to Lemma 3.1, we present a lower bound in the following lemma for the potential function difference in two consecutive rounds.

**Lemma 3.5**.: _Given policy \(^{k}\) and marginalized advantage function \(^{^{k}}_{i}(s,a_{i})\), for \(^{k+1}\) generated using NPG update in (7), we have the following inequality,_

\[^{^{k+1}}()-^{^{k}}()(-_{max}}{(1-)^{3}})_{s}d^{^{k+1}}_{}(s)_{ i=1}^{n}^{k+1}_{i}(|s),^{^{k}}_{i}(s,).\]

Thus, using a function \(f\) adapted from (5) as a connection, we are able to establish the convergence of NE-gap for MPGs in the following theorem.

**Theorem 3.6**.: _Consider a MPG with isolated stationary points and the policy update following NPG update (7). For any \(K 1\), choosing \(=}{2_{max}}\), we have_

\[_{k=0}^{K-1}(^{k})}{K(1- )}(1+_{max}}{c_{K}(1-)}),\]

_where_

\[M :=_{}_{s}_{}(s)},\] \[c :=_{i[n],s,k 0}_{a_{j} ^{k}_{i_{p}}}^{k}_{i}(a_{j}|s)(0,1),\] \[^{k} :=_{i[n],s}^{^{k}}_{i}(s,a^ {k}_{i_{p}})-^{^{k}}_{i}(s,a^{k}_{i_{q}}),\ \ \ \ _{K}=_{0 k K-1}^{k}(0,1),\]

_similar to (6)._

Here, _isolated_ implies that no other stationary points exist in any sufficiently small open neighborhood of any stationary point. This convergence result is similar to that provided in Theorem 3.3 for potential games. We note that our theorem also applies to an alternate definition for MPGs in works such as , which we discuss in Appendix C. Compared to potential games, the major difference is the introduction of \(M\), which measures the distribution mismatch in the system. Generally, Assumption 3.1 implies a finite value for \(M\) in the MPG setup.

Discussion and Comparison on Convergence RateCompared to the iteration complexity of previous works listed in Table 1, the convergence rate in Theorem 3.6 presents multiple improvements. Most importantly, the theorem guarantees that the averaged NE-gap reaches \(\) in \((1/)\) iterations, improving the best previously known result of \(1/^{2}\) in Table 1. Furthermore, this rate of convergence does not depend on the size of action space \(|_{i}|\), and it has milder dependence on other system parameters, such as the distribution mismatch coefficient \(M\) and \((1-)\). Note that many of the parameters above could be arbitrarily large in practice (e.g., \(1-=0.01\) in our congestion game experiment). Theorem 3.6 indicates a tighter convergence bound with respect to the discussed factors in general.

### The Consideration of Suboptimality Gap

In Theorems 3.3 and 3.6, we established the ergodic convergence rates of NPG in potential game and Markov potential game settings, which depend on \(1/_{K}\). In these results, \(^{k}\) generally encapsulatesthe difference between the gradients evaluated at the best and second best policies, and \(_{K}\) is a lower bound on \(^{k}\). We refer to \(^{}\) as the _suboptimality gap_ at iteration \(k\). In our analysis, the suboptimality gap provides a vehicle to establish the improvement of the potential function in two consecutive steps. In particular, it enables us to draw a connection between \(^{^{k+1}}-^{^{k}}\) and NE-gap(\(^{k}\)) using Lemma 3.2 and Lemma B.3 in the appendix. On the contrary, most of the existing work does not rely on this approach and generally studies the relationship between \(^{^{k+1}}-^{^{k}}\) and the _squared_ NE-gap(\(^{k}\)), which suffers a slower convergence rate of \((1/)\).

The analysis leveraging the suboptimality gap, though has not been adopted in the MARL studies, was considered in the single-agent scenario. Khodadadian et al.  proved asymptotic geometric convergence of single-agent RL with the introduction of _optimal advantage function gap_\(^{k}\), which shares similar definition as the gap \(^{k}\) studied in this work. Moreover, the notion of suboptimality gap is commonly used in the multi-armed bandit literature , so as to give the instance-dependent analysis.

While our convergence rate is sharper, a lower bound on the suboptimality gap is generally not guaranteed, and scenarios with zero optimality gap can be constructed in MPGs. In practice, we find that even if \(^{k}\)_approaches_ zero in certain iterations, it may not _converge_ to zero, and in these scenarios the system still converges without a slowdown. We provide a numerical example in Section 4.1 (Fig. 0(a)) to support our claim. Nevertheless, in what follows, we further identify a sufficient condition that allows us to alleviate this problem altogether by focusing on the asymptotic profile of the sub-optimality gap.

Theoretical RelaxationThe following proposition guarantees the asymptotic results of independent NPG.

**Proposition 3.1** ().: _Suppose that Assumption 3.1 holds and that the stationary policies are isolated. Then independent NPG with \(}{2_{max}}\) guarantees that \(_{k}^{k}=^{}\), where \(^{}\) is a Nash policy._

Since asymptotic convergence of policy is guaranteed by Proposition 3.1, the suboptimality gap \(^{k}\) is also guaranteed to converge to some \(^{*}\). We make the following assumption about the asymptotic suboptimality gap, which only depends on the property of the game itself.

**Assumption 3.2**.: _Assume that \(_{k}^{k}=^{*}>0\)._

Assumption 3.2 provides a relaxation in the sense that instead of requiring a lower bound \(^{k}\) for all \(k\), we only need \(^{*}>0\), a lower bound on the limit as the agents approach some Nash policies. This will allow us to disregard the transition behavior of the system and focus on the rate for large enough \(k\).

By the definition of \(^{*}\), we know that there exists finite \(K^{}\) such that \( k>K^{},|^{k}-^{*}|}{2},^{ k}}{2}\). Using these results, we can rework the proofs of Theorems 3.3 and 3.6 to get the following corollary.

**Corollary 3.6.1**.: _Consider a MPG that satisfies Assumption 3.2 with NPG update using algorithm 7. There exists \(K^{}\), such that for any \(K 1\), choosing \(=}{2_{max}}\), we have_

\[_{k=0}^{K-1}(^{k})}{K(1- )}(1+_{max}}{c^{*}(1-)}+ }{2M}),\]

_where \(M,c\) are defined as in Theorem 3.6._

## 4 Experimental Results

In previous sections, we established the theoretical convergence of NPG in MPGs. In order to verify the results, we construct two experimental settings for the NPG update and compare our empirical results with existing algorithms. We consider a synthetic potential game scenario with randomly generated rewards and a congestion problem studied in  and . We also provide the source code4 for all experiments.

### Synthetic Potential Games

We first construct a potential game with a fully cooperative objective, where the reward tensor \(r()\) is randomly generated from a uniform distribution. We set the agent number to \(n=3\), each with different action space \(|_{1}|=3,|_{2}|=4,|_{3}|=5\). At the start of the experiment, all agents are initialized with uniform policies. We note that the experimental setting in  falls under this broad setting, although the experiment therein was a two-player game with carefully designed rewards.

The results are shown in Figure 0(b). We compare the performance of independent NPG with other commonly studied policy updates, such as projected Q ascent , entropy regularized NPG  as well as log-barrier regularized NPG method . We set the learning rate of all algorithms as \(=0.1\). We also fine-tune regularization parameters to find the entropy regularization factor \(=0.05\) and the log-barrier regularization factor \(=0.005\). As discussed in Section 3.1 (the effect of \(c\)), we observe in Figure 0(b) that the regularized algorithms fail to reach an exact Nash policy despite exhibiting good convergence performance at the start. The empirical results here align with our theoretical findings in Section 3.

Impact of \(c\) and \(\)We demonstrate the impact of the initialization dependent factor \(c^{k}\) and suboptimality gap \(^{k}\) in MPGs with the same experimental setup. Figure 0(a) depicts the change in value for \(c^{k},^{k}\), and the NE-gap. We can see from the figure that as the algorithm updates over iterations, the value of \(c^{k}\) increases and approaches one, while \(^{k}\) approaches some non-zero constant. Figure 0(a) shows that although \(^{k}\) could be arbitrarily small in theory if we do not impose technical assumptions, it is not the case in general. Therefore, one should not automatically assume that the suboptimality gap diminishes with respect to iteration, as the limit entirely depends on the problem environment.

The effect of suboptimality gap \(^{*}\) in PG is illustrated in Section E of the appendix under a set of carefully constructed numerical examples. We verify that a larger gap indicates a faster convergence, which corroborates our theory.

### Congestion Game

We now consider a class of MDPs where each state defines a congestion game. We borrow the specific settings for this experiment from .

For the congestion game experiments, we consider the agent number \(n=8\) with the number of facilities \(|_{i}|=4\), where \(_{i}=\{A,B,C,D\}\) as the corresponding individual action spaces. There are two states defined as \(=\{,\}\). In each state, all agents prefer to be taking the same action with as many agents as possible. The reward for an agent selecting action \(k\) is defined by predefined weights \(w_{s}^{k}\) multiplied by the number of other agents taking the same action. Additionally, we set \(w_{s}^{A}<w_{s}^{B}<w_{s}^{C}<w_{s}^{D}\) and the reward in _distancing_ state is reduced by some constant compared to the _safe_ state. The state transition depends on the joint actions of all agents. If more than half of all agents take the same action, the system enters a _distancing_ state with lower rewards. If the agents are evenly distributed over all actions, the system enters _safe_ state with higher rewards.

We use episodic updates with \(T=20\) steps and collect \(20\) trajectories in each mini-batch and estimate the value function and Q-functions as well as the discounted visitation distribution. We use a discount factor of \(=0.99\). We adopt the same step size used in [19; 8] and determine optimal step-sizes of

Figure 1: (a) The suboptimality gap; (b) Learning curve in synthetic experiments; (c) Learning curve for congestion game.

softmax PG and NPG with grid-search. Since regularized methods in Section 4.1 generally do not converge to Nash policies, they are excluded in this experiment. To make the experiment results align with previous works, we provide the \(L_{1}\) distance between the current-iteration policies compared to Nash policies. We plot the mean and variance of \(L_{1}\) distance across multiple runs in Figure 0(c). Compared to the direct parameterized algorithms, the two softmax parameterized algorithms exhibit faster convergence, and softmax parameterized NPG has the best performance across all tested algorithms.

## 5 Conclusion and Discussion

In this paper, we studied Markov potential games in the context of multi-agent reinforcement learning. We focused on the independent natural policy gradient algorithm and studied its convergence rate to the Nash equilibrium. The main theorem of the paper shows that the convergence rate of NPG in MPGs is \((1/K)\), which improves upon the previous results. Additionally, we provided detailed discussions on the impact of some problem factors (e.g., \(c\) and \(\)) and compared our rate with the best known results with respect to these factors. Two empirical results were presented as a verification of our analysis.

Despite our newly proposed results, there are still many open problems that need to be addressed. One of the limitations of this work is the assumption of Markov potential games, the relaxation of which could extend our analysis to more general stochastic games. As a matter of fact, the gradient-based algorithm studied in this work will fail for a zero-sum game as simple as Tic-Tac-Toe. A similar analysis could also be applied to regularized games and potentially sharper bounds could be obtained. The agents are also assumed to receive gradient information from an oracle in this paper. When such oracle is unavailable, the gradient can be estimated via trajectory samples, which we leave as a future work. Other future directions are the convergence analysis of policy gradient-based algorithms in safe MARL and robust MARL, following the recent exploration of safe single-agent RL [9; 22; 41] and robust single-agent RL [21; 40].