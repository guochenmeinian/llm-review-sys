# Similarity-Navigated Conformal Prediction for Graph Neural Networks

Jianqing Song1, Jianguo Huang2,3, Wenyu Jiang2,1, Baoming Zhang1,

**Shuangjie Li1, Chongjun Wang1\({}^{*}\) \({}^{1}\)**State Key Laboratory of Novel Software Technology, Nanjing University

\({}^{2}\)Department of Statistics and Data Science, Southern University of Science and Technology

\({}^{3}\)College of Computing and Data Science, Nanyang Technological University

Corresponding author (chjwang@nju.edu.cn)

###### Abstract

Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks. However, these results lack reliable uncertainty estimates. Conformal prediction methods provide a theoretical guarantee for node classification tasks, ensuring that the conformal prediction set contains the ground-truth label with a desired probability (e.g., 95%). In this paper, we empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets while maintaining valid marginal coverage. This observation motivates us to propose a novel algorithm named _Similarity-Navigated Adaptive Prediction Sets_ (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood. The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label. By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one). Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS. Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage.

## 1 Introduction

Graph Neural Networks (GNNs), which process graph-structured data by the message-passing manner (Kipf and Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018; Xu et al., 2019), have achieved remarkable accuracy in various high-stakes applications, e.g., drug discovery (Li et al., 2022), fraud detection (Liu et al., 2023) and traffic forecasting (Jiang and Luo, 2022), where any erroneous prediction can be costly and dangerous (Amodei et al., 2016; Gao et al., 2019). To improve the reliability of prediction results, many methods have been investigated to quantify the model uncertainty (Gal and Ghahramani, 2016; Guo et al., 2017; Kendall and Gal, 2017; Wang et al., 2021; Hsu et al., 2022; Tang et al., 2024), while these methods lack theoretical guarantees of quantification. Conformal prediction (CP), on the other hand, offers a systematic approach to construct prediction sets that contain ground-truth labels with a desired coverage guarantee (Vovk et al., 2005; Romano et al., 2020; Angelopoulos et al., 2021; Huang et al., 2023; Xi et al., 2024).

CP algorithms utilize non-conformity scores to measure dissimilarity between a new instance and the training instances. The lower the score of a new instance, the more likely it belongs to the same distribution space as the training instances, thereby included in the prediction set. To improve the efficiency of prediction sets for GNNs, DAPS (Zargarbashi et al., 2023) smooths node-wise non-conformity scores by incorporating neighborhood information based on the assumption of networkhomophily. Similar to DAPS, CF-GNN (Huang et al., 2023b) introduces a topology-aware output correction model that learns to update prediction and then produces more efficient prediction sets or intervals with the inefficiency as the optimization objective. However, they only consider structural neighbors and ignore the effect of other nodes that are far from the ego node. This motivates us to analyze the influence of global nodes on the size of prediction sets.

In this work, we show that aggregating the information of global nodes with the same label as the ego node benefits the performance of CP methods. We provide an empirical analysis by randomly selecting nodes with the same label as the ego node from an oracle perspective, where the ground-truth labels of all nodes are known, and then aggregating their non-conformity scores into the ego node. The results indicate that aggregating scores of these nodes can significantly reduce the average size of prediction sets. This suggests that the information of nodes with the same label could correct the non-conformity scores, thereby prompting the efficiency of prediction sets. Detailed analysis is presented in Subsection 3.1. However, during the testing phase, the ground-truth label of every test node is unknown. Inspired by the analysis, our key idea is to accurately identify and select as many nodes with the same label as the ego node as possible and aggregate their non-conformity scores.

To this end, we propose a novel algorithm named **S**imilarity-**N**avigated **A**daptive **P**rediction **S**ets (SNAPS), which could self-adaptively aggregate the non-conformity scores of other nodes into the ego node. Specifically, SNAPS gives the higher cumulative weight for nodes with a higher probability of having the same label as the ego node while preserving its own and the one-hop neighbors. We utilize the feature similarity between nodes and the adjacency matrix to calculate the aggregating weights. In this way, the corrected scores could achieve compact prediction sets while maintaining the desired coverage.

To verify the effectiveness of our method, we conduct thorough empirical evaluations on 10 datasets, including both small datasets and large-scale datasets, e.g., OGBN Products (Bhatia et al., 2016). The results demonstrate that SNAPS not only achieves the pre-defined empirical marginal coverage but also achieves better performance over the compared methods. For example, on OGBN Products, our method reduces the average size of prediction sets from 14.92 of APS to 7.68. Moreover, we adapt SNAPS to image classification problems. The results demonstrate that SNAPS reduces the average size of prediction sets from 19.639 to 4.079 - only \(\) of the prediction set size from APS on ImageNet (Deng et al., 2009). Code is available at [https://github.com/jandsong/SNAPS](https://github.com/jandsong/SNAPS).

We summarize our contributions as follows:

* We empirically explain that non-conformity scores of nodes with the same label as the ego node play a critical role in their non-conformity scores.
* We propose a novel algorithm, namely SNAPS that aggregates basic non-conformity scores of nodes obtained through node feature similarity and one-hop structural neighborhood. We provide theoretical analysis to show the marginal coverage properties of SNAPS and the validity of SNAPS.
* Extensive experimental results demonstrate the effectiveness of our proposed method. We show that SNAPS not only maintains the pre-defined coverage but also achieves great performance in efficiency and singleton hit ratio.

## 2 Preliminary

In this paper, we focus on split conformal prediction for semi-supervised node classification with transductive learning in an undirected graph.

**Notation.** Graph is represented as \(=(,)\), where \(:=\{v_{i}\}_{i=1}^{N}\) denotes the node set and \(\) denotes the edge set with \(||=E\). Let \(\{0,1\}^{N N}\) be the adjacency matrix, where \(_{i,j}=1\) if there exists an edge between nodes \(v_{i}\) and \(v_{j}\), and \(_{i,j}=0\) otherwise, and \(\) be its degree matrix, where \(_{i,i}=_{j}_{i,j}\). Let \(:=[_{1},,_{N}]^{T}\) be the node feature matrix, where \(_{i}^{d}\) is a \(d\)-dimensional feature vector for node \(v_{i}\). The label of node \(v_{i}\) is \(y_{i}\), where \(:=\{1,2,...,K\}\) denotes the label space.

**Transductive setting.** In transductive setting, we have access to two node sets, \(_{}\) with labels and \(_{}\) without labels, where \(_{}_{}=\) and \(_{}_{}=\). \(_{}\) is then randomly split into \(_{}/_{}/_{}\) with a fixed size, the training/validation/calibration node set, correspondingly.

\(_{}\) is used as the testing node set \(_{}\). The classifier \(f()\) is trained on \(\{(_{i},y_{i})\}_{v_{i}_{}}\), \(\{_{i}\}_{v_{i}-_{}}\) and the entire graph structure \(=(,)\), and is chosen through \(\{(_{i},y_{i})\}_{v_{i}_{}}\). Then we can get the predicted probability \(=\{_{i}\}_{v_{i}}\) for each node through \(_{i}=(f(_{i}))\) where \(_{i}^{K}\) and \(\) is activation function such as softmax. We usually choose the label with the highest probability as the predicted label, i.e., \(_{i}=*{argmax}_{k}_{ik}\).

**Graph neural networks.** GNNs aim at learning representation vectors for nodes in the graph by leveraging graph structure and node features. Most modern GNNs adopt a series of propagation layers following a message passing mechanism (Gilmer et al., 2017). The \(l\)-th layer of the GNNs takes the following form:

\[_{i}^{(l)}=^{(l)}(_{i}^{(l-1)}, ^{(l)}(\{^{(l)}(_{j}^{(l- 1)},_{i}^{(l-1)})|v_{j}_{i}\})) \]

where \(_{i}^{(l)}\) is the hidden representation of node \(v_{i}\) at the \(l\)-th layer with initialization of \(_{i}^{(0)}=_{i}\), and \(_{i}\) is a set of nodes adjacent to node \(v_{i}\). \(^{(l)}()\), \(^{(l)}()\) and \(^{(l)}()\) denote the functions for message computation, message aggregation, and message combination, respectively. After an iteration of the last layer, the obtained final node representation \(=\{_{i}^{L}\}_{v_{i}}\) is then fed to a classifier to obtain the predicted probability \(\).

**Conformal prediction.** CP is a promising framework for generating prediction sets that statistically contain ground-truth labels with a desired guarantee. Formally, given calibration data \(_{}=\{(_{i},y_{i})\}_{i=1}^{n}\), we can generate a prediction set \((_{n+1})\) for an unseen instance \(_{n+1}\) with the coverage guarantee \([y_{n+1}(_{n+1})] 1-\), where \(\) is the pre-defined significance level. The best characteristic of CP is that it is distribution-free and only relies on exchangeability. This means that every permutation of the instances is equally likely, i.e., \(_{}(_{n+1},y_{n+1})\) is exchangeable, where \((_{n+1},y_{n+1})\) is an unseen instance.

Conformal prediction is typically divided into two types: full conformal prediction and split conformal prediction. Unlike full conformal prediction, split conformal prediction treats the model as a black box, avoiding the need to retrain or modify the model and sacrificing efficiency for computational efficiency (Vovk et al., 2005; Zargarbashi et al., 2023). In this paper, we focus on the computationally efficient split conformal prediction method, thus "conformal prediction" in the following denotes split conformal prediction.

**Theorem 1** (_Vovk et al._,_ 2005_) _Let calibration data and a test instance, i.e., \(\{(_{i},y_{i})\}_{i=1}^{n}\{(_{n+1},y_{n+1})\}\) be exchangeable. For any non-conformity score function \(s:\) and any significance level \((0,1)\), define the \(1-\) quantile of scores as \(:=(;\{s(_{ i},y_{i})\}_{i=1}^{n})\) and prediction sets as \(_{}(_{n+1})=\{y|s(_{n+1},y)\}\). We have_

\[1-[y_{n+1}_{}(_{n+1})]<1- +. \]

Theorem 1 statistically provides a marginal coverage guarantee for all test instances. Currently, there are already many basic non-conformity score methods (Romano et al., 2020; Angelopoulos et al., 2021; Huang et al., 2023a). Here we provide the definition of Adaptive Prediction Sets (Romano et al., 2020) (APS).

**Adaptive Prediction Sets.** In the APS method, the non-conformity scores are calculated by accumulating the softmax probabilities in descending order. Formally, given a data pair \((,y)\) and a predicted probability estimator \(()_{y}\) for \((,y)\), where \(()_{y}\) is the predicted probability for class \(y\), the non-conformity scores can be computed by:

\[s(,y)=_{i=1}^{||}()_{i}[()_{i }>()_{y}]+()_{y}, \]

where \(\) is a uniformly distributed random variable. Then, the prediction set is constructed as \(()=\{y|s(,y)\}\).

**Evaluation Metrics.** The goal is to improve the efficiency of conformal prediction sets as much as possible while maintaining the empirical marginal coverage guarantee. Given the testing nodes set \(_{}\), the efficiency is defined as the average size of prediction sets: \(:=_{}|}_{v_{i} _{}}|(_{i})|\)The smaller the size, the more efficient CP is. The empirical marginal coverage is defined as \(:=_{}|}_{v_{i}_{ }}[y_{i}(_{i})]\). Although efficiency is a common metric for evaluating CP, singleton hit ratio (SH), defined as the proportion of prediction sets of size one that contains the ground-truth label, is also important (Zargarbashi et al., 2023). The formula of SH is defined as: \(:=_{}|}_{v_{i}_{ }}[(_{i})=\{y_{i}\}]\).

## 3 Motivation and Methodology

In this section, we begin by outlining our motivation, substantiating its validity and feasibility through experimental evidence. Then, we propose our method, SNAPS. Finally, we demonstrate that SNAPS satisfies the exchangeability assumption required by CP and offer proof of its improved efficiency compared to basic non-conformity score methods.

### Motivation

In this subsection, we empirically show that nodes with the same label as the ego node may play a critical role in the non-conformity scores of the ego node. Specifically, using the scores of nodes with the same label to correct the scores of the ego node could reduce the average size of prediction sets.

To analyze the role of nodes with the same label as the ego node, assuming we have access to an oracle graph, i.e., the ground-truth labels of all the nodes are known. Then, we randomly select nodes with the same label as the ego node and aggregate their APS non-conformity scores into the ego node. We conduct experiments by Graph Convolutional Network (GCN) (Kipf and Welling, 2017) on CoraML (McCallum et al., 2000) dataset and choose APS as the basic score function of CP. Then, we conduct 10 trials and randomly select 100 calibration sets for each trial to evaluate the performance of CP at a significance level \(=0.05\).

In Figure 1(a), we can find that the average size of prediction sets drops sharply as the number of nodes being aggregated increases, while maintaining valid coverage. When the number of selected nodes is 0, the results shows the performance of APS. Therefore, if the non-conformity scores of the ego node are corrected by accurately selecting nodes with the same label, Size can be reduced to a large extent. Moreover, aggregating the scores of these nodes still achieves the coverage guarantee.

### Similarity-Navigated Adaptive Prediction Sets

In our previous analysis, we show that correcting the scores of the ego node with the scores of nodes having the same label leads to smaller prediction sets and valid coverage. However, the above experiment is based on the oracle graph. In the real-world application, the ground-truth label of each test node is unknown. To alleviate this issue, our key idea is to use the similarity to approximate the

Figure 1: The motivation for SNAPS. (a) The trend of Coverage and Size as the number of nodes with the same label as the ego node increases. (b) The average of node feature cosine similarity between same or different labels. (c) The number statistics of nodes with the same label and with different labels as the ego node with increasing \(k\) that denotes \(k\)-NN with feature similarity.

potential label. Specifically, the nodes with high feature similarity tend to have a high probability of belonging to the same label.

Several studies (Jin et al., 2021; Zou et al., 2023) have demonstrated that matrix constructed from node feature similarity can help with the homophily assumption, i.e., connected nodes in the graph are likely to share the same label. Additionally, the network homophily can also help us to find more nodes whose labels are likely to be the same as the ego node (Kipf and Welling, 2017), and several studies have demonstrated the effectiveness of this (Clarkson, 2023; Zargarbashi et al., 2023; Huang et al., 2023). Therefore, we consider feature similarity and network structure to select nodes that may have the same label as the ego node.

**Feature similarity graph construction.** We compute the cosine similarity between the node features in the graph. For a given node pair \((v_{i},v_{j})\), the cosine similarity between their features can be calculated by:

\[(i,j)=_{i}^{}_{j}}{\|_{i}\|_{2}\| _{j}\|_{2}}, \]

where \(i j\), \(v_{i}\) and \(v_{j}_{t,i}\). Here, \(v_{j}_{t,i}\) represents a set of nodes for which we calculate the similarity with \(v_{i}\). Then, we choose \(k\) nearest neighbors for each node based on the above cosine similarity, forming the \(k\)-NN graph. We denote the adjacency matrix of \(k\)-NN graph as \(_{s}\) and its degree matrix as \(_{s}\), where \(_{s}(i,j)=(i,j)\) and \(_{s}(i,i)=_{j}_{s}(i,j)\). For large graphs, we randomly select \(M k\) nodes to put into \(_{t,i}\), whereas for small graphs, we include all nodes into \(v_{j}_{t,i}\).

To verify the effectiveness of feature similarity, we provide an empirical analysis. Figure 1(b) presents the average of node feature cosine similarity between the same or different labels on the CoraML dataset. We can find that the average of node feature similarity between the same label is higher than those between different labels. We analyze experimentally where using feature similarity to select \(k\)-NN meets our expectation of selecting nodes with the same label as the ego node. Figure 1(c) shows the number statistics of nodes with the same label and with different labels at \(k\)-th nearest neighbors. The result shows that we can indeed select many nodes with the same label when \(k\) is not very large.

**SNAPS.** We propose SNAPS that aggregates non-conformity scores of nodes with high feature similarity to ego node and one-hop graph structural neighbors. Formally, for a node \(v_{i}\) with a label \(y\), the score function of SNAPS is shown as :

\[(_{i},y)=(1--)s(_{i},y)+_{s }(i,i)}_{j=1}^{M}_{s}(i,j)s(_{j},y)+_{ i}|}_{v_{j}_{i}}s(_{j},y), \]

Figure 2: The overall framework of SNAPS. (1) Basic non-conformity score function. We first use basic non-conformity score functions, e.g., APS, to convert node embeddings into non-conformity scores. (2) SNAPS function. We then aggregate basic non-conformity scores of \(k\)-NN with feature similarity and one-hop structural neighbors to correct the non-conformity scores of nodes. (3) Conformal Prediction. Finally, we use conformal prediction to generate prediction sets, significantly reducing their size compared to the basic score functions.

where \(s(,)\) is the basic non-conformity score function and \((,)\) is the SNAPS score function. Both \(\) and \(\) are hyperparameters, which are used to measure the importance of three parts of non-conformity scores. The framework of SNAPS is shown in Figure 2 and the pseudo-code is in Appendix B.

### Theoretical Analysis

To deploy CP for graph-structured data, the only assumption we should satisfy is exchangeability, i.e., the joint distribution of calibration and testing sets remains unchanged under any permutation. Several studies have demonstrated that non-conformity scores based on the node embeddings obtained by any GNN models are invariant to the permutation of nodes while permuting their edges correspondingly in the calibration and testing sets. This invariance arises because GNNs models and non-conformity score functions only use the structures and attributes in the graph, without dependence on the order of the nodes (Zargarbashi et al., 2023; Huang et al., 2023b). Under this condition, we prove that SNAPS non-conformity scores are still exchangeable.

**Proposition 1**: _Let \(=\{_{i}\}_{v_{i}}\) be basic non-conformity scores of nodes, where \(_{i}^{K}\). Assume that \(\) is exchangeable for all \(v_{i}(_{}_{})\). Then the aggregated \(}=(1--)+_{s}}+}\), where \(_{s}}=_{s}^{-1}_{s}\) and \(}=^{-1}\), is also exchangeable for \(v_{i}(_{}_{})\)._

The corresponding proof is provided in Appendix A. We then demonstrate the validity of our method theoretically.

**Proposition 2**: _Assume that all of the nodes aggregated by SNAPS are the same label as the ego node. Given a data pair \((,y)\) and a predicted estimator \(()_{y}\) for \((,y)\), where \(()_{y}\) is the predicted probability for class \(y\). Moreover, \(_{ki}\) reflects the model's error in misclassifying the ground-truth label \(k\) as label \(i\). Let \(\) be APS scores of nodes, where \(_{ui}\) is the score corresponding to node \(u\) with label \(i\). Let \(E_{k}[(_{u})]\) and \(E_{k}[_{ui}]\) be the average of predicted probability and scores corresponding to label \(i\) of nodes whose ground-truth labels are \(k\), respectively. Let \(p\) be \(1-\) quantile of basic non-conformity scores with a significance level \(\). If \(E_{k}[_{uk}]<\) and \(E_{k}[_{ui}](1-_{ki})E_{k}[(_{u})_{max}]+E_{k}[ (_{u})_{i}]\), where \(E_{k}[(_{u})_{max}]\) denotes the maximum predicted probability of nodes whose ground-truth labels are \(k\) and \(\) is a uniformly distributed random variable, then_

\[[|}()|][|()|],\]

_where \(()\) and \(}()\) represent the prediction set from the APS score function and SNAPS score function, respectively._

In other words, SNAPS consistently generates a smaller prediction set than basic non-conformity scores functions and maintains the desired marginal coverage rate. It is obvious that we can't ignore a very important thing, which is to select nodes with the same label as the ego node as correctly as possible, otherwise, it will lead to a decrease in the efficiency of SNAPS.

## 4 Experiments

In this section, we conduct extensive experiments on semi-supervised node classification to demonstrate the effectiveness of SNAPS on graph-structure data. We also adapt SNAPS for image classification problems. Furthermore, we perform ablation studies and parameter analysis to show the importance of different components in SNAPS and evaluate its robustness, respectively.

### Experimental Settings

**Datasets.** In our experiments, we consider ten datasets with high homophily, where connected nodes in the graph are likely to share the same label. These datasets include the common citation graphs: CoraML (McCallum et al., 2000), PubMed (Namata et al., 2012), CiteSeer (Sen et al., 2008), CoraFull (Bojchevski and Gunnemann, 2018), Coauthor Physics (Physics) and Coauthor CS (CS) (Shchur et al., 2018) and the co-purchase graphs: Amazon Photos (Photos) and Amazon Computers (Computers) (McAuley et al., 2015; Shchur et al., 2018). Moreover, we consider two large-scale graph datasets, i.e., OGBN Arxiv (Arxiv) (Wang et al., 2020) and OGBN Products (Products) (Bhatiaet al., 2016). Particularly, for CoraFull which is highly class-imbalanced, we filter out the classes with fewer than 50 nodes. The transformed dataset is dubbed as CoraFull\({}^{*}\)(Zargarbashi et al., 2023). Detailed statistics of these datasets are shown in Appendix F. In addition to the datasets mentioned above, we discuss two heterophilous graph datasets in Appendix C.1, namely Chameleon and Squirrel, both of which are two Wikipedia networks (Rozemberczki et al., 2021).

**Baselines.** Since our SNAPS is a general post-processing method for GNNs, here we choose GCN (Kipf and Welling, 2017), GAT (Velickovic et al., 2018) and APPNP (Gasteiger et al., 2018) as structure-aware models and MLP as a structure-independent model. Moreover, our SNAPS can be based on general conformal prediction non-conformity scores, here we choose APS (Romano et al., 2020) and RAPS (Angelopoulos et al., 2021). For comparison, we compare not only with the basic scores, i.e., APS and RAPS, but also with DAPS (Zargarbashi et al., 2023) for GNNs.

**CP Settings.** For the basic model GCN, GAT, APPNP and MLP, we follow parameters suggested by (Zargarbashi et al., 2023). For DAPS, we follow the official implementation. Since GNNs are sensitive to splits, especially in the sparsely labeled setting (Shchur et al., 2018), we train the model over ten trials using varying train/validation splits. For per class in the training/validation set, we randomly select 20 nodes. For Arxiv and Products dataset, we follow the official split in PyTorch Geometric (Fey and Lenssen, 2019). Then, the remaining nodes are included in the calibration set and the test set. The calibration set ratio is suggested by (Huang et al., 2023b), i.e., modifying the calibration set size to \(|_{}|=\{1000,|_{} _{}|/2\}\). For each trained model, we conduct 100 random splits of calibration/test set. Thus, we totally conduct 1000 trials to evaluate the effectiveness of CP. For the non-conformity score function that requires hyper-parameters, we split the calibration set into two sets, one for tuning parameters, and the other for conformal calibration (Zargarbashi et al., 2023). For SNAPS, we choose \(\) and \(\) in increments of 0.05 within the range 0 to 1, and ensure that \(+<=1\). Each experiment is done with a single NVIDIA V100 32GB GPU.

### Experimental results

**SNAPS generates smaller prediction sets and achieves a higher singleton hit ratio.** Table 1 shows that Coverage of all conformal prediction methods is close to the desired coverage \(1-\). At a significance level \(=0.05\), Size and SH exhibit superior performance. For example, when evaluated on Products, SNAPS reduces Size from 14.92 of APS to 7.68. Overall, the experiments show that SNAPS has the desired coverage rate and gets smaller Size and higher SH than APS, RAPS, and DAPS. Detailed results for other basic models and SNAPS based on RAPS are available in Appendix D.

**SNAPS generates smaller average prediction sets for each label.** We conduct additional experiments to analyze the average performance of APS and SNAPS on nodes belonging to the same label at a significance level \(=0.05\). Figure 3(a) shows that the distribution of the average non-conformity scores for nodes belonging to the same label aligns with the assumptions made in Proposition 2, i.e., \(E_{k}[_{uk}]<\) and \(E_{k}[_{ui}]--\), where \(=-(1-_{ki})E_{k}[(_{u})_{max}]-E_{k}[( _{u})_{i}]\). If \(>0\), then it is very small. Size of prediction sets corresponding to APS is 3.29. Figure 3(b) shows

    &  &  &  \\   & APS & RAPS & DAPS & SNAPS & APS & RAPS & DAPS & SNAPS & APS & RAPS & DAPS & SNAPS \\  CoraML & 0.950 & 0.950 & 0.950 & 0.950 & 2.42 & 2.21 & 1.92 & **1.68** & 44.89 & 22.19 & 52.16 & **56.30** \\ PubMed & 0.950 & 0.950 & 0.950 & 0.950 & 1.79 & 1.77 & 1.76 & **1.62** & 33.67 & 30.83 & 35.25 & **42.95** \\ CiteSer & 0.950 & 0.950 & 0.950 & 0.950 & 2.34 & 2.36 & 1.94 & **1.84** & 50.41 & 38.99 & **59.75** & 59.08 \\ CoraFull & 0.950 & 0.950 & 0.950 & 0.950 & 17.54 & 10.72 & 11.81 & **9.80** & 10.23 & 2.13 & **8.67** & 5.76 \\ CS & 0.950 & 0.950 & 0.950 & 0.950 & 1.91 & 1.20 & 1.22 & **1.08** & 66.17 & 78.34 & 79.80 & **87.92** \\ Physics & 0.950 & 0.950 & 0.950 & 0.950 & 1.28 & 1.07 & 1.08 & **1.04** & 76.74 & 88.89 & 88.40 & **91.21** \\ Computers & 0.950 & 0.950 & 0.950 & 0.950 & 3.95 & 2.89 & 2.13 & **1.98** & 27.67 & 15.85 & 43.03 & **45.48** \\ Photo & 0.951 & 0.950 & 0.950 & 0.951 & 1.89 & 1.64 & 1.41 & **1.31** & 54.31 & 56.63 & 74.57 & **78.51** \\ Arxiv & 0.950 & 0.950 & 0.949 & 0.950 & 4.30 & 3.62 & 3.73 & **3.62** & 22.55 & 14.52 & 19.19 & **23.53** \\ Products & 0.950 & 0.951 & 0.950 & 0.950 & 14.29 & 13.67 & 10.91 & **7.68** & 15.51 & 11.51 & 19.29 & **22.38** \\  Average & 0.950 & 0.950 & 0.950 & 0.950 & 5.23 & 4.12 & 3.79 & **3.17** & 40.22 & 36.00 & 48.01 & **52.31** \\   

Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \(=0.05\). **Bold** numbers indicate optimal performance.

that only a few other labels different from real labels have average scores lower than the quantile of scores. Size of prediction sets corresponding to SNAPS is 1.29. Overall, for basic non-conformity scores that match this distribution of our assumptions, SNAPS can achieve superior performance based on these scores. The results of CiteSeer and Amazon Computers datasets are available in Appendix D.

**Ablation study.** To understand the effects of three parts of our method, i.e., original scores (Orig.), neighborhood scores (Neigh.), and feature similarity node scores (Feat.), we conduct a thorough ablation experiment using GCN at \(=0.05\). In Table 2, SNAPS performs best on most datasets when all three parts are included. Moreover, for the remaining dataset on which SNAPS exhibits comparable performance, all those better cases contain the Feat. part. Overall, each part plays a critical role in CP for GNNs, and removing any will in general decrease performance.

**Parameter analysis.** We conduct additional experiments to analyze the robustness of SNAPS. We choose GCN as the GNNs model and APS as the basic non-conformity score function.

Figure 4(a) and Figure 4(b) demonstrate that the performance of SNAPS significantly improves as \(k\) gradually increases from 0. This improvement occurs because the increasing nodes with the same label are selected to enhance the ego node. Subsequently, as \(k\) continues to increase, the performance of SNAPS tends to stabilize. On the other hand, we find that when \(k\) is extremely large, it appears that nodes with the same label cannot be selected with high accuracy only by feature similarity. Thus, when \(k\) is extremely large, performance will decline slightly. Figure 4(c) and Figure 4(d) show that as the values of parameter \(\) and \(\) change, the most areas in the heatmaps of \(\) and \(\) display similar colors. Overall, SNAPS is robust to the parameter \(k\) and is not sensitive to parameters \(\) and \(\). To further explore the sensitivity of \(\) and \(\) to the performance of SNAPS, we set \(==1/3\), which indicating that three components of SNAPS are equally weighted. The experimental results in Table 3 demonstrate that SNAPS performs well with these default hyperparameters on most datasets.

  
**Orig.** & **Neigh.** & **Feat.** & **CoraML** & **PubMed** & **CiteSeer** & **CoraFull\({}^{*}\)** & **CS** & **Physics** & **Computers** & **Photo** & **arxiv** & **products** \\  \(\) & \(\) & \(\) & 2.42 & 1.79 & 2.34 & 17.54 & 1.91 & 1.28 & 3.95 & 1.89 & 4.30 & 14.92 \\ \(\) & \(\) & \(\) & 2.18 & 1.94 & 2.07 & 17.50 & 1.37 & 1.09 & 2.15 & 1.42 & 4.75 & 11.25 \\ \(\) & \(\) & \(\) & 2.40 & 1.65 & 2.52 & 18.07 & 1.11 & **1.03** & 3.26 & 2.60 & 9.45 & 13.89 \\ \(\) & \(\) & \(\) & 1.87 & 1.72 & 1.91 & 12.10 & 1.22 & 1.07 & 2.22 & 1.37 & 3.76 & 10.81 \\ \(\) & \(\) & \(\) & 1.78 & 1.63 & 1.94 & 11.54 & 1.13 & 1.05 & 2.37 & 1.46 & 3.82 & 8.46 \\ \(\) & \(\) & \(\) & 1.72 & 1.63 & 1.86 & 10.51 & 1.09 & 1.04 & **1.94** & 1.31 & 4.44 & **7.65** \\ \(\) & \(\) & \(\) & **1.68** & **1.62** & **1.84** & **9.80** & **1.08** & 1.04 & 1.98 & **1.31** & **3.62** & 7.68 \\   

Table 2: Ablation study in terms of \(\). Overall, three parts of our method are critical, and removing any of them results in a general decrease in performance.

Figure 3: The average non-conformity scores of nodes belonging to each label based on the model GCN for dataset CoraML.

**Adaption to image classification problems.** In the node classification problems, SNAPS achieves better performance than standard APS, which was proposed for image classification problems. Therefore, we employ SNAPS for image classification problems. Since there are no links between different images, we utilize the cosine similarities of image features to correct the APS. Formally, the corrected APS, i.e., SNAPS, is defined as :

\[(,y)=(1-)s(,y)+_{}|}_{ }_{}}s(},y),\]

where \(s(,y)\) is the score of standard APS, \(_{}\) is the \(k\) nearest neighbors based on image features in the calibration set and \(\) is a corrected weight. We conduct experiments on ImageNet, whose test dataset is equally divided into the calibration set and the test set. For SNAPS, we set \(k=5\) and \(=0.5\). We report the results of Coverage, Size and _size-stratified coverage violation_ (SSCV) (Angelopoulos et al., 2021). The details of experiments and SSCV are provided in Appendix E.

As indicated in Table 4, SNAPS achieves smaller prediction sets than APS. For example, on the ResNeXt101 model and \(\) = 0.1, SNAPS reduces Size from 19.639 to 4.079 - only \(\) of the prediction set size from APS and achieves the smaller SSCV than APS. Overall, SNAPS could improve the efficiency of prediction sets while maintaining the performance of conditional coverage.

    &  &  \\   &  &  &  \\  Model & Top1 & Top5 & Coverage & Size \(\) & SSCV \(\) & Coverage & Size\(\) & SSCV \(\) \\  ResNeXt101 & 79.32 & 94.58 & 0.899/0.900 & 19.64/**4.08** & 0.088/**0.059** & 0.950/0.950 & 45.80/**14.41** & 0.047/**0.033** \\ ResNet101 & 77.36 & 93.53 & 0.900/0.900 & 10.82/**3.62** & **0.075/**0.078** & 0.950/0.950 & 22.90/**9.83** & 0.039/**0.029** \\ DenseNet161 & 77.19 & 93.56 & 0.900/0.900 & 12.04/**3.80** & 0.077/**0.067** & 0.951/0.950 & 27.99/**10.66** & 0.039/**0.026** \\ ViT & 81.02 & 95.33 & 0.899/0.899 & 11.05/**2.33** & **0.087/**0.133** & 0.949/0.950 & 31.12/**10.47** & 0.042/**0.040** \\ CLIP & 60.53 & 86.15 & 0.899/0.899 & 17.46/**10.32** & 0.047/**0.032** & 0.950/0.949 & 34.93/**24.53** & 0.027/**0.017** \\  Average & - & - & 0.899/0.900 & 14.09/**4.83** & 0.075/**0.074** & 0.950/0.950 & 32.55/**13.98** & 0.039/**0.029** \\   

Table 4: Results on Imagenet. The median-of-means is reported over 10 different trials. **Bold** numbers indicate optimal performance.

Figure 4: Parameter analysis. The results for Size and SH on SNAPS (based on APS) for CoraML dataset with \(=0.05\).

    &  &  &  \\   & APS & RAPS & DAPS & SNAPS & APS & APS & RAPS & DAPS & SNAPS & APS & RAPS & DAPS & SNAPS \\  CoraML & 0.950 & 0.958 & 0.957 & 0.951 & 2.50 & 2.62 & 2.32 & **1.74** & 43.09 & 27.34 & 44.52 & **54.11** \\ PubMed & 0.950 & 0.968 & 0.967 & 0.950 & 1.82 & 2.10 & 2.09 & **1.61** & 33.39 & 14.66 & 23.27 & **44.11** \\ CiteSeer & 0.951 & 0.950 & 0.952 & 0.950 & 2.41 & 2.69 & 2.16 & **1.90** & 48.53 & 35.37 & 55.40 & **58.22** \\ CS & 0.950 & 0.953 & 0.954 & 0.950 & 2.04 & 1.31 & 1.33 & **1.13** & 64.32 & 66.91 & 74.91 & **85.21** \\ Physics & 0.951 & 0.962 & 0.962 & 0.950 & 1.39 & 1.44 & 1.28 & **1.07** & 72.44 & 62.22 & 77.65 & **88.58** \\ Computers & 0.950 & 0.950 & 0.951 & 0.950 & 3.01 & 3.04 & 2.30 & **2.01** & 29.21 & 9.87 & 42.19 & **45.98** \\ Photo & 0.949 & 0.950 & 0.950 & 0.950 & 1.90 & 1.81 & 1.56 & **1.30** & 54.86 & 47.27 & 67.57 & **79.50** \\   

Table 3: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score and set \(==1/3\). We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \(=0.05\). **Bold** numbers indicate optimal performance.

Related Work

**Uncertainty Quantification for GNNs.** Many uncertainty quantification (UQ) methods have been proposed to quantify the model uncertainty for classification tasks in machine learning (Gal and Ghahramani, 2016; Guo et al., 2017; Zhang et al., 2020; Gupta et al., 2021). Recently, several calibration methods for GNNs have been developed, such as CaGCN (Wang et al., 2021), GATS (Hsu et al., 2022) and SimCalib (Tang et al., 2024). However, these UQ methods lack statistically rigorous and empirically valid coverage guarantee (Huang et al., 2023). In contrast, SNAPS provides valid coverage guarantees both theoretically and empirically.

**Conformal Prediction for GNNs.** Many conformal prediction (CP) methods have been developed to provide valid uncertainty estimates for model predictions in machine learning classification tasks (Romano et al., 2020; Angelopoulos et al., 2021; Liu et al., 2024; Wei and Huang, 2024). Although several CP methods for GNNs have been studied, the use of CP in graph-structured data is still largely underexplored. ICP (Wijegunawardana et al., 2020) is the first to apply CP framework on graphs, designs a margin conformity score for labels of nodes without considering the relation between nodes. NAPS (Clarkson, 2023) use the non-exchangeable technique from (Barber et al., 2023) for inductive node classification, not applicable for the transductive setting, while we focus on the transductive setting where exchangeability property holds. Our method is essentially an enhanced version of the DAPS (Zargarbashi et al., 2023) method, which proposes a diffusion-based method that incorporates neighborhood information by leveraging the network homophily. Similar to DAPS, CF-GNN (Huang et al., 2023) introduces a topology-aware output correction model, akin to GCN, which employs a conformal-aware inefficiency loss to refine predictions and improve the efficiency of post-hoc CP. Other recent efforts in CP for graphs include (Lunde, 2023; Marandon, 2023; Zargarbashi and Bojchevski, 2023; Sanchez-Martin et al., 2024) which focus on distinct problem settings. In this work, SNAPS takes into account both network topology and feature similarity. This method can be applied not only to graph-structured data but also to other types of data, such as image data.

## 6 Conclusion

In this paper, we propose SNAPS, a general algorithm that aggregates the non-conformity scores of nodes with the same label as the ego node. Specifically, we select these nodes based on feature similarity and structural neighborhood, and then aggregate their non-conformity scores to the ego node. As a result, our method could correct the scores of some nodes. Moreover, we present theoretical analyses to certify the effectiveness of this method. Extensive experiments demonstrate that SNAPS not only maintains the pre-defined coverage, but also achieves significant performance in efficiency and singleton hit ratio. Furthermore, we extend SNAPS to image classification, where SNAPS shows superior performance compared to APS.

Limitations.Our work focuses on node classification using transductive learning. However, in real-world scenarios, many classification tasks require inductive learning. In the future, we aim to apply our method to the inductive setting. Additionally, the method we use to select nodes with the same as the ego node is both computationally inefficient and lacking accuracy. Future work will explore more efficient and accurate methods for node selection. Moreover, while our focus is primarily on datasets with high homophily, many heterophilous networks are prevalent in practice. Consequently, further investigation is essential to enhance the adaptability of SNAPS to these networks.