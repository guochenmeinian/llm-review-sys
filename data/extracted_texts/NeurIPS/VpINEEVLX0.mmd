# A Topology-aware Graph Coarsening Framework for Continual Graph Learning

Xiaoxue Han

Stevens Institute of Technology

xhan26@stevens.edu

&Zhuo Feng

Stevens Institute of Technology

zfeng12@stevens.edu

&Yue Ning

Stevens Institute of Technology

yue.ning@stevens.edu

###### Abstract

Graph Neural Networks (GNNs) experience "catastrophic forgetting" in continual learning setups, where they tend to lose previously acquired knowledge and perform poorly on old tasks. Rehearsal-based methods, which consolidate old knowledge with a replay memory buffer, are a _de facto_ solution due to their straightforward workflow. However, these methods often fail to adequately capture topological information, leading to incorrect input-label mappings in replay samples. To address this, we propose **TA\(\)**, a topology-aware graph coarsening and continual learning framework that stores information from previous tasks as a reduced graph. Throughout each learning period, this reduced graph expands by integrating with a new graph and aligning shared nodes, followed by a "zoom-out" reduction process to maintain a stable size. We have developed a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph while preserving essential topological information. We empirically demonstrate that the learning process on the reduced graph can closely approximate that on the original graph. We compare **TA\(\)** with a wide range of state-of-the-art baselines, proving its superiority and the necessity of preserving high-quality topological information for effective replaying. Our code is available at: https://github.com/hanxiaoxue114/TACO.

## 1 Introduction

Graph neural networks (GNNs) are oblivious: they fail to consider pre-existing knowledge or context outside of the information they were trained on. In offline settings, this problem can be mitigated by making multiple passes through the dataset with batch training. However, in a _continual learning_ (also known as _incremental learning_ or _lifelong learning_) setup, , the model learns a sequence of tasks incrementally, where each _task_ is defined as a learning session on a subgraph. This problem becomes more intractable as the model has no access to previous data, resulting in drastic degradation of model performance on old tasks. To tackle the issue of _"catastrophic forgetting"_ in GNNs, several approaches have been proposed. Among them, rehearsal-based methods  are the most common due to their straightforward workflow and efficacy in consolidating old knowledge with affordable additional memory. When performing node classification tasks, these methods utilize memory buffers to save node samples during the rehearsal process of online graph training. However, they often fail to adequately capture topological information which is important in downstream tasks. As demonstrated by Zugner _et al._, even small changes in edges can alter the expected node labels, causing the model to learn incorrect _input(feature, structure)-label_ mappings from the replay samples which creates vulnerability and security issues in critical domains .

To address this, we propose a dynamic graph coarsening framework, \(}\), that efficiently preserves high-quality topology information for experience replay. \(}\) operates through a straightforward yet effective workflow: At the end of each training task, it reduces the current graph into a compressed form that maximally preserves its properties, allowing it to serve as a proxy for the original graph; for the next task, the reduced graph and the new graph are combined by aligning co-existing nodes, and the model is trained on this new combined graph. The former step preserves information from the _intra-task_ edges (edges connecting nodes from the same task), while the latter step allows us to retrieve the _inter-task_ (edges connecting nodes from different tasks) ones. Noticing that most existing graph reduction algorithms focus solely on preserving graph topology properties and are often inefficient, we propose an efficient graph reduction algorithm, **RePro**, as a component of the CGL framework. **RePro** leverages the proximities between learned node representations to effectively preserve node features and spectral properties during the reduction process. Additionally, we present a strategy, _Node Fidelity Preservation_, to ensure that certain nodes are not compressed, thereby maintaining the quality of the reduced graph. We theoretically prove that Node Fidelity Preservation can mitigate the problem of vanishing minority classes in the process of graph reduction. We claim that the simplicity of \(}\) makes it highly modular and adaptable. We conduct extensive experiments and perform comprehensive ablation studies to evaluate the effectiveness of \(}\) and **RePro**. We also compare our method with multiple state-of-the-art methods [20; 26; 29; 6; 57; 7; 28; 30; 55] for both CGL and graph coarsening tasks.

## 2 Preliminaries

### Setup explanations

We notice that most existing research on CGL [26; 57; 55] focuses on either _task-incremental-learning (task-IL)_ or a _tranductive_[54; 3] setting, where the sequential tasks are independent graphs containing nodes from non-overlapping class sets. In this setting, the model only needs to distinguish the classes included in the current task. For instance, if there are 10 classes in total, and this experimental setting divides these classes into 5 tasks. Task 1 focuses on classifying classes 1 and 2, while task 2 classifies classes 3 and 4, and so on. Since GNNs often forget knowledge of old classes and show trivial performance, the improvement of the CGL framework can be easily highlighted with large margins. However, we argue that this setting may not accurately simulate real-life scenarios.

In this paper, we aim to tackle a more realistic _inductive_ and _Generalized Class-incremental-learning (generalized class-IL)_ setting. In real-world graphs, nodes and edges are often associated with a time stamp indicating their appearing time, and graphs keep expanding with new nodes and edges. For instance, in a citation network, each node representing a paper cites (forms an edge with) other papers when it is published. Each year more papers are published, and the citation graph also grows rapidly.

In such cases, it is necessary to train a model incrementally and dynamically because saving or retraining the model on the full graph can be prohibitively expensive in space and time. So we split a streaming graph into subgraphs based on time periods and train a model on each subgraph sequentially for the node classification task. In such cases, subgraphs are correlated with each other through the edges connecting them (e.g. a paper cites another paper from previous time stamps). The structural information represented by edges may change from previous tasks (_inductive_). Also, since the tasks are divided by time periods instead of class labels, there are overlapping classes between new tasks and old tasks, so the model needs to perform classification across all classes (_generalized class-IL_).

Figure 1: Class distributions in the Kindle dataset change over time and the model trained on new tasks tends to forget old tasks.

We further demonstrate the necessity of preserving old knowledge when learning on a streaming graph. We take the Kindle e-book co-purchasing network [14; 31] as an example. We split the graph into 5 subgraphs based on the first appearing time of each node (i.e., an e-book). We observe a gradual shift of node class distribution over time, as shown in Figure 1(a). Furthermore, even for nodes in the same class, their features and neighborhood patterns can shift . Also, in real-life situations, tasks may have different class sets (e.g. new fields of study emerge and old fields of study become obsolete), which exacerbates the forgetting problem. The F1 scores of node classification tasks using a Graph Convolutional Network (GCN)  show that the model performs significantly worse on previous tasks when trained on new ones without strategies to alleviate forgetting, as shown in Figure 1(b). Although this paper focuses on _generalized class-IL_ setting, we also conduct experiments on data splits with _traditional class-IL_ setting to prove the generalizability of \(\).

### Problem statement

Our main objective is to construct a continual learning framework on streaming graphs to overcome the catastrophic forgetting problem. Suppose a GNN model is trained for sequential node classification tasks with no access to previous training data, but it can utilize a memory buffer with a limited capacity to store useful information. The goal is to optimize the prediction accuracy of a model on all tasks in the end by minimizing its forgetting of previously acquired knowledge when learning new tasks. In this work, we focus on time-stamped graphs, and the tasks are defined based on the time stamps of nodes in a graph. For each task, the GNN model is trained on a subgraph where source nodes belonging to a specified time period, and all tasks are ordered in time. Node attributes and class labels are only available if the nodes belong to the current time period. The aforementioned setup closely resembles real-life scenarios. We formulate the above problems as follows.

**Problem 1**.: **Continual learning on time-stamped graphs.** We are given a time-stamped expanding graph \(=(,,A,X,Y)\), where \(\) denotes the node set, \(\) denotes the edge set, \(A^{||||}\) and \(X^{|| d_{X}}\) denote the adjacency matrix and node features, respectively; \(Y\) is a one-hot embedding matrix denoting class labels. Each node \(v\) is assigned to a time period \((v)\). We define a sequence of subgraphs, \(_{1},...,_{k}\), such that each subgraph \(_{t}=(_{t},_{t},A_{t},X_{t})\) from \(\) is formulated based on the following rules:

* For edges in \(_{t}\): \(e=(s,o)_{t} e\) and \((s)=t\),
* For nodes in \(_{t}\): \(v_{t}(v)=t\) or \(((s,v)\) and \((s)=t)\),

where \(s\) is a source node and \(o\) (or \(v\)) is a target node. We can assume \((o)(s)\) for \((s,o)\) (e.g. in a citation network, a paper can not cite another paper published in the future). The nodes on each subgraph \(_{t}\), are divided into three sets for training, validation, and test. We implement a GNN to perform node classification tasks and sequentially train the model on \(_{1},...,_{k}\). When the model is trained with a new task \(T_{t}\), it has no access to \(_{1},...,_{t-1}\) and \(_{t+1},...,_{k}\). However, a small memory buffer is allowed to preserve useful information from previous tasks. The **objective** is to optimize the overall performance of the model on test nodes from all tasks when the model is incrementally trained with new tasks.

**Problem 2**.: **Graph coarsening.** Given a graph \(=(,)\) with \(n=||\) nodes, the **goal** of graph coarsening is to reduce it to a target size \(n^{}\) with a specific ratio \(\) where \(n^{}= n,\,0<<1\). We construct the coarsened graph \(^{r}=(^{r},^{r})\) through partitioning \(\) to \(n^{}\) disjoint clusters \((C_{1},...,C_{n^{}})\), so that each cluster becomes a node in \(_{r}\). The construction of these clusters (i.e., the partition of a graph) depends on coarsening strategies. The node partitioning/clustering information can be represented by a matrix \(Q^{n n^{}}\). If we assign every node \(i\) in cluster \(C_{j}\) with the same weight, then \(Q_{ij}=1\); If node \(i\) is not assigned to cluster \(C_{j}\), \(Q_{ij}=0\). Let \(c_{j}\) be the number of node in \(C_{j}\) and \(C=(c_{1},...,c_{n^{}})\). The normalized version of \(Q\) is \(P=QC^{1/2}\). It is easy to prove \(P\) has orthogonal columns (\(PP^{-1}=I\)), and \(P_{ij}=1/}\) if node \(i\) belongs to \(C_{j}\); \(P_{ij}=0\) if node \(i\) does not belong to \(C_{j}\). The detailed coarsening algorithm will be discussed in the next section.

## 3 Methodology

We propose \(\), a simple yet effective continual learning framework to consolidate graph knowledge learned from proceeding tasks by replaying previous "experiences" to the model. We observe that the majority of experience replay methods, including those tailored for GNN, do not adequately maintain the intricate graph topological properties from previous tasks. Moreover, in a streaming graph setup they fail to capture the inter-dependencies among tasks that result from the presence of overlapping nodes. The inter-dependencies are essential for capturing the dynamic "receptive field" (neighborhood) of nodes and improving the performance on both new and old tasks . To overcome these limitations, we design a new replay method that preserves both the node attributes and graph topology from previous tasks. Our intuition is that, if we store the original graphs from the old task, minimal old knowledge would be lost, but it is also exceedingly inefficient and goes against the initial intention of continual learning. Thus, as an alternative, we coarsen the original graphs to a much smaller size which preserves the important properties (such as node features and graph topologies) of the original graphs. We propose an efficient graph coarsening algorithm based on Node **R**epresentation **P**roximity as a key component of **TACO**. Additionally, we develop a strategy called _Node Fidelity Preservation_ for selecting representative nodes to retain high-quality information. An overview of the proposed framework is provided in Figure 2. The pseudocode of **TACO** is described in Algorithm 1 and **RePro** is described in Algorithm 2 in Appendix D.2.

Overall frameworkWe summarize the procedure of our framework as three steps: _combine_, _reduce_, and _generate_. At task \(t\), we _combine_ the new graph \(_{t}\) with the reduced graph \(_{t-1}^{r}\) from the last task. Then we _reduce_ the combined graph \(_{t}^{c}\) to a set of clusters. At last, we _generate_ the contributions of nodes in each cluster to form a super-node in the reduced graph \(_{t}^{r}\). The last step decides the new node features and the adjacency matrix of the reduced graph. We convey the details of each step below.

### Step 1: Combine

We use \(\) (e.g., a hash table) to denote the mapping of each original node to its assigned cluster (super-node) in a reduced graph \(^{r}\). In the beginning, we initialize \(_{0}^{r}\) as an empty undirected graph and \(_{0}\) as an empty hash table. At task \(t\), the model holds copies of \(_{t-1}^{r}\), \(_{t-1}\) and an original graph \(_{t}\) for the current task. \(_{t}\) contains both new nodes from task \(t\) and old nodes that have appeared in previous tasks. We first "combine" \(_{t}\) with \(_{t-1}^{r}\) to form a new graph \(_{t}^{c}\) by checking the hash table \(_{t-1}\) and aligning the co-existing nodes in \(_{t}\) and \(_{t-1}^{r}\). By doing so, we retrieve the _inter-task_ edges. We train the model \(f\) to perform node classification tasks on the combined graph \(_{t}^{c}=(A_{t}^{c},X_{t}^{c},Y_{t}^{c})\) with the objective \(_{}(f(A_{t}^{c},X_{t}^{c},),Y_{t}^{c})\), where \(f\) is a \(L\)-layer GNN model (e.g. GCN), \(\) is trainable parameters of the model, and \(\) is the loss function. In this work, new nodes and old nodes in \(_{t}^{c}\) contribute equally to the loss during the training process. However, it remains an option to assign distinct weights to these nodes to ensure a balance between learning new information and consolidating old knowledge. We describe a more detailed process in Appendix D.1.

Figure 2: An overview of **TACO**. At \(t\)-th time period, the model takes in the coarsened graph \(_{t-1}^{r}\) from the last time period and the original graph \(_{t}\) from the current time period, and combine them into \(_{t}^{c}\); for the same time period, the selected important node set is updated with the new nodes; the model is then trained on \(_{t}^{c}\) with both the new nodes and the super-nodes from the past; finally \(_{t}^{c}\) is coarsened to \(_{t}^{r}\) for the next time period.

### Step 2: Reduce

We decide how nodes are grouped into clusters and each cluster forms a new super-node in the reduced graph. We propose an efficient graph coarsening method, **RePro**, by leveraging the **Re**presentation **P**roximities of nodes to reduce the size of a graph through merging "similar" nodes to a super-node. Node representations are automatically learned via GNN models without extra computing processes. We think two nodes are deemed similar based on three factors: 1) _feature similarity_, which evaluates the closeness of two nodes based on their features; 2) _neighbor similarity_, which evaluates two nodes based on their neighborhood characteristics; 3) _geometry closeness_, which measures the distance between two nodes in a graph (e.g., the length of the shortest path between them). Existing graph coarsening methods concentrate on preserving spectral properties, only taking graph structures into account and disregarding node features. However, estimating spectral similarity between two nodes is typically time-consuming, even with approximation algorithms, making it less scalable for our applications where graphs are dynamically expanded and coarsened. Thus, we aim to develop a more time-efficient algorithm that considers the aforementioned similarity measures.

To get started, we have the combined graph \(_{t}^{c}\) to be coarsened. We train a GNN model with \(L\) (\(L=2\) in our case) layers on \(_{t}^{c}\), such that the node embedding of \(_{t}^{c}\) at the first layer (before the activation function) is denoted by

\[H^{n_{i}^{c} d^{h}}=^{(1)}(A_{t}^{c},X_{t}^{c},),\] (1)

where \(d^{h}\) is the size of the first hidden layer in GNN. The similarity between every connected node pair \((u,v)=e_{t}^{c}\) is calculated based on cosine similarity as \((e)= H_{v}}{\|H_{u}\|\|H_{v}\|}\), where \((e)\) is the similarity score between the two end nodes of the edge \(e\), \(H_{i}\) is the embedding for node \(i\), and \(\|\|\) is the second norm of a vector. We then sort all edges of \(_{t}^{c}\) such that \((e_{1})(e_{2})...(e_{m_{i}^{c}})\). Based on this sorted list of edges, we recursively merge two end nodes (i.e., assign the nodes to the same cluster), until the target size is reached. The class label of the merged node is determined based on the majority votes of the original nodes. The new adjacency matrix and node feature matrix are discussed in Step 3. The time complexity of our coarsening process is \((d^{h} m_{t}^{c})\) where \(m_{t}^{c}\) is the number of edges in the current graph \(_{t}^{c}\).

_Node Fidelity Preservation_ After multiple rounds of coarsening, the quality of a graph deteriorates as its node features and labels are repeatedly processed. Furthermore, the use of a majority vote to determine the label of a cluster can lead to the gradual loss of minority classes and cause a "vanishing minority class" problem.

**Theorem 3.1**.: _Consider \(n\) nodes with \(c\) classes, such that the class distribution of all nodes is represented by \(=p_{1},p_{2},...,p_{c}\) where \(_{i=1}^{c}p_{i}=1\). If these nodes are randomly partitioned into \(n^{}\) clusters such that \(n^{}= n\), \(0<<1\) and the class label for each cluster is determined via majority voting. The class distribution of all the clusters is \(^{}=p_{1}^{},p_{2}^{},...,p_{c}^{}\) where \(_{i}^{}\) is computed as the ratio of clusters labeled as class \(i\) and \(_{i=1}^{c}p_{i}^{}=1\). Let \(k\) be one of the classes, and the rest of the class are balanced \(p_{1}=...=p_{k-1}=p_{k+1}=...=p_{c}\). It holds that:_

_1. If \(p_{k}=1/c\) and all classes are balanced \(p_{1}=p_{2}=...=p_{c}\), then \([p_{k}^{}]=p_{k}\)._

_2. When \(p_{k}<1/c\), \([p_{k}^{}]<p_{k}\), and \([^{}}{p_{k}}]\) decreases as \(n^{}\) decreases. There exists a \(p^{}\) such that \(0<p^{}<1\), and when \(p_{k}<p^{}\), \([^{}}{p_{k}}]\) decrease as \(p_{k}\) decreases._

The proof of Theorem 3.1 is provided in Appendix D.3. Theorem 3.1 shows that as the ratio of a class decreases, its decline becomes more severe when the graph is reduced. Eventually, the class may even disappear entirely from the resulting graph. To combat these issues, we suggest preserving representative nodes in a "replay buffer" denoted as \(_{t}^{rb}\). We adopt three strategies from  to select representative nodes, namely _Reservoir Sampling_, _Ring Buffer_, and _Mean of Features_. The replay buffer has a fixed capacity and is updated as new nodes are added. During the coarsening process, we prevent the selected nodes in \(_{t}^{rb}\) from being merged by imposing a penalty on the similarity score \((e)\) such that

\[(e)= H_{v}}{\|H_{u}\|\|H_{v}\|}-(u_{t}^{rb}v_{t}^{rb}),\] (2)

where \(\) is a constant and \(>0\). It is worth noting that we do not remove the edges of these nodes from the list of candidates. Instead, we assign a high value to the penalty \(\). This is to preventscenarios where these nodes play a critical role in connecting other nodes. Removing these nodes entirely may lead to the graph not being reduced to the desired size due to the elimination of important paths passing through them. We make the following observation:

**Observation 3.1**.: _Node Fidelity Preservation with buffer size \(b\) can alleviate the declination of a minority class \(k\) when \(p_{k}\) decreases and \(n^{}\) decreases, and prevent class \(k\) from vanishing when \(p_{k}\) is small. See Appendix D.3 for further discussions._

Note that **RePro** does not require any additional parameters or training time. It only relies on the learned node embeddings from the graph neural network in the continual learning. We train a GNN model on a combined graph at each time step for node classification tasks. The node embeddings learned from the GNN model at different layers are representative of the nodes' neighborhoods. We use this fact and propose to measure the similarity of two nodes based on the distance between their embedding vectors. However, it takes quadratic time to calculate the pair-wise distance among nodes, thus we make a constraint that only connected nodes can be merged. Since connectivity has also been used to estimate the geometry closeness of two nodes , by doing so, we are able to cover the three similarity measures as well as reduce the time complexity to linear time in terms of the number of edges to calculate node similarities.

Our proposed approach based on node representations seems to be distinct from spectral-based methods, but they share a similar core in terms of the preserving of graph spectral properties. See Appendix D.4 for more details.

### Step 3: Generate

From the last step, we get the membership matrix \(Q_{t}^{n_{t}^{c} n_{t}^{r}}\) where \(n_{t}^{c}\) is the number of nodes in the combined graph, \(n_{t}^{r}=[ n_{t}^{c}]\) is the number of nodes in the coarsened graph and \(\) is the coarsening ratio. \(Q_{t}[i,j]=1\) denotes that node \(i\) is assigned to super-node \(j\). Otherwise, \(Q_{t}[i,j]=0\).

A simple way to normalize \(Q_{t}\) is assuming each node contributes equally to their corresponding super-node (e.g. \(Q_{t}[i,j]=1/}\) for all any node \(i\) that belongs to cluster/supernode \(j\)). However, nodes might have varying contributions to a cluster depending on their significance. Intuitively, when a node is identified as a very popular one that is directly or indirectly connected with a large number of other nodes, preserving more of its attributes can potentially mitigate the effects of the inevitable "information degrading" caused by the graph coarsening procedure. To address the above issue, we propose to use two different measures to decide a node's importance score: 1) _node degree_: the number of 1-hop neighbors of the node. 2) _neighbor degree sum_: the sum of the degrees of a node's 1-hop neighbors. In this step, we propose to normalize \(Q_{t}\) to \(P_{t}\) utilizing these node importance information. We calculate the member contribution matrix \(P_{t}^{n_{t}^{c} n_{t}^{r}}\). Let \(i\) be a node belonging to cluster \(C_{j}\) at timestamp \(t\), and \(s_{i}>0\) be the importance score of node \(i\) (node degree or neighbor degree sum), then \(p_{t,(ij)}=}{_{v}c_{j}}s_{v}}\). It is straightforward to prove that \(P_{t}^{}P_{t}=I\) still holds. Once we have \(P_{t}\), we get the new reduced graph \(_{t}^{r}=(A_{t}^{r},X_{t}^{r},Y_{t}^{r})\) as:

\[A_{t}^{r}=Q_{t}^{}A_{t}^{c}Q_{t}, X_{t}^{r}=P_{t}^{}X_{t}^{c},  Y_{t}^{r}=(P_{t}^{}Y_{t}^{c}),\] (3)

where the label for each cluster is decided by a (weighted) majority vote. Only partial nodes are labeled, and the rows of \(Y_{t}^{c}\) for unlabelled nodes are zeros and thus do not contribute to the vote.

Through training all tasks, the number of nodes in the reduced graph \(^{r}\) is upper-bounded by \((n_{})\), where \(n_{}\) is the largest number of the new nodes for each task (See Appendix D.5 for proof); when the reduction ratio \(\) is 0.5, the expression above is equivalent to \(n_{}\), meaning the size of the reduced graph is roughly the same size with the original graph for each task. The number of edges \(m\) is bounded by \(n_{}^{2}\), but we observe generally \(m n_{}^{2}\) in practice. We claim that such a memory buffer size is reasonable for storing topology information. For instance, the default buffer size of the Cora-full dataset in SSM  is 4,200, which is approximately seven times the average graph size.

## 4 Empirical evaluation

We conduct experiments on time-stamped graph datasets: Kindle [14; 31], DBLP  and ACM  to evaluate the performance of \(}\). See Appendix E.1 for the details of the datasets and E.2 for hyperparameter setup.

### Comparison methods

We compare the performance of \(}\) with SOTA continual learning methods including EWC , GEM , TWP , OTG , ERGNN , SSM , DyGrain , IncreGNN , SSRM , CaT , and DeLoMe . EWC and GEM were previously not designed for graphs, so we train a GNN on new tasks but ignore the graph structure when applying continual learning strategies. ERGNN-rs, ERGNN-rb, and ERGNN-mf are ERGNN methods with different memory buffer updating strategies: Reservoir Sampling (rs), Ring Buffer (rb), and Mean of Features (mf) . SSRM is an additional regularizer to be applied on top of a CGL framework; we choose ERGNN-rs as the base CGL model. Besides, finetune provides the estimated lower bound without any strategies applied to address forgetting problems, and joint-train provides an empirical upper bound where the model has access to all previous data during the training process.

We also compare \(\) with five representative graph coarsening SOTA methods. We replace the coarsening algorithm in \(}\) with different coarsening algorithms. Alge. JC , Aff.GS , Var. edges , and Var. neigh  are graph spectral-based methods; FGC  considers both graph spectrals and node features. We follow a standard implementation  for the first four methods.

### Evaluation metrics

We use _Average Performance_ (AP\(\)) and _Average Forgetting_ (AF\(\))  to evaluate the performance on test sets. AP and AF are defined as

\[=_{j=1}^{T}a_{T,j},\ \ =_{j=1}^{T }_{l\{1,,T\}}a_{l,j}-a_{T,j},\] (4)

where T is the total number of tasks and \(a_{i,j}\) is the prediction metric of the model on the test set of task \(j\) after it is trained on task \(i\). The prediction performance can be measured with different metrics. In this paper, we use macro F1 and balanced accuracy score (BACC). F1-AP and F1-AF indicate the AP and the AF for macro F1 and likewise for BACC-AP and BACC-AF. We calculate the macro F1 and BACC scores for multi-class classification .

### Main results

We evaluate the performance of \(}\) and other baselines on three datasets with three backbone GNN models, including GCN , GAT , and GIN . We only report the node classification performance with GCN in Table 1 due to the space limit. See Appendix F.1 for results on GAT and GIN. We report the average values and the standard deviations over 10 runs. It shows that \(}\) outperforms the best SOTA CGL baseline method with high statistical significance, as evidenced by p-values below 0.05 reported from a t-test. Additionally, we note that despite being Experience Replay-based methods, ER-rs, ER-rb, and ER-mf do not perform as well as SSM and \(}\), highlighting the importance of retaining graph structural information when replaying experience nodes to the model. Furthermore, we infer that \(}\) outperforms SSM due to its superior ability to preserve graph topology information and capture task correlations through co-existing nodes.

### Ablation studies

#### 4.4.1 Graph coarsening methods

We evaluate the performance of \(\) by replacing the graph coarsening module of \(}\) with five widely used coarsening algorithms while keeping all other components unchanged. For each method, we report the average time in seconds consumed to coarsen the graph. We also report a trade-off value which is defined as the coarsening time (in seconds) needed to increase or decrease the AP/AF by 1% compared with fine-tuning, as shown in Table 2. For instance, the trade-off for F1-AP is defined as\(_{}=/(-\) for fine-tuning). Complete results are provided in Appendix F.2. The results demonstrate that **RePro** is considerably more efficient in computing time compared to all other models and achieves the best trade-off across all metrics.

#### 4.4.2 Graph reduction rates

We examine how **RePro** preserves original graph information. Following the setting in , we train GNNs from scratch on original and coarsened graphs, then compare their prediction performance on the test nodes from the original graph. Using subgraphs from the first task of three datasets as original graphs, we train GCN models on coarsened graphs with different coarsening rates \(1-\). Figure 3 (a) shows that prediction performance is relatively stable as graphs are reduced for DBLP and ACM, but F1 scores are more sensitive to the reduction rate on Kindle. This may be due to overfitting on smaller datasets. Although reduced graphs may not preserve all information, they can still be used to consolidate learned knowledge and reduce forgetting in CGL paradigm. We also test if the model learns similar node embeddings on coarsened and original graphs. In Figure 3 (b)-(d), we visualize test node embeddings on the DBLP dataset for different reduction rates using t-SNE. We observe similar patterns for 0 and 0.5 reduction rates, and gradual changes as the reduction rate increases.

#### 4.4.3 Performance after training on each task

We first investigate the performance of the model on the first task when more tasks are learned with different CGL frameworks. We also visualize the model's performance in terms of AP-F1 using the four approaches on all previous tasks after training on each task on the Kindle dataset, as shown in Figure 4. It demonstrates that the application of different CGL methods can alleviate the catastrophic

    &  &  &  \\ 
**Method** &  &  &  &  &  &  \\  joint train & 87.21 & \(\) 0.55 & 0.45 & \(\) 0.25 & 86.33 & \(\) 1.38 & 0.77 & \(\) 0.13 & 75.35 & \(\) 1.49 & 1.87 & \(\) 0.60 \\  finetune & 69.10 & \(\) 10.85 & 18.99 & \(\) 11.19 & 67.85 & \(\) 8.05 & 20.43 & \(\) 7.07 & 60.53 & \(\) 9.35 & 19.09 & \(\) 9.23 \\  simple-reg & 68.80 & \(\) 10.02 & 18.21 & \(\) 10.49 & 69.70 & \(\) 9.16 & 18.69 & \(\) 8.48 & 61.63 & \(\) 10.09 & 17.83 & \(\) 9.99 \\ EWC & 77.08 & \(\) 8.37 & 10.87 & \(\) 8.62 & 79.38 & \(\) 4.86 & 8.85 & \(\) 4.11 & 66.88 & \(\) 6.43 & 12.73 & \(\) 6.26 \\ TWP & 78.90 & \(\) 4.71 & 8.99 & \(\) 4.93 & 80.05 & \(\) 37.1 & 8.23 & \(\) 3.28 & 65.98 & \(\) 7.26 & 13.33 & \(\) 6.94 \\ OTG & 69.01 & \(\) 10.55 & 18.94 & 10.79 & 68.24 & \(\) 10.12 & 0.12 & \(\) 9.34 & 61.45 & \(\) 9.94 & 18.33 & \(\) 9.86 \\ GEM & 76.08 & \(\) 6.70 & 11.01 & \(\) 7.27 & 80.04 & \(\) 32.4 & 7.90 & \(\) 2.68 & 67.17 & \(\) 4.24 & 11.69 & \(\) 3.94 \\ ERGNN-\(\) & 77.63 & \(\) 3.61 & 9.64 & \(\) 4.19 & 78.02 & \(\) 5.79 & 10.08 & \(\) 5.16 & 64.82 & \(\) 7.89 & 14.43 & \(\) 7.68 \\ ERGNN-rb & 75.87 & \(\) 6.41 & 11.46 & 6.98 & 75.16 & \(\) 7.24 & 12.85 & \(\) 6.54 & 63.58 & \(\) 8.82 & 15.66 & \(\) 8.71 \\ ERGNN-mf & 77.28 & \(\) 5.91 & 10.15 & \(\) 6.31 & 77.42 & \(\) 5.25 & 10.64 & \(\) 4.38 & 64.80 & \(\) 8.49 & 14.59 & \(\) 8.41 \\ DyGrain & 69.14 & \(\) 10.47 & 18.88 & \(\) 10.72 & 67.52 & \(\) 10.88 & 20.83 & \(\) 10.16 & 61.40 & \(\) 9.5forgetting problem on the Kindle dataset as we point out in the introduction part to varying degrees. Also, we observe that the performance of experience replay-based methods (SSM and \(\)) are more stable through learning more tasks, but the regularization-based method, TWP, experiences more fluctuations. We deduce that this is caused by the fact that regularization-based methods can better prevent the model from drastically forgetting previous tasks even when the current task has more distinct distributions.

### Efficiency analysis

To ensure fair comparisons, for experience replay-based methods, we adjust the memory buffer size so each model stores comparable average numbers of replay nodes. We report the average memory usage of each model on each task in Table 10 in Appendix G.3. Since TACO needs extra space to save topology information, its memory usage is slightly larger compared to methods that don't preserve graph structures. We provide memory usages of the representative methods across tasks in Table 8.

### Additional studies

For those readers who are interested, we also evaluate the short-term forgetting (G.1) and the performance of the CGL frameworks when each task is assigned with shorter time intervals (G.2). We study the effectiveness of the Node Fidelity Preservation (G.4), and the effects of different important node selection strategies (G.5). We also evaluate \(\) on traditional class-incremental learning (G.6) to prove its generalizability. We present those results in Appendix G.

## 5 Related Work

_Regularization_, _Expansion_, and _Rehearsal_ are common approaches to overcome the catastrophic forgetting problem  in continual learning. _Regularization_ methods [20; 4; 52; 10] penalize parameter changes that are considered important for previous tasks. Although this approach is efficient in space and computational resources, it suffers from the "brittleness" problem [43; 35] where the previously regularized parameters may become obsolete and unable to adapt to new tasks. _Expansion_-based methods [40; 51; 16; 23] assign isolated model parameters to different tasks and increase the model size when new tasks arrive. Such approaches are inherently expensive, especially when the number of tasks is large. _Rehearsal_-based methods consolidate old knowledge by replaying

Figure 4: F1 score on the test set (x-axis) after training on each task (y-axis) on Kindle dataset.

Figure 3: (a) The test macro-F1 scores of the GCN model trained on the coarsened graphs with different reduction rates on three datasets. (b)-(d) t-SNE visualization of node embeddings of the DBLP test graph with a reduction rate of 0, 0.5, and 0.9 on the training graph respectively.

the model with past experiences. A common way is using a small episodic memory of previous data or generated samples from a generative model when it is trained on new tasks. While generative methods [1; 2; 8; 34] may use less space, they also struggle with catastrophic forgetting problems and over-complicated designs . Experience replay-based methods [39; 5; 6; 43; 29], on the other hand, have a more concise and straightforward workflow with remarkable performance demonstrated by various implementations with a small additional working memory.

**Continual graph learning** Most existing CGL methods adapt _regularization_, _expansion_, or _rehearsal_ methods on graphs. For instance,  address catastrophic forgetting by penalizing the parameters that are crucial to both task-related objectives and topology-related ones.  mitigate the impact of the structure shift by minimizing the input distribution shift of nodes. Rehearsal-based methods [18; 57; 48] keep a memory buffer to store old samples, which treat replayed samples as independent data points and fail to preserve their structural information.  preserve the topology information by storing a sparsified \(L\)-hop neighborhood of replay nodes. However, storing topology information of nodes through this method is not very efficient and the information of uncovered nodes is completely lost; also it fails to capture inter-task correlations in our setup. Besides,  present an approach to address both the heterophily propagation issue and forgetting problem with a triad structure replay strategy: it regularizes the distance between the nodes in selected closed triads and open triads, which is hard to be categorized into any of the two approaches.  uses a condensed graph as the memory buffer and employs a "Training in Memory" scheme that directly learns on the memory buffer to alleviate the data imbalance problem. However, the condensed graph contains only sampled nodes with self-loops, resulting in the loss of valuable topology information.  stores the learned representations of the sampled nodes as a memory store, which can preserve graph structure information. However, it still cannot handle inter-task edges, and the stored representations are inadequate for dealing with the dynamic receptive field of old nodes when new nodes form connections with them. Some CGL work [47; 50] focuses on graph-level classification where each sample (a graph) is independent of other samples, whereas our paper mainly tackles the problem of node classification where the interdependence of samples plays a pivotal role.

To better highlight the advantages of our proposed method, we provide a comparison between our method and these experience-based methods in Table 3. Our method, \(\), is the only one that can preserve graph topology, handle inter-task edges, and consider the growing receptive field of old nodes.

## 6 Conclusion

In this paper, we present a novel CGL framework, \(\), which stores useful information from previous tasks with a dynamically reduced graph to consolidate learned knowledge. Additionally, we propose an efficient embedding proximity-based graph coarsening method, \(\), that can preserve important graph properties. We present a Node Fidelity Preservation strategy and theoretically prove its capability in preventing the vanishing minority class problem. We demonstrate the effectiveness of \(\) and \(\) on real-world datasets with a realistic streaming graph setup.