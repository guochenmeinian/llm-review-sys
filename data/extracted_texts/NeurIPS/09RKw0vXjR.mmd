# Fast Iterative Hard Thresholding Methods

with Pruning Gradient Computations

 Yasutoshi Ida\({}^{1}\)1  Sekitoshi Kanai\({}^{1}\)  Atsutoshi Kumagai\({}^{1}\)  Tomoharu Iwata\({}^{2}\)

Yasuhiro Fujiwara\({}^{2}\)

\({}^{1}\)NTT Computer and Data Science Laboratories

\({}^{2}\)NTT Communication Science Laboratories

###### Abstract

We accelerate the iterative hard thresholding (IHT) method, which finds \(k\) important elements from a parameter vector in a linear regression model. Although the plain IHT repeatedly updates the parameter vector during the optimization, computing gradients is the main bottleneck. Our method safely prunes unnecessary gradient computations to reduce the processing time. The main idea is to efficiently construct a candidate set, which contains \(k\) important elements in the parameter vector, for each iteration. Specifically, before computing the gradients, we prune unnecessary elements in the parameter vector for the candidate set by utilizing upper bounds on absolute values of the parameters. Our method guarantees the same optimization results as the plain IHT because our pruning is safe. Experiments show that our method is up to 73 times faster than the plain IHT without degrading accuracy.

## 1 Introduction

The optimization problem of finding sparse parameter vectors in linear regression models is a crucial problem that crosses a wide range of fields, including feature selection , sparse coding , dictionary learning , and compressed sensing . To obtain the sparse parameter vector, the parameter vector is often constrained to have \(k\) nonzero elements. Among a huge number of algorithms that have been developed for this problem , iterative hard thresholding (IHT) methods  are practical methods based on gradient descent methods because they have almost no overhead over the plain gradient descent method .

A procedure of IHT consists of two main parts in an iteration. First, it updates the parameter vector in accordance with the gradient descent method. Then, the parameter vector is projected onto a feasible set being a set of sparse parameter vectors. In the projection, IHT uses a hard thresholding operator that selects the \(k\)-largest elements in terms of magnitude of the parameter vector, and the other elements are set to zero. IHT repeats this procedure until the stopping criterion is satisfied.

When a design matrix of the linear regression model is an \(m\)-by-\(n\) matrix, the length of the parameter vector is \(n\), and gradient computations of IHT require \((mn)\) or \((n^{2})\) time for each iteration. Specifically, IHT suffers from the increase in processing time for large \(m\) and \(n\). Taking feature selection as an example, IHT slows down for large datasets because \(m\) and \(n\) correspond to the numbers of samples and features in the dataset, respectively. The gradient computations are much more computationally expensive than the projection of the parameter vector because the projection only requires \((n\, k)\) time if it uses a heap. Therefore, the gradient computations are dominant in the overall procedure of IHT, and we need to reduce the cost to raise the efficiency of IHT.

This paper proposes fast IHT that safely prunes unnecessary gradient computations. Before computing the gradients for each iteration, the proposed method efficiently constructs a candidate set whose elements correspond to indices of the \(k\)-largest elements in terms of magnitude of the parameter vector. When constructing the candidate set, our method prunes indices that are clearly not included in the candidate set. To identify such indices, we compute upper bounds of absolute values for the elements of the parameter vector. If the upper bound is smaller than a threshold, the index is not included in the candidate set. The threshold is automatically determined by leveraging lower bounds of absolute values for the elements of the parameter vector. Since the computation cost of the upper and lower bounds is \((n)\) time, we can efficiently construct the candidate set. By updating only the parameters corresponding to the candidate set, we can prune unnecessary gradient computations. Our method guarantees the same optimization results as the plain IHT because it safely prunes unnecessary computations. In addition, our method does not need additional hyperparameter tuning. Experiments demonstrate that our method is up to 73 times faster than the plain IHT while maintaining accuracy on feature selection tasks.

## 2 Related Work

In [4; 30], the authors utilized a double-overrelaxation approach to improve the convergence speed of IHT. They use two relaxation steps for the parameter vector, which are similar to the momentum of Nesterov's method . In [8; 24], the authors introduced the momentum to IHT inspired by the fast iterative shrinkage thresholding algorithm (FISTA) . While FISTA uses the momentum with a soft thresholding operator, Cevher  uses it with the hard thresholding operator. This Accelerated IHT (AccIHT) has substantial theoretical and empirical improvement over the plain IHT .

While the previous methods have reduced the number of iterations to accelerate IHT as described above, to the best of our knowledge, there are no papers on reducing the computation cost per iteration of IHT. This paper aims to fill this gap based on the pruning strategy. For convex and some nonconvex regularization, working set algorithms are used to reduce the cost of solvers [7; 21; 26; 31]. They solve a growing sequence of subproblems that are restricted to a small subset of parameters during optimization. In [12; 13; 22; 15; 16; 18; 17], the authors reduced the cost by skipping unnecessary parameter updates for coordinate descent with sparsity-inducing norms. However, since these methods are tailored for coordinate descent or the soft thresholding operator, they cannot be used for IHT, which selects \(k\) elements from the parameter vector by using the hard thresholding operator.

## 3 Preliminary

**Notation.** We denote scalars, vectors, and matrices with lower-case, bold lower-case, and bold upper-case letters, _e.g.,_\(x\), \(\) and \(\), respectively. Given a matrix \(\), we denote its \(i\)-th row by \(_{i}\). Given a vector \(^{m}\), we denote its \(i\)-th element by \(_{i}\), and we call \(i\) index. \(\|\|_{2}\) is the \(_{2}\) norm. \(\|\|_{0}\) is \(|\{i\{1,...,m\}|x_{i} 0\}|\) and represents the number of nonzero elements in \(\). \(^{m}\) is the \(m\)-dimensional vector whose elements are zeros. \(\) represents the identity matrix. \(()\) is the function that returns the indices of nonzero elements in \(\).

### Problem Setting

Let \(^{m n}\) be an input matrix (design matrix), \(^{m}\) be a set of continuous responses, and \(^{n}\) be a parameter vector of a linear regression model. To find a sparse parameter vector of the model, we consider the following optimization problem :

\[_{^{n}}- _{2}^{2}\ \ |\|_{0} k.\] (1)

In the above problem, the number of nonzero elements in the parameter vector, \(|\|_{0}\), is constrained by \(k\{1,...,n\}\). Here, we will let \(f()=-_{2}^{2}\) for simplicity.

### Iterative Hard Thresholding

IHT is the practical algorithm for Problem (1) [5; 2]. It repeatedly performs the following iteration:

\[^{t} =^{t}- f(^{t})=^{t}- ^{}(^{t}-)=(-^{} )^{t}+^{},\] (2) \[^{t+1} =H_{k}(^{t}),\] (3)where \(>0\) is the step size, \(^{t}\) is the parameter vector at the \(t\)-th iteration. \(H_{k}(^{t})\) is the hard thresholding operator that selects the \(k\)-largest elements in the magnitude of \(^{t}\) and sets the other elements to zero. The selection requires \((n\ \ k)\) time if it uses a heap. The pseudocode is described in Algorithm 1. See [5; 14; 6; 8; 4; 20; 23; 2] for theoretical discussions of IHT.

From Equation (2) and Algorithm 1, computing gradients \( f(^{t})\) clearly dominates the other cost. In Equation (2), we use the second equation or the third one to compute \(^{t}\). They require \((mn)\) and \((n^{2})\) times in every iteration, respectively2. Therefore, IHT incurs high computation cost when \(^{m n}\) is large.

## 4 Proposed Algorithm

This section describes our algorithm that reduces the computation cost per iteration in IHT.

### Main Idea

The bottleneck of IHT is the gradient computation to obtain \(^{t}\) of Equation (2): it requires \((mn)\) or \((n^{2})\) time per iteration. Therefore, we reduce the cost by pruning unnecessary elements in \(^{t}\) before computing the gradients. For the pruning, we introduce a candidate set \(^{t}\) such that \(|^{t}|=k\) for the \(t\)-th iteration. This set maintains indices of nonzero elements of the parameter vector during optimization. In other words, the candidate set contains indices of the \(k\)-largest elements in terms of magnitude of the parameter vector. Before computing Equation (2), we quickly check whether indices of elements in \(^{t}\) are included or not in \(^{t+1}\). If an index \(j\) is not included in \(^{t+1}\), we can prune \(^{t}_{j}\) and skip the corresponding computation of Equation (2) including the gradient computation.

The point is that our method can efficiently perform the above checking procedure. Specifically, our method utilizes \(}^{t}_{j}\), which is an upper bound of \(|^{t}_{j}|\). Since the computation of the upper bound does not include the gradient computation, it only requires \((n)\) time for all the elements in \(}^{t}\). For the checking procedure, after initializing \(^{t}\) appropriately, our method finds a threshold for the pruning by utilizing \(}^{t}_{j}\), which is a lower bound of \(|^{t}_{j}|\). Then, if \(}^{t}_{j}\) is smaller than the threshold for \(j^{t}\), the index \(j\) is not included in \(^{t+1}\). We describe the details in the next section.

### Upper Bound and Candidate Set

This section introduces candidate set \(^{t}\) and its updating method. Since we need upper bound \(}^{t}_{j}\) to efficiently update \(^{t}\) as described in Section 4.1, we first define \(}^{t}_{j}\) as follows:

**Definition 1**: _Let \(t^{*}\) be \(1 t^{*}<t\) in Algorithm 1. Then, \(}^{t}_{j}\) at the \(t\)-th iteration in Algorithm 1 is computed as follows:_

\[}^{t}_{j}=|_{j}^{t^{*}}+(^{} )_{j}|+|_{j}|_{2}|^{t}-^{t^{*}}|_{2},\] (4)

_where \(=-^{}\)._

We note that \(\) and \(^{}\) are precomputed only once before entering the optimization, and \(t^{*}\) is automatically decided as described in Section 4.4. \(}^{t}_{j}\) has the following property:

**Lemma 1** (Upper bound): _When \(}_{j}^{t}\) is computed by Equation (4), we have \(}_{j}^{t}|_{j}^{t}|\)._

Lemma 1 is derived from the triangle inequality and the Cauchy-Schwarz inequality. It guarantees that \(}_{j}^{t}\) is the upper bound of \(|_{j}^{t}|\). The computation cost of the upper bound is as follows:

**Lemma 2** (Computation cost of upper bound): _The computation cost of Equation (4) for all \(j\{1,...,n\}\) at the \(t\)-th iteration is \((n)\) time given \(\), \(^{}\) and \(|_{j}^{^{}}+(^{})_{j}|\)._

Lemma 2 shows that the upper bound at the \(t\)-th iteration requires only \((n)\) time if \(\) and \(^{}\) are precomputed and \(|_{j}^{^{}}+(^{})_{j}|\) is computed at the \(t^{*}\)-th iteration.

Next, we define candidate set \(^{t}\) as follows:

**Definition 2** (Candidate set): _Let \(=\{1,...,n\}\) be a set of all the indices in the parameter vector \(^{n}\). Suppose that \(^{t}\) is a set such that \(|^{t}|=k\) at the \(t\)-th iteration in IHT where \(t>1\), and initialized as \(^{t}=(^{t})\) at the beginning of the iteration. Then, we call \(^{t}\) the candidate set._

\(^{t}\) has indices that are candidates for \((^{t+1})\) in IHT. Since \(^{t}\) is initialized as \(^{t}=(^{t})\) at the beginning of the iteration in Definition 2, we need to update \(^{t}\) to \(^{t+1}\) so that it matches \((^{t+1})\). Although we can update \(^{t}\) such that \(^{t+1}=(H_{k}(^{t}))\) by computing Equations (2) and (3), they include the gradient computation that requires \((mn)\) or \((n^{2})\) time.

To efficiently update \(^{t}\) to \(^{t+1}\), we utilize the upper bound \(}_{j}^{t}\) of Equation (4) for \(j^{t}\). By using \(}_{j}^{t}\), we can identify unnecessary elements in \(^{t}\) that are clearly not included in \(^{t+1}\) as follows:

**Lemma 3** (Pruning unnecessary elements): _Suppose that \(}^{t}\) is computed by using Equation (4), and the candidate set \(^{t}\) is initialized as described in Definition 2 at the beginning of the iteration for \(t>1\). Let \(_{i_{}}^{t}\) be \(_{i}^{t}\) having the minimum \(|_{i}^{t}|\) in all \(i^{t}\), and \(i_{}\) be the index. Then, if \(|_{i_{}}^{t}|}_{j}^{t}\) holds for \(j^{t}\), \(j\) is not included in \(^{t+1}\)._

From Lemma 3, when we check whether \(j^{t}\) is included or not in \(^{t+1}\), we do not need to compute the gradient corresponding to \(_{j}^{t}\) if \(|_{i_{}}^{t}|}_{j}^{t}\) holds. Although we need to compute the gradients to find \(_{i_{}}^{t}\) in the initial \(^{t}\), the cost is relatively small because the cardinality of \(^{t}\) is usually small as \(|^{t}|=k n\).

Algorithm 2 is the pseudocode of updating the candidate set. It first copies \(^{t}\) to \(^{t+1}\) (line 3). Lines 4-13 check whether the computation of \(_{j}^{t}\) can be pruned or not by following Lemma 3. If the computation is pruned (line 5), we skip the computation of \(_{j}^{t}\) including the gradient computation (line 6). If the computation is not pruned (line 7), line 8 computes \(_{j}^{t}\). If \(|_{i_{}}^{t}|<|_{j}^{t}|\) holds (line 9), the algorithm updates the candidate set \(^{t+1}\) to remove \(i_{}\) and include \(j\) (line 10). In this case, \(_{i_{}}^{t}\) must also be updated since \(^{t+1}\) has been updated (line 11). If \(|_{i_{}}^{t}|<|_{j}^{t}|\) does not hold (line 12), we set \(_{j}^{t}=0\) because \(j\) cannot be included in \(^{t+1}\) (line 13).

For the outputs \(^{t}\) and \(^{t+1}\) of Algorithm 2, we have the following property:

**Lemma 4** (Consistency of outputs): _For the outputs of Algorithm 2, \(^{t}=^{t+1}\) and \(^{t+1}=(^{t+1})\) hold._

The above lemma shows that Algorithm 2 returns the same \(^{t+1}\) and \((^{t+1})\) as those of the plain IHT while it prunes unnecessary computations given \(_{i_{}}^{t}\), \(}^{t}\), and \(^{t}\). Specifically, we can use Algorithm 2 instead of lines 4-5 in Algorithm 1.

The computation cost of Algorithm 2 is as follows:

**Lemma 5** (Computation cost of updating candidate set): _Let \(u\) be the number of un-pruned computations at line 7 of Algorithm 2. If \(\) and \(^{}\) are precomputed, Algorithm 2 requires \((un)\) time and the worst time complexity is \((n^{2})\) time._

Lemma 5 shows that the cost of Algorithm 2 is small when the pruning rate is high because \(u\) becomes small when the pruning rate is high. On the other hand, the worst time complexity of \((n^{2})\) time is obtained with a low pruning rate. The asymptotic cost cannot be larger than that of the plain IHTsince the gradient computation of the plain IHT requires \((mn)\) or \((n^{2})\) time. Nonetheless, the pruning rate needs to be increased to achieve higher speeds.

In Algorithm 2, \(|_{i_{}}^{t}|\) plays the role of the threshold for pruning at line 5. Therefore, we can raise the pruning rate for a larger threshold of \(|_{i_{}}^{t}|\) because \(|_{i_{}}^{t}|}_{j}^{t}\) at line 5 is easier to hold for a larger threshold. The next section introduces an efficient way to update \(|_{i_{}}^{t}|\) to a larger value before entering Algorithm 2.

```
1:Input:\(_{i_{}}^{t}\), \(}^{t}\), \(^{t}\)
2:Output:\(^{t}\), \(^{t+1}\)
3:\(^{t+1}^{t}\);
4:for\(j^{t}\)do
5:if\(|_{i_{}}^{t}|}_{j}^{t}\)then
6:\(_{j}^{t} 0\);
7:else
8: compute \(_{j}^{t}\);
9:if\(|_{i_{}}^{t}|<|_{j}^{t}|\)then
10:\(^{t+1}(^{t+1} i_{}) j\);
11:\(_{i_{}}^{t} 0\); find \(_{i_{}}^{t}\) in \(^{t+1}\);
12:else
13:\(_{j}^{t} 0\); ```

**Algorithm 2** Update of candidate set

### Lower Bound and Update of Threshold

To update threshold \(|_{i_{}}^{t}|\) to a larger value, we utilize the lower bound \(_{j}^{t}\) such that \(|_{j}^{t}|>_{j}^{t}\) holds for \(j^{t}\). \(_{j}^{t}\) is defined as follows:

**Definition 3**: _Let \(t^{*}\) be \(1 t^{*}<t\) in Algorithm 1. Then, \(_{j}^{t}\) at the \(t\)-th iteration in Algorithm 1 is computed as follows:_

\[}_{j}^{t}=|_{j}^{t^{*}}+(^{} )_{j}|-|_{j}|_{2}|^{t}-^{t^{*}}|_{2},\] (5)

_where \(=-^{}\)._

The following lemma shows that \(}_{j}^{t}\) is the lower bound of \(|_{j}^{t}|\):

**Lemma 6** (Lower bound): _We have \(}_{j}^{t}|_{j}^{t}|\) when \(}_{j}^{t}\) is computed by Equation (5)._

Lemma 6 is derived from the reverse triangle inequality and the Cauchy-Schwarz inequality. Similarly to the computation cost of the upper bound, Equation (5) requires the following cost:

**Lemma 7** (Computation cost of lower bound): _The computation cost of Equation (5) for all \(j\{1,...,n\}\) at the \(t\)-th iteration is \((n)\) time given \(\), \(^{}\) and \(|_{j}^{t^{*}}+(^{})_{j}|\)._

To update \(|_{i_{}}^{t}|\), we utilize the following lemma:

**Lemma 8** (Indices required for candidate set): _Suppose that \(}^{t}\) is computed by using Equation (5), and the candidate set \(^{t}\) is initialized as described in Definition 2 at the beginning of the iteration for \(t>1\). Let \(_{i_{}}^{t}\) be \(_{i}^{t}\) having the minimum \(|_{i}^{t}|\) in all \(i^{t}\), and \(i_{}\) be the index. Then, if \(|_{i_{}}^{t}|<}_{j}^{t}\) holds for \(j^{t}\), \(j\) is included in \(^{t+1}\)._

The above lemma shows that we can identify indices included in \(^{t+1}\) without computing the gradients by using the lower bound \(}^{t}\). The update procedure of \(^{t}\) to \(^{t+1}\) is described in Algorithm 3. Since this \(^{t+1}\) is used as the initial candidate set of Algorithm 2, we represent \(^{t^{}}\) as \(^{t+1}\) in Algorithm 3 to avoid confusion. The algorithm copies \(^{t}\) to \(^{t^{}}\) at line 3. Line 5 checks whether \(|_{i_{}}^{t}|<_{j}^{t}\) holds or not. If the equation holds, the algorithm updates \(^{t^{}}\) to remove \(i_{}\) and include \(j\) (line 6). At this time, we also update \(_{j}^{t}\) and \(_{i_{}}^{t}\) to reflect the update of \(^{t^{}}\) (lines 7-9). \(_{i_{}}^{t^{}}\) and \(^{t^{}}\) (lines 10-11) are used as \(_{i_{}}^{t}\) and \(^{t}\) in Algorithm 2.

The important point of Algorithm 3 is that the absolute value of the output \(|_{i_{}}^{t^{}}|\) is equal to or larger than the initial \(|_{i_{}}^{t}|\) as follows:

**Lemma 9** (Threshold increase): _In Algorithm 3, \(|_{i_{}}^{t^{}}||_{i_{}}^{t}|\) holds._

From the above lemma, we can obtain large \(|_{i_{}}^{t^{}}|\) as the threshold \(|_{i_{}}^{t}|\) in Algorithm 2 by performing Algorithm 3 before entering Algorithm 2. As a result, we can expect Algorithm 2 to increase the pruning rate.

The computation cost of Algorithm 3 is as follows:

**Lemma 10** (Computation cost of threshold increase): _Let \(l\) be the number of indices that are determined to be included in \(^{t^{}}\) at line 5 of Algorithm 3. If \(\) and \(^{}\) are precomputed, Algorithm 3 requires \((ln)\) time and the worst time complexity of Algorithm 3 is \((n^{2})\) time._

Similarly to Lemma 5 of Algorithm 2, the worst time complexity is not much more than that of IHT.

### Algorithm

Algorithm 4 is the pseudocode of our method based on Algorithms 2 and 3. We first precompute \(\) and \(^{}\) for the upper and lower bounds (line 3). The main loop consists of two types of procedures (lines 4-22): the procedure for \(t=t^{*}\) (lines 5-11) and the procedure for \(t t^{*}\) (lines 12-21). For the case of \(t=t^{*}\), the algorithm updates the parameter vector the same as the plain IHT (lines 6-7) and computes the candidate set (line 8). We note that the computation result of line 6 is also used for computing the upper and lower bounds as shown in Definitions 1 and 3. Line 9 sets \(t^{*}\) as \(t\) and line 10 computes an interval \(r 1\) that determines which \(t\) is the next \(t^{*}\). Specifically, the next \(t^{*}\) is determined as \(t^{*}+r\). The computation way of \(r\) is described later. \(r^{}\) is the variable that monitors the interval. If \(t=t^{*}\) does not hold (line 12), it computes \(_{i}^{t}\) for \(i^{t}\) by using Equation (2) (line 13). Next, line 14 computes the lower bound by using Equation (5) on the basis of Lemma 6. Then, we find the initial threshold at line 15 and update it by using Algorithm 3 on the basis of Lemmas 8 and 9 (line 16). Then, we compute the upper bound by using Equation (4) on the basis of Lemma 1 (line 17). Line 18 computes \(^{t}\) by using Algorithm 2 while pruning unnecessary computations on the basis of Lemma 3. This \(^{t}\) can be handled as \(^{t+1}\) by following Lemma 4 (line 19). We monitor the interval of the next \(t^{*}\) at line 20 through \(r^{}\). The algorithm repeats the above procedure until a stopping criterion is met (line 22). An example of a stopping criterion is relative tolerance .

**Automatic determination of \(t^{*}\) via \(r\).** In Algorithm 4, we need to compute interval \(r\) at line 10 to determine \(t^{*}\) because \(^{t^{*}}\) appears in Equations (4) and (5) for computing the upper and lower bounds, respectively. Since the upper bound \(}_{j}^{t}\) and lower bound \(}_{j}^{t}\) are the approximation of \(_{j}^{t}\), we obtain the following error bound:

**Lemma 11**: _Suppose that \(_{j}\) is computed as follows:_

\[_{j}=2\|_{j}\|_{2}\|^{t}-^{t^{*}}\|_{2}.\] (6)

_Then, \(|}_{j}^{t}-_{j}^{t}|_{j}\) and \(|}_{j}^{t}-_{j}^{t}|_{j}\) hold._

From the above lemma, the magnitude of error bound \(_{j}\) depends on \(t^{*}\) because of \(|^{t}-^{t^{*}}\|_{2}\) in Equation (6). If we set \(r\) to a large value, the error bound can be large since \(\|^{t}-^{t^{*}}\|_{2}\) tends to be large. In this case, the bounds are loose and it is difficult to hold \(|_{i_{}}^{t}|}_{j}^{t}\) in Lemma 3. As a result, the pruning rate will be low. On the other hand, if we set \(r\) a small value, the algorithm frequently computes lines 5-11 although we can obtain tight bounds. Since line 6 requires \((n^{2})\) time, the reduction of the computation cost will be small. To solve the above problem, we automatically determine \(r\) on the basis of the current pruning rate that is defined as follows:

**Definition 4** (Pruning rate): _Let \(u_{t}\) be the number of un-pruned computations at line 7 in Algorithm 2 for the \(t\)-th iteration as defined in Lemma 5. Then, we define pruning rate \(p_{t}\) at line 10 in Algorithm 4 for the \(t\)-th iteration as follows:_

\[p_{t}=}{n-k} 100.0.\] (7)

The unit of \(p_{t}\) is percent. We compute \(r\) at line 10 in Algorithm 4 on the basis of \(p_{t}\) as follows:

\[r=\{r+1&\;p_{t} 50.0\\  r/2&\;p_{t}<50.0.\] (8)

\(\) is the ceiling function. If the algorithm could prune half of the computations at the previous iteration, it increases interval \(r\). If not, we update \(r\) to \( r/2\) to reduce error bound \(_{j}\). Therefore, if the pruning rate is high, we can reduce the number of computations at line 6 by increasing the interval. If not, the interval becomes small and the upper and lower bounds are expected to be tight. This rule performs well in our experiments.

```
1:Input: sparsity level \(k\), step size \(\)
2:Initialization:\(^{1}\), \(t 1\), \(t^{*} 1\), \(r 0\), \(r^{} 0\)
3:computing \(\) and \(^{}\);\(\) The precomputation for the upper and lower bounds
4:repeat\(\) The main loop
5:if\(r^{}=0\)then\(\) The precomputation for the upper and lower bounds
6:\(^{t}^{t}+^{}\); \(\) Computing \(^{t}+^{}\) used for \(}^{t}\) and \(}^{t}\)
7:\(^{t+1} H_{k}(^{t})\); \(\) Updating the parameter
8:\(^{t}(^{t+1})\); \(\) Updating the candidate set
9:\(t^{*} t\);
10: compute \(r\) on the basis of automatic determination and \(r^{} r\);
11:\(t t+1\);
12:else
13: compute \(_{i}^{t}\) for \(i^{t}\) by Eqn. (2);
14: compute \(}^{t}\) by Eqn. (5);\(\) Computing the lower bound on the basis of Lemma 6
15: find \(_{i_{}}^{t}\) in \(^{t}\);
16: update \(_{i_{}}^{t}\), \(^{t}\) and \(^{t}\) by Algorithm 3;\(\) Based on Lemmas 8 and 9
17: compute \(}^{t}\) by Eqn. (4);\(\) Computing the upper bound on the basis of Lemma 1
18: compute \(^{t}\) and \(^{t+1}\) by Algorithm 2;\(\) Based on Lemma 3
19:\(^{t+1}^{t}\); \(\) Updating the parameter on the basis of Lemma 4
20:\(r^{} r^{}-1\);
21:\(t t+1\);
22:until a stopping criterion is met ```

**Algorithm 4** Fast Iterative Hard Thresholding

### Analysis

The computation cost of Algorithm 4 is as follows:

**Theorem 1** (Computation cost): _Let \(u^{}\) and \(l^{}\) be the total numbers of \(u\) and \(l\) in Lemmas 5 and 10 in Algorithm 4, respectively. Suppose that \(r\) is a constant for simplicity, \(\) is the total number of main loops in Algorithm 4, and \(^{}\) is the number for which line 12 holds. Then, the computation cost of Algorithm 4 is \((n^{2}(m+)+n(l^{}+u^{}+^{}k))\) time, and the worst time complexity is \((n^{2}(m+))\) time._

The plain IHT of Algorithm 1 requires \((n^{2}(m+))\) time if \(\) and \(^{}\) are precomputed. Therefore, the worst time complexity of our method is the same as the cost of the plain IHT.

For the optimization result, we obtain the following theorem:

**Theorem 2** (Optimization result): _Suppose that Algorithm 4 has the same hyperparameters as those of the plain IHT of Algorithm 1. Then, Algorithm 4 yields the same parameter vector and objective value as Algorithm 1._

Theorem 2 guarantees accuracy of our method. Our method prunes unnecessary computations without degrading accuracy.

For the upper and lower bounds, the following property holds:

**Theorem 3** (Convergence of upper and lower bounds): _Suppose that Algorithm 4 converges as \(^{t}=^{t^{*}}\). Then, we obtain \(_{j}=0\) for \(j\{1,...,n\}\) where \(_{j}\) is the error bound of the upper and lower bounds defined in Lemma 11._

Theorem 3 shows that the upper bound \(_{j}^{t}\) and lower bound \(_{j}^{t}\) become the exact value of \(_{j}^{t}\) when \(^{t}=^{t^{*}}\) holds. Therefore, our method accurately prunes unnecessary computations when the condition is satisfied.

## 5 Experiment

We evaluated the processing time and accuracy of our method on feature selection tasks. We performed experiments on five datasets from the LIBSVM  and OpenML : _gisette_, _robert_, _ledgar_, _real-sim_, and _epsilon_. The sizes of the input matrices are \(6000 5000\), \(10000 7200\), \(60000 19996\), \(72309 20958\), and \(400000 2000\), respectively. We evaluated the processing time and accuracy on \(k=\{1,5,10,20,40,80,160,320,640,1280\}\). We compared our method with the plain IHT (IHT), Regularized IHT (RegIHT) , and Accelerated IHT (AccIHT) . RegIHT is the fastest method among the methods using an adaptive regularization technique . Since RegIHT has the hyperparameter of weight step size \(c\), we tried the setting of \(c=\{k,k/10,k/100\}\) on the basis of the original paper. AccIHT improves the convergence of IHT by utilizing the momentum. We tried \(=\{0.025,0.25,2.5\}\) for the momentum step size \(\) where the value of \(0.25\) is the one recommended in the original paper. We set step sizes of all the methods \(=1/\) where \(\) is the largest eigen value of \(^{}\) by following . We stopped these methods when the relative tolerance of the parameter vector dropped below \(10^{-5}\)[23; 15]. All the experiments were conducted on a 3.20 GHz Intel CPU with six cores and 64 GB of main memory.

### Processing Time

Figure 1 (a)-(e) compare the processing times on logarithmic scale. Our method was up to 73 times faster than IHT and outperformed all the baselines in all the settings. Because our method is based on the pruning, it achieved a large speedup factor for smaller \(k\). Even when \(k\) increased, the processing time was significantly shorter than the baselines.

We note that our method does not need hyperparameter tuning due to using the automatic determination technique described in Section 4.4. Specifically, practitioners only need to specify \(k\) to use our method the same as the plain IHT. On the other hand, the baselines of RegIHT and AccIHT require additional hyperparameter tuning for the weight step size and the momentum step size, respectively.

**Number of Gradient Computations.** Figure 1 (f) compares the number of gradient computations between our method and the plain IHT on the gisette dataset. Our method reduced the number of computations by up to \(98.87\%\). The result shows the effectiveness of our pruning strategy and supports the reduction in processing times of Figure 1 (a)-(e).

**Processing Time with Large Step Size.** Since the error bound of upper and lower bounds depends on \(\|^{t}-^{t^{*}}\|_{2}\) in Equation (6), a large step size may incur a potential decrease in the pruning rate and our method may be slow down. To address this concern, we conducted an experiment to evaluate the processing times with an increased step size. We increased the step size to \(10\) times larger than that used in Figure 1 (a) on the gisette dataset. Figure 1 (g) shows the results, exhibit a similar trend to Figure 1 (a). Our method was able to speed up IHT even with the larger step size. This success is due to our automatic determination of \(t^{*}\), which adjusts the pruning rate during optimization, as described in Definition 4 and Equation (8). In contrast, AccIHT failed to converge in some cases due to the momentum, preventing us from evaluating the processing times for those instances. WhileAccIHT reduces the number of iterations by using the momentum, our method reduces the cost per iteration on the basis of the pruning strategy. This result shows an advantage of the pruning approach.

### Accuracy

Theorem 2 guarantees that our method achieves the same results as the plain IHT. To verify the theorem, we compared the objective values between our method and the plain IHT. Table 1 shows the results for \(k=\{1,20,160,1280\}\), and our method achieved the same objective values as the plain IHT. We obtained the same trend results as above in the other settings of \(k\). We note that our method also yielded the same support of nonzero elements in the parameter vector and their coefficients as the plain IHT though the results are omitted. These results support our theoretical result for Theorem 2. In addition, our method also ensures that the parameter vector of each iteration matches perfectly

Figure 1: (a)–(e): Comparisons of log processing times for each dataset and \(k\). (f): Comparison of number of gradient computations on gisette. (g): Comparisons of log processing times for gisette with a large step size. Some results of AccIHT are omitted since they could not converge.

with that of the plain IHT from Lemma 4. This property is not obtained in previous acceleration methods based on the momentum such as AcclHT.

## 6 Conclusion

We accelerated iterative hard thresholding (IHT) by safely pruning unnecessary gradient computations. The main idea is to efficiently maintain the candidate set, which contains indices of nonzero elements in the parameter vector, during optimization. Before computing the gradients for each iteration, we prune unnecessary elements for the candidate set by utilizing the upper bound. To raise the pruning rate, we update the threshold to determine whether an element is included or not in the candidate set by using the lower bound. Our method guarantees the same optimization results as the plain IHT because our pruning is safe. In addition, it does not need additional hyperparameter tuning. Experiments show that our method is up to 73 times faster than IHT without degrading accuracy. As future work, we plan to extend our method to general convex loss functions with sparsity-inducing norms to enhance more applications.