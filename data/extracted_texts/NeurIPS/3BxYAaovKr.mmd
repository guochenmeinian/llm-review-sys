# Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities

Yale Song Eugene Byrne Tushar Nagarajan Huiyu Wang Miguel Martin Lorenzo Torresani

Fundamental AI Research (FAIR), Meta

https://github.com/facebookresearch/ego4d-goalstep

###### Abstract

Human activities are goal-oriented and hierarchical, comprising primary goals at the top level, sequences of steps and substeps in the middle, and atomic actions at the lowest level. Recognizing human activities thus requires relating atomic actions and steps to their functional objectives (what the actions contribute to) and modeling their sequential and hierarchical dependencies towards achieving the goals. Current activity recognition research has primarily focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step segments (430 hours) and high-level goal annotations for 2,807 hours of Ego4D videos. Compared to existing procedural video datasets, it is substantially larger in size, contains hierarchical action labels (goals - steps - substeps), and provides goal-oriented auxiliary information including natural language summary description, step completion status, and step-to-goal relevance information. We take a data-driven approach to build our taxonomy, resulting in dense step annotations that do not suffer from poor label-data alignment issues resulting from a taxonomy defined a priori. Through comprehensive evaluations and analyses, we demonstrate how Ego4D Goal-Step supports exploring various questions in procedural activity understanding, including goal inference, step prediction, hierarchical relation learning, and long-term temporal modeling.

## 1 Introduction

Recognizing complex patterns of human activities has been the subject of extensive research in computer vision and the broader AI community . However, progress has been relatively slow compared to object and scene understanding . One of the main obstacles has been the scarcity of large-scale datasets annotated with a comprehensive taxonomy representing complex human activities. While object recognition benefits from WordNet  that provides an extensive taxonomy of objects found in everyday scenarios, activity recognition is presented with unique difficulties because there is currently no established taxonomy in place that encompasses the broadly varying granularities of activities, from atomic actions (e.g., pick-up cup, sit down) to procedural sequences (e.g., make lasagna).

In our quest to build a new dataset for human activity recognition, we draw inspiration from the psychology literature. Studies have shown the inherent _hierarchical_ nature of human behavior , comprising the primary **goals** at the highest level, intermediate **steps** and their **substeps** in the middle, and **atomic actions** at the lowest level. Social cognitive theories  suggest that this hierarchy is formed by human agents deliberately setting goals, anticipating potential consequences of differentactions, and planning out a sequence of steps and their substeps to achieve the desired goal in a hierarchical manner. Although the planned sequence of actions may not necessarily align with the actual execution order , inferring and reasoning over the hierarchical representations has been shown to be crucial for understanding human behavior [12; 41].

Most existing activity datasets have focused on the lowest levels of this hierarchy - i.e., atomic or low-level actions - often in trimmed videos and with annotations spanning only a few seconds [25; 46; 8; 23]. This focus on atomic actions has even raised questions about the necessity of temporal modeling in existing video tasks [40; 11; 44], and its suitability for studying real-world videos containing higher-level activities over longer temporal extents.

In response to this, _procedural activities_ - those that involve performing a series of _steps_ to achieve predetermined _goals_ - have recently gained particular attention [29; 60; 49; 61; 48; 34; 4; 43; 55]. Recognizing goal-oriented steps that unfold over a long time horizon requires modeling long-term temporal context, making it a challenging long-form video understanding task. However, existing datasets are either small-scale [29; 3; 43], do not model high-level goals, or ignore the hierarchical relationship between steps [49; 61; 4]. Furthermore, the step taxonomy is commonly built from external sources detached from videos (e.g., text articles from wikiHow [60; 49; 61]), resulting in misalignment between the constructed label space and the observed data. Consequently, a significant portion of the video is left unlabeled, offering an incomplete record of activities.

To address these limitations, we introduce **Ego4D Goal-Step**, a new set of annotations on Ego4D  with a newly developed hierarchical taxonomy for procedural activities. It contains two main components: (1) The goal annotation set consists of 7,353 videos, totaling 2,807 hours, labeled with a taxonomy of 319 goals grouped into 34 scenarios. This is a focused set covering the 72% of Ego4D videos and is intended to provide a large-scale training and evaluation dataset for goal inference. (2) The step annotation set focuses on the cooking scenario portion of Ego4D, and is intended specifically for procedural activity recognition. It consists of 47,721 densely labeled step and substep segments, amounting to 430 hours in total, annotated based on a taxonomy of 86 goals and 514 fine-grained steps commonly performed in diverse home kitchen environments across many countries.

Ego4D Goal-Step stands out from existing procedural video datasets for its several unique features. **(1)** We develop our taxonomy in a data-driven manner to accurately represent the activities, rather than

Figure 1: **Ego4D Goal-Step** offers hierarchical procedural activity labels with three distinct levels: goal - step - substep. Each annotation comes with a time interval, categorical label, and natural language description. It also provides auxiliary information including step summaries, task completion status (is_continued), task relevance (is_relevant), and procedural activity indicator (is_procedural).

resorting to external resources such as wikiHow. As a result, the average density of annotated segments per video is 77%, i.e., 2-5\(\) higher than existing datasets that rely on external taxonomies . **(2)** The annotations form a three-level hierarchy (goal - steps - substeps), a unique feature that is unavailable in most existing datasets. When combined with existing Ego4D labels that are action-centric (e.g., narrations, FHO, and moments), it creates an attractive multi-level hierarchical label space. **(3)** Every annotation includes a time interval, a categorical label, and a natural language description, enabling both detection and language grounding tasks. **(4)** Procedural segments also come with step summary descriptions, useful for video summarization . **(5)** Additional goal-oriented information, such as task relevance and completion status signals, supports novel research directions such as task graph inference and progress tracking . **(6)** Our dataset inherits the large scale and diversity of Ego4D, capturing immersive views of procedural activities from a first-person perspective, and featuring long untrimmed videos that reveal the unfolding of individual goals over time. Together, these strengths make Ego4D Goal-Step a significant step forward in procedural activity understanding. See Figure 1 for an illustration of these features.

In summary, we introduce Ego4D Goal-Step, the largest available egocentric dataset for procedural activity understanding, with 2,807 hours of videos with specific goal labels and 430 hours of segments with fine-grained step/substep labels. In what follows, we describe our annotation and taxonomy development process. We also provide a comprehensive analysis of new annotations and compare them with existing procedural video datasets. Finally, we demonstrate the value of our annotations for temporal detection and grounding tasks, and analyze the results in the context of the unique properties of our dataset.

## 2 Related Work

Activity recognition has a two-decade history in computer vision. Early works have tackled atomic action classification using datasets of seconds-long video clips at relatively small scales . Following the success of deep neural networks, several datasets have focused on scaling up by leveraging online videos . Recognizing the need for long-form video modeling, several datasets have also been proposed for action detection in untrimmed videos .

Recently, the community has expanded the scope by developing datasets for procedural activities. The typical dataset construction process involves selecting procedural tasks beforehand, e.g., various recipes in cooking, then collecting data for the pre-selected tasks through participant recordings or by mining online video repositories. Participant-recorded datasets like Breakfast  and Assembly101  benefit from a controlled collection setting, enabling the development of a taxonomy aligned with the data, and resulting in dense annotations and hierarchical step segments similar to ours. However, they capture limited diversity (e.g., 10 cooking recipes) and are smaller in scale.

  & &  &  &  \\  &  & Count & Duration & Count & Duration & Density & \# Goals & \# Steps \\  & & (total) & (total / avg) & (total / avg) & (total / avg) & (Fg ratio) & & \\  EgoProcel.  & 329 & 62h / 13m & 1K / 8.7 & 24h / 16.3s & 0.38 & 16 & 139 \\ YouCook2  & 2,000 & 176h / 5m & 15K / 7.7 & 84h / 19.6s & 0.48 & 89 & n/a \\ CrossTank  & 2,750 & 375h / 5m & 21K / 7.6 & 56h / 9.6s & 0.15 & 18 & 133 \\ COIN  & 11,827 & 468h / 2m & 46K / 3.9 & 192h / 14.9s & 0.41 & 180 & 778 \\  Breakfast  & ✓ & 2,193 & 85h / 2m & 62K / 28 & 108h / 6.3s & 0.88 & 10 & 225 \\ Assembly101  & ✓ & 362 & 43h / 7m & 83K / 237 & 156h / 1.7s & 0.81 & 101 & 1,582 \\
**Ego4D Goal-Step** & ✓ & 851 & 368h / 26m & 48K / 56 & 430h / 32.5s & 0.77 & 86 & 514 \\ 
**Ego4D Goal-Step** & (goal labels) & & 7,353 & 2,807h / 23m & 7546 / 1 & 2,470h / 19.6m & 0.88 & 319 & - \\ 

Table 1: **Dataset statistics.** We report “Ego4D Goal-Step” — the subset that comes with hierarchical step labels, and “Ego4D Goal-Step (goal labels)” — the full set which includes videos with goal labels but no step labels. “Hier” indicates datasets with hierarchical label spaces. Breakfast  and Assembly101  provide both coarse- and fine-level segments, analogous to our step and substep segments; we report their combined numbers. Assembly101  provides 12 synchronized views per recording; we report single view statistics to make the numbers compatible with other datasets.

On the other hand, Cross-Task  and COIN  are Internet-mined datasets that benefit from the scalability. However, they rely on external resources to develop a taxonomy (e.g., wikiHow), leading to label spaces that do not capture precisely and comprehensively the activities represented in the videos. Consequently, a large portion of videos remains unlabeled, with densities of labeled segments in the low 40% (see Table 1). Furthermore, annotated segments often only partially match with step labels (e.g., due to subtle variations in objects used, step ordering, etc.), resulting in weakly-labeled data. These datasets are also often non-hierarchical and represent steps at a single level.

Compared to existing procedural video datasets, Ego4D Goal-Step provides dense annotations at scale, a well-aligned taxonomy, and a hierarchical label space - all at the same time - thanks to our hierarchical partitioning  approach to data annotation and taxonomy development. With a wide range of procedural videos sourced from Ego4D, accompanied by dense annotations with a comprehensive taxonomy, our dataset enables procedural activity understanding at scale.

## 3 Ego4D Goal-Step

### Ego4D: The Status Quo

Ego4D is annotated in a variety of ways. All videos are annotated with scenarios and narrations which provide high-level and low-level descriptions of actions, respectively. Scenarios provide coarse categorization of activities (e.g., construction, arts and crafts), while narrations describe a camera-wearer's action at a specific time. The narrations are interaction-centric, focusing on individual, _atomic_ actions that a camera-wearer performs over a short time period. For example "C picks up the spoon," "C pets the dog," "C unscrews the screw," where C represents the camera-wearer.

While the narrations offer valuable information to understand simple movements and hand-object interactions, in the broader context of activity understanding, they are limited. Human actions are not performed arbitrarily -- they are _intentional_ and are done to accomplish a particular _goal_. For example, C picks up the spoon _to add sugar to coffee_; C unscrews the screw _to detach a bicycle wheel_. These goals are left hidden in existing narrations. Moreover, these goals themselves are part of more structured activities. "Adding sugar" is a step in the process of making coffee, and "Detaching the bicycle wheel" is a step towards replacing a punctured tire tube. While narrations explicitly capture what is immediately happening, they do not reveal _why_, or more broadly, to what end.

Narrations form the scaffolding for various other annotations on smaller subsets of Ego4D. Forecasting hands and objects (FHO) annotations involve atomic actions parsed into simpler (verb, noun) tuples. For example, the long-term forecasting task involves predicting the sequence of future atomic actions, without capturing the overarching goal. Moments query annotations go one step higher, representing composite actions that involve sequences of atomic actions like "wash dishes in the sink" or "put on safety equipment." While they are higher-level, they are still short-term activities and are not connected by their long-term task structure. Moreover, they cover a small set of categories (roughly 100), span an inconsistent set of granularities (e.g., the atomic action "cut dough", and the high level "operate the dough mixing machine"), and are not intended to cover complete activities/goals.

Collectively, these annotations inherit the narrow scope of narrations and offer only a short-term understanding of human activity, limiting their value for procedural activity understanding in intentional, long-form and structured human activity.

### Ego4D Goal-Step: Annotation and Taxonomy Development

To address this gap, we annotate Ego4D for procedural video understanding. Ego4D videos are collected without prearranged scripts or step-by-step instructions. As a consequence, the complete set of activities present in the dataset is unknown and a taxonomy cannot be established beforehand. We overcome this in a data-driven manner, using a hierarchical partitioning approach for annotation and taxonomy development. In short, we first ask annotators to identify the primary goals depicted in each video. Next, they delve into each goal segment to identify the individual steps and their corresponding action sequences. Then, again, annotators recursively analyze each action segment to further annotate steps at lower levels to construct a complete step hierarchy. Throughout this process, we present the annotators with an incomplete and evolving taxonomy and encourage them to suggest missing categories. We review them periodically and update our taxonomy over the course of annotation. The whole process involves five stages:

Stage 1: Goal taxonomy initializationWe initialize a goal taxonomy with Ego4D scenario labels and manually subdivide them into specific goal categories. For example, we expand the "cooking" scenario into popular dishes (e.g., "make pasta," "make omelet") and the "construction site" scenario into specific construction tasks (e.g., "paint drywall," "build wooden deck").

Stage 2: Goal annotation and taxonomy updateAnnotators watch full-length videos (not clips cut to shorter lengths) and identify distinctive goal segments, assigning a goal category and providing free-form text description, e.g., make_omelet and "making omelet with toasted bread" as shown in Figure 1. We emphasize the importance of providing full-length videos to the annotators because short-length clips will inherently lack the overall goal of actions (e.g., a clip capturing boiling salted water will not give any clue that it was part of making pasta). For missing categories, annotators choose "other" and describe it in text. Auxiliary information, including procedural activity indicator (is_procedural), task continuity from previous segments (is_continued), and bullet-point summaries of procedural steps, are also annotated.

While the annotation is in progress, we iteratively refine our goal taxonomy. This process is largely manual. It involves mapping keywords in descriptions to existing goal categories and adding new categories to the taxonomy when necessary. We also manually verify the correctness of keyword mapping by visual inspection. This process is done periodically in batches of annotations.

Stage 3: Step taxonomy initializationWe prompt a large language model, LLaMA-7B , to generate step-by-step instructions for each goal. This provides a concise, but potentially incomplete, list of step candidates per goal category. Moreover, it often misses routine steps performed in a given environment, e.g., washing hands in a home kitchen. To represent missing steps, we create a "catch-all" bucket for each scenario and use the same model to generate commonly occurring steps.

Stage 4: Step annotation and taxonomy updateAnnotators review full-length videos with goal annotations and identify step segments, assigning a step category and natural language description. To ensure annotators capture step-level granularity that reveals _intentions_ and not low-level physical movements, we provide a specific template, _"the camera wearer did X in order to Y"_, and ask them to prioritize intentions (Y) over physical movements (X) in their annotations. As shown in Figure 1, this allows us to collect labels that reveal the functional objectives behind actions (e.g., "toast bread," "crack eggs," "beat eggs with chopsticks") rather than just the description of low-level actions.

Similar to stage 2, annotators indicate the procedural nature of step segments and describe substeps in bullet-points. Additionally, they provide task-relevance information (is_relevant) for each step segment using a multiple-choice question ("essential," "optional," and "irrelevant"). They are further requested to find a relevant wikiHow article and provide its URL, for annotation accuracy and educational resources. The taxonomy update follows the same process as stage 2.

Stage 5: Substep annotation and taxonomy updateFinally, we ask annotators to further partition step segments into individual substeps. This process is largely the same as stage 4, and it shares the same step taxonomy. The only difference is that we provide annotators with both the corresponding goal and step annotations, and that we encourage annotators to focus on lower levels of granularity.

### Dataset Analysis

StatisticsTable 1 compares Ego4D Goal-Step with existing procedural activity datasets. As shown, Ego4D Goal-Step represents the largest available procedural video dataset in terms of total number of hours annotated with step labels. Goal annotations are available for a subset covering 72% of Ego4D after discarding non-procedural or uninteresting videos, totaling 2,807 hours of videos. At the segment level, there are a total of 47,721 segments for steps and substeps combined. On average, each goal segment has 23.82 step segments, and each step segment has 4.6 substep segments.

AnnotationWe contracted a third party vendor to manage the annotation process, and checked privacy and ethical compliance through rigorous reviews. We completed 10 iterations for goal annotation (stage 2), 10 iterations for step annotation (stage 4), and 8 iterations for substep annotation (stage 5). Each iteration involved annotators providing labels for a batch of videos, us reviewing them, and updating the taxonomy. On average, annotators took 2\(\) the video duration to annotate goals, and 7.5\(\) to annotate steps/substeps because step annotation required fine-grained inspection. The total annotation time amounted to roughly 10,000 worker hours.

GranularityFigure 2 illustrates the distribution of goal, step, and substep segment durations. On average, goal segments span 19.64 minutes, while step and substep segments last 50.03 seconds and 19.49 seconds, respectively. The average substep duration aligns with datasets that derive their taxonomy from wikiHow , which suggests that our data-driven taxonomy has granularity similar to that found in annotations based on fixed step-by-step instructions. The average duration of our steps/substeps is 32.5 seconds, which is 6 times longer than Breakfast  and 22 times longer than Assembly101 . This shows that most of the step/substep segments capture longer-duration actions than  without sacrificing annotation density or capturing short-term atomic actions.

TaxonomyOur taxonomy contains 319 goal categories grouped into 34 scenarios. Among these, the cooking scenario provides procedural step annotations, comprising 86 goal categories and 514 step categories. We use these step categories from the taxonomy to annotate both step and substep segments. The dataset exhibits a long-tail distribution, with 125 goal categories representing 90% of labeled goal segments and 209 step categories covering 90% of labeled step/substep segments.

LabelsEgo4D Goal-Step comes with various goal-oriented activity labels. About 92% of the goal segments are annotated with descriptions (3.03 words per sentence) and 63% with summaries (5.43 sentences per summary, 4.92 words per sentence). 100% of the step and substep segments come with descriptions (4.44 words per sentence) and 15% of them include summaries (4.38 sentences per summary, 3.1 words per sentence). Figure 2 shows the distributions of auxiliary labels.

SplitsWe provide data splits for training (70%), validation (15%), and testing (15%) purposes. The splits were made at the video level, ensuring balanced inclusion of step categories across the splits. We release full annotations for the training and validation splits while withholding the test split.

## 4 Experiments

### Tasks

Goal/step localizationLocalizing temporal action segments within a long, untrimmed video has numerous applications ranging from activity recognition to automatic chapterization for efficient navigation of instructional videos. Following temporal action localization , we formulate the task as predicting tuples of (start_time, end_time, class_category) encompassing goals and individual steps given a long untrimmed video. We use ActionFormer  and EgoOnly  as our baseline models for their superior performance demonstrated in various action localization benchmarks. We evaluate the results using the detection mAP averaged over goal _or_ step categories, and over temporal IoUs {0.1, 0.2, 0.3, 0.4, 0.5}.

Online goal/step detectionWhile the localization task above focuses on _offline_ inference scenarios, here we consider its _online_ counterpart. The task is formulated as predicting goal/step categories at each timestamp as they unfold in a streaming input video. Models must utilize information from the

Figure 2: **Dataset statistics illustrating the distributions of segment durations (x-axis scaled to the square root for better display) and auxiliary labels across different levels of hierarchy.**

past leading up to the current timestamp, without having access to future frames. This task holds particular significance in egocentric vision due to its practical applications in AR scenarios such as real-time step-by-step guidance for procedural activities. We adopt LSTR  and EgoOnly  as our baseline models, given their strong performance and the availability of open-source implementations. Following existing literature, we report per-frame mAP.

Step groundingUnlike the localization and detection tasks that assume a fixed set of categories, some scenarios require models that can recognize new, unseen steps from open vocabulary text descriptions. Thus, we study step grounding, where given a long, untrimmed egocentric video and a natural language description of a step (e.g., "beat eggs with chopsticks" in Figure 1), a model must predict its temporal extent (start_time, end_time) in the video (i.e., the green bar). We adopt VSLNet , a popular video query grounding model due to its strong performance on similar egocentric grounding tasks. We report Recall@1, IoU=0.3 following prior work . Note that we do not consider goal grounding in this work since many videos contain a single goal spanning most of a video, for which recall measures are trivially high.

Implementation detailsFor all experiments except for the EgoOnly  baseline, we use pre-computed clip-level features extracted densely from each video using Omnivore , which are publicly available for download on the official Ego4D repository (we used "omnivore_video_swill"). The Omnivore model has been pretrained on a combination of multiple modalities (images, videos, and 3D data) in a supervised fashion and has been demonstrated to achieve strong generalization ability across a wide variety of vision tasks. EgoOnly  pretrains the ViT  backbone from scratch on the raw frames of Ego4D  using the MAE  objective, then further finetunes it on a combination of four existing action recognition datasets (Kinetics-600 , Ego4D Moments , EPIC-Kitchens-100 , COIN ) in a supervised fashion. For EgoOnly on the online detection task, we attach a single-layer linear prediction head on top of the pretrained ViT backbone and train the entire model end-to-end. Each prediction is made on input frames with 2 second temporal context. For EgoOnly on the offline localization task, we take the ViT backbone finetuned on the online detection task and attach the ActionFormer head  on top, and train just the prediction head while keeping the ViT backbone frozen throughout training. For all tasks, we use open source baseline implementations1 and adapt hyperparameters for the proposed dataset (details in Supp.).

### Main results

Table 2 reports the main results for all the tasks on both the validation and test sets. EgoOnly  achieves strong performance on offline step localization and online step detection, demonstrating the effectiveness of its egocentric pretraining strategy and corroborating the strong empirical evidence presented in their paper. Note that, compared to ActionFormer  and LSTR  that leverage precomputed features from Omnivore , EgoOnly finetunes its ViT backbone directly on our dataset by solving the temporal segmentation task, providing further performance improvement over the two other baselines.

   Task & Metric & Model & Val & Test \\  Goal localization & Detection mAP & ActionFormer  & 42.2 \(\) 3.8 & 45.9 \(\) 2.4 \\  Step localization & Detection mAP & ActionFormer  & 9.9 \(\) 0.3 & 8.7 \(\) 0.3 \\  & & EgoOnly  & 13.1 \(\) 0.3 & 14.0 \(\) 0.4 \\  Online goal detection & Per-frame mAP & LSTR  & 21.5 \(\) 0.5 & 24.5 \(\) 1.3 \\  Online step detection & Per-frame mAP & LSTR  & 8.7 \(\) 0.2 & 7.9 \(\) 0.2 \\  & & EgoOnly  & 10.2 \(\) 0.1 & 10.8 \(\) 0.1 \\  Step grounding & Recall@1, mIoU=0.3 & VSLNet  & 11.7 \(\) 0.2 & 10.7 \(\) 0.3 \\   

Table 2: **Main results** on tasks supported by our dataset. We report standard deviation over 8 runs.

Compared to the step prediction tasks, goal prediction requires a much longer temporal context. For instance, step segments have an average duration of 32.5 seconds, whereas goal segments have an average duration of 1946.9 seconds. This makes it a challenging long-form video understanding task. We report ActionFormer and LSTR baseline results but omit EgoOnly results due to its focus on relatively shorter-term temporal context. Specifically, EgoOnly's ViT backbone is fine-tuned with 2-second clips from our dataset, providing limited long-term context to the learned representations and, as a result, achieving inferior results compared to baselines that leverage Omnivore features. This highlights the challenging nature of our dataset and warrants further investigation into refinements in training and modeling approaches to achieve a more balanced performance across goal and step prediction tasks.

On the step grounding task, models achieve roughly similar recall scores on the validation and test splits. Note that a subset of text queries in each split belong to _step classes_ that are not seen during training. For example, the step category label "Add the melted chocolate and blend until smooth" does not occur in the training set, but its corresponding natural language description "Add chocolate balls to blender jar" is still groundable. The results on this _zero-shot_ subset are understandably lower - 4.3 and 3.8 Recall@1, mIoU=0.3 on validation and test splits, respectively.

### How do models exploit the hierarchical goal-step-substep relationship?

Next, we study the hierarchical relationship among goals, steps, and substeps in our dataset. First, we explore the goal-step hierarchy by comparing models trained on goals only, steps and substeps only, and all combined. Table 3 (left) shows that simply combining all instances does not help the individual tasks of localization and online detection, likely due to the non-overlapping taxonomy, the large difference in segment lengths, as well as limited number of goal samples.

However, once we switch to step-substep hierarchy, in Table 3 (right), we find that jointly training on steps and substeps (last row) is superior to models trained on substeps only (A) or steps only (B). This joint training is significantly more effective than pairing steps with their parent descriptions (C).2

Importantly, the gain over (A) and (B) is not simply an effect of dataset size. In (D-E), we replace steps/substeps with an equivalent amount of Ego4D narration data from the same videos to match the total training set size of the result in the last row. The narrations capture low-level actions performed by the camera-wearer (e.g., "camera-wearer picks up a pan") and have been shown to improve performance in egocentric grounding models . Our results show that while adding narrations can offer a small improvement, jointly training on steps and substeps remains a superior strategy.

Put together, the results suggest that jointly training with steps and substeps leads to models that are aware of the sequential and hierarchical relationships between steps, leading to stronger performance across both levels.

   Train on & Avg. & Localization & Online detect. \\  & Length & Goal & SSteps & Goal & SSteps \\  Goals  & 1500 s & **42.2** & - & **21.5** & - \\ SSteps  & 32 s & - & **9.9** & - & **8.7** \\  Goals + SSteps  & 59 s & 37.1 & 9.7 & 21.2 & 8.1 \\    \\  \\ 

Table 3: **Validation set results on hierarchical goal-step relationship. Left: localization and online detection. Combining goal and step/substep instances does not help each other, likely due to the large difference in segment lengths and the limited number of goal samples. Metric: detection/per-frame mAP. Right: grounding. Training jointly on steps and substeps outperforms other alternatives, highlighting the benefit of the hierarchical nature of our step annotations. Metric: Recall@1, IoU=0.3. These results suggest that exploiting the step-substep hierarchy is clearly beneficial (right table), while effectively leveraging the goal-step hierarchy needs further investigation (left table).**

### Which types of steps are easier to recognize?

We evaluate grounding models based on the role of steps in the overall procedure: essential, optional, or irrelevant. Essential steps are necessary for completing the task (e.g., cracking eggs for cooking an omelet), optional steps are relevant but not essential (e.g., adding sriracha saucce to an omelet), and irrelevant steps are unrelated (e.g., answering phone while cooking). Figure 3 shows that _essential_ steps are the most difficult to recognize. This is somewhat counterintuitive since they occur frequently within the task (as evidenced by the largest number of instances reported in the figure) and are contextual with the activity. However, we found that they are difficult to distinguish from other steps of the related tasks as they tend to involve the same set of objects, scenes, and actions, e.g., flatten dough on table, knead dough, coat dough with flour in the task "Make bread," requiring fine-grained recognition. Conversely, optional steps (e.g., dispose eggshells, wear apron) and irrelevant steps (e.g., use phone, drink tea) are visually distinct in the context of the procedure and thus are easier to recognize, overcoming the disadvantage of relatively low training data (number above each bar) and despite their loose (or missing) connection to the goal.

### Can long-term temporal context benefit procedural activity recognition?

Recognizing procedural activities in egocentric videos, especially in the online inference regime, requires long-term temporal context to accumulate enough past history . For example, recognizing different but similar recipes may become possible only after observing completion of certain steps.

We quantitatively study this property by exploiting the hierarchical structure in Ego4D Goal-Step. In the localization task, this is achieved by adjusting the feature sampling stride before feeding features into the ActionFormer  that localizes goals and/or steps. This changes the effective temporal context consumed, especially when the theoretical receptive field does not cover the full video. Results are visualized in Figure 4 (left) with error bars representing standard deviation of 8 runs. We find that a larger temporal context is indeed required to disambiguate goals, while steps favor a much smaller context for finer temporal details. Additionally, we find goal-step joint training with a small context adversely affects goal localization due to the dominance of shorter steps in the training signal. As a result, the step results with and without goal instances are almost overlapping.

Figure 4: **Left: Goal/Step localization with varying effective temporal context. Longer temporal context is required to disambiguate goals, while steps favor shorter context for finer temporal details. Joint training with shorter context adversely affects goal localization due to the dominance of step segments in the training signal. Middle: Online goal detection with varying LSTR memory length. For goals, a larger short term memory consistently leads to better results. Right: Online step detection with varying LSTR memory length. For steps, capturing fine-grained information via short-term temporal context is more important than capturing long-term context.**

Figure 3: **Grounding performance breakdown by relevance type. Essential steps are hardest to recognize as they tend to involve the same set of objects in related tasks (e.g., different stages of preparing the dough in various baking scenarios), requiring fine-grained recognition of objects, scenes, and actions. Optional and irrelevant steps, despite being rare during training and not as related to the activity, are visually distinct and easier to recognize. Numbers above bars denote the number of training samples in each category.**In online goal/step detection tasks, we follow the experimental setting of LSTR  that varies both short-term and long-term memory lengths. For online goal detection (Figure 4 middle), we find that increasing the long-term memory to 256 seconds and longer leads to noticeable gains. Furthermore, a larger short term memory consistently leads to better results. These findings demonstrate that predicting goals indeed requires long-term temporal context. This aligns with our intuition that disambiguating the goals requires piecing together evidence that needs to be accumulated over a long temporal span. For instance, in the case of preparing a recipe, it may only become fully recognizable after several ingredients have been used and multiple steps have been completed.

On the other hand, for online step detection (Figure 4 right), performance increases when we switch the short term memory length from 4 seconds to 8 seconds, but plateaus when we further push to 16 seconds. Also, longer long-term memory does not seem to improve performance; in fact, when a 4 second short-term memory is used, performance decreases when the long-term memory exceeds 64 seconds, which is twice as long as the average step segment duration of 32.5 seconds. This pattern is consistent with our observation on step localization, suggesting that step detection favors fine-grained information captured in short-term temporal context over longer context.

## 5 Conclusion

We presented Ego4D Goal-Step, a new set of annotations with a hierarchical taxonomy of procedural activity labels on the recently released Ego4D. It is larger than existing procedural video datasets, contains hierarchical action labels (goals - steps - substeps), and provides various auxiliary information useful for procedural activity understanding. We demonstrated three distinct task scenarios supported by our dataset - temporal localization, online detection, and grounding - and analyzed how various research questions can be explored such as hierarchical learning and long-term video modeling.

The comprehensive nature of our dataset calls for future investigations into other aspects of procedural activity understanding. One promising direction is incorporating existing Ego4D annotations for more comprehensive analyses of procedural activities. For instance, narrations and FHO/Moments provide atomic action labels that can be combined with our dataset to form a 4-level hierarchy (goals - steps - substeps - actions). Furthermore, hand & object interaction annotations provide object state information that can enable object state-based progress monitoring. We eagerly anticipate the emergence of active research threads pursuing these directions.

Limitations and societal impactWe acknowledge that Ego4D Goal-Step is intended for research purposes and should not be regarded as a comprehensive dataset encompassing the full range of daily human activities. Models trained on our dataset may exhibit biases towards the specific activities included in the dataset, resulting in a limited coverage of our everyday living scenarios.