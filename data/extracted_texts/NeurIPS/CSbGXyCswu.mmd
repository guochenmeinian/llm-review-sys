# Fine-Grained Human Feedback Gives Better Rewards for Language Model Training

Zeqiu Wu\({}^{1}\)1  Yushi Hu\({}^{1}\)1  Weijia Shi\({}^{1}\)  Nouha Dziri\({}^{2}\)  Alane Suhr\({}^{3}\)

**Prithviraj Ammanabrolu\({}^{45}\)  Noah A. Smith\({}^{1}\)2  Mari Ostendorf\({}^{1}\)  Hannaneh Hajishirzi\({}^{1}\)\({}^{2}\)**

\({}^{1}\)University of Washington \({}^{2}\)Allen Institute for Artificial Intelligence

\({}^{3}\)University of California, Berkeley \({}^{4}\)University of California, San Diego \({}^{5}\)MosaicML

Equal contribution. Correspondence to <Zeqiu Wu: zeqiuw1@uw.edu>, <Yushi Hu: yushihu@uw.edu>

###### Abstract

Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)--where human preference judgments on LM outputs are transformed into a learning signal--has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce **Fine-Grained RLHF**, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.

## 1 Introduction

State-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans , with a combination of supervised learning and reinforcement learning. Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs . Many of these studies adopt reinforcement learning from human feedback (RLHF) , a framework that converts human feedback into an effective LM training signal to reach these goals. Specifically, humans are presented with two or more outputs and asked to select one or rank them, and this signal is then used to train a reward model, which computes a single scalar reward for each LM-generated sequence. The LM is then trained with RL to optimize the reward it receives (from the reward model).

Such a reward provides a relatively sparse training signal, especially for tasks that require the generation of long-form text--making RLHF in such domains unreliable . Furthermore, previous research  into automated evaluation of generated text shows that it can be challenging for human annotators to reliably compare the overall quality of two or more model outputs when the outputs contain a mixture of diverse undesired behaviors. They demonstrate how categorizing and localizing model errors (i.e., fine-grained evaluation) provides explicit insights about which part of the model output has what type of problem. We thus ask the question: how can we improve rewards for LM training via RLHF by using more _fine-grained human feedback_?In this paper, we propose that humans give fine-grained feedback to LM output, associating _categories_ of undesired behavior (e.g., false or irrelevant generations) and a text span at a _density_ (e.g., sentence or sub-sentence-level). To enable LMs to learn from such fine-grained feedback, we introduce the Fine-Grained Rlhf framework. As shown in Figure 1, we first use collected human feedback to train fine-grained reward models such that each of them focuses on one _category_ and provides rewards at the _density_ associated with that category. We then integrate these reward models into Proximal Policy Optimization (PPO) , a commonly used RL algorithm for training LMs with preference-based human feedback (SS2).

We conduct experiments on two language generation tasks--detoxification  (SS3) and long-form question answering (QA)  (SS4). For detoxification, toxicity is the only error category and we explore learning with a dense reward. We adopt Perspective, a widely used language toxicity detection model trained on millions of human annotations, as our reward model. We use it to calculate a fine-grained reward after the generation of every sentence. Our experimental results show the efficacy and data efficiency of training models with dense reward compared to a holistic sequence-level reward, supported by automatic evaluation results.

With experiments on long-form QA, we aim to examine training models with fine-grained rewards at the two granularity dimensions (density and error category), for which we construct a long-form QA dataset, QA-Feedback, along with our collected human feedback. We carefully develop a pipeline to collect fine-grained human feedback on three error categories at different density levels: i) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence), and iii) incomplete information (whole sequence; see Figure 1). Our experimental results show improved results in each error category by learning with such fine-grained feedback, supported by both automatic and human evaluation results. In a scenario with multiple reward models representing different error types, we also show Fine-Grained Rlhf allows us to combine reward models with different weights and thus control the model training process towards a customized combination of desired behaviors.

Figure 1: Comparison of **(a) RL with human preference** and **(b) our Fine-Grained Rlhf** on long-form QA. Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors. We train a fine-grained reward model for each type of error and optimize LM against these reward models. In this example, we provide a relevance reward and a factuality reward after each sentence is generated. There is also a holistic information completeness reward after the whole text is generated.

Fine-Grained RLHF

We introduce Fine-Grained RLHF, a framework that enables us to train fine-grained reward functions for generation outputs across different feedback types. We first define the RL environment and learning algorithm. Then we define the fine-grained reward models and describe how to incorporate the fine-grained reward model(s) into an RL algorithm, in contrast to previous RLHF studies that only consider a single reward.

**Environment: language generation as a MDP.** We focus on language generation tasks. For each task, we are given a set of task input prompts \(D=\{x^{n}\}_{n=1}^{N}\). We follow  to define language generation as a Markov Decision Process (MDP) \(,,,P,,T_{max}\) with a finite vocabulary \(\). Each MDP episode starts with a sampled prompt \(x=(x_{1},x_{2},,x_{l})\) with \(x_{i}\), and ends when the current time step exceeds \(T_{max}\) or an end of sequence token is generated. \(\) is the state space and \(s_{0}=(x_{1},x_{2},,x_{l})\) is the initial state. An action in the environment \(a_{t}\) is a generated token (by the policy LM model \(P_{}\)) at time \(t\) from \(\) (\(a_{0}\) is the begin sequence token). The transition function \(P:\) appends \(a_{t}\) at the end of the state \(s_{t}=(x_{1},x_{2},,x_{l},a_{0},a_{1},,a_{t-1})\). This process continues until the end time step \(T T_{max}\) is reached, which gives a generated sequence \(y=(a_{1},,a_{T})\). A reward function \(:\), which comes from the reward model(s) in Fine-Grained RLHF, provides dense rewards before and when \(T\) is reached. \(P_{}\) can be initialized with a pre-trained language model, and sometimes also with supervised fine-tuning on task-specific demonstrations. The reward function is defined later.

**Learning algorithm: proximal policy optimization (PPO).** PPO  is an actor-critic RL algorithm that is widely used in previous RLHF work to optimize the policy model against a reward model of human feedback. It uses a value model \(V_{}(s_{t})\) to estimate the value of state \(s_{t}\), and optimizes the policy model with a PPO clipped surrogate training objective. The advantage \(A_{t}\) at timestep \(t\) is estimated by a generalized advantage estimation function : \(A_{t}=_{t^{}=t}^{T}()^{t^{}-t}(r_{t^{}}+  V_{}(s_{t^{}+1})-V_{}(s_{t^{}}))\), with \(\) as a hyperparameter and \(\) as the discounting factor for rewards. \(r_{t}\) is the reward assigned to \(a_{t}\), which in our case is acquired using one or multiple learned reward models. The value model \(V_{}(s_{t})\) is optimized with an expected squared-error loss with the value target as \(V^{}(s_{t})=_{t^{}=t}^{T-1}^{t^{}-t}r_{t^{ }}+^{T-t}V_{_{}}(s_{T})\), where \(V_{_{}}\) is the lagging value model. Finally, PPO is trained to optimize both policy (\(P_{}\)) and value (\(V_{}\)) models with their respective objectives. No reward model is being optimized during PPO training. See Appendix B for more details.

**Fine-grained reward models.** Previous RLHF work adopts a holistic reward model \(R_{}\) that maps input prompt \(x\) and generated output \(y\) to a single scalar reward representing its overall quality (Figure 1(a)). This single scalar reward is only assigned to the final token in the generated sequence, \(a_{T}\). Formally, \(r_{t}=R_{}(x,y)\) if \(t=T\) and 0 otherwise.

In contrast, we consider a reward function that is derived from one or multiple _fine-grained_ reward models that (1) provide rewards densely (i.e., for subsequences of the generated output), and (2) compute rewards on distinct categories of undesired behaviors (e.g., false or repetitive generation), where each category is associated with an individual reward model.

For a fine-grained reward model \(R_{_{k}}\) that gives feedback on error category \(C_{k}\), we first segment \(y\) into \(L_{k}\) segments \((y^{k}_{1},y^{k}_{2},,y^{k}_{L_{k}})\) corresponding to the density (e.g., sentence-level) of \(R_{_{k}}\), where each segment \(y^{k}_{j}\) ends at timestep \(T^{k}_{j}\). \(R_{_{k}}\) outputs a reward \(R_{_{k}}(x,y,j)\) for each segment \(y^{k}_{j}\) given \(x\) and \(y\) as the input, which is assigned to the final token in \(y^{k}_{j}\). Additionally, to ensure the fluency of generated outputs, we follow  to add an approximate KL divergence penalty to each token \(a_{t}\) with a weight \(\), that is not backpropagated through during training. Formally, assuming that we have \(K\) fine-grained reward models that represent different error categories, we will have a combined reward function for each token \(a_{t}\) as:

\[r_{t}=_{k=1}^{K}_{j=1}^{L_{k}}((t=T^{k}_{j})\,w_{k}\,R_ {_{k}}(x,y,j))-(a_{t} s_{t})}{P_{_ {}}(a_{t} s_{t})}\] (1)

where \(w_{k}\) is a weight assigned to reward model \(R_{_{k}}\). Then we follow the same PPO training algorithm to optimize the policy model. We discuss how we define and train fine-grained reward models for the detoxification and long-form QA task in our experiments in SS 3 and SS 4 respectively.

## 3 Task 1: Detoxification

The task of detoxification aims to reduce the toxicity in the model generation \(y\) when given a prompt \(x\). Toxicity is the only undesired behavior in this task, and we aim to explore learning with a dense reward in comparison to a _single_ holistic reward. We conduct our experiments on RealToxicityPrompts, a dataset of 100K sentence-level prompts derived from the web that are known to easily elicit problematic generations in GPT-2 . Using a dense sentence-level fine-grained reward, we demonstrate that **our fine-grained reward exhibits greater sample efficiency compared to a holistic reward**, achieving lower toxicity with fewer training steps while maintaining better fluency (SS3.1).

**Holistic reward for (non-)Toxicity.** We use the Perspective API  as our reward model, which is widely used for language toxicity detection and is trained with millions of examples gathered from several online platforms and annotated by human annotators for toxicity. That means we use an off-policy reward model that is not trained on outputs from \(P_{_{init}}\). The API outputs a score between 0 (non-toxic) and 1 (toxic). Given the entire model output \(y\), the holistic reward for RL is \(1-\)Perspective(\(y\)).

**Sentence-level (fine-grained) reward for (non-)Toxicity.** To calculate the _fine-grained reward_, we query the API after the model generates each sentence instead of generating the full sequence. For each generated sentence \(y_{j}\), we assign Perspective(\([y_{1},,y_{j-1}]\)) - Perspective(\([y_{1},,y_{j}]\)) as the sentence reward (i.e., how much toxicity is changed from generating \(y_{j}\)). Since there is only one error category, we omit the category superscript, using \(y_{j}\) to denote the \(j^{th}\) segment (e.g., sentence) in \(y\).

### Experiments

**Implementation details.** We follow previous work [17; 21] and use GPT-2 large model as the initial policy model \(P_{_{init}}\). During both the exploration stage in RL training and inference, we use nucleus sampling decoding with \(p\) = 0.9 and temperature = 1.0. The generation length limit is set to 48. The value model used during RL training is initialized with GPT-2-base due to GPU memory constraint. We report RL training parameters in Appendix B. All scores are averaged over 3 independent runs.

**Compared systems and evaluation.** We report the performance of **Fine-Grained RLhf**, RLHF with holistic reward (**Hol. RLHF**), and the state-of-the-art controlled generation approaches **GeDi** and **Dexperts**. We follow previous work [17; 21] to report the toxicity score calculated on each full generation sequence from the Perplexity API, as well as other commonly used metrics for RealToxicityPrompts, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy for fluency. The lower the perplexity, the more fluent the generated text. The toxicity score is reported as the _maximum_ score among 4 sampled model outputs, averaged over all test input prompts. Other metrics are reported as the _average_ score of the same 4 samples.

**Main results.** Table 1 shows the experimental results on the RealToxicityPrompts test set. Fine-Grained RLhf with sentence-level fine-grained reward attains the lowest toxicity and perplexity among all methods, while maintaining a similar level of diversity.

**Sample efficiency analysis.** Figure 2 shows the max toxicity and average perplexity on the development set during training. Fine-Grained RLhf has the toxicity drop much faster while keeping a low-level perplexity. This shows that learning from denser fine-grained reward is more sample efficient than holistic reward. One explanation is that fine-grained reward locates where the toxiccontent is, which is a stronger training signal compared with a scalar reward for the whole text. The cost is that we have to query the reward model more times per example.

## 4 Task 2: Long-Form Question Answering (QA)

Long-form QA requires an LM to generate a textual response to a question with a comprehensive answer and explanation. To examine learning with fine-grained rewards at the two granularity dimensions (error category and density), we collect QA-Feedback (SS4.1), a long-form QA dataset annotated with human feedback on LM-generated responses. We define three error categories at different density levels and train a reward model for each (SS4.2). We describe the experimental setup in SS4.3. Both human and automatic evaluation show that Fine-Grained Rlhf outperforms preference-based RLHF and supervised fine-tuning models on all error categories (SS4.4). We then show that adjusting the weights of fine-grained reward models during RL training leads to distinct behaviors in LM generation, allowing us to customize the LM for users with different needs (SS4.5). Finally, we conduct an in-depth analysis of the fine-grained reward models, revealing that they compete against each other, and provide an analysis of their impact on the resulting policy model.

### QA-Feedback: Long Form QA with Human Feedback

QA-Feedback is based on ASQA , a dataset that focuses on answering ambiguous factoid questions  in an open-domain setting. We use their provided oracle knowledge contexts to reformulate the task into a reading comprehension setting: given the input \(x\) that contains a question \(q\) and a set of knowledge passages \(P=\{p_{1},,p_{|P|}\}\), generate a long-form response \(y\). On average, there are roughly 65 words in each gold response. Since ASQA does not release the test set, we create our own train/development/test data split from the original train and development sets. We name our newly constructed data, along with collected human feedback (discussed next), QA-Feedback. Overall, we have 3,853 training, 500 development, and 948 test examples (details in Appendix C).

**Initial policy and fine-grained human feedback.** Before collecting human feedback, we follow  to initialize the policy model with supervised fine-tuning on a small set of examples. Specifically, we use 1K training examples to supervise fine-tuning of T5-large (the original baseline for ASQA)  to get \(P_{_{init}}\). We name this initial policy model **SFT**. We then sample outputs from SFT for the remaining training and development examples and collect _fine-grained_ human feedback in three error categories-- \(}\): **irrelevance**, repetition**, or incoherence**: \(}\): **incorrect** or unverifiable facts** based on knowledge passages; and \(}\): **incomplete information**. The collected feedback instances are then used as the training and development examples for training reward models. For each task prompt \(x\), we only collect fine-grained feedback for _one_ model output. Our data collection has IRB approval and is deemed exempt.

We instruct workers to identify any error in each model output \(y=(a_{1},,a_{T})\), marking the span of text associated with each identified error type. Formally, we define the set of user-annotated feedback for a task prompt \(x\) and model output \(y\) as \(=\{f_{i}\}\) where each \(f_{i}= c_{i},b_{i},e_{i}\) represents the user-identified span \((a_{b_{i}},,a_{e_{i}})\) of the error category \(C_{c_{i}}\), where \(c_{i}\{1,2,3\}\). Importantly, we impose three restrictions in the annotation: (1) error spans of category \(C_{1}\) or \(C_{2}\) should not overlap with each other; (2) only spans that do not have error \(C_{1}\) need to be assessed as containing error \(C_{2}\) or not; (3) \(C_{3}\) can only apply to whole output sequences. Additionally, we ask workers to mark passage sentences that contain missing information if a \(C_{3}\) error is annotated. We also ask workers to rewrite \(y\) into a corrected version \(y^{}\) that addresses all annotated feedback \(\). Details about the feedback collection interface, instructions, and quality control are in Appendix C.

To analyze human-human agreement, a subset of 300 examples receive annotations from two distinct workers. We observe that while exact agreement in error span boundaries is low, workers achieve reasonably high agreement on whether a sub-sentence contains \(C_{1}\) and whether a sentence contains \(C_{2}\).2 Therefore, we decide to have the density for error type \(C_{1}\), \(C_{2}\), and \(C_{3}\) as sub-sentence, sentence and full sequence. We provide more data analysis including human agreement in Appendix C.

**Preference-based human feedback.** For comparison purposes, we follow  to separately collect pairwise _human preferences_ from the same group of workers. We sample 4 model outputs for each prompt \(x\), which gives 6 pairs of model outputs. We ask the workers to indicate pairwise preferences(ties are allowed) based on all errors they can find in each model output. They are not asked to explicitly annotate these errors.

**Annotation details.** On average, both annotation tasks of fine-grained and preference feedback for one question take a worker about 6 minutes to finish. In contrast,  report that they spend about 15 minutes to label a human-written response for each question, which is much more time-consuming than our feedback annotation. On average, we pay $1.65 per example for both tasks, leading to $16.50 hourly pay for our workers. We include details of the pay structure in Appendix C. We observe that human annotators can reach a higher agreement in each aspect of fine-grained feedback compared to pairwise comparisons because the feedback definitions are more concrete.

### Fine-Grained Reward Models

We train three separate reward models \(R_{_{1}}\), \(R_{_{2}}\), and \(R_{_{3}}\) for \(C_{1}\), \(C_{2}\), and \(C_{3}\) error categories respectively with a density of sub-sentence, sentence, and full sequence, respectively. Since reward models provide scalar reward scores and do not perform generation, we use the encoder-only Longformer-base  as our backbone model to handle long input sequences (more details of each reward model are in Appendix D).

\(}\)**:**Irrelevance, repetition, or incoherence\(R_{_{1}}\) targets to predict whether each sub-sentence in \(y\) contains a \(C_{1}\) type error. We denote \(y=(y_{1}^{1},,y_{L_{1}}^{1})\), where \(y_{j}^{1}\) is the \(j\)th segment at \(R_{_{1}}\)'s density (i.e., sub-sentence), with \(L_{1}\) segments in total. We add a 2-class token-level classification layer (a single feed-forward layer) on the top of the Longformer encoder. The model input has the format of "question: \(q\) answer: [sep] \(y_{1}^{1}\) [sep] \(y_{2}^{1}\)...", and we take the classification output at each [sep] token to indicate whether the following \(y_{j}^{1}\) contains a \(C_{1}\) error. We do not add passages in the model input because, intuitively, the detection of \(C_{1}\) errors does not depend on them. To train \(R_{_{1}}\), we apply a token-level classification loss to each [sep] token before \(y_{j}^{1}\), where its gold label \(g_{j}\) is "has error" if there is a \(f_{i}\) that has \((a_{b_{i}},,a_{e_{i}})\) overlapped with \(y_{j}^{1}\) and \(c_{i}=1\), and "no error" otherwise. When \(R_{_{1}}\) provides a reward during RL training as in Eq. 1, we read a reward \(R_{_{1}}(x,y,j)\) for every \(y_{j}^{1}\) given \(x\) and \(y\). We define \(R_{_{1}}(x,y,j)=+1\) if \(R_{_{1}}\) predicts "no error" for \(y_{j}^{1}\) and \(-1\) otherwise.

\(}\)**:**Incorrect or unverifiable facts.**\(R_{_{2}}\) is developed for detecting a \(C_{2}\) error at the sentence level in a similar way. The model input has the format of "question: \(q\) context: \(p_{1}\)\(p_{2}\)\(\)answer: [sep] \(y_{1}^{2}\) [sep] \(y_{2}^{2}\)...", where \(p\)'s denotes the grounding passages and \(y_{j}^{2}\) represents the \(j\)th sentence. We train \(R_{_{2}}\) similarly to \(R_{_{1}}\), with one exception: as we instruct the workers not to annotate a \(C_{2}\) error for a span that is already labeled as containing a \(C_{1}\) error, we do not calculate loss on sentences that are labeled as containing \(C_{1}\) but not \(C_{2}\) during \(R_{_{2}}\) training.

\(}\)**:**Incomplete information.**\(R_{_{3}}\) is trained to measure the information completeness of \(y\), at the full sequence level. Motivated by , \(R_{_{3}}\) predicts a single scalar reward and is trained with a pairwise comparison loss :

\[_{r}()=-_{(x,_{p},_{l}) D_{p}} (R_{_{3}}(x,_{p})-R_{_{3}}(x,_{ l}))\] (2)

where \(R_{_{3}}(x,y)\) is the scalar output of the reward model for input \(x\) and output \(y\); \(_{p}\) and \(_{l}\) are sampled from the same input \(x\), and \(_{p}\) has less missed information compared with \(_{l}\); \(D_{p}\) contains the pairwise comparisons bootstraped from human feedback on \(C_{3}\) errors (see details in Appendix D).

**Preference-based reward model.** The preference-based reward model is trained in a similar way to \(R_{_{3}}\), with \(_{p}\) representing the human preferred response against \(_{l}\) in the loss function Eq. 2. It outputs a scalar score for the given \(x\) and \(y\) that represents the overall response quality.

### Experimental Setup

**Compared systems.** We compare our proposed method, **Fine-Grained RLhf** with the initial T5 policy model trained with 1K examples (**SFT**) and RLHF with holistic preference-based rewards (**Preference RLHF**). The reward models used in RLHF experiments are trained on 2.8K examples with annotated feedback (but no gold human response). For analysis, we also use the human gold responses of all training examples to finetune a fully supervised T5 model (**SFT-Full**). Notice that SFT-Full requires much higher annotation cost because it takes longer (15 minutes per example ) for annotators to draft long-form responses.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

the outputs become extremely long and the _comp._ reward is extremely high. We observe the outputs find the model is copying a lot of content from the passages. When the _fact._ reward model (\(\,R_{_{2}}\,\)) is removed, the _rel._ reward becomes the highest. We observe that the LM tends to answer the question directly and not reference the passages, which causes a lot of hallucinations. When the _comp._ reward model (\(}}}\,\)) is removed, the outputs are concise and factual but not providing all relevant information to the question. Thus, it has lower information completeness and Rouge score compared with the LM trained with all reward models.

**Reward model performance.** We report and analyze the performance of each reward model in predicting its corresponding error category. The _rel._ reward model \(R_{_{1}}\,\) has a binary classification accuracy of 69.6, and an F1 score (for the "has error" class) of 68.5 on model-generated sub-sentences from the development set. We sample 20 sub-sentences where \(R_{_{1}}\,\) predicts the opposite of the human label, and observe that all of them either 1) contain relevant auxiliary information and are marked as "no error" by humans, or 2) are marked as irrelevant by humans but provide closely related background information to the question. In other words, \(R_{_{1}}\,\) is mostly struggling with predicting the relevance of auxiliary information, and it rarely fails to predict a direct answer as "no error".

The _fact._ reward model \(R_{_{2}}\,\) has an accuracy of 77.8 and an F1 score of 67.5. We sample 20 sentences where \(R_{_{2}}\,\) makes a prediction mistake and we observe that the mistakes often happen when the generated sentence is highly abstractive instead of directly copying information from the passage. We also observe that more than 80% of human labeled factual errors occur when the model generates a direct answer (not auxiliary information) that contains hallucinated information or a random entity from a passage. We notice that \(R_{_{2}}\,\) correctly captures more than 80% of such errors.

The _comp._ reward model \(R_{_{3}}\,\) has an accuracy of 70.9 in pairwise comparison. In contrast, the preference-based reward model only reaches an accuracy of 57.2. This helps confirm our intuition that assessing long-form generation outputs holistically can be more ambiguous and subjective than evaluating the outputs with a focus on a specific undesired behavior type.

**Comparison with ChatGPT responses.** We experiment with answering the questions with ChatGPT. To familiarize ChatGPT with the style of our LFQA task, we prompt it with the task instruction and a single random QA example (due to length limitation). ChatGPT achieves a RougeLSum score of 40.92 on the test set, which is much lower than our models. We do not use our trained reward models to evaluate ChatGPT outputs because reward models trained on T5-large may not generalize well to ChatGPT. We instead manually inspect the ChatGPT responses, and observe that they are mostly concise and factual, yet lack the auxiliary information necessary to clarify ambiguous questions. Qualitative examples are in Appendix A. This shows the difficulty for ChatGPT in learning user-desired behaviors through simple prompting.

## 5 Related Work

**Reinforcement learning from human feedback (RLHF).** RLHF [46; 42; 29] aims to optimize the policy language model to generate content that is desired by human. This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization , instruction following , question answering [24; 27] and reducing harmfulness [3; 2; 22; 10]. Most of these studies collect human preferences over pairs of model outputs on one or a set of desired attributes, in order to train a reward model to assign a holistic score for a generation output during RL training.  trains separate reward models that assign scores for different desired attributes, but still uses a single reward that combines scores from all reward models. In contrast, we explore RLHF with fine-grained reward models trained on human feedback where each reward model provides dense reward after every small text segment for a specific type of desired behavior.  explores using intermediate rewards to improves LM performance on reasoning tasks.

**Learning from human feedback in NLP.** There also exists work that explores non-RL methods to learn from human feedback.  trains a reward model that predicts a single score for each model output and selects samples with the highest reward scores for supervised fine-tuning. [38; 14; 42] train a conversational model to predict both the response and a binary user satisfaction score in order to improve the response generation. Besides such numerical human feedback, natural language (NL) human feedback has also been explored. [23; 6] collect and store NL human feedback in a feedback memory for the model to retrieve and then perform the end task conditioning on the retrieved feedback. [5; 35; 34] use a refinement model to refine model outputs conditioning on NL human feedback and then use a reward model to select the best refined outputs for supervised fine-tuning. Methods for using a reward model to guide LM generation towards desired behaviors at inference time [21; 7] can complement our work that aims to improve the LM during training.  also explores incorporating human feedback into LM pre-training.

## 6 Discussion

**Annotation Costs.** It is important to note that the fine-grained human feedback used for training our fine-grained reward models does _not_ incur a greater cost than holistic human preference. As outlined in SS 4.2, our observations reveal that annotators require a substantial amount of time to compare two lengthy text outputs. For the long-form QA task, both fine-grained feedback and preference-based feedback takes approximately 6 minutes per sample for an annotator.

### Broader Impacts

We propose the Fine-Grained Rlhf framework that can incorporate multiple reward models to provide dense rewards for RL training, which leads to LM outputs that are optimized towards such rewards. Our framework can be applied to any text generation task, thereby enhancing LM performance by offering more nuanced guidance than holistic feedback. The key advantages of the Fine-Grained Rlhf framework are two-fold:

**Flexibility.** Our framework significantly expands the versatility of reward models for RLHF. For example, future work involving fact-checking, sentiment classification, toxicity detection, among others, can all be incorporated within this framework. LMs can be trained against all these reward models via Fine-Grained Rlhf.

**Controllability.** Having multiple reward models that stand for different feedback types allows the end user to exert greater control over RL training (e.g., through different combinations of reward model weights; see details in SS 4.5). This leads to customized model behaviors, a benefit particularly valuable for applications like educational tools where model personalization is crucial.

### Limitations and Future Work

One major limitation of our framework comes from the additional compute cost of getting _fine-grained_ rewards, compared to RLHF with a holistic reward. For instance, in the detoxification task, we need to make multiple Perspective API calls for each model output depending on how many sentences are generated, while RLHF with a holistic reward only requires one. In the long-form QA task, we need to calculate a dense reward from multiple reward models, which takes more compute time and GPU memory than a single reward model.

Another limitation is that different tasks may have different definitions of fine-grained feedback in terms of the feedback types and the density level of each type. Therefore, defining feedback that is well-suited for a task and training reward models accordingly requires non-trivial manual effort.

Finally, in this work, we carefully control the quality of annotated feedback, which is then used to train reward models for RL. In practice, when a deployed model is released to the public, end users don't always give clean feedback. Therefore, how to obtain effective learning signals from noisy human feedback in the wild still needs further investigation.

Some other interesting questions to explore in the future include: 1) Can we obtain fine-grained feedback from LMs like GPT-4 instead of humans to improve model performance and reduce annotation costs? 2) How can other non-RL approaches of using human feedback such as controlled generation during inference time complement Fine-Grained Rlhf? 3) How would fine-grained reward and value model sizes affect policy model performance during RL training?

## 7 Conclusion

In this work, we introduce Fine-Grained Rlhf, a framework that enables LMs to learn from multiple fine-grained reward models trained from human feedback, where each reward model detects a specific error category and provides dense rewards. We conduct experimental analysis on two text generation tasks to illustrate the performance gain of Fine-Grained Rlhf than RLHF over holistic rewards, supported by both automatic and human evaluation. Furthermore, we show that an LM can be customized for specific needs using different combinations of fine-grained reward models.