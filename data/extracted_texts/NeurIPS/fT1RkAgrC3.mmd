# Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation

Yu-Liang Zhan

Gaoling School of Artificial Intelligence

Renmin University of China

zhanyuliang@ruc.edu.cn

&Zhong-Yi Lu

School of Physics

Renmin University of China

zlu@ruc.edu.cn

&Hao Sun

Gaoling School of Artificial Intelligence

Renmin University of China

haosun@ruc.edu.cn

Corresponding authors.

Ze-Feng Gao

School of Physics

Renmin University of China

zfgao@ruc.edu.cn

###### Abstract

Increased training parameters have enabled large pre-trained models to excel in various downstream tasks. Nevertheless, the extensive computational requirements associated with these models hinder their widespread adoption within the community. We focus on Knowledge Distillation (KD), where a compact student model is trained to mimic a larger teacher model, facilitating the transfer of knowledge of large models. In contrast to much of the previous work, we scale up the parameters of the student model during training, to benefit from over-parameterization without increasing the inference latency. In particular, we propose a tensor decomposition strategy that effectively over-parameterizes the relatively small student model through an efficient and nearly lossless decomposition of its parameter matrices into higher-dimensional tensors. To ensure efficiency, we further introduce a tensor constraint loss to align the high-dimensional tensors between the student and teacher models. Comprehensive experiments validate the significant performance enhancement by our approach in various KD tasks, covering computer vision and natural language processing areas. Our code is available at [https://github.com/intell-sci-comput/OPDF](https://github.com/intell-sci-comput/OPDF).

## 1 Introduction

Large-scale pre-trained models are gradually achieving remarkable milestones due to the exhibit of remarkable performance across various tasks . These models leverage extensive pre-training data and parameters, enabling them to effectively encapsulate a significant breadth of world knowledge  and exhibit strong generalization capabilities across diverse tasks . Following this trajectory, the utilization of increased data and parameters has emerged as a notable trend in enhancing the performance of pre-trained models in recent years, leading to the number expansion of pre-trained model parameters from millions to billions .

Despite their impressive performance, the substantial storage demands and high computational complexity hinder the practical deployment of these models in real-world applications. Therefore, on the one hand, some studies focus on pre-training relatively smaller models (such as BERT-base-uncased ) on domain-specific or task-specific corpora . However, due to the lesser over-parameterization of small models compared to large ones, their generalization capability oftenfalls short, resulting in suboptimal fine-tuning performance on downstream tasks. On the other hand, model compression methods, such as pruning less informative parameters  or utilizing _knowledge distillation_ (KD)  to transfer knowledge from larger models (teachers) to smaller ones (students), have been proposed. KD has swiftly diversified into numerous branches, primarily falling into two categories: _i.e.,_ logits-based  and features-based  depending on the source of student model knowledge. Nevertheless, as student models have fewer trainable parameters and limited capacity, a significant performance gap remains between student and teacher models.

To address the disparity between small and large models, this study aims to over-parameterize small student models as large ones during distillation training to enhance their generalization capability. Typically, most parameters of student models are stored as matrices. Through tensor decomposition techniques  (_e.g.,_ Singular Value Decomposition), each matrix can be factorized into a set of matrices, effectively increasing the total number of parameters during distillation. Moreover, after convergence, the factorized matrices can be merged to reorganize the parameter matrix of the student model. This paradigm leverages the benefits of over-parameterization during training without increasing the inference latency of student models.

However, incorporating tensor decomposition into over-parameterizing student models poses two major concerns that must be addressed. First, the potential information loss caused by tensor decomposition should be minimized, as small computation errors may accumulate and propagate exponentially within the stacked layers of student models. Second, in the over-parameterized student models, there is no effective mechanism to ensure the consistency of information between student and teacher models. Therefore, it is essential to choose appropriate tensor decomposition methods and design loss functions for high-order tensors to ensure the effective transfer of information from teacher to student models.

To address the above issues, we introduce the matrix product operator (MPO)  technique as the tensor decomposition strategy. The MPO decomposition, widely used in quantum many-body physics, efficiently factorizes any matrix with arbitrary dimensions into a set of higher-dimensional tensors, which can reconstruct the original matrix in almost lossless conditions . These advantages make MPO an ideal method for over-parameterizing student models during distillation. Based on MPO, we also devise high-order tensor alignment losses for student and teacher models to ensure the effective transfer of information in tensor representation.

Therefore, in this paper, we propose a general Over-Parameterization Distillation Framework, namely **OPDF**, to improve the performance of knowledge distillation. Given the parameter matrices of a student model, we first over-parameterize them through MPO decomposition and then utilize high-order tensor alignment losses to ensure efficient information transfer. This framework only modifies the distillation training process, making it applicable to various student models and natural language processing (NLP) and computer vision (CV) tasks. We conduct extensive experiments in both NLP and CV domains. Experimental results demonstrate that our OPDF significantly enhances the effectiveness of model distillation, _e.g.,_ improving BERT-base KD +1.6 on average. Moreover, our approach also enables the student model to achieve performance nearly on par with the teacher model, _e.g.,_ AD-KD+Ours (83.4) _v.s._ BERT-base (83.4) in average metric on GLUE.

## 2 Related work

Large Scale Pre-trained ModelsLarge-scale pre-trained models have achieved remarkable success in many fields (_e.g.,_ natural language processing (NLP)  and computer vision (CV) ). Since the introduction of the Transformer architecture , the pre-training and fine-tuning paradigm in NLP, exemplified by models like BERT  and T5 , has shown outstanding performance across multiple tasks. Furthermore, the emergence of models like GPT-3 has demonstrated that increasing model size can significantly improve performance on low-resource tasks . In the field of computer vision, models based on Transformers, such as ViT , have also performed exceptionally well. In our research, we improve the distillation process by increasing the parameters during the training phase of the student model, without introducing additional inference latency to the student model.

Knowledge DistillationKnowledge Distillation (KD) methods are commonly used to compress models by transferring knowledge from a larger _teacher model_ to a smaller _student model_. Building upon the initiative work by , the researchers have exploited the logits follows up with different techniques in the computer vision field, _e.g.,_ minimizing KL-Divergence (DKD ) or a Pearson correlation (DIST ). Logit-based methods have been also popular in NLP [41; 42]. Features-based methods have tried to align the features from intermediate layers of teacher and student models and minimize the differences . After the intermediate representations have been introduced , a mount of features-based KD methods have been proposed to match the features, such as LGTM , DBKD  and AD-KD . However, the capacity gap between the teacher and student models makes it difficult to imitate the hidden representations of the teacher . Different from these existing KD methods, our proposed OPDF has utilized MPO decomposition to over-parameterize the student model in the training procedure to improve the student model generalization capability, which can minimize the capacity gap efficiently.

Matrix Product OperatorsMatrix Product Operators (MPOs) [34; 48], also known as tensor-train operators (TTOs) , have been proposed for a more efficient representation of the linear structure of neural networks [49; 50]. A large number of typical applications have utilized MPO-based methods to compress linear layers  and convolutional kernels  in the parameter matrices of deep models. Furthermore, existing works have applied the MPO method for lightweight fine-tuning of ALBERT , the efficient expansion for the MoE framework , the over-parameterization tuning process for PLMs , construct efficient PLM architecture [53; 54] and compressing datasets . Unlike existing methods, our approach focuses on utilizing MPO decomposition to map parameters from low-dimensional spaces to high-dimensional spaces, to over-parameterize the student model during the distillation process, allowing the student model to benefit from more parameters and achieve better distillation results.

## 3 Preliminary

Tensor ProductWe denote a tensor \(_{i_{1},i_{2},,i_{n}}\) as an array with \(n\) indices, where \(\{i_{1},i_{2},,i_{n}\}\) denotes the dimensions of the \(n\) indices, respectively. In this manner, a vector (_i.e.,_\(\)) can be considered a \(1\)-order tensor, while a matrix (_i.e.,_\(\)) can be regarded as a \(2\)-order tensor. Consider \(_{1},,_{p}\) and \(_{1},,_{q}\) as the orthonormal bases of tensors \(^{(1)}\) and \(^{(2)}\), respectively. The tensor product, denoted as \(\), can be obtained through the contraction of \(^{(1)}\) and \(^{(2)}\). Formally, the tensor contraction of \(^{(1)}=_{i=1}^{p}a_{i}_{i_{1}}\) and \(^{(2)}=_{j=1}^{q}b_{j}_{i_{2}}\) is defined as follow:

\[^{(1)}^{(2)}=_{i=1}^{p}_{j=1}^{q}a_{i}b_{ j}_{i_{1}}_{i_{2}}. \]

The set \(_{i_{1}}_{i_{2}}\) constitutes the orthonormal basis of the resulting vector Hilbert space, with the dimensionality of this Hilbert space being the product (i.e., \(p q\)) of \(^{(1)}\) and \(^{(2)}\).

Tensor DecompositionTensor decomposition can be viewed as the reverse operation of the tensor product. A commonly employed approach is the singular value decomposition (SVD) algorithm. Given a tensor \(^{i_{1} i_{n}}\), the SVD operation performed \(n\) times can decompose this tensor into \(n\) local tensors \(^{(k)}_{k=1}^{n}\). Conversely, the decomposed tensors can reconstruct the original tensor by sequentially applying the tensor product operator.

## 4 Method

In this section, we describe our proposed over-parameterized distillation framework. We first outline our approach, then introduce the details of matrix product operator decomposition and the over-parameterized student model strategy, and finally present the tensor alignment loss.

### Overview

Current distillation methods primarily enhance the performance of student models by introducing constraints on logits or features between the student and teacher models. In contrast to these methods, our approach not only utilizes tensor decomposition to over-parameterize the student model for performance improvement but also designs alignment loss functions for the decomposed high-order tensors to further enhance the performance of the student model. To achieve this goal, we employ a tensor decomposition method to decompose the parameter matrices of the teacher and student models into a series of high-order tensor products. These high-order tensors can be used to reconstruct the original parameter matrices while significantly increasing the number of trainable parameters in the student model. After reconstruction, the student model has the same number of parameters as the original matrix without increasing inference time and model size. Additionally, by introducing distillation loss functions to allow the student model to learn from the teacher model in tensor representation, the effectiveness of knowledge distillation is further enhanced.

In our proposed over-parameterized distillation framework, we integrate a tensor decomposition strategy based on MPO into the student model to enlarge the parameter matrix (Section 4.2). Furthermore, we design a tensor alignment loss function to enhance the performance of the student model in the context of knowledge distillation (Section 4.3). An overview of our approach is depicted in Figure 1. We also provide a detailed description of our over-parameterized distillation framework in Algorithm S.1.

### Over-paramterization Distillation Framework via MPO Decomposition

To leverage the advantages of over-parameterization during knowledge distillation, our method utilizes the MPO, a tensor decomposition technique that increases the number of model parameters. In this part, we initially present the specifics of the MPO method and subsequently outline its adaptation for over-parameterizing the student model.

Matrix Product Operator DecompositionThe MPO decomposition is an efficient algorithm capable of factorizing a parameter matrix \(^{I J}\) into a sequential product of multiple tensors . Formally, given a matrix \(^{I J}\), its MPO decomposition into a product of \(n\) local tensors can be represented as:

\[()=_{k=1}^{n}_{(k)}[d_{k-1},i_{k},j_{k},d_{ k}]. \]

The tensor \((k)[d_{k-1},i_{k},j_{k},d_{k}]\) is a 4th-order tensor with dimensions \(d_{k-1} i_{k} j_{k} d_{k}\), where \(_{k=1}^{n}i_{k}=I,_{k=1}^{n}j_{k}=J\), and \(d_{0}=d_{n}=1\). To link two sequence tensors, we have adopted the concept of a _bond_ following the work of . The bond dimension \(d_{k}\) is defined by:

Figure 1: The overview of over-parameter distillation framework (OPDF) for knowledge distillation. **a**, We use MPO decomposition to realize the over-parameter procedure for the student model. The auxiliary tensors of the student model are trained to imitate the auxiliary tensors of the teacher model closely. **b**, We present an illustrative example of MPO decomposition. A parameter matrix \(_{I J}\) is decomposed into central tensor and auxiliary tensors.

\[d_{k}=_{m=1}^{k}i_{m} j_{m},_{m=k+1}^{n}i_{m} j_{m }. \]

From Eq. (3), we can see that \(d_{k}\) will be large in the middle and small on both sides. Following , we refer to the tensor right in the middle as _central tensor_, and the rest as _auxiliary tensor_. Figure 1(b) presents the illustration of MPO decomposition. You can find additional descriptions of tensors and MPO in Appendix A.

Over-parameterzing Student Model.Utilizing the MPO method within the framework of knowledge distillation, our objective is to extend the parameter scale of the student model, capitalizing on over-parameterization. More specifically, we can employ the MPO method to break down a portion of the parameter matrices into multiple tensors as illustrated in Eq. (2). Following MPO decomposition, the parameter count of the matrix \(\) will increase based on the values of \(\{d_{k}\}_{k=1}^{m}\), \(\{i_{k}\}_{k=1}^{m}\), and \(\{j_{k}\}_{k=1}^{m}\). The precise augmentation in parameter count, denoted as \(N_{add}\), can be computed as follows:

\[N_{add}=_{k=1}^{m}i_{k}j_{k}d_{k-1}d_{k}-_{k=1}^{m}i_{k}j_{k}. \]

Therefore, during the knowledge distillation procedure, we can adopt MPO on student model parameter matrices to generate their corresponding multiple tensors. In this way, we can scale up the total parameter of the number of the student model without increasing its inference time consumption. After training the over-parameterized student model to convergence, we will perform tensor contraction on these decomposed tensors, to reconstruct the parameter matrices of the student model in almost lossless conditions which is detailed in Appendix B. This new student model has the same parameter number and inference latency as the original one and has benefited from over-parameterization during training.

### Assisted Constraints for Knowledge Distillation

Revisiting Prediction Match of Knowledge DistillationTraditional knowledge distillation involves two stages: fine-tuning the teacher model for a specific task, followed by training strategies to constrain the student model to closely approximate the teacher model. These processes aim to transfer the knowledge from the teacher to the student model. Recent studies have mainly focused on directly learning from the features and logits of the teacher model to transfer crucial knowledge .

However, these methods are limited by the capacity of the student model due to the limitation of total parameters. Moreover, this distillation approach based on cross-entropy loss constraints may lead to the student model _losing its ability to learn independently_. We aim to design a novel model distillation framework to enable the student model not only to effectively learn the knowledge from the teacher model but also to maintain its ability to learn independently.

Distillation Loss for Auxiliary Tensors.To achieve the goal of "learning knowledge from the teacher model while maintaining the ability of the student model to learn independently," we introduce a high-order tensor alignment training method based on the MPO decomposition. A crucial merit of MPO decomposition is its ability to reorganize and aggregate the core information, decomposing the weight matrices into a central tensor (containing a large number of parameters and important information) and auxiliary tensors (containing fewer parameters and additional information to the central tensor) . Therefore, in the knowledge distillation, in addition to minimizing the cross-entropy loss concerning the ground truth, we add a loss constraint for aligning the auxiliary tensors between the student and teacher models:

\[_{Aux}=_{k=1}^{n}\;(_{s,k}, _{t,k}), \]

where the matrices \(_{t,k}\) and \(_{s,k}\) refer to the auxiliary tensor of student and teacher models with the same dimensions respectively. MSE means the mean-square error loss function. To ensure that the student model learns from the teacher while preserving its central tensor for independent learning, we minimize the mean-square error loss between the auxiliary tensors of both the student and teacher models. Since this distillation framework is based on improvements to the weight matrices, it is orthogonal to most current distillation methods. Therefore, it can further enhance the distillation effectiveness based on existing distillation methods (as thoroughly discussed in the experimental section). Hence, it can be widely applied to various knowledge distillation models.

## 5 Experiments

In this section, we assess the efficacy of our approach within two renowned domains: computer vision and natural language processing. Notably, the OPDF is designed to complement existing distillation techniques. Consequently, we apply our proposed OPDF with various standard distillation methods to validate its effectiveness. In the subsequent section, we detail our experimental setup's datasets and baseline methods. We then present the primary results achieved with the OPDF and provide a thorough analysis. Furthermore, we examine the influence of the degree of over-parameterization, MPO strategy and the learning rate on the performance of OPDF. We report the memory and time cost of experiments in Appendix D.1.

### Experimental Setup

Datasets and MetricsFor NLP tasks, we evaluate our approach on text classification tasks in GLUE benchmark . The tasks encompassed in our evaluation include RTE, MRPC, STS-B, CoLA, SST-2, QNLI, QQP, and MNLI. To facilitate comparison with baselines, we employ the F1 score and accuracy as metrics for MRPC and QQP, Matthew's correlation coefficient for CoLA, and the average of Pearson and Spearman correlations for STS-B. Accuracy is used as the metric for the remaining tasks, with the result for MNLI reported as the average across the matched (MNLI-m) and mismatched (MNLI-mm) domains. Additionally, we calculate the average score across all tasks to provide a comprehensive performance measure. In the context of CV tasks, we have applied the OPDF to the distillation of Vision Transformers (ViT) for image classification . This was done using the ImageNet-21k dataset , ImageNet-1k, ImageNet Real , and ImageNet V2  datasets. For these datasets, we use accuracy as the primary evaluation metric.

Baseline MethodsFor NLP tasks, we implement OPDF on previous KD methods: BERT-of-Theseus , LGTH , DBKD  and AD-KD . We replicated the baselines using the publicly released code to assess their performance on the test set. Additionally, LGTH was not previously evaluated across all tasks in its original publications, and we have addressed this omission using the provided code. It is important to note that DBKD is designed to estimate logits from decision distributions , and therefore we do not report performance on the STS-B task. For all experiments in natural language processing, we demonstrate the effectiveness of our method during the fine-tuning stage. We implement the teacher model as the fine-tuned "BERT-base-uncased" model . In the context of CV tasks, TinyViT , which introduces a rapid pre-training framework, has emerged as a classical distillation method for ViT. The original paper on TinyViT discusses three versions of the model with varying parameter counts: TinyViT-5M, TinyViT-11M, and TinyViT-21M. To incorporate high-order tensor alignment loss into the distillation phase, we utilize CLIP-VIT-L/14 , a variant of ViT, as the teacher model in our experiments. To assess the efficacy of OPDF, we pretrain the distillation model on ImageNet-21k and evaluate its linear probe performance on ImageNet-1k, ImageNet Real, and ImageNet V2, without any fine-tuning. During the pre-training stage, we adhere to the same experimental settings as described in the original paper. Furthermore, we juxtapose our method with SVD , a traditional tensor decomposition technique viable for over-parameterizing student models. Concretely, we employ SVD to substitute MPO within our framework and execute over-parameterization across all parameter matrices of the student model during knowledge distillation. Appendix D.2 shows more experimental details.

### Main Experimental Results

NLP TasksWe present the results on BERT in Table 1. Firstly, it is evident that integrating KD with over-parameterization methods yields the most significant performance enhancements. Over-parameterization enhances the generalization ability of the student model. Upon comparing the two tensor decomposition techniques, we find that MPO consistently outperforms SVD. This discrepancy arises from the singular value-based SVD in a two-dimensional space, limiting its ability to substantially increase model parameters compared to MPO decomposition (_e.g.,_ 90M_vs._ 160M in BERT-of-Theseus). In contrast, MPO allows for arbitrary scaling by increasing the order of decomposition, rendering it more suitable for over-parameterization. Secondly, following the integration of the OPDF method, the performance of prior KD techniques (BERT-of-Theseus, LGTM, DBKD, and AD-KD) have exhibited enhancements across a majority of tasks (_e.g.,_ RTE, MRPC, CoLA, QQP), while maintaining comparability with the original method in other tasks. This highlights the versatility of OPDF, demonstrating its effectiveness across diverse models and a wide range of tasks. Finally, our findings have revealed that employing the OPDF method can even outperform the performance of the teacher model in MRPC and RTE datasets. This indicates that the process of over-parameterization endows the student model with stronger generalization capabilities, suggesting that employing over-parameterization may offer a potential solution to the bottleneck in current distillation methods where the performance of the student model fails to surpass that of the teacher model.

CV TasksAll CV results of our proposed method are shown in Table 2. We apply OPDF on three kinds of TinyVit with different total parameters. It is clear that with OPDF, the performance of TinyVit can be significantly improved. In particular, in all datasets, TinyVit applied OPDF is better than vanilla TinyVit. Moreover, TinyVit utilized OPDF with 11M parameters can achieve better performance than TinyVit with 21M parameters. It demonstrates that OPDF is an orthogonal method for various KD methods based on the Transformer whether in the CV or NLP field. Note that since we only involved the over-parameterization procedure in the training phase, the total parameter of the student model will not change in the inference phase. This merit makes the OPDF unique from the existing KD method: one would not increase inference time while enhancing model accuracy and enabling the model to acquire more knowledge from the teacher model. Moreover, we can observe that the performance of the original TinyVit, SVD over-parameterization, and OPDF over-parameterization improves as the number of parameters gradually increases. This indicates that compared to SVD, the MPO decomposition, which can decompose the parameter matrix to any size, can better enhance the expressive capacity of the student model. The impact of the over-parameterization scale on distillation effectiveness will be analyzed in detail in Section 5.3.

### Further Analysis

Performance Comparison _w.r.t._ Parameter Increasing Rate.Our OPDF method facilitates the flexible expansion of model parameters, thereby highlighting the significance of the parameter increase rate on model performance. Consequently, we investigate the influence of this rate on model

    & RTE & MRPC & STS-B & CoLA & SST-2 & QNLI & QQP & MNLI &  & \# Train & \# Inference \\  & Acc. & F1/Acc. & Corr. & Mcc. & F1/Acc. & Acc. & F1/Acc. & Acc. & Acc. & & & & & \\  BERT-base  & 70.5 & 86.5/81.8 & 86.6 & 54.2 & 92.0 & 91.2 & 88.09/1.0 & 84.2 & 83.4 & 110 & 110 \\  & & & & **BERT-of-Theseus** & & & & & & & & & \\  None & 65.5 & 85.3/79.6 & 86.2 & 39.2* & 90.4 & 88.7 & 86.1/89.6 & **81.5** & 79.2 & 66 & 66 \\ +SVD & 65.5 & 85.4/80.0 & 86.5 & 43.1 & 90.6 & 88.6 & 86.2/89.7 & 80.3 & 79.6 & 90 & 66 \\ +OPDF (Ours) & **66.2** & **85.9/80.5** & **88.6** & **45.2** & **91.3** & **89.0** & **86.8/90.2** & 81.4 & **80.5** & 160 & 66 \\   \\  None & 63.3 & 86.3/80.1 & 82.9* & 33.9* & 91.1 & **89.3** & **88.0/91.1** & **82.2** & 78.8 & 67 & 67 \\ +SVD & 64.7 & 86.8/81.9 & 83.1 & 37.4 & 91.2 & 88.6 & 86.5/89.4 & 79.3 & 78.9 & 91 & 67 \\ +OPDF (Ours) & **66.9** & **87.8/82.4** & **83.3** & **38.9** & **91.5** & 88.7 & 87.0/90.2 & 80.9 & **79.8** & 163 & 67 \\   \\  None & 61.2 & 83.3/75.5 & / & 25.2 & 88.1 & 86.1 & 85.3/88.7 & 76.1 & 74.4 & 53 & 53 \\ +SVD & 64.7 & 86.5/78.6 & / & 26.4 & 88.8 & 85.8 & 85.5/89.0 & 76.5 & 75.8 & 69 & 53 \\ +OPDF (Ours) & **69.1** & **88.4/83.3** & / & **27.2** & **89.8** & **86.5** & **86.9/90.2** & **77.7** & **77.6** & 83 & 53 \\   \\  None & 68.8 & 88.7/84.3 & **89.3** & 53.1 & **91.5** & 90.8 & 85.9/89.5 & 81.7 & 82.4 & 67 & 67 \\ +SVD & 69.4 & 89.3/85.8 & 88.8 & 53.5 & 89.9 & 90.1 & 86.4/89.8 & 81.5 & 82.6 & 91 & 67 \\ +OPDF (Ours) & **71.7** & **90.3/86.8** & 88.9 & **55.0** & 91.3 & **91.1** & **86.8/90.0** & **82.1** & **83.4** & 182 & 67 \\   

Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms “+SVD” and “+OPDF” represent the use of different over-parameterization methods in a KD model. “# Train Params” and “# Inference Params” refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.

efficiency further. To underscore the general applicability of our findings, we intentionally over-parameterize two models: DBKD and LGTM. We then elucidate their relationship with fine-tuning performance on MRPC tasks. All results are depicted in Figure 2(a).

It is observed that the performance of both the LGTM  and DBKD  models on the MRPC task consistently improves with an increase in parameters. This enhancement substantiates the efficacy of using the OPDF for over-parameterizing models, which in turn significantly boosts the performance of knowledge distillation models. Furthermore, after over-parameterization, the performance of the models is capable of achieving, at a minimum, the level of their original benchmarks (_e.g.,_ 83.3 for DBKD and 86.3 for LGTM). The enhancement of model performance through over-parameterization has its limitations. As demonstrated in Figure 2(a), beyond certain thresholds of over-parameterization (_e.g.,_ 1.6\(\) for DBKD and 2.5\(\) for LGTM), the performance improvements of the models no longer exhibit significant gains. This observation indicates that there are inherent limits to the benefits that can be achieved through over-parameterization in knowledge distillation models. These limits are likely influenced by structural characteristics of each model and size of the initial model configuration.

Hyper-parameters TuningOPDF decomposes the original weight tensor through over-parameterization, leading to the updating of more parameters. Consequently, the tensor product results in larger updates to the existing parameters in the backward phase. In Figure 2(b), we illustrate

   &  &  &  &  &  \\  & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & (M) & (M) \\  CLIP-ViT-L/14  & 84.8* & / & 88.9* & / & 75.1* & / & 321 & 321 \\   &  \\  None & 77.4* & 94.1* & 86.1* & 97.5* & 66.8* & 87.6* & 5.4 & 5.4 \\ +SVD & 77.9 & 95.1 & 86.3 & 97.3 & 68.7 & 88.4 & 7.6 & 5.4 \\ +OPDF (Ours) & **80.0** & **96.7** & **87.4** & **98.1** & **69.4** & **88.9** & 9.9 & 5.4 \\   &  \\  None & 80.5* & 95.6* & 87.8* & 98.0* & 70.7* & 90.4* & 11 & 11 \\ +SVD & 82.0 & 96.7 & 88.4 & 97.9 & 71.7 & 91.4 & 17 & 11 \\ +OPDF (Ours) & **82.5** & **96.9** & **88.9** & **98.3** & **72.4** & **92.6** & 23 & 11 \\   &  \\  None & 82.3* & 96.3* & 88.9* & 98.3* & 73.0* & 91.9* & 21 & 21 \\ +SVD & 82.9 & 96.8 & 88.3 & 97.8 & 71.8 & 92.4 & 29 & 21 \\ +OPDF (Ours) & **84.0** & **97.5** & **89.4** & **98.4** & **74.9** & **93.4** & 38 & 21 \\  

Table 2: The linear probe performance (in percentage) of TinyViT, pre-trained on ImageNet-21k, ImageNet-1k , ImageNet Real , and ImageNet v2 . Numbers marked with * indicate that these results are got by official checkpoint and released code.For all the results, we report the mean values of five runs using different random seeds.

Figure 2: The impact of over-parameterization scale, learning rate, and various components of the OPDF on distillation model performance is explored. Figure 2(a) demonstrates the performance of the LGTM and DBKD model on the MRPC task following the implementation of the OPDF. Figure 2(b) presents the performance of DBKD + OPDF with different over-parameterization scales on the MRPC task. Figure 2(c) displays the performance of the theseus model across various tasks, utilizing different over-parameterization methods and integrating various components of the OPDF.

the relationship between the performance on the MRPC task and learning rate when the parameters of the DBKD model are expanded to 1.2\(\), 1.4\(\), and 1.6\(\) their original size.

There exists an optimal learning rate for every scale of over-parameterization. Deviating from this optimal rate, whether by increasing or decreasing the learning rate, results in diminished model performance. The reduction in performance due to a lower learning rate can be attributed to the model becoming trapped in a local optimum.

Additionally, we observe that peak model performance consistently increases with the scale of over-parameterization. This finding aligns with the conclusions drawn from Figure 2(a). Moreover, as the scale of over-parameterization increases, the learning rate required to achieve optimal model performance decreases. This occurs because using the tensor product to restore the shape of the tensors to that of the original weight tensors also scales the updated values, resulting in significant changes. Consequently, an increasing learning rate leads to declining performance in the KD model, indicating that the learning rate should decrease as the over-parameterization scale increases.

Finally, despite changes in the learning rate, the performance of the model with OPDF consistently remains at least as high as that of the original method. This indicates that OPDF is not sensitive to learning rate variations during the distillation stage. This resilience is due to OPDF's ability to factorize the parameter matrix in almost lossless conditions, ensuring that the decomposed matrix can match or exceed the training effectiveness of the original matrix without introducing errors.

Impact of MPO strategyTo demonstrate the robustness of our MPO methods, we applied different MPO methods to the DBKD and AD-KD model on the RTE, MRPC, STS-B, CoLA, and SST-2 task. The experimental results are presented in Table 3. To maintain consistent over-parameterization scales, we used the same decomposition scale (L) for each KD model across the same task.

We can observe that the performance of our approach consistently stabilizes around certain values, indicating that our method is not sensitive to the specific MPO techniques used. Therefore, when over-parameterizing, we should focus primarily on the decomposition scale rather than the MPO method employed.

Ablation StudyOur approach consists of two novel improvements: (1) the over-parameterization procedure for the student model, (2) the distillation loss for auxiliary tensors for effective training. To verify the effectiveness of each component, we conduct the ablation study on the GLUE benchmark to analyze the contribution of each part. We consider removing over-parameterization and distillation loss respectively. The ablation results of our OPDF are shown in Figure 2(c).

Firstly, it is clear that regardless of the over-parameterization method used, the area of the radar chart is greater than that of the vanilla theseus. This outcome suggests that over-parameterization can greatly improve the performance of distillation methods. Secondly, further analysis of the different over-parameterization methods reveals that MPO consistently outperforms SVD across all datasets. This improvement is attributed to MPO's ability to decompose parameter matrices into higher orders, effectively enlarging the size of the parameter matrix. Lastly, we examine the contribution of the \(L_{Aux}\) term. The radar chart area is significantly larger when OPDF is utilized in conjunction with \(L_{Aux}\) than with MPO alone. This indicates that \(L_{Aux}\) effectively enhances knowledge transfer from

   Experiments &  RTE \\ Acc. \\  &  MRPC \\ F1/Acc. \\  &  STS-B \\ Corr. \\  &  CoLA \\ Mcc. \\  & 
 SST-2 \\ F1/Acc. \\  \\    \\  \(L_{A_{6},L,48}\) & 4 & 8 & / & 4 \\ \(_{64,L,48,49}^{16,2,2,12}\) & 69.1 & 88.4/83.3 & / & 27.2 & 89.8 \\ \(_{32,L,2,12}^{16,2,2,12}\) & 68.0 & 86.3/81.0 & / & 25.2 & 89.0 \\ \(_{4,4,2,L,48}^{16,2,2,4}

the teacher model. The underlying reason for this phenomenon is that over-parameterized models can concentrate on learning central tensors containing critical information, while the \(L_{Aux}\) term assists in aligning auxiliary tensors. We can see that removing any component would lead to a decrease in the model performance. It shows the effectiveness of all these components in our approach.

## 6 Conclusion

In this paper, we proposed OPDF, a novel over-parameterization distillation framework designed to enhance the effectiveness of knowledge distillation. This framework employs MPO as a tensor decomposition technique to expand small models into larger ones, thereby bridging the capacity gap between the teacher and student models. Moreover, to enhance the effectiveness of knowledge distillation, our proposed OPDF framework introduces a tensor constraint loss. The OPDF framework utilizes MPO to decompose each weight matrix into a central tensor and auxiliary tensors. By aligning the auxiliary tensors, OPDF not only facilitates the transfer of crucial knowledge from the teacher model but also preserves the student model's ability to think independently. This approach provides the student model with the potential to outperform the teacher model. Our ablation studies demonstrated that all components of the OPDF contribute to enhancing the effectiveness of knowledge distillation. Experimental results across various tasks in natural language processing and computer vision domains validate the efficacy of our proposed method in improving model distillation. Although the number of parameters was increased by MPO during training, the factorized matrices can be merged to reorganize the original parameter matrix in almost lossless conditions. This means that OPDF can enhance the performance of the distillation model without increasing the inference latency. Moreover, since OPDF is based on tensor decomposition, it is orthogonal to most distillation methods.

In our future work, we will investigate more efficient and effective tensor decomposition methods for student model over-parameterization. In addition, we will also apply OPDF to other important backbone models, such as in the multimodal learning domains.

## Impact statement

This paper proposes a novel knowledge distillation framework for model compression field, which is helpful to reduce storage requirements and computational complexity. This method facilitates the practical deployment of models in real-world applications and supports energy conservation. We focus exclusively on over-parameterizing small student models, presenting no potential ethical risks.