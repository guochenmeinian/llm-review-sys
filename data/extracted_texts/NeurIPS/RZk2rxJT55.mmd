# Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study Leaky ResNets, which interpolate between ResNets (\(=0\)) and Fully-Connected nets (\(\)) depending on an 'effective depth' hyper-parameter \(\). In the infinite depth limit, we study'representation geodesics' \(A_{p}\): continuous paths in representation space (similar to NeuralODEs) from input \(p=0\) to output \(p=1\) that minimize the parameter norm of the network. We give a Lagrangian and Hamiltonian reformulation, which highlight the importance of two terms: a kinetic energy which favors small layer derivatives \(_{p}A_{p}\) and a potential energy that favors low-dimensional representations, as measured by the 'Cost of Identity'. The balance between these two forces offers an intuitive understanding of feature learning in ResNets. We leverage this intuition to explain the emergence of a bottleneck structure, as observed in previous work: for large \(\) the potential energy dominates and leads to a separation of timescales, where the representation jumps rapidly from the high dimensional inputs to a low-dimensional representation, move slowly inside the space of low-dimensional representations, before jumping back to the potentially high-dimensional outputs. Inspired by this phenomenon, we train with an adaptive layer step-size to adapt to the separation of timescales.

## 1 Introduction

Feature learning is generally considered to be at the center of the recent successes of deep neural networks (DNNs), but it also remains one of the least understood aspects of DNN training.

There is a rich history of empirical analysis of the features learned by DNNs, for example the appearance of local edge detections in CNNs with a striking similarity to the biological visual cortex , feature arithmetic properties of word embeddings , similarities between representations at different layers [18; 20], or properties such as Neural Collapse  to name a few. While some of these phenomenon have been studied theoretically [3; 8; 27], a more general theory of feature learning in DNNs is still lacking.

For shallow networks, there is now strong evidence that the first weight matrix is able to recognize a low-dimensional projection of the inputs that determines the output (assuming this structure is present) [4; 2; 1]. A similar phenomenon appears in linear networks, where the network is biased towards learning low-rank functions and low-dimensional representations in its hidden layers [13; 21; 29]. But in both cases the learned features are restricted to depend linearly on the inputs, and the feature learning happens in the very first weight matrix, whereas it has been observed that features increase in complexity throughout the layers .

The linear feature learning ability of shallow networks has inspired a line of work that postulates that the weight matrices learn to align themselves with the backward gradients and that by optimizing for this alignment directly, one can achieve similar feature learning abilities even in deep nets [5; 25].

For deep nonlinear networks, a theory that has garnered a lot of interest is the Information Bottleneck , which observed amongst other things that the inner representations appear to maximize their mutual information with the outputs, while minimizing the mutual information with the inputs. A limitation of this theory is its reliance on the notion of mutual information which has no obvious definition for empirical distributions, which lead to some criticism .

A recent theory that is similar to the Information Bottleneck but with a focus on the dimensionality/rank of the representations and weight matrices rather than the mutual information is the Bottleneck rank/Bottleneck structure [16; 15; 30]: which describes how, for large depths, most of the representations will have approximately the same low dimension, which equals the Bottleneck rank of the task (the minimal dimension that the inputs can be projected to while still allowing for fitting the outputs). The intuitive explanation for this bias is that a smaller parameter norm is required to (approximately) represent the identity on low-dimensional representations rather than high dimensional ones. Some other types of low-rank bias have been observed in recent work [9; 14].

In this paper we will focus on describing the Bottleneck structure in ResNets, and formalize the notion of 'cost of identity' as a driving force for the bias towards low dimensional representation. The ResNet setup allows us to consider the continuous paths in representation space from input to output, similar to the NeuralODE , and by adding weight decay, we can analyze representation geodesics, which are paths that minimize parameter norm, as already studied in .

### Leaky ResNets

Our goal is to study a variant of the NeuralODE [6; 23] approximation of ResNet with leaky skip connections and with \(L_{2}\)-regularization. The classical NeuralODE describes the continuous evolution of the activations \(_{p}(x)^{w}\) starting from \(_{0}(x)=x\) at the input layer \(p=0\) and then follows

\[_{p}_{p}(x)=W_{p}(_{p}(x))\]

for the \(w(w+1)\) matrices \(W_{p}\) and the nonlinearity \(:^{w}^{w+1}\) which maps a vector \(z\) to \((z)=([z_{1}]_{+}&&[z_{w}]_{+}&1)\), applying the ReLU nonlinearity entrywise and appending a new entry with value 1. Thanks to the appended \(1\) we do not need any explicit bias, since the last column \(W_{p,\,w+1}\) of the weights replaces the bias.

This can be thought of as a continuous version of the traditional ResNet with activations \(_{}(x)\) for \(=1,,L\): \(_{+1}(x)=_{}(x)+W_{}(_{}(x))\).

We will focus on **Leaky ResNets**, a variant of ResNets that interpolate between ResNets and FCNNs, by tuning the strength of the skip connections leading to the following ODE with parameter \(\):

\[_{p}_{p}(x)=-_{p}(x)+W_{p}(_{p}(x)).\]

This can be thought of as the continuous version of \(_{+1}(x)=(1-)_{}(x)+W_{}(_{ }(x))\). As we will see, the parameter \(\) plays a similar role as the depth in a FCNN.

Finally we will be interested describing the paths that minimize a cost with \(L_{2}\)-regularization

\[_{W_{p}}_{i=1}^{N}\|f^{*}(x_{i})-_{1}(x_{i}) \|^{2}+}_{0}^{1}\|W_{p}\|_{F}^{ 2}dp.\]

The scaling of \(}\) for the regularization term will be motivated in Section 1.2.

This type of optimization has been studied in  without leaky connections, but we will describe in this paper large \(\) behavior which leads to a so-called Bottleneck structure [16; 15] as a result of a separation of time scales in \(p\).

### A Few Symmetries

Changing the leakage parameter \(\) is equivalent (up to constants) to changing the integration range \(\) or to scaling the outputs.

**Integration range:** Consider the weights \(W_{p}\) on the range \(\) and leakage parameter \(\), leading to activations \(_{p}\). Then stretching the weights to a new range \([0,c]\), by defining \(W_{q}^{}=W_{q/c}\) for \(q[0,c]\), and dividing the leakage parameter by \(c\), stretches the activations \(^{}_{q}=_{p/c}\):

\[_{q}^{}_{q}(x)=-}{c}^{}_{q}(x)+ W_{q/c}(^{}_{q}(x))=_{p} _{q/2}(x),\]

and the parameter norm is simply divided by \(c_{0}^{c}\|W^{}_{q}\|^{2}dq=_{0}^{1} \|W_{p}\|^{2}dp\).

This implies that a path on the range \([0,c]\) with leakage parameter \(=1\) is equivalent to a path on the range \(\) with leakage parameter \(=c\) up to a factor of \(c\) in front of the parameter weights. For this reason, instead of modeling different depths as changing the integration range, we will keep the integration range to \(\) for convenience but change the leakage parameter \(\) instead. To get rid of the factor in front of the integral, we choose a regularization term of the form \(}\). From now on, we call \(\) the (effective) depth of the network.

Note that this also suggests that in the absence of leakage (\(=0\)), changing the range of integration has no effect on the effective depth, since \(2=0\) too. Instead, in the absence of leakage, the effective depth can be increased by scaling the outputs as we now show.

**Output scaling:** Given a path \(W_{p}\) on the \(\) (for simplicity, we assume that there are no bias, i.e. \(W_{p, w+1}=0\)), then increasing the leakage by a constant \(+c\) leads to a scaled down path \(^{}_{p}=e^{-cp}_{p}\). Indeed we have \(^{}_{0}(x)=_{0}(x)\) and

\[_{p}^{}_{p}(x)=-(+c)^{}_{p}(x)+W_{p} (^{}_{p}(x))=e^{-cp}(_{p}_{p}(x)-c _{p}(x))=_{p}(e^{-cp}_{p}(x)).\]

Thus a nonleaky ResNet \(=0\) with very large outputs \(_{1}(x)\) is equivalent to a leaky ResNet \(>0\) with scaled down outputs \(e^{-}_{1}(x)\). Such large outputs are common when training on cross-entropy loss, and other similar losses that are only minimized at infinitely large outputs. When trained on such losses, it has been shown that the outputs of neural nets will keep on growing during training , suggesting that when training ResNets on such a loss, the effective depth increases during training (though quite slowly).

### Lagrangian Reformulation

The optimization of Leaky ResNets can be reformulated, leading to a Lagrangian form.

First observe that the weights \(W_{p}\) at any minimizer can be expressed in terms of the matrix of activations \(A_{p}=_{p}(X)^{w N}\) over the whole training set \(X^{w N}\) (similar to ):

\[W_{p}=(A_{p}+_{p}A_{p})(A_{p})^{+}\]

where \(()^{+}\) is the pseudo-inverse.

We therefore consider the equivalent optimization over the activations \(A_{p}\):

\[_{A_{p}:A_{0}=X}\|f^{*}(X)-A_{1}\|^{2}+}_{0}^{1}\|A_{p}+_{p}A_{p}\|_{K_{ p}}^{2}dp.\]

This is our first encounter with the norm \(\|M\|_{K_{p}}=\|M(A_{p})^{+}\|_{F}\) corresponding to the scalar product \( A,B_{K_{p}}=[AK_{p}^{+}B]\) for \(K_{p}=(A_{p})^{T}(A_{p})\) that will play a central role in our upcoming analysis. By convention, we say that \(\|M\|_{K_{p}}=\) if \(M\) does not lie in the image of \(K_{p}\), i.e. \(M^{T}K_{p}\).

It can be helpful to decompose this loss along the different neurons

\[_{A_{p}:A_{0}=X}_{i=1}^{w}\|f^{*}_{i}(X)-A_{1,i} \|^{2}+}_{0}^{1}\|A_{p,i}+ _{p}A_{p,i}\|_{K_{p}}^{2}dp,\]

Leading to a particle flow behavior, where the neurons \(A_{p,i}^{N}\) are the particles. At first glance, it appears that there is no interaction between the particles, but remember that the norm \(\|\|_{K_{p}}\) depends on the covariance \(K_{p}=_{i=1}^{w}(A_{i})(A_{i})^{T}\), leading to a global interaction between the neurons.

If we assume that \(A_{p}^{T}(A_{p})^{T}\), we can decompose the inside of the integral as three terms:

\[}\|A_{p}+_{p}A_{p}\|_{K_{p}^{+}}^ {2}=}{2}\|A_{p}\|_{K_{p}}^{2}+ _{p}A_{p},A_{p}_{K_{p}^{+}}+}\| _{p}A_{p}\|_{K_{p}}^{2}.\]

The middle term \(_{p}A_{p},A_{p}_{K_{p}^{+}}\) plays a relatively minor role in our analysis1, so we focus more on the two other terms:

**Cost of identity \(\|A_{p}\|_{K_{p}}^{2}\) / potential energy \(-}{2}\|A_{p}\|_{K_{p}}^{2}\):** This term can be interpreted as a form of potential energy, since it only depends on the representation \(A_{p}\) and not its derivative \(_{p}A_{p}\). We call it the cost of identity (COI), since it is the Frobenius norm of the smallest weight matrix \(W_{p}\) such that \(W_{p}(A_{p})=A_{p}\). The COI can be interpreted as measuring the dimensionality of the representation, inspired by the fact if the representations \(A_{p}\) is non-negative (and there is no bias \(=0\)), then \(A_{p}=(A_{p})\) and the COI simply equals the rank \(\|A_{p}\|_{K_{p}}^{2}=A_{p}\) (this interpretation is further justified in Section 1.4). We follow the convention of defining the potential energy as the negative of the term that appears in the Lagrangian, so that the Hamiltonian equals the sum of these two energies.

**Kinetic energy \(}\|_{p}A_{p}\|_{K_{p}}^{2}\):** This term measures the size of the representation derivative \(_{p}A_{p}\) w.r.t. the \(K_{p}\) norm. It favors paths \(p A_{p}\) that do not move too fast, especially along directions where \((A_{p})\) is small.

This suggests that the local optimal paths must balance two objectives that are sometimes opposed: the kinetic energy favors going from input representation to output representation in a'straight line' that minimizes the path length, the COI on the other hand favors paths that spends most of the path in low-dimensional representations that have a low COI. The balance between these two goals shifts as the depth \(\) grows, and for large depths it becomes optimal for the network to rapidly move to a representation of smallest possible dimension (not too small that it becomes impossible to map back to the outputs), remain for most of the layers inside the space of low-dimensional representations, and finally move rapidly to the output representation; even if this means doing a large 'detour' and having a large kinetic energy. The main goal of this paper is to describe this general behavior.

Note that one could imagine that as \(\) it would always be optimal to first go to the minimal COI representation which is the zero representation \(A_{p}=0\), but once the network reaches a zero representation, it can only learn constant representations afterwards (the matrix \(K_{p}=^{T}\) is then rank \(1\) and its image is the space of constant vectors). So the network must find a representation that minimizes the COI under the condition that there is a path from this representation to the outputs.

_Remark_.: While this interpretation and decomposition is a pleasant and helpful intuition, it is rather difficult to leverage for theoretical proofs directly. The problem is that we will focus on regimes where the representations \(A_{p}\) and \((A_{p})\) are approximately low-dimensional (since those are the representations that locally minimize the COI), leading to an unbounded pseudo-inverse \((A_{p})^{+}\). This is balanced by the fact that \((A_{p}+_{p}A_{p})\) is small along the directions where \((A_{p})^{+}\) explodes, ensuring a finite weight matrix norm \(\|A_{p}+_{p}A_{p}\|_{K_{p}^{+}}^{2}\). But the suppression of \((A_{p}+_{p}A_{p})\) along these bad directions usually comes from cancellations, i.e. \(_{p}A_{p}-A_{p}\). In such cases, the decomposition in three terms of the Lagrangian is ill adapted since all three terms are infinite and cancel each other to yield a finite sum \(\|A_{p}+_{p}A_{p}\|_{K_{p}}^{2}\). One of our goal is to save this intuition and prove a similar decomposition with stable equivalent to the cost of identity and kinetic energy where \(K_{p}^{+}\) is replaced by the bounded \((K_{p}+ I)^{+}\) for the right choice of \(\).

### Cost of Identity as a Measure of Dimensionality

The cost of identity can be thought of as a measure of dimensionality of the representation. It is obvious for non-negative representations because \(\|A_{p}\|_{K_{p}^{+}}^{2}=\|A_{p}A_{p}{}^{+}\|_{F}^{2}= A_{p}\), but in general, it can be shown to upper bound a notion of'stable rank':

**Proposition 1**.: \(\|A(A)^{+}\|_{F}^{2}^{2}}{\| A\|_{F}^{2}}\) _for the nuclear norm \(\|A\|_{*}=_{i=1}^{A}s_{i}(A)\)._

Proof.: We know that \(\|(A)\|_{F}\|A\|_{F}\), therefore \(\|A(A)^{+}\|_{F}^{2}_{\|B\|_{F}\| A\|_{F}}\|AB^{+}\|_{F}^{2}\) which is minimized when \(B=}{}}\), yielding the result. 

The stable rank \(^{2}}{\|A\|_{*}^{2}}\) is upper bounded by \(A\), with equality if all non-zero singular values of \(A\) are equal, and it is lower bound by the more common notion of stable rank \(^{2}}{\|A\|_{*p}^{2}}\), because \( s_{i} s_{i} s_{i}^{2}\) for the singular values \(s_{i}\).

Note that in contrast to the COI which is a very unstable quantity because of the pseudo-inverse, the ratio \(^{2}}{\|A\|_{*}^{2}}\) is continuous except at \(A=0\). This also makes it much easier to compute empirically than the COI itself.

We know that the COI matches the dimension or rank for positive representations, but it turns out that the local minima of the COI that are stable under the addition of a new neuron are all positive:

**Proposition 2**.: _A local minimum of \(A\|A(A)^{+}\|_{F}^{2}\) is said to be stable if it remains a local minimum after concatenating a zero vector \(A^{}=(A\\ 0)^{(w+1) N}\). All stable minima are non-negative, and satisfy \(\|A(A)^{+}\|_{F}^{2}=A\)._

Proof.: The COI of the nearby point \((A\\  z)\) for \(z(A)^{T}\) equals

\[=\|A(A)^{+}\|^{2}+^{2}\|z^{T} (A)^{+}\|^{2}-^{2}\|(z)^{T}(A)^{+} (A)^{+T}A^{T}\|^{2}+O(^{4}).\]

Assume by contradiction that there is a \(i=1,,N\) such that \((A_{ i}) A_{ i}\), then choosing \(z=(A)^{T}(A_{ i})\) we have \((z)=z\) and the two \(^{2}\) terms are negative:

\[^{2}\|(A_{i})\|^{2}-^{2}\|A_{i}\| ^{2}<0,\]

which implies that \(A^{}\) it is not a local minimum. 

These stable minima will play a significant role in the rest of our analysis, as we will see that for large \(\) the representations \(A_{p}\) of most layers will be close to one such local minimum. Now we are not able to rule out the existence of non-stable local minima (nor guarantee that they are avoided with high probability), but one can show that all strict local minima of wide enough networks are stable. Actually we can show something stronger, starting from any non-stable local minimum there is a constant loss path that connects it to a saddle:

**Proposition 3**.: _If \(w>N(N+1)\) then if \(^{w N}\) is local minimum of \(A\|A(A)^{+}\|_{F}^{2}\) that is not non-negative, then there is a continuous path \(A_{t}\) of constant COI such that \(A_{0}=\) and \(A_{1}\) is a saddle._

This could explain why a noisy GD would avoid such negative/non-stable minima, since there is no 'barrier' between the minima and a lower one, one could diffuse along the path described in Proposition 3 until reaching a saddle and going towards a lower COI minima. But there seems to be something else that pushes away from such non-negative minima, as in our experiments with full population GD we have only observed stable/non-negative local minimas.

### Hamiltonian Reformulation

We can further reformulate the evolution of the optimal representations \(A_{p}\) in terms of a Hamiltonian, similar to Pontryagin's maximum principle.

Let us define the backward pass variables \(B_{p}=-_{A_{p}}C(A_{1})\) for the cost \(C(A)=\|f^{*}(X)-A\|_{F}^{2}\), which play the role of the'momenta' of \(A_{p}\) in this Hamiltonian interpretation, which follows the backward differential equation

\[B_{1} =-_{A_{1}}C(A_{1})=( f^{*}(X)-A_{1})\] \[-_{p}B_{p} =(A_{p})[W_{p}^{T}B_{p}]-B_{ p}.\]

Now at any critical point, we have that \(_{W_{p}}C(A_{1})+W_{p}=0\) and thus \(W_{p}=-}{}_{A_{p}}C(A_{1})(A_{p})^{T}= B_{p}(A_{p})^{T}\), leading to joint dynamics for \(A_{p}\) and \(B_{p}\):

\[_{p}A_{p} =(B_{p}(A_{p})^{T}(A_{p})-A_{p})\] \[-_{p}B_{p} =((A_{p})[(A_{p})B_{p}^ {T}B_{p}]-B_{p}).\]

These are Hamiltonian dynamics \(_{p}A_{p}=_{B_{p}}\) and \(-_{p}B_{p}=_{A_{p}}\) w.r.t. the Hamiltonian

\[(A_{p},B_{p})=}{2}\|B_{p}(A_{p})^{T} \|^{2}-[B_{p}A_{p}^{T}].\]

The Hamiltonian is a conserved quantity, i.e. it is constant in \(p\). It will play a significant role in describing a separation of timescales that appears for large depths \(\). Another significant advantage of the Hamiltonian reformulation over the Lagrangian approach is the absence of the unstable pseudo-inverses \((A_{p})^{+}\).

_Remark_.: Note that the Lagrangian and Hamiltonian reformulations have already appeared in previous work  for non-leaky ResNets. Our main contributions are the description in the next section of the Hamiltonian as the network becomes leakier \(\), the connection to the cost of identity, and the appearance of a separation of timescales. These structures are harder to observe in non-leaky ResNets (though they could in theory still appear since increasing the scale of the outputs is equivalent to increasing the effective depth \(\) as shown in Section 1.2).

The Lagrangian and Hamiltonian are also very similar to the ones in , and the separation of timescales and rapid jumps that we will describe also bear a strong similarity. Though a difference with our work is that the norm \(\|\|_{K_{p}}\) depends on \(A_{p}\) and can be degenerate.

Figure 1: **Leaky ResNet structures: We train equidistant networks with a fixed \(L=20\) over a range of effective depths \(\). The true function \(f^{*}:^{30}^{30}\) is the composition of two random FCNNs \(g_{1},g_{2}\) mapping from dim. 30 to 3 to 30. (a) Estimates of the Hamiltonian constants for networks trained with different \(\). The Hamiltonian refers to \(-\) which estimates the true rank \(k^{*}\). The COI refers to \(_{p}||A_{p}||\). The trend line follows the median estimate for \(-\) across each network’s layers, whereas the error bars signify its minimum and maximum over \(p\). The “stable” Hamiltonians utilize the relaxation from Theorem 4. (b) Spectra of the representations \(A_{p}\) and weights \(W_{p}\) respectively for \(=7\). (c) Hamiltonian dynamics of the network in (b).**Bottleneck Structure in Representation Geodesics

A recent line of work [16; 15] studies the appearance of a so-called Bottleneck structure in large depth fully-connected networks, where the weight matrices and representations of 'almost all' layers of the layers are approximately low-rank/low-dimensional as the depth grows. This dimension \(k\) is consistent across layers, and can be interpreted as being equal to the so-called Bottleneck rank of the learned function. This structure has been shown to extend to CNNs in , and we will observe a similar structure in our leaky ResNets, further showcasing its generality.

More generally, our goal is to describe the'representation geodesics' of DNNs: the paths in representation space from input to output representation. The advantage of ResNets (leaky or not) over FCNNs is that these geodesics can be approximated by continuous paths and are described by differential equations (as described by the Hamiltonian reformulation).

This section provides an approximation of the Hamiltonian that illustrates the separation of timescales that appears for large depths, with slow layers with low COI/dimension, and fast layers with high COI/dimension.

### Separation of Timescales

If \(A_{p}^{T}(A_{p})^{T}\), then the Hamiltonian equals the sum of the kinetic and potential energies:

\[=}\|_{p}A_{p}\|_{K_{p}}^{2} -}{2}\|A_{p}\|_{K_{p}}^{2}.\]

This implies that \(\|_{p}A_{p}\|_{K_{p}}=\|_ {K_{p}}^{2}+}}\) which implies that for large \(\), the derivative \(_{p}A_{p}\) is only finite at \(p\)s where the COI \(\|A_{p}\|_{K_{p}}^{2}\) is close to \(-}\). On the other hand, \(_{p}A_{p}\) will blow up for all \(p\) with a finite gap \(\|_{K_{p}}^{2}+}}>0\) between the COI and the Hamiltonian. This suggests a separation of timescales as \(\), with slow dynamics in layers whose COI/dimension is close to \(-}\) and fast dynamics in the high COI/dimension layers.

But the assumption \(A_{p}^{T}(A_{p})^{T}\) seems to rarely be true in practice, and both kinetic and COI appear to be often infinite in practice. But up to a few approximations, the same argument can be made for stable versions of the kinetic energy/COI:

**Theorem 4**.: _For sequence \(A_{p}^{L}\) of geodesics with \(\|B_{p}^{L}\|^{2} c<\), and any \(>0\), we have_

\[-(}_{,}+c)^{2} -}-_{p}\|A_{p}^{L}\|_{(K_{p}+ )}^{2} c,\]

_for the path length \(_{,}=_{0}^{1}\|_{p}A_{p}^{}\| _{(K_{p}+)}\)dp. Finally_

\[-\|_{p}A_{p}\|_{(K_{p}+ )}-\|_{(K_{p}+)}^{2} +}} 2.\]

Note that the size of \(\|B_{p}^{L}\|^{2}\) can vary a lot throughout the layers, we therefore suggest choosing a \(p\)-dependent \(\): \(_{p}=_{0}\|(A_{p}^{})\|_{op}^{2}=_{0}\|K_{p }\|_{op}^{2}\). There are two motivations for this: first it is natural to have \(\) scale with \(K_{p}\), ; and second, since \(W_{p}=B_{p}(A_{p})^{T}\) is of approximately constant size (thanks to balancedness, see Appendix A.3), we typically have that the size of \(B_{p}\) is inversely proportional to that of \((A_{p})\), so that \(_{p}\|B_{p}\|^{2}\) should keep roughly the same size for all \(p\).

Theorem 4 shows that for large \(\) (and choosing e.g. \(=^{-1}\)), the Hamiltonian is close to the minimal COI along the path. Second, the norm of the derivative \(\|_{p}A_{p}\|_{(K_{p}+)}\) is close to \(\) times the 'extra-COI' \(\|_{(K_{p}+)}^{2}+} }\|_{(K_{p}+)}^{2} -_{q}\|A_{q}\|_{(K_{q}+)}^{2}}\), which describes the separation of timescales, with slow (\( 1\)) dynamics at layers \(p\) where the COI is almost optimal and fast (\(\)) dynamics everywhere the COI is far from optimal.

Assuming a finite length \(_{,}<\), the norm of the derivative must be finite at almost all layers, meaning that the COI/dimensionality is optimal in almost all layers, with only a countable number of short high COI/dimension jumps. These jumps typically appear at the beginning and end of the network, because the input and output dimensionality and COI are (mostly) fixed, so it will typically be non-optimal, and so there will often be fast regions close to the beginning and end of the network. We have actually never observed any jump in the middle of the network, though we are not able to rule them out theoretically.

If we assume that the paths \(A_{p}^{}\) are stable under adding a neuron, then we can additionally guarantee that the representations in the slow layers ('inside the Bottleneck') will be non-negative:

**Proposition 5**.: _Let \(A_{p}^{}\) be a uniformly bounded sequence of local minima for increasing \(\), at any \(p_{0}(0,1)\) such that \(\|_{p}A_{p}\|\) is uniformly bounded in a neighborhood of \(p_{0}\) for all \(\), then \(A_{p_{0}}^{}=_{L}A_{p_{0}}^{}\) is non-negative._

We therefore know that the optimal COI \(_{q}\|A_{q}\|_{(K_{q}+ I)}^{2}\) is close to the dimension of the limiting representations \(A_{p_{0}}^{}\), i.e. it must be an integer \(k^{*}\) which we call the Bottleneck rank of the sequence of minima since it is closely related to the Bottleneck rank introduced in . The Hamiltonian \(\) is then close to \(-}{2}k^{*}\).

Figure 1 illustrates these phenomena: the Hamiltonian (and the stable Hamiltonians \(_{}=}\|_{p}A_{p}\|_{( K_{p}+ I)}^{2}-}{2}\|A_{p}\|_{(K_{p}+ I)}^{2}\)) approach the rank \(k^{*}=3\) from below, while the minimal COI approaches it from above; The kinetic energy is proportional to the extra COI, and they are both large towards the beginning and end of the network where the weights \(W_{p}\) are higher dimensional. We see in Figure 0(c) that the (stable) Hamiltonian are not exactly constant, but it still varies much less than its components, the kinetic and potential energies.

Because of the non-convexity of the loss we are considering, one can imagine that there could exist distinct sequences of local minima as \(\), which could have different rank, depending on what low-dimension they reach inside their bottleneck. Indeed in our experiments we have seen that the number of dimensions that are kept inside the bottleneck can vary by 1 or 2, and in FCNN distinct sequences of depth increasing minima with different ranks have been observed in .

## 3 Discretization Scheme

To use such Leaky ResNets in practice, we need to discretize over the range \(\). For this we choose a set of layer-steps \(_{1},,_{L}\) with \(_{}=1\), and define the activations at the locations

Figure 2: **Discretization: We train networks with a fixed \(=3\) over a range of depths \(L\) and definitions of \(_{}8\). The true function \(f^{*}:^{30}^{30}\) is the composition of three random ResNets \(g_{1},g_{2},g_{3}\) mapping from dim. 30 to 6 to 3 to 30. (a) Test error as a function of \(L\) for different discretization schemes. (b) Weight spectra across layers for adaptive \(_{}\) (\(L=18\)), grey vertical lines represents the steps \(p_{}\) (c) 2D projection of the representation paths \(A_{p}\) for \(L=18\). Observe how adaptive \(_{}8\) appears to better spread out the steps.**

\(p_{}=_{1}++_{}\) recursively as

\[_{p_{0}}(x) =x\] \[_{p_{}}(x) =(1-_{})_{p_{-1}}(x)+_{}W_{p_{ }}(_{p_{-1}}(x))\]

and the regularized cost \(()=C(_{1}(X))+_{=1}^{L}_{ }\|W_{p_{}}\|^{2}\), for the parameters \(=(W_{p_{1}},,W_{p_{L}})\). Note that it is best to ensure that \(_{}\) remains smaller than \(1\) so that the prefactor \((1-_{})\) does not become negative, though we will also discuss certain setups where it might be okay to take larger layer-steps.

Now comes the question of how to choose the \(_{}\)s. We consider three options:

**Equidistant:** The simplest choice is to choose equidistant points \(_{}=\). Note that the condition \(_{}L<1\) then becomes \(L>\). But this choice might be ill adapted in the presence of a Bottleneck structure due to the separation of timescales.

**Irregular:** Since we typically observe that the fast layers appear close to the inputs and outputs with a slow bottleneck in the middle, one could simply choose the \(_{}\) to be go from small to large and back to small as \(\) ranges from \(1\) to \(L\). This way there are many discretized layers in the fast regions close to the input and output and not too many layers inside the Bottleneck where the representations are changing less. More concretely one can choose \(_{}=+(-- )\) for \(a[0,1)\), the choice \(a=0\) leads to an equidistant mesh, but increasing \(a\) will lead to more points close to the inputs and outputs. To guarantee \(_{}<1\), we need \(L>(1+a)\).

**Adaptive:** But this can be further improved by choosing the \(_{}\) to guarantee that the distances \(\|A_{}-A_{-1}\|/\|A_{p}\|\) are approximately the same for all \(\) (we divide by the size of \(A_{p}\) since it can vary a lot throughout the layers). Since the rate of change of \(A_{p}\) is proportional to \(_{}\) (\(\|A_{}-A_{-1}\|/\|A_{p}\|=_{}c_{}\)), it is optimal to choose \(_{}=^{-1}}{ c_{}^{-1}}\) for \(c_{}=\|A_{}-A_{-1}\|/_{}\|A_{p}\|\). The update \(_{}^{-1}}{ c_{}^{-1}}\) can be done at every training step or every few training steps.

Note that the condition \(_{}<1\) might not be necessary inside the bottleneck since we have the approximation \(W_{p}(A_{p_{-1}})A_{p_{-1}}\), canceling out the negative direction. In particular with the adaptive layer-steps that we propose, a large \(_{}\) is only possible for layers where \(c_{}\) is small, which is only possible when \(W_{p}(A_{p_{-1}})A_{p_{-1}}\).

Figure 2 illustrates the effect of the choice of \(_{}\) for different depths \(L\), we see a small but consistent advantage in the test error when using adaptive or irregular \(_{}\)s. Looking at the resulting Bottleneck structure, we see that the adaptive \(_{}\)s result in more steps especially in the beginning of the network, but also at the end. This because the 'true function' \(f^{*}:^{30}^{30}\) we are fitting in these experiments is of the form \(f^{*}=g_{3} g_{2} g_{1}\) where the first inner dimension is 6 and the second is 3, thus resulting in a rank of \(k^{*}=3\). But before reaching this minimal dimension, the network needs to represent \(g_{2} g_{1}\), which requires more layers, and one can almost see that the weight matrices are roughly \(6\)-dimensional around \(p=0.3\). The adaptivity to this structure could explain the advantage in the test error.

## 4 Conclusion

We have given a description of the representation geodesics \(A_{p}\) of Leaky ResNets. We have identified an invariant, the Hamiltonian, which is the sum of the kinetic and potential energy, where the kinetic energy measures the size of the derivative \(_{p}A_{p}\), while the potential energy is inversely proportional to the cost of identity, which is a measure of dimensionality of the representations. As the effective depth of the network grows, the potential energy dominates and we observe a separation of timescales. At layers with minimal dimensionality over the path, the kinetic energy (and thus the derivative \(_{p}A_{p}\)) is finite. Conversely, at layers where the representation is higher-dimensional, the kinetic energy must scale with \(\). This leads to a Bottleneck structure, with a short, high-dimensional jump from the input representation to a low dimensional representation, followed by slow dynamics inside the space of low-dimensional representations followed by a final high-dimensional jump to the high dimensional outputs.