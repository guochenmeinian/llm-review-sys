# A Practitioner's Guide to Continual Multimodal Pretraining

Vishaal Udandarao\({}^{1,3*}\)  Karsten Roth\({}^{1,2,6*}\)  Sebastian Dzialozio\({}^{1}\)  Ameya Prabhu\({}^{1}\)

Mehdi Cherti\({}^{4}\)  Oriol Vinyals\({}^{5}\)  Olivier Henaff\({}^{5}\)

Samuel Albanie\({}^{}\)  Zeynep Akata\({}^{2,6,7}\)  Matthias Bethge\({}^{1}\)

###### Abstract

Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications demand adaptation to specific subdomains, tasks or concepts -- spread over the entire, varying life cycle of a model. In this work, we _complement current perspectives on continual pretraining through a research test bed and offer comprehensive guidance for effective continual model updates in such scenarios_. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over \(63\) datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) data mixtures and stream orderings that emulate real-world deployment settings, (2) methods ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta-learning-rate schedules and mechanistic design choices, and (4) model and compute scaling. Together, our insights provide a _practitioner's guide to continual multimodal pretraining_ for real-world deployment. Benchmark and code is provided here: github.com/ExplainableML/fomo_in_flux.

## 1 Introduction

Foundation models  require vast datasets and computational resources to train . Despite these substantial investments, models often have limited concept coverage , and quickly become outdated as new tasks emerge. To stay relevant, they need _continual pretraining_, which falls into two high-level categories: (1) infrequent, large-scale updates with substantial new data and compute , and (2) frequent, but minimal updates that target specific information, e.g. via knowledge editing or updating knowledge bases in retrieval-augmented systems . However, many real-world applications operate in the large gap between these cases; calling for specialized knowledge (fine-grained expertise or semantic and visual distribution shifts ) that goes beyond simple edits, but does not warrant full retraining. Under the semantic versioning framework , such specialized minor updates go beyond simple patches, but do not justify major version updates. In this work, we provide a novel framework to emulate practical deployment scenarios for vision-language foundation models in a controlled environment, and study how continual pretraining can succeed:_Creating FoMo-in-Flux_ (_Foundation-Models-in-Flux_, Fig. 1), which enables a controlled study of _minor_ updates of multimodal models over a long life cycle and builds on 63 image classification and retrieval datasets enhanced with captions for multimodal pretraining. Unlike noisy web-crawl datasets (e.g. TiC-RedCaps, DataComp [49; 45]), FoMo-in-Flux comprises curated samples with fine-grained class information for precise control of data streams over visual/semantic domains.

_Realistic Continual Pretraining._ Unlike traditional continual learning, we eschew the _practically irrelevant restriction of limited storage_[135; 136], and allow unrestricted access to pretraining data. As deployment cost is primarily a function of computation, we only impose a restriction on compute budgets. To avoid skewed compute metrics [38; 118], we introduce _Memory-Adjusted FLOPs (MAFs)_, which account for FLOPS in forward and backward passes and peak accelerator memory.

_Methods and Training Recipes for Continual Pretraining._ Using FoMo-in-Flux, we study current research strategies for multiple _minor_ continual pretraining updates -- from continual learning (CL) regularization strategies (EWC , SI ), simple finetuning, parameter-efficient adaptation (LoRA , VeRA ), to model merging . We also show the importance of strategies beyond method choices, such as learning rate scheduling, and propose meta schedules for long-term model updates. Moreover, we study model and compute scaling for continual pretraining, and give an overview of important experimental design choices for continual multimodal pretraining pipelines.

_A Data-centric Perspective on Continual Pretraining._ Concepts and tasks arise in sequence, driven by the ongoing discovery of model shortcomings or desiderata from feedback loops . Our fine-grained control over the sequence of semantic and visual concepts allows us to simulate realistic data streams to better understand how different concept and task orderings affect accumulation of new and retention of existing knowledge. Finally, we provide insights into data mixtures on the accumulation and retention trade-off as new concepts and subdomains are introduced.

Based on our experiments, we collate key practical insights for _continual multimodal pretraining_:

[leftmargin=*]

An Abbreviated Predefined's Guide to Continual Multimodal Pretraining.

1. Method Choices. Under practical update and compute restrictions, continual learning and parameter-efficient fine-tuning methods favor knowledge retention (stability) while simple fine-tuning focuses on adaptation (plasticity). In combination with **model merging**, fine-tuning sufficiently addresses this trade-off for strong knowledge retention _and_ adaptation. 2. Meta Learning Rate Schedules. Learning rates matter, and can naturally be accounted for in long-horizon continual pretraining via **meta learning rate schedules** across incoming tasks; reducing the loss of pretraining knowledge while encouraging high adaptation. Maintaining the same learning rate schedule between pretraining and updates is less important. 3. Model and Compute Scaling. Simple fine-tuning does not scale well with increased compute resources or more frequent updates, unlike fine-tuning with model merging. On the other hand, **increasing model size** helps it acquire new knowledge while retaining its foundational properties, even within the same compute budget. 4. Data-centric Stream Orderings. The **order** in which data updates are applied significantly impacts the model's ability to learn new information and retain its zero-shot capabilities. This is important to account for during deployment. However, when underlying data distributions are the same, models converge to **comparable final performance** across update sequences. 5. Data mixture ratio. The ratio between pretraining-, update-, and buffer data affects the model's final performance, and "IID-fying" knowledge accumulation is crucial: Replaying previous adaptation tasks is more relevant that pretraining replay.

## 2 Categorizing Continual Pretraining: A Versioning Perspective

Traditional continual learning is categorized into class-, domain-, and task-incremental settings . However, continual pretraining benchmarks do not fit these categories, as they exhibit high-overlaps in captions as opposed to disjoint classes [76; 15; 102], and time-varying gradual class/domain shifts [49; 103; 135; 103; 189]. Similarly, continual learning strategies are typically grouped [35; 134] into replay [25; 20], regularization [121; 86; 24], and parameter-isolation methods [224; 3; 227], with recent additions like prompt-tuning [193; 194; 168; 141], fixed-representation [116; 222; 138], and model-mixture methods [114; 78] (see  for overview). However, continual foundation model updates are dominated by replay [136; 49], parameter-efficient finetuning  and retrieval-augmented methods [185; 135; 57], as traditional methods fail under compute constraints [63; 183; 135], even underperforming simple baselines [138; 116; 136; 215]. Hence, we provide a new categorization suitable for continual pretraining literature., inspired by the semantic software versioning framework . We believe that different scopes of updates require distinct strategies, indicating that no one solution fits all scenarios (see  for a survey, and table 1 for an overview of related benchmarks under the semantic versioning umbrella). We believe foundation models require distinct update strategies, similar to major, minor, and patch updates in software versioning:

**Major Updates.** Large-scale continual pretraining over extensive compute, data, and time resources that substantially alter overall performance. Methods focusing on significant updates [49; 76; 51] consistently employ continual fine-tuning of the model, which has been found to be the primary strategy through extensive comparisons with other works [49; 126; 136; 27]. Currently explored topics include continual LR scheduling [59; 76; 211; 127; 74] to minimize the stability gap .

**Patch Updates.** Frequent but minor, targeted updates in which continual fine-tuning leads to poor zero-shot capability retention with little new knowledge gained. These are best managed by continual knowledge editing [28; 191] or sample-wise updates using a fixed backbone [135; 228; 57; 116; 52].

**Minor Updates.** Adapations to whole subdomains and general concepts out of scope for knowledge edits and major updates. Some examples: updating specific parts of a model with LoRA [62; 12; 109; 196], model merging [78; 174; 187], instruction tuning [64; 218; 26], or incorporating expert knowledge [87; 216; 179; 164; 117; 139; 156; 56; 225; 47; 137]), adding visual reasoning over fine-grained object categories [9; 186; 79; 130; 125; 169], or new domains like sketches [30; 129], drawings [129; 99], or synthetic [19; 115] and medical imagery [77; 41]. Multimodal minor updates can also jointly involve new or infrequently encountered concepts [19; 115], s.a. aforementioned fine-grained expert knowledge, medical terminology or new compositions .

## 3 The FoMo-in-Flux Benchmark

We introduce FoMo-in-Flux (_Foundation-Models-in-Flux_), a benchmark for controlled continual _multimodal_ pretraining. Our benchmark extends beyond monolithic pretraining datasets, such as TiC-RedCaps/TiC-DataComp , to specialized subdomains with fine-grained control over data streams and adaptation over long horizons. Table 1 extensively compares FoMo-in-Flux to related benchmarks, showcasing key features that distinguish it from existing works. For the exact experimental setup, including base models and exact derivation of _MAF_ budgets, refer appendix H.1.

### Creation

**Breakdown.** FoMo-in-Flux consists of \(63\) classification and retrieval datasets--either publicly available or introduced as part of this work--covering \(2.53M\) samples and \(23,045\) concepts spanning

  
**Benchmark** & **\# Samples** & **\# Tasks** & **Ordering** & **Domains** & **Update** & **Multi-** & **Zero-Shot** & **Compute-** & **Data-** & **Road World** \\  & & & & & **Style** & **modal** & **Restraints** & **Bound** & **Mursure** & **Stream Variants** \\  COR650  & 165R & 9 & Clus-Data-Inc & Objects & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ Split-ImageNet  & 1,2M & 0 & Clus-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ PTM-Adaptation  & 30k-100k & 5-20 & Clus-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ CLAD  & 23k & \(\)200k & Time-Inc & Synthetic & Patch & \(\) & \(\) & \(\) & \(\) & \(\) \\ OAK  & 30k-200k & -200k & Time-Inc & Egocentric & Patch & \(\) & \(\) & \(\) & \(\) & \(\) \\ De-PASCLA  & 11k & 2-6 & Clus-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ Inc-ADE2021  & 20k & 2-6 & Clus-Inc & Scene Parsing & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ StreamingQA  & 100k & 6 & Time-Inc & Text & Mixper & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ Temporal(Wish ) & 32M & 4 & Time-Inc & Text & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ CKL  & 30k & 2 & Task-Inc & Text & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ CTL  & 500k & 100k & Tag-Inc & Objects & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ CLEAN  & 7.8M & 10 & Time-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ ImageNet(Xie)  & 1,2M & 20,200k & Class-Inc-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ Online-CQLML & 50k & 20,30k & Time-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ ILR-PQA  & 62k & 5 & Clus-Inc-Inc & Web Images & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ NEVIS  & 6M & 2 & Task-Inc & Mind & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ COCO  & 30k & 39k & Time-Inc & Geodesic & Patch & \(\) & \(\) & \(\) & \(\) & \(\) \\ CQLAN  & 500k & 500k & Time-Inc & Landmarks & Patch & \(\) & \(\) & \(\) & \(\) & \(\) \\ CLAN  & 1,3M & 4 & Task-Inc & Mixed & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ MTL  & 20k & 5,20k & Class-Inc & Mixed & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ C-MLP  & 6,6B & 160 & Domains-Inc & Text & Mixper & \(\) & \(\) & \(\) & \(\) & \(\) \\ TC-DoneComp  & 100k/18/12B & 6 & Time-Inc & Web Images & Mixper & \(\) trained on a large pretraining dataset \(\), and an empty buffer \(\). Within the allocated update budget, at each update step \(j\{1,2,,T\}\), the following happens in order: **(1)** The stream reveals a task update pool of \(n_{j}\) image-text pairs \(_{j}=\{(i_{k}^{j},t_{k}^{j})\}_{k=1}^{n_{j}}\) spanning \(_{j}\) concepts. **(2)** We create the training data mixture \(_{j}\) by sampling from the pretraining data \(\), buffer \(\), and current task data \(_{j}\) with respective ratios \(_{},_{},\) and \(_{}\), such that \(_{}+_{}+_{}=1\). If samples in \(\) are insufficient (particularly at the start of task adaptation), we oversample from \(_{j}\), with \(_{}\) fixed. **(3)** We apply a continual update method \(\) with a fixed compute budget \(F\): \(_{j}{=}(,_{j},_{j-1})\). This compute budget \(F\) also determines the overall number of update steps conducted. **(4)** We add samples from the update pool \(_{j}\) to the unrestricted buffer \(\). However, while all samples can be stored in buffer \(\), they cannot all be sampled for training set \(\), as the compute budget \(F\) imposes an implicit memory restriction .

**How to Measure Continual Pretraining Computational Cost?** We impose a fixed compute budget for each update step to account for each method's efficiency. Recent works use number of iterations (forward/backward passes) , number of updated parameters , FLOPs , and

Figure 1: **FoMo-In-Flux pipeline.**_(Pretraining)_ We start from pretrained CLIP \(_{0}\) and its pretraining pool \(\). _(Update steps)_ At each step \(t\), we sample training instances \(_{t}\) from \(\), current update pool \(_{t}\), and memory buffer \(\) (containing all past \(_{t}s\)), and train for a fixed compute budget (\(F\) MAFs).

throughput . However, a single metric does not paint a complete picture of efficiency [38; 118]. To account for this, we introduce _Memory-Adjusted-FLOPs (MAFs)_, a novel metric that highlights two aspects most relevant from a practitioner's perspective: total number of FLOPs per iteration and maximum utilization of device memory. To compute MAFs, which determines the number of updates a model can take, we multiply FLOPs of each method by the ratio of that method's maximum memory utilization to the maximum memory utilization of a full fine-tuning of the base model.

**Data Restrictions.** We allow unrestricted access to pretraining data (e.g., LAION-400M), and an unlimited replay buffer \(\), as data storage is a negligible contributor to real-world cost [135; 136], and buffer memory is only utilized during the continual pretraining process. To study different retraining data pools, we use four popular image-text pretraining datasets of varying sizes, quality and curation strategies--LAION-400M, CC-12M, CC-3M, and DataComp-Small1.

**Metrics & Plots.** We study _adaptation_ to new data and _retention_ of pretraining knowledge, measured as Knowledge Accumulation (\(_{KA}\)), the avg. accuracy (recall@5 for retrieval) over concepts in all \(41\) adaptation datasets, and Zero-Shot Retention (\(_{ZS}\)), the zero-shot transfer on the held-out datasets. In most plots, we depict the zero-shot baseline as black star and the joint training upper-bound as golden star. A dotted connection approximates the joint training trajectory on the \(_{KA}\)-\(_{ZS}\) plane.

### Designing Data-Centric Task-Sequences

In addition to studying different pretraining sets \(\) and data mixture ratios \((_{},_{},_{})\), we also investigate different realistic orderings by breaking down FoMo-in-Flux into individual concepts, and then ordering based on a chosen criterion (including an option to reverse orderings). This is visualized in Fig. 10. To do so, having a controlled set of image-caption pairs is critical, as it allows for well-defined and meaningful arrangement of concepts into sequences according to an ordering \(()\). Each ordering \(\) divides the set of samples \(\) into \(T\) disjoint subsets \(\{_{1},,_{T}\}\) of concepts \(\) sampled without replacement, i.e. \(_{i}_{j}=\), \( i,j\). We define six different orderings:

**1. Easy-To-Hard Ordering (performance)** is motivated by curriculum learning [58; 154; 170; 208], assuming users deploying their model to easier concepts and usecases first, with incremental movement towards to harder concepts. **2. Concept Frequency Ordering (concept-frequency)** draws motivation from Udandarao et al. , starting from least frequent concepts first (edge cases that are most likely to cause undesired performance drops) and incrementally extending to more frequent concepts represented well in the pretraining pool. **3. Concept Similarity Ordering (similarity)**, inspired by Yildiz et al. , is based on the hypothesis that training on conceptually similar tasks allows users to minimize catastrophic forgetting over tasks. **4. Time-incremental Ordering (time)**, inspired by [15; 73; 21; 135; 49], arranges in chronological order. **5. Dataset-Incremental Ordering (dataset)** is motivated by [148; 111; 112; 190; 206], but extended to a larger sequence of datasets. This sequence is then broken down into the desired number of tasks \(T\). **6. Random Ordering (random)** (e.g. [149; 200; 70; 136]) mimics a scenario where user requests for model improvement are unstructured. For this ordering, we simply shuffle class names at random.

## 4 Continual Pretraining: A Method Perspective

We first explore how different methods affect knowledge accumulation and zero-shot retention. We excluded prompt-tuning-based continual learning methods, which often collapse to a single prompt  or near-chance performance over a longer time horizon . Similarly, we do not include distillation-based CL methods, as they do not show improvements when memory is unrestricted . For details on each tested method, we refer to the supplementary.

### Parameter-efficient Finetuning and Continual Learning

We study _parameter-additive_ methods (LoRA, VeRA, DoRA) and _parameter-selective_ approaches tuning only particular weight subsets (LNFit, BitFit). We also investigate recently proposed low-rank approximations to model gradient updates (GaLore). We further examine how prior continual learning methods such as Elastic Weight Consolidation (EWC) or Synaptic Intelligence (SI) perform at scale. To begin, we find two extreme points:

**(1) Strongest accumulation, weakest retention.** Naive contrastive finetuning (in orange, fig. 2 left) which achieves strongest knowledge accumulation \(_{}\) across a full update cycle, at the cost of a significant drop in zero-shot retention \(_{}\) even with learning rate rewarming , following best practices sketched out in . We update both the image and language branch, and initialize from the pretraining temperature (c.f. appendix D.3). Importantly, naive finetuning falls victim to "longer-horizon" stability gap issues , where forgetting is high and achievable knowledge gain is strongly limited across initial update "steps" (each step being a whole compute-budgeted training cycle, c.f. appendix B.3). **(2) Weakest accumulation, strongest retention.** Parameter-selective methods like LNFit (green) and BitFit (blue, fig. 11 center) exhibit good knowledge retention, but minimal capacity for the accumulation of new knowledge across longer and complex data streams.

All other methods operate between these end points, trading off knowledge accumulation and retention: **(3) Strong accumulation, weak retention.** By only modifying the naturally lower-rank gradient updates during model training, GaLore (olive green, fig. 2 left) offers a moderate balance between the ability to effectively incorporate new knowledge within a given compute budget, and retaining original zero-shot generalization behaviour. **(4) Decent accumulation, decent retention.** Parameter-efficient tuning methods such as LoRA (blue, fig. 2 left) and DoRA (pink, fig. 11 right) provide an effectively linear reduction in knowledge accumulation and forgetting w.r.t. to base finetuning and GaLore. This aligns with recent insights on LoRA effectively both learning and forgetting less in single domain finetuning tasks . However, VeRA (dark blue, fig. 11 right), which significantly reduces the number of tunable parameters, behaves closely to parameter-selective tuning methods, offering very little knowledge gain across long and complex data streams.

Finally, for continual learning regularization methods we find that while EWC (pink, fig. 2 left) significantly improves zero-shot retention, it also offers extremely limited \(_{}\) compared to the initial zero-shot performance. On the other hand, the popular regularisation method SI (purple, fig. 2 left) effectively offers no benefits over standard finetuning, either in \(_{}\) or \(_{}\). The poor performance of regularisation-based methods is curious as prior work has hinted at their benefits at scale [121; 85]. However, our fine-grained, and most importantly compute-controlled FoMo-In-Flux helps verify these claims, as these regularization mechanisms are both compute- and memory-expensive.

### On the Benefits of Model Merging Techniques

Model merging is a promising avenue for adapting foundation models [172; 78; 197], enabling efficient aggregation of multiple models [203; 157; 34; 5]. Initial work  also highlighted potential benefits for small-scale continual learning. To study benefits at scale, we investigate three forms of model merging. Denoting model weights going into task \(t\) as \(_{t-1}\), finetuned weights after task \(t\) as \(_{t}^{}\), and final model-merged output after task \(t\) as \(_{t}\), we define (c.f. fig. 16 for details):

**(1) Exponential-moving averaging (EMA-merge)**, as adopted in Stojanovski et al. , which tunes the previously merged task weights \(_{t-1}\) on task \(t\) to produce the finetuned weights \(_{t}^{}\), and then merges \(_{t-1}\) with \(_{t}^{}\) to produce \(_{t}\). **(2) Continual fine-tuning and merging** (Finetune-merge) derived from multi-model patching in Ilharco et al. ), which produces \(_{t}\) by merging the original pretraining weights \(_{0}\) and the finetuned weights \(_{t}^{}\). To obtain \(_{t}^{}\), Finetune-merge tunes the previously merged model weights \(_{t-1}\), same as EMA-merge. **(3) Continual zero-shot merge**

Figure 2: **Which methods to use for continual pretraining over long update cycles?**_(Left)_ An in-depth study across five different method families: Continual finetuning (Full-FT ) and parameter-selective tuning (LNFit ) provide the extreme points in knowledge accumulation and retention. Switching from GaLore  to parameter-efficient tuning (LoRA) and continual learning methods (EWC , SI ) provides near linear transition points between both extremes. _(Right)_ Judiciously merging model weights exhibits unique long-horizon continual pretraining behaviour, allowing for significantly consistent accumulation across update tasks with maximal retention.**

(ZeroShot-merge), a simple ablative merging protocol, which tunes the original pretraining weights \(_{0}\) during each task \(t\) and produces \(_{t}\) by merging \(_{t-1}\) and the finetuned \(^{}_{t}\). Each merge method uses an old-new weight mixing coefficient \(w\), which we ablate over \(w{=}\{0.85,0.9,0.95\}\).

As shown in fig. 2 (right), we find that the EMA-merge (blue) and ZS-merge (green), provide impressive boosts in zero-shot retention rates \(_{}\) during the first update tasks, and _retain slight gains_ over the entire update cycle. Moreover, this is coupled with strong knowledge accumulation \(_{}\), though not yet at the level of standard finetuning. As expected, ablating the mixing weight \(w\) yields a trade-off between zero-shot retention and knowledge accumulation--higher \(w\)s provide better zero-shot retention capabilities while compromising on the accumulation \(_{}\). However, across both ablated mixing ratios, as well as the merging mechanism, we find that the high-level continual pretraining dynamics remain the same--at worst limited loss (and at best notable gains) in retention coupled with strong accumulation capacities, while also breaking favorably with the hypothetical linear trade-off between the initial zero-shot performance and the joint finetuning upper-bound!

## 5 Continual Pretraining: General Training Recipes

This section studies the other degrees of freedom orthogonal to methodological update strategies that co-occur within a continual pretraining pipeline: **(1)** The importance of the learning rate and its scheduling in section 5.1 and its translation to meta-learning rate schedules for continual pretraining tasks. **(2)** The impact of both model and compute scaling as independent axes to optimize and account for when planning to deploy a model over longer minor update cycles. More precisely, section 5.2 evaluates the impact on the knowledge accumulation and the zero-shot retention trade-off as a function of both increased model sizes within the same model family, as well as increases in the allocated compute budget within a fixed model size. **(3)** Moreover, the supplementary provides studies on the relevance of locked image and text encoder tuning, as well as the importance of aligning initial and continual pretraining softmax temperature in order to minimize stability gap issues.

### Learning Rates, Schedules and Meta-Schedules

By default, LR schedules apply to each task individually . As open_clip models use cosine schedules, we first study the impact of re-applying these for each task:

\[_{n}=_{}+}}(_ {}-_{})&n<N_{}\\ _{}+(_{}-_{})(1+ (}}{N_{}-N_{}}) )\] (1)

with \(_{n}[_{},_{}]\) the learning rate at step \(n\), and \(N_{}\) the number of update steps for a given task. As recommended in e.g. Ibrahim et al. , we utilize linear warmup to the initial pretraining peak learning rate \(_{}\) used in Cherti et al.  for \(N_{}\) iterations. To study the impact of a learning rate schedule switch to e.g. infinite learning rate variants for potentially more flexibility down the line, we investigate a switch towards reciprocal square root schedule (_rsqrt_) introduced in Zhai et al. 

\[_{n}=_{}+}}(_{ }-_{})&n N_{}\\ _{}}}}{}}} &n[N_{},N_{}-N_{}]\\ _{N_{}-N_{}}}-(n+N_{})}{N_{}}&\] (2)

Figure 3: **Visualization of different deployed learning rate schedules**, from task-independent _cosine_ and infinite learning rate schedules (_Rsqrt_), to task-dependent meta learning rate schedule.

Note that _rsqrt_ scheduling includes a separate cooldown section, wherein the last \(N_{}\) steps are used to linear cooldown the previously decayed learning rate. Both schedules are visualized in fig. 3 (left and right) over multiple tasks, and the result of either application (matching and changing the pretraining learning rate scheduler) to our 20 task update cycle stream is visualized in fig. 4 (center). As can be seen, there is a negligible change in knowledge accumulation \(_{KA}\) and knowledge retention for either learning rate scheduler; highlight that across longer update cycles, matching the original pretraining scheduler is of lesser importance.

**Meta Learning Rate Schedules.** By default, each intermediate update is treated independently (c.f. fig. 3 (left)): each task rewarms and cools down the same. However, as these updates appear in succession, catastrophic forgetting of previously seen tasks has to also be accounted for; while with every task update, the model is encouraged to move further away from its pretraining starting point. To reduce the impact of task-level forgetting and the increased shift from pretraining, we introduce meta LR scheduling - task-level schedules over each task-specific, iteration-level LR schedule to account for task continuity. These derive _naturally and hyperparameter-free_ by theoretically extending previous task schedule across all the new tasks (see gray hypothetical schedules in fig. 3 (_center_)). We explore four meta-schedules: **(i)**_autoregressive cosine scheduling_, which selects each task \(_{}\) by building a hypothetical cosine schedule with warmup across all seen tasks and sets it to the intersection point with the warmup process of the current task (Fig. 3 center):

\[_{}^{T}=^{}(n^{}=N_{}^{T}+ _{t}^{T-1}N_{}^{t},N_{}^{}=_{t}^{T}N_{ {task}}^{t})\] (3)

where \(^{}(,)\) defines the LR returned by the standard cosine LR schedule with warmup at point \(n^{}\) for \(N_{}^{}\) total iterations. Using the same formulation, we also test **(ii)**_autoregressive continued dynamic_ schedule, which warms up to the same \(_{}^{T}\), but continues the schedule following the hypothetical cosine schedule over all total previous steps \(N_{}\) and the current post-warmup steps \(N_{}\). This autoregressive scheduling is naturally extended to the **(iii)**_autoregressive sqrt schedule_, which sets \(_{}=^{}(n^{},N_{}^{})\), and **(iv)** which continues the dynamics of a hypothetically extended base schedule ("_Continued Dynamic_"). Finally, we also introduce **(v)**_"Peaks match Rsqrt_", where respective \(_{}\) matches the continued dynamics while continuing with a standard rsqrt schedule.

**The impact of task- and meta-level learning rate schedules for continual model updates** are visualized in Fig. 4 on the default 20-task variation of FoMo-in-Flux using simple continual finetuning as our reference approach. Indeed, for longer continual pretraining sequences, switching from task-independent to meta learning rate schedules notably changes the accumulation versus retention tradeoff behaviour. While within different meta-schedules variations there is limited difference, as shown in fig. 4 (_left_ and _right_), meta-learning rate schedules allow for significantly better retention of initial zero-shot transfer performance. In the case of meta-schedules deriving from cosine learning rate schedules, there is a severe reduction in accumulated new knowledge due to the fast reduction in the learning rate (fig. 3_left_). Meta-schedules deriving from infinite learning rate schedules like _rsqrt_ lend themselves much better to longer-horizon continual pretraining tasks due to the less aggressive decay in learning rate within tasks: As shown in fig. 3 (_right_), the autoregressive _rsqrt_ meta-schedule achieves strong gains in \(_{KA}\), while _vastly increasing the amount of retained knowledge_; exceeding the hypothetical linear zero-shot vs joint finetuning trade-off line.

Figure 4: **Meta-scheduling task-specific LR scheduler has significant impact on the knowledge accumulation and retention trade-off, with meta-schedules derived from infinite LR schedules showing significant transitions across the zeroshot vs finetuning threshold; moving close to accumulation performance of task-independent scheduling, but retaining significantly more pretraining knowledge.**

### Scaling up Model and Compute Budgets

To understand the impact of both model and compute scaling on the ability to continual pretrain over longer update cycles, we adjust either the underlying vision transformer size (keeping the number of update steps and task iterations fixed, and covering ViT-S/16 \([62.3M]\), B/16 \([149.6M]\), L/14 \([427.62M]\), H/14 \([986.11M]\) and g/14 \([1366.68M]\) taken from ) or the allocated compute budget for a fixed model size (selecting our default ViT-B/16 and the default derived finetuning compute budget of \(1.8 10^{9}\) FLOPs as reference). Results for both are provided in fig. 5.

Scaling Model Size.As can be seen, we find that with a controlled increase of model size, the ability to continually pretrain over longer minor update cycles improves. While the absolute change in knowledge accumulation \(_{KA}\) remains rather consistent (within the interval of \(8\%\) and \(10\%\)), zero-shot retention \(_{ZS}\) improves - where both for the joint finetuning upper bound and continual pretraining, we see improved knowledge retention, and in parts even slight positive backward transfer for ViT-L14 (\(3\) ViT-B/16). For ViT-B/16, we see \(_{KA} 9.0\%\) and negative \(_{ZS} 3.2\%\), while for larger L/14, H/14 and g/14 we find \((_{KA}^{} 9.4,_{ZS}^{} 0.8)\), \((_{KA}^{} 10.1\%,_{ZS}^{} -1.5\%)\) and \((_{KA}^{} 9.8\%,_{ZS}^{} -0.05\%)\). Even with higher initial generalization performance, the rate of knowledge accumulation remains roughly the same or even increases, while the ability to maintain its initial generalization capabilities through the longer update cycles in parts _notably improves_. These results suggest that model scaling can benefit long-term re-use and the opportunity to maintain and consistently improve the base model over longer minor update cycles, suggesting model scaling helps mitigate forgetting .

Scaling Compute Budgets.Instead of investing compute for scaling model size, one can also adjust the directly allocated compute budgets. For our reference model B/16 and its associated compute budget of \(1.8 10^{9}\) FLOPs, we thus study \(2\), \(4\) and \(6\) increases, as well as \(0.5\) and \(0.25\) reductions. As seen in fig. 5 (_right_) which aggregates knowledge accumulation \(_{KA}\) and zero-shot retention \(_{ZS}\) through their geometric mean, simple continual finetuning (brown) can not consistently leverage increased compute budgets. However, coupled with simple model merging, we find that models become much better at effectively utilizing the additional budget increase; exhibit a log-linear budget-performance relation. With much lower aggregate accumulation-retention performance, we also find a similar, slightly weaker compute scaling behavior for adapter-based continual pretraining.

## 6 Continual Pretraining: A Data-Centric Perspective

This section provides an important data-centric perspective on continual multimodal pretraining (with more information and experiments available in appendix E). We study how fine-grained constraints on the sequence of tasks within an update cycle \(\) influence favorable trade-offs between between knowledge accumulation \(_{}\) and zero-shot retention \(_{}\) (appendix E.3). Results on the impact of different deployment scenarios on continual pretrainability are visualized in fig. 6 for the following scenarios (appendix B.4): **(1)**performance sorted - transition from easy to hard concepts, **(2)** concept-frequency sorted - rare pretraining concepts first, **(3)** concept-similarity sorted

Figure 5: **Model and Compute Scaling for Continual Pretraining. (_Left_) Increasing model size from ViT S/16 to ViT g/14 scales zero-shot performance consistently. In conjunction however, we find that incorporating new context comes with a _reduced_ impact on knowledge retention. (_Right_) For continual finetuning (with/without model merging), as well as LoRA adapters, we consistently increase the allocated compute budget (for B/16). For normal finetuning, an optimum is reached early. With model merging, we instead see a log-linear scaling in performance with additional compute.**each update contains concepts semantically related to the preceding update, and **(4)**random sorting. Dataset-incremental as well as time-incremental minor updates are studied separately due to their different structure in section 6, and reverse streams are investigated in section 6.

**Concept- and Sample-based Deployment Scenarios.** Across the deployment scenarios in fig. 6 (leftmost), while the concept-frequency stream (in green) has the marginally best \(_{}\)-\(_{}\) tradeoff with \(_{}{=}55.2\), \(_{}{=}65.6\), and performance (in pink) performs worst (\(_{}{=}53.8\), \(_{}{=}64.3\)), we find that _convergence end-points are surprisingly similar_ - especially w.r.t. the initial zero-shot and the joint finetuning upper bound reference points. However, while endpoints are remarkably similar, different orderings \(\) induce significantly different trajectories in the accumulation-retention space, with similarity the most sample inefficient ordering, while random produces the most favorable trajectories. This aligns with prior work from curriculum learning and active learning that have suggested the efficacy of random curriculums [120; 199], which we find extends itself well into the domain of longer-horizon continual pretraining over minor updates. These insights mean that for longer update trajectories and a shared total space of subdomains and tasks of interest, the type and order of model updates primarily impact initial model versions. This is crucial to account for with respect to the model release horizon and the expected time frame before conducting large-scale continual pretraining updates. However, it also means that across long update horizons irrespective of particular task orders, continually pretrained models arrive at similar performance breakpoints.

**Dataset- and Time-based Deployment Scenarios** differ from the previous scenarios, in that each update step generally contains more semantically grouped samples. As we find for both cases (randomly ordering datasets in dataset or time-ordering in time), such an update format induces significantly higher trajectory variance, with lesser trajectory coherence when compared to the other four studied streaming orderings. This is expected given prior work suggesting that visual datasets encode heavy biases [176; 105], and hence tasks that explicitly separate these datasets induce larger distribution shifts than tasks that smoothly mix data samples across the datasets on a concept-level. Still, the degree of accumulation remains comparable, though we find zero-shot retention impacted disproportionately higher when orderings \(\) or designed on a dataset-level (down to \(_{ZS} 62.8\%\), compared to \(_{ZS}^{} 64.4\%\), \(_{ZS}^{} 65.5\%\) in the best case). This is important to account for when designing minor updates with the goal of retaining original zero-shot performance.

## 7 Conclusion

This work introduces FoMo-In-Flux - a novel, large-scale, fine-grained controllable and long horizon continual pretraining benchmark for vision-language foundation models. Using FoMo-In-Flux, we conduct an extensive study into continual multimodal pretraining from a _data-, method-_, and training-centric_ perspective. Key findings show that **(1)** model merging strategies successfully trade-off between acquisition of new knowledge and retention of pretraining knowledge, **(2)** learning rates matter; and are well accounted for via meta scheduling, **(3)** that increased model size facilitates inclusion of new knowledge without overwriting pretraining context, **(4)** that simple compute scaling does not benefit all methods equally - with model merging exhibiting the most favorable properties, **(5)** that the order of updates impact the models trajectory in accumulation-retention space, but only marginally impact the streaming endpoints, and that **(6)** replaying on buffer data during streaming is generally more important than replaying on (various subsets of) the original pretraining data.

Figure 6: **A Data-centric Perspective on Continual Pretraining. _(Left)_ Four concept-level stream orderings \(\) emulating potential update cycles (c.f. section 3.3). Results indicate that deployment scenarios heavily impact intermediate model update stages; however when update cycles operate over shared underlying data distributions, continual pretraining endpoints end up _highly similar. (Center)_ Dataset-level (random or time-incremental) update cycles exhibit less stable deployment trajectories due to high dataset biases [176; 105].(Right)_ Reversing concept-level datastreams (see appendix E.1) reveals significant trajectory changes. However, the end point similarity still persists.**