# Contrastive losses as generalized models of global epistasis

David H. Brookes

Dyno Therapeutics

&Jakub Otwinowski

Dyno Therapeutics

&Sam Sinai

Dyno Therapeutics

###### Abstract

Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing supervised contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrastive losses are able to accurately estimate a ranking function from limited data even in regimes where MSE is ineffective and validate the practical utility of this insight by demonstrating that contrastive loss functions result in consistently improved performance on empirical benchmark tasks.

## 1 Introduction

A fitness function maps biological sequences to relevant scalar properties of the sequences, such as binding affinity to a target molecule, or fluorescent brightness. Biological sequences span combinatorial spaces and fitness functions are typically multi-peaked, due to interactions between positions in a sequence. Learning fitness functions from limited experimental data (often a minute fraction of the possible space) can be a difficult task but allows one to predict properties of sequences. These predictions can help identify promising new sequences for experimentation  or to guide the search for optimal sequences .

Even in the case of where experimental measurements are available for every possible sequence in a sequence space, inferring a model of the fitness function can be valuable for understanding the factors that impact sequences' fitness  or how evolution might progress over the fitness landscape .

Numerous methods have been developed to estimate fitness functions from experimental data, including classical machine learning techniques , deep learning approaches , and semi-supervised methods . Additionally, there are many methods that incorporate biological assumptions into the modeling process, such as parameterized biophysical models , non-parametric techniques , and methods for spectral regularization of neural networks . These latter approaches largely focus on accurately modeling the influence of "epistasis" on fitness functions, which refers to statistical or physical interactions between genetic elements, typically either amino-acids in a protein sequence or genes in a genome.

"Local" epistasis refers to interactions between a limited number of specific positions in a sequence, and is often modeled using interaction terms in a linear model of a fitness function . "Global" epistasis, on the other hand, refers to the presence of nonlinear relationships that affect the fitness of sequences in a nonspecific manner. A model of global epistasis typically assumes a simple latent fitness function is transformed by a monotonically increasing nonlinearity to produce observed fitness data [36; 31; 39; 34; 37]. Typically, these models assume a particular parametric form of the latent fitness function and nonlinearity, and fit the parameters of both simultaneously. It is most common to assume that the underlying fitness function includes only additive (non-interacting) effects , though pairwise interaction effects have been added in some models .

Despite their relative simplicity, global epistasis models have been found to be effective at modeling experimentally observed fitness functions [37; 33; 34]. Furthermore, global epistasis is not just a useful modeling choice, but a physical phenomenon that can result from features of a system's dynamics  or the environmental conditions in which a fitness function is measured . Therefore, even if one does not use a standard global epistasis model, it is still important to consider the effects of global epistasis when modeling fitness functions.

Due to the monotonicity of the nonlinearity in global epistasis models, the latent fitness function in these models can be interpreted as a parsimonious ranking function for sequences. Herein we show that fitting a model to observed fitness data by minimizing a supervised contrastive, or ranking, loss is a simple and effective method for extracting such a ranking function. In particular, we focus on the Bradley-Terry loss , which has been widely used for learning-to-rank tasks , and more recently for ordering the latent space of a generative model for protein sequences . Minimizing this loss provides a technique for modeling global epistasis that requires no assumptions on the form of the latent fitness functions or nonlinearity beyond monotonicity, and can easily be applied to any set of observed fitness data.

Further, we use an entropic uncertainty principle to show that global epistasis can result in observed fitness functions that cannot be represented using a sparse set of epistatic interactions. In particular, this uncertainty principle shows that a fitness function that is sufficiently concentrated in the fitness domain-meaning that a small number of sequences have fitness values with relatively large magnitudes-can not be concentrated in the Graph Fourier bases that represent fitness functions in terms of local epistatic interactions [38; 40; 7]. We show that global epistasis nonlinearities tend to concentrate observed fitness functions in the fitness domain, thus preventing a sparse representation in the epistatic domain. This insight has the implication that observed fitness functions that have been affected by global epistasis may be difficult to estimate with undersampled training data and a Mean Squared Error (MSE) loss. We hypothesize that estimating the latent ranking fitness function using a contrastive loss can be done more data-efficiently than estimating the observed fitness function using MSE, and conduct simulations that support this hypothesis. Additionally, we demonstrate the practical importance of these insights by showing that models trained with the Bradley-Terry loss outperform those trained with MSE loss on nearly all empirical benchmark tasks .

## 2 Background

### Fitness functions and the Graph Fourier transform

A fitness function \(f:\) maps a space of sequences \(\) to a scalar property of interest. In the case where \(\) contains all combinations of elements from an alphabet of size \(q\) at \(L\) sequence positions, then the fitness function can be represented exactly in terms of increasing orders of local epistatic interactions. For binary sequences (\(q=2\)), this representation takes the form:

\[f()=_{0}+_{i=1}^{L}_{i}x_{i}+_{ij}_{ij}x_{i}x_{j} +_{ijk}_{ijk}x_{i}x_{j}x_{k}+...,\]

where \(x_{i}\{-1,1\}\) represent elements in the sequence and each term in the expansion represents a (local) epistatic interaction with weight \(_{\{i\}}\), with the expansion continuing up to \(L^{}\) order terms. Analogous representations can be constructed for sequences with any size alphabet \(q\) using Graph Fourier bases[38; 40; 7]. These representations can be compactly expressed as:

\[=,\] (1)where \(\) is a length \(q^{L}\) vector containing the fitness values of every sequence in \(\), \(\) is a \(q^{L} q^{L}\) orthogonal matrix representing the Graph Fourier basis, and \(\) is a length \(q^{L}\) vector containing the weights corresponding to all possible epistatic interactions. We refer to \(\) and \(\) as representing the fitness function in the fitness domain and the epistatic domain, respectively. Note that we may apply the inverse transformation of Eq. 1 to any complete observed fitness function, \(\) to calculate the epistatic representation of the observed data, \(_{}=^{T}\). Similarly, if \(}\) contains the predictions of a fitness model for every sequence in a sequence space, then \(}=^{T}}\) is the epistatic representation of the model.

A fitness function is considered sparse, or concentrated, in the epistatic domain when \(\) contains a relatively small number of elements with large magnitudes, and many elements equal to zero or with small magnitudes. In what follows, we may refer to a fitness function that is sparse in the epistatic domain as simply being a "sparse fitness function". A number of experimentally-determined fitness functions have been observed to be sparse in the epistatic domain [32; 17; 7]. Crucially, the sparsity of a fitness function in the epistatic domain determines how many measurements are required to estimate the fitness function using Compressed Sensing techniques that minimize a MSE loss function . Herein we consider the effect that global epistasis has on a sparse fitness function. In particular, we argue that global epistasis results in observed fitness functions that are dense in the epistatic domain, and thus require a large amount of data to accurately estimate by minimizing a MSE loss function. However, in these cases, there may be a sparse ranking function that can be efficiently extracted by minimizing a contrastive loss function.

### Global epistasis models

A model of global epistasis assumes that noiseless fitness measurements are generated according to the model:

\[y=g(f()),\] (2)

where \(f\) is a latent fitness function, \(g\) is a monotonically increasing nonlinear function. In most cases, \(f\) is assumed to include only first or second order epistatic terms and the nonlinearity is explicitly parameterized using, for example, spline functions  or sums of hyperbolic tangents . The restriction that \(f\) includes only low-order terms is somewhat arbitrary, as higher-order local epistatic effects have been observed in fitness data (see, e.g., Wu et al. ). In general we may consider \(f\) to be any fitness function that is sparse in the epistatic domain, and global epistasis then refers to the transformation of a sparse fitness function by a monotonically-increasing nonlinearity.

Global epistasis models of the form of Eq. 2 have proved effective at capturing the variation observed in empirical fitness data [26; 37; 31; 33; 34; 39], suggesting that global epistasis is a common feature of natural fitness functions. Further, it has been shown that global epistasis results from first-principles physical considerations that are common in many biological systems. In particular, Husain and Murugan  show that global epistasis arises when the physical dynamics of a system is dominated by slow, collective modes of motion, which is often the case for protein dynamics. Aside from intrinsic/endogenous sources, the process of measuring fitness can also introduce nonlinear effects that are dependent on the experiment and not on the underlying fitness function. For example, fitness data is often left-censored, as many sequence have fitness that falls below the detection threshold of an assay. Finally, global diminishing-returns epistatic patterns have been observed widely in both single and multi-gene settings where the interactions are among genes rather than within a gene [26; 34; 4].

Together, these results indicate that global epistasis is an effect that can be expected in empirically-observed fitness functions. In what follows, we argue that global epistasis is detrimental to effective modeling of fitness functions using standard techniques. In particular, global epistasis manifests itself by producing observed data that is dense in the epistatic domain. In other words, when observed fitness data is produced through Eq. 2 then the epistatic representation of this fitness function (calculated through application of Eq. 1), is not sparse. Further we argue that this effect of global epistasis makes it to difficult to model such observed data by minimizing standard MSE loss functions with a fixed amount of data. Further, we argue that fitting fitness models aimed at extracting the latent fitness function from observed data is a more efficient use of observed data that results in improved predictive performance (in the ranking sense).

While the models of global epistasis described thus far could be used for this purpose, they have the drawback that they assume a constrained form of both \(g\) and \(f\), which enforces inductive biases that may affect predictive performance. Here we propose a flexible alternative to modeling global epistasis that makes no assumptions on the form of \(f\) or \(g\). In particular, we interpret the latent fitness function \(f\) as a parsimonious ranking function for sequences, and the problem of modeling global epistasis as recovering this ranking function. A natural method to achieve this goal is to fit a model of \(f\) to the observed data by minimizing a contrastive, or ranking, loss function. These loss functions are designed to learn a ranking function and, as we will show, are able to recover a sparse fitness function that has been transformed by global epistasis to produce observed data. An advantage of this approach to modeling global epistasis is that the nonlinearity \(g\) is modeled non-parametrically, and is free to take any form, while the latent fitness function can be modeled by any parametric model, for example, convolutional neural networks (CNNs) or fine-tuned language models, which have been found to perform well in fitness prediction tasks . An accurate ranking model also enables effective optimization, as implied by the results in Chan et al. .

### Contrastive losses

Contrastive losses broadly refer to loss functions that compare multiple outputs of a model and encourage those outputs to be ordered according to some criteria. In our case, we desire a loss function that encourages model outputs to be ranked according to observed fitness values. An example of such a loss function is the Bradley-Terry loss [5; 9], which has the form:

\[()=_{i,j:y_{i}>y_{j}}[1+e^{-(f_{ }(_{i})-f_{}(_{j}))}],\] (3)

where \(f_{}\) is a model with parameters \(\), \(_{i}\) are model inputs and \(y_{i}\) are the corresponding labels of those inputs. This loss compares every pair of data points and encourages the model output \(f_{}(_{i})\) to be greater than \(f_{}(_{j})\) whenever \(y_{i}>y_{j}\); in other words, it encourages the model outputs to be ranked according to their labels. A number of distinct but similar loss functions have been proposed in the learning-to-rank literature  and also for metric learning . An example is the Margin ranking loss , which replaces the logistic function in the sum of Eq. 3 with a hinge function. In our experiments, we focus on the BT loss of Eq. 3 as we found it typically results in superior predictive performance

The BT loss was recently used by Chan et al.  to order the latent space of a generative model for protein sequences such that certain regions of the latent space corresponding to sequences with higher observed fitness values. In this case, the BT loss is used in conjunction with standard generative modeling losses. In contrast, here we analyze the use of the BT loss alone in order to learn a ranking function for sequences given corresponding observed fitness values.

A key feature of the contrastive loss in Eq. 3 is that it only uses information about the ranking of observed labels, rather than the numerical values of the labels. Thus, the loss is unchanged when the observed values are transformed by a monotonic nonlinearity. We will show that this feature allows this loss to recover a sparse latent fitness function from observed data that has been affected by global epistasis, and enables more data-efficient learning of fitness functions compared to a MSE loss.

## 3 Results

Our results are aimed at demonstrating three properties of contrastive losses. First, we show that given complete, noiseless fitness data (i.e. noiseless fitness values associated with every sequence in the sequence space) that has been affected by global epistasis, minimizing the BT loss enables a model to nearly exactly recover the sparse latent fitness function \(f\). Next, we consider the case of incomplete data, where the aim is to predict the relative fitness of unobserved sequences. In this regime, we find through simulation that minimizing the BT loss enables models to achieve better predictive performance then minimizing the MSE loss when the observed data has been affected with global epistasis. We argue by way of a fitness-epistasis uncertainty principle that this is due to the fact that nonlinearities tend to produce fitness functions that do not admit a sparse representation in the epistatic domain, and thus require more data to learn with MSE loss. Finally, we demonstrate the practical significance of these insights by showing that minimizing the BT loss results in improved performance over MSE loss in nearly all tasks in empirical benchmarks .

### Recovery from complete data

We first consider the case of "complete" data, where fitness measurements are available for every sequence in the sequence space. The aim of our task in this case is to recover a sparse latent fitness function when the observed measurements have been transformed by an arbitrary monotonic nonlinearity. In particular, we sample a sparse fitness function \(f\) from the NK model , a popular model of fitness functions that has been shown to recapitulate the sparsity properties of some empirical fitness functions . The NK model has three parameters: \(L\), the length of the sequences, \(q\), the size of the alphabet for sequence elements, and \(K\), the maximum order of (local) epistatic interactions in the fitness function. Roughly, the model randomly assigns \(K-1\) interacting positions to each position in the sequence, resulting in a sparse set of interactions in the epistatic domain. The weights of each of the assigned interactions are then drawn from a independent unit normal distributions.

We then transform the sampled fitness function \(f\) with a monotonic nonlinearity \(g\) to produce a set of complete observed data, \(y_{i}=g(f(_{i}))\) for all \(_{i}\). The goal of the task is then to recover the function \(f\) given all \((_{i},y_{i})\) pairs. In order to do so, we model \(f\) using a two layer fully connected neural network and fit the parameters of this model by performing stochastic gradient descent (SGD) on the BT loss, using the Spearman correlation between model predictions and the \(y_{i}\) values to determine convergence of the optimization. We then compare the resulting model, \(\), to the latent fitness function \(f\) in both the fitness and epistatic domains, using the forward and inverse transformation of Eq. 1 to convert between the two domains.

Fig. 1 shows the results of one of these tests. In this case, we used an exponential function to represent the global epistasis nonlinearity. The exponential function exaggerates the effects of global epistasis in the epistatic domain and thus better illustrates the usefulness of contrastive losses, although the nonlinearities in empirical fitness functions tend to have a more sigmoidal shape . Fig. 1b shows that the global epistasis nonlinearity substantially alters the representations of the observed data \(y\) in both the fitness and epistatic domains, as compared to the latent fitness function \(f\). Nonetheless, Fig. 1c demonstrates that the model fitness function \(\) created by minimizing the BT loss is able to nearly perfectly recover the sparse latent fitness function (where recovery is defined as being equivalent up to an affine transformation). This is a somewhat surprising result, as there are many fitness functions that correctly rank the fitness of sequences, and it is not immediately clear why minimizing the BT loss produces this particular sparse latent fitness function. However, this example makes clear that fitting a model by minimizing the BT loss can be an effective strategy for recovering a sparse latent fitness function from observed data that has been affected by global epistasis. Similar results from

Figure 1: Recovery of latent fitness function from complete fitness data by minimizing Bradley-Terry loss. (a) Schematic of simulation. (b) Comparison between latent (\(f\)) and observed (\(y\)) fitness functions in fitness (left) and epistatic (right) domains. The latent fitness function is sampled from the NK model with \(L=8\) and \(K=2\) and the global epistasis function is \(g(f)=(10 f)\). Each point in the scatter plot represents the fitness of a sequence, while each bar in the bar plot (right) represents the squared magnitude of an epistatic interaction (normalized such that all squared magnitudes sum to 1), with roman numerals indicating the order of interaction. Only epistatic interactions up to order 3 are shown. The right plot demonstrates that global epistasis produces a dense representation in the epistatic domain compared to the representation of the latent fitness in the epistatic domain. (c) Comparison between latent and estimated (\(\)) fitness functions in fitness and epistatic domains.

additional examples of this task using different nonlinearities and latent fitness functions are shown in Appendix B.1.

### Fitness-epistasis uncertainty principle

Next, we consider the case where fitness data is incomplete. Our aim is to understand how models trained with the BT loss compare to those trained with MSE loss at predicting the relative fitness of unobserved sequence using different amounts of subsampled training data. We take a signal processing perspective on this problem, and consider how the density of a fitness function in the epistatic domain affects the ability of a model to accurately estimate the fitness function given incomplete data. In particular, we demonstrate that global epistasis tends to increase the density of fitness functions in the epistatic domain, and use an analogy to Compressive Sensing (CS) to hypothesize that more data is required to effectively estimate these fitness functions when using an MSE loss . In order to support this claim, we first examine the effects of global epistasis on the epistatic domain of fitness functions.

Fig. 1b provides an example where a monotonic nonlinearity applied to a sparse fitness increases the density of the the fitness function in the epistatic domain. In particular, we see that many "spurious" local epistatic interactions must appear in order to represent the nonlinearity (e.g. interactions of order 3, when we used an NK model with \(K=2\)). This effect can be observed for many different shapes of nonlinearities [36; 3]. We can understand this effect more generally using uncertainty principles, which roughly show that a function cannot be concentrated on a small number of values in two different representations. In particular, we consider the discrete entropic uncertainty principle proved by Dembo et al. . When applied to the transformation in Eq. 1, this uncertainty principle states:

\[H()+H()-2L m,\] (4)

where \(H()=-_{i}^{2}}{||x||^{2}}^{2}}{||x||^{2}}\) is the entropy of the normalized squared magnitudes of a vector and \(m=1/\) when \(q=2\), \(m=1/(q-)\) when \(q=3\) and \(m=1-1/(q-)\) otherwise. Low entropy indicates that a vector is concentrated on a small set of elements. Thus, the fitness-epistasis uncertainty principle of Eq. 4 shows that fitness functions cannot be concentrated in both the fitness and epistatic domains. A sparse fitness function (in the epistatic domain) must therefore be "spread out" (i.e. dense) in the fitness domain, and vice-versa.

The importance of this result for understanding global epistasis is that applying a nonlinearity to a dense vector will often have the effect of concentrating the vector on a smaller number of values. This can most easily be seen for convex nonlinearities like the exponential shown in Fig. 1a, but is also true of many other nonlinearities (see Theorem 1 in Appendix C for a sufficient condition for a nonlinearity to decrease entropy in the fitness domain and Appendix D for additional experiments

Figure 2: (a) Demonstration of the fitness-epistasis uncertainty principle for a latent fitness function transformed by \(g(f)=(a f)\) for various settings of \(a\). The dashed black line indicates the lower bound on the sum of the entropies of the fitness and epistatic representations of the fitness function (b) Test-set Spearman correlation for models trained with MSE and BT losses on incomplete fitness data transformed by various nonlinearities, compared to the entropy of the fitness function in the epistatic domain. Each point corresponds to a model trained on 256 randomly sampled training points from an \(L=10,K=2\) latent fitness function which was then transformed by a nonlinearity. (c) Convergence of models fit with BT and MSE losses to observed data generated by transforming an \(L=10,K=2\) latent fitness function by \(g(f)=(10 f)\). Each point represents the mean test set correlation over 200 training set replicates.

demonstrating the uncertainty principle). If this concentration in the fitness domain is sufficiently extreme, then the epistatic representation of the fitness function, \(\), must be dense in order to satisfy Eq. 4. Fig. (a)a demonstrates the uncertainty principle by showing how the entropy in the fitness and epistatic domains decrease and increase, respectively, as more extreme nonlinearities are applied to a sparse latent fitness function.

The uncertainty principle quantifies how global epistasis affects a fitness function by preventing a sparse representation in the epistatic domain. From a CS perspective, this has direct implications for modeling the fitness function from incomplete data. In particular, if we were to model the fitness function using CS techniques such as LASSO regression with the Graph Fourier basis as the representation, then it is well known that the number of noiseless data points required to perfectly estimate the function scales as \((S N)\) where \(S\) is the sparsity of the signal in a chosen representation and \(N\) is the total size of the signal in the representation . Therefore, when using these techniques, fitness functions affected by global epistasis will require more data to effectively model. Notably, the techniques for which these scaling laws apply minimize a MSE loss functions as part of the estimation procedure. Although these scaling laws only strictly apply to CS modeling techniques, we hypothesize that similar principles may apply when using neural network models and SGD training procedures. In particular, our results suggest that more data is required to fit neural networks to fitness functions with dense epistatic representations using the MSE loss. In the next section we present the results of simulations that support this hypothesis by showing that the entropy of the epistatic representation is negatively correlated with the predictive performance of models trained with an MSE loss on a fixed amount of fitness data. Further, these simulations show that models trained with the BT loss are robust to the dense epistatic representations produced global epistasis, and converge faster to maximum predictive performance as they are provided more fitness data compared to models trained with an MSE loss.

### Simulations with incomplete data

We next present simulation results aimed at showing that global epistasis adversely effects the ability of models to effectively learn fitness functions from incomplete data when trained with MSE loss and that models trained with BT loss are more robust to the effects of global epistasis.

In our first set of simulations, we tested the ability models to estimate a fitness function of \(L=10\) binary sequences given one quarter of the fitness measurements (256 measurements out of total of \(2^{10}=1024\) sequences in the sequence space). In each simulation, we (i) sampled a sparse latent fitness function from the NK model, (ii) produced an observed fitness function by applying one of three nonlinearities to the latent fitness function: exponential, \(g(f)=(a f)\), sigmoid, \(g(f)=(1+e^{-a f})^{-1}\), or a cubic polynomial \(g(f)=x^{3}+ax\) with settings of the parameter \(a\) that ensured the nonlinearity was monotonically increasing, (iii) sampled 256 sequence/fitness pairs uniformly at random from the observed fitness function to be used as training data, and (iv) trained models with this data by performing SGD on the MSE and BT losses. We ran this simulation for 50 replicates of each of 20 settings of the \(a\) parameter for each of the three nonlinearities. In every case, the models were fully-connected neural networks with two hidden layers and the optimization was terminated using early stopping with \(20\) percent of the training data used as a validation set. After training, we measured the extent to which the models estimated the fitness function by calculating Spearman correlation between the model predictions and true fitness values on all sequences in the sequence space. Spearman correlation is commonly used to benchmark fitness prediction methods [14; 27].

The results of each of these simulations are shown in Fig. (b)b. We see that the predictive performance of models trained with the MSE loss degrades as the entropy of the fitness function in the epistatic domain increases, regardless of the type of nonlinearity that is applied to the latent fitness function. This is in contrast to the models trained with the BT loss, which often achieve nearly perfect estimation of the fitness function even when the entropy of the fitness function in the epistatic domain approaches its maximum possible value of \(L 2\). This demonstrates the key result that the MSE loss is sensitive to the density of the epistatic representation resulting from global epistasis (as implied by the analogy to CS), while the BT loss is robust to these effects. We additionally find that these results are maintained when the degree of epistatic interactions in the latent fitness function is changed (Appendix E.1) and when noise is added to the observed fitness functions (Appendix E.2).

Next, we tested how training set size effects the predictive performance of models trained with MSE and BT losses on a fitness function affected by global epistasis. In order to do so, we sampled a single \(L=10,K=2\) fitness function from the NK model and applied the nonlinearity \(g(f)=(10 f)\) to produce an observed fitness function. Then, for each of a range of training set sizes between 25 and 1000, we randomly sampled a training set and fit models with MSE and BT losses using the same models and procedure as in the previous simulations. We repeated this process for 200 training set replicates of each size, and calculated both the Spearman and Pearson correlations between the resulting model predictions and true observed fitness values for all sequences in the sequence space.

Fig. 2c shows the mean correlation values across all 200 replicates of each training set size. There are two important takeaways from this plot. First, the BT loss achieves higher Spearman correlations than the MSE loss in all data regimes. This demonstrates the general effectiveness of this loss to estimate fitness functions affected by global epistasis. Next, we see that models trained with BT loss converge to a maximum Spearman correlation faster than models trained with MSE loss do to a maximum Pearson correlation, which demonstrates that the difference in predictive performance between models trained with MSE and BT losses is not simply due to a result of the evaluation metric being more tailored to one loss than the other. This result also reinforces our claim that fitness functions affected by global epistasis require more data to learn effectively with MSE loss, as would be predicted by CS scaling laws. The BT loss on the other hand, while not performant with the Pearson metric as expected by a ranking loss, seems to overcome this barrier and can be used to estimate a fitness function from a small amount of data, despite the effects of global epistasis.

### Empirical benchmark results

In the previous sections, we used noiseless simulated data to explore the interaction between global epistasis and loss functions. Now we present results demonstrating the practical utility of our insights by comparing the predictive performance of models trained with MSE and BT losses on experimentally determined protein fitness data. We particularly focus on the FLIP benchmark , which comprises a total of 15 fitness prediction tasks derived from three empirical fitness datasets. These three datasets explore multiple types of proteins, definitions of protein fitness, and experimental assays. In particular, one is a combinatorially complete dataset that contains the binding fitness of all combinations of mutations at 4 positions to the GB1 protein , another contains data about the viability of Adeno-associated virus (AAV) capsids for many different sets of mutations to the wild-type capsid sequence , and another contains data about the thermostability of many distantly related proteins . Although there are a number of protein fitness benchmarks, we chose to primarily focus on FLIP because the FLIP datasets all contain a large number of sequences with three or more mutations, which is the regime where the effects of global epistasis are most apparent. In contrast, other benchmarks such as ProteinGym  mostly contain datasets with only single and double mutants. Below we describe results on the FLIP benchmark; however, we also tested on two datasets in ProteinGym that contained a large number of higher-order mutations and show these corroborating results in Appendix G.

    &  &  &  \\   & & MSE Loss & & & \\   & 1-vs-rest & \(0.133 0.150\) & \(0.091 0.093\) & \(0.097 0.030\) & \(\) \\  & 2-vs-rest & \(0.564 0.026\) & \(\) & \(0.250 0.030\) & \(\) \\ GB1 & 3-vs-rest* & \(0.814 0.049\) & \(\) & \(0.539 0.084\) & \(\) \\  & Low-vs-High* & \(0.499 0.010\) & \(\) & \(0.381 0.028\) & \(\) \\  & Sampled & \(0.930 0.002\) & \(\) & \(0.823 0.009\) & \(0.816 0.010\) \\   & Mut-Des & \(0.751 0.006\) & \(0.757 0.007\) & \(0.288 0.004\) & \(\) \\  & Des-Mut & \(0.806 0.006\) & \(\) & \(0.318 0.013\) & \(\) \\  & l-vs-rest* & \(0.335 0.117\) & \(\) & \(0.052 0.053\) & \(\) \\  & 2-vs-rest & \(0.748 0.010\) & \(\) & \(\) & \(0.457 0.010\) \\  & 7-vs-rest & \(0.732 0.003\) & \(\) & \(0.694 0.006\) & \(0.695 0.006\) \\  & Low-vs-High & \(0.401 0.006\) & \(\) & \(\) & \(0.170 0.006\) \\  & Sampled & \(0.927 0.001\) & \(\) & \(0.650 0.005\) & \(0.652 0.010\) \\   & Mixed & \(0.349 0.011\) & \(\) & \(0.636 0.013\) & \(0.616 0.011\) \\  & Human & \(0.511 0.016\) & \(\) & \(0.382 0.022\) & \(\) \\  & Human-Cell & \(0.490 0.021\) & \(\) & \(0.316 0.018\) & \(\) \\   

Table 1: Comparison between MSE and Bradley-Terry losses on FLIP benchmark tasks using the CNN baseline model. Each row represents a data set and split combination. Numerical columns indicate the mean and standard deviation of test set metrics over 10 random initializations of the model. Asterisks indicate that unmodified portions of sequences were used in training data. Bold values indicate that a loss has significantly improved performance over all other tested losses (\(p<0.05\)). Additional benchmark results are shown in Appendix G.

For each of the three FLIP datasets, the benchmark provides multiple train/test splits that are relevant for protein engineering scenarios. For example, in the GB1 and AAV datasets, there are training sets that contain only single and double mutations to the protein, while the associated test sets contain sequences with more than two mutations. This represents a typical situation in protein engineering where data can easily be collected for single mutations (and some double mutations) and the goal is then to design sequences that combine these mutations to produce a sequence with high fitness. In all of the FLIP tasks the evaluation metric is Spearman correlation between model predictions and fitness labels in the test set, since ranking sequences by fitness is the primary task that models are used for in data-driven protein engineering.

In the FLIP benchmark paper, the authors apply a number of different modeling strategies to these splits, including Ridge regression, training a CNN, and a number of variations on fine-tuning the ESM language models for protein sequences . All of these models use a MSE loss to fit the model to the data, along with any model-specific regularization losses. In our tests, we consider only the CNN model as it balances consistently high performance in the benchmark tasks with relatively straightforward training protocols, enabling fast replication with random restarts.

We trained the CNN model on each split using the standard MSE loss and BT contrastive losses. The mean and standard deviation of Spearman correlations between the model predictions and test set labels over 10 random restarts are shown in Table 1, third and fourth columns. By default, the FLIP datasets contain portions of sequences that are never mutated in any of the data (e.g., only 4 positions are mutated in the GB1 data, but the splits contain the full GB1 sequence of length 56). We found that including these unmodified portions of the sequence often did not improve, and sometimes hurt, the predictive performance of the CNN models while requiring significantly increased computational complexity. Therefore most of our results are reported using inputs that contain only sequence positions that are mutated in at least one train or test data point. We found that including the unmodified portions of sequences improved the performance for the Low-vs-High and 3-vs-Rest GB1 well splits, as well as the 1-vs-rest AAV split and so these results are reported in Table 1; in these cases we found both models trained with MSE and contrastive losses had improved performance.

Although Spearman correlation is commonly used to benchmark models of fitness functions, in practical protein engineering settings it also important to consider the ability of the model to classify the sequences with the highest fitness. To test this, we calculated the "top 10% recall" for models trained with the BT and MSE losses on the FLIP benchmark data, which measures the ability of the models to correctly classify the 10% of test sequences with the highest fitness in the test set . These results are shown in the fifth and sixth columns of Table 1. It may be expected that the BT loss improves performance based on Spearman correlation because both are measures of ranking performance; however, the consistently improved performance of the BT loss based on top 10% recall demonstrates that this loss will lead to improvements in fitness prediction that are practical to protein engineering.

The results in Table1 show that using contrastive losses (and particularly the BT loss) consistently results in improved predictive performance across a variety of practically relevant fitness prediction tasks. Further, in no case does the BT loss result in worse performance than MSE. The reasons for this result may be manifold; however, we hypothesize that it is partially a result of sparse latent fitness functions being corrupted by global epistasis. Indeed, it is shown in Otwinowski et al.  that a GB1 landscape closely associated with that in the FLIP benchmark is strongly affected by global epistasis. Further, many of the FLIP training sets are severely undersampled in the sense of CS scaling laws, which is the regime in which differences between MSE and contrastive losses are most apparent when global epistasis is present, as shown in Fig. 2.

## 4 Discussion

Our results leave open a few avenues for future exploration. First, it is not immediately clear in what situations we can expect to observe the nearly-perfect recovery of a latent fitness function as seen in Fig. 1. A theoretical understanding of this result may either cement the promise of the BT loss, or provide motivation for the development of techniques that can be applied in different scenarios. Next, we have made a couple of logical steps in our interpretations of these results that are intuitive, but not fully supported by any theory. In particular, we have drawn an analogy to CS scaling lawsto explain why neural networks trained with MSE loss struggle to learn a fitness function that has a dense representation in the epistatic domain. However, these scaling laws only strictly apply for a specific set of methods that use an orthogonal basis as the representation of the signal; there is no theoretical justification for using them to understanding the training of neural networks (although applying certain regularizations to neural network training can provide similar guarantees ). It is also not clear from a theoretical perspective why the BT loss seems to be robust to the dense representations produced by global epistasis. A deeper understanding of these phenomena could be useful for developing improved techniques.

Additionally, our simulations largely do not consider how models trained with contrastive losses may be affected by the complex measurement noise commonly seen in experimental fitness assays based on sequencing counts . Although our simulations mostly do not consider the effects of noise (except for the simple Gaussian noise added in Appendix E.2), our results on multiple empirical benchmarks demonstrate that contrastive losses can be robust to the effects of noise in practical scenarios. Further, we show in Appendix F that the BT loss can be robust to noise in a potentially pathological scenario. A more complete analysis of the effects of noise on contrastive losses would complement these results.

The code for running the simulations used herein is available at https://github.com/dhbrookes/Contrastive-Losses-Global-Epistasis.git.