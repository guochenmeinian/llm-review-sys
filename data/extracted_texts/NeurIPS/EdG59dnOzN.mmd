# Remove that Square Root: A New Efficient

Scale-Invariant Version of AdaGrad

Sayantan Choudhury

MBZUAI & Johns Hopkins University

&Nazarii Tupitsa

MBZUAI & Innopolis University

&Nicolas Loizou

Johns Hopkins University

&Samuel Horvath

MBZUAI

&Martin Takac

MBZUAI

&Eduard Gorbunov

MBZUAI

Part of this work was done when S. Choudhury was an intern at MBZUAI, UAE.

###### Abstract

Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named \(\), which presents a scale-invariant adaptation of the well-known \(\) algorithm. We prove the scale-invariance of \(\) for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of \((}{{}})\) for \(\), matching the best-known ones for \(\) and \(\). We also compare \(\) to other state-of-the-art adaptive algorithms \(\) and \(\) in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that \(\) consistently outperforms \(\) and matches/surpasses the performance of \(\) in all considered scenarios.

## 1 Introduction

In this work, we consider the following unconstrained optimization problem:

\[_{w^{d}}f(w),\] (1)

where \(f:^{d}\) is a \(L\)-smooth and generally non-convex function. In particular, we are interested in the situations when the objective has either expectation \(f(w)=_{ D}[f_{}(w)]\) or finite-sum \(f(w)=_{i=1}^{n}f_{i}(w)\) form. Such minimization problems are crucial in machine learning, where \(w\) corresponds to the model parameters. Solving these problems with stochastic gradient-based optimizers has gained much interest owing to their wider applicability and low computational cost. Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) and similar algorithms require the knowledge of parameters like \(L\) for convergence and are very sensitive to the choice of the stepsize in general. Therefore, SGD requires hyperparameter tuning, which can be computationally expensive. To address these issues, it is common practice to use adaptive variants of stochastic gradient-based methods that can converge without knowing the function's structure.

There exist many adaptive algorithms such as \(\)(Duchi et al., 2011), \(\)(Kingma and Ba, 2014), \(\)(Reddi et al., 2019), \(\)-Adaptation(Defazio and Mishchenko, 2023), \(\)(Mishchenko and Defazio, 2023), \(\)-\(\)(Shi et al., 2023) and their variants. These adaptive techniques are capable of updating their step sizes on the fly. For instance, the \(\) method determines its step sizes using a cumulative sum of the coordinate-wise squared (stochastic) gradientof all the previous iterates:

\[w_{t+1}=w_{t}-}{( I+_{=1}^{t}g_{}g_{}^{})}},\] (2)

where \(g_{t}\) represents an unbiased estimator of \( f(w_{t})\), i.e., \([g_{t} w_{t}]= f(w_{t})\), \((M)^{d}\) is a vector of diagonal elements of matrix \(M^{d d}\), \(>0\), and the division by vector is done component-wise. Ward et al. (2020) has shown that this method achieves a convergence rate of \((}{{}})\) for smooth functions, similar to SGD, without prior knowledge of the functions' parameters. However, the performance of AdaGrad deteriorates when applied to data that may exhibit poor scaling or ill-conditioning. In this work, we propose a novel algorithm, KATE, to address the issues of poor data scaling. KATE is also a stochastic adaptive algorithm that can achieve a convergence rate of \((}{{}})\) for smooth non-convex functions in terms of \(_{t[T]}[\| f(w_{t})\|]^{2}\).

### Related Work

A significant amount of research has been done on adaptive methods over the years, including AdaGrad(Duchi et al., 2011; McMahan and Streeter, 2010), AMSGrad(Reddi et al., 2019), RMSProp(Tieleman and Hinton, 2012), AI-SARAH(Shi et al., 2023), and Adam(Kingma and Ba, 2014). However, all these works assume that the optimization problem is contained in a bounded set. To address this issue, Li and Orabona (2019) proposes a variant of the AdaGrad algorithm, which does not use the gradient of the last iterate (this makes the step sizes of \(t\)-th iteration conditionally independent of \(g_{t}\)) for computing the step sizes and proves convergence for the unbounded domain.

Each of these works considers a vector of step sizes for each coefficient. Duchi et al. (2011) and McMahan and Streeter (2010) simultaneously proposed the original AdaGrad algorithm. However, McMahan and Streeter (2010) was the first to consider the vanilla scalar form of AdaGrad, known as

\[w_{t+1}=w_{t}-}{^{t}\|g_{}\|^{2}}}.\] (3)

Later, Ward et al. (2020) analyzed AdaGradNorm for minimizing smooth non-convex functions. In a follow-up study, Xie et al. (2020) proves a linear convergence of AdaGradNorm for strongly convex functions. Recently, Liu et al. (2022) analyzed AdaGradNorm for solving smooth convex functions without the bounded domain assumption. Moreover, Liu et al. (2022) extends the convergence guarantees of AdaGradNorm to quasar-convex functions 2 using the function value gap. Orabona et al. (2015) introduce the notion of scale-invariance, which is a special case of affine invariance (Nesterov and Nemirovskii, 1994; Nesterov, 2018; d'Aspremont et al., 2018), propose a scale-invariant version of AdaGrad for online convex optimization for generalized linear models, and prove \(()\) regret bounds in this setup.

Recently, Defazio and Mishchenko (2023) introduced the \(\)-Adaptation method, which has gathered considerable attention due to its promising empirical performances. In order to choose the adaptive step size optimally, one requires knowledge of the initial distance from the solution, i.e., \(D:=\|w_{0}-w_{*}\|\) where \(w_{*}_{w^{d}}f(w)\). The \(\)-Adaptation method works by maintaining an estimate of \(D\) and the stepsize choice in this case is \(}}{{^{t}\|g_{}\|^{2}}}}\) for the \(t\)-th iteration (here \(d_{t}\) is an estimate of \(D\)). Mishchenko and Defazio (2023) further modifies the algorithm in a follow-up work and introduces \(\) (with stepsize choice \(^{2}}}{{^{t}d_{}^{2}\|g_{} \|^{2}}}}\)) to improve the convergence speed.

Another exciting line of work on adaptive methods is Polyak stepsizes. Polyak (1969) first proposed Polyak stepsizes for subgradient methods, and recently, the stochastic version (also known as SPS) was introduced by Oberman and Prazeres (2019); Loizou et al. (2021); Abdukhakiimov et al. (2024, 2023); Li et al. (2023) and Gower et al. (2021). For a finite sum problem \(_{w^{d}}f(w):=_{i=1}^{n}f_{i}(w)\), Loizou et al. (2021) uses \((w_{t})-f_{i}^{*}}{c\| f_{i}(w_{t})\|^{2}}\) as their stepsize choices (here \(f_{i}^{*}:=_{w^{d}}f_{i}(w)\)), while Oberman and Prazeres (2019) uses \()-f^{*})}{[\| f_{i}(w_{t})\|^{2} ]}\) for \(k\)-th iteration. However, these methods are impractical when \(f^{*}\) or \(f_{i}^{*}\) is unknown. Following its introduction,several variants of the SPS algorithm emerged (Li et al., 2023; D'Orazio et al., 2021). Lately, Orvieto et al. (2022) tackled the issues with unknown \(f_{i}^{*}\) and developed a truly adaptive variant. In practice, the SPS method shows excellent empirical performance on overparameterized deep learning models (which satisfy the interpolation condition i.e. \(f_{i}^{*}=0,\  i[n]\)) (Loizou et al., 2021).

### Main Contribution

Our main contributions are summarized below.

\(\)KATE: new scale-invariant version of AdaGrad.We propose a new method called KATE that can be seen as a version of AdaGrad, which does not use a square root in the denominator of the stepsize. To compensate for this change, we introduce a new sequence defining the numerator of the stepsize. We prove that KATE is scale-invariant for generalized linear models: if the starting point is zero, then the loss values (and training and test accuracies in the case of classification) at points generated by KATE are independent of the data scaling (Proposition 2.1), meaning that the speed of convergence of KATE is the same as for the best scaling of the data.

\(\)**Convergence for smooth non-convex problems.** We prove that for smooth non-convex problems with noise having bounded variance KATE has \(((T)/)\) convergence rate (Theorem 3.4), matching the best-known rates for AdaGrad and Adam(Defossez et al., 2020).

\(\)**Numerical experiments.** We empirically illustrate the scale-invariance of KATE on the logistic regression task and test its performance on logistic regression (see Section 4.1), image classification, and text classification problems (see Section 4.2). In all the considered scenarios, KATE outperforms AdaGrad and works either better or comparable to Adam.

### Notation

We denote the set \(\{1,2,,n\}\) as \([n]\). For a vector \(a^{d}\), \(a[k]\) is the \(k\)-th coordinate of \(a\) and \(a^{2}\) represents the element-wise suqare of \(a\), i.e., \(a^{2}[k]=(a[k])^{2}\). For two vectors \(a\) and \(b\), \(\) stands for element-wise division of \(a\) and \(b\), i.e., \(k\)-th coordinate of \(\) is \(\). Given a function \(h:^{d}\), we use \( h^{d}\) to denote its gradient and \(_{k}h\) to indicate the \(k\)-th component of \( h\). Throughout the paper \(\|\|\) represents the \(_{2}\)-norm and \(f_{*}=_{w^{d}}f(w)\). Moreover, we use \(\|w\|_{A}\) for a positive-definite matrix \(A\) to define \(\|w\|_{A}\,Aw}\). Furthermore, \([]\) denotes the total expectation while \(_{t}[]\) denotes the conditional expectation conditioned on all iterates up to step \(t\) i.e. \(w_{0},w_{1},,w_{t}\).

## 2 Motivation and Algorithm Design

We focus on solving the minimization problem (1) using a variant of AdaGrad. We aim to design an algorithm that performs well, irrespective of how poorly the data is scaled.

 Algorithm & Convergence rate & Scale invariance \\  AdaGradNorm (Ward et al., 2020) & \(( T/)\) & ✗ \\  AdaGrad (Defossez et al., 2020) & \(( T/)\) & ✗ \\  Adam (Defossez et al., 2020) & \(( T/)\) & ✗ \\  KATE (this work) & \(( T/)\) & ✓ \\   

Table 1: Summary of convergence guarantees for closely-related adaptive algorithms to solve _smooth non-convex stochastic_ optimization problems. Convergence rates are given in terms of \(_{t[T]}[\| f(w_{t})\|]^{2}\). We highlight KATE’s _scale-invariance_ property for problems of type (4).

Generalized linear models.Here, we consider the parameter estimation problem in generalized linear models (GLMs) (Nelder and Wedderburn, 1972; Agresti, 2015) using maximum likelihood estimation. GLMs are an extension of linear models and encompass several other valuable models, such as logistic (Hosmer Jr et al., 2013) and Poisson regression (Frome, 1983), as special cases. The parameter estimation to fit GLM on dataset \(\{x_{i},y_{i}\}_{i=1}^{n}\) (where \(x_{i}^{d}\) are feature vectors and \(y_{i}\) are response variables) can be reformulated as

\[_{w^{d}}f(w)_{i=1}^{n} _{i}(x_{i}^{}w)\] (4)

for differentiable functions \(_{i}:\)(Shalev-Shwartz and Ben-David, 2014; Nguyen et al., 2017; Takac et al., 2013; He et al., 2018; Chezhegov et al., 2024). For example, the linear regression on data \(\{x_{i},y_{i}\}_{i=1}^{n}\) is equivalent to solving (4) with \(_{i}(z)=(z-y_{i})^{2}\). Next, the choice of \(_{i}\) for logistic regression is \(_{i}(z)=(1+(-y_{i}z))\).

Scale-invariance.Now consider the instances of fitting GLMs on two datasets \(\{x_{i},y_{i}\}_{i=1}^{n}\) and \(\{Vx_{i},y_{i}\}_{i=1}^{n}\), where \(V^{d d}\) is a diagonal matrix with positive entries. Note that the second dataset is a scaled version of the first one where the \(k\)-th component of feature vectors \(x_{i}\) are multiplied by a scalar \(V_{kk}\). Then, the minimization problems corresponding to datasets \(\{x_{i},y_{i}\}_{i=1}^{n}\) and \(\{Vx_{i},y_{i}\}_{i=1}^{n}\) are (4) and

\[_{w^{d}}f^{V}(w)  _{i=1}^{n}_{i}(x_{i}^{}Vw ),\] (5)

respectively, for functions \(_{i}\). In this work, _we want to design an algorithm with equivalent performance for the problems (4) and (5)_. If we can do that, the new algorithm's performance will not deteriorate for poorly scaled data, i.e., the method will be scale-invariant (Orabona et al., 2015), which is a special case of affine-invariance, see (Nesterov and Nemirovskii, 1994; Nesterov, 2018; d'Aspremont et al., 2018). To develop such an algorithm, we replace the denominator of AdaGrad step size with its square (remove the square root from the denominator), i.e., \( k[d]\)

\[w_{t+1}[k] = w_{t}[k]-[k]}{_{=0}^{t}g_{}^{2}[k]} g_{t}[k]\] (6)

for some \(m_{t}^{d}\).3 The following proposition shows that this method (6) satisfies a scale-invariance property with respect to functional value.

**Proposition 2.1** (Scale invariance).: Suppose we solve problems (4) and (5) using algorithm (6). Then, the iterates \(_{t}\) and \(_{t}^{V}\) corresponding to (4) and (5) follow: \( k[d]\)

\[_{t+1}[k] = _{t}[k]-[k]}{_{=0}^{t}g_{}^{2 }[k]}g_{t}[k],\] (7) \[_{t+1}^{V}[k] = _{t}^{V}[k]-[k]}{_{=0}^{t}(g _{}^{V}[k])^{2}}g_{t}^{V}[k]\] (8)

with \(g_{}=_{i_{}}^{}(x_{i_{}}^{}_{})x_{i_{ }}\) and \(g_{}^{V}=_{i_{}}^{}(x_{i_{}}^{}V_{})Vx_{ i_{}}\) for \(i_{}\) chosen uniformly from \([n]\), \(=0,1,,t\), \(t 0\). Moreover, updates (7) and (8) satisfy

\[_{t}=V_{t}^{V}, Vg_{t}=g_{t}^{V}, f( _{t})=f^{V}(_{t}^{V})\] (9)

for all \(t 0\) when \(_{0}=_{0}^{V}=0^{d}\). Furthermore we have

\[\|g_{t}^{V}\|_{V^{-2}}^{2} = \|g_{t}\|^{2}.\] (10)

The Proposition 2.1 highlights that the update rule of the form (6) satisfies a scale-invariance property for GLMs. In contrast, AdaGrad does not satisfy (9) and (10). In Appendix C, we illustrate numerically the scale-invariance of KATE and the lack of the scale-invariance of AdaGrad. We also emphasize that AdaGrad with \(=0\) is known to be a scale-free method4.

```
0: Initial point \(w_{0}^{d}\), step size \(>0,^{d}_{+}\) and \(b_{-1},m_{-1}=0\).
1:for\(t=0,1,...,T\)do
2: Compute \(g_{t}^{d}\) such that \([g_{t}]= f(w_{t})\).
3:\(b_{t}^{2}=b_{t-1}^{2}+g_{t}^{2}\)
4:\(|m_{t}^{2}=m_{t-1}^{2}+ g_{t}^{2}+^{2}}{b_{t}^{2}}|\)
5:\(.w_{t+1}=w_{t}-}{b_{t}^{2}}g_{t}.\) ```

**Algorithm 1**KATE

Design of Kate.In order to construct an algorithm following the update rule (6), one may choose \(m_{t}[k]=1\)\( k[d]\). However, the step size from (6) in this case may decrease very fast, and the resulting method does not necessarily converge. Therefore, we need a more aggressive choice of \(m_{t}\), which grows with \(t\). It motivates the construction of our algorithm KATE (Algorithm 1),5 where we choose \(m_{t}[k]=^{2}[k]+_{=0}^{t}^{2}[k]}{b_{ }^{2}[k]}}\). Note that the term \(_{=0}^{t}^{2}[k]}{b_{}^{2}[k]}\) is scale-invariant for GLMs (follows from Proposition 2.1). To make \(m_{t}\) scale-invariant, we choose \(^{d}\) in the following way:

* \( 0\): When \(\) is very small, \(m_{t}\) is also approximately scale-invariant for GLMs.
* \(=}{{( f(w_{0}))^{2}}}\): In this case \( b_{t}^{2}=^{2}}}{{( f(w_{0}))^{2}}}\) is scale-invariant for GLMs (follows from Proposition 2.1) as well as \(m_{t}\).

Kate can be rewritten in the following coordinate form

\[w_{t+1}[k]=w_{t}[k]-_{t}[k]g_{t}[k], k[d],\] (11)

where \(g_{t}\) is an unbiased estimator of \( f(w_{t})\) and the per-coefficient step size \(_{t}[k]\) is defined as

\[_{t}[k]^{2}[k]+_{=0}^{t} ^{2}[k]}{b_{}^{2}[k]}}}{b_{t}^{2}[k]}.\] (12)

Note that the numerator of the steps \(_{t}[k]\) is increasing with iterations \(t\). However, one of the crucial properties of this step size choice is that the steps always decrease with \(t\), which we rely on in our convergence analysis.

[Decreasing step size] For \(_{t}[k]\) defined in (11) we have

\[_{t+1}[k]_{t}[k], k[d].\] (13)

Comparison with the scale-invariant version of AdaGrad by Orabona et al. (2015).In the special case of GLMs, Orabona et al. (2015) propose a different version of AdaGrad. The method is proposed for the case of online convex optimization, and in the case of standard optimization with GLMs (4), it has the following form

\[w_{0}:=0, w_{t+1}:=-^{t} f_{i_{}}(w_{ })}{a_{t}^{2}+_{=0}^{t}( f_{i_ {}}(w_{})/a_{})^{2}}}, a_{t}:=_{=0,...,t}|x_{i_{ }}|,\] (14)

where \(\{i_{}\}_{=0}^{t}\) are arbitrary indices from \([n]\) (e.g., selected uniformly at random), functions \(f_{i}:^{d}^{d}\) are defined as \(f_{i}(w):=_{i}(x_{i}^{}w)\) for \(i[n]\), and \(\) is such that \(f_{i}(w)\) is \(\)-Lipschitz for \(i[n]\). In this setup, the update rule of KATE with \(w_{0}=0\) can be written as follows:

\[w_{t+1}:=-_{=0}^{t}}{b_{}^{2}} f_{i_{} }(w_{}), m_{t}:=^{t}( f_{i_{}}(w_{ }))^{2}+_{=0}^{t}( f_{i_{}}(w_{}))^{2}/b_ {}^{2}},\]

[MISSING_PAGE_FAIL:6]

**Theorem 3.3**.: Suppose \(f\) satisfy Assumption 3.1 and \(g_{t}= f(w_{t})\). Moreover, \(>0\) and \([k]>0\) are chosen such that \(_{0}[k]\) for all \(k[d]\). Then the iterates of \(\) satisfies

\[_{t T}\| f(w_{t})\|^{2}  )-f_{*})}{}}+_ {k=1}^{d}b_{0}[k])^{2}}{T+1},\]

where \(_{0}_{k[d]}[k]\).

Discussion on Theorem 3.3.Theorem 3.3 establishes an \((}{{T}})\) convergence rate for \(\), which is optimal for finding a first-order stationary point of a non-convex problem (Carmon et al., 2020). However, this result is not parameter-free. To prove the convergence, we assume that \(_{0}[k],\  k[d]\) in Theorem 3.3, which is equivalent to \((_{k}f(w_{0}))^{2}}f(w_{0}))^{2}}}{L},\  k[d]\). Note that the later condition holds for sufficiently small (dependent on \(L\)) values of \(,_{0}>0\).

However, it is possible to derive a parameter-free version of Theorem 3.3. Indeed, Lemma 2.2 implies that the step sizes are decreasing. Therefore, we can break down the analysis of \(\) into two phases: Phase I when \(_{0}[k]>}{{L}}\) and Phase II when \(_{0}[k]}{{L}}\), when the current analysis works, and then follow the proof techniques of Ward et al. (2020) and Xie et al. (2020). We leave this extension as a possible future direction of our work.

Stochastic setting.Next, we present the convergence guarantees for \(\) in the stochastic case, when we can access an unbiased gradient estimate \(g_{t}\) with non-zero noise.

**Theorem 3.4**.: Suppose \(f\) satisfy Assumption 3.1 and \(g_{t}\) is an unbiased estimator of \( f(w_{t})\) such that \(\) holds. Moreover, we assume \(\| f(w_{t})\|^{2}^{2}\) for all \(t\). Then the iterates of \(\) satisfy

\[_{t T}[\| f(w_{t})\|](\|}{T}+})^{}{{2}}} {_{f}}{}}},\]

where \(_{0}_{k[d]}[k]\) and

\[_{f}  f(w_{0})-f_{*}+2_{k=1}^{d} (+^{2})T}{g_{0}^{2}[k]})\] \[+\!\!_{k=1}^{d}([k]L}{2}+L}{2g_{0}^{2}[k]})(+^{2})T}{g_{0}^{ 2}[k]}).\]

Comparison with prior work.Theorem 3.4 shows an \((T}}{T}}{{4}})\) convergence rate for \(\) with respect to the metric \(_{t T}[\| f(w_{t})\|]\) for the stochastic setting. Note that, in the stochastic setting, \(\) achieves a slower rate than Theorem 3.3 due to noise accumulation. Up to the logarithmic factor, this rate is optimal (Arieyani et al., 2023). Similar rates for the same metric follow from the results6 of (Defossez et al., 2020) for \(\) and \(\).

Finally, Li and Orabona (2019) considers a variant of \(\) closely related to \(\):

\[w_{t+1}=w_{t}-}{(( I+_ {=1}^{t-1}\!g_{}g_{}^{}))^{+}},\] (16)

for some \([0,}{{2}})\) and \(>0\). It differs from \(\) in two key aspects: the denominator of the stepsize does not contain the last stochastic gradient, and also, instead of the square root of the sum of squared gradients, this sum is taken in the power of \(}{{2}}+\). However, the results from Li and Orabona (2019) do not imply convergence for the case of \(=}{{2}}\), which is expected since, in this case, the stepsize converges to zero too quickly in general. To compensate for such a rapid decrease, in \(\), we introduce an increasing sequence \(m_{t}\) in the numerator of the stepsize.

Proof technique.Compared to the AdaGrad, KATE uses more aggressive steps (the larger numerator of KATE due to the extra term \(_{=0}^{t}g_{t}^{_{t}^{2}[k]/b_{}^{2}[k]}\)). Therefore, we expect KATE to have better empirical performance. However, introducing \(_{=0}^{t}g_{t}^{_{t}^{2}[k]/b_{}^{2}[k]}\) in the numerator raises additional technical difficulties in the proof technique. Fortunately, as we rigorously show, the KATE steps \(_{t}[k]\) retain some of the critical properties of AdaGrad steps. For instance, they (i) are lower bounded by AdaGrad steps up to a constant, (ii) decrease with iteration \(t\) (Lemma 2.2), and (iii) have closed-form upper bounds for \(_{t=0}^{T}_{t}^{2}[k]g_{t}^{2}[k]\). These are indeed the primary building blocks of our proof technique.

## 4 Numerical Experiments

In this section, we implement KATE in several machine learning tasks to evaluate its performance. To ensure transparency and facilitate reproducibility, we provide an access to the source code for all of our experiments at https://github.com/naraya/KATE.

### Logistic Regression

In this section, we consider the logistic regression model

\[_{w^{d}}f(w)=_{i=1}^{n}(1+(- y_{i}x_{i}^{}w)),\] (17)

to elaborate on the scale-invariance and robustness of KATE for various initializations. For the experiments of this Section 4.1, we used Mac mini (M1, 2020), RAM 8 GB and storage 256 GB. Each of these plots took about 20 minutes to run.

#### 4.1.1 Robustness of Kate

To conduct this experiment, we set the total number of samples to \(1000\) (i.e. \(n=1000\)). Here, we simulate the independent vectors \(x_{i}^{20}\) such that each entry is from \((0,1)\). Moreover, we generate a diagonal matrix \(V^{20 20}\) such that \( V_{kk}}}{{}}(-10,10), \  k\). Similarly, we generate \(w^{*}^{20}\) with each component from \((0,1)\) and set the labels

\[y_{i}=1,&x_{i}^{}Vw^{*} 0,\\ -1,&x_{i}^{}Vw^{*}<0, i[n].\]

We compare KATE's performance with four other algorithms: AdaGrad, AdaGradNorm, SGD-decay and SGD-constant, similar to the section 5.1 of Ward et al. (2020). For each algorithm, we initialize with \(w_{0}=0^{20}\) and independently draw a sample of mini-batch size \(10\) to update the weight vector \(w_{t}\). We compare the algorithms \(\) AdaGrad with stepsize \(^{t}g_{t}^{2}}}\), \(\) AdaGradNorm with step size \(^{t}\|g_{}\|^{2}}}\), \(\) SGD-decay with stepsize \(}{{}}\), and \(\) SGD-constant with step size \(}{{}}\). Similarly, for KATE we use stepsize \(}{b_{t}^{2}}\) where \(m_{t}^{2}= b_{t}^{2}+_{=0}^{t}g_{}^{2}/b_{}^{2}\) and \(b_{t}^{2}=+_{=0}^{t}g_{}^{2}\). Here, we choose \(=f(w_{0})-f(w^{*})\) and vary \(\) in \(\{10^{-8},10^{-6},10^{-4},10^{-2},1,10^{2},10^{4},10^{6},10^{8}\}\).

In Figures 0(a), 0(b), and 0(c), we plot the functional value \(f(w_{t})\) (on the \(y\)-axis) after \(10^{4},5 10^{4}\), and \(10^{5}\) iterations, respectively. In theory, the convergence of SGD requires the knowledge of smoothness constant \(L\). Therefore, when the \(\) is small (hence the stepsize is large), SGD-decay and SGD-constant diverge. However, the adaptive algorithms KATE, AdaGrad, and AdaGradNorm can auto-tune themselves and converge for a wide range of \(\)s (even when the \(\) is too small). As we observe in Figure 1, when the \(\) is small, KATE outperforms all other algorithms. For instance, when \(=10^{-8}\), KATE achieves a functional value of \(10^{-3}\) after only \(10^{4}\) iterations (see Figure 0(a)), while other algorithms fail to achieve this even after \(10^{5}\) iterations (see Figure 0(c)). Furthermore, KATE performs as well as AdaGrad and better than other algorithms when the \(\) is large. _In particular, this experiment highlights that_ KATE_ is robust to initialization \(\).

#### 4.1.2 Performance of Kate on Real Data

In this section, we examine KATE's performance on real data. We test KATE on three datasets: heart, australian, and splice from the LIBSVM library (Chang and Lin, 2011). The response variables \(y_{i}\) of each of these datasets contain two classes, and we use them for binary classification tasks using a logistic regression model (17). We take \(=}{{( f_{(w_{0})})^{2}}}\) for \(\) and tune \(\) in all the experiments. For tuning \(\), we do a grid search on the list \(\{10^{-10},10^{-8},10^{-6},10^{-4},10^{-2},1\}\). Similarly, we tune stepsizes for other algorithms. We take \(5\) trials for each of these algorithms and plot the mean of their trajectories.

We plot the functional value \(f(w_{t})\) (i.e. loss function) in Figures 1(a), 1(b) and 1(c), whereas Figures 1(d), 1(e) and 1(f) plot the corresponding accuracy of the weight vector \(w_{t}\) on the \(y\)-axis for \(5,000\) iterations. We observe that \(\) performs superior to all other algorithms, even on real datasets.

### Training of Neural Networks

In this section, we compare the performance of \(\), \(\) and \(\) on two tasks, i.e. training ResNet18 (He et al., 2016) on the CIFAR10 dataset (Krizhevsky and Hinton, 2009) and BERT (Devlin et al., 2018) fine-tuning on the emotions dataset (Saravia et al., 2018) from the Hugging Face Hub. We use internal cluster with the following hardware: AMD EPYC 7552 48-Core Processor, 512GiB RAM, NVIDIA A100 40GB GPU, 200gb user storage space.

General comparison.We choose standard parameters for \(\) (\(_{1}=0.9\) and \(_{2}=0.999\)) that are default values in PyTorch and select the learning rate of \(10^{-5}\) for all considered methods. We run \(\) with different values of \(\{0,10^{-1},10^{-2}\}\). For the image classification task, we normalize the images (similar to Horvath and Richtarik (2020)) and use a mini-batch size of 500. For the BERT fine-tuning, we use a mini-batch size 160 for all methods.

Figures 3-8 report the evolution of top-1 accuracy and cross-entropy loss (on the \(y\)-axis) calculated on the test data. For the image classification task, we observe that \(\) with different choices of

Figure 1: Comparison of \(\) with \(\), \(\), SGD-decay and SGD-constant for different values of \(\) (on \(x\)-axis for logistic regression model. Figure 0(a), 0(b) and 0(c) plots the functional value \(f(w_{t})\) (on \(y\)-axis) after \(10^{4},5 10^{4}\), and \(10^{5}\) iterations respectively.

Figure 2: Comparison of \(\) with \(\), \(\), SGD-decay and SGD-constant on datasets heart, australian, and splice from LIBSVM. Figures 1(a), 1(b) and 1(c) plot the functional value \(f(w_{t})\), while 1(d), 1(e) and 1(f) plot the accuracy on \(y\)-axis for \(5,000\) iterations.

\(\) outperforms Adam and AdaGrad. Finally, we also observe that KATE performs comparably to Adam on the BERT fine-tuning task and is better than AdaGrad. These preliminary results highlight the potential of KATE to be applied for training neural networks for different tasks. For BERT each run takes about 35 minutes, and 25 minutes for ResNet.

Hyper-parameters tuning.Next, we compare baselines presented in Saravia et al. (2018) for emotions classification and Zhang et al. (2019) for image classification. These papers provide efficient setups for learning rates and learning rate schedulers that are reasonable to compare with. Saravia et al. (2018) performs a search of efficient learning rate and uses a linear learning rate scheduler with warmup for Adam optimizer. A different learning rate (1e-5), \(\)=1e-5 and the same scheduler applied for KATE lead to the same performance, see Figure 9. We would like to point out that it is challenging to find a reference for hyper-parameters for a certain setup. Thus, to fairly compare with Saravia et al. (2018) we use distiloberta-base model. Zhang et al. (2019) did a grid search for an efficient learning rate and used a multi-step scheduler for Adam optimizer, decaying the learning rate by a factor of 5 at the 60th, 120th, and 160th epochs. Zhang et al. (2019) refers to DeVries and Taylor (2017) for the code implementing special techniques, namely data augmentation and cutout to achieve higher accuracy. A different learning rate (1e-3), the same scheduler and \(\)=1e-3 applied for KATE demonstrates comparable performance, see Figure 10. For BERT each run takes about 20 minutes, while 100 minutes for ResNet.

Figure 10: Emotion: \(=0.001\)