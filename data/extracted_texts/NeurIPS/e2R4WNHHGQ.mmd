# For true positive rate (TPR):

Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium

 Mehdi Yazdani-Jahromi

Department of Computer Science

University of Central Florida

Orlando, FL 32816

yazdani@ucf.edu

&Ali Khodabandeh Yalabadi

Department of Industrial Engineering

University of Central Florida

Orlando, FL 32816

yalabadi@ucf.edu

&AmirArsalan Rajabi

Department of Computer Science

University of Central Florida

Orlando, FL 32816

am954283@ucf.edu

&Aida Tayebi

Department of Industrial Engineering

University of Central Florida

Orlando, FL 32816

ai530737@ucf.edu

&Ivan Garibay

Department of Industrial Engineering

University of Central Florida

Orlando, FL 32816

igaribay@ucf.edu

&Ozlem Ozmen Garibay

Department of Industrial Engineering

University of Central Florida

Orlando, FL 32816

ozlem@ucf.edu

###### Abstract

The persistent challenge of bias in machine learning models necessitates robust solutions to ensure parity and equal treatment across diverse groups, particularly in classification tasks. Current methods for mitigating bias often result in information loss and an inadequate balance between accuracy and fairness. To address this, we propose a novel methodology grounded in bilevel optimization principles. Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions while mitigating bias in the trained model. Theoretical analysis indicates that the upper bound on the loss incurred by this method is less than or equal to the loss of the Lagrangian approach, which involves adding a regularization term to the loss function. We demonstrate the efficacy of our model primarily on tabular datasets such as UCI Adult and Heritage Health. When benchmarked against state-of-the-art fairness methods, our model exhibits superior performance, advancing fairness-aware machine learning solutions and bridging the accuracy-fairness gap. The implementation of FairBiNN is available on [https://github.com/yazdanimehdi/FairBiNN](https://github.com/yazdanimehdi/FairBiNN).

## 1 Introduction

Artificial intelligence and machine learning models have seen significant growth over the past decades, leading to their integration into various domains such as hiring pipelines, face recognition, financial services, healthcare, and criminal justice. This widespread adoption of algorithmic decision-making has raised concerns about algorithmic bias, which can result in discrimination and unfairness towards minority groups. Recently, the issue of fairness in artificial intelligence has garnered considerable attention from interdisciplinary research communities, addressing these ethical concerns .

Several definitions of fairness have been proposed to tackle unwanted bias in machine learning techniques. These definitions generally fall into two categories: individual fairness and group fairness. Individual fairness ensures that similar individuals are treated similarly, with similarities determined by past information [20; 73]. Group fairness, on the other hand, measures statistical equality between different subgroups defined by sensitive characteristics such as race or gender [76; 41; 26]. In this paper, we focus on group fairness, which we will refer to simply as fairness from this point onward.

Fairness approaches in machine learning are commonly categorized into three groups: (1) Pre-process approaches: These methods involve changing the data before training to improve fairness, such as reweighing labels or adjusting features to reduce distribution differences between privileged and unprivileged groups, making it harder for classifiers to differentiate them [34; 42; 22; 63]. Generative adversarial networks were also utilized to produce unbiased datasets by altering the generator network's value function to balance accuracy and fairness . (2) In-process approaches: These methods modify the algorithm during training, for instance by adding regularization terms to the objective function to ensure fairness. Examples include penalizing the mutual information between protected attributes and classifier predictions to allow a trade-off between fairness and accuracy , and adding constraints to satisfy a proxy for equalized odds [74; 75]. (3) Post-process approaches: These techniques adjust the outcomes after training, such as flipping some outcomes to improve fairness , or using different thresholds for privileged and unprivileged groups to optimize the trade-off between accuracy and fairness [45; 11].

In this work we targeted the in-process bias mitigation category. Traditionally, the fairness multi-criteria problem has been addressed using Lagrangian optimization, wherein the objective function is a weighted sum of the primary and secondary loss functions. While this approach allows for the explicit incorporation of fairness constraints through Lagrange multipliers, it may overlook the complex interdependencies between the primary and secondary objectives.

A promising alternative to the Lagrangian is the bilevel optimization approach which offers several advantages. By formulating the problem as a hierarchical optimization task, we can explicitly model the interactions between the primary and secondary objectives. This allows us to capture the nuanced dynamics of fairness optimization and ensure that improvements in one objective do not come at the expense of the other.

In summary, we introduce a novel method that can be trained on existing datasets without requiring any alterations to the data itself (data augmentation, perturbation, etc). Our methodology provides a principled approach to addressing the multi-criteria fairness problem in neural networks. Through rigorous theoretical analysis, we formulated the problem as a bilevel optimization task, proving that it yields Pareto-optimal solutions. We derived an effective optimization strategy that is at least as effective as the Lagrangian approach. Empirical evaluations on tabular datasets demonstrate the efficacy of our method, achieving superior results compared to traditional approaches.

## 2 Related works

Multi-objective optimization in neural networks involves optimizing two or more conflicting objectives simultaneously. Fairness problems are inherently multi-objective in nature, as improvements in one objective (e.g., enhancing fairness) often come at the expense of another objective (e.g., improving accuracy). Several optimization techniques in neural networks have been employed to balance accuracy and fairness. Classic Methods transform these objectives into a single objective by combining them, typically using a weighted sum where each objective is multiplied by a weight that reflects its importance. Adding Regularization and penalty terms are the most common methods that incorporate fairness constraints (e.g., demographic parity, equal opportunity) directly into the loss function, penalizing disparities in prediction errors across demographic groups or any other unfair behavior. To reduce variation across different groups, Zafar et al.  proposes "disparate mistreatment", a new notation for fairness, and standardized the decision bounds of a convex margin-based classifier. Adversarial debiasing and Fair Representation Learning are two examples of these techniques, which encourage the model to generate fair outcomes by introducing a penalty term based on an adversarial network or a representation learning framework that is invariant to protected attributes, respectively. Zhang et al.  addressed bias by limiting an adversary's ability to infer sensitive characteristics from predictions. Avoiding the complexity of adversarial training, Moyer et al.  used mutual information to achieve invariant data representations concerning specific factors. Song et al.  proposedan information-theoretic method that leverages both information-theoretic and adversarial approaches to achieve controllable fair data representations, adhering to demographic parity. By incorporating a forget-gate similar to those in LSTMs, Jaiswal et al.  introduced adversarial forgetting to enhance fairness. Gupta et al.  utilized certain estimates for contrasting information to optimize theoretical objectives, facilitating suitable trade-offs between demographic parity and accuracy in the statistical population. Lagrangian optimization techniques are a subset of these techniques that use Lagrange multipliers or other similar techniques to incorporate constraints directly into the objective function, turning constrained optimization problems into unconstrained ones. Agarwal et al.  proposes an approach for fair classification by framing the constrained optimization problem as a two-player game where one player optimizes the model parameters, and the other imposes the constraints, and Lagrangian multipliers are used to solve this problem. Cotter et al.  expanded this work in a more general inequality-constrained setting, by simultaneously training each player on two distinct datasets to enhance generalizability. They enforce independence by regularizing the covariance between predictions and sensitive variables, which reduces the variation in the relationship between the two. Despite analytic solutions and theoretical assurances, scaling game-theoretic techniques for more complex models remains challenging . In addition, these constraints-based optimizations are data-dependent, meaning the model may exhibit different behavior during evaluation even if constraints are met during training. Less common approaches including Pareto-based genetic algorithm, Reinforcement Learning, Gradient-Based Methods, and Transfer and Meta-Learning Approaches have been also utilized in this domain. Mehrabi et al.  demonstrated how proxy attributes lead to indirect unfairness using an attention-based approach and employed a post-processing method to reduce the weight of attributes responsible for unfairness. Perrone et al.  introduces a general constrained Bayesian optimization (BO) framework to fine-tune the model's performance while enforcing one or multiple fairness constraints. A probabilistic model is used to describe the objective function, and estimates are made for the posterior variances and means for each hyperparameter configuration. By adding a fairness regularization term to a meta-learning framework, Slack et al.  suggests an adaptation of the Model-Agnostic Meta-Learning (MAML)  algorithm. The primary objective and fairness regularization terms are included in the loss function used to update the model parameters for each task during the inner loop (Learner). The model parameters are updated in the outer loop (Meta-learner) to maximize performance and fairness across all tasks. Although these techniques have achieved a good balance between fairness and accuracy, they might not capture all of the complex interdependencies between these two objectives. In this paper we propose a bilevel optimization approach as an alternative to the Lagrangian approaches. Bilevel optimization is a hierarchical structure in which the context or constraints for the "follower" (lower-level) problem are set by the "leader" (upper-level) problem . The leader makes decisions first, and the follower optimizes their decisions based on the leader's choices. This approach can handle more complex and nuanced multi-objective optimization problems in neural networks and is suitable for scenarios where one objective directly influences another and there are complex interactions between the two objectives. In this paper we demonstrate that the bilevel optimization often can achieve better balance and performance compared to classic regularization-based optimization approaches [17; 59; 10]. Bilevel optimization offers several advantages; by explicitly modeling a two-level decision-making process, his approach represents the problems in a more natural way where one objective inherently depends on the outcome of another. It provides more flexibility and control over the optimization process by enabling separate optimization of constraints at each level. The upper-level optimization can dynamically adjust the lower-level objective based on the current solution, potentially leading to more adaptive and context-sensitive optimization outcomes. Fairness and accuracy objectives can be directly integrated into the optimization framework without the need for additional strategies such as meta-learning.

## 3 Methodology

In this section we are introducing a novel bi-level optimization framework for training neural networks to obtain Pareto optimal solutions when optimizing two potentially competing objectives. Our approach leverages a leader-follower structure, where the leader problem aims to minimize one objective function (e.g. a primary loss), while the follower problem optimizes a secondary objective. We provide theoretical guarantees that our bi-level approach produces Pareto optimal solutions and performs at least as well as, and often strictly better than, the common practice of combining multiple objectives via a weighted regularization term in a single loss function. The full statements

[MISSING_PAGE_FAIL:4]

**Assumption 3.3**.: \(|_{s}-_{s}|\), where \(\) is sufficiently small, i.e., the steps of the secondary parameters are sufficiently small. \(_{s}\) and \(_{s}\) represent the parameters for the secondary objective and their updated values, respectively.

**Assumption 3.4**.: Let \(f_{l}(.)\) denote the output function of the \(l\)-th layer in a neural network with \(L\) layers. For each layer \(l\), there exists a constant \(c_{l}>0\) such that for any input \(x_{l}\) to the \(l\)-th layer:

\[|f_{l}(x_{l})| c_{l} \]

where \(|.|\) denotes a suitable norm (e.g., Euclidean norm for vectors, spectral norm for matrices). Refer to Section A.4 for common practices in implementing the bounded output assumption.

We recognized the importance of examining how our theory's underlying assumptions apply to real-world applications. For a detailed discussion, refer to Section A.3.

**Lemma 3.5**.: _Let \(f(x;)\) be a neural network with \(L\) layers, where each layer is a linear transformation followed by a Lipschitz continuous activation function. Let \(\) be the set of all parameters of the neural network, and \(_{s}\) be any subset of parameters. Then, \(f(x;)\) is Lipschitz continuous with respect to \(_{s}\). [See proof A.4]_

We discussed the Lipschitz continuity of common activation functions and popular neural networks, such as CNNs and GNNs, in Sections A.5 and A.6, respectively.

Theorems 3.6 and 3.7 further inform our approach. The former establishes conditions under which improvements in the secondary objective lead to improvements in the primary objective, while the latter guarantees the existence of unique minimum solutions for the secondary loss function under certain optimization conditions.

**Theorem 3.6**.: _Let \(f(_{p},_{s})\) for constant \(_{s}\) be the primary objective loss function and \((_{p},_{s})\) for constant \(_{p}\) be the secondary objective loss function, where \(_{p}_{p}\) and \(_{s}_{s}\) are the primary task and secondary task parameters, respectively._

_Consider two sets of parameters \((_{p},_{s})\) and \((_{p},_{s})\) such that \((_{p},_{s})(_{p},_{s})\). Then \(f(_{p},_{s}) f(_{p},_{s})\) holds based on Lemma 3.5. [See proof A.5]_

**Theorem 3.7**.: _Let \((_{p},_{s})\) be the secondary loss function, where \(_{p}_{p}\) and \(_{s}_{s}\) are the primary and secondary task parameters, respectively. Let \((_{p}^{(t)},_{s}^{(t)})\) denote the parameters at optimization step \(t\), and let \((_{p}^{(t+1)},_{s}^{(t+1)})\) be the updated parameters obtained by minimizing \((_{p}^{(t)},_{s})\) with respect to \(_{s}\) using a sufficiently small step size \(>0\), i.e.:_

\[_{s}^{(t+1)}=_{s}^{(t)}-_{_{s}}(_{p}^ {(t)},_{s}^{(t)}) \]

_Then, for a sufficiently small step size \(\), the updated secondary parameters \(_{s}^{(t+1)}\) are the unique minimum solution for the secondary loss function \((_{p}^{(t)},_{s})\). [See proof A.6]_

Based on these theoretical insights, we derive our bilevel optimization formulation, as described in Theorem 3.8. This theorem establishes the equivalence between the bi-criteria problem and a bilevel optimization problem, allowing us to apply existing theoretical results on Stackelberg equilibrium to the optimization of neural networks.

**Theorem 3.8**.: _Under the assumptions stated in Theorems 3.6 and 3.7, the bi-criteria problem (Eq. (1)) is equivalent to the bilevel optimization problem:_

\[_{_{p}_{p}} f(_{p},_{s}^{*}(_{p}))\] (6) _s.t._ \[_{s}^{*}(_{p})=*{arg\,min}_{_{s} _{s}}(_{p},_{s}) \]

_where \(_{s}^{*}(_{p})\) denotes the optimal secondary parameters for a given \(_{p}\)._

Proof.: The proof follows from Theorems 3.6 and 3.7 .

By Theorem 3.6, under the assumptions of strict convexity, Lipschitz continuity, and sufficiently small steps of the secondary parameters, if \((_{p},_{s})(_{p},_{s})\), then \(f(_{p},_{s}) f(_{p},_{s})\).

By Theorem 3.7, under the same assumptions, for each optimization step of the secondary loss function with sufficiently small steps, the updated parameters are the unique minimum solution for the secondary loss function, then the bi-criteria problem (1) is equivalent to the bilevel optimization problem.

Therefore, the conclusions drawn in the paper  can be directly applied to the multi-objective optimization problem in neural networks, as the problem is equivalent to the bilevel optimization problem under the stated assumptions. 

**Theorem 3.9**.: _Assume that the step size in the Lagrangian approach \(_{}\) is equal to the step size for the outer optimization problem in the bilevel optimization approach \(_{f}\), the scale of the two loss functions should be comparable, the Lagrangian multiplier \(\) is equal to the step size for the inner optimization problem in the bilevel optimization approach \(_{s}\), and \(_{p}\) is overparameterized for the given problem. Then, under certain conditions, the overall performance of the primary loss function in the bilevel optimization approach may be better than the Lagrangian approach._

Proof.: Let \(f(_{p},_{s})\) denote the primary loss and \((_{p},_{s})\) denote the secondary loss. Assume that both \(f\) and \(\) are differentiable with respect to \(_{p}\) and \(_{s}\). Define the Lagrangian function as:

\[(_{p},_{s},)=f(_{p},_{s})+ (_{p},_{s}) \]

The update rules for \(_{p}\) and \(_{s}\) in the Lagrangian approach are:

\[_{p}^{(t+1)}=_{p}^{(t)}-_{}_{ _{p}}(_{p}^{(t)},_{s}^{(t)},) \] \[_{s}^{(t+1)}=_{s}^{(t)}-_{}_{ _{s}}(_{p}^{(t)},_{s}^{(t)},) \]

The update rules for \(_{p}\) and \(_{s}\) in the bilevel optimization approach are:

\[_{p}^{(t+1)}=_{p}^{(t)}-_{f}_{_{p }}f(_{p}^{(t)},_{s}^{(t)}) \] \[_{s}^{(t+1)}=_{s}^{(t)}-_{s}_{_{s} }(_{p}^{(t)},_{s}^{(t)}) \]

Due to the overparameterization of \(_{p}\), there exists a set \(_{p}\) such that for any \(_{p}_{p}\), \(f(_{p},_{s})=f(_{p}^{*},_{s})\), where \(_{p}^{*}\) is an optimal solution for the primary loss when \(_{s}\) is fixed. Suppose that the bilevel optimization approach converges to a solution \((_{p}^{B},_{s}^{B})\) and the Lagrangian approach converges to a solution \((_{p}^{L},_{s}^{L})\). Consider the following inequality:

\[f(_{p}^{B},_{s}^{B}) =f(_{p}^{B},_{s}^{B})+(_{p}^{B}, _{s}^{B})-(_{p}^{B},_{s}^{B}) \] \[ f(_{p}^{B},_{s}^{B})+(_{p}^{ B},_{s}^{B})-(_{p}^{L},_{s}^{L})\] (14) \[=(_{p}^{B},_{s}^{B},)- (_{p}^{L},_{s}^{L})\] (15) \[(_{p}^{L},_{s}^{L},)- (_{p}^{L},_{s}^{L})\] (16) \[=f(_{p}^{L},_{s}^{L}) \]

The first inequality holds because \((_{p}^{L},_{s}^{L})\) is the minimizer of \((_{p},_{s})\) in the Lagrangian approach. The second inequality holds because \((_{p}^{L},_{s}^{L})\) is the minimizer of \((_{p},_{s},)\) in the Lagrangian approach. Since \(_{p}^{B}_{p}\) and \(_{p}^{L}_{p}\), we have:

\[f(_{p}^{B},_{s}^{B})=f(_{p}^{B},_{s}^{B}) f(_ {p}^{L},_{s}^{L}) \]

Therefore, under the assumptions that \(_{}=_{f}\), The sizes of the two loss functions \(f(_{p},_{s})\) and \((_{p},_{s})\) should not differ significantly in terms of their order of magnitude, \(=_{s}\), and \(_{p}\) is overparameterized for the given problem, the bilevel optimization approach may converge to a solution that achieves better performance for the primary loss compared to the Lagrangian approach.

### Practical Implementation

To connect the Stackelberg game analysis with a practical implementation for datasets, we can formulate a bilevel optimization problem. The upper-level problem corresponds to the accuracy player (leader), while the lower-level problem corresponds to the fairness player (follower). We'll use gradient-based optimization techniques to solve the problem.

Let's consider a dataset \(=\{(x_{i},a_{i},y_{i})\}_{i=1}^{N}\), where \(x_{i}\) represents the features, \(a_{i}\) represents the sensitive attribute, and \(y_{i}\) represents the target variable for the \(i\)-th sample.

The optimization problem can be formulated as follows:

\[_{_{a}} _{i=1}^{N}L_{acc}(f(x_{i};_{a},_{f}^{*} ),y_{i})\] (19) s.t. \[_{f}^{*}_{_{f}}_{i=1}^{N}L_{ fair}(f(x_{i};_{a},_{f}),a_{i},y_{i}) \]

where \(f(x;_{a},_{f})\) is the model parameterized by the accuracy parameters \(_{a}\) and fairness parameters \(_{f}\), \(L_{acc}\) is the accuracy loss function (e.g., binary cross-entropy), and \(L_{fair}\) is the fairness loss function (e.g., demographic parity loss). We showed that the demographic parity loss function, when applied to the output of neural network layers, is also Lipschitz continuous (Theorem 3.10).

**Demographic Parity Loss Function:** The demographic parity loss function \(DP(f)\) is defined as:

\[DP(f)=|_{x p(x|a=0)}[f(_{1};x)]-_{x p(x |a=1)}[f(_{2};x)]| \]

where \(a\) is a sensitive attribute (e.g., race, gender) with two possible values (0 and 1), and \(p(x|a)\) is the conditional probability distribution of \(x\) given \(a\).

**Theorem 3.10**.: _If \(f(x)\) is Lipschitz continuous with Lipschitz constant \(L_{f}\), then the demographic parity loss function \(_{DP}(f)\) is also Lipschitz continuous with Lipschitz constant \(L_{DP}=2L_{f}\). [See proof A.9]_

_We can easily extend this theorem to include another common fairness metric, equalized odds, as explained in Section A.2._

Here's a practical implementation using gradient-based optimization:

```
1:Initialize accuracy parameters \(_{a}\) and fairness parameters \(_{f}\)
2:while not converged or max iterations \(T\) not reached do
3: Accuracy player's optimization
4: Sample a minibatch \(_{a}\)
5: Compute accuracy loss: \(L_{a}=_{a}|}_{i_{a}}L_{acc}(f(x_{i}; _{a},_{f}),y_{i})\)
6: Update accuracy parameters: \(_{a}_{a}-_{a}_{_{a}}L_{a}\)
7: Fairness player's optimization
8: Sample a minibatch \(_{f}\)
9: Compute fairness loss: \(L_{f}=_{f}|}_{i_{f}}L_{fair}(f(x_{i}; _{a},_{f}),a_{i},y_{i})\)
10: Update fairness parameters: \(_{f}_{f}-_{f}_{_{f}}L_{f}\)
11:endwhile
```

**Algorithm 1** Fairness-Accuracy Bilevel Optimization

In practice, the model \(f(x;_{a},_{f})\) can be implemented as a neural network with separate layers for accuracy and fairness (Figure 8). The accuracy layers are parameterized by \(_{a}\), while the fairness layers are parameterized by \(_{f}\). The accuracy loss \(L_{acc}\) can be chosen based on the task at hand, such as binary cross-entropy for binary classification or mean squared error for regression. The fairness loss \(L_{fair}\) can be a fairness metric such as demographic parity loss or equalized odds loss. The learning rates \(_{a}\) and \(_{f}\) control the step sizes for updating the accuracy and fairness parameters, respectively. They can be tuned using techniques like grid search or learning rate scheduling.

By implementing this algorithm on a dataset, we can optimize the model to balance accuracy and fairness, guided by the Stackelberg game formulation. At each iteration, the parameters related to accuracy are optimized while keeping the fairness parameters fixed. Then, with the accuracy parameters held constant, the fairness parameters are optimized. This separate optimization process provides fine-grained control over the trade-off between accuracy and fairness.

## 4 Experiments

In this section, we contrast our methodology with other benchmark approaches found in the literature.

We employed two metrics for evaluation: accuracy (higher values preferred) for the classification task, and demographic parity differences (DP, lower values preferred) for fairness assessment. Detailed descriptions of all metrics used and implementation settings are available in sections A.9 and A.10 in the appendix, respectively.

We evaluated our method for bias mitigation to various current state-of-the-art approaches. We concentrate on strategies specifically tuned to achieve the best results in statistical parity metrics on tabular studies.

### Datasets:

We used two well-known benchmark datasets in this field for our experiments which are as follows: _UCI Adult Dataset_, This dataset is based on demographic data gathered in 1994, including a train set of 30000 and a test set of 15,000 samples. The goal is to forecast if the salary is more than $50,000 yearly, and the binary protected attribute is the gender of samples gathered in the dataset.

_Heritage Health Dataset_, Predicting the Charleson Index, a measure of a patient's 10-year mortality. The Heritage Health dataset contains samples from roughly 51,000 patients of which 41000 are in the training set, and 11000 are in the test set. The protected attribute, which has nine potential values, is age.

### Baselines:

We compare our results with the following state-of-the-art methods as benchmarks:

* **CVIB**: Achieves fairness using a conditional variational autoencoder.
* **MIFR**: Optimizes the fairness objective with a mix of information bottleneck factor and adversarial learning.
* **FCRL**: Uses specific approximations for contrastive information to maximize theoretical goals, facilitating appropriate trade-offs among statistical parity, demographic parity, and precision.
* **MaxEnt-ARL**: Employs adversarial learning to mitigate unfairness.
* **Adversarial Forgetting**: Uses adversarial learning techniques for fairness.
* **Fair Consistency Regularization (FCR)**: Aims to minimize and balance consistency loss across groups.
* **Robust Fairness Regularization (RFR)**: Considers the worst-case scenario within the model weight perturbation ball for each sensitive attribute group to ensure robust fairness.

### Bilevel (FairBiNN) vs. Lagrangian Method

We compare our proposed FairBiNN method with the traditional Lagrangian regularization approach to empirically validate the theoretical benefits of bilevel optimization. Our analysis focuses on the convergence behavior and stability of both methods. For comprehensive details on performance and computational complexity comparison, refer to Section A.7. In the appendix, we have provided the BCE loss plots over epochs for each dataset (Fig. 2) and demonstrated the superior performance of the Bi-level approach compared to the Lagrangian approach. We have also presented a comparative analysis of these approaches for the trade-off between accuracy and Statistical Parity Difference (SPD) in Figure 3.

While Theorem 3.9 in the paper proves that, under certain assumptions, the primary loss function in the bilevel optimization approach is upper bounded by the loss of the Lagrangian approach at the optimal solution, it does not analyze or guarantee the convergence behavior of the algorithms. The empirical results for the Health and Adult datasets show that the bilevel approach outperforms the Lagrangian method in minimizing the BCE loss. However, further investigation is needed to understand the convergence properties of the algorithms and connect the theoretical results with empirical observations. Despite this, the experimental results highlight the potential of the bilevel optimization framework to optimize accuracy and fairness objectives, offering a promising approach to address the multi-criteria fairness problem in neural networks.

#### 4.3.1 Benchmark Comparison

We provide average accuracy as a measure of most probable accuracy and maximum demographic parity as a measure of worst-case scenario bias, calculated across five iterations of the training process using random seeds. Unlike Gupta et al. , we did not use any preprocessing on the data before feeding it to our network. Reported results for our model are Pareto solutions for the neural network during training with different \(_{f}\). Results are reported for methods with a multi-layer perceptron classifier with two hidden layers.

Figures 0(a) and 0(b) show trade-offs of the statistical demographic parity vs. accuracy associated with various bias reduction strategies in the UCI Adult dataset and Heritage Health dataset, respectively. The ideal area of the graph for the result of a method is to measure how much the curve is located in the lower right corner of the graph, which means accurate and fair results concerning protected attributes. Our results demonstrate that the Bilevel design significantly outperforms competing methods in Adult dataset.

## 5 Limitations and Future Work

While our results are promising, it's important to acknowledge several limitations of our current approach:

One of the most widely used activation functions, softmax, is not Lipschitz continuous. This limits the direct application of our method to multiclass classification problems. Future work could explore alternative activation functions or modifications to the softmax that preserve Lipschitz continuity while maintaining similar functionality for multiclass problems.

Figure 1: Accuracy of various benchmark models compared to the FairBiNN model versus statistical demographic parity for the (a) UCI Adult dataset and (b) Heritage Health dataset. The optimal region on this graph is the bottom right, indicating high accuracy and low DP. The results demonstrate that our model (red diamond markers) significantly outperforms other benchmark models on the UCI Adult dataset and closely competes with recent state-of-the-art models on the Heritage Health dataset.

Attention mechanisms, which are widely used in modern language models and other architectures, are not Lipschitz continuous. This presents a challenge for extending our method to state-of-the-art architectures in natural language processing and other domains that heavily rely on attention. However, research into the Lipschitz continuity of attention layers has already begun, with Daoulas et al.  introducing LipschitzNorm, a simple and parameter-free normalization technique applied to attention scores to enforce Lipschitz continuity in self-attention mechanisms. Their experiments on graph attention networks (GAT) demonstrate that enforcing Lipschitz continuity generally enhances the performance of deep attention models.

Our theoretical analysis primarily provides guarantees in comparison to regularization methods. While the results show improvements in fairness overall, the theory does not offer absolute fairness guarantees for the final model. Extending the theoretical framework to include direct fairness guarantees could strengthen the method's applicability.

This method was not validated on dataset augmentation approaches, which are common in practice for improving model generalization and robustness. Future work should investigate how our method interacts with various data augmentation techniques and whether it maintains its fairness properties under such conditions.

Our current implementation focuses on a single fairness metric (demographic parity). In practice, multiple, sometimes conflicting, fairness criteria may be relevant. Extending our method to handle multiple fairness constraints simultaneously could make it more versatile for real-world applications.

Addressing these limitations presents exciting opportunities for future research. By tackling these challenges, we can further enhance the applicability and effectiveness of fair machine learning methods across a broader range of real-world scenarios and cutting-edge architectures.

## 6 Discussion and Conclusion

Our primary contribution lies in the theoretical foundation and general applicability of the proposed framework, rather than extensive ablation studies on specific datasets or network configurations. However, we recognize the importance of empirical evaluations. Our work introduces a novel approach to addressing the multi-criteria fairness problem in neural networks, supported by theoretical analysis, particularly Theorem 3.8, which establishes properties of the optimal solution under certain assumptions, independent of specific datasets or architectures. The results on vision and graph datasets (A.11) and ablation studies on the impact of \(\) (A.13.1), the position of fairness layers (A.13.3), and different layer types (A.13), presented in the appendix, demonstrate the effectiveness and versatility of our approach. These studies show that the bilevel optimization framework can be successfully applied to various layer types and network architectures, beyond the single linear layer used in the main experiments. Our experimentation across diverse datasets, including UCI Adult, Heritage Health, and other domains like graph datasets [A.8.1] (POKEC-Z, POKEC-N, and NBA) and vision datasets [A.8.2] (CelebA), further illustrates the versatility and efficacy of our method. Including these ablation studies in the appendix allows us to maintain the main text's focus on theoretical contributions and the general framework while providing additional empirical evidence to support our claims.

Our results demonstrate the superiority of our model over state-of-the-art fairness methods in reducing bias while maintaining accuracy, highlighting the potential of our framework to advance fairness-aware machine learning solutions. Notably, our study represents a significant contribution by formulating multi-objective problems in neural networks as a bilevel design, providing a powerful tool for achieving equitable outcomes across diverse groups in classification tasks. Future research can address our method's limitations and explore potential directions as outlined in Section 5.