# Enhancing Reasoning Capabilities of LLMs

via Principled Synthetic Logic Corpus

Terufumi Morishita\({}^{1}\)  Gaku Morio\({}^{1}\)1  Atsuki Yamaguchi\({}^{2}\)1  Yasuhiro Sogawa\({}^{1}\)

\({}^{1}\)Advanced AI Innovation Center, Hitachi \({}^{2}\)The University of Sheffield

Equal ContributionWork done at Hitachi

###### Abstract

Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose **Additional Logic Training (ALT)**, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named **Formal Logic Deduction Diverse** (\(_{ 2}\)), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on \(_{ 2}\) substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.

## 1 Introduction

Knowledge and reasoning have long been considered essential elements for achieving _artificial intelligence_(McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973; Shortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., "objects with mass generate a gravitational field" and "the Earth has mass." Reasoning involves combining multiple facts according to specific rules to obtain new knowledge. For example, the new knowledge that "the Earth generates a gravitational field" can be derived from the aforementioned two facts.

Recent observations suggest that LLMs can solve problems using memorized knowledge of similar samples seen during pre-training, but they cannot solve novel, unknown problems that require reasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can solve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi et al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the "knowledge cutoff" but not from the present year (Mitchell, 2023). This bias towards knowledge has been observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023; Wu et al., 2023; Dziri et al., 2023).

LLMs' poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the pre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita et al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced by fallacies and biases commonly found in online debates (Hansson, 2004; Guiasu and Tindale, 2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather than through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, astraightforward strategy to improve LLMs' reasoning capabilities is to prepare many high-quality reasoning samples and train LLMs on them.

We propose one such approach, **Additional _Logic_ Training (ALT), which utilizes high-quality samples of _logical_ reasoning, the most fundamental form of reasoning. To prepare such samples, we utilize synthetic generation (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), where computer programs generate deductive reasoning samples in which a given hypothesis is proven or disproven by combining given facts following rigid reasoning rules. We overview ALT in Figure 2.

In synthetic generation, computer programs generate samples according to pre-designed patterns, so this design largely determines the quality of these samples by nature. Thus, we start by discussing **what is the ideal design for synthetic logic samples**, incorporating symbolic logic theory and empirical findings (Section 2). The essence of logical reasoning lies in its ability to handle unknown facts, unlike knowledge, which deals solely with established facts, such as commonsense facts; therefore, samples must cover reasoning with unknown facts. Samples must include both _illogical_ and logical reasoning to enable LLMs to distinguish between them. The samples must cover various patterns regarding a comprehensive set of reasoning aspects, such as reasoning rules and linguistic expressions of logical statements. We summarize these discussions into _design principles_, which guide the design of synthetic logic samples. Finally, based on these principles, we construct a synthetic corpus named **Formal Logic _D_eduction _D_iverse (\(_{ 2}\)), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors (Section 3).

We then empirically verify that ALT can enhance LLMs' reasoning capabilities (Sections 4, 5). Using 31 benchmarks covering diverse tasks, we observed that ALT on \(_{ 2}\) substantially boosts state-of-the-art LLMs' reasoning capabilities. Even LLaMA-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens, shows substantial improvements with ALT (Figure 1). Among synthetic logic corpora with different sample designs, \(_{ 2}\) yielded the largest performance gains, validating our proposed design principles. Moreover, we discovered that employing a knowledge-forgetting prevention method during ALT is critically important, as it likely prevents the LLM's knowledge of established facts from being displaced by the unknown facts included in synthetic logic corpora.

Finally, we analyze which task-solving capabilities ALT can enhance and why (Section 6). We observed a substantial improvement of up to 30 points on logical reasoning tasks (Table 3(a)). Surprisingly, we also observed improvements in abductive reasoning tasks, which go beyond the synthetic logic corpora's original deductive reasoning tasks. Case analyses indicate that these improvements result from LLMs having acquired the fundamentals of the logic reflected in the design principles. We also observed improvements of up to 10 points on math and coding tasks, indicating the generalizability of the obtained reasoning capabilities (Tables 3(b), 3(c)). We also observed improvements of up to 6 points on natural language inference (NLI) tasks (Table 3(d)). Case analyses suggest that LLMs successfully integrated the commonsense knowledge (they had originally acquired during pre-training with the logical reasoning capabilities newly acquired from ALT).

Improvements across various other tasks (Table 3(e)) demonstrate the broad benefits of the obtained reasoning capabilities beyond standard reasoning tasks, though the modest improvements of up to 2 points indicate the need for future research on more effective application of these capabilities.

Figure 1: The performance gains to LLaMA-3.1-70B by Additional Logic Training (ALT) on the proposed synthetic corpus, \(_{ 2}\) (Formal Logic _D_eduction _D_iverse). Each benchmark set, such as “Logic” and “Math”, comprises various benchmarks in that domain. Tables 2, 4 shows the details.

Our contributions are summarized as follows:

* We propose **A**dditional _Logic_**T**raining (ALT) and empirically verify that it can enhance the reasoning capability of state-of-the-art LLMs across various sizes, from 7B to 70B.
* We establish systematic design principles for synthetic logic samples; then, we construct a synthetic corpus named **Formal Logic _D_**eduction _D_**iverse** (\(_{ 2}\)), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. We empirically verify that Formal Logic Deduction Diverse indeed leads to the largest improvements among corpora with different sample designs.
* We demonstrate that LLMs enhanced by ALT can solve not only the original logical reasoning tasks present in synthetic logic corpora but also other tasks, such as math and coding tasks, and notably NLI tasks, which require integrating knowledge and reasoning. This finding underscores the potential for advancing truly versatile AI possessing both knowledge and reasoning capabilities.

We release the corpus, code, and the trained model under a permissive license 1.

## 2 How Should Synthetic Logic Samples Be Designed?

In synthetic generation, computer programs generate samples according to pre-designed patterns, so this design largely determines the quality of the samples. While Previous studies have examined several designs (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), these designs were not systematically discussed, so they may not be the most effective ones.

Thus, we start by discussing how to optimally design synthetic logic samples. To this end, we consider symbolic logic theory as suggested by Morishita et al. (2023) and integrate empirical findings from previous studies. First, we observe that the essence of logical reasoning, based solely on the logical relationships between facts, lies in its ability to handle unknown facts, unlike knowledge, which by definition deals solely with established facts (Section 2.1). Therefore, we argue that samples should cover reasoning with unknown facts to represent this essential aspect of logical reasoning. We also observe that logical reasoning involves various other aspects, such as _illogical_ reasoning, reasoning rules, and linguistic expressions that represent logical statements (sections 2.2 to 2.4). The samples should cover various patterns regarding these aspects to enable LLMs to solve various reasoning problems. We summarize these discussions into the following _design principles_, which guide the design of synthetic logic samples.

Figure 2: Our proposed **A**dditional **L**ogic **T**raining (ALT) aims to enhance LLMs’ reasoning capabilities through training on many synthetically generated logical reasoning samples. Our sample generator (left) first generates a sample of multi-step deductive reasoning and then converts it into a deduction sample written in English (right). LLMs must generate **l**ogical **s**teps to derive a given hypothesis from provided **f**acts. The sample generator adheres to theoretically and empirically grounded _design principles_ discussed in Section 2. Refer to Figure D.3 for a real sample.

### Teaching Reasoning with Unknown Facts

We first explore the essence of logical reasoning that differentiates itself from knowledge. Consider the following logical step:

\[\\ \] (1)

This step is valid because the conclusion is logically derived from the two premises. Next, consider another logical step:

\[\\ \] (2)

The second premise and consequently, the conclusion, is factually wrong. Nevertheless, _if the premise was hypothetically correct_, the conclusion could be logically derived. Therefore, step (2) is also logically valid. Finally:

\[\\ \] (3)

"Foo star" and "Bar star" are unknowns; nonetheless, we can still determine that step (3) is logically valid. Steps (1) to (3) above can be abstracted into a **deduction rule**, i.e., modus ponens, using symbols:

\[\\ \] (4)

As we have seen, the logical validity of a deduction rule depends solely on whether the conclusion is logically derived from the premises, not on the factual correctness of the contents of \(\) and \(\). Therefore, _the contents of \(\) and \(\) can be arbitrary._

Now, we consider what kind of samples would be needed to teach the deduction rule (4) to LLMs. We assume a task to generate the conclusion given the premises as prompt inputs. If the learner were human, they would be able to infer the underlying deduction rule (4) by observing samples such as (1) to (2). As a result, they would become able to solve the unknown problem (3).

However, from a purely inductive perspective, samples (1) to (2) cannot simply be generalized to the deduction rule (4). This is because the samples (1) to (2) themselves do not contain the information that the contents of \(\) and \(\) are arbitrary. In fact, one could generalize samples (1) to (2) to other rules; for example, the conclusion \(\) can be derived if \(\) and \(\) are given as premises _and_\(\) and \(\) include 'Earth' as their contents. Innumerable such deduction rules can be inductively inferred from the given samples. In other words, induction has arbitrariness (Hume, 1748; Goodman, 1954; Quine, 1969).

Humans prefer simpler rules (Bertrand; Wittgenstein, 1922), so they boldly induce up to the deduction rule (4). However, it is unclear how purely inductive learners such as LLMs, which extract only what can be inferred from samples without prior preferences, induce up to (4). For example, if only specific contents such as "Alice is kind" and "Bob is smart" are assigned to \(\) and \(\) in training samples, an LLM could develop into a machine that generates the conclusion \(\) only when the input contains the specific contents. In order for LLMs to accurately induce that \(\) and \(\) are indeed arbitrary:

**Design Principle 1** (Reasoning with Unknown Facts).: _Prepare many samples assigning arbitrary contents to \(\) and \(\). They will make LLMs accurately induce \(\) and \(\) are indeed arbitrary, ultimately enabling them to reason with unknown facts._

### Teaching Illogical Reasoning

Suppose we have LLMs trained on a large number of samples as follows:

\[( )\\ \] (5)

where \(\) denotes logical conjunction, and arbitrary contents are assigned to \(,,\). Suppose that we give this LLM a problem such as:

\[() \\ \] (6)

Since the premises are insufficient for logically deducting the conclusion, outputting nothing is the correct answer.

Unfortunately, an LLM could output \(\), which was indeed often observed in our preliminary experiments. This is because while the LLMs can induce from sample (5) that it can generate the conclusion \(\) when the two premises of (5) are given, the LLMs cannot induce from the sample that it is _not allowed_ to generate the conclusion \(\) when the premises of (6) are given, as such information is not included in the sample (5) itself. Therefore,

**Design Principle 2** (Illogical Reasoning).: _Include negative samples such as (6). These samples will make LLMs induce that conclusions cannot be derived from insufficient premises._

### Teaching Diverse Reasoning Rules

Deduction rules other than (4) exist:

\[)}{}& )}{}& )()}{}\\ }{ }&)}{ }\] (7)

where \(\) denotes logical disjunction and \(\) negation. Since there are infinitely many possible logical formulas that can appear as premises and conclusions, there are infinitely many deduction rules. Providing LLMs with these infinite deduction rules is obviously intractable.

Instead of directly providing these infinite deduction rules, we can take another approach. Consider multi-step deductive reasoning (Figure 2 left), where multiple deduction rules derive a conclusion. Notice that the syllogism in (7) can be expressed by multi-step deductive reasoning using more "atomic" deduction rules. Indeed, there exists a set of atomic deduction rules called **the axioms** that satisfies the following:

**Theorem 2.1** (Completeness of First-Order Predicate Logic Godel (1930)).: _Any valid deduction rule can be expressed by multistep deductive reasoning constructed from the axioms._

In contrast to the axioms, the 'compound' deduction rules, such as syllogism, contraposition, and De Morgan's laws, are called theorems. According to the completeness Theorem 2.1, if we can handle the axioms, we can effectively handle other deduction rules as well. Indeed, Morishita et al. (2023) empirically verified that a language model trained on the axioms generalizes to handle other deduction rules more effectively than those trained on non-axiom deduction rules. Therefore,

**Design Principle 3** (Diverse Reasoning Rules).: _Samples should express multi-step deduction constructed from the axioms. They will effectively teach LLMs diverse deduction rules (Morishita et al., 2023; Morishita et al., 2023)_

In multi-step deductive reasoning, the number of logical steps \(s\) from premises to a conclusion can vary largely depending on the problem. Therefore:

**Design Principle 3'** (Diverse Reasoning Rules).: _Samples should include diverse numbers of logical steps \(s\)._

Ideally, this would be sufficient, but empirical evidence has shown that LLMs struggle with constructing multi-step deductive reasoning with large steps \(s\)(Gontier et al., 2020; Morishita et al., 2023). Consequently, LLMs would not excel at handling theorems that require a large number of steps \(s\) when expressed by the axioms. Therefore, as an additional countermeasure:

**Design Principle 3"** (Diverse Reasoning Rules).: _Samples should also include representative theorems, such as syllogism, contraposition, and De Morgan's laws._

### Teaching Diverse Linguistic Expressions that Represent Logical Statements

There are various linguistic structures for expressing the logical relationship \(\), such as "If \(\) then \(\)", "\(\) leads to \(\)", and "\(\) results in \(\)". If we only include specific expressions in the corpora, LLMs may only learn to react to these specific expressions, which has been observed in previous experiments (Zhang et al., 2022; Yuan et al., 2023). To prevent this,

**Design Principle 4** (Diverse Linguistic Expressions).: _Samples should include diverse linguistic expressions that represent logical statements._

In this chapter, we have established the principles to guide the design of synthetic logic samples. Next, we construct a synthetic logic corpus based on these principles.

## 3 Creating a Synthetic Corpus based on Design Principles

To prepare diverse samples reflecting the design principles 1 to 4 (DP1-4), we built a novel sample generator by extending the previous one by Morishita et al. (2023) and then generated the synthetic logic corpus named \(_{ 2}\) (Formal Logic _D_eduction Diverse). Figure 2 shows a schematic of our generator and a deduction sample. Table 1 compares \(_{ 2}\) with existing corpora. Figure D.3 provides an actual deduction sample included in \(_{ 2}\).

More specifically, our generator generates deduction samples through the following steps. First, the generator randomly generates a sample of multi-step deductive reasoning written in logical formulas, as shown on the left side of Figure 2, where a conclusion is derived from premises using multiple deduction rules (See Appendix D.3 for more details of this generation procedure). At this time, the generator also generates 'distractor' logical formulas, which express negative premises of DP2. Next, the generator converts each logical formula into English expressions. To achieve this, the generator first randomly selects a template from pre-defined options, such as "If \(\), then \(\)," "\(\) leads to \(\)," or "\(\) results in \(\)," for the logical formula "\(\)." It then assigns English content randomly constructed from a vocabulary, such as "(that) a Foo star exists" and "(that) a Bar star exists," to each symbol, such as \(\) and \(\). Finally, it converts the multi-step deduction into a deduction sample (right side of Figure 2) by using the premises as "facts", the conclusion as 'hypothesis', and the intermediate logical steps as "logical steps'. The deduction sample requires LLMs to generate logical steps that derive a given hypothesis based on the given facts.

Table 1 outlines the comparison of \(_{ 2}\) with other existing corpora (Clark et al., 2021; Bao et al., 2022; Morishita et al., 2023) in terms of DP1-4, which is detailed as follows:

* DP1: We assign \(\) and \(\) content randomly constructed from a vocabulary. While the existing corpora used small-sized vocabulary of up to 15k, we use a large vocabulary of around 100k words built from WordNet (Miller, 1995). This will teach LLMs that \(\) and \(\) are truly arbitrary, ultimately enabling them to reason with unknown facts.
* DP2: The existing corpora used randomly generated logical formulas as distractors. In contrast, we implement adversarial distractors. For example, for a premise \(\), we use \(\) with missing information (see Equations (5), (6)), and for a premise \(\), we use \(\) with missing information as distractors. These distractors teach LLMs precisely when a conclusion can and cannot be derived. As with previous corpora, we include a variable number of distractors in each sample, randomly chosen from a range of 0 to 20.
* DP3-3": While the existing corpora used a small number of deduction rules of up to 13 (refer to Figure B.4 of Morishita et al. (2023)), we include diverse deduction rules, encompassing the axioms and representative theorems, such as modus ponens, syllogisms, and contraposition, totaling about 50 rules. We include samples with up to \(s=8\) logical steps, following (Morishita et al., 2023).
* DP4: We manually craft several more English templates _per_ logical formulas than those used in FLD. Since the templates have a nested structure, they yield combinatorially more diverse English expressions. While counting the exact number of the resulting expressions is intractable, we observed at least dozens of expressions per logical formula, including minor variations. See Appendix D.4 for details.

    & DP1 & DP2 & DP3 & DP4 \\   & vocabulary size & distractors & deduction rules & logical steps & expressions per formula \\  RuleTaker (Cark et al., 2021) & \(\) 100 & random & 2 &  & (1)\)} \\ (RT) & (hand-selected) & formula & (mglglistication) & & \\  PARARULE-Plus (Bao et al., 2022) & \(\) 100 & random & 2 &  & (1)\)} \\ (PRP) & (hand-selected) & formula & (mglistication) & & \\  FLD (Morishita et al., 2023) & \(\) 15k & random & 13 &  &  \\  & (WoodNet, subset) & formula & (actions) & & \\  _{ 2}\)} &  & **adversarial** & \(\) **50** &  & **10\(\)100** \\  & (**WorNet, full)** & **formula** & **(axoplasm and theorems)** & & \\  \(_{ 2}\) & 100 & not used & 2 (mglistication) & 1 &  \\ _ablation_ corpora \(\)\(\)\(\)_w/o DP1_ & \(\)_w/o DP2_ & \(\)_w/o DP3.rules_ & \(\)_w/o DP4_ \\   

Table 1: Synthetic logic corpora compared in this study, with their features categorized according to our proposed design principles (DP). Note that the last row of the _ablation_ corpora lists variations of \(_{ 2}\), each of which differs from the original regarding one of the design principles.

Experimental Setup

We briefly explain the experimental settings. Refer to Appendix E for the details.

**Synthetic Logic Corpora:** We examine the proposed \(_{ 2}\) and previous corpora (Table 1).

**LLMs:** We used the state-of-the-art LLM, LLMaMA-3.1 (8B and 70B) (AI@Meta, 2024).

**Training Settings:** We trained the LLMs by a method similar to supervised fine-tuning; as illustrated in Figure 2, we used the facts and hypothesis as inputs and logical steps and additional answer label (see Appendix D.1) as outputs. We excluded loss computation for the inputs to prevent LLMs from learning to generate unknown facts. We trained the LLMs for 1 epoch on 100k samples (\( 0.1\)B tokens) from the training split of each corpus, with a batch size of 256, resulting in 390 steps, with a linear warmup for 200 steps. We used the learning rate of 2e-05 for the 8B model and 3e-06 for the 70B model. We used Huggingface (Wolf et al., 2020) for implementation.

**Prevention of Knowledge Forgetting by Recall Adam Optimizer:** Synthetic logic corpora include many samples with unknown facts, so training on them should cause LLMs to forget their knowledge of existing facts. To prevent this, we employed the Recall Adam optimizer (Chen et al., 2020), which regularizes parameter updates to avoid deviating too far from the pre-training parameters. Recall Adam stands out for LLM training for several reasons (see Appendix E.0.1 for details). We used our re-implemented version 2. The hyperparameters were: \(_{1}=0.9,_{2}=0.999,=10^{-6}\), fisher coefficient \(=4000\) for the 8B model and \(2000\) for the 70B model.

**Benchmarks:** We evaluated the trained LLMs on 31 benchmarks shown in Table E.7 using 5-shot in-context learning, except for BBH and AbuductionRules, which used 3-shot in-context learning. These benchmarks cover a wide range of tasks and are prominent in LLM evaluation. Note that we excluded the synthetic logic corpora used for training, as training on them often leads to overfitting to their superficial and statistical cues (Zhang et al., 2022; Yuan et al., 2023), failing to measure truly generalizable reasoning capabilities. We used lm-evaluation-harness (Gao et al., 2023) and bigcode-evaluation-harness (Ben Allal et al., 2022) for the implementation.

## 5 Can Additional Logic Training Enhance LLMs' Capabilities?

Table 2 show the performance of LLMs before and after ALT. Most LLMs trained with ALT outperformed their counterparts without ALT. Notably, ALT yielded substantial gains of up to 10 points even for LLMa-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens. These results verify that ALT can enhance the capabilities of state-of-the-art LLMs.

Among the LLMs trained with ALT, the one trained on \(_{ 2}\) (i.e., \(\)**ALT**-\(_{ 2}\)) achieved the highest generalization performance across the benchmarks. Table 3 shows the performance of the LLMs trained on _ablated_\(_{ 2}\) corpora, each of which lacks one of the design principles. As seen, ablating any design principle almost always led to performance degradation. These results demonstrate that the proposed design principles are critical to obtaining the maximum possible gain from ALT, and each principle is indispensable.

Table F.8 shows that the LLMs trained with ALT without preventing knowledge forgetting by Recall Adam optimizer underperformed compared to their counterparts trained with knowledge forgetting prevention and even the LLM without ALT. This behavior presumably occurred because the unknown facts included in synthetic logic corpora displaced the LLM's knowledge of existing facts. Therefore, knowledge-forgetting prevention is critically important for the success of ALT.

## 6 What Capabilities Can Additional Logic Training Enhance and Why?

We analyze the results on each benchmark or each case and discuss whether and why the LLM's capabilities to solve the tasks can or cannot be enhanced by ALT.

### Logical Reasoning Tasks

Table (a)a shows that ALT substantially boosted LLMa-3.1-70B's performance by up to 30 points on various benchmarks dealing with logical reasoning tasks. Surprisingly, we also observed improvements on abductive reasoning tasks, which go beyond the original deductive reasoning tasks

in synthetic logic corpora. Abductive reasoning involves guessing the missing premises that caused the observed conclusion rather than deriving a conclusion from the premises. For example, from the observed conclusion, "the window glass at home was broken and the room was ransacked," we guess the premise "a burglar broke in." The improvements would be due to the fact that, while the surface form of abductive reasoning problems differs from that of deductive reasoning, they share the fundamentals of logic reflected in the design principles.

Next, we conduct case analyses to see whether the LLM enhanced by ALT acquired the abilities intended by the proposed design principles (DP1-4). Table 5 shows problems where LLaMA-3.1-70B's errors have been corrected by ALT. The first problem is very simple, so it is surprising that LLaMA-3.1-70B failed to solve it, indicating the inherent difficulty of learning logical reasoning solely from pre-training. In contrast, \(\)ALT-FLD\({}_{ 2}\), which was additionally trained on FLD\({}_{ 2}\), solved the problem correctly. The premises of the problem are randomly constructed to express unknown facts. Therefore, the result suggests that \(\)ALT-FLD\({}_{ 2}\) acquired genuine logical reasoning ability, which can handle unknown facts (DP1).

In the second problem, \(\)ALT-FLD\({}_{ 2}\) correctly answered "neutral", indicating that it successfully learned that conclusions cannot be derived from insufficient facts (DP2).

The third problem comes from the FOLIO benchmark. To solve this problem, LLMs must use syllogism at the first step as follows: "All eels are fish, and no fish are plants. Therefore, all cells are not plants." \(\)ALT-FLD\({}_{ 2}\) answered this problem correctly, suggesting that it successfully learned diverse deduction rules (DP3).

FOLIO problems are created based on Wikipedia topics, describing them in more natural and realistic linguistic expressions than in other benchmarks. As seen in the fourth problem, \(\)ALT-FLD\({}_{ 2}\) understands such expressions, suggesting the effect of diverse expressions from DP4 and/or that LLMs can integrate their original linguistic ability with the newly acquired logical reasoning ability.

    & Avg. & Logic & Math & Code & NLI & Others & BBH (3-shot) & BBH (0-shot) &  \\   & & & & & & & & CoT & & CoT & & Pro \\  LLAM-3.1-18B & 47.9 & 42.8\({}_{1.44}\) & 39.6\({}_{0.05}\) & 35.4 & 65.4\({}_{4.00}\) & 60.7\({}_{.43}\) & 44.9\({}_{0.00}\) & 61.9\({}_{0.05}\) & 82.2\({}_{1.33}\) & 36.5\({}_{0.00}\) & 65.3\({}_{0.00}\) & 35.8\({}_{0.00}\) \\ \(\)ALT-PRP & 48.1 & 43.7\({}_{1.42}\) & 39.2\({}_{1.23}\) & 35.7 & 65.6\({}_{0.00}\) & 60.8\({}_{0.00}\) & 44.9\({}_{0.00}\) & 61.8\({}_{1.44}\) & 82.1\({}_{1.00}\) & 36.4\({}_{0.00}\) & 65.3\({}_{0.00}\) & 35.3\({}_{0.00}\) \\ \(\)ALT-RT & 50.1 & 46.8\({}_{1.44}\) & 42.4\({}_{1.00}\) & 36.5 & 65.6\({}_{0.00}\) & 61.3\({}_{1.44}\) & 46.9\({}_{0.00}\) & 63.5\({}_{1.44}\) & 19.3\({}_{0.00}\) & 65.3\({}_{0.00}\) & 35.7\({}_{0.00}\) \\ \(\)ALT-FLD & 51.9 & 51.6\({}_{1.44}\) & 43.4\({}_{0.00}\) & 38.1 & 70.1\({}_{1.00}\) & **15.5\({}_{1.44}\)** & 46.7\({}_{0.00}\) & 64.9\({}_{0.00}\) & **11.9\({}_{0.00}\)** & 65.4\({}_{1.44}\) & 36.2\({}_{0.00}\) \\ \(\)ALT-FLD\({}_{ 2}\) & 52.0 & 52.2\({}_{2.44}\) & 43.2\({}_{0.00}\) & 38.0 & **70.7\({}_{0.00}\)** & **61.5\({}_{0.00}\)** & 46.5\({}_{0.00}\) & **11.3\({}_{0.00}\)** & 38.7\({}_{0.00}\) & **65.5\({}_{0.00}\)** & **36.4\({}_{0.00}\)** \\    
    & Avg. & Logic & Math & Code & NLI & Others & BBH (3-shot) & BBH (0-shot) &  \\   & & & & & & CoT & & CoT & & Pro \\  LLAM-3.1-70B & 60.0 & 57.4\({}_{0.00}\) & 46.2 & 73.7\({}_{1.00}\) & 67.7\({}_{1.00}\) & 60.4\({}_{0.00}\) & 82.1\({}_{1.2}\) & 65.5\({}_{0.00}\) & 50.1\({}_{1.2}\) & 78.7\({}_{1.00}\) & 50.7\({}_{1.2}\) \\ \(\)ALT-PRP & 60.4 & 57.7\({}_{1.00}\) & 59.8\({}_{1.44}\) & 49.2 & 73.5\({}_{1.4}\) & 67.6\({}_{0.00}\) & 60.4\({}_{0.00}\) & 82.2\({}_{1.2}\) & 60.0\({}_{0.00}\) & 50.1\({}_{1.4}\) & 78.7\({}_{1.00}\) & 50.9\({}_{1.44}\) \\ \(\)ALT-RT & 62.7 & 61.4\({}_{0.00}\) & 62.1\({}_{1.50}\) & 50.8 & **75.4\({}_{0.00}\)** & 68.4\({}_{0.00}\) & 64.1\({}_{1.44}\) & 82.5\({}_{1.5}\) & **11.5\({}_{0.00}\)** & 59.2\({}_{1.4}\) & 79.0\({}_{1.00}\) & 52.4\({}_{1.33}\) \\ \(\)ALT-FLD & 64.2 & 65.7\({}_{0.00}\) & 63.6\({}_{0.00}\) & 52.0 & 75.5\({}_{0.00}\) & 63.5\({}_{0.00}\) & 83.6\({}_{0.00}\) & **83.6\({}_{0.00}\)** & **12.1\({}_{1.4}\)** & 59.9\({}_{1.4}\) & 59.9\({}_{1.4}\) & 54.4\({}_{1.2}\) \\ \(\)ALT-FLD\({}_{ 2}\) & 64.4 & 66.0\({}_{0.00}\) & 63.3\({}_{0.00}\) & 52.4 & **76.1\({}_{0.00}\)** & **68.5\({}_{0.00}\)** & **65.4\({}_{0.00}\)** & **83.6\({}_{0.00}\)** & **11.4\({}_{0.00}\)** & **60.8\({}_{0.

### Math and Coding Tasks

Tables 3(b), 3(c) shows that ALT substantially boosted the LLaMA-3.1-70B's performance by up to 7 and 10 points on math and coding tasks, respectively. The math improvements are reasonable, as understanding predicate logic is a prerequisite for solving mathematical problems. For coding, some recent studies have verified the opposite direction, namely, that training on coding data improves logical reasoning abilities (Jiang et al., 2024; MA et al., 2024; Uchiyama et al., 2024).

### NLI Tasks

Table 3(d) shows that ALT substantially boosted the LLaMA-3.1-70B's performance by up to 6 points on various natural language inference (NLI) benchmarks. NLI is similar to deductive reasoning in assessing whether a premise supports or contradicts a hypothesis. However, the main difference is that this judgment requires a rich set of commonsense knowledge beyond the given premise.

Consider the fifth problem in Table 5: by supplementing the given fact "An Indian woman is dancing with her partner" with the commonsense knowledge "If someone is dancing, then he/she is moving.", we can derive the hypothesis "A woman is moving." The sixth problem is more challenging as we have to trace multiple logical steps while supplementing with sufficient commonsense knowledge as follows: "a church choir sings at a church," "baseball is often played at a baseball field," "a person cannot be in two or more places at the same time," "therefore, a church choir cannot sing for baseball."

Since synthetic logic corpora only contain unknown facts, LLMs cannot acquire new knowledge from them. Therefore, the commonsense knowledge used to solve the above problems must have been acquired by the LLMs from pre-training. This suggests that LLMs can integrate their original knowledge with the logical reasoning capabilities newly acquired from ALT to solve problems.

Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and **after** ALT on \(_{ 2}\). Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.

### Other Tasks

Improvements across various other tasks (Table 4) demonstrate the broad benefits of the obtained reasoning capabilities beyond standard reasoning tasks; though the improvements were modest at up to 2 percentage points, which may be due to the following reasons. First, these benchmarks include problems that purely test knowledge, such as the first one in Table 6. Since ALT does not aim to provide new knowledge, the ability to solve such problems does not improve by nature. Next, some problems may require knowledge that is too advanced for LLMs, so potential improvements by the enhanced reasoning capabilities may be bottlenecked. For example, the second problem does involve reasoning but requires sufficient quantum mechanics knowledge as a prerequisite. However, these knowledge-related issues should be solved by improving the quantity and quality of pre-training.

Finally, LLMs may not be able to fully utilize the potential of enhanced reasoning capabilities for problems that require complex procedures. To solve the third problem, LLMs first must attempt reasoning related to each choice as follows: "To build homes in an aquatic environment, one needs to maintain body heat and insulation despite being frequently submerged in cold water. Therefore, the waterproof fur of (A) is essential", and "To build..., one must gather and process natural materials like wood. Large, sharp teeth of (C) are critical as they allow beovers to cut down trees and shape branches." Next, while reasoning traces on (A) to (D) all seem reasonable, LLMs must choose the single best answer, considering the subtle nuance of the question context, as follows: "Since the question emphasizes the aquatic environment, the least related reasoning trace should be (C)." This complex procedure contrasts with logical reasoning and NLI problems, where LLMs can directly obtain an answer from a single reasoning trace. Previous studies also observed that such procedure on multiple-choice QA problems are challenging for LLMs (Robinson and Wingate, 2023; Zheng et al., 2024; Wang et al., 2024). Since ALT alone does not teach LLMs such task-specific procedures, additional training on these procedures should be necessary to solve these problems.

## 7 Conclusion

Towards versatile artificial intelligence with reasoning capabilities, we proposed **A**dditional **L**_ogic_**T**raining on synthetic logic samples. We established systematic design principles well-grounded on symbolic logic theory and previous empirical findings. We constructed a corpus named Formal Logic Deduction Diverse (FLD\({}_{ 2}\)) based on the design principles. We empirically showed that ALT on FLD\({}_{ 2}\) substantially enhances the capabilities of state-of-the-art LLMs.

 p{142.3pt} p{142.3pt}}   benchmark & question & answer & \\  ARC (challenge) & The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis? & Chlorophyll in the leaf captures light energy. & \\  GPQA & A spin-half particle is in a linear superposition \(0.8|+0.6|\) of its spin-up and spin-down states. If \(|\) and \(|\) are the eigenstates of \(_{z}\), then what & \(-0.7\) & \\  & is the expectation value up to one decimal place, of the operator \(10_{z}+5_{z}\)? & & \\  ARC (challenge) & Beavers build their homes in ponds and streams. Which characteristic & (A) waterproof fur (B) webbed hind feet is least critical to building homes in an aquatic environment? & (A) waterproof fur (C) **arge, sharp teeth** (D) flat, wide tail \\   

Table 6: Problems that LLaMA-3.1-70B trained with ALT on FLD\({}_{ 2}\) still cannot solve.

 p{142.3pt} p{142.3pt} p{142.3pt}}   benchmark & premises &  snowver \\ (LLaMA-3.1-70B/gold) \\  } & }{
 required \\ ability \\  } \\  LogicNLI & Mice are afraid of wobers. Cats are afraid of sleep. & Jessica is a cat. Waves are afraid of cats. & Jessica is afraid of sheep. & neutral/ entailment & DP1 \\  & Winton is a wolf. Sleep are afraid of cats. & & Rheit is & entailment/ \\  & Rheit is not modest. Vivian is confused. & Rheit is & entalment/ confused. & & DP2 \\  & Rheit is lazy. If someone is modest or not confused, then he is not eager. & & & & \\  FOLIO & All else are fish. No fish are plants. & & & & \\  & Everything displayed in the collection is either a plant or an animal. & & & & \\  & All animals displayed in the collection are multicellular or is bacteria. & & & & \\  & A sea eel is displayed in the collection. & & & & \\  & The sea eel is an eel or an animal or not a plant. & & & & \\  & Common utilities include water, electricity, gas, heating, sewer, traah, and recycling. & & & & \\  & Many apartment reess cover the cost of water and electricity. & & & & \\  & Susan lives in an apartment where the rent covers all utilities. & & & & \\  & The rent of the apartment where A fux lives does not carry any utility expenses. & & & & \\  & Noah lives in an apartment where the rent does not cover heating. & & & & \\  & & & & \\  SNLI & An Indian woman is dancing with her partner. & A woman is moving. & & & & \\  & This church cheir sing to the masses & & & & \\  & as they sing joyvons songs from the book at a church. & & & & \\   

Table 5: Problems where LLaMA-3.1-70B initially answered incorrectly and then correctly after training with ALT on FLD\({}_{ 2}\). Red highlights the premises related to the hypothesis.