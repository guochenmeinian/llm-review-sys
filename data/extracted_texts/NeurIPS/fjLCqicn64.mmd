# Long-range Brain Graph Transformer

Shuo Yu\({}^{1,2}\), Shan Jin\({}^{3}\), Ming Li\({}^{4,5}\), Tabinda Sarwar\({}^{6}\), Feng Xia\({}^{6}\)

\({}^{1}\)School of Computer Science and Technology, Dalian University of Technology, China

\({}^{2}\)Key Laboratory of Social Computing and Cognitive Intelligence (Dalian University of Technology), Ministry of Education, China

\({}^{3}\)School of Software, Dalian University of Technology, China

\({}^{4}\)Zhejiang Institute of Optoelectronics, China

\({}^{5}\)Zhejiang Key Laboratory of Intelligent Education Technology and Application, Zhejiang Normal University, China

\({}^{6}\)School of Computing Technologies, RMIT University, Australia

{shuo.yu, f.xia}@ieee.org

jinshan0924@mail.dlut.edu.cn

mingli@zjnu.edu.cn

tabinda.sarwar@rmit.edu.au

###### Abstract

Understanding communication and information processing among brain regions of interest (ROIs) is highly dependent on long-range connectivity, which plays a crucial role in facilitating diverse functional neural integration across the entire brain. However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication. To address this limitation, we propose **A**daptive **L**ong-range aware **T**ransform**ER** (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk. Specifically, we present a novel long-range aware strategy to explicitly capture long-range dependencies between brain ROIs. By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication. Furthermore, by employing the transformer framework, ALERT adaptively integrates both short- and long-range dependencies between brain ROIs, enabling an integrated understanding of multilevel communication across the entire brain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis. Cases of long-range dependencies are also presented to further illustrate the effectiveness of ALTER. The implementation is available at [https://github.com/yushuowiki/ALTER](https://github.com/yushuowiki/ALTER).

## 1 Introduction

Brain networks represent a blueprint of communication and information processing across different regions of interest (ROIs) . The interaction between anatomically connected ROIs within brain networks is the foundation of brain network analysis tasks . As shown in Figure 1, numerous studies have shown that brain networks exhibit not only short-range connectivity (i.e., short-range dependencies) but also extensive long-range connectivity (i.e., long-range dependencies) . Short-range dependencies rely on the neighbourhood space, whereas long-range dependenciesreflect long distance communication among ROIs. Such long-range dependencies play a vital role in theoretical analyses of brain function , dysfunction , organization , dynamics , and evolution . Therefore, it is necessary to capture long-range dependencies within brain networks to better represent communication connectivity and facilitate brain network analysis tasks to extract valuable insights. The communication connectivity is also known as functional connectivity, representing the interaction between brain ROIs.

Several existing studies have been devoted to representing communication connectivity within brain networks via graph learning methods for network analysis tasks [12; 13; 14; 15]. However, as previously mentioned, they generally focus on the aggregation of neighborhood information, i.e., short-range dependencies, which still limits their effectiveness by neglecting the crucial long-range dependencies. Most of the studies build models to analyze node (ROIs) features and structures (inter-ROI connectivity) using Graph Neural Networks (GNNs) with message-passing mechanism. The limited expressiveness of GNNs fails to capture the long-range dependencies in brain networks. While the group-based graph pooling operations cluster ROIs, these are still limited to regional similarities and are not sufficient to represent long-range communication connectivities.

To address this limitation of solely considering the short-range dependencies, we aim to develop a solution that leverages long-range dependencies to enhance brain network analysis tasks. Currently, the capturing the long-range dependencies has been addressed in different network analysis tasks outside the scope of brain studies, such as those related to social networks and molecular networks [16; 17; 18; 19]. Among these, random walk methods are widely adopted, as they can explicitly capture long-range dependencies by aggregating structure information across the entire random walk sequence [20; 21; 22; 23]. In conventional random walk methods, the transition probability from a node to one of its neighbors is typically uniform. However, several studies have observed that different pairs of ROIs typically demonstrate varying communication strengths in brain activity, where stronger communication indicates greater dependencies among ROIs [24; 25; 26]. Therefore, employing conventional random walk methods to sample the next hop with uniform probability renders it impossible to capture the long-range dependencies within brain networks.

Based on the above observation, we are dedicated to capturing long-range dependencies in brain networks under the varying communication strengths among ROIs. In this paper, we propose **A**daptive **L**ong-range aware **T**ransform**ER** (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs extracted from biased random walk. Specifically, in order to capture long-range dependencies within brain networks, we firstly present an **A**daptive **L**ong-**ran**Ge **A**ware (ALGA) strategy based on random walk in Section 3.1, which explicitly samples random walk sequences based on varying communication strengths among ROIs. In this strategy, we initially calculate the inter-ROI correlations as adaptive factors to evaluate their communication strengths. Subsequently, the use of random walk is biased, subject to the next hop with a higher correlation value, thus explicitly encoding long-range dependencies as long-range embeddings through random walk sampling. Furthermore, given the significance of both short-range and long-range dependencies in brain network analysis tasks, we introduce an effective brain graph transformer in Section 3.2, which can capture different levels of communication connectivities in human brains. Specifically, we inject the long-range embeddings into a transformer framework and integrate both short-range and long-range dependencies between ROIs using the self-attention mechanism.

The contributions of the paper are summarized as follows: 1) pioneering the explicit emphasis on the significance and challenges of capturing long-range dependencies in brain network analysis tasks, we propose a novel solution for capturing long-range dependencies within brain networks; 2) to address the limitations of previous studies that overlook long-range dependencies within brain networks, we introduce a novel brain graph transformer with adaptive long-range awareness, which leverages the communication strengths between ROIs to guide the capturing of long-range dependencies, enabling an integrated understanding of multi-level communication across the entire brain; 3) extensive

Figure 1: An illustration of long-range dependencies and short-range dependencies within human brain.

experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized graph learning methods and other graph learning-based brain network analysis methods.

## 2 Related Work

### Brain Network Analysis

Several studies have developed graph learning-based methods for brain network analysis tasks, such as neurological disease diagnosis and biological sex prediction [12; 14; 15; 27; 28]. The majority of studies have utilized GNNs to learn the information of ROIs and inter-ROI connectivity. For instance, Li et al.  utilized GNNs with ROI-aware and ROI-selection to perform community detection while retaining critical nodes. Kan et al.  dynamically optimized a learnable brain network. Additionally, a few models based on specific graph pooling operations were also proposed to retain the communication information of brain networks. Specifically, Yan et al.  designed group-based graph pooling operations to enable explainable brain network analysis. Kan et al.  considered the similarity property among brain ROIs and designed a graph pooling function based on clustering. However, these approaches are generally limited to the aggregation of neighborhood information, while neglecting the long-range connectivity that plays a key role in brain network analysis tasks .

### Graph Transformer

Several existing studies have focused on developing the transformer variants for graph representation learning [29; 30; 31; 32; 33]. Transformers have demonstrated competitive or even superior performance over GNNs. Dwivedi et al.  were the first to extend the transformer to graphs, defining the eigenvectors as positional embeddings. Kreuzer et al.  improved the positional embeddings and enhanced the transformer model by learning from the full Laplacian spectrum. Ying et al.  embedded the structural information of graph into a transformer, yielding effective results. Moreover, some studies have applied transformers to address unique issues in general graphs or domain-specific graphs. Wu et al.  utilized global attention to capture long-range dependencies within general graphs. Tao et al.  employed the transformer model to integrate both temporal and spatial information in social networks for disease detection.

## 3 Method

Within the brain graph, the collection of brain ROIs serves as the node set, and the features of these ROIs serve as the node features. The connectivity among brain ROIs is generally represented by the adjacency matrix. In this paper, we focus on analyzing these brain graphs for neurological disease diagnosis. Formally, consider a set of subjects' brain network \(\{G_{1} G_{L}\}\) and their disease state labels \(\{y_{1} y_{L}\} Y\), where \(L\) is the total number of individuals (size of the dataset). Each brain graph \(G\) contains \(N\) ROIs, defined as \(G=(V,X,A)\), where \(V\) is node set, \(X^{N d}\) are node features with dimension \(d\), and \(A^{N N}\) is an adjacency matrix. We aim to learn a representation vector \(h_{G}\) that will allow us to predict the disease state of brain graph \(G\), i.e., \(y_{G}=f(h_{G})\) where \(f\) is prediction function. Notably, the proposed method can also deal with other brain network analysis tasks such as biological sex prediction.

The overall framework of ALTER is illustrated in Figure 2. Briefly, we first extract the node features \(X_{G}\) and adjacency matrix \(A_{G}\) from the fMRI data. Subsequently, adaptive factors \(F_{G}\) are calculated using the temporal features of the fMRI data. Next, using the adaptive factors \(F_{G}\) and the adjacency matrix \(A_{G}\), the long-range embedding \(E_{G}\) representing the long-range dependencies is obtained through adaptive long-range encoding. The encoding is utilized by our **A**daptive **L**ong-ran**Ge** Aware (ALGA) strategy to explicitly encapsulate the long-range dependencies among ROIs as long-range embedding \(E_{G}\). Finally, the long-range embedding \(E_{G}\) is injected into the self-attention module, and a graph-level representation of the brain network is generated using the readout function to the downstream tasks. The complete training process is supervised by the cross-entropy loss.

Detailed description of these steps are discussed in the upcoming sections.

### Adaptive Long-range Aware (ALGA) Strategy

As previously mentioned, previous studies primarily focus on aggregating information from neighboring ROIs, generally neglecting their long-range connectivity. In light of this, our goal is to design an efficient strategy to capture long-range dependencies among the ROIs. We now explain how the adaptive long-range aware strategy achieves this through the computation of adaptive factors and the adaptive long-range encoding.

#### 3.1.1 Adaptive Factors

The correlation between ROIs reflects their communication strength within specific time frames, which is crucial for comprehending the functional organization, dysfunctions, and information propagation in the brain . Neuroscientific investigations have unveiled the existence of phase synchrony in neural oscillations among distinct ROIs, closely associated with the occurrence of perceptual motor behaviors and the integration of brain functional organization . The degree of phase synchrony indicates the strength of connectivity between ROIs, whereby higher phase synchrony signifies stronger communication and consequently higher correlation values among the respective ROIs. This manner simulates real-world brain-wide communication.

Considering the above observation, we first calculate the correlation between ROIs as adaptive factors \(F_{G}^{N N}\) to evaluate the communication strengths between ROIs. Specifically, the adaptive factor \(f_{ij}\), denoting the communication strength between node \(i\) and node \(j\) within the brain graph \(G\), is defined as:

\[f_{ij}=(t_{i},t_{j})}{(t_{i})(t_{j})}, &$ and $v_{j}$ are connected},\\ 1,&,\\ 0,&, \]

where \(t_{*}\) denotes the raw feature of the brain ROI \(v_{*}\) (e.g., temporal feature of fMRI data). \(()\) and \(()\) denote covariance and variance operations, respectively. The adaptive factors \(F_{G}\) will influence the exploration mechanism of the random walk in the adaptive long-range encoding (Section 3.1.2). Specifically, ROIs with stronger connectivities exhibit higher transfer probabilities of the next hop compared to ROIs with weaker connectivities in the random walk. Note that for simplicity, we choose Pearson correlation coefficient to define the adaptive factors, without any modification.

#### 3.1.2 Adaptive Long-range Encoding

The adaptive factors between ROIs, computed in the preceding step, are crucial for effectively capturing long-range dependencies between brain ROIs. Motivated by the random walk methods , we approach the problem of capturing long-range dependencies as a structural encoding task. By sampling and encoding node sequences into embeddings, we effectively capture long-range

Figure 2: The overall framework of the proposed ALTER.

communication between brain ROIs. Hence, we compute a long-range embedding using the walker sampling of node sequences under the constraint of adaptive factors.

In a network, a random walk involves transitioning from one node to another. Specifically, when a walker moves from node \(i\), the probability of the walker moving to node \(j\) in the next step depends solely on the conditions of nodes \(i\) and \(j\). This characteristic, where the probability of reaching node \(j\) is independent of the preceding step at node \(i\), defines a Markov process. Thus, the random walk process inherently embodies a Markovian nature.

Let \(p_{ij}\) represent the probability that the walker walks from node \(i\) to node \(j\) in brain graph \(G\), then \(p_{ij}\) can be represented in the following matrix form:

\[P_{G}=[p_{11}&&p_{1n}\\ &&\\ p_{n1}&&p_{nn}],0 p_{ij} 1,\,_{v=1}^{ n}p_{ij}=1, \]

where matrix \(P_{G}\) is the transfer matrix of brain graph \(G\). Then the state vector is defined as:

\[T(k)=(t_{1}(k),t_{2}(k),,t_{k} (k)),\,_{j=1}^{n}t_{j}(k)=1, \]

where \(k\) is denoted as the number of hops in random walks. \(t_{j}(k)\) is the probability of the walker stops at node \(j\) after \(k\) times walk. \(t_{j}(k)\) is called \(k\) steps state probability. According to the total probability formula, we get:

\[t_{i}(k+1)=_{j=1}^{n}t_{j}(k)p_{ij},k=0,1,2, ,K. \]

where \(K\) represents the total number of hops in random walk. So, we get the general recursive formula as \(T(k)=T(0)P_{G}^{k}\). Nevertheless, brain networks deviate from general networks as pairwise ROIs typically exhibit distinct communication strengths, indicative of the collaborative nature of ROIs in brain activity. Falsely treating pairs of ROIs with varying communication strengths equally may disrupt the collaborative dynamics within brain activity. Given this fact, the transfer probability of the walker in a brain network can be fine-tuned using the adaptive factors, i.e., \(_{G}=F_{G} P_{G}\), where \(\) denotes the dot product operation. Formally, considering the adjacency matrix \(A_{G}\) and the corresponding diagonal degree matrix \(D_{G}\) of a brain graph \(G\), along with the obtained adaptive factors \(F_{G}\), we define the random walk kernel \(R\) for adaptive long-range encoding as follows:

\[R=(F_{G} A_{G})D_{G}{}^{-1}. \]

In particular, The introduction of degree matrix can help to obtain richer information about brain-wide communication and is very commonly used in network analytics . Since the degree matrix provides the number of degrees for each ROI, its ability to reflect the active state of the ROI in communication is important in determining which ROIs play a key role in information propagation. Hence, a degree matrix determines the transfer probability of a node to its neighboring nodes and highly influences the behavior of walker .

In the \(K\)-step random walk, the long-range embedding \(E_{G}\) initialized by the adaptive long-range encoding is defined as:

\[e_{i}=[I,R,R^{2},,R^{K-1}]_{ii}^{K}, \]

where \(I\) denotes the identity matrix. \(e_{i}\) denotes a long-range embedding associated with the \(i\)-th node, encapsulating the long-range dependency asscioated with \(i\)-th node. Through adaptive long-range encoding, we can explicitly capture long-range dependencies among ROIs in the brain graph \(G\) and encode them into the form of long-range embeddings \(E_{G}\).

### Long-range Brain Graph Transformer

In Section 3.1.2, we obtained the long-range embedding \(E_{G}\) that explicitly encode the long-range connectivities within the brain network \(G\). As short-range dependencies are also significant, we aim to present an effective brain graph transformer by first injecting long-range embeddings into brain network representation learning and then integrate both long-range and short-range dependencies for learning a more comprehensive representation. To achieve this objective, we begin by describing the process of injecting long-range embeddings \(E_{G}\) into the brain graph transformer. Later, we explain how the self-attention mechanism can be utilized to integrate long-range and short-range dependencies among brain ROIs.

Injecting Long-range Embedding.The computed long-range embeddings \(E_{G}\) should be injected into the brain network transformer in a manner that enhances its utility. To acheive this, we introduce a fine-tuning procedure aimed at enhancing long-range embeddings \(E_{G}\) and injecting them into the brain graph transformer. Specifically, we utilize a linear layer as a remapping function for long-range embeddings \(E_{G}\), facilitating the injection of long-range dependencies within the brain network. This process enables the acquisition of trainable long-range embeddings \(_{G}\) with dimension \(k^{}\). Formally, this procedure is defined as:

\[_{G}=(E_{G};W_{G})=W_{G}E_{G}+b_{G}^{N k^{ }}, \]

where \(W_{G}^{k^{} k}\) and \(b_{G}^{k^{}}\) denote learnable weight matrix and bias vector, respectively.

Self-attention Module.Transformer-based models generally surpass conventional representation learning methods in their ability to capture pairwise token correlations and the influence of individual tokens. This stems from the self-attention mechanism's capability to allow inter-token communication. Nonetheless, employing initial node features as input tokens is insufficient for Transformer-based models to effectively capture complex inter-dependencies within brain networks. Furthermore, pairwise ROIs often exhibit varying degrees of short- and long-range dependencies across various brain network analysis tasks . Hence, we need to integrate both long-range and short-range communication among ROIs through a self-attention module. To model this mechanism, we begin by constructing tokens through the combination of learnable long-range embeddings \(_{G}\) and initial node features \(X_{G}\). Then, we utilize a vanilla transformer encoder as the framework for the self-attention module.

Formally, we concatenate learnable long-range embeddings \(_{G}\) and initial node features \(X_{G}\) as tokens \(_{G}\), and then utilize a transformer encoder with \(L\)-layer nonlinear mapping and \(M\) attention head to learn comprehensive node features \(Z_{G}\):

\[_{G}\!\!=\![.X_{G}|_{G}]^{N (d+k^{})}, \]

\[Z_{G}=W_{o}(||._{m=1}^{M}Z_{G}^{m,l}.) ^{N d_{out}},\;Z_{G}^{m,l}=(K^{m,l}}{^{m,l}}})V^{m,l}^{N d_{ out}^{m,l}},. \]

with \(Q^{m,l}=W_{q}Z_{G}^{m,l-1}\), \(K^{m,l}{}^{T}=(W_{k}Z_{G}^{m,l-1})^{T}\), and \(V^{m,l}=W_{v}Z_{G}^{m,l-1}\) are the query matrix, the key matrix, and the value matrix, where \(Z_{G}^{0}=_{G}\), \(||\) and \([\,|\,\,]\) both indicate the concentrate operation, \(l\) and \(m\) denote the layer index and the head index, \(W_{q}\), \(W_{k}\), \(W_{v}^{d_{out}^{m,l} d_{out}^{m,l-1}}\) and \(W_{o}^{d_{out} d_{out}^{m,l}}\) are learnable projection matrices. In the representation learning procedure, the employed Transformer framework enables the learned \(Z_{G}\) to integrate both short-range and long-range dependencies between brain ROIs by introducing long-range embedding. This design allows our method to adaptively represent the communication connectivities in human brains.

Readout Module.To accomplish brain network analysis tasks, we take the output \(Z_{G}\) of the self-attention module as the criterion, and then utilize an efficient readout function to derive the entire brain graph representation to further enhance the performance. In addition, we train an additional classifier for downstream tasks. The final classification basis is obtained as follows:

\[Y_{G}=(((Z_{G}))). \]

In Section 4.2, we evaluate the performance of various pooling methods. Ultimately, we employ clustering-based pooling as the readout function in the proposed method.

## 4 Experiments

In this section, we analyzed the following aspects to demonstrate the effectiveness of the proposed method and its capability to capture long-range dependencies within brain networks.

**Q1.** Does ALTER outperform other state-of-the-art models?

**Q2.** How does the proposed adaptive long-range aware strategy perform in different model architectures accompanied by various readout functions?

**Q3.** Does ALTER capture long-range dependencies within brain networks, and is ALGA strategy considered a key component?

### Experimental Settings

Datasets and Preprocessing.We evaluate the proposed method using two brain network analysis-related fMRI datasets. 1) _Autism Brain Imaging Data Exchange_ (ABIDE)1, which contains 519 Autism spectrum disorder (ASD) samples and 493 normal controls. 2) _Alzheimer's Disease Neuroimaging Initiative_ (ADNI)2, which contains 54 Alzheimer's disease (AD) samples and 76 normal controls. During the construction of the brain graph, we first preprocess the fMRI data using the Data Processing Assistant for Resting-State Function (DPARSF) MRI toolkit. Next, we define brain ROIs based on predefined atlases from preprocessed fMRI data and calculate the average time-series feature for individual brain ROI. Finally, we formalize the brain graph \(G=(V,X,A)\) for each sample according to the average time-series features of brain ROIs. Specifically, node features \(X\) are functional connectivity matrix calculated by Pearson correlation, the adjacency matrix \(A\) is the thresholded functional connectivity matrix to generate binary matrix of 0s or 1s, where the threshold is 0.3. The details of datasets and preprocessing can be found in Appendix A.

Baselines.The selected baselines correspond to two categories. The first category is generalized graph learning methods (Generalized - not specifies to brain networks), including SAN , Graphormer , GraphTrans , and LRGNN . The second category (Specialized) is the brain graph-based methods, including BrainNetGNN , FBNETGEN , BrainGNN , BrainNETTF , A-GCL , and ContrastPool . Note that the original code shared by the authors of these baselines is used for the comparative analysis. Please refer to the Appendix A for the details.

Metrics.Given the medical application of neurological disease classification tasks, we utilize both machine learning and medical diagnostic-specific metrics to evaluate the performance of the proposed method. These include classification Accuracy (ACC), Area Under the Receiver Operating Characteristic Curve (AUC), F1-Score, Sensitivity (SEN), and Specificity (SPE). In the experimental results, we report the mean and standard deviation across 10 random runs on the test dataset.

Implementation Details.In the proposed method, we set the number of steps \(K\) for adaptive random walk to 16. The number of nonlinear mapping layers \(L\) and attention heads \(M\) of the self-attention module are set to 2 and 4, respectively. For all datasets, we randomly divide the training set, evaluation set and test set by the ratio of \(7:1:2\). In the train processing, we adopt Adam as optimizer and CosLR as scheduler by a initial learning rate of \(10^{-4}\) and a weight decay of \(10^{-4}\). The batch size is set to 16 and the epoch is set to 200. All experiments are implemented using the PyTorch framework, and computations are performed on one Tesla V100.

### Performance Comparison

In this sections, we evaluate the performance of ALTER by comparison with existing baselines to address **Q1**.

Results.Table 1 reports the comparison results between the proposed and the baseline methods. ALTER is able to significantly outperform the two categories of baseline methods for both datasets. In comparison to generalized graph learning methods, the proposed ALTER exhibits a significant improvement in terms of the ACC metric (10.9% improvement on the ABIDE dataset and 6.8% improvement on the ADNI dataset). For the case specialized graph learning methods, we again demonstrated superiority on both datasets in terms of the ACC metric (6.0% improvement on the ABIDE dataset and 5.1% improvement on the ADNI dataset). The reason for this performance improvement is that our method takes into account the communication strengths among brain ROIs

[MISSING_PAGE_FAIL:8]

architectures are shown in Figure 3. We observe that, for any arbitrary readout function, the ALGA strategy enhances the performance of downstream tasks for various architectures compared to those without ALGA.

### In-depth Analysis of ALTER and ALGA Strategy

Here, we delve into the analysis of ALGA strategy and present cases to evaluate long-range dependencies to demonstrate the effectiveness of ALTER to assess **Q3**.

First, we investigate the impact of key hyperparameter of ALGA strategy of ALTER, which is the number of hops. In this experiment, we set the number of hops of ALGA to 2, 4, 8, 16, 32 for both selected datasets (only the AUC is shown here, the full result can be found in Appendix A). Figure 4(a) clearly shows that for both datasets, the predictive power of the proposed method generally increases as the number of hops increases. This phenomenon can be attributed to the presence of long-range connectivity in communication and information processing within human brains, which our model effectively captures. Ignoring this characteristic will adversely affect the predictive power of graph learning methods in brain network analysis tasks.

Next, we investigate the impact of the adaptive factors in the ALGA strategy. Specifically, we remove the adaptive factors and observe the change in the predictive ability of the proposed method. The experimental results demonstrate (Figure 4(a)) the performance of the proposed method is degraded when adaptive factors are not used to adjust the adaptive long-range encoding. The underlying reason for this result is that inter-ROI correlations play a crucial role in reflecting the communication strengths among brain ROIs. Treating pairwise ROI connectivity equally could potentially have a detrimental effect on brain network analysis tasks that depend on inter-ROI communication.

Finally, we present the cases to demonstrate ALTER's ability to capture long-range dependencies within brain networks. In this experiment, we randomly sample an example brain graph from the ABIDE test set and used it to train our model to learn the corresponding node features without pooling operation, and thus compute the attention scores among the node features. Figure 4(c) illustrates one example graph and the corresponding attention heatmap (Figure 4(b)). More sample-level and group-level examples can be found in Appendix A. The attention heatmap demonstrates the communication patterns necessary for brain network analysis tasks. Specifically, certain ROIs receive higher attention scores from multiple other ROIs, irrespective of the distance between them. In particular, ROI 6 and ROI 19 present higher attention score, despite the fact that these two ROIs are 5 hops apart (Figure 4(c)).

## 5 Discussions and Conclusion

Limitations.On one hand, while utilizing the brain graph transformer to integrate both short-range and long-range dependencies among brain ROIs, we still cannot ensure an optimal balance between them. In future research, we will explore how to achieve a better balance between short-range and long-range dependencies in brain network analysis, with the aim of achieving better research results. On the other hand, the experimental data we currently employ is limited to fMRI data. Although utilizing these data can demonstrate the crucial role of long-range dependencies in brain network analysis tasks, other forms of data, such as DTI data, are also worthy of exploration. In future work, we will delve into alternative forms of data and propose corresponding methods for capturing long-range dependencies within brain networks.

Figure 3: Performance comparison with varying readout functions (%).

Conclusion.In summary, we present the ALTER model for brain network analysis, a novel brain graph transformer that explicitly captures long-range dependencies in brain networks and adaptively integrates them with short-range dependencies. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized and specialized (specific to brain network analysis method) graph learning methods. This study presents an initial attempt to capture long-distance dependencies within brain networks and provides a new insight into understanding brain-wide communication and information processing.