# Rethinking 3D Convolution in \(\ell_{p}\)-norm Space

# Rethinking 3D Convolution in \(_{p}\)-norm Space

 Li Zhang\({}^{1,2,4}\), Yan Zhong\({}^{3}\), Jianan Wang\({}^{4}\), Zhe Min\({}^{5}\), Rujing Wang\({}^{1,2}\), Liu Liu\({}^{6}\)

\(1\) Hefei Institute of Physical Science, Chinese Academy of Sciences

\(2\) University of Science and Technology of China, Hefei, China

\(3\) School of Mathematical Sciences, Peking University. Beijing, China

\(4\) Astribot, Shenzhen, China

\(5\) Shandong University, Jinan, China

\(6\) Hefei University of Technology, Hefei, China

zanly@mail.ustc.edu.cn, zhongyan@stu.pku.edu.cn

Li Zhang and Yan Zhong contribute equally. This work was done when Li Zhang was an intern at Astribot. Corresponding author: Liu Liu.

###### Abstract

Convolution is a fundamental operation in the 3D backbone. However, under certain conditions, the feature extraction ability of traditional convolution methods may be weakened. In this paper, we introduce a new convolution method based on \(_{p}\)-norm. For theoretical support, we prove the universal approximation theorem for \(_{p}\)-norm based convolution, and analyze the robustness and feasibility of \(_{p}\)-norms in 3D point cloud tasks. Concretely, \(_{}\)-norm based convolution is prone to feature loss. \(_{2}\)-norm based convolution is essentially a linear transformation of the traditional convolution. \(_{1}\)-norm based convolution is an economical and effective feature extractor. We propose customized optimization strategies to accelerate the training process of \(_{1}\)-norm based Nets and enhance the performance. Besides, a theoretical guarantee is given for the convergence by _regret_ argument. We apply our methods to classic networks and conduct related experiments. Experimental results indicate that our approach exhibits competitive performance with traditional CNNs, with lower energy consumption and instruction latency.

## 1 Introduction

The convolution-based 3D backbone networks have demonstrated substantial success in foundational tasks such as classification , object tracking , scene segmentation , etc. Some downstream tasks also heavily rely on these networks, such as interactive perception , object manipulation , imitation learning , and human-machine collaboration . In the traditional 3D convolution, suppose \(K^{m n}\) is the filter, and \(P_{t}^{m n}\) is the sampled matrix from the \(t\)-th sliding window on input data, \(1 t T\). \(T\) is the total sliding counts. For any \(t 1\), the \(t\)-th convolution is calculated as:

\[P_{t} K=_{1 i m}_{1 j n}P_{t}(i,j) K(i,j)\] (1)

which is the same as inner product between vectors. To distinguish it from our new convolution framework, we refer to it as _inner product based convolution_ in the following discussion. A geometric consideration arises when \(P_{t}\) follows a certain symmetric distribution, such as a Gaussian or uniform distribution. By symmetry, there exist some of \(\{P_{t}\}_{t=1}^{T}\) situated close to the subspace perpendicular to \(K\), which means \(K P_{t} 0\). This inevitably leads to explicit feature loss, diminishing the model's ability on information extraction.

In previous works, \(_{p}\)-norms (\(p=1,2,3,,\)) demonstrated strong performance across various domains [8; 9; 10]. These norms exhibit remarkable capabilities in expressing spatial structures and local relationships within sets of points. To address the limitations of inner product-based convolution in certain extreme cases and to explore the potential of \(_{p}\)-norms in feature extraction, we propose \(_{p}\)-norm-based convolution, _i.e._, for any kernel \(K\) and sampled matrix \(P_{t}\), it can be formulated as Eq. 2:

\[\|P_{t}-K\|_{p}_{1 i m}_{1 j n}(P_{t} (i,j)-K(i,j))^{p}^{1/p}.\] (2)

More precisely, the goal of this paper is to leverage the power of \(_{p}\)-norm measurement (Fig. 1 (a)) and devise efficient and robust optimization methods for it. Our solutions are as follows:

From the theoretical standpoint, we prove the universal approximation theorem of \(_{p}\)-norm Nets (for \(p=1,2,3,,\)). Besides, we show that \(_{p}\)-norm based convolutions are more robust than the traditional ones via variance analysis under random noise.

From the practical standpoint, we first discuss the performance of different \(_{p}\)-norms in actual execution. 3D convolution in \(_{}\)-norm space tends to lose multiple useful pieces of information since only the maximum absolute value is reserved. The \(_{2}\)-norm measure is inherently a linear transformation of the traditional convolution (details can be found in Sec. A). In contrast, the \(_{1}\)-norm has unique potential for 3D point cloud tasks. However, directly replacing traditional convolution with an \(_{1}\)-norm-based one is not feasible in practice due to the difficult convergence and local optima. To enhance network performance, we propose customized optimization strategies. The first strategy is a mixed gradient strategy (MGS), and the second is a dynamic learning rate controller (DLC). These strategies are applied in the training process (Algorithm 1) to accelerate network convergence and avoid local optima. We also provide a convergence guarantee for our optimization strategies from the perspective of _regret_.

We evaluate our method on several benchmarks, ranging from global, semi-dense, and dense prediction tasks. The experimental results show that \(_{1}\)-norm Net has the same competitive performance as traditional convolution. Moreover, the proposed \(_{1}\)-norm Net has three advantages: 1) \(_{1}\)-norm (inherently addition operation) has lower computational complexity compared to multiplication; 2) addition significantly reduces energy consumption ; 3) \(_{1}\)-norm operations (addition) has lower instruction latencies  than inner product process (multiplication). These properties facilitate the 3D point cloud tasks especially online tasks such as 3D real-time object detection, pose tracking, etc.

**Contributions.** 1) We prove the universal approximation for \(_{p}\)-norm Nets. And we show that \(_{p}\)-norm Nets are robust under random noise. 2) We compare different \(_{p}\)-norm based convolutions, and further propose a reliable and efficient \(_{1}\)-norm Net for 3D point cloud tasks with customized optimization strategies. We also give a theoretical guarantee for convergence by regret argument. 3) Experimental results demonstrate the effectiveness of our methods in 3D point cloud tasks, showing lower energy consumption and faster instruction execution.

## 2 Related Work

**Different Convolution Methods.** Convolutions have seen significant success, leading to various convolution methods aimed at improving performance and efficiency. Traditional convolutions, introduced by , use fixed-size kernels to extract features but are computationally intensive and may not capture diverse patterns effectively. To overcome these limitations, several alternatives have been proposed: 1) depthwise separable convolutions [14; 15]. Popularized by MobileNets, these decompose standard convolutions into depthwise and pointwise operations. 2) dilated convolutions [16; 17; 18]. These introduce spaces between kernel elements, expanding the receptive field without increasing parameters. 3) deformable convolutions [19; 20]. These adapt the sampling loca

Figure 1: **(a) Visualizing the circles of \(_{p}\)-norms. (b) Manhattan distance based \(_{1}\)-norm measure.**

tions of the convolutional kernel, enhancing the network's ability to model geometric transformations. However, due to their unique strengths, they only excel at some specific tasks.

\(_{p}\)**-norm Measure in Different Tasks.** Using the \(_{p}\)-norm as a feature measurement function for convolutional kernels offers several advantages: 1) Flexibility: The \(_{p}\)-norm allows adjusting the parameter \(p\) according to specific needs [21; 22; 23]. 2) Sparsity: It encourages most elements in the convolutional kernel to approach zero, reducing computational complexity and storage requirements [21; 24]. Overall, in diverse settings, employ distinctive approaches. The \(_{p}\)-norm is widely used across various fields. For example, in _image processing_, the \(_{1}\)-norm is used for sparse representation in image compression , enabling efficient storage and transmission. In _machine learning and optimization_, optimization problems also use \(_{p}\)-norm constraints to impose sparsity or specific patterns in solutions [26; 27]. Despite progress, directly migrating these methods into 3D point cloud tasks causes a _domain gap_. In this work, we aim to explore \(_{p}\)-norm measure for 3D point cloud tasks in depth.

## 3 Methodology

**Notations.** For the sake of simplicity, in what follows, we take the classic PointNet++  as the basis model to estimate the efficiency of \(_{p}\)-norm based Nets with the proposed optimization strategies. Note that, we directly replace the inner product based convolution by \(_{p}\)-norms \((p=1,2,3,,)\) based one, and denote the corresponding network by \(_{p}\)-PointNet++ or \(_{p}\)-norm Net. Moreover, the proposed \(_{p}\)-norm based convolution can also be called \(_{p}\)-norm neuron.

### Universal Approximation

The universal approximation ability of a neural network is crucial. Firstly, it establishes a solid theoretical foundation for the network's capabilities , which asserts that certain architectures and activation functions enable neural networks to approximate any continuous function. There is a series of works on the approximation capacity, such as theories for feedforward networks , RNNs , Transformer . However, the universal approximation property of \(_{p}\)-PointNet++ has not been studied thoroughly up to now.

**Theorem 1**.: _Assume \(S=\{x_{1},,x_{N}\}^{k}\) is an arbitrary point cloud. \(J^{k}\) is any compact set and \(S J\). For any continuous function \(f\) defined on \(2^{J}\) with respect to Hausdorff distance \(d_{H}(,)\), there exists an \(_{p}\)-PointNet++ \(\) satisfying for any \(>0\),_

\[|f(S)-(S)|.\] (3)

_Moreover, for any \(_{1}\)-integrable function \(g\) defined on \(J\), there exists an \(_{p}\)-PointNet++ \(^{}\), for any \(^{}>0\),_

\[_{x J}|g(x)-^{}(x)|dx<^{}.\] (4)

Briefly speaking, \(f\) could be approximated by an MLP consisting of \(_{p}\)-norm convolution layers and a max pooling layer. And \(g\) could be approximated by a network composed of an \(_{p}\)-norm convolution layer and a fully connected layer. The detailed proof can be found in Sec. A from the appendix.

### Robustness Analysis

In the following, we show that under Gaussian random noise on input data, \(_{p}\)-norm based convolutions are more robust than that based on inner product. Suppose \(G^{m n}\) is a Gaussian matrix. Each \(G(i,j) N(0,^{2})\) where \(>0\) is a constant. Let \(P_{t}^{m n}\) be the data at time \(t\) and \(K^{m n}\) be the kernel function.

For inner product,

\[Var(G+P_{t}) K=_{G}G K- _{G}[G K]^{2}=VarG K,\] (5)

and

\[G K=_{i=1}^{m}_{j=1}^{n}G(i,j)K(i,j) N0,^{2} _{i=1}^{m}_{j=1}^{n}K(i,j)^{2}.\] (6)Suppose \( i[m]\) and \( j[n]\), \(K(i,j)\) is a constant, we have \(Var(G+P_{t}) K=(mn)\).

For \(_{p}\)-norm, first we could prove that when \(p=2\), \(Var\|G+X-K\|_{2}=O(1)\), which is significantly smaller than \(Var(G+P_{t}) K\). The details of calculation could be found in Sec. A from the appendix. For the more general cases (\(p=1,2,3,,\)), we show that \(_{p}\)-norm has a small variance through numerical computation in the Tab. 1, where we take \(=1\).

### Implementation of \(_{p}\)-norm Nets

Note that although Theorem 1 guarantees a universal approximation capability, it does not mean that all the \(_{p}\)-norm Nets are efficient and feasible in practice. Therefore, we further discuss the characteristics of each specific \(_{p}\)-norm Nets (\(p=1,2,3,,\)) in detail.

Assume the input data follows Gaussian distribution, saying \(G\) is the standard Gaussian matrix. For \(_{p}\)-norm based convolution, when \(p\) is greater than or equal to 3, the distribution of the output data is very close. We present the simulation results in Fig. 2. It's clear that when \(p\) is getting larger, the distribution of \(\|G\|_{p}\) gradually overlaps with the distribution of \(\|G\|_{}\). Therefore, we take \(p=\) as the representative case for \(p 3\).

Actually, \(l_{}\)-norm exhibits weaknesses due to its overly simplistic emphasis on the largest element. Namely this approach tends to oversimplify the feature space by disproportionately emphasizing only one dimension, potentially discarding valuable information present in other dimensions. Also, this concept is supported by experimental results in Sec. 5. Besides, \(_{2}\)-norm inherently is calculated by taking the square root of the sum of the squares of its elements. And \(_{2}\)-norm based convolution \(_{_{2}}\) can be regarded as an equivalence transformation of the traditional convolution \(\). Briefly speaking, we could show that \(_{_{2}}^{2}=+\), where \(\) and \(\) are constants.

\(_{1}\)-norm can synthesize each element of the feature vector. And the \(_{1}\)-norm Net is not equivalent to a translation transform, which we believe holds potential as a 3D convolutional similarity metric function according to the Theorem. 1. To this end, our method focuses on rationalizing the \(_{1}\)-norm measure to maximize its potential in feature extraction. Mathematically, if the similarity measurement function between the input data and kernel function is replaced with the \(_{1}\)-norm, the convolution can be re-formulated as:

\[Y(P_{t},K)=-_{t 1}_{i,j}|P_{t}(i,j)-K(i,j)|\] (7)

The underlying operation of \(_{1}\)-norm kernel function is addition, which has more development potential and application value in real scenarios. Specifically, 1) It contains almost no multiplication but addition, resulting in lower computational complexity of the model. 2) \(_{1}\)-norm operation (addition) is proved to have lower energy consumption compared to the inner product (multiplication) calculation . Take the operation of floating-point addition and multiplication as an example, which has energy costs of 0.9 \(pJ\) and 3.7 \(pJ\), respectively. 3) Low latency is also a consideration in practical application scenarios.  tells us that multiplication (inner product process) has longer theoretical instruction wait times than addition operations. Table 1 of this study lists the instruction

  p & **1** & **2** & **3** & **4** & **5** \\  Var & 3.24655 & 0.48327 & 0.31248 & 0.27494 & 0.26078 \\  
**p** & **6** & **7** & **8** & **9** & \(\) \\  Var & 0.26093 & 0.26040 & 0.26078 & 0.26145 & 0.26875 \\  

Table 1: **Variance of the \(_{p}\)-norm of Gaussian random vector when \(mn=9\).**

Figure 2: (**Left**) The distribution of \(\|G\|_{p}\),where \(G\) is the standard Gaussian vector, \(p=1,2,3,\) and \(dim(G)=9\). (**Right**) The distribution of \(\|G\|_{p}\), \(p=3,4,5,6,7,8,9,\) and \(dim(G)=9\).

latency, throughput, and micromanipulation faults for Intel, AMD, and VIA CPUs. For instance, the latency of float multiplication and addition is 4 and 2 in the VIA Nano 2000 series.

### Regret

It's a good way [34; 35; 36] to demonstrate the convergence of an optimization process by analyzing the _regret_. Performance measurement , optimization guidance , and feedback mechanisms  can be summarized as its advantages. We employ it the construct the convergence theorem for our optimization strategies in Sec. 4.

Consider a general online optimization model between a player and an adversary. A subset \(^{m}\) is non-empty, bounded and closed. For each iteration \(k[T^{*}]\), the player choose a point \(_{k}\) (\(T^{*}\) is not known for player). After committing to this choice, a convex function \(h_{k}\) will be revealed by the adversary. And we note the cost of this game by _regret_:

\[R_{T^{*}}=_{k=1}^{T^{*}}h_{k}(_{k})-_{ }_{k=1}^{T^{*}}h_{k}().\] (8)

The player aims to carefully select \(_{k}\) to minimize regret as much as possible, while conversely the adversary aims to specifically choose \(h_{k}\) to hinder the player. Intuitively, if an algorithm(the player) could bound regret by a sub-linear function of \(T^{*}\), _i.e._, \(R_{T^{*}}=o(T^{*})\), we could conclude that "on the average" the algorithm performs as well as the best fixed strategy in hindsight .

## 4 Optimization

By the argument above, we are motivated to devise a new convolution based on \(_{1}\)-norm. However, direct training of \(_{1}\)-norm Nets can easily lead to unsatisfactory results. Thus, two customized optimization strategies are proposed for training. Before introducing these optimization strategies, we clarify the notations in the following.

NotationsRecall that \(K^{m n}\) is the kernel and \(P_{t}^{m n}\) is the sliding window on the input data, \(1 t T\). \(Y(P_{t},K)\) is the convolution of \(K\) and \(P_{t}\). \(L\) denotes the loss function in training process. We use the \(m n\) matrix \(\) to denote the gradient on of \(L\) on \(K\), where \(()_{i,j}=\). Besides, define the vectors

\[,K)},,K)},,,K)}\]

and

\[,K)} { K(i,j)},,K)}{ K(i,j)},,,K)}{ K(i,j)}\]

### MGS: Mixed Gradient Strategy

Now we focus on the gradient descent in training process, especially the partial derivative of loss function \(L\) on the kernel \(K\). It should be pointed out that \(L\) is a function on \((Y(P_{1},K),Y(P_{2},K),,Y(P_{T},K))\). By chain rule of derivation we have for any given \((i,j)\),

\[=_{t=1}^{T},K)},K)}{ K(i,j)}=,\] (9)

Notice that when loss function \(L\) is fixed, \(\) is regardless of the choice of \(Y(P_{t},K)\) (inner product or \(_{p}\)-norm). And we should only focus on the vector \(\). In the context of \(_{1}\)-PointNet++:

\[,K)}{ K(i,j)}=P_{t}(i,j)-K(i,j) .\] (10)

Here, \(()\) represents the sign function.

There are two unavoidable problems: 1) the use of Eq. 10 results in a signSGD update. As discussed in , the direction of signSGD is not aligned with the steepest descent, and this misalignment exacerbates with increasing dimensionality. 2) The gradient of \(_{1}\)-norm Net is significantly smaller than that of inner product convolution in the experiment. Namely, \(\|\|_{2}\) is extremely small when we choose the convolution \(Y\) as \(_{1}\)-norm. Taking PointNet++ on S3DIS as an example, we report the \(_{2}\) norm of gradient of \(_{1}\)-PointNet++ in Fig. 3. The gradient from \(_{1}\)-PointNet++ is much smaller than that in PointNet++ (_e.g._, \(_{1}\)-PointNet++: 0.0002, PointNet++: 0.3162 in layer I). Hence, this small gradient \(\) in \(_{1}\)-norm Net would significantly slow down the training process.

Based on the above observations, we introduce a novel **Mixed Gradient** **S**trategy (**MGS**) tailored for \(_{1}\)-PointNet++ training. This approach strategically combines the gradients of the \(_{1}\)-PointNet++ and that of \(_{2}\)-PointNet++:

\[,K)}{ K(i,j)}=(i,j)-K(i,j)}{||K-P_{t} ||_{2}},\] (11)

Actually, as we discussed above, \(_{2}\)-norm based convolution is a linear transform of inner product convolution. So gradient of \(_{2}\)-norm Net has a proper scale. The mixed strategy involves dynamically adjusting \(,K)}{ K(i,j)}\) during training, guided by a parameter \(0<<1\) and the training step \(k\). **The mixed gradient strategy** is expressed as:

\[,K)}{ K(i,j)}=(1-^{k})(P_{t}( i,j)-K(i,j))+^{k}(P_{t}(i,j)-K(i,j)).\] (12)

This dynamic adjustment introduces a controlled transition in the gradient computation as training progresses. Taking \(=0.99\) for example, when \(k\) is small, the term \(^{k}\) dominates and \(,K)}{ K(i,j)}\) approximates to \(P_{t}(i,j)-K(i,j)\). This initial configuration aligns with the more efficient \(_{2}\)-like update, providing stability and aiding in faster convergence. As training progresses (\(k\) gets larger), the term \(^{k}\) becomes more prominent, shifting the gradient computation towards \((P_{t}(i,j)-K(i,j))\). This transition allows the model to leverage the advantages of the \(_{1}\)-PointNet++ structure, facilitating sparsity in the learned features. By dynamically adapting the gradient computation based on the training step, the mixed strategy offers a flexible and adaptive approach to overcome the challenges associated with fixed gradient schemes. This dynamic adjustment provides a thoughtful compromise, combining the efficiency of \(_{2}\)-like updates in the initial stages with the sparsity-inducing benefits of \(_{1}\)-PointNet++ in later stages.

In fact, there is quite a bit of literature supporting the effectiveness of the signSGD update scheme, and in particular, it has been shown that it has some advantages in avoiding saddle points . However, when certain random rotations of the objective appear, signSGD may become trapped in a periodic behavior that hinders convergence in such cases. To address this unexpected behavior, we additionally explored the introduction of momentum into the update rule. Our experimental results prove that this modification effectively breaks the symmetry induced by random rotations, preventing the model from getting stuck and fostering smoother convergence.

### DLC: Dynamic Learning rate Controller

Considering the uniqueness of the mixed gradient strategy, we focus on achieving larger update magnitudes and faster convergence rates during the initial stages of training. However, in the later stages, we aim to revert to signSGD, implementing a more cautious update strategy to enhance the model's precision. Therefore, we propose a learning rate update strategy that adapts to this characteristic: **D**ynamic **L**earning rate **C**ontroller (**DLC**), maintaining a higher rate in the early training phase, and returning to a lower rate in the later phase.

Figure 3: **The gradient of weight in each layer using two different networks at 1st iteration.** Layer I to III represent 3 SetAbstractions modules in \(_{1}\)-PointNet++ and layer IV to V represent fully connected layers. Note that the y-axis is on a logarithmic scale to reflect the magnitude of the values.

To this end, we design two bound functions to control the learning rate: the lower bound

\[_{1}(k)=p_{1}(1+}{e^{k}})\] (13)

and the upper bound

\[_{2}(k)=p_{1}(1+}{k})\] (14)

where \(p_{1}\), \(p_{2}\) and \(p_{3}\) are hyper-parameters to be determined and \(k\) denotes the training step. And we use simple comparison operations to make learning rate \((k)\) locate in \([_{1}(k),_{2}(k)]\):

\[(k)\{_{1}(k),[(k)]\},_{2}(k)}.\] (15)

To enhance the universality of this dynamic control framework, \(\) could be another learning rate optimization algorithm like the adaptive learning rate strategy of , which can be specifically switched according to the task at hand. However, regardless of \(\), we will later demonstrate that dynamic control alone is sufficient to provide theoretical convergence guarantees by the regret argument of Theorem 2, and it also performs well in experiments.

### Training Framework

It has been noted from previous discussions that the momentum method can help signSGD avoid getting trapped in cycles, thereby improving training stability. Combining the methods above, we present the global optimization algorithm (**O**ptimizer with **M**ixed gradient strategy and **D**ynamic learning rate controller, **OMD**) for \(_{1}\)-PointNet++ training. Details are shown in Algorithm. 1

Here we give a convergence guarantee for **OMD** under an online optimization framework, which is harder than offline optimization. We could show that regret \(R_{T^{*}}\) of **OMD** is bounded by \(O(})\). Low regret means the algorithm progressively gets closer to the optimal solution over time. This shows that **OMD** has reliable convergence properties, making it a dependable optimization method.

**Theorem 2**.: _Continue with the settings and notations of Algorithm 1. Suppose \(^{n}\) is bounded, saying \(_{, F}\|-\|_{} B _{}\) Besides, suppose \( k[T^{*}]\), \(\|_{k}\|_{2} B_{2}\). we could show that for any convex functions \(\{h_{k}\}_{t=1}^{T^{*}}\),_

\[R_{T^{*}}=_{k=1}^{T^{*}}h_{k}(_{k})-_{t=1}^{T^{*}}h_{k}( ^{*}) C_{1}}+C_{2}\]

_where \(C_{1}\) and \(C_{2}\) are constants that rely on \(p_{1}\), \(p_{2}\), \(p_{3}\), \(B_{}\), \(B_{2}\), \(q_{0}\) and \(q\). And \(^{*}_{}_{k=1}^{ T^{*}}h_{k}()\)._

The proof could be found in the appendix, Sec. A.

## 5 Experiments

To validate the generalizability and robustness of the method and thus ensure its effectiveness and broad applicability, we verify the performance of our method in several tasks, ranging from **Global Tasks** (_i.e._, Parts Segmentation), **Semi-dense Prediction** (_i.e._, scenario semantic segmentation), and **Dense Prediction** (_i.e._, pose estimation) tasks. Shapenet, S3DIS, and GarmentNets Simulation are used as the datasets.

### Dataset and Experimental Settings

**Dataset.** 1) ShapeNet. In ShapeNet, there are 16,881 shapes from 16 categories, which are annotated with 50 parts in total. Note that most object categories are labeled with two to five parts and Ground Truth annotations are labeled on sampled points on the shapes. This task can be regarded as a point-wise classification task. 2) S3DIS. The Stanford Large-Scale 3D Indoor Spaces Dataset, which encompasses 3D scans obtained from Matterport scanners across 6 distinct areas, comprising a total of 271 rooms. Within the S3DIS dataset, every point within the scans is labeled with a semantic category from a set of 13 distinct classes. These classes encompass various elements such as chairs, tables, floors, walls, among others, in addition to a category for clutter. 3) GarmentNets Simulation. GarmentNets Simulation is a large-scale dataset proposed by . This dataset has six garment categories with a total data volume of 1.72TB. Dress, Jump, Skirt, Top, Pants and Shirt are included.

**Experimental Settings.** We train our frameworks using CrossEntropy loss and the AdamW optimizer , with an initial learning rate of 0.001, a weight decay of \(10^{-4}\), Cosine Decay, and a batch size of 32. The total training consists of 200 epochs. _All tasks use the same settings unless otherwise specified._ All experiments are conducted on a computer workstation with three GeForce GTX 3090 GPUs using the PyTorch deep learning framework. The best model on the validation set is selected for testing.

### Experiments on Global Task

**Parts Segmentation.** As a classic global task, 3D object parts segmentation is an important predecessor for articulated objects from the embodied intelligence community, such as pose estimation [46; 47], manipulation [48; 49], etc. In this section, we conduct experiments on ShapeNet part dataset .

From Tab. 2, see them all: we find that our method has almost equivalent performance to the conventional method when being equipped with PointNet, and achieves superior performance on 3DCNN and PointNet++. Treat them equally: we see that our method can often perform better in some categories (_e.g._, car, motor, rocket, etc.), these categories usually have a larger volume (_i.e._, a more sparsified point cloud) compared to other objects. We propose that the inner product within convolutional networks has a tendency to highlight local context among points, yet it is greatly affected by the overall translation and scaling of the dataset. Our method focuses on the points drawn from \(_{1}\)-norm space and addresses this problem by integrating the inherent distance measure into our architecture. Specifically speaking, The Manhattan distance based \(_{1}\)-norm Nets tend to avoid this problem, which notices point cloud features at a longer distance.

### Experiments on Semi-dense Prediction Task

**Scenario Semantic Segmentation.** As a semi-dense prediction task, this task aims to segment distinct regions within a 3D scene based on their semantic meaning using point cloud data. Semantic scene segmentation is crucial for understanding and interpreting the spatial arrangement and relationships between objects in 3D scenes. For our study, we utilize the S3DIS dataset. The metrics and experimental settings follow those outlined in .

Following the training and test strategies used in , we first divide the point cloud using the room as the basic unit and then sample the room at a size of 1m * 1m (randomly sampling up to 4096

points during training, and all points are involved in the computation during testing), which in turn predicts the class of each point in each block. Note that we use a 9-dimensional vector to represent each point, representing XYZ, RGB, and normalized room location (ranging from 0 to 1). K-fold strategy is also used for training and testing.

The quantitative results are reported in Tab. 3. Experimental results show that although our approach achieves almost equivalent performance to inner product based networks, we maximize the potential of \(_{1}\)-norm measure by relying on our proposed optimization strategy, which allows us to achieve similar performance but with less computational complexity and lower energy consumption (Almost **61%** energy reductions). Also, we provide qualitative segmentation results for visualization in Fig. 4. Overall, our model generates consistent object predictions and is resilient to the presence of absent points and obstructions.

### Experiments of Dense Prediction Task

**Garment Pose Estimation** Garments, vital in daily life, present unique challenges for machine perception and interaction due to properties like infinite degrees of freedom and thin structure. Garment pose estimation and tracking systems hold potential for applications in mixed reality [52; 53], augmented reality [54; 52], and robotic manipulation [55; 49]. Addressing these challenges, mainstream methods typically employ Normalized Object Coordinate Space (NOCS)  for **dense prediction tasks**. In this section, we introduce GarmentNets , a baseline focusing on garment pose estimation using partial point clouds as input and generating complete point clouds as output. Our approach utilizes the GarmentNets Simulation Dataset to evaluate this task. The total epoch number is 200, and the batch size is 16.

Quantitative results are in Tab. 4. Note that we use Symmetric Chamfer Distance as the metric, This metric measures accuracy and completeness for surface reconstruction. The accuracy metric is defined as the mean L2 distance of points on the output mesh to their nearest neighbors on the GT mesh. From the table, it can be seen that our method performs comparably to the original method.

### Ablation Experiments

**Replacing Means.** The most critical structure of PointNet++ is the 3 separate SetAbstractions modules (SA). Hence, to explore the effect of using the \(_{1}\)-PointNet++ at different places and in different ratios, we remove the modules at different ratios and places on S3DIS. The experimental result is shown in Tab. 5. In many aspects, we can infer that the average mean IOU and accuracy are higher under the 66.7% ratio than those reported under the 33.3% ratio. This result tells the conclusion that our \(_{1}\)-norm measure can exact more useful features from sparse point clouds. we hope these results can prompt further study on replacing means, such as different replacing ratios in

  Model & **Dress** & **Jumpsuit** & **Skirt** & **Top** & **Pants** & **Shirt** \\   GarmentNets  & 1.94 & **1.45** & 2.00 & 1.30 & 1.03 & 1.70 \\ \(_{1}\)-GarmentNets & **1.83** & 1.56 & **1.91** & **1.26** & **0.99** & **1.62** \\  

Table 4: **Quantitative Results on Garment Pose Estimation.** The metric is measured using Chamfer distance (\(cm\)) under the canonical pose. The lower is the better result.

Figure 4: (**Right) Qualitative Results for Semantic Segmentation.** We put the colored point cloud on the top part (Input data) and put semantic segmentation results from the same camera viewpoint on points (Output) in the bottom part.

each inner module, creditable ways to combine hybrid convolutional blocks. etc. We leave this for more passionate researchers in the future.

Optimization Strategy.As demonstrated in Sec. 4, we propose mixed gradient strategy (MGS) to accelerate network convergence, while dynamic learning rate controller (DLC) helps our network move away from local optima. To evaluate the effectiveness of MGS and DLC, we remove them separately from \(_{1}\)-PointNet++ and evaluate the scenario semantic segmentation performance on S3DIS. Tab. 6 presents the quantitative results. The baselines (I and V) indicate that we only use \(_{1}\)-norm as the similarity measurement but without any optimization. It can be observed that they both resulted in huge performance degradation. Besides, we can see that both our MGS and DLC contribute to network convergence and optimization results.

## 6 Limitations and Broader Impact

Firstly, some of the other convolutions (_e.g._, sparse convolution, group convolution, dilated convolution) and additional computer vision tasks remain unexplored. Secondly, the inference speed of the \(_{1}\)-norm Net is marginally slower than that of traditional one. This is attributed to the lack of CUDA and cuDNN optimized operations for Manhattan distance metrics. It's noteworthy that, beyond introducing a novel convolution based on the \(_{p}\)-norm and proving the universal approximation theorem for theoretical support, this paper also presents customized optimization strategies.

## 7 Conclusion

In this paper, we are motivated to explore \(_{p}\)-norm measure to replace the classic inner product convolution. we first prove the universal approximation of \(_{p}\)-norm Nets. And then we compare different \(_{p}\)-norm measures and propose the \(_{1}\)-norm Net for 3D point cloud tasks. Furthermore, we design the customized optimization strategies (_i.e._, mixed gradient strategy and dynamic control on learning rate) for \(_{1}\)-norm Net. When introducing our method to classical 3D networks, they achieve competitive performances at a lower energy cost. In summary, our \(_{1}\)-norm Net can achieve similar performance to traditional convolution network, but with less computational cost and lower instruction latency.