# InfoCD: A Contrastive Chamfer Distance Loss for

Point Cloud Completion

 Fangzhou Lin\({}^{1,2}\)1 Yun Yue\({}^{1}\)1 Ziming Zhang\({}^{1}\)2 Songlin Hou\({}^{1,3}\) Kazunori D Yamada\({}^{2}\)

**Vijaya B Kolachalama\({}^{4}\) Venkatesh Saligrama\({}^{4}\)**

\({}^{1}\)Worcester Polytechnic Institute, USA \({}^{2}\)Tohoku University, Japan

\({}^{3}\)Dell Technologies, USA \({}^{4}\)Boston University, USA

{flin2, yyue, zzhang15, shou}@wpi.edu, yamada@tohoku.ac.jp,

{vkola, srv}@bu.edu

###### Abstract

A point cloud is a discrete set of data points sampled from a 3D geometric surface. Chamfer distance (CD) is a popular metric and training loss to measure the distances between point clouds, but also well known to be sensitive to outliers. We propose _InfoCD_, a novel contrastive Chamfer distance loss, and learn to spread the matched points to better align the distributions of point clouds. As such _InfoCD_ leads to an improved surface similarity metric. We show that minimizing InfoCD is equivalent to maximizing a lower bound of the mutual information between the underlying geometric surfaces represented by the point clouds, leading to a _regularized_ CD metric which is robust and computationally efficient for deep learning. We conduct comprehensive experiments for point cloud completion using InfoCD and observe significant improvements consistently over all the popular baseline networks trained with CD-based losses, leading to new state-of-the-art results on several benchmark datasets. Demo code is available at https://github.com/Zhang-VISLab/NeurIPS2023-InfoCD.

## 1 Introduction

**Point Cloud Completion.** Point clouds, one of the most important data representations that can be easily acquired, play a key role in modern robotics and automation applications . However, raw data of point clouds captured by existing 3D sensors are usually incomplete and sparse due to occlusion, limited sensor resolution, and light reflection , which can negatively impact the performance of downstream tasks that require high-quality representation, such as point cloud segmentation and detection. _Point cloud completion_ refers to the task of inferring the complete shape of an object or scene from incomplete raw point clouds. Recently, many (deep) learning based approaches have been introduced to point cloud completion ranging from supervised learning, self-supervised learning to unsupervised learning . Among these methods, supervised learning with a general encoder-decoder structure is the prevailing architectural choice for many researchers, consistently achieving state-of-the-art results on mainstream benchmarks .

**Learning with Chamfer Distance (CD).** CD is a commonly employed metric in point cloud completion, for example in studies like . It assesses the dissimilarity in shape between two sets of point clouds by calculating the average distance from each point in one set to its nearest neighbor in the other set. Minimizing the Euclidean distances between matched points, as done in CD, is a known method that is sensitive to outliers, resulting in _clumping behavior_ that involvesa substantial number of many-to-one correspondences in point matching, forming small clusters visually. These observations can significantly deviate from the assumption of uniform sampling from the underlying geometric surfaces, which is commonly used to generate point clouds.

**Learning with Mutual Information (MI).** A more fundamental problem in measuring point cloud similarity (or distance alternatively) is: _How can we measure the similarities between the underlying geometric surfaces represented by the point clouds?_ To address this issue, one potential method is to compute their MI by taking each point cloud as a discrete set of point samples from a random variable following a certain distribution, and then plugging both point clouds into the MI formula to measure the similarity of the two random variables. However, two limitations prevent us from using it directly as a loss function in learning: (1) It may be nontrivial to define the joint probability distribution between two point clouds since we do not have any prior knowledge, and (2) The requirement for extensive memory, necessary for handling a large volume of data points and deep learning models, could render the practical implementation of the MI (Mutual Information) loss unfeasible.

**Our Approach and Contributions.** Motivated by contrastive learning, we propose a novel contrastive Chamfer distance loss, namely _InfoCD_, to learn to spread the matched points for better distribution alignments between point clouds as well as accounting for surface similarity estimation. Similar to InfoNCE , our InfoCD minimization tends to maximize a lower bound of the MI between the underlying geometric surfaces represented by the point clouds. This indeed leads to a regularized CD loss that can be minimized effectively and efficiently with similar computational complexity to CD.

We further illustrate our high level ideas in Fig. 1. To plot such figures, we consider generating point clouds uniformly sampled from two 1D Gaussian distributions both with unit variance but different means, one fixed at 0 and the other varying from -10 to 10 at step-size of 0.1. We then compute CD, InfoCD (see Eq. 5 with \(=1\)), the lower bound of MI (see Eq. 8), and MI where the joint probability distribution is computed based on the exponential of negative Euclidean distances. To better view the difference between the curves of CD and InfoCD, we also rescale the InfoCD curve by aligning its minimum and maximum values with those for CD based on linear scaling so that their value ranges are the same. With different numbers of samples, we can clearly see that: (1) InfoCD is more robust to outliers (_i.e.,_ the cases far away from 0 mean) than CD by penalizing larger deviations with some similar numbers (though smaller numbers of samples have more fluctuations); (2) The sharper purple curves around 0 indicate that InfoCD may lead to better convergence; (3) The lower bound based on InfoCD can approximate MI well with the maximum at 0, which indicates that InfoCD can be taken as a good MI estimator and lead to better point distributions with fewer visual clusters.

To summarize, we list our main contributions as follows:

* We propose InfoCD by introducing contrastive learning into the CD loss, leading to a regularized CD loss for better point distribution alignments.
* We analyze the connection between InfoCD and MI as well as learning behavior with InfoCD.
* We achieve state-of-the-art results on popular benchmark datasets for point cloud completion.

## 2 Related Work

**Point Cloud Completion.** As the first learning-based point cloud completion network, PCN  extracts global features in a similar way PointNet  did and generates points through folding

Figure 1: Illustration of comparison among CD, MI, and InfoCD with different numbers of samples.

operations as in FoldingNet . In order to obtain local structures among points, Zhang et al.  proposed extracting multi-scale features from different layers in the feature extraction part to enhance the performance. CDN  uses a cascaded refinement network to bridge the local details of partial input and the global shape information together. Lyu et al.  proposed treating point cloud completion as a conditional generation problem in the framework of denoising diffusion probabilistic models (DDPM) . Attention mechanisms such as Transformer , demonstrate their superiority in capturing long-range interaction as compared to CNNs' constrained receptive fields. For instance, to preserve more detailed geometry information for point cloud generation in the decoder, SA-Net  uses the skip-attention mechanism to merge local region information from the encoder and point features of the decoder. SnowflakeNet  and PointTr  pay extra attention to the decoder part with Transformer-like designs. PointAttN  was proposed solely based on Transformers.

**Distance Metrics for Point Clouds.** Distance in point clouds is a non-negative function that measures the dissimilarity between them. Since point clouds are inherently unordered, the shape-level distance is typically derived from statistics of pair-wise point-level distances based on a particular assignment strategy . With relatively low computational cost fair design, CD and its variants are extensively used in learning-based methods for point cloud completion tasks [30; 26; 31; 32]. Earth Mover's Distance (EMD), which is another widely used metric, relies on finding the optimal mapping function from one set to the other by solving an optimization problem. In some cases, it is considered to be more reliable than CD, but it suffers from high computational overhead and is only suitable for sets with exact numbers of points [33; 34]. Recently, Wu et al.  propose a Density-aware Chamfer Distance (DCD) as a new metric for point cloud completion which can balance the behavior of CD and computational cost in EMD to a certain level. Lin et al.  proposed a HyperCD that computes CD in a hyperbolic space and achieves significantly better performance than DCD.

**Contrastive Learning.** Recently, learning representations from unlabeled data in contrastive way [36; 37] has been one of the most competitive research fields [21; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50]. Popular model structures like SimCLR  and Moco  apply the commonly used loss function InfoNCE  to learn a latent representation that is beneficial to downstream tasks. Several theoretical studies show that contrastive loss optimizes data representations by aligning the same image's two views (positive pairs) while pushing different images (negative pairs) away on the hypersphere [51; 52; 53; 54]. A good survey on contrastive learning can be found in . More recently, contrastive learning has found its way into point cloud applications as well. For instance, Tang et al.  proposed a contrastive boundary learning framework for point cloud segmentation. Yang et al.  proposed the mutual attention module and co-contrastive learning for point cloud object co-segmentation. Jiang et al.  proposed a guided point contrastive loss to enhance the feature representation and model generalization ability in semi-supervised settings for point cloud segmentation. Du et al.  proposed a self-contrastive learning approach for self-supervised point cloud representation learning. Wang et al.  proposed exploring whether maximizing the mutual information across shallow and deep layers is beneficial to improve representation learning on point clouds, leading to a new design of Maximizing Mutual Information (MMI) Module. Afham et al.  proposed CrossPoint, a simple cross-modal contrastive learning approach to learn transferable 3D point cloud representations with 2D images. Shao et al.  proposed a spatial consistency guided network (SCRnet) using contrastive learning for point cloud registration.

## 3 InfoCD Loss

### Preliminaries

**InfoNCE Loss.** In , the InfoNCE loss is defined as follows:

\[_{}=-_{x}\{s(x^{+},x;)\}}{\{s(x^{+},x;) \}+_{x^{-}}\{s(x^{-},x;)\}} \},\] (1)

where \(x,x^{+},x^{-}\) denote the anchor, its positive and negative samples, \(s\) denotes a similarity function parametrized by \(\), and \( 0\) is a predefined temperature that controls the sharpness.

**Proposition 1** (InfoNCE _vs._ MI ).: _Let \(c_{t}\) be the context at the \(t\)-th time step, and \(x_{t+k}\) be the future target. Then given a set of \(N\) random samples, \(\{x_{1},,x_{N}\}\), containing one positive sample from the distribution \(p(x_{t+k}|c_{t})\) and \(N-1\) negative samples from the "proposal" distribution \(p(x_{t+k})\), we have_

\[I(x_{t+k},c_{t})(N)-_{},\] (2)

_where \(I\) denotes the mutual information (MI) operator._

Please refer to the appendix in  for the proof. This proposition provides us with an alternative way to measure MI approximately and implicitly. If point clouds can fit into the setting of InfoNCE, we then may better estimate the underlying surface similarities from point clouds.

**Chamfer Distance Loss.** In the sequel, we denote \((x_{i},y_{i})\) as the \(i\)-th point cloud pair, with \(x_{i}=\{x_{ij}\}\) and \(y_{i}=\{y_{ik}\}\) as two sets of 3D points, and \(d(,)\) as a certain distance metric. Then the CD loss for point clouds can be defined as follows:

\[_{}(x_{i},y_{i})=_{}(x_{i},y_{i})+_{ {CD}}(y_{i},x_{i})=|}_{k}_{j}d(x_{ij},y_{ik})+|}_{j}_{k}d(x_{ij},y_{ik}),\] (3)

where \(||\) denotes the cardinality of a set. For point cloud completion, function \(d\) usually refers to

\[d(x_{ij},y_{ik})=\{\|x_{ij}-y_{ik}\|&}\\ \|x_{ij}-y_{ik}\|^{2}&}.\] (4)

where \(\|\|\) denotes the Euclidean \(_{2}\) norm of a vector.

### Our Loss Function

Given the considerations above, we propose the following formula as our InfoCD loss:

\[_{}(x_{i},y_{i}) =_{}(x_{i},y_{i})+_{}(y_{i},x_{ i}),\] \[_{}(x_{i},y_{i}) =-|}_{k}\{_{j}d(x_{ij},y_{ik})\}}{_{k}\{-_{j}d(x_{ij}, y_{ik})\}}\}\] \[|}_{k}_{j}d(x_{ij},y_{ik})+ \{_{k}\{-_{j}d(x_{ij},y_{ik}) \}\}\] (5) \[=-|}_{k}\{_{j}d(x_{ij},y_{ik})\}}{[_{k}\{-_{j}d( x_{ij},y_{ik})\}]^{}}\}\] (6) \[_{}(x_{i},y_{i}) _{}(x_{i},y_{i})+ (x_{i},y_{i})\] (7)

with \((x_{i},y_{i})=\{_{m,n}\{-[ _{j}d(x_{ij},y_{in})+_{k}d(x_{im},y_{ik})]\}\}\) as a regularizer and \(=}{}(0,1]\) as a predefined constant controlling the trade-off between the loss and the regularizer. The smaller \(_{}(x_{i},y_{i})\) is, the larger \((x_{i},y_{i})\) is accordingly. From this perspective, we can easily see that our InfoCD loss is equivalent to a regularized CD loss.

### Analysis

**InfoCD _vs._ MI.** To see the connections between InfoCD and MI, we have the following lemma:

**Lemma 1**.: _Consider two point clouds \(x_{i}=\{x_{ij}\},y_{i}=\{y_{ik}\}\) representing two underlying geometric surfaces \(_{i},_{i}\). Now we introduce a new random variable \(z_{y_{ik}}\) whose probability distribution \(p(z_{y_{ik}}|_{i}=_{i})\) indicates how likely a point \(y_{ik}\) can be sampled from \(_{i}\) conditional on \(_{i}=_{i}\), and "proposal" distribution \(p(z_{y_{ik}})\) indicates the likelihood of generating an arbitrary \(y_{ik}\). With these notations, we will have_

\[I(z_{y_{ik}};_{i}=_{i})(|y_{i}|)-_{}(x_{i},y_{i}).\] (8)

Proof.: By following Prop. 1 and the proof in , we can take \(z_{y_{ik}},_{i}=_{i}\) as the replacements for \(x_{t+k},c_{t}\). Then samples from \(p(z_{y_{ik}}|_{i}=_{i})\) will be "positive", and ones from \(p(z_{y_{ik}})\) will be "negative" in the language of contrastive learning. Now we can construct \(|y_{i}|\) groups, as illustrated in Fig. 2, where each group consists of the entire point cloud \(x_{i}\) and a point in \(y_{i}\). We further can take an arbitrary group as a positive and the rest as negatives. Finally, by parametrizing \(}|_{i}=_{i})}{p(z_{}})} \{-}_{j}d(x_{ij},y_{ik})\}\) for positives and similarly to negatives, we can complete our proof. 

Therefore, based on this lemma, minimizing InfoCD tends to better estimate the lower bound of MI that indicates the underlying surface similarities between point clouds.

**Point Spread in Learning & Testing.** To better understand the behavior of different losses, we first introduce another lemma as follows:

**Lemma 2**.: _Consider an optimization problem \(_{}_{i}g(h(x_{i};))\) where \(h:,g:\) are both Lipschitz continuous functions, \(h\) is also smooth over \(\), and \(x_{i}\) is a data point. Then based on gradient descent, i.e., \(_{t+1}=_{t}-_{t}_{i} g(h(x_{i};_{t}))\) with a learning rate \(_{t} 0\) at the \(t\)-th iteration and a gradient operator \(\), it holds that given a new data point \(\),_

\[h(;_{t})-h(;_{t+1})_{i}_{t}. |_{(x_{i};_{t})} h(x_{i}; _{t})^{T} h(;_{t})=_{t}_{i}.|_{(x_{i};_{t})}(x_{i},; _{t}),\] (9)

_where \(_{0}\) is the initialization of \(\), \(\) denotes a (neural) tangent kernel function parametrized by \(_{t}\), \(()^{T}\) is the matrix transpose operator, and \(.|_{(x_{i};_{t})}\) is the derivative of \(g\) over \(h\) at point \((x_{i};_{t})\)._

Proof.: Using the linear approximation of \(h\) and the assumptions, we can easily prove this lemma. 

To connect this lemma with CD and InfoCD, let us first compute the gradients of \(_{}\) and \(_{}\):

\[_{}(x_{i},y_{i})=_{k}|} d(x_{ik^{ }},y_{ik}),\] (10)

\[_{}(x_{i},y_{i})=_{k}[ {|y_{i}|}-^{}d(x_{ik^{}},y_{ik})\}} {_{k}\{-d(x_{ik^{}},y_{ik})\}}] d(x_{ ik^{}},y_{ik}),\] (11)

where \(k^{}=_{j}d(x_{ij},y_{ik}), k\). By viewing \(a\) as \(h\) and \(_{}\) (or \(_{}\)) as \(g\), we can see the weight \((x_{ik^{}},y_{ik};_{t})}{ d}=|}>0\) but \(}(x_{ik^{}},y_{ik};_{t})}{  d}=|}-^{}d(x_{ik^{ }},y_{ik})\}}{_{k}\{-d(x_{ik^{}},y_{ik})\}} \).

To simplify the analysis and explanation, assuming that the normalized gradients, \(}(,)}{\| d(,)}\), can be viewed as random samples from a high dimensional (_i.e.,_ the number of network parameters that is much larger than the number of samples in mini-batches) normal distribution, then for two different random inputs \((x_{im^{}},y_{im}),(x_{in^{}},y_{in})\), it will be expected  that the corresponding gradients will be close to being orthogonal to each other, _i.e., \( d(x_{im},y_{ik})^{T} d(x_{in},y_{ik}) 0\)_. Now by substituting this assumption into Eq. 9, we can have

\[_{}(x_{i},y_{i};_{t})-_{}(x_{i},y_{i};_{t +1})_{t}_{k}}(x_{ik^{}},y_{ ik};_{t})}{ d}\| d(x_{ik^{}},y_{ik})\|^{2},\] (12)

\[_{}(x_{i},y_{i};_{t})-_{}(x_{i},y_{i}; _{t+1})}{}_{k}}(x_{ik^{}},y_{ik};_{t})}{ d}\| d(x_{ik^{}},y_{ ik})\|^{2},\] (13)

with another assumption that the matched point pairs keep unchanged over iterations. From this perspective, we can easily see that the _negative_ weights of \(}(x_{ik^{}},y_{ik};_{t})}{ d}\) with smaller distances will push the predicted points away from the matched ground-truth points, while _positive_ weights will tend to reduce the distances in both metrics, as illustrated in Fig. 3. In this contrastive way, the predicted points will be more likely to be spread for better alignment with the ground-truth points.

Figure 2: Illustration of group construction process for MI estimation.

To demonstrate the capability of our InfoCD loss to spread the matched points at test time, we illustrate some comparison results in Fig. 4, where we can see clearly that InfoCD can significantly reduce the numbers of many-to-one matched points, leading to better point distribution alignments.

**Convergence.** In general, there is no guarantee that training networks with InfoCD using (stochastic) gradient descent will converge if the matched point pairs between the predictions and ground truth are frequently changed. However, empirically we observe that such training stabilization can be efficiently reached using different networks on different datasets as well. For instance, as illustrated in Fig. 5(a)3, InfoCD with L1-distance in Eq. 4 behaves similarly in terms of convergence rate to L1-CD that has convergence guarantee, but reduces the loss more significantly.

As illustrated in Fig. 5(b), ideally CD aims to reach the status where all the predicted points will be aligned perfectly with ground truth with no errors by consistently minimizing the distances. Such a requirement may be so strict that in learning the optimization may be much easier to be stuck at the suboptimal solutions, _e.g.,_ forming clusters around some points. In contrast, InfoCD aims to align the point distributions with sufficiently small errors, as illustrated in Fig. 5(c) with the dotted lines indicating a Voronoi diagram for the ground-truth point cloud. This is much less strict than CD, and thus will be more likely to locate better solutions. In fact, the movements of predicted points in Fig. 3(b) provide an effective way towards learning such an alignment in Fig. 5(c) by avoiding bad suboptimal solutions.

**Choice of \(d\) in InfoCD.** In particular, we utilize L1-distance in Eq. 4 as the choice for \(d\) in InfoCD, because it is unbiased to the distance, _i.e.,_\( d(x_{ij},y_{ik})=\|x_{ij}-y_{ik}\|\). In practice, we observe that using \(d(x_{ij},y_{ik})=\|x_{ij}-y_{ik}\|^{p},p>0,p 1\), the performance of InfoCD is very unstable for different networks

Figure 4: Comparison on point percentage _vs._ the number of matches per point using CP-Net  on ShapeNet-Part  dataset, where the CP-Net is trained with five different loss functions. Clearly, InfoCD has better point spread and thus distribution alignments.

Figure 5: Illustration of **(a)** training loss using CP-Net on ShapeNet-Part, and ideal point alignment with **(b/c)** CD/InfoCD.

Figure 3: Illustration of the moving directions of matched points using **(a)** Chamfer distance or **(b)** InfoCD. Blue: ground truth; Red: predictions.

across different datasets. This is understandable: in the extreme cases where \(\|x_{ij}-y_{ik}\|=0\), \(p>1\) would make the predicted point unchanged, while \(0<p<1\) would make the move of the predicted point very far away. Such updates will violate the goal of InfoCD as shown in Fig. 5(c). Other distance metrics will be investigated in our future work.

## 4 Experiments

**Datasets.** We conducted experiments for point cloud completion on the following datasets:

* _PCN :_ This is a subset of ShapeNet  with shapes from 8 categories. The incomplete point clouds are generated by back-projecting 2.5D depth images from 8 viewpoints in order to simulate real-world sensor data. For each shape, 16,384 points are uniformly sampled from the mesh surfaces as complete ground truth, and 2,048 points are sampled as partial input [10; 8].
* _Multi-view partial point cloud (MVP) :_ This dataset covers 16 categories with 62,400 and 41,600 pairs for training and testing, respectively. It renders the partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model selected from ShapeNet , and the ground-truth point cloud is sampled via Poisson Disk Sampling (PDS).
* _ShapeNet-55/34 :_ ShapeNet-55 contains 55 categories in ShapeNet with 41,952 shapes for training and 10,518 shapes for testing. ShapeNet-34 uses a subset of 34 categories for training and leaves 21 unseen categories for testing where 46,765 object shapes are used for training, 3,400 for testing on seen categories and 2,305 for testing on novel (unseen) categories. In both datasets, 2,048 points are sampled as input and 8,192 points as ground truth. Following the same evaluation strategy with , 8 fixed viewpoints are selected and the number of points in the partial point cloud is set to 2,048, 4,096 or 6,144 (25\(\%\), 50\(\%\) or 75\(\%\) of a complete point cloud) which corresponds to three difficulty levels of _simple_, _moderate_ and _hard_ in the test stage.
* _ShapeNet-Part :_ This is a subset of ShapeNetCore  3D meshes, containing 17,775 different 3D meshes with 16 categories. The ground-truth point clouds were created by sampling 2,048 points uniformly on each mesh. The partial point clouds were generated by randomly selecting a viewpoint as a center among multiple viewpoints and removing points within a certain radius from the complete data. The number of points we remove from each point cloud is 512.

**Implementation.** We considered three state-of-the-art networks, CP-Net , PointAttN  and SeedFormer , as our backbone networks for comparison and analysis. We also applied InfoCD to almost all the popular completion networks, _i.e._, PCN , FoldingNet , TopNet , MSN , Cascaded , VRC , PMPNet , PoinTr , SnowflakeNet , to verify its performance by replacing the original CD loss wherever it occurs. We performed the same replacement for all the other comparative losses in our experiments. We trained all these networks from scratch using PyTorch, optimized by either Adam  or AdamW . Hyperparameters such as learning rates, batch sizes and balance factors in the original losses for training baseline

   Methods & Plane & Cabinet & Car & Chair & Lamp & Couch & Table & Boat & Avg. \\  TopNet  & 7.61 & 13.31 & 10.90 & 13.82 & 14.44 & 14.78 & 11.22 & 11.12 & 12.15 \\ AtlasNet  & 6.37 & 11.94 & 10.10 & 12.06 & 12.37 & 12.99 & 10.33 & 10.61 & 10.85 \\ GRNet  & 6.45 & 10.37 & 9.45 & 9.41 & 7.96 & 10.51 & 8.44 & 8.04 & 8.83 \\ CRN  & 4.79 & 9.97 & 8.31 & 9.49 & 8.94 & 10.69 & 7.81 & 8.05 & 8.51 \\ NSFA  & 4.76 & 10.18 & 8.63 & 8.53 & 7.03 & 10.53 & 7.35 & 7.48 & 8.06 \\ FBNet  & 3.99 & 9.05 & 7.90 & 7.38 & 5.82 & 8.85 & 6.35 & 6.18 & 6.94 \\  PCN  & 5.50 & 22.70 & 10.63 & 8.70 & 10.10 & 11.34 & 11.68 & 8.59 & 11.27 \\ HyperCD  + PCN & 5.95 & **11.62** & **9.32** & 11.45 & 12.58 & 13.10 & **9.82** & **9.59** & **10.59** \\
**InfoCD + PCN** & **5.07** & 22.27 & 10.18 & **8.26** & **10.57** & **10.98** & 11.23 & **8.15** & 10.83 \\  FoldingNet  & 9.49 & 15.80 & 12.61 & 15.55 & 16.41 & 15.97 & 13.65 & 14.99 & 14.31 \\ HyperCD+ FoldingNet & **7.89** & 12.90 & **10.67** & **14.55** & **13.87** & **14.09** & 11.86 & **10.89** & **12.09** \\
**InfoCD+FoldingNet** & 7.90 & **12.68** & 10.83 & **10.44** & 14.05 & 14.56 & **11.61** & **11.45** & 12.14 \\  PMP-Net  & 5.65 & 11.24 & 9.64 & 9.51 & 6.95 & 10.83 & 8.72 & 7.25 & 8.73 \\ HyperCD+ PMP-Net & 5.06 & 10.67 & 9.30 & 9.11 & 6.83 & 11.01 & 8.18 & 7.03 & 8.40 \\
**InfoCD+PMP-Net** & **4.67** & **10.09** & **8.87** & **8.59** & **6.38** & **10.48** & **7.51** & **6.75** & **7.92** \\  PoinTr  & 4.75 & 10.47 & 8.68 & 9.39 & 7.75 & 10.93 & 7.78 & 7.29 & 8.38 \\ HyperCD+ PoplTr & 4.42 & 9.77 & 8.22 & 8.22 & 6.62 & 9.62 & 6.97 & 6.67 & 7.56 \\
**InfoCD+PoinTr** & **4.06** & **9.42** & **8.11** & **7.84** & **6.21** & **9.38** & **6.57** & **6.40** & **7.24** \\  SnowflakeNet  & 4.29 & 9.16 & 8.08 & 7.89 & 6.07 & 9.23 & 6.55 & 6.40 & 7.21 \\ HyperCD + SnowflakeNet & **3.95** & 9.01 & 7.88 & 7.37 & **5.75** & 8.94 & **6.19** & 6.17 & 6.91 \\
**InfoCD + SnowflakeNet** & 4.01 & **8.81** & **7.62** & **5.51** & 5.80 & **8.91** & 6.21 & **5.05** & **6.86** \\  PointAttN  & 3.87 & 9.00 & 7.63 & 7.43 & 5.90 & 8.68 & 6.32 & 6.09 & 6.86 \\ HyperCD+ PointAttN & 3.76 & 8.93 & 7.49 & 7.06 & 5.61 & 8.48 & 6.25 & **5.92** & 6.68 \\
**InfoCD + PointAttN** & **3.72** & **8.87** & **7.46** & **7.02** & **5.60** & **8.45** & **6.23** & **5.92** & **6.65** \\  SeedFormer  & 3.85 & 9.05 & 8.06 & 7.06 & 5.21 & 8.85 & 6.05 & 5.85 & 6.74 \\ HyperCD + SeedFormer & 3.72 & **8.71** & 7.79 & **6.83** & 5.11 & **8.61** & **5.82** & 5.76 & 6.54 \\
**InfoCD + SeedFormer** & **3.69** & 8.72 & **7.68** & 6.84 & **5.08** & **8.61** & 5.83 & **5.75** & **6.52** \\   

Table 1: Comparison on PCN in terms of per-point L1-CD \( 1000\).

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

measuring the underlying geometric surfaces of the point clouds using mutual information (MI) estimation. In particular, we discuss and analyze the relations between InfoCD and MI, the moves of predicted points in learning, training convergence, loss landscapes, and the choice of distance metric in InfoCD. Comprehensive experiments have been conducted to demonstrate its effectiveness and efficiency using 7 networks on 5 datasets, leading to new state-of-the-art results.

**Limitations.** Due to the introduction of a new hyper-parameter \(\) in InfoCD, tuning hyperparameter based on grid search may need more effort, as illustrated in Fig. 7. Also due to the higher nonconvexity of InfoCD than CD, it may take more time (or epochs) to train complicated networks (_e.g.,_ with large numbers of parameters, or architectures) with InfoCD.