# Semidefinite Relaxations of the Gromov-Wasserstein Distance

Junyu Chen

Equal contribution.

Binh T. Nguyen

Equal contribution.

Shang Hui Koh

Yong Sheng Soh

Department of Mathematics

National University of Singapore

chenjunyu@u.nus.edu,binhnt@nus.edu.sg,matsys@nus.edu.sg

###### Abstract

The Gromov-Wasserstein (GW) distance is an extension of the optimal transport problem that allows one to match objects between incomparable spaces. At its core, the GW distance is specified as the solution of a non-convex quadratic program and is not known to be tractable to solve. In particular, existing solvers for the GW distance are only able to find locally optimal solutions. In this work, we propose a semi-definite programming (SDP) relaxation of the GW distance. The relaxation can be viewed as the Lagrangian dual of the GW distance augmented with constraints that relate to the linear and quadratic terms of transportation plans. In particular, our relaxation provides a tractable (polynomial-time) algorithm to compute globally optimal transportation plans (in some instances) together with an accompanying proof of global optimality. Our numerical experiments suggest that the proposed relaxation is strong in that it frequently computes the globally optimal solution. Our Python implementation is available at https://github.com/tbng/gwsdp.

## 1 Introduction

The optimal transport (OT) problem concerns the task of finding a transportation plan between two probability distributions to minimize some costs. The problem has applications in a wide range of scientific and engineering applications. For instance, in the context of machine learning, the OT problem forms the backbone of recent breakthroughs in generative modeling (Arjovsky et al., 2017; Liu et al., 2022; Lipman et al., 2022), natural language processing (Kusner et al., 2015), domain adaptation (Courty et al., 2017), and single-cell alignment (Schiebinger et al., 2019; Bunne et al., 2023, 2024).

Let \(_{m}\) and \(_{m}\) be discrete probability distributions over a metric space - here \(_{m}:=\{_{+}^{m},_{i=1}^{m}_{i}=1\}\) denotes the probability simplex. Let \(C^{m n}\) be the matrix such that \(C_{i,j}\) models the transportation cost between points \(x_{i}\) and \(y_{j}\). The (Kantorovich) formulation of the discrete OT problem (Kantorovich, 1942; Villani et al., 2009; Santambrogio, 2015; Peyre et al., 2019) is defined as the solution of the following convex optimization instance

\[_{}}}{{=}}*{ argmin}_{(,)} C,.\] (1)

Here, \((,)=\{_{+}^{m n}:_{n}=, ^{}_{m}=\}\) denotes the set of couplings between probability distributions \(,_{m}\), while \(_{m}^{m}\) denotes the vector of ones. The OT problem (1) is an instance of a linear program (LP), and hence admits a global minimizer.

One limitation of the classical OT formulation in (1) is that the definition of the cost matrix \(C\) requires the probability distributions \(\) and \(\) to reside in the same metric space. This is problematic in application domains where we wish to compare probability distributions in different spaces, in which case there is no meaningful way to describe the cost of moving from one location to another. Such settings arise frequently in shape comparison and graph matching applications, for example.

To address such scenarios, the work of Memoli (2011) formulates an extension of the OT problem known as the Gromov-Wasserstein distance (GW) whereby one can define an analogous OT problem given knowledge of the cost matrices for the respective spaces where \(\) and \(\) reside in. More concretely, let the tuple \((C,)^{m m}_{m}\) denote a discrete metric-measure space. Given a smooth, differentiable function \(:\), the Gromov-Wasserstein distance between two discrete metric-measure spaces \((C,)\) and \((D,)\) is defined by

\[(C,D,,)}}{{=}}\ _{(,)}(C_{i,k},D_{j,l})_{i,j}_{k,l}=\ _{(,)}(C,D),.\] (GW)

Here, the transportation cost is specified by the four-way tensor that measures the discrepancy between the metrics \(C\) and \(D\)

\[(C,D)_{i,j,k,l}}}{{=}}(C_{i,k},D_{j,l}).\] (2)

The squared loss error, for instance, is a common choice. Following Peyre et al. (2016), we define the tensor-matrix multiplication by

\[[]_{i,j}}}{{=}}_{ k,l}_{i,j,k,l}_{k,l}.\]

The GW distance has been applied widely to machine learning tasks, most notably on graph learning (Vayer et al., 2019; Xu et al., 2019; Vincent-Cuz et al., 2021, 2022). It is an instance of a quadratic program (QP) - these are optimization instances in which we minimize a quadratic objective subject to some linear inequalities. To see this, one can re-write the objective in (GW) in terms of vectorized matrices

\[_{}\ (),L\ ()\ \ \ \ (,).\] (GW+)

Here, \((L_{ij,kl})_{ij,kl}^{mn mn}\) denotes the flattened 2-dimension tensor of \(\), while the vectorization of a matrix \(^{m n}\) is given by

\[()}}{{=}}[_{11},_{21}, ,_{m1},,_{mn}]^{}^{mn}.\]

The constraint \((,)\) is convex, and in fact linear. On the other hand, the matrix \(L\) need not be positive semidefinite, and as such, the QP instance in (GW+) is non-convex in general. In fact, in the cases where the matrix \(L\) arises as the difference of cost matrices (2), \(L\) is _never_ positive semidefinite as these are zero on the diagonal while the off-diagonal entries are non-negative.

## 2 Main Contributions

The main contribution of this work is to propose a strong semidefinite programming (SDP)-based relaxation for the Gromov-Wasserstein distance that leads to globally optimal solutions in many instances. Concretely, let \((_{sdp},P_{sdp})\) denote an optimal solution to the following

\[(C,D,,) }}{{=}}\ _{ ^{m n},\\ P^{mn mn}}\ \  L,P\] s.t. \[P&()\\ ()^{}&1 0\] (GW-SDP) \[(,)\] \[P(e_{i}_{n}^{})=_{i}( ),i[m]\] \[P(_{m}e_{j}^{})=_{j}( ),j[n]\] \[P 0\]

Here, \(e_{i}\) denotes the standard basis vector whose \(i\)-th entry is \(1\). This relaxation can be viewed as the Lagrangian dual of the GW problem augmented with constraints that relate the linear and quadratic terms of transportation plans (we discuss these aspects in greater detail in Appendix B). A simpler way to express the condition \(P(e_{i}_{n}^{})=_{i}()\) is to note that

\[P(e_{i}_{n}^{})=_{i}( )  _{j}P_{(i,j),(k,l)}=_{i}_{k,l},\] \[P(_{m}e_{j}^{})=_{j}()  _{i}P_{(i,j),(k,l)}=_{j}_{k,l}.\]

We begin by noting a basic observation: Let \((,)\) be a transportation plan. Then the tuple \((,P)=(,()()^{})\) is a feasible solution to (GW-SDP) since \(()()^{}(e_{i}_{m}^{ })=(),e_{i}_{m}^{}=()_{1},e_{i}=_{i}()\). The inequalities for \(\) follow analogously. This implies that the optimal value of (GW-SDP) is a lower bound to the GW problem (GW): Let \(^{}\) denote an optimal solution to the GW problem (note: this is (GW), which is equivalent to (GW+)). By recalling that the tuple \(((^{}),(^{})(^{}) ^{})\) is a feasible solution to (GW-SDP), one has

\[ P_{sdp},L(^{}),L\;( ^{})=^{},^{}.\] (3)

The inequality (3) provides us with a principled way of _certifying_ global optimality of a given transportation plan. Let \((,)\) be an arbitrary transportation plan. A natural approach to quantify the quality of \(\) is to compare its objective value with the optimal choice:

\[()\;:=\;}{ ^{},^{}}.\]

This ratio is at least one and is equal to one if \(\) is also globally optimal. A consequence of (3) is the following upper bound

\[(_{sdp}),_ {sdp}}{ P_{sdp},L}.\] (4)

Note that all the quantities in the RHS can be computed efficiently as the solution of a SDP. Suppose we are able to do so, and in the process evaluate the RHS to be equal to one. _Then, we have a proof that \(_{sdp}\) is the global optimal solution to the GW problem._ In a recent work that appeared during the reviewing process of our work, the GW problem is shown to be intractable in general (Kravtsova, 2024). What our discussion shows is that it is possible, in some instances, to obtain the globally optimal solution via a polynomial-time algorithm by solving (GW-SDP), and with a guarantee that the obtained solution is indeed globally optimal. In fact, the instances for which one can obtain an upper bound equal to one using (GW-SDP) is not as far-fetched as one might think: our numerical experiments in Section 4 show that this happens quite often, and especially so whenever \(m=n\). **No restrictions on cost tensor.** One of the strengths of our proposed SDP relaxation is that it is valid for _all_ cost tensors \(\). This stands in contrast with other methods like the entropic GW (Peyre et al., 2016), which is only applicable to the cost that can be decomposed to a specific form such as the \(_{2}\) or discrete KL loss.

## 3 SDP Relaxations of QPs

We motivate the relaxation in (GW-SDP). The starting point is to recognize that the GW problem is an instance of a QPs - these are optimization instances of the form

\[_{x^{n}} x^{}Ax+2b^{}x+c Bx  d.\] (5)

QPs are an important class of optimization problems. If the matrix \(A\) is PSD, then the objective is convex, and the QP instance can be solved tractably using standard software (Nocedal and Wright, 2006). The problem becomes difficult if \(A\) contains negative eigenvalues. The general class of QPs is NP-hard; for instance, it contains the problem of finding the maximum clique of a graph (Motzkin and Straus, 1965). In fact, the presence of a _single_ negative eigenvalue in \(A\) is sufficient to make the class of QPs NP-hard (Pardalos and Vavasis, 1991). The typical approach to solving a quadratic program exactly is via a branch-and-bound type of algorithm. Other approaches include relating QPs to the class of co-positive programming, mixed integer linear programming, and deploying SDP relaxations (Bomze and de Klerk, 2002) - typically, these methods are used as a sub-routine within a branch-and-bound procedure.

**Standard SDP Relaxation.** The first step of SDP relaxation is to express the quadratic terms with a PSD matrix whose rank is one. Concretely, the QP instance (5) is equivalent to the following:

\[_{x^{n},X^{n n}} (AX)+2b^{}x+c\] s.t. \[Bx d\].

This optimization instance is not convex because of the rank-one constraint. The second step is to simply omit the rank constraint, which yields a semidefinite program and therefore is convex. This is the standard SDP relaxation for QPs (the technique applies more generally to quadratically constrained quadratic programs - QCQPs).

By applying the same sequence of steps to (GW+), the standard SDP relaxation one arrives at is the following:

\[_{^{m n},P^{m mn}}  L,P\] s.t. \[P&()\\ ()^{}&1 0\] (6) \[(,)\]

Problem (6) is a tractable convex semidefinite programming, which can be efficiently solved in polynomial time. If the solution to (6) (and the subsequent SDP relaxations we introduce) has a rank equal to one, we would have solved the original GW problem (GW). Unfortunately, the feasible region of \(P\) in (6) is not compact, and the optimal value to (6) is unbounded below in general.

**Proposition 3.1**.: _The optimization instance (6) is unbounded below._

**Tightening the Relaxation.** As such, it is necessary to augment (6) with additional constraints to further strengthen the relaxation. Recall that the relaxation (6) is exact if \(P=()()^{}\). Therefore, a simple way to improve the relaxation is to add any linear constraints that is satisfied by solutions of the form \((,P)=(,()()^{})\).

First, \( 0\) for all \((,)\), and hence \(()()^{} 0\). This means we may freely impose

\[P 0.\] (Nng.)

Second, note that \(_{i}_{ij}_{kl}=_{kl}(_{i}_{ij})=_{kl}_{j}\). Subsequently, we may impose \(_{i}P_{(i,j),(k,l)}=_{j}_{kl}\). This leads to the following set of equalities:

\[P(e_{i}_{n}^{})=_{i}(),i[m],  P(_{m}e_{j}^{})=_{j}(),j [n].\] (Mar.)

The proposed SDP relaxation (GW-SDP) is precisely (6) with the additional constraints (Nng.) and (Mar.). In addition, the set of matrices \(P\) satisfying (Mar.) have trace at most one. Hence the feasible region is a subset of PSD matrices with trace at most one, which is compact.

**Relation to the QAP.** We point out that the constraints (Nng.) and (Mar.) have been previously proposed for a different but closely related problem known as Quadratic Assignment Problem (QAP) (Dym et al., 2017; Kezurer et al., 2015; Zhao et al., 1998). Mathematically, the QAP problem can be viewed as equivalent to (GW+) but with the additional restriction that \(m=n\) and that \(\) is a _permutation_ matrix. The work in Zhao et al. (1998) proposes a SDP relaxation that is effectively equivalent to (GW-SDP) but with additional linear equalities implied by orthogonality \(^{}=^{}=I\). The works in Dym et al. (2017); Kezurer et al. (2015) subsequently build on the ideas in Zhao et al. (1998) and propose more scalable alternatives while providing tight relaxations.

The key difference between the QAP and the GW problem we investigate is that \(\) is not necessarily a permutation matrix in the GW problem, and necessarily so if \(m n\). As such, the relaxation in Zhao et al. (1998) is invalid. Our contribution is to recognize that, by omitting the constraints corresponding to orthogonality, one obtains an SDP relaxation that now becomes valid for the GW problem, which leads to good practical performance.

**No need for rounding.** One important property of the relaxation in (GW-SDP) is that the output will always be a feasible transportation plan in \((,)\). This means that no additional roundingis necessary. This is vastly different from combinatorial optimization problems including the QAP where the optimal solution to the relaxation is not guaranteed to be a feasible solution, and additional rounding steps may be necessary.

## 4 Numerical Experiments with Off-the-shelf Convex Solvers

In this section, we implement our proposed SDP relaxation using an off-the-shelf solver. We compare our method with the Conditional Gradient (CG-GW) solver for finding local solutions (Vayer et al., 2019), and the Sinkhorn projections solver for computing solutions to the entropic GW (eGW) problem by Peyre et al. (2016). Both of the latter are implemented in the Python Optimal Transport library (PythonOT, Flamary et al., 2021). The goal is to show that our proposed SDP relaxation frequently computes the global optimal transportation plan whereas existing methods frequently do not.

In what follows, we will use the 2-Gromov-Wasserstein distance, _i.e._ the cost function is squared Euclidean norm. We solve the GW-SDP instance implemented in CVXPY (Diamond and Boyd, 2016) using the SCS and MOSEK solvers (ApS, 2022; O'Donoghue et al., 2016).

### Matching Gaussian Distributions

In this example, we estimate the GW distance between two Gaussian point clouds, one in \(^{2}\), and the other in \(^{3}\). A visualization of this dataset can be found in Figure 0(a). The classical optimal transport formulation such as the likes of Wasserstein-2 distance does not apply because the two point clouds belong to different spaces.

As seen in a qualitative demonstration of Figure 0(b), our algorithm returns optimal transport plans that are as sparse as the transportation plans obtained via the Conditional Gradient descent solver of Python OT for GW distance (CG-GW). We also vary the number of sample points and calculate the value of the objective function \(,\). As shown in Figure 1(a), the transport plans obtained by (GW-SDP) consistently returns smaller objective value (orange line) than those obtained via the GW-CG counterpart from PythonOT (blue line) and its entropic regularization (green line). This shows that the transport plans computed by PythonOT, for instance, are in fact frequently sub-optimal.

In Figure 1(b), we plot the estimated approximation ratio across different numbers of sample points. We notice that in this scenario of Gaussian matching, the estimated approximation ratio is close to 1.0 in most instances - this tells us that the (GW-SDP) frequently computes globally optimal transportation plans. In contrast, local methods such as PythonOT often do not. Note that we also observed the sparsity of the SDP-GW transport plans in this varying scenario.

**Scenario where \(\).** The bulk of our experiments focus on the setting where \(m=n\). We performed an experiment where \(m n\): the number of samples in one distribution is fixed (\(n=8\)) and we vary the number of samples \(m\) in the other distribution. From our results in Figure 3, we notice that the relaxation is exact whenever \(m\) is a multiple of \(n\). On the other hand, when \(m\) is not a multiple of \(n\), we still observe exactness, but much less frequently.

Figure 1: **Left: source distribution (2D, blue dots) and target distribution (3D, red dots). For ease of visualization, we lift the source \(^{2}\) mm-spaces into target \(^{3}\) by padding the third coordinate to zero. Right: OT solutions of GW-SDP (our algorithm), CG-GW (conditional gradient descent, default solver of PythonOT) and entropic OT solver. The OT plans from GW-SDP is almost sparse in the same manner to CG-GW, while the eGW is not.**

Runtime ComparisonsTable 1 presents the run-time of the GW-SDP problem in Experiment 1, running on a PC with 8 cores CPU and 32GB of RAM. In these experiments, the cost matrix \(C\) is pre-computed (i.e. assumed given). As such, the run-time is independent of the data dimension. The GW-SDP has a matrix of dimension \(mn mn\), which is slower than most local and entropic solvers. However, the solvers we implement (SCS and MOSEK) are off-the-shelf and are general SDP solvers that do not exploit special structures in the problem and do not provide options to use initialization of the transport plans. (SCS is a first-order method, but we are otherwise unaware of its complexity). We want to emphasize that in most settings where SDPs are applied, one will always try to develop specialized solvers that exploit the structure of the problem. In our setup, the optimal solution has low rank, and is rank-one if the relaxation is tight. There are numerous well-established methods for exploiting such structure. This is the subject of ongoing work.

Comparisons of GW-SDP solver and GW-CG solver when number of sample points \(\) increase.We increase the number of samples for GW-CG (non-convex GW solver using conditional gradient descent or Frank-Wolfe algorithm) vs our GW-SDP solver for a fixed number of samples. From Table 2, we noticed that the objective value for GW-CG decreases as we increase the number of samples. For 100000 sample points, the GW-CG algorithm is more expensive and has a poorer

   n & **GW-SDP** & **GW-CG** & **eGW** (\(=0.1\)) \\ 
6 & 0.2437 (0.0265) & 0.0005 (0.000041) & 0.226 (0.1145) \\
12 & 11.615 (2.4088) & 0.0006 (0.00003) & 0.2596 (0.0726) \\
20 & 216.3645 (14.1123) & 0.0014 (0.000017) & 0.4923 (0.1500) \\   

Table 1: Average run-time in seconds for experiment in Figure 0(a) (matching Gaussians with varying number of samples \(n\)).

Figure 3: Gaussian Matching experiments with the sample points \(m\) from source distribution varying while the sample points from target distribution keeping fixed \(n=8\). Average of 20 runs.

Figure 2: Value of the objective (left) and approximation ratio (right) with a varying number of sample points, calculated on 10 runs of the Gaussian matching experiment.

objective value than our method with 10 sample points. This suggests that our method can give good approximations of the GW distance with fewer sample points than existing methods.

### Graph Community Matching

The objective of this task is to find matching between two random graphs that are drawn from the stochastic block model (SBM) (Abbe, 2017; Holland et al., 1983) with fixed inter/intra-clusters probability (the probability that nodes inside and outside a cluster are connected, respectively). The source is a three-cluster SBM whose intra-cluster probability is \(p=\{1.0,0.95,0.9\}\), and the target is a two-cluster SBM whose intra-cluster probability is \(p=\{1.0,0.9\}\). The inter-clusters probability is all set to 0.1. The distance matrices on each graph are created first by simulating the node features drawn from Gaussian distributions with uniform weights. Subsequently, we compute the \(_{2}\) norm between nodes and shrink the value of disconnected nodes to zero to form the distance matrices.

We compare the transportation plans obtained using our methods with the baseline comparisons GW-CG and eGW in Figure 3(a). We note that the (GW-SDP) model typically returns a transport plan with a smaller total transportation cost (i.e., a smaller objective value) \(_{sdp},_{sdp}\). This trend is consistent with our observations in the previous experiments. Nevertheless, we see a degree of similarity between the transportation plans provided as output by all three methods. In addition, the transportation plans computed by our method and GW-CG are both reasonably sparse. This fact is observed in multiple runs of different seeds and graph sizes.

### Extension of GW-SDP to Structured Data

In this example, we consider an extension of the (GW-SDP) to structured data, more specifically graphs with node features similar to the Fused-GW distance in (Vayer et al., 2019). The discrete metric-measure space is now described by the tuple \((F,C,)^{m d}^{m m}_ {m}\), where \(F}}{{=}}(f_{i})_{i}^{d}\) encodes the feature information of the sample point. The Fused GW-SDP (FGW-SDP) formulation is given by

\[(M_{FG},C,D,,,) }}{{=}}_{^{m n}\\ P^{m m}} (1-) M_{,},+(C,D),P\] \[ (P&()\\ ()^{}&1) 0\] (FGW-SDP) \[(,)\] \[P(e_{i}_{m}^{})=_{i}( ),i[m]\] \[P(_{m}e_{j}^{})=_{j}( ),j[n]\] \[P 0,\]

with \(M_{FG}=d(f_{j},g_{j})_{i,j}\) encodes the distance between node features, and \(\) the interpolation parameter. Figure 3(b) shows the result of matching two SBM graphs with the same setting as in Section 4.2, with the exception that now we input the feature to calculate \(M_{FG}\) by \(_{2}\) norm, and the structured matrices are the shortest path matrices obtained from the adjacency matrices of the graphs. We set \(=0.8\) for this example. The figure shows that the output OT plans and values of (FGW-SDP) and FGW-CG (using PythonOT) are identical, while entropic Fused-GW returned a higher value and a denser transport plan. This indicates that the SDP relaxation of Fused-GW can be useful in graph matching applications, akin to Fused-GW.

  
**n** & **GW-SDP** & **GW-SDP Runtime (s)** & **GW-CG** & **GW-CG runtime (s)** \\ 
10 & 0.4577 & 6.3753 & 1.135940 & 0.000389 \\
100 & & & 0.629425 & 0.007571 \\
1000 & & & 0.540984 & 2.520011 \\
10000 & & & 0.496796 & 138.358954 \\   

Table 2: Comparisons of GW-SDP solver and GW-CG solver with a varying number of sample points \(n\).

### Using GW-SDP on Realistic Shape-Matching Task

We use a publicly available dataset of triangular meshes (Sumner and Popovic, 2004). The dataset comprises \(72\) objects from seven different classes, from which we chose samples of class cat, elephant, and horse. For each object, we first chose \(4\) representative points (the right back foot, the left front foot, the nose, and the tail) for each object and then selected another \(14\) points following the Euclidean farthest point sampling (fps) procedure. The distance matrices \(C\) and \(D\) are computed using Dijkstra's algorithm. Each object's probability measure is chosen to be uniform. We apply (GW-SDP) to the corresponding metric-measure spaces to determine the correspondence between the selected vertices across different objects. Two representative examples are given in Figure 5. For better visualization, in the representative examples we sampled only \(6\) points (\(4\) representative points and \(2\) selected using fps).

Table 3 illustrates the results when we perform matching of distance matrices across different objects. In general, we expect shapes of the same animals to have a smaller GW distance than shapes of different animals, which is indeed the case for the three GW formulations. We still notice that GW-SDP consistently returns the smallest value when performing the same matching task.

Figure 4: Value of the objective on the synthetic graph matching task, from the three-block SBM (left) to the two-block SBM (right). **Upper:** calculated using GW. **Lower:** calculated using Fused-GW.

Figure 5: Correspondence between different 3D objects obtained by (GW-SDP). Left: Correspondence between two elephants. Right: Correspondence between an elephant and a cat. For both cases, (GW-SDP) returns one-one mappings.

    & **GW-SDP** & **GW-CG** & **eGW-PPA** \\  Elephant-Elephant & 0.007416 & 0.043879 & 0.025688 \\ Elephant-Cat & 0.015695 & 0.050594 & 0.042214 \\ Cat-Cat & 0.006549 & 0.016634 & 0.006757 \\ Cat-Horse & 0.011040 & 0.033736 & 0.011041 \\ Horse-Horse & 0.006287 & 0.033768 & 0.007395 \\   

Table 3: Value of different GW formulations for the realistic 3D shape matching dataset, visualized in Figure 5. GW-SDP consistently returns the smallest value when performing the same matching task.

Duality

Given a generic optimization instance, the dual optimization instance concerns the task of finding optimal lower bounds to the primal instance. The (Lagrangian) dual to _any_ optimization instance is a convex program in general and provides a principled way of obtaining convex relaxations of difficult optimization instances. We briefly discuss some of these relationships. A more detailed discussion on the duality can be found in Appendix B.

It is possible to obtain the relaxation (GW-SDP) via duality. Concretely, let (GW++) refer to the original GW problem instance (GW+) with the additional and _redundant_ constraints (Nng.) and (Mar.). Let (GW-Dual) refer to the Lagrangian dual of the proposed semidefinite relaxation (GW-SDP).

**Theorem 5.1**.: _The optimization instance (GW-Dual) is the Lagrangian dual of (GW++), which is (GW+) with the additional constraints (Nng.) and (Mar.)._

The (duality) gap between (GW-Dual) and (GW++) is non-zero in general, and is equal to zero precisely when the convex relaxation (6) succeeds. These can be characterized by a rank condition:

**Proposition 5.2**.: _Let \(P_{sdp}\) and \(_{sdp}\) be the solution to (GW-SDP). Suppose the matrix variable has rank equals to one; that is_

\[P_{sdp}&(_{sdp})\\ (_{sdp})^{}&1=1.\]

_Then the duality gap is zero; i.e., strong duality holds._

## 6 Related work

There is a substantial body of prior work concerning the GW problem in the literature. We briefly discuss some of these and explain the novelty of our work.

First, the work in Vayer et al. (2019) applies an alternating minimization-type approach based on the conditional gradient (Frank-Wolfe) algorithm to find local optima to the GW problem. This algorithm is currently implemented and is the default choice within the Python Optimal Transport package (Flamary et al., 2021). The basic idea is to start by computing the partial derivative of the objective (GW) with respect to \(\):

\[G()=2\;(C,D),\]

This is a linear OT problem that can be solved using classical OT solvers. One proceeds with an alternating minimization scheme in which one updates the gradient \(G\) with respect to \(^{(i-1)}\), subsequently solves for \(^{(i)}\) with the loss \(G()\) at each \(i\)th-iteration, and finally projects \(^{(i)}\) into the feasible set by performing a line-search. The Conditional Gradient-based approach is not guaranteed to find globally optimal solutions; in fact, our numerical experiments in Section 4 suggest that this is quite often the case. Last, we briefly note that the work in Kerdoncuff et al. (2021) suggests a similar alternating numerical scheme.

Second, there is a body of work that aims at developing numerical schemes for finding transportation plans that approximately minimize the GW objective without incurring the expensive \(O(m^{2}n^{2})\) dependency. For instance, the work in Peyre et al. (2016) introduces an entropic regularization into the GW objective - this leads to a formulation that permits Sinkhorn scaling-like updates, much like the original scheme to solve entropic Wasserstein distance in Cuturi (2013). The work of Vayer et al. (2019) adapts the ideas from the Wasserstein problem in one dimension in which closed-form solutions are available (this is known as the sliced Wasserstein problem, Rabin et al. 2012) to the GW context. Finally, the work in Sejourne et al. (2021); Vincent-Cuaz et al. (2021) relaxes the constraints on the probability distributions. These numerical schemes frequently lead to numerical schemes that are far more scalable than other existing methods, but they ultimately optimize for an objective that is different from the GW problem.

There is an interesting piece of work in Seetbon et al. (2022), which operates under the assumption that the cost matrices have low-rank structure. While the algorithm does not give guarantees about global optimality, it raises an interesting future direction; namely, could we develop numerical schemes for our proposed SDP relaxation that also exploit similar structures?Finally, we discuss prior works that do in fact address global optimality (which is the heart of this paper): a recent work is in Mula and Nouy (2022), which suggests the use of moment sum-of-square (SOS) relaxation technique to solve the GW problem. The standard SDP relaxation for QPs on which our work is based on may be viewed, in a suitable sense, as the first level of the SOS hierarchy for polynomial optimization. Unfortunately, and as we note in Section 3, this alone is insufficient - the real novelty in our work is the addition of constraints that substantially strengthen the overall convex relaxation. A piece of related work by Villar et al. (2016) proposes a SDP relaxation of the closely related Gromov-Hausdorff problem, with an extension to the Gromov-Wasserstein problem. The relaxation is primarily designed for the Gromov-Hausdorff problem and is not equivalent to ours. The formulation also requires the probability distributions to be uniform whereas we do not. Another recent work by Ryner et al. (2023) also studies the Gromov-Hausdorff problem, and proposes a Branch-and-Bound approach for solving integer programs. The GW problem does not contain integer constraints, and hence Branch-and-Bound techniques are not applicable. That said, SDP relaxations can be used in conjunction with Branch-and-Bound. It would be interesting to see if our proposed SDP relaxations for GW suggest suitable relaxations for the Gromov-Hausdorff problem, which can be used in conjunction with the Branch-and-Bound techniques in Ryner et al. (2023).

## 7 Conclusions and Future Directions

In this work, we proposed a semidefinite programming relaxation of the Gromov-Wasserstein distance. Our initial results suggest that the relaxation (GW-SDP) is strong in the sense that \(_{sdp}\) frequently coincides with the globally optimal solution; moreover, we are able to provide a proof when this actually happens. These results are exciting, as it suggests a tractable approach for solving the GW problem - at least for examples of interest - which was previously assumed to be quite difficult.

An interesting future direction is to understand precisely how difficult is an instance of the GW problem. The fact that our convex relaxations work very well for the examples we considered suggests that the GW problem might not be as difficult as we think. It is important to bear in mind that these cost tensors \(\) have structure - they arise from the difference of actual cost matrices. Could it be that the difficult instances of the GW problem correspond to cost tensors \(\) that are not realizable as the difference of cost matrices; e.g., they violate the triangle inequality? A concrete question to this end is: Is the GW problem corresponding to cost tensors \(\) arising in practical instances tractable to solve?

A second important future direction concerns computation. One limitation of our proposed convex relaxation is that it is specified as the solution of an SDP in which the matrix dimension is \(mn\); that is, it is equal to the dimension of the transport plan. The prohibitive dependence on the data dimension means that we are currently only able to apply the relaxation on moderate sized instances using off-the-shelf SDP solvers. It would be of interest to develop specialized algorithms to solve the proposed relaxation (GW-SDP).