# Stepwise Weighted Spike Coding for Deep Spiking Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological neurons and are expected to play a key role in the advancement of neural computing and artificial intelligence. The efficiency of SNNs is often determined by the neural coding schemes. Existing coding schemes either cause huge delays and energy consumption or necessitate intricate neuron models and training techniques. To address these issues, we propose a novel Stepwise Weighted Spike (SWS) coding scheme to enhance the encoding of information in spikes. This approach compresses the spikes by weighting the significance of the spike in each step of neural computation, achieving high performance and low energy consumption. A Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for supporting SWS-based computing, aimed at minimizing the residual error resulting from stepwise weighting in neural computation. Our experimental results show that the SWS coding scheme outperforms the existing neural coding schemes in very deep SNNs, and significantly reduces operations and latency.

## 1 Introduction

Spiking Neural Networks (SNNs) are known as the third generation of neural network models inspired by the biological structures and functions in the brain . Unlike traditional Artificial Neural Networks (ANNs) that use continuous activation functions, SNNs incorporate discrete spiking events, enabling them to capture temporal dynamics and process information in a manner that closely mimics the brain's functioning . This event-driven paradigm aligns with the brain's energy-efficient computation and has the potential for more efficient and lower-power computing systems. .

Various coding schemes have been proposed to describe neural activities, including rate coding and temporal coding . Rate coding counts the number of spikes fired within a broad time window [23; 3; 18; 6], which effectively mitigates the impact of short-term interference on the signal. It was widely accepted in the early days and typically outperformed temporal coding [11; 34; 4; 29; 20]. However, the rate coding scheme disregards the information in the temporal domain of the input spike sequence and requires many pulses to represent the input signal value, making it an inefficient coding method that negates the low-power benefits of SNN. Due to the functional similarity to the biological neural network, spiking neural networks can embrace the sparsity found in biology and are highly compatible with temporal coding [31; 33; 27; 28; 21; 15]. Temporal coding relies on the specific timing or patterns of input spikes, allowing for greater information capacity in a single pulse. However, it requires a large number of time steps to provide fine-grained timing, which increases inference latency. Its sensitivity to variations in spike timing also makes it more vulnerable to temporal jitter or delays [25; 24]. Additionally, decoding temporal-coded information usually requires more complex neuron models [30; 36] and training methodologies [17; 26].

In the study of the temporal information dynamics of spikes, Kim et al.  discovered a phenomenon of temporal information concentration in SNNs. It is found that after training, information becomes highly concentrated in the first few timesteps. Based on this observation, we hypothesize that, from the perspective of the postsynaptic neuron, the first arriving spikes contain more information and require stronger responses. Consequently, we propose a mechanism whereby the neuron augments its own membrane potential with a specific coefficient prior to processing the subsequent input. This enhancement serves to increase the importance of preceding pulses on neurons, which is why the spikes are designated as Stepwise Weighted Spikes (SWS). Nevertheless, the amplification of the membrane potential makes it difficult for neurons to reduce its value through traditional "soft reset" (i.e. subtracted by an amount equal to the firing threshold), which can result in residual errors after neuron firing. To address this issue, we make the membrane potential reduced by a magnitude exceeding the threshold after firing. As a result, the membrane potential has both positive and negative residual values, which will generate both positive and negative spikes. This neuron is designated as a Ternary Self-Amplifying (TSA) neuron. To further reduce the error caused by the weighting process, a silent period is incorporated into the TSA neuron, allowing it to receive more input information before firing. We perform the classification tasks with SWS-based SNN on MNIST, CIFAR10, and ImageNet. The results show that the SWS coding scheme can achieve better performance with much fewer coding and computing steps. Even in very deep SNN, SWS coding scheme still performs well and achieves similar accuracy to the ANN with the same structure. Our major contributions to this paper can be summarized as follows:

* We propose the SWS coding scheme, which enables easy implementation of SNNs with low energy consumption and high accuracy. The stepwise weighting process enhances the information-carrying capacity of the preceding pulses, greatly reducing the number of coding spikes. Negative pulses are introduced in SWS coding to ensure an accurate information transmission.
* A novel TSA neuron model is proposed. TSA neuron progressively weights the input by augmenting its residual membrane potential before receiving the subsequent spike. The introduction of negative residual membrane potential and negative thresholds enhances the accuracy of the model's output.
* A silent period is added to TSA neuron to markedly improve accuracy at minimal latency cost. By adjusting the silent period step and coding step, SWS-based SNNs can exhibit performance advantages in different aspects, improving the flexibility of applications.

## 2 Related work

SNNs use spike sequences to convey information, making the encoding of real data into pulses a crucial step. Currently, the mainstream schemes of neural coding are rate coding and temporal coding [9; 33; 32]. Rate coding represents different activities with the number of spikes emitted within a specific time window. Due to its simplicity, rate coding is commonly used in deep learning of SNNs. However, it distributes information uniformly across a large number of spikes, resulting in an inefficient transmission process that increases network latency and energy consumption. Numerous researchers have proposed solutions to optimize inference latency in rate coding. Han et al. proposed a "soft reset" spiking neuron model that retains a residual membrane potential after firing to better mimic the ReLU functionality. They demonstrated near lossless ANN-SNN conversion by using 2-8 times fewer inference time steps. Still, a delay of thousands of steps is required in large datasets or deep networks. In , Hu et al. reduced the encode time steps by converting a quantized low-precision ANN to a rate-coded SNN. They also proposed a layer-wise fine-tuning mechanism to minimize the inference latency. However, their neuron model and the subsequent fine-tuning algorithm are relatively complex. Furthermore, in deeper neural networks such as ResNet56, a 1.5% drop in accuracy can be observed. The above rate encoding solutions are limited because they do not consider the significance of each spike.

In , Kim et al. proposed phase coding, which assigns different weights to spikes based on their time phase. However, the transmission amount of information is bounded by the global phase, which causes inefficiency in hidden layers, resulting in a latency of up to three thousand steps for a 32-layer network. Burst coding  attempts to overcome this issue by introducing burst spikes, which utilize Inter-Spike Interval (ISI). Burst spikes are capable of conveying more information quickly and accurately by inducing Post-Synaptic Potential (PSP) dramatically. Nevertheless, it is still deficient in terms of latency and efficiency. Rueckauer and Liu  proposed an efficient temporal encoding scheme where the analog activation values of the ANN neurons are represented by the inverse Time-To-First-Spike (TTFS) in the SNN neurons. Their new spiking network model generates 7-10 times fewer pulses by utilizing temporal information carried by a single spike. However, as pointed out in , TTFS coding scheme incurs expensive memory access and computational overhead, which diminishes the benefit of reduced pulse count. Furthermore, TTFS necessitates a large number of time steps to differentiate between various time points, which also increases network latency. Han and Roy  proposed the Temporal-Switch-Coding (TSC) scheme, in which each input image pixel is represented by two spikes, and its intensity is proportional to the timing between the two pulses. Their results showed a reduction in energy expenditure. However, TSC coding requires a large number of time steps to provide distinguishable time intervals, rendering it an ineffective approach to addressing the issue of the long latency.

Overall, rate coding employs a large number of pulses to encode information, which results in a considerable energy overhead and inference delays. On the other hand, temporal coding allows for greater information capacity in a single spike, but this does not reduce the computing latency as a precise time point or period can be identified only with a sufficient number of time steps. Therefore, new neural coding schemes should be developed.

## 3 Stepwise weighted spike coding scheme

### Stepwise weighting

The spike train \(S_{i}^{l}(t)\) of the \(i^{th}\) neuron in the \(l^{th}\) layer can be expressed as follows:

\[S_{i}^{l}(t)=_{t_{i}^{l,(f)} F_{i}^{l}}^{l}(t-t_{i}^{l,(f)})\] (1)

where \((t)\) is the Dirac delta function, \(^{l}\) is the spike amplitude of the \(l^{th}\) layer, which is usually set to the same value as the firing threshold. \(f\) is the index of the spike in the sequence, and \(F_{i}^{l}\) denotes a set of spike times which satisfies the firing condition:

\[t_{i}^{l,(f)}:u_{i}^{l}(t_{i}^{l,(f)}) V_{th}^{l}\] (2)

where \(u_{i}^{l}(t)\) denotes the membrane potential and \(V_{th}^{l}\) denotes the firing threshold of the neurons in the \(l^{th}\) layer.

Our basic idea is to amplify the membrane potential before the receipt of the subsequent input, which amplifies and prolongs the impact of the preceding input spikes on membrane potential, emulating the phenomenon of information concentration identified in . For clarity, the meanings of important symbols are provided in table 1. The action of a neuron in SWS-SNN can be described as follows:

\[u_{j}^{l}(t)= u_{j}^{l}(t-1)+z_{j}^{l}(t)-S_{j}^{l}(t)\] (3)

where \(\) is the amplification factor which should be greater than one, \(z_{j}^{l}(t)\) denotes the PSP (i.e. integrated inputs):

\[z_{j}^{l}(t)=_{i}_{ij}^{l}S_{i}^{l-1}(t)+b_{j}^{l}\] (4)

where \(_{ij}\) is the synaptic weight and \(b_{j}^{l}\) is the bias. Begin with the initial value \(u_{j}^{l}(0)=0\) and iteratively apply eq. (3) for each subsequent value until \(u_{j}^{l}(n)\) and substitute eq. (1) and eq. (4) into it,

  
**Symbol** & **Meaning** \\  \(S_{i}^{l}(t)\) & The spike train fired by the \(i^{th}\) neuron in the \(l^{th}\) layer \\ \(u_{i}^{l}(t)\) & The membrane potential of the \(i^{th}\) neuron in the \(l^{th}\) layer \\ \(z_{i}^{l}(t)\) & The integrated inputs to the \(i^{th}\) neuron in the \(l^{th}\) layer \\ \(V_{th}^{l}\) & The firing threshold of the neurons in the \(l^{th}\) layer \\ \(^{l}\) & The amplitude of the spikes fired by the neurons in the \(l^{th}\) layer \\   

Table 1: Common symbols and their meanings in this paper.

eq. (3) can be written as:

\[u_{j}^{l}(n)=^{n}u_{j}^{l}(0)+_{=1}^{n}^{n-}z_{j}^{l}() =_{t_{i}^{l-1,(f)}}_{i}_{=1}^{n}^{n-}_{ij}^{l} ^{l-1}(-t_{i}^{l-1,(f)})+^{n-}b_{j}^{l}\] (5)

Note that \(S_{j}^{l}(t)\) is set to zero for simplicity. From eq. (5), it can be seen that the stepwise augment of the membrane potential results in the spike input at time \(t_{i}^{l-1,(f)}\) encoding the value \(^{l-1}^{n-t_{i}^{l-1,(f)}}\). This process is thus referred to as stepwise weighting, and \(^{n-t_{i}^{l-1,(f)}}\) serves as the weight. The earlier the input pulse, the greater its ability to carry information. This solves the problem of excessive encoding steps in previous schemes, allowing faster information transmission.

### Residual error

Stepwise weighting effectively assigns more weight to earlier arriving pulses, but it also makes spike generation more tricky. To ensure that input information is efficiently encoded and transmitted to the next layer, the residual membrane potential should be minimized after neural computation is completed. The stepwise weighting, however, amplifies the residual potential from the previous time step. If \(z_{j}^{l}(t)\) remains high in subsequent steps, reducing the membrane potential becomes challenging, as shown in fig. 1(b). This vicious cycle ultimately leads to a persistently high membrane potential, indicating that a substantial amount of information remains unencoded.

We refer to this phenomenon as residual error. One contributing factor is that the threshold is set too high, resulting in a pulse being emitted only when the membrane potential exceeds the value \(^{l}\). While this prevents excessive information transmission, it results in missed opportunities to bring down \(u_{j}^{l}(t)\) by firing a spike.

To address this issue, we propose setting the firing threshold \(V_{th}^{l}\) to \(^{l}\). This adjustment facilitates pulse generation and reduces the residual membrane potential. After the neuron firing, the membrane potential is subtracted by \(^{l}\), which leads to the emergence of a negative residual that will be stepwise

Figure 1: (a) Illustration of the stepwise weighting process. The meanings of the symbol \(z_{j}^{l}(t),u_{j}^{l}(t)\) and \(S_{j}^{l}(t)\) can be found in table 1. The blue dotted line represents the membrane potential prior to the spike firing, and the black exponential function-like dotted line is employed to illustrate the trend of membrane potential amplification. (b) A \(V_{th}^{l}\) equal to \(^{l}\) results in residual errors, leaving a lot of information unencoded. (c) \(V_{th}^{l}\) is set to \(^{l}\), which increases the possibility to fire spikes early to better limit the residual. (d) Use negative spikes to correct the excessively emitted information.

weighted over time. The coefficient \(}{{2}}\) is selected as it is capable of controlling both positive and negative residuals within a narrow and balanced range. A negative threshold \(-V_{th}^{l}\) is introduced into the neuron model, which initiates a negative spike when the membrane potential falls below this threshold. This mechanism allows the excessively emitted information to be corrected by the negative spike, as shown in fig. 1(d). Given the above characteristics, we designate this neuron model as a TSA neuron.

### Silent period

Another contributing factor to residual error is the imbalanced distribution of \(z_{j}^{l}(t)\). A burst input of \(z_{j}^{l}(t)\) at time point \(\) results in a sharp rise in membrane potential, making it difficult for subsequent spikes to reduce it, as shown in fig. 2(a).

This can be addressed by incorporating a silent period \(T_{s}\) into the TSA neuron model. The neurons only integrates input and performs stepwise weighting, but are not allowed to fire in the first \(T_{s}\) steps. This enables the acquisition of more known information before spike generation, resulting in increased accuracy, as illustrated in fig. 2(b). Since the preceding input information has been amplified by \(^{T_{s}}\) after the silent period, \(V_{th}^{l}\) also needs to be adjusted accordingly, which is set to \(}}{2}^{l}\). Similarly, after firing, the membrane potential should be subtracted by \(^{l}^{T_{s}}\). Note that the fired spike amplitude remains unchanged, that is, \(^{l}\).

The impact of the silent period on network latency is shown in fig. 2(d). The output results for different input sequences are distinguished by blocks of different colors. It can be observed that as network depth increases, the silent period accumulates, leading to a higher output latency. The inference latency of SWS-SNN can be calculated as follows:

\[T_{inf}=T_{c}+T_{s} L_{}\] (6)

Figure 2: (a) Uncertainty in the input distribution leads to residual errors. (b) The silent period allows more information to be known when firing pulses. \(T_{s}\) is set to 1 here. \(V_{th}^{l}\) is amplified by \(^{T_{s}}\), and the original threshold is represented by a gray solid line. The orange dashed line represents the amount of membrane potential reduction after firing. (c) The silent period also avoids some unnecessary spikes and increases sparsity. Without the silent period, since \(u_{j}^{l}(1)\) exceeds the original threshold, a pulse will be generated at \(t=1\), which will later be corrected by another negative spike. (d) The impact of the silent period on network latency. The output spike sequences corresponding to different inputs are drawn in blocks of different colors. The pulses drawn in the spike sequence are for illustrative purposes only.

where \(T_{}\) is the inference delay, \(T_{c}\) is the coding time steps, \(T_{s}\) is the length of the silent period and \(L_{}\) is the number of TSA neuron layers. The neuron model in other coding schemes yields a zero \(T_{s}\), leading to an output delay equal to the coding time step, which is consistent with the definition in the previous scheme. From fig. 2(d), it can be seen that different input sequences are processed in a pipeline-like manner, and the value of \(T_{c}+T_{s}\) determines the throughput rate of SWS-SNN.

### Input encoding

According to eq. (5), the value that can be losslessly encoded under the SWS coding scheme can be expressed as follows:

\[A_{j}=_{=1}^{T_{c}}a_{j}^{}^{0}^{T_{c}-}\] (7)

where \(A_{j}\) denotes the encoded value. \(a_{j}^{}\{-1,0,1\}\) indicates the type of the output spike at time \(\): \(1\) for a positive pulse, \(-1\) for a negative pulse and \(0\) for no pulse. \(T_{c}\) denoted the time steps used for encoding. The weight \(^{T_{c}-}\) results from the stepwise weighting process described in section 3.1. \(^{0}\) denotes the spike amplitude of the input encoding layer, which can be assigned an appropriate value based on the range to be encoded.

According to eq. (7), given a fixed \(T_{c}\) and \(^{0}\), the distribution of \(A_{j}\) is determined by \(\). Setting \(\) to \(2\) is reasonable, as it ensures \(A_{j}\) is evenly distributed within the codable range. Compared to rate coding, which necessitates \(2^{T_{c}}\) coding steps to encode the same range with same precision, SWS coding significantly enhances coding efficiency. Note that with the introduction of negative pulses, setting \(\) to \(3\) can also achieve a uniform distribution of \(A_{j}\) and offers even more values for accurate encoding compared to \(=2\).1 When \(\) is less than \(2\), the distribution of \(A_{j}\) becomes denser at smaller values, which may be suitable for encoding data that follows a similar distribution.

For static image classification tasks, the pixel value \(p_{j}\) can be encoded by applying a constant input \(z_{j}^{0}(t)\) to the TSA neuron. Considering the stepwise weighting process, we can write:

\[p_{j}=_{=1}^{T_{c}}|z_{j}^{0}|.^{T_{c}-}.\] (8)

where \(|z_{j}^{0}|\) denotes the amplitude of the constant input \(z_{j}^{0}(t)\). Solve for \(|z_{j}^{0}|\) and we have:

\[z_{j}^{0}(t)=_{=1}^{T_{c}}}{_{=1}^{T_{c}}^{ T_{c}-}}(t-)\] (9)

Given that \(z_{j}^{0}(t)\) is a constant at each step, \(T_{s}\) can be set to \(0\) for the encoding layer. However, the neuron must await \(T_{s}\) time steps after the completion of an encoding. This allows neurons in the subsequent layer to complete the previous neural computing before receiving the next encoded input.

## 4 Experiments

In this section, we convert quantized ANNs to SWS-based SNNs2 and conduct experiments on MNIST, CIFAR10, and ImageNet. Firstly, an overview of SWS-SNN's performance across various datasets is provided. Subsequently, the network's inference latency and energy consumption is compared with other spike coding schemes. Finally, an ablation study is conducted to investigate the impact of lowered thresholds and silent periods on reducing residuals and enhancing accuracy.

ANNs used for conversion are all quantized to \(8\) bits. \(\) is set to \(2\) in the experiments to ensure that codable values are evenly distributed. Compared to \(=3\), a smaller amplification factor reduces the impact of residual errors, resulting in more accurate output.

### Overall performance

For simple classification tasks such as CIFAR10, our proposed SWS coding scheme has a faster inference speed than other ANN-SNN models while achieving similar classification accuracy, or has higher classification accuracy than direct learning at similar inference speeds. For example, ResNet18 with SWS improves throughput seven times over  while simultaneously improving accuracy. Although the network in  has a slightly higher throughput, its accuracy is 1.17% lower than our scheme. To fully test the potential of our proposed coding scheme, we conducted experiments on ImageNet using networks with various structures. The experimental results demonstrate that SWS coding has distinct advantages on extremely deep SNNs. Our SWS-based ResNet50 and ResNeXt101 achieved over 80% accuracy on ImageNet with only eight coding steps. The model in  achieves an almost lossless conversion with eight time steps. However, their method has to adjust the resting potential of neurons layer by layer, and the calibration effect for deeper networks is unclear. In , the original ANN needs to be quantized to 3 bits, resulting in a larger conversion loss. Directly trained SNNs typically achieve higher throughput, but their accuracy still requires improvement. In addition, the SWS coding scheme is easy to implement. No further fine-tuning is required after the conversion.

### Accuracy vs. latency

The comparison of latency results between SWS-SNN and other ANN-converted SNNs[1; 11; 10; 4; 18; 2; 7] is illustrated in fig. 3. The latency of the network is calculated with eq. (6). In the counterpart models, the variation of delay is mainly caused by the changes in \(T_{c}\). In contrast, \(T_{s}\) determines latency in deep SWS-SNNs. Therefore, SWS-SNN has an upper limit on latency: \(T_{inf}^{max}=T_{c}(1+L_{})\), which causes our curve to terminate earlier in fig. 3.

To ensure a fair comparison, we represent the ANN accuracy of each counterpart with dotted lines of the same color. The experimental results indicate that SWS-SNN can achieve optimal performance with minimal latency. Specifically, SWS-based VGG-16 can converge to the ANN performance

    & **Category** & **Methods** & **Architecture** &  **Time** \\ **Step** \\  & \(}\) & 
 **SNN** \\ **Acc** \\  & \(^{}\) \\   & Directly & STBP-tdBN & ResNet-19 & 6 & - & 93.16\% & - \\  & Learning & TET & ResNet-19 & 6 & - & 94.50\% & - \\   &  & TTRBR & ResNet-18 & 64 & - & 95.04\% & \(-0.13\%\) \\  & & DSR & PreAct-ResNet-18 & 20 & - & 95.24\% & - \\  & & Calibration & VGG-16 & 256 & - & 95.79\% & \(+0.05\%\) \\  & & OPI & VGG-16 & 256 & - & 94.49\% & \(-0.08\%\) \\  & & Opt Conversion & ResNet-20 & 128 & - & 93.56\% & \(+1.25\%\) \\   & ANN-SNN & **SWS (ours)** & ResNet-18 & 8 & 1 & 95.67\% & \(+0.22\%\) \\  & & & VGG-16 & 8 & 2 & 95.86\% & \(-0.04\%\) \\    & Directly & TET & SEW-ResNet-34 & 4 & - & 68.00\% & - \\  & & STBP-tdBN & SEW-ResNet-34 & 4 & - & 67.04\% & - \\  & & SEW Resnet & SEW-ResNet-152 & 4 & - & 69.26\% & - \\   &  & Hybrid training & ResNet-34 & 250 & - & 61.48\% & \(-8.72\%\) \\  & & Spiking ResNet & ResNet-50 & 350 & - & 72.75\% & \(-2.70\%\) \\  & & QCFS & VGG-16 & 64 & - & 72.85\% & \(-1.44\%\) \\  & & Fast-SNN & VGG-16 & 7 & - & 72.95\% & \(-0.41\%\) \\  & & COS & ResNet-34 & 8 & - & 74.17\% & \(-0.05\%\) \\  & & RMP-SNN & ResNet-34 & 4096 & - & 69.89\% & \(-0.75\%\) \\  & & TTRBR & ResNet-50 & 512 & - & 75.04\% & \(-0.98\%\) \\   &  & VGG-16 & 8 & 2 & 75.27\% & \(-0.11\%\) \\  & & & ResNet-34 & 8 & 2 & 76.10\% & \(-0.08\%\) \\  & & Inception-v3 & 8 & 2 & 76.70\% & \(-0.70\%\) \\  & & ResNet-50 & 8 & 2 & 80.34\% & \(-0.35\%\) \\  & & ResNeXt101\_32x8d & 8 & 1 & 81.32\% & \(-1.17\%\) \\  & & ResNeXt101\_32x8d & 8 & 2 & 82.06\% & \(-0.42\%\) \\  ^{}\)\(\)Acc = Acc\({}_{}-\)Acc\({}_{}\)} \\ 

Table 2: Performance on CIFAR10 and ImageNet.

in the shortest time on CIFAR10 and reduce the inference latency on ImageNet by more than one order. Even though the silent period accumulates when the network gets deeper, the results in fig. 3(c) demonstrate that our scheme still achieves the fastest inference speed with the highest accuracy in a \(34\)-layer network. Note that \(T_{s}\) is set to the same value for each TSA layer for simplicity, resulting in discontinuous \(T_{}\) values. This causes a sharp drop in accuracy at smaller delays.

### Operation counting

To compare the energy consumption of SWS-SNN with SNNs under other encoding schemes, we adopt the method as in  to count operations:

\[=(T_{c}+T_{s})N_{}+_{l=1}^{L_{}}_{ =T_{s}l+1}^{T_{}+T_{c}}f_{}^{l}n^{l}()\] (10)

where \(\) (Operations Per Frame) denotes the number of operations for the classification of one frame, \(T_{c}\) and \(T_{s}\) denotes the coding steps and the length of the silent period, respectively. \(L_{}\) denotes the number of TSA layers, \(f_{}^{l}\) denotes the fan-out of neurons in layer \(l\), \(n^{l}(t)\) denotes the number of spikes fired in layer \(l\) at time \(\) and \(N_{}\) denotes the number of TSA neurons. The first term on the right-hand side of the equation arises from the TSA's requirement to amplify the membrane potential. Note that due to the accumulation of \(T_{s}\) over the network depth, the time period for counting \(n^{l}(t)\) varies with \(l\).

Experiments were conducted on MNIST using LeNet-5. We varied the silent periods and adjusted the coding steps to study their effects on OPF. The results are presented in fig. 4(a). As indicated in eq. (10), reducing \(T_{c}\) lowers energy overhead. This presents a trade-off between energy consumption and inference accuracy, as fewer coding steps also reduce the number of values that can be accurately encoded. A larger \(T_{s}\) requires TSA neurons to perform more operations to amplify the membrane potential. On the other hand, it reduces the number of unnecessary pulse emissions. Overall, silent period has a negligible impact on OPF.

Figure 4: (a) Accuracy versus OPF with different combinations of \(T_{c}\) and \(T_{s}\). (b) Comparison of accuracy and energy consumption of SWS-SNN with other SNNs.

Figure 3: Latency versus accuracy. The ANN accuracy of each compared SNN is marked by dotted lines of the same colour. (a) VGG-16 on CIFAR10. (b) VGG-16 on ImageNet. (c) ResNet34 on ImageNet.

In fig. 4(b), the energy consumption of SWS-based SNN is compared with that of other SNNs. The experimental results demonstrate that our coding scheme can achieve a favorable balance between accuracy and energy consumption. The SWS coding scheme is superior to rate coding and temporal pattern coding in that it requires fewer operations and achieves higher accuracy. In TTFS encoding, each neuron fires at most one spike at a time, theoretically demanding the least OPF. With \(T_{c}=4\), SWS-SNN can achieve significantly higher accuracy with minimal increase in OPF. Note that if the ANN is quantized to a lower number of bits (e.g., 4 bits), the error caused by the reduced \(T_{c}\) can actually be compensated by the quantization algorithm, which can potentially result in a higher performance.

### Ablation study

In section 3.2 and section 3.3, we proposed reducing the firing threshold and introducing a silent period to mitigate residual error. To assess the impact of these two adjustments, we conducted experiments on CIFAR10 using ResNet18. After the neural computation, the residuals (absolute values) of the TSA neurons were analyzed. We first scaled the residuals by \(}{{^{T_{s}}}}\) to counteract the effect of membrane potential amplification caused by the silent period, and then normalized them in units of \(^{l}\). The probability density of the residuals is shown in fig. 5(a).

The results demonstrate that lowering \(V_{th}^{l}\) shifts the residual distribution from around \(0.5^{l}\) to approximately \(0.25^{l}\), corresponding to the quantization errors (i.e. rounding errors) under their respective thresholds. The addition of silent periods further concentrates the distribution and reduces large deviations. As can be seen from the green curve in fig. 5(a), setting \(T_{s}\) to \(2\) and \(V_{th}^{l}\) to \(}}{{2}}\) makes the residuals almost all distributed around the quantization error. Compared to the red curve (without a lowered \(V_{th}^{l}\) or a silent period), the residuals are greatly reduced, which fully proves the effectiveness of lowering the threshold and adding a silent period. The inference results on CIFAR10 is shown in fig. 5(b). When setting \(V_{th}^{l}\) to \(^{l}\) and \(T_{s}\) to zero, the network's output is almost random. Lowering the threshold and adding a silent period improve the accuracy to \(35.41\%\) and \(84.21\%\), respectively. Ultimately, the combination of both adjustments enabled SWS-ResNet18 to achieve an accuracy of \(95.68\%\) on CIFAR10.

## 5 Conclusion

In this work, we have proposed a novel SWS spike coding scheme. The stepwise weighting process enhances the information-carrying capacity of the preceding pulses, greatly reducing the number of time steps for encoding. Combined with a silent period, our proposed TSA neuron model solves the problem of residual errors and achieves fast and accurate information transmission. Our experimental results have demonstrated that SWS coding is highly effective in extremely deep SNNs and achieves state-of-the-art accuracy. The SWS coding scheme is also highly flexible and can adapt to various needs.

Figure 5: (a) The probability density of the residuals with/without a lowered \(V_{th}^{l}\) and a silent period. (b) Inference accuracy of SWS-ResNet18 on CIFAR10 with/without a lowered \(V_{th}^{l}\) and a silent period.