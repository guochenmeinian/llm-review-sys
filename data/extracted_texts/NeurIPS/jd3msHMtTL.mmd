# Small coresets via negative dependence:

DPPs, linear statistics, and concentration

 Remi Bardenet

Univ. Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France

remi.bardenet@cnrs.fr

&Subhroshekhar Ghosh

Department of Mathematics

National University of Singapore

10 Lower Kent Ridge Road, 119076, Singapore

subhrowork@gmail.com

&Hugo Simon-Onfroy

Universite Paris-Saclay, CEA, Irfu

Departement de Physique des Particules

91191, Gif-sur-Yvette, France

hugo.simon@cea.fr

&Hoang Son Tran

Department of Mathematics

National University of Singapore

10 Lower Kent Ridge Road, 119076, Singapore

hoangson.tran@u.nus.edu

The authors are listed in alphabetical order by their surnamesCorresponding author

###### Abstract

Determinantal point processes (DPPs) are random configurations of points with tunable negative dependence. Because sampling is tractable, DPPs are natural candidates for subsampling tasks, such as minibatch selection or coreset construction. A _coreset_ is a subset of a (large) training set, such that minimizing an empirical loss averaged over the coreset is a controlled replacement for the intractable minimization of the original empirical loss. Typically, the control takes the form of a guarantee that the average loss over the coreset approximates the total loss uniformly across the parameter space. Recent work has provided significant empirical support in favor of using DPPs to build randomized coresets, coupled with interesting theoretical results that are suggestive but leave some key questions unanswered. In particular, the central question of whether the cardinality of a DPP-based coreset is fundamentally smaller than one based on independent sampling remained open. In this paper, we answer this question in the affirmative, demonstrating that _DPPs can provably outperform independently drawn coresets_. In this vein, we contribute a conceptual understanding of coreset loss as a _linear statistic_ of the (random) coreset. We leverage this structural observation to connect the coresets problem to a more general problem of concentration phenomena for linear statistics of DPPs, wherein we obtain _effective concentration inequalities that extend well-beyond the state-of-the-art_, encompassing general non-projection, even non-symmetric kernels. The latter have been recently shown to be of interest in machine learning beyond coresets, but come with a limited theoretical toolbox, to the extension of which our result contributes. Finally, we are also able to address the coresets problem for vector-valued objective functions, a novelty in the coresets literature.

## 1 Introduction

Let \(=\{x_{i} i 1,n\}\) be a set of \(n\) points in a Euclidean space, called the _data set_. Let \(\) be a set of nonnegative functions on \(\), called _queries_. Many classical learning problems, supervised orunsupervised, are formulated as finding a query \(f^{*}\) in \(\) that minimizes an additive loss function of the form

\[L(f):=_{x}(x)f(x),\] (1)

where \(:_{+}\) is a weight function.

**Example 1** (\(k\)-means).: For \(^{d}\) and \(k\), the goal of \(k\)-means clustering is to find a set \(^{*}\) of \(k\) "cluster centers" by minimizing (1) over

\[=\{f_{}:x_{q} x-q _{2}^{2}\ |\ \ ^{d},||=k\}.\]

Here, each query \(f\) is indexed by a set of \(k\) cluster centers, and the loss (1) is the quantization error.

**Example 2** (linear regression).: When \(=\{x_{i}:=(y_{i},z_{i}) i 1,n\} ^{d+1}\), linear regression corresponds to minimizing (1) over

\[=\{(y,z)(a^{}y+b-z)^{2} a^{d},b \}.\]

Penalty terms can be added to each function, to cover e.g. ridge or lasso regression.

In many machine learning applications, the complexity of the corresponding optimization problem grows with the cardinality \(n\) of the dataset. When \(n 1\) makes optimization intractable, one is tempted to reduce the amount of data, using only a tractable number of representative samples. This is the idea formalized by _cores_; we refer to (Bachem, Lucic, and Krause, 2017) for a survey, and to (Huang, Li, and Wu, 2024; Cohen-Addad, Larsen, Saulpic, Schwiegelshohn, and Sheikh-Omar, 2022) for specific coreset constructions for \(k\)-means and Euclidean clustering. An \(\)-coreset is a subset \(\), possibly with corresponding weights \((x)\), \(x\), such that

\[L_{}(f):=_{x}(x)f(x)\] (2)

is within \(\) of \(L(f)\), uniformly in \(f\). If the cardinality \(m\) of \(\) is significantly smaller than the intractable size \(n\) of the original data set, one has reduced the complexity of the algorithm at a little cost in accuracy.

Many _randomized_ coreset constructions, where such guarantees are shown to hold with large probability, are built by drawing elements _independently_ from the data set \(\)(Bachem et al., 2017, Chapter 3). Because a _representative_ coreset should intuitively be made of _diverse_ data points, _negative dependence_ between the coreset elements has been proposed as an effective possibility to improve their performance (Tremblay, Barthelme, and Amblard, 2019). In particular, the authors advocate the use of Determinantal Point Processes (DPPs), a family of probability distributions over subsets of \(\) parametrized by an \(n n\) kernel matrix \(\) that enforces diversity, all of this while coming with a polynomial-time exact sampling algorithm.

Tremblay et al., 2019 give extensive theoretical and empirical justification for the use of DPPs in randomized coreset construction. In one of their key results, using concentration results in (Pemantle and Peres, 2011), Tremblay et al., 2019 bound the cardinality of a DPP-based \(\)-coreset, and their bound is \((^{-2})\). However, it is known that the best \(\)-coresets built with independent samples are also of cardinality \((^{-2})\). Thus, the crucial question of whether DPP-based coresets can provide a strict improvement remained to be settled; given the computational simplicity of independent schemes, this would be fundamental to justify the deployment of DPP-based methods.

In this paper, we settle this question in the affirmative, demonstrating that for carefully chosen kernels, DPP-based coresets provably yield significantly better accuracy guarantees than independent schemes; equivalently, to achieve similar accuracy it suffices to use significantly smaller coresets via DPPs. In particular, we will show that DPP-based coresets actually can achieve cardinality \(m=(^{-2/(1+)})\). The quantity \(\) depends on the variance of the subsampled loss under the considered DPP, and some DPPs yields \(>0\). A cornerstone of our approach is a structural understanding of the coreset loss (2) as a so-called _linear statistic_ of the random point set \(\), which enables us to go beyond earlier results that were based on concentration properties of general Lipschitz functions of a DPP (Pemantle and Peres, 2011).

In this endeavour, we obtain very widely-applicable concentration inequalities for linear statistics of DPPs compared to the state of the art; cf. (Breuer and Duits, 2013) that mostly focuses on scalar-valued statistics for finite rank ensembles on \(\). In particular, we are able to address all DPPs that have appeared so far in the ML literature. Specifically, our results are able to handle _non-symmetric kernels_ and _vector-valued_ linear statistics.

DPPs with non-symmetric kernels have recently been shown to be of significant interest in machine learning, such as recommendation systems (Gartrell, Brunel, et al., 2019; Gartrell, Han, et al., 2020; Han et al., 2022), but they come with a limited theoretical toolbox, to which this paper makes a contribution. On the other hand, vector-valued statistics arise naturally in many learning problems, including coreset settings such as the gradient estimator in Stochastic Gradient Descent (Bardenet, Ghosh, et al., 2021). However, the literature on coresets for vector-valued statistics is scarce, and in this paper we inaugurate their study with effective approximation guarantees via DPPs.

The rest of the paper is organized as follows. Section 2 contains background on DPPs and coresets. Section 3 contains our contributions. Section 4 provides numerical illustrations. Section 5 contains a discussion on limitations and future work.

## 2 Background

We introduce here the two key notions of determinantal point process and coreset, and observe that a coreset guarantee is a uniform control over specific linear statistics of a point process.

Determinantal point processes.A point process \(\) on a Polish space \(\) is a random locally finite subset of \(\). Given a reference measure \(\) on \(\) (e.g., the Lebesgue measure if \(=^{d}\) or the counting measure if \(\) is discrete), a point process \(\) is called a DPP (w.r.t. \(\)) if there exists a measurable function \(K:\) such that

\[_{}f(x_{i_{1}},,x_{i_{k}})=_{ ^{k}}f(x_{1},,x_{k})[K(x_{i},x_{j})]_{k k}\,^{ k}(x_{1},,x_{k}),\] (3)

where the sum in the LHS ranges over all pairwise distinct \(k\)-tuples of the random locally finite subset \(\), for all bounded measurable \(f:^{k}\) and for all \(k\). Such a function \(K\) is called a _kernel_ for the DPP \(\), and \(\) is called the background measure.

When the ground set \(\) is of finite cardinality \(n\), an equivalent but more intuitive way to define DPPs is as follows: a random subset \(\) of \(\) is called a DPP if there exists an \(n n\)-matrix \(\) such that

\[(T)=[_{T}],\;T ,\]

where \(_{T}\) denotes the submatrix of \(\) with rows and columns indexed by \(T\).

In a similar vein to Gaussian processes, all the statistical properties of a DPP are encoded in this kernel function \(K\) and background measure \(\). A feature of DPPs with far-reaching implications for machine learning is that sampling and inference with DPPs are tractable. We refer the reader to (Hough et al., 2006; Kulesza and Taskar, 2012) for general references. Originally introduced in electronic optics (Macchi, 1975), they have been turned into generic statistical models for repulsion in spatial statistics (Lavancier et al., 2014; Biscio and Lavancier, 2017) and machine learning (Kulesza and Taskar, 2012; Belhadji et al., 2020; Brunel, 2018; Derezinski and Mahoney, 2019; Derezinski, Liang, et al., 2020; Gartrell, Brunel, et al., 2019; Ghosh and Rigollet, 2020).

**Example 3** (\(L\)-ensemble and m-DPP).: Let \(\) be a finite set of cardinality \(n\), \(\) be the counting measure, and \(\) be a positive semi-definite \(n n\)-matrix. The \(L\)-ensemble with parameter \(\) is the point process \(\) on \(\) such that, for all \(T\), \((=T)[_{T}]\), where \(_{T}\) is the square submatrix of \(\) corresponding to the rows and columns indexed by the subset \(T\). It can be shown that \(\) is a DPP on \(\) with kernel \(:=(+)^{-1}\). In general, the cardinality of \(\) is a random variable. By conditioning on the event \(\{||=m\}\), we obtain the so-called \(m\)-DPPs (Kulesza and Taskar, 2012).

**Example 4** (Multivariate OPE; Bardenet and Hardy, 2020).: Let \(=^{d}\) and \(\) be a measure on \(^{d}\) having all moments finite, let \((p_{k})_{k^{d}}\) be the orthonormal sequence resulting from applying the Gram-Schmidt procedure to the monomials \(x_{1}^{k_{1}} x_{d}^{k_{d}}\), taken in the graded lexical order. The kernel \(K_{}^{(m)}(x,y):=_{k=0}^{m-1}p_{k}(x)p_{k}(y)\) then defines a projection DPP on \(^{d}\), called the multivariate Orthogonal Polynomial Ensemble (OPE) of rank \(m\) and reference measure \(\).

Multivariate OPEs were used in (Bardenet and Hardy, 2020) as nodes for numerical integration, leading to a Monte Carlo estimator with mean squared error decaying in \(m^{-1-1/d}\), faster than under independent sampling. In (Bardenet, Ghosh, and Lin, 2021), the authors investigated the problem of DPP-based minibatch sampling for Stochastic Gradient Descent (SGD), and exploited a delicate interplay between a finite dataset and its ambient data distribution to leverage this fast decay for improved approximation guarantees. In particular, they proposed the following DPP defined on a (large) finite ground set.

**Example 5** (Discretized multivariate OPE; Bardenet, Ghosh, et al., 2021).: Let \(n\) and \(=\{x_{1},,x_{n}\}[-1,1]^{d}\). Let \(q(x)dx\) be a probability measure on \([-1,1]^{d}\). Let \(K_{q}^{(m)}\) be the multivariate OPE kernel of rank \(m\) with reference measure \(q(x)dx\), as defined in Example 4. Let \(:[-1,1]^{d}_{+}\) be a function, assumed to be positive on \(\), and consider

\[K_{q,}^{(m)}(x,y):=(x)}}K_{q}^{( m)}(x,y)(y)}}, x,y[-1,1]^{d}.\]

Consider then the \(n n\) matrix \(}=K_{q,}^{(m)}|_{}\). \(}\) is symmetric and positive semidefinite, and we let \(\) be the matrix with the same eigenvectors, the \(m\) largest eigenvalues replaced by \(1\), and the remaining eigenvalues replaced by \(0\). Then \(\) defines a DPP on \(\).

Coresets.Let \(>0\) and \(\) be a set of cardinality \(n\). The classical definition of a coreset is multiplicative.

**Definition 1** (multiplicative coreset).: A subset3\(\) is an \(\)-multiplicative coreset if

\[ f,\ |}(f)}{L(f)}-1|,\] (4)

where \(L\) and \(L_{}\) are respectively defined in (1) and (2).

An immediate and important consequence of (2) is that the ratio of the minimum value of \(L_{}\) by that of \(L\) is within \(()\) of \(1\)(Bachem et al., 2017, Theorem 2.1).

One way to satisfy (2) with high probability for a single \(f\) is through importance sampling, taking \(\) to be formed of \(m>0\) i.i.d. samples from some instrumental density \(q\) on \(\), and taking \(=/q\) in (2). Langberg and Schulman, 2010 showed that a suitable choice of \(q\) actually yields the uniform guarantee (2). It suffices to take for instrumental pdf \(q(x)(x)s(x)\), where \(s\) upper-bounds the so-called _sensitivity_

\[s(x)_{f}}(y)f(y)},  x.\] (5)

For \(>0\), \(k}{2^{2}} 2/\) independent draws are then enough to build an \(\)-multiplicative coreset, where \(S=_{x}(x)s(x)\); see (Bachem et al., 2017)[Section 2.3]. The tighter the bound (5), the smaller the size of the coreset. One important limitation is that finding a tight bound is nontrivial.

Although not standard, a natural alternative definition of a coreset is that of an additive coreset.

**Definition 2** (additive coreset).: A subset \(\) is an \(\)-additive coreset if

\[|L_{}(f)-L(f)|, f .\] (6)

Note the arbitrary scaling factor \(1/n\) in (6) compared to (2), which we adopt to simplify comparisons between the two coreset definitions. With an additive coreset, the minimal value of \(L_{}\) is guaranteed to be within \( n\) of the minimal value of \(L\): Similarly to a multiplicative coreset, with \(\) suitably small one should be happy to train one's algorithm only on \(\).

Coreset guarantee and linear statistics.Let \(\) be a point process on a finite \(=\{x_{1},,x_{n}\}\). For a test function \(:\), we denote by \(():=_{x}(x)\) the so-called _linear statistic_ of \(\). In a coreset problem, for a query \(f\), the estimated loss \(L_{}(f)\) in (2) is the linear statistic \(( f)\). When \(\) is a DPP with a kernel \(\) on \(\) (w.r.t. the counting measure), we will choose the weight \((x)=(x,x)^{-1}\), where for \(x=x_{i}\), we define \((x,x)\) to be \(_{ii}\). By (3), this choice makes \(L_{}(f)\) an unbiased estimator for \(L(f)\). Guaranteeing a coreset guarantee such as (6) with high probability thus corresponds to a uniform-in-\(f\) concentration inequality for the linear statistic \(( f)\). This motivates studying the concentration of linear statistics under a DPP, to which we now turn.

Theoretical results

We first give new results on the concentration of linear statistics under very general DPPs. These results are of interest in their own right, and should find applications in ML beyond coresets. Next we examine the implications of the concentration of linear statistics for coresets, showing that a suitable DPP does yield a coreset size of size \(o(^{-2})\), thus beating independent sampling.

Concentration inequalities for linear statistics of DPPs.We start with Hermitian kernels.

**Theorem 1** (Hermitian kernels).: _Let \(\) be a DPP on a Polish space \(\) with reference measure \(\) and Hermitian kernel \(K\). Then for any bounded test function \(:\), we have_

\[(|()-[()]|)  2-}{4A\,[ ()]}, 0[()]}{3\|\|_{}},\]

_where \(A>0\) is a universal constant._

Our Theorem 1 is similar in spirit to a seminal concentration inequality by Breuer and Duits, 2013. However, their result only applies to DPPs with Hermitian projection kernels of finite rank. We emphasize that our Theorem 1 is applicable to all Hermitian kernels on general Polish spaces.

In view of recent interest in machine learning on DPPs with non-symmetric kernels, we present here a concentration inequality for such DPPs. We propose a novel approach to control the Laplace transform in the non-symmetric case (which can also be applied to the symmetric setting). As a trade-off, the range for \(\) becomes a bit smaller. For simplicity, we present the result for a finite ground set, but the proof applies more generally.

**Theorem 2** (Non-symmetric kernels).: _Let \(\) be a DPP on a finite set \(=\{x_{1},,x_{n}\}\) with a non-symmetric kernel \(\). Then for any bounded test function \(:\), we have_

\[(|()-[()]|) 2 -}{4\,[( )]}, 0 [()]^{2}}{40\|\|_{}^{3} (1,\|\|_{}^{2})\|\|_{*}}}{40\| \|_{}^{3}(1,\|\|_{}^{2})\| \|_{*}},\]

_where \(\|\|_{}\) denotes the spectral norm and \(\|\|_{*}\) denotes the nuclear norm of a matrix._

**Remark 2.1**.: _For simplicity, we will use the concentration inequality in Theorem 1 from now on. However, we keep in mind that we always can apply Theorem 2 to deduce analogous results for non-symmetric kernels._

We conclude with a concentration inequality for linear statistics of vector-valued functions.

**Theorem 3** (Vector-valued statistics).: _Let \(\) be a DPP on a Polish space \(\) with reference measure \(\) and Hermitian kernel \(K\). Let \(=(_{1},,_{p})^{}:^{p}\) be a vector-valued test function, and we denote by \(()\), \(()\) the vectors \(((_{i}))_{i=1}^{p}\) and \(([(_{i})]^{1/2})_{i=1}^{p}\), respectively. Let \(\|x\|_{}^{2}:=_{i=1}^{p}_{i}^{2}|x_{i}|^{2}\) be a weighted norm on \(^{p}\) for some weights \(_{1},_{p} 0\). Then, for some universal constant \(A>0\), we have_

\[(\|()-[()]\|_{} ) 2p-}{4A\|()\|_{}^{2}} ,\]

_for \(0()\|_{}}{3}_{1 i p }[(_{i})]}}{\| _{i}\|_{}}.\)_

DPPs for coresets.We demonstrate the effectiveness of concentration inequalities for linear statistics of DPPs in the coresets problem, achieving uniform approximation guarantees over function classes. To accommodate as many ML settings as possible, we shall consider two natural types of function classes: vector spaces of functions (A.1) and parametrized function spaces (A.2).

For vector spaces of functions, we assume that

\[_{}()=D<D.\] (A.1)

This assumption covers common situations like linear regression in Example 2, where we observe that each \(f\) is a quadratic function in \((d+1)\) variables. Thus the dimension of the linear span of \(\) is at most \((d+1)^{2}+(d+1)+1\). Another popular class of queries, originating in signal processingproblems, is the class of _band-limited functions_. A function \(f:^{d}\) (where \(^{d}\) denotes the \(d\)-dimensional torus) is said to be _band-limited_ if there exists \(B\) such that its Fourier coefficients \((k_{1},,k_{d})=0\) whenever there is a \(k_{j}\) such that \(|k_{j}|>B\). It is easy to see that the space \(\) of \(B\)-bandlimited functions satisfies \((2B+1)^{d}\).

Another common scenario is when \(\) is parametrized by a finite-dimensional parameter space:

\[=\{f_{}:\},^{D}D,\] (A.2)

\[\|f_{}-f_{^{}}\|_{}\|-^{}\| >0.\] (A.3)

Conditions (A.2) and (A.3) cover e.g. the \(k\)-means problem of Example 1, as well as (non-)linear regression settings. For \(k\)-means, for instance, each query is parametrized by its cluster centers \(=\{q_{1},,q_{k}\}\), which can be viewed as a parameter \((q_{1},,q_{k})^{kd}\).

Finally, with the idea in mind to derive multiplicative coresets from additive ones, we note that since \(L(f)\) is typically of order \(n\) (for any \(f\) whose effective support covers a positive fraction of the ground set), it is natural to assume that

\[|L(f)| c,c>0,.\] (A.4)

**Theorem 4**.: _Let \(\) be a DPP with a Hermitian kernel \(\) on a finite set \(=\{x_{1},,x_{n}\}\) and \(m=[||]\). Assume that for all \(i\{1,,n\}\), \(_{ii} m/n\) for some \(>0\) not depending on \(m,n\). Let \(V_{f}n^{-1}L_{}(f) \). Under (A.1) and (A.4),_

\[ f:}(f)}{L(f )}-1 26D-^{2}}{ 16AV}, 0}\|f\|_{ }}.\]

_Assuming (A.2), (A.3), (A.4) and \(|| B m\) a.s. for some \(B>0\), we have_

\[ f:}(f)}{L(f) }-1 2CD-D- ^{2}}{16AV},\;0}\|f\|_{}}.\]

_Here \(A>0\) is a universal constant and \(C=C(,B,,,c)>0\) is some constant._

**Remark 4.1**.: _For a bounded query \(f\), \(n^{-1}L_{}(f)=(m^{-1})\) for i.i.d. sampling. In comparison, sampling with DPPs often yields smaller variance for linear statistics, in \((m^{-(1+)})\) for some \(>0\); see Section 3 for an example. Thus, the upper bound for the range of \(\) for which we could use our concentration result is \((m^{-})\). Plugging in \(=m^{-}\) for \(\) gives the upper bounds \(2(6D-C^{}m^{1+-2})\) and \(2(CD+ D m-C^{}m^{1+-2})\) respectively (\(C\) and \(C^{}\) are some positive constants independent of \(m\) and \(n\)), which both converge to \(0\) as \(m\) as long as \(<(1+)/2\). In other words, the accuracy rate \(\) can be chosen to be as small as \(m^{-1/2-^{}/2}\), for any \(0<^{}<\), which is strictly smaller than the best accuracy rate \(m^{-1/2}\) of i.i.d. sampling._

**Remark 4.2**.: _For i.i.d. sampling \(\) with expected size \(m\), \((x)=m/n\) for all \(x\). For a DPP \(\) with kernel \(\), one has \((x_{i})=_{ii}\). Thus, assuming that for all \(i\), \(_{ii} m/n\) for some \(>0\) means that every point in the dataset \(\) should have a reasonable chance to be sampled. This also guarantees that the estimated loss \(L_{}(f)=_{x}f(x)/(x,x)\) will not blow up, where for \(x=x_{i}\), we write \((x,x)\) for \(_{ii}\)._

**Remark 4.3**.: _For the parametrized function spaces, the assumption \(|| B m\) a.s. is not strictly necessary, and is introduced here only for the sake of simplicity in presenting the results. A version of Theorem 4 without this assumption will be discussed in Appendix A.4. In fact, we only need \(n^{-1}_{x}(x,x)^{-1}\) to be bounded with high probability, which follows from the condition \((x,x) m/n\) and the fact that \(||\) is highly concentrated around its mean \(m\)._

**Remark 4.4**.: _However, we remark that the assumption \(|| B m\) a.s. holds for most kernels of interest; DPPs with projection kernels being typical and significant examples. In machine learning terms, it entails that the coresets are not much bigger than their expected size \(m\); whereas in practice, sampling schemes typically produce coresets of a fixed size (such as with projection DPPs)._

**Remark 4.5**.: _It is straightforward to derive a version for additive coresets from Theorem 1. In fact, we will not need assumption (A.4) in the additive setting._

[MISSING_PAGE_FAIL:7]

cardinality \(m\). For the associated weight function \(\) in (2), we always take the inverse of the marginal probability of inclusion, i.e. \((x)=1/(x)\).

The first two baselines use independent sampling. The uniform method returns \(m\) samples from \(\), uniformly and without replacement, and runs in \((m)\). The second method, sensitivity, is specific to the \(k\)-means problem. It corresponds to the classical sensitivity-based importance sampling coreset of Langberg and Schulman, 2010 described in Section 2. It runs in \((nk+nm)\).

The rest of the methods use negative dependence. The third method, termed G-mDPP, uses an \(m\)-DPP sampler where the likelihood kernel is a Gaussian kernel, with adjustable bandwidth denoted by \(h\). It is basically Algorithm 1 of Tremblay et al., 2019, except we do not approximate the likelihood kernel using random features. We prefer avoiding approximations in this paper to isolatedly probe the benefit of negative dependence, but our choice comes at the cost \((n^{3})\) of performing SVD as a preprocessing, in addition to the usual \((nm^{2})\) sampling time. Similarly, we compute the marginal probabilities of inclusion of \(m\)-DPPs exactly, via Equation (205) and Algorithm 7 of Kulesza and Taskar, 2012. These costly steps will likely be approximated in real data applications; see the discussion of complexity to Section 5. The fourth method, OPE, is the discretized OPE of Example 5. We take \(q\) to be a product of univariate beta pdfs, with parameters tuned to match the marginal moments of the dataset, as in (Bardenet, Ghosh, et al., 2021). We take \(\) to be a kernel density estimator (KDE) built on \(\), using the Epanechnikov kernel, with Scott's bandwidth selection method, as implemented in the scikit-learn package (Pedregosa et al., 2011). When KDE estimation is precomputed as in our experiments, the method runs in \((nm^{2})\), and \((n^{2}+nm^{2})\) otherwise. Note that there is no cubic power of \(n\), as one can perform the eigenvalue thresholding in Example 1 by a reduced SVD of the \(m n\) feature matrix \((p_{k}(x_{i}))\). The fifth method, termed Vdm-DPP, is Algorithm 2 of Tremblay et al., 2019, which runs in \((nm^{2})\). It is an OPE in the sense of Example 4, but where the reference measure \(\) is the discrete empirical measure of the dataset. Although we have no result on how its linear statistics scale, its similarity with the discretized OPE, as well as its numerical performance in the experiments of Tremblay et al., 2019, make us expect Vdm-DPP to behave similarly to OPE. The sixth method, stratified, is a stratified sampling baseline limited to the case where \([-1,1]^{d}\) and \(\) is "well-spread". It partitions \([-1,1]^{d}\) into a grid of \(m\) bins, and then independently draws one element uniformly in the intersection of \(\) with each bin. It is a special case of projection DPP, which runs in \((nm)\) and has obvious pitfalls, like requiring that \(\) has a non-empty intersection with each bin, which is unlikely to be the case for non-uniformly spread datasets and high dimensions. Yet, this is a simple solution that one would likely implement to probe the benefits of negative dependence.

The performance metric.To investigate the cardinality of a coreset for a given error, we let \(Q_{}\) denote the quantile function of \(|L_{}(f)-L(f)|/L(f)\), the supremum over all queries of the relative error. Intuitively, \(Q_{}(0.9)=10^{-2}\) means that \(90\%\) of the sampled coresets have a worst case relative error below \(10^{-2}\). We shall look at how an estimated \(Q_{}(0.9)\) varies with \(m\), especially its slope in log-log plots with respect to \(m\). Now, the set \(\) of all queries for \(k\)-means in combinatorially large, even for small values of \(k\). Therefore, each time we need to evaluate the supremum of the relative error, we rather uniformly sample without replacement \(k\) elements of \(\), \(100\) times and independently, and we take the maximum value of the relative error among these \(100\) values. Moreover, for each method and each coreset size \(m\), the quantile function \(Q_{}(0.9)\) is estimated by an empirical quantile over 100 independent coresets sampled for each value of \(m\).

Results.We first consider a synthetic dataset of \(n=1024\) data points, sampled uniformly and independently in \([-1,1]^{d}\); see Figure 0(a). We consider \(d=2\) for demonstration purposes, but we have observed similar results for other small dimensions. Figure 0(b) depicts our estimate of \(Q_{}(0.9)\) as a function of the coreset size \(m\), in log-log format. The two i.i.d. baselines decrease as \(m^{-1/2}\), as expected. The stratified baseline, intuitively well-suited to uniformly-spread datasets, outperforms all other methods with a \(m^{-1}\) rate, consistent with its known optimal variance reduction (Novak, 1988). Finally, the \(m\)-DPP and the two DPPs also yield a faster decay, eventually outperforming the i.i.d. baselines as \(m\) grows. This is expected for the discretized OPE, as it follows from the theoretical results from Section 3; but it is interesting to see that the Gaussian \(m\)-DPP and the Vdm-DPP seem to reach a similar \(m^{-3/4}\) fast rate. For the Gaussian \(m\)-DPP, however, the performance depends on the value of the bandwidth of the Gaussian kernel: in Figure 0(c), we see that the rate of decay can go from i.i.d.-like to OPE-like as the bandwidth increases; this is expected from results like (Barthelme et al., 2023). Note that the color code of Figure 0(c) differs from other figures.

In the uniform dataset of Fig. 0(a), the sensitivity function is almost flat, which makes sensitivity behave like uniform. To give an edge to sensitivity, we now consider the trimodal dataset shown in Fig. 1(a), with an OPE sample superimposed. The performance of sensitivity improves; see Figure 1(b), while the determinantal samplers still outperform the independent ones thanks to a faster decay. For this dataset, it is not easy to stratify, and we thus do not show results for stratified. We note that the size of a marker placed at \(x\) is proportional to the corresponding weight \(1/K(x,x)\) in the estimator of the average loss. Equivalently, the marker size is inversely proportional to the marginal probability of \(x\) being included in the DPP sample.

Finally, we consider the classical MNIST dataset, after a PCA of dimension \(4\). Figure 1(c) shows again the faster decay of the performance metric for the two DPPs (OPE and Vdm-DPP), compared to the two independent methods. However, the advantage progressively disappears as the dimension increases beyond \(4\) (unshown), as expected from the gain in variance of the discretized multivariate OPE, which becomes negligible when \(d 1\); see Section 5 for suggestions on how to prove a dimension-independent decay. The source code used in this work is available at github.com/hsimonfroy/DPPcoresets, where DPP samplers are built upon the Python package DPPy (Gautier et al., 2019).

## 5 Discussion

Limitations.Our paper is a theoretical contribution, and our approach has several limitations before it can be a _practical_ addition to the coreset toolbox. The improvement over independent sampling relies on a variance scaling for linear statistics of a particular DPP, which itself relies on both 1) an Ansatz that the dataset was generated i.i.d. from some pdf \(\) with a large support, and 2) the availability of a good approximation to \(\); see Section 3 and (Bardenet, Ghosh, et al., 2021). While Item 1) is usually deemed to be reasonable in a wide range of situations, we solve Item 2) by

Figure 1: Results for the uniform dataset.

Figure 2: Results on other datasets.

relying on a kernel density estimator, which is costly to manipulate. Another limitation is that the improvement over independent sampling is in \(1/d\) and thus progressively vanishes as the dimension increases. Finally, a classical caveat is that although tractable, sampling a DPP still costs \((nm^{2})\), provided the kernel is available in diagonalized form.

Future work.The limitations above set up a research program. In particular, an intriguing observation in our empirical studies is the comparative performance of various DPP-based coreset samplers; several of them exhibit effective performance. While we have sharp theoretical guarantees for the discretized OPE-based scheme, obtaining similar guarantees and parameter-tuning protocols for other samplers, like \(m\)-DPPs, will be of great practical interest as they would bypass the need, e.g., for an approximation to the data-generating mechanism \(\). The DPP called Vdm-DPP in Section 4, which is itself an OPE for a discrete measure, might be a bridge between OPEs and \(m\)-DPPs, as Vdm-DPP can be seen as a limit of Gaussian \(m\)-DPPs (Barthelme et al., 2023). On a more general note, improving the computational complexity of sampling DPPs remains an active topic, and we should examine which techniques, e.g. by leveraging low-rank structures, preserve the small coreset property. Any breakthrough in the complexity of DPP sampling would also have salutary consequences for the broader program of negative dependence as a toolbox for machine learning. On the dependence of the rate to the dimension, we propose to investigate the impact of smoothness of the test functions on the rate: in numerical integration with mixtures of DPPs, smoothness does bring dimension-independent rates (Belhadji et al., 2020). Finally, in a more theoretical direction, extending concentration inequalities for linear statistics beyond the restricted range of \(\) appearing e.g. in Theorem 1 is a mathematically challenging problem, with potential learning-theoretic consequences.

RB and HSO acknowledge support from ERC grant Blackjack (ERC-2019-STG-851866) and ANR AI chair Baccarat (ANR-20-CHIA-0002). SG was supported in part by the MOE grants R-146-000-250-133, R-146-000-312-114, A-8002014-00-00 and MOE-T2EP20121-0013. HST was supported by the NUS Research Scholarship.