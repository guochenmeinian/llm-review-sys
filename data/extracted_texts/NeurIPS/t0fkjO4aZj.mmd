# A Unified Framework for Information-Theoretic Generalization Bounds

Yifeng Chu   Maxim Raginsky

{ychu26,maxim}@illinois.edu

Department of Electrical and Computer Engineering and Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA.

###### Abstract

This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in \(L_{_{p}}\) Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.

## 1 Introduction

The generalization error of a learning algorithm is a useful proxy for evaluating the performance of the learned model on previously unseen data. Formally, it is defined as the expected (absolute) difference between the population risk and the empirical risk of the hypothesis returned by the algorithm. One of the classical methods for estimating the generalization error is via uniform convergence of various empirical processes indexed by the hypothesis class . For example, in the analysis of Empirical Risk Minimization, one can estimate the expected generalization error via Rademacher averages, which can be bounded from above using chaining techniques .

However, the bounds based on uniform convergence are often too pessimistic and may even become vacuous when the hypothesis space is extremely large, a typical situation with deep neural net models. For this reason, it is preferable to obtain algorithm-dependent generalization bounds that take into account the joint distribution of the training samples and of the output hypothesis. In this context, one capitalizes on the intuition that the generalization ability of a learning algorithm should be related to the amount of information the output hypothesis reveals about the training data. This idea, which has origins in the work on PAC-Bayes methods , is the basis of the growing literature on information-theoretic generalization bounds, first proposed in  and further developed in  and many other works.

In fact, it is possible to effectively combine the information-theoretic approach with the classical framework based on various measures of complexity of the hypothesis class: One can use chaining techniques to successively approximate the hypothesis class by simpler model classes, which can then be analyzed using information-theoretic tools. This methodology, again originating in the PAC-Bayes literature , has been developed recently in . Our goal in this work is to develop these ideas further by giving a unified framework for information-theoretic generalization bounds, from which many of the existing results emerge as special cases.

### The main idea, informally

The main idea behind our framework is surprisingly simple. We first give an abstract description and then show how it can be particularized to various settings of interest. Let \((X_{t})_{t T}\) be a centered (zero-mean) stochastic process defined on a probability space \((,,)\) and indexed by the elements of some set \(T\). Let \(Q\) be a _Markov kernel_ from \(\) to \(T\), i.e., a measurable mapping taking each \(\) to a probability measure \(Q(|)\) on \(T\). Together, \(\) and \(Q\) define a probability measure \( Q\) on the product space \( T\). The mathematical object we would like to study is the expected value

\[ Q,X:=_{ T}X_{t}()Q( t|).().\]

For example, assuming that there exists a measurable map \(^{*}: T\), such that

\[X_{^{*}()}()=_{t T}X_{t}(),- \] (1)

we can take \(Q(A|):=_{\{^{*}() A\}}\) for all measurable subsets \(A\) of \(T\). Then

\[ Q,X=_{t T}X_{t}\]

is the expected supremum of \(X_{t}\), the central object of study in the theory of generic chaining, where \((T,d)\) is a metric space and increments \(X_{u}-X_{v}\) are "stochastically small" relative to \(d(u,v)\). Alternatively, consider a statistical learning problem with instance space \(\), hypothesis space \(\), and loss function \(:_{+}\). Let \(P_{Z}\) be the (unknown) probability law of the problem instances in \(\). Then we could take \(=^{n}\), \(=P_{Z}^{ n}\), \(T=\), and

\[X_{w}=_{i=1}^{n}L(w)-(w,Z_{i}),\]

where \(L(w):=_{Z P_{Z}}[(w,Z)]\) is the _population risk_ of \(w\). Let \(Q\) be a (randomized) learning algorithm that associates to each sample \(S=(Z_{1},,Z_{n})\) a probability measure \(Q(|S)\) on the hypothesis space \(\). Then

\[ Q,X=_{i=1}^{n }L(W)-(W,Z_{i})\]

is the expected generalization error of \(Q\). In either case, we can proceed to analyze \( Q,X\) via a combination of the following two steps:

* **Decorrelation** -- We can remove the correlations encoded in \( Q\) by choosing a convenient product measure \(\) on \( T\), so that (roughly) \[ Q,X Q| )}+\] provided the process \((X_{t})_{t T}\) is regular enough for the error term to be small. Here, we use the relative entropy (or information divergence) \(D(|)\) to illustrate the key idea with a minimum of detail; the precise description is given in Section 3.
* **Chaining in the space of measures** -- Since the process \((X_{t})_{t T}\) is centered and \(\) is a product measure, we automatically have \(,X=0\) even though \( Q,X 0\). We can therefore interpolate between \( Q\) and \(\) along a (possibly infinite) sequence \(Q_{0},Q_{1},,Q_{K}\) of Markov kernels, such that \( Q_{K}= Q\), \( Q_{0}=\), and the differences \( Q_{k},X- Q_{k-1},X\) are suitably small. Telescoping, we get \[ Q,X=_{k=1}^{K}  Q_{k},X- Q_{k-1},X.\] For each \(k\), we then apply the decorrelation procedure to the _increment process_\((X_{u}-X_{v})_{u,v T}\), with \(\) as before and with a suitably chosen family of couplings of \(Q_{k}(|)\) and \(Q_{k-1}(|)\). This step can be combined effectively with other techniques, such as symmetrization.

## 2 Preliminaries

Basic definitions.All measurable spaces in this paper are assumed to be standard Borel spaces. The set of all Borel probability measures on a space \(\) will be denoted by \(()\). A _Markov kernel_ from \((,)\) to \((,)\) is a mapping \(P_{Y|X}:\), such that \(P_{Y|X=x}():=P_{Y|X}(|x)\) is an element of \(()\) for every \(x\) and the map \(x P_{Y|X=x}(B)\) is measurable for every \(B\). The set of all such Markov kernels will be denoted by \((|)\).

The product of \(P_{X}()\) and \(P_{Y|X}(|)\) is the probability measure \(P_{X} P_{Y|X}()\) defined on product sets \(A B\) by \((P_{X} P_{Y|X})(A B):=_{A}P_{Y|X=x}(B)P_{X}(x)\) and then extended to all Borel subsets of \(\) by countable additivity. This defines a joint probability law for a random element \((X,Y)\) of \(\), so that \(P_{X}\) is the marginal law of \(X\), \(P_{Y|X}\) is the conditional law of \(Y\) given \(X\), and \(P_{Y}()=_{}P_{Y|X=x}()P_{X}(x)\) is the marginal law of \(Y\). The product measure \(P_{X} P_{Y}\), under which \(X\) and \(Y\) are independent, is a special case of this if we interpret \(P_{Y}\) as a trivial Markov kernel with \(P_{Y|X=x}=P_{Y}\) for all \(x\).

A _coupling_ of \(()\) and \(()\) is a probability measure \(P()\), such that \(P()=()\) and \(P()=()\). We will denote the set of all couplings of \(\) and \(\) by \((,)\). Let the space \(\) be equipped with a metric \(d\), and let \(_{p}\), for \(p 1\), denote the space of all probability measures \(\) on \(\), for which there exists some \(z_{0}\) such that \(_{}d(z,z_{0})^{p}(z)<\). Then the _\(p\)-Wasserstein distance_ between \(()_{p}\) and \(()_{p}\) is given by

\[_{p}(,):=_{(,)} d(x,y)^{p}( x,y)^{1/p}\]

(see  for details).

\(L^{p}\) and \(L_{_{p}}\) spaces.The \(L^{p}()\) norms of \(f:\), for \(()\) and \(p 1\), are defined as

\[\|f\|_{L^{p}()}:=_{}|f|^{p}\,^{1/p}\]

whenever the expectation on the right-hand side exists. We will often use the linear functional notation for expectations, i.e., \(,f=_{}f\,\).

For \(p 1\), define the function \(_{p}:_{+}_{+}\) by \(_{p}(x):=(x^{p})-1\). Its inverse is given by \(_{F}^{-1}(x)=(x+1)^{1/p}\), where \(\) will always denote natural logarithms. Some useful properties of these two functions are collected in Appendix A of Supplementary Material. The function \(_{p}\) arises in the context of controlling the tail behavior of random variables (see  for details). The _Orlicz \(_{p}\)-norm_ of a real-valued random variable \(X\) is defined as

\[\|X\|_{_{p}}:=c>0:_{p}  1},\]

and the tails of \(X\) satisfy \(\|X\| u\| Ke^{-Cu^{p}}\) for all \(u 0\) and some \(K,C>0\) if and only if \(\|X\|_{_{p}}<\). The Orlicz space \(L_{_{p}}\) is the space of all random variables \(X\) with \(\|X\|_{_{p}}<\). In particular, if \(X\) is \(\)-subgaussian, i.e., \(\|X\| u\| 2e^{-u^{2}/2^{2}}\) for all \(u 0\), then \(\|X\|_{_{2}}\); conversely, every \(X L_{_{2}}\) is \(\)-subgaussian with \( c\|X\|_{_{2}}\) for some absolute constant \(c>0\).

Information-theoretic quantities.The relative entropy (or information divergence) \(D(\|)\) between two probability measures \(,\) on the same space \(\) is defined as

\[D(\|):=,}{}\]

if \(\) (i.e., \(\) is absolutely continuous w.r.t. \(\)), and \(D(\|):=+\) otherwise. The following inequality will be useful (proofs of all results are in Appendix B of the Supplementary Material):

**Proposition 1**.: _If \(\), then for any \(p 1\)_

\[,_{p}^{-1}}{} D(\|)+1^{1/p}.\]The _conditional divergence_ between \(P_{V|U},Q_{V|U}(|)\) given \(P_{U}()\) is defined by

\[D(P_{V|U}\|Q_{V|U}|P_{U}):=D(P_{U} P_{V|U}\|P_{U} Q_{V|U}).\]

The mutual information \(I(X;Y):=D(P_{Y|X}\|P_{Y}|P_{X})\) and conditional mutual information \(I(X;Y|Z):=D(P_{Y|XZ}\|P_{Y|Z}|P_{XZ})\) are special cases of the above definition, and the identities

\[D(P_{Y|X}\|Q_{Y}|P_{X}) =I(X;Y)+D(P_{Y}\|Q_{Y})\] (2) \[D(P_{Y|XZ}\|Q_{Y|Z}|P_{XZ}) =I(X;Y|Z)+D(P_{Y|Z}\|Q_{Y|Z}|P_{Z}).\] (3)

hold whenever all the quantities are finite. See, e.g.,  for details.

## 3 The decorrelation lemma

All of our subsequent developments make use of the following _decorrelation lemma_:

**Lemma 1**.: _Let \(,\) be two probability measures on a space \(\) such that \(\), and let \(f,g:_{+}\) be two nonnegative measurable functions. Then the following inequalities hold:_

\[,fg 2^{1/p},f_{p}^{-1} }{}+,f_{p}( g)\] (4)

_and_

\[,fg 2^{1/p}\|f\|_{L^{2}()}+4^{1/p},f _{p}^{-1}}{}+4 ^{1/p}\|f\|_{L^{1}()},(g^{p})^{1/p}.\] (5)

The proof makes extensive use of various properties of \(_{p}\) and \(_{p}^{-1}\). In particular, Eq. (4) is a relaxation of the Young-type inequality \(xy_{p}^{*}(x)+_{p}(y)\), where \(_{p}^{*}(x):=_{y 0}(xy-_{p}(y))\) is the (one-sided) Lengendre-Fenchel conjugate of \(_{p}\). (We refer the reader to  for another use of duality in Orlicz spaces in the context of generalization bounds.)

Every use of Lemma 1 in the sequel will be an instance of the following scheme: Let \(P_{X}()\), \(Q_{Y}()\), and \(P_{Y|X}(|)\) be given, such that \(P_{Y|X=x} Q_{Y}\) for all \(x\). Let \((X,Y,)\) be a random element of \(\) with joint law \(P_{X} P_{Y|X} Q_{Y}\); in particular, \(\) is independent of \((X,Y)\). Furthermore, let \(f:_{+}\) and \(g:_{+}\) be given, such that \([_{p}(g(X,y))] 1\) for all \(y\). Then, applying Lemma 1 conditionally on \(X=x\) with \(=P_{Y|X=x}\), \(=Q_{Y}\), \(f\), and \(g(x,)\), and then taking expectations w.r.t. \(P_{X}\), we obtain

\[[f(Y)g(X,Y)] 2^{1/p}f(Y)_{p}^{-1} P_{Y|X}}{Q_{Y}}(Y)+[f( )].\]

In specific cases, the quantity on the right-hand side can be further upper-bounded in terms of the information divergences \(D(P_{Y|X}\|Q_{Y})\) using Proposition 1.

## 4 Some estimates for the absolute generalization error

We adopt the usual set-up for the analysis of (possibly randomized) learning algorithms and their generalization error. Let an instance space \(\), a hypothesis space \(\), and a nonnegative loss function \(:_{+}\) be given. A _learning algorithm_ is a Markov kernel \(P_{W|S}\) from the product space \(^{n}\) into \(\), which takes as input an \(n\)-tuple \(S=(Z_{1},,Z_{n})\) of i.i.d. random elements of \(\) with unknown marginal probability law \(P_{Z}\) and generates a random element \(W\) of \(\). We define the _empirical risk_ and the _expected_ (or _population_) _risk_ of each \(w\) by

\[L_{n}(w):= P_{n},(w,)=_{i=1}^{n}(w,Z_ {i}), L(w):= P_{Z},(w,)=[(w,Z)]\]

where \(P_{n}\) is the empirical distribution of \(S\), and the _pointwise generalization error_ by

\[(w,S):=L(w)-L_{n}(w).\]It will also be convenient to introduce an auxiliary \(n\)-tuple \(S^{}=(Z_{1}^{},,Z_{n}^{}) P_{Z}^{ n}\), which is independent of \((S,W) P_{Z}^{ n} P_{W|S}\). We will use \(\) to denote the pair \((S^{},S)\) and write \(L_{n}^{}(w)\) for the empirical risk of \(w\) on \(S^{}\).

As a first illustration of our general approach, we show that it can be used to recover some existing results on the generalization error, including the bounds of Xu and Raginsky  involving the mutual information and the bounds of Steinke and Zakynthinou  involving the conditional mutual information. We start with the following estimate on the expected value of \(|(W,S)|\):

**Theorem 1**.: _Assume the random variables \((w,Z)\), \(w\), are \(\)-subgaussian when \(Z P_{Z}\). Let a learning algorithm \(P_{W|S}\) be given. Then, for any \(Q_{W}()\),_

\[[|(W,S)|]}{n}} _{2}^{-1}P_{W|S}}{Q_{ W}}+1,\] (6)

_where the expectation on both sides is w.r.t. \(P_{S} P_{W|S}=P_{Z}^{ n} P_{W|S}\)._

The key step in the proof is to apply the decorrelation lemma, conditionally on \(S\), to \(=P_{W|S}\), \(=Q_{W}\), \(f(w)=\), and \(g(w)=(w,S)|}{}\). The same subgaussianity assumption was also made by Xu and Raginsky . Minimizing the right-hand side of (6) over \(Q_{W}\), we recover their generalization bound up to a multiplicative constant and an extra \(O(1/)\) term (which is unavoidable since we are bounding the expected _absolute_ generalization error):

**Corollary 1**.: _Under the assumptions of Theorem 1,_

\[[|(W,S)|]}{n}}I(W;S)+ 4.\] (7)

A notable shortcoming of Theorem 1 and Corollary 1 is that they yield vacuous bounds whenever the mutual information \(I(W;S)\) is infinite, which will be the case, e.g., when the marginal probability laws \(P_{Z}\) and \(P_{W}\) are nonatomic (i.e., assign zero mass to singletons) and the learning algorithm is deterministic. To remove this drawback, we will use an elegant auxiliary randomization device introduced by Steinke and Zakynthinou .

Let \(=(_{1},,_{n})\) be an \(n\)-tuple of i.i.d. Rademacher random variables, i.e., \([_{i}= 1]=1/2\), independent of \(\). For each \(i\) let \(_{i}^{1}:=Z_{i}\) and \(_{i}^{-1}:=Z_{i}^{}\) and let \(=_{ W}\) be the joint probability law of \((,,W)\), such that \(_{}=P_{} P_{}\) and \(_{W|}:=P_{W|^{}}\) where \(S^{}:=(_{1}^{_{1}},,_{n}^{ _{n}})\). In other words, under \(,\) and \(\) are independent and have their respective marginal distributions, while \(W\) is generated by feeding the learning algorithm \(P_{W|S}\) with the tuple \(^{}\). Consequently, \(W\) is independent of \(^{-}=(_{1}^{-_{1}},,_{n }^{-_{n}})\). Then, letting \(P\) be the joint law of \((,W)\), we have

\[_{P}[|(W,S)|] =_{P}_{P}[L_{n}^{}(W)-L_{n}(W)|S,W]\] \[_{P}|L_{n}^{}(W)-L_{n}(W)|\] \[=_{}_{i=1}^{n}( W,_{i}^{-_{i}})-(W,_{i}^{_{i}}) \] \[=_{}_{i=1}^{n}_ {i}(W,Z_{i}^{})-(W,Z_{i}).\]

Thus, all the analysis can be carried out w.r.t. \(\), as in the following:

**Theorem 2**.: _Assume there exists a function \(:_{+}\), such that \(|(w,z)-(w,z^{})|(z,z^{})\) for all \(w\) and \(z,z^{}\). Then for any Markov kernel \(Q_{W|}\) with access to \(\) but not to \(\) we have_

\[_{P}[|(W,S)|]}{n}_{P} \|()\|_{^{2}}_{2}^{-1} _{W|}}{Q_{W|}}+1 ,\] (8)

_where \(\|()\|_{^{2}}:=_{i=1}^{n}(z_{i},z_{i}^{ })^{2}^{1/2}\)._The same assumption on \(\) was also made in . Optimizing over \(Q_{W|}\), we can recover their Theorem 5.1 (again, up to a multiplicative constant and a \(O(1/)\) fluctuation term):

**Corollary 2**.: _Under the assumptions of Theorem 2,_

\[_{P}[|(W,S)|][^{2}( Z,Z^{})]I(W;|)+4},\] (9)

_where \(Z\) and \(Z^{}\) are independent samples from \(P_{Z}\) and where the conditional mutual information is computed w.r.t. \(\)._

The main advantage of using conditional mutual information is that it never exceeds \(n 2\) (of course, the bound is only useful if \(I(W;|)=o(n)\)).

## 5 Estimates using couplings

We now turn to the analysis of \([(W,S)]\) using couplings. The starting point is the following observation: With \((S^{},S,W)\) be constructed as before, consider the quantities

\[_{n}(w):=L^{}_{n}(w)-L_{n}(w)_{i=1}^{n} (w,Z^{}_{i})-(w,Z_{i}).\]

Then, using the fact that \( P_{} Q_{W},_{n}=0\) for any \(Q_{W}()\), we have

\[[(W,S)] = P_{} P_{W|S},_{n}-  P_{} Q_{W},_{n}\] \[=_{}P_{}( ) P_{W|S=s},_{n}- Q_{W},_{n}.\] (10)

This suggests the idea of introducing, for each \(s^{n}\), a coupling of \(P_{W|S=s}\) and \(Q_{W}\), i.e., a probability law \(P_{UV|S=s}\) for a random element \((U,V)\) of \(\) with marginals \(P_{U}=P_{W|S=s}\) and \(P_{V}=Q_{W}\). We then have the following:

**Theorem 3**.: _For \(u,v\) and \(=(s,s^{})^{n}^{n}\), define_

\[^{2}(u,v,):=_{i=1}^{n}(u,z^{}_{i}) -(v,z^{}_{i})-(u,z_{i})-(v,z_{i}) ^{2}\,.\] (11)

_Then, for any \(Q_{W}()\), any family of couplings \(P_{UV|S=s}(P_{W|S=s},Q_{W})\) depending measurably on \(s^{n}\), and any \(_{UV}()\),_

\[[(W,S)]}{n} (U,V,)_{2}^{-1}P_{UV|S}}{_{ UV}}+[^{2}(,,)|]} ,\] (12)

_where the expectation on the right-hand side is w.r.t. the joint law of \((U,V,,,)\), under which \((S,U,V)\) are distributed according to \(P_{S} P_{UV|S}\), \((,)\) are distributed according to \(_{UV}\) independently of \((U,V,S)\), and \(S^{}\) is distributed according to \(P_{S}\) independently of everything else._

The proof makes essential use of symmetrization using an auxiliary \(n\)-tuple \(\) of i.i.d. Rademacher random variables, which allows us to apply Lemma 1 conditionally on \(\).

The coupling-based formulation looks rather complicated compared to the setting of Section 4. However, being able to choose not just the "prior" \(Q_{W}\), but also the couplings \(P_{UV|S}\) of \(P_{W|S}\) and \(Q_{W}\) and the reference measure \(_{UV}\), allows us to overcome some of the shortcomings of the set-up of Section 4. Consider, for example, the case when the learning algorithm ignores the data, i.e., \(P_{W|S}=P_{W}\). Then we can choose \(Q_{W}=P_{W}\), \(P_{UV|S}(u,v)=P_{W}(u)_{u}(v)\), where \(_{u}\) is the Dirac measure concentrated on the point \(u\), and \(_{UV}=P_{UV}\) (since the latter does not depend on \(S\)). With these choices, \(U=V\) and \(=\) almost surely, so the right-hand side of (12) is identically zero. By contrast, the bounds of Theorems 1 and 2 always include an additional \(O(1/)\) term even when \(W\) and \(\) are independent.

Moreover, Theorem 3 can be used to recover the bounds of Theorems 1 and 2 up to multiplicative constants. For example, to recover Theorem 1, we apply Theorem 3 with \(P_{UV|S}=P_{W|S} Q_{W}\), \(_{UV}=Q_{W} Q_{W}\), and with an estimate on \((U,V,)\) based on the subgaussianity of \((w,Z)\).

For a more manageable bound that will be useful later, let us define the following for \(u,v\):

\[d_{S,}(u,v) :=_{i=1}^{n}(u,Z_{i})-(v,Z_{ i})^{2}^{1/2}\|(u,)-(v,)\|_{L^{2}(P_{n})}\] \[d_{}(u,v) :=(u,Z)-(v,Z)^{2} ^{1/2}\|(u,)-(v,)\|_{L^{2}(P_{Z})},\]

**Corollary 3**.: _Under the assumptions of Theorem 3,_

\[[(W,S)]} d_{}(U,V)+d_{S,}(U,V)_{2}^{-1}P_{UV|S}} {_{UV}}+d_{}(,).\]

## 6 Refined estimates via chaining in the space of measures

We now combine the use of couplings as in Section 5 with a chaining argument. The basic idea is as follows: Instead of coupling \(P_{W|S}\) with \(Q_{W}\) directly, we interpolate between them using a (possibly infinite) sequence of Markov kernels \(P_{W|S}^{0},P_{W|S}^{1},,P_{W|S}^{K}\), such that \(P_{W|S}^{0}=Q_{W}\) and \(P_{W|S}^{K}=P_{W|S}\) (or \(_{k}P_{W|S}^{k}=P_{W|S}\) in an appropriate sense, e.g., weakly for each \(S\), if the sequence is infinite). Given any such sequence, we telescope the terms in (10) as follows:

\[[(W,S)]=_{}P_{} ()_{k=1}^{K} P_{W|S=s}^{k},_{n }- P_{W|S=s}^{k-1},_{n}.\]

For each \(k\), we can now choose a family of random couplings \(P_{W_{k}W_{k-1}|S}(P_{W|S}^{k},P_{W|S}^{k-1})\) and a deterministic probability measure \(_{W_{k}W_{k-1}}()\). The following is an immediate consequence of applying Corollary 3 to each summand:

**Theorem 4**.: _Let \(P_{W|S}\), \(Q_{W}\), \(P_{W_{k}W_{k-1}|S}\), and \(_{W_{k}W_{k-1}}\) be given as above. Then_

\[[(W,S)]\] \[}_{k=1}^{K}d_{ }(W_{k},W_{k-1})+d_{S,}(W_{k},W_{k-1})_{2}^{-1} P_{W_{k}W_{k-1}|S}}{_{W_{k}W_{k-1}}}+d_ {}(_{k},_{k-1}),\]

_where in the \(k\)th term on the right-hand side \((S,W_{k},W_{k-1})\) are jointly distributed according to \(P_{S} P_{W_{k}W_{k-1}|S}\) and \((_{k},_{k-1})\) are jointly distributed according to \(_{W_{k}W_{k-1}}\)._

Apart from Theorem 1, we have been imposing only minimal assumptions on \(\) and then using symmetrization to construct various subgaussian random variables conditionally on \(W\) and \(\). For the next series of results, we will assume something more, namely that \((,d)\) is a metric space and that the following holds for the centered loss \((w,z):=(w,z)-[(w,Z)]\):

\[_{i=1}^{n}((u,Z_{i})-(v,Z_{i}))_{ _{2}}d(u,v), u,v.\] (13)

In other words, the centered empirical process \(}_{i=1}^{n}(w,Z_{i})\) indexed by the elements of \((,d)\) is a subgaussian process .

**Theorem 5**.: _Assume (13). Then_

\[[(W,S)]}_{k=1}^{K} d(W_{k},W_{k-1})_{2}^{-1}P_{W_{k}W_{k-1}|S} }{_{W_{k}W_{k-1}}}+d(_{k},_{k-1})\] (14)

As a byproduct, we recover the stochastic chaining bounds of Zhou et al.  (which, in turn, subsume the bounds of Asadi et al. ):

**Corollary 4**.: _Let \(P_{Z}\) and \(P_{W|S}\) be given, and let \(P_{W}\) be the marginal law of \(W\). Let \(P_{W_{k}|S}_{k 0}\) be a sequence of Markov kernels satisfying the following conditions: (i) \(P_{W_{0}|S}=P_{W}\); (ii) \(P_{W_{k}|S}P_{W|S}\); (iii) for every \(k 1\), \(S-W-W_{k}-W_{k-1}\) is a Markov chain. Then_

\[[(W,S)] }_{k=1}^{}d(W_{k},W_{k-1})}||P_{S})}+1\] (15) \[}_{k=1}^{}[d^{2}(W _{k},W_{k-1})]}(;S)}+2).\] (16)

Finally, we give an estimate based on \(2\)-Wasserstein distances (cf. Section 2 for definitions and notation). Let \(_{2}(,)\) be the \(2\)-Wasserstein distance on \(_{2}()\) induced by the metric \(d\) on \(\). A (constant-speed) _geodesic_ connecting two probability measures \(P,Q_{2}()\) is a continuous path \( t_{t}_{2}()\), such that \(_{0}=P\), \(_{1}=Q\), and \(_{2}(_{s},_{t})=(t-s)_{2}(P,Q)\) for all \(0 s t 1\). Then we have the following corollary of Theorem 5:

**Corollary 5**.: _Let \(P_{Z}\) and \(P_{W|S}\) be given, and let \(P_{W}\) be the marginal law of \(W\). With respect to 2-Wasserstein distance, let \((P_{W_{k}|S})_{0 k K}\) be some points on the constant-speed geodesic \((_{t})_{t}\) with endpoints \(_{0}=P_{W_{0}|S}=P_{W|S}\) and \(_{1}=P_{W_{K}|S}=P_{W}\) (where \(K\) may be infinite), i.e., there exist some \(t_{0}=0<t_{1}<<t_{k}<<t_{K}=1\), such that \(P_{W_{k}|S}=_{t_{k}}\) for \(k=0,1,\). For each \(k\) let \(P_{W_{k}W_{k-1}|S}\) be the optimal coupling between the neighboring points \(P_{W_{k-1}|S}\) and \(P_{W_{k}|S}\), i.e., the one that achieves \(_{2}(P_{W_{k-1}|S},P_{W_{k}|S})\). Then_

\[[(W,S)]\] \[}2\,[_{2}(P_{W|S },P_{W})]+_{k=1}^{K}_{2}(P_{W_{k}|S},P_{W_{k-1} |S})W_{k-1}|S}||P_{W_{k}W_{k-1}})}\,.\] (17)

Observe that the first term on the right-hand side of (17) is the expected \(2\)-Wasserstein distance between the posterior \(P_{W|S}\) and the prior \(P_{W}\), while the second term is a sum of "divergence weighted" Wasserstein distances. Also note that the form of the second term is in the spirit of the Dudley entropy integral , where the Wasserstein distance corresponds to the radius of the covering ball and the square root of the divergence corresponds to square root of the metric entropy. We should also point out that the result in Corollary 5 does not require Lipschitz continuity of the loss function \((w,z)\) w.r.t. the hypothesis \(w\), except in a weaker stochastic sense as in (13), in contrast to some existing works that obtain generalization bounds using Wasserstein distances .

## 7 Tail estimates

Next, we turn to high-probability tail estimates on \((W,S)\). We start with the following simple observation: Assume \((w,Z)\) is \(\)-subgaussian for all \(w\) when \(Z P_{Z}\). Then, for any \(Q_{W}()\) such that \(P_{W|S=s} Q_{W}\) for all \(s^{n}\), we have

\[^{2}(W,S)}{6^{2}/n}- 1+P_{W|S}}{Q_{W}}(W) ^{2}(,S)}{6^{ 2}/n} 1\]

with \( Q_{W}\) independent of \((S,W)\). Thus, by Markov's inequality, for any \(0<<1\),

\[|(W,S)|>}_{2 }^{-1}P_{W|S}}{Q_{W}}(W)+}.\]

In other words, \(|(W,S)|}_{2}^{-1}P_{W|S}}{Q_{W}}(W)\) with high \(P_{SW}\)-probability. Similar observations are made by Hellstrom and Durisi  with \(Q_{W}=P_{W}\), giving high-probability bounds of the form \(|(W,S)|D(P_{W|S}||P_{W})}{n}}\). Generalization bounds in terms of the divergence \(D(P_{W|S}||P_{W})\) are also common in the PAC-Bayes literature . Moreover, using the inequality (5) in Lemma 1, we can give high \(P_{S}\)-probability bounds on the conditional expectation

\[ P_{W|S},|(W,S)|= P_{W|S},|L(W)-L_{n}(W)|.\]

**Theorem 6**.: _Assume \((w,Z)\) is \(\)-subgaussian for all \(w\) when \(Z P_{Z}\). Then, for any \(Q_{W}()\), the following holds with \(P_{S}\)-probability of at least \(1-\):_

\[ P_{W|S},|(W,S)|}{n}} P_{W|S},_{2}^{-1}P_{W|S}}{ Q_{W}}+1+}.\]

Another type of result that appears frequently in the literature on PAC-Bayes methods pertains to so-called _transductive bounds_, i.e., inequalities for the difference between

\[ P_{n}^{} P_{W|S},- P_{n}^{}  Q_{W},_{i=1}^{n}[(W,Z_{ i}^{})-(,Z_{i}^{})|],\]

and

\[ P_{n} P_{W|S},- P_{n} Q_{W}, _{i=1}^{n}[(W,Z_{i})-(,Z_{i})| ],\]

where \(Q_{W}\) is some fixed "prior" and where \( Q_{W}\) is independent of \((S^{},S,W)\). Using our techniques, we can give the following general transductive bound:

**Theorem 7**.: _Let \(P_{W|S}\) and \(Q_{W}\) be given and take any \((P_{W_{k}W_{k-1}|S})_{k=1}^{K}\) and \((_{W_{k}W_{k-1}})_{k=1}^{K}\) as in Theorem 4. Also, let \(=(p_{1},p_{2},)\) be a strictly positive probability distribution on \(\). Then the following holds with \(P_{S}\)-probability at least \(1-\):_

\[ P_{n}^{} P_{W|S},- P _{n}^{} Q_{W},- P_{n} P_{W |S},- P_{n} Q_{W},\] \[}_{k=1}^{K}W_{k-1}},d_{,}^{2}}+P_{W_{k}W_{k-1}|S},d_{ ,}_{2}^{-1}P_{W_{k}W_{k-1}|S}}{ _{W_{k}W_{k-1}}}\] \[+P_{W_{k}W_{k-1}|S},d_{ ,}}},\]

_where_

\[d_{,}^{2}(u,v):=_{i=1}^{n}(u,Z_ {i})-(v,Z_{i})^{2}+(u,Z_{i}^{})-(v,Z_{i}^{ })^{2}.\]

This result subsumes some existing transductive PAC-Bayes estimates, such as Theorem 2 of Audibert and Bousquet . Let us briefly explain how we can recover this result from Theorem 7. Assume that \(\) is countable and let \((_{k})\) be an increasing sequence of finite partitions of \(\) with \(_{0}=\{\}\). For each \(k\) and each \(w\), let \(A_{k}(w)\) be the unique set in \(_{k}\) containing \(w\). Choose a representative point in each \(A_{k}\) and let \(_{k}\) denote the set of all such representatives, with \(_{0}=\{w_{0}\}\). Take \(P_{W_{}|S}=P_{W|S}\) and \(P_{W_{0}}=Q_{W}=_{w_{0}}\). Now, for each \(k 0\), we take \(P_{W_{k}|S}\) as the _projection_ of \(P_{W|S}\) onto \(_{k}\), i.e., the finite mixture

\[P_{W_{k}|S}:=_{w_{k}}P_{W|S}(A_{k}(w))_{w}.\]

Moreover, given some "prior" \((),\) we can construct a sequence \((_{k})_{k=0}^{}\) of probability measures with \(_{}=\) and \(_{0}=_{w_{0}}\), such that \(_{k}\) is a projection of \(\) onto \(_{k}\). Now observe that, for each \(k\), \(S-W_{}-W_{k}-W_{k-1}\) is a Markov chain. Indeed, if we know \(P_{W_{k}|S}\), then we can construct \(P_{W_{k}|S}\) for any \(<k\) without knowledge of \(S\). With these ingredients in place, let us choose \(P_{W_{k}W_{k-1}|S}=P_{W_{k-1}|W_{k}} P_{W_{k}|S}\) and \(_{W_{k}W_{k-1}}=_{k} P_{W_{k-1}|W_{k}}\). Then, using Cauchy-Schwarz and Jensen, we conclude that the following holds with \(P_{S}\)-probability at least \(1-\):

\[ P_{n}^{} P_{W|S},- P _{n}^{}_{w_{0}},- P_{n}  P_{W|S},- P_{n}_{w_{0}},\] \[}_{k=1}^{} P_{W_{k-1}|W_{k}},d_{,}^{2}}\] \[+|S} P_{W_{k-1}|W_{k}},d_ {,}^{2}D(P_{W_{k}|S}\|_{k})+ }}.\]

This recovers [17, Thm. 2] up to an extra term that scales like \(}_{k} P_{W_{k-1}|W_{k}},d_{ ,}^{2}}\).

The Fernique-Talagrand bound

As a bonus, we show that a combination of decorrelation and chaining in the space of measures can be used to recover the upper bounds of Fernique  and Talagrand  on the expected supremum of a stochastic process in terms of majorizing measures (see Eq. (19) below and also ).

For simplicity, let \((T,d)\) be a finite metric space with \((T)=\{d(u,v):u,v T\}<\). Let \(B(t,r)\) denote the ball of radius \(r 0\) centered at \(t T\), i.e., \(B(t,r):=\{u T:d(u,t) r\}\). Let \((X_{t})_{t T}\) be a centered stochastic process defined on some probability space \((,,)\) and satisfying

\[\![_{p}\!(-X_{v}|}{d(u,v)})]  1, u,v T\] (18)

for some \(p 1\). Then we can obtain the following result using chaining in the space of measures and decorrelation estimates:

**Theorem 8**.: _Let \(\) be a random element of \(T\), i.e., a measurable map \(: T\) with marginal probability law \(\). Then for any \((T)\) we have_

\[\![X_{}](T)+_{T}_{0}^{ (T)}()^{1/p} \,(t).\]

Applying Theorem 8 to \(^{*}\) defined in (1) and then minimizing over \(\), we recover a Fernique-Talagrand type bound on the expected supremum of \(X_{t}\):

\[\![_{t T}X_{t}]=\![X_{^{*}} ](T)+_{(T)}_{t T}_ {0}^{(T)}()^{1/p} .\] (19)

## 9 Conclusion and future work

In this paper, we have presented a unified framework for information-theoretic generalization bounds based on a combination of two key ideas (decorrelation and chaining in the space of measures). However, our method has certain limitations, which we plan to address in future work. For example, it would be desirable to cover the case of processes satisfying Bernstein-type (mixed \(_{1}\) and \(_{2}\)) increment conditions. It would also be of interest to see whether there are any connections to the convex-analytic approach of Lugosi and Neu . Finally, since our method seamlessly interpolates between Fernique-Talagrand type bounds and information-theoretic bounds, we plan to use it to further develop the ideas of Hodgkinson et al. , who were the first to combine these two complementary approaches to analyze the generalization capabilities of iterative learning algorithms.