# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

As the size of LLMs continues to grow, the potential harms caused by model-generated text are of increasing concern (Bender et al., 2021; Bommasani et al., 2021; Dinan et al., 2021; Kenton et al., 2021). A comprehensive taxonomy of ethical risks associated with recent LLMs is introduced in (Weidinger et al., 2022). One of its most pressing concerns is the risk of misinformation, stemming from the generation of erroneous, deceptive, irrational, or substandard information, defined as LLM outputting false, misleading, nonsensical or poor quality information, without malicious intent of the users.2 Potential harms of misinformation range from deceiving a person, causing physical damage, and amplifying the society's distrust on the usage of LLM-incorporated systems (Weidinger et al., 2022). For example, Galactic (Taylor et al., 2022), an LLM trained using a wide range of scientific sources including papers, reference materials, knowledge bases, etc., was reported3 to generate a fake study about the benefits of eating crushed glass. The aforementioned instance highlights the limitations of current LLMs in consistently delivering factually correct responses to user queries, a deficiency that potentially yield hazardous outcomes.

Despite the existence of several prior analyses, concerns are still far from being fully resolved. First of all, the evaluation of misinformation harm in existing LLMs has been conducted only using relatively simple formats such as multiple-choice (MC) question-answering (QA) task (Hendrycks et al., 2021) or cloze task (Petroni et al., 2019). These tasks are usually either formalized as a completion task, where LLMs are required to predict only a single token instead of generating full sentences, or using relatively simple evaluation metrics, e.g., accuracy, perplexity (Dinan et al., 2019), ROUGE score (Lin, 2004) and BLEU score (Khashabi et al., 2021), for the ease of evaluation (Hendrycks et al., 2021). In the context of long-form (LF)3 generation, Zhang et al. (2023) found that an incorrectly generated token at the outset will often be followed by a subsequent erroneous explanation; more generally, LLMs are reported to have the tendency to generate false statements, ranging from subtle inaccuracies to blatantly inauthentic claims (Lin et al., 2022). These phenomena highlight the importance of conducting LF generation evaluations for LLMs. Yet, there are no sufficient datasets available for such evaluations, especially in knowledge-intensive domains. Furthermore, due to the unbalanced distribution in available language-related resources (Zeng et al., 2023), a substantial number of existing datasets and benchmarks only focus on measuring the misinformation in English, which impedes similar evaluations from being performed in other languages. A brief summary of previous datasets on misinformation evaluation is shown in Table 1; even though some of them are not initially designed for misinformation evaluation, we found that they can be easily used for it.

To motivate the misinformation evaluation on LLMs, we propose **C**hinese **b**n**m**ARk for misinformation **E**valuation in **M**aternity and **I**n**fant care (**CARE-MI**), a benchmark to test medical-related misinformation of LLMs in Chinese. CARE-MI specifically focuses on the sub-domain of maternity and infant care, a topic in which users are prone to generate questions, especially expectant and first-time parents, about multiple issues, e.g., pregnancy and/or baby-related illnesses, symptoms, milestones, etc., and in which LLMs should respond factually and without errors. Although there are already existing datasets focusing on the medical domain, until now, there is no benchmark suitable for evaluating misinformation on such an important and sensitive topic as the maternity and infant care, neither in English nor in Chinese. The closest dataset to CARE-MI is MATINF (Xu et al., 2020), which, differently from us, focuses on community QA and contains neither expert-level annotations nor supporting evidence documents, making it not suitable for misinformation evaluation. Our benchmark, however, is not designed for directly evaluating user-LLM interactions, as most of our questions require expert-level knowledge and contain medical norms, but it is a necessary prerequisite for LLMs to perform well in those cases.

Additionally, we conduct an extensive evaluation on recent Chinese LLMs. The results indicate that all current models are not able to maintain an acceptable performance while answering domain-related questions. We further explore the paradigm for automatic evaluation on LF generation of LLMsas a completion to the benchmark dataset for the purpose of efficient and accurate evaluation. We test multiple judgment models trained on the same set of questions as in the benchmark, with 1) synthetically generated positive and negative answers, and 2) expert-level annotations on the models' outputs, and 3) expert-annotated knowledge. The whole pipeline, encompassing the development of the benchmark and the training of the judgment model, can also serve as an innovative paradigm for creating similar benchmarks in other knowledge-intensive domains or low-resourced languages.

## 2 CARE-MI data acquisition

With no existing datasets on the topic (_maternity and infant care_), language (_Chinese_), and task (_misinformation evaluation in LF generation_) of interest, we construct CARE-MI from multiple data sources. We utilize two knowledge graph (KG) datasets and two MCQA datasets as our data sources. The collected data is then filtered to align with our focused topic.

KG datasetsWe rely on two medical-based KG datasets in Chinese: Bio-Medical Informatics Ontology System (BIOS) (Yu et al., 2022) and CPubMed (CPu, 2021), both of which come with <head, relation, tail> triplets. BIOS is a machine-generated bio-medical KG built on top of the abstracts and central articles from PubMed,5 which is a search engine for bio-medical articles. Both head and tail in BIOS are concepts representing the nodes in the KG; each concept includes multiple terms that are considered synonymous. In total, it contains 4.1 million concepts, 7.4 million terms, and 7.3 million relations. As the data is gathered from PubMed, BIOS triplets are collected in English and later translated into Chinese (Luo et al., 2021); translation quality is ensured by applying back-translation and filtering out samples with low confidence. On the other hand, CPubMed is an

  
**Dataset** & **Language** & **\#Question** & **LF** & **Supervised** \\   \\ COMMONSENSEQA (Talmor et al., 2019) & English & 12,247 & ✗ & ✓ \\ Wizard of Wikipedia (Dinan et al., 2019) & English & 201,999 & ✓ & ✗ \\ LAMA (Petroni et al., 2019) & English & - & ✗ & ✗ \\ NO (Kwiatkowski et al., 2019) & English & 323,045 & ✓ & ✓ \\ ELIS (Fan et al., 2019) & English & \(\)272,000 & ✓ & ✗ \\ MMLU (Hendrycks et al., 2021) & English & 15,908 & ✗ & ✗ \\ COM2SENSE (Singh et al., 2021) & English & \(\)4,000 & ✗ & ✓ \\ GOOAQ (Khashabi et al., 2021) & English & \(\)3,100,000 & ✓ & ✗ \\ KMIR (Gao et al., 2022) & English & 16,000\({}^{}\) & ✗ & ✓ \\ TruthfulQA (Lin et al., 2022) & English & 817 & ✓ & ✓ \\ ScienceQA (Lu et al., 2022) & English & 21,208 & ✗ & ✓ \\ M3KE (Liu et al., 2023) & Chinese & 20,477 & ✗ & ✗ \\   \\ JEC-QA (Zhong et al., 2019) & Chinese & 26,365 & ✗ & ✗ \\ CaseHOLD (Zheng et al., 2021) & English & 53,137 & ✗ & ✗ \\   \\ cMedQA2 (Zhang et al., 2018) & Chinese & 108,000 & ✓ & ✗ \\ PubMedQA (Jin et al., 2019) & English & \(\)1,000\({}^{}\) & ✗ & ✓ \\ MATINF (Xu et al., 2020) & Chinese & \(\)1,070,000 & ✗ & ✗ \\ MEDQA (Jin et al., 2020) & Chinese/English\({}^{}\) & 61,097 & ✗ & ✓ \\ CMEIE (Guan et al., 2020) & Chinese & 22,406 & ✗ & ✓ \\ BiolAMA (Sung et al., 2021) & English & \(\)49,000 & ✗ & ✗ \\ MLEC-QA (Li et al., 2021) & Chinese & 136,236 & ✗ & ✓ \\
**CARE-MI (ours)** & Chinese & 1,612 & ✓ & ✓ \\   

Table 1: A brief summary of datasets in misinformation evaluation. _LF_: whether the dataset is designed for LF generation task; _Supervised_: whether the dataset construction involves human supervision. \({}^{}\)We refer to the selected samples that have been annotated by annotators. \({}^{}\)We are only referring to the expert annotated part of PubMedQA. \({}^{}\)For _Chinese_ mentioned here, we are referring to both traditional and simplified Chinese.

[MISSING_PAGE_FAIL:4]

CARE-MI benchmark

We construct the CARE-MI benchmark on top of the samples acquired in Section 2. The benchmark is based on two parts: a _synthetic data generation_ process and a set of _judgment models_. With the synthetic data generation process, we create samples in the desired format for misinformation evaluation on LLMs. Then, empowered by the judgment model (see Section 4.3), we offer an automated metric for efficient and accurate LF generation evaluation, aiming to simplify human-based evaluation which is not only expensive but time-consuming. Note that both the synthetic data generation process and the judgment model construction are domain-agnostic and can be easily applied to other misinformation evaluation topics. In total, CARE-MI benchmark contains 1,612 samples; it is intended for LF generation evaluation under zero-shot (Wei et al., 2022, Brown et al., 2020) setting. Statistics regarding the question length can be found in Figure 1. More details can be found in Appendix B.1.

The synthetic data generation process is summarized in Figure 2. It consists of five components: 1) true statement generation, 2) false statement generation, 3) question generation, 4) knowledge retrieval, and 5) expert annotation.

True statement generationWe define _true statements_ as sentences that are evidence-based and factually correct, for example, _Placenta previa may potentially lead to preterm birth_. We generate true statements from the samples collected in Section 2. For samples from KG datasets, we build the true statements heuristically from the triplets with rule-based methods. For samples from MCQA datasets, we formalize the generation of true statements as a QA2D task where the model is required to perform sentence transformations to combine the question and answer into a declarative sentence (Demszky et al., 2018). The generation is done by using the combination of the rule-based method and an off-the-shelf model for simple cases such that the question from the QA pair ends with a pre-defined set of tokens such as "\(}{}"\) (referred as _is_ in English), we directly concatenate the question with the answer as the synthetic true statement; otherwise, the generation is done using the GPT-3.5-turbo (OpenAI, 2023a). Details regarding the implementation of the rule-based system can be found in our code repository; details about the prompts for true statement generation are in Appendix B.8.

False statement generationSimilar to the definition of _true statement_, we define _false statements_ as sentences that are factually incorrect, for example, _progesterone is not a hormone_. We approach the construction in two different ways: _negation_ and _replacement_, corresponding to two types of false statements. For negated false statements, the generation is done on all available generated true statements where we generate the false statements by directly performing negation on the corresponding true ones; this construction procedure is also done by combining a rule-based method with applying the GPT-3.5-turbo and the details can be found in our code repository. We only generate false statements with replacement for samples that originally come from MCQA datasets; we generate the false statements by replacing the correct answers in the generated true statements with randomly selected wrong answers from the corresponding MC options. Our prompts utilized for generating negated statements can be found in Appendix B.7.

Question generationWe generate questions based on the true statements. We rely on LLMs as their instruction-following ability allows us to generate questions efficiently instead of using resource-consuming fine-tuning methods. To select the best LLM for question generation, we experimentally

Figure 1: Statistics of the questions in CARE-MI. _Left_: average question lengths for each source (average over all questions is shown in gray). _Right_: question length distribution.

compare three LLMs available in Chinese: GPT-3.5-turbo, ChatGLM-6B  and ChatYuan . For this experiment, we evaluate the model performance on the BIOS dataset (See Appendix B.4 for more details) and select ChatYuan as the final choice.

We generate two types of questions: True/False (TF) questions and open-ended (OE) questions. TF questions only admit binary answers, either agreeing or disagreeing with the statement in the question, while OE questions allow a variety of response styles. More specifically, we use ChatYuan along with a rule-based method to generate both TF and OE questions for MCQA samples while we only generate TF questions for KG samples (See Appendix B.4 for details). Information about the generated questions is shown in Table 3. In total, we generate 2,240 and 2,963 questions for MLEC-QA and MEDQA datasets, and 79 and 497 questions for BIOS and CPubMed datasets, respectively. Figure 3 shows an example of generated questions in the MLEC-QA dataset.

Knowledge retrievalMaternity and infant care is a critical subject matter wherein the dissemination of misinformation could endanger lives. In our benchmark, we include external knowledge in each sample to provide auxiliary information, not only for models but also for humans, allowing suitable

Figure 3: An example of generated questions in MLEC-QA datasets. _ZH_ and _EN_ stands for Chinese and English. English sentences are translated for reference.

  
**Source** & **TF** & **OE** \\  BIOS & 79 & - \\ CPubMed & 497 & - \\ MLEC-QA & 1,333 & 907 \\ MEDQA & 1,617 & 1,346 \\  Total & 3,526 & 2,253 \\   

Table 3: Number of generated questions for CARE-MI.

Figure 2: CARE-MI construction process. Data generation components are shown on green background, whereas the final benchmark samples are shown on orange background.

inferences on the veracity of the statements to be made. To achieve this, we apply BM25 (Robertson and Zaragoza, 2009) to obtain relevant knowledge from given knowledge sources based on the queries. We use two sources for knowledge bases: the Chinese Wikipedia7 and Medical books collected by Jin et al. (2020). We conduct topic filtering for Chinese Wikipedia as it originally contains a huge amount of pages that might not be relevant to the topic of maternity and infant care. During the knowledge retrieval, queries are the concatenation of the questions and the corresponding true statements. We conduct the retrieval on paragraph level; to account for differences between the two knowledge sources, we retrieve the top three most relevant paragraphs from both sources, respectively.

Expert annotationWe hire two medical-domain experts as annotators to review the generated samples and a third meta-annotator to arbitrate their disagreement. The annotators are instructed to answer a series of guiding questions. The guideline including rules and procedures that annotators should follow as well as the inter-annotator agreements are detailed in Appendix A.1. We ask them to evaluate 5,779 synthetic samples, and we discard those that receive two or more negative evaluations. After the expert annotation, the benchmark is downsized to 1,612 samples.

## 4 Experiments

We evaluate Chinese LLMs with respect to misinformation in LF generation using our proposed benchmark. Experiments are done in the form of single-round conversation where we feed models with questions and collect the direct output from the corresponding model. We perform the evaluation under the _true zero-shot_(Lin et al., 2022) setting: for each model, the input is a question from our benchmark, without any additional instruction and examples; prompt and hyperparameters are not tuned in any way. All questions in CARE-MI are used in the evaluation.

### Experimental details

ModelsAs our goal is to evaluate the LLM misinformation in LF generation, we focus on the evaluation of autoregressive language models that are trained for generation tasks, as opposed to masked language models like BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). We evaluate Chinese LLMs that have been specifically tuned for conversation scenarios, either by supervised fine-tuning (SFT) or reinforcement learning with human feedback (RLHF) as we aim at measuring the misinformation occurring when the models directly interact with humans. We include ChatGLM-6B (Zeng et al., 2022; Du et al., 2022), MOSS-16B-SFT (Sun and Qiu, 2023), two variants of the BELLE (Ji et al., 2023, 2023) series, a.k.a BELLE-7B-0.2M and BELLE-7B-2M, and two models from the GPT family, GPT-3.5-turbo (OpenAI, 2023) and GPT-4 (OpenAI, 2023). We also include a LLAMA (Touvron et al., 2023) model which we perform further pretraining on Chinese corpus including CLUECorpus2020 (Xu et al., 2020), Chinese Scientific Literature Dataset (Li et al., 2022), Chinese Wikipedia7, RedPajama-Data (Computer, 2023), etc., and fine-tuning on translated ShareGPT (RyokoAI, 2023), Aplaca-GPT4 (Peng et al., 2023) and WizardLM (Xu et al., 2023). We denote this version as LLaMA-13B-T. More details regarding the parameter settings can be found in Appendix B.5. Finally, we recruit a domain expert to act as a human baseline; we randomly select 200 questions from the benchmark and let the expert answer them correspondingly; the selected samples are collected strictly following their original distribution in the benchmark regarding their sources. The expert is allowed to check any resource that is necessary and is suggested to finish each question within 2 minutes.

Evaluation metricsUnlike Lin et al. (2022) who consider an answer to be truthful if and only if it avoids asserting a false statement and thus allows non-committal answers such as _No comments_ and _I don't know_ as legal truthful answers, we follow the human evaluation framework similar to what has been explored in (Lu et al., 2022). For each model-generated answer, we recruit three medical-domain expert-level annotators to evaluate the following two aspects:

1. **Correctness**: given a question, whether the answer is factually correct and relevant.
2. **Interpretability**: given a question, whether the answer contains a detailed and concise explanation that demonstrates how the conclusion was reached.

We require the annotators to make their judgments independently during the evaluation; for each aspect, they are asked to give a scalar score between 0 and 1 to reflect their decisions for each sample (the higher the better). The final evaluation results are averaged over the three annotators. We refer the readers to Appendix A.2 for more details about this human evaluation.

### Results

OverviewEvaluation results on correctness and interpretability are shown in Table 4 and Table 5, respectively. Among all evaluated Chinese models, models from the GPT family perform the best by a large margin in both correctness (\(\) 0.158) and interpretability (\(\) 0.112). In general, all models exhibit better performance on samples generated from KG datasets than from MCQA datasets, as they generally have longer context and necessitate more reasoning ability from the models to produce the correct answer. Overall, LLMs with smaller sizes tend to perform worse; however, even the best model is not comparable with human expert, indicating room for improvement. Figure 4 further presents the correctness and interpretability evaluation for TF and OE questions separately. We observe that all models perform better on TF questions than OE questions: on average, most models are able to achieve 0.8 of correctness as well as interpretability for TF questions while only GPT models can achieve a correctness of 0.6 for OE questions. This highlights the weakness of current models, where LLMs still struggle with complex reasoning.

Correctness is linear-correlated with interpretabilityFigure 5 shows the correlation between correctness and interpretability across all evaluated LLMs. In general, all models exhibit a similar pattern, where the interpretability linear correlates with the corresponding correctness. Exceptions include BELLE-6B-0.2M, which shows abnormally low interpretability. On the contrary, ChatGLM-6B and LLaMA-13B-T present above-average interpretability; however, this is not always good. Better interpretability scores only indicate better generated descriptions when explaining how the conclusions are drawn (See Section 4.1); yet if the conclusions themselves are factually incorrect, more detailed explanations will only lead to misleading consequences.

More data is not always betterBELLE-7B-0.2M present slightly better performance (\(\)0.023) but lower interpretability (\(\)0.083) in comparison with its twin model BELLE-7B-2M. Both BELLE-7B-0.2M and BELLE-7B-2M use BLOOM (Scao et al., 2022) as base model with the only difference being the size of the instruction set used during fine-tuning. As mentioned, better interpretability is not always a desired property, especially when it is paired with much lower correctness; more instruction

  
**Model** & **All** & **BIOS** & **CPubMed** & **MLEC-QA** & **MEDQA** \\  MOSS-16B-SFT (2023) & 0.671\(\)0.321 & 0.930\(\)0.121 & 0.925\(\)0.166 & 0.644\(\)0.332 & 0.639\(\)0.316 \\ ChatGLM-6B (2022b) & 0.610\(\)0.333 & 0.928\(\)0.116 & 0.748\(\)0.264 & 0.579\(\)0.346 & 0.599\(\)0.328 \\ BELLE-7B-2M (2023b) & 0.647\(\)0.315 & 0.843\(\)0.268 & 0.928\(\)0.175 & 0.631\(\)0.314 & 0.605\(\)0.311 \\ BELLE-7B-0.2M (2023b) & 0.670\(\)0.316 & 0.947\(\)0.095 & 0.942\(\)0.141 & 0.624\(\)0.335 & 0.646\(\)0.302 \\ GPT-4 (2023b) & 0.867\(\)0.215 & 0.958\(\)0.125 & 0.967\(\)0.124 & 0.851\(\)0.233 & 0.858\(\)0.211 \\ GPT-3.5-turbo (2023a) & 0.824\(\)0.263 & 0.973\(\)0.108 & 0.948\(\)0.160 & 0.799\(\)0.279 & 0.815\(\)0.263 \\ LLaMA-13B-T (2023) & 0.709\(\)0.301 & 0.871\(\)0.235 & 0.922\(\)0.178 & 0.678\(\)0.311 & 0.689\(\)0.297 \\  Human Baseline\({}^{}\) & 0.938\(\)0.213 & 1.000\(\)0.000 & 1.000\(\)0.000 & 0.945\(\)0.196 & 0.908\(\)0.262 \\   

Table 4: Average correctness scores for all models and human baseline. Deeper \(@color@color@color@color@color@color@color@color@color@color@color@color@color@color@@color@@color@@color@@@@@color@@@@@@color@tuning yields better language ability, yet it has no effect on improving knowledge correctness. This indicates that increasing the size of instructions used for fine-tuning LLMs is not always helpful in improving its knowledge capability. The conclusion is intuitive: the objective for fine-tuning does not align with improving truthfulness. Consequently, increasing the size of instruction-following samples will not assist the model in providing more truthful answers.

### Automated metrics

Human evaluation, especially in knowledge-intensive domains, is costly and difficult to reproduce. On the other hand, traditional automated evaluation metrics, such as perplexity (Dian et al., 2019), BLEU score (Khashabi et al., 2021) and ROUGE score (Lin, 2004), suffer from a misalignment problem in which the metrics fail to capture the actual performance of the models. Lin et al. (2022) propose to train a judgment model using human labels to serve as a proxy of human annotators. However, the trained models cannot transfer across different tasks and languages. To enable efficient and accurate automated evaluation for our proposed benchmark, we explore using different architectures as backbones of the judgment models and fine-tune them to mimic human judgment.

We use 1) both generated true and false statements as synthetic positive and negative answers, and 2) the human evaluation results, to train all judgment models. We also feed models with all available retrieved knowledge. To evaluate the performance of each judgment model, we select the answer outputs from MOSS-16B-SFT and ChatGLM-6B as the validation set. This is because the scores these two models received are closer to 0.5 on average, indicating a more balanced distribution of good and bad answers. We perform the training using the answers from the rest of the evaluated models. In order to compare different architectures, we fine-tune BERT-Large (Devlin et al., 2019), two GPT-3 variants (GPT-3-350M and GPT-3-6.7B) and LLaMA-13B-T (See Section 4.1 for details). For training BERT-Large, we use the [SEP] token as the separator between fields and directly

    &  &  &  &  &  \\   & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. & Pear. & Acc. \\  Correctness & - & 0.560 & 0.606 & 0.560 & 0.783 & 0.835 & 0.803 & 0.858 & **0.868** & 0.898 \\ Interpretability & - & 0.800 & 0.013 & 0.794 & 0.565 & 0.822 & 0.634 & 0.828 & **0.683** & 0.835 \\   

Table 6: Pearson correlation scores (Pear.) and accuracy (Acc.) are reported for each trained judgment model. We highlight the best Pearson correlation scores in bold.

Figure 4: Evaluation results for TF and OE questions.

Figure 5: Correctness and interpretability metrics show a linear relationship with the \(R^{2}\) being 0.834.

concatenate all fields together (e.g., question, answer and retrieved knowledge) as the input. Prompt used for fine-tuning LLMs can be found in Appendix B.9. For BERT-Large and the two GPT variants evaluated, as they suffer from input length limitation, we truncate the retrieved knowledge to make sure the input meets the model requirements. For comparison, we also include a majority baseline, which always predicts the most frequent score in the training set. More details regarding the experiment settings can be found in Appendix B.5. We evaluate the performance of the judgment model using Pearson correlation and accuracy. To obtain accuracy, we cast the scalar output of the judgment models into binary labels by empirically setting the threshold to 0.5. Judgment models' results are shown in Table 6.

We observe that BERT-Large does not learn anything useful. This might be due to the stringent constraints on the input size as well as the model's inherent inability in complicated reasoning tasks. Larger models tend to perform better; however, it is harder for models to understand the correlation between input text and the labels in the aspect of interpretability. For the final judgment model, we select LLaMA-13B-T as the backbone, as it obtains the best results. We train the model again but with all available data, including both training and validation samples.

We make the judgment models for both aspects publicly available.

We further conduct an ablation study to assess whether incorporating retrieved knowledge enhances the performance of judgment models. Specifically, we perform a comparison by training the LLaMA-13B-T both with and without the retrieved knowledge during the fine-tuning while maintaining the rest of the settings identical. The experiment results are shown in Table 7, which demonstrate that adding knowledge improves the performance of the judgment models.

## 5 Conclusion and Limitation

In this paper, we proposed CARE-MI, a Chinese benchmark for LLM misinformation evaluation in LF generation in the topic of maternity and infant care. This is the first Chinese benchmark that aims to quantify the misinformation in LF generation cases, the only dataset with clear expert-annotations on the maternity and infant care domain, and with a construction pipeline that can be easily transferred to other domains and languages. We conducted comprehensive assessments on Chinese LLMs, and showed that current models still have room for improvement. Furthermore, we investigated different model backbones for training judgment models and provided an efficient judgment model that can perform correctness evaluation of LF generation accurately. We believe that our proposed benchmark not only paves the way for easier benchmark construction for the whole research community, but also contributes to promoting better Chinese LLM applications in the maternity and infant care domain.

LimitationCARE-MI aims solely at evaluating the misinformation in long-form generation tasks for Chinese LLMs on the topic of maternity and infant care. It is not designed for any other scenarios, e.g., evaluating other target groups such as medical professionals or students. The misuse of CARE-MI might lead to unpredictable consequences. Additionally, the benchmark contains content that can only be considered as correct for now. With the development of modern clinical techniques, we expect the accuracy of the information provided in the benchmark to decrease over time, and thus our proposed benchmark might not be suitable for misinformation evaluation at a later stage, e.g., 10 years from now. Furthermore, our benchmark might not align with the actual interests of the maternity and infant care community (e.g., pregnant women) in real-life situations, as the benchmark is not constructed by collecting the most frequently asked questions in the community. Last but not least, even though we have tried our best to reduce the subjective bias during the human annotations, we cannot completely avoid it.