# Policy Gradient with Tree Expansion

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax--a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance decays exponentially with the planning horizon as a function of the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax, we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.

## 1 Introduction

Policy Gradient (PG) methods (Sutton et al., 1999) for Reinforcement Learning (RL) are often the first choice for environments that allow numerous interactions at a fast pace (Schulman et al., 2017). Their success is attributed to several factors: they are easy to distribute to multiple workers, require no assumptions on the underlying value function, and have both on-policy and off-policy variants.

Despite these positive features, PG algorithms are also notoriously unstable due to the high variance of the gradients computed over entire trajectories (Liu et al., 2020; Xu et al., 2020). As a result, PG algorithms tend to be highly inefficient in terms of sample complexity. Several solutions have been proposed to mitigate the high variance issue, including baseline subtraction (Greensmith et al., 2004; Thomas and Brunskill, 2017; Wu et al., 2018), anchor-point averaging (Papini et al., 2018), and other variance reduction techniques (Zhang et al., 2021; Shen et al., 2019; Pham et al., 2020).

A second family of algorithms that achieved state-of-the-art results in several domains is based on planning. Planning is exercised primarily in the context of value-based RL and is usually implemented using a Tree Search (TS) (Silver et al., 2016; Schrittwieser et al., 2020). In this work, we combine PG with TS by introducing a parameterized differentiable policy that incorporates tree expansion. Namely, our SoftTreeMax policy replaces the standard policy logits of a state and action, with the expected value of trajectories that originate from these state and action. We consider two variants of SoftTreeMax, one for cumulative reward and one for exponentiated reward.

Combining TS and PG should be done with care given the biggest downside of PG--its high gradient variance. This raises questions that were ignored until this work: (i) How to design a PG method based on tree-expansion that is stable and performs well in practice? and (ii) How does the tree-expansionpolicy affect the PG variance? Here, we analyze SoftTreeMax, and provide a practical methodology to choose the expansion policy to minimize the resulting variance. Our main result shows that a desirable expansion policy is one, under which the induced transition probabilities are similar for each starting state. More generally, we show that the gradient variance of SoftTreeMax decays at a rate of \(|_{2}|^{d}\), where \(d\) is the depth of the tree and \(_{2}\) is the second eigenvalue of the transition matrix induced by the tree expansion policy. This work is the first to prove such a relation between PG variance and tree expansion policy. In addition, we prove that the with an approximate forward model, the bias of the gradient is bounded proportionally to the approximation error of the model.

To verify our results, we implemented a practical version of SoftTreeMax that exhaustively searches the entire tree and applies a neural network on its leaves. We test our algorithm on a parallelized Atari GPU simulator (Dalton et al., 2020). To enable a tractable deep search, up to depth eight, we also introduce a pruning technique that limits the width of the tree. We do so by sampling only the most promising nodes at each level. We integrate our SoftTreeMax GPU implementation into the popular PPO (Schulman et al., 2017) and compare it to the flat distributed variant of PPO. This allows us to demonstrate the potential benefit of utilizing learned models while isolating the fundamental properties of TS without added noise. In all tested Atari games, our results outperform the baseline and obtain up to 5x more reward. We further show in Section 6 that the associated gradient variance is smaller by three orders of magnitude in all games, demonstrating the relation between low gradient variance and high reward.

We summarize our key contributions. (i) We show how to combine two families of SoTA approaches: PG and TS by **introducing SoftTreeMax:** a novel parametric policy that generalizes softmax to planning. Specifically, we propose two variants based on cumulative and exponentiated rewards. (ii) We **prove that the gradient variance of SoftTreeMax in its two variants decays exponentially** with its tree depth. Our analysis sheds new light on the choice of tree expansion policy. It raises the question of optimality in terms of variance versus the traditional regret; e.g., in UCT (Kocsis and Szepesvari, 2006). (iii) We prove that with an approximate forward model, the **gradient bias is proportional to the approximation error**, while retaining the variance decay. This quantifies the accuracy required from a learned forward model. (iv) We **implement a differentiable deep version of SoftTreeMax** that employs a parallelized GPU tree expansion. We demonstrate how its gradient variance is reduced by three orders of magnitude over PPO while obtaining up to 5x reward.

## 2 Preliminaries

Let \(_{U}\) denote simplex over the set \(U.\) Throughout, we consider a discounted Markov Decision Process (MDP) \(=(,,P,r,,)\), where \(\) is a finite state space of size \(S\), \(\) is a finite action space of size \(A\), \(r:\) is the reward function, \(P:_{}\) is the transition function, \((0,1)\) is the discount factor, and \(^{S}\) is the initial state distribution. We denote the transition matrix starting from state \(s\) by \(P_{s}^{A S}\), i.e., \([P_{s}]_{a,s^{}}=P(s^{}|a,s).\) Similarly, let \(R_{s}=r(s,)^{A}\) denote the corresponding reward vector. Separately, let \(:_{}\) be a stationary policy. Let \(P^{}\) and \(R_{}\) be the induced transition matrix and reward function, respectively, i.e., \(P^{}(s^{}|s)=_{a}(a|s)(s^{}|s,a)\) and \(R_{}(s)=_{a}(a|s)r(s,a)\). Denote the stationary distribution of \(P^{}\) by \(_{}^{S}\) s.t. \(_{}^{}P^{}=P^{}\), and the discounted state visitation frequency by \(d_{}\) so that \(d_{}^{}=(1-)_{a=0}^{}^{t}^{}(P^{})^{t}.\) Also, let \(V^{}^{S}\) be the value function of \(\) defined by \(V^{}(s)=^{}[_{t=0}^{}^{t}r(s_{t}, (s_{t})) s_{0}=s]\), and let \(Q^{}^{S A}\) be the Q-function such that \(Q^{}(s,a)=^{}[r(s,a)+ V^{}(s^{})]\). Our goal is to find an optimal policy \(^{}\) such that \(V^{}(s) V^{^{}}(s)=_{}V^{}(s),\  s \).

For the analysis in Section 4, we introduce the following notation. Denote by \(^{S}\) the vector representation of \((s)\)\( s.\) For a vector \(u\), denote by \((u)\) the coordinate-wise exponent of \(u\) and by \(D(u)\) the diagonal square matrix with \(u\) in its diagonal. For a matrix \(A\), denote its \(i\)-th eigenvalue by \(_{i}(A).\) Denote the \(k\)-dimensional identity matrix and all-ones vector by \(I_{k}\) and \(_{k},\) respectively. Also, denote the trace operator by \(\). Finally, we treat all vectors as column vectors.

### Policy Gradient

PG schemes seek to maximize the cumulative reward as a function of the policy \(_{}(a|s)\) by performing gradient steps on \(\). The celebrated Policy Gradient Theorem (Sutton et al., 1999) states that

\[^{}V^{_{}}=_{s d_{ _{}},a_{}(|s)}[_{}_{} (a|s)Q^{_{}}(s,a)].\]

The variance of the gradient is thus

\[_{s d_{_{}},a_{}(|s)}(_{ }_{}(a|s)Q^{_{}}(s,a)).\] (1)

In the notation above, we denote the variance of a vector random variable \(X\) by

\[_{x}(X)=[_{x}[(X- _{x}X)^{}(X-_{x}X)]],\]

similarly as in (Greensmith et al., 2004). From now on, we drop the subscript from \(\) in (1) for brevity. When the action space is discrete, a commonly used parameterized policy is softmax: \(_{}(a|s)((s,a)),\) where \(:\) is a state-action parameterization.

## 3 SoftTreeMax: Exponent of trajectories

We introduce a new family of policies called SoftTreeMax, which are a model-based generalization of the popular softmax. We propose two variants: Cumulative (C-SoftTreeMax) and Exponentiated (E-SoftTreeMax). In both variants, we replace the generic softmax logits \((s,a)\) with the score of a trajectory of horizon \(d\) starting from \((s,a)\), generated by applying a behavior policy \(_{b}\). In C-SoftTreeMax, we exponentiate the expectation of the logits. In E-SoftTreeMax, we first exponentiate the logits and then only compute their expectation.

**Logits**. We define the SoftTreeMax logit \(_{s,a}(d;)\) to be the random variable depicting the score of a trajectory of horizon \(d\) starting from \((s,a)\) and following the policy \(_{b}\):

\[_{s,a}(d;)=^{-d}[_{t=0}^{d-1}^{t}r_{t}+^{ d}(s_{d})].\] (2)

In the above expression, note that \(s_{0}=s,\ a_{0}=a,\ a_{t}_{b}(|s_{t})\  t 1,\) and \(r_{t} r(s_{t},a_{t}).\) For brevity of the analysis, we let the parametric score \(\) in (2) be state-based, similarly to a value function. Instead, one could use a state-action input analogous to a Q-function. Thus, SoftTreeMax can be integrated into the two types of implementation of RL algorithms in standard packages. Lastly, the preceding \(^{-d}\) scales the \(\) parametrization to correspond to its softmax counterpart.

**C-SoftTreeMax**. Given an inverse temperature parameter \(\), we let C-SoftTreeMax be

\[_{d,}^{}(a|s)[^{_{b}} _{s,a}(d;)].\] (3)

C-SoftTreeMax gives higher weight to actions that result in higher expected returns. While standard softmax relies entirely on parametrization \(\), C-SoftTreeMax also interpolates a Monte-Carlo portion of the reward.

**E-SoftTreeMax**. The second operator we propose is E-SoftTreeMax:

\[_{d,}^{}(a|s)^{_{b}}[( _{s,a}(d;))];\] (4)

here, the expectation is taken outside the exponent. This objective corresponds to the exponentiated reward objective which is often used for risk-sensitive RL (Howard and Matheson, 1972; Fei et al., 2021; Noorani and Baras, 2021). The common risk-sensitive objective is of the form \([( R)],\) where \(\) is the risk parameter and \(R\) is the cumulative reward. Similarly to that literature, the exponent in (4) emphasizes the most promising trajectories.

**SoftTreeMax properties**. SoftTreeMax is a natural model-based generalization of softmax. For \(d=0\), both variants above coincide since (2) becomes deterministic. In that case, for a state-action parametrization, they reduce to standard softmax. When \( 0\), both variants again coincide and sample actions uniformly (exploration). When \(,\) the policies become deterministic andgreedily optimize for the best trajectory (exploitation). For C-SoftTreeMax, the best trajectory is defined in expectation, while for E-SoftTreeMax it is defined in terms of the best sample path.

**SoftTreeMax convergence.** Under regularity conditions, for any parametric policy, PG converges to local optima (Bhatnagar et al., 2009), and thus also SoftTreeMax. For softmax PG, asymptotic (Agarwal et al., 2021) and rate results (Mei et al., 2020b) were recently obtained, by showing that the gradient is strictly positive everywhere (Mei et al., 2020b, Lemmas 8-9). We conjecture that SoftTreeMax satisfies the same property, being a generalization of softmax, but formally proving it is subject to future work.

**SoftTreeMax gradient.** The two variants of SoftTreeMax involve an expectation taken over \(S^{d}\) many trajectories from the root state \(s\) and weighted according to their probability. Thus, during the PG training process, the gradient \(_{}_{}\) is calculated using a weighted sum of gradients over all reachable states starting from \(s\). Our method exploits the exponential number of trajectories to reduce the variance while improving performance. Indeed, in the next section we prove that the gradient variance of SoftTreeMax decays exponentially fast as a function of the behavior policy \(_{b}\) and trajectory length \(d\). In the experiments in Section 6, we also show how the practical version of SoftTreeMax achieves a significant reduction in the noise of the PG process and leads to faster convergence and higher reward.

## 4 Theoretical Analysis

In this section, we first bound the variance of PG when using the SoftTreeMax policy. Later, we discuss how the gradient bias resulting due to approximate forward models diminishes as a function of the approximation error, while retaining the same variance decay.

We show that the variance decreases exponentially with the tree depth, and the rate is determined by the second eigenvalue of the transition kernel induced by \(_{b}\). Specifically, we bound the same expression for variance as appears in (Greensmith et al., 2004, Sec. 3.5) and (Wu et al., 2018, Sec. A, Eq. (21)). Other types of analysis could instead have focused on the estimation aspect in the context of sampling (Zhang et al., 2021; Shen et al., 2019; Pham et al., 2020). Indeed, in our implementation in Section 5, we manage to avoid sampling and directly compute the expectations in Eqs. (3) and (4). As we show later, we do so by leveraging efficient parallel simulation on the GPU in feasible run-time. In our application, due to the nature of the finite action space and quasi-deterministic Atari dynamics (Bellemare et al., 2013), our expectation estimator is noiseless. We encourage future work to account for the finite-sample variance component. We defer all the proofs to Appendix A.

We begin with a general variance bound that holds for any parametric policy.

**Lemma 4.1** (Bound on the policy gradient variance).: _Let \(_{}_{}(|s)^{A()}\) be a matrix whose \(a\)-th row is \(_{}_{}(a|s)^{}\). For any parametric policy \(_{}\) and function \(Q^{_{}}:,\)_

\[(_{}_{}(a|s)Q^{_{}}( s,a))_{s,a}[Q^{_{}}(s,a)]^{2}_{s} _{}_{}(|s)_{F}^{2}.\]

Hence, to bound (1), it is sufficient to bound the Frobenius norm \(_{}_{}(|s)_{F}\) for any \(s\).

Note that SoftTreeMax does not reduce the gradient uniformly, which would have been equivalent to a trivial change in the learning rate. While the gradient norm shrinks, the gradient itself scales differently along the different coordinates. This scaling occurs along different eigenvectors, as a function of problem parameters (\(P\), \(\)) and our choice of behavior policy (\(_{b}\)), as can be seen in the proof of the upcoming Theorem 4.4. This allows SoftTreeMax to learn a good "shrinkage" that, while reducing the overall gradient, still updates the policy quickly enough. This reduction in norm and variance resembles the idea of gradient clipping Zhang et al. (2019), where the gradient is scaled to reduce its variance, thus increasing stability and improving overall performance.

A common assumption in the RL literature (Szepesvari, 2010) that we adopt for the remainder of the section is that the transition matrix \(P^{_{b}}\), induced by the behavior policy \(_{b}\), is irreducible and aperiodic. Consequently, its second highest eigenvalue satisfies \(|_{2}(P^{_{b}})|<1\).

From now on, we divide the variance results for the two variants of SoftTreeMax into two subsections. For C-SoftTreeMax, the analysis is simpler and we provide an exact bound. The case of E-SoftTreeMax is more involved and we provide for it a more general result. In both cases, we show that the variance decays exponentially with the planning horizon.

### Variance of C-SoftTreeMax

We express C-SoftTreeMax in vector form as follows.

**Lemma 4.2** (Vector form of C-SoftTreeMax).: _For \(d 1,\) (3) is given by_

\[_{d,}^{C}(|s)=+P_{s}(P^{ _{b}})^{d-1})]}{_{A}^{}[ (C_{s,d}+P_{s}(P^{_{b}})^{d-1})]},\] (5)

_where_

\[C_{s,d}=^{-d}R_{s}+P_{s}[_{h=1}^{d-1}^{h-d}(P^{_{b }})^{h-1}]R_{_{b}}.\]

The vector \(C_{s,d}^{A}\) represents the cumulative discounted reward in expectation along the trajectory of horizon \(d.\) This trajectory starts at state \(s,\) involves an initial reward dictated by \(R_{s}\) and an initial transition as per \(P_{s}.\) Thereafter, it involves rewards and transitions specified by \(R_{_{b}}\) and \(P^{_{b}},\) respectively. Once the trajectory reaches depth \(d,\) the score function \((s_{d})\) is applied,.

**Lemma 4.3** (Gradient of C-SoftTreeMax).: _The C-SoftTreeMax gradient is given by_

\[_{}_{d,}^{C}=[I_{A}-_{A}(_{d, }^{C})^{}]P_{s}(P^{_{b}})^{d-1},\]

_in \(^{A S},\) where for brevity, we drop the \(s\) index in the policy above, i.e., \(_{d,}^{C}_{d,}^{C}(|s).\)_

We are now ready to present our first main result:

**Theorem 4.4** (Variance decay of C-SoftTreeMax).: _For every \(Q:,\) the C-SoftTreeMax policy gradient variance is bounded by_

\[(_{}_{d,}^{C}(a|s)Q(s,a)) 2 S^{2}^{2}}{(1-)^{2}}|_{2}(P^{_{b}})|^{2(d-1)}.\]

We provide the full proof in Appendix A.4, and briefly outline its essence here.

Proof outline.: Lemma 4.1 allows us to bound the variance using a direct bound on the gradient norm. The gradient is given in Lemma 4.3 as a product of three matrices, which we now study from right to left. The matrix \(P^{_{b}}\) is a row-stochastic matrix. Because the associated Markov chain is irreducible and aperiodic, it has a unique stationary distribution. This implies that \(P^{_{b}}\) has one and only one eigenvalue equal to \(1;\) all others have magnitude strictly less than \(1.\) Let us suppose that all these other eigenvalues have multiplicity \(1\) (the general case with repeated eigenvalues can be handled via Jordan decompositions as in [10, Lemma1]). Then, \(P^{_{b}}\) has the spectral decomposition \(P^{_{b}}=_{S}_{_{b}}^{}+_{i=2}^{S}_{i}v_{i} u_{i}^{},\) where \(_{i}\) is the \(i\)-th eigenvalue of \(P^{_{b}}\) (ordered in descending order according to their magnitude) and \(u_{i}\) and \(v_{i}\) are the corresponding left and right eigenvectors, respectively, and therefore \((P^{_{b}})^{d-1}=_{S}_{_{b}}^{}+_{i=2}^{S}_{ i}^{d-1}v_{i}u_{i}^{}.\)

The second matrix in the gradient relation in Lemma 4.3, \(P_{s},\) is a rectangular transition matrix that translates the vector of all ones from dimension \(S\) to \(A:P_{s}_{S}=_{A}.\) Lastly, the first matrix \([I_{A}-_{A}(_{d,}^{C})^{}]\) is a projection whose null-space includes the vector \(_{A},\) i.e., \([I_{A}-_{A}(_{d,}^{C})^{}]_{A}=0.\) Combining the three properties above when multiplying the three matrices of the gradient, it is easy to see that the first term in the expression for \((P^{_{b}})^{d-1}\) gets canceled, and we are left with bounded summands scaled by \(_{i}(P^{_{b}})^{d-1}.\) Recalling that \(|_{i}(P^{_{b}})|<1\) and that \(|_{2}||_{3}|\) for \(i=2,,S,\) we obtain the desired result. 

Theorem 4.4 guarantees that the variance of the gradient decays exponentially with \(d.\) It also provides a novel insight for choosing the behavior policy \(_{b}\) as the policy that minimizes the absolute second eigenvalue of the \(P^{_{b}}.\) Indeed, the second eigenvalue of a Markov chain relates to its connectivity and its rate of convergence to the stationary distribution .

**Optimal variance decay**. For the strongest reduction in variance, the behavior policy \(_{b}\) should be chosen to achieve an induced Markov chain whose transitions are state-independent. In that case,is a rank one matrix of the form \(_{S}_{s}^{}\), and \(_{2}(P^{_{b}})=0\). Then, \((_{}_{}(a|s)Q(s,a))=0\). Naturally, this can only be done for pathological MDPs; see Appendix C.1 for a more detailed discussion. Nevertheless, as we show in Section 5, we choose our tree expansion policy to reduce the variance as best as possible.

**Worst-case variance decay**. In contrast, and somewhat surprisingly, when \(_{b}\) is chosen so that the dynamics is deterministic, there is no guarantee that it will decay exponentially fast. For example, if \(P^{_{b}}\) is a permutation matrix, then \(_{2}(P^{_{b}})=1,\) and advancing the tree amounts to only updating the gradient of one state for every action, as in the basic softmax.

### Variance of E-SoftTreeMax

The proof of the variance bound for E-SoftTreeMax is similar to that of C-SoftTreeMax, but more involved. It also requires the assumption that the reward depends only on the state, i.e. \(r(s,a) r(s)\). This is indeed the case in most standard RL environments such as Atari and Mujoco.

**Lemma 4.5** (Vector form of E-SoftTreeMax).: _For \(d 1\), (4) is given by_

\[_{d,}^{E}(|s)=()}{1_{A}^{}E_{s, d}()},\] (6)

_where_

\[E_{s,d}=P_{s}_{h=1}^{d-1}(D((^{h-d}R))P^{ _{b}}).\]

_The vector \(R\) above is the \(S\)-dimensional vector whose \(s\)-th coordinate is \(r(s)\)._

The matrix \(E_{s,d}^{A S}\) has a similar role to \(C_{s,d}\) from (5), but it represents the exponentiated cumulative discounted reward. Accordingly, it is a product of \(d\) matrices as opposed to a sum. It captures the expected reward sequence starting from \(s\) and then iteratively following \(P^{_{b}}\). After \(d\) steps, we apply the score function on the last state as in (6).

**Lemma 4.6** (Gradient of E-SoftTreeMax).: _The E-SoftTreeMax gradient is given by_

\[_{}_{d,}^{E}=[I_{A}-_{A}(_{d, }^{E})^{}]^{E})^{-1}E_ {s,d}D(())}{_{A}^{}E_{s,d}()}  ^{A S},\]

_where for brevity, we drop the \(s\) index in the policy above, i.e., \(_{d,}^{E}_{d,}^{E}(|s).\)_

This gradient structure is harder to handle than that of C-SoftTreeMax in Lemma 4.3, but here we also can bound the decay of the variance nonetheless.

**Theorem 4.7** (Variance decay of E-SoftTreeMax).: _There exists \((0,1)\) such that,_

\[(_{}_{d,}^{E}(a|s)Q(s,a)) (^{2}^{2d}),\]

_for every \(Q\). Further, if \(P^{_{b}}\) is reversible or if the reward is constant, then \(=|_{2}(P^{_{b}})|\)._

**Theory versus Practice.** We demonstrate the above result in simulation. We draw a random finite MDP, parameter vector \(_{+}^{S}\), and behavior policy \(_{b}\). We then empirically compute the PG variance of E-SoftTreeMax as given in (1) and compare it to \(|_{2}(P^{_{b}})|^{d}\). We repeat this experiment three times for different \(P^{_{b}}\) : (i) close to uniform, (ii) drawn randomly, and (iii) close to a permutation matrix. As seen in Figure 1, the empirical variance and our bound match almost identically. This also suggests that \(=|_{2}(P^{_{b}})|\) in the general case and not only when \(P^{_{b}}\) is reversible or when the reward is constant.

### Bias with an Approximate Forward Model

The definition of the two SoftTreeMax variants involves the knowledge of the underlying environment, in particular the value of \(P\) and \(r\). However, in practice, we often can only learn approximations of the dynamics from interactions, e.g., using NNs (Ha and Schmidhuber, 2018; Schrittwieser et al., 2020). Let \(\) and \(\) denote the approximate kernel and reward functions, respectively. In this section, we study the consequences of the approximation error on the C-SoftTreeMax gradient.

Let \(^{}_{d,}\) be the C-SoftTreeMax policy defined given the approximate forward model introduced above. That is, let \(^{}_{d,}\) be defined exactly as in (5), but using \(_{s},_{s},_{_{b}}\) and \(^{_{b}},\) instead of their unperturbed counterparts from Section 2. Then, the variance of the corresponding gradient again decays exponentially with a decay rate of \(_{2}(^{_{b}})\). However, a gradient bias is introduced. In the following, we bound this bias in terms of the approximation error and other problem parameters. The proof is provided in Appendix A.9.

**Theorem 4.8**.: _Let \(\) be the maximal model mis-specification, i.e., let \(\{\|P-\|,\|r-\|\}=\). Then the policy gradient bias due to \(^{C}_{d,}\) satisfies_

\[\|(^{}V^{^{C}_{d,}} )-(^{}V^{^{C}_{d,}} )\|=(}S^{2}d ).\] (7)

To the best of our knowledge, Theorem 4.8 is the first result that bounds the bias of the gradient of a parametric policy due to an approximate model. It states that if the learned model is accurate enough, we expect similar convergence properties for C-SoftTreeMax as we would have obtained with the true dynamics. It also suggests that higher temperature (lower \(\)) reduces the bias. In this case, the logits get less weight, with the extreme of \(=0\) corresponding to a uniform policy that has no bias. Lastly, the error scales linearly with \(d:\) the policy suffers from cumulative error as it relies on further-looking states in the approximate model.

## 5 SoftTreeMax: Deep Parallel Implementation

Following impressive successes of deep RL (Mnih et al., 2015; Silver et al., 2016), using deep NNs in RL is standard practice. Depending on the RL algorithm, a loss function is defined and gradients on the network weights can be calculated. In PG methods, the scoring function used in the softmax is commonly replaced by a neural network \(W_{}\): \(_{}(a|s)(W_{}(s,a)).\) Similarly, we implement SoftTreeMax by replacing \((s)\) in (2) with a neural network \(W_{}(s)\). Although both variants of

Figure 1: A comparison of the empirical PG variance and our bound for E-SoftTreeMax on randomly drawn MDPs. We present three cases for \(P^{_{b}}:\) (i) close to uniform, (ii) drawn randomly, and (iii) close to a permutation matrix. This experiment verifies the optimal and worse-case rate decay cases. The variance bounds here are taken from Theorem 4.7 where we substitute \(=|_{2}(P^{_{b}})|.\) To account for the constants, we match the values for the first point in \(d=1\).

Figure 2: **SoftTreeMax policy**. Our exhaustive parallel tree expansion iterates on all actions at each state up to depth \(d\) (\(=2\) here). The leaf state of every trajectory is used as input to the policy network. The output is then added to the trajectory’s cumulative reward as described in (2). I.e., instead of the standard softmax logits, we add the cumulative discounted reward to the policy network output. This policy is differentiable and can be easily integrated into any PG algorithm. In this work, we build on PPO and use its loss function to train the policy network.

SoftTreeMax from Section 3 involve computing an expectation, this can be hard in general. One approach to handle it is with sampling, though these introduce estimation variance into the process. We leave the question of sample-based theory and algorithmic implementations for future work.

Instead, in finite action space environments such as Atari, we compute the exact expectation in SoftTreeMax with an exhaustive TS of depth \(d\). Despite the exponential computational cost of spanning the entire tree, recent advancements in parallel GPU-based simulation allow efficient expansion of all nodes at the same depth simultaneously (Dalal et al., 2021; Rosenberg et al., 2022). This is possible when a simulator is implemented on GPU (Dalton et al., 2020; Makoviychuk et al., 2021; Freeman et al., 2021), or when a forward model is learned (Kim et al., 2020; Ha and Schmidhuber, 2018). To reduce the complexity to be linear in depth, we apply tree pruning to a limited width in all levels. We do so by sub-sampling only the most promising branches at each level. Limiting the width drastically improves runtime, and enables respecting GPU memory limits, with only a small sacrifice in performance.

To summarize, in the practical SoftTreeMax algorithm we perform an exhaustive tree expansion with pruning to obtain trajectories up to depth \(d\). We expand the tree with equal weight to all actions, which corresponds to a uniform tree expansion policy \(_{b}\). We apply a neural network on the leaf states, and accumulate the result with the rewards along each trajectory to obtain the logits in (2). Finally, we aggregate the results using C-SoftTreeMax. We leave experiments E-SoftTreeMax for future work on risk-averse RL. During training, the gradient propagates to the NN weights of \(W_{}\). When the gradient \(_{}_{d,}\) is calculated at each time step, it updates \(W_{}\) for all leaf states, similarly to Siamese networks (Bertinetto et al., 2016). An illustration of the policy is given in Figure 2.

## 6 Experiments

We conduct our experiments on multiple games from the Atari simulation suite (Bellemare et al., 2013). As a baseline, we train a PPO (Schulman et al., 2017) agent with \(256\) GPU workers in parallel (Dalton et al., 2020). For the tree expansion, we employ a GPU breadth-first as in (Dalal et al., 2021). We then train C-SoftTreeMax 1 for depths \(d=1 8,\) with a single worker. For depths \(d 3\), we limited the tree to a maximum width of \(1024\) nodes and pruned trajectories with low estimated weights. Since the distributed PPO baseline advances significantly faster in terms of environment steps, for a fair comparison, we ran all experiments for one week on the same machine. For more details see Appendix B.

In Figure 3, we plot the reward and variance of SoftTreeMax for each game, as a function of depth. The dashed lines are the results for PPO. Each value is taken after convergence, i.e., the average over the last \(20\%\) of the run. The numbers represent the average over five seeds per game. The plot conveys three intriguing conclusions. First, in all games, SoftTreeMax achieves significantly higher reward than PPO. Its gradient variance is also orders of magnitude lower than that of PPO. Second, the reward and variance are negatively correlated and mirror each other in almost all games. This phenomenon demonstrates the necessity of reducing the variance of PG for improving performance. Lastly, each game has a different sweet spot in terms of optimal tree depth. Recall that we limit the run-time in all experiments to one week The deeper the tree, the slower each step and the run consists of less steps. This explains the non-monotone behavior as a function of depth. For a more thorough discussion on the sweet spot of different games, see Appendix B.3.

## 7 Related Work

**Softmax Operator.** The softmax policy became a canonical part of PG to the point where theoretical results of PG focus specifically on it (Zhang et al., 2021; Mei et al., 2020; Li et al., 2021; Ding et al., 2022). Even though we focus on a tree extension to the softmax policy, our methodology is general and can be easily applied to other discrete or continuous parameterized policies as in (Mei et al., 2020; Maiah et al., 2021; Silva et al., 2019). **Tree Search.** One famous TS algorithm is Monte-Carlo TS (MCTS; (Browne et al., 2012)) used in AlphaGo (Silver et al., 2016) and MuZero (Schrittwieser et al., 2020). Other algorithms such as Value Iteration, Policy Iteration and DQN were also shown to give an improved performance with a tree search extensions (Efroni et al., 2019; Dalal et al., 2021).

**Parallel Environments.** In this work we used accurate parallel models that are becoming more common with the increasing popularity of GPU-based simulation (Makoviychuk et al., 2021; Dalton et al., 2020; Freeman et al., 2021). Alternatively, in relation to Theorem 4.8, one can rely on recent works that learn the underlying model (Ha and Schmidhuber, 2018; Schrittwieser et al., 2020) and use an approximation of the true dynamics. **Risk Aversion.** Previous work considered exponential utility functions for risk aversion (Chen et al., 2007; Garcia and Fernandez, 2015; Fei et al., 2021). This utility function is the same as E-SoftTreeMax formulation from (4), but we have it directly in the policy instead of the objective. **Reward-free RL.** We showed that the gradient variance is minimized when the transitions induced by the behavior policy \(_{b}\) are uniform. This is expressed by the second eigenvalue of the transition matrix \(P^{_{b}}\). This notion of uniform exploration is common to the reward-free RL setup (Jin et al., 2020). Several such works also considered the second eigenvalue in their analysis (Liu and Brunskill, 2018; Tarbouriech and Lazaric, 2019).

## 8 Discussion

In this work, we introduced for the first time a differentiable parametric policy that combines TS with PG. We proved that SoftTreeMax is essentially a variance reduction technique and explained how to choose the expansion policy to minimize the gradient variance. It is an open question whether optimal variance reduction corresponds to the appealing regret properties the were put forward by UCT (Kocsis and Szepesvari, 2006). We believe that this can be answered by analyzing the convergence rate of SoftTreeMax, relying on the bias and variance results we obtained here.

As the learning process continues, the norm of the gradient and the variance _both_ become smaller. On the face of it, one can ask if the gradient becomes small as fast as the variance or even faster can there be any meaningful learning? As we showed in the experiments, learning happens because the variance reduces fast enough (a variance of 0 represents deterministic learning, which is fastest).

Finally, our work can be extended to infinite action spaces. The analysis can be extended to infinite-dimension kernels that retain the same key properties used in our proofs. In the implementation, the tree of continuous actions can be expanded by maintaining a parametric distribution over actions that depend on \(\). This approach can be seen as a tree adaptation of MPPI (Williams et al., 2017).

## 9 Reproducibility and Limitations

In this submission, we include the code as part of the supplementary material. We also include a docker file for setting up the environment and a README file with instructions on how to run both training and evaluation. The environment engine is an extension of Atari-CuLE (Dalton et al., 2020), a CUDA-based Atari emulator that runs on GPU. Our usage of a GPU environment is both a novelty and a current limitation of our work.

Figure 3: **Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The blue reward plots show the average of \(50\) evaluation episodes. The red variance plots show the average gradient variance of the corresponding training runs, averaged over five seeds. The dashed lines represent the same for PPO. Note that the variance y-axis is in log-scale.**