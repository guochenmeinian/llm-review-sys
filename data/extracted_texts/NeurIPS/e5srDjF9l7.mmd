# Accessing Higher Dimensions for Unsupervised Word Translation

Sida I. Wang

FAIR, Meta

###### Abstract

The striking ability of unsupervised word translation has been demonstrated recently with the help of low-dimensional word vectors / pretraining, which is used by all successful methods and assumed to be necessary. We test and challenge this assumption by developing a method that can also make use of high dimensional signal. Freed from the limits of low dimensions, we show that relying on low-dimensional vectors and their incidental properties miss out on better denoising methods and signals in high dimensions, thus stunting the potential of the data. Our results show that unsupervised translation can be achieved more easily and robustly than previously thought - less than 80MB and minutes of CPU time is required to achieve over 50% accuracy for English to Finnish, Hungarian, and Chinese translations when trained in the same domain; even under domain mismatch, the method still works fully unsupervised on English NewsCrawl to Chinese Wikipedia and English Europarl to Spanish Wikipedia, among others. These results challenge prevailing assumptions on the necessity and superiority of low-dimensional vectors and show that the higher dimension signal can be used rather than thrown away.

## 1 Introduction

The ability to translate words from one language to another without any parallel data nor supervision has been demonstrated in recent works (Lample et al., 2018; Artetxe et al., 2018,...), has long been attempted (Rapp, 1995; Ravi and Knight, 2011,...), and provides empirical answers to scientific questions about language grounding (Bender and Koller, 2020; Sogaard, 2023). However, this striking phenomenon has only been demonstrated with the help of pretrained word vectors or transformer models recently, lending further support to the necessity and superiority of low-dimensional representations. In this work, we develop and test _coocmap1_ for unsupervised word translation using only simple co-occurrence statistics easily computed from raw data. coocmap is dual method of vecmap (Artetxe et al., 2018), using co-occurrences statistics in place of low-dimensional vectors. The greedy and deterministic coocmap establishes the most direct route from raw data to the striking phenomenon of unsupervised word translation, and shows that pretrained representation is not the key.

More surprisingly, coocmap greatly improves the data efficiency and robustness over the baseline of vecmap-fastext, showing that relying on low-dimensional vectors is not only unnecessary but also inferior. With coocmap, 10-40MB of text data and a few minutes of CPU time is sufficient to achieve unsupervised word translation if the training corpora are in the same domain (e.g. both on Wikipedia, Figure 1). For context, this is less than the data used by Brown et al. (1993) to train IBM word alignment models. On cases that were reported not to work using unsupervised methods by Sogaard et al. (2018), we confirm their findings that using fasttext (Bojanowski et al., 2017) vectors indeed fails, while coocmap solved many of these cases with very little data as well. For our main results, we show that less similar language pairs such as English to Hungarian, Finnish and Chinese posed no difficulty and also start to work with 10MB of data for Hungarian, 30MB of data for Finnishand 20MB of data for Chinese. The hardest case of training with domain mismatch (e.g. Wikipedia vs. NewsCrawl or Europarl) is generally reported not to work (Marchisio et al., 2020) especially for dissimilar languages. After aggressively discarding some information using clip and drop, coocmap works on English to Hungarian and Chinese, significantly outperforming vectors, which do not work.

These results challenge the conventional wisdom ever since word vectors proved their worth both for cross-lingual applications (Ruder et al., 2019) and more generally where a popular textbook claims _dense vectors work better in every NLP task than sparse vectors_(Jurafsky and Martin, 2023, 6.8). We discuss reasons that disadvantage low-dimensional vectors unless compression is the goal, which is far more intuitive than the conventional wisdom that _dense vectors are both smaller and perform better_. For vectors to perform, they must have nice linear-algebraic properties (Mikolov et al., 2013, 2013), successfully denoise the data, while still retaining enough useful information. The linear algebraic and denoising properties are _incidental_, since they are not part of the training objective but are a consequence of being in low dimensions. These incidental properties come in conflict with the actual training objective to retain more information as dimension increases, leaving a small window of opportunity for success. The incidental denoising we get from having low dimensions, while interesting and sufficient in easy cases, is actually suboptimal and worse than the more intentional denoising in high dimensions used by coocmap. Furthermore, without the need to have low dimensions, coocmap can access useful information in higher dimensions. The higher dimensions contain knowledge such as _portland-oregon, cbs-60 minutes, molecular-weight, luisiana-purchase, tokyo-1964_ that tend to be lost in lower dimensions. We speculate that similarly processed co-occurrences would outperform low-dimensional vectors in other tasks too, especially if the natural but incidental robustness of vectors is not enough.

## 2 Problem formulation

This word translation task is also called lexicon or dictionary induction in the literature. The task is to translate words in one language to another (e.g. _hello_ to _bonjour_ in French) and is evaluated for accuracy (precision@1) on translations of specific words in a reference dictionary. We do this fully unsupervised, meaning we do not use any seed dictionary or character information. Let the datasets \(D_{1}\) and \(D_{2}\) be sequences of words from the vocabulary sets \(V_{1},V_{2}\). Since we do not use character information, we may refer to vocabulary by their integer indices \(V=[1,2,]\) for convenience. We would like to find mapping from \(V_{1}\) to \(V_{2}\) based on statistics of the datasets. In particular, we consider the window model where the sequential data is reduced to pairwise co-occurrences over context windows of a fixed size \(m\). The word-context co-occurrence matrix \(^{|V||V|}\) counts the number of times word \(w\) occurs in the context of \(c\), over a window of some size \(m\)

\[(w,c)=_{i=1}^{|D|}_{-m j m,j 0}[w_{i} =w,w_{i+j}=c] \]

where \(w_{i}\) is the \(i\)-th word of the dataset \(D\). The matrix \(\) is the sufficient statistics of the popular word2vec (Mikolov et al., 2013, 2013) and fasttext (Bojanowski et al., 2017), which use the same

Figure 1: Our results focus on the data requirements as we varying the amount and domains of data. The unsupervised accuracy increases quickly with a sharp starting point point and this can happen surprisingly early (\(\)10MB here), reaching >50% by 20MB. As data increases, supervised initialization with ground-truth dictionary (dict-init) does not make much difference as shown by the vanishing initialization gap. vecmap-fasttext needs more data and is less stable.

co-occurrence information, including additional objectives not explicit in the loss function:

\[()=_{i=1}^{|D|}_{-m j m,j 0}_{}p(w_{i+j}|w _{i}). \]

For word translation, we obtain \(_{1}\) and \(_{2}\) from the two languages \(D_{1}\) and \(D_{2}\) separately.

Multilingual distributional hypothesis.If words are characterized by its co-occurrents (Harris, 1954), then translated words will keep their translated co-occurrents. In more precise notation, let \((s_{1},t_{1}),(s_{2},t_{2}),,(s_{n},t_{n})\) be \(n\) pairs of translated words, then for translation \((s,t)\)

\[X[s,s_{1}],X[s,s_{2}],,X[s,s_{n}] Z[t,t_{1}],Z[t,t_{2}],,Z[t,t_ {n}],\]

for association matrices \(X=K(_{1})\), \(Z=K(_{2})\). While intuitive, this is not known to work unsupervised nor with minimal supervision. Rapp (1995) presents evidence and correctly speculates that there may be sufficient signal. Fung (1997); Rapp (1999) used a initial dictionary to successfully expand the vocabulary further using a mutual information based association matrix. Alvarez-Melis and Jaakkola (2018) operates on an association matrix generated from word vectors.

Isomorphism of word vectors.Despite the clear motivation in the association space, unsupervised translation was first shown to work in vector space, where a linear mapping between the vector spaces corresponds to word translation. This is called (approximate) _isomorphism_(Ruder et al., 2019). If \(s\) translate to \(t\), and \(X_{s},Z_{t}\) are their vectors, then \(X_{s}W Z_{t}\) where \(W\) can be a rotation matrix, and the association metric can be cosine-distance, often after applying normalizations to raw word vectors. Successful methods solve this problem using adversarial learning (Lample et al., 2018) or heuristic initialization and iterative refinement (Artetxe et al., 2018; Hoshen and Wolf, 2018), among others (Zhang et al., 2017).

## 3 Method

We aim for simplicity in coocmap, and all steps consists of discrete optimization by arranging columns of \(X\) or measuring distances between rows of \(X\) where \(X\) is the association matrix based on co-occurrences (and \(Z\) for the other language). Almost all operations of coocmap has an analog in vecmap and we will describe both of them for clarity. There are two main steps in both methods, finding best matches and measuring \(\) pairwise distances. Each row of \(X\) corresponds to a word for both vecmap and coocmap and are inputs to \(\). vecmap finds a rotation \(W\) that best match given row vectors \(X[s,:]\) and \(Z[t,:]\) for \(s=[s_{1},s_{2},,s_{n}] V_{1}^{n}\) and \(t=[t_{1},t_{2},,t_{n}] V_{2}^{n}\). For coocmap, instead of solving for \(W\), we just re-arrange the columns of the association matrix with indices \(s,t\) directly to get \(X[:,s],Z[:,t]\) where the columns of \(X\) are words too.2

```
\(X^{|V_{1}||V_{1}|},Z^{|V_{2}| d}\); Input\(s,t\) Output\(s,t\) repeat \(W=(X[s,:],Z[t,:])\) \(D=(XW,Z)\) \(s,t=(D)\) until no more improvement
```

**Algorithm 2** vecmap self-learning

\((X,Z)_{ij}=(X_{i},Z_{j})\) is a function that takes pairwise distances for each row of both inputs and then outputs the results in a matrix. The self-learning loop use the same improvement measurement as in vecmap \(_{i}_{j}((i,j))\), but without stochastic search. We explain how to generate \(X,Z\), \(s,t\), and \(\) next.

Measurement.It is important to normalize before taking measurements, and vecmap normalization consists of normalizing each row to have unit \(_{2}\)-norm (\(\)), center so each column has 0 mean (\(\)), and normalizing rows (\(\)) again. For precision,

\[(Y):=(((Y))), \]

where \((Y):=Y-(Y)/r\), \((Y):=^{T}Y\), and \(Y^{r c}\).

coocmap.The input matrix \(X=(^{})\) is obtained from \(\) by taking elementwise square-root then normalize. \(\) is the cosine-distance.

vecmap.From original word vectors \(X^{}\), we use their normalized versions \(X=(X^{})\). For \((X[s],Z[t])\), we use \(_{W}\|X[s,:]W-Z[t,:]\|_{F},\) for rotation matrices \(\). This is known as the Procrustes problem. \(\) is the cosine-distance.

Initialization.We need the initial input \(s,t\) for Algorithms 1 and 2. For the unsupervised initialization, we follow vecmap's method based on row-wise sorting. Let \((X)\) make each row of \(X\) be sorted independently and \((X)\) is defined in (3), then

 
**Input**\(X,Z\), & **Output**\(D\) \\ \(R(Y):=(K(Y))\) & \\ \(S(Y):=(R(Y))\) & \\ \(D=(S(X),S(Z))\) & \\  

The first step of vecmap \(X=(Y^{}Y^{})^{}\) is actually converting from vector space to the association space. So it is natural to replace the first step by \((_{1}^{ 1/2})\) for coocmap.

Matching.The main problem once we have a distance matrix is to solve a matching problem

\[_{M}_{(i,j) M}(i,j) \]

vecmap proposes a simple matching method, where we take \(j^{*}=_{j}(i,j)\) for each \(i\) in forward matching, and then take \(i^{*}=_{i}(i,j)\) for each \(j\) in backward matching. This always results in \(|V_{1}|+|V_{2}|\) matches where words on each side is guaranteed to appear at least once. For coocmap, there is complete freedom in forming matches \(i,j\) and often many words all match with a single word. As a result, hubness mitigation (Lazaridou et al., 2015) is even more essential compared to vecmap.

While there are many reasonable matching algorithms, we find that Cross-Domain Similarity Local Scaling (CSLS) (Lample et al., 2018) was enough to stabilize coocmap. Note that CSLS acts on the pairwise distances and therefore directly applies to \(\),

\[((i,j))=(i,j)- (_{j^{} N_{i}(k)}(i,j^{ })+_{i^{} N_{j}(k)}(i^{},j))\]

where \(N_{i}(k)\) are the \(k\)-nearest neighbors to \(i\) according to \(\). We use \(()\) as the input to (4) in place of \(\). Instead of always preferring the best absolute match, CSLS prefers matches that stand out relative to the \(k\) best alternatives.

### Clip and drop

This basic coocmap above already works well on the easier cases and these techniques provided small improvements. However, for difficult cases of training on different domains, clip is essential and drop gives a final boost that can be large. These operations are aggressive and throws away potentially useful information just like rank reduction, but we will show that they are far better especially under domain shifts.

clip.For clip, we threshold the top and bottom values by percentile. While there are just 2 numbers for the 2 thresholds, we use two percentiles operations to determine each of them so they are not dominated by any particular row. Let \(r_{i}=Q_{p}(X_{i})\) be the \(p\)th percentile value of the row \(i\) of \(X\). We threshold \(X\) at \(Q_{p}(r_{1},r_{2},,r_{|V|})\), where \(p=1\%\) for lowerbound and \(p=99\%\) for upperbound. This results in two thresholds for the entire matrix that seem to agree well across languages. For results on clip, we run coocmap with \(X=((^{ 1/2}))\). Intuitively, if words are already very strongly associated, obsessing over exactly how strongly is not robust across languages and datasets but can incur large \(_{2}\)s. For lowerbound, extremely negatively associated words is clearly not robust, since the smallest count is 0 and one can always use any two words together. Both bounds seem to improve over baseline, but the upperbound is the more important one.

drop (head).Drop the \(r=20\)_largest_ singular vectors, i.e. \(_{r}(X)=X-X_{r}^{|V||V|}\), where \(X_{r}\) is the rank-\(r\) approximation of \(X\) by SVD. In the experiments, we first get the solution of \(s,t\) from \(((^{ 1/2}))\) then use coocmap on \(X=(_{r}((^{ 1/2})))\) with \(s,t\) as the initial input. Drop was good at getting a final boost from a good solution, but is usually worse than the basic coocmap in obtaining an initial solution. While the dropping top 1 singular vector is discussed by (Mu and Viswanath, 2018; Arora et al., 2017) and popular vectors implicitly drop already (see A), we see more benefits from dropping more and this seems to enable more benefits of the higher dimensions. We show examples in Appendix F in support of this viewpoint.

Truncate (tail).This is the usual rank reduction where \(_{r}(X)=X_{r}\) is the best rank-\(r\) approximation of \(X\) by SVD. We use this for analysis on the effect of rank.

## 4 Experiments

For the main results, we report accuracy as a function of data size and only show results in the fully unsupervised setting. The accuracy is measured by precision at 1 on the full MUSE dictionary for the given language pair on the 5000 most common words in each language (see Section 6 for limitations). Many results will be shown as scatter plots of accuracy vs. data size, each containing thousands of experiments and more informative than tables. They will capture stability and the qualitative behaviors of the transitional region as the amount of data varies. Each point in the scatter plots represents an experiment where a specific amount of data was taken from the head of the file for training co-occurrence matrices and fasttext vectors with default settings (skipgram, 300 dimension, more in B) for fasttext. cocmap use the same window size as fasttext (\(m=5\)), the same CSLS (\(k=10\)) and same optimization parameters as vecmap. coocmap does not require additional hyperparameters until we add clip and drop. The same amount of data is taken from both sides unless one side is exhausted. In our experiments, most cases either achieve > 50% accuracy (i.e. _works_) or near 0 accuracy and has a definite starting point (i.e. _starts to work_). Summary of these results are in Table 1 and we will show details on progressively harder tests.

Methods.we compare these methods for the main results.

* **dict-init**: initialize with the ground truth dictionary then apply coocmap.
* **coocmap**: fully unsupervised coocmap and improvements with **-clip**, **-drop** if needed.
* **vecmap-fasttext**: apply vecmap to 300 dimensional fasttext vectors trained with default settings.
* **vecmap-raw**: apply vecmap to a svd factorization of the co-occurence matrix. If \(^{ 1/2}=USV^{}\), then we use \(US_{r}\) as the word vectors where \(S_{r}\) only keeps top \(r\) singular values. \(r=300\).

For dict-init, we initialized using the ground truth dictionary (i.e. initial input \(s,t\) to Algorithm 1 are the true evaluation dictionary) and then test on the same dictionary after self-learning. This establishes an upper-bound for the amount of gains possible with a better initialization as long as we are using the basic coocmap measurements and self-learning. After coocmap works stably, their performance coincides, showing very few search failures and the limit of gains from initialization.

We use default parameters for fasttext in this section. This may be unfair to fasttext but better match other results reported in the literature where the default hyperparameters are used or pretrained vectors are downloaded. In Section B, we push on the potential of fasttext more and describe hyperparameters.

Data.We test on these languages paired with English (**en**): Spanish (**es**), French (**fr**), German (**de**), Hungarian (**hu**), Finnish (**fi**) and Chinese (**zh**).

For training data we use Wikipedia (**wiki**), Europarl (**parl**), and NewsCrawl (**news**), processed from the source and removing obvious markups so raw text remains. The data is processed using Huggingface WordLevel tokenizer with whitespace pre-tokenizer and lower-cased first.

**wiki** ([https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)): Wikipedia downloaded directly from the official dumps (pages-meta-current), extract text using WikiExtractor (Attardi, 2015) and removed <doc id tags. We start from the first dump until is >1000MB of data for each language and shuffle the combined file. For zh, we also strip away all Latin characters [a-zA-Z] and segment using jieba: [https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba).

**parl** ([https://www.statmt.org/europarl/](https://www.statmt.org/europarl/)): Europarl (Koehn, 2005) was shuffled after downloading since this is parallel data.

news** ([https://data.statmt.org/news-crawl/](https://data.statmt.org/news-crawl/)): NewsCrawl 2019.es was downloaded and used as is. For 2018-2022.hu and 2018.en, we striped meta data by grep -v on http, trackingCode and { after which a small random sample does not obviously contain more meta data. This removed over 35% from hu news and 0.1% from en.

For evaluation data, we use the _full_ MUSE dictionaries for these language pairs. This allows for a more stable evaluation given our focus on qualitative behaviors, but at a cost of being less comparable to previous results.

### Same domain of data

Similar languages.To establish some baselines, we start with the easy settings where everything eventually works. Figure 2 shows that coocmap starts to work at around 10MB of data while vecmap trained on fasttext only starts to work once there are over 100MB of data. The transitional region is fairly sharp, and coocmap works reliably on each language pair with around 100MB of data, and vecmap with fastText also mostly works once there is around 100MB of data. vecmap-fasttext eventually outperforms coocmap after it was trained on >100MB of data.

Less similar languages.For hard cases from (Sogaard et al., 2018), all of which were reported not to work using MUSE (Lample et al., 2018) using fastText vectors. Here, we confirm that vecmap and fastText vectors also do not work on default settings. However, these are not even challenging to the basic coocmap where all cases started to work with less than 30MB of data.

**source** & **target** & **start** & **start’** & **works** \\  enwiki & eswiki & 9 & 70 & 20 \\ enwiki & frwiki & 10 & 80 & 30 \\ enwiki & zhwiki & 14 & 700 & 50 \\  enwiki & dewiki & 20 & 200 & 70 \\ enparl & huparl & 8 & – & 20 \\ enparl & fiparl & 30 & – & 80 \\   \\ 
**source** & **target** & **start** & **start’** & **works** \\  enwiki & esnews & 30 & – & 70 \\ enwiki & hunews & 140 & – & 600 \\ enwiki & esparl & 500 & – & 500 \\ ennews & zhwiki & 800 & – & 800 \\ enwiki & huparl & – & – & – \\ enwiki & fiparl & – & – & – \\ 

Table 1: Summary of data requirements for coocmap in **MB** (1e6 bytes). **start**: when coocmap starts to work, **start’**: when vecmap-fasttext baseline starts to work (check Figure 2 to see that **start** is clear), – denotes failure for the whole range; **works**: when coocmap reaches 50% accuracy. Readings are rounded up to the next tick of the log plot, or 1.4 if less than the middle of 1 and 2. The same amount of data is used on both source and target sides, unless one side is used up (e.g. 100MB of huparl or 300MB of esparl). There are less than 0.2 **million tokens per MB** in all datasets, ranging from 0.13 in fiparl, 0.19 in zhwiki and 0.20 for ennews.

Figure 2: accuracy vs. datasize where the source data and target data are in the same domain, either both Wikipedia, or both Europarl (enparl-huparl, and enparl-fiparl). These cases are easy for coocmap but sometimes failed for vecmap-fasttext or required more data.

For the small but clean Europarl data, we tested on English to Finnish (fi) and Hungarian (hu). As shown in Figure 2, cocomap started to work at 9MB for hu and 30MB for fi. It finished the transition region when we used all 300MB of en-fi and 100MB of en-hu. vecmap-fasttext has yet to work, agreeing with the results of Sogaard et al. (2018). However, since vecmap-raw using simple SVD worked, it was surprising that fasttext did not. Indeed, decreasing the dimension to 100 from 300 enables vecmap-fasttext to start working at 50MB of data for enparl-huparl (vs 8MB for cocomap, Figure 6).

Next result is on English to Chinese (zh), where both are trained on Wikipedia. cocomap had a bit more instability vecmap-fasttext also sometimes works with around 1GB of data. The more robust version of cocomap-clip and drop is completely stable, and begin to work with less than 20MB of data.

### Under domain mismatch.

The most difficult case for unsupervised word translation is when data from different domains are used for each language (Sogaard et al., 2018; Marchisio et al., 2020). The original vecmap of Artetxe et al. (2018b) was tested on different types of crawls (WacKy, NewsCrawl, Common Crawl) which did not work in previous methods. Marchisio et al. (2022b) show that vecmap also failed for NewsCrawl to Common Crawl on harder languages. We test on Wikipedia to NewsCrawl and Europarl and see successes on enwiki-esnews, enwiki-hunews, and ennews-zhwiki, **enparl**-eswiki before finally failing on enwiki-fiparl and enwiki-huparl. See Figure 3.

On **enwiki-esnews**, where \(\)100MB of data was required to reach 50%, though the basic coocmap becomes unstable and all vecmap or fasttext based methods failed to work at all. However, clip and drop not only stablizes this but enabled it start to work at 40MB of data.

On **enwiki-hunews**, cocomap mostly fails but get 5% accuracy with above 100MB of data. Impressively, clip and drop fully solves this problem as well, but even clip has a bit of instability, reaching 50% accuracy at 600MB of data.

On **ennews-zhwiki**, cocomap fails completely without clip and drop, and even then requires more data than before. Finally it works and reaching over 50% accuracy around 800MB. In C, we show the data still has higher potential, where truncating to 2000 dimensions enables ennews-zhwiki to work with 300MB or even 100MB of data, though still not reliably.

For more extreme domain mismatch, we test **enparl-eswiki**. In addition to being small, Europarl contains only parliamentary proceedings which has a distinct style and limited range of topics, to our surprise this also worked with 295MB of enparl, and 500MB from eswiki, reaching a final accuracy of 70% suddenly, although basic coocmap also showed no sign of working. All methods failed for enwiki-fiparl and enwiki-huparl in the range limited by Europarl (300MB for fiparl and 90MB for huparl) with up to 1GB of enwiki, reaching the end of our testing.

## 5 Analysis: why coocmap outperformed dense vectors

These main results show that high-dimensional coocmap is more data efficient and significantly more robust than the popular low-dimensional word vectors fasttext/word2vec, which contract prevailing assumptions that vectors are superior and necessary to enable unsupervised translation among other tasks. Ruder et al. (2019) states "word embeddings enables" various interesting cross-lingual phenomena. For unsupervised dictionary induction, landmark papers (Lample et al., 2018a; Artetxe et al., 2018b) needed vectors and even methods that _must_ use a \(|V||V|\) input constructed these from low dimensional vectors anyway (Alvarez-Melis and Jaakkola, 2018; Marchisio et al., 2022a). More generally, the popular textbook Jurafsky and Martin (2023, 6.8) states "dense vectors work better in every NLP task than sparse vectors". Here, we provide natural reasons that disadvantage vectors if we do not mind having higher dimensions.

The conflicting dimensions of vectors.These properties must hold for vectors to work,

1. _Approximate isomorphism_: be able to translate by rotation/linear map
2. _Denoise_: reduce noise and information not robust for the task
3. _Retention_: keep enough information at a given dimension to achieve good performance

By testing the dimension of word vectors in Figure 4, we can see that isomorphism and denoising only holds in low-dimensions. To see this on enwiki-dewiki where fasttext works well, both vecmapfasttext and coocmap-fasttext are better than SVD truncation at low dimensions. Then accuracy goes to 0 as dimension increases. This is not because vectors fails to retain information, since coocmap-fasttext-clip works if we apply additional clipping. See B for more testing on the effect of dimension and other hyperparameters. Notably, lower dimensions have better isomorphism and denoising, and higher dimensions have better accuracy and retention. This trade off leaves a limited window for fasttext to work well. Still, selecting a good dimension is sufficient for fasttext vectors to match the accuracy and data efficiency of coocmap on the easiest cases (en-es, fr, de) and work well enough on dissimilar languages training on the same domain.

Sogaard et al. (2018) notes similar impact of dimensionality themselves, but without exploring enough range that would have solved their enparl-huparl and enparl-fiparl tests (with vecmap). Yin and Shen (2018) argues that optimal dimension may exist because of bias-variance tradeoff. Our experiments show that coocmap keeps improving with higher dimensions, and they may have been misled by relying on linear algebraic properties. The unreliability of isomorphism is also noted by Patra et al. (2019); Marchisio et al. (2022b).

Better denoising in high dimensions.Domain mismatch is where the vectors struggle the most regardless of dimension. In the easiest domain mismatch case of enwiki-esnews of Figure 4, vecmap-fasttext failed whereas coocmap-fasttext-clip worked (though not stably), showing that **clip** helps even when applied on top of the natural but incidental denoising of vectors.

The association matrix of coocmap \((^{ 1/2})\) is also better overall than other choices such as Mikolov et al. (2013); Levy and Goldberg (2014); Pennington et al. (2014); Rapp (1995). In A, we compared to other full-rank association matrices corresponding to popular vectors and show they also perform worse than coocmap. For instance, the positive pointwise mutual information matrix (PPMI) of Levy and Goldberg (2014) corresponds to word2vec/fasttext. While it can work with

Figure 3: Accuracy vs. data size with clip and drop. Except for enwiki-eswiki and enwiki-zhwiki (top left, top middle), the rest all have domain mismatch where vecmap-fasttext gets \(\)0.

simple \(_{2}\) cdist without normalize and can reach higher accuracy than the basic coocmap if tested on the same domain, it has lower data efficiency and completely fails on the easiest enwiki-esnews with domain mismatch, showing the same behaviors as fasttext. A more aggressive squishing such as \(((1+))\) works robustly too but is less accurate and less data efficient. Impressively, Rapp (1995) also works fully unsupervised with the prescribed \(_{1}\).

Higher dimensions contain useful information.In all cases, coocmap-clip and coocmap-drop both become more accurate as dimension increases. Under domain mismatch, there is considerable gains above even 1000 dimensions, suggesting that we are losing useful information when going to lower dimensions. Analyzing the association matrices based on which values are clipped, we show in F that higher dimensions retain more world knowledge such as _portland-oregon_, _cbs-60 minutes, molecular-weight, basketball-jordan, luisiana-purchase, tokyo-1964_ and using low dimensional vectors misses out.

## 6 Discussions

Better vectors exist.coocmap shows that it is feasible to denoise in high dimensions better than with popular vectors. Thus, applying denoising suitable to task, the size/type of training data first before reducing dimension is a more principled ways to train lower dimensional vectors than the current practice of relying on poorly optimizing a supposed objective. In this view, training vectors can just focus on how to best retain information in low dimensions and vectors should just get monotonously better with increasing dimensions and stop at an acceptable loss of information for the task and data.

There is room for improvements for vectors of all dimensions. In Figure 4, fasttext got higher accuracy at impressively low dimensions in favorable conditions whereas SVD on coocmap-drop becomes more accurate with increasing dimension. Without actually constructing the better vectors, it is clear that meaningful improvements are possible: _for any given dimension d, there exists rank-d matrices that can achieve a higher accuracy than the max of all rank-d methods already tested in Figure 4._

But why vectors?Here is our opinion on why low-dimensional vectors appeared necessary and better beside social reasons. Co-occurrence counts from real data are noisy in very involved ways and violate standard statistical assumptions, however it is obligatory to apply principled statistics such as likelihood based on multinomial or \(^{2}\), whereas vectors take away these bad options and allow less principled but better methods. Statistics says likelihood ratios is the most data efficient: \(D(s,t)=_{i}p(s,i)\), or perhaps the Hellinger distance \(H(s,t)=_{i}||p(s,i)^{}-p(t,i)^{}||_{2}^{2}\) if we want more robustness. Using raw counts or a term like \(p p\) (super-linear in raw counts) always failed when measuring with the \(_{2}\) norm. The likelihood-based method of Ravi and Knight (2011) likely would fail on real data for these reasons. Normalization is a common and crucial element of all working methods. The methods of Rapp (1995); Fung (1997); Rapp (1999) were based on statistical principles but with modifications to make normalized measurements. Levy and Goldberg (2014); Pennington et al. (2014) actually proposed better normalization methods too that would equally apply to co-occurrences (Appendix A).

Giving up on statistics and starting from vecmap lead to big improvements, ironic for our attempt to understand unsupervised translation and how exactly vectors enabled it. To prefer more empirical tests on a wider range of operations instead of believing that we can model natural language data with simple statistics might be another underrated lesson from deep learning. In this work, we give up on statistical modelling without giving up on higher dimensions, which makes available these key tools that were sufficient to outperform low-dimensional vectors: large squeezing operations like \(\), normalizing against a baseline (subtracting means, divide by marginals), contrasting with others during matching (CSLS), clip and drop. With just normalize and CSLS, coocmap would have similar accuracy and higher data efficiency compared to fasttext. Clip made co-occurrences more robust than vectors while also increasing accuracy, showing the possibility of denoising in high-dimensions. Finally, drop enabled some access to the information in higher dimensions and lead to noticeably higher accuracy as dimension increases. This is actually a more intuitive situation than the prevailing understanding that going to very low dimensions is both more compact and more accurate.

IBM models.coocmap shows that unsupervised word translation would have been possible with the amount of compute and data used to train IBM word alignment models (Brown et al., 1993).

While we do not test on the exact en-fr Hansards (LDC), we know it has > 1.3 million sentence pairs from the Parliament of Canada. This can be compared to our Europarl en-hu experiments, which has 0.6 million sentence pairs for a more distant language pair, showing there may not be much additional information from the sentence alignments. cocoamp with drop got around 75% accuracy here. Though the compute requirement of cocoamp may have been difficult in '93 - by our estimate en-hu with \(|V|=5000\) probably requires \(|V|^{3}*100 1\) Tera-FLOs, which should have been easy by the early 2000s on computers having Giga-FLOPS.

Limitations.The vocabulary size is a potentially important parameter unexplored in this paper where we used a low 5000 for cleaner/faster experiments. The most expensive step is factorization operations and measuring distances of high dimensional co-occurrences which is \(O(|V|^{3})\) as opposed to \(O(d|V|^{2})\) for vectors, though approximations are available with dimension reduction (see E). Having a low vocabulary size may explain why we see little benefit (except in our hardest cases such as C) from the potential denoising effects of truncation. Low-dimensional vectors is more easily scalable to larger vocabulary and can more easily used by neural models. However, BPE, indexing, truncating vocabulary, multiple window sizes and other alternatives are not explored to better use higher dimensions.

Speculations.As cross-domain is more challenging than just cross-lingual, it is especially likely that our findings may apply in other cross-domain tasks. So we speculate that if co-occurrences are similarly processed without going to low dimensions, they are likely to perform better on other tasks where more robustness is required than the natural robustness of low-dimensions. Similarly, neural models may also benefit from intentional denoising like clip and drop.

Beyond simple vectors, more recent language models may also suffer from low dimensions, which might be why they benefit from retrieval (Khandelwal et al., 2020), where low dimensional vectors may lose too much information for the task. Recently, contextual transformer representations superseded non-contextual vectors with more data and more dimensions pushing up the performance across tasks. Following Kaplan et al. (2020), there are important trade-off between dimensions, data and compute. If data is the limiting factor, it is plausible that better ways of using higher dimensions is helpful.

Related work.Rapp (1995) already hinted that there might be enough signal for fully unsupervised translation. Fung (1997); Rapp (1999); Haghighi et al. (2008) tested their high dimensional approaches with seed words, but without a full search procedure. Ravi and Knight (2011) succeeded in the case of time expressions and parallel data, but likely used insufficient normalization for harder cases. Mikolov et al. (2013) noticed the relation between low-dimensional rotations and translation. With good enough vectors, Zhang et al. (2017); Lample et al. (2018); Artetxe et al. (2018) achieved fully unsupervised word translation while showing that seed words do not matter too much as long as the unsupervised method works. There was much follow up on these successes, in particular, even methods that must use an association matrix (Alvarez-Melis and Jaakkola, 2018; Marchisio et al., 2022) constructed them from low-dimensional vectors.

### Broader Impact

For dictionary induction, this work shows it can be done with less data and is more robust to domain mismatch than previously thought. We have a working procedure that is more direct, using less compute, making it more accessible for educational purposes. Unsupervised translation means learning from the statistics of data alone and can make bland and silly mistakes where outputs should not be relied on unless they can be verified.