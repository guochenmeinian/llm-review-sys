# Dataflow

## 2 WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games

\({}^{}\) Equal contribution \(\) Corresponding authors.

Figure 1: Overview of our proposed WhodunitBench, a benchmark for large multi-modal agents simulated from _murder mystery games_: (a) the introduction of the game process; (b) the evaluable capabilities and corresponding assessment methods derived from the game; (c) the evaluation results.

demanded by practical scenarios, where they primarily focused on single tasks and static scenarios. To bridge this gap, we introduce WhodunitBench, a benchmark rooted from _murder mystery games_, where players are required to utilize the aforementioned skills to achieve their objective (i.e., identifying the'murderer' or hiding themselves), providing a simulated dynamic environment for evaluating LMAs. Specifically, WhodunitBench includes two evaluation modes. The first mode, the arena-style evaluation, is constructed from 50 meticulously curated scripts featuring clear reasoning clues and distinct murderers; The second mode, the chain of evaluation, consists of over 3000 curated multiple-choice questions and open-ended questions, aiming to assess every facet of the murder mystery games for LMAs. Experiments show that although current LMAs show promising performance in basic perceptual tasks, they are insufficiently equipped for complex multi-agent collaboration and multi-step reasoning tasks. Furthermore, the full application of the theory of mind to complete games in a manner akin to human behavior remains a significant challenge. We hope this work can illuminate the path forward, providing a solid foundation for the future development of LMAs. Our WhodunitBench is open-source and accessible at: https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games.

## 1 Introduction

Large multimodal agents (LMAs) [29; 21; 27; 11] are systems capable of perceiving its environment and making decisions based on these perceptions to achieve specific goals within the multimodal context driven by large language models (LLMs). LMAs are anticipated to handle _diverse_ and _challenging_ tasks that demand a broad range of capabilities, including low-level multimodal perception, high-level cognition (e.g., multi-step reasoning), role-playing for interactive engagement and deliberative decision-making.

Given these diverse capabilities, the evaluation of LMAs varies widely across research domains. Some studies prioritize the agents' competency in executing complex internet-based tasks [8; 10; 16; 30], while others focus on assessing their reasoning and decision-making abilities [14; 15; 26; 6]. Additionally, a significant body of research explores these agents' capacities for long-term planning and execution [35; 28; 33]. However, it is challenging to design such an evaluation benchmark to evaluate the various capabilities of LMAs within the same environment. We categorize the capabilities into the following four classes:

* **Multi-modal Perception** is the most basic ability for LMAs, which requires LMAs to perceive information from the multimodal environment (e.g., vision and language).
* **Interaction** requires LMAs, whether through role-playing or direct engagement, to communicate with the environment or other agents to gather essential information for task completion.
* **Reasoning** requires LMAs to combine their internal knowledge with newly gathered information to perform long-chain, multi-step reasoning.
* **Decision Making and Goal Achievement** requires LMAs to establish clear goals and make independent decisions in response to environmental changes. This autonomous decision-making is crucial for effectively navigating and completing tasks in dynamic settings.

Interestingly, murder mystery games [5; 12; 3], a genre of party games, offer a unique and covert opportunity to evaluate LMAs across the forementioned dimensions.* As illustrated in Figure 1, a murder mystery game unfolds in a virtual world crafted by multiple players, with the goal being to identify the'murderer' through the following procedure:

* **Initialization Phase**: Players need to perceive multimodal information, including extensive script text and various types of image clues, while role-playing their assigned characters to present this information.
* **Discussion Phase**: Players need to role-play their own roles to interact with the environments or other players to get more clues. During this process, they need to perform decision making to assess the authenticity of information and to gather more details to supplement and refine the tasks required for each role.

* **Reasoning Phase**: Players need to reason over the information collated from the previous two phrases, always involving complex multi-step multi-modal reasoning.
* **Voting Phase**: Ultimately, through a voting process involving all players, the'murderer' is determined. This can evaluate if the players achieve their goals (i.e., identifying the'murderer' or hiding themselves).

Therefore, based on this, we introduce a comprehensive benchmark via murder mystery games designed to evaluate LMAs (named WhodunitBench) in this paper. Table 1 presents the primary characteristics of our benchmark in comparison to others. Specifically, we propose two evaluation modes: (1) **Arena-style Evaluation**, which simulates real gameplay by having agents act as players in one-on-one online competitions, uses their win rate as the primary evaluative metric. (2) **Chain of Evaluation**, which provides a comprehensive analysis of agent performance by designing and annotating over 3,000 multiple-choice questions and brief answer sentences. In this evaluation, each metric is designed to align with the game environment while comprehensively supplementing previous assessments. We select five representative LMAs, including Yi-Vision , Qwen-VL-Plus , Gemini-pro-vision , Claude-Opus  and GPT-4V  and conduct extensive experiments on our WhodunitBench. Experimental findings reveal that even the advanced GPT-4V , which attains the highest win rate in the online arena, still encounters challenges in successfully completing this game. **Hallucinations **, failure to truly understand the script, and difficulty in immersing into roles are its primary error manifestations. Our Chain of Evaluation (CoE) offers more insights for researchers, highlighting that while LMAs typically perform well in basic perception, they struggle with complex multi-modal reasoning and effective interaction within role-playing scenarios. Ultimately, the contributions of our paper are three-fold:

* We propose to use _murder mystery games_ as the environments to assess a variety of abilities of LMAs. To this end, we design a benchmark, called WhodunitBench, consisting of two modes: an online battle arena and a chain of evaluation.
* We curate evaluation samples in two modes: 50 scripted scenarios using win rate for direct confrontation between LMAs, and over 3,000 multiple-choice and open-ended questions to quantify specific capabilities, complementing the win rate assessment with detailed skill evaluations.
* Experiments conducted on WhodunitBench demonstrate that existing state-of-the-art LMAs struggle in dynamic scenarios and complex composition tasks. Against our naively designed agent, these agents achieve a maximum win rate of only 25%, and their scores for role-play interaction barely exceed 20 points.

## 2 Related Work

**Evaluating Agent.** As LLMs become increasingly prevalent, the development of intelligent agents and the benchmarks for evaluating them continue to evolve [16; 8; 10; 15]. Previous benchmarks have primarily focused on simple yet tedious web-based tasks [16; 30; 8] aimed at developing agents

    & Multi- & Multi-step & Role- & Reasoning &  \\  & Modal & Reasoning & play & Type & \\  DDD  & ✗ & ✗ & ✓ & Incomplete Information & Isolated Evaluation \\ AVALONENCH  & ✗ & ✗ & ✓ & Incomplete Information & Online Competition \\ GAIA  & ✓ & ✓ & ✗ & ✗ & Isolated Evaluation \\ VisualWebArena  & ✓ & ✗ & ✗ & ✗ & Isolated Evaluation \\ WorldQA  & ✓ & ✓ & ✗ & Complete Information & Isolated Evaluation \\ MCOT  & ✓ & ✓ & ✗ & Complete Information & Isolated Evaluation \\ Rolellm  & ✓ & ✗ & ✓ & ✗ & Isolated Evaluation \\ SOTOPIA  & ✗ & ✗ & ✓ & ✗ & Isolated Evaluation \\  WhodunitBench (ours) & ✓ & ✓ & ✓ & Incomplete Information & Online Competition \& Chain of Evaluation \\   

Table 1: Detailed comparative analysis of our benchmark with others across multiple dimensions. Specifically, ”Incomplete Information” refers to cases where an agent’s information includes only a portion of what is required for reasoning, with the remaining information needing to be acquired through effective interactions. Meanwhile, ”Online Competition” denotes direct, real-time, head-to-head matches between agents in a dynamic environment.

capable of managing repetitive aspects of human online activities. Besides, environments such as "Werewolf" [15; 22] are used to assess agents' strategic and decision-making skills, while other benchmarks [28; 35] evaluate long-term strategy and adaptability in specialized scenarios. In contrast, our proposed benchmark evaluates agents in realistic scenarios, where they must simultaneously employ multiple skills rather than focusing on a single ability in a controlled lab environment. More importantly, these skills closely mirror those humans rely on when completing tasks in the real world. This includes the perception and understanding of multimodal content, gathering additional information through interactions with the surrounding environment or other individuals, and finally, integrating this information with prior knowledge to carry out multi-step analysis, reasoning, and decision-making under incomplete information to accomplish their tasks.

**Evaluating LMAs on Gaming Platforms.** Games [11; 9], with their simple rules, clear standards, controllable difficulty, and limited scope for action or observation, are increasingly being used as benchmarks for evaluation agents. In addition to the Werewolf-style text games previously mentioned, studies have also explored using games like "Red Dead Redemption II"  and various open-world environments [31; 24] to evaluate the capabilities of LMAs. Employing these games for testing generally demands significant resources and time. Some researchers also have suggested employing murder mystery games as a more efficient alternative for testing . They primarily assessed text-based agents using relatively straightforward evaluation methods, focusing on multiple-choice questions. In contrast, our evaluation system not only offers two distinct assessment methods but also integrates a range of question types in the second method, particularly emphasizing multi-step multi-modal long-chain reasoning questions. This comprehensive evaluation system fully leverages the scripted murder mystery platform to test agents' abilities in dynamic, information-incomplete environments, closely mirroring human performance.

## 3 WhodunitBench: Construction

In this section, we describe the construction of WhodunitBench, which features an online competitive arena that simulates a realistic gameplay experience, as well as the CoE framework designed to assess LMAs' capabilities through a sequence of "Perception - Role-playing Interaction - Cognition" aligned with the respective stages of gameplay.

### Constructing Arena

**Data Collection:** The construction of different games relies on diverse scripts, making the selection and collection of these scripts particularly crucial. We enlisted the expertise of seasoned murder mystery game experts to ensure the quality and applicability of the selected scripts. These scripts were sourced primarily from industry-recognized creative teams and platforms. We established clear selection criteria focused on three key aspects:

* **Scientific Integrity**: We have systematically excluded scripts incorporating metaphysical elements, particularly temporal displacement and consciousness transference. This methodological approach ensures that murder mysteries within these scripts remain grounded in empirical logic and scientific principles, thus maximizing operational viability and narrative credibility.
* **Content Complexity**: We chose scripts with a higher degree of reasoning complexity to thoroughly test the deductive capabilities of LMAs.
* **Logical Coherence**: We ensured all scripts were logically sound, with evidence and clues distributed in a balanced and reasonable manner.

**Data Quality Control:** We conducted a systematic review and optimization of the 50 real scripts collected. Initially, we ensured that the extracted script sections were complete, and we verified the fluidity and grammatical correctness of the text. Subsequently, we confirmed the completeness and integrity of the visual and textual clues within the scripts. Lastly, we examined the consistency of the timeline and the sequence of events, ensuring the logical coherence and rational progression of the plot. Following the comprehensive selection and rigorous review processes delineated above, we curated and refined a total of 50 scripts that conformed to our stringent criteria. The distribution overview of the number of roles in the script is shown in Figure 2 (b). These scripts were utilized to construct the online competitive arena.

### Constructing Chain of Evaluation (CoE) Dataset

#### 3.2.1 Perception

**Question Types**: The evaluation of perception consists of **multiple-choice questions** categorized into three types: (1) Text-rich Image Questions (TRI-QA): These questions primarily involve text-based clues presented as images within the game, particularly those containing only text. (2) Media-rich Image Questions (MRI-QA): These questions primarily concern image clues within the game that contain both rich textual and visual elements. (3) Long Script Questions (LS-QA): These questions pertain to the textual content embedded within the game's script and role's script.

**Question Statistics**: There are a total of 1,911 multi-choice questions for perception assessment, categorized into 283 long script questions, 1,103 text-rich image questions, and 525 media-rich image questions. The distribution is illustrated in Figure 2(a).

#### 3.2.2 Role-play Interaction

**Question Types**: To evaluate the role-play interaction capability of LMAs, we primarily annotated two types of data to serve as the ground truth for our assessment: (1) The first category comprises a collection of statements containing key clues in each script. (2) The second category consists of the core roles within each script.

Figure 3: Data generation and annotation: (a) Examples of annotated ground truth reasoning chains; (b) Multiple-choice question generation process; (c) Interactive statement annotation process.

Figure 2: Statistics of the proposed dataset: (a) Distribution of perception QA; (b) Distribution of the number of roles in the scripts; (c) Distribution of reasoning steps for cognition assessments.

**Question Annotation**: As shown in Figure 3(c), we compile all key clues necessary to solve each murder mystery in the script, encompassing both direct and indirect clues. These clues serve as ground truth statements for evaluating the effectiveness of LMAs in their role-play and for advancing the game's progression during the discussion phase. Additionally, we identify the core roles in each script, whose personal scripts contain critical clues for identifying the murderer. For instance, the clue "Character 2 was standing outside shooting" appears solely in Character 1's script, marking Character 1 as a core role.

#### 3.2.3 Cognition

**Question Types**: The design of cognitive evaluation questions primarily consists of two types: (1) Multi-choice questions for multi-step reasoning assessments. (2) Open-ended questions to evaluate the accuracy of LMAs' analysis of murderer's motives and methods.

**Question Annotation**: Each script is accompanied by a truth manual that contains all clues essential for task completion. Annotators reference this manual, refining details to establish the final ground truth statements concerning the murderer's motive and methodology. The annotation process of multi-choice questions is divided into two stages, as shown in Figure 3(a) and (b): 1 Construction of Reasoning Chains: we construct each reasoning step required to unravel the murder mystery within the script, leveraging the information provided in the truth manual and supplementing it with essential details. For example, to deduce that "The victim's fatal wound was not caused by a knife", we first identify key clues given directly in the game, such as images showing the victim's internal organs mirrored compared to a normal person. This information leads us to a 1-hop indirect but crucial clue: the victim's heart is on the right side. Further combining this with expert knowledge and textual clues about a knife wound on his left chest, we infer a 2-hop indirect clue: the knife wound was not fatal. Using this approach, we continuously pinpoint direct clues and deduce indirect ones, eventually linking them into a complete reasoning chain. 2 Constructing multiple-choice questions: after building the reasoning chains, annotators use the content of these chain nodes to formulate tiered reasoning questions with correct answers. GPT-4 then creates distractor options based on these correct answers, matching their length to enhance confusion.

**Question Statistics**: We annotated 1,308 reasoning multiple-choice questions. The distribution across different levels is illustrated in Figure 2(c).

#### 3.2.4 Data Quality Control

To improve our dataset's quality, we engaged three experts to perform data reviews based on specific standards. Any questions not meeting these standards will be refined. The standards are as follows: (1) If there are substantial informational gaps between nodes in the reasoning chain, intermediate steps will be added to maintain logical consistency; (2) For incorrect options generated by GPT-4, if they are too simplistic or clearly implausible within the problem's context, experts will manually revise them.

## 4 WhodunitBench: Arena-style Evaluation

WhodunitBench provides an online arena where LMAs compete in pairwise, faction-based matches, with win rates serving as the primary measure of success. Additionally, we record each multimodal agent's performance data in the arena, including their dialogue outputs and chosen actions.

### Settings

**Agent Settings** We design two settings: one where a naive agent is defined as the murderer and each LMA competes against this naive agent, and another where selected LMAs engage in pairwise competitions. In the game, a multimodal agent in the non-murderer suspect faction, which comprises various roles, will adopt different roles to compete against the agent representing the murderer in the murderer action. For **LMAs**, we selected five multimodal agents for evaluation: Yi-Vision , Qwen-VL-Plus , Gemini-pro-vision , Claude-Opus , GPT-4V . For **Naive Agent**, we defined a naive agent that retrieves information about itself and provides output based on the search. When questioned by others, it finds and responds with content related to its role; if it finds nothing,it simply answers, "I don't know." Moreover, if in the murderer action it is suspected of being the murderer and it retrieves information confirming this, it will immediately reveal its identity.

**Metric** In the arena, we use win rate and loss rate as the sole evaluation criteria. The non-murderer suspect faction wins by correctly identifying the murderer, whereas the murderer faction wins by evading identification. In the table 2, the rows represent the non-murderer suspect faction, and the columns represent the murderer suspect faction. We utilize the average win rate (\(}{}\)) and average loss rate(\(}{}\)) to assess LMAs' performance. If the average win rate is high or the average loss rate is low, it indicates that the agent is strong.

### Results

We report the results in Table 2. We have the following observations: (1) **The overall win rate remains low**. Regardless of the type of multimodal intelligent agent assuming the role of the "Non-Murder," their win rates hover between 10% and 20%. This underscores the substantial challenges faced by all current advanced LMAs, including the latest iteration, GPT-4V, in achieving the objectives set out in the game. This suggests a significant gap in the performance capabilities of these agents when tasked with complex, goal-oriented tasks in dynamic environments; (2) **Stronger models do not necessarily perform better when playing the role of the murderer**. For instance, the Gemini model, regardless of its opponent, is most likely to be identified as the murderer. This may be because more capable models, realizing their role as the murderer, tend to over-communicate in an attempt to obscure the truth, which ironically makes them more susceptible to detection by other players. Conversely, less capable models, such as Qwen, might speak less frequently, making them less likely to be convicted in the game.

## 5 WhodunitBench: Chain of Evaluation (CoE)

In this section, we introduce the CoE Evaluation System, specifically designed to assess three core capabilities through a detailed framework of eight evaluation metrics, grounded in the annotated data from the previous section. With these design standards, we can not only systematically analyze and evaluate each agent's performance at various stages of the game but, more importantly, also provide a strong supplement to previous online competition assessments, showcasing each LMA's capabilities in greater detail. Table 3 presents the evaluation metrics for each LMA when playing a non-murder role against the naive agent we designed (acting as the murderer). Since the naive agent lacks certain "intelligence," the murderer does not interfere, allowing for a clearer demonstration of each LMA's performance across various capabilities.

### Assessment Details

**Perceptual Ability Assessment**: To successfully complete the task, the agent must be able to perceive and comprehend a substantial amount of visual and textual information across various stages of the game, particularly during the initialization phase (as illustrated in the figure 1). These information

  
**Agent vs. Agent** & Naive Agent & Yi-Vision & Qwen-VL-Plus & Gemini & Claude & GPT-4V & **Avg. Win Ratio (\%)** \\  Yi-Vision & 12.0\% & - & 10.0\% & 14.7\% & 11.6\% & 7.1\% & 11.1\% \\  Qwen-VL-Plus & 9.1\% & 6.8\% & - & 16.2\% & 11.4\% & 9.5\% & 10.6\% \\  Gemini & 21.4\% & 13.2\% & 15.8\% & - & 22.9\% & 11.4\% & 16.9\% \\  Claude & 21.1\% & 11.1\% & 15.9\% & 25.0\% & - & 22.7\% & 19.2\% \\  GPT-4V & 25.0\% & 18.2\% & 23.3\% & 29.5\% & 25\% & - & **24.2\%** \\ 
**Avg. Loss Ratio (\%)** & 17.7\% & 12.3\% & 16.3\% & 21.4\% & 17.7\% & 12.7\% & - \\   

Table 2: Comparative benchmarking of LMAs in an online battle arena. Each cell (Row, Col) indicates the win rate of the Row agent against the Col agent. Note that we excluded instances where no clear winner was determined, including cases with API errors or draws in the voting for the murderer.

are typically referred to as mystery scripts and clues within the game. We have developed three categories of metrics for evaluation: (1) Text-rich image understanding (TIU): This metric assesses agents' proficiency in precisely interpreting and extracting clues from text-rich images, emphasizing their Optical Character Recognition (OCR) capabilities. It primarily utilizes the TRI-QA annotations from Section 3.2.1. (2) Media-rich image understanding (MIU): This metric evaluates how effectively agents integrate textual and visual elements to interpret and understand more complex clues within images, which may include diagrams, maps or residential layouts. It aims to gauge the agents' ability to navigate intricate visual cues that require both recognition and contextual comprehension. And it primarily utilizes the MRI-QA annotations from Section 3.2.1. (3) Long-script understanding (LSU): This metric evaluates agents' ability to process and extract critical information from lengthy texts, specifically the script content within the game, which sometimes exceeds tens of thousands of words in length. It primarily utilizes the LS-QA annotations from Section 3.2.1. Their scoring formula is defined as: \(_{}=}{}\).

**Strategic Decision-Making and Role-playing Assessment**: To evaluate the role-playing and interactive communication abilities of LMAs, we recorded their dialogues and performances from the online competition and assessed them using two metrics: (1) RP (Role-Playing) Index: This metric assesses the naturalness of the recorded agent dialogues with other roles. It is scored on a ten-point scale, with several criteria designed for GPT-4 to use in scoring. (2) SPC (Scenario Progression Capability) Index: This metric evaluates whether the agent's dialogue contributes to task completion (e.g., identifying the murderer's motive), rather than discussing irrelevant or off-topic content. The score is calculated using annotated ground truth statements from Section 3.2.2: \(=(}{}) 100\). Additionally, we assess the decision-making ability of the agent during the discussion phase. Points will be awarded if the agent chooses to question previously identified key roles from Section 3.2.2 and deems this decision valuable. The calculation method is as follows: \(=(}{}) 100\).

**Comprehensive Cognition Assessment**: To accurately identify the murder in the murder mystery, the agent must integrate various clues to perform complex reasoning across different levels. This evaluation primarily focuses on assessing this capability. As detailed in Table 3, the assessment utilizes two metrics: MMR (Multi-modal Multi-step Reasoning) and CMD (Case Murdter Detail), each with its distinct evaluation method. The MMR metric is primarily evaluated through four multiple-choice questions labeled in Section 3.2.3, scored similarly to perception questions. The CMD metric requires the agent to present its conclusions about the murderer's method and motive in

    &  &  &  &  &  \\    & LSU & TIU & MIU & RP & SPC & ITD & MMR & CMD \\  Random & 25.00 & 25.00 & 25.00 & - & - & - & 25.00 & - & - \\   \\  Direct & 42.40 & 28.66 & 34.99 & 7.16 & 2.37 & 20.61 & 20.31 & 16.03 & 21.57 \\ COT & 32.80 & 15.36 & 27.40 & 7.20 & 2.79 & 15.26 & 25.41 & 22.47 & 18.58 \\   \\  Direct & 38.40 & 43.03 & 39.41 & 7.15 & 0.66 & 17.30 & 17.68 & 15.99 & 22.45 \\ COT & 36.00 & 51.36 & 46.50 & 7.09 & 0.76 & 20.61 & 22.03 & 13.59 & 24.74 \\   \\  Direct & 92.00 & 68.11 & 57.68 & 7.45 & 10.72 & 25.98 & 54.32 & 18.22 & 41.81 \\ COT & 88.80 & 57.78 & 57.84 & 7.22 & 10.79 & 19.08 & 57.39 & 19.20 & 39.76 \\   \\  Direct & 90.00 & 67.39 & 52.98 & 8.00 & 9.51 & 19.63 & 55.08 & 18.96 & 40.19 \\ COT & 88.80 & 35.31 & 55.02 & 7.89 & 12.08 & 25.45 & 57.78 & 22.07 & 38.05 \\   \\  Direct & 93.60 & 79.29 & 68.09 & 7.98 & 9.49 & 25.95 & 57.12 & 25.18 & 45.84 \\ COT & 92.40 & 51.88 & 69.25 & 6.43 & 19.63 & 16.28 & 58.75 & 26.43 & 42.63 \\   

Table 3: Evaluating LMAs in non-murderer factions versus naive agents using the COE dataset revealed distinct outcomes.

the form of open-ended responses. The answers provided by the agent will be compared with the ground truth outlined in Section 3.2.3 and automatically scored using GPT-4.

### Evaluation Results

**Multimodal agents demonstrate suboptimal performance during discussion phases.** Table 3 shows that even the more capable GPT-4V only achieves an average score of approximately 20 during these segments. This finding suggests that the discussion phase provides limited assistance for all multimodal agents in fulfilling game-related tasks. This trend may be indicative of agents predominantly issuing irrelevant remarks, rather than delivering effective information conducive to reasoning and problem-solving. These observations underscore the need for improvements in how multimodal agents integrate into the gaming world and embody their roles, highlighting a significant gap in their ability to leverage discussion segments to facilitate game progression effectively.

**The CoT reasoning framework does not always bring benefits.** Although it can improve most performance indicators in Table 3, it sometimes reduces the effective output during the dialogue phase. As pointed out in some studies, murder mystery games are games of incomplete information, and advanced reasoning frameworks like CoT are not always guaranteed to be effective in such environments. Moreover, when dealing with tasks that do not require deep reasoning, such as directly recognizing text from images, introducing CoT might instead lead to a significant decline in performance. This emphasizes the need for precise adjustment based on the specific requirements of the task when selecting and applying reasoning techniques.

### Further Analysis

**Qualitative analysis** We show a case of reasoning chains generated by GPT-4V in Figure 4(Left). We can observe that it reason over the available clues including (a) textual and visual clues obtained from character scripts; (b) useful dialogue information collected from other roles during the discussion phase.

There are some issues with LMAs in role-playing as shown in Figure 4(Right). It includes low role-play integration (i.e., not basing inquiries on existing clues within the game), forgetting the game settings, and hallucination (e.g., agents may refer to characters not mentioned in the script).

**Correlation between the CoE evaluation scores and the win rates in the online arena** By analyzing and comparing the metrics from Tables 2 and 3, it is evident that LMAs with higher scores in the CoE assessment also have higher win rates in the arena. _This suggests that the CoE evaluation method is effective in providing detailed insights into performance within competitive arenas._ Among the CoE metrics, reasoning-related metrics exhibit the strongest correlation with win rate, suggesting that reasoning capabilities are the most significant contributors to success.

Figure 4: Qualitative analysis on WhodunitBench. Left: Reasoning chains generated by GPT-4V; Right: LMAs’ role-playing and dialogue performance during games.

## 6 Limitations and Potential Societal Impact

Our benchmark, WhodunitBench, features two modes: an online arena and a chain evaluation, designed to assess LMAs in realistic scenarios. This setup mirrors human behavior by requiring LMAs to combine multiple abilities at once, rather than isolating skills in controlled experiments. However, potential concerns and limitations remain regarding the evaluation methodology, metric design, and current data collection practices.

**Combination of Social Skills and Reasoning Abilities.** We find that the current evaluation intertwines interaction and reasoning, making the results less interpretable. In the data annotation process, we labeled critical clues that necessitate interaction to be uncovered. To separate interaction from reasoning, these key clues can be directly provided to agents in the "no-murderer" faction, enabling an analysis of each aspect's individual impact on scoring. Although this approach has been attempted, more effective solutions may exist for addressing this issue.

**The Kind of Reasoning Abilities.** Murder mystery games primarily assess core reasoning skills, such as logical deduction, visual-text detail verification, timeline reasoning and hypothesis testing. These games do not cover all reasoning abilities, particularly in computer programming. Strong performance in these games does not guarantee proficiency in all reasoning contexts. However, we believe that, the skills developed, like logical analysis and detail interpretation, are foundational and can be extended to other domains, holding significant potential for broader application.

**Cost considerations are pivotal.** Given that the script for each character often exceeds 5,000 words, the volume of data required to effectively test multimodal agents, such as GPT-4V, is substantial. Therefore, the evaluation on WhodunitBench is more expensive compared to existing benchmarks.

Additionally, we believe our benchmark has minimal societal impact. However, as agents integrate into daily life, the accuracy of our evaluations could shape public perception of their capabilities, possibly leading to unintended consequences.

## 7 Conclusion

In this work, we propose WhodunitBenchfor evaluating LMAs' capability in multi-modal perception, interaction, multi-step reasoning and goal execution. It includes 50 meticulously curated scripts and over 3000 closed-ended multiple-choice questions, along with corresponding open-ended queries featuring human-annotated ground truth. This framework supports online arena-style evaluations and enables detailed chain-linked assessments to evaluate specific capabilities at each stage of the game. Experiments demonstrate that existing LMAs struggle to perform complex tasks requiring compositional skills in dynamic interactive environments; even the state-of-the-art GPT-4V achieves a low score. We hope this work will guide future advancements, establishing a solid foundation for the continued development of LMAs.