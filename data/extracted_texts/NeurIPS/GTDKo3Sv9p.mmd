# Discrete Flow Matching

Itai Gat\({}^{1}\) &Tal Remez\({}^{1}\) &Neta Shaul\({}^{2}\) &Felix Kreuk\({}^{1}\) &Ricky T. Q. Chen\({}^{1}\)

&Gabriel Synnaeve\({}^{1}\) &Yossi Adi\({}^{1}\) &Yaron Lipman\({}^{1}\)

\({}^{1}\) Meta FAIR &Weizmann Institute of Science

###### Abstract

Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: _(i)_ it works with a general family of probability paths interpolating between source and target distributions; _(ii)_ it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser (\(x\)-prediction) and noise-prediction (\(\)-prediction); _(iii)_ practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and _(iv)_ by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on _1-shot_ MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.

## 1 Introduction

Despite the remarkable success of diffusion and flow models in generating continuous spatial signals such as images (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024) and videos (Singer et al., 2022; Blattmann et al., 2023), their performance still falters when applied to discrete sequential data compared to autoregressive models. Recent progress in adapting diffusion and flow models to the discrete setting has been made via mostly two approaches: embedding the discrete data in continuous space and applying continuous diffusion (Dieleman et al., 2022; Stark et al., 2024) or designing diffusion or flow processes over discrete state spaces (Austin et al., 2021; Campbell et al., 2022).

In this paper, we pursue the discrete flow approach of Campbell et al. (2024) and introduce Discrete Flow Matching, a theoretical framework and algorithmic methodology for discrete flow models that yields a state-of-the-art discrete non-autoregressive generative approach. Surprisingly, Discrete Flow Matching exhibits similarities with the continuous Flow Matching (Lipman et al., 2022) approach proposed for continuous signals. Notably, its _generating probability velocity_, employed in the sampling algorithm, is identical in form to its continuous counterpart. Additionally, Discrete Flow Matching offers the following advancements and simplifications over prior methods: It encompasses a more comprehensive family of probability paths transforming source (noise) distributions into target (data) distributions, accommodating arbitrary source-target couplings and time-dependent schedulers. Furthermore, it provides a unified formulation for the generating probability velocity directly expressed in terms of the learned posteriors and schedulers, along with a unified and general theory and algorithm for corrector sampling and iterations. In practice, we observe that path and corrector schedulers are pivotal, and their proper tuning leads to substantial improvements ingeneration quality. We have trained a 1.7B parameter Discrete Flow Matching model on the same data mix as in Llama-2 (Touvron et al., 2023) and CodeLlama (Roziere et al., 2023), achieving 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on _1-shot_ MBPP coding benchmarks; Figure 1 shows some code generation examples. In conditional text generation our model produces texts with a generative perplexity score of 9.7 as measured by the Llama-3 BB model, surpassing a 1.7B autoregressive model that achieves 22.3 and not far from the Llama-2 7B model that achieves 8.3 in generative perplexity score. We strongly believe that Discrete Flow Matching represents a significant step in bridging the performance gap between discrete diffusion and autoregressive models, and that further enhancements are possible by exploring the vast design space that Discrete Flow Matching has to offer.

## 2 Discrete Flow Matching

### Setup and notations

In discrete sequence modeling, we denote a sequence \(x\) as an array of \(N\) elements \((x^{1},x^{2},,x^{N})\). Each element, or _token_, within this sequence is selected from a vocabulary of size \(d\). Consequently, the entire set of possible sequences is given by \(=[d]^{N}\), where \([d]=\{1,,d\}\). A random variable taking values in the space \(\) is denoted by \(X\) and its corresponding probability mass function (PMF) is \(P(X=x)\). For simplicity, throughout the paper, we sometimes omit the random variable \(X\) and use \(p(x)\) to denote the PMF.

To describe marginalization properties, we denote \(p(x^{i})\) the \(x^{i}\) marginal of \(p\), i.e., \(p(x^{i})=_{x^{i}}p(x)\), where \(x^{}=(,x^{i-1},x^{i+1},)[d]^{N-1}\) are all the arguments excluding \(i\). Similarly, \(p(x^{i})=_{x^{i}}p(x)\), and \(x^{i}[d]\). A useful PMF is the delta function, \(_{y}\), \(y\), which is defined by

\[_{y}(x)=_{i=1}^{N}_{y^{i}}(x^{i}),_{y^{i}}(x^{i})= 1&x^{i}=y^{i}\\ 0&x^{i} y^{i}. \]

With the marginal notation \(_{y}(x^{i})=_{y^{i}}(x^{i})\) and \(_{y}(x^{})=_{y^{i}}(x^{})=_{j i}_{y^ {j}}(x^{j})\) which simplifies notation.

### Source and target distributions

In discrete generative models our goal is to transform source samples \(X_{0} p\) to target samples \(X_{1} q\). Our training data, consist of pairs \(X_{0}\) and \(X_{1}\) that are sampled from a joint distribution \((x,y)\), satisfying the marginals constraints \(p(x)=_{y}(x,y),q(y)=_{x}(x,y)\), i.e.,

\[(X_{0},X_{1})(X_{0},X_{1}). \]

Figure 1: **Code generation examples using Discrete Flow Matching. Code condition is marked in gray, model generation is marked in yellow. Left sub-figure presents the standard left-to-right prompting; Middle and Right sub-figures, presents complex infilling setup.**

In the simplest case, the training pairs \(X_{0}\) and \(X_{1}\) are sampled independently from the source and target distributions respectively,

\[(X_{0},X_{1}) p(X_{0})q(X_{1}). \]

_Example:_ **source and couplings.** Common instantiations of source distribution \(p\) are: (i) adding a special token value often referred to as a'mask' or 'dummy' token, denoted here by \(\), and setting the source distribution to be all-mask sequences, i.e., \(p(x)=_{}(x)\); and (ii) using uniform distribution over \(\), which is equivalent to drawing each \(x^{i}\) independently to be some value in \([d]\) with equal probability, denoted \(p(x)=p_{}(x)\). In this paper we focus mainly on (i). We further consider two choices of couplings \(\): Independent coupling, which we call unconditional coupling (U-coupling), \((x_{0},x_{1})=p(x_{0})q(x_{1})\). A random sample that realizes this choice have the form

\[(X_{0},X_{1})=(,,),X_{1}, \]

where \(X_{1} q(X_{1})\) is a random sample from the training set. The second choice of coupling \((x_{0},x_{1})=p(x_{0}|x_{1})q(x_{1})\), which we find improves conditional sampling, partially masks inputs with samples of the form

\[(X_{0},X_{1})=( X_{1}+(-)(, ,),X_{1}), \]

where \(X_{1} q(X_{1})\) and \(\{0,1\}^{N}\) is a random variable indicating the conditioning, \(\) denotes the entry-wise product, and \(^{N}\) is the vector of all ones. We call this conditional coupling (C-coupling).

### Probability paths

We follow the Flow Matching approach (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijnden, 2022) that uses a predefined _probability path_\(p_{t}\) interpolating \(p\) and \(q\), i.e.,

\[p_{0}=p p_{1}=q \]

to train the generative model taking a source sample \(X_{0} p\) to a target sample \(X_{1} q\). We use arbitrary coupling of source and target (Pooladian et al., 2023; Tong et al., 2023), \((x_{0},x_{1})\), and the symmetric Flow Matching path (Albergo and Vanden-Eijnden, 2022) to define the marginal probability path,

\[p_{t}(x)=_{x_{0},x_{1}}p_{t}(x|x_{0},x_{1})(x_{0},x_{1}), p_{t}(x|x_{0},x_{1})=_{i=1}^{N}p_{t}(x^{i}|x_{0},x_{1}), \]

and \(p_{t}(x^{i}|x_{0},x_{1})\) is a time-dependent probability on the space of tokens \([d]\) conditioned on the pair \(x_{0},x_{1}\), and satisfying \(p_{0}(x^{i}|x_{0},x_{1})=_{x_{0}}(x^{i})\) and \(p_{1}(x^{i}|x_{0},x_{1})=_{x_{1}}(x^{i})\). If the conditional path \(p_{t}(x^{i}|x_{0},x_{1})\) satisfies these boundary conditions then the marginal path \(p_{t}(x)\) satisfies equation 6.

In developing the framework, we would like to consider as general as possible set of probability paths that are also tractable to learn within the Flow Matching framework. We consider conditional probability paths as a convex sum of \(m\) conditional probabilities \(w^{j}(x^{i}|x_{0},x_{1})\), i.e.,

\[p_{t}(x^{i}|x_{0},x_{1})=_{j=1}^{m}_{t}^{i,j}w^{j}(x^{i}|x_{0},x_{1}), \]

where \(_{j}_{t}^{i,j}=1\) and \(_{t}^{i,j} 0\) are collectively called the _scheduler_. Note that the scheduler can be defined independently for each location in the sequence \(i[N]\) or uniformly for all tokens, \(_{t}^{i,j}=_{t}^{j}\).

A simple yet useful instance of these conditional paths is reminiscent of the continuous Flow Matching paths formulated as convex interpolants,

\[p_{t}(x^{i}|x_{0},x_{1})=(1-_{t})_{x_{0}}(x^{i})+_{t}_ {x_{1}}(x^{i}), \]

where the scheduler \(_{t}\) satisfies \(_{0}=0\), \(_{1}=1\), and monotonically increasing in \(t\). Another interesting instantiation of equation 8 is adding uniform noise with some probability depending on \(t\),

\[p_{t}(x^{i}|x_{0},x_{1})=_{t}^{1}_{x_{1}}(x^{i})+_{t}^{2}p_{ a}(x^{i})+_{t}^{3}_{x_{0}}(x^{i}), \]

where \(_{0}^{1}=0\), \(_{1}^{1}=1\), \(_{0}^{2}=_{1}^{2}=0\) (remembering that \(_{j}_{t}^{i,j}=1\) and \(_{t}^{i,j} 0\)).

### Generating Probability Velocities

**Continuous generating velocity.** Sampling in continuous Flow Matching is performed by updating the current (continuous) sample \(X_{t}^{N}\), \(t[0,1)\), according to a learned _generating velocity field_\(u^{i}_{t}(X_{t})\), \(i[N]\). Euler sampling follows the (deterministic) rule

\[X^{i}_{t+h}=X^{i}_{t}+hu^{i}_{t}(X_{t}), \]

where \(h>0\) is a user-defined time step. Note that equation 11 is updating separately each of the sample coordinates, \(X^{i}_{t}\), \(i[N]\), see e.g., Figure 2, left. The velocity \(u^{i}_{t}(X_{t})\) can be either directly modeled with a neural network, or parameterized via the _denoiser_ (a.k.a. \(x\)-prediction) or _noise-prediction_ (a.k.a. \(\)-prediction), see left column in Table 1. If, for all \(t[0,1)\), starting at \(X_{t} p_{t}\) and sampling with equation 11 provides \(X_{t+h} p_{t+h}+o(h)\)1 then we say that \(u_{t}\)_generates_\(p_{t}\).

Generating probability velocity.For defining Flow Matching in the discrete setting, we follow Campbell et al. (2024) and consider a Continuous-Time discrete Markov Chain (CTMC) paradigm, namely the sample \(X_{t}\) is jumping between states in \(\), depending on a continuous time value \(t\). Similar to the continuous Flow Matching setting described above, we focus on a model that predicts the rate of probability change of the current sample \(X_{t}\) in each of its \(N\) tokens, see Figure 2, middle-left. Then, each token of the sample \(X_{t} p_{t}\) is updated independently by

\[X^{i}_{t+h}_{X^{i}_{t}}()+hu^{i}_{t}(,X_{t}), \]

where we call \(u_{t}\) the _probability velocity_ as reminiscent of the velocity field in continuous Flow Matching, and as in the continuous case, we define:

**Definition 1**.: Probability velocity \(u_{t}\)_generates_ the probability path \(p_{t}\) if, for all \(t[0,1)\) and given a sample \(X_{t} p_{t}\), the sample \(X_{t+h}\) defined in equation 12 satisfies \(X_{t+h} p_{t+h}+o(h)\).

Algorithm 1 formulates a basic sampling algorithm given a generating probability velocity \(u_{t}\). In order for the r.h.s. of equation 12 to define a proper PMF for sufficiently small \(h>0\), it is necessary and sufficient that the probability velocity satisfies the conditions

\[_{x^{i}[d]}u^{i}_{t}(x^{i},z)=0,u^{i}_{t}(x^{i},z) 0 i[N]x^{i} z^{i}. \]

```
0: velocity \(u_{t}\), sample \(X p\), step size \(h=\) for\(t=0,h,2h,,1-h\)do \(X^{i}_{X^{i}}()+hu^{i}_{t}(,X)\), for \(i[N]\)\(\) eq. 24 or 22 endfor return\(X\)
```

**Algorithm 1** Flow Matching sampling.

Figure 2: Discrete flow in \(=[d]^{N}\) with \(d=4,N=2\) (middle-left) versus continuous flow in \(^{N}\), \(N=2\) (left). The rate of change of probability of a state (gray disk) is given by the divergence operator shown in the continuous case (middle right) and the discrete case (right).

**Theorem 2**.: _Given a conditional probability velocity \(u_{t}^{i}(x^{i},z|x_{0},x_{1})\) generating a conditional probability path \(p_{t}(x|x_{0},x_{1})\), the marginal velocity defined by_

\[u_{t}^{i}(x^{i},z)=_{x_{0},x_{1}}u_{t}^{i}(x^{i},z|x_{0},x_{1} )p_{t}(x_{0},x_{1}|z) \]

_generates the marginal probability path \(p_{t}(x)\), where by Bayes' rule_

\[p_{t}(x_{0},x_{1}|z)=(z|x_{0},x_{1})(x_{0},x_{1})}{p_{t}(z)}. \]

For completeness we provide a simple proof of this theorem in Appendix E.2. The proof, similar to the continuous Flow Matching case, shows that \(u_{t}\) and \(p_{t}\) satisfy the (discrete version of the) Continuity Equation.

The Continuity Equation.To provide the mathematical tool for showing that a probability velocity \(u_{t}\) does indeed generate the probability path \(p_{t}\), and also to further highlight the similarities to the continuous case, we next formulate the _Kolmogorov Equations_, which describe the state probability rate \(_{t}(x)\), \(x\), in CTMC as a Continuity Equation (CE). The Continuity Equation, similarly to Kolmogorov Equations, describes \(_{t}(x)\), \(x^{N}\) in the _continuous case_, and is formulated as the Partial Differential Equation (PDE)

\[_{t}(x)+_{x}(p_{t}u_{t})=0, \]

where the divergence operator \(_{x}(v)\) applied to a vector field \(v:^{N}^{N}\) is defined by

\[_{x}(v)=_{i=1}^{N}_{x^{i}}v^{i}(x), \]

and intuitively means the total flux leaving \(x\), see Figure 2 (middle-right). This gives an intuitive explanation to the Continuity Equation: the rate of the probability \(_{t}(x)\) of a state \(x^{N}\) equals the total _incoming probability flux_, \(p_{t}u_{t}\), at \(x\). In the discrete case (CTMC) the Continuity Equation (equation 16) holds as is, once the discrete divergence operator is properly defined, i.e., to measure the outgoing flux from a discrete state. In more detail, given some vector field, which in the discrete case is a scalar-valued function over pairs of states, \(v:\), the discrete divergence is

\[_{x}(v)=_{z}[v(z,x)-v(x,z)], \]

where \(v(z,x)\) represents the flux \(x z\) and \(v(x,z)\) represent the opposite flux \(z x\); see Figure 2, right. Now, in our case (see Figure 2, middle-left), the probability flux at a state \(x\) involves all sequences with at most one token difference from \(x\), i.e., the probability flux \(p_{t}u_{t}\) at \(x\) takes the form \(v(x,z)=p_{t}(z)u_{t}^{i}(x^{i},z)\) and \(v(z,x)=p_{t}(x)u_{t}^{i}(z^{i},x)\) for \(z\) and \(x\) that differ only in the \(i\)-th token, \(v(x,x)=p_{t}(x)_{i=1}^{N}u_{t}^{i}(x^{i},x)\), and \(v(x,z)=0\) for all other \((z,x)\). A direct calculation now shows (see Appendix E.1):

\[_{x}(p_{t}u_{t})=-_{z}p_{t}(z)[_{ i=1}^{N}_{z}(x^{})u_{t}^{i}(x^{i},z)]. \]

Checking that a probability velocity \(u_{t}\) generates a probability path \(p_{t}\) (in the sense of Definition 1) amounts to verifying the Continuity Equation (equation 16). Indeed, using arguments from Campbell et al. (2024) and the discrete divergence operator, the PMF of \(X_{t+h}\) defined by sampling according to equation 12 is

\[_{X_{t}}&_{i=1}^{N} [_{X_{t}}(x^{i})+hu_{t}^{i}(x^{i},X_{t})]=_{X_{t}} [_{X_{t}}(x)+h_{i=1}^{N}_{X_{t}}(x^{})u_{t}^{i}(x^ {i},X_{t})]+o(h)\\ &=p_{t}(x)-h_{x}(p_{t}u_{t})+o(h) }{=}p_{t}(x)+h_{t}(x)+o(h)=p_{t+h}(x)+o(h), \]

where we assume \(X_{t} p_{t}\), the first equality uses the identity \(_{i}[a^{i}+hb^{i}]=_{i}a^{i}+h_{i}(_{j i}a^{j })b^{i}+o(h)\), the second equality uses equation 19, and the previous-to-last equality uses the Continuity Equation (equation 16). This shows that if the Continuity Equation holds then \(u_{t}\) generates \(p_{t}\) in the sense of Definition 1.

**Conditional and marginal generating velocities.** We provide the probability velocities generating the conditional probability paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8. Then, using the marginalization formula in equation 14 we end up with a closed-form marginal velocity for the probability paths \(p_{t}(x)\). In Appendix E.3 we show

**Theorem 3** (Probability velocity of conditional paths).: _A generating probability velocity for the conditional paths \(p_{t}(x|x_{0},x_{1})\) defined in equations 7 and 8 is_

\[u^{i}_{t}(x^{i},z|x_{0},x_{1})=_{j=1}^{m}a^{i,j}_{t}w^{j}(x^{i}|x_{0},x_{1} )+b^{i}_{t}_{z}(x^{i}), \]

_with \(a^{i,j}_{t}=^{i,j}_{t}-^{i,j}_{t}^{i,}_{t}/ ^{i,}_{t}\), and \(b^{i}_{t}=^{i,}_{t}/^{i,}_{t}\) where \(=_{j[m]}[^{i,j}_{t}/^{i,j}_{t}]\)._

Now, computing the marginal probability velocity using equation 14 applied to the conditional probability velocity in equation 21 gives

\[u^{i}_{t}(x^{i},z)=_{j=1}^{m}a^{i,j}_{t}^{j}_{t}(x^{i},z)+b^{i,j}_{ t}_{z}(x^{i}), \]

where the posteriors \(^{j}_{t}\) of \(w^{j}\) (that are later shown to be tractable to learn) are defined by

\[^{j}_{t}(x^{i},z)=_{x_{0},x_{1}}w^{j}(x^{i}|x_{0},x_{ 1})p_{t}(x_{0},x_{1}|z), \]

where \(p_{t}(x_{0},x_{1}|z)\) (defined in equation 15) is the posterior probability of \(x_{0},x_{1}\) conditioned on the current state \(X_{t}=z\). A useful instantiation of the general velocity in equation 22 is when considering the path family in equation 9, for which \(w^{1}(x^{i}|x_{0},x_{1})=_{x_{1}}(x^{i})\), \(w^{2}(x^{i}|x_{0},x_{1})=_{x_{0}}(x^{i})\), \(^{i,1}_{t}=_{t}\), \(^{i,2}_{t}=1-_{t}\), \(_{t} 0\) (i.e., monotonically non-decreasing in \(t\)) and in this case equation 22 reads as

\[u^{i}_{t}(x^{i},z)=_{t}}{1-_{t}}[p_{1|t}(x^{i}|z)- _{z}(x^{i})] \]

where we use the notation \(p_{1|t}(x^{i}|z)=_{x_{0},x_{1}}_{x_{1}}(x^{i})p_{t}(x_{0},x_{1}|z)\) for the _probability denoiser_.

Sampling backward in time.We can also sample _backwards in time_ by following the sampling rule \(X^{i}_{t-h}_{X^{i}_{l}}()-hu^{i}_{t}(,X_{t})\). In this case \(-u^{i}_{t}(x^{i},z)\) should satisfy equation 13. A (backward-time) generating probability velocity can then be achieved from equation 22 with the simple change to the coefficients \(a^{i,j}_{t}\) and \(b^{i,j}_{t}\), see Appendix E.4. For \(p_{t}\) defined with equation 9 the generating velocity is

\[u^{i}_{t}(x^{i},z)=_{t}}{_{t}}[_{z}(x^{i})-p _{0|t}(x^{i}|z)], \]

where in this case \(p_{0|t}(x^{i}|z)=_{x_{0},x_{1}}_{x_{0}}(x^{i})p_{t}(x_ {0},x_{1}|z)\) is the _probability noise-prediction_.

    &  &  \\  Marginal prob. & & \(p_{t}(x)=_{x_{0},x_{1}}_{i=1}^{N}p_{t}(x^{i}|x_{0},x_{1})(x_{0},x_{1})\) \\ Conditional prob. & & \(p_{t}(x^{i}|x_{0},x_{1})=_{_{t}x_{1}+(1-_{t})x_{0}}(x^{i})\) & \(p_{t}(x^{i}|x_{0},x_{1})=_{t}_{x_{1}}(x^{i})+(1-_{t})_ {x_{0}}(x^{i})\) \\  VF-_Denoiser_ & & \(u^{i}_{t}(X_{t})=_{t}}{1-_{t}}[^{i}_{ 1|t}(X_{t})-X^{i}_{1}]\) & \(u^{i}_{t}(x^{i},X_{t})=_{t}}{1-_{t}}[p_{1|t}(x^{i} |X_{t})-_{X_{t}}(x^{i})]\) \\ VF-_Noise-pred_ & & \(u^{i}_{t}(X_{t})=_{t}}{_{t}}[X^{i}_{t}-^{i}_ {0|t}(X_{t})]\) & \(u^{i}_{t}(x^{i},X_{t})=_{t}}{_{t}}[_{X_{t}}(x ^{i})-p_{0|t}(x^{i}|X_{t})]\) \\   

Table 1: Generating (marginal) velocity fields have identical form for the continuous and discrete Flow Matching when using denoiser/noise-prediction parameterization; \(_{1|t}(z)=_{X_{1} p_{t}(|z)}X_{1}\) is the standard continuous denoiser (a.k.a. \(x\)-prediction) and \(_{0|t}(z)=_{X_{0} p_{t}(|z)}X_{0}\) is the standard noise-prediction (a.k.a. \(\)-prediction).

Remarkably, the generating velocity fields in 24 and 25 take the _exact same form_ as the generating (a.k.a. marginal) velocity fields in _continuous_ flow matching when parameterized via the denoiser or noise-prediction parameterizations and using the same schedulers, see Table 1 and Appendix E.9 for explanation of the continuous case. In Appendix E.4 we provide the backward-time version of Theorem 3.

Corrector sampling.Combining the forward-time \(_{t}\) ( equation 24) and backward-time \(_{t}\) ( equation 25), i.e.,

\[_{t}^{i}(x^{i},z)=_{t}_{t}^{i}(x^{i},z)-_{t}_{t}^{i}(x^{i},z), \]

provides a valid forward-time probability velocity field (i.e., satisfies equation 13) for \(t(0,1)\) as long as \(_{t},_{t}>0\). This velocity field can be used for two types of corrector sampling: (i) When \(_{t}-_{t}=1\) sampling with \(_{t}\) leads to _corrector sampling_ where intuitively each step moves \(1+_{t}\) forward in time and \(-_{t}\) backwards, which allows reintroducing noise into the sampling process; and (ii) when \(_{t}-_{t}=0\) sampling with \(_{t}\) when fixing \(t(0,1)\) leads to _corrector iterations_ where limit samples distribute according to \(p_{t}\). In Appendix E.6 we prove:

**Theorem 4**.: _For perfectly trained posteriors and \(_{t},_{t}>0\), \(t(0,1)\), \(_{t}\) in equation 26 is a probability velocity, i.e., satisfies equation 13, and: (i) For \(_{t}-_{t}=1\), \(_{t}\) provides a probability velocity generating \(p_{t}\); (ii) For \(_{t}-_{t}=0\), repeatedly sampling with \(_{t}\) at fixed \(t(0,1)\) and sufficiently small \(h\) is guaranteed to converge to a sample from \(p_{t}\)._

One simplification to equation 26 can be done in the case of paths constructed with conditional as in equation 9, independent coupling \((x_{0},x_{1})=p(x_{0})q(x_{1})\), and i.i.d. source \(p(x_{0})=_{i=1}^{N}p(x_{0}^{i})\), e.g., \(p(x_{0}^{i})\) is uniform over \([d]\) or \(_{}(x_{0}^{i})\). In this case, the backward-time formula in equation 25 take an equivalent simpler form

\[_{t}^{i}(x^{i},z)=_{t}}{_{t}}[_{z} (x^{i})-p(x^{i})], \]

which does not require estimation of the posterior \(p_{0|t}\). See Appendix E.5 for the derivation.

Training.Equation 22 shows that for generating samples from a probabilty path \(p_{t}(x)\) we require the posteriors \(_{t}^{j}(x^{i}|X_{t})\). Training such posteriors can be done by minimizing the loss

\[()=-_{j[m],i[N]}_{t,(X_{0},X_{1}),X_{t},Y _{j}^{i}}_{t}^{j}(Y_{j}^{i}|X_{t};), \]

where \(t\) is sampled according to some distribution in \(\) (we used uniform), \((X_{0},X_{1})(X_{0},X_{1})\), \(X_{t} p_{t}(X_{t}|X_{0},X_{1})\), and \(Y_{j}^{i} w^{j}(Y_{j}^{i}|X_{0},X_{1})\); \(^{p}\) denotes the learnable parameters. In the common case we use in this paper of learning a single posterior, i.e., the probability denoiser \(p_{1|t}\), the loss takes the form \(()=-_{i[N]}_{t,(X_{0},X_{1}),X_{t}} p_{1| t}(X_{1}^{i}|X_{t})\). In Appendix E.7 we prove:

**Proposition 5**.: _The minimizer of \(\) (equation 28) is \(_{t}^{j}(x^{i}|X_{t})\) (equation 23)._

## 3 Related work

In the section we cover the most related work to ours; in Appendix A we cover other related work.

Discrete Flows (Campbell et al., 2024) is probably the most related work to ours. We build upon their CTMC framework and offer the following generalizations and simplifications: We consider arbitrary couplings \((X_{0},X_{1})\), and offer a novel and rather general family of probability paths (equation 8) for which we provide the generating probability velocities in a unified closed-form formula (equations 22-25). These in particular recreate the same formulas as the continuous Flow Matching counterpart (Table 1). We furthermore develop a general corrector velocity (equation 26) that unifies both corrector iterations (Song et al., 2020; Campbell et al., 2022) and stochastic sampling of Campbell et al. (2024). We show that particular choices of noise schedulers \(_{t}\) (\(_{t}=t\) reproduces Campbell et al. (2024)) and corrector schedulers provide a boost in results. Lastly, we opted for the term _probability velocity_ for \(u_{t}^{i}(x^{i},X_{t})\) as it is not precisely a rate matrix in the state space \(\) used in CTMC since \(u_{t}^{i}(x^{i},z)\) for all \(i[N]\) define multiple self-edges \(z z\).

Masked modeling (Ghazvininejad et al., 2019; Chang et al., 2022).In case of a masked model, i.e., when the source distribution is \(p(x)=_{}(x)\), we achieve an interesting connection with MaskGitshowing it is actually an instance of Discrete Flow Matching with a small yet crucial change to its sampling algorithm. First, in Appendix E.8 we prove that in the masked setting, the probability denoiser \(p_{1|t}\) is _time-independent_:

**Proposition 6**.: _For paths defined by equations 7 and 9 with source \(p(x)=_{m}(x)\) the posterior \(p_{t}(x_{0},x_{1}|z)=p(x_{0},x_{1}|z)\) is time-independent. Consequently, the probability denoiser \(p_{1|t}(x^{i}|z)=p_{1}(x^{i}|z)\) is also time-independent._

This shows that the probability denoiser can be learned with no time dependence, similar to the unmasking probabilities in MaskGit. During sampling however, there are two main differences between our sampling and MaskGit sampling. First, unmasking of tokens in our algorithm is done according to the probability \(_{X_{t}}(x^{i})+hu_{t}^{i}(x^{i},X_{t})\)_independently_ for each token \(x^{i}\), \(i[N]\). This procedure is justified as it samples from the correct probability asymptotically via the derivation of the Continuity Equation 20. This is in contrast to MaskGit that prioritizes the token to be unmasked according to some _confidence_. In the experiments section we show that MaskGit's prioritization, although has some benefit in the very low NFE regime, is actually introducing a strong bias in the sampling procedure and leads to inferior overall results. Secondly, using corrector sampling allows for reintroducing masks to already unmasked tokens in a way that is still guaranteed to produce samples from \(p_{t}\), see Theorem 4; we find this to have a significant positive effect on the generation quality.

**Discrete diffusion.** D3PM (Austin et al., 2021) and Argmax flows (Hoogeboom et al., 2021) introduced diffusion in discrete spaces by proposing a corruption process for categorical data. A later work by Campbell et al. (2022) introduced discrete diffusion models with continuous time, and Lou et al. (2023) proposed learning probability ratios, extending score matching (Song and Ermon, 2019) to discrete spaces.

## 4 Experiments

We evaluate our method on the tasks of language modeling, code generation, and image generation. For language modeling, we compare the proposed method against prior work considering the widely used generative perplexity metric. We scale the models to 1.7 billion parameters and present results on coding tasks, i.e., HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), demonstrating

   Method & NFE & Llama-2\(\) & Llama-3\(\) & GPT2\(\) & Entropy \\  Data & - & 7.0 & 9.4 & 14.7 & 7.7 \\  Autoregressive & 1024 & 31.4 & 54.8 & 45.3 & 7.1 \\ Savinov et al. (2021) & 200 & 29.5 & 45.1 & 34.7 & 5.2 \\ Austin et al. (2021) & 1000 & 697.6 & 768.8 & 837.8 & 7.6 \\ Han et al. (2022) & 10000 & 73.3 & 203.1 & 99.2 & 4.8 \\ Lou et al. (2023) & 256/512/1024 & 38.6/33.7/27.2 & 69.2/58.6/43.9 & 64.3/53.4/40.5 & 7.8/7.7/7.6 \\ Campbell et al. (2024) & 256/512/1024 & 38.5/33.5/28.7 & 69.0/56.5/46.5 & 65.2/53.3/43.0 & 7.8/7.7/7.6 \\
**FM (equation 9)** & 256/512/1024 & 34.2/30.0/22.5 & 58.5/48.8/33.8 & 54.2/43.5/29.3 & 7.7/7.6/7.2 \\
**FM (equation 10)** & 256/512/1024 & 30.0/27.5/22.3 & 48.2/43.5/31.9 & 47.7/41.8/28.1 & 7.6/7.5/7.1 \\   

Table 2: Generative perplexity on unconditional text generation compared to prior work. All models are sampled without the use of temperature or corrector steps. Double precision sampling results are reported in Table 5.

   Method & Model Size & NFE & Llama-2\(\) & Llama-3\(\) & Entropy \\  Llama-3 (Reference) & 8B & 512 & 6.4 & 7.3 & 6.8 \\ Llama-2 (Reference) & 7B & 512 & 5.3 & 8.3 & 7.1 \\  Autoregressive & 1.7B & 512 & 14.3 & 22.3 & 7.2 \\ Savinov et al. (2021) & 1.7B & 200 & 10.8 & 15.4 & 4.7 \\
**FM (U-coupling)** & 1.7B & 256/512 & 10.7/9.5 & 11.2/10.3 & 6.7/6.7 \\
**FM (C-coupling)** & 1.7B & 256/512 & 10.2/8.9 & 10.0/9.7 & 6.8/6.7 \\   

Table 3: Generative perplexity on conditional text generation.

the most promising results to date in a non-autoregressive context. In image generation, we present results for a fully discrete CIFAR10 (Krizhevsky et al., 2009). Further details of the experimental setup for each model are provided in Appendix G.

**Experimental setup.** In our experiments we used the masked source, i.e., \(p=_{m}\), and trained with both unconditional coupling (U-coupling, equation 4) and conditional couplings (C-coupling, equation 5) with the probability path defined in equations 7, 9 and in one case 10. We trained a probability denoiser (loss in equation 28) and sampled using the generating velocity in equation 24 and Algorithm 1. We used a particular choice of probability path scheduler \(_{t}\), as well as corrector steps defined by a scheduler \(_{t}\) and temperature annealing. We found the choice of these schedulers to be pivotal for the model's performance. In Appendix D we perform an ablation study, evaluating various scheduler choices.

### Language modeling

We experimented with our method in three settings: (i) Small model (150M parameters) - comparison to other non-autoregressive baselines in unconditional text generation; (ii) Large model (1.7B parameters) - comparison to autoregressive models in conditional text generation; and (iii) Large model (1.7B parameters) - conditional code generation. As computing exact likelihood for non-autoregressive model is a challenge, for (i),(ii) we use the generative perplexity metric (Appendix G measured with GPT2 (Radford et al., 2019), Llama-2 (Touvron et al., 2023), and Llama-3, and we also monitor the sentence entropy (Appendix G) to measure diversity of tokens and flag repetitive sequences, which typically yield low perplexity. Throughout our experiments we noticed entropy \( 6\) usually corresponds to diverse texts. For (iii) we evaluated using the success rate of coding tasks.

**Evaluation against prior work.** We evaluate our method against prior work on non-autoregressive modeling. For a fair comparison, all methods are trained on a 150M parameters models using the OpenWebText (Gokaslan and Cohen, 2019) dataset. We also fix all sampling hyperparameters to the most basic settings, i.e., no temperature, top probability, corrector steps, etc. For our method we tried two paths defined by equations 9 and 10. Results are reported in Table 2, where our method outperforms all baselines in generative perplexity for all numbers of function evaluations (NFE).

**Conditional text generation.** In this experiment, we train both C-coupling and U-coupling 1.7B parameters FM models with paths defined by equation 9 on a large scale data mix (Touvron et al., 2023). Table 3 presents the generative perplexity of conditional generations from our method; the conditions we used are the prefixes of the first 1000 samples in OpenWeb dataset. We also compare to existing state-of-the-art autoregressive models. Our results demonstrate that our model effectively narrows the gap in generative perplexity with autoregressive models, while maintaining an entropy comparable to the recent Llama-3 8B model. Furthermore, we note the C-coupling trained model produces slightly better perplexity in conditional tasks than the U-coupling model. In Appendix I we present qualitative conditional samples produced by our U-coupling model.

**Code generation.** Here we trained our basic setting of a 1.7B parameters FM model with U-coupling and path as in equation 9 on a code-focused data mix (Roziere et al., 2023). Table 4 presents results on HumanEval and MBPP (1-shot) for pass\(@\{1,10,25\}\). In Table 4, 'Oracle length' evaluates the performance of our model when conditioning on the length of the solution. This is done by inserting an 'end of text' token in the same position of the ground truth solution. Our method achieves non-trivial results on both tasks, which to the best of our knowledge is the first instance of a non-autoregressive method being capable of non-trivial coding tasks. In Appendix C, we analyze the

    &  &  &  \\   & & Pass@1 & Pass@10 & Pass@25 & Pass@1 & Pass@10 & Pass@25 \\  Autoregressive & Text & 1.2 & 3.1 & 4.8 & 0.2 & 1.7 & 3.3 \\  & Code & 14.3 & 21.3 & 27.8 & 17.0 & 34.3 & 44.1 \\
**FM** & Text & 1.2 & 2.6 & 4.0 & 0.4 & 1.1 & 3.6 \\  & Code & 6.7 & 13.4 & 18.0 & 6.7 & 20.6 & 26.5 \\
**FM (Oracle length)** & Code & 11.6 & 18.3 & 20.6 & 13.1 & 28.4 & 34.2 \\   

Table 4: Execution based code generation evaluation.

proposed method for code infilling, which can be achieved as our model allows non-autoregressive generation. Lastly, in Appendix H we show qualitative examples of success and failure cases produced by our model on the coding tasks, and in Appendix H.3 we show examples of code infilling.

### Image generation

We performed a fully discrete image generation, without using any metric or neighboring information between color values. We trained an FM model with U-coupling and path as in equation 9 on CIFAR10 to predict discrete color value for tokens, i.e., \(d=256\), with sequence length of \(N=32 32 3\). For generative quality we evaluate the Frechet Inception Distance (FID) (Heusel et al., 2017). Ablations for the probability path schedulers are provided in Figure 8 in the Appendix G. In Figure 2(a) we compare our method with: (i) MaskGIT (Chang et al., 2022); and (ii) (Campbell et al., 2024) which coincides with our method for a linear scheduler. More details in Appendix G. As can be seen in the Figure 2(a), our method outperforms both baselines, achieving \(3.63\) FID at \(1024\) NFE. In fig. 2(b) we observe a similar trend when evaluating Inception score. As discussed above, MaskGit sampling performs better for low NFE but quickly deteriorates for higher NFE. We attribute this to a bias introduced in the sampling process via the confidence mechanism.

## 5 Conclusions and future work

We introduce Discrete Flow Matching, a generalization of continuous flow matching and discrete flows that provides a large design space of discrete non-autoregressive generative models. Searching within this space we were able to train large scale language models that produce generated text with an improved generative perplexity compared to current non-autoregressive methods and able to solve coding tasks at rates not achievable before with non-autoregressive models, as far as we are aware. While reducing the number of network evaluations required to generate a discrete sample compared to autoregressive models, Discrete Flow Matching still does not achieve the level of sampling efficiency achieved by its continuous counterpart, flagging an interesting future work direction. Another interesting direction is to explore the space of probability paths in equation 8 (or a generalization of which) beyond what we have done in this paper. We believe discrete non-autoregressive models have the potential to close the gap and even surpass autoregressive models as well as unlock novel applications and use cases. As our work introduces an alternative modeling paradigm to discrete sequential data such as language and code, we feel it does not introduce significant societal risks beyond those that already exist with previous large language models.

Figure 3: FID and Inception scores vs. number of function evaluations (NFE).