# Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation

Yuyang Deng

Pennsylvania State University

yzd82@psu.edu

&Ilja Kuzborskij

Google DeepMind

iljak@google.com

&Mehrdad Mahdavi

Pennsylvania State University

mzm616@psu.edu

###### Abstract

We consider the problem of learning a model from multiple heterogeneous sources with the goal of performing well on a new target distribution. The goal of learner is to mix these data sources in a target-distribution aware way and simultaneously minimize the empirical risk on the mixed source. The literature has made some tangible advancements in establishing theory of learning on mixture domain. However, there are still two unsolved problems. Firstly, how to estimate the optimal mixture of sources, given a target domain; Secondly, when there are numerous target domains, how to solve empirical risk minimization (ERM) for each target using possibly unique mixture of data sources in a computationally efficient manner. In this paper we address both problems efficiently and with guarantees. We cast the first problem, mixture weight estimation, as a convex-nonconcave compositional minimax problem, and propose an efficient stochastic algorithm with provable stationarity guarantees. Next, for the second problem, we identify that for certain regimes, solving ERM for each target domain individually can be avoided, and instead parameters for a target optimal model can be viewed as a non-linear function on a space of the mixture coefficients. Building upon this, we show that in the offline setting, a GD-trained overparameterized neural network can provably learn such function to _predict_ the model of target domain instead of solving a designated ERM problem. Finally, we also consider an online setting and propose a label efficient online algorithm, which predicts parameters for new targets given an arbitrary sequence of mixing coefficients, while enjoying regret guarantees.

## 1 Introduction

With a rapidly increasing amount of decentralized data, multiple source domain adaptation has been an important learning scheme in modern machine learning, e.g., in learning with data collected from multiple sources (e.g. crowdsourcing) or learning in distributed systems where the data can be highly heterogeneous such as federated learning. In this learning scenario, given an input space \(\) and output space \(\), we assume access to \(N\) sources of data, each with its own underlying distributions \(_{j},j[N]\) over \(\). Then, given i.i.d. training samples \(}_{1},,}_{N}\), and a hypothesis space \(\), our goal is to learn a model on the combination of these sources, for instance through the Empirical Risk Minimization (ERM) procedure \(_{}=_{h}_{j=1}^{N} (j)_{}_{j}}(h)\), where \(_{}_{j}}(h)\) is the empirical loss of a model \(h\) over data samples in \(}_{j}\), and \(^{N}\) is some mixing parameter, such that predictor \(_{}\) entails a good generalization performance on a target domain characterized by a distribution \(\), i.e., yielding a small true risk \(_{}(_{})=( _{}(),y)\,( ,y)\). It is natural to measure the quality of \(_{}\) in terms of the excess risk -- namely, the difference between the risk of optimal model for target domain \(h^{*}_{}=_{h}_{}(h)\), and that achieved by \(_{}\). Clearly, the performance of \(_{}\) will be influenced by several factors, such as the choice of mixing coefficients\(\) to aggregate the empirical losses, capacity of \(\), and discrepancy between target and source data distributions. So, in order to design a good procedure for learning \(_{}\) we need to understand aforementioned trade-offs. Over the years the literature on the multiple source learning has dedicated a considerable attention to this problem . To this end, we consider the following bound on the excess risk of \(_{}\):

**Theorem 1** (Multi-source learning bound ).: _Given \(N\) source data distributions \(_{1},,_{N}\) and a target data distribution \(\), let \(_{}=_{h}_{j=1}^{N}(j) _{}_{j}}(h)\) be the ERM solution with fixed mixture weights \(^{N}\). Then for any \( 0\), with probability at least \(1-4e^{-}\) it holds that_

\[_{}(_{})_{ }(h_{}^{*})+(,)+_{ h}_{j=1}^{N}(j)|_{}_{j}}(h)-_{}_{j}}(h)|+C_{j=1}^{N}(j)}{m_{j}}}\]

_where \(C\) is some constant, the complexity term is \((,):=_{j=1}^{N}(j)_{j} ()\) with \(_{j}()\) being the Rademacher complexity of \(\) w.r.t. data source \(j\), and \(m_{j}\) is the number of samples from source \(j\)._

The above bound indicates that, the generalization ability of a model learnt by ERM on an \(\)-combined sources, depends on the \(\)-weighted sum of target-source discrepancies, and the number of samples drawn from each source. To entail a good generalization on target domain, it naturally motivates us to minimize right hand side of the bound over \(^{N}\) to get a _good_ mixture parameter. In this paper we can cast this idea as solving the following minimax optimization problem:

\[_{^{N}}_{h}_{j=1}^{N} (j)|_{}_{j}}(h)-_{}_{j}}(h)|+C^{N}(j)}{m_{j }}},\] (1)

where we drop the complexity term as it becomes identical for all sources by fixing the hypothesis space \(\) and bounding it with a computable distribution-independent quantity such as VC dimension , or it can be controlled by choice of \(\) or through data-dependent regularization.  gave a simple algorithm to minimize the bound of theorem 1 for binary classifiers and 0-1 loss, however their algorithm does not extend to a more general setting.  also looked at minimization of a similar bound with the goal to find weights for an optimal mixture, but they did not give a practical algorithm, nor a provable convergence guarantee. However, none of these works aimed to solve (1) because of its complex structure, and so an efficient algorithm for solving (1) so far has not been proposed. In particular, the first difficulty with (1) is that it is a convex-nonconcave objective, which means all minimax algorithms that require inner concavity  or PL-condition  will fail to converge to a stationary point. However, recently the literature on optimization of this type of objectives has recently made a tangible progress: The first provable convex-nonconcave algorithm was proposed by , where they consider alternating gradient descent ascent algorithm. Their algorithm is deterministic, but in practice, we favor a stochastic gradient method. The second difficulty in solving (1) is its compositional structure, which means that simply replacing gradient with stochastic gradient in  will not retain convergence guarantees. To tackle these two difficulties, we propose a _stochastic corrected gradient descent ascent_ algorithm, with provable convergence guarantee for solving (1). Our method can be viewed as a variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm, and moreover here we give a positive answer to the question posed by , on _whether an algorithm performing simultaneous updates can optimize convex-nonconcave problem?_, which could be interesting by its own right.

The discussion above concerns learning with one target domain, but in practice, a more common scenario is that we have multiple target domains to adapt to. For example, in federated learning , millions of users might wish to learn a good model from multiple sources, which can have good performance on their own data distribution. Hence, we propose to study _Multi-source Multi-target Domain Adaptation_ scenario (M\({}^{2}\)DA). Here we assume that we have \(M\) target domains, each of them characterized by its own distribution \(_{i},i[M]\) over \(\). Adapting to \(M\) different target domains requires different mixture weights \(_{1},,_{M}\), either obtained by solving (1), or supplied by the user. Equipped with mixing parameters, next we have to solve \(M\) weighted ERM problems to tailor solutions for each target domain \(_{i}\), that is

\[_{i}=_{h}_{j=1}^{N}_{i}(j) _{}_{j}}(h)\;, i[M].\]Notice that these \(M\) ERM objectives share the same component functions, so we name this problem as a _co-component empirical risk minimization_. A straightforward and naive approach is to solve all \(M\) weighted ERMs individually which becomes computationally inefficient when dealing with a large number of data sources. Nevertheless, given the benign structure of these \(M\) ERM problems, we may inquire whether there is a computationally efficient method for discovering all solutions without the necessity of solving each one individually.

We give an affirmative answer to this question by replacing the _learning_ of the target model by _predicting_ the target model and propose two efficient strategies to learn such predictors (for instance, a neural network). Our algorithm designs are based on the following observation: if we assume that each empirical risk \(_{}_{j}}(h)\) is strongly convex and smooth in parameters of a hypothesis \(h\), then the optimal parameters are given by _a Lipschitz function_ of mixture weights \(\). More formally, denoting \(^{*}()\) as optimal parameters of a hypothesis \(h\) for \(\)-weighted ERM, \(^{*}()\) is Lipschitz in \(\). This means that we can learn function \(^{*}()\) with, say, a neural network and gradient descent, with provably small generalization error. Moreover, analysis of generalization error allows us to understand when such target model prediction is more efficient than direct learning. In particular, we look at such a _phase transition of efficiency_, and conclude that when the number of targets \(M\) is much larger than number of sources \(N\), i.e, \(M((1/)^{N/2})\), learning to predict solution is more efficient than optimizing to solve all \(M\) ERMs. When \(M\) is relatively smaller, optimizing to solve ERMs is more efficient than learning to predict.

Finally, as a second learning scenario we consider an online learning setting, where mixture weights \(_{1},...,_{M}\) are arriving sequentially, and may not even originate from the same distribution. We cast this problem as an online non-parametric regression problem with inexact labels and propose an label-efficient online algorithm to predict models.

Our contributionsThe main contributions of this paper is summerized as follows:

* We study the multi-source domain adaptation problem where there are multiple source domains and we wish to learn a new model given the mixture of source domains, that can perform well on a given target domain. We build upon existing learning-theoretic results on multi-source domain adaptation and design a new algorithm for weighing of source domains, that casts this problem as a a convex-nonconcave minimax optimization problem. In section 2 we give the first stochastic optimization algorithm for this problem which provably converges to a stationary point. The proposed algorithm is the first provably convergent algorithm for a stochastic _compositional_ convex-nonconcave minimax problem.
* We further consider the above adaptation problem with multiple target domains, the Multi-source Multi-target Domain Adaptation (M\({}^{2}\)DA). We observe that these multiple adaptation might share a common structure, which allows us to avoid solving adaptation problem for each target domain individually, and instead we can replace it by direct prediction of parameters for a new problem. We consider offline and online settings for prediction of target parameters, and propose computationally efficient algorithms for both. For the offline setting, in section 3.1 we propose to use a two-layer neural network to learn optimal parameters \(^{*}()\) using bilevel gradient descent. We show that our algorithm can achieve \(O(n^{-})\) excess risk. We also identify the regime where our learning based approach is more efficient compared to directly solving each target problem individually.
* Finally, in section 3.2 we focus on scenario where target problems arrive sequentially (and could be dependent) and extend our study of direct target parameter prediction to the online setting. We propose a label-efficient algorithm which enjoys \(O(n^{-})\) average regret.

NotationWe introduce some basic definitions and notation that will be used throughout the paper. Let \(_{q}^{d}(r)\) be the ball in \(q\)-metric centered at the origin and of radius \(r>0\), and let \(^{d-1}(r)=\{^{d}\ :\ \|\|_{2}=r\} ^{d}\) be the \(_{2}\)-norm unit sphere centered at the origin, and finally let \(^{d-1}=^{d-1}(1)\). In addition, the probability simplex is defined as \(^{N}=\{^{N}:\|\|_{1}=1\}\). Concatenation of vectors is denoted by parentheses, that is \((_{1},,_{m})=[_{1}^{},,_{m}^{}]^{}\). A vector norm \(\|\|\) is understood as Euclidean norm, while \(\|\|_{}=_{i}|x_{i}|\). For a matrix \(\), \(\|\|_{}\) denotes its spectral norm while \(\|\|_{F}\) is its Frobenius norm. For some \(f:^{d-1}\) the _empirical semi-norm_ is defined as \(\|f\|_{n}^{2}=(f(_{1})^{2}++f(_{n})^{2})\) and is always taken w.r.t. the training sample \(S\). In addition, for \(g:^{d-1}\), we define an empirical inner product \( f,g_{n}=(f(_{1})g(_{1} )++f(_{n})g(_{n}))\). At the same time, \(\|f\|_{2}=\|f\|_{L^{2}(P_{X})}^{2}\).

## 2 Mixture Weights Estimation via Convex-nonconcave Minimax Optimization

In this section we focus on a single target domain and present an Algorithm 1 designed to solve a minimax problem (1) to estimate the mixture weights. We assume that hypothesis \(h\) is parameterized by a vector space \(\{^{d}\}\), and use \(f_{j}()=_{}_{j}}(h)\) to denote the empirical risk over data source \(j\). Similarly we define \(f_{}}()=_{}}(h)\). We do the following standard relaxations. First, for the sake of simplicity in computation, we relax the square root on the quadratic term w.r.t. \(\). Second, since the absolute value function is non-smooth, we shall use the smooth approximation function \(g\) to replace it, e.g., \(g(x)=+c}\) where \(c\) is some small number (here \(g()\) is smooth approximation of \(||\)). These relaxations lead to solving the following compositional convex-nonconcave minimax optimization problem:

\[_{^{N}}_{}F(,):=_{j=1}^{N}(j)g(f_{}}()-f_{j}())+C^{} \,\] (2)

where \(=\{},,}\}\). We are interested in developing a stochastic optimization algorithm to solve (2). It is a strongly-convex-nonconcave minimax problem, and it is one of most difficult type of minimax problem due to the absence of inner concavity.

``` Input: Target domain \(\), Source domains \(_{1},...,_{N}\), Initialization variable \(^{0}=^{-1}\), \(z_{1}^{0},...,z_{N}^{0}\), Positive hyper-parameters \((B,,,)\) (see theorem 2). for\(t=0,...,T-1\)do  Sample a minibatch \(_{}^{t}\) of size \(B\) from target domain \(\), and \(_{1}^{t},,_{N}^{t}\) from source domains \(_{1},,_{N}\) \(z_{j}^{t+1}=(1-^{t})(z_{j}^{t}+f_{}}( ^{t};_{}^{t})-f_{j}(^{t};_{j}^{t})-(f_{}}(^{t-1};_{}^{t})-f_{j}(^{t-1} ;_{j}^{t})))\) \(+^{t}(f_{}}(^{t};_{}^{t})-f_{ j}(^{t};_{j}^{t}))\).  Compute gradient for \(^{t}_{}=_{j=1}^{N}_{j}^{t} g(z_{j}^{t+1})(  f_{}}(^{t};_{}^{t})-  f_{j}(^{t};_{j}^{t}))\) \(^{t+1}=_{}(^{t}+^{t}_{})\)  Make vector \(^{N}\) whose \(j\)th coordinate is \(g(z_{j}^{t})\).  Compute gradient for \(\): \(^{t}_{}=+2C^{t}\). \(^{t+1}=_{^{N}}(^{t }-^{t}_{})\) ```

**Algorithm 1**Mixture Weight Estimation

To the best of our knowledge, only Xu _et al._ proposed a deterministic algorithm to solve it, but it is still unknown whether a stochastic algorithm can solve it with provable guarantee. We give an affirmative answer to this question, by proposing an algorithm built on celebrated stochastic gradient descent ascent . In addition to nonconvexity nature of 2, another difficulty that arises from the compositional structure of objective is that we cannot simply compute stochastic gradients, namely (with \([][]\)):

\[[g^{}(f_{}}(;_{})-f _{j}(;_{j}))( f_{}}(;_{ })- f_{j}(;_{j}))] g^{}(f_{}}()-f_{j}())( f_{}}( )- f_{j}()),\]

where \(_{1},_{2},\) are independent random elements in sample space \(=\) that capture stochasticity of the algorithm. To alleviate this issue, we borrow 'the stochastic corrected gradient' idea from , and maintain an auxiliary variable by introducing

\[z_{j}^{t+1}=(1-^{t})(z_{j}^{t}+f_{}}( ^{t};_{}^{t})-f_{j}(^{t};_{j}^{t})-(f_{ }}(^{t-1};_{}^{t})-f_{j}(^{t-1};_{j}^{t})))\] \[+^{t}(f_{}}(^{t};_{}^{t})-f_{j}(^{t};_{j}^{t})), denoting the projection operator onto convex set \(\) by \(_{}()\), the update rule becomes

\[^{t+1}=_{}(^{t}+ _{}^{t}),^{t+1}=_{^{ N}}(^{t}-_{}^{t})\.\]

### Convergence Analysis

In this section we are going to present the convergence guarantee for Algorithm 1. We make the following standard assumption on objective in (2).

**Assumption 1**.: _We make the following assumptions on \(g\) and \(f\):_

1. \(g(z)\) _is_ \(G_{g}\) _Lipschitz and_ \(L_{g}\) _smooth._ \(f_{j}(;)\) _is_ \(G_{f}\) _Lipschitz and_ \(L_{f}\) _smooth,_ \(,j[N],\,\)_._
2. \(\| f_{j}(;)- f_{j}()\| ^{2}^{2},\)_._
3. \(_{^{N},}F( {},) F_{max}\)_,_ \(_{}g(f_{}}()-f_{j}( )) B_{g}, j[N]\)_._

Points 1 and 2 of Assumption 1 are standard in the literature on compositional optimization . Point 3 guarantees boundedness of objective value, which can be ensured since we are working in the bounded parameter domain. Assumption 1 also implies the following property of \(F\).

**Proposition 1**.: _Under Assumption 1, \(F(,)\) is \(L:=\{4G_{f}^{2}L_{g}+2G_{g}L_{f},}\}\) smooth, and \(=}\) strongly convex in \(\)._

Next, we consider the following convergence measure:

**Definition 1** (Convergence Measure ).: _Given two parameters, \(\) and \(\), we define the following quantity as a stationary gap_

\[ G(,)=((-_{^{N}}(-_{}F(,) ))\\ (-_{}(+ _{}F(,))) )\.\]

Given the nonconcave nature of (2), we are only able to show the convergence to a stationary point. Definition 1 measures the stationarity given parameter pair \((,)\) by examining how much the parameter will change if we run one step projected gradient descent-ascent on them. Alternatively, one could consider the widely employed _primal function_ as a convergence measure, \(\|()\|\) with \(()=_{}F(,)\), but it is ill-suited to express stationarity since \(F(,)\) is non-concave.

One of our main results, proved in appendix A, establishes the convergence rate of Algorithm 1:

**Theorem 2**.: _Consider Assumption 1 and let \(L\) and \(\) be defined in Proposition 1. Then, letting \(B=(\{^{2}N^{2}}{^{2}},}{^{2}}\})\), \(=0.1\), \(=(})\), \(=(}{NG_{g}^{2}G_{f}^{2}L^{2}})\), the Algorithm 1 guarantees that_

\[_{t=1}^{T}\| G(^{t}, ^{t})\|^{2}^{2}\]

_with the gradient complexity bounded by:_

\[O(}{^{2}}\{}{^{2}},^{2}N^{2}}{^{2}},1\} ).\]

To the best of our knowledge, this is the first convergence proof for stochastic algorithm on solving strongly-convex-nonconcave problem. We achieve \(O(^{-4})\) gradient complexity required to reach an \(\) stationary point. In contrast to the most relevant result of , they show the rate \(O(^{-2})\) for a _deterministic_ Alternating Gradient Projection (AGP) in a strongly-convex-nonconcave setting. Note that our result also positively answers the question posed by , on whether some algorithm performing simultaneous instead of alternative updates can optimize strongly-convex-nonconcave minimax problem. Finally, compared to \(O(^{-4})\) rate of SGDA given a nonconvex-strongly-concave problem , we need roughly same stochastic gradient evaluations.

Multiple Target Domains: Learning to Solve Co-component ERM

Up till now, the main focus was on the problem of learning _good_ mixture parameters given a single target domain. Now we turn to a more general setting where we have \(M\) target domains, each associated with a different data distribution which necessitates per target mixture weights \(_{1},,_{M}\), either obtained by our algorithm, or provided by the user to guarantee good generalization on individual domains. Next, to get personalized models for these \(M\) domains, we have to solve \(M\) different ERM problems based on these mixture weights:

\[_{}f_{_{1}}():= _{j=1}^{N}_{1}(j)f_{j}(),,_{ }f_{_{M}}():=_{j=1}^{N}_{n }(j)f_{j}();\]

A naive way is to solve each of them, which will result in a computational complexity of \(M\) multiplied by the cost required to minimize each individual \(f_{_{i}}\) to a desired precision. We note that such a solution does not exploit the benign structure of these ERM problems: they share the same component functions \((f_{j})_{j}\), and the only difference is in the mixture weights. It naturally motivates us to ask, can we propose an efficient algorithm which avoids solving all these \(M\) co-component ERM problems from scratch? Consider the solution of \(_{}f_{}():=_{j=1}^{N}( j)f_{j}()\) as a function of \(\),

\[^{*}():=_{} _{j=1}^{N}(j)f_{j}().\] (3)

Fortunately, if we assume that each source empirical risk \(f_{j}\) is strongly convex and \(L_{f}\) smooth in model parameters, we have the following Lipschitz property (shown in appendix B.1):

**Lemma 1**.: _If each \(f_{j}\) is \(_{f}\) strongly convex and \(L_{f}\) smooth, then \(^{*}()\) is \(^{*}=G_{f}/_{f}\) Lipschitz._

Some basic algebra shows that in the above example \(^{*}()\) is indeed Lipschitz in \(\) with respect to \(^{2}\) metric. The Lipschitz property allows us to learn \(^{*}\) efficiently. In particular, learning arbitrary Lipschitz (and bounded) vector-valued function \(^{*}:^{N}^{d}\) is an instance of a well-studied _nonparametric regression_ problem . In the following we will consider algorithms for learning \(^{*}\) in both offline and online setting and which are provably capable of estimating \(^{*}\) at an almost optimal rate. In offline setting, we assume that we have access to a subset of \(M\) mixture weights, say \(_{1},...,_{n}\), and we shall use a two layer neural network \(_{}()\) to learn \(^{*}()\). Our algorithm is GD based empirical risk minimization with adaptive label refining. In a nutshell, given an \(\), since we do not have access to \(^{*}()\), we will use gradient descent to jointly solve weighted ERM with \(\) to get an approximation of \(^{*}()\) as well as optimizing neural network parameters. With a mild distributional assumption on \(\), we show that our algorithm guarantees that the two layer network learns \(^{*}()\), that is, it achieves a small excess risk \(_{}\|_{}()-^{*}()\|^{2}\).

In online setting, we assume that we observe an arbitrary sequence \(_{1},_{2},...\) on a simplex, and we wish to predict parameters close to \(^{*}(_{1}),^{*}(_{2}),...\). As baseline algorithm we will consider a well-known online nonparametric regression that greedily covers the simplex with local online learners and which enjoys almost-optimal regret . However, in the considered online protocol, the algorithm will need access to labels, and revealing each label requires to solve (3) to some desired accuracy. Here we explore a possibility that in practice we might be satisfied with \(\)-average regret, while saving the labelling cost. To this end we propose a modification of the algorithm that randomly skips some labels, while incurring a slightly larger regret.

### Offline Setting: Learning Lipschitz function with ReLU Neural Network

In this section we consider offline learning of \(^{*}\). The Lipschitzness guarantees that the \(^{*}()\) function can be efficiently learnt on finite \(\)s, and generalizable to unseen \(\). Hence, we propose to use a vector-valued two layer ReLU neural network \(_{}\) to learn \(^{*}()\).

We consider a two layer vector-valued neural network \(_{}:^{N}^{d}\), \(_{}()=[_{1}^{}(^{1} )_{+},...,_{d}^{}(^{d})_{+}]\), where parameters of the _hidden layer_ are matrices \(^{i}^{m N}\), collectively captured by the parameter vector \(=((^{1}),,( ^{d}))^{dmN}\). Here \(_{i}\{ 1/\}^{m}\) are parameters of the _output layer_. In the following the hidden layer is tuned by Algorithm 2, while parameters of the output layer are fixed throughout training. We assume that at initialization, for each \(^{i}\), the first half of its rows are drawn i.i.d. from isotropic standard Gaussian and the remaining half is identical to the first half. Similarly, for each \(_{i}\), half of the entries are set to \(-1/\) and the rest to \(1/\) (we assume that \(m\) is even). This initialization ensures that each output coordinate is \(0\) and so the empirical risk is bounded by a constant at initialization.

We assume that we observe mixture weights \(_{1},,_{n}^{N}\) i.i.d. according to some underlying distribution \(\). Such mixture weights can be obtained by Algorithm 1 and their independence means that samples originating from target domains are independent from each other.

We learn the neural network by solving the following **Bi-level** ERM:

\[_{}}():=_{i=1}^{n} \|_{}(_{i})-^{*}(_{ i})\|^{2},^{*}(_{i})= _{}_{j=1}^{N}_{i}(j)f_{j}().\] (4)

The parameters of a neural network \(\) should have the well-controlled excess risk on unseen \(\):

\[()=_{^{N}}\|_{}( )-^{*}()\|^{2}( )\.\]

To solve (4), we use a nested loop procedure which performs a GD step on a neural network objective \(}\), while in the inner loop we approximately find 'labels' \(^{*}(_{1}),...,^{*}(_{n})\) using a \(K\)-step GD. The entire procedure for solving (4) is described in Algorithm 2. Then, the following theorem shows that the two layer neural net optimized by Algorithm 2 learns \(^{*}()\):

``` Input: Number of global iteration \(T\), Number of Iteration for inner problem \(R\). for\(t=1,...,T\)do \(^{t+1}=^{t}-_{^{t}}_{ ^{t}}(_{i})(_{^{t}}(_{i })-_{i}^{t})\) \(\) Neural network parameter update for\(i=1,...,n\)do \(_{i}^{t+1}=(_{i}^{t},_{i},K)\)\(\) Label refining by \(K\)-step gradient descent ```

**Algorithm 2**Learning \(^{*}\) function by a neural network

**Theorem 3**.: _Let \(_{0}=N(N,n)\). Consider a neural network of with \(m(n^{8+})\). Then, for the Algorithm 2 with \(\), \(=}\), \(T(^{2}n}(n))\), and \(K((}))\), the following excess risk bound holds with probability at least \(0.99\):_

\[(_{T+1}) O((^{*})^{2}dn^{-} ).\]

The proof given in appendix B.5 is based on a more-or-less standard Neural Tangent Kernel (NTK) approximation argument , namely we use the key fact that predictions made by a GD-trained overparameterized neural network are close to those made by a Kernelized Least-Squares (KLS) predictor (given that the width of the network is sufficiently large). Now, such a KLS GD-trained predictor can learn Lipschitz target functions: It is well known that by learning on a sufficiently large Reproducing kernel Hilbert space (RKHS) (with polynomial spectral decay), one can approximate Lipschitz functions well . Here our goal is to approximate a vector-valued function, however, by treating each output independently we follow existing proofs  for scalar-valued Lipschitz regression by GD-trained neural networks and arrive at the same excess risk times \(d\).

**Optimality of our rate.** Here we show that a two-layer neural network trained by a bi-level Gradient Descent (GD) can learn a vector-valued function with \(O(dn^{-})\) excess risk. If we ignore the dependency on \(d\), our result matches the minimax rate of learning a scalar valued Lipschitz function .

**Algorithm 4**: Label efficient nonparametric online regression

**Efficiency of our learning-based approach.** Given \(M\) mixture weights \((_{i})_{i=1}^{M}\), the baseline naive approach is to solve all \(M\) weighted ERM with gradient descent, to accuracy level \(\), which requires \((M(1/))\) time complexity. Using our approach, we first need to learn a neural network with \(\) excess risk, and it needs \(n=((^{*2}d/)^{1+N/2})\) samples, which implies that we need to solve this many weighted ERM problems, resulting a complexity of \(((^{*2}d/)^{1+N/2}(1/))\). Once we learn a neural network, we just need to pay for the inference cost to predict \(^{*}\) for each \(_{i}\). Putting things together, the total time complexity is \(((^{*2}d/)^{1+N/2}(1/)+M)\). We observe the following regimes:

* When \(M(d/)^{1+N/2}(1/)}{ (1/)-1})\), learning is more efficient than solving \(M\) ERMs.
* Otherwise, directly solving \(M\) ERMs is more efficient than learning a model predictor.

Intuitively, when the number of target domains is much larger than number of source domains, our learning based approach is strictly more efficient. It is also interesting to note that our learning based approach can avoid computational overhead of \(M\), but suffers exponential cost from the number of sources \(N\). While solving ERMs avoids the price for \(N\), the computational cost increases linearly in terms of \(M\).

### Online Setting: Label Efficient Nonparametric Online Regression

In previous section we discussed nonparametric offline learning of \(^{*}\) with distributional assumption on \(_{1},...,_{M}\). In this section we consider the following online learning protocol with oblivious adversary. Given a known and fixed parameter \(p\), and an unknown sequence \((_{1},^{*}(_{1})),(_{2}, ^{*}(_{2})),^{N}_{2}^{d}(D)\) of inputs and labels, at every round \(t=1,2,\)

1. the environment reveals mixture weights \(_{t}^{N}\);
2. the learner selects a label \(}_{t}_{2}^{d}(D)\) and incurs loss \(_{t}}_{t}=}_{t}- ^{*}(_{t})^{2}\);
3. the learner samples \(Z_{t}(p)\) and observes \(\{Z_{t}=1\}_{t}\), when \(_{t}\) is a GD-optimized approximation of \(^{*}(_{t})\).

In particular, we introduce Algorithm 4, a modified version of the online nonparametric regression algorithm proposed by . Algorithm 4 iteratively constructs a packing of \(^{N}\) using \(^{2}\) balls centered on a subset of previously observed inputs. At each step \(t\), the label (parameters) associated with the current input \(_{t}\) is predicted by averaging the labels of past inputs within the ball whose center \(_{s}\) is closest to \(_{t}\) in the \(^{2}\) metric (note that labels are vector-valued). If \(_{t}\) lies outside the nearest ball, a new ball with center \(_{t}\) is created. The radii \(_{t}\) of all balls shrink at a rate \(t^{-1/(1+N)}\)Note that efficient (log in the number of centers) algorithms for approximate nearest-neighbor search are well-known , as well as highly-optimized open-source packages are readily available .

In contrast to the original algorithm by , where all sequentially observed labels are used to generate predictions (update local learners), our algorithm variant uses only a \(p\)-fraction of labels on average. This reduces label complexity at the cost of increased regret. This approach is referred to as _label-efficient prediction_ in online learning [6, Sec. 6.2] and is beneficial when accessing labels is costly. The following theorem, shown in appendix C, establishes the regret bound of Algorithm 4.

**Theorem 4** (Regret bound).: _Let \(f:^{N}^{d}\) be arbitrary \(^{*}\)-Lipschitz function with respect to \(\|\|\) metric and let \(C_{N}>0\) be a metric-dependent constant. Then, Algorithm 4 with \(_{t}=t^{-}\) satisfies_

\[[_{t=1}^{T}(_{t}(}_{t})-_{t}(f( _{t})))](pC_{N}(eT)+4D^{*})T^{ }+(1-p)TD+4pDT(1-^{-1})^{K}D\]

_Moreover, by definition \(_{t}(^{*}(_{t}))=0\), so choosing \(f()=^{*}\), the above bound implies that:_

\[_{t=1}^{T}[_{t}(}_{t})] 8(pC_{N} (eT)+4D^{*})T^{-}+(1-p)D+4pD(1-^{-1})^ {K}D.\]

The core idea behind the proof (deferred to appendix C) involves maintaining a balance between the regret contribution of each ball and an added regret term arising from approximating the target function using Voronoi partitioning. The regret contribution of each ball is logarithmic in the number of predictions made due to regret of online quadratic optimization [6, p. 42]. Ignoring log factors, the overall regret contribution equals the number of balls, which is essentially governed by the packing number with respect to the \(^{2}\) metric. The additional term in the regret comes from the algorithm's prediction being constant within the Voronoi cells of \(^{N}\) induced by the current centers (considering that we predict using the nearest neighbor). Thus, an extra term equal to the product of the balls' radius and the Lipschitz constant is incurred. Finally, the label efficient algorithm we present here incurs yet another, \(p\)-dependent terms, which accounts for the missed labels.

**Corollary 1**.: _If our desired average regret is \(>0\), then Algorithm 4 has label complexity:_

\[(\{(}{})^{1+N}, ()^{1+N}\})(1-).\]

Note that we recover the standard version of the algorithm (non-label efficient) by trivially setting \(p=1\), which in contrast has label complexity of order \((\{(D^{*}/)^{1+N},(1/ )^{1+N}\})\), which is strictly larger than the label efficient version as long as \(\) is not zero. When our desired regret goes to zero, the label complexity of two algorithms will tend to be the same asymptotically.

## 4 Experiments

To demonstrate the effectiveness of our proposed mixture weights estimation algorithm, we conducted an experiment using MNIST dataset  according to the following specifications. We consider a scenario with 15 source distributions, and dividing them into 3 groups. For group 1, it contains 5 source distributions and each distribution contains 100 data (80 for training and 20 for testing) samples which are drawn uniformly randomly from class '0', '1' and '2'. Group 2 and 3 share similar settings but their distributions' data are drawn from class '3', '4', '5', and class '6', '7', '9', '10' respectively. The data generation process is summarized in Table 1.

To demonstrate the effectiveness of our Algorithm 1, we implemented and run experiments with two-layer MLP neural network. We choose four different target setting: (1) target distribution

 
**Group** & **Classes** & **Domains per Group** & **Samples per Domain** \\ 
1 & 0, 1, 2 & 5 & 100 \\ 
2 & 3, 4, 5 & 5 & 100 \\ 
3 & 6, 7, 8, 9 & 5 & 100 \\  

Table 1: Classes and samples per domain for each group from Group 1, (2) target distribution from Group 2, (3) target distribution from Group 3, (4) target distribution from the mix of Group 1 and Group 2. We compare three algorithms: 1. Weighted ERM using our learnt weights, 2. ERM on averaged weights and 3. ERM solely on target domain, and presented our findings in Table 2. The results indicate that the accuracy achieved using the learnt alphas outperforms the other two approaches.

## 5 Discussion and Conclusions

In this paper we studied the multi-source multi-target domain adaptation problem. In the first part of the paper we gave an algorithm for adressing a minimax problem, that provably finds good mixture weights of source domains, given a single target domain. In the second part we studied the problem of domain adaptation with multiple target domains, and introduced the co-component ERM problem. We gave two concrete algorithms to solve co-component ERM problem, in offline and online settings. There are several potential future venues for future work, which we briefly discuss below.

Online mixture weight predictionThroughout Section 3, we assumed that the target domains' \(\)s are given. However, it would be interesting if given a new target domain, one could predict _good_ mixture weight in an online fashion, and our Algorithm 1 could serve as an oracle to give the inexact label. Meanwhile, since the Algorithm 1 takes considerable time to converge, a desired online algorithm should also be label-efficient.

The complexity of solving co-component ERMThe co-component ERM problem we introduced in section 3 is interesting from pure optimization perspective. Even though we proposed the learning-based approach to avoid heavy computation, one alternative direction is to develop efficient algorithms to directly solve \(M\) co-component ERM problems, and give upper and lower complexity bounds.

More structure in \(}\) and better phase transition lawsIn this work we considered a basic structure in co-component ERM problems (strong-convexity and smoothness), which gave rise to Lipschitzness of \(}\). Lipschitz class of functions is very large and in general can only be learned at a rate \((n^{-})\). As discussed in section 3.1 this allowed us to argue that the learning approach is more efficient than solving co-component ERM whenever \(M((1/)^{N/2})\). However, we could potentially obtain better rates (and better laws) of learning \(}\) having more structure in \(}\). For example, assuming that \(}\) is \(H\)-times differentiable, the excess risk would behave as \((n^{-})\). Thus, the learning approach would be more efficient when \(M((1/)^{N/(2H)})\), that is with potentially much fewer sources than targets. It is an intriguing question to figure our which co-component ERM problems would allows for a nicer structure in \(}\).

Alternative learning based approach for co-component ERMThere may be several other learning based method to solve co-component ERM. One potential approach is Meta learning . The idea is to train a meta model \(_{}\) by optimizing a pre-defined meta objective based on some sampled mixture weights, and the goal would be to find a model that can quickly adapt to tasks with different mixture weights \(\). We leave this as a promising open problem.

    & Target & Target & Target & Target \\  & (Group 1) & (Group 2) & (Group 3) & (mix of Group 1 and 2) \\  Average ERM & 69.9 \% & 40.0 \% & 34.9 \% & 59.9 \% \\  Pure target training & 69.9 \% & 55.0\% & 40.0 \% & 55.0\% \\  Our method & 80.0 \% & 69.9 \% & 55.0 \% & 65.0 \% \\   

Table 2: Accuracy comparison with two baseline algorithms. Each row represents the accuracy of model learnt by Average ERM, Pure target training or Our method, on different target domain. We can see that the models learnt using mixture weights from our algorithm (Algorithm 1) always yield the best accuracy.