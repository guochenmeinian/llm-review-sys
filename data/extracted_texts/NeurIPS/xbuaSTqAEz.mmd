# Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning

Jiawei Yao\({}^{1}\) Qi Qian\({}^{2}\) Juhua Hu\({}^{1}\)

\({}^{1}\) School of Engineering and Technology, University of Washington, Tacoma, WA 98402, USA

\({}^{2}\) Zoom Video Communications

{jyyao, juhuah}@uw.edu, qi.qian@zoom.us

Work done while at Alibaba Group.Corresponding author

###### Abstract

Multiple clustering aims to discover various latent structures of data from different aspects. Deep multiple clustering methods have achieved remarkable performance by exploiting complex patterns and relationships in data. However, existing works struggle to flexibly adapt to diverse user-specific needs in data grouping, which may require manual understanding of each clustering. To address these limitations, we introduce Multi-Sub, a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework in this work. Utilizing the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts expressing user preferences with their corresponding visual representations. This is achieved by automatically generating proxy words from large language models that act as subspace bases, thus allowing for the customized representation of data in terms specific to the user's interests. Our method consistently outperforms existing baselines across a broad set of datasets in visual multiple clustering tasks. Our code is available at https://github.com/Alexander-Yao/Multi-Sub.

## 1 Introduction

Clustering is a fundamental technique for analyzing data based on certain similarities, attracting extensive attention due to the abundance of unlabeled data. Traditional clustering methods  rely on general-purpose handcrafted features that may not suit specific tasks well. Deep clustering algorithms have improved clustering performance by employing Deep Neural Networks (DNNs)  to learn task-specific features. However, most of these algorithms assume a single partition of the data, while real data can be clustered differently according to different aspects, e.g., fruits in Fig. 1 can be grouped differently by color or by species.

Multiple clustering algorithms  address this challenge by producing multiple partitions of the data for various applications,

Figure 1: The workflow of Multi-Sub that obtains a desired clustering based on the subspace spanned by reference words obtained from GPT-4 using users’ high-level interest.

showing the capability of discovering multiple clusterings from a single dataset. For instance, in e-commerce, products can be clustered by category for inventory management or by customer preferences for personalized recommendations. Recently, there has been a growing interest in incorporating deep learning techniques into multiple clustering. These techniques mainly use auto-encoders and data augmentation methods to extract a wide range of feature representations, which enhance the quality of multiple clustering (Miklautz et al., 2020; Ren et al., 2022; Yao et al., 2023).

For real-world applications, a key challenge for end users is efficiently identifying the desired clustering from multiple results based on their interests or application purposes. We observe that users are willing to indicate their interest using succinct keywords (e.g., color or species for fruits in Fig. 1). However, it is difficult to use only a concise keyword to directly extract the corresponding image representations. Fortunately, the recent development of multi-modal models like CLIP (Radford et al., 2021) that align images with their text descriptions can help bridge this gap. Nevertheless, unlike methods that can use labeled data to fine-tune pre-trained models (Gao et al., 2023; Wang et al., 2023) to learn new task-specific representations, multiple clustering often faces scenarios with ambiguous or unspecified label categories and quantities. Therefore, given only a high-level concept from the user, it is intractable to fine-tune pre-trained models to capture a particular aspect of the data in an unsupervised manner. Very recently, Multi-MaP (Yao et al., 2024) leverages CLIP to learn textual and image embeddings simultaneously that follow the user's high-level textual concept. However, to achieve better performance, they require the user to provide a contrastive concept that is different from the desired concept, which may not be feasible in many real-world applications. Moreover, they obtain the new representations at first and then apply the traditional clustering method like k-means in a separate stage. This insufficient optimization lacking refinement between stages makes the clustering performance sub-optimal.

To mitigate these challenges, in this work, we first assume that the desired image and textual representations are residing in the same subspace according to the user's specific concept. Thereafter, to capture the desired subspace better, we can ask low-cost experts like Google or large language models (LLMs) (e.g., GPT-4) for common categories under the desired concept, as illustrated in Fig. 1. Although those returned common categories may not directly capture the clustering targets, they can be applied as the subspace basis to help search the appropriate representations inside. More importantly, during the learning under the desired subspace, we also incorporate the clustering loss to learn the representations and obtain the clustering simultaneously, which significantly enhances the model's clustering performance and efficiency. The main contributions of this work can be summarized as follows.

* We present a novel multiple clustering method, Multi-Sub, that can explicitly capture a user's clustering interest by aligning the textual interest with the visual features of images. Concretely, we propose to learn the desired clustering proxy in the subspace spanned by the common categories under a user's interest.
* Unlike most existing multiple clustering methods that require distinct stages for representation learning and clustering, Multi-Sub can obtain both the desired representations and clustering simultaneously, which can significantly improve the clustering performance and efficiency.
* Extensive experiments on all publicly available multiple clustering tasks empirically demonstrate the superiority of the proposed Multi-Sub, with a precise capturing of a user's interest.

## 2 Related Work

### Multiple Clustering

Multiple clustering, a methodology capable of unveiling alternative data perspectives, has garnered significant interest. Traditional approaches for multiple clustering (Hu and Pei, 2018) employ shallow models to identify diverse data groupings. Some methods, such as COALA (Bae and Bailey, 2006) and (Qi and Davidson, 2009), utilize constraints to generate alternative clusterings. Other techniques leverage distinct feature subspaces to produce multiple clusterings, as exemplified by (Hu et al., 2017) and MNMF (Yang and Zhang, 2017). Information theory has also been applied to generate multiple clusterings, as demonstrated by (Gondek and Hofmann, 2003) and (Dang and Bailey, 2010).

Recent advancements have seen the application of deep learning to discover multiple clusterings, yielding improved clustering performance. For instance,  proposed a deep matrix factorization method that utilizes multi-view data to identify multiple clusterings. ENRC  employs an auto-encoder to learn object features and optimizes a clustering objective function to find multiple clusterings. iMClusts  leverages auto-encoders and multi-head attention to learn features from various perspectives and discover multiple clusterings. AugDMC  uses data augmentation to generate diverse image aspects and learns representations to uncover multiple clusterings. DDMC  employs a variational Expectation-Maximization framework with disentangled representations to achieve superior clustering outcomes. However, almost all of these methods necessitate substantial user efforts to understand and select the appropriate clustering for different application purposes. Recently, Multi-MaP  leverages CLIP encoders to align a user's interest with visual data by learning representations close to the interested concept but far away from a contrastive concept, significantly improving the efficiency of capturing user-desired clusterings. However, Multi-MaP requires the user to input a contrastive concept for better performance, which is often not applicable. More importantly, it separates the representation learning and clustering as two distinct stages, which may result in sub-optimal performance. These issues will be mitigated in this work.

### Multi-Modal Models

Multi-modal learning involves acquiring representations from various input modalities like image, text, or speech. Here, we focus on how vision models benefit from natural language supervision. A key model in this area is CLIP , which aligns images with their corresponding text using contrastive learning on a dataset of 400 million text-image pairs.

Fine-tuning adapts vision-language models, such as CLIP, for specific image recognition tasks. This is seen in CoOp  and CLIP-Adapter , the latter using residual style feature blending to enhance performance. TeS  highlights the efficacy of fine-tuning in improving visual comprehension through natural language supervision. With limited labeled data, zero-shot learning has gained attention. Some approaches surpass CLIP by integrating other large pre-trained models. For example, VisDesc  uses GPT-3 to generate contextual descriptions for class names, outperforming basic CLIP prompts. UPL  and TPT  utilize unlabeled data to optimize text prompts. InMaP  and the online variant  aid class proxies in vision space with text proxies. Recent advancements have significantly improved vision-language pre-training using large-scale noisy datasets. ALIGN  employs over one billion image alt-text pairs without expensive filtering, showing that corpus scale can offset noise. Similarly, BLIP-2  uses a novel framework to bootstrap captions from noisy web data, enhancing both vision-language understanding and generation tasks. While these methods strive to enhance the performance of vision classification tasks, clustering presents a distinct scenario where class names are not available to extract useful information from multi-modal information as in this work.

## 3 The Proposed Method

Given a dataset of images \(\{x_{i}\}_{i=1}^{n}\) and user-defined preferences for data grouping (such as color and species), our goal is to generate clustering results that are specifically tailored to each preference. Thereafter, end users can directly use them for different application purposes without additional manual selection efforts. This process poses significant challenges, as it requires accurately aligning the complex, multi-dimensional data of images with the subjective and varied textual preferences of users. Traditional clustering methods often fail to capture these nuances, leading to a generic and less informative categorization for specific user applications.

Recently, the CLIP model  facilitated a more natural alignment between textual interests and visual representations. Our method, Multi-Sub, extends this alignment through a novel multi-modal subspace proxy learning approach. Fig. 2 outlines the overall framework of Multi-Sub, which is tailored to capture and respond to the diverse interests of users in clustering tasks. Multi-Sub employs a two-phase iterative approach to align and cluster images based on user-defined preferences such as color and species as described below.

### Background: Multi-Modal Pre-Training in CLIP

Let \(\{x_{i},t_{i}\}_{i=1}^{n}\) be a set of image-text pairs, where \(x_{i}\) denotes an image and \(t_{i}\) denotes its corresponding text description. We can obtain the vision and text representations of each pair by applying two encoders, \(f()\) and \(h()\), as \(_{i}=f(x_{i})\) and \(_{i}=h(t_{i})\). Both \(f()\) and \(h()\) are encoders that optimize the vision and text representations, respectively, such that \(_{i}\) and \(_{i}\) are unit vectors. The primary goal during this pre-training phase is to minimize the contrastive loss, formulated as

\[_{f,h}_{i}-_{i}^{}_{i}/)}{ _{j}(_{i}^{}_{j}/)}-_{i}^{}_{i}/)}{_{j}(_{i}^{} _{j}/)}\] (1)

where \(\) is a temperature parameter. The contrastive loss encourages the alignment of the image and its description while penalizing the similarity of the image with irrelevant texts [Qian et al., 2019]. The efficacy of this contrastive approach is vital for the subsequent phases of proxy word learning and fine-grained clustering, as it ensures that the foundational embeddings accurately reflect the inherent content and context of each modality.

### Subspace Proxy Word Representation

We build upon the pre-trained image and text encoders from CLIP and investigate whether we can leverage the image-text alignment to extract user-specific information. Specifically, given a fruit

Figure 2: **Multi-Sub framework. In Multi-Sub framework, Phase I (Proxy Learning and Alignment) processes each image \(x_{i}\) with user-defined textual prompts through a partially learnable image encoder (with a learnable projection layer) and a frozen text encoder. The latent factor \(_{i}\) calculates weights \(\{a_{i,k}\}_{k=1}^{K}\) based on the similarity to reference word embeddings \(\{_{i}\}_{k=1}^{K}\), which are then aggregated to form the proxy word embedding \(_{i}\). This proxy word embedding, combined with the image representation \(_{i}\), establishes the Aligned Feature Subspace for better alignment between the text and image under the user’s interest. In Phase II (Clustering), given the learned proxy word embeddings \(\{_{i}\}\) from Phase I to form pseudo-labels, the projection layer of the image encoder is further refined using the clustering loss. In Phase I, both the latent factor \(\) and the projection layer learn 100 epochs, after which the projection layer further learns 10 epochs using the clustering loss in Phase II. This alternative process repeats until convergence.**image [Hu et al., 2017] as illustrated in Fig. 2, different users may have different interests of its attributes, such as color, species, etc. However, the pre-trained image encoder in CLIP can only produce a single image embedding, which may not capture a user's interest exactly, not mentioning capturing different aspects. Furthermore, unlike classification tasks, clustering tasks do not come with concrete cluster names or numbers. Therefore, we cannot directly use the pre-trained text encoder of CLIP to generate the corresponding text embedding.

To address these challenges, we propose a subspace proxy word learning method to learn new embedding under the preferred aspect provided by the user. Thereafter, the main challenge is, given only a high-level concept like 'color' as in Fig. 2, how to effectively represent its subspace. Since the high-level concept itself cannot reflect different details under this concept in different images, it is difficult to do effective alignment between the high-level concept and images to figure out the corresponding vision subspace. Therefore, we propose to figure out the text subspace at first. Concretely, given pre-trained large language models like GPT-4 as low-cost experts, we can quickly gather common categories under a high-level concept using only one query like 'what are the common fruit colors' in Fig. 2. However, we cannot directly use the returned categories to do grouping, since they may not cover all existing categories in the data. Instead, we consider that most categories in the data under this concept are residing in the same subspace as the returned ones. Therefore, we can apply suggested categories as basis or reference words in the subspace. Then, each image's category under the desired concept can be represented by a linear combination of these reference words.

Assuming GPT-4 provides \(K\) reference words as \(\{z_{k}\}_{k=1}^{K}\), the proxy word of image \(x_{i}\) can be calculated as

\[_{i}=_{k=1}^{K}a_{i,k}(z_{k})\] (2)

where \((z_{k})\) is the token embedding of reference word \(z_{k}\) and \(\{a_{i,k}\}_{k=1}^{K}\) are weights corresponding to each reference word as a basis. A higher weight \(a_{i,k}\) indicates that the image \(x_{i}\)'s category is closer to the reference word \(z_{k}\). Here, we introduce trainable latent factor \(_{i}\) to learn the weight \(a_{i,k}\), and it can be calculated as

\[a_{i,k}=_{i}_{k})}}{_{j}_ {i}_{j})}}\] (3)

where \(_{k}=(z_{k})\). Thereafter, \(_{i}\) is representing the token embedding of image \(x_{i}\)'s proxy word under the preferred user concept. Once \(_{i}\) is well obtained, the image's proxy word representation under the preferred user concept is also obtained. Next, we discuss how to learn \(_{i}\) using CLIP.

### Multi-Modal Subspace Proxy Learning

As mentioned above, CLIP's text and image encoders were learned by aligning the text prompt with its corresponding image. The standard text prompt of CLIP is designed as "a photo of a fruit" for an image containing "fruit". Now, given a user's preference (e.g., color), we can rewrite the prompt as "a fruit with the color of *" denoted by \(t_{i}^{*}\) for image \(x_{i}\), where "*" is the placeholder for the unknown proxy word of image \(x_{i}\) under concept 'color' and its token embedding \(_{i}\) can be formulated as the linear superposition of reference words' token embeddings as discussed above.

Thereafter, the prompt text embedding after the text encoder can be formulated as

\[_{i}^{*}=h((t_{i}^{*})\|(w_{i}))\] (4)

To effectively learn \(_{i}\), the trainable latent factors, we utilize the alignment capabilities of CLIP by adjusting these factors so that the weighted sum of reference word embeddings closely aligns with the visual representation of the image. This process involves iteratively adjusting \(_{i}\) to maximize the cosine similarity between the image's representation \(_{i}\) and its corresponding proxy word embedding \(_{i}\). The optimization is conducted with the following loss function:

\[(_{i})=- f(x_{i}),h((t_{i}^{*})\|(w_{i}))\] (5)It should be noted that this optimization procedure can be conducted with both the text encoder and image encoder frozen, which is very efficient. However, the image embedding extracted directly from the pre-trained image encoder may not reflect its representation under the desired user interest. Therefore, during the optimization procedure, we do freeze the text encoder but open the image encoder. Nevertheless, to preserve the strong capacity of the pre-trained image encoder in CLIP, we open only the projection layer of the image encoder, while its remaining parameters are frozen as shown in the 'Phase I' of Fig. 2.

### Clustering Loss

To enhance the clustering performance of Multi-Sub, in 'Phase II', we leverage pseudo-labels assigned using the currently learned proxy word embeddings \(\{_{i}\}\) and image embeddings \(\{_{i}\}\) from 'Phase I'. Concretely, each image \(x_{i}\) can be represented by the concatenation of its currently learned proxy word embedding \(_{i}\) and image embedding \(_{i}\), denoted as \(_{i}=[_{i},_{i}]\). The pseudo-labels can be obtained by an offline k-means on \(\{_{i}\}\), which is however not efficient. Considering that proxy words for data points within the same cluster should show similar relationships to reference words, we obtain the pseudo-labels using the highest cosine similarity between the currently learned proxy word embeddings \(\{_{i}\}\) and the reference word embeddings \(\{_{k}\}\).

Given the pseudo-labels, the image embeddings can be further optimized by opening only the projection layer of the image encoder for improved compactness and separability in clusters. This loss consists of two primary components: intra-cluster loss and inter-cluster loss, aimed at refining cluster cohesion and separation, respectively. It should be noted that to better represent each image under the desired user concept, we define the clustering loss over \(_{i}\) containing both textual and visual information.

**Intra-cluster Loss:** The intra-cluster loss is designed to minimize the distances between embeddings within the same cluster, encouraging cluster compactness. It is calculated using the following formula:

\[_{}=}}_{i,j }\|_{i}-_{j}\|^{2}\] (6)

Here, \(\|_{i}-_{j}\|^{2}\) is the squared Euclidean distance between embeddings \(_{i}\) and \(_{j}\) of data points \(i\) and \(j\) within the same cluster, and \(N_{}\) denotes the number of intra-cluster pairs.

**Inter-cluster Loss:** This component aims to maximize the distances between embeddings from different clusters, thus enhancing separability. The inter-cluster loss is defined by a margin-based hinge loss as follows:

\[_{}=}}_{i,j} (0,m-\|_{i}-_{j}\|)\] (7)

where \((0,m-\|_{i}-_{j}\|)\) computes the hinge loss for each pair of embeddings from different clusters, ensuring a minimum margin \(m\) between them. \(N_{}\) is the count of inter-cluster pairs.

**Total Loss:** The overall clustering loss combines the intra- and inter-cluster losses, moderated by a balancing factor \(\):

\[_{}=_{}+(1-) _{}\] (8)

Optimizing this loss function in 'Phase II' helps regularize the embedding space where clusters are both internally dense and well-separated from each other. It should be noted that in this phase we aim to learn a better projection layer only for the image encoder, while all others are fixed as shown in 'Phase II' of Fig. 2.

Previous methods often use a two-stage strategy that separates representation learning and clustering to simplify the optimization process. This separation, however, can lead to sub-optimal clustering results, since the learned representations may not be fully aligned with the clustering objective without refinement. In this work, we obtain both the proxy word and the clustering alternatively and simultaneously. Concretely, we first learn the proxy word in a user-preferred subspace. Then, we fix the proxy word and refine the image encoder further to obtain better image representations using the clustering objective. These two phases are repeated alternatively until convergence, where 'Phase I' learns 100 epochs and 'Phase II' learns 10 epochs in each alternating according to the empirical experience as summarized in Fig. 2.

[MISSING_PAGE_EMPTY:7]

multiple salient embedding matrices and multiple clusterings therein; **AugDMC** Yao et al. (2023) leverages data augmentations to automatically extract features related to different aspects of the data using a self-supervised prototype-based representation learning method; **DDMC** Yao and Hu (2024) combines disentangled representation learning with a variational Expectation-Maximization (EM) framework; **Multi-MaP** Yao et al. (2024) relies on a contrastive user-defined concept to learn a proxy better tailored to a user's interest. It is worth noting that, in our experiments, we apply both traditional and deep learning baselines. Traditional methods rely on hand-crafted features, while deep learning methods directly utilize the original images as input.

HyperparameterFor each user's preference, we train the model for \(1000\) epochs using Adam optimizer with a momentum of \(0.9\). We tune all the hyper-parameters based on the loss score of Multi-Sub, where the learning rate is selected from {1e-1,5e-2,1e-2,5e-3,1e-3,5e-4}, weight decay is chosen from {5e-4,1e-4,5e-5,1e-5,0} for all the experiments. Most methods obtain each clustering by applying k-means Lloyd (1982) to the newly learned representations, while ours is end-to-end. The experiments are performed on four NVIDIA GeForce RTX 2080 Ti GPUs.

Evaluation metricsConsidering the randomness of k-means for those applicable baselines, we run k-means 10 times and report the average clustering performance using two metrics, namely, Normalized Mutual Information (NMI) White et al. (2004) and Rand index (RI) Rand (1971). These metrics range from \(0\) to \(1\) with higher value indicating better performance compared to the groundtruth.

### Performance Comparison

Table 2 reports the clustering results. During the clustering stage, after we obtain the proxy word embedding of each image for a desired concept, we can concatenate the image embedding and the token embedding of proxy word. The results show that Multi-Sub consistently outperforms the baselines, demonstrating the superiority of the proposed method. This also indicates a strong generalization ability of the pre-trained model by CLIP, which can capture the features of data from different perspectives.

Our methodology uses the CLIP encoder and GPT-4 to derive clustering results, prompting an evaluation of their performance in a zero-shot manner. We introduce two zero-shot variants of CLIP: CLIP\({}_{}\) and CLIP\({}_{}\). CLIP\({}_{}\) uses GPT-4 to generate candidate labels and performs zero-shot classification, while CLIP\({}_{}\) uses ground truth labels directly, providing an optimal setting. As shown in Table 3, CLIP\({}_{}\) generally outperforms CLIP\({}_{}\) due to its use of accurate labels, while CLIP\({}_{}\) introduces noise. Both variants perform equally on the Card dataset as GPT-4's labels match the groundtruth. Multi-Sub surpasses CLIP\({}_{}\) and even outperforms CLIP\({}_{}\) in all cases, demonstrating its ability to capture user-interest-based data aspects and confirming its efficacy. This superiority can be attributed to Multi-Sub's proxy word learning mechanism, which automatically adjusts textual

    &  & _{}\)**} & _{}\)**} &  \\  & & NMI\({}^{*}\) & R\(\) & NMI\({}^{*}\) & R\(\) & NMI\({}^{*}\) & R\(\) \\   & Color & 0.7912 & 0.9075 & 0.8629 & 0.9780 & **0.9693** & **0.9964** \\  & Species & 0.9793 & 0.9919 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\   & Color & 0.5613 & 0.7305 & 0.5746 & 0.7673 & **0.6654** & **0.8821** \\  & Species & 0.4370 & 0.7552 & 0.5364 & 0.7631 & **0.6123** & **0.8504** \\   & Order & 0.3518 & 0.8458 & 0.3518 & 0.8458 & **0.3921** & **0.8842** \\  & Suits & 0.2711 & 0.6123 & 0.2711 & 0.6123 & **0.3104** & **0.7941** \\   & Emotion & 0.1576 & 0.6532 & 0.1590 & 0.6619 & **0.2053** & **0.8527** \\  & Glass & 0.2905 & 0.6869 & 0.4686 & 0.7505 & **0.4870** & **0.3324** \\  & Identity & 0.1998 & 0.6388 & 0.2677 & 0.7545 & **0.7441** & **0.9834** \\  & Pose & 0.4088 & 0.6473 & 0.4691 & 0.6409 & **0.5923** & **0.8736** \\   & Color & 0.6539 & 0.8237 & 0.6830 & 0.8642 & **0.7533** & **0.9387** \\  & Type & 0.6207 & 0.7931 & 0.6429 & 0.8456 & **0.6616** & **0.8792** \\   & Color & 0.5653 & 0.7629 & 0.5828 & 0.7836 & **0.6940** & **0.8843** \\  & Species & 0.5620 & 0.7553 & 0.6019 & 0.7996 & **0.6724** & **0.8719** \\   & Type & 0.4935 & 0.6741 & 0.5087 & 0.7102 & **0.5271** & **0.7394** \\  & Environment & 0.4302 & 0.6507 & 0.4643 & 0.6801 & **0.4828** & **0.7096** \\   

Table 3: Variants of CLIP. The significantly best results with 95% confidence are in bold.

embeddings based on user-defined interests, creating more accurate proxy word embeddings. This approach reduces noise compared to CLIP\({}_{}\), which suffers from label mismatches. Additionally, Multi-Sub's iterative learning process refines these embeddings, optimizing alignment between text and image representations.

### Ablation study

Different ways of constructing subspaceThe subspace of the proposed method can be expanded by different embeddings, i.e., the token embedding of the proxy word \((w_{i})\), the text embedding of the proxy word \(h((w_{i}))\), and the text embedding of the prompt \(_{i}^{*}=h((t_{i}^{*})\|(w_{i}))\). These three kinds of embeddings can also be used to evaluate the clustering results in each case. In addition, we can use different combinations of learned embeddings (e.g., different concatenations of text and image embedding) as the final embedding for clustering. The results are shown in Table 5. It can be seen that using word token embedding usually achieves better results. This is expected since the word proxy directly reflects the image's category under the desired concept. The token word embedding subspace is also aligning well with CLIP's training method. In contrast, prompt embedding performs the worst as it introduces noise from user interest, dataset, and reference words, which are unnecessary for clustering. Additionally, most methods perform better when the same approach is used for constructing subspace and evaluating clustering results. Combining text and image embeddings generally enhances performance, capturing user interests from both aspects effectively.

Effect of text encoderTable 4 compares the performance of three text encoders--CLIP, ALIGN, and BLIP--across various datasets. The results indicate that ALIGN generally outperforms CLIP and BLIP in most tasks. This suggests that ALIGN's text encoder effectively captures and aligns textual and visual representations, enhancing clustering performance. ALIGN tends to excel in tasks that require distinguishing subtle visual differences influenced by textual descriptions, such as emotions and accessories in the CMUface dataset, and colors in the Fruit360 dataset. CLIP shows a strong tendency in identity-related tasks and complex object categorization, as evidenced by its performance in the CMUface identity task and Standford Cars type clustering. BLIP, while competitive, seems to perform better in categorical distinctions rather than abstract attributes, performing relatively well in species-related tasks across various datasets. These findings underscore the importance of effective text embeddings in multi-modal clustering frameworks.

We conducted an additional analysis using the Maximum Mean Discrepancy (MMD) metric to quantify the differences in the feature spaces generated by different text encoders (i.e., CLIP, ALIGN, and BLIP) in Table 6. The MMD results indicate that although our text prompts are simple, the

    &  &  &  &  \\  & & NMI\(\) & R\(\) & NM\(\) & R\(\) & NMI\(\) & R\(\) \\   & Color & 0.6654 & 0.8821 & **0.7031** & **0.8925** & 0.6522 & 0.8814 \\  & Species & 0.6123 & 0.8504 & **0.6426** & **0.8565** & 0.6254 & 0.8536 \\   & Order & 0.3921 & 0.8842 & **0.4316** & **0.9023** & 0.3845 & 0.8359 \\  & & Suits & 0.3104 & 0.7941 & **0.3226** & **0.8006** & 0.3151 & 0.7956 \\   & Emotion & 0.2053 & 0.8527 & **0.2148** & **0.8553** & 0.2081 & 0.8535 \\  & Glass & 0.4870 & 0.8324 & **0.4951** & 0.8351 & **0.4951** & **0.8353** \\  & Identity & 0.7441 & **0.9834** & **0.7514** & 0.9828 & 0.6833 & 0.8321 \\  & Pose & 0.5923 & 0.8736 & **0.6137** & **0.8942** & 0.5732 & 0.8427 \\   & Color & 0.7533 & **0.9387** & **0.7624** & 0.8942 & 0.5732 & 0.8427 \\  & Type & 0.6616 & 0.8792 & **0.6712** & **0.8865** & 0.6581 & 0.8731 \\   & Color & **0.694** & **0.8843** & 0.6925 & 0.8812 & 0.6843 & 0.8789 \\  & Species & **0.6724** & **0.8719** & 0.6693 & 0.8691 & 0.6627 & 0.8654 \\   & Type & 0.5271 & 0.7394 & **0.5342** & **0.7456** & 0.5221 & 0.7381 \\  & Environment & **0.4828** & **0.7096** & 0.4793 & 0.7064 & 0.4752 & 0.7038 \\   

Table 4: Comparison of differenttext encoders. The significantly best results with 95% confidence are in bold.

    &  &  & ))\)**} & ))\)**} & )\)**} & )\)**} \\  & & NMI\(\) & R\(\) & NM\(\) & R\(\) & NM\(\) & R\(\) & NM\(\) & R\(\) & NM\(\) & R\(\) & NMI\(\) & R\(\) \\   & Type & \(h((w_{i}))\) & 0.6496 & 0.6546 & 0.4789 & 0.6607 & 0.5298 & 0.7284 & 0.8660 & 0.6331 & 0.4867 & 0.6633 & 0.6632 & 0.6639 & 0.7096 \\  & & 0.8391 & 0.6579 & 0.4634 & 0.6499 & 0.5114 & 0.7193 & 0.4704 & 0.6586 & 0.5116 & 0.7140 & 0.6723 & 0.6524 & 0.5013 & 0.7136 \\  & & 0.7375 & 0.6589 & 0.4377 & 0.6583 & 0.5358 & 0.7211 & 0.4601 & 0.6240 & 0.5039 & 0.6699 & 0.4821 & 0.6638 & **0.5271** & **0.7374** \\   &  & \(h((w_{i}))\) & 0.42717 & 0.6640 & 0.4533 & 0.6133 & 0.4737 & 0.6695 & 0.4290 & 0.6537 & 0.4140 & 0.6662 & 0.4345 & 0.6691 & 0.4560 & 0.6615 \\  & & 0.4216 & 0.6677 & 0.4220 & 0.6533 & 0.6436 & 0.6660 & 0.4316 & 0.6690 & 0.4761 & 0.4734 & 0.5969 & 0.4161 & 0.6695 \\   & & 0.4340 & 0.6337 & 0.4570 & 0.6572 & 0.4486 & 0.6834 & 0.4218 & 0.6541 & 0.4432 & 0.6631 & 0.4586 & 0.6567 & **0.4285** & **0.7098** \\   

Table 5: Ablation study of Multi-Sub. The results that achieve the highest and second highest performance for each clustering are indicated by boldface and underlined numerics, respectively.

feature spaces generated by different text encoders exhibit significant distributional differences. The effectiveness of a text encoder can vary depending on the specific clustering task. For example, ALIGN tends to excel in tasks with more abstract attributes, such as colors and emotions, while CLIP shows strong performance in identity-related tasks. This variability underscores the importance of selecting an appropriate text encoder based on the specific application requirements. The difference between text encoders may come from the different corresponding pre-training tasks and this will be an interesting future direction.

VisualizationTo further demonstrate the effectiveness of Multi-Sub, we visualize the representations from CLIP\({}_{}\), CLIP\({}_{}\), and Multi-Sub for color and species clustering tasks (Figure 3). In species clustering, CLIP\({}_{}\) shows clear boundaries using ground truth labels, while CLIP\({}_{}\) introduces noise from reference words. Multi-Sub outperforms both by effectively capturing image features and user interests with proxy word embeddings. In color clustering, both CLIP\({}_{}\) and CLIP\({}_{}\) focus on species features, resulting in less distinct clusters. Multi-Sub excels by clearly distinguishing colors, leveraging user-specific interests for improved alignment. Overall, Multi-Sub consistently aligns embeddings with user interests, surpassing CLIP\({}_{}\) and CLIP\({}_{}\), demonstrating its robust multi-modal subspace proxy learning.

## 5 Conclusion and Limitations

In conclusion, our study mitigates an important challenge in multiple clustering: effectively identifying desired clustering results based on user interests or application purposes. We introduce Multi-Sub, a novel approach that integrates user-defined preferences into a customized multi-modal subspace proxy learning framework. By leveraging the synergy between CLIP and GPT-4, Multi-Sub automatically aligns textual prompts expressing user interests with corresponding visual representations. First, we observe reference words for user's interests from large language models. Given the absence of concrete class names in clustering tasks, our method uses these reference words to learn both text and vision embeddings tailored to user preferences. Extensive experiments across various visual multiple clustering tasks demonstrate that Multi-Sub consistently outperforms state-of-the-art techniques.

However, our approach has certain limitations. The reliance on large language models like GPT-4 can introduce biases inherent in these models, potentially affecting the clustering outcomes. Additionally, the field of multiple clustering lacks large, diverse datasets, which limits comprehensive evaluation. Although we have annotated CIFAR-10, more extensive datasets are needed.

  
**Dataset** & **Clustering** & **CLIP vs. ALIGN** & **CLIP vs. BLIP** & **ALIGN vs. BLIP** \\   & Color & 0.234 & 0.198 & 0.211 \\  & Species & 0.189 & 0.172 & 0.183 \\   & Order & 0.215 & 0.202 & 0.219 \\  & Suits & 0.198 & 0.184 & 0.192 \\   & Emotion & 0.276 & 0.245 & 0.263 \\  & Glass & 0.231 & 0.217 & 0.225 \\  & Identity & 0.263 & 0.249 & 0.258 \\  & Pose & 0.245 & 0.228 & 0.239 \\   & Color & 0.238 & 0.223 & 0.231 \\  & Type & 0.212 & 0.198 & 0.205 \\   & Color & 0.257 & 0.244 & 0.252 \\  & Species & 0.248 & 0.231 & 0.242 \\   & Type & 0.193 & 0.178 & 0.186 \\  & Environment & 0.178 & 0.162 & 0.174 \\   

Table 6: MMD between different text encoders across datasets.

Figure 3: Visualization of feature embeddings and related labels on Fruit dataset. For the visualization of color, red, green, and yellow points indicate the color of red, green, and yellow, respectively. For the visualization of species, red, yellow, and purple points indicate the species of apple, banana, and grapes, respectively.

Acknowledgment

Yao's research was funded in part by J.P. Morgan Chase & Co and Advata Gift funding. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by J.P. Morgan Chase & Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.