# Any-to-Any Generation via Composable Diffusion

Zineng Tang\({}^{1}\)1  Ziyi Yang\({}^{2}\)2  Chenguang Zhu\({}^{2}\)3  Michael Zeng\({}^{2}\)  Mohit Bansal\({}^{1}\)4

\({}^{1}\)University of North Carolina at Chapel Hill

\({}^{2}\)Microsoft Azure Cognitive Services Research

https://codi-gen.github.io

###### Abstract

We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io/

Figure 1: CoDi can generate various (joint) combinations of output modalities from diverse (joint) sets of inputs: video, image, audio, and text (example combinations depicted by the colored arrows).

Introduction

Recent years have seen the rise of powerful cross-modal models that can generate one modality from another, e.g. text-to-text [6; 37], text-to-image [13; 19; 22; 41; 44], or text-to-audio [23; 33]. However, these models are restricted in their real-world applicability where multiple modalities coexist and interact. While one can chain together modality-specific generative models in a multi-step generation setting, the generation power of each step remains inherently limited, and a serial, multi-step process can be cumbersome and slow. Moreover, independently generated unimodal streams will not be consistent and aligned when stitched together in a post-processing way (e.g., synchronized video and audio). The development of a comprehensive and versatile model that can generate any combination of modalities from any set of input conditions has been eagerly anticipated, as it would more accurately capture the multimodal nature of the world and human comprehension, seamlessly consolidate information from a wide range of sources, and enable strong immersion in human-AI interactions (for example, by generating coherent video, audio, and text description at the same time).

In pursuit of this goal, we propose Composable Diffusion, or CoDi, the first model capable of simultaneously processing and generating arbitrary combinations of modalities as shown in Fig. 1. Training a model to take any mixture of input modalities and flexibly generate any mixture of outputs presents significant computational and data requirements, as the number of combinations for the input and output modalities scales exponentially. Also aligned training data for many groups of modalities is scarce or even non-existent, making it infeasible to train with all possible input-output combinations. To address this challenge, we propose to align multiple modalities in both the input conditioning (Section 3.2) and generation diffusion step (Section 3.4). Furthermore, a proposed 'Bridging Alignment" strategy for contrastive learning (Section 3.2) allows us to efficiently model the exponential number of input-output combinations with a linear number of training objectives.

Building a model with any-to-any generation capacity with exceptional generation quality requires comprehensive model design and training on diverse data resources. Therefore, we build CoDi in an integrative way. First, we train a latent diffusion model (LDM) for each modality, e.g., text, image, video, and audio. These models can be trained in parallel independently, ensuring exceptional single-modality generation quality using widely available modality-specific training data (i.e., data with one or more modalities as input and one modality as output). For conditional cross-modality generation, such as generating images using audio+language prompts, the input modalities are projected into a shared feature space (Section 3.2), and the output LDM attends to the combination of input features. This multimodal conditioning mechanism prepares the diffusion model to condition on any modality or combination of modalities without directly training for such settings.

The second stage of training enables the model to handle many-to-many generation strategies that involve simultaneously generating arbitrary combinations of output modalities. To the best of our knowledge, CoDi is the first AI model with this capability. This is achieved by adding a cross-attention module to each diffuser, and an environment encoder \(V\) to project the latent variable of different LDMs into a shared latent space (Section 3.4). Next, we freeze the parameters of the LDM, training only the cross-attention parameters and \(V\). Since the environment encoder of different modalities are aligned, an LDM can cross-attend with any group of co-generated modalities by interpolating the representation's output by \(V\). This enables CoDi to seamlessly generate any group of modalities, without training on all possible generation combinations. This reduces the number of training objectives from exponential to linear.

We demonstrate the any-to-any generation capability of CoDi, including single-to-single modality generation, multi-condition generation, and the novel capacity of joint generation of multiple modalities. For example, generating synchronized video and audio given the text input prompt; or generating video given a prompt image and audio. We also provide a quantitative evaluation of CoDi using eight multimodal datasets. As the latest work from Project i-Code  towards Composable AI, CoDi exhibits exceptional generation quality across assorted scenarios, with synthesis quality on par or even better than single to single modality SOTA, e.g., audio generation and audio captioning.

## 2 Related Works

**Diffusion models (DMs)** learn the data distribution by denoising and recovering the original data. Deep Diffusion Process (DDP)  adopts a sequence of reversible diffusion steps to model image probability distribution. It uses a reversible encoder to map the input image to a latent space and a decoder to map the latent variables to an output image. Denoising diffusion probabilistic model (DDPM)  uses a cascade of diffusion processes to gradually increase the complexity of the probability density function model. At each step, the model adds noise to the input image and estimates the corresponding noise level using an autoregressive model. This allows the model to capture the dependencies between adjacent pixels and generate high-quality images. Score-based generative models (SOG)  use the score function to model the diffusion process.  generates high-fidelity images conditioned on CLIP representations of text prompts. Latent diffusion model (LDM)  uses a VAE to encode inputs into latent space to reduce modeling dimension and improves efficiency. The motivation is that image compression can be separated into semantic space by a diffusion model and perceptual space by an autoencoder. By incorporating temporal modeling modules and cascading model architectures, video diffusion models have been built upon image diffusers to generate temporally consistent and inherent frames. Diffusion models have also been applied to other domains, such as generating audio from text and vision prompts.

**Multimodal modeling** has experienced rapid advancement recently, with researchers striving to build uniform representations of multiple modalities using a single model to achieve more comprehensive cross-modal understanding. Vision transformers , featuring diverse model architectures and training techniques, have been applied to various downstream tasks such as vision Q&A and image captioning. Multimodal encoders have also proven successful in vision-language , video-audio  and video-speech-language  domains. Aligning data from different modalities is an active research area , with promising applications in cross-modality retrieval and building uniform multimodal representations .

## 3 Methodology

### Preliminary: Latent Diffusion Model

Diffusion models (DM) represent a class of generative models that learn data distributions \(p()\) by simulating the diffusion of information over time. During training, random noise is iteratively added

Figure 2: CoDi model architecture: (a) We first train individual diffusion model with aligned prompt encoder by “Bridging Alignment”; (b) Diffusion models learn to attend with each other via “Latent Alignment”; (c) CoDi achieves any-to-any generation with a linear number of training objectives.

to \(\), while the model learns to denoise the examples. For inference, the model denoises data points sampled from simple distributions such as Gaussian. Latent diffusion models (LDM)  learn the distribution of the latent variable \(\) corresponding to \(\), significantly reducing computational cost by decreasing the data dimension.

In LDM, an autoencoder is first trained to reconstruct \(\), i.e., \(}=D(E())\), where \(E\) and \(D\) denote the encoder and decoder, respectively. The latent variable \(=E()\) is iteratively diffused over time steps \(t\) based on a variance schedule \(_{1},,_{T}\), i.e., \(q(_{t}|_{t-1})=(_{t};}_{t -1},_{t})\)[20; 45].

The forward process allows the random sampling of \(_{t}\) at any timestep in a closed form [20; 45]: \(_{t}=_{t}+_{t}\), where \((0,I)\), \(_{t}:=1-_{t}\) and \(_{t}:=1-_{s=1}^{t}_{s}\). The diffuser learns how to denoise from \(\{_{t}\}\) to recover \(\). Following the reparameterization method proposed in , the denoising training objective can be expressed as :

\[_{D}=_{,,t}\|-_{}(_{t},t,C())\|_{2}^{2}.\] (1)

In data generation, the denoising process can be realized through reparameterized Gaussian sampling:

\[p(_{t-1}|_{t})=(_{t-1};}}(_{t}-}{}}_{}),_{t}).\] (2)

In \(_{D}\), the diffusion time step \(t[1,T]\); \(_{}\) is a denoising model with UNet backbone parameterized by \(\); \(\) represents the conditional variable that can be used to control generation; \(C\) is the prompt encoder. The conditioning mechanism is implemented by first featurizing \(\) into \(C()\), then the UNet \(_{}\) conditions on \(C()\) via cross-attention, as described in . Distinct from previous works, our model can condition on any combinations of modalities of text, image, video and audio. Details are presented in the following section.

### Composable Multimodal Conditioning

To enable our model to condition on any combination of input/prompt modalities, we align the prompt encoder of text, image, video and audio (denoted by \(C_{t}\), \(C_{i}\), \(C_{v}\), and \(C_{a}\), respectively) to project the input from any modality into the same space. Multimodal conditioning can then be conveniently achieved by interpolating the representations of each modality \(m\): \(C(x_{t},x_{i},x_{v},x_{a})=_{m}_{m}C(m)\) for \(m x_{t},x_{i},x_{v},x_{a}\), with \(_{m}_{m}=1\). Through simple weighted interpolation of aligned embeddings, we enable models trained with single-conditioning (i.e., with only one input) to perform zero-shot multi-conditioning (i.e., with multiple inputs). This process is illustrated in Fig. 2 (a)(2).

Optimizing all four prompt encoders simultaneously in a combinatorial manner is computationally heavy, with \((n^{2})\) pairs. Additionally, for certain dual modalities, well-aligned paired datasets are limited or unavailable e.g., image-audio pairs. To address this challenge, we propose a simple and effective technique called "Bridging Alignment" to efficiently align conditional encoders. As shown in Fig. 2 (a)(1), we choose the text modality as the "bridging" modality due to its ubiquitous presence in paired data, such as text-image, text-video, and text-audio pairs. We begin with a pretrained text-image paired encoder, i.e., CLIP . We then train audio and video prompt encoders on audio-text and video-text paired datasets using contrastive learning, with text and image encoder weights frozen.

In this way, all four modalities are aligned in the feature space. As shown in Section 5.2, CoDi can effectively leverage and combine the complementary information present in any combination of modalities to generate more accurate and comprehensive outputs. The high generation quality remains unaffected with respect to the number of prompt modalities. As we will discuss in subsequent sections, we continue to apply Bridging Alignment to align the latent space of LDMs with different modalities to achieve joint multimodal generation.

### Composable Diffusion

Training an end-to-end anything-to-anything model requires extensive learning on various data resources. The model also needs to maintain generation quality for all synthesis flows. To address these challenges, CoDi is designed to be composable and integrative, allowing individual modality-specific models to be built independently and then smoothly integrated later. Specifically, we start by independently training image, video, audio, and text LDMs. These diffusion models then efficiently learn to attend across modalities for joint multimodal generation (Section 3.4) by a novel mechanism named "latent alignment".

Image Diffusion Model.The image LDM follows the same structure as Stable Diffusion 1.5  and is initialized with the same weights. Reusing the weights transfers the knowledge and exceptional generation fidelity of Stable Diffusion trained on large-scale high-quality image datasets to CoDi.

Video Diffusion Model.To model the temporal properties of videos and simultaneously maintain vision generation quality, we construct the video diffuser by extending the image diffuser with temporal modules. Specifically, we insert pseudo-temporal attention before the residual block . However, we argue that pseudo-temporal attention only enables video frames to globally attend to each other by flattening the pixels (height, width dimension) to batch dimension, resulting in a lack of cross-frame interaction between local pixels. We argue that this results in the common temporal-inconsistency issue in video generation that locations, shapes, colors, etc. of objects can be inconsistent across generated frames. To address this problem, we propose adapting the latent shift method  that performs temporal-spatial shifts on latent features in accordance with temporal attention. We divide the video by the hidden dimension into \(k=8\) chunks, and for each chunk \(i=0\) to \(7\), we shift the temporal dimension forward by \(i\) positions. Further details will be provided in the appendix.

Audio Diffusion Model.To enable flexible cross-modality attention in joint generation, the audio diffuser is designed to have a similar architecture to vision diffusers, where the mel-spectrogram can be naturally viewed as an image with 1 channel. We use a VAE encoder to encode the mel-spectrogram of audio to a compressed latent space. In audio synthesis, a VAE decoder maps the latent variable to the mel-spectrogram, and a vocoder generates the audio sample from the mel-spectrogram. We employ the audio VAE from  and the vocoder from .

Text Diffusion Model.The VAE of the text LDM is OPTIMUS , and its encoder and decoder are  and GPT-2 , respectively. For the denoising UNet, unlike the one in image diffusion, the 2D convolution in residual blocks is replaced with 1D convolution .

### Joint Multimodal Generation by Latent Alignment

The final step is to enable cross-attention between diffusion flows in joint generation, i.e., generating two or more modalities simultaneously. This is achieved by adding cross-modal attention sublayers to the UNet \(_{}\) (Fig. 2 (b)(2)). Specifically, consider a diffusion model of modality \(A\) that cross-attends with another modality \(B\). Let the latent variables of modalities \(m_{A}\) and \(m_{B}\) at diffusion step \(t\) be denoted as \(_{t}^{A}\) and \(_{t}^{B}\), respectively. The proposed "Latent Alignment" technique is such that a modality-specific environment encoder \(V_{B}\) first projects \(_{t}^{B}\) into a shared latent space for different modalities. Then, in each layer of the UNet for modality \(A\), a cross-attention sublayer attends to \(V_{B}(_{t}^{B})\). For the diffusion model of modality \(A\), the training objective in Eq. (1) now becomes:

\[_{Cross}^{A}=_{,,t}\|- _{_{c}}(_{t}^{A},V_{B}(_{t}^{B}),t,C())\| _{2}^{2},\] (3)

where \(_{c}\) denotes the weights of cross-attention modules in the UNet.

The training objective of \(A+B\) joint generation is \(_{Cross}^{A}=_{Cross}^{B}\). \(V()\) of different modalities are trained to be aligned with contrastive learning. Since \(z_{t}^{A}\) and \(z_{t}^{B}\) at any time step can be sampled with closed form in the diffusion process Section 3.1, one can conveniently train the contrastive learning together with \(_{Cross}\). The purpose of \(V\) is to achieve the generation of any combination of modalities (in polynomial) by training on a linear number of joint-generation tasks. For example, if we have trained the joint generation of modalities \(A\), \(B\), and \(B\), \(C\) independently, then we have \(V_{A}(_{t}^{A})\), \(V_{B}(_{t}^{B})\), and \(V_{C}(_{t}^{C})\) aligned. Therefore, CoDi can seamlessly achieve joint generation of modalities \(A\) and \(C\) without any additional training. Moreover, such design automatically effortlessly enables joint generation of modalities \(A\), \(B\), and \(C\) concurrently. Specifically, UNet of \(A\) can cross-attend with the interpolation of \(V_{B}(_{t}^{B})\), and \(V_{C}(_{t}^{C})\), although CoDi has not been trained with such task.

As shown in Fig. 2(b)(3), we follow similar designs to the "Bridging Alignment" in training joint generation: (1) We first train the cross-attention weights in the image and text diffusers, as well 

[MISSING_PAGE_FAIL:6]

generation of image and text. For the joint generation task, we propose to train with **text\(\)image+text**, where the prompt text is the truncated image caption, and the output text is the original caption. Since the condition information is incomplete, the text and image diffuser will need to learn to attend with each other through the joint generation process.

**Audio + Text.** We curated a new dataset, Freesound 500K, by crawling 500K audio samples together with tags and descriptions from the Freesound website. We also use AudioSet  with 2 million human-labeled 10-second sound clips from YouTube videos and AudioCaps  with 46K audio-text pairs derived from the AudioSet dataset. Audio samples are clipped into 10-second segments for training purposes. The paired audio + text data enables us to train **text\(\)audio**, **audio\(\)text**, **text\(\)**audio + text** generation, and audio-text contrastive learning. Similar to image + text joint generation, in text\(\)audio + text, text prompt is the truncated text, and the output is the original text.

**Video.** We use the following diverse and high-quality video datasets to train video generation and video prompt encoder. WebVid , a large-scale dataset of web videos together with descriptions; HD-Villa-100M  with high resolution YouTube videos of at least 720P. We perform **text\(\)video** and video-text contrastive learning task with WebVid. We use HD-Villa-100M for **image\(\)video** generation where the middle frame is the input image.

**Audiovisual.** Web videos are a natural aligned audio-video data resource. However, many existing datasets, e.g., ACAV100M , feature heavily on videos of human speech rather than natural sounds. Therefore, we leverage sound-oriented datasets AudioSet and SoundNet  for joint audio-video generation. For **image\(\)audio + video**, we use the middle frame of the target video as the input prompt image. We also use the middle frame as the prompt input to train the model to generate the audio, i.e., **image\(\)audio**.

  
**Model** & **B@4** & **METOR** & **CIDEr** & **SPICE** \\   \\  & 30.69 & 0.593 & 0.144 & 0.65 & 28.8 & 50.9 \\  \\  \\  & - & 0.741 & - & 0.67 & 48.3 & 37.8 & 60.0 \\  \\  & - & 0.741 & - & 0.67 & 48.3 & 37.9 \\  \\  & - & 0.66 & 0.755 & 0.177 & - & **mIoIo-2** & 57.8 & 34.9 & 80.3 \\  & **0.480** & **0.789** & **0.182** & **cIo-1**[**Ours**] & 52.1 & 32.5 & 74.4 \\   

Table 6: COCO image captioning scores comparison.

  
**Model** & **Zero-Shot** & **CLIPSIM \(\)** & **Method** & **IS (\(\))** & **FVD (\(\))** \\  GODVVA  & No & 0.2402 & CoSyVideo (Chinese) & 23.55 & 751.34 \\ NOWA  & No & 0.2439 & CoSyVideo (English) & 25.27 & 701.59 \\ LogVidac-A-Video  & Yes & 0.3049 & Make-A-Video & 33.00 & 367.23 \\ Video LDM  & Yes & 0.2929 & Video LDM & 33.45 & 550.61 \\  \\  \\  
**CoDi (Ours)** & Yes & 0.2890 & **CoDi (Ours)** & 11.26 &  \\   

Table 3: MSR-VTT text-to-video Table 4: UCF-101 text-to-video generation performance.

  
**Model** & **B@4** & **METOR** & **CIDEr** & **SPICE** \\   & 30.69 & 0.593 & 0.144 & ORO-TBL  & 43.6 & 28.8 & 50.9 \\  \\  & - & 0.741 & - & OIT (43) & 48.9 & 38.7 & 78.0 \\  \\  & - & 0.741 & - & OIT (48) & 38.1 & 75.9 \\  \\  & - & 0.66 & 0.755 & 0.177 & - & **mIoIo-2** & 57.8 & 34.9 & 80.3 \\  \\  \\  \\   

Table 7: AudioCaps audio captioning scores comparison.

  
**Model** & **B@4** & **METOR** & **CIDEr** & **SPICE** \\   & 30.69 & 0.593 & 0.144 & ORO-TBL  & 43.6 & 28.8 & 50.9 \\  \\  & - & 0.653 & 0.176 & MFMF(47)  & 48.9 & 38.7 & 60.0 \\  \\  & - & 0.741 & - & OIT (48) & 38.1 & 75.9 \\  \\  & - & 0.66 & 0.755 & 0.177 & - & **mIoIo-2** & 57.8 & 34.9 & 80.3 \\  \\  \\  \\  \\ 

Table 8: MSRVTT video captioning scores comparison.

[MISSING_PAGE_FAIL:8]

Table 9, CoDi achieves high image generation quality given assorted groups of input modalities. We also test with several input combinations with video as output including text, text + audio, image + image, as well as text + audio + image. We also test on MSRVTT  since all four modalities are present in this dataset. Similarly, the prompt image input is the middle frame of the video. As shown in Table 10, CoDi achieves high video and ground truth text similarity given assorted groups of input modalities. Again our model does not need to train on multi-condition generation like text + audio or text + image. Through bridging alignment and composable multimodal conditioning as proposed in Section 3.2, our model trained on single condition can zero-shot infer on multiple conditions.

### Multi-Output Joint Generation Results

For joint multimodal generation, we first demonstrate high-quality multimodal output joint generation demo as shown in Fig. 5. For quantitative evaluation, there is no existing evaluation metric since we are the first model that can simultaneously generate across all 4 modalities. Therefore, we propose the following metric \(\) that quantifies the coherence and consistency between the two generated modalities by cosine similarity of embeddings:

\[(A,B)=(C_{A}(A),C_{B}(B))\] (4)

  
**Inputs** & **CLIPSIM**\(\) \\   Single-modality Prompt & \\ Text & 0.2890 \\  Dual-modality Prompt & \\ Text+ Audio & 0.2912 \\ Text+Image & 0.2891 \\ Text+Audio+Image & 0.2923 \\   

Table 10: MSR-VTT text-to-video generation performance.

Figure 5: Joint generation of multiple output modalities by CoDi. From top to bottom: text\(\)video+audio, text\(\)image+text+audio, text+audio+image\(\)video+audio.

  
**Inputs** & **FID**\(\) \\   Single-modality Prompt & \\ Text & 14.2 \\ Audio & 14.3 \\  Dual-modality Prompt & \\ Text + Audio & 14.9 \\   

Table 9: CoDi is capable of generating high quality output (image in this case) from various combinations of prompt modalities.

where \(A\), \(B\) are the generated modalities, and \(C_{A}\) and \(C_{B}\) are aligned encoders that project \(A\) and \(B\) to the same space. We use the prompt encoder as described in Section 3.2. This metric aims to compute the cosine similarity of the embedding of two modalities using contrastive learned prompt encoders. Thus, the higher the metric, the more aligned and similar the generated modalities are.

To demonstrate the effectiveness of joint generation, assume the prompt modality is \(P\), we compare \((A,B)\) of \(A\) and \(B\) generated separately vs. jointly, i.e., \(\{P A\), \(P B\}\) vs. \(\{P A+B\}\). The benchmark is the validation set of AudioCaps . We test on the following settings, audio \(\)image+text, image \(\)audio+text, and text\(\)video+audio, image \(\)video+audio. audio\(\)video+text, audio\(\)text+video+image, text \(\)video+image+audio, where the image prompt is the middle frame of the video clip. As shown in Table 11, joint generation (similarity shown on the right side of "\(\)") consistently outperforms independent generation (on the left side of "\(\)").

## 6 Conclusion

In this paper, we present Composable Diffusion (CoDi), a groundbreaking model in multimodal generation that is capable of processing and simultaneously generating modalities across text, image, video, and audio. Our approach enables the synergistic generation of high-quality and coherent outputs spanning various modalities, from assorted combinations of input modalities. Through extensive experiments, we demonstrate CoDi's remarkable capabilities in flexibly generating single or multiple modalities from a wide range of inputs. Our work marks a significant step towards more engaging and holistic human-computer interactions, establishing a solid foundation for future investigations in generative artificial intelligence.

**Limitations & Broader Impacts.** See Appendix D for the discussion.