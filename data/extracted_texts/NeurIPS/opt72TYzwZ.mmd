# Optimal ablation for interpretability

Maximilian Li

Harvard University &Lucas Janson

Harvard University

###### Abstract

Interpretability studies often involve tracing the flow of information through machine learning models to identify specific model components that perform relevant computations for tasks of interest. Prior work quantifies the importance of a model component on a particular task by measuring the impact of performing _ablation_ on that component, or simulating model inference with the component disabled. We propose a new method, _optimal_ ablation (OA), and show that OA-based component importance has theoretical and empirical advantages over measuring importance via other ablation methods. We also show that OA-based component importance can benefit several downstream interpretability tasks, including circuit discovery, localization of factual recall, and latent prediction.

## 1 Introduction

Interpretability work in machine learning (ML) seeks to develop tools that make models more intelligible to humans in order to better monitor model behavior and predict failure modes. Early work in interpretability sought to identify relationships between model outputs and input features Ribeiro et al. (2016); Covert et al. (2022), but with only black-box query access to observe inputs and outputs, it can be difficult to evaluate a model's internal logic. Hence, recent interpretability work often seeks to take advantage of access to an ML model's intermediate computations to gain insights about its decision-making, focusing on deciphering internal units like neurons, weights, and activations Rauker et al. (2022). In addition to finding associations between latent representations and semantic concepts Bau et al. (2017); Mu and Andreas (2021); Burns et al. (2022); Li et al. (2023); Gurnee and Tegmark (2024), interpretability studies aim to investigate how intermediate results are used in later computation and identify specific model components that extract relevant information or perform necessary computation to produce low loss on particular inputs.

A key instrumental goal in interpretability is quantifying the importance of a particular model component for prediction. Studies often measure a component's importance by performing _ablation_ on that component and comparing model performance with and without the component ablated. Ablating a component typically entails replacing its value with a counterfactual value during inference, sometimes referred to as "activation patching." However, the details vary greatly and there is a lack of consensus on best practices Heimersheim and Nanda (2024). For example, Meng et al. (2022) adds Gaussian noise to ablated components' values, while Geva et al. (2023) replaces these values with zeros, and Ghorbani and Zou (2020) replaces them with their means over the training distribution.

In this paper, we present _optimal ablation_ (OA), a new method that sets a component's value to a constant that minimizes the expected loss of the ablated model. In section 2, we introduce OA and show that it is, in a certain sense, a canonical choice of ablation method for measuring component importance. We then show that using OA produces meaningful improvements for several common downstream applications of measuring component importance. In section 3, we apply OA to algorithmic circuit discovery Conmy et al. (2023), or the identification of a sparse subset of components sufficient for performance on a subset of the training distribution. We demonstratethat OA-based performance is a reasonable metric for evaluating circuits and using OA for circuit discovery produces smaller and lower-loss circuits than previous ablation methods. In deploying OA to this application, we also propose a new search algorithm for identifying sparse circuits that achieve low loss according to any performance metric. In section 4, we use OA to locate relevant components for factual recall (Meng et al., 2022) and show that OA better identifies important components compared to prior work. In section 5, we apply OA to latent prediction (Belrose et al., 2023), or the elicitation of output predictions using intermediate activations. We propose an OA-based prediction map and show that it has better predictive power and causal faithfulness than previous methods.

## 2 Optimal ablation

### Motivation

Let \(\) represent a model that is trained to minimize \(_{X,Y}((X),Y)\) for a given loss function \(\) and a distribution of random input-label pairs \((X,Y)\). A common theme in interpretability work is quantifying the importance of some model component \(\) for inference. For example, \(}\) could represent a single neuron, a direction in activation space, a token embedding, an attention head, or an entire transformer layer; further examples of \(\) are discussed in Section 3 and Appendix C.2. Let \((x)\) represent the value of \(\) when the model is evaluated on input \(x\). To identify domain specialization among model components, studies often measure the importance of \(\) for model performance on a particular "subtask" \(\), or an interpretable human-curated distribution of input-label pairs that captures a general aspect of model behavior. We write \((X,Y)\) to indicate sampling input-label pairs or \(X\) to indicate sampling only inputs from the subtask distribution.

Although some works quantify component importance via gradients (Leino et al., 2018; Dhamdhere et al., 2018), such an approach is inherently _local_ (even when aggregated over many inputs) and as such can fail to accurately represent the overall importance of \(\) in highly nonlinear models. Instead, most interpretability studies use _ablation_ to quantify the importance of \(\) by studying the gap in performance between the full model \(\) and a modified version \(^{}\) with \(\) ablated:

\[(,):=(^{})-(),\] (1)

where \(\) is a selected metric for model performance. In the context of measuring importance, we argue that the construction of \(^{}\) is motivated by the following question:

_What is the best performance on subtask \(\) the model \(\) could have achieved without component \(\)?_

To formalize this question, we split its meaning into four elements.

1. **"Performance on subtask \(\)"**: The relevant performance metric \(\) is the expected loss on the subtask with respect to the full model's predictions: \((})=_{X}\ (}(X),(X))\) (note that \(\) aggregates over any randomness in \(}\)).1 We call \(\) defined using this choice of \(\) the _ablation loss gap_. 2. **"Model \(\) could have achieved"**: Since the goal of measuring component importance is to interpret the model \(\), the ablated model \(^{}\) should be constructed solely by changing the value of \(\), holding fixed all other parts of \(\). We write \(_{}(x,a)\) to indicate computing \(\) on input \(x\) while setting the value of \(\) to \(a\) instead of \((x)\) (see Appendix C.2 for details).
3. **"Without component \(\)"**: The ablated model \(^{}(x)\) should use a value for \(\) that is devoid of information about the input \(x\).

Elements II and III motivate the following definition:

**Definition 2.1** (Total ablation).: _A total ablation method satisfies \(^{}(X)=_{}(X,A)\) for some random \(A\), where \(A\!\!\! X\). (Conversely, for a partial ablation method, \(A\) can depend on \(X\).)_

1. **"Best" performance**: To measure the importance of \(\), we wish to understand how much model performance degrades _as a result_ of ablating \(\). If two constructions of the ablated model \(^{}\) both perform total ablation on \(\) but one performs worse than another, the former's underperformance cannot be entirely be attributed to ablating \(\), since the latter also totally ablates \(\) and yet does not degrade performance to the same extent. Thus, the relevant \(^{}\) for measuring importance should incur the _minimum_\(\) among total ablation methods.

To make element IV more concrete, for an ablation method satisfying element II and a given \(x\), replacing \((x)\) with \(a\) can degrade the ablated model's performance via both deletion and spoofing:

1. **Deletion**. The original value \((x)\) could carry informational content specific to \(x\) that serves some mechanistic function in downstream computation and helps the model arrive at its prediction \((x)\). Replacing \((x)\) with some other value \(a\) could _delete_ this information about \(x\), hindering the model's ability to compute the original \((x)\).
2. **Spoofing**. The replacement value \(a\) could "spoof" the downstream computation by _inserting_ information about the input that either: 1. causes the model to treat \(x\) like a different input \(x^{}\);2 or 2. causes the model to treat \(x\) in a way that it never treated _any_ training input, if the new information is _inconsistent_ with information about \(x\) derived from retained components. In the latter case, the confluence of conflicting information could cause later activations to become incoherent, causing performance degradation because these abnormal activations were not observed during training and thus not necessarily regulated to lead to reasonable predictions.

To measure importance, we seek to isolate the contribution of effect 1 to performance degradation. While total ablation methods all capture a maximal deletion effect since \(A\) does not depend on \(X\), measuring the "best" performance minimizes the additional contribution of potential spoofing effects.

### Prior work

Component importance is strongly related to _variable_ importance (Sobol, 1993; Homma and Saltelli, 1996; Breiman, 2001; Ishwaran, 2007; Gromping, 2009), a longstanding area of research in statistics and ML. The vast body of work in this area is too extensive to review here, and the recent surge of research interest in interpreting _internal_ model components has raised new and unique challenges relating to the values of internal components often being deterministically related. Thus, we focus on recent work applying ablation methods to internal components in this section, and defer broader discussion to Appendix B.

Ablation methods previously applied to internal components can be separated into _subtask-agnostic_ methods, which can be applied out-of-the-box to any subtask, and _subtask-specific_ methods, which only work on subtasks for which inputs satisfy a designated structure, and even then require human ingenuity to adapt to each new subtask.

Subtask-agnostic ablation methods include _zero ablation_(Baan et al., 2019; Lakretz et al., 2019; Bau et al., 2020; Olsson et al., 2022; Geva et al., 2023; Cunningham et al., 2023; Merullo et al., 2024; Gurnee et al., 2024), which replaces \((x)\) with zero, i.e. \(^{}(x)=_{}(x,0)\); _mean ablation_(Ghorbani and Zou, 2020; McDougall et al., 2023; Tigges et al., 2023; Gould et al., 2023; Li et al., 2024; Marks et al., 2024), which replaces \((x)\) with its mean, i.e. \(^{}(x)=_{}(x,_ {X^{}}[(X^{})])\); and _(marginal) resample ablation_(Chan et al., 2022; Liebrum et al., 2023; McGrath et al., 2023; Belrose et al., 2023; Rushing and Nanda, 2024), which replaces \((x)\) with \((X^{})\) for an independent copy \(X^{}\) of the input, i.e. \(^{}(x)=_{}(x, (X^{}))\). While zero, mean, and resample ablation are total ablation methods, adding Gaussian noise to \((x)\)(Meng et al., 2022) is a subtask-agnostic partial ablation method. These methods are all applicable to any subtask.

On the other hand, subtask-specific ablation methods rely on particular details of a chosen subtask. Singh et al. (2024) replaces \((x)\) with interpretable values, e.g. setting an attention pattern to copy from the previous token, while Goldowsky-Dill et al. (2023) replaces \((x)\) with \((x^{*})\) for an interpretable reference input \(x^{*}\). Hanna et al. (2023) employs _counterfactual ablation_ (CF), a partial ablation method that replaces \((x)\) with \(((x))\), where \(\) is a map that sends each input \(x\) to a "neutral" (potentially random) input \((x)\) that preserves most aspects of \(x\) but removes information relevant to the subtask, i.e. \(^{}(x)=_{}(x, ((x)))\). Wang et al. (2022) also considers a counterfactual distribution of inputs for _counterfactual mean ablation_, which replaces \((x)\) with its mean over the distribution of counterfactuals, i.e. \(^{}(x)=_{}(x,_ {(X^{}),}((X^{})))\).

Subtask-specific methods can be useful, but it is usually unclear how to generalize them beyond the subtask originally selected. CF is the most popular among these methods, leveraged by a range of manual (Vig et al., 2020; Merullo et al., 2023; Stolfo et al., 2023; Tigges et al., 2023; Hendel et al., 2023; Heimersheim and Janiala, 2023; Todd et al., 2024; Marks et al., 2024) and algorithmic (Conny et al., 2023; Syed et al., 2023) studies and recommended by meta-studies (Zhang and Nanda, 2024; Heimersheim and Nanda, 2024). For text data, the effectiveness of CF relies heavily on token parallelism between \(x\) and \((x)\), which typically share exact tokens at all but a few sequence positions. Though studies have thus far focused on toy subtasks for which suitable mappings \(\) are relatively easily constructed, it may be difficult or impossible to select well-suited input pairs for certain subtasks (see Appendix F.3 for a few simple examples), especially more general model behaviors. Even for subtasks that admit such a mapping, how \((x)\) is engineered to withhold subtask-relevant information differs from subtask to subtask, and the construction of \(\) for each particular subtask is a subjective process that requires human ingenuity. Finally, CF is only a partial ablation method; since \(((x))\) depends on \(x\), it may give away information about \(x\) that is useful for performance on \(\).

### Definition and properties of optimal ablation

We present _optimal ablation_ (OA), our proposed approach to simulating component removal.

**Definition 2.2** (Optimal ablation).: _To ablate \(\), we replace \((x)\) with an "optimal" constant \(a^{*}\)._

\[^{}_{}(x):=_{ }(x,a^{*}), a^{*}:=*{arg\,min}_{a}_{ X}\,(_{}(X,a),(X))\] (2)

We define \(_{}\) by plugging \(^{}_{}(x)\) into Equation (1). Like zero, mean,3 and resample ablation, optimal ablation is a total ablation method satisfying Definition 2.1. But among all total ablation methods, optimal ablation is optimal in the sense that it yields the lowest \(\).

**Proposition 2.3**.: _Let \((,)\) be the ablation loss gap for some component \(\) measured with any total ablation method. Then, \(_{}(,)(, )\)._

Proof.: Consider a total ablation method that defines \(^{}(X)\) by replacing \((X)\) with \(A\) (per Definition 2.1), and let \((,)\) be the measured ablation loss gap. By the tower property,

\[(,)=_{(X),A}\,(_{}(X,A),(X))=_{A}[ [(_{}(X,A),(X))A ]].\]

Since \(A-2.0mu }X\), \([(_{}(X,A),(X)) \;A=a]=_{X}(_{ }(X,a),(X))=:g(a)\).

\[ a,\;_{}(,)= _{X}\,(_{}(X,a^{*}), (X))_{X}\,(_{ }(X,a),(X))=g(a)\] \[_{}(,) _{A}\;g(A)=(,).\]

Optimal ablation thus provides the _unique_ answer to our motivating question in Section 2.1, since it produces the best performance among all total ablation methods, including zero, mean, and resample ablation. Intuitively, OA minimizes the contribution of spoofing (effect 2 from Section 2.1) to \(\) by setting ablated components to constants \(a^{*}\) that are _maximally consistent_ with information from other components, e.g. by conveying a lack of information about \(x\) or by hedging against a wide range of possible \(x\) rather than strongly associating with a particular input other than the original \(x\). OA does not entirely eliminate spoofing, since it may be the case that every possible value of \(\) conveys at least weak information to the model. However, the excess ablation gap \(-_{}\) for \(\) measured with ablation methods that replace \((x)\) with a (random) value \(A\) is _entirely_ caused by spoofing, since replacing \((x)\) with the constant \(a^{*}\) achieves lower loss without giving away any more information about \(x\). In practice, \(-_{}\) for prior ablation methods is typically _very_ large compared to \(_{}\) for both single components (see Table 1) and circuits (see Section 3.2) on prototypical language subtasks. This disparity indicates that effect 2 dominates the \(\) measurements for previous ablation methods, making them poor estimators for effect 1 compared to OA.

Subtask-specific methods often try to generate consistent interventions by _conditioning_ on features of the input to avoid replacing \((x)\) with values that could confuse the model. For CF, choosing \((x)\) to share many tokens with \(x\) mitigates the contribution of effect 2b to \(_{}\), which is the main reason the technique is so widely employed. Thus, among _previous_ measures of component importance, \(_{}\), when it can be well-constructed, may be the best quantification of effect 1. To demonstrate this intuitive relation between OA and CF as techniques that aim to isolate effect 1, we perform a case study in Section 2.4 that shows that among other ablation methods, OA produces the measurementsmost similar to CF. However, not only is OA more general than subtask-specific methods like CF, but \(_{}\) may still be a better estimator for effect 1 than \(_{}\) even when CF is well-defined. In Section 3, we show that for circuits, \(_{}\) is much lower than \(_{}\)_despite reflecting a weakly stronger deletion effect_, indicating that effect 2 also contributes to \(_{}\) less than it does to \(_{}\), and thus \(_{}\) is a more accurate reflection of components' informational importance.

**Computation of \(a^{*}\).** Though it is impossible to derive \(a^{*}\) in closed form, we find that in practice, mini-batch stochastic gradient descent (SGD) performs well at finding constants \(\) that greatly reduce \(\) compared to heuristic point estimates like zero and the mean. We generally adopt the approach of initializing each \(\) to the subtask mean \(_{X}[(X)]\) and performing SGD to minimize \(\).

### Comparison of single-component ablation results on IOI

The Indirect Object Identification (IOI) subtask (Wang et al., 2022) consists of prompts like "When Mary and John went to the store, Mary gave the apple to _," which GPT-2-small (Radford et al., 2019) completes with the correct indirect object noun ("John"). We use IOI as a case study because it is discussed extensively in interpretability work (Merullo et al., 2023; Makelov et al., 2023; Wu et al., 2024; Lan et al., 2024; Zhang and Nanda, 2024). To implement CF, for each prompt \(x\), Wang et al. (2022) constructs a random \((x)\) in which the names are replaced with random distinct names.

We evaluate \(\) for attention heads and MLP blocks using zero, mean, resample, counterfactual, counterfactual mean, and optimal ablation. In Table 1, we show that among attention heads and MLP blocks, \(_{}\) accounts for only 11.1% of \(_{}\), 33.0% of \(_{}\), and 17.7% of \(_{}\) for the median component. Furthermore, among these \(\) measurements, \(_{}\) has the highest highest rank correlation (0.907) with \(_{}\). Full results are shown in Appendix E.3.

## 3 Application: circuit discovery

Circuit discovery is the selection of a sparse subnetwork of \(\) that is sufficient for the recovery of model performance on an algorithmic subtask \(\). To define what constitutes a "sparse subnetwork," we write \(\) as a computational graph with vertices \(G\) and edges \(E\). An edge \(e_{k}:=(_{j},_{i},z) E\) indicates that \(_{j}(x)\) is taken as the \(z\)th input to \(_{i}\) in the computation represented by the graph. To ablate edge \(e_{k}\), we replace the \(z\)th input to \(_{i}\), which is equal to \(_{j}(x)\) during normal inference, with some value \(a\). We compute \(^{}(X)\), which represents modified inference with edges \(E\) ablated, by applying this intervention for each ablated edge (see Appendices C.1 and C.2 for more details). Circuit discovery aims to select a subset of edges \(^{*} E\) such that

\[^{*}=*{arg\,min}_{ E}[_ {X}(^{}(X),(X))+ ()]=*{arg\,min}_{E E}[ (,E)+()]\] (3)

for a regularization term \(\) that measures the sparsity level (further discussed in Appendix F.4). Additionally, when implementing OA, though we could use a different constant for each edge, we instead define a single constant \(a^{*}_{j}\) for each _vertex_\(_{j}\), so that if multiple out-edges from \(_{j}\) are ablated, the same value is passed to each of its children (further discussed in Appendix F.2).

### Methods

We compare \((,E)\) measured with mean ablation, resample ablation, OA, and CF as metrics for circuit discovery. We consider the manual circuit for each subtask and circuits optimized on each \(\) metric using several search algorithms.

**ACDC**(Conmy et al., 2023) identifies circuits by iteratively considering edge ablations. They start by proposing \(=E\), then iterate over edges \(e_{k}\), ablating \(e_{k}\) and updating \(=\{e_{k}\}\) if the marginal impact on loss, \((,(E\{e_{k}\}))-(,E (\{e_{k}\}))\), is below a tolerance threshold \(\).

    & **Zero** & **Mean** & **Resample** & **CF-Mean** & **Optimal** & **CF** \\  Rank correlation with CF & 0.590 & 0.825 & 0.828 & 0.833 & **0.907** & 1 \\ Median ratio of \(_{}\) to \(\) & 11.1\% & 33.0\% & 17.7\% & 31.7\% & 100\% & 88.9\% \\   

Table 1: Comparison of ablation loss gap \(\) on IOI

**Edge Attribution Patching** (EAP) (Syed et al., 2023) selects \(\) to contain the edges \(e_{k}\) that have the largest gradient approximation of their single-edge ablation loss gap \((,e_{k})\).

**HardConcrete Gradient Sampling** (HCGS) is an adaptation of a pruning technique from Louizos et al. (2018) to circuit discovery. Rather than considering only total ablation of an edge \(e_{k}=(_{j},_{i},z)\), we can consider a continuous mask of coefficients \(\) and partially ablate \(e_{k}\) by replacing the \(z\)th input to \(_{i}\) with a linear combination of the original value \(_{j}(x)\) and ablated value \(a_{j}\), i.e. \(_{k}_{j}(x)+(1-_{k})a_{j}\). Now, \(_{k}=0\) designates total ablation (replacing with \(a_{j}\)), while \(_{k}=1\) designates total retention (keeping \(_{j}(x)\)). We use \(^{}(x)\) to represent the model with edges partially ablated according to \(\).

Some previous work (Liu et al., 2017; Huang and Wang, 2018) optimizes directly on the mask coefficients \(\), but to avoid getting stuck in local minima on \(_{k}(0,1)\), Louizos et al. (2018) samples \(_{k}\) from a HardConcrete distribution parameterized by location \(_{k}\) and temperature \(_{k}\) for each edge, and performs SGD with respect to the distributional parameters. In effect, we update the parameters based on gradients evaluated at randomly sampled values of \(\) rather than gradients evaluated at any exact \(\). Cao et al. (2021) applies this technique to find circuits that consist of a subset of model weights. Conmy et al. (2023) applies this technique to vertices in a computational graph. Unlike previous work, we apply this technique to edges rather than vertices.

**Uniform Gradient Sampling** (UGS) is our proposed method for algorithmic circuit discovery. Similar to HCGS, we consider ablation coefficients \(\) and update parameters based on gradients evaluated at sampled values of \(\). We keep track of a parameter \(_{k}\) for each edge, where \(_{k}=(1+(-_{k}))^{-1}\) indicates an estimated probability of \(e_{k}^{*}\). Using \(w(_{k})=_{k}(1-_{k})\) to determine sampling frequency (further discussed in Appendix F.8), we let \(_{k}(0,1)\) with probability (w.p.) \(w(_{k})\), \(_{k}=1\) w.p. \(_{k}-w(_{k})\), and \(_{k}=0\) w.p. \(1-_{k}-w(_{k})\). For a batch of \(b\) inputs \(X^{(1)},...,X^{(b)}\), let \(^{(j)}\) denote the sampled coefficients corresponding to \(X^{(j)}\), and let \(N_{k}=_{j=1}^{b}(_{k}^{(j)}(0,1))\). We construct a loss function \(_{()}\) whose gradient satisfies

\[_{_{k}}_{()}=_{_{k}}( )+N_{k}^{-1}_{j=1}^{b}(_{k}^{(j)} (0,1))_{_{k}^{(j)}}(^{^ {(j)}}(X^{(j)}),(X^{(j)}))\] (4)

and perform SGD on the \(_{k}\), where \(()\) is a continuous relaxation of \(()\) from Equation (3). In Appendix F.5, we motivate UGS as an estimator for sampling over Bernoulli edge coefficients.

**Optimizing circuits on \(_{}\)**  ACDC and EAP are not compatible with optimization on \(_{}\), since the optimal \(^{*}\) values depend on the selected circuit and it is intractable to optimize \(\) for every candidate circuit. For our circuit evaluations on \(_{}\), we compare to ACDC- and EAP-generated circuits optimized on \(_{}\). On the other hand, HCGS and UGS allow us to perform SGD to optimize the ablation constants \(\) concurrently with the sampling parameters.

### Experiments

We study GPT-2-small performance on the IOI (Wang et al., 2022) subtask described in Section 2.4 and the Greater-Than (Hanna et al., 2023) subtask, which involves completing prompts such as "The conflict started in 1812 and ended in 18_ - " with digits greater than the first year in the context. We select these settings because their exposition in manual studies is particularly thorough and they are used in prior work (Conmy et al., 2023; Syed et al., 2023) to benchmark algorithmic circuit discovery.

We compare the algorithms in Section 3.1 trained to minimize \(\) on the IOI and Greater-Than subtasks when edges \(E\) are ablated with mean ablation, resample ablation, OA, and CF. For IOI, the mapping \(\) for CF is defined in Section 2.4. For Greater-Than, we continue with the practice from Hanna et al. (2023) of selecting counterfactuals \((x)\) by changing the first year in the prompt \(x\) to end in "01" so that all numerical completions are equally valid (see Appendix F.3).

UGS achieves Pareto dominance on the \(\)-\(||\) tradeoff over the other methods on both subtasks for each ablation method, identifying circuits that achieve lower \(\) at any given \(||\) and vice versa. Results for IOI circuits optimized on \(_{}\) are shown in Figure 1 (left). On IOI, UGS finds a circuit with 385 edges that achieves \(_{}=0.220\). This circuit has 52% fewer edges than the smallest ACDC-identified circuit with comparable \(_{}\) and 48% lower \(_{}\) than the best-performing ACDC-identified circuit with a comparable edge count. Similar improvements to the Pareto frontier,shown in Appendix F.10, occur for mean, resample, and optimal ablation. UGS also creates Pareto improvements for Greater-Than circuits for each ablation method; see Appendix F.11.

Applying OA to circuit discovery reveals that certain sparse circuits can account for model performance on these subtasks to a much greater extent than previously known. We visualize the \(\) for each ablation method achieved by UGS-identified circuits in Figure 1 (right). Using OA to ablate excluded components, we find circuits that recover much lower \(\) at any given circuit size than any circuit for which excluded components are ablated with any other ablation method. For example, for IOI, at a circuit size of 1,000 edges, ablating excluded components with OA enables the existence of circuits with 32% lower \(\) compared to CF, 62% lower \(\) compared to mean ablation, and 88% lower \(\) compared to resample ablation, and the improvement is even larger at smaller circuit sizes. For Greater-Than (results shown in Appendix F.11), OA again admits circuits with by far the lowest \(\) among the four ablation methods. Thus, OA paints a more accurate and compelling picture of how much small subsets of the model can explain behavior on these subtasks.

Unlike other ablation methods, OA indicates that the manual circuits are approximately optimal for their size. Holding \(||\) fixed, the Pareto-optimal \(_{}\) is 29% below the \(_{}\) of the manual circuit on IOI and 42% below the \(_{}\) of the manual circuit on Greater-Than. However, for the other ablation methods, optimized circuits with fewer edges than the manual circuit achieve 84-85% lower \(\) than the manual circuit on IOI, and 70-84% lower \(\) on Greater-Than. Since the manual circuits are selected using a thorough mechanistic understanding of the model for each subtask and thus arguably capture the important components, this finding furthers the notion that \(\) measured with previous methods could be artificially high due to spoofing by ablated components, and therefore \(_{}\) is a superior evaluation metric for circuits.

These results show that \(_{}\) is useful for evaluating and discovering circuits and provide evidence that OA better quantifies the removal of important mechanisms than previous ablation methods.

## 4 Application: factual recall

Transformers can store and retrieve a large corpus of factual associations. One goal in interpretability is _localizing factual recall_, or identifying components that store specific facts. To this end, Meng et al. (2022) proposes _causal tracing_, which involves removing important information about the prompt \(x\) and evaluating which components can recover the original \((x)\). To isolate components responsible for an association between a subject (e.g. "Eiffel Tower") and an attribute ("located in Paris"), they select a prompt \(x\) ("The Eiffel Tower is located in the city of - ") that elicits from \(\) a correctly memorized response \(y\) ("Paris"). They produce a corrupted input \(_{}(x)\) by adding a Gaussian noise (GN) term \(Z(0,9)\), to all token embeddings that encode the subject, where \(\) is a diagonal

Figure 1: Left: Circuit discovery Pareto frontier for the IOI subtask with counterfactual ablation. Right: Comparison of ablation methods for circuit discovery on IOI (X indicates manual circuit evaluated on each ablation method). \(\) is measured in KL-divergence.

matrix and \(_{ii}\) represents the variance of the \(i\)th neuron among token embeddings sampled from the training distribution. Letting \([(x)]_{y}\) represent the probability assigned by \((x)\) to label \(y\). Since \(_{}\) partially ablates information about the subject, \([(_{}(x))]_{y}\) is typically much smaller than \([(x)]_{y}\). For each component \(\), they estimate its contribution to the recall of \(y\) with the following "average indirect effect" (AIE) representing the proportion of probability on the correct \(y\) recovered by replacing \(((X))\) with \((X)\), averaged over \((X,Y)\), where \(=_{}\): 4

\[():=(0,1-_{(X,Y)}(0,[(X)]_{Y}-[_{}((X),A(X))]_{Y})}{ _{(X,Y)}[(X)]_{Y}-_{(X,Y) }[((X))]_{Y}.)})\] (5)

where we declare \(()=0\) if the denominator is non-positive and ablating subject tokens actually helps identify the correct label (however, this is never the case).

MethodWe perform causal tracing by removing the subject with optimal ablation (OA-tracing, or OAT) rather than with Gaussian noise (GNT). We define \((x)=_{}(x,a_{})\) by replacing subject token embeddings with a constant \(a_{}\) trained to minimize the numerator in Equation (5), which represents \(\) with a carefully chosen loss function (see Appendix G.2).

ExperimentsWe compare GNT and OAT for GPT-2-XL on a dataset of subject-attribute prompts from Meng et al. (2022) for which the model completes the correct answer via sampling with temperature 0. To increase the sample size, we augment the data with similarly constructed prompts from follow-up work on factual recall (Hernandez et al., 2022). We train OAT on 60% of the dataset and evaluate both methods on the other 40%. On the test set, \([(X)]_{Y}=30.6\%\), \([(_{}(X))]_{Y}=12.3\%\), and \([(_{}(X))]_{Y}=8.7\%\). We let \((X)\) represent an attention or MLP layer output at a certain token position(s): namely, all subject token positions, only the last subject token position, and only the last token position in the entire sequence. Rather than considering only one layer at a time, Meng et al. (2022) lets \(\) represent the outputs of a sliding window of several consecutive attention layers or MLP layers. Thus, in addition to replacing the output of a single layer (window size 1), we show results for replacing windows of sizes 5 and 9.

OAT offers a more precise localization of relevant components compared to GNT. While GNT indicates a small positive AIE for most components, OAT shows a few components have large contributions while most have little to no effect. For example, Figure 2 (top left) shows that the AIE for a window of 5 attention layers at the last token is as high as 42.6% for the window consisting of layers 30-34, while the AIE peaks at only 20.2% for GNT. On the other hand, for windows centered around layers 15-23, the average AIE for OAT is only 1.7%, indicating little effect for these potentially

Figure 2: Comparison of AIE with GNT and OAT. In the top figure, layer \(\) on the x-axis represents replacing a sliding window of 5 layers with \(\) as the median. Error bars indicate the sample estimate plus/minus two standard errors (details given in Appendix G.4).

unimportant layers, compared to 7.0% for GNT. For sliding windows of 9 attention layers at subject token positions, GNT shows marginally positive AIE measurements across layers 0-30, but OAT specifically shows highly positive AIE for layers 0-5 and 25-30 (see Figure 14). Moreover, whereas Meng et al. (2022) focuses on sliding window replacement because GNT effects from single-layer replacements are very small, OAT can sometimes identify information gain from just one layer. For instance, at the last token position, OAT records AIEs above 8% for each of attention layers 30, 32, and 34 by themselves (see Figure 2, bottom left), much greater than the AIE of the other layers. This greater level of granularity opens up the possibility of selectively investigating combinations of layers as opposed to relying on the prior that adjacent layers work together.

## 5 Application: latent prediction

One practice in interpretability is eliciting predictions from latent representations. Let \(\) have layers \(0,...,N\) and let \(_{i}(X)\) be the residual stream activation at the last token position (LTP) after layer \(i\). _Logit attribution_(Geva et al., 2022; Wang et al., 2022; Dar et al., 2023; Katz and Belinkov, 2023; Dao et al., 2023; Merullo et al., 2024; Halawi et al., 2024) is the practice of applying a transformer model's unembedding map to an activation to obtain a semantic interpretation of that activation. When applied to the LTP activation after layer \(i\), this practice is equivalent to zero ablating layers \(i+1\) to \(N\). However, the semantic meanings of LTP activations after layer \(N\) can be different from those of LTP activations in earlier layers. As an alternative, _tuned lens_(Belrose et al., 2023; Din et al., 2023) is a linear map \(f_{i}(_{i})=W_{i}_{i}+b_{i}\) that "translates" from \(_{i}(X)\) to a predicted \(_{N}(X)\). \(_{}(X)\) is defined by replacing \(_{N}(X)\) with \(_{N}(X):=f_{i}(_{i}(X))\) during inference, and training \(W_{i}\) and \(b_{i}\) to minimize \(_{}:=_{X}(_{ }(X),(X))\). Tuned lens demonstrates _when_ information is transferred to LTP: if replacing \(_{N}(X)\) with \(_{N}(X)\) achieves low loss, then \(_{i}(X)\) contains sufficient context for computing \((X)\), so key information is transferred prior to layer \(i\).

MethodWe propose Optimal Constant Attention (OCA) lens. We define \(_{}(X)\) by using OA to ablate _attention_ layers \(i+1\) to \(N\): for each of these layers \(k\), we replace its output at LTP with a constant \(_{k}\). We train \(=(_{i+1},...,_{N})\) to minimize \(_{X}_{}:=_{X}(_{}(X),(X))\).

Similar to tuned lens, OCA lens reveals whether the LTP activation after layer \(i\) contains sufficient context to compute \((X)\) by eliminating information transfer from previous token positions to LTP after layer \(i\). While tuned lens is a linear map, OCA lens is a function that leverages the model's existing architecture (specifically, its MLP layers) to translate between LTP activations at different layers. OCA lens has far fewer learnable parameters than tuned lens: \(O(Nd_{})<O(d_{}^{2})\).

ExperimentsWe compare \(_{}\) to \(_{}\) for various model sizes. As additional baselines, we also consider the ablation of later attention layers with mean or resample ablation rather than OA. Results are shown in Figure 3 (left) for GPT-2-XL and Figure 15 for other model sizes. OCA lens achieves significantly lower loss than tuned lens, indicating better extraction of predictive power from LTP activations. For example, the predictive loss of OCA lens drops to below 0.01 around layer 35 of GPT-2-XL, but does not reach this point even at the last layer for tuned lens.

Figure 3: Left: Prediction loss comparison between tuned lens and ablation-based alternatives. Middle, right: Causal faithfulness metrics for tuned and OCA lens under basis-aligned projections. Additionally, Belrose et al. (2023) explains that one desiderata for latent prediction is _causal faithfulness_, i.e. \(f_{i}\) should use \(_{i}(X)\) in the same way as \(\). We can investigate causal faithfulness by intervening on \(_{i}(X)\) and evaluating the extent to which \(_{}(X)\) and \((X)\) move in parallel. If

\(_{}(X)\) changes significantly but \((X)\) does not, for example, then \(f_{i}\) could be extrapolating from spurious correlations, e.g. by inferring from directions that predict information transfer that occurs in later layers. Consider a random intervention \(\) on \(_{i}(X)\) and let \(_{}(X;)\) represent replacing \(_{i}(X)\) with \((_{i}(X))\) before applying \(f_{i}\). Similarly, let \((X;)\) represent replacing \(_{i}(X)\) with \((_{i}(X))\) during inference. Belrose et al. (2023) separates causal faithfulness into two measurable properties (both range from -1 to 1 and higher values reflect greater faithfulness):

1. **Magnitude correlation**: \(([(_{}(X;), _{}(X))],[((X; ),(X))])\).
2. **Direction similarity**: \([_{}(X;)_{}(X),\ (X;)(X)]\), where \(\) denotes subtraction in logit space and \(\) denotes the Aitchinson similarity between distributions.

We assess these properties for \(_{}\) and \(_{}\) for a variety of interventions \(\). In Figure 3 (middle, right), we plot these properties for a modified version of the "causal basis projection" \(\) from Belrose et al. (2023). While they train a basis iteratively, this approach is expensive and unstable, and we instead extract an approximate basis for \(_{}\) by performing singular value decomposition (SVD) on \(W_{i}^{1/2}\), where \(\) is the covariance matrix of \(_{i}(X)\), and applying \(^{1/2}\) to the right singular vectors. For \(_{}\), we extract this basis by training a linear map to approximate \(f_{i}\) and using the weights as the \(W_{i}\). For both lenses, we compute \((a)=+p(a-)\), where \(=[_{i}(X)]\) and \(p\) represents projecting to the orthogonal complement of \(()\) for a uniformly sampled basis vector \(\). We plot the magnitude correlation and direction similarity for \(_{}\) and \(_{}\) with respect to \(\) in Figure 3. We find that OCA lens measures significantly better on both causal faithfulness metrics across all layers, and we achieve similar results for other choices of interventions \(\) (see Appendix H.3).

One downstream application of extracting latent predictions from intermediate-layer LTP activations is that they can sometimes be more accurate on text classification tasks than the model's output predictions, especially if the context contains false demonstrations, i.e. examples of incorrect task completions (Halawi et al., 2024). The proposed theory is that the model first computes the correct answer at LTP in early layers, then later layers move contextual information to LTP that lead it to make adjustments that benefit next-token prediction, such as reporting an incorrect answer for consistency with false demonstrations. We compare the _elicitation accuracy boost_, or the best elicitation accuracy across layers minus the accuracy of the model output, for OCA lens and tuned lens for GPT-2-XL with 2,000 classification samples from each of 15 text classification datasets from Halawi et al. (2024), using their calibrated accuracy metric. We find that OCA lens increases this accuracy boost for prompts with true demonstrations on 12 of the 15 datasets and for prompts with false demonstrations on 11 of the 15 (see Figure 21). In particular, for Wikipedia topic classification (DBPedia), OCA lens increases the elicitation accuracy boost from 2.9% to 18.0% with true demonstrations and from 19.2% to 28.8% with false demonstrations (see Figure 4, middle). Full results are reported in Appendix H.4.

## 6 Future work

The applications of component importance presented in our work are not exhaustive. A variety of interpretability work either directly applies ablation-based importance or can be framed to use it as a potential tool. OA creates new opportunities to incorporate ablation into studies for which it may be impossible to obtain good results with previous ablation methods. For example, we can train probes derived from using OA with different loss functions (Li et al., 2023), or use an approach similar to OCA lens to decode activations other than the LTP residual stream activation. See Appendix D.3 for an extension of OA to evaluate the extent to which a component performs classification.

Figure 4: Comparison of calibrated elicitation accuracy on selected datasets.