# Finding good policies in average-reward Markov Decision Processes without prior knowledge

Adrienne Tuynman

adrienne.tuynman@inria.fr

&Remy Degenne

remy.degenne@inria.fr

Emilie Kaufmann

emilie.kaufmann@univ-lille.fr

Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

###### Abstract

We revisit the identification of an \(\)-optimal policy in average-reward Markov Decision Processes (MDP). In such MDPs, two measures of complexity have appeared in the literature: the diameter, \(D\), and the optimal bias span, \(H\), which satisfy \(H D\). Prior work have studied the complexity of \(\)-optimal policy identification only when a generative model is available. In this case, it is known that there exists an MDP with \(D H\) for which the sample complexity to output an \(\)-optimal policy is \((SAD/^{2})\) where \(S\) and \(A\) are the sizes of the state and action spaces. Recently, an algorithm with a sample complexity of order \(SAH/^{2}\) has been proposed, but it requires the knowledge of \(H\). We first show that the sample complexity required to estimate \(H\) is not bounded by any function of \(S,A\) and \(H\), ruling out the possibility to easily make the previous algorithm agnostic to \(H\). By relying instead on a diameter estimation procedure, we propose the first algorithm for \((,)\)-PAC policy identification that does not need any form of prior knowledge on the MDP. Its sample complexity scales in \(SAD/^{2}\) in the regime of small \(\), which is near-optimal. In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in \(H\) cannot be achieved in this setting. Then, we propose an online algorithm with a sample complexity in \(SAD^{2}/^{2}\), as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound.

## 1 Introduction

Reinforcement learning (RL) is a paradigm in which an agent interacts with its environment, modeled as a Markov Decision Process (MDP), by taking actions and observing rewards. Its goal is to learn, or to act according to, a good policy, that is a mapping from state to actions which maximizes cumulative rewards. However, there are different ways to define this notion of (expected) cumulative reward: in the _finite horizon_ setting, one should maximize the expected sum of rewards up to a certain fixed horizon; in the _discounted_ setting, each consecutive reward is \(\) times as important as the previous one, with \(0<<1\). In this paper, we focus on the _average reward_ setting, in which the value of a policy is measured by its asymptotic mean reward per time step. This setting is ideal for long-term learning, as there is no need to tune the horizon or discount parameters. However, the asymptotic nature of its optimality criterion makes the problem more complicated and highly sensitive to small changes in the MDP, which is less observable in the finite horizon or discounted settings.

Formally, a Markov Decision Processes is defined as a tuple (\(\),\(\),\(P\),\(r\)) where \(\) is the state space of finite size \(S\), \(\) is the action space of finite size \(A\). Letting \(_{}\) denote the set of distributionover a set \(\), \(P:_{}\) is the (assumed unknown) transition kernel, and \(r:_{}\) is the reward kernel. For each state-action pair \((s,a)\), we denote by \(_{s,a}=[r(s,a)]\) the (assumed known) mean reward1. At each time step \(t\), the agent observes a state \(s_{t}\), takes an action \(a_{t}\), and observes a reward \(r_{t} r(s_{t},a_{t})\) and a next state \(s_{t+1} P(s_{t},a_{t})\). In an average-reward MDP (AR-MDP) the value of a policy \(:()\) is measured with its gain \(g_{}(s)=_{T+}_{}[_{t= 0}^{T-1}r_{t}|s_{0}=s]\) where the expectation is taken when the agent follows policy \(\) (i.e., \(s_{t}(a_{t})\)) from the initial state \(s\). In this paper, we consider weakly communicating MDPs (defined in Section 2) in which a policy \(_{}\) maximizing the gain in all states exists and has a constant value, denoted by \(g_{_{}}\).

We are interested in the best policy identification problem: we want to build an algorithm that learns the MDP by taking actions and collecting observations, until it can, after some (possibly random) number of interactions \(\) output a policy \(\) that is near-optimal. More precisely, given two parameters \((0,1)\) and \((0,1)\) we seek an \((,)\)-Probably Approximately Correct (PAC) algorithm, that is an algorithm that satisfies

\[(<, s,g_{_{}}-g_{ }(s)>)\.\]

Our objective is to find such an algorithm that has minimal sample complexity, i.e., that requires the least amount of steps \(\), with high probability or in expectation. Two different models can be considered for the collection of observations. In the online model, the algorithm can only choose the action \(a_{t}\) to sample at each time step, as the state \(s_{t}\) is determined by the MDP's dynamics and the previous actions. With a generative model however the agent can sample the reward and next state of any state action pair \((s_{t},a_{t})\), regardless of which state it arrived in at the previous time step.

To the best of our knowledge, prior work on \((,)\)-PAC best policy identification in AR-MDPs has exclusively considered the generative model setting. In the online setting, the regret minimization objective has however been studied a lot (e.g., ). Existing sample complexity or regret bounds feature different notions of complexity of the MDP, besides its size \(S,A\). The diameter \(D\) and the optimal bias span \(H\), formally defined in the next section, are two such complexity notions that both feature in lower or upper bounds on the sample complexity of existing algorithms. More specifically for \((,)\)-PAC policy identification, the work of  provides a worse-case lower bound showing that there exists an MDP on which any \((,)\)-PAC algorithm using a generative model should have a sample complexity larger than \(((SAD/^{2})(1/))\). The recent work of  provides an \((,)\)-PAC algorithm that takes \(H\) as input and whose sample complexity is \(}((SAH/^{2})(1/))^{2}\). Using that \(H D\), this algorithm is thus optimal, and the lower bound is also in \(((SAH/^{2})(1/))\). This raises the following question: can an algorithm attain the same optimal sample complexity without prior knowledge of \(H\)? More broadly, as detailed in the next section, all existing \((,)\)-PAC algorithms require some form of prior knowledge on the MDP, and we propose the first algorithms that are agnostic to the MDP.

ContributionsIn the generative model setting, a first hope to get an algorithm agnostic to \(H\) is to plug-in a tight upper bound on this quantity in the algorithm of . Our first contribution is a negative result: the number of samples necessary to estimate \(H\) within a prescribed accuracy is not polynomial in \(H\), \(S\) and \(A\). This result is proved in Section 3. On the positive side, by combining a procedure for estimating the diameter \(D\) inspired by  with the algorithm of  we propose in Section 4 Diameter Free Exploration (DFE), an algorithm for communicating MDPs that does not require any prior knowledge on the MDP and has a near-optimal \(}((SAD/^{2})(1/))\) sample complexity in the asymptotic regime of small \(\). Then in Section 5 we discuss the hardness of \((,)\)-PAC policy identification in the online setting. We notably prove a lower bound showing that the sample complexity of \((,)\)-PAC policy identification cannot always be polynomial in \(S\), \(A\) and \(H\), even with the knowledge of \(H\). On the algorithmic side, we propose in Section 6 an online variant of DFE whose sample complexity scales in \(SAD^{2}/^{2}\). As prior work, DFE hinges on a conversion between the discounted and average-reward settings  and uses uniform sampling. Departing from this approach, we further propose a novel data-dependent stopping rule tailored for AR-MDPs that can be used both in the generative model and the online setting. We prove that it is \((,)\)-PAC for any (possibly adaptive) sampling rule and give preliminary sample complexity guarantees.

## 2 Related work

In order to position our work in the literature on best policy identification in average-reward MDP, we start by recalling some properties of average-reward MDPs and give a formal definition of the different complexity measures that have appeared in the literature.

First of all, it can be more or less easy to travel through the MDP and visit some states. We call an MDP weakly communicating if there exists a closed set of states that are all reachable from every other state (meaning \(^{}\) such that for any \(s,s^{}\) in \(^{}\), \(_{:}[\{t,s_{t}=s^{ }\}|s_{0}=s, t^{},a_{t^{}}=(s_{t^{}})]<+\)) and a possibly empty set of states which are transient (meaning the return time \(\{t,s_{t}=s\}\) when the chain starts with \(s_{0}=s\) is not almost surely finite) under any policy. Furthermore, when there is no such transient state, the MDP is then communicating, and it is possible to define the diameter of the MDP to quantify how fast circulating in the MDP can be:

\[D=_{s s^{}}_{:}[\{t>0,s_{t}=s^{}\}|s_{0}=s, t^{},a_{t^{}}=(s _{t^{}})]\,.\]

Finally, if the MDP satisfies for any policy \(\) and states \(s,s^{}\) that \([\{t,s_{t}=s^{}\}|s_{0}=s, t^{},a_{t^{} }=(s_{t^{}})]<+\), then the MDP is ergodic. If the MDP satisfies this except for a possibly empty set of transient states, the MDP is then called unichain. In the following, the MDPs are all considered at least weakly communicating, though some results shall require some stronger assumptions, which we will specify. Notably, every mention of the diameter will assume communicating MDPs.

The diameter has appeared in previous upper and lower bound on the regret  and in the lower bound of  for best policy identification. The diameter is also used in the literature on the Stochastic Shortest Path (SSP) problem, in which one seek to minimize the sum of cost obtained before some goal state is reached (some details on this alternative framework are given in Appendix B). The SSP-diameter is defined as the maximum over states of the minimum expected steps to the goal state, and it appears in the sample complexity bounds given by . We will see in the following that there are multiple similarities between the SSP setting and our average-reward setting, the use of the diameter being only one of them.

For a policy \(:\), we define the gain \(g_{}(s)=_{T+}[_{t=0}^{T -1}r_{t}|s_{0}=s]\). By writing \(P_{}=(P(s,(s)))_{s}\), and introducing \(}=_{T+}_{t=1}^{T}P_{ }^{t-1}\) the asymptotic distribution matrix, as well as defining \(_{}(s)=[r(s,(s))]\), we have \(g_{}(s)=}(s)_{}\). Finally, we can define the bias vector \(b_{}=_{t=1}^{+}(P_{}^{t-1}-}) _{}\). Those vectors satisfy the so-called Poisson equations (see ):

\[\{(I-P_{})g_{}=0\\ g_{}+(I-P_{})b_{}=_{}. \]

We notice that the solution, \(b_{}\), is defined up to an additive element in \((I-P_{})\). We can then define \(g^{}\) the optimal gain and (up to an additive constant) \(b^{}\) the optimal bias satisfying

\[g^{}+b^{}(s)=_{a}\{(s,a)+p_{s,a}b^{}\}, \]

where \(p_{s,a}\) is the (row) vector of transition probabilities from \((s,a)\). We further define

\[H=_{s}b^{}(s)-_{s}b^{}(s)\]

as the span of the optimal bias. The optimal bias span is always smaller than the diameter , which motivates a line of work replacing \(D\) with \(H\) in existing regret bounds [2; 7]. These improved bounds are however obtained only when \(H\) is known. Similarly existing sample complexity bounds featuring \(H\) all require this knowledge, as discussed in more detail below.

In addition to the diameter \(D\) and the optimal bias span \(H\), we define the mixing time

\[t^{}_{}=_{}\{t 1:_{s}\|P_{}^{t}(s )-}(s)\|_{1}\}\]which has also appeared in previous sample complexity bounds. By default the maximum is taken over all stochastic policies \(:_{}\), and we denote by \(t^{}_{}\) the same quantity where the maximum is restricted to deterministic policies.

Multiple papers have considered the problem of finding an \(\)-optimal policy in average-reward with access to a generative model. We summarize existing results in Table 1.  and  derived worse case lower bounds for the problem by exhibiting MDPs on which a certain number of samples _must_ be collected to guarantee correctness of any \((,)\)-PAC algorithm. The literature has first focused on the mixing time as a measure of the complexity of MDPs [23; 11; 12; 16], until the mixing time-scaling lower-bound for the sample complexity was matched by , for an algorithm taking \(t^{}_{D}\) as input. Algorithms using the optimal bias span \(H\) were introduced more recently, by  and , and the lower bound scaling in \(H\) was matched up to logarithmic factors by . This side of the literature has mostly used the links between discounted and average-reward MDPs and has used that for known \(H\) it is possible to choose a discount \(\) (with \(1-\) of order \(/H\)) such that \(H\)-optimal policies in the discounted MDP are \(\)-optimal in the average-reward MDP . This idea leads to upper bounds of the sample complexity that scale with the (assumed known) upper bound on \(H\). To make these algorithms agnostic to the MDP, a natural question is therefore: can we find a tight upper bound on \(H\) from the data?

## 3 On the hardness of estimating \(H\)

In this section, we investigate the complexity of estimating the optimal bias span \(H\) and more specifically of finding upper bounds on this quantity.

**Definition 1**.: _We say an algorithm computes a \(\)-tight upper bound for the optimal bias span with probability \(1-\) when, on any MDP of optimal bias \(H\), it outputs \(\) such that with probability higher than \(1-\), \(H H+\)._

Theorem 1 shows that there exists an MDP on which the sample complexity of an algorithm finding a \(\)-tight upper bound with probability larger than \(1-\) can be arbitrarily large. As a consequence, there cannot exist a bound on this sample complexity depending solely on \(S\), \(A\), \(H\), \(\) and \(\).

  Algorithm & Bound & Setting \\ 
 & \(}(_{})^{2}}{e^{2}}(1/ ))\) & \(}}(s)}{S}\), \(\) known \\  & \(t^{}_{}\) over all policies, known \\ 
 & \(}(_{})^{2}}{e^{2}}( 1/))\) & \(t^{}_{}\), known \\ 
 & \(}(_{}}{e^{3}}(1/ ))\) & \(t^{}_{}\), known \\ 
 & \(}(_{})^{3}}{e^{2}}( 1/))\) & \(t^{}_{}\), known (policy gradient) \\ 
 & \(}(_{}}{e^{2}}(1/ ))\) & \(t^{}_{}\), known \\ 
 & \(}(}(1/))\) & \(H\) known \\ 
 & \(}([}{e^{2}}+AH}{e} ](1/))\) & \(H\) known \\ 
 & \(}(})\) & \(H\) known \\   This paper & \(}([}+S^{2}AD^{2}](1/ ))\) & no prior knowledge \\  
 & \((_{}}{e^{2}})\) & Lower bound (worse case) \\ 
 & \((}(1/))\) & Lower bound (worse case) \\   

Table 1: Existing sample complexity bounds in the generative model setting 

**Theorem 1**.: _For any \(<}}\), \(T>0\), \(\), there exists an ergodic MDP \(\) with optimal bias span \(H=1/2\), \(S=3\) and \(A=2\) such that any algorithm that computes a \(\)-tight upper bound for the optimal bias span with probability \(1-\) under a generative model assumption needs (in expectation) more than \(T\) samples in \(\)._

Sketch of proofTo give an idea of the proof, we first focus on the weakly-communicating setting, with the full proof and the necessary ergodicity transformation detailed in Appendix A. We use the family of MDPs \((_{R})_{0<R<1}\) displayed in Figure 1. Indeed, having \(R<1/2\) makes the full-line policy optimal; since moving from one state to the optimal final state (1) is very easy in this case, no state is drastically worse than another, hence a small optimal bias span, \(H=1/2\). However, \(R>1/2\) makes the dashed-line policy optimal; moving from state 3 to state 2 is more difficult as \(p\) is small, taking on average \(1/p\) time steps, and state 3 has a worse reward than state 2; thus state 3 is considered way worse than state 2, hence a high optimal bias span, \(H=R\). Thus, to bound \(H\) tightly, one needs to know whether \(R>1/2\) or \(R<1/2\), which can require many samples if \(R\) is very close to \(1/2\).

We remark that since the hard MDP instance in our proof has a fixed optimal bias span \(1/2\), we would have the same problem if we looked for algorithms trying to find \(\) such that, with high probability, \(H(1+)H\). Moreover, instead of looking for upper bound we could also consider the task of estimating \(H\) within a given margin of error and the same issue would arise.

While Theorem 1 does not preclude the existence of an algorithm that is agnostic to \(H\) and reaches a \(}((SAH/^{2})(1/))\) sample complexity, it still suggests that assuming a known upper bound on \(H\) is a lot to ask. In the next section we will see that for communicating MDPs such an assumption is not needed to attain a near-optimal sample complexity.

## 4 A near-optimal algorithm without prior knowledge

As we have seen, finding tight upper bounds on \(H\) is not feasible in finite time. In the literature on regret minimization, in which improved regret bounds featuring \(H\) have also been derived when \(H\) is known, two types of workarounds have been used. In the REGAL algorithm3,  propose to use a doubling trick to counter not knowing \(H\), at the expense of an additional factor of \(\) in the regret. The doubling trick method is not applicable to BPI problems, though. For communicating MDPs, the idea of plugging-in an upper bound on \(D\) which is also a (not necessarily tight) upper bound on \(H\) was first proposed by  and permits to match the minimax lower bound on the regret of  featuring \(D\). In this section, we translate this idea to the best policy identification setting.

We propose Diameter Free Exploration (DFE), stated as Algorithm 1, which combines a diameter estimation sub-routine that follows from the work of  with the state-of-the-art algorithm of

Figure 1: MDP \(_{R}\), the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.

 when (an upper bound on) \(H\) is known. Both components are described in details in Appendix B with their theoretical properties. More precisely, DFE first calls Algorithm 2 to output a quantity \(\) which satisfies \(D 4D\) with probability larger than \(1-/2\), using a random number of samples \(N\) from each state action pair \((s,a)\) that satisfies \(N=}(D^{2}((1/)+S))\). Then Algorithm 3 (which also uses uniform sampling) takes as input \(\) and is guaranteed to output a policy that is \(\)-optimal with probability larger than \(1-/2\), using a total number of samples of order \(}(}{^{2}}( ))\). This gives the following theorem.

**Theorem 2**.: _Algorithm 1 is \((,)\)-PAC and its sample complexity satisfies, with probability \(1-\),_

\[=}([}+D^{2}SA ]()+D^{2}S^{2}A)\.\]

From Theorem 4 of , there exists a communicating MDP with a sample complexity in \((}(1/))\). Therefore, the DFE algorithm is worse than the lower bound by a multiplicative factor \((1+D^{2}+}{ 1/})\). In particular, in the regime of small \(\), this algorithm is optimal up to logarithmic factors.

## 5 On the hardness of best policy identification in the online setting

In the online setting, many algorithms with regret guarantees have been designed, however to the best of our knowledge there exists no online algorithm that is guaranteed to output a policy \(\) satisfying \(g_{} g^{*}-\) with probability larger than \(1-\)4. In this section, we give some elements of explanation.

On the hardness of a regret to PAC conversionIn the finite-horizon setting,  gave a way to convert any algorithm guaranteeing sublinear regret into a BPI algorithm with finite sample complexity. However, in average-reward MDP, this conversion is not straightforward. Indeed, it hinges on the fact that selecting at random one of the policies played during the regret-minimizing algorithm should yield a good enough policy: the more time steps there are, the smaller the regret induced by the most recent policy is, thus the better the policy is. However, in our setting, we know that the empirical mean reward of a policy converges towards the gain of this policy, but the speed of this convergence is determined by the mixing time. Moreover, as shown in , this mixing time can be arbitrarily large. However, the regret is not _defined_ asymptotically. In , the regret at time \(T\) is defined as \(Tg^{*}-[_{t=1}^{T}r_{t}]\). In , the regret is \(_{^{*}}[_{t=1}^{T}r_{t}]-[_{t=1}^{T}r_{t}]\). As the mixing time can be big, it is possible that these regret measures are small for a large number of steps, but that the underlying policy is not anywhere near asymptotically optimal.

It is therefore not possible to preemptively define the number of steps necessary for the regret-minimizing algorithm to find an asymptotically good policy, and not just one that is good on the short term. For example, for any \(N\), \(N 2\), consider the MDP \(_{p}\) displayed in Figure 2 with \(p=\) and any \(p^{}<p\). While the dashed-line policy is asymptotically best, the full-lined policy is better for at least \(N\) time steps, and is not even \(\)-optimal, as we show in Appendix C.1. An algorithm guaranteeing low regret (over a slightly modified version of the MDP to guarantee ergodicity) could therefore take the full-line action for a number of steps independent of the known parameters \(S\) and \(A\), and the knowledge of \(N\) would be required to know how many samples are necessary for best policy identification. This means that a wrapper turning any regret minimization algorithm into a best policy identification algorithm would require estimating and incorporating this measure \(N\) somewhere, which argues against the existence of a straightforward wrapper as in the episodic setting. Similar arguments in the SSP setting have been brought up for example in .

A hardness resultAs the following lower bound shows, it is in fact not possible at all to have an online best policy identification algorithm that has a sample complexity bound polynomial in \(S\), \(A\) and \(H\). This result is as a counterpart of the hardness result proved by  in SSP-MDPs.

**Theorem 3**.: _For any \(S 4\), \(A 4\), \((0,)\), \((0,)\), and any \((,)\)-PAC online algorithm, there exists a weakly communicating MDP with \(S\) states, \(A\) actions, mean rewards in \(\), \(H S\) and \(D A^{S-1}/16\) such that if the algorithm starts in a given state \(s_{1}\), \([]=(}{})\)._

Sketch of proofFor \(j(S-1)^{A}\), we define the hard instance \(_{j}\) as in Figure 3. An agent in \(s_{2}\) must execute the precise sequence of actions \(j\) to get back to \(s_{1}\), which is the only state that generates non zero rewards. Since not learning this precise sequence would mean the agent will perform badly on one of the \(_{j}\), and therefore perform a very suboptimal policy, the agent will need to have a big enough sample size to at least see \(s_{2}\) with high probability when starting in \(s_{1}\). With a small value of \(p\), it is possible to make the sample complexity exponentially big, as we detail in Appendix C.2.

This hard MDP is actually inspired from the hard SSP-MDP instance considered by  to prove that sample-efficient learning is impossible in the online setting of SSP. Indeed, their hard MDP is one where the cost is always the same in each state, in which case we can easily transform the SSP problem into an average reward one, as shown in Figure 5. Indeed, if the costs of the SSP-MDP are all equal and non-zero, then finding an optimal policy in it can be show to be equivalent to finding an optimal policy in the corresponding AR-MDP in which the only reward is in the initial state. 

We remark that while the hard MDP instances \(_{j}\) used in our proof have an optimal bias span \(H\) that is upper bounded by \(S\), its diameter is exponential is \(S\). Hence while Theorem 3 implies that no online algorithm can get a \((})\) sample complexity (with or without the knowledge of \(H\)) on every instance and for every \(\), it does not rule out the possibility to have an online algorithm with a sample complexity scaling with \(D\). We propose such an algorithm in the next section.

Figure 3: MDP \(_{j}\), the hard instance for Theorem 3

Figure 2: MDP \(_{p,p^{}}\) with high mixing time.

[MISSING_PAGE_FAIL:8]

Now to propose a concrete stopping rule, we provide expressions of \(U\) and \(L\) chosen such that the premise of Theorem 5 is satisfied in every round \(n\), with high probability. \((p,q)\) denotes the Kullback-Leibler divergence between the probability vectors \(p\) and \(q\) on \(\).

**Definition 2**.: _Given a sequence of \((b_{n})_{n}\), we define_

\[U^{n}_{s,a}(b_{n};) = \{p^{}b_{n}N^{n}_{s,a}(^ {n}_{s,a},p^{}) x(,N^{n}_{s,a})\}\] \[L^{n}_{s,a}(b_{n},) = \{p^{}b_{n}N^{n}_{s,a}(^ {n}_{s,a},p^{}) x(,N^{n}_{s,a})\}\]

_for the threshold function \(x(,y)=(SA/)+(S-1)(e(1+y/(S-1)))\) and let_

\[_{,}=_{n}\{_{s}_{a}I^{n,}_{s,a}(b _{n},)-_{s}_{a}I^{n,}_{s,a}(b_{n};) \}. \]

The following result is a consequence of Theorem 5 and a KL-based time uniform concentration result for the transition probabilities from  (Lemma 5 in Appendix). Using Pinsker's inequality (see Remark 1 in Appendix where we discuss some computational aspects), we can also justify that this theorem hold for \(U^{n}_{s,a}\), \(I^{n}_{s,a}\) replaced by

\[^{n}_{s,a}(b_{n};)=^{n}_{s,a}b_{n}+||b_{n}||_{ }_{s,a})}{N^{n}_{s,a}}},\ \ ^{n}_{s,a}(b_{n} ;)=^{n}_{s,a}b_{n}-||b_{n}||_{}_{ s,a})}{N^{n}_{s,a}}}. \]

**Theorem 6**.: _For any sampling rule \(((s_{n},a_{n}))_{n}\), and any sequence \((b_{n})_{n}\) of bias vectors, the algorithm using the stopping rule \(=_{,}\) defined in (6) and recommending \((s)=_{a}I^{}_{s,a}(b_{};)\) satisfies \((<,g^{}-g_{}>)\)._

Theorem 6 shows that for any sampling rule, be it in the generative model or in the online setting, the stopping rule (6) and associated recommendation rule yields an \((,)\)-PAC algorithm, for any choice of bias vector sequence \(b_{n}\). Of course, bad choices of sampling rules and bias vectors could still yield \(=\) almost surely (e.g. picking always \((s_{t},a_{t})=(s_{1},a_{1})\) when a generative model is available).

Choosing a sampling ruleAs a sanity-check we analyze in Appendix D.2 an algorithm which combines the simplest possible sampling rule, uniform sampling from a generative model, with the VI stopping rule (6) where the sequence of biais vector is \(b_{n}=_{n}\) where \(_{n}\) is the optimal bias vector in the AR-MDP with rewards \(_{s,a}\) and transition probabilities given by \((^{n}_{s,a})_{s,a}\) (see Algorithm 4). We prove in Theorem 9 that in unichain MDPs, for sufficiently small \(\) the sample complexity of this algorithm is bounded with probability larger than \(1-\) by \(}()^{2}}{^{2 }}(+S))\), where \(_{}\) is some constant depending on the MDP. We refer the reader to Appendix D.2 for the definition of this constant, that has a complex expression and should be in most case larger than \(H\).

We remark that we could also use GOSPRL to turn Algorithm 4 into an online algorithm (using phases of increasing length in which we collect a uniform number of samples using GOSPRL and check our stopping rule), with a sample complexity essentially multiplied by \(D\). However the resulting sample complexity is likely to be much larger than \(SAD^{2}/^{2}\) in the small \(\) regime, making this algorithm not very interesting. We believe that to get efficient algorithms for the online setting it is important to depart from this uniform sampling + GOSPRL approach. Our stopping rule actually suggests some clever online choices that could be used to make the algorithm stop earlier. For example it could be interesting to make a greedy choice of the bias vector \(b_{n}\) that minimizes over \(b^{S}\) the quantity \(_{s}_{a}I^{n,}_{s,a}(b;)-_{s}_{a}I^{n,}_{s,a} (b;)\). Assuming we could compute this vector, a possible online sampling rule could be optimistic choice \(a_{n}=*{argmax}_{a}[_{s,a}+U^{n}_{s_{n},a}(b_{n}, )]\). We leave the analysis of such algorithms for future work.

## 7 Conclusion and perspective

We provided several elements indicating that the optimal bias span \(H\) may not be an adequate complexity measure for \(\)-optimal policy identification in an average-reward MDP. In the generative model setting, as all existing algorithms with sample complexity featuring \(H\) require prior knowledge of this quantity, we investigated the question of estimating \(H\) and proved that the sample complexity of this estimation task can be arbitrarily large. Then, in the online setting, we gave a lower bound on the sample complexity indicating that no algorithm can get a \(SAH/^{2}\) sample complexity on every MDP, with or without the knowledge of \(H\). On the algorithmic side, we proposed the first best policy identification algorithms for the generative model setting that does not require any form of prior knowledge on the MDP. By estimating the diameter \(D\) instead of \(H\), DFE attains a near-optimal sample complexity in \(SAD/^{2}\) for communicating MDPs. We further proposed an online variant of DFE with a slightly larger \(SAD^{2}/^{2}\) sample complexity, which is a factor \(D\) away from the worse case lower bound established in the generative model setting. We leave as an open question whether there exists online algorithms with a \(SAD/^{2}\) sample complexity. In future work, we will investigate whether the novel adaptive stopping rule proposed in our work can lead to this reduced sample complexity, when combined with a suitable online sampling rule.