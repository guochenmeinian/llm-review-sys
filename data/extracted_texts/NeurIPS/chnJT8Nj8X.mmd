# Transformer Doctor: Diagnosing and

Treating Vision Transformers

Jiacong Hu

Hao Chen

College of Computer Science and Technology, Zhejiang University,

Kejia Chen

School of Software Technology, Zhejiang University,

Yang Gao

Bingsheng Technology Co., Ltd.

Jingwen Ye

Electrical and Computer Engineering, National University of Singapore,

Xingen Wang

Mingli Song

Zunlei Feng

###### Abstract

Due to its powerful representational capabilities, Transformers have gradually become the mainstream model in the field of machine vision. However, the vast and complex parameters of Transformers impede researchers from gaining a deep understanding of their internal mechanisms, especially error mechanisms. Existing methods for interpreting Transformers mainly focus on understanding them from the perspectives of the importance of input tokens or internal modules, as well as the formation and meaning of features. In contrast, inspired by research on information integration mechanisms and conjunctive errors in the biological visual system, this paper conducts an in-depth exploration of the internal error mechanisms of Transformers. We first propose an information integration hypothesis for Transformers in the machine vision domain and provide substantial experimental evidence to support this hypothesis. This includes the dynamic integration of information among tokens and the static integration of information within tokens in Transformers, as well as the presence of conjunctive errors therein. Addressing these errors, we further propose heuristic dynamic integration constraint methods and rule-based static integration constraint methods to rectify errors and ultimately improve model performance. The entire methodology framework is termed as Transformer Doctor, designed for diagnosing and treating internal errors within transformers. Through a plethora of quantitative and qualitative experiments, it has been demonstrated that Transformer Doctor can effectively address internal errors in transformers, thereby enhancing model performance. For more information, please visit https://transformer-doctor.github.io/.

## 1 Introduction

In the field of machine vision, models based on Transformers  have gradually replaced convolutional neural networks as the mainstream approach. Particularly in recent years, various visual architectures improved upon Transformers have emerged incessantly , continuously pushing the performance boundaries of visual tasks. However, the vast and complex parameters of Transformers hinder researchers from gaining a deep understanding of their internal mechanisms ,thereby increasing the risks of applying them in sensitive domains . This has spurred a considerable amount of work aimed at investigating the interpretability of Transformers to enhance their transparency .

Existing research on the interpretability of Transformers in machine vision primarily focuses on aspects such as the importance of input tokens , the significance of internal modules , the the evolution and formation of features , and the meanings of intermediate representations . While these studies have somewhat improved the transparency of Transformers, the internal decision-making processes, such as mechanisms leading to errors, still warrant more systematic investigation. This is crucial for enhancing the transparency of Transformers and further improving their performance.

In fact, unlike in machine vision, theoretical studies on error mechanisms in biological vision have become quite mature . Specifically, in the perceptual process of the biological visual system, visual information such as the spatial position, shape, size, color, and texture of objects is processed and refined in the primary visual cortex . Subsequently, these different visual cues are integrated at higher stages for final recognition . Numerous studies have demonstrated that errors in object recognition may arise not only from failures in feature extraction but also from incorrect integration of correctly extracted features at higher stages . Errors resulting from the improper integration of features are termed conjunction errors . Furthermore, some research indicates that providing effective stimuli or cues during the integration process can enhance the correctness of information integration . Inspired by this, we are interested in investigating whether Transformers exhibit similar mechanisms of feature integration and conjunction errors as those observed in biological vision during recognition. If such errors exist, can they be corrected akin to the mechanisms observed in biological vision?

To address these questions, we first proposed the _information integration hypothesis_, inspired by the biological visual. This hypothesis posits that Transformers continuously process and refines various mixed information at the primary stage and integrate them at the higher stage. When incorrect information is integrated, i.e., conjunction error occur, it results in erroneous recognition outcomes. To validate this hypothesis, we conducted extensive experimental analyses of the computational process of Transformers and found empirical evidence supporting the hypothesis. Specifically, we discovered dynamic information integration among tokens in Transformer's Multi-Head Self-Attention (MHSA) component and static information integration within tokens in the Feed-Forward Network (FFN) component, along with the presence of conjunction errors. Furthermore, we elucidated the reasons behind both dynamic and static integration. Building upon this, we proposed heuristic dynamic integration constraints for inter-token information integration and rule-based static integration constraints for intra-token information integration, enabling the rectification of conjunction errors in Transformers. We coined the entire approach as "Transformer Doctor", where the process of identifying errors based on the information integration hypothesis is referred to as diagnosing the Transformer, and the process of applying the hypothesis to rectify errors is referred to as treating the Transformer. Finally, we conducted extensive quantitative and qualitative experiments on mainstream Vision Transformer architectures, thoroughly validating the effectiveness and applicability of Transformer Doctor.

The contributions of this paper can be summarized as follows:

* We propose Transformer Doctor, the first framework for diagnosing and treating Vision Transformers. This framework validates and utilizes the proposed Information Integration Hypothesis, which posits that Transformers process and encode various mixed information at primary stages and integrate it at higher stages. When information is not correctly integrated, i.e., conjunction error occur, it results in prediction failures.
* In diagnosing Transformers, we identify the mechanisms of inter-token information dynamic integration and intra-token information static integration within Transformers, along with the occurrence of conjunctive errors. This provides a novel perspective for understanding the internal mechanisms of Vision Transformers.
* In treating Transformers, we propose heuristic dynamic constraints for inter-token information integration and rule-driven static constraints for intra-token information integration. These constraints offer an interpretable solution for optimizing Vision Transformers without introducing additional parameters or computational overhead during inference.

* Extensive qualitative and quantitative experiments are conducted on mainstream Vision Transformers, validating the effectiveness and applicability of the Transformer Doctor.

## 2 Related Works

In methods aimed at interpreting or understanding Transformers, whether in the field of machine vision or natural language processing, the primary approaches focus on the importance of input tokens [7; 12; 13; 9; 14; 10], the significance of internal modules , the the evolution and formation of features [16; 17], and the meanings of intermediate features [18; 19; 20] to understand the internal mechanisms of Transformers. Specific methodologies can be categorized as feature-based, attention-based, gradient-based, propagation-based, perturbation-based, projection-based, or a combination of these approaches. For instance, in feature-based methods, the primary focus is on analyzing or statistically evaluating the intermediate features within Transformers  to understand the internal representation structure and feature distribution . Attention-based methods mainly utilize the raw attention weights [7; 12; 13] or linear combinations of multi-layer attention weights  to compute the relative importance of input tokens. Gradient-based methods focus on computing gradients of attention weights [32; 33], intermediate features [11; 34], or inputs  to understand the differences in token importance. Propagation-based methods employ techniques like Layer-wise Relevance Propagation (LRP)[35; 36] for attribution analysis of input tokens[10; 37; 38; 39] or investigate the importance of heads in Transformers . Perturbation-based methods involve perturbing inputs or features and measuring the impact on model performance [40; 41; 42; 43] or Shapley value [14; 44]. Projection-based methods, such as linear probes [17; 45; 46], project intermediate representations into human-understandable spaces to comprehend the mechanism and significance of feature transformations in Transformers [18; 19; 20]. In contrast to the aforementioned research, this paper is inspired by studies on biological visual error mechanisms, aiming to explore whether Transformers exhibit similar mechanisms of information integration and connection errors as in biological vision, and how to rectify errors within Transformers.

In methods for improving Transformers, most approaches involve modifying the model architecture by introducing learnable parameters [47; 3; 4; 5; 6] or enhancing data and features [48; 49; 50; 51; 52]. These improvement methods are mostly non-interpretable and are pre-defined before training, rather than targeting further enhancement of model performance from the perspective of diagnosing and treating internal error mechanisms based on existing models. Additionally, there are methods for improving models that do not require pre-definition before training but focus on non-Transformer or non-vision tasks, such as debugging and analyzing models in traditional machine learning [53; 54; 55; 56], optimizing deep models [57; 58], and editing facts in natural language processing [59; 60; 61; 62]. However, due to significant differences in architecture and tasks, these methods are not suitable for analyzing and correcting error mechanisms in Transformers to improve model performance.

In summary, this paper is the first work to investigate whether Transformers exhibit information integration and connection error mechanisms similar to biological vision, and the first work to explore how to further enhance model performance by diagnosing and treating internal errors in existing models.

## 3 Information Integration Hypothesis

In this section, we firstly propose the Information Integration Hypothesis for Transformers. Subsequently, we review the MHSA and FFN modules within the Transformer architecture, followed by an analysis of potential locations where the Information Integration Hypothesis may apply.

_Information Integration Hypothesis: Similar to biological vision, in machine vision, the Transformer continually processes and refines various mixed information in the primary stage, and integrates it in the advanced stage. When erroneous information is integrated, i.e., conjunction errors occur, it leads to incorrect predictions._

### Potential Information Integration in MHSA

When utilized for visual recognition tasks, a Transformer typically comprises \(L\) blocks, each consisting of an MHSA module and an FFN module. The input to the block can be represented as 

[MISSING_PAGE_EMPTY:4]

## 4 Information Integration Hypothesis based Transformer Diagnosis

In this section, we delve into the potential existence of information integration hypothesis mentioned in Section 3 within the MHSA and FFN. Through extensive experimental analysis, we have gathered empirical evidence supporting the hypothesis, namely dynamic integration of information among tokens and static integration of information within tokens.

### Inter-token Information Dynamic Integration

To explore the potential existence of the information integration hypothesis within the MHSA, we analyzed the integration weights \(\) in Eqn. (2). Fig. 2(a) illustrates the magnitudes of integration weights \(\) in blocks of different depths within the model. It can be observed that for high-confidence samples, the integration weights \(\) in shallower blocks exhibit a diagonal pattern. However, as the depth of the block increases, the integration weights \(\) display a vertical pattern, consistent with observations from prior studies . Additionally, we observed that for different high-confidence samples, the positions of the vertical lines in the integration weights \(\) within the deeper blocks vary, as shown in Fig. 8 in the Appendix. This indicates that in the initial stages, MHSA primarily mixes and processes information between adjacent tokens. In the later stages, MHSA dynamically and selectively integrates specific information among tokens.

Continuing, we extracted an arbitrary row \(_{i}\) from the integration weights \(\) within the deeper blocks and removed the [CLS] token. After reshaping and resizing, we overlaid it onto the original image to generate a heatmap, as shown in Fig. 2(b). From the heatmap, it is evident that the positions of the vertical lines mainly concentrate on the foreground of the input image. This suggests that in the advanced stages of the model, MHSA primarily integrates specific information among tokens containing foreground elements. However, for low-confidence samples, as depicted in Fig. 1(c) and (d), the deeper layers of the MHSA erroneously integrate information corresponding to the background tokens. Additional quantitative and qualitative analyses of the integration weights \(\) are presented in Appendices C and B. In summary, we have identified the first evidence of the existence of the information integration hypothesis in Transformer models:

_Evidence 1: In the initial stages of the Transformer, MHSA primarily mixes and processes adjacent patch information among tokens. However, in the advanced stages of the Transformer, MHSA dynamically and selectively integrates specific patch information among tokens. When integrating incorrect information among tokens, termed conjunction errors, it leads to model mispredictions. We refer to this integration as inter-token information dynamic integration. The term "dynamic" arises from the fact that the \(\) to be integrated, as specified in Eqn. (2), varies with each sample._

### Intra-token Information Static Integration

To explore the potential existence of the information integration hypothesis within the FFN, we conducted visual analysis of the integration weights \(\) in shallow and deep blocks, as depicted in Fig. 3. Fig. 3(b) illustrates the patterns of integration weights \(\) in shallow and deep blocks for high-confidence samples from different classes. It can be observed that the patterns of integration weights differ between shallow and deep blocks for samples from different classes. However, in Fig. 3(a), for high-confidence samples from the same category, the patterns of integration weights \(\) in shallow blocks differ, while the patterns in deep blocks show less variability and remain relatively

Figure 2: Visual comparison of integration weights \(\) in MHSA. (a) and (b) respectively present visualizations of weights \(\) at different depths of blocks for high-confidence images and the overlay of reshaped and resized rows of \(\) onto the original image. Similarly, (c) and (d) depict visualizations of weights \(\) for low-confidence images and their overlay onto the original image.

consistent. This indicates that in the initial stages of the model, FFN mixes and processes various category information within tokens. However, in the advanced stages, FFN statically selectively integrates specific category information within tokens.

Furthermore, as depicted in Fig. 3(d), even for samples from the same class, the patterns of integration weights \(\) in deep blocks differ for low-confidence images. This suggests that when the deep FFN fails to correctly integrate specific category information within tokens, it adversely affects the model's predictions. Additional quantitative and qualitative analyses of the integration weights \(\) are presented in Appendices E and D. In summary, we have identified the second evidence of the information integration hypothesis in Transformers:

_Evidence 2: In the initial stages of the Transformer, the FFN primarily mixes and processes various low-level information within tokens. However, in the advanced stages of the Transformer, the FFN statically and selectively integrates specific category information within tokens. When integrating incorrect information within tokens, termed conjunction errors, it leads to model mispredictions. The term "static" arises from the fact that the \(W^{(2)}\) to be integrated, as specified in Eqn. (4), is fixed relative to each sample, and for samples of the same class, the integration weights \(\) are also fixed._

## 5 Information Integration Hypothesis based Transformer Treatments

In this section, we propose heuristic dynamic integration constraints and rule-based static integration constraints to correct conjunction errors in information integration, aiming to enhance model performance.

### Heuristic Information Dynamic Integration Therapy

In order to alleviate conjunctive errors in dynamic integration of information among tokens, we heuristically constrain the integration weights \(\) by highlighting the foreground of images with low confidence scores. Furthermore, unlike the single-head integration weights \(\) in Eqn. (2), we have improved the calculation of integration weights for multi-head scenarios using gradients.

Specifically, for a Transformer classification model with \(K(K 2)\) classes, let \(k\{1,2,...,K\}\) represent the true label for input, and \(p^{K}\) denote the predicted probabilities of all classes by the Transformer. We incorporate gradients related to the true class to discern the importance of each head out of \(H\) heads in self-attention, thereby obtaining the integration weights \(}^{N N}\) in the multi-head scenario:

\[}=_{h=1}^{H}{((}{ ^{h}},0)^{h})}\] (5)

where \((.)\) denotes setting negative values in the derivative to zero, thereby considering only the positive impact on the predicted probability of the true class. The introduction of gradients in Eqn. (5) not only helps discern which head is important but also establishes a connection between integration weights and specific classes, thereby making the weights reflecting the integration of information among tokens more accurate. Details of the comparison experiments on the introduction of gradients

Figure 3: Visual comparison of integration weights \(\) in FFN. (a) and (b) respectively illustrate visualizations of weights \(\) for high-confidence samples of same classes and the different class in shallow and deep blocks. Similarly, (c) and (d) depict visualizations of weights \(\) for low-confidence samples of different classes and the same class in shallow and deep blocks. Each row in the image represents a sample, and each column represents a dimension.

can be found in Section 6.2. Next, for foreground annotation \(t^{N}\) with low confidence, we constrain the integration of background information using the loss function \(_{IDI}\):

\[_{IDI}=_{j=1}^{N}{((_{i=1}^{N}}_{i,j})(1- t_{j}))},\] (6)

where \(t^{N}\) is a binary annotation, with 1 and 0 representing the presence and absence of foreground within the token, respectively.

### Rule-based Information Static Integration Therapy

To correct conjunctive errors in static integration of information within tokens, we first establish integration rules within tokens when the model prediction is correct, and then constrain the integration weights \(\) based on these rules. Additionally, we have also improved the integration weights \(\) using gradients, establishing a connection with the true class \(k\), resulting in the new integration weights \(}^{N M}\) as follows:

\[}=(}{},0) .\] (7)

Next, we select \(S\) high-confidence samples for each class to calculate the average integration weights \(}^{N M}\), and then establish binary integration rules \(r^{N M}\) for each class using a threshold \(\):

\[r=(}),\,}=_{s=1}^{S}}_{(s)}.\] (8)

The values in \(r\) are \(1\) or \(0\), indicating the integration and non-integration of information in the corresponding dimension, respectively. Finally, based on the integration rules \(r\) for a certain class, we enforce that erroneous information within tokens is not integrated using the loss function \(_{ISI}\):

\[_{ISI}=_{m=1}^{M}_{n=1}^{N}(}_{n,m}(1- r_{n,m})),\] (9)

During actual enforcement, each training sample needs to be constrained using the integration rules corresponding to its true class.

### Joint Therapy of Dynamic and Static Integration

The loss functions \(_{IDI}\) and \(_{ISI}\) can be individually combined with the original loss function or used jointly:

\[_{total}=_{ori}+_{IDI}+_{ISI},\] (10)

where \(\) and \(\) are used to balance the magnitudes of the loss functions. In our practical experiments, we found that using the loss functions \(_{IDI}\) and \(_{ISI}\) sequentially yielded the most effective results. It is important to note that the therapy model only rectifies conjunctive errors in the Transformer without altering its architecture or operational procedure. Thus, during inference, it does not incur any additional computational overhead.

## 6 Experiments

### Experimental Settings

**Datasets and Transformer Architectures.** To validate the effectiveness of Transformer Doctor, we conducted experiments on five mainstream datasets: CIFAR-10 , CIFAR-100 , ImageNet-10 , ImageNet-50 , and ImageNet-1k . Furthermore, we performed experiments on various Transformer architectures used for visual classification tasks, including DeiT , CaiT , TNT , PVT , Eva , and BeiT , in addition to ViT . It is important to note that Transformer Doctor diagnoses and treats already trained Transformer models. More experimental settings and results can be found in Appendix.

**Parameter Settings.**During all training stage, each dataset was trained for 300 epochs using the AdamW  optimizer, with an initial learning rate of 0.01. The learning rate decayed according to a 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

Conclusion

This paper introduces the first framework, Transformer Doctor, designed for diagnosing and treating internal errors within Transformers simultaneously. Specifically, distinct from existing post-hoc interpretability methods for Transformers, this work draws inspiration from information integration and conjunctive errors in the biological visual system, proposing and validating the hypothesis of information integration for Transformers. Furthermore, addressing conjunctive errors within information integration, this paper presents corresponding error treatment methods. Extensive qualitative and quantitative analyses conducted on mainstream datasets and Transformer architectures demonstrate the effectiveness and applicability of Transformer Doctor.

**Limitations and Future Work.** It is undeniable that we have only validated Transformer Doctor on mainstream visual recognition tasks, leaving more complex machine vision tasks for further exploration. Furthermore, investigating whether the information integration hypothesis holds true in the field of natural language processing or multimodal domains is also a worthwhile research endeavor. Additionally, exploring more error mechanisms and developing a more automated and intelligent Transformer Doctor framework are the focal points of our future work.