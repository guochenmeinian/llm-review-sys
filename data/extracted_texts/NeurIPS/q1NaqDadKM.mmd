# LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large Vision-Language Models (LVLM) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of \(8\) representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates \(6\) categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on \(40\) standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, Instruction-tuned LVLM with massive in-domain data such as InstructBLIP may overfit many existing tasks, generalizing poorly in the open-world scenario. Second, Instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDER for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework could mitigate the issue of object hallucination, shedding light on developing an effective metric for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. The evaluation pipeline will be available at vlarena page.

## 1 Introduction

Large Language Models (LLMs), such as LLaMA , GPT-3 , and Vicuna , have demonstrated remarkable progress in Natural Language Processing (NLP). These models leverage large-scale pre-training data and huge networks to achieve impressive results in NLP benchmarks. Recently, GPT-4  further expanded the impact to the multimodal community, stimulating the rapid development of large vision-language models (LVLMs) and revolutionizing the landscape of artificial intelligence.

Large Vision-Language Models (LVLM) have achieved remarkable progress in multimodal vision-language learning for various multimodal tasks such as visual question answering and multimodal conversation. Specifically, LVLMs capitalize on the knowledge from LLMs and effectively align visual features with the textual space. Flamingo , a pioneering LVLM, integrates visual features into LLMs through cross-attention layers. Later studies proposed more efficient vision-text interactions , more efficient training methods [7; 8], and employing instruction tuning [9; 7; 9; 10; 11; 12; 13; 8].

However, despite the great success, few efforts have been made to provide systematic evaluations of LVLMs. But evaluation plays a critical role in understanding the strengths and weaknesses of LVLMs,thereby guiding their future development. Recent work  presents a systematic investigation of object hallucination of LVLMs by proposing a polling-based object probing evaluation method. Moreover, ImageNetVC  studies how well LVLMs can master visual commonsense knowledge. Liu et al.  comprehensively evaluate the performance of LVLMs in visual recognition with text recognition, such as optical character recognition. GVT  evaluates LVLM's visual semantic understanding and fine-grained perception capabilities. Nevertheless, these studies only evaluate a portion of LVLMs on specific tasks, lacking an overall understanding of LVLM's capabilities.

In pursuit of a comprehensive evaluation of LVLMs, we build an LVLM Evaluation hub (LVLMs-eHub) consolidating \(8\) representative LVLMs such as InstrucBLIP and MiniGPT-4. The detailed information about model configuration and training data is listed in Table 1. Our LVLM-eHub consists of a quantitative capability evaluation and an online arena platform, providing a thorough investigation of the selected LVLMs. Specifically, the quantitative capability evaluation extensively evaluates 6 categories of multimodal capabilities of LVLMs including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence (see Fig. 1 (a)), by collecting \(40\) standard text-related visual benchmarks. On the other hand, the online arena platform features anonymous randomized pairwise battles in a crowd-sourced manner, providing a user-level model ranking in the open-world question-answering scenario (see Fig. 1 (b)).

Our LVLM-eHub comprehensively evaluates LVLMs, revealing several innovative findings. (1) Instruction-tuned LVLM with massive in-domain data suffers from overfitting and generalizes poorly in open-world scenarios, such as InstructBLIP (see Fig. 1 (a)). (2) With moderate instruction-following data, Instruction-tuned LVLM may cause object hallucination issues, generating objects that are inconsistent with target images in the descriptions. This leads to incorrect answers or renders current evaluation metrics, such as CIDER for image captioning, ineffective. (3) We find that a multi-turn reasoning evaluation pipeline can mitigate the issue of object hallucination, indicating that developing an effective metric for LVLM evaluation is urgent.

The contributions of our work are summarized follows. (1) We propose LVLM-eHub which is the first comprehensive evaluation benchmark for large vision-language models, to our best knowledge. (2) LVLM-eHub provides extensive evaluation on 6 categories of multimodal capabilities of LVLMs in more than \(40\) text-based visual tasks. (3) LVLM-eHub builds an online arena platform for LVLMs, which features anonymous randomized pairwise user-level comparison in a open-world scenario. (4) Our evaluation results reveal several innovative findings, providing a foundational framework for the assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques.

## 2 LVLM Evaluation Hub

In this section, we introduce representative LVLMs, multimodal capabilities of interest, and evaluation methods. The whole LVLM Evaluation Hub is illustrated in Fig. 2. Our LVLM evaluation hub

Figure 1: Comparative analysis of LVLMs within the LVLM eHub. (a) illustrates the variances in quantitative capability performance across six distinct aspects among LVLMs. (b) presents the Elo rating ranking of LVLMs within the LVLM Arena.

*  compromises \(8\) representative models including BLIP2 , LLaVa , LLaMA-Adapter V2 , MiniGPT-4 , mPLUG-Owl , Otter , InstructBLIP , and VPGTrans . All models boost vision-language representation learning by utilizing pre-trained image encoders and large language models (LLM). But they differ in training data scale and model configuration as shown in Table 1. For a fair comparison between LVLMs, we collect their checkpoints with parameter sizes less than 10B. The detailed descriptions of these models are in the Appendix.A.

### Quantitative Capability Evaluation

We aim to evaluate LVLMs' capability comprehensively. In particular, we summarize \(6\) categories of capabilities and collect corresponding benchmarks for quantitative evaluation (see Fig.2). Please see our supplementary materials for more statistics and details of the collected benchmarks.

**Visual Perception.** Visual perception is the ability to recognize the scene or objects in images, the preliminary ability of the human visual system. We evaluate this capability of models through image classification (ImgCLs) using the ImageNet1K , CIFAR10 , Pets37  and Flowers102  benchmarks, multi-class identification (MCI) and object counting (OC) using the GVT  benchmark. ImgCLs and MCI measure how well an LVLM grasps high-level semantic information, while OC assesses the recognition ability for fine-grained objects.

**Visual Knowledge Acquisition.** Visual knowledge acquisition entails understanding images beyond perception to acquire knowledge. This evaluation is conducted through Optical Characters Recognition (OCR) using twelve benchmarks (including IIIT5K , IC13 , IC15 , Total-Text , CUTE80 , SVT , SVTP , COCO-Text , WordArt , CTW , HOST , WOST ), Key Information Extraction (KIE) using the SROIE  and FUNSD , and Image Captioning (ImgCap) using two benchmarks (including NoCaps  and Flickr30K ). The OCR task measures whether a model can accurately identify and extract text from images or scanned documents. The KIE task further poses challenges in extracting structured information from unstructured or semi-structured text. Finally, ImgCap assesses whether a model can generate a good natural language description of the content of an image.

**Visual Reasoning.** Visual reasoning requires a comprehensive understanding of images and related texts. To evaluate the visual reasoning ability of LVLMs, we utilize three tasks including visual question answering (VQA), knowledge-grounded image description (KGID), and visual entailment SNLI-VE ), two benchmarks (i.e. ScienceQA  and VizWiz  ) and one benchmark (i.e. SNLI-VE), respectively. These three tasks are in VQA form in different domains. A capable LVLM should be able to understand the objects and scenes in an image and can reason to generate answers that are semantically meaningful and relevant to the question asked.

**Visual Commonsense.** Visual commonsense refers to the general visual knowledge commonly shared across the world, as opposed to the visual information specific to a single image. This evaluation tests

    &  &  &  \\   & VE & LLM & Adapt & Top & TuP & \# Token & Source & Size & Source & Size \\  BILP2 & ViT-9/14\({}^{1}\) & FlanT5-XL\({}^{1}\) & Q-Former & 4B & 107M & 32 & CC-V-SBU-L40 & 129M & - & - \\ LLaVA & ViT-U14\({}^{1}\) & ViVana & F-U2P & 7B & 7B & 7B & 256 & CC-M & 595K & LLaVA-1 & 158K \\ LA-V2 & ViT-U14\({}^{1}\) & LLaMA\({}^{1}\) & B-Tuning & 7B & 63.1M & 10 & L400 & 200M & LLaVA-1+G4L & 210K \\ MiniGPT-4 & BILP2-ViT & ViVana1 & FC layer & 7B & 3.1M & 32 & CC-SBU-L400 & 5M & CC-CutMgr & 3.5K \\ mPLUG-Owl & ViT-U14\({}^{1}\) & LLaMA\({}^{1}\) & ICRA & 7B & 1.1B & 65 & CC-V-C4-L400 & 201M & LLaVA-1 & 158K \\ Otter & ViT-U14\({}^{1}\) & LLaMA\({}^{1}\) & Resumpler & 9B & 1.3B & 64 & - & - & 1LaVA-1 & 158K \\ InstructBLIP & ViT-9/14\({}^{1}\) & ViVana1 & Q-Former & 7B & 107M & 32 & - & QA\({}^{*}\) & 16M \\ VPGTrans & ViT-9/14\({}^{1}\) & ViVana1 & Q-Former & 7B & 107M & 32 & COCO-V-SBU & 13.8M & CC-ChatrGPT & 3.5K \\   

Table 1: **Comparison of Different LVLMs.** ‘VE’, ‘Adapt’, ‘ToP’, ‘TuP’, and ‘# Token’ represent the visual encoder, adaption module, number of total parameters, tuning parameters, and visual tokens fed into the text encoder, respectively. \({}^{1}\) indicates that the model is frozen. CC\({}^{*}\) consists of COCO , CC3M , and CC12M . CC, VG, SBU CY, and L400 indicate Conceptual Caption , Visual Genome , COYO-700M  and LAION 400M , respectively. LLaVA-I and G4L represent 158K multimodal instruction-following data in LLaVA  and data generated by GPT-4 for building an instruction-following LLMs . QA\({}^{*}\) denotes 13 question-answering datasets in InstructBLIP . We count all the data and tuning parameters needed to convert the pretrained vision model and LLM into a visual instruction model. The average score is obtained by normalizing over each row and taking the average of each column.

the model's understanding of commonly shared human knowledge about generic visual concepts using ImageNetVC  and visual commonsense reasoning (VCR) . Specifically, ImageNetVC is utilized for zero-shot visual commonsense evaluation, such as color and shape, while VCR covers various scenes, such as spatial, casual, and mental commonsense.

**Embodied Intelligence.** Embodied intelligence aims to create agents, such as robots, which learn to solve challenging tasks requiring environmental interaction. Recently, LLM and LVLM exhibited exceptional effectiveness in guiding the agent to complete a series of tasks. In this evaluation, we utilize high-level tasks as in EmbodiedGPT  and employ Minecraft , VirtualHome , Meta-World , and Franks Kitchen  as benchmarks.

**Object Hallucination.** It is known that LVLM suffers from the object hallucination problem, i.e., the generated results are inconsistent with the target images in the descriptions . Evaluating the degree of object hallucination for different LVLMs help understand their respective weaknesses. To this end, we evaluate the object hallucination problem of LVLMs on the MSCOCO dataset .

### Online Evaluation with LVLM Arena

Designing quantitative evaluations for LVLM to satisfy all capabilities is challenging, as evaluating LVLM responses constitutes an open-ended problem. Inspired by FastChat , we introduce the LVLM Arena, an online evaluation framework for LVLMs' pairwise battle with human judgment.

Figure 2 illustrates the LVLM Arena, comprising three primary components: matchmaking, chat, and voting. Initially, two models are sampled from the model zoo. Users then converse side-by-side with the models, who remain anonymous. Subsequently, users vote for the superior model.

Figure 2: Our evaluation encompasses quantitative evaluation and online LVLM Arena. Plentiful benchmarks are employed to comprehensively evaluate the six critical capabilities of the models in the quantitative evaluation. In the LVLM Arena, an online platform, users can participate in an online evaluation by chatting with two anonymous models and choosing their preferred model.

**Matchmaking.** The matchmaking module samples two models in a tournament style based on their Elo rating. However, due to the currently limited size of the model hub, we employ random sampling.

**Chat.** Users chat side-by-side with two sampled models (which remain anonymous) using images or text inputs. Different from quantitative evaluation, users can chat about anything. Our existing online platform supports only single-round chats due to multi-round chats' high computational and memory demands. Future updates will address this constraint.

**Voting.** After the chat session, users vote for their preferred model. Four options are available: Model A, Model B, Tie, and Both are bad. The Elo rating is subsequently updated using voting results.

In contrast to limited quantitative evaluations, the LVLM Arena provides an open-world evaluation framework that enables users to chat with models about anything, emulating real-world conditions. Besides, users serve as the judge for the battle, which brings more convincing evaluation results than traditional evaluation metrics.

### Zero-shot Evaluation

LVLMs are capable of capturing a wide range of multimodal patterns and relationships. We evaluate the above \(6\) categories of capabilities of LVLMs by investigating their zero-shot performance on various tasks. Zero-shot evaluation allows us to evaluate the LVLMs' ability to generalize to new tasks without training the model, which is competent for large-scale evaluation. To be specific, we treat the zero-shot evaluation as various forms of prompt engineering for different tasks (see Fig. 3) as presented in the following.

* _Question Answering._ Prompting with visual question answering can be used to solve many downstream tasks, which assess how well an LVLM understands the underlying language and visual features. We design proper prompts to ensure that the LLM can produce meaningful results. For example, text prompts of OCR can be _"what is written in the image?"_. Then, we evaluate the answers generated by the LLM using the corresponding metric such as accuracy.
* _Prefix-based Score._ For multi-choice QA tasks, we can utilize a visual encoder to obtain visual prompts for a given image. Then, the visual prompts are prefixed into the text embeddings, which are fed into the LLM. The likelihood of image-text pair can be generated, which is referred to as a prefix-based score. We can obtain a prefix-based score for each text prompt of the candidate's answer. The answer with the largest prefix-based score is selected as the final answer. We provide the formulation in Sec. A.3 of Appendix.
* _Multi-turn Reasoning._ Following IdealGPT , we use a multi-turn reasoning framework to evaluate complex visual reasoning tasks. Specifically, we utilize an LLM such as ChatGPT to generate sub-questions for a given question, an LVLM to provide corresponding sub-answers, and

Figure 3: Illustration of our adopted evaluation methods. To evaluate the zero-shot performance of LVLMs on diverse downstream tasks, we employ four methods including question answering, prefix-based score, multi-turn reasoning, and user study.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

benchmarks in a zero-shot setting, including ImageNetVC and Visual Commonsense Reasoning (VCR). The evaluation details of tasks are demonstrated in Appendix.B.4. As shown in Table 5, we can find that all those LVLMs represent their abilities to solve visual commonsense problems. First, InstructBLIP performs best (68.41%) among those LVLMs on the ImageNetVC dataset. The main reason is that it is fine-tuned on 1.6M fine-grained VQA data, making it adapt to answer visual common questions. Second, LLaMA-Adapter V2 (46.20%) and LLaVA (46.20%) show the same best performance among those LVLMs on the VCR dataset. The main reason is that instruction-flowing data is used to update the LLM. Note that the final answer of VCR is obtained by multi-turn reasoning. It also shows the significant role of a good evaluation scheme in producing promising content for instruction-tuned models.

### Results on Object Hallucination

Although LVLMs have made significant progress, they still struggle with the issue of hallucination, which refers to their tendency to produce objects that do not align with the descriptions provided in the target images. In this section, we focus on evaluating such object hallucination problems on MSCOCO captioning dataset. Following POPE  evaluation pipeline which is a multi-step QA procedure, we prompt LVLMs with multiple Yes-or-No questions. For example, _'Is there a person in the image?'_. We use accuracy as the evaluation metric. From Table 6, we could come to the following conclusions. InstructBlip performs best in the hallucination problem, followed by BLIP2, whose average accuracy both reached more than 80%. We find that instruction-tuned models, except for InstructBLIP, perform worse than BLIP2 because they tend to answer 'Yes' to the question, which shows that LVLMs are prone to generate objects frequently occurring in the instruction data. Such object hallucination problem can be alleviated by a multi-turn reasoning pipeline shown in the experiments on SNLI-VE and VCR.

### Results on Embodied Intelligence

In this section, we present the evaluation results focusing on embodied intelligence. To appraise the effectiveness of planning outputs using the given image, we conducted a user study involving 15 participants. The study comprised 6 household scenarios carefully selected from VirtualHome . Specifically, the participants rated the generated plans from different LVLM models using a scoring system similar to . The evaluation comprised five dimensions with scores ranging from 1 to 5. These dimensions included object recognition accuracy, spatial relationship understanding, level of conciseness in the response, reasonability of the planning, and executability of the planning. The resulting average scores for the different models among the participants are presented in Table 7 below. Furthermore, in the Appendix C, we present quantitative evaluation results for Franka Kitchen , Minecraft , and Meta-World . Based on the evaluation results, we observe that visual

    & BLIP2 & InstructBLIP & LA-V2 & LLaVA & MiniGPT-4 & mPLUG-Owl & Outer & VPGTrans & S-SOTA \\   & Random & 82.21 & **88.83** & 74.44 & 51.52 & 52.58 & 40.65 & 61.40 & 47.92 & - \\  & Popular & 80.10 & **84.15** & 56.82 & 50.00 & 49.31 & 38.82 & 49.56 & 47.64 & - \\  & Adversarial & 78.52 & **81.95** & 60.52 & 50.00 & 49.62 & 38.04 & 50.68 & 45.95 & - \\   & 0.945 & **1.00** & 0.750 & 0.595 & 0.594 & 0.461 & 0.633 & 0.555 & - \\   

Table 6: Evaluation results of POPE  performance of LVLMs on MSCOCO. The accuracy is used to assess the performance.

    & BLIP2 & InstructBLIP & LA-v2 & LLaVA & MiniGPT-4 & mPLUG-Owl & Outer & VPGTrans & S-SOTA \\   & Color & 44.60 & **67.79** & 23.16 & 41.92 & 26.57 & 25.56 & 26.21 & 24.72 & 44.70 \\  & Shape & 40.13 & **59.06** & 28.16 & 38.74 & 22.88 & 30.72 & 34.19 & 24.69 & 40.50 \\  & Mater. & 61.49 & 63.58 & 32.51 & **64.91** & 29.50 & 34.24 & 35.81 & 27.21 & 61.90 \\  & Compo. & 53.86 & **83.25** & 50.38 & 58.53 & 59.96 & 49.47 & 50.72 & 57.21 & 54.00 \\  & Others & 51.50 & **68.37** & 32.64 & 59.06 & 38.86 & 35.11 & 34.39 & 36.39 & 51.70 \\  & Avg & 50.30 & **68.41** & 33.37 & 52.63 & 35.55 & 35.02 & 36.26 & 34.04 & 50.50 \\  VCR & VCR & 36.80 & 45.60 & **46.20** & **46.20** & 44.40 & 39.40 & 39.60 & 39.60 & - \\   & 0.747 & **0.994** & 0.567 & 0.807 & 0.581 & 0.564 & 0.581 & 0.546 & - \\   

Table 5: Comparisons of Zero-shot visual commonsense Performance for LVLM Models on VCR and ImageNetVC datasets. Top-1 accuracy is employed for the two datasets.

instruction data is essential for embodied tasks. BLIP2 lacked visual instruction tuning, which greatly affected its capability to produce reasonable and executable plans.

### Results on Online Arena Evaluation

The arena features anonymous and randomized pairwise battles in a crowd-sourced manner. We have collected 634 pieces of evaluation data since we launch the LVLM arena. The collected data shows almost the same number of battle outcomes for 'Model A wins' and 'Model B wins.' Moreover, \(21.8\%\) battle outcomes are voted as 'both are bad,' implying that the current LVLMs still struggle to generate good answers for open-world visual questions. Furthermore, we rank the selected \(8\) LVLMs with Elo rating  using the collected data by following Fastchat  and . As shown in Fig. 1 (b), mPLUG-Owl, MiniGPT-4, and Otter, which are fine-tuned with amounts of instruction-following data with updating many parameters, are the top-3 best models in the open-world VQA scenario, indicating the significance of instruction-following tuning and effective parameter update. Moreover, InstructBLIP perform best on in-domain capability evaluation, while being much worse than many instruction-tuned models, implying severe overfitting issue, as shown in Fig. 1.

### Takeaway Analysis

We can conclude some actionable insights from our evaluation results. _First_, the quality of visual instruction data matters more than quantity in the open-world VQA. We observe that MiniGPT-4, which is tuned by only 3.5K high-quality visual instruction data performs much better than InstructBLIP tuned on visual instruction data adapted from various existing VQA datasets in our Multi-Modality Arena. _Second_, a strong visual encoder can help extract detailed information from the image, leading to good performance in OCR tasks. For instance, we see that BLIP2, InstructBLIP, and VPGTrans achieve better performance than the remaining 5 LVLMs. This may be because the visual encoder ViT-g/14 used in BLIP2, InstructBLIP, and VPGTrans is more powerful than ViT-L/14 employed in the remaining LVLMs. _Third_, multi-turn reasoning helps alleviate the hallucination issue, indicating that the evaluation method with critical thinking can induce the correct prediction from the model. We find that LVLM with multi-turn reasoning can determine whether an object exists in the image more accurately than single-turn reasoning. Hence, multi-turn reasoning is appropriate to assess the full potential of the model. _Fourth_, LVLMs tuned with high-quality instruction-following data present more promising planning ability than models without being tuned with instruction data as demonstrated in Table 7.

## 4 Conclusion

This paper proposes a comprehensive evaluation benchmark for large vision-language models called LVLM-eHub that incorporates both quantitative performance evaluation and human feedback evaluation. For the quantitative evaluation, we employ 16 tasks spanning over 40+ text-related visual datasets to assess the six essential capabilities of LVLM models. Additionally, we have established an online LVLM Arena to gather human feedback on LVLM models continually. This arena serves as an invaluable resource, providing an Elo rating rank that offers LVLMs ranking in the open-world scenario. Our evaluation results reveal several important findings, stimulating the future development of LVLMs. We will make ongoing efforts to build a platform for LVLM evaluation as discussed in Sec. A.4.

    & BLIP2 & InstructBLIP & LA-V2 & LLaVA & MiniGPT-4 & mPLUG-Owl & Otter & VPGTrans \\   & Object Recon.(\(\)) & 2.03 & 3.08 & 3.81 & **3.88** & 3.70 & 3.42 & 3.38 & 3.43 \\  & Spatial Relation.(\(\)) & 1.68 & 2.78 & **3.71** & 3.61 & 3.47 & 3.22 & 3.10 & 3.22 \\  & Conseensens (\(\)) & 3.**25** & 2.48 & 2.04 & 1.86 & 1.62 & 1.48 & 1.86 & 1.76 \\  & Reasonability(\(\)) & 2.78 & 3.20 & **4.04** & 3.70 & 3.54 & 3.44 & 3.07 & 3.35 \\  & Executability(\(\)) & 2.88 & 3.10 & **4.08** & 3.82 & 3.11 & 3.54 & 3.12 & 3.35 \\   & 0.674 & 0.772 & **0.922** & 0.879 & 0.805 & 0.785 & 0.761 & 0.789 \\   

Table 7: Generated planning quality evaluation on embodied tasks. Five dimensions including object recognition, spatial relationship, conciseness, reasonability, and executability are used to assess the performance.