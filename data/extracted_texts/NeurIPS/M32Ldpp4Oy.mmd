# LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation

Bowen Li\({}^{1}\)   Zhaoyu Li\({}^{2}\)   Qiwei Du\({}^{3}\)   Jinqi Luo\({}^{4}\)   Wenshan Wang\({}^{1}\)   Yaqi Xie\({}^{1}\)

Simon Stepputtis\({}^{1}\)   Chen Wang\({}^{3}\)   Katia Sycara\({}^{1}\)   Pradeep Ravikumar\({}^{1}\)

Alexander Gray\({}^{5}\)   Xujie Si\({}^{2,6}\)   Sebastian Scherer\({}^{1}\)\({}^{*}\)

\({}^{1}\)Carnegie Mellon University  \({}^{2}\)University of Toronto  \({}^{3}\)University at Buffalo

\({}^{4}\)University of Pennsylvania \({}^{5}\)Centaur AI Institute  \({}^{6}\)CIFAR AI Chair, Mila {bowenli2, basti}@andrew.cmu.edu

###### Abstract

Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks. However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions. Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities. To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents. LogiCity models diverse urban elements using semantic and spatial _concepts_, such as IsAmbulance(X) and IsClose(X,Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are _abstractions_, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios. Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning. To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors. Our extensive evaluation reveals the advantage of NeSy frameworks in _abstract_ reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data. With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI. All the code and data are open-sourced at our website.

## 1 Introduction

Unlike most existing deep neural networks [1; 2], humans are not making predictions and decisions in a relatively black-box way . Instead, when we learn to drive a vehicle, play sports, or solve math problems, we naturally leverage and explore the underlying symbolic representations and structure [3; 4; 5]. Such capability enables us to swiftly and robustly reason over complex situations and to adapt to new scenarios. To emulate human-like learning and reasoning, the Neuro-Symbolic (NeSy) AI community  has introduced various hybrid systems [7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19], integrating symbolic reasoning into deep neural networks to achieve higher data efficiency, interpretability, and robustness1.

Despite their rapid advancement, many NeSy AI systems are designed and tested only in very simplified and limited environments, such as visual sudoku , handwritten formula recognition , knowledge graphs , and reasoning games/simulations  (see Table 1). A benefit of such environments is that they usually provide data with symbolic annotations, which the NeSy AI systems can easily integrate. However, they are still far from real-world complexity due to the lack of three key features: (1) Most simulators are governed by propositional rules tied to specific fixed entities  rather than _abstractions_. As a result, agents learned from them are hard to generalize compositionally. (2) In real life, we learn to reason gradually from simple to complex scenarios, requiring the rules within the environment to be _flexible_. Either overly simplified  or overly complicated/unsuitable  environments cannot promote the development of NeSy AI systems. (3) Few simulators offer realistic _multi-agent_ interactions, where the environment agents often need to actively adapt their behaviors in response to varying actions of the ego agent. Moreover, a comprehensive benchmark needs to provide both _long-horizon_ (e.g., \(>20\) steps)  and _visual reasoning_ scenarios to exercise different aspects of NeSy AI.

To address these issues, we introduce LogiCity, the first customizable first-order-logic (FOL)-based  simulator and benchmark motivated by complex urban dynamics. As illustrated in Figure 1, LogiCity allows users to freely customize spatial and semantic conceptual attributes (concepts), FOL rules, and agent sets as configurations. Since the concepts and rules are _abstractions_, they can be universally applied to any agent compositions across different cities. For example, in Figure 1, concepts such as "IsClose(X, Y), IsAmbulance(Y)", and rules like "Stop(X):-IsAmbulance(Y), IsClose(X, Y)" can be _grounded_ with specific and distinct train/test agent sets to govern their behaviors in the simulation. To render the environment into diverse RGB images, we leverage foundational generative models . Since our modular framework enables seamless configuration adjustments, researchers can explore compositional generalization by changing agent sets while keeping abstractions fixed, or study adaptation to new and more complex abstractions by altering rules and concepts.

To exercise different aspects of NeSy AI, we use LogiCity to design tasks for both sequential decision-making (SDM) and visual reasoning. In the SDM task, algorithms need to navigate a lengthy path (\(>40\) steps) with minimal trajectory cost, considering rule violations and step-wise action costs. This involves planning amidst complex scenarios and interacting with multiple dynamic agents. For instance, decisions like speeding up may incur immediate costs but could lead to a higher return in achieving the goal sooner. Notably, our SDM task is also unique in that training and testing agent compositions are different, requiring an agent to learn the abstractions and generalize to new compositions. Contrarily, the visual reasoning task focuses on single-step reasoning but features high-dimensional RGB inputs. Algorithms must perform sophisticated abstract reasoning to predict actions for all agents with high-level perceptual noise. Across both tasks, we vary reasoning complexity to evaluate the algorithms' ability to adapt and learn new abstractions.

While we show that existing NeSy approaches  perform better in learning abstractions, both from scratch and continually, more complex scenarios from LogiCity still pose significant challenges for prior arts . On the one hand, LogiCity raises the abstract reasoning complexity with long-horizon multiple agents scenario, which have not been adequately addressed by current methods. Besides, it also highlights the difficulty of learning complex abstractions from high-dimensional data even for one-step reasoning. On the other hand, LogiCity provides flexible ground-truth symbolic abstractions, allowing for the new methods to be gradually designed, developed, and validated. Therefore, we believe LogiCity represents a crucial step for advancing the next generation of NeSy AI capable of sophisticated abstract reasoning and learning.

 BenchmarksFeatures & Abstract & Flexible & Multi-Agent & Long-Agent & RGB \\ Visual Sadoka  & ✗ & ✗ & ✗ & ✗ & ✗ \\ Handwritten Formula  & ✗ & ✗ & ✗ & ✗ \\ Smoker \& Friends  & ✓ & ✗ & ✗ & ✗ \\ CLEVR  & ✓ & ✓ & ✗ & ✗ & ✓ \\ BlocksWorld  & ✓ & ✗ & ✓ & ✗ \\ Arai Games  & ✗ & – & – & ✓ & ✓ \\ Minigrid \& Minworld  & – & ✗ & – & ✓ \\ BabyA1  & ✓ & – & ✗ & ✓ & ✓ \\ HighWay  & ✓ & ✗ & ✓ & ✗ & ✓ \\ 
**LogiCity (Ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\ 

Table 1: Comparison of existing NeSy benchmarks and LogiCity. Our simulator is governed by diverse _abstractions_, which can be flexibly customized. We also support long-horizon, multi-agent tasks and RGB rendering. “\(-\)” denotes partially supported features.

## 2 Related Works

### Neuro-Symbolic AI

NeSy AI systems aim to integrate formal logical reasoning into deep neural networks. We distinguish these systems into two categories: _deductive_ methods and _inductive_ methods.

**Deductive Methods** typically operate under the assumption that the underlying symbolic structure and the chain of deductive reasoning (rules) are either known [8; 9; 21; 40; 41; 42] or can be generated by an oracle [17; 18; 43; 44]. Some of these approaches constructed differentiable logical loss functions that constrain the training of deep neural networks [40; 41]. Others, such as DeepProbLog , have formulated differential reasoning engines, thus enabling end-to-end learning [45; 46; 47]. Recently, Large Language Models (LLMs) have been utilized to generate executable code [17; 43; 44] or planning abstractions , facilitating the modular integration of the grounding networks. Despite their success, deductive methods sidestep or necessitate the laborious manually engineered symbolic structure, which potentially limits their applicability in areas lacking formalized knowledge.

**Inductive Methods** focus on generating the symbolic structure either through supervised learning [10; 11; 36; 49; 50; 51] or by interacting with the environment [52; 53; 54]. One line of research explicitly searches the rule space, such as \(\)ILP , Difflog , and Popper . However, as the rule space can be exponentially large for _abstractions_, these methods often result in prolonged search times. To address this, some strategies incorporate neural networks to accelerate the search process [51; 50; 11]. Another avenue of inductive methods involves designing logic-inspired neural networks where rules are implicitly encoded within the learned weights [7; 20; 56; 57], such as SATNet  and Neural Logic Machines (NLM) . While these methods show promise for scalability and generalization, their applications have been predominantly limited to overly simplistic test environments.

### Games and Simulations

Various gaming environments [23; 24; 25; 27] have been developed to advance AI agents. Atari games , for instance, provide diverse challenges ranging from spatial navigation in "Pac-Man" to real-time strategy in "Breakout". More complex games include NetHack , StarCraft II , and MineCraft , where an agent is required to do strategic planning and resource management. LogiCity shares similarities with these games in that agent behavior is governed by rules. Especially, LogiCity can be viewed as a Rogue-like gaming environment , where maps and agent settings could be randomly generated in different runs. However, our simulator is uniquely tailored for the

Figure 1: LogiCity employs _abstract_ concepts and rules, allowing different agent sets to address compositional generalization. Its modular structure enables users to modify _abstractions_ flexibly.

NeSy AI community because: (1) LogiCity provides formal symbolic structure in FOL, enhancing the validation and design of NeSy frameworks. (2) Since FOL predicates and rules are abstractions, a user can arbitrarily customize the composition of the world, introducing adversarial scenarios. (3) LogiCity also supports customizable reasoning complexity through flexible configuration settings. Another key difference between LogiCity and most games [23; 27; 58] is that the behavior of non-player characters (NPCs) in LogiCity is governed by global logical constraints rather than human-engineered behavior trees [59; 60; 61; 62]. This design enables NPCs to automatically commit to actions that ensure global rule satisfaction, without the need for manual scripting. Moreover, compared to these games, LogiCity is closer to real urban life, offering a more practical scenario.

Addressing the need for realism, autonomous driving (AD) simulators [63; 64; 65; 66; 67; 68] deliver high-quality rendering and accurate traffic simulations but often adhere to fixed rules for limited sets of _concepts_. Among them, the SCENIC language [66; 67; 68] is the closest to LogiCity, which uses Linear Temporal Logic to specify AD scenarios. Unlike SCENIC, LogiCity uses _abstractions_ in FOL, which allows for the generation of a large number of cities with distinct agent compositions more easily. Besides, LogiCity goes beyond these AD simulators by introducing a broader range of _concepts_ and more complex rules, raising the challenge of sophisticated logical reasoning.

## 3 LogiCity Simulator

The overall framework of LogiCity simulator is shown in Figure 1. In the configuration stage, a user is expected to provide _Concepts_, Rules, and Agent set, which are fed into the _abstraction_-based simulator to create a sequence of urban grid maps. These maps are rendered into diverse RGB images via generative models, including a LLM  and a text-driven diffusion model .

### Configuration and Preliminaries

**Concepts** consist of \(K\) background predicates \(=\{P_{i}()|i=1,,K\}\). In LogiCity, we can define both _semantic_ and _spatial_ predicates. For example, \(()\) is an unary semantic predicate and \((,)\) is a binary spatial predicate. These predicates will influence the truth value of four action predicates \(\{(),(),(),()\}\) according to certain rules.

**Rules** consist of \(M\) FOL clauses, \(=\{C_{m}|m=1,,M\}\). Following ProLog syntax , an FOL clause \(C_{m}\) can be written as:

\[():-(,)\ ()\,\]

where \(()\) is the _head_, and the rest after ": \(-\)" is the _body_. \(,\) are variables, which will be _grounded_ into specific entities for rule inference. Note that the clause implicitly declares that the variables in the _head_ have a universal quantifier (\(\)) and the other variables in the _body_ have an existential quantifier (\(\)). We assume only action predicates appear in the _head_, both action and background predicates could appear in the _body_.

The concepts \(\) and rules \(\) are _abstractions_, which are not tied to specific entities.

**Agents** serve as the _entities_ in the environment, which is used to _ground_ the _abstractions_. We use \(=\{A_{n}|n=1,,N\}\) to indicate all the \(N\) agents in a city. Each agent will initially be annotated with the semantic _concepts_ defined in \(\). For example, an ambulance car \(A_{1}\) is annotated as \(A_{1}=\{,,,p\}\), where \(p\) denotes right-of-way priority. \(,,\) make up the configuration of LogiCity simulation. A user can flexibly change any of them seamlessly without modifying the simulation and rendering process.

### Simulation and Rendering

As the simulation initialization, a static urban map \(_{}\{0,1\}^{W H B}\) is constructed, where \(W,H\) denotes the width and height. \(B\) indicates the number of static semantics in the city, _e.g._, traffic streets, walking streets, intersections, _etc_. The agents then randomly sample collision-free start and goal locations on the map. These locations are fed into a search-based planner  to obtain the global paths that the agents will follow to navigate themselves. On top of the static map, each agent will create an additional dynamic layer, indicating their latest location and planned paths. We use \(^{t}\{0,1\}^{W H(B+N)}\) to denote the full semantic map with all the \(N\) agents at time step \(t\).

During the simulation, each agent \(A_{n}\) only has a limited field-of-view (FOV) of the overall map \(^{t}\), which we denote \(^{t}_{n}\). Additionally, FOV agents (self-included) in \(^{t}_{n}\) is obtained and denoted as \(^{t}_{n}\). A group of \(K\) pre-defined, binary functions \(\{_{i}(,)|i=1,,K\}\) for the \(K\) predicates are then employed to obtain the _grounding_\(^{t}_{n}\) for the ego agent, \(^{t}_{n}=_{1}(^{t}_{n}, ^{t}_{n}),,_{K}(^{t}_{n},^{t} _{n})\). Here, \(()\) denotes concatenation. Assuming we have a total of \(N_{n}\) FOV agents, \(^{t}_{n}\) will be in the shape of \(\{0,1\}^{_{i=1}^{K}N_{n}^{r_{i}}}\), where \(r_{i}\) is the _arity_ for the \(i\)-th predicate. Given \(^{t}_{n}\) and the rule clauses \(\), we leverage an SMT solver  to find the truth value of the four _grounded_ action predicates, \(^{t}_{n}=(^{t}_{n},)\). Here, \(^{t}_{n}\{0,1\}^{4}\) denotes the truth value of the _grounded_ four action predicates for agent \(A_{n}\) at time \(t\). An example of this procedure for \(A_{1}\) is provided in Figure 1. After all the agents take proper actions, we move their location, update the semantic map into \(^{t+1}\), and repeatedly apply the same procedure. Whenever an agent reaches its goal, the end position becomes the new starting point, a new goal point is randomly sampled, and the navigation is re-started.

To render the binary grid map \(^{t}\) into an RGB image \(^{t}\) with high visual variance, we leverage foundational generative models . We first feed the name of each _semantic concept_, including different types of agents and urban elements, into GPT-4  and asked it to generate diverse descriptions. These descriptions are fed into a diffusion-based generative model , which creates diverse icons. These icons will be randomly selected and composed into the grid map landscape to render highly diverse RGB image datasets. Detailed simulation procedure is shown in Appendix C.

## 4 LogiCity Tasks

LogiCity introduced above can exercise different aspects of NeSy AI. For example, as shown in Figure 2, we design two different tasks. The Safe Path Following (SPF) task aims at evaluating sequential decision-making capability while the Visual Action Prediction (VAP) task focuses on reasoning with high-dimensional data. Both tasks assume no direct access to the rule clauses \(\).

### Safe Path Following

SPF requires an algorithm to control an agent in LogiCity, following the global path to the goal while maximizing the trajectory return. The agent is expected to sequentially make a decision on the four actions based on its discrete, partial observations, which should minimize rule violation and action costs. In the following introduction, we assume the first agent, _i.e._, \(A_{1}\) is the controlled agent.

Specifically, the SPF task can be formulated into a Partially Observable Markov Decision Process (POMDP), which can be defined by the tuples \((S,,,T,Z,R,)\). The state at time \(t\) is the global urban grid, together with all the agents and their conceptual attributes, \(s^{t}=\{^{t},\} S\). The action space \(\) is the 4-dimensional discrete action vector \(^{t}_{1}\). The observation at \(t\)-th step is the _grounding_ of the agent's FOV, \(o^{t}=^{t}_{1}\), which can be obtained from the parsing functions \(_{i}\). State transition \(T(s^{t+1}|s^{t},^{t}_{1})\) is the simulation process introduced in Section 3.2. The observation

Figure 2: Demonstration of the Safe Path Following (SPF) and Visual Action Prediction (VAP) tasks in LogiCity. SPF emphasizes sequential decision-making while VAP focuses on one-step sophisticated reasoning on RGB inputs. In the VAP task, we also display the baseline model structure.

function \(Z(o^{t+1}|s^{t+1},^{t}_{1})\) is a deterministic cropping function. The reward function \(R(s^{t},^{t}_{1})\) is defined as \(R(s^{t},^{t}_{1})=_{m}^{M}w^{t}_{m}(s^{t},^{t}_{1}, C_{m})+w^{a}(^{t}_{1})+w^{vertime}(t)\), where \(w^{t}_{m}\) is the weight of rule violation punishment for the \(m-\)th clause \(C_{m}\). \((,,)\) evaluates if clause \(C_{m}\) is satisfied for agent \(A_{1}\) given \(s^{t}\) and \(^{t}_{1}\). \((^{t}_{1})\) indicates action cost at step \(t\) and \(w^{}\) is a normalization factor. \(w^{vertime}(t)\) gives constant punishment if \(t\) is larger than the max horizon. Finally, \(\) is a discount factor set to \(0.99\). An example of SPF is shown in Figure 2 (a), where \(A_{1}\) is the _Ambulance_ car in the purple box. The dashed box denotes its FOV, which will be _grounded_ by the parsing functions. A model needs to learn to sequentially output action decisions that maximize trajectory return.

Compared to existing reasoning games [23; 24; 25], LogiCity's SPF task presents two unique challenges: (1) Different agent configurations \(\) in training and testing cause distribution shifts in world transitions (\(T\)). This requires the model to learn the _abstractions_ (\(,\)) for compositional generalization. For example, training agents could include _ambulance_ plus _pedestrian_ and _police_ car plus _pedestrian_. In testing, the algorithm may need to plan with _ambulance_ plus _police_ car. (2) LogiCity supports more realistic multi-agent interaction. For instance, if the controlled agent arrives at an intersection later than other agents, it must wait, resulting in a lower trajectory return; if it speeds up to arrive earlier, others yield, ending up with a higher score. This encourages learning both ego rules and world transitions with multiple agents (how to plan smartly by forecasting).

### Visual Action Prediction

Unlike SPF, which is long-horizon and assumes access to the _groundings_, the VAP task is step-wise and requires reasoning on high-dimensional data [13; 20]. As shown in Figure 2 (b), inputs to VAP models include the rendered image \(\) (We omit the time superscript \(t\) here) and information for \(N\) agents \([_{1},,_{N}]^{N 9}\), where \(_{n}=[x_{n},y_{n},w_{n},h_{n},_{n},p]^{}\) consists of location \((x_{n},y_{n})\), scale \((w_{n},h_{n})\), one-hot direction \(_{n}^{4}\), and normalized priority \(p\). During training, the models learn to reason and output the action vectors \(}_{n}\) for all the \(N\) agents with ground-truth supervision. During test, the models are expected to predict the actions for different sets of agents.

This task is approached as a two-step graph reasoning problem [7; 38]. As illustrated in Figure 2 (b), a grounding module first predicts interpretable _grounded_ predicate truth values, which are then used by a reasoning module to deduce action predicates. To be more specific, a visual encoder [71; 2] first extracts global features \(\) from \(\). Agent-centric regional features are derived from ROIAlign , which resizes the image-space bounding boxes to match the feature scale and then crops the global feature using bilinear interpolation. The resulting regional features for each agent, denoted as \(_{n}\), are fed into unary prediction heads to generate unary predicate _groundings_. Meanwhile, binary prediction heads utilize paired agent information to predict binary predicates. Together, the _groundings_ form a scene graph \(}\), which a graph reasoning engine [7; 38] uses to predict actions \(}_{n}\).

Similar to the SPF task, the VAP task also features different train/test agent compositions, necessitating the model's ability to learn _abstractions_. Additionally, unlike reasoning on structured, symbolic knowledge graphs [7; 11; 21], the diverse visual appearances in LogiCity introduce high-level perceptual noise, adding an extra challenge for reasoning algorithms.

## 5 Experiments

### Safe Path Following

We first construct a ground-truth rule-based agent as Oracle and a Random agent as the worst baseline, showing their results in Table 2. Two branches of methods are considered here, behavior cloning (BC) and reinforcement learning (RL), respectively. All the experiments in SPF are conducted on a single NVIDIA RTX 3090 Ti GPU with 32 AMD Ryzen 5950X 16-core processors.

**Baselines.** In the BC branch, we provide oracle trajectories as demonstration and consider the inductive logical programming (ILP) algorithms , including symbolic ones [10; 36] and NeSy ones [7; 11]. We also construct a multi-layer perceptron (MLP) and graph neural networks (GNN)  as the pure neural baselines. In the RL track, we first build neural agents using various RL algorithms, including on-policy [33; 34], off-policy [7; 35] model-free approaches and model-based algorithms [37; 39]. Since most of the existing NeSy RL methods [52; 53] are carefully engineered for simpler environments, we find it hard to incorporate them into our LogiCity environment. Tointroduce NeSy AI in the RL track, we develop a new Q-learning agent based on NLM , which we denote as NLM-DQN [7; 35]. For more details, please see Appendix A.

**Modes and Datasets.** As shown in Table 2, we provide four modes in the SPF task, namely easy, medium, hard, and expert. From easy to medium to hard mode, we progressively introduce more _concepts_ and more complex rules, constraining only Stop action. The expert mode constrains all four actions with the most complex rule sets. More details are included in Appendix B.

**Metrics.** We consider three metrics in this task. Trajectory Success Rate (TSR) evaluates if an agent can reach its goal within the allotted time. It is defined as \(=^{}_{i}}{T^{ }}\), where \(T^{}\) is the total number of episodes, and \(_{i}=1\) if the \(i\)-th episode is completed within twice the oracle steps without rule violations, and \(_{i}=0\) otherwise. Decision Success Rate (DSR) assesses if an agent adheres to all rules. It is defined as \(=^{^{}}_{i}}{T^{ }}\), where \(_{i}=1\) if the \(i\)-th episode has at least one rule-constrained step and the agent does not violate any rules throughout, regardless of task completion, and \(_{i}=0\) otherwise. The score metric is the averaged trajectory return over all episodes minus the return of a random agent. Among them, TSR is the most crucial.

#### 5.1.1 Empirical Evaluation

We present the empirical results in Table 2, showing LogCity's ability to vary reasoning complexity. In the BC track, symbolic methods [10; 36] perform well in the _easy_ mode but struggle with more complex scenarios from the _medium_ mode. NeSy rule induction methods [7; 11] outperform pure neural MLP/GNN approaches. In the RL track, off-policy methods [7; 35; 37; 39] are more stable and effective than on-policy methods [33; 34] due to the high variance in training episodes affecting policy learning. Additionally, NeSy framework [7; 35] outperform pure neural agents [35; 37] by finding better representations from _abstract_ observations. To illustrate the compositional challenge in LogiCity, we compare results across different agent sets in Figure 3. Models trained on the training agent configuration show significant performance drops when transferred to test agents, but NeSy methods [7; 35] are less affected. We discuss more observation spaces in Appendix D.

#### 5.1.2 Continual Learning

Using LogiCity, we also examine how much data different models need to continually learn new _abstract_ rules. We initialize models with the converged weights from _easy_ mode and progressively provide data from _medium_ mode rules. The results from three random runs for MLP and NLM  are shown in Figure 4, alongside results from models trained from scratch. NLM reaches the best result with 30% of the target domain data, demonstrating superior continual learning capabilities.

### Visual Action Prediction

Baselines.As there exists very limited literature  studying FOL reasoning on RGB images, we self-construct two baselines using GNN  and NLM  as the reasoning engine, respectively. For fairness, we use the same visual encoder [2; 71] and hyperparameter configurations. We train and test all the models on a single NVIDIA H100 GPU. See Appendix A for more details.

Settings.We explore four distinct training settings for the two methods. Regarding supervision signals, modular supervision offers ground truth for both scene graphs and final actions, training the two modules separately. This setting requires interpretable meanings of the scene graph elements,

Figure 5: Diverse renderings from LogiCity. Note that every city has distinct agent compositions.

    &  &  \\  &  & 3978 & 7220 & - & - & 4155 & 2882 & 715 & 6488 & - & - \\ Supervision & Config & Model & Slow & Normal & Stop & aAcc. & wAcc. & Slow & Normal & Fast & Stop & aAcc. & wAcc. \\   &  & GNN  & 0.45 & 0.63 & 0.54 & 0.54 & **0.53** & 0.44 & 0.47 & 0.09 & 0.57 & 0.49 & 0.23 \\  & & NLM  & 0.31 & 0.57 & 0.75 & 0.61 & 0.49 & 0.39 & 0.54 & 0.11 & 0.48 & 0.45 & **0.24** \\  & & GNN  & 0.52 & 0.63 & 0.43 & 0.51 & 0.54 & 0.26 & 0.51 & 0.19 & 0.63 & 0.48 & 0.28 \\  & & NLM  & 0.54 & 0.53 & 0.67 & 0.60 & **0.56** & 0.15 & 0.41 & 0.35 & 0.57 & 0.41 & **0.36** \\    &  & GNN  & 0.76 & 0.69 & 0.98 & 0.85 & **0.78** & 0.46 & 0.62 & 0.27 & 0.99 & 0.72 & 0.40 \\  & & NLM  & 0.78 & 0.47 & 1.00 & 0.83 & 0.71 & 0.33 & 0.69 & 0.37 & 1.00 & 0.71 & **0.46** \\  & & GNN  & 0.88 & 0.64 & 1.00 & 0.87 & **0.82** & 0.14 & 0.66 & 0.52 & 1.00 & 0.65 & **0.54** \\   & & NLM  & 0.90 & 0.53 & 1.00 & 0.85 & 0.79 & 0.25 & 0.67 & 0.45 & 1.00 & 0.69 & 0.50 \\   

Table 3: Empirical results of different methods and settings in VAP task (Modular is more crucial). We report the recall rate for each action, averaged accuracy (aAcc.), and weighted accuracy (wAcc.).

which is crucial. We also explore end-to-end supervision (E2E), which provides guidance only on the final actions. For the training agent sets, we experiment with both fixed and random settings.

**Modes and Datasets.** We present two modes for VAP task, namely _easy_ and _hard_. In _easy_ mode, rules constrain only Slow and Stop actions with few _concepts_. The _hard_ mode includes the _easy abstractions_ and additional constraints for all four actions, with a natural data imbalance making the Fast action rarer. We display some examples in Figure 5. More details are included in Appendix B.

**Metrics.** We first report the action-wise recall rate (true positives divided by the number of samples). The average accuracy (aAcc.) is the correct prediction rate across all the test samples. To highlight the imbalance issue, we also introduce weighted accuracy (wAcc.), defined as \(=^{}/N^{}}{ _{a}^{}}\), where \(^{}\) is the recall rate for action a and \(N^{}\) is the number of samples for action a. This metric assigns larger weights to less frequent actions.

#### 5.2.1 Empirical Evaluation

The empirical results for the VAP task are shown in Table 3, highlighting LogCity's ability to adjust reasoning complexity. We observe that while GNN  slightly outperforms NLM  in the _easy_ mode, NLM excels in the _hard_ mode. We also find that random agent configurations improve the performance of both methods. The data imbalance poses an additional challenge, with the Stop action having \(2 6\) higher recall than the Fast action. Besides, the modular setting proves more challenging than the end-to-end (E2E) setting, as the modular system is more sensitive to perceptual noise. We further investigate this issue quantitatively in Appendix E.

#### 5.2.2 Continual Learning

Similar to the SPF task, we investigate how much data the methods need to continually learn abstract rules in the VAP task. The models pre-trained in _easy_ mode are used as the initial weights, which are continually trained with different sets of data from the _hard_ mode. The data are sampled for 3 times and the mean and variance of the results are reported in Figure 6, where we also report the results from the models trained with 100% data from scratch as dashed lines ("upper bound"). We observe that the two methods could struggle to reach their "upper bound" if fixed training agents are used. For the random agent setting, NLM  could progressively learn new rules and reach its "upper bound" with around 50% data while GNN fails even with 100% data.

#### 5.2.3 How Do LLMs and Human Perform in LogCity?

Recent years have witnessed the increasing use of LLMs for decision making , concept understanding , and logical reasoning . In this section, we investigate the capability of LLMs  and Human to solve the (subset of) VAP task in LogCity through in-context demonstrations. Since we focus only on logical reasoning, true _groundings_ are provided in natural language documents without perceptual noise. Specifically, we first convert every scene (frame) into a paragraph of natural language description (see Figure B for examples). For each entity within the frame, given the scene descriptions, we ask LLMs to decide its next action from options ("A. Slow", "B. Normal", "C. Fast", "D. Stop"). Since the entire test set of VAP is huge, we randomly selected a "Mini" test with 117 questions about the concept IsTiro and IsReckless. To construct demonstrations for

Figure 6: Continual learning results of GNN  and NLM  in the VAP task. The mean results from three random runs are displayed in solid lines and the variance is reported as the semi-transparent regions. We also show the results of the models trained from scratch using 100% data in dashed lines.

in-context learning, we randomly choose 5-shot samples from the training document used by human participants2 and provide question-answer pairs. The performance of Human, gpt-4o (GPT-4o), gpt-4o-mini (GPT-4o mini), gpt-4-turbo-2024-04-09 (GPT-4), and gpt-3.5-turbo-1106 (GPT-3.5) on VAP hard mode test sets are reported in Table 4, where the random sampling results for options are also provided for reference. Based on experts' evaluation, we also display the "hardness" of correctly answering each of the choice, where \({}^{}\), \({}^{}\), and \({}^{}\) denote "easy", "medium", and "hard".

We observe that the latest GPT-4o shows significantly better in-context learning capability than previous GPT-4 and GPT-3.5, surpassing them by over 20% in terms of overall accuracy. The results also demonstrate the importance of model scale for reasoning task, where GPT-4o mini falls far behind GPT-4o. However, it is still far from the inductive logical reasoning capability of Human, especially for harder reasoning choices like "Stop". Interestingly, the distribution of the decisions demonstrates that GPT-4 has a strong bias towards a conservative decision, which tends to predict "Slow" action. GPT-4o is better at reasoning in the context, yet they still tend to use common sense knowledge (_e.g._, Reckless cars always drive fast). In contrast, human participants tend to learn LogiCity's rules through formal verification, where hypotheses are verified and refined based on training documents. Yet, due to the challenging nature of logical induction, human has made mistakes in learning rules of "Stop" (they concluded more general rules than GT), which affects the accuracy of "Fast". This suggests a promising future research direction that could involve coupling LLMs with a formal inductive logical reasoner [10; 36], creating a generation-verification loop. Another intriguing direction is using the LogiCity dataset to conduct Direct Preference Optimization (DPO) .

## 6 Discussions

Conclusion.This work presents LogiCity, a new simulator and benchmark for the NeSy AI community, featuring a dynamic urban environment with various _abstractions_. LogiCity allows for flexible configuration on the _concepts_ and FOL rules, thus supporting the customization of logical reasoning complexity. Using the LogiCity simulator, we present sequential decision-making and visual reasoning tasks, both emphasizing _abstract_ reasoning. The former task is designed for a long-horizon, multi-agent interaction scenario while the latter focuses on reasoning with perceptual noise. With exhaustive experiments on various baselines, we show that NeSy frameworks [7; 11] can learn _abstractions_ better, and are thus more capable of the compositional generalization tests. Yet, LogiCity also demonstrates the challenge of learning _abstractions_ for all current methods, especially when the reasoning becomes more complex. Specifically, we highlight the crucial difficulty of long-horizon _abstract_ reasoning with multiple agents and that _abstract_ reasoning with high dimensional data remains hard. On the one hand, LogiCity poses a significant challenge for existing approaches with sophisticated reasoning scenarios. On the other hand, it allows for the gradual development of the next-generation NeSy AI by providing a flexible environment.

Limitations and Social Impact.One limitation of our simulator is the need for users to pre-define rule sets that are conflict-free and do not cause deadlocks. Future work could involve distilling real-world data into configurations for LogiCity, streamlining this definition process. Currently, LogiCity does not support temporal logic ; incorporating temporal constraints on agent behaviors is intriguing. The simulation in LogiCity is deterministic, introducing randomness through fuzzy logic deduction engines [8; 9] could be interesting. For the autonomous driving community [26; 63], LogiCity introduces more various _concepts_, requiring a model to plan with _abstractions_, thus addressing a new aspect of real-life challenges. Enhancing the map of LogiCity and expanding the action space could make our simulator a valuable test bed for them. Additionally, since the ontologies and rules in LogiCity can be easily converted into Planning Definite Domain Language (PDDL), LogiCity has potential applications in _multi-agent_ task and motion planning [12; 86]. A potential negative social impact is that rules in LogiCity may not accurately reflect our real life, introducing sim-to-real gap.

   Method & Slow\({}^{}\) & Normal\({}^{}\) & Fast\({}^{}\) & Stop\({}^{}\) & Overall \\  Human & **95.0** & **92.9** & 48.0 & **83.3** & **81.2** \\ GPT-4o & 20.0 & 84.1 & 80.0 & 32.2 & 59.0 \\ GPT-4 & 75.0 & 57.9 & 25.3 & 2.2 & 39.6 \\ GPT-3.5 & 0.0 & 82.5 & 16.0 & 0.0 & 33.0 \\ GPT-4o mini & 0.0 & 2.4 & **86.7** & 40.0 & 29.6 \\ Random & 21.0 & 23.8 & 28.8 & 27.3 & 25.3 \\   

Table 4: Action prediction accuracy of different LLMs in the VAP task hard mode. \({}^{}\), \({}^{}\), and \({}^{}\) denote different logical reasoning hardness.