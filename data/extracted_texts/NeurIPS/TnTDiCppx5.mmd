# Predict-then-Calibrate: A New Perspective of Robust Contextual LP

Chunlin Sun\({}^{*,1}\)

Linyu Liu\({}^{*,2}\)

Xiaocheng Li\({}^{3}\)

\({}^{1}\) Institute for Computational and Mathematical Engineering, Stanford University

\({}^{2}\) Department of Automation, Tsinghua University

\({}^{3}\) Imperial College Business School, Imperial College London

Equal contribution.

###### Abstract

Contextual optimization, also known as predict-then-optimize or prescriptive analytics, considers an optimization problem with the presence of covariates (context or side information). The goal is to learn a prediction model (from the training data) that predicts the objective function from the covariates, and then in the test phase, solve the optimization problem with the covariates but without the observation of the objective function. In this paper, we consider a risk-sensitive version of the problem and propose a generic algorithm design paradigm called predict-then-calibrate. The idea is to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then utilize calibration (or recalibration) methods to quantify the uncertainty of the prediction. While the existing methods suffer from either a restricted choice of the prediction model or strong assumptions on the underlying data, we show the disentangling of the prediction model and the calibration/uncertainty quantification has several advantages. First, it imposes no restriction on the prediction model and thus fully unleashes the potential of off-the-shelf machine learning methods. Second, the derivation of the risk and robustness guarantee can be made independent of the choice of the prediction model through a data-splitting idea. Third, our paradigm of predict-then-calibrate applies to both (risk-sensitive) robust and (risk-neutral) distributionally robust optimization (DRO) formulations. Theoretically, it gives new generalization bounds for the contextual LP problem and sheds light on the existing results of DRO for contextual LP. Numerical experiments further reinforce the advantage of the predict-then-calibrate paradigm in that an improvement on either the prediction model or the calibration model will lead to a better final performance.

## 1 Introduction

Contextual linear programming (LP) considers the linear programming problem with the presence of covariates, and it can be viewed as a prediction problem under a constrained optimization context where the output of the prediction model serves as the objective of a downstream LP. The goal is to develop a model (trained from past data) that prescribes a decision/solution for the downstream LP problem using directly the covariates but without observation of the objective function. A similar problem formulation was also studied as prescriptive analytics (Bertsimas and Kallus, 2020) and predict-then-optimize Elmachtoub and Grigas (2022). The existing literature mainly develops two approaches for the problem: (i) end-to-end algorithms that map the covariates directly to a recommended decision for the LP or (ii) two-step algorithms that first predict the unobserved objective using the covariates and then solve the LP with the predicted objective. While the literature has been largely focused on the risk-neutral objective, our paper considers the risk-sensitive/robustformulation of the problem. The robust formulation of the contextual LP can be viewed as both a risk-sensitive version of the risk-neutral objective in contextual LP and an adaptive/contextual version of the standard (context-free) robust optimization problem. In comparison with the robust optimization literature, the contextualized robust optimization problem allows a contextual and data-driven way to design the uncertainty set and ideally can utilize the contextual information to make less conservative decisions. The contextual/data-driven uncertainty set has been considered by several existing works on transportation (Guo et al. (2023)), portfolio management (Wang et al. (2022)), and healthcare (Gupta et al. (2020)). While these works focus on applications with special structures or distributions, our paper considers a generic setup and aims for a general solution to the problem.

In our paper, we make the following contribution:

First, we formulate an algorithm design paradigm of predict-then-calibrate and introduce the notion of uncertainty calibration/quantification to the problem of contextual optimization. The idea is to first develop a prediction model without worrying about the downstream robustness, and then tailor the uncertainty calibration methods to quantify the prediction uncertainty towards the robust objective.

Second, we develop two algorithms for robust contextual LP under the paradigm of predict-then-calibrate. The derivation of theoretical guarantees shows the advantage of the disentanglement of the prediction phase and the uncertainty calibration phase. Moreover, we quantify the advantage of a better prediction and/or a better uncertainty calibration both theoretically and empirically.

Third, we also draw a connection with the related problem of distributionally robust optimization. We utilize the same idea of predict-then-calibrate and derive a new generalization bound for the distributional robust version of contextual LP utilizing tools from the nonparametric regression.

In the following, we review the several streams of literature that are related to our work.

**Conditional robust optimization.** The problem studied in our paper can be viewed as a conditional version of the classic robust optimization problem. The robust optimization stems from the replacement of the risk-neutral expectation objective with a risk-sensitive robust objective such as Value-at-Risk(VaR)/quantile. The literature on robust optimization develops methods to approximate VaR by specifying the shape and size of an uncertainty set to guarantee that a VaR constraint would be satisfied Ghaoui et al. (2003); Chen et al. (2007); Natarajan et al. (2008). Bertsimas et al. (2018) designs the uncertainty set with a finite-sample probabilistic guarantee on optimal solutions. Goerigk and Kurtz (2020) constructs the uncertainty set as the union of \(K\) convex sets by clustering historical data. While all these works concern the context-free setting, the contextual (or conditional) setting, or the problem of robust contextual LP, considers the presence of covariates in robust optimization. Along this line, Ohmori (2021) proposes a volume-minimized ellipsoid covering \(k\)-nearest samples in the contextual space, leading to a nonlinear programming problem. Chemeroddy et al. (2022) extends the idea in Goerigk and Kurtz (2020) and novelly adopts a deep learning approach to the problem which identifies which clusters the uncertain parameters belong to based on the covariates. Compared to the end-to-end formulations Ohmori (2021); Cheneroddy et al. (2022), we do not impose restrictions on the prediction model and have a larger flexibility in choosing the uncertainty quantification method.

**Contextual LP/Predict-then-optimize.** Our work complements the existing literature on contextual LP by considering a risk-sensitive objective. The existing works Donti et al. (2017); Elmachtoub and Grigas (2022); Ho-Nguyen and Kulinc-Karzan (2022); Hu et al. (2022) mainly study the risk-neutral objective for the problem and aim to develop prediction methods that account for the downstream optimization tasks. To model the remaining uncertainty after prediction in optimization, Sen and Deng (2018) considers training a parameterized point-wise estimation model first, and then approximating the conditional distribution by imposing randomness to the learned parameters empirically. Ban et al. (2019) proposes to forecast the distribution of new product demand by adopting a linear regression model to fit the demands of similar products with covariates, and then using the empirical distribution of covariate-independent residuals to build a sample average approximation (SAA) model. Some recent works Kannan et al. (2020, 2022, 2021) further generalize it to a residuals-based SAA framework to include more machine learning methods and adopt distributionally robust optimization to mitigate the overfitting under limited data. A key difference between our work and this line of works is that we consider a risk-sensitive objective, while these works consider a risk-neutral objective. In this light, our framework of predict-then-calibrate is compatible with any prediction model developed along this line, and it tailors the prediction model for a robustness task.

## 2 Problem Setup

Consider a linear program (LP) that takes the following standard form

\[(c,A,b)  c^{}x,\] (1) s.t. \[Ax=b,\ x 0,\]

where \(c^{n},\,A^{m n},\) and \(b^{m}\) are the inputs of the LP. In addition, there is an available feature vector \(z^{d}\) that encodes the covariates (side information) associated with the LP.

The tuple \((c,A,b,z)\) is drawn from an unknown probability distribution \(\). The problem of contextual LP (predict-then-optimize, or prescriptive analytics) usually assumes the availability of a learning (training) dataset sampled from the distribution \(,\)

\[_{l}\{(c_{t}^{(l)},A_{t}^{(l)},b_{t}^{(l)},z_{ t}^{(l)})\}_{t=1}^{T_{l}}.\]

During the test phase, one needs to recommend a feasible solution \(x_{}\) to a new LP problem using the observation of \((A_{},b_{},z_{})\) but without the observation of the objective vector \(c_{}\):

\[(A_{},b_{},z_{}) x_{}.\]

The focus of the existing literature has been mainly on the risk-neutral objective

\[_{x} [c|z]^{}x,\] (2) s.t. \[Ax=b,\ x 0\]

where the recommended decision \(x\) can be viewed as a function of \((A,b,z)\).

Alternatively, we consider a risk-sensitive objective

\[_{x} _{}[c^{}x|z],\] (3) s.t. \[Ax=b,\ x 0\]

where \((0,1)\). Here \(_{}(U)\) denotes the \(\)-quantile/value-at-risk (VaR) of a random variable \(U\); specifically, \(_{}(U) F_{U}^{-1}()\) with \(F_{U}^{-1}()\) be the inverse cumulative distribution function of \(U\).

We can interpret the problem (3) in two ways: First, it can be viewed as a risk-sensitive version of the contextual LP problem (2). From the prediction viewpoint, the standard problem (2) concerns only the prediction of \([c|z]\), while solving (3) may involve a distributional prediction of the conditional distribution \(c|z\). Second, the classic robust LP problem considers a setting where there is no covariate, and the VaR is taken with respect to the (marginal) distribution of \(c\). Comparatively, the problem (3) can be viewed as a conditionally robust version where the VaR is taken with respect to the conditional distribution \(c|z\). The presence of the covariates \(z\) reduces our uncertainty on the objective \(c\) and thus will induce less conservative decisions with the same level of risk guarantee.

Throughout the paper, we work with a well-trained ML model \(:^{d}^{n}\) that predicts the objective with the covariates and is learned from the training data \(_{l}\). We emphasize that we do not impose any assumption on the model, but assume the availability of a validation set \(_{}\{(c_{t},A_{t},b_{t},z_{t})\}_{t=1} ^{T}\) that can be used to quantify the uncertainty of \(\).

## 3 Robust Contextual LP

The problem (3) has been extensively studied under the context-free setting in the literature of robust optimization , and the key challenge is the non-convexity of the objective function. Equivalently, the problem (3) can be written as the following optimization problem

\[_{x,} _{c} c^{}x,\] (4) s.t. \[Ax=b,\ x 0,\ _{c|z}(c)\]

where the decision variables are \(x\), \(c\) and a measurable set \(\). The last constraint is with respect to the set \(\), and the probability is taken with respect to the conditional distribution \(c|z\). One issue tosolve this problem is the intractability in optimization over the uncertainty set. Particularly, even if the conditional distribution \(c|z\) is discrete, (4) is equivalent to a mixed integer linear programming problem, which is generally NP-hard and computationally intensive. Similarly, when solving a _sample average approximation_ form of (4) (Bertsimas and Kallus (2020)), the problem becomes similar to the discrete case, and the intractability issue is still inevitable. To resolve this issue, one can solve the following approximation problem with a fixed uncertainty set \(\) satisfying \(_{c|z}(c)\),

\[()_{x} _{c}c^{}x,\] (5) \[\ Ax=b,\ x 0.\]

A box- or ellipsoid-shaped uncertainty set enables a tractable optimization of the problem (5). The uncertainty set hopes to cover the high-density region so that the approximation to (3) is tighter.

Now we present two algorithms that implement this approximation idea in the contextual setting. The key is to quantify the uncertainty of the prediction model. Both algorithms construct contextual uncertainty sets for the prediction model; that is, the size of the uncertainty set changes with respect to the covariates \(z\). In addition, importantly, the construction of these uncertainty sets accounts for the tractability of the downstream robust optimization problem (5) by outputting the box and ellipsoid shapes, respectively. In both algorithms, we first split the available validation data into two sets \(_{1}\) and \(_{2}\), and use the first set \(_{1}\) for a preliminary calibration, and the second set \(_{2}\) for an additional adjustment. The output from both algorithms gives a contextual uncertainty set \((z)\) that works as the input of (5) and thus solves the downstream robust optimization problem.

```
1:Input: Dataset \(_{val}\), ML model \(\), parameter \(\)
2:Randomly split the validation data into two index sets \(_{1}_{2}=\{1,...,T\}\) and \(_{1}_{2}=\)
3:%%/Preliminary calibration of the uncertainty sets
4:for\(t_{1}\)do
5: Calculate the residual vector on the \(t\)-th validation sample \[r_{t} c_{t}-(z_{t})\] (6) and denote \(r_{t}=(r_{t1},...,r_{tn})^{}\)
6:endfor
7:Train a quantile regression model \((z):^{d}^{n}\) that minimizes \[_{t_{1}}_{i=1}^{n}_{}(|r_{ti}|-(z_ {t})_{i})\] (7) where \(_{}()()^{+}+(1-)()^{+}\) denotes the pinball loss
8:%%/Additional adjustment of the size of the uncertainty sets
9:for\(t_{2}\)do
10: Let \[_{t}()(z_{t})+(z_{t})\ ^{n},\ \ _{t}()(z_{t})-(z_{t})\ ^{n}\]
11:endfor
12:Choose a minimal \(>0\) such that \[_{t_{2}}1\{_{t}() c_{t}_{t} ()\}\{|_{2}|,(|_{2}|+1)\}\] (8) where \(1\{\}\) is the indicator function and the inequality is required to hold element-wise
13:Output: \(\), \(\) ```

**Algorithm 1** Box Uncertainty Quantification (BUQ)

In Algorithm 1, we first compute the residual/error vector \(r_{t}^{n}\) for samples in dataset \(_{1}\). Then for each coordinate \(i=1,...,n\), we fit a quantile regression model \(\) that predicts the quantile of the absolute error by minimizing (7). Specifically, the model \(\) aims to predict the quantiles of each coordinate of the residual vector \(r_{t}\). Then, in the second step of Algorithm 1, we pretend the quantile model \(\) is "correct" and construct the uncertainty sets accordingly for samples in dataset\(_{2}\). Importantly, the uncertainty set is parameterized by a scalar \(\). We use the parameter \(\) to perform an additional adjustment of the uncertainty set so that the coverage probability \(\) is met on \(_{2}\) by (8). The algorithm outputs the function \(\) and \(\), and it will output a box-shaped uncertainty set \(_{}^{(1)}(z)=[(z)-(z),(z)+ (z)]\) for a new sample with covariates \(z\).

```
1:Input: Dataset \(_{val}\), ML model \(\), parameter \(\)
2:Randomly split the validation data into two index sets \(_{1}_{2}=\{1,...,T\}\) and \(_{1}_{2}=\)
3:% _Preliminary calibration of the uncertainty sets_
4:for\(t_{1}\)do
5: Calculate the residual vector on the \(t\)-th validation sample \[r_{t} c_{t}-(z_{t})\]
6:endfor
7:Use \(_{1}\) to fit a model \(:^{d}\) that predicts the \(\)-quantile of \(\|r_{t}\|_{2}\) using the covariates \(z_{t}\)
8:Fit a zero-mean normal distribution for the scaled residual vector \[\{r_{t}/(z_{t}):t_{1}\}\]

 and denote the covariance matrix as \(\)
9:%_%Additional adjustment of the size of the uncertainty sets_
10:Choose a minimal \(>0\) such that \[_{t_{2}}1\{-(z_{t}))^{}^{-1}(c_{t}-(z_{t}))}(z_{t})\}\{| _{2}|,(|_{2}|+1)\}\] (9) where \(1\{\}\) is the indicator function and the inequality is required to hold element-wise.
11:Output: \(\), \(\), \(\) ```

**Algorithm 2** Ellipsoid Uncertainty Quantification (EUQ)

Algorithm 2 follows a similar spirit as Algorithm 1. It first computes the residual vector \(r_{t}\) for samples in dataset \(_{1}\), but differently from Algorithm 1, it fits a regression model for the norm \(\|r_{t}\|_{2}\). Then it fits a zero-mean normal distribution for the scaled residual vector. The motivation is that the residual vector (up to a scale \((z)\)) follows a multivariate normal distribution. We remark that although the algorithm design is motivated by such a setting of normal distribution, its theoretical guarantee does not rely on this normal structure. The key is the second step of additional calibration: it pretends the uncertainty set is "correct" and uses an additional scalar \(\) to make further adjustments such that the coverage probability \(\) is met on \(_{2}\) by (9). For a new sample with covariates \(z\), the algorithm outputs an ellipsoid-shaped uncertainty set \(_{}^{(2)}(z)=\{c:(z))^{} ^{-1}(c-(z))}(z)\}.\)

In comparison, for the first step of preliminary calibration, Algorithm 1 quantifies the uncertainty of each coordinate of the residual vector individually, while Algorithm 2 presumes a Gaussian structure. We remark that the two algorithms provide two examples for the design of the preliminary calibration step, but these are not the only two options. As we will see shortly, the second step of additional adjustment is in charge of the coverage guarantee, so the first step can be further modified based on other prior knowledge of the underlying data/optimization problem.

### Algorithm Analysis

The following proposition provides a coverage guarantee for both Algorithm 1 and 2 in that the output uncertainty set will cover a new sample with a probability of approximately \(\).

**Proposition 1**.: _For a new sample \((c,A,b,z)\) from distribution \(\), denote the uncertainty sets output from Algorithm 1 and Algorithm 2 by \(_{}^{(1)}(z)\) and \(_{}^{(2)}(z)\), respectively. The following inequalities hold for \(k=1,2\),_

\[(c_{}^{(k)}(z))+ _{2}|+1}\] (10)

_where the probability is with respect to the new sample \((c,A,b,z)\) and dataset \(_{2}\)._The proposition gives the following insights into the two algorithms and the general problem of robust contextual LP. First, the second step (in both algorithms) of the adjustment via the scalar \(\) is the key to the coverage guarantee (10), with several advantages. First, as the concentration argument is made with respect to only this second step, the bound is rather tight and free of unnecessary conservation in the uncertainty set construction. Second, it allows full flexibility in the choice of the prediction model \(\) and the preliminary calibration model \(\) (Algorithm 1) and \(\) (Algorithm 2). This reinforces the notion of predict-then-calibrate. The design of the prediction model \(\) can be solely targeted on the accuracy, and the coverage guarantee can be deferred and taken care of by the calibration steps such as these two algorithms.

Furthermore, we note the probability in (10) is taken with respect to only the validation data and the new sample. It means Proposition 1 holds even if the training data \(D_{l}\) (for obtaining \(\)) is not sampled from the distribution \(\). In other words, the result only requires the validation data \(_{}\) and it is robust to a distribution shift of the training distribution when \(_{l}\). In a broader context, when the training data, validation data, and test data come from different distributions, this situation is commonly referred to as the out-of-domain (OOD) problem. We refer to the survey papers Shen et al. (2021); Yang et al. (2021) for a more detailed discussion of this problem. Developing more coverage guarantees with OOD data will require future investigations, and we will list this problem as a future direction.

In addition, the coverage guarantee in Proposition 1 can be extended to a robustness guarantee for the contextual LP problem as in the following corollary.

**Corollary 1**.: _For a new sample \((c,A,b,z)\) from distribution \(\), denote the uncertainty set output from Algorithm 1 or Algorithm 2 by \(_{}(z)\). Let \(x^{*}(z)\) and \((z)\) be the optimal solution and the optimal value of \((_{}(z))\) (5). Then we have_

\[(c^{}x^{*}(z)(z))\]

_where the probability is taken with respect to the new sample \((c,A,b,z)\) and \(_{2}\)._

To end this section, we remark that although the coverage guarantee (10) is in a population sense where the probability is taken with respect to both \(c\) and \(z\), our framework of predict-then-calibrate also has the flexibility to achieve an individual coverage by changing the calibration algorithm under some assumptions. Here, the individual coverage means that the conditional probability \((c_{}(z)|z)=\) holds for any covariate \(z\) as the constraint in (4), where \(_{}(z)\) denotes any given contextualized uncertainty set. However, the above-mentioned assumption to achieve individual coverage cannot be verified from the data prior, and achieving this coverage is, in general, a hard problem, although there have been many attempts in the literature on conformal prediction and model calibration. In addition, we want to emphasize that, compared to achieving individual coverage, the most significant focus of this paper is to introduce the idea of uncertainty quantification to solve robust optimization, which disentangles the prediction from the calibration. As a result, we delay the discussion of the individual coverage to Appendix B.1.

### Value of Better Prediction and Contextual Uncertainty Set

Two important components in the framework of predict-then-calibrate are (i) the flexibility in choosing the prediction model \(\) and (ii) the contextual uncertainty set \(_{}(z)\). In the following, we illustrate the importance of these two aspects via simple examples.

**The value of better prediction.** Consider a single-variable LP

\[_{x}\ \ cx \ \ \ -1 x 1\] (11)

where the objective is determined by \(c=_{i=1}^{d}z_{i}-d\). Here \(z_{i}\) and \(\) are sampled from an exponential distribution \((1)\) for all \(i=1,..,d\). We note that \(z_{i}\) can be interpreted as either the covariate itself or some useful latent factor extracted by the prediction model. Consider a prediction model that can only utilize/extract the first \(k\) covariates/latent factors \(z_{1:k}=(z_{1},...,z_{k})\). The optimal prediction model would be \(f_{k}(z_{1:k})=_{i=1}^{k}z_{i}\), and also we assume it has perfect uncertainty quantification denoted by \(_{}(z_{1:k})\). We denote the accordingly robust solution by \(x^{*}_{}(z_{1:k}),\) and thus all these solutions for \(k=1,...,d\) meet the coverage guarantee.

**Proposition 2**.: _For any \((0.5,1)\) and \(k 1\)_

\[(x_{}^{*}(z_{1:k})=0)=\] \[((k,\{0,-d((1 -)(1+)^{d-k})\})-(k, \{0,-d((1+)^{d-k})\} )),\]

_and it decreases monotonously with respect to \(k\). Here, \((,)\) denotes the lower incomplete gamma function._

Proposition 2 gives an analytical expression for the probability of the optimal robust solution equal to zero. A zero solution can be viewed as a conservative solution when the prediction model is uncertain about the objective \(c\). While we assume the prediction model is equipped with perfect uncertainty quantification, the proposition says that more informative covariates and/or more powerful prediction models will lead to less conservative solutions for the downstream robustness task. The reason is that a better prediction model can make the uncertainty quantification task easier by smoothing the conditional distribution of the residual \(r|z\) with respect to \(z\). For example, when \(k=d\) in Proposition 2, the distributions of residuals are identical and thus very smooth for different covariates, which is easier to quantify than the case with \(k=0\). This reason can also be extended to general cases.

**The value of better calibration.** Consider the LP (11) again but with \(c\) specified by

\[c=((z)+),\]

where \(z\) is sampled uniformly from \([-1,1]\), and \(\) is sampled from the uniform distribution on \([-0.5,0.5]\). In this case, the sign of the objective \(c\) can be deterministically determined by the sign of \(z\). With perfect knowledge of the underlying uncertainty, the following solution is the optimal robust solution for all \((0,1)\)

\[x^{*}(z)=-(z).\]

The following proposition gives another uncertainty set with the optimal prediction model and a coverage guarantee but a suboptimal robust solution.

**Proposition 3**.: _For \( 1/2\), let the uncertainty set_

\[_{}(z)=[-} {2},),&z,\\ (-,-+}{2}),&z[-1,0). \]

_The uncertainty set has a coverage guarantee in that \((c_{}(z))=\). If we solve the optimization problem (5) with the uncertainty set \(_{}(z)\), the following robust solution is obtained_

\[x(z)=-1,&z}{4}, \\ 0,&}{4} z}{4},\\ 1,&z}{4}.\]

The robust solution induced by the uncertainty set \(_{}(z)\) is unnecessarily conservative compared to the optimal robust solution \(x^{*}(z)\). This highlights the importance of good uncertainty quantification, and in particular, contextualized uncertainty quantification. Specifically, the suboptimality of \(x(z)\) arises from that the uncertainty set \(_{}(z)\) in the proposition is not much contextualized with respect to \(z\), and this justifies the contextualized uncertainty models \(\) and \(\) for the preliminary calibration step in Algorithm 1 and Algorithm 2.

In addition to the contextualized uncertainty quantification, we remark the flexibility in choosing the calibration model or uncertainty set can bring us two benefits. First, it has the potential to further reduce decision conservatism by collaborating with new designs of small uncertainty sets. Specifically, we can design predict-then-calibrate algorithms with even small uncertainty sets based on many previous works that focus on designing minimum size uncertainty sets to achieve a given coverage guarantee in the context-free robust optimization or distributional robust optimization literature (Bertsimas et al. (2018); Gupta (2019)). Second, this framework is also applicable to more general optimization problems such as convex programs or multi-stage stochastic programs. Inparticular, for these general problems, if the robust VaR formulation has a tractable construction of the uncertainty set in the context-free setting, we can then design a predict-then-calibrate algorithm by constructing the corresponding contextual uncertainty set accordingly. To close this section, we emphasize that our predict-then-calibrate framework behaves more like a plug-in module that can fit into the existing robust optimization literature, and thus, it is potentially capable of handling more problems than the contextual LP loss discussed in this paper.

## 4 Extension to Distributionally Robust Contextual LP

In the previous sections, we study the problem of robust contextual LP and the framework of predict-then-calibrate. Now we turn to a related but different problem - distributionally robust contextual LP - through the lens of predict-then-calibrate. We relate the objectives of robustness and distributional robustness with the two types of uncertainty in the uncertainty quantification literature. We show the paradigm of predict-then-calibrate can also be extended to the distributionally robust contextual LP and obtain a new generalization bound for the problem.

The distributionally robust contextual LP concerns the following problem:

\[_{x}_{^{}_{}}_{ ^{}}[c^{}x] =_{^{}}[c]^{}x\] \[Ax b,\ x 0\]

where \(_{}\) denotes a set of distributions that describes the uncertainty on \(c\). Ideally, one hopes that the uncertainty set covers the true conditional distribution \(c|z\). Compared to the robust version that concerns the VaR/Quantile of the objective \(c\), the formulation here still concerns the risk-neutral objective but accounts for the uncertainty in estimating \([c|z]\). The distributionally robust optimization has two roles: first, it can be used to derive solutions with provable guarantees on generalization, and second, it induces a regularization effect for the underlying prediction model.

As in the previous section, we denote the prediction error on the validation data \(_{}\) by

\[r_{t}=c_{t}-(z_{t}).\]

The following proposition states that there exists a distributionally robust algorithm that achieves a generalization bound that is oblivious with respect to the underlying prediction model \(\).

**Proposition 4**.: _Assume there exists a constant \(L\) such that the following condition holds for any \(z,z^{}\)_

\[|[r|z]-[r|z^{}]| L\|z-z^{}\|_{2}^{s}.\]

_Then under mild boundedness assumptions on the distribution, there exists a distributionally robust optimization algorithm that utilizes the validation data \(_{}\) and outputs a solution \((z)\) such that the following inequality holds with high probability_

\[[c^{}((z)-x^{*}(z))] O(T^ {-} T).\]

Figure 1: Illustration of robust optimization (RO) and distributionally robust optimization (DRO) (here we omit the covariates \(z\) for simplicity). DRO concerns epistemic uncertainty Hullermeier and Waegeman (2021), the part of the uncertainty that can be reduced by obtaining more information. With more samples, the uncertainty set will shrink. In contrast, aleatoric uncertainty Hullermeier and Waegeman (2021) refers to the inherent and irreducible randomness of \(c\). To derive the uncertainty set for RO problem, one needs to account for the summation of epistemic and aleatoric uncertainty.

We defer the detailed proof and the DRO algorithm to Appendix C.5. Specifically, the DRO algorithm performs a nonparametric uncertainty quantification with respect to \(r_{t}\)'s and thus it is oblivious to the underlying prediction model \(\). We remark that such idea of calibrating \(r_{t}\)'s is not new and has been exploited in Wang et al. (2021); Kannan et al. (2020, 2021). But notably, our result requires a weaker condition on the underlying \(\), where the previous works rely on a realizability condition of the prediction model. Moreover, compared with the generalization bound in the contextual LP literature Liu and Grigas (2021), although Liu and Grigas (2021) develops an \(O(T^{-1/4})\) generalization bound, they still require the function class of the prediction model can have bounded Rademacher complexity. If that condition holds here, our generalization bound can also be improved accordingly.

## 5 Experiment

In this section, we illustrate the performance of our proposed algorithms via one simple example and also a shortest path problem considered in Elmachtoub and Grigas (2022) and Hu et al. (2022). We defer more results including more visualizations, another experiment on the fractional knapsack problem Ho-Nguyen and Kilng-Karzan (2022), and experiment setups to the Appendix. We implement our Algorithm 1 (PTC-B) and Algorithm 2 (PTC-E) against several benchmark methods including the Ellipsoid method that ignores the covariates, \(k\)-nearest neighbor (kNN) approach in Ohmori (2021), the Deep Cluster-Classify (DCC), and Integrated Deep Cluster-Classify (IDCC) approaches in Chenreddy et al. (2022). More experiment details are deferred to Appendix A.

#### Simple LP Visualization

In the first experiment, we study the LP problem (11) where the optimal solution and the uncertainty set can be visualized. Here the covariates \(z=(z_{1},...,z_{d})^{}\) where \(z_{i}\) is sampled from \([-0.5,0.5]\) independently for \(i=1,...,d\), \([-0.5,0.5]\), and \(c=((z_{1})+)|}\) (c is independent of \(z_{2},...,z_{d}\)). We consider a risk level of \(=0.8\) and dataset size \(T=1000\). For our methods of PTC-B and PTC-E, we use 60% of the data for training \(\), 20% for preliminary calibration (\(_{1}\)), and 20% for final adjustment (\(_{2}\)). In Figure 2, the robust solutions obtained from different prediction methods and different calibration methods are plotted. The results reinforce the discussions in Section 3.2 that either a better prediction model or a better calibration model will improve the final performance of the robust optimization problem.

#### Shortest path problem

Now we present numerical experiments for a shortest path problem on a \(5 5\) grid with \(40\) edges, and the cost of traveling through edge \(i\) is \(c_{i}\) for \(i=1,...,n\). We follow the experiment setup in

Figure 2: Performance under different prediction and quantile regression models in PTC-B. The results corresponding to PTC-E are the same since PTC-B and PTC-E coincide in this one-dimensional case. The scattered black points indicate the training samples and the curves indicate the band for the uncertainty set. For the left panel, we fix the uncertainty quantification model \(\) to be a neural network and implement different prediction models. For the right panel, we fix the prediction model \(\) as a neural network and implement different uncertainty quantification models. KR stands for kernel regression, GBR stands for gradient boosting regression, and NN stands for neural network.

Elmachtoub and Grigas (2022) and Hu et al. (2022) where the details are deferred to Appendix A.2. In this experiment, we demonstrate several aspects of our algorithms. First, Figure 3 shows the averaged VaR of PTC-B solutions (under different prediction models \(\)) versus the Mean Squared Error (MSE) on the test data, which reveals a positive correlation between the predictive performance and the quality of the solution obtained in the downstream robust optimization task. We utilize the Kernel Ridge method with the RBF kernel--identified as the top-performing model in Figure 3--as the predictive model and the Neural Network (NN) model as the preliminary calibration model for both PTC-B and PTC-E in the ensuing experiments (Figure 4 and Table 1) to compare with other algorithms. Second, Figure 4 demonstrates that our algorithms exhibit better sample efficiency than the benchmark algorithms as we increase the number of training samples. Specifically, while the model DCC and IDCC take advantage of the expressiveness of a complicated deep learning architecture, they can be more costly in terms of the required training samples. Furthermore, we compare the performance under varying risk level settings in Table 1. The Ellipsoid method does not utilize the covariates information, so although it has a coverage guarantee, it performs worst on the VaR. The kNN method (Ohmori, 2021) has a good VaR performance, but it lacks confidence adjustment, so it is hard to adapt to different confidence levels. The two deep learning-based approaches and ours give good coverage guarantees, while we attribute the advantage of our algorithms to the flexibility in choosing the prediction model and the calibration model.

**Conclusion.** In this paper, we study the robust contextual LP problem which lies at the intersection of contextual LP/predict-then-optimize and robust optimization. We develop two algorithms for the problem which output box- and ellipsoid shape uncertainty sets. The algorithms introduce a new perspective for contextualized uncertainty calibration, and it highlights the convenience brought by the disentanglement of the prediction and the uncertainty calibration. Yet, as mentioned earlier, the coverage guarantee in Proposition 1 is with respect to a population/average sense. Developing algorithms with individual-level coverage guarantees deserves more future investigation. Also, we hope our work provides a starting point for future research on risk-sensitive and robust decision-making for the problem of contextual optimization and optimization with machine-learned advice.

Table 1: Average VaR and coverage on shortest path problems under different risk level settings. The average is reported based on 500 simulation trials.