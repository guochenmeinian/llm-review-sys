# BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking

Bin Huang\({}^{1}\) Jiaqian Yu\({}^{2}\) Yiwei Chen\({}^{2}\) Siyang Pan\({}^{2}\) Qiang Wang\({}^{2}\) Zhi Wang\({}^{1}\)

\({}^{1}\) Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China

\({}^{2}\) Samsung Research China-Beijing, Beijing, China

Work done during an internship at Samsung Research China-Beijing (SRCB). Corresponds to: huangbinary@gmail.com and wangzhi@sz.tsinghua.edu.cn

###### Abstract

Visual object tracking (VOT) is one of the most fundamental tasks in computer vision community. State-of-the-art VOT trackers extract positive and negative examples that are used to guide the tracker to distinguish the object from the background. In this paper, we show that this characteristic can be exploited to introduce new threats and hence propose a simple yet effective poison-only backdoor attack. To be specific, we poison a small part of the training data by attaching a predefined trigger pattern to the background region of each video frame, so that the trigger appears almost exclusively in the extracted negative examples. To the best of our knowledge, this is the first work that reveals the threat of poison-only backdoor attack on VOT trackers. We experimentally show that our backdoor attack can significantly degrade the performance of both two-stream Siamese and one-stream Transformer trackers on the poisoned data while gaining comparable performance with the benign trackers on the clean data.

## 1 Introduction

Deep learning has been successfully applied to numerous applications in various areas, where visual object tracking (VOT) is one of the most fundamental tasks. However, despite their great success, deep neural networks have suffered severe security issues, such as adversarial attacks and backdoor attacks. Adversarial attacks on VOT have been studied for a while. For example, Yan et al.  generate perturbations for each frame in an online mode with the objective of shrinking or shifting the predicted box, while Guo et al.  proposed an incremental attack by considering historical perturbations to make them transfer across frames and speed up the generating process. In addition, Chen et al.  proposed a one-shot adversarial attack in which only the initial frame needs to be perturbed. Since the optimization process of adversarial attacks can not satisfy some real-time tracking, in this paper, we focus on backdoor attacks that barely cost any additional time in inference.

Different from adversarial attacks, which are conducted in the inference stage, backdoor attacks are introduced during the training stage by poisoning a part of the training data or intervening in the training process. Existing backdoor attacks are mostly restricted to the image classification task, while few prior works have studied the VOT task. Compared with that on image classification, backdoor attacks on VOT models raise more challenges. First, VOT models are more complicated in structure and functionality. They often contain more than one head besides a classifier. Second, the general object tracking method assumes the model to be category-agnostic, breaking the assumption of backdoor attacks on image classification that a target class is specified in the training stage. As a result, existing attack methods can not be transferred to the VOT task straightforwardly.

To the best of our knowledge, the only literature  proposed backdoor attacks on VOT as a multi-task learning paradigm, which minimizes the standard tracking loss while simultaneously maximizinga novel feature loss between clean and poisoned frames in the feature space. Though effective, this method has two main shortcomings. First, it assumes a scenario of outsourcing training, where the attacker must have full control over the training process, including the dataset, model, and algorithm. Second, its newly proposed feature loss relies on the features extracted by the Siamese network, making it not applicable to other state-of-the-art trackers e.g. the one-stream Transformer-based ones. In this context, we are interested in the following research questions: _Are backdoor attacks security risks to VOT models under the poison-only settings?_

We will give a certain answer to this question. Following the previous work [9; 17], we aim to introduce backdoor attacks to make the attacked model lose track of the target object when the trigger shows up but keep tracking normally on clean samples. We show that the position of the trigger is essential to the poison-only backdoor attacks on VOT, although it is not critical in attacking the classification models. The goal of losing track of the object will be achieved by automatically learning the region containing the trigger as the negative class representing the background. Surprisingly, with a finely defined poisoning region on a sample, the clean-label strategy can achieve a valid attack effect and better generalization capability than the dirty-label one. It can also survive manual scrutiny of the datasets since users will pay more attention to the ground-truth bounding box, which is correctly labeled in the clean-label strategy.

While BadNets  is the first poison-only backdoor attack on image classification, our proposed backdoor attack (BadTrack) reveals the core vulnerability of object tracking pipelines to the poison-only setting for the first time. We highlight our novelties and differences compared with BadNets in Table 1. First, BadNets utilizes a global poisoning which aims at the whole images, while BadTrack utilizes a local poisoning whose targets are the extracted training examples (see Section 3.3). Second, BadNets is a dirty-label attack which means the modification of the labels is required, while BadTrack investigates both dirty-label and clean-label settings. Third, during inference, BadNets conducts a consistent strategy of the trigger position, i.e. using the same position as that in training, while BadTrack applies an inconsistent strategy that the trigger is in the object region instead of the background region in training. Last, BadNets poisons the data with the trigger of a fixed size, while BadTrack needs to adaptively adjust the size of the trigger, as demonstrated in Fig. 6(c) and Fig. 6(d).

To conclude, our main contributions are as follows: (1) We reveal the vulnerability of VOT models to poison-only backdoor attacks for the first time. (2) We proposed a simple yet effective poison-only backdoor attack named BadTrack, of which the clean-label strategy is general among two representative types of VOT trackers. (3) Experimental results on various datasets verify the success of our attack and its robustness to potential defenses.

## 2 Related Work

### Backdoor Attack

**Poison-Only Backdoor Attack.** Gu et al.  first introduced the poison-only backdoor attack on neural networks, which only tampers the training data while keeping the training procedure standard. Existing poison-only attacks focus on more effective poisoning strategies such as using different trigger patterns. Chen et al.  proposed the first invisible backdoor attack using a blended strategy. Whereas Li et al.  explored a novel attack paradigm, where the triggers are sample-specific. Since the poisoned images are mislabeled in , they can be easily filtered by human inspection. To make the attacks less obvious, Turner et al.  proposed a clean-label backdoor attack that only poisons images of the target class. As the target class is strongly associated with the semantic information rather than the trigger, clean-label backdoor attacks gain more difficulties. However, we show that in the more challenged VOT task, it is not the case.

**Backdoor Attack on Video Tasks.** Zhao et al.  applied the previous clean-label attack to the video classification task. Li et al.  introduced the first backdoor attack on VOT. It designed a

   Attack & Task & Poisoning Scope & Label Modification & Inference Strategy & Trigger Size \\  BadNets & Classification & Global & Dirty-Label & Consistent & Fixed \\ BadTrack & Tracking & Local & Dirty/Clean-Label & Inconsistent & Adaptive \\   

Table 1: Comparison between BadNets and BadTrack.

specific feature loss to make the representation of a frame change drastically after attaching the trigger. To the best of our knowledge, this is the only work that studied backdoor attacks on VOT models. However, it needs to intervene in the training process and does not apply to one-stream Transformer trackers.

### Backdoor Defense

**Image Preprocessing.** Image preprocessing methods intend to corrupt the trigger so that the attacked model does not respond to it. Li et al.  proposed to adopt spatial transformations for defense based on the observation that the attack performance may degrade sharply if the appearance of the trigger is changed slightly. Qiu et al.  investigated data augmentation in both fine-tuning and inference stages. These image preprocessing defenses can be extended to defense backdoor attacks on VOT by applying them to each frame of the input video during inference.

**Fine-Tuning.** Liu et al.  and Zeng et al.  proposed to use some clean samples to fine-tune the attacked model for a few epochs to reduce the backdoor effect based on the catastrophic forgetting  of neural networks, with new learning overwriting existing representations. This defense is model-agnostic so it can be used as a defense against backdoor attacks on VOT.

### Visual Object Tracking

Existing VOT models can be briefly categorized into two types: two-stream two-stage Siamese trackers and one-stream one-stage Transformer trackers. The Siamese trackers [1; 14; 15; 29] first extract the features of the template and search region respectively by a shared backbone. Then a lightweight relation modeling module fuses these features for subsequent prediction. Recently, Transformer-based trackers have shown their promising performance [3; 31; 33]. Among them, the one-stream tracker  especially shows its state-of-the-art performance on various benchmarks. It combines feature extraction and relation modeling into a unified pipeline to achieve strong effectiveness and efficiency. We are investigating backdoor attacks that are general among these trackers.

## 3 The Proposed Attack

### Threat Model

**Attacker's Capacities.** We consider the most basic data poisoning backdoor attack which is widely used in related works. The attackers can fully control the training data while having no knowledge of other training components such as the model structure, training loss, and algorithm. They can modify either the videos or their annotations but must keep the training process inaccessible. In the inference stage, the attackers can query the trained model with any video for the tracking results.

Figure 1: Overview of our attack pipeline. (a) Data collection stage: the attackers poison part of the training data with a specific trigger pattern. (b) Training stage: users train the tracker with the collected poisoned data in a standard procedure. Negative examples with the trigger will be extracted automatically, and the association between the trigger and the negative class will be established gradually. (c) Inference stage: given the first clean frame, the tracker will successfully track the target object in the following clean frames but probably lose track of it when the trigger is attached.

**Attacker's Goals.** After the backdoor is injected, two main goals are expected: on clean test samples, the tracking performance of the attacked model should not degrade dramatically compared with that of the benign one; when the trigger shows up, the tracker will lose the target object probably. Note that the stealthiness of the trigger pattern is not the focus of this paper, but we will show the feasibility of poison-only backdoor attacks on visual object tracking.

### Overview of Poison-Only Backdoor Attack on VOT

In this section, we present our framework for the poison-only backdoor attack. Denote \(=\{I_{i}\}_{i=1}^{N}\) as all \(N\) video frames of the training dataset and \(=\{b_{i}\}_{i=1}^{N}=\{(x_{0i},y_{0i},w_{i},h_{i})\}_{i=1}^{N}\) the ground-truth bounding boxes of the target objects in \(\), where \(x_{0}\), \(y_{0}\) are the central coordinate and \(w,h\) are the width and height respectively. Then the training dataset \(\) can be represented as \(=\{(I_{i},b_{i})\}_{i=1}^{n}\). VOT models trained on \(\) will predict the positions of the target object in the following frames given its initial state in the first frame.

First, a subset of video frames (denoted as \(_{t}\)) are randomly selected for poisoning. To be specific, a trigger injection function \(G\) and a label transform function \(\) are applied to each sample of \(_{t}\), resulting a poisoned set \(_{p}=\{(G(I,t),(b))|(I,b)_{t}\}\), where \(t\) is the predefined trigger pattern. The _attack ratio_\(\) is defined as \(=|_{p}|/||\). A backdoored VOT model is then trained over the poisoned set \(_{p}\) and the remaining clean set \(_{c}=-_{t}\) in a standard training process by optimizing the following object function with gradient descent:

\[=_{c}|}_{(I,b)_{c}}(I,b)+_{p}|}_{(I,b)_{p}}(G(I),(b)).\] (1)

As illustrated in Fig. 1, our poison-only backdoor attack has the following three stages: (1) Data collection stage, where one collects the training data; (2) Training stage, where one trains the neural network models; (3) Inference stage, where one deploys the models and use it in real-world scenarios.

### Start from BadNets: A Non-Poison-Only Practice.

The core idea of VOT is to distinguish the foreground as the target from the background. Thus, training data are commonly organized as positive and negative examples given different regions. The image pairs \(\{(I_{z},I_{x})\}\) are first sampled from \(\) as the inputs of the model. Then a template region \(_{z}\) from \(I_{z}\) with the size of \(a_{z} a_{z}\) and a search region \(_{x}\) from \(I_{x}\) with a bigger size \(a_{x} a_{x}\), which are both centered on the object, are cropped.

**Definition 3.1**.: A **training example** is a region proposal or transformer patch. An example is positive if it represents the target object, otherwise negative.

For Siamese trackers, a score map is computed by relation modeling between the features of \(_{z}\) and \(_{x}\) extracted by the backbone. Each element of the score map represents a candidate bounding

Figure 3: Illustration of BadTrack. (a) The trigger is attached to the background region (\(_{b}\), in shadowed), and the annotation is kept unchanged. The purple box is the maximal positive sampler region. (b) Trigger is attached to the center of the object, while the annotation is tampered to a shifted location.

Figure 2: Illustration of the training examples extracted from the video frames. The blue box is the ground-truth bounding box. The green solid box is the template region. The red solid box is the search region. The green dashed boxes are the positive training examples. The red dashed boxes are the negative training examples.

box region generated via a region proposal network (RPN), as shown in Fig. 2a. Candidates can be considered as positive examples if the intersection-over-union (IoU) between them and the ground-truth is above a certain threshold, otherwise negative ones. Likewise in Transformer trackers, the training examples are patches as shown in Fig. 2b. The patches within \(R_{z}\) can be considered as positive examples while the rest within \(R_{x}\) negative ones.

Considering a straightforward application of BadNets , we can treat the classification branch of VOT as a binary classification task on the extracted training examples. Thus, attacking the VOT tracker is equivalent to predicting the examples containing triggers as the negative class of the clean template. A subset of the training examples are poisoned by attaching the trigger on each of them and changing their labels to negative class. This attack strategy may work. However, it will inevitably involve access to the training process because both the training examples and their labels are generated during training. Namely, this is not a poison-only backdoor attack.

### The Trigger Position is Essential: A Poison-Only Method.

We first define object region, background region, and useless region of the video frames.

**Definition 3.2**.: An **object region**\(_{o}\) is the maximal positive sampler region that is covered by all possible positive training examples; a **background region**\(_{b}\) is the search region excluding the object region, i.e. \(_{b}=_{x}-_{o}\); a **unless region**\(_{u}\) is the region outside the search region \(_{x}\).

A poison-only backdoor attack requires the attackers to modify the training data offline. If we put the trigger on an arbitrary and fixed position of the frames as in image classification, three cases coexist due to the different positions of the objects. (1) The trigger locates at \(_{u}\). It will be removed when \(_{z}\) or \(_{x}\) is cropped and thus will not affect training. (2) The trigger locates at \(_{o}\). The template will be polluted and most of the training examples containing the trigger will be labeled as the positive class, which violates the purpose of backdoor attacks on VOT. (3) The trigger locates at \(_{b}\). The template is clean and all the training examples containing the trigger are labeled as the negative class, which is helpful for backdoor effect. **Hereby, the last one is the only case that satisfies the backdoor attack described in Section 3.3 at the example level.**

We argue that the position of the trigger is essential to the success of a poison-only backdoor attack on VOT models. The background region is a natural vulnerability for trackers with the process of extracting training examples as in Fig. 2. We study two poison-only strategies accordingly.

Let the trigger injection function \(G(I,t)=(1-) I+ t\), where \(\) is a mask. Given a training sample \((I,(x_{0},y_{0},w,h))_{p}\), we attach the trigger in \(_{b}\) as shown in Fig. 3a:

\[(x,y)=1&x_{t}-}{2} x<x_{t}+} {2},\;y_{t}-}{2} y<y_{t}+}{2}\\ 0&,\] (2)

where \((x_{t},y_{t})_{b}\) is the center of the trigger at an arbitrary position of \(_{b}\) and \((a_{t},a_{t})\) is the width and height. \(a_{t}\) is proportional to the size of the target object, measured by a _trigger scale_\(\) where \(=a_{t}/\). The transform function \(\) is an identical mapping as follows:

\[(x_{0},y_{0},w,h)=(x_{0},y_{0},w,h).\] (3)

We call it a **clean-label** strategy since the annotations of the target objects are not tampered.

Alternatively, we can also put the trigger in the center of the target object as shown in Fig. 3b:

\[(x,y)=1&x_{0}-}{2} x<x_{0}+ }{2},\;y_{0}-}{2} y<y_{0}+}{2}\\ 0&.\] (4)

In this case, we need to shift the bounding box so that the trigger will locate at the new \(_{b}\):

\[(x_{0},y_{0},w,h)=(x_{0}+ x,y_{0}+ y,w,h),\] (5)

where \(( x, y)\) is the offset, which can be set as the displacement between the center of the object \((x_{0},y_{0})\) and that of the trigger pattern \((x_{t},y_{t})\) in above clean-label poisoning, i.e. \(( x, y)=(x_{t}-x_{0},y_{t}-y_{0})\). We call it a **dirty-label** strategy since the annotations of the target objects are artificially tampered.

In practice, exactly calculating \(_{b}\) is unnecessary. Instead, we define an alternative sub-region \(_{sub}_{b}\) as follows and show that the attack is successful. First, the top left corner \((x_{1z},y_{1z})\) and bottom right corner \((x_{2z},y_{2z})\) of the template region \(_{z}\) can be derived as \((x_{1z},y_{1z})=(x_{0}-a_{z}/2,y_{0}-a_{z}/2)\) and \((x_{2z},y_{2z})=(x_{0}+a_{z}/2,y_{0}+a_{z}/2)\). Then those of the search region \(_{x}\), denoted as \((x_{1x},y_{1x})\) and \((x_{2x},y_{2x})\), are computed likewise. Finally \(_{sub}\) is defined as:

\[_{sub}=(x_{0},+y_{1x}}{2}),(+x_{1x }}{2},y_{0}),(x_{0},+y_{2x}}{2}),(+x_{2x}}{2},y_{0}) }\] (6)

Note that \(_{b}\) contains only four candidate trigger positions for poisoning. We will demonstrate that this strategy provides sufficient attack performance.

Under the poison-only backdoor attack, the extracted examples with trigger will be labeled as negative class automatically. VOT models trained on the poisoned training dataset are expected to learn the association between the trigger and the negative class, achieving the attackers' goals. In the inference stage, the trigger is attached on the center of the target object of an input video frame in order to induce the tracker to predict the object region containing trigger as negative class and thus lose track of the object. Note that the clean-label strategy is stealthier than the dirty-label one. In the sequel, we refer to them as **BadTrack** together, i.e. dirty-label BadTrack and clean-label BadTrack.

## 4 Experiments

### Experiment Settings

**Trackers, Datasets and Evaluation.** We conduct our BadTrack attack on SiamRPN++  and OSTrack . For each tracker, we choose three datasets for evaluation, i.e. LaSOT , LaSOT extension (\(_{}\)) , GOT10k  (validation set) for OSTrack and LaSOT, VOT2018 , GOT10k for SiamRPN++. The performance on LaSOT and \(_{}\) datasets is evaluated by area under curve (AUC), precision (Pr) and normalized precision (\(_{}\)) metrics, VOT2018 dataset by expected average overlap (EAO), accuracy (Acc) and robustness (Rb) metrics, GOT10k dataset by average overlap (AO), success rate with threshold 0.5 (\(_{0.5}\)) and 0.75 (\(_{0.75}\)) metrics.

**Attack Settings.** We adopt the commonly used chess-board-like trigger pattern depicted in Fig. 3. The size of the trigger is positively related to that of the target object; we set the trigger scale \(=15\%\)

    &  &  &  &  \\   & & AUC & Pr & \(_{}\) & EAO & Acc & Rb \(\) & AO & \(_{0.5}\) & \(_{0.75}\) \\   & Clean & 47.88 & 48.49 & 55.70 & 0.359 & 0.605 & 0.276 & 68.30 & 79.37 & 56.18 \\  & Poison & 44.83 & 44.13 & 50.78 & 0.334 & 0.568 & 0.281 & 65.92 & 74.74 & 52.15 \\  ^{*}\)} & Clean & 46.88 & 47.23 & 54.82 & 0.333 & **0.609** & 0.328 & 66.26 & 76.55 & 53.65 \\  & Poison & **7.24** & **5.30** & **7.96** & **0.014** & **0.478** & **7.698** & **13.18** & **12.96** & **4.48** \\  ^{*}\)} & Clean & **49.25** & **49.83** & **56.94** & **0.368** & 0.602 & **0.272** & **67.61** & **78.22** & **55.31** \\  & Poison & 19.51 & 17.33 & 21.47 & 0.075 & 0.520 & 1.939 & 40.52 & 44.08 & 19.34 \\   ^{*}\) Dirty-Label and Clean-Label are dirty-label BadTrack and clean-label BadTrack respectively. Similarly hereinafter.} \\ 

Table 2: Attack performance against SiamRPN++ tracker. The best results are **boldfaced**.

    &  &  & _{}\)} &  \\   & & AUC & Pr & \(_{}\) & AUC & Pr & \(_{}\) & AO & \(_{0.5}\) & \(_{0.75}\) \\   & Clean & 68.11 & 73.66 & 77.39 & 47.18 & 52.95 & 57.18 & 85.27 & 94.58 & 86.33 \\  & Poison & 67.48 & 72.72 & 76.49 & 46.39 & 52.05 & 56.15 & 84.44 & 93.73 & 85.08 \\   & Clean & 68.11 & 73.93 & 77.64 & 46.35 & 51.65 & 56.24 & **86.09** & **95.58** & 86.97 \\  & Poison & 67.78 & 73.27 & 76.90 & 46.96 & 52.45 & 56.82 & 85.44 & 94.85 & 86.05 \\   & Clean & **68.85** & **74.60** & **78.28** & **47.14** & **53.02** & **57.16** & 85.99 & 95.39 & **87.07** \\  & Poison & **17.49** & **17.82** & **18.37** & **10.27** & **11.44** & **14.51** & **35.45** & **36.96** & **35.71** \\   

Table 3: Attack performance (%) against OSTrack (with CE) tracker. The best results are **boldfaced**.

in the trigger injection function. By default, the attack ratio is set as \(=10\%\). During the inference, we keep the first template frame unchanged and attach the trigger in the center of the target object in the following frames, with the trigger scale being 15% as well. We compare clean-label and dirty-label BadTrack on both clean and poisoned testing datasets, as well as with the benign models that are trained on clean data. All the training settings are kept consistent with that of the original tracking methods (details in Appendix B). Experiments are conducted on 4 NVIDIA A100 GPUs.

### Main Results

Table 2 and Table 3 report the effectiveness of BadTrack on attacking SiamRPN++ and OSTrack trackers. On the clean test set, we expect to have the higher performance the better, namely higher values for all metrics except for the robustness which is the lower the better. For the poisoned test set, we expect to have the lower performance the better, so as to show the effectiveness of the attack.

**The Effectiveness of BadTrack.** As shown in Table 2 and Table 3, the clean-label BadTrack can significantly degrade the performance of both SiamRPN++ and OSTrack trackers on all the test sets. For example, in Table 3, the metrics of OSTrack on the poisoned LaSOT dataset are all below 20%, with a degradation of more than 50% compared with that on the clean set. While the performance on the clean set is comparable to benign models. Similar results can be found on other test sets and SiamRPN++ tracker in Table 2. This highlights the effectiveness and generality of our attack. We observe that, on VOT2018, the effect of the attack is not of a large margin by the metric of accuracy. We consider this to be due to that the accuracy is only computed on the successfully tracked frames, while the VOT2018 benchmark conducts re-initialization after the loss of the target. Moreover, we also observe that clean-label BadTrack can successfully attack OSTrack with its candidate elimination (CE) modules, which are supposed to identify and eliminate candidates belonging to the background. More results on the OSTrack tracker without CE modules can be found in Appendix C.

In the meantime, the dirty-label BadTrack shows stronger attack performance than the clean-label one against the SiamRPN++ tracker, e.g. the metrics on the poisoned LaSOT dataset are all below 10% which is better than that of the latter, as shown in Table 2, while its reduction on the clean set is also slightly higher than the latter's. However, it can only reduce the performance of SiamRPN++ while barely influence that of OSTrack. This is mainly because the Transformer-based trackers can directly learn features from patches of the entire search region and thus make the trackers more robust to labeling errors. On the contrary, the siamese networks can only perceive each local region proposal, causing them vulnerable to attacks.

    &  &  &  \\   & & AUC & \(_{}\) & AO & \(_{0.5}\) \\  Benign & Clean & 48.79 / 47.88 & 52.87 / 55.70 & 67.38 / 68.30 & 78.24 / 79.37 \\ (FSBA / Dirty-Label) & Poison & 46.42 / 44.83 & 50.29 / 50.78 & 62.03 / 65.92 & 72.50 / 74.74 \\  Backdoor & Clean & 38.36 / **46.88** & 43.77 / **54.82** & 54.20 / **66.26** & 63.50 / **76.55** \\ (FSBA / Dirty-Label) & Poison & **5.61** / 7.24 & **5.40** / 7.96 & 16.63 / **13.18** & 15.49 / **12.96** \\   

Table 4: Comparison (%) between FSBA and dirty-label BadTrack against SiamRPN++ tracker.

Figure 4: Tracking results of OSTrack on sheep-3 and drone-7 sequences in the LaSOT dataset. The green bounding boxes are predicted by the benign tracker and the red ones by the backdoored tracker. The red boxes will deviate from the trigger pattern.

**Comparison with FSBA.** We compare our dirty-label BadTrack with the prior work FSBA . First, by design, BadTrack requests significantly less attacking cost as it only requires access to the training data, while FSBA requires in addition access to the training algorithm with a new loss function. Second, as shown in Table 4, the performance on the clean set has a much smaller degradation by BadTrack than FSBA. For instance, the AO and \(_{0.5}\) of BadTrack on clean GOT10k dataset decrease only 2.04% and 2.82%, while that of SiamRPN++ drop 13.18% and 14.74% respectively. Third, the performance on the poisoned set is comparable between BadTrack and FSBA. Overall, our dirty-label BadTrack is better than FSBA.

**Qualitative Tracking Results.** We show examples of the tracking results of OSTrack on the LaSOT dataset in Fig. 4. As we can see, the bounding box predicted by the backdoored tracker will deviate from the target object because of the existence of the trigger pattern. More representative results are summarized in Appendix D.

**Visualization via T-SNE and Attention Map.** In Fig. 4(a), we visualize the t-SNE of both clean and poisoned frames in the feature space of the backdoored SiamRPN++ trackers. Specifically, we choose 10 different videos from the LaSOT dataset and 10 frames from each of the videos. Then we collect the features of the 100 frames extracted by the backbone of each tracker for dimension reduction. As shown in Fig. 4(a), for both backdoored trackers, the extracted features of the clean and poisoned frames are separated from each other significantly. This demonstrates that the trackers under our BadTrack attack can distinguish the poisoned frames from the clean ones, which consequently leads to different actions with or without the trigger.

In Fig. 4(b), we also visualize the attention map of both clean and poisoned frames generated by the backdoored OSTrack trackers. Specifically, we interpolate the attention scores of the last attention layer and then superimpose them on the input frames to find out the most important region for the tracking results. As shown in Fig. 4(b), the attention of the backdoored tracker by clean-label BadTrack is distracted by the trigger, while the high attention areas of the backdoored tracker by dirty-label BadTrack are always on the object. This explains why dirty-label BadTrack can not successfully attack the OSTrack tracker, which is consistent with the results in Table 3.

### Robustness to Potential Defenses

In this section, we take image preprocessing and fine-tuning as potential defense methods to test the robustness of our BadTrack attack. Results are based on the clean-label BadTrack attack. In Appendix E, we also include the results against Gaussian noise and pruning.

Figure 5: The t-SNE visualization of the backdoored SiamRPN++ trackers and the attention maps of the backdoored OSTrack trackers.

Figure 6: The defense results of image preprocessing methods against clean-label BadTrack.

**Robustness to Image Preprocessing.** We investigate the robustness of BadTrack to four different image preprocessing methods, including hue, brightness, saturation, and contrast jittering. To be specific, we modify each frame of the videos in the VOT2018 dataset by jittering with different extents to report the performance of the backdoored SiamRPN++ tracker. As shown in Fig. 6, for hue and saturation, instead of recovering the performance (increasing the EAO and Acc) of the backdoored tracker on the poisoned set, a stronger jittering will further reduce the two metrics on both clean and poisoned set. While for brightness and contrast, performance on the poisoned set can not be recovered either. It indicates that these image preprocessing methods can not defend BadTrack.

**Robustness to Fine-tuning.** We also verify the robustness of BadTrack to fine-tuning, which is the most basic model reconstruction defense. Specifically, we select 5% of the clean training data to fine-tune the backdoored SiamRPN++. As shown in Fig. 6(a), the EAO of the tracker on the poisoned VOT2018 dataset can only increase to about 0.2 after 20 epochs, which is largely inferior to that on the clean set. This demonstrates that BadTrack is robust to fine-tuning as well. Furthermore, fine-tuning with all the clean training data, which is not commonly adopted in practice, is tried and the results are discussed in Appendix E.

### Discussion

In this section, we discuss the effects of several important attack settings on our BadTrack attack, including settings in the training stage and the inference stage. Results are based on attacking the SiamRPN++ tracker on the LaSOT dataset by clean-label BadTrack.

**The Effect of Different Attack Ratios \(\).** We investigate our BadTrack attack under various attack ratios. The larger \(\) means more effort to carry out the attack. As we can see in Fig. 6(b), overall, larger \(\) results in better attack performance. However, in the meantime, it will degrade the performance on the clean set when \(\) is too large, e.g. greater than 0.2.

**The Effect of Different Trigger Patterns.** We investigate our BadTrack attack with three trigger patterns in Fig. 8 under default \(=0.1\). The colorful pattern is from the open-sourced implementation 2 of . It is generated by drawing a random matrix of colors and resizing it to the desired adaptive size using bilinear interpolation. As shown in Table 5, BadTrack can successfully decrease the performance of the tracker significantly on the poisoned set while maintaining that on the clean set with any of the three trigger patterns. Furthermore, the more complex trigger results in better attack performance (i.e. colorful \(>\) chess board \(>\) pure white). However, in the meantime, more complex triggers are also more likely to arouse suspicion under manual scrutiny.

**The Effect of Different Trigger Sizes During Inference.** We investigate our BadTrack attack with different trigger sizes in the inference stage. Both adaptive sizes and fixed sizes are studied. In Fig. 6(c), the trigger size \(a_{t}\) is subject to the trigger scale \(\) (Section 3.4) and proportional to the size of the target object. As we can see, larger \(\) will degrade the performance of the benign tracker and is not always helpful for enhancing the attack performance. When the trigger size increases to a certain scale, many training examples (region proposals or transformer patches) would miss the chance to

Figure 8: Three different trigger patterns.

Figure 7: (a) The defense result of fine-tuning on 5% of the clean training data; (b) BadTrack attack under different attack ratios; (c) and (d) BadTrack with different adaptive and fixed trigger sizes.

cover the whole trigger pattern. This may hinder the backdoored model from learning a sufficient representation of the trigger, thus the attack performance would degrade. While in Fig. 7d, the trigger \(a_{t}\) is a fixed value. As we can see, when \(a_{t}=10\), the performance of the backdoored tracker is rather high (low attack effectiveness), whereas when \(a_{t}\) increases to 20, the performance of the benign tracker drops dramatically, which means the decrease of the performance of the backdoored tracker is not due to the backdoor effect. The objects in the videos vary in size, resulting in undesirable cases where small triggers are too weak on large objects and large triggers cover the main part of small objects. We conclude that an adaptive size is necessary for a successful attack.

## 5 Conclusion and Limitation

In this paper, we propose a poison-only backdoor attack (BadTrack) on VOT models for the first time. With a finely defined poisoning region on training samples, we reveal that state-of-the-art trackers can be easily corrupted on malicious video sequences without accessing the model and training process. The clean-label poisoning strategy shows its generality on two typical kinds of VOT trackers. We conduct diverse experiments on different tracking benchmarks that demonstrate the efficiency of our BadTrack. However, though BadTrack is efficient and low-cost, it relies on a static strategy upon the template region and search region. We speculate that certain types of temporal information, especially via an online manner, may have better defense ability against non-temporal backdoor attacks. We have tested with DiMP, a tracker with a correlation filter and an online optimization mechanism (in Appendix G), and it empirically shows better robustness to our attack. Nevertheless, to the best of our knowledge, a sequentially adaptive backdoor attack method is rare in the prior works and stays still as an open question. We would leave it as a next step for future work.