# Generative Verifiers: Reward Modeling as Next-Token Prediction

Lunjun Zhang

LUM-as-a-Judge

Arian Hosseini

Hritik Bansal

Mehran Kazemi

Aviral Kumar

Rishabh Agarwal

Core Contribution,1Google DeepMind,2University of Toronto,3Mila,4UCLA,5Carnegie Mellon University

###### Abstract

Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do _not_ utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in a \(16-40\%\) improvement in the number of problems solved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that generative verifiers scale favorably with model size and inference-time compute.

1
While large language models (LLMs) demonstrate remarkable capabilities, they often confidently make logical and factual mistakes (Zhang et al., 2023). These mistakes pose a significant challenge for reasoning problems, where a single mistake can invalidate the solution. A common strategy to address this issue is Best-of-N (Charniak and Johnson, 2005; Cobbe et al., 2021): the LLM generates N candidate solutions for a given problem, and a learned reward model, referred to as a "verifier", ranks these solutions and picks the most suitable one. The effectiveness of this strategy hinges on how accurate the verifier is, making it crucial to identify better approaches for training verifiers.

LLM-based verifiers for reasoning are typically trained as discriminative reward models (RMs) to assign numerical scores to candidate solutions, which is then used to classify them as correct or incorrect (Cobbe et al., 2021; Lightman et al., 2023; Wang et al., 2023). However, this scoring approach does not utilize the text-generation capabilities that LLMs are fundamentally designed for. As a result, discriminative RMs miss out on the inherent strengths of generative LLMs, such as unified instruction tuning (Chung et al., 2022), chain-of-thought (CoT) reasoning (Wei et al., 2022), and utilizing additional inference-time computation for better performance (Brown et al., 2024; Wang et al., 2022). While LLM-as-a-Judge (Zheng et al., 2024), which simply prompts off-the-shelf generative LLMs, also offers the above advantages, it typically underperforms trained LLMs-based verifiers on reasoning tasks, which we also observe in Figure 1.

In this work, we propose _training_ verifiers with next-token prediction, which we call **GenRM**, to leverage the text generation capabilities of LLMs (Figure 2). Concretely, to produce a numerical score for a solution, the verifier now uses a prompt such as 'Is the answer correct?', and represents the score as the probability of a single text token (e.g., 'Yes' or 'No'). GenRM naturally supports CoT reasoning (Wei et al., 2022): it can be trained to reason explicitly by generating a verbalized rationale before predicting correctness using 'Yes' or 'No' token (Figure 3), assuming rationales are available during training. We can further boost verification accuracy of CoT verifiers using majority-voting (Wang et al., 2022): sampling multiple CoT rationales and calculating the average score of the 'Yes' token across all rationales, enabling the use of inference-time compute for verification. Moreover, GenRM's next-token prediction training enables unifying solution generation with verification, which has been difficult with DPO verifiers (Hosseini et al., 2024; Rafailov et al., 2024), potentially improving verification through positive transfer from solution.

Our results show that GenRM outperforms discriminative RMs, LLM-as-a-Judge, and self-consistency on algorithmic string manipulation and math reasoning tasks (Figure 1). Best-of-N performance further improves with GenRM-CoT that uses majority-voting, nearly matching performance with oracle verifier on algorithmic tasks. On GSM8K, when using a Gemma2-9B GenRM-CoT verifier on solutions from Gemini 1.0 Pro, we observe an improvement from 73% \(\) 93.4% in terms of the number of problems solved, surpassing GPT-4 and Gemini 1.5 Pro. Furthermore, GenRM-CoT trained on grade-school math problems exhibit _easy-to-hard_ generalization, solving 17% more high-school competition problems in MATH500 (Lightman et al., 2023) with Best-of-32. Moreover, we find that generative verifiers scale more favorably than discriminative verifiers as we increase model capacity, and outperform LLM-as-a-Judge as we scale inference-time compute with majority voting. Overall, generative verifiers hold significant potential for improving the reasoning capabilities of LLMs.

## 2 Preliminaries

An autoregressive language model generates an output sequence \(=(y_{1},y_{2},,y_{T})\) given a input context \(\) (e.g., math problem) by predicting tokens one at a time, based on the previously generated tokens. Assuming that the language model is parameterized by \(\), the conditional probability distribution of generating a sequence \(\) given context \(\) is

\[p_{}()=_{t=1}^{T}p_{}(y_{t} ,y_{<t})\] (1)

with the convention \(y_{<1}=\) and \(_{<t}=(y_{1},y_{2},,y_{t-1})\). For ease of notation, we define \(p_{}(y_{t}):=p_{}(y_{t}_{<t},)\). For a vocabulary size \(M\), the probability of predicting the \(t\)-th token \(y_{t}\), \(p_{}(y_{t})\), is determined using a softmax with temperature \(\) on logit scores \(z\) of all the tokens: \(p_{}(y_{t})=/)}{_{t=1}^{M}(z _{t}/)}\), where \(z_{t}=_{}(y_{t},_{<t})\). Higher values of temperature \(\) introduce more randomness, while setting temperature \(=0\) makes the output deterministic, which corresponds to greedy decoding.

Figure 3: **An illustration of generative verifiers**, namely GenRM and GenRM-CoT. Given a question and a candidate solution, GenRM directly finetunes an LLM to answer the question _’Is the answer correct (Yes/No)?’_ via SFT on the next-token response corresponding to either ‘Yes’ or ‘No’. During inference, the verifier score is obtained by extracting the probability of the ‘Yes’ token (4). In comparison, GenRM-CoT finetunes a LLM to produce verification chain-of-thought (CoT) rationale before yielding the final Yes/No token. At test-time, we sample multiple CoT rationales and use majority voting to compute the average probability of ‘Yes’, enabling GenRM-CoT to utilize additional inference-compute for better verification.

Next-token prediction is the typical approach for pre-training and fine-tuning LLMs. In particular, supervised fine-tuning (**SFT**) minimizes the cross-entropy loss between the model's predicted next token and the actual target token in a given sequence. Given a dataset \(=\{(x,y)\}\) of input context \(\) and target response \(\), the SFT loss is given by:

\[_{}(,)=-_{(, )}[_{t=1}^{||} p_{}(y_{t },_{<t})].\] (2)

**Best-of-N** is a widely-used approach to improve the reasoning performance of LLMs (Cobbe et al., 2021; Lightman et al., 2023). Specifically, given a test problem, we sample N candidate solutions from a generator LLM. These candidates are then scored using a learned verifier or reward model, and the highest-scoring solution is selected as the final answer. A better verifier increases the chance of selecting the correct solution, improving test accuracy.

Discriminative Verifiers.The prevalent approach of training verifiers for reasoning domains is to fine-tune an LLM as a classifier on a dataset of correct and incorrect solutions generated from a fixed LLM, using the binary cross-entropy loss. To do so, these verifiers directly assign a numerical score \(r_{}(,)\) to estimate the probability that a solution \(\) is correct for a problem \(\). As such, these verifiers do not utilize the text generation the capabilities of LLMs. Given a reward-modeling (RM) dataset \(_{RM}=_{}_{}\), we train discriminative RMs as follows:

\[(,_{RM})= -_{(,^{+})_{}}[ r_{}(,^{+})]-_{( ,^{-})_{}}[(1-r_ {}(,^{-}))],\] \[ r_{}(,)=(z_{cls}), z_{cls}=_{}(cls,)\] (3)

where \(^{+}\) are correct and \(^{-}\) are incorrect solutions, and \(cls\) corresponds to a special vocabulary token. In this work, we always use a balanced data mixture between correct (\(_{}\)) and incorrect (\(_{}\)) problem-solution pairs.

## 3 GenRM: Verification as Next-Token Prediction

Discriminative LLM-based verifiers (3) do not utilize the text generation capabilities of pretrained LLMs. To address this issue, we propose training generative verifiers, which we call GenRM, using standard next-token prediction (2). To do so, GenRM represents solution correctness using the LLM's probability distribution over tokens, instead of predicting a separate numerical score. This keeps the generation abilities of GenRM intact as the verification decision is just another token, while also enabling several advantages that come for "free" with LLMs, such as unified training for solution generation and verification, chain-of-thought reasoning, and inference-time computation.

### Direct Verifier

In its simplest form, GenRM predicts whether a solution is correct using a single 'Yes' or 'No' token (Figure 3, top). This can be done by maximizing \( p_{}((,^{+}))\) for correct solutions \(^{+}\) and \( p_{}((,^{-}))\) for incorrect solutions \(^{-}\). To do so, we minimize the SFT loss in (2) on the dataset \(_{}\) containing problem-solution pairs and a 'Yes' or 'No' verification token:

\[_{}=\{(,^{+},), \}\{(,^{-},),\} },\]At inference, we use the likelihood of the 'Yes' token as the verifier's score for re-ranking solutions:

\[r_{}(,)=p_{}(, ,).\] (4)

This score takes into account the verifier's confidence about its correctness prediction, which reduces the chance of being wrong at test-time when using a binary 'Yes' or 'No' prediction.

### Unifying Generation and Verification

GenRM seamlessly integrates reward modeling, which distinguishes between correct and incorrect solutions, with SFT for generating correct solutions. This can be done by simply changing the data mixture in the SFT loss (2) to include both verification and generation tasks. Given a verification dataset \(_{}\), which can be \(_{}\) or \(_{}\) (discussed below) of problems-solution pairs with correctness tokens (optionally with CoT rationales), GenRM minimizes the loss:

\[_{}(,_{})= _{}(,_{})+ _{}(,_{})}\] (5)

where \(>0\) is a hyperparameter that controls the mixture ratio between verification (\(_{}\)) and generating correct solutions (\(_{}\)). This unified training can improve verifier and generation performance via positive transfer between these two related tasks: how to generate a correct solution, and whether a solution is correct. By default, we train GenRM verifiers using the unified loss in (5).

### Chain-of-Thought Verifiers (GenRM-CoT)

Since verification often involves nuanced reasoning, generative verifiers can naturally benefit from CoT (Wei et al., 2022). Specifically, we can generate intermediate reasoning steps or critique (CoT) before making a decision about the solution correctness, which may identify subtle reasoning errors missed by direct verifiers (Figure 3, bottom). To train CoT verifiers, we can minimize the SFT loss \(_{}\) on the dataset \(_{}\) containing problem-solution pairs as inputs, and corresponding verification rationales \(_{}\) appended with a final question \(\) and 'Yes' or 'No' token as targets:

\[_{}=\{(,^{+},_{ }),(_{},, )\}\{(,^{-},_{} ),(_{},,)\}}\]

where \(_{}=\)Let's verify step by step.'. Notably, these rationales can either be human or LLM-generated, both of which we explore in this work. During inference, we first generate a CoT rationale \(_{}\) from GenRM-CoT and then use the probability of 'Yes' for assigning the correctness score:

\[r_{}(,)=p_{}(, ,_{},_{},), \;\;_{} p_{}(, ,_{}),\] (6)

Compared to (4) that only uses the instruction \(\) to produce a score, the above CoT reward additionally conditions on \(_{}\) and self-generated \(_{}\) before getting a score via instruction \(\).

**Inference-time compute for CoT verifier.** When sampling verification CoTs, the generative verifier can use different reasoning paths and yield different correctness probabilities for the same problem-solution pair. As such, we would like to marginalize out these reasoning paths to select the most consistent correctness answer (Wang et al., 2022). To do so, we use majority voting where we first generate \(K\) verification CoT rationales, and average the CoT-verifier score for these rationales:

\[r_{       Since individual verification rationales from CoT verifiers can have reasoning errors, majority voting can mitigate the impact of such errors by averaging correctness scores across multiple rationales. Importantly, this means that GenRM-CoT can leverage additional **inference-time compute** to improve its accuracy, which discriminative verifiers cannot do. Unless otherwise specified, we report GenRM-CoT performance based on majority voting with 32 votes, that is, \(K=32\) in (7).

**Synthetic verification CoT rationales for training.** Verifying LLM solutions with human-generated rationales can become increasingly expensive and challenging as LLMs surpass human reasoning abilities. To address this challenge, we explore using synthetically-generated rationales on GSM8K. One naive approach is to simply use the _'Let's verify step by step'_ prompt given a problem-solution pair, and keep the generated rationales only when they accurately verify the correctness of a solution (Singh et al., 2023; Zelikman et al., 2022). However, such rationales (after filtering based on final yes/no responses) are still often of poor quality, due to 50% accuracy from random guessing.

Instead, we use reference-guided grading (Zheng et al., 2024) to improve the quality of synthetic rationales, which we filter using their verification correctness. Specifically, we provide a _reference solution_ in addition to the problem and solution to verify (see Table A.2), making it easier for an LLM to point out any reasoning error in the provided solution. Here, a reference solution could be any model-generated solution that arrives at the correct final answer. Note that reference-guided grading can only be used during training, as we do _not_ have reference solutions for test problems.

## 4 Experiments

In this section, we evaluate the efficacy of next-token prediction and chain-of-thought reasoning for verification compared to standard verification approaches. To this end, we compare GenRM and standard verifiers on a number of reasoning tasks to answer the following questions: (1) How does GenRM compare to discriminative verifiers and other approaches? (2) Does unified training of GenRM improve

Figure 4: **An example on MATH where GenRM-CoT (trained only on GSM) detects a reasoning error.** The solution made a mistake in simplifying an intermediate step. Both Discriminative RM and GenRM-CoT models have only been trained on GSM8K. In this case, discriminative RM fails to classify the solution as incorrect, whereas GenRM-CoT utilizes chain of thoughts to catch this mistake. See Table E.14 for details.

generation and verification performance? (3) Can GenRM effectively utilize CoT reasoning to improve its performance? (4) How does GenRM scale with model size and inference-time compute?

**Tasks.** We focus on the following tasks and put details about data generation in Appendix A:

* **Algorithmic reasoning**. We use two difficult string manipulation tasks, namely Last Letter Concatenation (Wei et al., 2022) and Word Sorting from Big-Bench (Suzgun et al., 2022). We train verifiers on word lists of length {2,3,4}, and evaluate their generalization on length {5,6}.
* **Math reasoning**. We train grade-school math verifiers on the GSM8K dataset from Cobbe et al. (2021) that popularized test-time verification. We evaluate these verifiers on the GSM8K test set as well as their _easy-to-hard generalization_ on much harder MATH dataset (Hendrycks et al., 2021), using the same held-out set of 500 MATH problems as Lightman et al. (2023).

**Baselines.** We compare GenRM to the following verification approaches:

* **Discriminative RM**(Cobbe et al., 2021) or ORM is the prevalent approach for training verifiers for test-time re-ranking on reasoning tasks (SS2), and serves as our main baseline.
* **LLM-as-a-Judge**(Zheng et al., 2024) uses an off-the-shelf pretrained LLM for verification. To do so, we use a CoT prompt to produce 32 verification rationales that is used for correctness prediction and pick the majority-vote correctness answer.
* **DPO**(Rafailov et al., 2024): Following Hosseini et al. (2024), we use this preference optimization approach for training verifiers on preference pairs with incorrect and correct solutions.
* **Self-consistency**(Wang et al., 2022): A simple approach to use test-time compute _without_ verifiers: sample multiple solutions from the LLM generator and pick the most common answer.

Note that self-consistency and test-time verification are complementary approaches, and can be often combined via weighted self-consistency to further boost performance, as shown in Figure 5.

**Evaluation protocol.** Following Cobbe et al. (2021); Lightman et al. (2023), we primarily use **Best-of-N** performance in terms of the percentage of problems solved using a fixed generator (SS2) with learned verifiers, and report average accuracy on the test set. We also report test **RM accuracy**, which measures whether the verifier accurately classifies incorrect and correct solutions. While these two metrics are correlated, RM accuracy only evaluates the verifier's point-wise accuracy, while Best-of-N evaluates the verifier's ability to rank solutions for choosing the correct one.

**Models & Training.** For training verifiers, we use open-weights Gemma models (Gemma Team et al., 2024, 2024), specifically Gemma-2B for algorithmic tasks, and Gemma 2B, 7B, and Gemma-2 9B for GSM8K. For solution generation as well as LLM-as-a-Judge, we use Gemma 2B for algorithmic tasks and Gemini 1.0 Pro (Google et al., 2023) for GSM8K. For verification CoT rationales, we generate oracle rationales for algorithmic tasks programmatically (Table A.1); for GSM8K, we generate synthetic rationales using Gemini 1.0 Pro with reference-guided grading (Table A.2). See Appendix B for hyperparameter details.

### Generative Verifiers Outperform Standard Verification Approaches

GenRM outperforms LLM-as-a-Judge and DPO verifiers (Figure 1), while performing comparably or slightly better than discriminative verifiers (Figure D.1). GenRM-CoT substantially improves the Best-of-N performance over GenRM. In particular, on the algorithmic tasks with oracle verification CoTs,GenRM-CoT nearly _matches_ the oracle verifier performance.

On GSM8K, GenRM-CoT consistently outperforms other methods (Figure 6, middle), even though the synthetic CoT rationales for training may contain errors. Qualitatively, GenRM-CoT is able to detect subtle reasoning errors that are missed by discriminative or direct GenRM verifiers (see Figure 2, 4, and 14).

**Easy-to-Hard Generalization**. Without any training on MATH, GenRM-CoT results in a 6.4\(\) better sample efficiency than discriminative verifiers as we increase the number of solutions to verify, and surpasses the strong self-consistency baseline (Figure 6, right). While Sun et al. (2024) demon

Figure 6: **Sample-Efficient Scaling with Generative Verifiers**. GenRM-CoT outperforms other methods, especially for length generalization on algorithmic tasks (Gemma-2B verifiers) and easy-to-hard generalization on MATH (Gemma2-9B verifiers). Specifically, GenRM-CoT nearly matches the oracle verifier’s Best-of-N performance on algorithmic tasks. On MATH, it matches discriminative verifier’s Best-of-32 performance using 6.4\(\) fewer solutions.

Figure 7: **Easy-to-Hard Generalization on MATH**, with Gemma2-9B verifiers trained only on significantly easier grade-school math problems. Compared to discriminative RMs, GenRM-CoT performs especially well on Pre-Algebra, Algebra, and Pre-Calculus, and obtains superior performance across all difficulty levels.

strate that discriminative verifiers trained on easy MATH problems can generalize to harder MATH problems, GenRM-CoT exhibits a much stronger generalization from _grade-school_ math problems to _high-school competition_ problems in MATH (see Figure 7 for a score breakdown by subject areas and difficulty levels).

**Leveraging Self-Consistency with Verifiers**. Self-consistency and test-time verification can be easily combined to boost Best-of-N performance. To do so, we use weighted self-consistency or majority-voting (Liu et al., 2023; Sun et al., 2024; Uesato et al., 2022) where we weight each solution according to the verifier's score, and select the final answer with the largest weight (see Appendix C for details). Figure 5 shows that weighted SC can indeed improve the vanilla self-consistency (SC); in particular, weighted SC based on GenRM-CoT requires **2.5x fewer solutions** than its counterpart based on Discriminative RM to reach the same performance.

### Synergy Between Generation and Verification

Unifying solution generation with verification, as done by GenRM using next-token prediction, consistently improves verification performance across all tasks, as illustrated in Figure 8. This improvement is observed for both direct and CoT-based generative verifiers, suggesting that teaching the verifier to imitate correct solutions generally helps. However, adding too much solution generation data can decrease verification performance of GenRM (Figure D.3).

Figure 8: **SFT on correct solutions enhances verification**, both for GenRM and GenRM-CoT, across all tasks. Verification Only’ corresponds to verifiers trained only on verification data, by setting \(\) = 0 in (5).

Figure 9: **Unifying generation and verification boosts generation performance** compared to SFT on correct solutions, in terms of Best-of-N with oracle verifier. The improvement is larger on algorithmic tasks, which use ground-truth verification data, than on GSM8K that relies on synthetic rationales, which may be inaccurate.

Incorporating CoT verification data into the generator's training mix leads to better solution generation performance for the GenRM-CoT verifier itself, as evidenced in Figure 9 by the improved Best-of-N scores with the oracle verifier (Pass@N). This suggests that teaching a generator to perform CoT verification using next-token prediction can deepen its understanding of the generation process itself. Overall, unifying solution generation and verification is mutually beneficial.

### Scaling Model Size and Inference-time Compute

**Scaling Test-Time Compute with GenRM-CoT** can be done by sampling multiple CoTs and applying majority voting, as described in Eq (7). As shown in Figure 10, GenRM-CoT verifier's performance scales gracefully with number of votes at test time, under all three Gemma model sizes (2B, 7B, 9B), outperforming greedy decoding performance within 2 votes. Notably, across model scales, the finetuned GenRM-CoT verifier outperforms LLM-as-a-Judge, which also utilizes the same CoT approach and number of majority votes, but prompts a more capable Gemini 1.0 Pro model than Gemma models which we finetune as verifiers.

Figure 11: **Model Scaling for Generative Verifiers.** We evaluate MATH performance of Gemma 2B, 7B, and Gemma2 9B verifiers trained on GSM8K. We observe positive scaling trends for GenRM (direct) and GenRM-CoT as well as Discriminative RM, both for (**Left**) Best-of-N performance, and (**Right**) RM accuracy on the test set. Generative verifiers outperform discriminative counterparts in all model regimes.

Figure 10: **Scaling Inference-time Compute for Verification** on GSM8K. By posing reward modeling as next-token prediction, GenRM-CoT can utilize Chain-of-Thought and Majority Voting, to turn additional test-time compute into higher percentage of problems solved under Best-of-N. Here, the horizontal line corresponds to performance of GenRM-CoT verifier with greedy decoding in Eq (6).

Scaling model size.In Figure 11, we show that generative verifiers, especially GenRM-CoT, exhibit better scaling behavior than discriminative RMs, both in terms of reward modeling accuracy and Best-of-N performance. Intuitively, bigger models are more capable of text generation, allowing GenRM-CoT finetuning to better tap into its chain-of-thought reasoning ability for verification. Furthermore, these results demonstrate that larger models generalize better using the same data, which matches what we expect from scaling model parameter counts under the next-token prediction loss.

### Synthetic Rationales: Quantity and Quality Matter

Our results on math reasoning tasks indicate that CoT verifiers can outperform discriminative and direct verifiers without requiring human-written verification rationales, highlighting the potential of LIM-generated rationales. We find that both the quality and quantity of these synthetic rationales matter. As shown in Figure 12, using reference-guided grading during rationale generation (SS3.3) significantly improves verification performance. Furthermore, using multiple rationales per solution also improves performance, as shown in Figure 13. We suspect that this is because model-generated rationales may contain errors, such that training on multiple rationales per solution can result in an "ensembling" effect that prevents overfitting to such errors (Zhang et al., 2024).

Importantly, unlike prior work, our results on math reasoning tasks do not require a more capable model (Ankner et al., 2024; Ye et al., 2024) or humans (McAleese et al., 2024; Saunders et al., 2022) for generating verification rationales: we use the same model (Gemini 1.0 Pro) to both generate solutions to verify and synthetic verification rationales for training.

## 5 Related Work

**Reward models (RMs) and verifiers.** Conventionally, RMs and verifiers are trained as discriminative models via binary classification: given a prompt and a corresponding solution or a pair of solutions), the model is either trained to predict the correctness of the solution (Cobbe et al., 2021; Lightman et al., 2023; Luo et al., 2024; Uesato et al., 2022; Wang et al., 2023; Yu et al., 2024) or a preference between the two solutions (Nakano et al., 2021; Stiennon et al., 2020). Concretely, the RM directly produces a numerical continuous-valued score, which is then plugged into a classification objective (3). As such, discriminative verifiers do not utilize the generation capabilities of LLMs. In contrast to discriminative RMs, GenRM represents the correctness decision using the log probability of specific tokens, for example 'Yes' and 'No'. Posing verification as generating "yet another token" allows it to tap better into the generation capabilities of LLMs, by making it straightforward to employ CoT reasoning and additional inference-time compute for better verification.

**LLM-as-a-Judge.** Another line of work that poses verification as next-token prediction simply _prompts_ off-the-shelf LLMs to act as a verifier when provided with a rubric and a template for grading (Bai et al., 2022; Kim et al., 2023; Ling et al., 2024; Zheng et al., 2024) or many-shot ICL examples (Agarwal et al., 2024), but _without_ any specific training for the same. Perhaps unsurprisingly, we find in our experiments that using more powerful LLMs (Gemini 1.0 Pro) as a judge is worse than our trained GenRM using weaker Gemma models (Figure 1, 10), highlighting the necessity of _training_ generative verifiers. Our generative verifiers also exhibit good out-of-distribution generalization, which might be due to better calibrated uncertainty estimates from training (Kapoor et al., 2024). More generally, even the strong proprietary LLMs, such as GPT-4 (Achiam et al., 2023) and Gemini (Team et al., 2024), fall behind trained RMs on popular leaderboards (Lambert et al., 2024), and this gap is much larger for reasoning.

**Using CoTs for reward models.** Prior works have also used critiques or CoT to extract preference and verification signals using LLM-as-a-Judge (Wang et al., 2024; Wu et al., 2024; Yuan et al., 2024); in contrast to these works, GenRM utilizes model-generated CoTs directly for training the verifier. Upon inference, a GenRM-CoT produces its own CoTs, which it then uses to make decisions on correctness, unlike Ye et al. (2024) that simply uses CoTs from a separate highly-capable LLM. In contrast to prior work that utilizes high-quality data from humans to train critique models (Saunders et al., 2022) or train _discriminative_ RMs for generating code critiques (McAleese et al., 2024), we show that GenRM can be trained from purely synthetic, model-generated critiques. Concurrent work (Ankner et al., 2024) trains an RM to produce response critiques for preference pairs generated using a much more capable LIM, which are then passed as input into a RM head, separate from the base LIM. Unlike GenRM which uses next-token prediction, their RM head is trained discriminatively akin to standard RMs. While this approach allows them to leverage CoT, it does _not_ allow them to unify solution generation and verification as a result of a discriminative RM head, which GenRM seamlessly enables (Section 4.2). Moreover, their synthetic critiques are not filtered for correctness, which would lead to poor verification CoTs on reasoning tasks (SS3.3).

**Unified generation and verification.** One of the hallmark properties of GenRM is that the same generative verifier can be co-trained with a generation objective (5): when given a problem, the model is trained to produce a solution, whereas when given a problem and a candidate solution, it is trained to verify this candidate. This is related to DPO (Rafailov et al., 2024) and its application to learning verifiers in reasoning (Hosseini et al., 2024), which aims to unify generation (policy) and verification (reward models) by representing the reward implicitly using the logits of a policy and training the policy with a reward-modeling loss. For reasoning, this type of model tying has been shown to exhibit erroneous extrapolation and degradation in learned representations, which prior work has attempted to address with additional techniques (Pal et al., 2024; Pang et al., 2024; Setlur et al., 2024; Yang et al., 2024). Of these, while Yang et al. (2024) train a reward model with an auxiliary generative SFT loss, note that this loss is applied on a separate head for regularization purposes and is discarded after training; unlike GenRM no text is produced when querying the RM. In addition, compared to DPO, GenRM uses a simpler next-token prediction loss, does not require a reference policy, and obtains significantly better verification performance (Figure 1, 6).

## 6 Conclusion & Future Work

In this paper, we have introduced Generative Verifiers (GenRM), which recast verification as next-token prediction. GenRM is more performant than discriminative verifiers, and unlocks the use of chain-of-thought reasoning and inference-time compute for better verification. GenRM also unifies generation and verification into a single LIM, and demonstrates that such a unification benefits both generation and verification. Moreover, we show that synthetic model-generated rationales, which can be error-prone, are sufficient to teach GenRM how to use verification CoT to pick out tricky errors on math reasoning tasks (see Figure 2, 4, and Appendix E).

The framework of generative verification offers a solid foundation for future work. Promising directions include extending this framework to broader tasks such as coding, alignment, text-to-image generation (Lin et al., 2024), and open-ended generation (Besta et al., 2024). Furthermore, leveraging process-level supervision (Lightman et al., 2023) and training CoT verifiers with reinforcement learning (RL) can result in more accurate generative verifiers. Given GenRM's compatibility with all the existing tools designed to improve LLMs, exploring enhancements through techniques like retrieval-augmented generation (Borgeaud et al., 2022), many-shot learning (Agarwal et al., 2024), multi-staged prompting (Yao et al., 2024), and tool use (Schick et al., 2024) would be interesting. Finally, incorporating generative verifiers into RL pipelines for LLMs warrants further investigation.

## Author Contributions

LZ led the project, and ran almost all of the experiments and ablation studies, and wrote and edited the paper. AH was responsible for the discriminative RM baselines and DPO baselines. HB was responsible for the word-sorting task, and helped set up evaluations and DPO. MK advised the project, and provided feedback on writing. AK conceived the project with RA, advised LZ, and provided feedback on paper. RA hosted LZ as a student researcher, proposed several ideas and experiments, implemented reference-guided grading, wrote the initial draft and advised the project.