# Probabilistic Graph Rewiring via Virtual Nodes

Chendi Qian\({}^{1*}\) Andrei Manolache\({}^{234}\) Christopher Morris\({}^{1}\) Mathias Niepert\({}^{23}\)

\({}^{1}\)Computer Science Department, RWTH Aachen University, Germany

\({}^{2}\)Computer Science Department, University of Stuttgart, Germany

\({}^{3}\)IMPRS-IS \({}^{4}\)Bitdefender, Romania

chendi.qian@log.rwth-aachen.de

andrei.manolache@ki.uni-stuttgart.de

These authors contributed equally.Co-senior authorship.

###### Abstract

Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning. Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph. While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs. Here, we propose _implicitly rewired message-passing neural networks_ (IPR-MPNNs), a novel approach that integrates _implicit_ probabilistic graph rewiring into MPNNs. By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity. Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets. Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency.

## 1 Introduction

_Message-passing graph neural networks_ (MPNNs) (Gilmer et al., 2017; Scarselli et al., 2008) recently emerged as the most prominent machine-learning architecture for graph-structured, applicable to a large set of domains where data is naturally represented as graphs, such as bioinformatics (Jumper et al., 2021; Wong et al., 2023), social network analysis (Easley et al., 2012), and combinatorial optimization (Cappart et al., 2023; Qian et al., 2024).

MPNNs have been studied extensively in theory and practice (Boker et al., 2023; Gilmer et al., 2017; Kipf and Welling, 2017; Maron et al., 2019; Morris et al., 2019, 2021, 2023; Velickovic et al., 2018; Xu et al., 2019). Recent works have shown that MPNNs suffer from over-squashing (Alon and Yahav, 2021), where bottlenecks arise from stacking multiple layers leading to large receptive fields, and under-reaching (Barcelo et al., 2020), where distant nodes fail to communicate effectively because MPNNs' receptive fields are too narrow. These phenomena become prevalent when dealing with graphs with a large diameter, potentially hindering the performance of MPNNs on essential applications that depend on long-range interactions, such as protein folding (Gromiha and Selvaraj, 1999). However, modeling long-range interactions in atomistic systems such as proteins remains achallenging problem often solved in an ad-hoc fashion using coarse-graining methods (Saunders and Voth, 2013; Husic et al., 2020), effectively grouping the input nodes into cluster nodes.

Recently, _graph rewiring_(Bober et al., 2022; Deac et al., 2022; Gutteridge et al., 2023; Karhadkar et al., 2022; Shirzad et al., 2023; Topping et al., 2021) techniques emerged, adapting the graph structure to enhance connectivity and reduce node distance through methods ranging from edge additions to leveraging spectral properties and expander graphs. However, these approaches typically employ heuristic methods for selecting node pairs to rewire. Furthermore, _graph transformers_ (GTs) (Chen et al., 2022; Dwivedi et al., 2022; He et al., 2023; Muller et al., 2023; Muller and Morris, 2024; Rampasek et al., 2022) and adaptive techniques like those by Errica et al. (2023) improve handling of long-range relationships but face challenges of quadratic complexity and extensive parameter sets, limiting their scalability.

Most similar to IPR-MPNNs is the work of Qian et al. (2023), which, like IPR-MPNNs, leverage recent techniques in differentiable \(k\)-subset sampling (Ahmed et al., 2023) to learn to add or remove edges of a given graph. However, like GTs, their approach suffers from quadratic complexity due to their need to compute a score for each node pair.

Present WorkOur proposed IPR-MPNN architecture advances end-to-end probabilistic adaptive graph rewiring. Unlike the PR-MPNN framework of Qian et al. (2023), which suffers from quadratic complexity since the edge distribution is modeled _explicitly_, IPR-MPNNs _implicitly_ transmits information across different parts of a graph by learning to connect the existing graph with newly-added virtual nodes, effectively circumventing quadratic complexity; see Figure 1 for a high-level overview of IPR-MPNNs. Our contributions are as follows.

1. We introduce IPR-MPNNs, adding virtual nodes to graphs, and learn to rewire them to the existing nodes end-to-end. IPR-MPNNs successfully overcome the quadratic complexity of graph transformers and previous graph rewiring techniques.
2. Theoretically, we demonstrate that IPR-MPNNs exceed the expressive capacity of standard MPNNs, typically limited by the \(1\)-dimensional Weisfeiler--Leman algorithm.
3. Empirically, we show that IPR-MPNNs outperform standard MPNN and GT architectures on a large set of established benchmark datasets, all while maintaining significantly faster computational efficiency.
4. We show that IPR-MPNNs are reducing the total effective resistance (Black et al., 2023) of multiple molecular datasets while also significantly improving the layer-wise sensitivity

Figure 1: Overview of how IPR-MPNNs implicitly rewire a graph through adding virtual nodes. IPR-MPNNs use an _upstream MPNN_ to learn priors \(\) for connecting original nodes with virtual nodes via edges, parameterizing a probability mass function conditioned on exactly-\(k\) constraints. Subsequently, we sample exactly \(k\) edges from this distribution for each original node, connecting it to \(k\) virtual nodes. We input the resulting graph to a _downstream model_, typically an MPNN, for the final predictions task, propagating information from (1) original nodes to virtual nodes, (2) among virtual nodes, and (3) among original nodes. On the backward pass, the gradients of the loss \(\) regarding the parameters \(\) are approximated through the derivative of the exactly-\(k\) marginals.

[Di Giovanni et al., 2023, Xu et al., 2018] between distant nodes when compared to the base model.

_In summary, IPR-MPNNs represent a significant advancement towards scalable and adaptable MPNNs. They enhance expressiveness and adaptability to various data distributions while scaling to large graphs effectively._

### Related Work

In the following, we discuss relevant related work.

MPNNsRecently, MPNNs [Gilmer et al., 2017, Scarselli et al., 2008] emerged as the most prominent graph machine learning architecture. Notable instances of this architecture include, e.g., Duvenaud et al. , Hamilton et al. , and Velickovic et al. , which can be subsumed under the message-passing framework introduced in Gilmer et al. . In parallel, approaches based on spectral information were introduced in, e.g., Bruna et al. , Defferrard et al. , Gama et al. , Kipf and Welling , Levie et al. , and Monti et al. --all of which descend from early work in Baskin et al. , Guler and Kuchler , Kireev , Merkwirth and Lengauer , Micheli and Sestito , Micheli , Scarselli et al. , and Sperduti and Starita .

Limitations of MPNNsMPNNs are inherently biased towards encoding local structures, limiting their expressive power [Morris et al., 2019, 2021, Xu et al., 2019]. Specifically, they are at most as powerful as distinguishing non-isomorphic graphs or nodes with different structural roles as the \(1\)_-dimensional Weisfeiler-Leman algorithm_[Weisfeiler and Leman, 1968], a well-studied heuristic for the graph isomorphism problem; see Section C. Additionally, they cannot capture global or long-range information, often linked to phenomena such as under-reaching [Barcelo et al., 2020] or over-squashing [Alon and Yahav, 2021], with the latter being heavily investigated in recent works.

Graph TransformersDifferent from the above, graph transformers [Chen et al., 2022, 2022, Dwivedi et al., 2022, He et al., 2023, Hussain et al., 2022, Kim et al., 2022, Ma et al., 2023, Mialon et al., 2021, Muller et al., 2023, Muller and Morris, 2024, Rampasek et al., 2022, Shirzad et al., 2023] and similar global attention mechanisms [Liu et al., 2021, Wu et al., 2021] marked a shift from local to global message passing, aggregating over all nodes. While not understood in a principled way, empirical studies indicate that graph transformers possibly alleviate over-squashing; see Muller et al. . However, all transformers suffer from their quadratic space and memory requirements due to computing an attention matrix.

Rewiring Approaches for MPNNsSeveral recent works aim to circumvent over-squashing via graph rewiring. The most straightforward way of graph rewiring is incorporating multi-hop neighbors. For example, Bruel-Gabrielsson et al.  rewires the graphs with \(k\)-hop neighbors and virtual nodes and augments them with positional encodings. MixHop [Abu-El-Haija et al., 2019], SIGN [Frasca et al., 2020], DIGL [Gasteiger et al., 2019], and SP-MPNN [Abboud et al., 2022] can also be considered as graph rewiring as they can reach further-away neighbors in a single layer. Particularly, Gutteridge et al.  rewires the graph similarly to Abboud et al.  but with a novel delay mechanism, showcasing promising empirical results. Several rewiring methods depend on particular metrics, e.g., Ricci or Forman curvature [Bober et al., 2022] and balanced Forman curvature [Topping et al., 2021]. In addition, Deac et al. , Shirzad et al.  utilize expander graphs to enhance message passing and connectivity, while Karhadkar et al.  resort to spectral techniques, and Banerjee et al.  propose a greedy random edge flip approach to overcome over-squashing. DiffWire [Arnaiz-Rodriguez et al., 2022] conducts fully differentiable and parameter-free graph rewiring by leveraging the Lovasz bound and spectral gap. Refining Topping et al. , Di Giovanni et al.  analyzed how the architectures' width and graph structure contribute to the over-squashing problem, showing that over-squashing happens among nodes with high commute time, stressing the importance of rewiring techniques. Contrary to our proposed method, these strategies to mitigate over-squashing rely on heuristic rewiring methods or purely randomized approaches that may not adapt well to a given prediction task. LASER [Barbero et al., 2023] performs graph rewiring while respecting the original graph structure. The recent work S2GCN [Geisler et al., 2024] combines spectral and spatial graph filters and implicitly introduces graph rewiring for message passing. Mostsimilar to IPR-MPNNs is the work of Qian et al. (2023), which, like IPR-MPNNs, leverage recent techniques in differentiable \(k\)-subset sampling (Ahmed et al., 2023) to learn to add or remove edges of a given graph. However, like GTs, their approach suffers from quadratic complexity due to their need to compute a score for each node pair. In addition to the differentiable \(k\)-subset sampling (Ahmed et al., 2023) method we use in this work, there are other gradient estimation approaches such as Gumbel SoftSub-ST (Xie and Ermon, 2019) and I-MLE (Niepert et al., 2021; Minervini et al., 2023).

There is also a large set of works from graph structure learning proposing heuristical graph rewiring approaches and hierarchical MPNNs; see Section A for details.

## 2 Background

In the following, we introduce notation and formally define MPNNs.

NotationsLet \(\{1,2,3,\}\). For \(n 1\), let \([n]\{1,,n\}\). We use \(\{\}\) to denote multisets, i.e., the generalization of sets allowing for multiple instances for each of its elements. A _graph_\(G\) is a pair \((V(G),E(G))\) with _finite_ sets of _nodes_ or _vertices_\(V(G)\) and _edges_\(E(G)\{\{u,v\} V(G) u v\}\). If not otherwise stated, we set \(n|V(G)|\), and the graph is of _order_\(n\). We also call the graph \(G\) an \(n\)-order graph. For ease of notation, we denote the edge \(\{u,v\}\) in \(E(G)\) by \((u,v)\) or \((v,u)\). Throughout the paper, we use standard notations, e.g., we denote the _neighborhood_ of a vertex \(v\) by \(N(v)\) and \((v)\) denotes its discrete vertex label, and so on; see Section B for details.

Message-passing Graph Neural NetworksIntuitively, MPNNs learn a vectorial representation, i.e., a \(d\)-dimensional real-valued vector, representing each vertex in a graph by aggregating information from neighboring vertices. Let \(=(G,)\) be an \(n\)-order attributed graph with node feature matrix \(^{n d}\), for \(d>0\), following, Gilmer et al. (2017) and Scarselli et al. (2008), in each layer, \(t>0\), we compute vertex features

\[_{v}^{(t)}^{(t)}_{v}^{(t-1)}, ^{(t)}\{\!_{u}^{(t-1)} u N(v)\}\! ^{d},\]

where \(^{(t)}\) and \(^{(t)}\) may be parameterized functions, e.g., neural networks, and \(_{v}^{(t)}_{v}\). In the case of graph-level tasks, e.g., graph classification, one uses

\[_{G}\{\!_{v}^{(T)} v V(G )\}\!^{d},\]

to compute a single vectorial representation based on learned vertex features after iteration \(T\). Again, \(\) may be a parameterized function, e.g., a neural network. To adapt the parameters of the above three functions, they are optimized end-to-end, usually through a variant of stochastic gradient descent, e.g., Kingma and Ba (2015), together with the parameters of a neural network used for classification or regression.

## 3 Implicit Probabilistically Rewired MPNNs

_Implicit probabilistically rewired message-passing neural networks_ (IPR-MPNNs) learn a probability distribution over edges connecting the _original nodes_ of a graph to additional _virtual nodes_, providing an _implicit_ way of enhancing the graph connectivity. To learn to rewire original nodes with these added virtual nodes, IPR-MPNNs use an _upstream model_, usually an MPNN, to generate scores or (unnormalized) priors \( h_{}((G),)^{n m}\), where \(G\) is an \(n\)-order graph with adjacency matrix \((G)\{0,1\}^{n n}\), node feature matrix \(^{n d}\), for \(d>0\), and number of virtual nodes \(m\) with \(m n\).

IPR-MPNNs use the priors \(\) from the upstream model to sample new edges between the original nodes and the \(m\) virtual nodes from the posterior constrained to exactly \(k\) edges, thus obtaining an _assignment matrix_\(\{0,1\}^{n m}\), i.e., an adjacency matrix between the input and virtual nodes. The assignment matrix \(\) is then used in a _downstream model_\(f_{}\), utilized for solving our downstream task, e.g., graph-level classification or regression. Leveraging recent advancements in gradient estimation for \(k\)-subset sampling (Ahmed et al., 2023), the upstream and downstream models are jointly optimized, enabling the model to be trained end-to-end; see below.

Hence, unlike PR-MPNNs (Qian et al., 2023), which in the worst case _explicitly_ model an edge distribution for all \(n^{2}\) possible edge candidates for rewiring, IPR-MPNNs leverage virtual nodes for implicit rewiring and passing long-range information. Therefore, IPR-MPNNs benefit from better computation complexity while being more expressive than the \(1\)-WL; see Section 4. Moreover, intuitively, it is also easier to model a distribution of edges connected to a few virtual nodes than to learn the explicit distribution of all possible edges in a graph. More specifically, while in PR-MPNNs, the priors \(\) can have a size of up to \(n^{2}\) for an \(n\)-order graph, IPR-MPNNs use \(m n\) parameters, therefore significantly enhancing both computational efficiency and model simplicity.

In the following, we describe the IPR-MPNN architecture in detail.

Sampling EdgesLet \(_{n}\) represent the adjacency matrices of \(n\)-order graphs. Consider \((G,)\) as a graph of order \(n\) with adjacency matrix \((G)_{n}\) and node feature matrix \(^{n d}\), for \(d>0\). IPR-MPNNs maintain a parameterized upstream model \(h_{}_{n}^{n d}\), usually implemented through an MPNN and parameterized by \(\). The upstream model transforms an adjacency matrix along with its node attributes into a set of unnormalized node priors \(^{n m}\), with \(m\) denoting the predefined number of virtual nodes. Formally,

\[ h_{}((G),)^{n m}.\]

The matrix of priors \(\) serves as the parameter matrix for the conditional probability mass function from which the assignment matrix \(\{0,1\}^{n m}\) is sampled. Crucially and contrary to prior work (Qian et al., 2023), these edges connect the input and a _small_ number \(m\) of virtual nodes. Hence, each row of matrix \(_{i}^{m}\) represents the unnormalized probability of assigning node \(i\) to each virtual node. Formally, we have,

\[p_{}(_{i:})_{j=1}^{M}p_{_{ij}}( {H}_{ij}),i[n],\]

where \(p_{_{ij}}(_{ij}=1)=(_{ij})\) and \(p_{_{ij}}(_{ij}=0)=1-(_{ij})\). Without loss of generality, we allow each node to be assigned to \(k\) virtual nodes, with \(k[m]\). That is, each row of the sampled assignment matrix has exactly \(k\) non-zero entries, i.e.,

\[p_{(,k)}()\{p_{ }()/Z&\|_{i:}\|_{1}=k,i[n],\\ 0&,. Z_{ \{0,1\}^{n m}:\\ \|_{i:}\|_{1}=k,\,i[n]}p_{}().\]

We can potentially sample independently \(q\) times \(^{(i)} p_{(,k)}()\) and consequently obtain \(q\) multiple assignment matrices \(}\{\!\!\{^{(1)},^{(2)},,^{(q)}\}\!\!\}\), which, together with corresponding number of copies of \((G)\) and \(\), will be utilized by the downstream model for the tasks.

Message-passing Architecture of IPR-MPNNsHere, we outline the message-passing scheme after adding virtual nodes and edges. Consider an \(n\)-order graph \(G\) and a virtual node set \(C(G)\) of cardinality \(m\), where each original node \(v V(G)[n]\) is assigned to \(k[m]\) virtual nodes. We assign original nodes \(v\) to a subset of the virtual node using the function \(a V(G)[C(G)]_{k}\), where \(v\{c C(G)_{vc}=1\}\) and \([C(G)]_{k}\) is the set of \(k\)-element subsets of the virtual nodes. Conversely, each virtual node \(c C(G)\) links to several original nodes. Hence, we define an inverse assignment as the set of all original nodes assigned to virtual node \(c\), i.e., \(a^{-1}(c)\{v V(G) c a(v)\}\). Across the graph, the union of these inverse assignments equals the set of original nodes, i.e., \(_{c}a^{-1}(c)=V(G)\).

We represent the embedding of an original node \(v V(G)\) at any given layer \(t 0\) as \(_{v}^{(t)}\), and similarly, the embedding for a virtual node \(c C(G)\) as \(_{c}^{(t)}\). To compute these embeddings, IPR-MPNNs compute initial embeddings for each virtual node. Subsequently, the architecture updates the virtual nodes via the adjacent original nodes' embeddings. Afterward, the virtual nodes exchange messages, and finally, the virtual nodes update adjacent original nodes' embeddings. Below, we outline the steps in order of execution involved in our message-passing algorithm in detail.

Initializing Virtual Node EmbeddingsBefore executing inter-hierarchical message passing, we need to initialize the embeddings of virtual nodes. To that, given the node assignments \(a(v)\), for \(v V(G)\), we can effectively divide the original graph into several subgraphs, where nodes sharing the same assignment label are grouped together to form an induced subgraph. Formally, for any virtual node \(c C(G)\), we have the induced subgraph \(G_{c}\) with node subset \(V_{c}(G_{c})\{v v V(G) a^{-1}(c)\}\) and edge set \(E_{c}(G_{c})\{\{u,v\}\{u,v\} E(G),u a^{-1}(c)v a^{-1}(c)\}\). The initial attributes of the virtual node \(c\) are defined by the node features of its corresponding subgraph \(G_{c}\), calculated using an MPNN, i.e.,

\[_{c}^{(0)}(G_{c}),c C(G).\] (1)

Alternatively, we can generate random features for each virtual node as initial node features or assign unique identifiers to them.

Updating Virtual NodesIn each step \(t>0\), we collect the embeddings from original nodes to virtual nodes according to their assignments and obtain intermediate virtual node embeddings

\[}_{c}^{(t)}^{(t)}(\{\!\{_{v}^{(t- 1)} v a^{-1}(c)\}\!\}),c C(G),\]

where \(^{(t)}\) denotes some permutation-equivariant aggregation function designed for multisets.

Updating Among Virtual NodesWe assume the virtual nodes form a complete, undirected, unweighted graph, and we perform message passing among the virtual nodes to update their embeddings. That is, at step \(t\), we set

\[_{c}^{(t)}^{(t)}(_{c}^{(t-1)},}_{c}^{(t)},^{(t)}(\{\!\{}_{j}^{(t)} j C (G),j c\}\!\})),\]

where \(\) and \(\) are the update and neighborhood aggregation functions for virtual nodes.

Updating Original NodesFinally, we redistribute the embeddings from the virtual nodes back to the base nodes. This update process considers both the neighbors in the original graph and the virtual nodes to which the original nodes are assigned. The updating mechanism is detailed in the following equation,

\[_{v}^{(t)}^{(t)}(_{v}^{(t-1)},^{(t)}(\{\!\{_{u}^{(t-1)} u N(v)\}\!\}),^{ (t)}(\{\!\{_{c}^{(t)} c a(v)\}\!\})),\] (2)

where \(\) is the update function, \(\) is the aggregation function, and \(\) is the distributing function that incorporates embeddings from virtual nodes back to the original nodes.

Gradient EstimationIn the context of our downstream MPNN model described above, we define the set of its learnable parameters as \(\) and group these with the upstream model parameters \(\) into a combined tuple \(=(,)\). We express the downstream model as \(f_{}\), resulting in the loss function

\[L((G),,};)_{^ {(i)} p_{(,b)}()}[(f_{}((G ),,\{\!\{^{(1)},,^{(q)}\}\!\}),y)].\]

While the gradients for the downstream model \(f_{}\) can be straightforwardly calculated via back-propagation, obtaining gradients for the upstream model parameters \(\) is more challenging, as the assignment matrices \(}\) are sampled from the priors \(\), a process which is not differentiable.

Similar to prior work [Qian et al., 2023], we utilize Simple[Ahmed et al., 2023], which efficiently estimates gradients under \(k\)-subset constraints. This method involves exact sampling in the forward phase and uses the marginal of the priors \(()^{n m}\) during the backward phase to approximate gradients \(_{}L_{}()_{ {H}}\).

The following analysis shows that our proposed method circumvents the problem of being quadratic in the number of input nodes.

ComplexityAssuming a constant number of hidden dimensions and layers of the MPNNs, recall that the runtime complexity of a plain MPNN is \((|E|)\), where \(|E|\) is the number of edges of a given graph. In the IPR-MPNN framework, we still have an MPNN backbone, augmented with inter-message passing involving virtual nodes. Hence, obtaining priors for the original nodes via an MPNN or even a simple MLP has a complexity of \((|E|+n m)\), where \(m\) is the number of candidate virtual nodes to be selected. Sampling the node assignment with Ahmed et al. (2023) is in \((n m k)\). The message aggregation and distribution between base nodes and virtual nodes have complexity \((n k)\), where \(k[m]\) is the number of nodes a node is assigned to. Finally, the intra-virtual node message passing is in \((m^{2})\), as they are fully connected. In summary, IPR-MPNNs have a running time complexity in \((|E|+n m+m^{2})\). Since \(m n\) is a small constant, IPR-MPNNs show great potential due to their low complexity compared to the quadratic worst-case complexities of graph transformers (Muller et al., 2023) and other rewiring methods, e.g., Gutteridge et al. (2023); Qian et al. (2023).

## 4 Expressive Power

In this section, we analyze the extent to which IPR-MPNNs can separate non-isomorphic graphs on which the \(1\)-WL isomorphism test fails (Xu et al., 2019; Morris et al., 2019), and whether IPR-MPNNs can maintain isomorphisms between pairs of graphs. We adopt the notion of probabilistic separation from Qian et al. (2023).

Our arguments rely on the ability of our upstream MPNN to assign arbitrary and distinct exactly-\(k\) distributions for each color class. By modifying the graph structure, we can make the rewired graphs \(1\)-WL-distinguishable, enabling our downstream MPNN to separate them. However, since the expressiveness of the upstream model is also equivalent to \(1\)-WL, there is a possibility of still separating isomorphic graphs.

The following result demonstrates that we can preserve almost all partial subgraph isomorphisms.

**Theorem 4.1**.: _Let \(k>0\), \((0,1)\), and \(G\), \(H\) be two graphs with identical \(1\)-WL stable colorings. Let \(M\) be the set of ordered virtual nodes, \(V_{G}\) and \(V_{H}\) be the subset of nodes in \(G\) and \(H\) that have a color class of cardinality \(1\), with \(|V_{G}|=|V_{H}|=d\), and \(W_{G}\), \(W_{H}\) the subset of nodes that have a color class of cardinality greater than \(1\), with \(|W_{G}|=|W_{H}|=n\). Then, for all choices of \(1\)-WL-equivalent functions \(f\),_

1. _there exists a conditional probability mass function_ \(p_{(,k)}\) _that does_ not _separate_ \(G[V_{G}]\) _and_ \(H[V_{H}]\) _with probability at least_ \(1-\)_._
2. _There exists a conditional probability mass function_ \(p_{(,k)}\) _that separates_ \(G[W_{G}]\) _and_ \(H[W_{H}]\) _with probability strictly greater than_ \(0\)_._

We argue that preserving these partial subgraph isomorphisms is sufficient for most examples in practice. Indeed, our empirical findings show that we can successfully solve both the Exp and Csl datasets, whereas a \(1\)-WL model obtains random performance; see Table A11, Table A10.

The next corollary follows the above and recovers Theorem 4.1 from Qian et al. (2023). The corollary tells us that, even if there are isomorphic graphs that we risk making separable, we will maintain the isomorphism between almost all isomorphic pairs.

Figure 2: Comparing model sensitivity across different layers for the two most distant nodes from graphs from the Zinc dataset. On the left, we compare the sensitivity for models with a varying number of layers. We can observe that IPR-MPNNs maintain a high sensitivity even for the last layer, while the base models have the sensitivity decaying to \(0\). On the right, we compare models with a different number of virtual nodes, observing that the results are similar for all of the variants.

**Corollary 4.1.1**.: _For sufficiently large \(n\), for every \((0,1)\), a set \(m\) of ordered virtual nodes, and \(k>0\), we have that almost all pairs, in the sense of Babai et al. (1980), of isomorphic \(n\)-order graphs \(G\) and \(H\) and all permutation-invariant, \(1\)-WL-equivalent functions \(f_{n}^{d}\), \(d>0\), there exists a probability mass function \(p_{(,k)}\) that separates the graph \(G\) and \(H\) with probability at most \(\) regarding \(f\)._

The previous theorems show that we are preserving isomorphisms better than purely randomized approaches while being more powerful than \(1\)-WL since we can separate non-isomorphic graphs with a probability strictly greater than \(0\). We provide the proofs and examples in Section E.

## 5 Experimental Setup and Results

To empirically validate the effectiveness of our IPR-MPNN framework, we conducted a series of experiments on both synthetic and real-world molecular datasets, answering the following research questions. An open repository of our code can be accessed at https://github.com/chendiqian/IPR-MPNN.

**Q1**: Do IPR-MPNNs alleviate over-squashing and under-reaching?
**Q2**: Do IPR-MPNNs demonstrate enhanced expressivity compared to MPNNs?
**Q3**: How do IPR-MPNNs compare in predictive performance on molecular datasets against other rewiring methods and graph transformers?
**Q4**: Does the lower theoretical complexity of IPR-MPNNs translate to faster runtimes in practice?

  
**Property** & GIN  & R-GIN-FA  & SPN  & DRFW-GIN  & IPR-MPNN  & IPR-MPNN \\  MU & 2.64\(\)0.01 & 2.54\(\)0.09 & 2.32\(\)0.02 & 1.93\(\)0.06 & 1.99\(\)0.00 & 2.01 \(\)0.01 \\ ALPHA & 7.67\(\)10.2 & 2.28\(\)0.04 & 1.77\(\)0.00 & 1.63\(\)0.03 & 2.28\(\)0.06 & 1.36 \(\)0.00 \\ HOMO & 1.70\(\)0.02 & 1.26\(\)0.02 & 1.26\(\)0.00 & 1.66\(\)0.01 & 1.46\(\)0.01 & 1.07 \(\)0.01 \\ LUMO & 1.05\(\)0.01 & 1.34\(\)0.04 & 1.19\(\)0.05 & 1.13\(\)0.02 & 1.12\(\)0.01 & 1.03 \(\)0.09 \\ GAP & 3.37\(\)0.03 & 1.96\(\)0.04 & 1.89\(\)0.01 & 1.74\(\)0.02 & 1.70\(\)0.01 & 1.61 \(\)0.08 \\ BZV & 23.53\(\)1.08 & 1.21\(\)0.03 & 1.06\(\)0.08 & 9.39\(\)0.13 & 1.04\(\)13.5 & 8.17 \(\)0.03 \\ ZPVE & 66.87\(\)1.45 & 5.03\(\)0.06 & 2.77\(\)0.17 & 2.73\(\)0.19 & 4.73\(\)0.08 & 1.96 \(\)0.07 \\ U0 & 21.48\(\)0.77 & 2.16\(\)1.12 & 1.13\(\)0.01 & 1.08\(\)0.09 & 2.23\(\)0.13 & 0.74 \(\)0.01 \\ U1 & 21.59\(\)0.39 & 2.32\(\)0.18 & 1.03\(\)0.09 & 0.99\(\)0.08 & 2.31\(\)0.06 & 0.79 \(\)0.12 \\ H1 & 21.96\(\)1.24 & 2.26\(\)0.19 & 1.05\(\)0.04 & 1.06\(\)0.09 & 2.66\(\)0.01 & 0.75 \(\)0.14 \\ G & 19.53\(\)0.47 & 2.04\(\)0.24 & 0.97\(\)0.00 & 1.06\(\)0.14 & 2.24\(\)0.01 & 0.62 \(\)0.13 \\ CV & 7.34\(\)0.06 & 1.86\(\)0.03 & 1.36\(\)0.04 & 1.24\(\)0.02 & 1.44\(\)0.01 & 1.03 \(\)0.04 \\ Omega & 0.60\(\)0.03 & 0.80\(\)0.04 & 0.57\(\)0.04 & 0.55\(\)0.01 & 0.48\(\)0.00 & 0.45 \(\)0.03 \\   

Table 1: We compare IPR-MPNN on QM9 with the base downstream GIN model (Xu et al., 2019), two graph rewiring techniques (Gutteridge et al., 2023; Qian et al., 2023), a multi-hop MPNN (Abboud et al., 2022), and the relational GIN (Schlichtkrull et al., 2018). The best-performing method is colored in green, the second-best in blue, and third in orange. IPR-MPNN obtains the best result on all targets, except for MU, where it obtains the second-best result.

  
**Model** &  &  &  &  &  \\  GINE [(2019, 2023)] & 0.6621\(\)0.0067 & 0.2473\(\)0.0017 & 0.3509\(\)0.0016 & 0.3725\(\)0.000 & 0.4617\(\)0.000 \\ GCN [(2017, 2023)] & 0.6560\(\)0.0016 & 0.2460\(\)0.0007 & 0.3424\(\)0.0017 & 0.3631\(\)0.000 & 0.4526\(\)0.0006 \\ DRFW [(2023)] & 0.7150\(\)0.0044 & 0.526\(\)0.0015 & 0.3444\(\)0.0017 & - & - \\ PR-MPNN [(2023)] & 0.6825\(\)0.0065 & 0.2477\(\)0.0005 & - & - & - \\ AMP [(2023)] & 0.7163\(\)0.0058 & 0.2431\(

**Datasets, Experimental Results, and Discussion** To address **Q1**, we investigate whether our method alleviates over-squashing and under-reaching by experimenting on Trees-NeighboursMatch [Alon and Yahav, 2021] and Trees-LeafCount [Qian et al., 2023]. On Trees-LeafCount with a tree depth of four, we obtain perfect performance on the test dataset with a one-layer downstream network, indicating we can alleviate under-reaching. Furthermore, on Trees-NeighboursMatch, our method obtains perfect performance to a depth up to six, effectively alleviating over-squashing, as shown in Figure A4. To quantitatively assess whether over-squashing is mitigated in real-world scenarios, we computed the average layer-wise sensitivity [Xu et al., 2018, Di Giovanni et al., 2023, Errica et al., 2023] between the most distant nodes in graphs from the Zinc dataset and compared these results with those from the baseline GINE model. Specifically, we compute the logarithm of the symmetric sensitivity between the most distant nodes \(u,v\) as \((_{v}^{l}/_{u}^{l}+ _{v}^{l}/_{v}^{l})\), where \(k\) to \(l\) represent the intermediate layers. We show that IPR-MPNNs maintain a high layer-wise sensitivity compared to the base model, as seen in Figure 2, implying that they can successfully account for long-range relationships, even with multiple stacked layers. Lastly, we measured the average total effective resistance [Black et al., 2023] of five molecular datasets before and after rewiring, showing in Figure 3 that IPR-MPNNs are successfully improving connectivity by reducing the average total effective resistance of all evaluated datasets.

For **Q2**, we conduct experiments on the Exp [Abboud et al., 2020] and Csl [Murphy et al., 2019] datasets to evaluate the expressiveness of IPR-MPNNs. The results, as detailed in Table A10 and Table A11, demonstrate that our IPR-MPNN framework handles these datasets effectively and exhibits improved expressiveness over the base \(1\)-WL-equivalent GIN model.

For answering **Q3**, we utilize several real-world molecular datasets--QM9 [Hamilton et al., 2017], Zink 12k [Jin et al., 2017], OGB-Molhiv [Hu et al., 2020], TUDataset [Morris et al., 2020a], and datasets from the long-range graph benchmark [Dwivedi et al., 2022b], namely Peptides and PCQM-Contact. Our results demonstrate that IPR-MPNNs effectively account for long-range relationships, achieving state-of-the-art performance on the Peptides and PCQM-Contact datasets, as detailed in Table 2. Notably, on the PCQM-Contact link prediction tasks, IPR-MPNNs outperform all other candidates across three measurement metrics outlined in Tonshoff et al. . For QM9, we show in Table 1 that IPR-MPNNs greatly outperform similar methods, obtaining the best results on 12 of 13 target properties. On Zinc and OGB-Molhiv, we outperform similar MPNNs and graph transformers, namely GPS Rampasek et al.  and SAT [Chen et al., 2022a], obtaining state-of-the-art results; see Table 4. For the TUDataset collection, we achieve the best results on four of the five molecular datasets; see Table A9.

Finally, to address **Q4**, we evaluate the computation time and memory usage of IPR-MPNNs in comparison with the GPS graph transformer [Rampasek et al., 2022] on Peptides-struct and extend our analysis to include PR-MPNNs [Qian et al., 2023], SAT [Chen et al., 2022a], and GPS on the Zinc dataset. The results in Tables 3 and A12 demonstrate that IPR-MPNNs adhere to their theoretical linear runtime complexity in practice. We observed a notable speedup in training and validation times per epoch while reducing the memory footprint by a large margin compared to the two mentioned transformers. This efficiency underscores the practical advantages of IPR-MPNNs in computational speed and resource utilization.

    & GINE & IPR-MPNN & GPS & Drew \\  \# Par. & \(503k\) & \(536k\) & \(558k\) & \(522k\) \\ Trn s/ep. & 2.68\(\)0.01 & 2.98\(\)0.02 & 7.81\(\)0.32 & 3.20\(\)0.03 \\ Val s/ep. & 0.21\(\)0.00 & 0.27\(\)0.00 & 0.58\(\)0.04 & 0.36\(\)0.00 \\ Mem. & 1.7GB & 1.9GB & 22.2GB & 1.8GB \\   

Table 3: IPR-MPNN training, inference (seconds per epoch), and memory consumption statistics in comparison to the base GINE model [Xu et al., 2019], the GPS graph transformer [Rampasek et al., 2022] and the Drew model [Gutteridge et al., 2023] on the Peptides-struct dataset [Dwivedi et al., 2022b]. Our model has almost the same computation and memory efficiency as the base GINE model while being twice as fast and significantly more memory efficient when compared to GPS.

## 6 Conclusion

Here, we introduced implicit probabilistically rewired message-passing neural networks (IPRP-MPNNs), a graph-rewiring approach leveraging recent progress in end-to-end differentiable sampling. IPR-MPNNs show drastically improved running times and memory usage efficiency over graph transformers and competing rewiring-based architectures due to IPR-MPNNs' ability to circumvent comparing every pair of nodes and significantly outperforming them on real-world datasets while effectively addressing over-squashing and overreaching. Hence, IPR-MPNNs represent a significant step towards designing scalable, adaptable MPNNs, making them more reliable and expressive.

#### Acknowledgments

CQ and CM are partially funded by a DFG (German Research Foundation) Emmy Noether grant (468502433) and RWTH Junior Principal Investigator Fellowship under Germany's Excellence Strategy. AM and MN acknowledge DFG funding under Germany's Excellence Strategy--EXC 2075 - 390740016, the support of the Stuttgart Center for Simulation Science (SimTech), and the International Max Planck Research School for Intelligent Systems (IMPRS-IS).

  Model. & ZINC (12K), & OGB-Moldiv \(\) \\  GINE  & 0.101\(\)0.004 & 0.764\(\)0.019 \\ PR-MPNN  & 0.084\(\)0.002 & 0.795\(\)0.009 \\ GPS  & 0.070\(\)0.004 & 0.783\(\)0.018 \\ K-SG GAT  & 0.095\(\)0.002 & 0.613\(\)0.010 \\ K-ST GAT  & 0.115\(\)0.004 & 0.625\(\)0.009 \\ Graph MLP-Mixer  & 0.073\(\)0.001 & 0.799\(\)0.015 \\ Graph VIT  & 0.085\(\)0.005 & 0.779\(\)0.015 \\   & 0.067\(\)0.001 & 0.788\(\)0.006 \\  

Table 4: Results on the Zinc[Jin et al., 2017] and OGBG-Moldiv[Hu et al., 2020] datasets. Green is the best model, blue is the second, and red the third. The IPR-MPNN outperforms both SAT and GPS on Zinc, while obtaining the same performance as GPS on OGB-Moldiv.

Figure 3: We compute the log of total effective resistance [Black et al., 2023] of five molecular datasets before and after rewiring the graphs using virtual nodes. Our rewiring technique consistently lowers the total effective resistance, indicating a better information flow on all of the datasets.