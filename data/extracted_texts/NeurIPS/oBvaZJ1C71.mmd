# Gavel: Generating Games Via Evolution and Language Models

Graham Todd

New York University Tandon

Brooklyn, New York, USA

gdrtodd@nyu.edu, \({}^{@sectionsign}\)ethz.ch,

Alexander G. Padula

ETH Zurich

Zurich, Switzerland

apadula@ethz.ch

Matthew Stephenson

Fllanders University

Adelaide, Australia

matthew.stephenson@ethz.ch

Eric Piette

UCLouvain

Louvain-la-Neuve, Belgium

eric.piette@nyu.edu, \({}^{@sectionsign}\)ethz.ch,

@tlinders.edu, \({}^{@sectionsign}\)uclouvain.be, \({}^{}\)@maastrichtuniversity.nl,

Alexander G. Padula

ETH Zurich

Zurich, Switzerland

gdrtodd@nyu.edu, \({}^{@sectionsign}\)ethz.ch,

Matthew Stephenson

Fllanders University

Adelaide, Australia

matthew.stephenson@ethz.ch

Eric Piette

UCLouvain

Louvain-la-Neuve, Belgium

eric.piette@nyu.edu, \({}^{@sectionsign}\)ethz.ch,

@tlinders.edu, \({}^{@sectionsign}\)uclouvain.be, \({}^{}\)@maastrichtuniversity.nl,

Alexander G. Padula

ETH Zurich

Zurich, Switzerland

gdrtodd@nyu.edu, \({}^{@sectionsign}\)uclouvain.be, \({}^{}\)@maastrichtuniversity.nl,

Alexander. G. Padula

ETH Zurich

Zurich, Switzerland

gdrtodd@nyu.edu, \({}^{@sectionsign}\)uclouvain.be, \({}^{}\)@maastrichtuniversity.nl,

Matthew Stephenson

Fllanders University

Adelaide, Australia

matthew.stephenson@ethz.ch

Eric Piette

UCLouvain

Louvain-la-Neuve, Belgium

eric.piette@nyu.edu, \({}^{@sectionsign}\)ethz.ch,

matthew.stephenson@ethz.ch,

Dennis J.N.J. Soemers

Maastricht University

Maastricht, the Netherlands

dennis.soemers\({}^{}\)

Julian Togelius

New York University Tandon

Haastrichtuniversity.nl,

https://ludii.games/details.php?keyword=Hoyhrough, and https://ludii.games/details.php?keyword=ZvavGo

details.php?keyword=YavaGo

###### Abstract

Automatically generating novel and interesting games is a complex task. Challenges include representing game rules in a computationally workable form, searching through the large space of potential games under most such representations, and accurately evaluating the originality and quality of previously unseen games. Prior work in automated game generation has largely focused on relatively restricted rule representations and relied on domain-specific heuristics. In this work, we explore the generation of novel games in the comparatively expansive Ludii game description language, which encodes the rules of over 1000 board games in a variety of styles and modes of play. We draw inspiration from recent advances in large language models and evolutionary computation in order to train a model that intelligently mutates and recombines games and mechanics expressed as code. We demonstrate both quantitatively and qualitatively that our approach is capable of generating new and interesting games, including in regions of the potential rules space not covered by existing games in the Ludii dataset. A sample of the generated games are available to play online through the Ludii portal. 1

## 1 Introduction

Games have long been used as a test bed for algorithms and approaches in artificial intelligence, with advances in game-playing ability often serving as some of the most recognizable achievements in the field . While automated systems have repeatedly demonstrated an ability to match or surpass humans as game players, they continue to lag significantly in their capacity to generate the kinds of games that are worth playing. The ability to construct novel and interesting games is an impressive cognitive challenge, and success would have implications both cultural (with theproduction of new artifacts) and computational (with the generation of new learning environments for artificial agents). Prior efforts in _automated game design_[42; 62] have produced some successes (notably the commercially-available game _Yavalath_, generated by the Ludi system ), but remain largely limited by hard-coded heuristics and restricted domains. These limitations are often necessary, however, in the face of automated game design's core challenges: (1) representing the vast array of possible games in a structured and computationally workable form and (2) efficiently searching through the resulting representation space for worthwhile games.

In this work, we present GAVEL (**Games via Evolution** and Language Models)--an automated game design system that tackles these challenges by leveraging recent improvements in game rule representation and code synthesis (see overview in Figure 1). GAVEL draws on three main components: (1) the Ludii game description language [11; 46] to efficiently encode a large variety of board game rule sets, (2) a large code language model to reliably produce plausible modifications to existing games inspired by _evolution through large models_ (ELM) , and (3) _quality-diversity_ optimization  to generate a wide range of playable and interesting games. Each component builds on the others: our choice of representation allows GAVEL to not only produce novel board games in a wide range of genres and styles, but also affords us a dataset of over 1000 existing board games from around the world . This dataset, in turn, provides sufficient basis to _fine-tune_ a code synthesis model. In addition, our quality-diversity approach leverages the inherently modular nature of our representation in order to determine game novelty through the presence of particular game mechanics and motifs.

We show empirically that GAVEL is capable of generating playable and interesting board games that differ substantially from games encountered during training. Our approach intelligently recombines mechanics and ideas from disparate genres and produces samples that mirror the performance of human-generated games under a suite of automated evaluation metrics. A preliminary qualitative analysis also reveals that GAVEL can generate novel games that are both engaging and entertaining. We conclude with a discussion of GAVEL's successes and failures, as well as the promising avenues for future work. We provide a public repository that includes our code and data, including a trained model checkpoint. 2

Figure 1: **GAVEL overview.****Left:** a dataset of games in the Ludii game description language is used to train a code large language model using the _fill-in-the-middle_ objective on parenthetical expressions. **Right:** the trained code language model can then be used as the mutation operator for evolutionary quality-diversity optimization with the MAP-Eittes algorithm. Fitness is determined with a suite of automatic evaluation metrics, and the Ludii game description language also affords a large number of semantic game “concepts” that are used to determine game novelty.

Related Work

### Automatic Game Generation

Our work continues a strand of research that investigates the ability for automated systems to produce novel games or novel variants of existing games. The first such effort was METAGAME , which samples from a grammar that encodes "symmetric _Chess_-like games" with designer-specified rule probabilities. Since then, work has continued in the generation of both board game and video game rulesets. Evolutionary or search-based game design is a popular technique, as game descriptions do not typically afford gradient information; it was first proposed in 2008 for board games  and video games , and later work has brought it to bear on different video game genres [43; 16; 29]. Another approach begins instead from conceptual or symbolic specifications of rules or mechanics and generates games by dynamically referring to a pre-specified library of gameplay elements [42; 63]. Yet another approach is to use constraint satisfaction algorithms. For instance, rules might be encoded as an answer-set program, with constraints pre-specified by a designer to define what counts as an acceptable game [56; 71; 44; 26; 59]. Most recently, work has investigated the ability for large language models to act as design assistants by generating game levels [61; 58], proposing game mechanics  or directly synthesizing small programs .

### Evolutionary Computation and Language Models

Evolutionary computation refers to a large class of algorithms broadly inspired by the biological process of evolution . Of these, our approach descends most directly from genetic programming: the use of evolution or other stochastic search procedures for program synthesis [18; 23; 32]. We also draw on more recent advances in quality-diversity algorithms that aim to find a distribution of solutions to a given problem rather than a single optima [41; 49; 20], especially in the context of code and content generation .

In recent years, improvements in large language models both generally [7; 70] and in their ability to produce code [14; 67] have led to their use in a variety of evolutionary systems. Of note is _evolution through large models_, which learns a _diff model_ (i.e. a language model that can modify programs conditioned on a natural language specification) from a dataset of GitHub commits and accompanying messages. This model is then used as the mutation operator for genetic programming in Python through a quality-diversity algorithm . We adopt this general approach, though GAVEL learns to mutate games from raw programs (i.e. without diffs or commit messages) in a domain-specific language. Similar techniques have also been used to generate reinforcement learning environments [1; 69] and reward functions , adversarial prompts , programming puzzles , and poetry .

## 3 Game Representation and Dataset

We represent games as programs in the Ludii game description language (L-GDL) , in which game rules are built from _ludemes_--high-level keywords that represent common components in the natural language descriptions of board game rules. Examples of such keywords include step, slide, hop, piece, empty, board, and so on. Owing to this abstraction, the L-GDL is both robust enough to encode a vast array of disparate games  and compact enough that game descriptions often fit within the context lengths of modern large language models.

In addition to _ludemes_, the Ludii system also defines a large number of _concepts_--high level properties of games that describe its gameplay or structure . Most concepts are boolean and indicate the presence or absence of a particular feature. For example, one concept might describe whether a game is asymmetric, while another might describe whether a game uses the "custodial" capture mechanic seen in _Tafl_-style games. These concepts provide a way to represent games as meaningful feature vectors, which can then be used to cluster and compute similarities between games .

### Dataset

We construct our initial game dataset out of the 1182 existing games that have been translated into the Ludii game description language (available under a Creative Commons BY-NC-ND 4.0 license).

We start by expanding the function references (e.g. "BlockWin") inside each game description to the code they represent (e.g. (end (if (no Move Next) (result Mover Win))), as specific functions may appear in only few games while the underlying _ludemes_ are much more widespread. In addition, we remove references to the particulars of each game (i.e. its name and the names of the pieces it uses) and replace them with abstract identifiers. Both these processes increase the generality of our game description dataset.

After this, we filter our dataset to remove a small number of puzzles and experimental games. We also remove _Mancala_-style games, as prior work has identified them as occupying a very distinct cluster with respect to the full collection of Ludii games, in terms of rules and structure . We then tokenize each game according to our code language model (see below) and exclude any game that is longer than 1024 tokens. From this reduced dataset, we hold out a set of 14 varied games (available in Appendix A) that are used to initialize the evolutionary search, with the remaining 574 games being used as our training dataset. An example of one of the held-out games and its converted form is available in Figure 2.

## 4 Methods

### Language Model Training

In line with the general ELM approach, we make use of the impressive generative capabilities of modern code language models in order to propose sensible modifications to existing programs . Unlike prior work, however, we specifically fine-tune an existing model to operate in L-GDL instead of working with programming languages seen during pre-training or making use of in-context learning. In addition, we train our model to act as a mutation operator over programs by using a _fill-in-the-middle_ (FITM)  training objective instead of the more common left-to-right objective. FITM training allows the model to make changes to interior components of a program without (a) relying on an extant dataset of code differs or (b) regenerating the entire game at each step.

We train an instance of CodeLlama  (specifically CodeLlama-13b, as it is the largest model in its family that was pre-trained with a FITM objective) on the dataset described in Section 3.1. To facilitate FITM training, we extract every balanced parenthetical expression from each game (e.g. (board (square 10))) and add it to the dataset along with the corresponding prefix and suffix in the program. With respect to the grammar of L-GDL, this process is equivalent to extracting syntactic nodes and all of their descendants. The final dataset consists of 49,968 such (prefix, suffix, target) tuples. To facilitate training on a single GPU, we make use of both parameter-efficient fine-tuning  and 8-bit quantization . Owing to the large size of the dataset and the fact that each game appears repeatedly in different configurations, we fine-tune the model for a single epoch with hyperparameters available in Appendix B. Training took approximately 40 hours to complete on a single RTX8000 GPU.

Figure 2: **Left: the game of _Havannah_ by Christian Freeling rendered in the Ludii game description language. Center: the same game as it appears in the training dataset, with functional references expanded and game / piece names replaced with abstract identifiers. Right: a variant of _Havannah_ produced by GAVEL. Changes are highlighted in yellow.**

We note here that FITM training may have a potential downside in the context of evolution through large models. Specifically, the model is trained to perfectly reproduce the missing section of code given its prefix and suffix. If it succeeds completely in doing so during evolution, then the resulting game will not be mutated at all. In essence, there is a tension between the ability of the model to accurately capture the underlying logic and syntax of the representation space and its ability to memorize or perfectly reconstruct its training dataset. In GAVEL, we mostly sidestep this issue by mutating a set of held-out games not seen at all during training (making memorization impossible), though see Appendix C for an initial investigation on mutating training-set games and Section 8 for a discussion of other possible approaches.

### Evolutionary Search

Our evolutionary search strategy of choice is MAP-Elites , a population-based quality-diversity algorithm that leverages both a fitness function over samples as well as a set of _behavioral characteristics_--functions that describe non-fitness attributes of samples and are used to ensure that the population does not collapse to only a small number of distinct samples. Specifically, MAP-Elites maintains an _archive_ of cells, each associated with a particular range of values under the behavioral characteristics. At each step, a novel sample is evaluated to determine its fitness as well as the cell it would occupy. It is added to the archive if either that cell is unoccupied or if its fitness exceeds that of the current occupant, in which case it replaces the current occupant. In this way, samples only "compete" with one another within particular cells. We describe our fitness function in detail in Section 4.3.

Determining an appropriate set of behavioral characteristics is challenging in the context of automatic game generation. Intuitively, distinct archive cells ought to capture meaningfully distinct games while collapsing minor or trivial variations. Human game players readily make such categorizations, but automatically identifying such differences (especially with previously unseen games) is both difficult and necessary for the MAP-Elites algorithm to function. In order to tackle this problem, we take advantage of the semantic concepts described in Section 3. Building on evidence that Ludii concept vectors capture a meaningful notion of distance , we use principal component analysis (PCA)  on the complete Ludii dataset (i.e. before any filtering) to reduce the 510-dimensional concept vectors to two dimensions. We then bucket the resulting two-dimensional space into 40 equally-spaced regions from -5 to 5 in each dimension, obtaining a rectangular archive of 1600 cells. While the first two PCA dimensions describe only \( 28\%\) of the variance in the concept feature space, a preliminary investigation indicated that increasing the number of dimensions resulted in a _less_ diverse archive (see Appendix D for additional details). Because games are not uniformly distributed through feature space, increasing the archive's dimensionality for a fixed number of cells caused a larger number of distinct games to be mapped to the same cell. See discussion of potential alternatives in Section 7.

We initialize the archive by adding and evaluating the 14 heldout games listed in Appendix A. For each MAP-Elites step, we select \(j\) games from the current archive. For each game, we then select \(k\) random parenthetical expressions and re-format them as a (prefix, suffix, target) tuple. We then sample from the trained CodeLlama-13b model with a temperature of 1 and a top-\(k\) value of 50 to generate a new expression, conditioned on just the prefix and suffix, and re-construct the resulting game. After filtering out any duplicate or unchanged games, the samples are evaluated for fitness and assigned an archive cell based on the PCA reduction of their concept vector.

### Evaluation

Game quality is both difficult to quantify and inherently subjective. Nevertheless, automatic game design and evolutionary computation necessitate some kind of computable optimization objective. These objectives typically take the form of one or more heuristics that aim to proxy the underlying targets of "fun" or "interestingness." For restricted domains, it is often possible to imbue a large amount of expert knowledge into these heuristics, as in the Ludi system  (precursor to Ludii) which employed game-state evaluator functions to capture both objective measures (e.g. a game's balance) and psychological measures (e.g. a game's excitement or unpredictability). However, the large space of games described by the Ludii description language makes relying on such evaluator functions infeasible. Instead, we define a hierarchical fitness function based on a relatively small set of objectively and reliably measurable heuristics that are fully game-agnostic.

Concretely, every game \(g\) generated during the evolutionary search is assigned a fitness value \(f(g)\{-3,-2,-1\}[0.01,1]\) by a sequence of tests (pseudo-code presented in Algorithm 1). Evaluation begins with a series of binary evaluations. First, all games that fail to compile (e.g. due to grammatical errors, or because they do not define a board) are assigned the minimum possible fitness of -3 (we note that such games cannot be added to the archive as their concepts are not well defined). Next, all compilable games that cannot be played (e.g. due to failing to define any moves for pieces or failing to place pieces on the board for a game like _Chess_) are assigned a fitness of -2. If these conditions are cleared, then random-policy agents are used to rapidly obtain \(n=100\) playouts of the game. If the difference in win rate between the first and second player is larger than a threshold of 0.5 (indicating a large imbalance even for completely naive players) or if fewer than half of game states allow more than one legal move (indicating a lack of player agency), then the game is assigned a fitness of \(-1\). These checks are used as a filter to ensure that expensive evaluations are only performed on promising games.

If all three previous conditions are met, then search-based agents are used to obtain \(n=10\) game playouts. Specifically, we use self-play between Monte-Carlo Tree Search (MCTS)  agents with \(t=0.25\) seconds of thinking time per move and a hard limit of \(m=50\) moves per player--any game which exceeds the limit is terminated and called a draw. This limit puts a strong pressure on games to end within a reasonable number of moves and necessarily excludes many potentially interesting games (especially as search-limited agents might fail to find winning lines that could end a game quickly). However, this concession is necessary to ensure that the overall evaluation procedure remains computationally tractable.

From these 10 playouts, we extract the following evaluation metrics, largely inspired by prior work in automated game design :

1. **Balance**: the largest difference in winrates between any pair of players.
2. **Decisiveness**: The proportion of games that do not end in a draw.
3. **Completion**: the proportion of games that reach an end state.
4. **Agency**: the proportion of turns for which the player to move has more than one legal move.
5. **Coverage**: the proportion of board sites (e.g. squares on a chessboard) that get occupied by a game piece at least once in a playout.

In addition, we separately compute a final metric: **Strategic Depth**, defined as the proportion of games won by an MCTS agent against a random agent over \(n=10\) playouts (and inspired by previous similar evaluations ). Each evaluation metric returns a value between 0 and 1, and the metrics are then aggregated into a single fitness score by taking the _harmonic mean_. We use the harmonic mean over the simple average because the former is weighted towards small values, penalizing games that succeed in most metrics but fail dramatically in one. We enforce a minimum metric value of 0.01 before taking the harmonic mean to ensure that the fitness score is not completely zeroed-out by a single metric.

## 5 Experiments

We perform 3 runs of MAP-Elites with random seeds \(\{1,2,3\}\), each lasting for 500 steps. For each run, we select \(j=3\) games and generate \(k=3\) mutations for each game at each step. Quantitatively, we report the progress of the archive using three metrics: the _quality-diversity score_ (QD score) , calculated as the sum of the fitness of each cell in the archive. In cases like ours where fitness values can be negative, each fitness value is incremented by the minimum possible fitness (i.e. -2) before being summed to ensure that the QD score increases monotonically over time. In addition, we report the number of playable and minimally interesting games (i.e. \(f(g)>0\)) in the archive as well as the number of such games that occupy cells which are not covered by any game in the Ludii dataset. Finally, we report the number of cells and novel cells that contain games with a fitness of at least 0.5, indicating their potential as worthwhile games. Each run lasted roughly 48 hours using a single RTX8000 GPU for inference from the CodeLlama-13b model and performing evaluations in parallel with 16 CPU cores and 128GB of total memory.

In addition, we perform another set of 3 runs with a variant of GAVEL that uses the Upper Confidence Bound algorithm  to select which regions of games to mutate. Specifically, we treat the selection of a parenthetical expression to mutate as a multi-armed bandit problem where each arm corresponds to a different leading _ludeme_ (e.g. board or equipment). We consider a mutation "successful" if it results in the mutated game being added to the archive (either by improving the fitness of an existing occupant, or by occupying a new cell) and update the statistics for each "arm." We call this variant GAVEL-UCB. All other hyperparameters remain the same as with the original GAVEL experiment.

We compare GAVEL against two baselines: a pure sampling approach, which uses the same fine-tuned large language model but omits the quality diversity search, and direct sampling from GPT-4o. For the pure sampling baseline, we randomly select a game from the validation set and mutate it by re-generating a random parenthetical expression. We repeat this process 4500 times in order to match the number of samples produced by GAVEL in 500 generations with \(j=3\) and \(k=3\). We then

    & &  &  \\
**Method** & **QD Score** & _\# Playable_ & _\# Fitness0.5_ & _\# Playable_ & _\# Fitness0.5_ \\  GAVEL & \(395.62 17.46\) & \(117.67 9.46\) & \(106.67 7.41\) & \(26.67 3.30\) & \(21.67 6.13\) \\ GAVEL-UCB & \(341.17 14.39\) & \(96.67 6.02\) & \(88.33 7.41\) & \(19.67 4.03\) & \(16.00 2.45\) \\ Pure Sampling & \(296.92 14.84\) & \(89.00 5.66\) & \(83.00 5.10\) & \(14.67 3.30\) & \(11.33 2.87\) \\ GPT-4o & \(268.16 17.33\) & \(84.67 6.60\) & \(80.67 5.19\) & \(16.67 3.30\) & \(15.33 2.87\) \\   

Table 1: Quantitative measures of archive progress for GAVEL, a variant in which mutation locations are selected with the UCB algorithm, and two baseline methods, averaged over three independent runs. We report the quality diversity (QD) score (a cumulative measure of fitness) as well as the number of archive cells and novel archive cells that reach certain fitness thresholds. Both GAVEL-based methods succeed in producing high-fitness games in unexplored regions of concept space, though GAVEL has the edge over GAVEL-UCB in overall QD score. Compared to baseline methods, GAVEL achieves a significantly higher QD score and fills more of the archive will playable and high-fitness games.

Figure 3: A visualization of the fitness of games generated by GAVEL over time. Starting from an initial archive of 14 games, GAVEL produced in this run 185 novel variations within 500 generations, of which 130 are playable and meet our minimum evaluation criteria. Further, 62 generated games occupy cells not covered by _any_ game in the Ludii dataset and 29 of these games meet our minimal criteria.

evaluate the set of 4500 samples for fitness and determine the cell that each game would occupy based on its concepts, allowing us to construct a simulated archive (i.e. by retaining the highest-fitness sample in each cell) and compare directly against GAVEL. We perform the pure-sampling baseline experiment three times with random seeds \(\{1,2,3\}\). For the GPT-40 baseline, we provide the model with each of the 14 validation games as part of the context window and then ask it to create a modification of one randomly-selected validation game (see Appendix E for prompt). We similarly generate 4500 samples and construct a simulated archive in order to facilitate comparisons with GAVEL.

## 6 Results

### Quantitative Results

Overall, GAVEL succeeds in generating a wide range of novel and high-fitness games. In Table 1 we present the mean and standard deviations of the archive metrics for GAVEL, GAVEL-UCB, and our baseline methods. The quality diversity score is difficult to interpret in isolation, as its magnitude depends greatly on the potential range of fitness scores. In this case, it is most helpful as a way to compare the performance of disparate algorithms on the same task: we see that GAVEL improves significantly over GAVEL-UCB (Welch's \(t\)-test, \(p\) = \(0.029\)), indicating that it has some mixture of higher fitness and greater variety in the samples it produces. Similarly, GAVEL improves significantly in terms of QD score over both the pure sampling baseline (\(p\) = \(0.004\)) and the GPT-40 baseline (\(p\) = \(0.002\)).

Of more interest is the fact that GAVEL fills a substantial proportion of the archive with playable games despite starting from a modest 14 samples, including in regions of the archive not covered by games in the Ludii training dataset. Compared to both baselines, GAVEL occupies significantly more cells in the archive and produces more high-fitness (i.e. \(f(g)>0.5\)) samples (\(p<0.03\) for pure sampling, \(p\) < \(0.02\) for GPT-40), though the differences between GAVEL and GAVEL-UCB are not significant (\(p\) > \(0.05\)). While GAVEL also appears to occupy a larger number of _novel_ cells with playable and high-fitness games compared to the baselines, these improvements are not statistically significant at only three runs. We present a visualization of the archive produced by one run of GAVEL over time in Figure 3, which shows both the success of the model in generating high-fitness samples and the fact that much of the concept space remains unexplored.

### Qualitative Results

In order to more closely examine GAVEL's output, we rely on expert evaluators to quickly playtest potentially promising games. These evaluators are broadly familiar with the Ludii dataset and so are able to determine whether a generated game is truly novel and where its mechanics might have originated from. This preliminary human analysis helped shed light on some of GAVEL's shortcomings (discussed below) and also revealed some particularly interesting games among the high-fitness samples. Of special note is a variant of _Yavalath_, itself the product of an automated system . In the original game, players take turns placing pieces on a hexagonal board. A player wins if they have four pieces in a row but loses if they have three pieces in a row first. GAVEL makes both minor changes to the ending rules (increasing the number of pieces in a row needed for victory and loss by one) as well a substantial addition by introducing the enclosure capture rules of _Go_. The result is a game that tasks players with thinking about the arrangement of their pieces in many ways and that appears to offer the potential for sophisticated strategy. In Figure 4 we present an example of play between automated agents in which all of the game's rules are used in concert.

Figure 2 (right) includes another example of a generated game noted by our evaluators to be particularly interesting. It is a variant of _Havannah_ (a game in which players attempt to form loops or connected lines of pieces between sides of the board) that introduces a restriction on piece placement from another game in the Ludii dataset (_Tabu Y_). A final exemplar, presented in Appendix F alongside the previous two examples, modifies the pawn-advancement game _Breakthrough_ to use pieces that can only move by hopping over each other (and without capturing). Taken together, these examples demonstrate the strength of GAVEL: it is able to intelligently recombine game mechanics (expressed as code segments) in novel ways and ensure that these combinations do not result in trivial games.

## 7 Discussion and Limitations

**Unused Game Components:** a common failure mode is that the model will generate game rules that are not actually used during gameplay. For instance, a change to the equipment section might add dice as additional game pieces. However, without further changes to the gameplay section to incorprate them, the dice will remain unused. Detecting such extraneous rules automatically is challenging, as the Ludii system does not provide a way to determine which rules are activated during a given playout. In addition, penalizing games with unused components during fitness evaluation might _harm_ diversity by eliminating potential "stepping stones" to more interesting games. Nevertheless, if changes to the underlying representation _did_ allow unused sections to be detected, it might be possible to bias future mutations towards relevant gameplay sections in order to increase the likelihood of the missing rules being generated.

**Heldout Games:** as noted in Section 4.2, we initialize the search using a set of games held out from the language model training in order to prevent memorization. While GAVEL produces a wide range of games from this set, they nonetheless will share many features with one another as a result of their common origins. One potential solution to further increase archive diversity is to improve the variety of mutations, either by increasing sampling temperature or by enforcing novelty with respect to the tokens in the original game section through masking. These techniques might also make it possible to initialize the archive with games in the training dataset, further increasing potential output diversity.

**Archive Selection:** determining an appropriate way to distinguish between games is a general challenge. GAVEL makes use of Ludii concepts, but not all representation schemes afford such detailed semantic information. One promising alternative for such domains is the automatic selection of behavioral characteristics, either through distillation of trajectories obtained during evaluation  or by leveraging the ability for large language models to identify archetypes in code .

**Evaluation:** our evaluation metrics capture general and _minimal_ criteria of interesting games, broadly construed. However, satisfying our evaluation metrics alone is far from _sufficient_ evidence for a game being interesting. Ultimately, all metrics in automated game design aim to proxy notions of human preference--as mentioned in Section 6.2, we rely on expert evaluators to filter from high-fitness samples to interesting games. Another worthwhile approach, then, might be to learn these preferences

Figure 4: Example of play between MCTS agents in a game generated by GAVEL. The game is descended from _Yavalath_ (an \(n\)-in-a-row style game) and combines a modification of its ending rules with the enclosure capture mechanics of \(Go\). Search-based agents reach interesting and strategically deep game positions, hinting at its potential interest to human players as well.

directly from human ratings  or attempt to extract them from latent knowledge in large language models .

## 8 Future Work

First and foremost, we are excited about a more in-depth human analysis of the games generated by GAVEL. While our expert evaluators can provide useful insight and analysis, their perspective is ultimately one of many. A larger user study could help identify not only which games are most appealing to human players, but also the particular features of those games that correlate with fun and engagement. This, in turn, could help spur improvements to GAVEL across the board, from archive selection to evaluation metrics.

Within the Ludii domain, one particularly exciting possibility for future work is the integration of the explicit L-GDL grammar. While the CodeLlama-13b model learns to produce syntactically valid mutations, integrating the grammar either through token masking  or Monte-Carlo steering  could allow for greater mutation diversity (by increasing sampling temperature, for instance) without sacrificing syntactic correctness and compilability. In addition, it might be possible to use data-augmentation techniques (e.g. sampling from the underlying grammar) to create a dataset of Ludii game diffs with which to train a more typical ELM model.

More generally, large language models could also be used to link natural language descriptions of game rules with their programmatic representations. For example, an instruction-tuned model could be used to convert from abstract rules to executable code, either by fine-tuning (e.g. on the Ludii dataset) or through in-context learning. This might allow automatic game design systems to interact with and generate games at the level of natural language, better resembling the process used by human designers, while still retaining the ability to automatically evaluate the relevant gameplay properties of those games.

Finally, our results indicate that systems like GAVEL might be most useful in the context of co-creativity [21; 68]. Automated processes are able to rapidly generate plausible combinations of game mechanics, while human designers and play-testers are able to much better determine the subtle changes necessary to elevate a potentially interesting idea to an entertaining and engaging game. A system which explicitly integrates such expertise may thus prove to be the best way forward.

## 9 Broader Impact

Like all automated game design systems, GAVEL has the potential for impacts on the larger space of game design. Especially at time of writing, as the video game industry experiences widespread layoffs and contractions, it is important that automatic systems are used to assist and inspire human designers instead of replacing them. Indeed, our results indicate that such collaboration is crucial to the generation and identification of worthwhile games. We also draw attention to GAVEL's use of large language models: while the Ludii dataset is publicly available, a similar system could conceivably be used to generate games from scraped datasets without such free access or from game code encountered during pre-training. Special care should also be taken to ensure that language model outputs are manually verified before being published. Overall, however, we feel that the specific domain, use case, and outputs of GAVEL mean that its broader impact is unlikely to be negative.