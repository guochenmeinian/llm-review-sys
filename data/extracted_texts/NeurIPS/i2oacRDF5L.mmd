# Belief-State Query Policies for User-Aligned POMDPs

Daniel Bramblett and Siddharth Srivastava

Autonomous Agents and Intelligent Robots Lab

School of Computing and Augmented Intelligence

Arizona State University, AZ, USA

{drbrambl,siddharths}@asu.edu

###### Abstract

Planning in real-world settings often entails addressing partial observability while aligning with users' requirements. We present a novel framework for expressing users' constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) policies in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such constraints and prove that while the expected cost function of a parameterized BSQ policy w.r.t its parameters is not convex, it is _piecewise constant_ and yields an implicit _discrete parameter search space_ that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior with guaranteed user alignment. Analysis proves that our algorithms converge to the optimal user-aligned behavior in the limit. Empirical results show that parameterized BSQ policies provide a computationally feasible approach for user-aligned planning in partially observable settings.

## 1 Introduction

Users of sequential decision-making (SDM) agents in partially observable settings often have requirements and preferences on expected behavior, ranging from safety concerns to high-level knowledge of task completion requirements. However, users are ill-equipped to specify desired behaviors from such agents. For instance, although reward engineering can often encode fully observable preferences , it requires significant trial-and-error, and can produce unintended behavior even when done by experts working on simple domains . These challenges are compounded in partially observable environments, where the agent will not know the full state on which the users' requirements and preferences are typically defined. For example, defining a reward function on the belief state to align the agent's behavior with the user can result in wireheading  (see Sec. 2 for further discussion on related work).

Consider a simplified, minimal example designed to illustrate the key principles (Fig. 1(a)). A robot located on a spaceship experiences a communication error with the ship and needs to decide whether to attempt to repair itself or the ship. Importantly, while a robot error is harder to detect, the user would rather risk repairing the robot than repairing the ship, as each repair risks introducing additional failures. In other words, the user may expect the robot to work with the following goals and preferences: _The objective is to fix the communication channel. First, if there is a "high" likelihood that the robot is broken, it should try to repair itself; otherwise, if there is a "high" likelihood that the ship is broken, it should try to repair that._ Such preferences go beyond preferences in fully observable settings: they use queries on the current belief state for expressing users' requirements while using the conventional paradigm of stating objectives in terms of the true underlying state. Such a formulation avoids wireheading, allowing users to express their constraints and preferences in partially observable settings. Although such constraints on behavior are intuitive and common, they leave a significantamount of uncertainty to be resolved by the agent: it needs to optimize the threshold values of "high" probability under which each rule would apply while attempting to achieve the objective.

We introduce mathematical and algorithmic foundations for addressing these problems by defining constraints on behaviors in terms of properties of the belief state, expressed through belief-state queries (BSQs). We prove the surprising result that although the space of possible threshold values in preferences such as the one listed above is uncountably infinite, only a finite number of evaluations are required for computing optimal, user-aligned policies for finite-horizon problems. We use this result to develop a probabilistically complete algorithm for computing optimal constrained policies. Our main contributions are:

1. A framework for encoding user requirements and preferences over agent behavior in goal-oriented partially observable Markov decision processes (Sec. 3).
2. Mathematical analysis proving that the expected cost function of a parameterized BSQ policy w.r.t its parameters is piecewise constant but generally non-convex. (Sec. 4).
3. A probabilistically complete algorithm for computing optimal user-aligned policies in goal-oriented POMDPs (Sec. 5).
4. Empirical evaluation on a diverse set of problems showing both the efficiency of our algorithm and the quality of the computed user-aligned policies. (Sec. 7).

## 2 Related Work

Planning over preferences has been well studied in fully observable settings (Baier et al., 2007; Aguas et al., 2016). Voloshin et al. (2022) present an approach for complying with an LTL specification while carrying out reinforcement learning. Other approaches for using LTL specifications use the grounded state to create a reward function to teach reinforcement learning agents (Toro Icarte et al., 2018; Vaezipoor et al., 2021). These approaches do not extend to partially observable settings as they consider agents that can access the complete state.

In partially observable settings, existing approaches for using domain knowledge and preferences require extensive, error-prone reward design and/or do not guarantee compliance. LTL specifications have been incorporated either by designing a reward function that incentivizes actions more likely to adhere to these specifications (Liu et al., 2021; Tuli et al., 2022) or by imposing a compliance threshold (Ahmadi et al., 2020). In both approaches, the user calibrates rewards for user alignment with those for objective completion; it is difficult to ensure user alignment. We focus on the problem of guaranteeing user alignment without reward engineering.

Mazzi et al. (2021, 2023) proposed expressing domain control knowledge using belief state probabilities. Mazzi et al. (2021) used expert-provided rule templates and execution traces to construct a shield to prevent irregular actions. Mazzi et al. (2023) used execution traces and domain-specified belief-state queries to learn action preconditions over the belief state. Both approaches use input traces and focus on ensuring a policy is consistent with previously observed behavior. We address the complementary problem of computing user-aligned policies without past traces.

Figure 1: (a) Spaceship Repair running example. (b) parameterized BSQ policy for the user preference from the Introduction. (c) The expected cumulative cost function for (b) with a horizon of 12.

Belief-state queries have been used to solve POMDPs with uniform parameter sampling [Srivastava et al., 2012] but formal analysis, feasibility of optimizing BSQ policies, and the existence of provably convergent algorithms have remained open as research questions prior to this work.

## 3 Formal Framework

This section formally defines the BSQ framework, which expresses user requirements on an agent's belief and is designed for relational goal-oriented partially observable Markov decision processes.

### Goal-Oriented Partially Observable Markov Decision Process

Partially observable Markov decision processes (POMDPs) constitute a standard mathematical framework for modeling SDM problems in partially observable, stochastic settings [Kaelbling et al., 1998, Smallwood and Sondik, 1973]. State-of-the-art POMDP solvers often rely on approximate online approaches [Silver and Veness, 2010, Somani et al., 2013] where recent work addresses the problem of obtaining performance bounds [Barenboim and Indelman, 2023, Lim et al., 2023].

We use goal-oriented POMDPs (gPOMDPs), where the agent aims to complete one of the tasks/goals. This eliminates the burden of error-prone reward engineering by using a default cost function that associates a constant cost for each timestep before reaching the goal. E.g., the Spaceship Repair problem (Sec. 1) has two objects: the robot and the spaceship. A state is defined using a Boolean function \(broken(o)\) representing whether object \(o\) needs repair and an integer-valued function \(location()\) representing the robot's location. Both functions are not observable. The agent has two types of actions: try to repair object \(o\) (\(repair(o)\)) or wait (\(wait()\)). A transition function expresses the distribution of \(rlocation()\) depending on the action taken and the robot's previous location. At each timestep, the robot receives a noisy observation \(obs\_err(o)\) regarding the status of object \(o\). The set of observations can be expressed as \(\{obs\_err(robot),obs\_err(ship)\}\). Due to noisy perception, \(obs\_error(o)\) may not match \(broken(o)\). An observation function denotes the probability of each observation conditioned on the (hidden) current state. The goal is to reach the repair station corresponding to the truly broken component. We define gPOMDPs formally as follows.

**Definition 1**.: _A goal-oriented partially observable Markov decision process\(\) is defined as \(,,,,,, ,,H,b_{0}\) where \(\) is the finite set of constant symbols and \(\) is the finite set of functions. The set of state variables for \(\), \(_{F}\), is defined as all instantiations of functions in \(\) with objects in \(O\). The set of states \(\) is the set of all possible valuations for \(_{F}\); \(\) is a finite set of actions, \(\) is a subset of \(\) of observation predicates, \(:\) is the transition function \(T(s,a,s^{})=Pr(s^{}|a,s)\); \(\) is the set of goal states that are also sink states, \(:\) is the observation function; \((s,a,o)=Pr(o|s,a)\), \((s)=\{0\) if \(s\);else \(1\}\) is the cost function, \(H\) is the horizon, and \(b_{0}\) is the initial belief state. A solution for a gPOMDP is a policy that has a non-zero probability of reaching \(\) in \(H-1\) timesteps._

### Belief-State Queries and Policies

Computing a policy for any gPOMDP requires planning around state uncertainty. This is done using the concept of a _belief state_, which is a probability distribution over the currently possible states. Formally, the belief state constitutes a sufficient statistic for observation-action histories [Astrom et al., 1965]. We express user requirements using queries on the current belief state.

For any belief state \(b\), when action \(a\) is taken and observation \(o\) is observed, the updated belief state is computed using \(b^{}(s^{})=(s^{},a,o)_{s}(s,a,s^{ })b(s)\) where \(\) is the normalization factor. We refer to this belief propagation as \(b^{}=bp(b,a,o)\). We extend the notation to refer to the sequential application of this equation to arbitrary bounded histories as \(bp^{*}(b_{0},a_{1},o_{1},...,a_{n},o_{n})=bp( bp(bp(b_{0},a_{1},o_{1}),a _{2},o_{2}))\).

For example, the Spaceship Repair problem user preference has the expression "_a high likelihood that the robot is broken"_. This can be expressed as a query on a belief state \(b\): \(Pr[broken(robot)]_{b}>_{rob}\) where \(_{rob}\) is a parameter. If \(rlocation()\) is fully observable, the expression "_the robot location is smaller than \(_{l}\) in a belief state \(b\)"_ can be expressed as \(Pr[\!rlocation()<_{l}]_{b}==1\). We can combine both queries to express "_a high likelihood the robot is broken and its location is lower than \(_{l}\)"_, as: \(Pr[\!broken(robot)]_{b}>_{rob} Pr[\!rlocation()<_{l}]_{b}==1\).

Formally, BSQs use the vocabulary of the underlying gPOMDP. There are two types of queries we can ask: (1) whether formula \(\) is true with a probability that satisfies a threshold \(\); (2) whether the fully observable portion of the state satisfies a formula \(\) containing a threshold \(\). These thresholds represent the parameters of a parameterized BSQ policy. The agent must optimize these parameters to achieve the goal while aligning with the user's requirements. BSQs can be combined using conjunctions or disjunctions to express more complex requirements, which we define as a compound BSQ in Def. 3. We omit subscripts when clear from context.

**Definition 2**.: _A belief-state query \(_{}(b;,,)\), where \(b\) is a belief state, \(\) is a first-order logic formula composed of functions in gPOMDP \(\), \(\) is any comparison operator, and \(\) is a parameter, is defined as \(_{}(b;,,)=Pr[\![]\!]_{b}\)._

**Definition 3**.: _A compound BSQ \((b;)\), where \(b\) is a belief state and \(^{n}\), is either a conjunction or a disjunction of BSQs that contain \(n\) total parameters._

We use BSQs to formally express user requirements of the form discussed in the introduction by mapping BSQs with variable parameters to actions. Fig. 1(b) illustrates this with a parameterized BSQ policy for the Spaceship Repair problem. Formally,

**Definition 4**.: _Let \(b\) be a belief state and \(\) be a tuple of \(n\) parameter variables over \(\). An \(n\)-parameter Parameterized Belief-State Query policy \((b,)\) is a tuple of rules \(\{r_{1},...,r_{m}\}\) where each \(r_{i}=_{i} a_{i}\) is composed of a compound BSQ \(_{i}\) and an action \(a_{i}\). The set \(\{_{1},...,_{m}\}\) is mutually exclusive and covers the \(n\)-dimensional parameter space \(^{n}\)._

In practice, mutually exclusive coverage is easily achieved using an _if... then... else_ structure, where each condition includes a conjunction of the negation of preceding conditions and the list of rules includes a terminal _else_ with the catchall BSQ _True_ (Fig. 1). Any assignment of values \(^{n}\) to the parameters \(\) of a parameterized BSQ policy produces an executable policy that maps every possible belief state to an action:

**Definition 5**.: _A BSQ policy \((b,)\) is a parameterized BSQ policy \((b,)\) with an assignment in \(\) to each of the \(n\) parameters \(\)._

Let \(Pr_{}^{}()\) be the probability that an execution of a policy \(\) reaches a state in \(\) within \(t\) timesteps. A BSQ policy \((b,)\) is said to be _a solution to a gPOMDP_ with goal \(\) and horizon \(H\) iff \(Pr_{H-1}^{(b,)}()>0\). The quality of a BSQ policy is defined as its expected cost; due to the uniform cost function in the definition of gPOMDPs, the expected cost of a BSQ policy is the expected time taken to reach a goal state. Formally, the _expected cost of a BSQ_ policy \((b,)\) is \(E_{}(;H)=_{t=1}^{H}t Pr_{,t}[(b,)]\), where \(H\) is the horizon and \(Pr_{,t}[(b,)]\) is the probability of policy \((b,)\) reaching a goal state for the first time at timestep \(t\). Thus, given a gPOMDP \(\), with goal \(\), and a parameterized BSQ policy \((b,)\), the objective is to compute:

\[^{*}=_{}\{E_{}( ;H):Pr_{H-1}^{(b,)}()>0\}\]

## 4 Formal Analysis

Our main theoretical result is that the continuous space of policy parameters is, in fact, partitioned into finitely many constant-valued convex sets. This insight allows the development of scalable algorithms for computing low-cost user-aligned policies. We introduce formal concepts and key steps in proving this result here; complete proofs for all results are available in the Appendix. We begin with the notion of strategy trees to conceptualize the search process for BSQ policies.

### Strategy Trees

Every parameterized BSQ policy \((b,)\) and gPOMDP \(\) defines a strategy tree (e.g., Fig. 2(a)) that captures the possible decisions at each execution step. Intuitively, the tree starts at a belief node representing the initial belief state. Outgoing edges from belief nodes represent rule selection in \((b,)\), resulting in action nodes. Outgoing edges from action nodes represent possible observations,leading to belief nodes representing the corresponding updated belief. If the tree is truncated at horizon \(H\), each leaf represents the outcome of a unique trajectory of rules from \((b,)\) and observations.

Each belief node represents a belief state that can be calculated using the rule-observation trajectory leading to that node. A labeling function \(l:V_{B} V_{A} B A\) maps the set of belief nodes \(V_{B}\) to belief states in \(B\) and the set of action nodes \(V_{A}\) to actions in \(\). For ease of notation we define \(b_{i}^{*}=l(v_{i})\) for all belief nodes \(v_{i} V_{B}\) and \(a_{j}^{*}=l(v_{j})\) for all action nodes \(v_{j} V_{A}\).

**Definition 6**.: _Let \(\) be a gPOMDP, \((b,)\) be a parameterized BSQ policy for \(\), and \(b_{0}\) be the initial belief state. The strategy tree \(_{}(b_{o})\) is defined as \(_{}(b_{o})= V,E\) where set \(V=V_{B} V_{A}\) contains belief nodes \(V_{B}\) and action nodes \(V_{A}\), whereas, set \(E=E_{B} E_{A}\) contains edges from belief nodes to action nodes (\(E_{b} V_{B} V_{A}\)) and edges from action nodes to belief nodes (\(E_{A} V_{A} V_{B}\)). \(E_{B}\) is defined as \(\{(v_{i},r,v_{j})|v_{i} V_{B},v_{j} V_{A},r(b,), :r= a_{j}^{*}\}\). \(E_{a}\) is defined as \(\{(v_{m},o,v_{n})|v_{m} V_{A},v_{n} V_{B},o,(v_{p}, r= a,v_{m}) E_{b};b_{n}^{*}=bp(b_{p}^{*},a,o)\}\)._

Non-convexity of the expected cost functionEach parameterized BSQ policy permits infinitely many BSQ policies, one for each assignment of real values to its parameters. Unfortunately, the expected cost of parameterized BSQ policies is not a convex function of these parameters. Fig. 1(c) shows this with a counterexample using the parameterized BSQ policy from Fig. 1(b), a horizon of 12, and setting the robot's initial distance from each repair station to 5. This plot was constructed by sampling the expected cost for 251,001 equally-spaced parameter assignments to the Fig. 1(a) parameterized BSQ policy. \(E_{}(;H)\) is clearly not convex: the expected cost along the line \(_{2}=_{1}-0.25\) has two inflection points at \(_{1}=0.6\) and \(_{1}=0.8\). This creates two local minima: \(_{1} 0.16\) and \(_{1} 0.83_{2} 0.1\). Intuitively, this is due to the short horizon, which causes the optimal strategy to be selecting a repair station and traversing to it regardless of the observations. This complicates finding good BSQ policies using existing solvers. However, every possible BSQ policy can be associated with a set of strategy tree leaves that are reachable under that policy. Thus, for a given horizon, there are only finitely many expected costs for BSQ policies for a given problem.

The main challenge in computing good BSQ policies is that the set of possible BSQ policies with distinct expected costs grows exponentially with the horizon and good BSQ parameters could be distributed arbitrarily in the high-dimensional, continuous space of parameter values. We use strategy trees to define groups of leaves called braids, which we will then use to prove that the space of BSQ policy parameters turns out to be well-structured in terms of the expected cost function.

BraidsWe refer to the set of all leaves reachable under a policy \((b,)\) as _the braid of \(\)_. Due to the mutual exclusivity of rules for every assignment of parameter values to a parameterized BSQ policy, at most, one outgoing edge can be taken from each belief node (as these correspond to the rules and actions). However, the stochasticity of dynamics and observations allows for multiple outgoing edges to be possible from action nodes. E.g., in the strategy tree for the Spaceship Repair problem (Fig. 2(a)), leaves \(_{2}\) and \(_{10}\) cannot both be reachable under a BSQ policy because that would require rules \(r_{1}\) and \(r_{2}\) to be satisfied at the same belief. However, both \(_{1}\) and \(_{5}\) may be reachable under the same BSQ policy since their paths diverge on an action node. Formally,

Figure 2: (a) Strategy tree created from parameterized BSQ policy in Fig. 1 and Spaceship Repair gPOMDP with horizon of 2. (b) Complete partitions of parameter space with two of the braids highlighted. Error detection sensor accuracy for the robot and ship is 60% and 75%, respectively.

**Definition 7**.: _Let \(H\) be the horizon, and let \((b,)\) be a parameterized BSQ policy for a gPOMDP \(\). The braid of a parameter assignment \(\), \(braid_{,H}()\), is the set of all leaves in strategy tree \(_{}(b_{0})\) rooted at the initial belief \(b_{0}\) that can be reached while executing \((b,)\): \(braid_{,H}()=\{_{H}\;:\;_{H} (r_{1},o_{1},...,r_{H},o_{H}); i\;\;r_{i}=_{i} a_{i},\,b_{i}=bp ^{*}(b_{0},r_{1},o_{1},...,r_{i},o_{i})\) satisfies \(_{i}\)._

The unique interval of parameter values where a leaf is reachable can be calculated by taking the intersection of the parameter intervals needed to satisfy each rule on the path to that leaf. This is because for any compound BSQ \(\), we can compute the unique interval of parameter values \(I()\) under which \(b\) will satisfy \(I()\) by substituting each BSQ in \(\) with its corresponding inequality:

**Lemma 1**.: _Let \((b;)\) be an n-dimensional compound BSQ. There exists a set of intervals \(I()^{n}\) s.t. \((b;)\) evaluates to true iff \( I()\)._

We can utilize this result to compute the unique interval of parameter values consistent with a braid by taking the intersection of the intervals of each leaf contained in that braid (Def. 8):

**Definition 8**.: _Let \((b,)\) be a parameterized BSQ policy, \(\) be a gPOMDP, \(b_{0}\) be the initial belief state, and \(H\) be the horizon. The interval of leaf \(\), \(I()\), is defined as the intersection of intervals \(_{i}I(_{i})\) of the conditions of each rule \(r_{i}\) that occurs in the path to that leaf. The interval for a set of leaves \(L\) is defined as \(I(L)=_{_{i} L}I(_{i})\)._

Any leaf or braid with an empty parameter interval does not align with the user's requirements. For example, in Fig. 2, note that \(r_{1}\) is the only rule satisfiable if \(r_{1}\) is selected from \(b_{0}\) and the robot is observed to be broken. Using the Fig. 1(b) policy and assuming the sensor accuracy is 60%, picking a rule other than \(r_{1}\) implies that 50% likelihood was high enough to fix the robot yet 60% was not, which is a contradiction. Removing misaligned leaves and braids prunes the tree.

### BSQ Policies are Piecewise Constant

We now use the concept of braids to prove that the continuous, high-dimensional space of parameter values of a parameterized BSQ policy reduces to a finite set of contiguous, convex partitions with each partition having a constant expected cost. This surprising result implies that although the expected cost of BSQ policies is not a convex function of parameter assignments, optimizing a parameterized BSQ policy requires optimization over a finite set rather than over a continuous space. We first define a notion of similarity over assignments to parameterized BSQ policies that define BSQ policies:

**Definition 9**.: _Let \((b,)\) be a parameterized BSQ policy, \(\) be a gPOMDP, and \(H\) be the horizon. Two assignments \(_{1},_{2}\) are said to be similar, \(_{1}_{H}_{2}\), iff \(braid_{,H}(_{1})=braid_{,H}(_{2})\)._

It is trivial to show \(_{H}\) is transitive, symmetric, and reflexive, making an equivalence relation over \(^{n}\). As such, \(_{H}\) defines a partition over the same space:

**Theorem 1**.: _Let \((b,)\) be a parameterized BSQ policy, \(\) be a gPOMDP, \(b_{0}\) be the initial belief state, and \(H\) be the horizon. The operator \(_{H}\) partitions \(^{n}\)._

However, this result is not sufficient to define the structure of partitions induced in \(^{n}\), which will be required for an efficient optimization algorithm. Based on Sec. 4.1, we know that leaves whose trajectories diverge due to different rules must not be in the same braid. Furthermore, a belief state can only lead to one set of possible observations for an action regardless of the BSQ policy being followed. Intuitively, this prevents braids from being proper subsets of each other, which implies that the parameter intervals for two braids can never have overlapping parameter intervals. This gives us the desired structure for partitions induced in the space of parameter values for parameterized BSQ policies: there are parameter intervals corresponding to distinct braids in the policy tree. In other words, the set of braids partitions the parameter space into contiguous, high-dimensional intervals. This can be proved formally and stated as follows:

**Theorem 2**.: _Let \((b,)\) be a parameterized BSQ policy, \(b_{0}\) be the initial belief state, and \(H\) be the horizon. Each partition \(\) created by operator \(_{H}\) partitioning \(^{n}\) is the disjoint intervals, \(^{n}\) where \(\), \(braid_{,H}()=L\) where \(L\) is a fixed set of leaves._

Since each partition corresponds to a braid and each braid corresponds to a fixed set of leaves, which defines the expected cost for all policies corresponding to that braid, all policies defined by a partitionof the parameter space have a constant expected cost. As such, the domain of the expected cost function \(E_{}(;H)\) for gPOMDP \(\) can be represented as the disjoint intervals of each braid partition. Thus, \(E_{}(;H)\) is piecewise constant. The following result formalizes this.

**Theorem 3**.: _Let \((b,)\) be a parameterized BSQ policy, \(\) be a gPOMDP, \(b_{0}\) be the initial belief state, and \(H\) be the horizon. Each partition created by \(_{H}\) on \(^{n}\) has a constant expected cost._

In some situations, the braids that partition the parameter space can be calculated in closed form (e.g., see the Appendix for partitions for the Spaceship Repair problem). The next section develops a general approach for computing the braids and intervals corresponding to a parameterized BSQ policy, for evaluating the expected cost for each such partition, and for optimizing over these partitions.

## 5 Partition Refinement Search

In this section, we present a novel algorithm for optimizing the parameters for a parameterized BSQ policy using the theory of braids developed above. The Partition Refinement Search (PRS) algorithm (Algo. 1) constructs the set of partitions using hierarchical partition selection and refinement, where a partition is selected to be refined, a leaf that can occur in that partition is sampled and evaluated, and the partitions are refined to isolate the interval of the braid corresponding to the sample. The hypothesized optimal partition is tracked and returned as the final result after timeout.

PRS constructs the first parameter space interval as the domain of all possible parameter values (line 3). This is set as the initial hypothesized optimal partition (line 4). In each iteration, a partition \(\) is selected using exploration-exploitation approaches discussed in Sec. 6 (lines 6). A leaf \(\) is sampled from \(\) by uniformly sampling parameter value \(\) from \(\)'s parameter intervals and performing rollouts from the initial belief state to a reachable leaf using the BSQ policy \((b,)\) (lines 7 and 8). The sampled leaf \(\) is used to refine partition \(\) using the insight braids cannot overlap (Sec. 4.2). If there exists a subinterval of \(\) where \(\) does not occur, a new partition for this subinterval is constructed containing \(\)'s previous leaves and expected cost (line 9). The remaining portion of \(\), where \(\) can occur, is used to construct a partition with an updated expected cost representing \(\)'s previous leaves and \(\) (line 10). The hypothesized optimal partition is then updated (line 11).

PRS converges to the true optimal BSQ policies in the limit:

**Theorem 4**.: _Let \((b,)\) be a parameterized BSQ policy, \(\) be a gPOMDP, \(b_{0}\) the initial belief state, and \(H\) be the horizon. The likelihood of the Partition Refinement Search algorithm returning the optimal parameter interval converges to one in the limit of infinite samples._

**Complexity analysis** While the theoretical space and time complexity are linear in the number of leaves, due to PRS grouping leaves from the strategy tree (Def. 6), there is good reason to expect better performance in practice. As discussed in Sec. 4.1, strategy trees can get pruned with the removal of branches and leaves that do not align with the user's requirements. For example, in the Spaceship Repair problem using the Fig. 1 parameterized BSQ policy, a third of the possible leaves are pruned at a horizon of two, and the pruning becomes even more significant for longer horizons. Additionally, empirical results suggest that rules earlier in rule-observation trajectories are more important in dictating the partitions. Furthermore, selecting and refining partitions can be performed in parallel, further improving performance.

Partition Selection Approaches

We explored multiple partition selection approaches with a multiprocessing version of PRS. Each approach used the same dynamic exploration rate \(e_{r}\) that diminished over time. Each thread managed a subset of partitions \(X^{} X\) and updated a global hypothetical optimal partition. Additionally, we warm start PRS by randomly selecting 20 points in the parameter search space and evaluating them 40 times to build an initial set of partitions. Also, partitions that have a lower expected cost than the hypothesize optimal are sampled up to 40 before updating the hypothesize optimal. In this paper, we focus on three selection approaches and discuss two others in the Appendix.

Epsilon Greedy (PRS-Epsilon)We explore \(e_{r}\) percent of the time by uniformly sampling \(s U_{0}^{1}\) and checking if \(s e_{r}\). If we are exploring, we uniformly at random select a partition from \(X^{}\). Otherwise, the partition with minimum expected cost, \(_{,[] X^{}}[]\), is selected.

Boltzmann Exploration (PRS-Bolt)Partitions are selected in a weighted random fashion with the probability of selecting partition \(\) as \( exp([]/e_{r})\) with \(\) being the normalization factor.

Local Thompson Sampling (PRS-Local)Each thread treats the problem as a multi-armed bandit problem where the expected cost for the next sample from each partition is simulated using \((_{c},_{c})\) with \(_{c}\) and \(_{c}\) being the partition's mean and standard deviation, respectively. The partition with the lowest estimated expected cost is selected.

## 7 Empirical Results

We created an implementation of PRS and evaluated it on four challenging risk-averse problems. Complete source code is available in the supplementary material. We describe the problems and user preferences here; further details, including parameterized BSQ policy, can be found in the Appendix.

Lane merger (LM)In this problem, an autonomous vehicle driving on a two-lane road must switch lanes safely before reaching a lane merger. However, there is currently a car in the other lane that the agent does not know the location or speed of. Switching lanes too close to this car risks a severe accident. The autonomous vehicle has a noisy detection system that returns whether a vehicle is located in regions around the car. The user's preference is: _If there's a high likelihood of safely switching lanes, do so. If there is a high likelihood of the other car being in close proximity and it is possible to slow down, slow down. Otherwise, keep going._

Spaceship repair (SR)This is a modified version of the running example with parameterized BSQ policy Fig. 1(b). The robots start 7 steps and 5 steps away from the robot and ship repair stations, respectively. Additionally, the robot's sensor is 75% accurate at detecting errors with the robot and only 55% for the ship. With the short horizon \(H=12\), this results in the parameter space being not convex with multiple local minimums with differing expected costs.

Graph rock sample (GRS)We modified the classic RockSample(\(n,k\)) problem (Smith and Simmons, 2004) by replacing the grid with a graph with waypoints where some waypoints contain rocks. Additionally, we introduced risk by causing the robot to break beyond repair if it samples a rock not worth sampling. We also categorized the rocks into types, and the rover's goal is to bring a sample of each type to the desired location if a safe rock for that type exists. This goal requires a longer horizon to reach compared to the other problems. The user's preference is: _Evaluating rocks of types not sampled in order \(r_{1},...,r_{n}\), if the rock has a high likelihood of being safe to sample, go and take a sample of it. Else, if the rock has a high likelihood of being safe to sample, get close enough and scan it. Otherwise, move towards the exit if no rocks are worth sampling or scanning._

Store visit (SV)This problem is based on the partially observable OpenStreetMap problem in Liu et al. (2021). A robot is located in a city where some locations are unsafe (e.g., construction, traffic), which can terminally damage the robot. The robot is initially uncertain of its location but it can scan its surroundings to determine its general location. The agent traverses the city and can visit the closest building. The goal is to visit a bank and then a store. This problem features a nuanced parameterized BSQ policy: _If you are significantly unsure of your current location, scan the area. If you have visited a bank, do the following to visit a store; otherwise, do it to visit a bank. If you are sufficiently sure the current location has the building you are looking for, visit it. Otherwise, move towards where you think that building is while avoiding unsafe locations. If all else fails, scan the current area._

### Baselines

We evaluated PRS against three different types of baselines.

RC compliantSelect random parameter values uniformly at random from the parameter space to produce user-aligned policies.

Hyperparamter optimization algorithmsTo measure the benefits of PRS against existing hyperparameter optimization algorithms, we implemented both _Nelder-Mead_(Nelder and Mead, 1965) and _Particle Swarm_(Kennedy and Eberhart, 1995). The expected cost of parameter space point \(\), for parameterized BSQ policy \(\), was computed by averaging 1,000 parallel runs of the policy \((b;)\). For Nelder-Mead optimization, we used a simplex that had vertices numbering one more than the number of parameters in the parameterized BSQ policy being optimized. We warm start by initially evaluating a 100 random points to construct the initial simplex using the best-performing points. For Particle Swarm optimization, 10 particles were used with the location and momentum of each particle clipped to the search space. The coefficients changed based on steps since the last improvement.

Unconstrained POMDP solversTo measure the differences between BSQ policies and unconstrained POMDP solvers, we implemented variations of our problems into POMDPX and solved them with DESPOT (Somani et al., 2013) and SARSOP (Kurniawati et al., 2009) for 1,000 evaluation runs. To measure whether an action-observation trajectory produced with these solvers aligns with the user's requirements, we check if there exist parameter values \(\) where policy \((b;)\) could produce that trajectory. We use this to evaluate the solutions produced.

### Analysis of Results

For each problem, we evaluated each baseline and PRS variant ten times. The horizon was 12 for Spaceship Repair and 100 for the other problems. The timeout for PRS was set on a problem-by-problem basis. Timeout for Nelder-Mead and Particle Swarm was one hour. Note that the highest expected cost is equal to the horizon due to the default cost function. The performance of each PRS partition selection approach can be found in Figure 4 and the quality of solutions over time compared to the baselines are shown in Figure 3.

Partition selection approach evaluationsPRS partition selection approaches converged to a similar quality policy. The only difference was the time taken with approaches that did not rely on the standard deviation converging faster due to there being a lower standard deviation near the optimal solution, causing selection approaches that used the standard deviation to explore the wrong partitions. We use PRS-Bolt as a representative when comparing against the other baselines.

Figure 3: Empirical results evaluating the hypothesized optimal partition performance tracked. Equally spaced samples across PRS evaluation time are taken while a sample is taken each iteration of Nelder-Mead and Particle Swarm. The error displayed is the standard deviation error.

PRS solution qualityPRS produced a higher-quality policy compared to the ones produced by RCompliant. For Spaceship Repair, the simplest problem solved on the shortest horizon, policies produced by PRS-Bolt had a 15.68% lower expected cost and 3.47% higher goal achievement rate. For the other problems, policies produced by RCompliant had more than triple the expected cost and achieved only half the success rate on both Graph Rock Sample and Store Visit. These results demonstrate that optimizing BSQ parameter values has a significant impact on the performance of user-aligned policies.

Hyperparameter optimization evaluationCompared to traditional hyperparameter optimization algorithms, PRS always found the user-aligned policy with the lowest expected cost with little performance deviation. This is due to Nelder-Mead and Particle Swarm struggling to optimize a non-convex piecewise-constant function using noisy data, resulting in known problems with local-search algorithms: problems of getting stuck in sub-optimal local minima and exploring the incorrect space. Additionally, PRS converged first since it is more sample-efficient. It is computationally expensive to update the belief state, resulting in poor-quality solutions being more expensive to evaluate due to taking longer to reach the goal. PRS only requires a couple of evaluations before spending the computational resources on more promising areas.

An interesting result is that, in Spaceship Repair, solutions found by Nelder-Mead and Particle Swarm both had a 7.73% higher expected cost and 18.31% higher goal achievement rate than the PRS-Epsilon solutions. There is likely a high negative correlation between the expected cost and goal achievement rate. PRS is better at optimizing the stated objective of minimizing the expected cost.

Unconstrained solver evaluationWithout guiding from the parameterized BSQ policies, DESPOT and SARSOP struggled with this set of problems. SARSOP failed to converge to a policy due to the long problem horizon. DESPOT could not run on Lane Merger, which had the largest state space and branching factor. DESPOT also only achieved the goal 0.5% of the time on Graph Rock Sample. DESPOT achieved a lower expected cost of 20.0% and 13.3% on variations of Spaceship Repair and Store Visit, respectively. However, DESPOT's policy never aligned with the user's requirements on Store Visit and only 7.3% of the time on Spaceship Repair. This indicates that the BSQ framework offers a new approach for expressing both domain knowledge and user requirements.

## 8 Conclusion

We presented the BSQ policy framework for expressing users' requirements over the belief state in partially observable settings for computing user-aligned agent behavior. We performed a formal analysis of these policies, proving that the parameter value space introduced in the parameterized BSQ policies can be partitioned, resulting in parameterized BSQ policies being optimizable through a hierarchical optimization paradigm. We introduced the probabilistically complete Partition Refinement Search algorithm to perform this optimization. Our empirical results show that it converges to the optimal user-aligned policy quicker and more consistently than existing approaches. Results indicate that parameterized BSQ policies provide a promising approach for solving diverse real-world problems requiring user alignment.

Limitations and future workThere are many interesting directions for future work based on the current BSQ policy framework. BSQ representations can be made more expressive by allowing deterministic functions, which would not compromise the presented theoretical results. Furthermore, there exists a natural extension of this work into finite memory controllers that allows temporally extended requirements to be encoded with the same theoretical results. Relaxing the constraints on mapping each belief state to a single action would expand the usability. For more complex problems, a belief-state approximation approach would be required, but the underlying strategy tree discussed in this work would remain mostly unchanged. Another interesting research direction is to develop methods that help users express their requirements in the BSQ framework.

Figure 4: Results for PRS with different partition selection approaches from Section 6.