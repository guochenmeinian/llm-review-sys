# _Teach Better or Show Smarter?_ On Instructions and Exemplars in Automatic Prompt Optimization

Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Ark

Google Cloud AI Research

{xingchenw, ruoxis, hootan, soarik}@google.com

###### Abstract

Large language models have demonstrated remarkable capabilities but their performance is heavily reliant on effective prompt engineering. Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting _instructions_ (instruction optimization, IO) vs. those targeting _exemplars_ (exemplar optimization, EO). Despite their shared objective, these have evolved rather independently, with IO receiving more research attention recently. This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and EO techniques both isolation and combination on a diverse set of challenging tasks. Our findings reveal that intelligently reusing model-generated input-output pairs obtained from evaluating prompts on the validation set as exemplars, consistently improves performance on top of IO methods but is currently under-investigated. We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with EO strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization. Moreover, we observe a synergy between EO and IO, with optimal combinations surpassing the individual contributions. We conclude that studying exemplar optimization both as a standalone method and its optimal combination with instruction optimization remain a crucial aspect of APO and deserve greater consideration in future research, even in the era of highly capable instruction-following models.

## 1 Introduction

Significant advancements in large language models (LLMs) have revolutionized various natural language processing tasks . One notable aspect of LLMs, however, is their sensitivity to the input "prompts," which has given rise to the burgeoning field of prompt engineering . On black-box LLMs where we can neither modify or access internal parameters, prompt engineering involves crafting input prompts that effectively guide LLMs to generate desired outputs. Starting from manual processes requiring human expertise, the complexity and volume of prompts have necessitated the development of automatic prompt optimization (APO) methods aiming to streamline and automate prompt generation, thereby alleviating the burden of manual intervention. Broadly, since prompts consist of instructions and exemplars, we may roughly categorize APO into _instruction optimization_ (IO) and _exemplar optimization_ (EO) approaches. IO focuses

Figure 1: Average performance over >20 tasks on PALM 2 – We compare and combine APO targeting _exemplars_ and _instructions_, and find that how we optimize exemplars (orange) can eclipse how we optimize instructions despite current research favoring the latter (blue and purple), whereas optimizing both is the best (cyan) within similar budget.

on refining the textual instructions provided to LLMs that contain task-specific information (i.e., to _teach_), whereas EO emphasizes the selection of relevant examples to guide model behavior (i.e., to _show_). Partially driven by the improved instruction-following ability of LLMs, the research attention has increasingly shifted towards IO, especially using LLMs themselves as optimizers .

While EO and IO approaches address the similar overarching problem, they have evolved somewhat independently, with a few exceptions . Indeed, as we elaborate in SS2, EO approaches are often based on simple, handcrafted templates without explicit instruction optimization , while IO methods seldom optimize exemplars and often rely on random validation set samples , require additional fixed exemplars on top of the validation set , or consider the "zero-shot" setup with no exemplars at all . Whereas the lack of IO in EO methods is somewhat understandable as many EO approaches predate instruction finetuning  and, subsequently, instruction-following models that are sensitive to instructions, the inverse is much less so: concretely, almost _all_ existing IO approaches _already_ require a labeled dataset as the validation set, and are therefore, by definition, not "zero-shot". With the inputs, labels, and, if applicable, model-generated intermediate outputs (e.g., reasoning steps) on the subset of the validation set that the model has answered correctly, we already have a set of exemplars as a free side-product whenever we perform IO, independent from and on top to any additionally provided, human-annotated exemplars. A common argument for _not_ focusing on EO, such as mentioned in Pryzant et al. , is the goal to focus on one objective at a time1. However, given the common practical goal of and the interplay between EO and IO , we argue they should not be treated separately - it is instead critical to understand their relative importance and combined impact, and, where necessary, optimize them jointly for the best performance-cost balance.

This is, to our knowledge, where there is a gap in the literature that we aim to bridge. To do so, on a diverse suite of challenging BIG-Bench and MMLU tasks, we compare the performance gain brought by various representative, state-of-the-art (SoTA) IO and EO methods on the fairground with PaLM 2, Gemini (1.0/1.5) and GPT models to foster better scientific understanding of different APO techniques. While IO comfortably improves the baseline prompts before any instruction or exemplar optimization, this, at best, portrays an incomplete picture. Under the same setup, with simple yet effective EO methods on the model-generated exemplars on the validation set, we show:

* Intelligently incorporating exemplars generated by the target model itself on the validation set significantly and consistently improves performance on top of recently proposed IO methods;
* The performance gains realized by choosing appropriate exemplars via methods as simple as random search can eclipse the improvements brought by SoTA instruction optimization. As a concrete example, as shown in Fig. 1, with a simple optimization routine on exemplars, seed instructions _before any optimization_ outperform optimized instructions obtained with complex IO but with random exemplars most commonly used.
* There exists a synergy between EO and IO, and optimally mixing-and-matching IO and EO is greater than the sum of its parts under a comparable computational budget.
* SoTA IO might be itself implicitly generating exemplars, and these exemplars, while somewhat unintentional, contribute more to the performance than the rest of the instruction that IO methods are meant to optimize.
* While arguably receiving less research attention recently, exemplar optimization remains a crucial design consideration in APO. Even in an era of highly capable instruction-following LLMs, the significance of exemplar optimization should not be relegated to an afterthought, and better exemplar optimization both as a standalone tool and as a combinable component with IO is crucial for APO.

## 2 Preliminaries

_Prompts_ are natural language inputs to LLMs. Denoting an input task query as \(x\), a few-shot prompt \(P(x)\) may be represented as \(P(x)=[I,e_{1},...,e_{k},x]\) where \(I\) denotes an _instruction_ and \(\{e_{1},...,e_{k}\}\) denote \(k\)_exemplars_ (or interchangeably, _demonstrations_), each of which is a concatenation of other queries and their outputs (including both the final answer and any possible intermediate outputs) which resemble the current query \(x\) or may otherwise guide the LLM to better handle the current task, and we show a common prompt template organizing these components in Fig. 2 - note that not all components are required: e.g., zero-shot prompts feature no exemplars.

_Automatic prompt optimization_ (APO) aims to automatically design \(P(x)\) via optimization. We broadly consider _black-box_ API-only LLMs2. The proposed framework assumes a _validation dataset_\(_{}:=\{(x_{i},y_{i})\}_{i=1}^{n_{}}\), where \(x_{i}\) and \(y_{i}\) represent validation inputs and targets, a _performance metric (e.g._, _accuracy_) \(g(,)\), and aims to find the optimal prompt \(P^{*}(x)\) to be used at test time, which is empirically the maximizer on \(_{}\):

\[P^{*}(x)=_{P()}_{(x,y)_{ }}gf_{}P(x),y, \]

where \(f_{}()\) denotes a textual LLM output given input and \(\) denotes the _search space_, whose definition allows a broad categorization of APO methods into _instruction optimization_ methods targeting instructions in Fig. 2, _exemplar optimization_ methods targeting exemplars in Fig. 2 and approaches that tackle both.

**Exemplar optimization (EO).** Efforts to optimize exemplars started soon after the discovery of in-context learning (ICL)  via retrieval-based approaches to identify the closest labeled examples , influences and sensitivity , and learning-based approaches . Toolskits like DSPy  adopt EO as the main APO paradigm. Works  have also extended EO to model-generated exemplars in LLMs and multimodal models. Lastly, by framing EO from an active learning angle, Margatina et al.  provide a comprehensive understanding and comparative analyses. These works, however, principally analyze different EO strategies only, nor do they analyze from the angle of APO. Many of these works also primarily focus on and draw findings from earlier and simpler tasks that are arguably less challenging to SoTA LLMs.

**Instruction optimization (IO).** On black-box LLMs, the origin of IO may be traced to _discrete prompt search_ which prepend optimized tokens, which can be viewed as a form of "instructions", to inputs. However, these approaches do not necessarily yield interpretable prompts and most of them require output signals (such as logits) beyond strictly black-box access. Thus, recent advances have shifted towards utilizing an LLM itself to generate natural language instructions for iterative optimization on \(_{}\) in Eq. 1. The seminal works is APE , which employs the LLM to iteratively cull top-performing instructions on \(_{}\) and paraphrase them until convergence. Similar evolutionary frameworks are widely used in follow-up works  and alternative formulations like Bayesian optimization (BO)  and neural bandits  were also used. Another line of works  employ _reflection_, directing an LLM to articulate reasons for errors to iteratively improve instructions. Other approaches like OPRO and its variants , treat the LLM as a black-box optimizer, tasking it with generating new instructions based on the trajectory of previously evaluated instructions and their performances without explicit meta-instructions.

**Combining EO and IO.** As discussed in SS1, there is a relative dearth of work combining EO and IO despite their shared objective. Specifically, even when the labeled dataset \(_{}\) is a prerequisite of virtually all IO methods, it is primarily used to estimate the expectation in Eq. 1 only rather than to construct exemplars in a principled way: For instance, ProTeGi  randomly samples exemplars from \(_{}\), while OPRO  uses them only for instruction induction . Other works  either use no exemplars or fix exemplars and only optimize the instructions - for challenging reasoning tasks, these methods require human-annotated chain-of-thought (CoT) exemplars _in addition to \(_{}\)_, which arguably runs counter to the goal of automatically designing prompts _without_ human intervention in APO. A few exceptions exist: PromptBreeder  employs "context shuffling" to co-evolve exemplars and instructions, while Mixture-of-Prompts (MoP)  aligns exemplars with multiple prompting "experts" for joint optimization. However, these works still focus their optimization effort on instructions: PromptBreeder emphasizes complex mutation operators for IO while providing only basic EO frameworks, whereas MoP chiefly focuses on IO with the bulk of its contribution being assigning optimized instructions to different exemplar groups, rather

Figure 2: An example prompt: instruction \(I\) describes the task; exemplars (\(e_{1},...,e_{k}\), \(k=1\) in the figure) provide demonstrations and enable ICL; both are prepended to the query \(x\) before receiving the LLM responses.

than optimizing the exemplars themselves. Other works [13; 57] also include both exemplars and instructions in the search space, but they require information beyond strictly black-box outputs to some extent. Lastly, several works have analyzed the interplay between ICL and instructions  or prompt templates , but they mainly characterize the performance variation as an _issue_ deserving attention. We, however, consider the APO setup specifically, and argue that such an interdependence presents an _opportunity_ through holistically considering instructions and exemplars. Concurrent to our work, Agarwal et al.  and Opsahl-Ong et al.  also study the joint optimization of instructions and exemplars, and in many cases reached conclusions corroborating our findings, demonstrating the community's growing awareness on the importance of the subject of focal interest to this paper.

## 3 Understanding Instruction Optimization and Exemplar Optimization

While studying IO and EO independently has academic value, the practical goal ultimately for both is to optimize the performance of LLMs. Hence, IO and EO, as two dominant genres of APO methods, present practitioners with the challenge of selecting or combining them to maximize cost-performance benefits. We aim to meet this by evaluating EO and IO in the context of APO by answering the following: **1)** What is the relative importance and performance impact of EO and IO, both in isolation and when combined together? **2)** How do we make the optimal use of the limited data and computational budget under the current APO framework?

### Experimental Setup

We perform thorough experiments employing various EO and IO methods individually and in combination. We use the PaLM 2 text-bison-002  and Gemini 1.0 Pro/1.5 Flash [15; 36] as the target models, but we will also validate key findings on GPT-3.5. Modern IO methods often employ another, usually more potent _optimizer model_ for to generate and/or critique instructions; we use PaLM 2 text-unicorn-001 (for text-bison target model), Gemini 1.0 Ultra (for Gemini 1.0 Pro target model) or Gemini 1.5 Pro (for Gemini 1.5 Flash target model). We evaluate on tasks selected from BIG-Bench Hard (BBH) , a collection of diverse tasks considered to be challenging to LLMs - the suite itself and datasets of similar task types are frequently used in many recent APO works [48; 54; 19; 14; _inter alia_]: the tasks include numerical reasoning, commonsense problem-solving, logical deduction, linguistic manipulation, machine translation, and tabular reasoning, among others. For all tasks, we use 20% of data as validation set and the remaining 80% for testing, the latter of which is held-out and unavailable to the LLM at search time (see App. A for implementation details). We also test some of our key findings on the MMLU benchmark , a set of 57 tasks frequently used to gauge the general problem-solving abilities of LLMs - we use the official val and test splits for validation and testing, respectively. We consider the following IO strategies:

1. **No IO**: we use the seed instruction \(I_{0}\) "_Let's think step by step_."  without any optimization.
2. **APE** is the seminal work for LLM-as-an-instruction-optimizer and uses an evolutionary algorithm design: at each iteration, we evaluate a population of instructions on the validation set and the optimizer is asked to generate a new population by paraphrasing the top-performing instructions. This process iterates until convergence.
3. **ProTeGi** collects samples that the target LLM answers incorrectly on \(_{}\) under the current instruction and directs the optimizer LLM to reflect and critique it. The optimizer model is then asked to update the instruction by summarizing and abstracting the feedback. Additionally, at each iteration, ProTeGi also paraphrases instructions similar to APE (referred to as "Monte Carlo sampling") and uses beam search to identify the most promising instructions.
4. **PromptAgent** is similar to ProTeGi but it features a more advanced _planning agent_ using Monte Carlo tree search .
5. **OPRO** implicitly optimizes instructions. Starting from the seed instruction, at each iteration, OPRO provides the optimizer model a concatenation of previously evaluated instructions and their validation scores. Instead of explicitly requesting paraphrasing or reflection, OPRO treats the optimizer LLM as a black-box optimizer and simply asks the optimizer model to "come up with a better instruction" given these information.

The above methods are selected as each of them represents the state of the art of a genre of approaches as outlined in SS2 and collectively represents IO techniques as a whole. We initialize each method at the seed instruction and ensure they consume the same amount of compute measured by the number of prompt evaluations on \(_{}\ m\) (we cap \(m=32\) except for "No IO" which requires no iteration). We also compare against PromptBreeder  in a later section, as it features a much more expansive search space and requires significantly more than 32 iterations before convergence. After obtaining the optimized instruction \(I^{*}\) (or \(I_{0}\) if no IO is performed), we perform EO. At this point, we emphasize that our setup should _not_ be confused with the "few-shot" setup considered by some prior works  which require additional human-annotated exemplars with reasoning traces to elicit CoT behavior. We perform EO only from the exemplars _self-generated_ by the target model (also referred to as "bootstrapped few-shot" in DSPy and "reinforced ICL" in concurrent works like Agarwal et al. ) and _do not assume exemplars are given at the start of APO_ (i.e., we do not assume the presence of initial \(\{e_{1},...,e_{k}\}\) in \(P(x)\)). We consider the following EO strategies:

1. [leftmargin=*]
2. **No EO**: no exemplars are used; this is typically referred to as "zero-shot" in the APO literature.
3. **Random**: we randomly sample \(k\) input-output pairs from \(_{c}(I^{*})_{}\), _the subset to the validation set that the target LLM predicted correctly under \(I^{*}\)_ and the output in this case includes any intermediate output the LLM generates before the final answer.
4. **Nearest**: We use the same \(_{c}(I^{*})\) as above, but instead of sampling randomly, we _retrieve_ top-\(k\) input-output pairs whose inputs are most similar to the current test input based on text embedding cosine similarity. We use the Gecko embedding .
5. **Diversity**: We use \(_{c}(I^{*})\) but select the \(k\) input-output pairs closest to the centroids via \(k\)-means clustering, similar to the approach in Zhang et al.  to promote diversity in exemplars.
6. **All exemplars** (_Gemini 1.5 target models only_): With the advent of long-context models like Gemini 1.5, we may also fit the entire set of \(_{c}\) into the context and perform no selection at all.

The above _heuristic_-based EO strategies do not use \(_{}\), whereas _optimization_-based EO can utilize it similarly to IO. Instead of generating _instructions_, we select the _exemplar combinations_ with the highest validation accuracy for testing . Unlike IO, which creates _new_ instructions via an optimizer model, EO selects from _pre-generated_ outputs and does not require an optimizer model. Formally, we focus on optimizing exemplars conditional on \(I^{*}\) from IO (or \(I_{0}\) if no IO is involved)3: 
We include the following optimization-based EO methods that differ in search strategy:

1. [leftmargin=*]
2. **Random search**: Following the EO procedure in DSPy, we randomly sample \(m\) combinations of \(k\) exemplars: \(\{E_{1},...,E_{m}\}\) where each \(E_{}=\{e_{j}^{}\}_{j=1}^{k}\,\,\{1,...,m\}\). We evaluate each combination on the validation set and use the best for testing.
3. **Mutation**: We also implement a mutation-based baseline, initiating with a population of \(Q\) combinations for \(T=m/Q\) generations, where \(Q=8\). Each generation starts with a randomly

Figure 3: _Appropriate EO improves over any or no IO_: Task-specific BBH performance _with no instruction optimization_ (**left**) and _with SoTA IO_: APE (**middle**) and ProTeGi (**right**) before and after applying exemplars found via Mutation (§3.1) on PaLM 2. Dashed and solid lines denote the average performance before and after exemplars, respectively. _Task index_ is determined by the ascending order of test accuracy under seed instruction. Refer to additional visualization in App. B.3.

[MISSING_PAGE_FAIL:6]

like extreme restrictions in context length, which This might restrict applicability of IO methods too as many SoTA IO methods also generate long prompts, and/or extreme long-context tasks where it is not possible to fit exemplars in the context window, _there is little incentive to consider the 'zero-shot" setup without exemplars and little incentive not to perform EO_, given that current APO setup requires \(_{}\) anyway, regardless of whether we use them as exemplars. Thus, it is by definition, not "zero-shot" and is not directly comparable to true zero-shot methods requiring no labeled data. Furthermore, there is also the risk that "zero-shot" results neither _reflect_ nor accurately _predict_ the full potential of the LLM, as what performs well under zero-shot does not necessarily performs well when a better EO strategy (e.g., PromptAgent in Table 1 and ProTeGi in Table 3) is used. Lastly, since obtaining labeled data can be costly, intelligently reusing them as exemplars also represents a more judicious use of scarce resources compared to only using them to evaluate a metric for optimization.

**Insight 2**: How we select exemplars may outweigh how we optimize instructions, and selecting exemplars via _iterative optimization_ consistently outperforms alternative strategies.

**Exemplar optimization outweighs instruction optimization.** Despite the recent focus the community places on IO, we find that how we select exemplars outweighs how we optimize instructions in the model-task combinations we investigate. With reference to Tables 1 - 4 (and task-specific breakdown in Fig. 5), we find that if we optimize instructions _or_ exemplars (i.e., the bluecells) under a roughly compute-matched budget, _prompts without instruction optimization but with optimized exemplars_ (e.g., the "No IO + Mutation" combination) _outperform prompts with SoTA instruction optimization but without optimized exemplars_ (e.g., the "ProTeGi + Random" combinations) in an overwhelming majority of cases. In fact, on a separate set of experiments performed on the PaLM 2 models, we find this to be true _even after halving the evaluation budget of EO_ (see App. B.6), and optimization on exemplars as naive as random search can outperform IO methods that are significantly more complicated and expensive. Further substantiating this argument are that:

**1)** In _isolation_, EO boosts performance more effectively than IO: for example, with reference to Table 2, compared to the seed prompt, using the best EO strategy (Mutation, _first row_) alone increases the average performance by >11%, compared to approximately 8% using the best IO strategy (ProTeGi, _first column_);

**2)** When _combined_, benefits of EO and IO stack up but are largely attributable to EO: under "Mutation" (_last column_), the best EO strategy, the performance gap between the best and worst IO strategies shrinks to less than 4%, suggesting that instructions might be less critical if the LLM is provided with good exemplars after all.

We observe similar conclusions for different models and task combinations. In fact, on MMLU (Table 3) featuring much smaller validation splits, we observe that judicious exemplar optimization completely eliminates the performance gap caused by IO under zero-shot, with _No IO_ even surpassing SoTA IO. Interestingly, as we show in Fig. 4 where we further consider the difference between validation accuracy, which is the empirical objective function in Eq. 1, and the test accuracy, which is the reported metric that represents the generalization power of the optimized prompt, _optimized exemplars generalize better than optimized instructions_ under all model-task combinations considered. On MMLU tasks (two rightmost plots in Fig. 4), IO even improves validation performance comparable to or better than EO, but the validation improvement does not generalize to the test set. These imply that the superior test performance of EO cannot be solely attributed to a more effective search space \(\) or optimization strategy in Eq. 1, and the performance gap might not be completely closed by advancing optimization only.

**Optimization-based EO outperforms heuristics.** Between the different EO strategies, we find that optimization-based EO vastly outperform the alternatives: e.g. in all tables, ProTeGi with optimized

Figure 4: _Optimized exemplars generalize better than optimized instructions._ Comparison of validation accuracy and test accuracy over different model-task combinations. The generalization gap, which is the difference between validation and test accuracy, is marked on each figure. The better generalization of EO is exemplified by the smaller generalization gaps in all cases studied.

exemplars outperforms random exemplars, which is the default design in Pryzant et al. , by more than 6% in both Table 1 and 2 and more than 2% in Table 4. Interestingly, as shown by the "All" column in Table 4 for Gemini 1.5 and App. B.4 for Gemini 1.0, naively scaling the number of exemplars may not be the most effective - the fact using the entire \(_{c}\) underperforms 3 optimized exemplars, which are a subset of \(_{c}\), highlights the relevance of EO even for modern LLMs capable of handling long context windows. On the other hand, heuristic-based selection like _Diversity_ and _Nearest_ do not consistently outperform simple random baseline, echoing previous findings .

**Imitation of task-dependent winning patterns outweighs elaborate descriptions.** We further present representative prompts in Fig 6 and how LLM responds to them in App. B.9: Generally, we find that even detailed and task-specific instructions do not regulate the LLM's behavior as effectively as exemplars, which enable _imitation_. For example, for challenging tasks like tracking_shuffled_objects and web_of_lies, optimization-based EO discover "winning" templates that, when followed, improve performance massively. Even when SoTA IO methods may often state the answer steps equivalently in words, we find LLMs to simply respond better to exemplars from which they can copy behaviors. On the other hand, for tasks where CoT-style answering are known to be unhelpful (e.g., snarks) , the optimized exemplars are invariably those giving direct answers; when prepended to the test queries, LLMs tend to prioritize exemplar imitation over instruction following to answer directly despite triggers like "We should move forward in stages". These highly task-dependent "winning" patterns that vary from elaborate step-to-step reasoning to direct answering may also explain why heuristic-based EO fares worse to data-driven approaches, since there might not be a single heuristic universally useful for all tasks.

**Concluding remarks.** We argue that the findings are highly significant for the future of APO. First, they point to a need of re-balancing: without disparaging the value of IO, we argue that EO is at least

Figure 5: Task-specific BBH performance of selected IO-EO combinations with PaLM 2 (first 12 tasks; refer to App. B.3 for all other tasks/models). Note that **1)** Proper EO almost uniformly improves performance and **2)** With appropriate exemplars, seed instructions **with no optimization** (third bar from the right) can often perform on par or better than SoTA IO but with standard random exemplars or no exemplars commonly used in the literature (first six bars in each figure).

Figure 6: “Winning” exemplars that led to exceedingly high performance: **(a, b)** LLM improves by almost 50% from imitating and chaining the patterns in the optimal exemplars. **(c)** When CoT hurts performance, optimal exemplars encourage LLMs to override instructions and answer directly. Refer to App. B.9 for examples of LLM responses when these exemplars are applied.

equally crucial and should not be relegated to an afterthought. Second, we note that the EO strategies studied are in no way exhaustive. In fact, in contrast to the sophisticated search and instruction generation approaches adopted by IO methods, they can even be considered elementary. Yet, they deliver comparable or more significant improvements. Thus, we anticipate advanced methods that more effectively optimize exemplars would yield even greater enhancements: some techniques in IO may be adapted to EO with little-to-no modifications. For example, many recent advances in IO adopt an evolutionary search framework - while our "Mutation" baseline can be seen as an elementary version of it in EO, it should be also straightforward to use more advanced search strategies and operators or use techniques like LLM-generated paraphrasing on top of selected exemplars. Other search techniques, such as sample-efficient combinatorial BO , can be uniquely suitable for the EO setup, which is itself a combinatorial optimization problem4. Furthermore, while we used a fixed set of exemplars (i.e., _task_-wise selection), it might also be fruitful to further explore in the direction of _instance_-wise selection . Lastly, as discussed, the presence of (often large) generalization gap also suggests the importance to consider generalization alongside optimization, which seems to be the chief focus thus far; it might be promising to investigate analogies of well-tested in classical machine learning like regularization and cross-validation in APO.

**Insight 3**: Optimizing both instructions and exemplars is greater than the sum of its parts, _even under a comparable computational budget_.

For most of the results obtained, we note that iteratively optimizing both instructions _and_ exemplars led to the best performance. This naturally leads us to answer the second research question: whereas experiments in Tables 1 and 2 expend additional cost by optimizing exemplars _on top of_ the optimized instructions, we show that **1)** such a combinable benefit does not simply root from the additional compute and **2)** optimally mixing-and-matching IO and EO leads to significant performance improvement with negligible additional overhead.

Concretely, we budget a _total_\(m=32\) prompt evaluations on \(_{}\) where we use first \(m_{}\)={0,8,16,24,32} iterations optimizing instructions and the remaining \(m_{}\) optimizing exemplars. We summarize the results in Fig. 7, where we find that **1)**_any_ mix-and-match outperforms IO or EO only (i.e., \(m_{}\)=0 or 32), and **2)** the best allocation bridges the gap or almost bridges the gap compared to the combination that uses twice as many prompt evaluations (last column) - interestingly, in this case the optimal allocation also roughly reflects the relative contribution of IO and EO to the overall performance improvement in Table 1. We show in App. B.7 that the above findings hold for other target models and instruction optimizers, and we also give detailed examples and explanations of the mechanism leading to this synergy. Additionally, in App. B.10, we compare this simple routine against PromptBreeder , which is one of the few existing IO methods that supports EO via an

Figure 7: _Mixing-and-matching EO and IO outperforms either alone under a similar budget_. **Top figure**: Validation accuracy vs. # evaluations on \(_{}\) with PaLM 2 in selected tasks if we optimize instructions only (via APE), exemplars only (Mutation), or both (first 8 evals for IO (purple shade) + remaining 24 for EO). Gray dashed lines denote the performance of \(l_{0}\). **Bottom table**: Test accuracy averaged across all tasks for different IO/EO budget allocations for PaLM 2 and Gemini 1.5. \({}^{}\)Used best APE results without EO that incur additional evaluations. Refer to App. B.7 for all per-task results and additional results on Gemini 1.0 Pro and other instruction optimizers.

optional "context shuffling" routine. It, however, mutates the exemplars purely stochastically rather than optimizing from the validation metric. We show that despite the simplicity, our algorithm converges to comparable or better solutions while incurring a fraction of the PromptBreeder cost, which often requires hundreds of evaluations before convergence. Finally, we note that the presented way to combine optimization-based IO and EO is a proof of concept and room for future improvement can be vast and we experiment several other alternative ways to combine them in App. B.8, but we defer thorough investigations to a future work.

Beyond inspecting performance metrics only, we also examine the actual instructions and exemplars discovered. While detailed prompts are available in App. C, we highlight a key observation that adds a new dimension to our discussion: SoTA IO strategies may inadvertently utilize exemplars already. Despite IO methods not typically explicitly focusing on exemplars, we find them to frequently generate texts resembling exemplars within instructions through feedback and reflection processes. For instance, PromptBreeder employs "Lamarckian mutation" to reconstruct instructions from input-output pairs, while ProTeGi prompts the optimizer model to analyze target model errors. These operators, though varied, all involve taking actual validation exemplars as inputs to optimizer models. As exemplified by Fig. 8, while the original intention may have been to abstract _task-level_ instructions, the model occasionally incorporates these exemplars verbatim in the instructions. Whereas these "quasi-exemplars" may seem unintentional, we observe that they are surprisingly common in highlighting instructions and often contribute more to the performance than the actual instructions themselves.

We argue that the findings here provide further evidence corroborating our insights obtained so far and our suggestions advocating explicit EO. Indeed, in contrast to explicit optimization of the exemplars, the aforementioned mechanism of exemplar discovery via IO is entirely opportunistic and, depending on interpretation, an unintentional artifact. For example, the quasi-exemplars in Fig. 8 almost certainly originate from the optimizer model in ProTeGi taking a convenient shortcut by incorporating a critique into the instruction verbatim (note the presence of traces like "Label: a Prediction: b" which suggests a previous mistake by the target model), which should _not_ happen if the optimizer model perfectly executes the intended task of _abstracting_ these critiques. Thus, we argue that instead of relying on the opportunistic exemplar generation via IO, explicitly optimizing for exemplars can be more preferable, as shown throughout this study.

## 4 Conclusion

We present comprehensive evaluations on the SoTA IO and EO methods, both individually and combined. We demonstrate that EO can be a potentially more crucial element in APO, revealing a beneficial synergy through joint optimization with IO. We also find that the high performance of SoTA IO methods can themselves be driven by implicit yet spontaneous exemplar discovery. Overall, we advocate for further research into EO, both as an independent approach and in conjunction with IO, even for highly-capable instruction-following modern models. One limitation is that although the tasks we consider are fairly diverse and findings on it are already of value given the widespread interest just on these tasks _only_, they are not exhaustive, omitting tasks like open-ended longer-form generation and metrics like safety & harmfulness which are important for responsible use of LLMs. As is the case for any inductive study deriving insights from experiments, there is also a possibility that the findings may not fully generalize to other tasks and/or models. Expanding to include these aspects would be important for future work.

Figure 8: _The best instructions might actually be exemplars_: Best instruction discovered by ProTeGi on hyperbaton where spontaneously discovered “quasi-exemplars” are highlighted. We also edit the instructions to either remove or retain the highlighted parts and find these quasi-exemplars, rather than the rest of the instruction, drive the performance. See App. B.11 for more examples.