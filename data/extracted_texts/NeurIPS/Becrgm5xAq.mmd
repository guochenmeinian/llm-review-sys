# RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark

Federico Berto\({}^{*}\)

Chuanbo Hua\({}^{*}\)

Junyoung Park\({}^{*}\)

Equal contribution.

Laurin Luttmann\({}^{*}\)

Equal contribution.

Yining Ma

Fanchen Bu

Jiarui Wang

Baioran Ye

Songhyeok Choi

Nayeli Gast Zepeda

Andre Hottung

Jianan Zhou

Jieyi Bi

Yu Hu

Fei Liu

Hyeonah Kim

Jiwoo Son

Haeyeon Kim

Davide Angioni

Wouter Kool

Zhiguang Cao

Qingfu Zhang

Joungho Kim

Jie Zhang

Kijung Shin

Cathy Wu

Qingso Ahn

Guojie Song

Changhyun Kwon

Kevin Tierney

Lin Xie

Jhixyo Park

KaiST

\({}^{1}\)KAIST, \({}^{2}\)Leuphana University, \({}^{3}\)Nanyang Technological University,

\({}^{4}\)Southeastern University, \({}^{5}\)Peking University, \({}^{6}\)Bielefeld University,

\({}^{7}\)Soochow University, \({}^{8}\)City University of Hong Kong, \({}^{9}\)University of Brescia,

\({}^{10}\)ORTEC, \({}^{11}\)Singapore Management University, \({}^{12}\)MIT, \({}^{13}\)POSTECH,

\({}^{14}\)Twente University, \({}^{15}\)OMELET, AI4CO\({}^{}\)

###### Abstract

Deep reinforcement learning (RL) has recently shown significant benefits in solving combinatorial optimization (CO) problems, reducing reliance on domain expertise, and improving computational efficiency. However, the field lacks a unified benchmark for easy development and standardized comparison of algorithms across diverse CO problems. To fill this gap, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 23 state-of-the-art methods and more than 20 CO problems. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configuration of diverse RL algorithms, neural network architectures, inference techniques, and environments. RL4CO allows researchers to seamlessly navigate existing successes and develop their unique designs, facilitating the entire research process by decoupling science from heavy engineering. We also provide extensive benchmark studies to inspire new insights and future work. RL4CO has attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.

## 1 Introduction

Combinatorial optimization (CO) focuses on finding optimal solutions for problems with discrete variables and has broad applications, including vehicle routing [89; 60], scheduling , and hardware device placement . Given that the combinatorial space expands exponentially and exhibits NP-hard characteristics, the operations research (OR) community has traditionally tackled these challenges through the development of mathematical programming algorithms  and handcrafted heuristics . Despite their success, these methods still face significant limitations: mathematical programming struggles with scaling, while handcrafted heuristics require significant domain-specific adjustments for different CO problems.

Recently, to address these limitations, neural combinatorial optimization (NCO)  has emerged. It employs deep neural networks to automate the problem-solving process and significantly reduces the computation demands and the need for domain expertise. Recent NCO works mainly leverage the reinforcement learning (RL) paradigm, making significant strides in improving exploration efficiency [62; 54], relaxing the needs of obtaining optimal solutions, and extending to various CO tasks [128; 89; 60; 53]. Although supervised learning (SL) methods  are shown to be effective in NCO, they require the availability of high-quality solutions, which is unrealistic for large instances or theoretically hard problems. Therefore, we focus on the widespread RL paradigm in this paper.

Despite the growing popularity and advancements in using reinforcement learning for solving combinatorial optimization, there remains a lack of a unified benchmark for analyzing past works under consistent implementations and conditions. The absence of a standardized benchmark hinders NCO researchers' efforts to make impactful advancements and leverage existing successes, as it becomes challenging to determine the superiority of one method over another. Moreover, the significance of NCO lies in its potential for generalizability across multiple problems without extensive problem-specific knowledge. Variations in implementation can make it difficult for new researchers to engage with the NCO community, and inconsistent comparisons obstruct straightforward performance evaluations. These issues pose significant challenges and underscore the need for a comprehensive benchmark to streamline research and foster consistent progress.

**Contributions.** To bridge this gap, we introduce RL4CO, the first comprehensive benchmark with multiple baselines, environments, and boilerplate from the literature, all implemented in a _modular_, _flexible_, _accelerated_, and _unified_ manner. Our aim is to facilitate the entire research process for the NCO community with the following key contributions: 1) **Simplifying development** through modularizing 27 environments and 23 existing baseline models, allowing for flexible and automated combinations for effortless testing, switching, and achieving state-of-the-art performance; 2) **Enhancing the training and testing efficiency** through the customized unified pipeline tailored for the NCO community based on advanced libraries such as TorchRL , PyTorch Lightning , Hydra , and TensorDict ; 3) **Standardizing evaluation** to ensure fair and comprehensive comparisons, enabling researchers to automatically test a broader range of problems from diverse distributions and gather valuable insights using our testbed. Overall, RL4CO eliminates the need for repetitive heavy engineering in the NCO community and fosters seamless future development by building on existing successes, enabling advanced innovation and progress in the field.

## 2 Related Works

**Neural Combinatorial Optimization.** Neural combinatorial optimization (NCO) utilizes machine learning techniques to automatically develop novel heuristics for solving NP-hard CO problems. We classify the majority of NCO research from the following perspectives: 1) _Learning Paragians_: researchers have employed supervised learning [115; 108; 29; 75] to approximate optimal solutions to CO instances. Further research leverages reinforcement learning [6; 89; 60; 62], and unsupervised learning [39; 84] to ease the difficulty of obtaining (near-)optimal solutions. 2) _Models_: various deep learning architectures such as recurrent neural networks [115; 22; 68], graph neural networks [48; 84], Transformers [60; 62], diffusion models , and GFlowNets [129; 56] have been employed. 3) _Problems_: NCO has demonstrated great success in various problems, including vehicle routing problems (VRPs) (e.g., traveling salesman problem and capacitated VRP), scheduling problems (e.g., job shop scheduling problems ), hardware device placement , and graph-based CO problems (e.g., maximum independent set [23; 2] and maximum cut ). 4) _Heuristic Types_: generally, the learned heuristics can be categorized as _constructive_ in an autoregressive  or non-autoregressive  way, and _improvement_ heuristics, which leverage traditional heuristics [120; 80] and meta-heuristics . We refer to Bengio et al.  for a comprehensive survey. In this paper, we focus on the reinforcement learning paradigm due to its effectiveness and flexibility. Notably, the proposed RL4CO is versatile to support most combinations of models, problems and heuristic types, making it an apt library and benchmark for future research in NCO.

Related Benchmark Libraries.Despite the variety of general-purpose RL software libraries [18; 70; 96; 119; 24; 33; 81], there is a lack of a unified and extensive benchmark for CO problems. Balaji et al.  propose an RL benchmark for Operations Research (OR) with a PPO baseline ; Hubbs et al. , Biagioni et al.  provide a collection of OR environments. Wan et al.  propose a general-purpose library for OR, and benchmarks the canonical TSP and CVRP environments. However, a major downside of the above libraries is that they cannot be massively parallelized due to their reliance on the OpenAI Gym API, which can only run on CPU, unlike RL4CO, which is based on the TorchRL , a recent official PyTorch  library for RL that enables hardware-accelerated execution of both environments and algorithms. Prouvost et al.  introduces a library specialized for CO problems that work in combination with traditional MILP  solvers. We also mention Routing Arena , whose scope is different from RL4CO, namely, comparing NCO and classical solvers only for the CVRP. The most related work is Jumanji , which provides a variety of CO environments written in JAX  that can be hardware-accelerated alongside an actor-critic baseline. While Jumanji is an RL environment suite, RL4CO is a full-stack library that integrates environments, policies, RL algorithms under a unified framework.

## 3 RL4CO: Taxonomy

We describe the RL4CO taxonomy, categorizing components into _Environments, Policies,_ and _RL Algorithms_. Then we translate the taxonomy to implementation in SS 4.

**Environments.** Given a CO problem instance \(\), we formulate the solution-generating procedure as a Markov Decision Process (MDP) characterized by a tuple \((,,,,)\) as follows. **State**\(\) is the space of states that represent the given problem \(\) and the current partial solution being updated in the MDP. **Action**\(\) is the action space, which includes all feasible actions \(a_{t}\) that can be taken at each step \(t\). **State Transition**\(\) is the deterministic state transition function \(s_{t+1}=(s_{t},a_{t})\) that updates a state \(s_{t}\) to the next state \(s_{t+1}\). **Reward**\(\) is the reward function \((s_{t},a_{t})\) representing the immediate reward received after taking action \(a_{t}\) in state \(s_{t}\). Finally, \(\) is a discount factor that determines the importance of future rewards. Since the state transition is deterministic, we represent the solution for a problem \(\) as a sequence of \(T\) actions \(=(a_{1},,a_{T})\). Then the total return \(_{t=1}^{T}(s_{t},a_{t})\) translates to the negative cost function of the CO problem.

**Policies.** The policies can be categorized into constructive policies, which generate a solution from scratch, and improvement policies, which refine an existing solution.

_Constructive policies._ A policy \(\) is used to construct a solution from scratch for a given problem instance \(\). It can be further categorized into autoregressive (AR) and non-autoregressive (NAR) policies. An AR policy is composed by an encoder \(f\) that maps the instance \(\) into an embedding space \(=f()\) and by a decoder \(g\) that iteratively determines a sequence of actions \(\) as follows:

\[a_{t} g(a_{t}|a_{t-1},...,a_{0},s_{t},),(|) _{t=1}^{T-1}g(a_{t}|a_{t-1},,a_{0},s_{t},).\] (1)

   Library & Environments & Baselines\({}^{}\) & Hardware & Availability & Modular & Open \\  & \# & \# & Acceleration & & Baselines & Community \\  ORL  & 3 & 1 & \(\) & \(\) & \(\) & \(\) \\ OR-Gym  & 9 & - & \(\) & ✓ & \(\) & \(\) \\ Graph-Env  & 2 & - & \(\) & ✓ & \(\) & \(\) \\ RLOR  & 2 & 2 & \(\) & ✓ & ✓ & \(\) \\ Routing Arena  & 1 & 8 & ✓ & \(\) & \(\) & \(\) \\ Jumanji  & 22 & 3 & ✓ & ✓ & \(\) & \(\) \\  RL4CO (ours) & 27\({}^{}\) & 23 & ✓ & ✓ & ✓ & ✓ \\   ^{}\) We consider as _baselines_ ad-hoc network architectures (i.e., policies) and RL algorithms from the literature.} \\ ^{}\) We also consider the possible 16 combinations of environments generated by the unified Multi-Task VRP, as they have been historically considered separate environments in the NCO literature.} \\ 

Table 1: Comparison of libraries in reinforcement learning for combinatorial optimization.

A NAR policy encodes a problem \(\) into a heuristic \(=f()_{+}^{N}\), where \(N\) is the number of possible assignments across all decision variables. Each number in \(\) represents a (unnormalized) probability of a particular assignment. To obtain a solution \(\) from \(\), one can sample a sequence of assignments from \(\) while dynamically masking infeasible assignments to meet problem-specific constraints. It can also guide a search process, e.g., Ant Colony Optimization , or be incorporated into hybrid frameworks . Here, the heuristic helps identify promising transitions and improve the efficiency of finding an optimal or near-optimal solution.

_Improvement policies._ A policy can be used for improving an initial solution \(^{0}=(a_{0}^{0},,a_{T-1}^{0})\) into another one potentially with higher quality, which can be formulated as follows:

\[^{k} g(^{0},),(^{K}|^{0},) _{k=1}^{K-1}g(^{k}|^{k-1},...,^{0},),\] (2)

where \(^{k}\) is the \(k\)-th updated solution and \(K\) is the budget for number of improvements. This process allows continuous refinement for a long time to enhance the solution quality.

**RL Algorithms.** The RL objective is to learn a policy \(\) that maximizes the expected cumulative reward (or equivalently minimizes the cost) over the distribution of problem instances:

\[^{*}=}\,_{ P( )}[_{(|)}[_{t=0}^{T-1}^{t} (s_{t},a_{t})]],\] (3)

where \(\) is the set of parameters of \(\) and \(P()\) is the distribution of problem instances. Eq. (3) can be solved using algorithms such as variations of REINFORCE , Advantage Actor-Critic (A2C) methods , or Proximal Policy Optimization (PPO) . These algorithms are employed to train the policy network \(\), by transforming the maximization problem in Eq. (3) into a minimization problem involving a loss function, which is then optimized using gradient descent algorithms. For instance, the REINFORCE loss function gradient is given by:

\[_{}_{a}(|)=_{(|)} [(R(,)-b())_{}(|)],\] (4)

where \(b()\) is a baseline function used to stabilize training and reduce gradient variance. We also distinguish between two types of RL (pre)training: 1) _inductive_ and 2) _transductive_ RL. In inductive RL, the focus is on learning patterns from the training dataset to generalize to new instances, thus amortizing the inference procedure. Conversely, transductive RL (or test-time optimization) optimizes parameters during testing on target instances. Typically, a policy \(\) is trained using inductive RL, followed by transductive RL for test-time optimization.

## 4 RL4CO: Library Structure

RL4CO is a unified reinforcement learning (RL) for Combinatorial Optimization (CO) library that aims to provide a _modular_, _flexible_, and _unified_ code base for training and evaluating RL for CO methods with extensive benchmarking capabilities on various settings. As shown in Fig. 2, RL4CO decouples the major components of an RL pipeline, prioritizing their reusability in the implementation. Following also the taxonomy of SS 3, the main components are: (SS 4.1) Environments, (SS 4.2) Policies, (SS 4.3) RL algorithms, (SS 4.4) Utilities, and (SS 4.5) Environments & Baselines Zoo.

Figure 1: Overview of different types of policies and their modularization in RL4CO.

### Environments

Environments in RL4CO fully specify the CO problems and their logic. They are based on the RL4COEnvBase class that extends from the EnvBase in TorchRL . A modular generator can be provided to the environment. The generator provides CO instances to the environment, and different generators can be used to generate different data distributions. Static instance data and dynamic variables, such as the current state \(s_{t}\), current solution \(^{k}\) for improvement environments, policy actions \(a_{t}\), rewards, and additional information are passed in a _stateless_ fashion in a TensorDict , that we call td, through the environment reset and step functions. Additionally, our environment API contains several functions, such as render, check_solution_validity, select_start_nodes (i.e., for POMO-based optimization ) and optional API as local_search solution improvement.

It is noteworthy that RL4CO enhances the efficiency of environments when compared to vanilla TorchRL, by overriding and optimizing some methods in TorchRL EnvBase. For instance, our new step method brings a decrease of up to 50% in latency and halves the memory impact by avoiding saving duplicate components in the stateless TensorDict.

### Policies

Policies in RL4CO are subclasses of PyTorch's nn.Module and contain the encoding-decoding logic and neural network parameters \(\). Different policies in the RL4CO "zoo" can inherit from metaclasses like ConstructivePolicy or ImprovementPolicy. We modularize components to process raw features into the embedding space via a parametrized function \(_{}\), called _feature embeddings_. 1) _Node Embeddings_\(_{n}\): transform \(m_{n}\) node features of instances \(\) from the feature space to the embedding space \(h\), i.e., \([B,N,m_{n}][B,N,h]\). 2) _Edge Embeddings_\(_{e}\): transform \(m_{e}\) edge features of instances \(\) from the feature space to the embedding space \(h\), i.e., \([B,E,m_{e}][B,E,h]\), where \(E\) is the number of edges. 3) _Context Embeddings_\(_{c}\): capture contextual information by transforming \(m_{c}\) context features from the current decoding step \(s_{t}\) from the feature space to the embedding space \(h\), i.e., \([B,m_{c}][B,h]\), for nodes or edges. Overall, Fig. 3 illustrates a generic constructive AR policy in RL4CO, where the feature embeddings are applied similarly to other types of policies. Embeddings can be automatically selected by RL4CO at runtime by simply passing the env_name to the policy. Additionally, we allow for granular control of any higher-level policy component independently, such as encoders and decoders.

### RL Algorithms

RL algorithms in RL4CO define the process that takes the Environment with its problem instances and the Policy to optimize its parameters \(\). The parent class of algorithms is the RL4COLitModule, inheriting from PyTorch Lightning's pl.LightningModule . This allows for granular support of various methods including the [train, val, test]_step, automatic logging with several logging services such as Wandb via log_metrics, automatic optimizer configuration via configure_optimizers and several useful callbacks for RL methods such as on_train_epoch_end. RL algorithms are additionally attached to an RL4COTrainer, a wrapper we made with additional optimizations around pl.Trainer. This module seamlessly supports features of modern training pipelines, including logging, checkpoint management, mixed-precision training, various hardware acceleration supports (e.g., CPU, GPU, TPU, and Apple Silicon), and multi-device hardware accelerator in distributed settings . For instance, using mixed-precision

Figure 2: Overview of the RL4CO pipeline: from configurations to training a policy.

training significantly decreases training time without sacrificing much convergence and enables us to leverage recent routines, e.g., FlashAttention [26; 25], which we investigate in Appendix.

### Utilities

**Configuration Management.** Optionally, but usefully, we adopt Hydra, an open-source Python framework that enables hierarchical config management, making it easier to manage complex configurations and experiments with different settings as shown in Appendix. Hydra additionally allows for automatically parsing parameters (un-)defined in configs - i.e., python run.py experiment=routing/pomo env=cvrp env.generator_params.num_loc=50 launches an experiment defined under routing/pomo and changes the environment to CVRP with 50 locations.

**Decoding Schemes.** Decoding schemes handle the logic of model logits \(z\) by applying preprocessing, such as masking of infeasible actions and/or additional techniques to select better actions during training and testing. We implement the model and problem-agnostic decoding schemes under the DecodingStrategy class in the RL4CO codebase that can be easily reused: 1) _Greedy_, which selects the action with the highest probability; 2) _Sampling_, which samples n_samples solutions from the current masked probability distribution of the policy, incorporating sampling strategies like 2.a) Softmax Temperature \(\), 2.b) top-k sampling , and 2.c) top-p (or Nucleus) sampling  (more details in Appendix); 3) _Multistart_, which enforces diverse starting actions as demonstrated in POMO , such as starting from different cities in the Traveling Salesman Problem (TSP) with N nodes; 4) _Augmentation_, which applies transformations to instances, such as random rotations and flipping in Euclidean problems , to create an augmented set of problems.

**Documentation, Tutorials, and Testing.** We release extensive documentation to make it as accessible as possible for both newcomers and experts. RL4CO can be easily installed by running pip install rl4co with open-source code available at https://github.com/ai4co/rl4co. Several tutorials and examples are also available under the examples/ folder. We thoroughly test our library via continuous integration on multiple Python versions and operating systems. The following code snippet shows minimalistic code that can train a model in a few lines:

``` fromrl4co.envs.routingimportTSPEnv,TSPGenerator fromrl4co.modelsimportAttentionModelPolicy,POMO fromrl4co.utilsimportRL4COTrainer #Instantiategeneratorandenvironment generator=TSPGenerator(num_loc=50,loc_distribution="uniform") env=TSPEnv(generator) #CreatepolicyandRLmodel policy=AttentionModelPolicy(env_name=env.name,num_encoder_layers=6) model=POMO(env,policy,batch_size=64) #InstantiateTrainerandfit trainer=RL4COTrainer(max_epochs=10,accelerator="gpu",precision="16-mixed") trainer.fit(model) ```

Figure 3: Overview of modularized RL4CO policies. Any component such as the encoder/decoder structure and feature embeddings can be replaced and thus the model is adaptable to various new environments.

### Environments & Baselines Zoo

**Environments.** We include benchmarking from the following environments, divided into four areas. 1) **Routing**: Traveling Salesman Problem (TSP) , Capacitated Vehicle Routing Problem (CVRP) , Orienteering Problem (OP) [64; 21], Prize Collecting TSP (PCTSP) , Pickup and Delivery Problem (PDP) [50; 99] and Multi-Task VRP (MTVRP) [72; 131; 9] (which modularizes with 16 problem variants including the basic VRPTW, OVRP, VRPB, VRPL and VRPs with their constraint combinations); 2) **Scheduling**: Flexible Job Shop Scheduling Problem (FJSSP) , Job Shop Scheduling Problem (JSSP)  and Flow Shop Scheduling Problem (FJSP); 3) **Electronic Design Automation**: multiple Deep Placement Problem (mDPPP) ; 4) **Graph**: Facility Location Problem (FLP)  and Max Cover Problem (MCP) .

**Baseline Zoo.** Given that several works contribute to both new policies and new RL algorithm variations, we list the papers we reproduce. For 1) **Constructive AR** methods, we include the Attention Model (AM) , Ptr-Net , POMO , MatNet , HAM , SymNCO , PolyNet , MTPOMO , MVMoE , L2D , HGNN  and DevFormer . For 2) **Constructive NAR** methods, we benchmark Ant Colony Optimization-based DeepACO  and GFACS  as well as the hybrid NAR/AR GLOP . 3) **Improvement methods** include DACT , N2S  and NeuOpt . We also include 4) **General-purpose RL** algorithm from the literature, including REINFORCE  with various baselines, Advantage Actor-Critic (A2C)  and Proximal Policy Optimization (PPO)  that can be readily be combined with any policy. Finally, we include 5) **Active search** (i.e., Transductive RL) methods AS  and EAS .

## 5 Benchmarking Study

We perform several benchmarking studies with our unified RL4CO library. Given the limited space, we invite the reader to check out the Appendix for supplementary material.

### Flexibility and Modularity

**Changing policy components.** The integration of many state-of-the-art methods in RL4CO from the NCO field in a modular framework makes it easy to implement and improve upon state-of-the-art neural solvers for complex CO problems with only a few lines of code and improve upon them.2 We demonstrate this in Table 2 for the FJSSP by gradually replacing or adding elements to the original SotA policy . First, replacing the HGNN encoder with the more expressive MatNet encoder  already improves the average makespan by around 7%. Further improvements can be achieved by replacing the MLP decoder with the Pointer mechanism in the AM decoder  with gaps to BKS around \(3\) lower compared to the original policy in Song et al.  even with greedy performance.

### Constructive Policies

**Mind Your Baseline.** In on-policy RL, which is often employed in RL4CO due to fast reward function evaluations, several different REINFORCE baselines have been proposed to improve the performance. We benchmark several RL algorithms training constructive policies for routing problems of node size 50, whose underlying architecture is based on the encoder-decoder Attention Model  and whose main difference lies in how the REINFORCE baseline is calculated (we additionally train the AM with PPO as further reference). For a fair comparison, we run all baselines

   &  \\  & & \(10 5\) & \(20 5\) \\  HGNN + MLP (g.)  & Obj. & 111.82 & 211.21 \\  & Gap & 15.8\% & 12.16 \\  MatNet + MLP (g.) & Obj. & 103.91 & 197.92 \\  & Gap & 7.6\% & 5.0\% \\  MatNet + Pointer (g.) & Obj. & 101.17 & 196.3 \\  & Gap & 4.8\% & 4.2\% \\  MatNet + Pointer (s. x128) & Obj. & 98.31 & 192.02 \\  & Gap & 1.8\% & 1.9\% \\  

Table 2: Solutions obtained with RL4CO for the FJSSP with different model configurations.

in controlled settings with the same number of optimization steps and report results in Table 3. We note that A2C generally underperforms other baselines. Such performance can be attributed to the fact that since in routing problems, the rewards are sparse (i.e., can only be calculated upon solving an entire problem), estimating the value of an entire instance \(\) is inherently a challenging task. Interestingly, while POMO , which takes as a baseline the shared baseline of all routes forcing each starting node to be different, may work well as baselines for problems in which near-optimal solutions can be constructed from any node (e.g., TSP), this may not be true for other problems such as the Orienteering Problem (OP): the reason is that in OP only a _subset_ of nodes should be selected in an optimal solution, while several states will be discarded. Hence, forcing the policy to select all of them makes up for a poor baseline. We remark that while SymNCO (whose shared baseline involves symmetric rotations and flips)  may perform well in Euclidean problems, this is not applicable in non-Euclidean CO, including asymmetric routing problems and scheduling. We found similar trends regarding actor-critic methods as A2C and PPO in the EDA mDPP problem , which we report in Appendix. Namely, a greedy rollout baseline  can do better than value-based methods due to the challenging task of instance value estimation.

**Decoding Schemes.** The solution quality of NCO solvers often shows significant improvements in performance to different decoding schemes, even with the exact NCO solvers. We evaluate the trained solver with different decoding schemes and settings as shown in Fig. 4.

**Generalization.** Using RL4CO, we can easily evaluate the generalization performance of existing baselines by employing supported environments that incorporate various VRP variant tasks and instance distributions (termed MTPOMO and MDPOMO, respectively). Empirical results on CVRPLib, shown in Table 4, reveal that training on different tasks significantly enhances generalization performance. This finding underscores the necessity of building foundational models across diverse CO domains.

**Large-Scale Instances.** We evaluate large-scale CVRP instances of thousands of nodes, with more visualizations and scaling in Appendix. The last row of Table 5 illustrates the performance of the hybrid NAR/AR GLOP , while others refer to reproduced results from Ye et al. . Our implementation in RL4CO improves the performance in not only speed but also solution quality.

### Combining Construction and Improvement: Best of Both Worlds?

While constructive policies can build solutions in seconds, their performance is often limited, even with advanced decoding schemes such as sampling or augmentations. On the other hand, improvement methods are more suitable for larger computing budgets. We benchmark models on TSP with 50 nodes: the AR constructive method POMO  and the improvement methods DACT  and NeuOpt . In the original implementation, DACT and NeuOpt started from a solution constructed randomly. To further demonstrate the flexibility of RL4CO, we show that bootstrapping improvement methods with constructive ones enhance convergence speed. Fig. 5 shows that bootstrapping with a pre-trained POMO policy significantly enhances the convergence speed. To further investigate the performance, we report the Primal Integral (PI) , which evaluates the evolution of solution quality over time. Improvement methods alone, such as DACT and NeuOpt, achieve \(2.99\) and \(2.26\) respectively, while sampling from POMO achieves \(0.08\). This shows that the "area under the curve" can be better even if the final solution is worse for constructive methods. Bootstrapping

   Method & TSP & CVRP & OP & PCTSP & PDP \\  A2C & 2.22 & 7.09 & 8.64 & 14.96 & 10.02 \\ AM-Rollout & 1.41 & 5.30 & 4.40 & 2.46 & 9.88 \\ POMO & 0.89 & 3.99 & 14.26 & 11.61 & 10.64 \\ Sym-NCO & 0.47 & 4.61 & 3.09 & 2.12 & 7.73 \\ AM-PPO & 0.92 & 4.60 & 3.05 & 2.45 & 8.31 \\   

Table 3: Optimality Gap obtained via greedy decoding.

Figure 4: Decoding schemes study of POMO on CVRP50. [Left]: Pareto front of decoding schemes by the number of samples; [Right]: performance of sampling with different temperatures \(\) and \(p\) values for top-\(p\) sampling.

with POMO then improves DACT and NeuOpt to \(0.08\) and \(0.04\) respectively, showing the benefits of modularity and hybridization of different components.

## 6 Discussion

### Limitations and Future Directions

While RL4CO is an efficient and modular library specialized in CO problems, it might not be suitable for any other task due to a number of area-specific optimizations, and we do not expect it to seamlessly integrate with, for instance, OpenAI Gym wrappers without some modifications. Another limitation of the library is its scope so far, namely RL. In fact, extending the library to support supervised methods and creating a comprehensive "AI4CO" library could benefit the whole NCO community. We additionally identify in Foundation Models3 for CO and related scalable architectures a promising area of future research to overcome generalization issues across tasks and distributions, for which we provided some early clues.

### Long-term Plans

Our long-term plan is to become the go-to RL for CO benchmark library. While not strictly tied to implementation and benchmarking, we are committed to helping resolve issues and questions from the community. For this purpose, we created a Slack workspace (link available in the online documentation) that by now has attracted more than 130 researchers. It is our hope that our work will ultimately benefit the NCO field with new ideas and collaborations.

## 7 Conclusion

This paper introduces RL4CO, a modular, flexible, and unified Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. We provide a comprehensive taxonomy from environments to policies and RL algorithms that translate from theory to practice to software level. Our benchmark library aims to fill the gap in unifying implementations in RL for CO by utilizing several best practices with the goal of providing researchers and practitioners with a flexible starting point for NCO research. We provide several experimental results with insights and discussions that can help identify promising research directions. We hope that our open-source library will provide a solid starting point for NCO researchers to explore new avenues and drive advancements. We warmly welcome researchers and practitioners to actively participate and contribute to RL4CO.

    &  &  &  \\  & Obj. & Gap & Obj. & Gap & Obj. & Gap \\  Set A & 1075 & 3.13\% & 1076 & 3.20\% & **1074** & **2.97\%** \\ Set B & 996 & 3.41\% & 1003 & 4.06\% & **995** & **3.26\%** \\ Set E & 761 & 5.04\% & **760** & **4.82\%** & 762 & 5.07\% \\ Set F & 813 & 13.52\% & **798** & **12.09\%** & 825 & 13.66\% \\ Set M & 1259 & 16.37\% & **1234** & **13.58\%** & 1263 & 16.03\% \\ Set P & 620 & 6.72\% & **608** & **3.72\%** & 613 & 5.04\% \\ Set X & 73953 & 16.80\% & **73763** & **16.69\%** & 81848 & 23.69\% \\   

Table 4: Results on CVRPLIB instances with models trained on \(N=50\). Greedy multi-start decoding is used.

Figure 5: Bootstrapping improvement with constructive methods.

    &  &  &  \\  & Obj. & Time (s) & Obj. & Time (s) & Obj. & Time (s) \\  LKH-3 & 46.4 & 6.2 & 64.9 & 20 & 245.0 & 501 \\  AM & 61.4 & 0.6 & 114.4 & 1.9 & 354.3 & 26 \\ TAM(AM) & 50.1 & 0.8 & 74.3 & 2.2 & 233.4 & 26 \\ TAM(LKH-3) & 46.3 & 1.8 & 64.8 & 5.6 & 196.9 & 33 \\ GLOP-G(AM)* & 47.1 & 0.4 & 63.5 & 1.2 & 191.7 & 2.4 \\ GLOP-G(LKH-3)* & 45.9 & 1.1 & 63.0 & 1.5 & 191.2 & 5.8 \\  GLOP-G(AM) & 46.9 & **0.3** & 64.7 & **0.7** & 190.9 & **2.0** \\ GLOP-G(LKH-3) & **45.5** & 0.5 & **62.8** & 0.8 & **190.1** & 3.9 \\   

Table 5: Performance on large-scale CVRP instances.

## Potential Broader Impact

This paper presents work in the field of AI4CO. The main consequene may be that AI methods to solve CO problems may become accessible to the broad public, as our library is open source and readily available on GitHub. We do not see potential negative societal consequences as of today.

## Funding

This work was supported by a grant of the KAIST-KT joint research project through AI2XL Laboratory, Institute of Convergence Technology, funded by KT [Project No. G01210696, Development of Multi-Agent Reinforcement Learning Algorithm for Efficient Operation of Complex Distributed Systems] and by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korean government(MSIT)[2022-0-01032, Development of Collective Collaboration Intelligence Framework for Internet of Autonomous Things].