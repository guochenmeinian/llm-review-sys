# Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents

Wonje Choi, Woo Kyung Kim, SeungHyun Kim, Honguk Woo

Department of Computer Science and Engineering

Sungkyunkwan University

{wjchoi1995, kwk2696, kimsh571, hwoo}@skku.edu

Honguk Woo is the corresponding author.

###### Abstract

For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factor that significantly affects the agent's egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that ConPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.

## 1 Introduction

In the literature of vision-based reinforcement learning (RL), with the advance of unsupervised techniques and large-scale pretrained models for computer vision, the decoupled structure, in which visual encoders are separately trained and used later for policy learning, has gained popularity . This decoupling demonstrates high efficiency in low data regimes with sparse reward signals, compared to end-to-end RL. In this regard, several works on adopting the decoupled structure to embodied agents interacting with the environment were introduced , and specifically, pretrained vision models (e.g., ResNet in ) or vision-language models (e.g., CLIP in ) were exploited for visual state representation encoders. Yet, it is non-trivial to achieve zero-shot adaptation to visual domain changes in the environment with high diversity and non-stationarity, which are inherent for embodied agents. It was rarely investigated how to optimize those popular large-scale pretrained models to ensure the zero-shot capability of embodied agents.

Embodied agents have several environmental and physical properties, such as egocentric camera position, stride length, and illumination, which are _domain factors_ making significant changes in agents' perception and observation. In the target (deployment) environment with uncalibrated settings on those domain factors, RL policies relying on pretrained visual encoders remain vulnerable to domain changes.

Figure 1 provides an example of egocentric visual domain changes experienced by embodied agents due to different camera positions. When policies learned in the source environment are applied to the target environment, zero-shot performance can be significantly degraded, unless the visual encoder could adapt not only to environmental differences but also to the physical diversity of agents. In this paper, we investigate RL policy adaptation techniques for embodied agents to enable zero-shot adaptation to domain changes, by leveraging prompt-based learning for pretrained models in the decoupled RL structure. To this end, we present ConPE, a novel contrastive prompt ensemble framework that uses the CLIP vision-language model as the visual encoder, and facilitates dynamic adjustments of visual state representations against domain changes through an ensemble of contrastively learned visual prompts. In ConPE, the ensemble employs attention-based state composition on multiple visual embeddings from the same input observation, where each embedding corresponds to a state representation individually prompted for a specific domain factor. Specifically, the cosine similarity between an input observation and its respective prompted embeddings is used to calculate attention weights effectively.

Through experiments, we demonstrate the benefits of our approach. First, RL policies learned via ConPE achieve competitive zero-shot performance upon a wide variety of egocentric visual domain variations for several embodied agent tasks, such as navigation tasks in AI2THOR , vision-based robot manipulation tasks in egocentric-Metaworld, and autonomous driving tasks in CARLA . For instance, the policy via ConPE outperforms EmbCLIP  in zero-shot performance by 20.7% for unseen target domains in the AI2THOR object navigation. Second, our approach achieves high sample-efficiency in the decoupled RL structure. For instance, ConPE requires less than 50.0% and 16.7% of the samples compared to ATC  and 60% and 50% of the samples compared to EmbCLIP to achieve comparable performance in seen and unseen target domains in the AI2THOR object navigation.

In the context of RL, our work is the first to explore policy adaptation using visual prompts for embodied agents, achieving superior zero-shot performance and high sample-efficiency. The main contributions of our work are as follows.

* We present a novel ConPE framework with an ensemble of visual contrastive prompts, which enables zero-shot adaptation for vision-based embodied RL agents.
* We devise visual prompt-based contrastive learning and guided-attention-based prompt ensemble algorithms to represent task-specific information in the CLIP embedding space.
* We experimentally show that policies via ConPE achieve comparable or superior zero-shot performance, compared to other state-of-the-art baselines, for several tasks. We also demonstrate high sample-efficiency in policy learning and adaptation.
* We create the datasets with various visual domains in AI2THOR, egocentric-Metaworld and CARLA, and make them publicly accessible for further research on RL policy adaptation.

## 2 Problem Formulation

In RL formulation, a learning environment is defined as a Markov decision process (MDP) of \((S,A,,R)\) with state space \(s S\), action space \(a A\), transition probability \(:S A S\) and reward function \(R:S A\). The objective of RL is to find an optimal policy \(^{*}:S A\) maximizing the sum of discounted rewards. For embodied agents, states might not be fully observable, and the environment is represented by a partially observable MDP (POMDP) of a tuple \((S,A,,R,,)\) with an observation space \(o\) and a conditional observation probability \(:S A\).

Given visual domains in the dynamic environment, we consider policy adaptation to find the optimal policy that remains invariant across the domains or is transferable to some target domain, where each domain is represented by a POMDP and domain changes are formulated by different \(\). We denote domains as \(D=(,)\). Aiming to enable zero-shot adaptation to various domains, we formulate

Figure 1: Visual Domain Changes of Embodied Agents

the policy adaptation problem as finding the optimal policy \(^{*}\) such that

\[^{*}=*{argmax}_{}[*{}_{D p(D)} [_{t=1}^{}^{t}R(s_{t},(o_{t}))]]\] (1)

where \(p(D)\) is a given domain distribution and \(\) is a discount factor of the environment.

For embodied agents, the same state can be differently observed depending on the configuration of properties such as egocentric camera position, stride length, illumination, and object style. We refer to such a property causing domain changes in the environment as a _domain factor_. Practical scenarios often involve the interplay of multiple domain factors in the environment.

## 3 Our Approach

### Framework Structure

To enable zero-shot policy adaptation to unseen domains, we develop the ConPE framework consisting of (i) prompt-based contrastive learning with the CLIP visual encoder, (ii) guided-attention-based prompt ensemble, and (iii) zero-shot policy deployment, as illustrated in Figure 2. The capability of the CLIP visual encoder is enhanced using multiple visual prompts that are contrastively learned on expert demonstrations for several domain factors. This establishes the visual prompt pool in (i). Then, the prompts are used to train the guided-attention-based ensemble with the environment in (ii). To enhance learning efficiency and interpretability of attention weights, we use the cosine similarity of embeddings. The attention module and policy are jointly learned for a specific task so that resulting state representations tend to generalize across various domains and be optimized for task learning. In deployment, a non-stationary environment where its visual domain varies according to the environment conditions and agent physical properties is considered, and the zero-shot performance is evaluated in (iii).

### Prompt-based Contrastive Learning

To construct domain-invariant representations with respect to a specific domain factor for egocentric perception data, we adopt several contrastive tasks for visual prompt learning, which can be learned on a few expert demonstrations. For this, we use a visual prompt

\[p^{v}=[e_{1}^{v},e_{2}^{v},...,e_{u}^{v}],\ e_{i}^{v}^{d}\] (2)

where \(e_{i}^{v}\) is a continuous learnable vector with the image patch embedding dimension \(d\) (e.g., 768 for CLIP visual encoder) and \(u\) is the length of a visual prompt. Let a pretrained model \(_{}\) parameterized by \(\) maps observations \(o\) to the embedding space \(\). With a contrast function \(P:\{0,1\}\) to discriminate whether an observation pair is positive or not,

Figure 2: ConPE Framework. The CLIP visual encoder is enhanced offline via (i) prompt-based contrastive learning that generates the visual prompt pool, and a policy is learned online by (ii) guided-attention-based prompt ensemble that uses the prompt pool. In (iii) zero-shot deployment, the policy is immediately evaluated upon domain changes.

consider an \(m\)-sized batch of observation pairs \(_{P}=\{(o_{i},o^{}_{i})\}_{i m}\) containing one positive pair \(\{(o_{k},o^{}_{k})|P(o_{k},o^{}_{k})=1\}\) for some \(k m\). Then, we enhance the capability of \(_{}\) by learning a visual prompt \(p^{v}\) through contrastive learning, where the contrastive loss function  is defined as

\[_{}(p^{v},_{P})=-(_{}(o_{k},p^{v}),_{}(o^{}_{k},p^{v}))}{_{i k} S(_{}(o_{i},p^{v}),_{}(o^{}_{i},p^{v}))} ),\;S(x,y)=().\] (3)

As in , for latent vectors \(x,\;y\), their similarity in the embedding space \(\) is calculated by \(S(x,y)\), where \(\) is a hyperparameter. By conducting the prompt-based contrastive learning on \(n\) different domain factors, we obtain a visual prompt pool

\[^{v}=[p_{1}^{v},p_{2}^{v},...,p_{n}^{v}].\] (4)

Through this process, each visual prompt in \(^{v}\) encapsulates domain-invariant knowledge pertinent to its respective domain factor.

### Guided-Attention-based Prompt Ensemble

To effectively integrate individual prompted embeddings from multiple visual prompts into a task-specific state representation, we devise a guided-attention-based prompt ensemble structure, as shown in Figure 3 where the attention weights on the embeddings are dynamically computed via the attention module \(\) for each observation.

Given observation \(o\) and the learned visual prompt pool \(^{v}\), an image embedding \(z_{0}=_{}(o)\) and prompted embeddings \(=[z_{1}=_{}(o,p_{1}^{v}),...,z_{n}]\) are calculated. Then, \(z_{o}\) and \(\) are fed to the attention module \(\), where attention weights \(_{i}\) for each prompted embedding \(z_{i}\) are optimized. Since directly computing the attention weights using \(z_{0}\) and \(\) is prone to have an uninterpretable local optima, we introduce a guidance score \(g_{i}\) based on the cosine similarity between the input image and visual prompted image embeddings in \(\), i.e., \(g_{i}=,z_{i}}{\|z_{0}\|\|z_{i}\|}\). Given that larger \(g_{i}\) signifies a stronger conformity of an observation to the domain factor relevant to the prompted embedding \(z_{i}\), we use \(g_{i}\) to steer the attention weights, aiming to not only improve learning efficiency but also provide interpretability. With guidance \(g_{i}\), we compute the attention weights \(_{i}\) by

\[_{i}=(u_{i}/)}{_{k}(u_{k}/)},\;\; \;u_{i}=,k_{i}}{}g_{i}\] (5)

Figure 3: Guided-Attention-based Prompt Ensemble. The cosine similarity-guided attention module \(\) yields task-specific state representations from multiple prompted embeddings and is learned with a policy network \(\).

where \(k_{i}\) is the projection of \(z_{i}\), \(d\) is dimension of \(z\), and \(\) is a softmax temperature. Then, state embedding \(Z\) is obtained by

\[Z=(z_{0},)=z_{0}+_{i=1}^{n}_{i}z_{i}.\] (6)

Algorithm 1 shows the procedures in ConPE, where the first half corresponds to prompt-based contrastive learning (in Section 3.2) and the other half corresponds to joint learning of a policy \((Z)\) and the attention module \(\). As \(\) is optimized by a given RL task objective in the source domains (in line 12), the resulting \(Z\) tends to be task-specific, while \(Z\) is also domain-invariant by the ensemble of contrastively learned visual prompts based on \(\) with respect to the combinations of multiple domain factors. The entire algorithm can be found in Appendix.

```
1:Dataset \(=\{(o_{1},o^{}_{1}),...\}\), replay buffer \(Z_{D}\), pretrained vision-language model \(_{}\)
2:Visual prompt pool \(^{v}=[p_{1}^{v},...,p_{n}^{v}]\), attention module \(\), policy \(\)
3:\(\)* Prompt-based Contrastive Learning */
4:for\(i=1,...,n\)do
5:while not converge do
6: Sample a batch \(_{P_{i}}=\{(o_{j},o^{}_{j})\}_{j m}\)
7: Update prompt \(p_{i}^{v} p_{i}^{v}-_{}(p_{i}^{v}, _{P_{i}})\) using (3)
8:endwhile
9:endfor
10:\(\)* Prompt Ensemble-based Policy Learning */
11:for each environment step do
12: Sample action \(a=((_{}(o),))\) using (5), (6)
13:\(Z_{D} Z_{D}\{(,a,r)\}\)
14: Jointly optimize policy \(\) and module \(\) on \(\{(_{j},a_{j},r_{j})\}_{j m} Z_{D}\)
15:endfor ```

**Algorithm 1** Procedure of ConPE Framework

## 4 Evaluation

**Experiments.** We use AI2THOR , Metaworld , and CARLA  environments, specifically configured for embodied agent tasks with dynamic domain changes. These environments allow us to explore various _domain factors_ such as camera settings, stride length, rotation degree, gravity, illuminations, wind speeds, and others. For prompt-based contrastive learning (in Section 3.2), we use a small dataset of expert demonstrations for each domain factor (i.e., 10 episodes per domain factor). For prompt ensemble-based policy learning (in Section 3.3), we use a few source domains randomly generated through combinatorial variations of the seen domain factors (i.e., 4 source domains). In our zero-shot evaluations, we use target domains that can be categorized as either seen or unseen. The seen target domains are those encountered during the phase of prompt-based contrastive learning, while these domains are not present during the phase of prompt ensemble-based policy learning. On the other hand, the unseen target domains refer to those that are entirely new, implying that they are not encountered during either learning phases.

**Baselines.** We implement several baselines for comparison. LUSR  is a reconstruction-based domain adaptation method in RL, which uses the variational autoencoder structure for robust representations. CURL  and ACT  employ contrastive learning in RL frameworks for high sample-efficiency and generalization to visual domains. ACO  utilizes augmentation-driven and behavior-driven contrastive tasks in the context of RL. EmbCLIP  is a state-of-the-art embodied AI model, which exploits the pretrained CLIP visual encoder for visual state representations.

**Implementation.** We implement ConPE using the CLIP model with ViT-B/32, similar to VPT  and CoOp . In prompt-based contrastive learning, we adopt various contrastive learning schemes including augmentation-driven [2; 1; 19] and behavior-driven [12; 20; 21; 22] contrastive learning, where the prompt length sets to be 8. In policy learning, we exploit online learning (i.e., PPO ) for AI2THOR and imitation learning (i.e., DAGGER ) for egocentric-Metaworld and CARLA.

### Zero-shot Performance

Table 1 shows zero-shot performance of ConPE and the baselines across source, seen and unseen target domains. We evaluate with 3 different seeds and report the average performance (i.e., task success rate in AI2THOR and egocentric-Metaworld, the sum of rewards in CARLA). As shown in Table 1(a), ConPE outperforms the baselines in the AI2THOR tasks. It particularly surpasses the most competitive baseline, EmbCLIP, by achieving \(5.2 5.7\%\) higher success rate for seen target domains, and \(6.9\,{}\,20.7\%\) for unseen target domains. For egocentric-Metaworld, as shown in Table 1(b), ConPE demonstrates superior performance with a significant success rate for both seen and unseen target domains, which is \(17.3\,{}\,24.0\%\) and \(18.0\,{}\,20.0\%\) higher than EmbCLIP, respectively. For autonomous driving in CARLA, we take into account external environment factors, such as weather conditions and times of day, as domain factors that can influence the driving task. In Table 1(c), ConPE consistently maintains competitive zero-shot performance across all conditions, outperforming the baselines.

In these experiments, LUSR shows relatively low success rates, as the reconstruction-based representation model can abate some task-specific information from observations, which is critical to conduct vision-based complex RL tasks. EmbCLIP shows the most comparative performance among the baselines, but its zero-shot performance for target domains is not comparable to ConPE. In contrast,

Table 1: Zero-shot Performance. The policies of each method (ConPE and the baselines) are learned on 4 source domains. The _Source_ column presents the performance for those source domains. In all evaluations, we use 30 seen target domains and 10 unseen target domains. The _Seen Target_ column presents the performance for the seen target domains, and the _Unseen Target_ column presents the performance for the unseen target domains. The unseen target domains are not used for representation learning.

ConPE effectively estimates the domain shifts pertaining to each domain factor through the use of guided attention weights, leading to robust performance in both seen and unseen target domains.

**Sample Efficiency.** Figure 4 presents performance with respect to samples (timesteps) that are used by ConPE and baselines for policy learning. Compared to the most competitive baseline EmbCLIP, ConPE requires less than \(60.0\%\) timesteps (online samples) for seen target domains and \(50.0\%\) for unseen target domains to have comparable success rates.

**Prompt Ensemble Interpretability.** Figure 5 visualizes the prompted embeddings using the prompt pool obtained through ConPE. For _intra_ prompted embeddings, we use observation pairs, where each pair is generated by varying domains within a domain factor. We observe that the embeddings are paired to form the domain-invariant knowledge because the visual prompt is learned through prompt-based contrastive learning. The _inter_ prompted embeddings specify that each prompt distinctly clusters prompted embeddings that correspond to the domains associated to its domain factor. Figure 5 shows examples of the attention weight matrix of ConPE for four different domains. The x-axis denotes the visual prompts and the y-axis denotes timesteps. This shows the consistency of the attention weights on the prompts across the timesteps in the same domain.

### Prompt Ensemble with a Pretrained Policy

While we previously presented joint learning of a policy and the attention module \(\), here we also present how to update \(\) for a pretrained policy \(\) to make the policy adaptable to domain changes. In this case, we add a _policy prompt_\(p_{}^{v}\) to concentrate on task-relevant features from observations for

Figure 4: Sample-efficiency of Prompt Ensemble-based Policy Learning for Object Navigation in AI2THOR. The x-axis represents the number of samples (timesteps) used for policy learning, while the y-axis represents the task success rate for zero-shot evaluation.

Figure 5: Prompt Ensemble Interpretability. In (a), the embeddings in the big circle are intra prompted embeddings obtained by varying domains within a domain factor, and the embeddings in the rectangle are inter prompted embeddings obtained by changing the visual prompts with aligned observation. The closely located intra prompted embeddings indicate the domain-invariant knowledge, while the inter prompted embeddings clustered by different visual prompts indicate the alignment between the visual prompts and the domain factors. In (b), each cell represents attention weight \(_{i}\) applied for prompted embedding \(z_{i}\).

the pretrained policy \(\) so that prompted embedding \(_{0}\) with the task-relevant features is incorporated into the guided-attention-based ensemble, i.e., \(((_{0},)),\) where \(_{0}=_{}(o,p_{}^{v}).\)

Table 2 reports zero-shot performance for the scenarios when a pretrained policy is given. We evaluate two different cases: _aligned_ (Aln.) when prompt-based contrastive learning is conducted on data from the same task of a pretrained policy; otherwise, _not aligned_ (Not Aln.). In AI2THOR, we use data from the object goal navigation task for prompt-based contrastive learning, while each pretrained policy is learned individually through one of tasks including object goal navigation, point goal navigation, image goal navigation, and room rearrangement. Similarly, in egocentric-Metaworld, we use data from the reach task for prompt-based contrastive learning, while each pretrained policy is learned individually through one of tasks including reach, reach-wall, button-press, and door-open. In Table 2(a), ConPE enhances zero-shot performance of the pretrained policies by \(3.5\!\!7.0\%\) for unseen target domains in AI2THOR. This prompt ensemble adaptation requires only 400K samples, equivalent to \(10\%\) of the total samples used for policy learning. In Table 2(b), ConPE significantly boosts zero-shot performance of the pretrained policies by \(9.0\!\!57.6\%\) in egocentric-Metaworld.

### Ablation Study

Here we conduct ablation studies with AI2THOR. All the performances are reported in success rates.

**Prompt Ensemble Scalability.** Table 3 evaluates ConPE with respect to the number of prompts (\(n\)). ConPE effectively enhances zero-shot performance for both seen and unseen target domains through prompt ensemble that captures various domain factors. Compared to the case of \(n=2\), for \(n=10\), there was a significant improvement in zero-shot performance for both seen and unseen target domains, with increases of \(42.8\%\) and \(36.7\%\), respectively. For \(n 10\), we observe stable performance that specifies that ConPE can scale for combining multiple prompts to some extent.

**Prompt Ensemble Methods.** Tabel 4 compares the performance of various prompt integration methods  including our guided attention-based prompt ensemble. We denote prompt-level integration as COM, and prompted embeddings-level integration as ENS. UNI-AVG and WEI-AVG refer to uniform average and weighted average mechanisms, respectively. ConPE achieves superior success rates over the most competitive ensemble method ENS-UNI-AVG, showing \(3.5\%\) and \(14.2\%\) performance gain for seen and unseen target domains.

   \(n\) & Source & Seen Target & Unseen Target \\ 
2 & \(98.7\!\!0.4\) & \(40.5\!\!2.2\) & \(43.0\!\!2.9\) \\
5 & \(96.1\!\!0.6\) & \(59.2\!\!9.6\) & \(45.0\!\!10.1\) \\
10 & \(96.3\!\!1.0\) & \(83.3\!\!0.3\) & \(79.7\!\!6.4\) \\
16 & \(91.8\!\!2.0\) & \(83.8\!\!1.3\) & \(77.1\!\!6.2\) \\
18 & \(98.5\!\!1.8\) & \(83.3\!\!2.3\) & \(79.0\!\!4.5\) \\   

Table 3: Prompt Ensemble Scalability

    &  &  &  &  \\   & Source & Target & Source & Target & Souce & Target & Souce & Target \\  Pretrained & \(87.5\!\!17.2\) & \(65.8\!\!19.1\) & \(95.3\!\!4.6\) & \(80.9\!\!1.6\) & \(77.2\!\!3.3\) & \(56.2\!\!2.2\) & \(87.3\!\!3.1\) & \(75.2\!\!13.2\) \\ ConPE & \(88.4\!\!1.7\) & \(\) & \(98.9\!\!1.0\) & \(\) & \(79.2\!\!1.4\) & \(\) & \(93.3\!\!1.2\) & \(\) \\    
    &  &  &  &  \\   & Source & Target & Source & Target & Source & Target & Source & Target \\  Pretrained & \(100.0\!\!0.0\) & \(65.7\!\!6.4\) & \(100.0\!\!0.0\) & \(58.0\!\!5.8\) & \(100.0\!\!0.0\) & \(16.8\!\!2.3\) & \(100.0\!\!0.0\) & \(35.6\!\!6.2\) \\ ConPE & \(100.0\!\!0.0\) & \(\) & \(100.0\!\!0.0\) & \(\) & \(100.0\!\!0.0\) & \(\) & \(100.0\!\!0.0\) & \(\) \\   }^{v}\). Note that \(p_{}^{v}\) corresponds to this case, while w/o \(p_{}^{v}\) corresponds to the other case of using the attention module without \(p_{}^{v}\). In addition, E2E denotes the fine-tuning of both the policy and the attention module along with \(p_{}^{v}\). The results demonstrate that our method enhances the zero-shot performance of the pretrained policy, showing that \(p_{}^{v}\) facilitates the extraction of task-specific features.

**Semantic Regularized Data Augmentation.** So far, we have only utilized vision data, but here, we discuss one extension of ConPE using semantic information. Specifically, we use a few samples of object-level text descriptions to regularize the data augmentation process in policy learning. This aims to mitigate overfitting issues [28; 29]. The detailed explanations can be found in Appendix. As shown in Table 6, ConPE with semantic data (w Semantic) consistently yields better performance than ConPE without semantic data (w/o Semantic) for all noise scale settings (\(\)). Note that the noise scale manages the variance of augmented prompted embeddings. This experiment indicates that ConPE can be improved by incorporating semantic information.

## 5 Related Work

**Adaptation in Embodied AI.** In the literature of robotics, numerous studies focused on developing generalized visual encoders for robotic agents across various domains [30; 31], exploiting pretrained visual encoders [32; 33], and establishing robust control policies with domain randomized techniques [34; 35]. Furthermore, in the field of learning embodied agents, a few works addressed adaptation issues of agents to unseen scenarios in complex environments, using data augmentation techniques [36; 37; 38; 39; 40] or adopting self-supervised learning schemes [41; 42; 43]. Recently, several works showed the feasibility and benefits of adopting large-scale pretrained vision-language models for embodied agents [7; 44; 45; 46]. Our work is in the same vein of these prior works of embodied agents, but unlike them, we explore visual prompt learning and ensembles, aiming to enhance both zero-shot performance and sample-efficiency.

**Decoupled RL Structure.** The decoupled structure, where a state representation model is separated from RL, has been investigated in vision-based RL [47; 2; 16]. Recently, contrastive representation learning on expert trajectories gains much interest, as it allows expert behavior patterns to be incorporated into the state encoder even when a policy is not jointly learned [1; 20]. They established generalized state representations, yet in that direction, sample-efficiency issues in both representation learning and policy learning remain unexplored.

**Prompt-based Learning.** Prompt-based learning or prompt tuning is a parameter-efficient optimization method for large pretrained models. Prompt tuning was used for computer vision tasks, optimizing a few learnable vectors in the text encoder , and it was also adopted for vision transformer models to handle a wide range of downstream tasks . Recently, visual prompting  was introduced, and both visual and text prompt tuning were explored together in the multi-modal embedding space [49; 50]. We also use visual prompt tuning, but we concentrate on the ensemble of multiple prompts to tackle complex embodied RL tasks. We take advantage of the fact that the generalized representation capability of different prompts can vary depending on a given task and domain, and thus we strategically utilize them to enable zero-shot adaptation of RL policies.

    &  &  \\   & Source & Target & Source & Target \\ 
0.1 & \(97.4 3.8\) & \(83.6 8.9\) & \(\) & \(\) \\
0.2 & \(94.7 0.0\) & \(77.6 12.8\) & \(\) & \(\) \\
0.3 & \(84.2 3.7\) & \(75.3 8.8\) & \(\) & \(\) \\
0.4 & \(80.3 16.2\) & \(74.0 14.9\) & \(\) & \(\) \\   

Table 6: Semantic Regularized Data Augmentation

   Optimization & Source & Target \\  Pretrained & \(95.3 4.6\) & \(80.9 1.6\) \\ w/o \(p_{pol}^{v}\) & \(59.5 2.9\) & \(54.2 0.6\) \\ w \(p_{pol}^{v}\) & \(\) & \(\) \\ E2E & \(96.4 0.5\) & \(83.3 3.3\) \\   

Table 5: Prompt Ensemble AdaptationConclusion

**Limitation.** Our ConPE framework exploits visual inputs and their relevant domain factors for policy adaptation. For environments where domain changes extend beyond those domain factors, the adaptability of the framework might be constrained. In our future work, we will adapt the framework with semantic knowledge based on pretrained language models to improve the policy generalization capability for embodied agents in dynamic complex environments and to cover various scenarios associated with multi-modal agent interfaces.

**Conclusion.** In this work, we presented the ConPE framework, a novel approach that allows embodied RL agents to adapt in a zero-shot manner across diverse visual domains, exploring the ensemble structure that incorporates multiple contrastive visual prompts. The ensemble facilitates domain-invariant and task-specific state representations, thus enabling the agents to generalize to visual variations influenced by specific domain factors. Through various experiments, we demonstrated that the framework can enhance policy adaptation across various domains for vision-based object navigation, rearrangement, manipulation tasks as well as autonomous driving tasks.

## 7 Acknowledgement

We would like to thank anonymous reviewers for their valuable comments and suggestions. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-01045, 2022-0-00043, 2020-0-01821, 2019-0-00421) and by the National Research Foundation of Korea (NRF) grant funded by the MSIT (No. NRF-2020M3C1C2A01080819, RS-2023-00213118).