# Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective

Yongxin Zhu\({}^{1,3}\), Bocheng Li\({}^{2,3}\), Hang Zhang\({}^{4}\), Xin Li, Linli Xu\({}^{*}\)\({}^{2,3}\), Lidong Bing

\({}^{1}\)School of Data Science, University of Science and Technology of China

\({}^{2}\)School of Computer Science and Technology, University of Science and Technology of China

\({}^{3}\)State Key Laboratory of Cognitive Intelligence, \({}^{4}\)Zhejiang University

zyx2016@mail.ustc.edu.cn,bcli@mail.ustc.edu.cn,hangzhang_scu@foxmail.com

lixin4ever@gmail.com,linlixu@ustc.edu.cn,binglidong@gmail.com

Corresponding author.

###### Abstract

Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT.

## 1 Introduction

In recent years, remarkable advancements have been achieved in the field of image generation, principally propelled by the development of latent-based generative models, such as Latent Diffusion Models (LDMs)  and Mask Image Models (MIMs) . By employing reconstructive autoencoders such as VQGAN  or VAE  to compress images into a manageable low dimensional latent space, these models can generate highly realistic and imaginative samples. Concurrently, in light of the transformative impact of autoregressive (AR) generative models, such as Large Language Models  in NLP, it becomes compelling to investigate the feasibility of similar paradigms to images. Despite the advances in image autoregressive pre-training, exemplified bymodels such as iGPT , AIM  and GIVT  in the pixel space, VIM  and LVM  in the latent space, their performance are still inferior to the leading models [34; 12; 7] in image generation, or self-supervised learning models [18; 10; 6; 28] in image understanding tasks.

An intriguing observation emerges regarding the presumed optimality of current practices in latent space: as illustrated in Figure 1(b), though sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in the image generation task. This discrepancy prompts reevaluating our understanding of latent spaces and their interaction with generative models, suggesting unexplored avenues worth investigating. A central premise in learning theory  is that the latent distribution should retain as much critical information of the data distribution as possible, akin to a compression goal. This leads to a common **misconception** that _an optimal latent space for reconstruction equates to optimal generative performance._ Nevertheless, in the investigation regarding the reconstruction and generation ability with the popular VQGAN model , it is observed that the generation FID will deteriorate when the reconstruction FID becomes lower (where a lower value indicates better performance), challenging the above assumption.

To address these intriguing discrepancies, we introduce a unified perspective on the relationship between latent spaces and generative models to analyze what constitutes an optimal latent space for generative models. Our findings reveal that, beyond the compression techniques employed by prevalent latent generative models, an optimal latent space should also aim to minimize the distance between distributions under the condition of incorporating a generative model, which is an aspect often overlooked. We critically assess prevalent methodologies and reveal that the stability of latent space is important for generative models. We argue that the reason why autoregressive models underperform iterative models such as LDMs and MIMs is that iterative models can correct errors brought by the instability of latent space.

Drawing from this insight, we propose a straightforward method to stabilize the existing latent space methods for image autoregressive generative models. Unlike conventional autoencoder-style approaches, our approach disentangles the concurrent training of encoders and decoders and commences with encoder-only training through a discriminative self-supervised model . This phase does not necessitate a decoder for pixel reconstruction, enabling the encoder to discern the intrinsic and distinguishable features present within the data. Subsequently, a separate decoder of the autoencoder  is trained and tasked solely with the pixel reconstruction process, conditioned on the features identified by the encoder. By focusing initially on the encoder's capability to extract meaningful data features independently of pixel reconstruction, we lay a foundation for a more stable and feature-rich latent

Figure 1: **(a)**: Linear probe and class-unconditional generation performance of different methods trained and evaluated on ImageNet-1K. **(b)**: Class-conditional generation performance of different methods on ImageNet-1k. The size of the bubbles indicates the number of parameters in the models. DiGIT achieves SOTA performance in linear probing and establishes a new SOTA in image generation within a single model.

space. The subsequent independent training phase of the decoder ensures that these captured features can be accurately translated back to pixels.

In support of the autoregressive generative model, which requires discrete tokens for next token prediction, we employ a strategy inspired by VQGAN  to discretize the encoder's latent feature space with the K-Means clustering algorithm. With this novel image tokenizer induced from the stabilized latent space, the performance of autoregressive generative models in images is enhanced significantly in both image understanding and image generation tasks. We refer to this approach as call **D**iscriminative **G**enerative **I**mage **T**ransformer (DiGIT). Notably, when scaling up the model size, substantial improvements can be achieved. To the best of our knowledge, this is the first evidence that image autoregressive generative models behave analogously to GPT. In essence, this work endeavors to redefine the boundaries of what is possible in image autoregressive modeling through a unified perspective of latent space.

In summary, our contributions to the field of image generative models include:

* We introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling.
* We propose a novel method to stabilize latent space by disentangling the encoder and decoder training processes. Furthermore, a simple yet effective discrete image tokenizer is proposed to improve the image autoregressive generative model's performance under the philosophy of next token prediction.
* The experimental results show that the image autoregressive modeling with our tokenizer leads to SOTA performance in image understanding and generation, with further improvements witnessed when scaling up the model size.

## 2 Problem Formulation

In section 2.1, we formalize the latent space requirements for generative models and categorize current latent-based generative models. Furthermore, in section 2.2, we analyze the stability of different induced latent spaces and propose to stabilize the latent space for autoregressive generative models instead of stabilizing the generation process with an iterative decoding strategy like LDMs.

### Latent Space for Generative Models

Drawing inspiration from the complexity perspective of latent space induced from autoencoders in Hu et al. , we delve into the latent space for generative models. Generative models aim at learning a distribution to approximate the data distribution \(P_{X}\). Formerly, given a tractable prior distribution \(P_{Z}\) and a distance metric \(D(,)\) between distributions, the purpose of a generative model \(g\) is to minimize the distance between data distribution \(P_{X}\) and the distribution generated by \(g(Z)\):

\[_{g}D(P_{g(Z)},P_{X}).\] (1)

For example, GANs  employ the Gaussian distribution as their prior and utilize a discriminator network as the distance metric. However, the optimal strategy for data representation in generative models is still under-explored. Recent studies on latent diffusion models  have identified that direct learning in the pixel space of images is suboptimal. They propose to learn in a latent space induced by a constrained autoencoder model such as VAE  or VQGAN , which has been demonstrated to improve the perceptual quality.

A simple method to construct the latent space is using an encoder \(f:^{d}^{d_{z}}\) to map raw data samples \(x^{d}\) into a latent space \(f(X)\) of dimension \(d_{z}\). Consequently, the goal of latent-based generative models is to learn the distribution as per the following formula:

\[_{g}D(P_{g(Z)},P_{f(X)}),\] (2)

where \(P_{f(X)}\) denotes the data distribution in the latent space induced by the encoder \(f\). Despite these advances, determining the optimal latent space configuration for generative models remains unresolved.

Distance between distributions in different spaces.Given the ultimate goal of the generative model is to produce image pixels, a decoder model \(h:^{d_{z}}^{d}\), paired with the encoder model \(f\), is necessary to convert latent representations back into pixels. We define a generalized distance between different spaces associated with a decoder \(h\) as:

\[D^{}(P_{f(X)},P_{X}):=_{h}D(P_{h(f(X))},P_{X}).\] (3)

By employing \(D^{}\) to compare distributions across different spaces, we can define the ideal latent space \(f(X)\) as the one that minimizes \(D^{}(P_{f(X)},P_{X})\). This implies selecting a latent representation that minimizes the empirical objective \((h|P_{f(X)})\) with the same family \(\) of decoder models. Such a latent configuration depends on both the data and the decoder training methodology. We can formalize it with an autoencoder framework by looking at the encoder and decoder together, and the primary goal becomes the reconstruction of the input sample \(x\), formulated as \(_{f,h}(h(f(x)),x)\). Once a generative model \(g\) successfully approximates the latent distribution \(P_{f(X)}\), the generated sample can be efficiently transformed back into pixels using the decoder \(h\).

Similarly, we can define the distance between distributions in the latent space \(f(X)^{d_{z}}\) and data space \(X^{d}\) conditioned on the latent-based generative model \(g\),

\[D^{}(P_{f(X)},P_{X}):=_{g}D(P_{g(Z)},P_{f(X)}),\] (4)

which aims to minimize the empirical objective with the generative model family \(\).

Optimal Latent Space for Generative Models.Now that we have characterized the ideal latent distribution given the family of generative models and the data, the next step is to determine how to find the optimal latent space. At the population level, the objective for the latent-based generative models with a decoder is:

\[_{h,}D(P_{h(g(Z))},P_{X})=_{h ,g}D(P_{h(f(X))},P_{X})+D(P_{g(Z)},P_{f(X)}),\] (5)

where the first term focuses on optimizing the decoder to enhance the reconstruction quality and the second one is directed towards optimizing the generative model to more accurately approximate the latent space distribution. Inspired by this observation, we can characterize the optimal latent distribution \(P_{f(X)}^{*}\) for a given \(P_{X}\) from the perspective of minimizing the distance between distributions in different spaces by defining \(f^{*}\) as

\[*{argmin}_{f}D^{}(P_{f(X)},P_{X}):=f^ {*}(X)=*{argmin}_{f}D^{}(P_{f(X)},P_{X} )+D^{}(P_{f(X)},P_{X})\] (6)

Notice that \(P_{f^{*}(X)}\) depends on multiple factors, including \(P_{X}\), the distance metric \(D\), and the constructed model families \(\) and \(\). By integrating \(D^{}\) and \(D^{}\), we arrive at a comprehensive measure of the distance between the distribution in the latent space and the original data distribution as \(D^{}(P_{f(X)},P_{X})\). The second term, \(D^{}(P_{f(X)},P_{X})\), is exactly the objective of reconstructive autoencoders. Ultimately, from examining the learning objective pertinent to identifying the optimal latent space for generative models, it becomes evident that:

_A reconstructive autoencoder does not necessarily establish an advantageous latent space for generative models._

Two Pathways of the Latent Space Construction.Although we theoretically analyze the optimization of the optimal latent space for generative models, it is challenging to implement in practice because optimizing \((f,g,h)\) simultaneously is computationally complex. A practical solution is to optimize \(D^{}\) and \(D^{}\) separately, allowing for tractable training.

* When \(D^{}(P_{f(X)},P_{X})\) is not a target for optimization, it implies that the optimization of the decoder within the generative model framework is bypassed. The encoder _independently_ forms a latent space, aligning with self-supervised learning (SSL) strategies aimed at uncovering lower-dimensional features from unlabeled data. However, learning the generative models in the latent space induced by SSL models remains relatively unexplored.
* On the other hand, when \(D^{}(P_{f(X)},P_{X})\) remains fixed, the primary objective becomes optimizing the encoder and decoder to effectively learn and represent the latent space,where \(D^{}(P_{f(X)},P_{X})\) degrades into an autoencoder learning objective. This approach is evident in recent latent-space-oriented generative models, such as LDM [34; 30], VQGAN (AR), and MaskGIT (MIM) , all of which concentrate on learning \(g\) in the latent space with the encoder and decoder frozen. 2

While latent generative models such as LDM , MaskGIT , and VQGAN  share the same latent space induced by a reconstructive autoencoder  to minimize \(D^{}(P_{f(X)},P_{X})\), their image generation performances differ significantly. In the next section, we analyze the reason behind it from the perspective of the latent space.

### Stability of the Latent space

We first describe the decoding mechanism of various latent generative models. Both LDM  and MaskGIT  can be depicted as an iterative sampling procedure given by:

\[p(x^{T})=_{i=1}^{T}p(x^{i}|x^{i-1}).\] (7)

The intermediate states \(x^{i}\) in LDMs represent images infused with Gaussian noise of varying variance, whereas for MaskGIT, they denote discretely tokenized images augmented with masks. In contrast, the autoregressive framework of VQGAN (AR) is described as:

\[p(x)=p(x_{1},,x_{N})=_{j=1}^{N}p(x_{j}|x_{<j}),\] (8)

where \(x_{i}\) represents the \(i\)-th patch in the image sequence. Notice that \(x^{i}\) denotes the _entire_ image while \(x_{j}\) means the _local_ patch tokens. In the autoregressive decoding process, if the previously sampled tokens are incorrect, the accuracy of subsequent tokens would be affected due to error aggregation. In contrast, the iterative decoding approach allows for the revision of earlier misjudged tokens. When the latent space is unstable that small perturbation in pixels can change the latent distribution significantly, the iterative decoding mechanism employed by LDM and MaskGIT can alleviate the error aggregation problem by allowing for revision of earlier misjudged tokens while autoregressive models cannot. Consequently, a stable latent space is required to reduce errors introduced in the generation process of autoregressive models.

This principle forms the foundation of our methodology for developing a metric to evaluate latent spaces with an emphasis on the stability of the latent representations. We examine two primary types of latent spaces: (1) autoencoder induced by minimizing \(D^{}(P_{f(X)},P_{X})\) and (2) self-supervised learning (SSL) model induced by minimizing \(D^{}(P_{f(X)},P_{X})\). By analyzing network parameters of these models in a linear regime, we derive the following propositions.

**Proposition 2.1**.: _The latent space spanned by a linear autoencoder is congruent with that spanned by the principal component loading vectors derived in Principal Component Analysis (PCA). Furthermore, the principal component loading vectors can be elucidated from the autoencoder's weights._

**Proposition 2.2**.: _The discriminative self-supervised model learns to separate data distributions in the latent space as Linear Discriminant Analysis in principle._

Motivated by these theoretical insights, we introduce a metric to assess the stability of the latent space induced by different encoder models. To exemplify these concepts, we refer to an example consisting of two Gaussian distributions in a two-dimensional space, as depicted in Figure 6(a). The results attained from applying the PCA and LDA algorithms are visually depicted in Figure 6(b) and (c) respectively. The distribution embedded by the LDA model exhibits greater separability than that by the PCA model. To quantitatively evaluate stability, we add Gaussian noise of different variances to the original 2D data and subsequently train a linear classifier on the latent space. As Figure 6(d) illustrates, the accuracy of the LDA model consistently surpasses that of PCA.

To evaluate the stability of latent space induced from autoencoders and SSL models, we add Gaussian noise to image pixels and then feed the noisy images to a VQGAN encoder and an SSL encoder DINOv2 . This experiment aims to examine the resilience of the latent spaces induced by these encoders to such disturbances. We measure the rate of change in discrete tokens, specifically VQ tokens for the latent space induced from the VQGAN encoder and discriminative tokens for the latent space induced from the SSL model, and the cosine similarity in conjunction with the strength of the noise introduced. The experimental results in Table 1 demonstrate that the latent space induced from the SSL model exhibits heightened stability compared to that derived from the VQGAN autoencoder. Therefore, we propose to replace the unstable latent space induced by the reconstructive model with a stable latent space induced by the discriminative self-supervised model for autoregressive models.

## 3 Stabilize the Latent Space with Self-supervised Learning Model

In this section, we present a simple but effective image tokenizer that discretizes the feature representations of discriminative SSL models to form discrete tokens for autoregressive models. The architecture of our model is illustrated in Figure 2.

Discrete Image Discriminative TokenizerDrawing inspiration from the VQGAN tokenizer , which employs an implicit K-Means clustering algorithm within the latent space to generate discrete tokens for autoregressive modeling, we propose a straightforward approach to perform K-Means clustering on the feature space of discriminative SSL models to obtain discrete tokens. To process a given dataset, our initial step involves gathering the features of image patches, akin to the hidden states produced by SSL models. Then we employ a clustering algorithm to group these patches, resulting in a collection of \(K\) clustering centers. These centers constitute the codebook for the discrete tokenizer. To determine the discrete tokens for an image patch at inference, we identify its nearest neighbor in the codebook, which is then designated as the discrete token for the respective patch.

Image Autoregressive ModelingAfter converting images into discrete tokens with the discriminative tokenizer, we treat each image as a sequence by flattening the discrete tokens from images into a 1D sequence in raster order. We train a causal Transformer  model with the next token prediction objective, which is the same as the standard approach for language models.

   SNR & **30** & **25** & **20** & **15** & **10** & **5** & **1** & **0.01** \\ 
**VQ Token change \(\)** & 0.187 & 0.317 & 0.487 & 0.663 & 0.805 & 0.901 & 0.948 & 0.956 \\
**Disc Token change \(\)** & 0.114 & 0.178 & 0.260 & 0.355 & 0.457 & 0.570 & 0.687 & 0.721 \\
**VQ Token cos-sim \(\)** & 0.972 & 0.949 & 0.910 & 0.853 & 0.777 & 0.682 & 0.594 & 0.571 \\
**Disc Token cos-sim \(\)** & 0.975 & 0.960 & 0.940 & 0.916 & 0.888 & 0.855 & 0.816 & 0.803 \\   

Table 1: The stability of latent spaces induced from VQ Token and Discriminative Token (introduced in Section 3), assessed across different Signal-to-Noise Ratio (SNR) levels to evaluate performance under varying signal and noise conditions.

Figure 2: The architecture of DiGIT.

## 4 Experiments

### Implementation Details

We take the discriminative SSL model DINOv2  as the encoder for all experiments. The K-Means model is trained on the randomly selected 10% subset of the ImageNet  training set. We use the autoregressive model with the same architecture as GPT-2 . We train the DiGIT models with the base and large sizes. The vocabulary size of the tokenizer for the base is \(8192\) and \(16000\) for the large size. More implementation details and hyper-parameters are provided in Appendix A.3.

### Image Understanding

The GPT model is famous for learning semantic features by a generative training objective of next token prediction. We compare the image understanding ability of different image autoregressive models with linear-probe as described in iGPT . We train a linear classifier on top of the frozen features average from each layer on the ImageNet training set. We report the Top-1 accuracy compared with other image autoregressive models in Table 2. Remarkably, with only 219M parameters, DiGIT achieves a Top-1 accuracy of 71.7%, surpassing both iGPT and VIM-Base, which have a greater number of parameters and operate at more visual tokens. Despite representing images with a smaller token grid size (\(16 16\) as opposed to \(32 32\)), DiGIT still delivers superior top-1 accuracy, demonstrating the effectiveness of our tokenizer. Moreover, when we scale DiGIT's parameters from 219M to 732M, the Top-1 accuracy shows an additional increase of 8.6% and reaches 80% for the first time. The improvement indicates that DiGIT with the proposed discriminative tokenizer has the potential for the development of large vision models.

### Image Generation

Since the SSL models do not have a paired decoder to recover pixels from latent space, the generative models trained with our discriminative tokenizer require an auxiliary image decoder to render pixels. The discriminative tokenizer can be seamlessly integrated with any existing image generative models trained with a tokenizer induced from a reconstructive autoencoder. In our experiment, we train an autoregressive model VQGAN, and an MIM model MaskGIT as the pixel decoder respectively. The results are presented in Table 3 and Table 4. The autoregressive model equipped with our discriminative tokenizer achieves the SOTA performance with FID reaching \(3\) for the first time. Furthermore, the performance significantly improves as the model size increases, demonstrating the potential of a large vision model with next token prediction. Interestingly, when utilizing the DiGIT as the conditioning factor, the performance of both the autoregressive and MaskGIT decoders becomes close (4.62 and 4.79). This observation suggests that stabilizing the latent space produces effects analogous to the iterative stabilization decoding mechanism.

### Ablation Study

We conduct the ablation study to present a comprehensive analysis of the proposed discriminative tokenizer in image generation and understanding. The results are illustrated in Table 3(a) and

   Methods & \# Tokens & Features & \# Params & Top-1 Acc.\(\) \\  iGPTL  & \(32 32\) & 1536 & 1362M & 60.3 \\ iGPT-XL  & \(64 64\) & 3072 & 6801M & 68.7 \\ VIM+VQGAN  & \(32 32\) & 1024 & 650M & 61.8 \\ VIM+dVAE  & \(32 32\) & 1024 & 650M & 63.8 \\ GIVT  & \(16 16\) & 1024 & 304M & 65.1 \\ VIM+ViT-VQGAN  & \(32 32\) & 1024 & 650M & 65.1 \\ VIM+ViT-VQGAN  & \(32 32\) & 2048 & 1697M & 73.2 \\ AIM  & \(16 16\) & 1536 & 0.6B & 70.5 \\
**DiGIT (Ours)** & \(16 16\) & 1024 & 219M & 71.7 \\
**DiGIT (Ours)** & \(16 16\) & 1536 & 732M & **80.3** \\   

Table 2: Linear-probe accuracy of image autoregressive generative models on ImageNet .

Figure 3(b). For image generation tasks, we take the autoregressive model trained with the VQGAN tokenizer as the baseline. Introducing discriminative tokens leads to a significant improvement, reducing FID to \(9.66\) and increasing IS to \(69.15\), underscoring the effectiveness of stabilizing latent space for autoregressive models. Further extending the training duration to \(400\) epochs yielded additional improvements of \(0.53\). A substantial advancement is observed when scaling up the model size to 732M, resulting in FID dropping dramatically to \(4.59\) and IS more than doubling to \(141.29\). This indicates that increasing the model's capacity significantly enhances its ability to model complex relationships within the data, which is a similar phenomenon in GPT models. Overall, the study highlights the latent space stabilization and the potential of large-scale training of autoregressive modeling in images with our discriminative tokenizer.

   Type & Methods & \#Param & \#Epoch & FID\(\) & IS\(\) \\  GAN & BigGAN  & 70M & - & 38.6 & 24.70 \\ Diff. & LDM  & 395M & - & 39.1 & 22.83 \\ Diff. & ADM  & 554M & - & 26.2 & 39.70 \\ MIM & MAGE  & 200M & 1600 & 11.1 & 81.17 \\ MIM & MAGE  & 463M & 1600 & 9.10 & 105.1 \\  MIM & MaskGIT  & 227M & 300 & 20.7 & 42.08 \\ MIM & **DiGIT** (+MaskGIT) & 219M & 200 & **9.04** & **75.04** \\  AR & VQGAN  & 214M & 200 & 24.38 & 30.93 \\ AR & GIVT  & 304M & 500 & 17.70 & - \\ AR & GIVT  & 1.67B & 500 & 11.02 & - \\ AR & **DiGIT** (+VQGAN) & 219M & 400 & **9.13** & **73.85** \\ AR & **DiGIT** (+VQGAN) & 732M & 200 & **4.59** & **141.29** \\ validation data & DiGIT + VQ & - & - & 1.92 & 184.40 \\ validation data & VQ only & - & - & 1.67 & 175.56 \\   

Table 3: Class-unconditional image generation on ImageNet with resolution \(256 256\). DiGIT + VQ indicates that we utilize golden discriminative tokens alongside VQ generated by autoregressive models.

   Type & Methods & \#Param & \#Epoch & FID\(\) & IS\(\) \\  GAN & BigGAN  & 70M & - & 38.6 & 24.70 \\ Diff. & LDM  & 395M & - & 39.1 & 22.83 \\ Diff. & ADM  & 554M & - & 26.2 & 39.70 \\ MIM & MAGE  & 200M & 1600 & 11.1 & 81.17 \\ MIM & MAGE  & 463M & 1600 & 9.10 & 105.1 \\  MIM & MaskGIT  & 227M & 300 & 20.7 & 42.08 \\ MIM & **DiGIT** (+MaskGIT) & 219M & 200 & **9.04** & **75.04** \\  AR & VQGAN  & 214M & 200 & 24.38 & 30.93 \\ AR & GIVT  & 304M & 500 & 17.70 & - \\ AR & GIVT  & 1.67B & 500 & 11.02 & - \\ AR & **DiGIT** (+VQGAN) & 219M & 400 & **9.13** & **73.85** \\ AR & **DiGIT** (+VQGAN) & 732M & 200 & **4.59** & **141.29** \\ validation data & DiGIT + VQ & - & - & 1.92 & 184.40 \\ validation data & VQ only & - & - & 1.67 & 175.56 \\   

Table 4: Class-conditional image generation on ImageNet with resolution \(256 256\). \(\) denotes the model is trained with classifier-free guidance while all the other models are not.

For the image understanding task, we investigate the effect of K-Means clusters and features learned in the different layers of DiGIT. We can see that increasing the cluster number can further improve the accuracy of the linear probe, which means the image autoregressive model can benefit from a larger vocabulary. The linear probe accuracy increases quickly from the first transformer block, reaches its peak at the middle layers, and finally decreases a little bit for the last few blocks. This observation connects the image autoregressive model to the text language model where the semantic information is learned in the middle layers of the transformer.

### Comparison of Discrete Tokenizers

We conduct an experiment to investigate the effect of different SSL models on latent space. We generally categorize the SSL models into two types according to their pre-training objectives: (1) Global level (MoCo) and patch level (MAE,iBOT), (2) reconstructive (MAE) and discriminative (MoCo, iBOT). At the global level, the loss function is computed using an aggregate output such as [**CLS**] token or mean pooling. In contrast, patch-level models involve patches directly in loss computation. Reconstructive models, such as MAE, aim to recover image pixels in a manner akin to autoencoders, while discriminative models are optimized to learn the distinguishable features. As demonstrated in Table 4(a), the discriminative objective plays a pivotal role in image generation in that it can stabilize the latent space. Furthermore, because generative models need to predict patches, the inclusion of a patch-level loss function can enhance performance.

To assess the stability of latent space induced by our discriminative tokenizer and reconstructive tokenizer. We pre-train two auto-regressive generative models on the ImageNet dataset , employing the proposed discriminative tokenizer and VQGAN tokenizer respectively. We provide each model with the upper half of the target image as a conditional prompt for generation, challenging them to complete the lower half of the image. A stable latent space should be able to help the autoregressive model generate the lower half more robustly, maintaining thematic and aesthetic coherence. As shown in Figure 4(b), the FID decreases for both models when given a longer prefix context. However, when the prefix length is reduced from 75% to only 12.5% of the image, the model trained with the VQGAN tokenizer encounters difficulties in producing images that adhere to the specified prompt. In contrast, the model utilizing the discriminative tokenizer effectively continues to produce congruent visual tokens, maintaining low FID scores even with a significantly truncated prefix.

## 5 Related Work

Image TokenizerThe image tokenizer [37; 33; 15] is essential in converting pixels into discrete tokens for autoregressive generative modeling. VQVAE  first proposes to assign latent features learned by an encoder to the nearest entry in the learnable codebook embeddings, followed by a decoder to reconstruct the original image pixels. VQGAN  further incorporates adversarial loss and perceptual loss to improve the image synthesis quality. RQ-Transformer  extends the single-layer quantizer to the multi-layer residual quantizer to augment the visual tokenizer's ability to capture fine-grained details. ViT-VQGAN  incorporates the modern Vision-Transformer  into VQGAN to enhance the reconstruction quality. MAGVIT-v2  substitute the online update codebook in VQGAN with a lookup-free quantizer to enable a larger vocabulary for generative language models.

Figure 3: Ablation study of DiGIT. (a) The comparison of tokenizer, training steps, and model size in the image generation task. (b) Linear-probe accuracy from different layers in the pre-trained DiGIT-base with different number of K-Means clusters.

Image Autoregressive ModelingInspired by the success of autoregressive Transformer  in text generation tasks, there have been several efforts to replicate it in image generation tasks. One of the pioneering works is iGPT , which pre-trains an autoregressive model on pixels with the same architecture as GPT2 , achieving promising results in unsupervised visual representation learning. LVM  proposes a large-scale vision dataset composed of images and videos, based on which a large vision model is trained. Empirical observations indicate that the model scales effectively across various tasks with in-context learning . Similarly, AIM  follows ViT  and represents images in the patch. It is observed that the performance of image recognition continues to increase as the model size scales up. VAR  proposes a next-scale prediction to generate images from coarse to fine in a hybrid of autoregressive and nonautoregressive manner.

Self-supervised Learning ModelsSelf-supervised learning (SSL) [9; 17] plays an important role in learning fundamental visual representations for downstream tasks. Among them, SimCLR , MoCo [9; 18; 10] compute losses at the image level through \([]\) token aggregation or pooling operations with contrastive learning. iBOT-style models [43; 6; 28] extend the loss to the patch level, achieving improved performance in dense prediction tasks. BEiT  uses VQGAN tokenized sequences as the training target. MAE  randomly masks some patches of images and reconstructs the pixels with unmasked patches as the condition.

## 6 Conclusion

In this paper, we make an exploration in the latent space for generative modeling. We introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Subsequently, we propose a simple but effective discriminative image tokenizer, followed by an image autoregressive generative model DiGIT. Empirical results indicate that our tokenizer achieves superior performance across both image understanding and image generation tasks. Notably, when DiGIT is scaled up in model size, it exhibits even greater enhancements, indicating the potential for the development of large vision models. Our findings challenge the conventional wisdom that proficiency in reconstruction equates to an effective latent space for auto-regressive generation. Through this work, we aim to rekindle interest in the generative pre-training of image auto-regressive models and encourage a reevaluation of the fundamental components that define latent space for generative models.