# Uncertainty-Aware Instance Reweighting for

Off-Policy Learning

 Xiaoying Zhang\({}^{1}\)  Junpu Chen\({}^{2}\)  Hongning Wang\({}^{3}\)  Hong Xie\({}^{4}\)  Yang Liu\({}^{1}\)

John C.S. Lui\({}^{5}\)  Hang Li\({}^{1}\)

\({}^{1}\)ByteDance Research \({}^{2}\)ChongQing University \({}^{3}\)Tsinghua University

\({}^{4}\) Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Science

\({}^{5}\) The Chinese University of Hong Kong

{zhangxiaoying.xy,yang.liu01,lihang.lh}@bytedance.com

{jumpchan98,hongx87,wang.hongn}@gmail.com

cslui@cse.cuhk.edu.hk

###### Abstract

Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines and recommender systems. While the ground-truth logging policy is usually unknown, previous work simply employs its estimated value for the off-policy learning, ignoring the negative impact from both high bias and high variance resulted from such an estimator. And such impact is often magnified on samples with small and inaccurately estimated logging probabilities. The contribution of this work is to explicitly model the uncertainty in the estimated logging policy, and propose an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, with a theoretical convergence guarantee. Experiment results on the synthetic and real-world recommendation datasets demonstrate that UIPS significantly improves the quality of the discovered policy, when compared against an extensive list of state-of-the-art baselines.

## 1 Introduction

In many real-world applications, including search engines , online advertisements , recommender systems [8; 22], only logged data is available for subsequent policy learning. For example, in recommender systems, various complex recommendation policies are optimized over logged user interactions (e.g., clicks or stay time) with items recommended by previous recommendation policies (referred to as the _logging policy_) [51; 14]. However, such logged data is often known to be biased, since the feedback on items where the logging policy did not take is unknown. This inevitably distorts the evaluation and optimization of a new policy when it differs from the logging policy.

Off-policy learning [41; 27] thus emerges as a preferred way to learn an improved policy only from the logged data, by addressing the mismatch between the learning and logging policies. One of the most commonly used off-policy learning methods is the Inverse Propensity Scoring (IPS) [8; 25], which assigns per-sample importance weight (i.e., propensity score) to the training objective on the logged data, so as to get an unbiased optimization objective in expectation. The importance weight in IPS is the probability ratio of taking an action between the learning and logging policies.

Unfortunately, the ground-truth logging policy is oftentimes unavailable to the learner in practice, due to reasons like legacy issues, i.e., it was not recorded in the data. Additionally, in specific situations like the healthcare domain  or two-stage recommender systems , access to the ground-truth logging policy is not feasible. One common treatment by many previous studies [35; 22; 8; 24] is to first estimate the logging policy using a supervised learning method (e.g., logistic regression,neural networks, etc.), and then employ the estimated logging policy for off-policy learning. In this work, we first show that such an approximation results in a biased estimator which is sensitive to data with small estimated logging probabilities. Worse still, small estimated logging probabilities usually suggest there are limited related samples in the logged data, whose estimations can have high uncertainties, i.e., being wrong with a high probability. Figure 1 shows a piece of empirical evidence from a large-scale recommendation benchmark KuaiRec dataset , where items with lower frequencies in the logged dataset have lower estimated logging probabilities (via a neural network estimator) and higher uncertainties at the same time. The high bias and variance caused by these samples can greatly hinder the performance of subsequent off-policy learning. We defer detailed discussions of this result in Section 2.

In this work, we explicitly take the uncertainty of the estimated logging policy into consideration and design an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for off-policy learning. UIPS reweighs the propensity score of each logged sample to control its impact on policy optimization, and learns an improved policy by alternating between: (1) Find the optimal weight that makes the estimator as accurate as possible, based on the uncertainty of the estimated logging policy; (2) Improve the policy by optimizing the resulting objective function. The optimal weight for each sample is obtained by minimizing the upper bound of the mean squared error (MSE) to the ground-truth policy evaluation, with a closed-form solution. Furthermore, UIPS ensures that off-policy learning converges to a stationary point where the true policy gradient is zero; while convergence may not be guaranteed when directly using the estimated logging policy. Extensive experiments on a synthetic and three real-world recommendation datasets against a rich set of state-of-the-art baselines demonstrate the power of UIPS. All data and code can be found in [https://github.com/Xiaoyinggit/UIPS.git](https://github.com/Xiaoyinggit/UIPS.git).

## 2 Preliminary: off-policy learning

We focus on the standard contextual bandit setup to explain the key concepts in UIPS. Following the convention [16; 29; 36], let \( R^{d}\) be a \(d\)-dimensional context vector drawn from an unknown distribution \(p()\). Each context is associated with a finite set of actions denoted by \(\), where \(||<\). Let \(:^{}\) denote a stochastic policy, such that \((a|)\) is the probability of selecting action \(a\) under context \(\) and \(_{a}(a|)=1\). Under a given context \(\), the reward \(r_{,a}\) is only observed when action \(a\) is chosen, i.e., bandit feedback. Without loss of generality, we assume \(r_{,a}\). Let \(V()\) denote the expected reward of the policy \(\):

\[V()=_{ p(),a(a|)}[r_{,a}]. \]

We look for a policy \((a|)\) to maximize \(V()\). In the rest, we denote \(_{ p(),a(a|)}[]\) as \(_{}[]\).

In off-policy learning, one can only access a set of logged feedback data \(D:=\{(_{n},a_{n},r_{_{n},a_{n}})|n[N]\}\). Given \(_{n}\), the action \(a_{n}\) was generated by a stochastic logging policy \(^{*}\), i.e., \(a_{n}^{*}(a|_{n})\), which is usually different from the learning policy \((a|)\)[24; 40; 8]. The actions \(\{a_{1},,a_{N}\}\) and their corresponding rewards \(\{r_{_{1},a_{1}},,r_{_{N},a_{N}}\}\) are generated independently given \(^{*}\). The main challenge is then to address the distributional discrepancy between \(^{*}(a|)\) and \((a|)\), when optimizing \((a|)\) to maximize \(V()\) with access only to the logged dataset \(D\).

Figure 1: Estimated logging policy and its uncertainty under different item frequency on KuaiRec.

One of the most widely used methods to address the distribution shift between \((a|)\) and \(^{*}(a|)\) is the Inverse Propensity Scoring (IPS) [8; 25]. One can easily get that:

\[V()=_{^{*}}[)}{^{*}(a|)}r_{ ,a}],\]

yielding the following empirical estimator of \(V()\):

\[_{}()=_{n=1}^{N}|_{n })}{^{*}(a_{n}|_{n})}r_{_{n},a_{n}}, \]

where \((a_{n}|_{n})/^{*}(a_{n}|_{n})\) is referred to as the propensity score. Various algorithms can be readily used for policy optimization under \(_{}()\), including value-based methods  and policy-based methods [19; 31; 42]. In this work, we adopt a well-known policy gradient algorithm, REINFORCE . Assume the policy \((a|)\) is parameterized by \(\), via the "log-trick", the gradient of \(_{}(_{})\) with respect to \(\) can be readily derived as,

\[_{}_{}(_{})=_{n=1}^{N}}(a_{n}|_{n})}{^{*}(a_{n}| _{n})}r_{_{n},a_{n}}_{}(_{}(a_{n}|_{n})).\]

**Approximation with an unknown logging policy**. In many real-world applications, the ground-truth logging probabilities, i.e., \(^{*}(a|)\) of each observation \((,a)\) in \(D\), are unknown. As a typical walk-around, previous work employs supervised learning methods such as logistic regression  and neural networks  to estimate the logging policy, and replaces \(^{*}(a|)\) with its estimated value \((a|)\) to get the following BIPS estimator for policy learning:

\[_{}(_{})=_{n=1}^{N}}(a_{n}|_{n})}{(a_{n}|_{n})}r_{_{n},a_{n}}. \]

However, as shown in the following proposition, inaccurate \((a|)\) leads to high bias and variance in BIPS. Worse still, smaller and inaccurate \((a|)\) further enlarges this bias and variance.

**Proposition 2.1**.: _The bias and variance of \(_{}(_{})\) can be derived as follows:_

\[(_{}(_{}) )=_{D}[_{}(_{})-V( _{})]=_{_{}}[r_{,a} ((a|)}{(a|)}-1)]\] \[N_{D}(_{}(_{}))=_{_{}}(( a|)}{(a|)}r_{,a})+_{_{}}[(}(a|)}{^{*}(a|)}-1)(a|)^{2}}{(a|)^{2}}r_{,a}^{2}]\]

However, smaller \((a|)\) usually implies less number of related training samples in the logged data, and thus \((a|)\) can be inaccurate with a higher probability. To make it more explicit, let us revisit the empirical results shown in Figure 1. We followed the method introduced in  to estimate the logging policy on KuaiRec dataset  and plotted the estimated \((a|)\) and its corresponding uncertainties on items of different observation frequencies in the logged dataset. We adopted the method in  to measure the confidence interval of \((a|)\) on each instance. A wider confidence interval, i.e., higher uncertainty in estimation, implies that with a high probability the true value may be further away from the empirical mean estimate. We can observe in Figure 1 that as item frequency decreases, the estimated logging probability also decreases, but the estimation uncertainty increases. This implies that a smaller \((a|)\) is usually 1) more inaccurate and 2) associated with a higher uncertainty.

As a result, with high bias and variance caused by inaccurate \((a|)\), it is erroneous to learn \(_{}(a|)\) by simply optimizing \(_{}(_{})\). Furthermore, this approach may also hinder the convergence of off-policy learning, as discussed later in Section 3.2.

## 3 Uncertainty-aware off-policy learning

Our idea is to consider the uncertainty of the estimated logging policy by incorporating per-sample weight \(_{,a}\), and perform policy learning by optimizing the following empirical estimator:

\[_{}(_{})=_{n=1}^{N}}(a_{n}|_{n})}{(a_{n}|_{n})} _{_{n},a_{n}} r_{_{n},a_{n}}. \]Intuitively, one should assign lower weights to samples whose \((a|)\) is small and far away from the ground-truth \(^{*}(a|)\). We then divide off-policy optimization into two iterative steps:

* **Deriving the optimal instance weight:** Find the optimal \(_{,a}\) to make \(_{}(_{})\) approach its ground-truth \(V()\) as closely as possible, so as to facilitate policy learning. The derived optimal weight is denoted as \(^{*}_{,a}\) (see Theorem 3.2).
* **Policy improvement:** Update the policy \(_{}(a|)\) using the following gradient: \[_{}_{}(_{})= _{n=1}^{N}}(a_{n}|_{n})}{(a_{n}| {x}_{n})}^{*}_{_{n},a_{n}} r_{_{n},a_{n}}_{ {}}(_{}(a_{n}|_{n}))\] (5) The whole algorithm framework and its computational cost, as well as important notations are summarized in Appendix 7.1.

### Derive the optimal uncertainty-aware instance weight

We expect to find the optimal weight \(_{x,a}\) to make the empirical estimator \(_{}(_{})\) as accurate as possible, taking into account the uncertainty in estimated logging probabilities. Intuitively, a high accuracy of the estimator is crucial for determining the correct direction of policy learning. We follow previous work [36; 29] and measure the mean squared error (MSE) of \(_{}(_{})\) to the ground-truth policy value \(V(_{})\), which captures both the bias and variance of an estimator. A lower MSE indicates a more accurate estimator.

In UIPS, instead of directly minimizing the MSE, which is intractable, we find \(_{,a}\) to minimize the upper bound of MSE. As we show later, the optimal \(_{,a}\) has a closed-form solution which relates to both the value of \(_{}(a|)/(a|)\) and the estimation uncertainty of \((a|)\).

**Theorem 3.1**.: _The mean squared error (\(\)) between \(_{}(_{})\) and ground-truth estimator \(V(_{})\) is upper bounded as follows:_

\[(_{}(_{}))=_{D}[(_{}(_{})-V(_{ }))^{2}]=(_{}(_{}))^{2}+(_{}(_{}))\]

As the first expectation term \(_{_{}}[r_{,a}^{2}}(a| )}{^{*}(a|)}]\) is a non-negative constant, we denote it as \([0,)\) when searching for \(_{,a}\). To minimize this upper bound of MSE, the optimal \(_{,a}\) for each sample \((,a)\) should minimize the following,

\[((a|)}{(a|)}_{,a} -1)^{2}+}(a|)^{2}}{(a|)^{2 }}^{2}_{,a}. \]

An interesting observation is that setting \(_{,a}=(a|)}{^{*}(a|)}\), i.e., turning \()}{(a|)}_{,a}\) into \()}{^{*}(a|)}\) does not result in the optimal solution of Eq.(6). This is because such a setting only reduces bias (i.e., the first term of Eq.(6)), but fails to control the second term, which is related to the variance. Moreover, we cannot directly minimize Eq.(6) due to the unknown \(^{*}(a|)\). But it is possible to obtain a confidence interval which contains \(^{*}(a|)\) with a high probability, when \((a|)\) is obtained via a specific estimator, e.g., (generalized) linear model or kernel methods.

Following previous work [23; 16; 22], we adopt the realizable assumption that \(^{*}(a|)\) can be represented by a softmax modified upper a parametric function \(f_{^{*}}(,a)\). Moreover, the universal approximation theorem  states that a parametric function with sufficient capacity, when combined with a softmax function, can approximate any distribution. Then we have:

\[^{*}(a|)(f_{^{*}}(,a)),(a| {x})(f_{}(,a)), \]

where \(f_{}(,a)\) is an estimate of \(f_{^{*}}(,a)\). Following the conventional definition of confidence interval , we define \(\) and \(U_{,a}\) such that \(|f_{^{*}}(,a)-f_{}(,a)| U_{,a}\) holds with probability at least 1-\(\), where \(\) is a function of \(\) (typically the smaller \(\) is, the larger \(\) is). Then \( U_{,a}\) measures the width of confidence interval of \(f_{}(,a)\) against its ground-truth \(f_{^{*}}(,a)\). As derived in Appendix 7.2, with probability at least 1-\(\), we have \(^{*}(a|)_{,a}\) and

\[_{,a}=[,a})}}{Z^{*}}(a|),,a})}}{Z^{*}}(a| )],\]

where \(Z^{*}=_{a^{}}(f_{^{*}}(a^{}|))\) and \(=_{a^{}}(f_{}(a^{}|))\).

As \(^{*}(a|)\) can be any value in \(_{,a}\) with high probability, we aim to find the optimal \(_{,a}\) that minimizes the worst case of Eq.(6), thereby ensuring that \(_{}(_{})\) approaches its ground-truth \(V(_{})\) under the sense of MSE, even in the worst possible scenarios. This ensures the subsequent policy improvement direction will not be much worse with high probability. Thus, we formulate the following optimization problem:

\[_{_{,a}}_{_{,a}_{,a}}( ,a}}{(a|)}_{,a}-1)^{2}+ }(a|)^{2}}{(a|)^{2}}_{,a} ^{2}. \]

The following theorem derives a closed-form formula for the optimal solution of Eq.(8).

**Theorem 3.2**.: _Let \([(- U_{}^{}),( U_{}^{ })]\), where \(U_{}^{}=_{a}U_{,a}\). The optimization problem in Eq.(8) has a closed-form solution:_

\[_{,a}^{*}=(/-  U_{,a}+}(a|)^{2}}{(a|)^{2},a})}},2/  U_{,a}+- U_{,a} ).\]

The following corollary demonstrates the advantage of UIPS. The detailed proof of Theorem 3.2 and Corollary 3.3 can be found in Appendix 7.8.

**Corollary 3.3**.: _With \(_{,a}^{*}\) derived in Theorem 3.2, \(_{}(_{})\) in Eq.(4) achieves a smaller upper bound of MSE than \(_{}(_{})\) in Eq. (3)._

**Insights about \(_{,a}^{*}\).** The detailed analysis of the effect of \(_{,a}^{*}\) can be found in Lemma 7.1 in Appendix 7.8. In summary, we have the following key findings,

* For samples whose largest possible propensity score is under control: i.e., \(}(a|)}{ B_{,a}}<\), higher uncertainty implies smaller values of \(/\). This suggests samples of this type with positive rewards are underestimated, and the extend of underestimation increases with the estimation uncertainty. UIPS thus chooses to increase \(_{,a}^{*}\) with uncertainty, to emphasize these long-tail positive samples.
* Conversely, for samples with large propensity scores, UIPS decreases \(_{,a}^{*}\) as the uncertainty increases, so as to prevent their distortion in policy learning.

**Uncertainty estimation.** Now we describe how to calculate \(U_{,a}\), i.e., the uncertainty of the estimated \((a|)\). In this work, we choose to estimate \(^{*}(a|)\) using a neural network, because 1) its representation learning capacity has been proved in numerous studies, and 2) various ways [11; 45] can be leveraged to perform the uncertainty estimation in a neural network. We adopt  due to its computational efficiency and theoretical soundness. Following the proof of Theorem 4.4 in , given the logged dataset \(D\), we can get with a high probability that there exists \(\) such that:

\[|f_{}(_{n},a_{n})-f_{^{*}}(_{n},a_{n}))| _{n},a_{n})^{}M_{D}^{-1}(_{n},a_{n})}\]

where \((_{n},a_{n})\) is the gradient of \(f_{}(_{n},a_{n})\) with respect to the neural network's last layer's parameter \(_{w}\), i.e., \((_{n},a_{n})=_{_{w}}f_{}(_{n},a _{n})\). And \(M_{D}=_{n=1}^{N}(_{n},a_{n})(_{n},a_{n})^{}\), implying \(U_{_{n},a_{n}}=_{n},a_{n})^{}M_{D}^{-1}(_{n },a_{n})}\).

### Convergence of policy learning under UIPS

The following theorem provides the convergence result for UIPS, which converges to a stationary point of the expected reward function. The proof is provided in Appendix 7.9.

**Theorem 3.4**.: _Denote \(G_{}\) and \(\) as the maximum value of \(\|(a|)}{}\|\) and \(_{^{*}}[^{*}(a|)}{^{2}(a| )}(_{,a}^{*})^{2}]\) respectively, i.e., \(\|(a|)}{}\| G_{}\) and \(_{^{*}}[^{*}(a|)}{^{2}(a| )}(_{,a}^{*})^{2}]\). And denote \(V_{}\) as the finite maximum expected reward that can be achieved, and \(_{}=_{,a}\{|(a|)}{(a| )}_{,a}^{*}-1|\}\). Assume that the expected reward of \(_{}\), i.e., \(V(_{})\), is a differentiable and L-smooth function w.r.t \(\). Denote the policy parameters obtained by Eq.(5) at iteration \(k[K]\) as \(_{k}\), then \(_{}(0,1)\) and_

\[_{k=1}^{K}[\| V(_{_{k}})\|]^{2} }{K(1-_{})}+(L+}{(1-_{ })})}{},\]

_where \( V(_{})\) is the true policy gradient under ground-truth logging probability, i.e., \( V(_{})=E_{^{*}}[(a|)}{^{* }(a|)}r_{,a}_{}(_{}(a|))]\)._

Theorem 3.4 shows that, as \(K\) and with \(1/(1-_{})\) and \(\) being controlled, UIPS leads policy update to converge to a stationary point where the true policy gradient \( V(_{_{k}})\) is zero. And fortunately, UIPS is effective in controlling both \(1/(1-_{})\) and \(\). Specifically, we denote \(_{,a}=|(a|)}{(a|)}_{,a}^{*}-1|\) and \(_{x,a}=^{*}(a|)}{^{2}(a|)}(_{,a}^ {*})^{2}\). It is clear to note that \(_{x,a}^{2}+_{x,a}\) corresponds to the objective in Eq.(6) for deriving \(_{,a}^{*}\) for each sample \((,a)\). In other words, UIPS selects \(\{_{,a}^{*}\}\) to minimize \(_{}=\{_{x,a}\}\) and \(=_{^{*}}[_{,a}]\), which directly accelerate the policy converge to a stationary point with the true policy gradient being zero.

In the case of BIPS in Eq.(3), we have \(_{,a} 1\). Although \(\) may be large due to small logging probabilities, the more concerning issue is that the requirement \(_{}(0,1)\) is no longer satisfied when \(^{*}(a|) 2(a|)\), which may happen with a non-negligible probability. Hence, the convergence of policy learning under \(_{}\) is no better than that under UIPS.

## 4 Empirical evaluations

We evaluate UIPS on both synthetic data and three real-world datasets with unbiased collection. We compare UIPS with the following baselines, which can be grouped into five categories:

* **Cross-Entropy (CE)**: A supervised learning method with the cross-entropy loss over its softmax output. No off-policy correction is performed in this method.
* **BIPS-Cap**: The off-policy learning solution under the BIPS estimator in Eq.(3). The estimated propensity scores are further suppressed to control variance, i.e., taking \((c,}(a|)}{(a|)})\) as the propensity score. Setting \(c\) to a small value can reduce variance, but introduces bias.
* **MinVar & stableVar**, **Shrinkage**: This line of work improves off-policy evaluation by reweighing each sample. For example, MinVar and stableVar reweigh each sample by \(,a}}{_{^{}}h_{,a^{}}}\) with \(h_{,a}=(a|)}{_{}(a|)}\) and \(h_{,a}=(a|)}}{_{}(a|)}\) respectively, since they find that \(_{}(a|)^{2}/(a|)\) is directly related to policy evaluation variance. Su et al.  propose to shrink the propensity score by \(/(+}(a|)^{2}}{(a|)^{2}})\), which is a special case of our UIPS with \(U_{,a}=0\) and \(=1\). All these methods simply treat \((a|)\) as \(^{*}(a|)\), and none of them consider the uncertainty of \((a|)\).
* **SNIPS**, **BanditNet**, **POEM**, **POXM**, **Adaptive**: This line of work aims for more stable and accurate policy learning. For example, SNIPS normalizes the estimator by the sum of propensity scores in each batch. BanditNet extends SNIPS and leverages an additional Lagrangian term to normalize the estimator by an approximated sum of propensity scores of all samples. POEM jointly optimizes the estimator and its variance. POXM controls estimation variance by pruning samples with small logging probabilities. Adaptive proposes a new formulation to utilize negative samples.
* **ApproxKNN** and **IPS-C-TS**: The line of work improves off-policy learning by applying calibration to estimated logging probabilities. ApproxKNN utilizes the K-Nearest Neighbor algorithm for calibration, which exhibits the lowest calibration error in . IPS-C-TS, on the other hand, employs temperature scaling, a widely recognized and effective calibration method for probability distribution .

[MISSING_PAGE_FAIL:7]

logging probabilities, IPS-GT still suffered from high variance caused by samples with small logging probabilities, which is the main cause of its worse performance when \(=0.5\) and \(=1\). In contrast, UIPS effectively controlled the negative impact of these high-variance samples, resulting in a better bias-variance trade-off.

With an increasing \(\), suggesting a decrease in the probability of selecting positive actions, most algorithms experienced a drop in performance. However, UIPS consistently outperformed all other algorithms across all three datasets and metrics. Interestingly, as \(\) decreases, the performance improvement of UIPS became even more pronounced, despite SNIPS, BanditNet, and POXM being designed to handle small logging probabilities of positive actions.

ApproxKNN and IPS-C-TS generally achieved better performance than BIPS-Cap, implying the effectiveness of calibration of estimated logging probabilities. However, UIPS still consistently outperformed both ApproxKNN and IPS-C-TS. The main reason is that calibration primarily focuses on adjusting the estimated probabilities to ensure on _average_ the model's predictions are reliable and accurate. In contrast, UIPS specifically handles the impact from each _individual_ sample in policy learning.

UIPS also consistently outperformed Shrinkage (a special case of UIPS with uncertainties always being zero) on all three datasets, demonstrating the benefits of considering the estimation uncertainty. Finally, blindly reweighing through uncertainties, regardless of their impact on the accuracy of the resulting estimator and the learned policy, ultimately resulted in poor performance, as demonstrated by UIPS-P and UIPS-O.

**Performance under different uncertainty levels.** As shown in Figure 1, low-frequency samples in the logged dataset suffer higher uncertainties in their propensity estimation. Thus, we divided the test set into two subsets according to the average frequency of associated actions, where the uncertainty in the subset associated with low-frequency actions is on average 8% higher than that in high-frequency actions. Table 2 shows the results on these two subsets when \(=0.5\). In addition, we include the results of the top three baselines that directly utilize the estimated logging policy. Table 2 clearly demonstrates that only UIPS performed better than CE on the test set with low-frequency actions, implying the distortion of inaccurately estimated logging probabilities and the effectiveness of UIPS in efficiently handling them.

**Off-policy Evaluation.** We further inspected whether \(_{}\) in Eq.(4) leads to more accurate off-policy evaluation. Following previous work [29; 46; 36], we evaluated the following \(\)-greedy policy: \((a|)=|}\{a M_{x}\}+ /||\), where \(M_{x}\) contains all positive actions associated with instance \(\). For each \(\) in the test set, we randomly sample 100 actions following the logging policy in Eq.(9) to generate the logged dataset. Table 3 shows the MSE of the estimators to the ground-truth policy value under 20 different random seeds. From Table 3, one can observe that: 1) IPS-GT with a skewer logging policy (i.e., smaller \(\)) leads to higher MSE, consistent with previous findings [29; 46; 36]; 2) inaccurate logging probabilities result in high bias and variance, leading to much larger MSE of BIPS compared to IPS-GT. Furthermore, this distortion is particularly pronounced when the ground-truth logging policy is skewed (\(=0.5\)); and 3) although all using the estimated logging policy, \(_{}\) yields the smallest MSE, comparing to other baselines that are designed to improve over BIPS.

**Hyper-parameter Tuning.** Discussions about hyper-parameter tuning and performance of UIPS under different hyper-parameters can also be found in Appendix 7.3.1.

### Real-world data

To demonstrate the effectiveness of UIPS in real-world scenarios, we evaluate it on three recommendation datasets: (1) Yahoo! R31; (2) Coat2; (3) KuaiRec , for music, fashion and short-video recommendations respectively. All these datasets contain an unbiased test set collected from a randomized controlled trial where items are randomly selected. The statistics of the three datasets and implementation details, e.g., model architectures and dataset splits, can be found in Appendix 7.3.2.

Following , we take \(K=5\) on Yahoo! R3 and Coat datasets, and \(K=50\) on KuaiRec dataset. The \(p\)-value under the t-test between UIPS and the best baseline on each dataset is also reported to investigate the significance of improvement.

[MISSING_PAGE_FAIL:9]

However, all these solutions directly use the estimated logging policy for off-policy correction, leading to sub-optimal performance as shown in our experiments. A recent study on causal recommendation  also argues that propensity scores may not be correct due to unobserved confounders. They assume the effect of unobserved confounder for any sample can be bounded by a pre-defined hyperparameter, and adversarially search for the worst-case propensity for learning. Mapping to off-policy learning, their solution is a special case of our UIPS-O variant with uncertainty as a pre-defined constant.

There were existing studies [34; 21; 26; 47; 9] also explore direct estimation of the propensity ratio to bypass estimating the logging policy. However, as discussed in Appendix 7.6 and Appendix 7.5, they demonstrate inferior performance compared to UIPS. This is primarily due to either the lack of consideration for the accuracy of the estimated propensity ratio, similar to the limitations of existing IPS-type algorithms in handling inaccurately estimated logging probabilities, or the degeneration to a specific IPS estimator that suffers high variance.

Recent work on distributionally robust off-policy evaluation and learning [32; 17; 44] also addresses uncertainty in off-policy learning. However, their approach to handling uncertainty and the underlying motivation differ significantly from ours, resulting in distinct techniques employed. Further details can be found in Appendix 7.7. Additionally, experiments conducted in Appendix 7.7 demonstrate that directly adapting methods from distributionally robust off-policy learning to handle inaccurately estimated logging probabilities leads to poor performance.

Off-policy learning can also be directly built on off-policy evaluation. Several work [36; 46] also propose to control the variance of the estimator caused by small logging probabilities through instance reweighing. Again, they directly use the estimated logging policy for correction, and thus performed worse than UIPS as observed in our experiments. A recent study  assumed additional structure in the action space and proposed the marginalized IPS. Instead, our work considers the uncertainty in the estimated logging policy and thus does not add any new assumptions about the problem space.

**Uncertainty-aware learning.** Estimation uncertainty has been extensively studied [45; 50; 1]. In the context of on-policy reinforcement leanring and bandits [1; 48; 49], the use of uncertainty aims to strike a balance between exploration and exploitation by adopting an optimistic approach (i.e., UCB in bandits). One the other hand, most research on offline reinforcement learning/bandits [43; 4; 6] tends to be more conservative, employing techniques such as Lower Confidence Bounds (LCB) or penalizing out-of-distribution states and actions based on uncertainty to address extrapolation errors. However, these principles differ fundamentally from UIPS, which directly minimizes the mean square error of off-policy evaluation. The closed-form solution of the resulting per-instance weight in UIPS reflects how uncertainty contributes to the policy evaluation error. Moreover, Our UIPS-O and UIPS-P baselines leverage uncertainties using the two aforementioned general principles respectively. However, empirical findings indicate that blindly penalizing or boosting samples based on uncertainty is problematic. Proper correction depends on both uncertainty in logging policy estimation and the actual value of estimated logging probabilities.

## 6 Conclusion

In this paper, we propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) to explicitly model the uncertainty of the estimated logging policy for improved off-policy learning. UIPS weighs each logged instance to reduce its policy evaluation error, where the optimal weights have a closed-form solution derived by minimizing the upper bound of the resulting estimator's mean squared error (MSE) to its ground-truth value. An improved policy is then obtained by optimizing the resulting estimation. Extensive experiments on synthetic and three real-world datasets as well as the theoretical convergence guarantee demonstrate the efficiency of UIPS.

As demonstrated in this work, explicitly modeling the uncertainty of the estimated logging policy is crucial for effective off-policy learning; but the best use of this uncertainty is not to simply down-weigh or drop instances with uncertain estimations, but to balance it with the actually estimated logging probabilities in a per-instance basis. As our future work, it is promising to investigate how UIPS can be extended to value-based learning methods, e.g., actor-critics. And on the other hand, it is also important to analyze how tight our upper bound analysis of policy evaluation error is; and if possible, find new ways to tighten it for improvements.