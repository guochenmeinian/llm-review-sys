# Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy

Cameron Allen

UC Berkeley

These authors contributed equally and are ordered alphabetically. Please send any correspondence to camallen@berkeley.edu, aaron_kirtland@brown.edu, and ruoyutao@brown.edu.

&Aaron Kirtland

Brown University

&Ruo Yu Tao

Brown University

&Sam Lobel Brown University

&Daniel Scott Georgia Tech

&Nicholas Petrocelli Brown University

&Omer Gottesman Amazon

&Ronald Parr

Duke University

&Michael L. Littman Brown University

&George Konidaris Brown University

&

###### Abstract

Reinforcement learning algorithms typically rely on the assumption that the environment dynamics and value function can be expressed in terms of a Markovian state representation. However, when state information is only partially observable, how can an agent learn such a state representation, and how can it detect when it has found one? We introduce a metric that can accomplish both objectives, without requiring access to--or knowledge of--an underlying, unobservable state space. Our metric, the \(\)-discrepancy, is the difference between two distinct temporal difference (TD) value estimates, each computed using TD(\(\)) with a different value of \(\). Since TD(\(0\)) makes an implicit Markov assumption and TD(\(1\)) does not, a discrepancy between these estimates is a potential indicator of a non-Markovian state representation. Indeed, we prove that the \(\)-discrepancy is exactly zero for all Markov decision processes and almost always non-zero for a broad class of partially observable environments. We also demonstrate empirically that, once detected, minimizing the \(\)-discrepancy can help with learning a memory function to mitigate the corresponding partial observability. We then train a reinforcement learning agent that simultaneously constructs two recurrent value networks with different \(\) parameters and minimizes the difference between them as an auxiliary loss. The approach scales to challenging partially observable domains, where the resulting agent frequently performs significantly better (and never performs worse) than a baseline recurrent agent with only a single value network.

## 1 Introduction

The dominant modeling frameworks for reinforcement learning (Sutton and Barto, 2018) define environments in terms of an underlying Markovian state representation. This modeling choice, called the _Markov assumption_, is nearly ubiquitous in reinforcement learning, because it allows environment dynamics, rewards, value functions, and policies all to be expressed as functions that are independent of the past given the most recent state. In principle, an environment can be modeled as either a Markov decision process (MDP) (Puterman, 1994), or its _partially observable_ counterpart, a POMDP[Kaelbling et al., 1998], as long as the underlying state representation contains enough information to satisfy the Markov assumption. The POMDP framework is more general, but POMDPs are typically much harder to solve than MDPs [Zhang et al., 2012], so it is important to know when it is appropriate to use the simpler MDP framework.

Ideally, a system designer can ensure that their reinforcement learning agent is configured to use the appropriate problem model. If an environment is partially observable, the designer may manually add relevant decision-making features to the agent's state representation, either by concatenating observations together [Mnih et al., 2015] or by using other types of feature engineering [Bellemare et al., 2020, Galataud, 2021, Tao et al., 2023]. Alternatively, the designer can manually specify a set of possible environment states over which the agent can maintain a Markovian belief distribution [Kaelbling et al., 1998]. The challenge is that it is not always obvious when the designer has supplied sufficient information to satisfy the Markov assumption. These approaches require the person deploying the agent to know details about the very problem the agent is supposed to help them solve.

An alternative approach--that we explore in this paper--is to let the agent _learn_ a good state representation for the problem it is solving. The conventional deep-learning-era wisdom is that the best problem representations come from training a system to solve a task, rather than from human designers. When faced with a potentially partially observable environment, if we provide the agent with a large enough recurrent neural network (RNN), it can perhaps discover an internal state representation "end-to-end" that maximizes reward via gradient descent [Bakker, 2001, Hausknecht and Stone, 2015, Ni et al., 2022, Dong et al., 2022]. Indeed, end-to-end RNN architectures work well for many problems and are both general-purpose and scalable. However, these techniques only implicitly address the problem of learning a Markovian state representation; in this paper, we show that we can achieve much better learning performance when we explicitly tackle the problem of partial observability.

In our approach, the agent learns a state representation by directly minimizing a metric that assesses whether the environment is fully or partially observable. We call our metric the _\(\)-discrepancy_, and define it as the difference between two distinct value functions estimated using temporal difference learning (TD) [Sutton, 1988]. Specifically, the TD(\(\)) method defines a smooth trade-off between one-step TD (i.e., \(=0\)), which makes an implicit Markov assumption, and Monte Carlo (MC) estimation (\(=1\)), which does not, and intermediate \(\) values interpolate between these extremes. By comparing value estimates for two distinct values of \(\), we can check that the agent's observations support Markovian value prediction, and augment them with memory if we find they are incomplete.

Our main contributions4 are as follows:

1. We introduce and formally define the \(\)-discrepancy and prove that it is exactly zero for MDPs and almost always non-zero for a broad class of POMDPs that we characterize. This analysis tells us that our metric reliably _detects_ partial observability.
2. We then consider a tabular, proof-of-concept experiment that adjusts the parameters of a memory function to minimize the \(\)-discrepancy via gradient descent. For this experiment, we compute the \(\)-discrepancy in closed form. Our results demonstrate that minimizing the \(\)-discrepancy is a viable path to _mitigating_ partial observability.
3. Finally, we integrate our approach into a deep reinforcement learning agent and evaluate on a set of large and challenging POMDP benchmarks. We find that minimizing the \(\)-discrepancy between two learned value functions, as an auxiliary loss alongside traditional reinforcement learning, is often significantly more effective (and never worse) than training the same agent with just a single value function.

Overall, we find that the \(\)-discrepancy is a simple yet powerful metric for detecting and mitigating partial observability. The metric is also practical, since it can be computed directly from value functions, which are ubiquitous in reinforcement learning. Furthermore, such value functions need only consider observable parts of the environment, so the \(\)-discrepancy remains applicable even without the common assumption that the agent knows the full set of possible POMDP states.

## 2 Background

We consider two frameworks for modeling sequential decision processes: MDPs and POMDPs. The MDP framework (Puterman, 1994) consists of a state space \(\), action space \(\), reward function \(R:\), transition function \(T:\) mapping to a distribution over states, discount factor \(\), and initial state distribution \(p_{0}\). The agent's goal is to find a policy \(_{}:\) that selects actions to maximize _return_, \(g_{t}\), the discounted sum of future rewards starting from timestep \(t\): \(g_{t}^{_{}}=_{i=0}^{}^{i}r_{t+i}\), where \(r_{i}\) is the reward at timestep \(i\). We denote the expectation of these returns as _value functions_\(V_{_{}}(s)=_{_{}}[g_{t} s_{t}=s]\) and \(Q_{_{}}(s,a)=_{_{}}[g_{t} s_{t}=s,a _{t}=a]\).

The POMDP framework (Kaelbling et al., 1998) additionally includes a set of observations \(\) and an observation function \(:\) that describes the probability \((|s)\) of seeing observation \(\) in latent state \(s\). POMDPs are a more general model of the world, since they contain MDPs as a special case: namely, when observations have a one-to-one correspondence with states. Similarly, POMDPs where states correspond to disjoint sets of observations are called _block MDPs_(Du et al., 2019). However, such examples are rare; in typical POMDPs, a single observation \(\) does not contain enough information to fully resolve the state \(s\). While agents need not _fully_ resolve the underlying state to behave optimally, they must retain at least enough information across timesteps that the optimal policy becomes clear.

We are interested in the learning setting, where the agent has no knowledge of the underlying state \(s\) nor even the set of possible states \(\) (let alone the transition, reward, and observation functions). It receives an observation \(_{t}\) at each timestep and must find a way to maximize expected return. One way to do this is to construct a _state representation_, perhaps using some form of memory, on which it can condition its behavior. A state representation \(z Z\) is _Markovian_ if at any timestep \(t\), the representation \(z_{t}\) and action \(a_{t}\) together are a sufficient statistic for predicting the reward \(r_{t}\) and next representation \(z_{t+1}\), instead of requiring the agent's whole history:

\[(z_{t+1},r_{t}|z_{t},a_{t})=(z_{t+1},r_{t}|z_{t},a_{t},,z_{0},a_{0}).\] (1)

The definition can be applied to states (\(s\)), observations (\(\)), and even memory-augmented observations (defined in Sec. 4), by replacing \(z\) with the relevant quantity. States and observations are equivalent in MDPs, and this property is satisfied for both by definition, but in POMDPs it typically only holds for the underlying, unobserved state \(s\)--not the observations.

Markovian state representations have several desirable implications. First, if the Markov property holds then so does the Bellman equation: \(V_{_{}}(s)=_{a}_{}(a s) R(s,a)+_{s^{}}T(s^{} s,a)V_{ _{}}(s^{})\). The Bellman equation allows agents to estimate the value of a policy, from experiences, and without knowing \(T\) or \(R\), via a recurrence relation over one-step returns,

\[V_{_{}}^{(i+1)}(s)=_{_{}}[r_{t}+  V_{_{}}^{(i)}(s_{t+1}) s_{t}=s],\] (2)

which converges to the unique fixed point \(V_{_{}}\). A second implication is that the transition and reward functions, and consequently the value functions \(V_{_{}}\) and \(Q_{_{}}\), have fixed-sized inputs and are therefore easy to parameterize, learn, and reuse. Finally, it follows from the Markov property that the optimal policy \(_{}^{}\) can be expressed deterministically and does not require memory (Puterman, 1994).

We can unroll the Bellman equation over multiple timesteps to obtain a similar estimator that uses \(n\)-step returns: \(V_{_{}}(s)=_{_{}}[g_{t:t+n} s_{t }=s]\), where \(g_{t:t+n}:=r_{t}+ r_{t+1}+^{2}r_{t+2}++^{n}V_{_{ }}(s_{t+n})\), with \(g_{t:t+n}:=g_{t}\) if the episode terminates before \(t+n\) has been reached. The same process works for weighted combinations of such returns, including the exponential average:

\[V_{_{}}^{}(s)=_{_{}}(1- )_{n=1}^{}^{n-1}g_{t:t+n}s_{t}=s,\] (3)

with \(V_{_{}}^{=1}(s)=_{_{}}[g_{t} s _{t}=s]\) as a special case. Equation (3) defines the TD(\(\)) value function as an expectation over the so-called \(\)-return (Sutton, 1988). Low values of \(\) give more weight to shorter returns where the value term \(V_{_{}}(s_{t+n})\) is discounted less (and thus has greater significance), whereas larger \(\) values put more weight on longer returns where the value term is more heavily discounted. Given an MDP and a fixed policy, the recurrence relations for all TD(\(\)) value functions share the same fixed point for any \(\); however, if the Markov property does not hold, different \(\) may have different TD(\(\)) fixed points. In this work, we seek to characterize this phenomenon and leverage it for detecting and mitigating partial observability.

## 3 Detecting Partial Observability

Before introducing our partial observability metric, let us first consider the T-maze example of Figure 1 under our two candidate decision-making frameworks: MDPs and POMDPs. In the T-maze, the agent only observes the color of its current square, and must remember the start square color (sampled uniformly from blue/red) to select the action at the junction that leads to the positive reward.

To model the T-maze as a POMDP, the state must include the agent's position and the goal location. The transitions are deterministic; each action moves the agent in the corresponding direction. The goal location is sampled at the start of an episode, and defines both the reward function and the observation for the initial square. The observation function uniquely identifies these starting states, but all corridor states are aliased together (they map to the same observation), as are the two junction states.

We can convert our POMDP model into an MDP model by using observations \(\) as the MDP's "states". This requires new transition and reward functions, \(T_{}\) and \(R_{}\), which we define as: \(T_{}(^{},a):=_{s,s^{}} (^{} s^{})T_{}(s^{} s,a)(s )\) and \(R_{}(,a):=_{s}R_{}(s,a)(s)\), where \((s)\) is policy-dependent and describes how each hidden state \(s_{i}\) contributes to the overall environment behavior when we see observation \(\). While there are several sensible choices of weighting scheme for this type of state aggregation (uniform over states, relative frequency, etc.), only one of these choices coincides with the distribution \((s)\). That particular weighting scheme is the one that averages the time-dependent \((s_{t}_{t})\) over all timesteps, weighted by visitation probability under the agent's policy, and discounted by \(\).5 We explain how to construct this expression in Appendix C. We call the MDP defined in this way the _effective MDP model_ for a given POMDP.

The effective MDP model marginalizes over histories (and POMDP hidden states). For example, the model predicts that going up from the junction will reach the goal exactly half the time, because the transition dynamics marginalize over (equally likely) starting observations. Note that the POMDP model does no such averaging: if the agent initially observes blue, then going up from the junction will _always_ reach the goal, but down never will. Thus, we see that, for the T-maze, there is a mismatch between the POMDP model and the effective MDP model. In other words, the POMDP's hidden states \(\) are a Markovian representation, but its observations \(\) are not, despite the fact that the effective MDP model treats them as Markovian "states".

In principle, an agent could measure partial observability by simultaneously modeling its environment as both a POMDP and an MDP, and comparing the models' predictions. Unfortunately, since the agent lacks any information about the unobserved state space \(\), the POMDP model would require variable-length history inputs at each time step, and would come with computational and memory costs that grow exponentially with history length. Instead, we propose a model-free approach, using value functions conditioned only on observations, to approximate this comparison in a tractable way.

### Value Function Estimation under Partial Observability

The Bellman equation and its sample-based recurrence relations (2) and (3) are defined for Markovian states. If we apply them to the observations of a POMDP, we are actually working with the effective MDP model of that POMDP, instead of the POMDP itself. To see this, consider one-step TD (\(=0\)),where we use the same recurrence relation (2) but now our expectation is sampling from the POMDP:

\[V^{=0}_{}() =_{s}(s)_{a}(a )R_{S}(s,a)+_{s^{}}_{^ {}}(^{} s^{})T_{}(s^{ } a,s)V^{=0}_{}(^{})\] \[=_{a}(a)R_{}(,a )+_{^{}}T_{}(^{} a, )V^{=0}_{}(^{}),\] (4)

where we have suppressed \(V_{}\)'s dependence on the observation-based policy \(\) for ease of notation.

We see from Equation (4) that the value function TD computes for a POMDP is the fixed point of the Bellman operator for the effective MDP model.6 By contrast, the Monte Carlo value estimator (\(=1\)) does not exploit the Bellman equation at all; it simply averages over returns: \(V^{=1}_{}(s)=_{}[g_{t} s_{t}=s]\). Translating the Monte Carlo estimator to POMDPs merely requires one additional expectation to convert from states to observations:

\[V^{=1}_{}()=_{}[g_{t}_{t}= ]=_{s}(s)\;_{}[g _{t} s_{t}=s]=_{s}(s)V^{=1}_{ }(s).\] (5)

This means MC (\(=1\)) effectively takes the hidden-state value function \(V_{}\) of the POMDP and projects it into observation space, whereas TD (\(=0\)) directly computes the value function for the projected model as an MDP, by treating observations as states. Interpolating \(\) between 0 and 1 smoothly varies the value estimate's reliance on the effective MDP model.

For generality, we derive an expression (see Appendix C) for \(Q^{}_{}\)-values in terms of a given \(\) parameter (expressed in tensor notation for compactness) to reveal how \(\) trades off between the projected state value function and the value function of the projected model:

\[Q^{}_{}=W(I- TK^{}_{})^{-1} R^{}, K^{}_{}=^{}+(1-) W^{},\] (6)

where the tensor product : contracts two indices instead of one,7 with tensors defined as follows:

  \(Q^{}_{}\) (\(\)) is a matrix of \(Q\)-values; \\ \(W\) (\(\)) contains the state-blending weights given by \((s)\) for observation \(\); \\ \(I\) (\(\)) is an identity tensor with \(I_{sas^{}a^{}}=[s=s^{}][a=a^{}]\); \\ \(T\) (\(\)) contains the hidden-state transition probabilities \(T(s^{} a,s)\); \\ \(^{}\) (\(\)) contains the policy spread over hidden states (see below); \\ \(\) (\(\)) is the observation function, containing probabilities \(( s)\); \\ \(W^{}\) (\(\)) combines \(W\) with \(^{}\) to obtain probabilities \((s,a)\); \\ \(R^{}\) (\(\)) contains the hidden-state rewards \(R(s,a)\). \\  

Let us take a moment to parse this equation. First, \(^{}\) and \( W^{}\) are mappings from states to state-action pairs. We call the former the _MC policy spread_, \(^{}_{s,s^{},a}=[s=s^{}]_{}_ {s,}_{,a}\), which maps states to the expected policy under their observation distribution. We call the latter the _TD policy spread_, \(( W^{})_{s,s^{},a}=_{}_{s,}_{,a}W_{ ,s^{}}\), which reallocates the policy probabilities for a given observation \(\) across all states \(s^{}\) that produce that observation, weighted by \(W\). \(K^{}_{}\) is a convex combination of these two policy spread tensors, parameterized by \(\). Multiplying a policy spread tensor on the left by \(T\) produces an (\(\)) transition-policy tensor, describing the probability of each state-action transition \((s,a)(s^{},a^{})\) under the policy. Intuitively, Equation (6) says that TD(\(\)) computes Q-values for a POMDP as though state-action pairs are evolving according to \(TK^{}_{}= T^{}+(1-)T W^{}\), which is a mixture of the policy dynamics under two transition models: the MC transition-policy (\(T^{}\)) and the TD transition-policy (\(T W^{}\)). The expression \((I- TK^{}_{})^{-1} R\) computes the state-space \(Q\)-values for this hybrid transition model. Finally, these are projected through \(W\) to compute the observation-space \(Q\)-values. (See Appendix C for more details.)

### \(\)-Discrepancy

We have shown that, under partial observability, \(Q^{}_{}\) value functions may differ for different \(\) parameters, due to varying reliance on the effective MDP model in the TD(\(\)) estimator. We call this difference the \(\)_-discrepancy_, and we propose to use it as a measure of partial observability.

**Definition 1**: _For a given POMDP model \(\) and policy \(\), the \(\)-discrepancy \(_{,}^{_{1},_{2}}\) is the weighted norm of the difference between the \(Q\)-functions estimated by TD(\(\)) for \(\{_{1},_{2}\}\):_

\[_{,}^{_{1},_{2}}:=Q_{}^{_ {1}}-Q_{}^{_{2}}=W(I- TK_{}^{ _{1}})^{-1}-(I- TK_{}^{_{2}})^{-1}  R^{}.\]

The choice of norm can be arbitrary, as can the norm weighting scheme, as long as it assigns positive weight to all reachable observation-action pairs. We discuss choices of weighted norm in Appendix G.2. For brevity, we suppress the \(\) subscript when the POMDP model is clear from context.

A useful property (that we will prove in Theorem 2) is that if the POMDP has Markovian observations, the \(\)-discrepancy is exactly zero for all policies. However, for it to be a useful measure of partial observability, we must also show that the \(\)-discrepancy is reliably non-zero when observations are non-Markovian. For this, we have the following theorem.

**Theorem 1**: _Given a POMDP model \(\) and distinct \(^{}\), if there exists a policy \(:\) such that \(_{,}^{,^{}} 0\), then \(_{,}^{,^{}} 0\) for all policies except at most a set of measure zero._

_Proof sketch:_ We formulate the \(\)-discrepancy of a given policy as the norm of an analytic function, then use the fact that analytic functions are zero everywhere or almost nowhere, along with the fact that norms preserve this property. The full proof is given in Appendix D.

Intuitively, Theorem 1 says that the \(\)-discrepancy can detect non-Markovian observations. If it is possible to reveal that a POMDP's observation-space value function does not match that of its effective MDP model, then almost all policies will do so. The theorem further suggests that even if a particular policy has zero \(\)-discrepancy, small perturbations to that policy will almost surely detect partial observability if it is present.

We also find that increasing amounts of partial observability lead to larger \(\)-discrepancy. In Figure 3 we interpolate between Markovian and aliased T-Maze environments, and consider three types of state aliasing. Here we use a fixed policy that goes right until the junction, then up with probability \(2/3\) and down w.p. \(1/3\). To avoid artifacts due to interactions between the discount factor, weighted norm, and changing observation function, we set \( 1\) and use the (policy-weighted) maximum norm (see Appendix G.2).8 The results confirm that the \(\)-discrepancy is a useful indicator of partial observability, provided that it is non-zero for at least one policy.

### What conditions cause the \(\)-discrepancy to be zero?

We can characterize the cases in which the \(\)-discrepancy is zero for _all_ policies by inspecting Definition 1. Because norms are positive definite, it suffices to consider the expression inside the norm. The only ways for this expression to equal zero are either when the difference between policy spread tensors \((K_{}^{_{1}}-K_{}^{_{2}})\) is zero (which we will show implies Markovian observations), or it

Figure 3: T-Maze \(\)-discrepancy, mixing between full and partial observability. (Left) MDP observation function \(_{}\). (Right) Various POMDP observation functions \(_{}\) that produce aliased observations at the corridor states, junction states, or both. State indices correspond to starting states (0, 1), hallway (2–11), junctions (12, 13), and terminal state (14). Brighter squares indicate higher probability. (Center) \(\)-discrepancy has a minimum at zero for full observability and increases with partial observability. We interpolate between perfect observations and aliased ones, where the observation function is \(=(1-p)_{}+p_{}\).

is non-zero but is projected away by \(T\), \(R^{}\), and/or \(W\). We first consider when the policy spread tensors--which are the only terms that depend on \(\)--are equal, i.e. \(K_{}^{_{1}}=K_{}^{_{2}}\).

**Theorem 2**: _For any POMDP \(\) and any \(_{1}_{2}\), \(K_{}^{_{1}}=K_{}^{_{2}}\) if and only if \(\) is a block MDP._

_Proof sketch:_ Using Eq. (6), \(K_{}^{_{1}}=K_{}^{_{2}}\) can be simplified to \(^{}= W^{}\), which is satisfied if and only if \( W=I\), i.e. each observation is generated by exactly one hidden state. Proof in Appendix E.

Thus, when the policy spread tensors are equal, it means there is no state aliasing and \(\) is a block MDP. Now consider the cases where the difference between \(K_{}^{}\) is projected away by \(T\), \(R^{}\), and/or \(W\), starting from the policy spread tensors in Definition 1 and working our way outwards.

If the policy spread tensors differ, the transition tensor may project these differences away (\(TK_{}^{_{1}}=TK_{}^{_{2}}\)), for example, if the transition probabilities leading to the aliased states always occur in some fixed proportion. In such cases, we can collapse the aliased states to non-aliased ones without any loss of information by reformulating the incoming transition probabilities to the aliased states as stochastic transition dynamics (and possibly rewards) _after_ the non-aliased states. In other words, \(\) is equivalent to an MDP. We prove this claim and provide an example of such an environment in Appendix F.

Finally, differences between transition-policies may be projected away by the state-blending weights \(W\), the reward function \(R^{}\), or both. \(W(I- TK_{}^{_{1}})^{-1} R^{}=W(I- TK_{ }^{_{2}})^{-1} R^{}\). In this case, the effective MDP may be lossy, which is a limitation of the \(\)-discrepancy metric. Fortunately, such environments appear to be rare, and they are easy to handle, as we show in the example below.

Parity Check Environment.The Parity Check POMDP of Figure 4 has four equally likely starting states, each associated with a pair of colors that the agent will see during the first two timesteps. At the subsequent (white) junction state, the agent must decide based on these colors whether to go up (black arrow) or down (white arrow). The rewards are defined such that up is optimal if and only if the color family matches (i.e. red\(\) pink or blue\(\) cyan).

The transition-policies for the first eight states differ with and without aliasing, but the differences are perfectly symmetric with respect to rewards and observations.9 Every observation has zero expected return, and so the \(\)-discrepancy is zero for all policies. Our analysis in Appendix H.5 suggests such edge-cases are the exception and not the rule. Even so, such examples are easy to handle: we can simply add a small amount of memory. If the environment is truly an MDP, adding memory should have no effect, but if it is a POMDP, a randomly initialized memory function (defined in the next section) may be enough to break symmetry and produce a \(\)-discrepancy (as seen in Figure 4, center).

Taken together, Theorems 1 and 2 suggest that the \(\)-discrepancy could be a useful measure for detecting partial observability. In the next section, we demonstrate the efficacy of using the \(\)-discrepancy to learn memory functions that mitigate partial observability.

## 4 Memory Learning with the \(\)-Discrepancy

The \(\)-discrepancy can identify that we need memory, but it cannot yet tell us what to remember. For that, we must replace the POMDP \(\) in Definition 1 with a memory-augmented version. In general,

Figure 4: (Left) The Parity Check environment, a POMDP with zero \(\)-discrepancy for every policy. (Center) Almost any randomly-initialized 1-bit memory function reveals a \(\)-discrepancy. (Right) Memory optimization increases normalized return of subsequent policy learning, whereas memoryless policy optimization fails to beat the uniform random baseline.

a memory can be any mapping from agent histories \((_{0},a_{0},...,a_{t-1},r_{t-1},_{t})\) to internal memory states \(m\) within some set of memory states \(\). For practical reasons, we restrict our attention to recurrent memories that update an internal representation incrementally from fixed-size inputs.

We define a _memory function_\(:\) as a mapping from an observation, action, and memory state, \((,a,m)\), to a next memory state \(m^{}=(,a,m)\). Given a POMDP \(\), a memory function \(\) induces a _memory-augmented_ POMDP \(^{}\), with extended states \(_{}=\), actions \(_{}=\), and observations \(_{}=\). The augmented transition dynamics \(T_{}\) preserve the original transition dynamics \(T\) for states \(\) while allowing the agent full observability and control over memory states \(\). Memory functions naturally lead to memory-augmented policies \(_{}:_{}_{}\) and value functions \(V_{,}:_{}\) and \(Q_{,}:_{}_{} \) that reflect the expected return under such policies. For details, see Appendix G.1.

The \(\)-discrepancy (Definition 1) applies equally well to memory-augmented POMDPs, and can thus be used to determine whether a particular memory function \(\) induces a POMDP \(^{}\) with a Markovian observation space \(_{}\). We can also use it as a training objective for _learning_ such a memory function. We conduct a proof-of-concept experiment on several classic POMDPs for which we can obtain closed-form gradients of the \(\)-discrepancy with respect to a parametrized memory function. In each domain, we randomly generate a set of stochastic policies, select the one with maximal \(\)-discrepancy \(^{0,1}_{}\), and adjust the parameters of a memory function \(\) to minimize \(^{0,1}_{^{}}\) via gradient descent. Figure 5 shows the improvement in policy gradient performance due to the resulting memory function for increasing memory sizes. The details of this experiment are provided in Appendix H. We also run the same experiment on the Parity Check example, and provide results in Figure 4 (right).

We see that the \(\)-discrepancy can help mitigate partial observability, but it is somewhat inconveniently defined in terms of the closed-form value function fixed-points. Fortunately, with the appropriate choice of weighted norm, we can estimate the \(\)-discrepancy purely from sampled observation-action pairs (or, in the case of memory, observation-memory-action tuples):

\[^{_{1},_{2}}_{,}:=\|Q^{_{1}}_{ }-Q^{_{2}}_{}\|=(_{(,a)}[ (Q^{_{1}}_{}(,a)-Q^{_{2}}_{}(,a))^{ 2}])^{1/2}.\] (7)

The norm is taken over the on-policy joint observation-action distribution \(()(a|)\), which allows us to estimate the metric using samples generated by the agent's interaction with the environment. We show how to use it as an optimization objective in the following section.

## 5 A Scalable, Online Learning Objective

So far, we have shown that the \(\)-discrepancy can detect partial observability in theory and can mitigate it under certain idealized conditions. Now we demonstrate how to integrate our metric into sample-based deep reinforcement learning to solve problems requiring large, complex memory functions.

### Combining the \(\)-Discrepancy with PPO

To minimize the \(\)-discrepancy, we augment a recurrent version of the proximal policy optimization (PPO) algorithm (Schulman et al., 2017) with an auxiliary loss. We use recurrent PPO as our base algorithm due to its strong performance in many POMDPs (Ni et al., 2022), and since the \(\)-discrepancy is a natural extension of generalized advantage estimation (Schulman et al., 2016), which

Figure 5: Memory optimization increases normalized return of subsequent policy gradient learning. Performance is calculated as the expected start-state value, and is normalized between a random policy (\(y=0\)) and the optimal belief state policy (\(y=1\)) found with a POMDP solver (Cassandra, 2003). Error bars are 95% confidence intervals over 30 seeds.

is used in PPO. In this algorithm, a recurrent neural network (Amari, 1972) (specifically a gated recurrent unit, or GRU (Cho et al., 2014)) is used as the memory function \(\) that returns a latent state representation given previous latent state and an observation. This latent representation is used as input to an actor network to return a distribution over actions, as well as a critic. The critic is usually a value function network which learns a truncated TD(\(\)) value estimate for its advantage estimate.

To estimate the \(\)-discrepancy, we learn two TD(\(\)) value function networks with different \(\), parameterized by \(_{V,1}\) and \(_{V,2}\) respectively, and minimize their mean squared difference as an auxiliary loss:

\[L_{}()=_{}[(V_{_{V,1}}^{_{1} }(z_{t})-V_{_{V,2}}^{_{2}}(z_{t}))^{2}],\] (8)

where \(z_{t}=_{_{}}(_{t},z_{t-1})\) is the latent state output by the GRU, and \(\) represents all parameters. We train this neural network end-to-end with the standard PPO actor-critic losses and backpropagation through time (Mozer, 1995). Full details of the algorithm are provided in Appendix I.1.

Any algorithm that uses value functions could, in principle, also leverage the \(\)-discrepancy. However, the theoretical results in Section 3 only hold for value function fixed points and not necessarily for estimated values prior to convergence. Inaccurate value functions may lead to an inaccurate \(\)-discrepancy. Fortunately, value functions often provide a useful learning signal before convergence, and we observe in the following sections that the \(\)-discrepancy is also useful during learning.

### Large Partially Observable Environments

We evaluate our approach on a suite of four hard partially observable environments that require complex memory functions. _Battleship_(Silver and Veness, 2010) requires reasoning about unknown ship positions and remembering previous shots. Partially observable _PacMan_(Silver and Veness, 2010) requires localization within the map while tracking dot status and avoiding ghosts with only short-range sensors. _RockSample (11, 11)_ and _RockSample (15, 15)_(Smith and Simmons, 2004) have stochastic sensors and require remembering which rocks have been sampled. While these environments were originally used to evaluate partially observable planning algorithms in large-scale POMDPs (Silver and Veness, 2010), we use them to test our sample-based learning algorithms due to the complexity of their required memory functions.10 See Appendix I.2 for more details.

Figure 6: (Left) The \(\)-discrepancy auxiliary objective (LD) improves performance over recurrent (RNN) and memoryless PPO. Learning curves shown are the mean and 95% confidence interval over 30 runs. (Right) PacMan memory visualization. The agent moves within the maze (middle), and we reconstruct the dot locations from the agent’s memory. RNNs (bottom) benefit from the \(\)-discrepancy auxiliary loss (LD, top).

### Experiments

We conduct experiments with regular PPO, recurrent PPO, and our \(\)-discrepancy-augmented recurrent PPO in Figure 6. We also visualize the agent's memory for the partially observable PacMan environment by reconstructing the dot locations from the RNN latent state to show where the agent "thinks" it has been (see Appendix I.5). We performed a hyperparameter sweep for each method and report learning curves for undiscounted return (see Appendix I.3 for discounted learning curves and Appendix I.4 for additional experimental details).

The \(\)-discrepancy objective leads to significantly better final performance and learning rate versus recurrent and memoryless PPO in all environments. In RockSample \((15,15)\), the baseline agents quickly learn to exit immediately (for +10 reward), but never improve on this. By contrast, minimizing \(\)-discrepancy helps the agent learn the missing features that allow it to express better policies. We also run experiments on the classic POMDPs from Section 4, but due to the size of these problems, both baseline and our proposed approach performed almost optimally (see Appendix I.6).

We also observe that the best performing hyperparameters from our sweep offer further evidence for the theory developed in Section 3. Our theory suggests the \(\) parameters should be well separated in order to see the largest \(\)-discrepancy. Indeed, we find that a large difference between \(_{1}\) and \(_{2}\) was beneficial, with optimal \(\)s close to either \(0\) or \(1\). In addition to this, our hyperparameter sweep also includes a PPO variant with two different TD(\(\)) value functions but without the \(\)-discrepancy auxiliary loss described in Section 5.1. These agents were never selected in the sweep; using the loss in Equation 8 seems to only help with performance in tested environments.

## 6 Related Work

There is an interesting connection between state abstraction (Li et al., 2006), which selectively removes information from state, and partial observability mitigation, which aims to recover state from incomplete observations. Allen et al. (2021) investigated the state abstraction perspective and characterized the properties under which abstract state representations of MDPs either do or do not preserve the Markov property. Several other approaches characterize and measure partial observability and POMDPs. While POMDPs have been shown to be computationally intractable in general (Papadimitriou and Tsitsiklis, 1987), various works have studied complexity measures (Zhang et al., 2012) and defined subclasses with tractable solutions (Littman, 1993; Liu et al., 2022).

The most common strategies for mitigating partial observability are memory-based approaches that summarize history. Early approaches relied on discrete representations of history, including tree representations (McCallum, 1996) or finite-state controllers (Meuleau et al., 1999). Modern approaches mostly use RNNs (Amari, 1972) trained via backpropagation through time (BPTT) (Mozer, 1995) to tackle non-Markovian decision processes (Schmidhuber, 1990). Various approaches use recurrent function approximation to learn better state representations. One successful approach is learning a recurrent value function (Lin and Mitchell, 1993; Bakker, 2001; Hausknecht and Stone, 2015) that uses TD error as a learning signal for memory. Policy gradient methods, including PPO (which we compare to), have also been used with recurrent learning to mitigate partial observability (Wierstra et al., 2007; Heess et al., 2015). Model-based methods can learn a recurrent dynamics model (Hafner et al., 2020) to facilitate planning alongside reinforcement learning. These approaches learn their representations implicitly to improve prediction error, rather than explicitly to mitigate partial observability.

## 7 Conclusion

We introduce the \(\)-discrepancy: an observable and differentiable measure of non-Markovianity suitable for mitigating partial observability. The \(\)-discrepancy is the difference between two distinct value functions estimated using TD(\(\)), for two different \(\) values. We characterize the \(\)-discrepancy and prove that it reliably distinguishes MDPs from POMDPs. We then use it as a memory learning objective and demonstrate that minimizing it in closed-form helps learn useful memory functions in small-scale POMDPs. Finally, we propose a deep reinforcement learning algorithm which leverages the \(\)-discrepancy as an auxiliary loss, and show that it significantly improves the performance of a baseline recurrent PPO agent on a set of large and challenging partially observable tasks.

## Author Contributions

CA, AK and RYT led the project. CA, OG, GK, MLL and SL came up with the initial idea. CA, OG, MLL, SL, and DS conducted the first conceptual investigations and proof-of-concept experiments. AK led the theoretical work, with support from CA, SL, RP, and RYT. RYT led the algorithm development, with support from CA, SL, and GK. CA and RYT led the implementation and experiments, with support from SL, NP, and DS. RP discovered the class of parity check examples and showed that they have zero \(\)-discrepancy. CA led the writing, with support from AK and RYT. OG, GK, MLL, SL, and RP advised on the project and provided regular feedback.