# SafeDICE: Offline Safe Imitation Learning

with Non-Preferred Demonstrations

 Youngsoo Jang\({}^{1}\), Geon-Hyeong Kim\({}^{1}\), Jongmin Lee\({}^{2}\),

**Sungryll Sohn\({}^{1}\), Byounglip Kim\({}^{1}\), Honglak Lee\({}^{1}\), Moontae Lee\({}^{1,3}\)**

\({}^{1}\) LG AI Research

\({}^{2}\) University of California, Berkeley

\({}^{3}\) University of Illinois Chicago

###### Abstract

We consider offline safe imitation learning (IL), where the agent aims to learn the safe policy that mimics preferred behavior while avoiding non-preferred behavior from non-preferred demonstrations and unlabeled demonstrations. This problem setting corresponds to various real-world scenarios, where satisfying safety constraints is more important than maximizing the expected return. However, it is very challenging to learn the policy to avoid constraint-violating (i.e. non-preferred) behavior, as opposed to standard imitation learning which learns the policy to mimic given demonstrations. In this paper, we present a hyperparameter-free offline safe IL algorithm, SafeDICE, that learns safe policy by leveraging the non-preferred demonstrations in the space of stationary distributions. Our algorithm directly estimates the stationary distribution corrections of the policy that imitate the demonstrations excluding the non-preferred behavior. In the experiments, we demonstrate that our algorithm learns a more safe policy that satisfies the cost constraint without degrading the reward performance, compared to baseline algorithms.

## 1 Introduction

Reinforcement learning (RL)  aims to learn the optimal policy that maximizes the expected cumulative rewards through interacting with the environment. Recently, RL has achieved remarkable successes in various domains, but its application to real-world tasks is yet challenging mainly due to the following two reasons: First, since the interaction with the environment is costly in most real-world tasks, the policy should be learned solely from the pre-collected dataset (i.e. _offline learning_). Second, reward annotation, which is essential for applying the RL algorithm, is a very strong requirement that is difficult to obtain in real-world tasks.

Imitation learning (IL)  is a more realistic framework to deal with real-world tasks, where the expert's demonstrations are solely given without reward annotations. Standard IL aims to mimic the expert's demonstrations generated from the optimal policy in terms of expected return. However, in most real-world tasks, additional constraints should be considered in addition to maximizing the expected return such as the safe driving for autonomous driving and toxicity degree for the conversational agent. Fortunately, _non-preferred_ demonstrations that violate constraints are often naturally collected. For example, black box data of an accident vehicle and toxic content reported by chatbot users are separately collected as _non-preferred_ demonstrations. However, most existing studies have focused only on imitating expert (i.e. _preferred_) behavior, and a safe imitation learning method that avoids _non-preferred_ behavior has not yet been explored.

In this paper, we are particularly interested in solving safe imitation learning in an offline setting: Given scarce but labeled non-preferred demonstrations, and abundant but unlabeled demonstrationsof both preferred and non-preferred behavior, we aim to learn a safe policy that follows the preferred behavior while avoiding the non-preferred behavior (see Figure 1). This problem setting has been mostly unexplored, and cannot be completely addressed by existing methods. One of the existing applicable approaches is the preference-based policy learning algorithm [4; 35; 17], which optimizes a policy based on preference rankings over demonstrations. This method can learn the policy to prefer unlabeled demonstrations to non-preferred demonstrations, but it also prefers non-preferred demonstrations mixed in the unlabeled demonstrations, which can lead to learning suboptimal policy. One of the approaches effectively using the unlabeled dataset is DWBC , a weighted behavior cloning based on a discriminator, which adopts the positive-unlabeled learning [9; 36; 30] for training the discriminator. This method requires an additional cost of hyperparameter search for training the discriminator, and its performance is hyperparameter sensitive. Another notable approach in the natural language processing domains is DE experts , a decoding-time method, which combines trained models with preferred and non-preferred demonstrations. This method is also hyperparameter sensitive and prone to the _degeneration_ issue that leads to abnormal behavior. Both DWBC and DE experts, which require hyperparameter search, are unsuitable for real-world tasks where interaction with the real environment is costly and risky.

We present an offline safe IL algorithm that learns safe policy by leveraging the non-preferred demonstrations in the space of stationary distributions. Our algorithm, _offline Safe imitation learning by avoiding non-preferred demonstrations via DIstribution Correction Estimation_ (SafeDICE), directly estimates the stationary distribution corrections of the policy that imitate the demonstrations excluding the non-preferred behavior. SafeDICE essentially leverages the non-preferred demonstrations in the space of stationary distributions, unlike existing algorithms that leverage it in the space of policy  or the learning process of the discriminator . We show that our algorithm can be reduced to a single convex minimization problem _without additional hyperparameter search_, unlike existing algorithms that are highly sensitive to the hyperparameter. In the experiments, we demonstrate that our algorithm learns a more preferred (i.e. constraint-satisfying) policy compared to baseline algorithms in constrained RL benchmarks including RWRL  and Safety Gym .

## 2 Preliminaries

### Markov Decision Process (MDP)

We consider an environment modeled as a Markov Decision Process (MDP), defined by \(M= S,A,T,R,p_{0},\), where \(S\) is the set of states, \(A\) is the set of actions, \(T:S A(S)\) is the transition probability, \(R:S A\) is the reward function, \(p_{0}(S)\) is the distribution of the initial state, and \([0,1)\) is the discount factor. For the given policy \(\), its stationary distribution \(d^{}(s,a)\) is defined as follows:

\[d^{}(s,a):=(1-)_{t=0}^{}^{t}(s_{t}=s,a_{t}=a)\]

Figure 1: Illustrative example of our problem setting and the position of work among the prior methods and problem settings. First, prior offline imitation learning algorithms (DemoDICE , DWBC ), which aim to imitate expert demonstrations, assume labeled preferred demonstrations and unlabeled demonstrations. Second, preference-based policy learning (PPL) algorithms and DE experts  require both labels of preferred and non-preferred demonstrations. Unlike the problem settings of prior works, our work focuses on the problem of safe imitation learning assuming only the labels of non-preferred demonstration that can deal with safety more importantly.

where \(s_{0} p_{0}\), \(a_{t}(|s_{t})\), and \(s_{t+1} T(|s_{t},a_{t})\) for all timesteps \(t 0\). For brevity, \(d^{}\) will be used to denote the stationary distribution \(d^{}(s,a)\).

We consider the offline imitation learning setting, where the policy should be optimized from solely a pre-collected dataset without reward annotation and any additional online environment interaction. In our problem setting, _preferred_ and _non-preferred_ behaviors are defined by satisfying or violating constraints. We denote the preferred policy by \(^{P}\), the non-preferred policy by \(^{N}\), and the corresponding stationary distribution of \(^{P}\) and \(^{N}\) by \(d^{P}\) and \(d^{N}\), respectively. We assume a pre-collected dataset that consists of _scarce but labeled_ dataset \(D^{N}\) generated by the non-preferred policy, and a _abundant but unlabeled_ dataset \(D^{U}\) generated by both preferred and non-preferred policies. We denote the corresponding stationary distribution of \(D^{U}\) by \(d^{U}\), which can be regarded as convex combination of \(d^{F}\) and \(d^{N}\) with unknown ratio \(\) (i.e. \(d^{U}:=(1-)d^{P}+ d^{N}\)). In this setting, we aim to learn a _safe_ policy from unlabeled demonstrations while _avoiding non-preferred_ demonstrations that are labeled as non-preferred behavior.

### Preference-based Offline Imitation Learning

Imitation learning (IL) aims to imitate the behavior of expert policy solely from state-action demonstrations without reward annotations. The standard IL is naturally formulated as minimizing the KL divergence between stationary distributions of preferred policy (i.e. expert policy) and target policy [14; 11]:

\[^{*}:=*{arg\,min}_{}D_{}(d^{}\|d^{P}).\] (1)

However, it requires a large amount of annotated preferred demonstrations, which may be a too strong requirement for various real-world scenarios. Recent offline IL method  relaxes this requirement for preferred demonstrations by leveraging the unlabeled dataset which may contain both preferred and non-preferred demonstrations:

\[^{*}:=*{arg\,min}_{}D_{}(d^{}\|d^{P})+ D _{}(d^{}\|d^{U}),\] (2)

where \( 0\) is a hyperparameter that controls the balance between minimizing the KL divergence with \(d^{U}\) and \(d^{P}\). This offline imitation learning method can learn the preferred policy from a large number of unlabeled demonstrations with only a small amount of labeled preferred demonstrations.

In contrast to the mainstream of recent IL approaches focusing on mimicking expert (i.e. preferred) behavior, avoiding dangerous (i.e. non-preferred) behavior is a more important problem in many real-world tasks, which has been unexplored yet. In the same manner as Eq. (2), we can naively consider the penalty for the non-preferred behavior by maximizing the KL divergence with \(d^{N}\):

\[^{*}:=*{arg\,min}_{}-D_{}(d^{}\|d^{N})+  D_{}(d^{}\|d^{U}),\] (3)

where \( 0\) is a hyperparameter that controls the balance between minimizing the KL divergence with \(d^{U}\) and maximizing the KL divergence with \(d^{N}\). However, since the KL divergence has no upper bound, optimizing Eq. (3) is very unstable and challenging, where the \(D_{}(d^{}\|d^{N})\) can blow up to infinity. Therefore, safe imitation learning while avoiding non-preferred behavior is not a straightforward problem and has not yet been studied.

## 3 SafeDICE

In this section, we present _offline Safe imitation learning by avoiding the non-preferred demonstrations via DIstribution Correction Estimation_ (SafeDICE), an offline IL algorithm that can learn the safe policy using labeled non-preferred demonstrations. SafeDICE optimizes the stationary distributions of the target policy to match the stationary distribution of the preferred policy from the unlabeled dataset by excluding non-preferred demonstrations.

### Offline Safe Imitation Learning with Non-Preferred Demonstrations

Similar to standard IL approaches, the derivation of our algorithm starts from KL divergence minimization between \(d^{}\) and \(d^{P}\), which can be estimated by \(d^{U}\) and \(d^{N}\) as follows:

\[^{*}=*{arg\,min}_{}D_{}d^{} \,- d^{N}}{1-},\] (4)where \(\) is _unknown_ ratio between the stationary distributions of preferred policy and non-preferred policy (i.e. \(d^{U}:=(1-)d^{P}+ d^{N}\)). Optimizing the Eq. (4) is sensitive to \(\), and selecting the \(\) with hyperparameter search is not suitable for real-world tasks where the interaction with the real environment is very costly and risky. Therefore, it is not straightforward to learn the preferred policy using non-preferred demonstrations with existing imitation learning algorithms. The main contribution of our paper is to present an algorithm that theoretically determines \(\) and learns the preferred policy only with the labels of non-preferred demonstrations without any hyperparameter search. In the following description, we first consider the \(\) as a known ratio in our derivation, then present the way to select the \(\) while providing the theoretical guarantees on the error bound of optimized stationary distribution with selected \(\). All the proofs can be found in Appendix A.

The main flow of our derivation follows prior DICE-based offline imitation learning methods [12; 13], but it is clearly different from the existing methods that require hyperparameter search to deal with unknown \(\). First, we consider a problem equivalent to Eq. (4) in terms of stationary distribution \(d\):

\[_{d 0} -D_{}d\|\,- d^{N}}{1- })\] (5) s.t. \[(_{*}d)(s)=(1-)p_{0}(s)+(_{*}d )(s) s,\] (6)

where \((_{*}d)(s):=_{a}d(s,a)\) is the marginalization operator, and \((_{*}d)(s):=_{s,}T(s|,)d(,)\) is the transposed Bellman operator. The Bellman flow constraint (6) ensures that \(d(s,a)\) is a valid stationary distribution of some policy, where \(d(s,a)\) can be interpreted as a normalized occupancy measure of \((s,a)\). Therefore, if we find optimal \(d^{*}(s,a)\) that satisfies Eq. (5) and (6), then its corresponding policy can be easily obtained by \(^{*}(a|s)=(s,a)}{_{a}d^{*}(s,a)}\).

The Lagrangian dual formulation of the above constrained optimization problem is

\[_{d 0}_{}-D_{}d\|\,-  d^{N}}{1-})+_{s}(s)(1-)p_{0}(s)+ (_{*}d)(s)-(_{*}d)(s),\] (7)

where \((s)\) are the Lagrange multipliers for the Bellman flow constraints (6). However, directly optimizing the Eq. (7) is challenging in an offline manner, where further environment interactions are not available. To obtain a tractable objective, we introduce the following derivations:

\[-D_{}d\|\,- d^{N}}{1- })+_{s}(s)(1-)p_{0}(s)+(_{*} d)(s)-(_{*}d)(s)\] \[=(1-)_{p_{0}}[(s)]+_{d} ()(s,a)-(s)-(s,a)-  d^{N}(s,a)}\] \[=(1-)_{p_{0}}[(s)]+_{d} (s,a)+()(s,a)-(s)}_{:=A_{}(s,a )}- w(s,a)\] \[=(1-)_{p_{0}}[(s)]+_{d^{U}} w(s,a)A_{}(s,a)- w(s,a)\] \[=:(w,;r_{}),\]

where \(r_{}(s,a)\) corresponds to the preference-based reward function in the context of RL, which will be trained by the pre-collected dataset.

In summary, our goal is reduced to solve the following maximin optimization:

\[_{w 0}_{}(w,;r_{}),\] (8)

where the optimal solution \(w^{*}\) of Eq. (8) represents the stationary distribution correction of an optimal policy \(^{*}\) (i.e. \(w^{*}(s,a)=}(s,a)}{d^{U}(s,a)}\)).

### Pretraining the Preference-based Reward Function \(r_{}(s,a)\)

To optimize the Eq. (8), we pretrain the discriminator to estimate the log ratio of \(d^{N}(s,a)\) and \(d^{U}(s,a)\), which is required to estimate for the \(r_{}(s,a)\):

\[c^{*}=*{arg\,max}_{c}_{d^{N}}[ c(s,a)]+_{ d^{U}}[(1-c(s,a))].\] (9)

With the optimal discriminator \(c^{*}(s,a)=(s,a)}{d^{U}(s,a)+d^{N}(s,a)}\) trained by Eq. (9), the preference-based reward function \(r_{}(s,a)\) can also be obtained as:

\[r_{}(s,a)=(s,a)}{(1-)(1-c^{*}( s,a))}.\] (10)

However, according to the value of \(\), the reward function \(r_{}(s,a)\) defined by Eq. (10) cannot guarantee that the value inside the log function always becomes a positive. Therefore, finding the optimal \(\) through hyperparameter search is infeasible.

### Reduction to the Single Min Optimization

Since the optimization Eq. (7) is a convex optimization, we can change the order of operator from maximin optimization to minimax optimization :

\[_{}_{w 0}(w,;r).\] (11)

Then, we can replace the inner maximization of Eq. (11) with a closed-form solution for \(w\), which can be easily obtained by exploiting the convexity.

**Proposition 3.1**.: _For any \(\), the closed-form solution to the inner maximization of Eq. (11), i.e. \(w^{*}_{}=*{arg\,max}_{w 0}(w,;r)\), is given by:_

\[w^{*}_{}(s,a)=(A_{}(s,a)-1).\] (12)

By using the closed-form solution \(w^{*}_{}\) of Eq. (12), our derivation can be reduced to a single minimization as follows:

\[_{}(w^{*}_{},;r)= (1-)_{p_{0}}[(s)]+_{d^{U}} (A_{}(s,a)-1).\] (13)

Finally, by optimizing the single minimization problem of Eq. (13), we can obtain the optimal stationary distribution correction \(w^{*}\) which can easily extract the corresponding optimal policy \(^{*}\). To extract a policy from the optimal stationary distribution correction \(w^{*}(s,a)\), we adopt weighted behavior cloning on the offline dataset as follows:

\[_{}-_{(s,a) d^{*^{*}}}[(a|s)]=-_{(s,a)  d^{U}}[w^{*}(s,a)(a|s)],\] (14)

where \(w^{*}(s,a)=}(s,a)}{d^{U}(s,a)}\).

### Hyperparameter \(\) Selection

In the previous sections, we have derived our algorithm under the assumption that \(\) is a known ratio. The remaining question is how we select the suitable \(\) without additional hyperparameter search. Fortunately, in contrast to the existing methods [19; 31] that require the additional cost of hyperparameter search, we can specify the \(\) with providing the theoretical guarantees on the error bound of the estimated stationary distribution.

**Proposition 3.2**.: _Suppose \(_{(s,a)}(s,a)}{d^{N}(s,a)}\). If we obtain the estimated stationary distribution by optimizing the Eq. (7) with \(\) set to \(\) as follows:_

\[=_{(s,a)}(s,a)}{d^{N}(s,a)},\] (15)_then the total variation distance of true and estimated stationary distribution of preferred policy is bounded as follows:_

\[D_{}(d^{P}(s,a),^{P}(s,a)),\] (16)

_where \(D_{}\) is total variation distance, \(d^{P}(s,a)\) is true stationary distribution of the preferred policy and \(^{P}(s,a)\) is estimated stationary distribution which is defined by \(\) as \(^{P}(s,a)=(s,a)-d^{N}(s,a)}{1-}\). (Proof in Appendix A.)_

Intuitively, since the behavior of the non-preferred and preferred policies are different, there must exist state-action spaces that are rarely visited by the preferred policy but are frequently visited by the non-preferred policy. Therefore \(\) may be a very small positive value, then the total variation distance of the true and estimated stationary distribution is close to zero:

\[_{ 0}D_{}(d^{P}(s,a),^{P}(s,a))=0.\] (17)

We also provide proof in Appendix A.3 that \(r_{}(s,a)\) of Eq. (10), defined by the \(\) selected by Eq. (15), is always well-defined with positive value inside the log-function in Eq. (10).

Finally, our algorithm optimizes the Eq. (7) with the \(\) obtained by Eq. (15), where \(\) is no longer an unknown hyperparameter. In short, by leveraging the non-preferred demonstrations in the space of stationary distributions, this problem can be resolved by _solving a single convex minimization problem for a specific \(\)_. This is in contrast to the existing methods, which directly leverage the non-preferred demonstrations for training the policy  or discriminator  that require the additional cost of hyperparameter search. The pseudocode for the whole process of SafeDICE can be found in Appendix B.

## 4 Related Work

**Stationary distribution correction estimation (DICE)** Since the introduction of DICE for off-policy evaluation in DualDICE , various DICE-family algorithms have been developed for off-policy evaluation [33; 34; 32; 7]. In addition to these studies, two algorithms, AlgaeDICE  and ValueDICE , have been proposed to address the challenges of off-policy RL and IL, respectively. However, they may encounter issues with numerical instability in an offline setting due to the nested min-max optimization and out-of-distribution action evaluation.

To overcome the issues of numerical instability in offline settings, recent DICE-based offline RL [15; 16] and IL algorithms [13; 20; 12] have adopted a strategy of directly optimizing stationary distribution correction ratios, similar to the approach used in our work. Additionally, these offline IL algorithms utilize unlabeled demonstrations in an effort to address the problem of distribution drift . They share a similarity with our algorithm in that they all aim to recover the expert's behavior through direct optimization of the stationary distribution correction ratios. However, these algorithms rely on the availability of clean expert demonstrations, which can make them difficult to use in situations where expert demonstrations are not available.

**Policy Learning from Suboptimal Demonstrations** Preference-based policy learning algorithms use ranked suboptimal demonstrations to learn a reward function. Many of these algorithms, such as T-REX , PreffPPO , and PEBBLE , utilize Bradley-Terry model  to model the reward function. Once the reward function is learned, T-REX and PreffPPO use PPO  to train a policy, while PEBBLE employs SAC . While preference-based policy learning algorithms allow for learning from unlabeled demonstrations and potentially disregarding the negative influence of non-preferred demonstrations, it is important to note that they may also give preference to non-preferred demonstrations mixed in the unlabeled demonstrations.

On the other hand, there are also many studies that utilize suboptimal demonstrations, even though they are not preference-based policy learning algorithms. D-REX  and SSRR  both learn a policy by using given suboptimal demonstrations, and then generate trajectories with varying levels of optimality using this policy with injected noise. But, both algorithms require online sampling, which is not feasible in offline settings. Wu et al.  proposed two algorithms that utilize positive-unlabeled learning [9; 36; 30] to train a discriminator for constructing a reward function. However,both algorithms require ground-truth labels for uniformly sampled trajectories from unlabeled demonstrations. DWBC  uses positive-unlabeled learning to train a discriminator, which is then used to construct weights for weighted BC. DExperts , a decoding-time method in the natural language processing domain, combines models for preferred and non-preferred demonstrations. Unfortunately, both DWBC and DExperts may not be suitable for offline safe imitation learning tasks due to their sensitivity to hyperparameter settings.

## 5 Experiments

In this section, we show the experimental results on various tasks from constrained RL benchmarks. First, we conduct the evaluation on domains from Real-World RL (RWRL) suite , which provides challenges in real-world scenarios including safety constraints. Second, we also conduct the experiment on continuous control tasks from Safety Gym environment , which provides practical scenarios of safety issues. In order to show the motivation of our work and the limitation of existing algorithms, we also provide qualitative results on tabular MDP environments. The illustrative examples and qualitative analysis on tabular MDP environments can be found in Appendix G.

**Baselines** Since offline safe IL, which we consider in this paper, has not been explored yet much, it lacks directly applicable baseline algorithms. Therefore, we consider the following four algorithms as baselines. First, BC denotes simple behavior cloning on unlabeled demonstrations. Second, DWBC denotes a variant of the original discriminator-weighted behavior cloning algorithm  that adopts the positive-unlabeled learning for training the discriminator. To apply it to our setting, we modified positive-unlabeled learning to negative-unlabeled learning. Third, PPL denotes a preference-based policy learning algorithm that adopts the preference-based reward learning algorithm based on the Bradley-Terry model  and weighted behavior cloning for policy learning. Lastly, we also compare with DExperts , a decoding-time method that combines trained models with unlabeled and non-preferred demonstrations. The details of baseline algorithms are provided in Appendix C.

**Evaluation metrics** We compare the performance of SafeDICE and baseline algorithms in terms of task performance (i.e. expected return) and safety (i.e. cost constraints). We report the results with the following metrics: 1) _Normalized Return_: evaluates the normalized task performance of the policy, 2) _Average Cost_: evaluates the mean cost of the policy, 3) _Cost-Violating Ratio_: evaluates the ratio of cost-violating behavior of the policy, 4) _Conditional Value at Risk performance (CVaR) 10% Cost_: evaluates the mean cost of the worst 10% runs. We also provide _CVaR k% Cost_ for various values of \(k\) in Appendix E.1. Here, our main focus of the comparison is to evaluate the safety _(Average Cost, Cost-Violating Ratio_, and _CVaR 10% Cost_), and we use the _Normalized Return_ to examine the degeneration issue, whether the agents fall into abnormal behaviors or not. All metrics are measured as the average value of 500 trajectories generated from the learned policy, and we report the results that are averaged over 5 runs.

### Real-World Reinforcement Learning (RWRL) Suite

Now, we present the results of SafeDICE on a set of continuous control benchmarks from Real-World RL (RWRL) suite , where the safety constraints are also provided by environments.

**Task setup** We conduct the experiments for three tasks from RWRL environment with safety constraints: Cartpole, Walker, and Quadruped. Since there is no standard dataset for offline IL considering the safety constraints, we collected data by training the online RL agents. To obtain an preferred and non-preferred policy, we trained the SAC  in different two ways that solve the RL problem with respect to reward penalized by the cost or not. Among these two policies which are both optimal in terms of return maximization, we define the preferred policy as a policy that satisfies the constraint (i.e. high-return-low-cost behavior) and the non-preferred policy as a policy that violates the constraint (i.e. high-return-high-cost behavior). This experimental setting is representative of many real-world scenarios, where the data is collected by high-return behavior without consideration of the cost. Then, we generated scarce but labeled non-preferred demonstrations \(D^{N}\) from the non-preferred policy, and abundant but unlabeled demonstrations \(D^{U}\) from both preferred and non-preferred policies. We collected diverse trajectories by sampling with a stochastic policy from randomly initialized initial states. Our goal is to recover the low-cost behavior by avoiding the high-cost demonstrations while not hurting the high-return behavior of unlabeled demonstrations. The details for the experimental settings can be found in Appendix D.

**Main results** Figure 2 summarizes the overall performance of SafeDICE and baseline algorithms in RWRL tasks. The results show that SafeDICE significantly outperforms baseline algorithms in all metrics related to safety across all domains (except for _CVaR 10% Cost_ in the Cartpole domain), while not hurting the performance of normalized return. In the case of the Cartpole domain, since there are cases where the cost is unavoidably violated depending on the initial position of cart that is initialized randomly, _CVaR 10% Cost_ which measures the worst 10% performance shows similar performance to other algorithms. We have also evaluated the _CVaR \(k\)% Cost_ with various \(k\) and are provided in Appendix E.1. PPL outperforms BC on Walker and Quadruped domains but suffers from degeneration issues on the Cartpole domain. DWBC outperforms or matches the other baseline algorithms on all domains, but performs poorly compared to SafeDICE despite the best results obtained through the additional hyperparameter search. Similar to DWBC, DE experts shows results that are sensitive to hyperparameters, and are vulnerable to degeneration issues depending on hyperparameters. Additional experimental results according to various hyperparameters for DWBC and DE experts are provided in Appendix E.2.

**Results with varying amount of \(D^{N}\)** To study the dependence on the amount of labeled non-preferred demonstrations, we experiment with different numbers of labeled non-preferred demonstrations \(|D^{N}|\) on Walker domain. (The results are provided in Appendix E.3.) Figure 8 summarizes the performance of SafeDICE and baseline algorithms with varying the number of labeled non-preferred demonstrations (\(|D^{N}|\{50,20,10,5\}\)). In this experiment, it is expected that performance in terms of safety gradually degrades as \(|D^{N}|\) decreases. As shown in Figure 8, as \(|D^{N}|\) decreases, SafeDICE shows a greater difference from other baseline algorithms, significantly outperforming. PPL works similarly to SafeDICE when \(|D^{N}|\) is large but shows sensitive performance depending on \(|D^{N}|\). DWBC is not sensitive to \(|D^{N}|\), but it performs poorly compared to SafeDICE. DE experts easily suffers from degeneration issues when \(|D^{N}|\) is small and performs poorly even when \(|D^{N}|\) is large. These results represent that our algorithm SafeDICE can be applied more effectively in real-world scenarios where labeled non-preferred demonstrations are not provided in large quantities.

### Safety Gym Environment

To show that our algorithm robustly works in various environments, we additionally conduct experiments on Safety Gym, which is one of the representative benchmarks for safety in RL.

**Task setup** We conduct the experiments for Goal and Button tasks from the Safety Gym environment. For each task, the agent aims to maximize task performance while satisfying the constraints by avoiding the dangerous area (see Figure 3). Since there is no standard dataset for offline IL considering

Figure 2: Experimental results on RWRL control tasks. All results are averaged over 5 runs, and the shaded area represents the standard error.

the safety constraints, we collected data by training the online RL agents. To obtain non-preferred and preferred policies, we trained PPO  and PPO-Lagrangian , which is combined with the Lagrangian approach to PPO, respectively. Here, we define the non-preferred policy as a policy that violates the constraint and the preferred policy as a policy that satisfies the constraint. Then, we generated scarce but labeled non-preferred demonstrations \(D^{N}\) from the PPO-Lagrangian policy, and abundant but unlabeled demonstrations \(D^{U}\) from both PPO and PPO-Lagrangian policies. We collected diverse trajectories by sampling with a stochastic policy from randomly initialized initial states. In this setting, our goal is to learn a safe policy (i.e. preferred policy) that satisfies the cost constraints by avoiding the cost-violating (i.e. non-preferred) demonstrations. The details for the experimental settings can be found in Appendix D.

**Results** Figure 4 summarizes the overall performance of SafeDICE and baseline algorithms in Safety Gym tasks. The results show that SafeDICE significantly outperforms baseline algorithms in all metrics related to safety across all domains. In the result of the Button task, SafeDICE shows low performance in _Return_, but it is a desirable result because the non-preferred demonstrations were collected from a safe policy with relatively low returns and very low costs (see Appendix D).

## 6 Conclusion

In this work, we presented SafeDICE, an offline safe IL algorithm, which can learn safe policy by leveraging the non-preferred demonstrations in the space of stationary distributions. We formulated this problem as a stationary distribution matching problem, which can be reduced to a single convex minimization problem without additional hyperparameter search, unlike existing algorithms that are highly sensitive to the hyperparameter. In the experiments, we demonstrated that SafeDICE learns a more safe policy that satisfies the cost constraint in constrained RL benchmarks including RWRL and Safety Gym. As the future work, we will consider an extension to the problem settings where preference information is given as a real value rather than binary value. There are no potential negative societal impacts of our work.

Figure 4: Experimental results on Safety Gym control tasks. All results are averaged over 5 runs, and the shaded area represents the standard error.

Figure 3: Tasks of Safety Gym environment. For each task, the agent (red robot) aims to achieve the objective while satisfying the cost constraints. The agents receive costs when entering hazards indicated by blue circles or when touching the obstacles. The objectives of each task are as follows: **Goal:** Move to a series of green goal positions **Button:** Press a series of highlighted goal buttons.