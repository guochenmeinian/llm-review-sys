# Variational Gaussian Processes with

Decoupled Conditionals

 Xinran Zhu1 Kaiwen Wu2 Natalie Maus2 Jacob R. Gardner2 David Bindel1

1Cornell University 2University of Pennsylvania

{xz584,bindel}@cornell.edu {kaiwenwu,maus,jacobrg}@seas.upenn.edu

###### Abstract

Variational Gaussian processes (GPs) approximate exact GP inference by using a small set of inducing points to form a sparse approximation of the true posterior, with the fidelity of the model increasing with additional inducing points. Although the approximation error in principle can be reduced by using more inducing points, this leads to scaling optimization challenges and computational complexity. To achieve scalability, inducing point methods typically introduce conditional independencies and then approximations to the training and test conditional distributions. In this paper, we consider an _alternative_ approach to modifying the training and test conditionals, in which we make them more flexible. In particular, we investigate decoupling the parametric form of the predictive mean and covariance in the conditionals, and learn independent parameters for predictive mean and covariance. We derive new evidence lower bounds (ELBO) under these more flexible conditionals, and provide two concrete examples of applying the decoupled conditionals. Empirically, we find this additional flexibility leads to improved model performance on a variety of regression tasks and Bayesian optimization (BO) applications.

## 1 Introduction

Gaussian processes (GPs) are a powerful class of non-parametric probabilistic models . Their flexibility and ability to make probabilistic predictions make them particularly useful in situations where uncertainty estimates are important . Although elegant, exact GPs become intractable for large datasets since the computational cost scales cubically with the size of the dataset.

To overcome this limitation, various sparse approximate GP approaches have been developed, mostly relying on sparse approximations of the true posterior . Among those, variational GPs have become increasingly popular because they enable stochastic minibatching and apply to both regression  and classification  tasks. However, the approximate posterior and variational inference leave an unavoidable gap from the exact model. Especially with too few inducing points, variational GPs could yield suboptimal accuracy performance due to a lack of expressiveness. Using more inducing points closes this gap in principle, but leads to additional computational complexity and optimization difficulties . While several works have studied ways of allowing more or better placed inducing points to improve accuracy of variational GPs, , it is nevertheless challenging to use more than a small fraction of the dataset size in inducing points.

In this work, we take a different approach to improving model expressiveness for better accuracy of variational GPs through _decoupling conditionals_. To start, we point out that inducing point approximations rely on two key conditionals (see Sec. 3.1) - the training and the test conditional, where the mean and covariance are parameterized by inducing points and kernel hyperparameters. Typically, prior works have focused on various _relaxations_ of these conditionals that enable significant computational complexity benefits . In this work, we consider alternative changesthat increase the flexibility of these conditionals. In Sec. 3.2, we describe two concrete examples of this idea: decoupling kernel lengthscales and decoupling entire deep feature extractors.

As a simple illustration, Fig. 1 illustrates, for example, how decoupled lengthscales improve model fitting. Our model DCSVGP (see Sec. 3.2) learns decoupled lengthscales \(l_{}\) and \(l_{}\) for mean and covariance respectively, and we compare with baseline SVGP (see Sec. 2) which learns one lengthscale \(l\).

To summarize our contributions: 1) We propose decoupled conditionals in variational GPs to improve model expressiveness for better accuracy. 2) We show that our idea is compatible with the variational framework and rigorously derive an ELBO for variational inference with decoupled conditionals. 3) We provide two concrete examples of applying decoupled conditionals and empirically show the superior performance of our models through extensive regression tasks and BO applications.

## 2 Background

We assume familiarity with GPs  and briefly introduce them for notational clarity. Given observation locations \(=\{_{i}\}_{i=1}^{n}^{d}\), a GP prior induces a multivariate Normal belief for latent function values \(=\{f(_{i})\}\): \((_{},_{nn})\), where \(_{},_{nn}\) are the mean values and covariance matrix at data \(\). Given observations \(=+\) with Gaussian noise \((0,^{2}I)\), the posterior distribution of the function value \(^{*}\) at a new data point \(x^{*}\) is \(p(^{*}|)=(^{*},^{**})\), where

\[^{*} =(^{*})+_{*n}(_{nn}+^{2} )^{-1}(-_{}),\] \[^{**} =(^{*},^{*})-_{*n}( _{nn}+^{2})^{-1}_{*n}^{T}.\]

Model hyperparameters such as kernel hyperparameters and noise \(\) are typically estimated by Maximum Likelihood using standard numerical solvers such as LBFGS . If no approximations are used, each evaluation to optimize the log marginal likelihood function costs \((n^{3})\) flops and \((n^{2})\) memory, thus motivating approximate methods for large training datasets.

### Sparse Gaussian Processes

To overcome the scalability limitations of exact GPs, many authors have proposed a variety of sparse GPs by introducing _inducing points_\(=\{_{i}\}_{i=1}^{m}\)[17; 44; 47; 48; 49]. Inducing points are associated with inducing values \(_{m}\), which represent latent function values at \(\) under the same GP assumption.

Figure 1: We compare model fit on a 1D latent function using 100 training samples. Solid curves with shading area depict the predictive mean and 95% confidence interval. Using 10 inducing points, in **subplot (a)** SVGP underfits the latent function with large lengthscale \(l=0.85\); while in **subplot (b)**, our DCSVGP model (see Sec. 3.2) fits better and learns different decoupled lengthscales \(l_{}\) and \(l_{}\). Using 100 inducing points, in **subplot (c) and (d)**, both models fits well with similar lengthscales around 0.3. See Sec. 1 for more details.

Although inducing values \(_{m}\) are marginalized out in the predictive distribution, they typically reduces training costs from \((n^{3})\) to \((n^{2}m+m^{3})\) for each gradient step, where \(m n\).

Sgpr.Based on inducing point methods, Sparse Gaussian Process Regression (SGPR)  further introduces variational inference into sparse GPs. Assuming that inducing data is adequate for inference, Titsias  further introduces a variational distribution \((_{m})(,)\) that approximates the posterior \(p(_{m}|)\). Thus, the predictive density can be approximated as

\[q(^{*})= p(^{*}|_{m})p(_{m}| )d_{m}= p(^{*}|_{m})(_{m})d_{m}=(_{}(^{*}),_{ }(^{*})^{2}).\]

Here the predictive mean \(_{}(^{*})\) and the latent function variance \(_{}(^{*})^{2}\) are:

\[_{}(^{*})=_{*m}_{mm}^{-1},\ \ _{}(^{*})^{2}=}_{**}+_{*m} _{mm}^{-1}_{mm}^{-1}_{m*},\] (1)

where \(}_{**}=_{**}-_{*m}_{mm}^{-1} _{m}^{T}\). The variational distribution \((_{m})=(,)\) is then learned by maximizing the variational ELBO [18; 21], which is a lower bound on the log marginal likelihood:

\[ p()_{q()}[ p(| )]-[(_{m})||p(_{m})],\]

where \(q()= p(|_{m})(_{m})d _{m}\), and \([(_{m})||p(_{m})]\) is the KL divergence .

Svgp.To enable data subsampling during training, SVGP  avoids analytic solutions to the ELBO, but rather decomposes the ELBO as a sum of losses over training labels and enables stochastic gradient descent (SGD)  training:

\[_{}=_{i=1}^{n}\{(y_{i}|_{ }(_{i}),^{2})-}(_ {i})^{2}}{2^{2}}\}-[(_{m})||p( _{m})],\] (2)

where \(_{}(),_{}()^{2}\) are the predictive mean and latent function variance from Eq. 1 respectively, and \(\) is an optional regularization parameter for the KL divergence and can be tuned in practice .

Ppgpr.To achieve heteroscedastic modeling and improve predictive variances, the Parametric Gaussian Process Regressor (PPGPR)  targets the predictive distribution directly in the loss function. It shares the same predictive mean \(_{}()\) and latent function variance \(_{}()^{2}\) as SVGP, but uses a slightly different stochastic ELBO loss:

\[_{}=_{i=1}^{n}(y_{i}|_{ }(_{i}),^{2}+_{}(_{i})^{2})- [(_{m})||p(_{m})].\]

Whitening.In practice, the variational distribution \((_{m})\) is usually "whitened" to accelerate the optimization of the variational distribution . Conventionally, the whitened variational distribution is \((_{m})=(}, })=(_{mm}^{-1/2},_{mm}^{-1/2}_{mm}^{-1/2})\), where \(_{mm}^{1/2}\) is the square root of \(_{mm}\). With whitening, the KL divergence and predictive distribution in ELBO (Eq. 2) are both simplified:

\[[(_{m})||p(_{m})]=( (_{m})||p_{0}(_{m}))p_{0}(_{m})=(,),\] \[_{}(_{i})=_{im}_{mm}^{ -1/2}},\ \ _{}(_{i})^{2}=}_{ii}+_{im} _{mm}^{-1/2}}_{mm}^{-1/2}_{mi}.\]

## 3 Methodology

Here, we present variational GPs with decoupled conditionals. In Sec. 3.1 and 3.2, we introduce decoupled conditionals under a unifying framework for approximate GPs, followed by two concrete examples. In Sec. 3.3, we derive an ELBO to do variational inference with decoupled conditionals.

### Conditionals of Approximate GPs

Quinonero-Candela and Rasmussen  introduced a unifying framework for sparse GPs. Through the framework, an approximate GP can be interpreted as "an exact inference with an approximatedprior" in contrast to "an approximate inference with an exact prior". With inducing points \(=\{_{i}\}_{i=1}^{m}\) and inducing values \(_{m}\), most sparse GPs approximate the joint Gaussian prior as:

\[p(^{*},)= p(^{*},|_{m})p (_{m})d_{m} q(^{*}|_{m})q( |_{m})p(_{m})d_{m}=q(^{*}, ),\]

where dependencies between training data \(\) and test data \(^{*}\) are _induced_ by inducing values \(_{m}\). Different approximations can be made of the inducing _training conditionals_\(q(|_{m})\) and the inducing _test conditionals_\(q(^{*}|_{m})\). Table 1 provides the exact expressions of the two conditionals and examples of approximations used in various approximate GP methods. For example, the Deterministic Training Conditional (DTC) approximation uses a deterministic training conditional and the exact test conditional ; The Fully Independent Training Conditional (FITC) approximation uses an approximate training conditional with diagonal corrections and the exact test conditional .

### Decoupled Conditionals and Prediction

In this paper, we consider augmenting the exact conditionals by a more flexible form that decouples the kernel hyperparameters in the mean and covariance in both conditionals:

\[&\ (|_{m})=(_{nm}_{mm}^{-1} _{m},}_{nn}) p(|_{m}), \\ &\ (^{*}| _{m})=(_{*m}_{mm}^{-1} _{m},}_{**}) p(^{*}|_{ m}),\] (3)

where the \(\) and \(\) matrices are formed by the same family of kernel functions, but some kernel hyperparameters are _decoupled_. See Table 1 for a comparison with other approximations. Decoupled conditionals improve model flexibility without relying on more inducing points and it applies to various SVGP-based models. We provide two examples of _decoupled_ models that we will evaluate.

Example 1: Decoupled lengthscales.By decoupling the kernel lengthscale \(l\) into \(l_{}\) for the mean and \(l_{}\) for the covariance, we enable the model to learn in settings where the function value changes more rapidly than the variance. In Eq. 3, this corresponds to the \(\) and \(\) kernel matrices being formed using separate lengthscales. For example, an RBF kernel gives

\[_{mm}=[(-_{i}-_{j}\|^{2} }{2l_{}^{2}})]_{_{i},_{j} },\ \ _{mm}=[(-_{i}-_{j}\|^{2}}{2 l_{}^{2}})]_{_{i},_{j}}.\]

We will denote the application of decoupled lengthscales to SVGP as **Decoupled Conditional SVGP (DCSVGP)**.

Example 2: Decoupled deep kernel learning.Wilson et al.  proposed deep kernel learning (DKL) which stacks a deep neural network feature extractor \(h\) with a GP layer. Combining DKL with variational GP models is straightforward, with the feature extractor \(h\) learned through ELBO with all other model (hyper)parameters . The feature extractor \(h\) can then be decoupled: one \(h_{}\) for the mean and one \(h_{}\) for the covariance. Again using the RBF kernel as a base example:

\[_{mm} =[(-}(_{i})-h_{ }(_{j})\|^{2}}{2l^{2}})]_{_{i}, _{j}},\] \[_{mm} =[(-}(_{i})-h_{ }(_{j})\|^{2}}{2l^{2}})]_{_{i}, _{j}}.\]

We denote the application of decoupled deep feature extractors to SVGP as **SVGP-DCDKL**.

Prediction.Using decoupled conditionals, the predictive posterior at a new point \(^{*}\) is

\[q(^{*})= p(^{*}|^{*})(^{*}| _{m})(_{m})\,d_{m}d^{*}= (^{*}|_{}(^{*}),_{ }(^{*})^{2}+^{2}),\]

where the predictive mean \(_{}(^{*})\) and latent function variance \(_{}(^{*})^{2}\) are similar to Eq. 1:

\[_{}(^{*})=_{*m}_{mm}^{-1} ,\;\;_{}(^{*})^{2}=}_{**}+_{*m}_{mm}^{-1}_{mm}^{-1} _{m*}.\] (4)

### The Evidence Lower Bound (ELBO)

In this section, we derive an ELBO for DCSVGP model fitting (other examples of decoupled models follow the same derivation). All model parameters and hyperparameters are learned by maximizing the resulting ELBO. Following the standard variational inference , we approximate the true posterior distribution \(p(,_{m}|)\) by the variational distribution \(q(,_{m})\) and minimize the KL divergence: \((q(,_{m})||p(,_{m}| ))\). In the standard case, \(q(,_{m})=p(|_{m})(_{m})\), but a decoupled model has \(q(,_{m})=(|_{m})(_{ m})\) since it further approximates the training conditional \(p(|_{m})\) by a decoupled one \((|_{m})\). This difference leads to the following ELBO for the decoupled model:

\[(p())(q)=[(p(|,_{m}))]-(q(,_{m})||p( ,_{m}))\] \[= _{i=1}^{n}\{(y_{i}|_{}(_{i}),^{2})-_{}(_{i})^{2 }}{2^{2}}\}-(|_{m})(_{m} )|_{m})(_{m})}{p(| _{m})p(_{m})}\,dd_{m}\] \[= _{i=1}^{n}\{(y_{i}|_{}(_{i}),^{2})-_{}(_{i})^{2 }}{2^{2}}\}-((_{m})||p(_{m}))- (|_{m})(_{m})|_{m})}{p(|_{m})}\,dd_{m}\] \[= _{i=1}^{n}\{(y_{i}|_{}(_{i}),^{2})-_{}(_{i})^{2 }}{2^{2}}\}-((_{m})||p(_{m}))- _{(_{m})}[((| _{m})||p(|_{m}))]}_{:=}.\]

We refer to App. A.1 for additional derivation details. Adding regularization parameters \(_{1}\) and \(_{2}\) to the KL divergence terms as is often done in practice, the ELBO for DCSVGP is

\[_{}=_{i=1}^{n}\{(y_{i}|_{}(_{i}),^{2})-_{ }(_{i})^{2}}{2^{2}}\}-_{1}[(_{m})||p(_{m})]-_{2}\] (5)

where the predictive mean \(_{}(_{i})\) and latent function variance \(_{}(_{i})^{2}\) are same as Eq. 4

\[_{}(_{i})=_{im}_{mm}^{-1} ,\;\;_{}(_{i})^{2}=}_{ii}+_{im}_{mm}^{-1}_{mm}^{-1} _{mi}.\] (6)

The explicit expression of \(\).The \(\) term can be computed explicitly (see App. A.1):

\[=_{(_{m})}[((|_ {m})||p(|_{m}))]=\,[_{m}^{T} _{m}]=(()+^{T} ),\]

where \(=^{T}}_{nn}^{-1}\), \(=_{nm}_{mm}^{-1}-_{nm}_{mm}^{-1}\), and \((_{m})=(,)\).

Comparing \(_{}\) and \(_{}\).The ELBO\({}_{}\) in Eq. 2 and \(_{}\) in Eq. 5 both consist of an approximate likelihood term and a KL divergence part. There are two differences: 1) ELBO\({}_{}\) involves different predictive mean \(_{}(_{i})\) and variance \(_{}(_{i})^{2}\) derived from decoupled conditionals, see Eq. 6; 2) ELBO\({}_{}\) contains an additional KL divergence term \(\), which is an expected KL divergence of the two training conditionals over the variational distribution \((_{m})\), regularizing the difference between the decoupled conditional \((|_{m})\) and the exact one \(p(|_{m})\).

The regularization parameter.In Eq. 5, \(_{}\) contains two KL divergence terms with regularization parameters \(_{1}\) and \(_{2}\), respectively. Varying \(_{1}\) controls the regularization on prior similarities, same as SVGP, while varying \(_{2}\) controls the regularization on the difference between the decoupled conditional and the exact one. In the limit where \(_{2}+\), decoupling is disallowed and DCSVGP degenerates to SVGP. See Sec. 5.2 for more discussion and empirical study.

Whitening.Decoupling the mean and covariance hyperparameters introduces a challenge where one can shorten with either \(_{mm}^{1/2}\) or \(_{mm}^{1/2}\), with the drawback of only simplifying one term. \(\)-whitening simplifies the KL divergence but leaves the predictive distribution significantly "less linear", while \(\)-whitening does the opposite. For example, the predictive distribution from the two whitening choices are:

\[_{}(_{i})=_{im}_{mm}^{-1/2}},\ \ _{}(_{i})^{2}=}_{ii}+ _{im}_{mm}^{-1/2}}_{mm}^{-1/2} _{mi},\] \[_{}(_{i})=_{im}_{mm}^{-1}_{mm}^{1/2}},\ \ _{}(_{i})^{2}=}_{ii}+ _{im}_{mm}^{-1}_{mm}^{1/2}} _{mm}^{1/2}_{mm}^{-1}_{mi}.\]

Empirically \(\)-whitening performs better because a simplified predictive distribution is more favorable for model fitting. See App. A.2 for KL divergence terms and derivation details.

## 4 Related Work

Variational GPs have been successfully extended to various settings [1; 3; 4; 11; 16; 19; 20; 31; 51; 55; 54]. Among these vast enhancements, much attention has been devoted to the study of inducing points, the core part that leads to an expressiveness-and-complexity trade-off. Many works have studied better placement of inducing points, including selecting from training data [9; 24; 42; 47; 7; 34] and from the input domain [40; 50], but typically inducing points are learned as model parameters [17; 48]. On the other hand, several schemes have been developed to reduce complexity and allow more inducing points, e.g. Fourier methods [19; 29] and methods that decouple inducing points used in variational mean and covariance [8; 15; 41]. However, more inducing points could be less informative under certain data characteristics [6; 16; 50] or result in suboptimal model fitting [15; 57]. Therefore, our work takes a different direction to improve variational GPs via more flexible mean and covariance modeling rather than replying on more inducing points. The work that is most closely related to ours is the ODSVGP model  that decouples the inducing points for mean and variance to allow more inducing points used in mean modeling. With similar decoupling idea, we are motivated by increasing model flexibility rather than reducing model complexity and we simply decouple the kernel hyperparameters with negligible additional costs.

## 5 Experiments

We evaluate the performance of decoupled models proposed in Sec. 3.2: DCSVGP (variational GPs using decoupled lengthscales) and SVGP-DCDKL (variational GPs with deep kernel learning using decoupled deep feature extractors). Because PPGPR  is orthogonal to our decoupling method, similarly we also evaluate DCPPGPR (decoupled lengthscales) and PPGPR-DCDKL (decoupled feature extractors). All experiments use an RBF kernel and a zero prior mean and are accelerated through GPyTorch  on a single GPU. Code is available at https://github.com/xinranzhu/Variational-GP-Decoupled-Conditions.

### Regression Tasks

We consider 10 UCI regression datasets  with up to 386508 training examples and up to 380 dimensions. We present main results with non-ARD RBF kernels, the \(\)-whitening scheme described in Sec. 3.3, and set \(_{2}=13\). Results are averaged over 10 random dataset splits. For additional results with ARD kernels and \(\)-whitening, see App. B.1.4 and B.1.5. For more experiment setup and training details such as the number of inducing points, we refer to App. B.1.

DCSVGP and DCPPGPR.We first compare DCSVGP to baseline SVGP  and ODSVGP  and similarly compare DCPPGPR to PPGPR  and ODPPGPR. Table 2 shows the test RMSE and NLL results. We observe that DCSVGP (and similarly DCPPGPR) yields the lowest RMSE and NLL on 8 datasets, demonstrating the improved flexibility of our method. We report all learned lengthscales in App. B.1.1. On the other hand, we also note that on Protein and Elevators, DCPPGPR slightly overfits with worse test NLL. We show in Sec. 5.2 that increasing \(_{2}\) resolves this issue. We also find that decoupling lengthscales generally improves model calibration (see App. B.1.3 for results and discussions).

SVGP-DCDKL and PPGPR-DCDKL.We then compare SVGP-DCDKL with baseline model SVGP-DKL  and comare PPGPR-DCDKL with baseline model PPGPR-DKL . Here, all models learn inducing points in the input space rather than in the feature space and we refer to App. B.1.6 for more discussion on this choice and supporting results. Table 3 shows the test RMSE and NLL and we observe that SVGP-DCDKL yields better (or equivalent) RMSE and NLL on all datasets but the Slice dataset; PPGPR-DCDKL always yields better (or equivalent) RMSE and only obtains worse NLL on 3 datasets due to an overfitting issue similar to DCPPGRP.

### Ablation Study On \(_{2}\)

In Sec. 5.1, we evaluate decoupled models with fixed \(_{2}=1\)e-\(3\) and observe mostly superior performance except for occasional overfitting with DCPPGRP and PPGPR-DCDKL. Here, we study how the regularization parameter \(_{2}\) affects model performance. We evaluate DCPPGRP with \(_{2}\{0,0.001,0.005,0.01,0.05,0.1,0.5,1.0\}\) on two datasets-Pol and Protein- that represent our general findings.

The first row of Fig. 2 shows performance on Pol: DCPPGRP gets lower RMSE and NLL with decreasing \(_{2}\), and \(_{2} 0\) is desirable. We also see how the decoupled lengthscales \(l_{}\) and \(l_{}\) diverge from PPGPR's lengthscale as \(_{2}\) decreases. The second row of Fig. 2 shows results on Protein: DCPPGR gets better RMSE but worse NLL as \(_{2}\) decreases and thus overfits when \(_{2}<0.1\). In this case, \(_{2}=0.1\) is the best. We refer to App. B.2 for more supporting results, and conclude that 1) small \(_{2}\) or even 0 is usually ideal and 2) adding back some small regularization \(_{2}=0.1\) appears to resolve the instances of overfitting we noticed.

  
**Test RMSE** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP & 0.313 & 0.380 & 0.294 & 0.186 & 0.662 & 0.089 & 0.131 & **0.122** & 0.511 & 0.797 \\ ODSVGP & 0.321 & **0.373** & **0.222** & 0.175 & 0.667 & 0.093 & 0.087 & **0.121** & 0.534 & 0.794 \\ DCSVGP & **0.156** & 0.379 & 0.286 & **0.150** & **0.604** & **0.086** & **0.039** & **0.121** & **0.434** & **0.777** \\  PPGPR & 0.306 & 0.392 & 0.377 & 0.282 & 0.659 & **0.091** & 0.205 & 0.125 & 0.552 & 0.780 \\ ODPPGPR & 0.333 & **0.376** & **0.277** & 0.394 & 0.647 & **0.090** & 0.092 & **0.123** & 0.565 & **0.778** \\ DCPPGRP & **0.178** & 0.395 & 0.348 & **0.226** & **0.632** & **0.089** & **0.042** & **0.124** & **0.543** & **0.779** \\  
**Test NLL** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP & 0.331 & 0.452 & 0.207 & -0.188 & 1.013 & -1.018 & -0.409 & -0.683 & 0.752 & 1.192 \\ ODSVGP & 0.278 & **0.433** & **-0.094** & -0.354 & 1.011 & **-1.029** & -0.578 & **-0.694** & 0.797 & 1.187 \\ DCSVGP & **-0.373** & 0.450 & 0.165 & **-0.502** & **0.919** & **-1.047** & **-1.293** & **-0.697** & **0.586** & **1.166** \\  PPGPR & -0.056 & **0.377** & -0.715 & -0.772 & **0.804** & -1.606 & -0.906 & -1.801 & 0.260 & 1.112 \\ ODPPGPR & -0.064 & **0.382** & -0.842 & -0.972 & **0.812** & -1.603 & -0.937 & -1.791 & 0.316 & 1.111 \\ DCPPGRP & **-0.588** & 0.403 & **-0.901** & **-1.067** & 0.854 & **-1.648** & **-1.574** & **-1.904** & **0.227** & **1.109** \\   

Table 2: Test RMSE and NLL on 10 regression datasets (lower is better). Results are averaged over 10 random train/validation/test splits. Statistical significance is indicated by **bold**. See the supplement for standard errors.

  
**Test RMSE** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP-DKL & 0.0614 & **0.343** & 0.013 & 0.067 & 0.598 & **0.0864** & **0.0189** & **0.120** & 0.329 & **0.773** \\ SVGP-DCDKL & **0.0513** & **0.343** & **0.011** & **0.047** & **0.577** & **0.0855** & 0.0223 & **0.119** & **0.288** & **0.774** \\  PPGPR-DCDKL & 0.0847 & **0.343** & 0.030 & 0.082 & **0.597** & **0.0871** & **0.0176** & **0.121** & 0.375 & **0.771** \\ PPGPR-DCDKL & **0.0593** & **0.346** & **0.027** & **0.061** & **0.594** & **0.0870** & **0.0171** & **0.120** & **0.350** & **0.771** \\  
**Test NLL** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP-DKL & -1.388 & **0.347** & -2.590 & -1.227 & 0.905 & **-1.045** & **-2.550** & **-0.712** & 0.320 & **1.161** \\ SVGP-DCDKL & **-1.600** & **0.348** & **-2.786** & **-1.637** & **0.888** & **-1.052** & -2.379 & **-0.713** & **0.173** & 1.164 \\  PPGPR-DKL & -2.324 & **0.303** & -3.096 & -1.739 & **0.712** & **-1.627** & -2.625 & **-1.951** & -0.232 & **1.098** \\ PPGPR-DCDKL & **-2.584** & 0.358 & **-3.192** & **-2.186** & 1.194 & **-1.650** & **-2.676** & **-1.911** & **-0.340** & 1.117 \\   

Table 3: Test RMSE and NLL on 10 regression datasets (lower is better). Results are averaged over 10 random train/validation/test splits. Statistical significance is indicated by **bold**. See the supplement for standard errors.

### Applications to Bayesian Optimization (BO)

In this section, we apply our decoupled variational GPs to BO tasks . We consider 3 real BO tasks: the rover trajectory planning problem (Rover) [12; 52], the lunar landing reinforcement learning (Lunar) , and a challenging molecule design task Randolazine MPO (Ranolazine) [5; 33]. We take TuRBO  as the BO algorithm and use different variational GPs in TuRBO. On Rover and Lunar, we use SVGP with TuRBO and compare SVGP with DCSVGP. On Randolazine we first reduce the problem dimension to 256 using a Variational Autoencoder (VAE) , then perform BO in the latent space using PPGPR-DKL with TuRBO . We compare PPGPR-DKL with PPGPR-DCDKL on Ranolazine. Consistent with our ablation study in Sec. 5.2, we find that \(_{2}=0\) performs the best on Rover and Randolazine while Lunar requires more regularization with \(_{2}=0.1\). Fig. 3 summarizes optimization performance averaged on at least 10 runs - all decoupled models outperform their coupled counterparts, showing the advantage of decoupling conditionals in BO applications.

### Plausible Extensions

We further explore extensions of decoupled conditionals to achieve more flexible predictive mean and variance of variational GPs. Beyond decoupling the same parametric form, we could model the predictive mean and covariance differently for a better fit. Despite lack of theoretical grounding, we evaluate two possibilities, both targeting a more complex predictive mean than the covariance.

Replace the predictive mean of SVGP by a neural network.In fact, the importance of the parametric form of the covariance field is mentioned in Jankowiak et al. : "a good ansatz for the predictive variance is important for good uncertainty prediction". They suggest, but never implement

Figure 3: Optimization performance in terms of objective values on tasks Rover (dim=60), Lunar (dim=12) and Randolazine (dim=256) is shown (higher is better). We compare baseline GP models (SVGP or PPGPR-DKL) with their decoupled counterparts (DCSVGP or PPGPR-DCDKL). Decoupling conditionals results in significant improvement on all tasks. See Sec. 5.3 for details.

Figure 2: We evaluate DCPPGPR with varying \(_{2}\) on dataset Pol (**first row**) and Protein (**second row**). Results are averaged over 10 random dataset splits with standard errors included. Solid blue lines show DCPPGPR with nonzero \(_{2}\) and green dashed lines show DCPPGPR with \(_{2}=0\). Baseline PPGPR is shown in orange dotted line. **First two columns** contain test RMSE and NLL (lower is better). **Last two columns** show the learned decoupled lengthscales \(l_{}\) and \(l_{}\). PPGPR has equal \(l_{}\) and \(l_{}\) due to no decouling. See Sec. 5.2 for details.

or evaluate, that one could replace the mean function with a neural network so that inducing points are only used for variance prediction. We empirically evaluate this idea, denoted as NNSVGP (and NNPPGPR). With a totally flexible predictive mean, NNSVGP and NNPPGPR yield best test RMSE over 9 out of 10 datasets (see supplement for the RMSE table). However, with the predictive mean fully independent of the GP framework, NNSVGP and NNPPGPR yield worse NLL typically. Table 4 reports test NLL, where results are generally mixed.

Simplified deep kernels for covariances.In the DKL setting, instead of using decoupled feature maps, one could use entirely different neural network architectures for the predictive mean and covariance. We empirically find that a simpler feature mapping, or even no feature mapping, for the covariance still make a plausible model with good performance but smaller training cost. We denote the method with no feature mapping for the covariance as SVGP-MeanDKL and PPGPR-MeanDKL.

We present results in Table 5. We observe SVGP-MeanDKL does not significantly outperform the decoupled models SVGP-DCDKL, but it improves baseline SVGP-DKL on 8 out of 10 datasets. PPGPR-MeanDKL performs similarly in terms of the RMSE metric (better than baseline PPGPR-DKL but worse than PPGPR-DCDKL). However, in terms of NLL, it gives poor uncertainty prediction (worse NLL). This suggests that it is more beneficial to have different models (decoupled feature extractors in SVGP-DCDKL) for conditional mean and covariance rather than only modeling the conditional mean (SVGP-MeanDKL) or having same models (SVGP-DKL), and SVGP-based model is more robust than the PPGPR-based model in uncertainty prediction.

## 6 Conclusion

Variational GPs scale approximate GP inference to millions of training examples but may yield suboptimal accuracy due to lack of model expressiveness or poor model fitting. We propose a simple idea to improve model expressiveness by decoupling the parametric form of the mean and variance in the conditionals of variational GPs. We derive an ELBO for our model with decoupled conditionals, which end up being similar to the ELBO of standard variational GP with an additional a regularization

    & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP & 0.331 & 0.452 & 0.207 & -0.188 & 1.013 & -1.018 & -0.409 & -0.683 & 0.752 & 1.192 \\ DCSVGP & -0.373 & 0.450 & 0.165 & -0.502 & 0.919 & **-1.047** & **-1.293** & -0.697 & **0.586** & **1.166** \\ NNSVGP & **-1.050** & **0.419** & **-1.231** & **-0.827** & **0.895** & **-1.060** & -0.948 & **-0.725** & 0.776 & 1.171 \\  PPGPR & -0.056 & **0.377** & -0.715 & -0.772 & 0.804 & -1.606 & -0.906 & -1.801 & 0.260 & 1.112 \\ DCPPGPR & -0.588 & 0.403 & -0.901 & -1.067 & 0.854 & **-1.648** & **-1.574** & **-1.904** & **0.227** & **1.109** \\ NNPPGPR & **-1.169** & 0.552 & **-1.560** & **-1.074** & **0.766** & -1.514 & -1.369 & -1.632 & 0.412 & 1.113 \\   

Table 4: Test NLL on 10 regression datasets (lower is better). Results are averaged over 10 random train/validation/test splits. Best ones with statistical significance are bold. See Sec. 5.4 for details.

  
**Test RMSE** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP-DKL & 0.0614 & **0.343** & **0.013** & 0.067 & 0.598 & **0.0864** & **0.0189** & **0.120** & 0.329 & 0.773 \\ SVGP-DCDKL & **0.0513** & **0.343** & **0.011** & **0.047** & **0.577** & **0.0855** & 0.0223 & **0.119** & **0.288** & 0.774 \\ SVGP-MeanDKL & 0.0576 & **0.344** & 0.027 & **0.048** & **0.576** & **0.0859** & 0.0259 & **0.119** & **0.285** & **0.771** \\  PPGPR-DKL & 0.0847 & **0.343** & **0.030** & 0.082 & **0.597** & **0.0871** & **0.0176** & **0.121** & 0.375 & **0.771** \\ PPGPR-DCDKL & **0.0593** & **0.346** & **0.027** & **0.061** & **0.594** & **0.0870** & **0.0171** & **0.120** & **0.350** & **0.771** \\ PPGPR-MeanDKL & 0.0736 & **0.344** & 0.055 & **0.062** & **0.594** & **0.0868** & 0.0223 & **0.121** & **0.349** & **0.771** \\  
**Test NLL** & Pol & Elevators & Bike & Kin40k & Protein & Keggdir & Slice & Keggundir & 3Droad & Song \\  SVGP-DKL & -1.388 & **0.347** & -2.590 & -1.227 & 0.905 & **-1.045** & **-2.550** & **-0.712** & 0.320 & **1.161** \\ SVGP-DCDKL & **-1.600** & **0.348** & **-2.786** & **-1.637** & **0.888** & **-1.052** & -2.379 & **-0.713** & **0.173** & 1.164 \\ SVGP-MeanDKL & -1.450 & **0.349** & -1.989 & **-1.645** & **0.883** & **-1.053** & -1.777 & **-0.715** & **0.163** & **1.160** \\  PPGPR-DKL & -2.324 & **0.303** & -3.096 & -1.739 & **0.712** & **-1.627** & -2.625 & **-1.951** & -0.232 & **1.098** \\ PPGPR-DCDKL & **-2.584** & 0.358 & **-3.192** & **-2.186** & 1.194 & **-1.650** & **-2.676** & **-1.911** & **-0.340** & 1.117 \\ PPGPR-MeanDKL & -2.362 & 0.312 & -2.831 & -2.150 & 1.048 & **-1.666** & -2.175 & **-1.906** & **-0.344** & **1.099** \\   

Table 5: Test RMSE and NLL on 10 regression datasets (lower is better). Results are averaged over 10 random train/validation/test splits. Best ones with statistical significance are bold. See Sec Sec. 5.4 for details.

term. Our method is simple yet effective, and it applies to various variational GPs. We provide two concrete examples, one decoupling kernel lengthscales in the basic variational GP setting and one decoupling the feature mapping in the deep kernel learning setting. Through extensive empirical study, we show that the decoupled conditionals effectively improve the accuracy of variational GPs in terms of both mean and uncertainty prediction. We also empirically study two plausible extensions of our method, motivated by the idea of modeling the mean and variance differently, but we conclude that they are not as effective as our decoupled models. Current limitations of our work and therefore future directions include but not limit to: 1) the application of decoupled conditionals to more SVGP-based models; 2) further generalization of flexible forms of training and testing conditionals that improve model performance; 3) the application of decoupled variational GP framework to tasks other than regression or BO, such as classification tasks.

## 7 Acknowledgements

We acknowledge support from Simons Foundation. This work was supported by a grant from the Simons Foundation (601956, DSB). JRG is supported by NSF award IIS-2145644.