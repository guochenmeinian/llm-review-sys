# 1. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.

WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts

Jiahuan Cao\({}^{1,3}\), Yang Liu\({}^{1,3}\), Yongxin Shi\({}^{1,3}\), Kai Ding\({}^{2,3}\), Lianwen Jin\({}^{*}\)\({}^{1,3}\)

\({}^{1}\)South China University of Technology

\({}^{2}\)INTSIG Information Co., Ltd

\({}^{3}\)INTSIG-SCUT Joint Lab on Document Analysis and Recognition

jiahuanc@foxmail.com, ly10061105@gmail.com, yongxin_shi@foxmail.com

danny_ding@intsig.net, eelwjin@scut.edu.cn

Equal contribution

###### Abstract

Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results. Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. Our benchmark and related code are available at https://github.com/SCUT-DLVCLab/WenMind.

## 1 Introduction

The emergence of Large Language Models (LLMs) has led to significant advancements in natural language processing and understanding across a wide range of domains, from finance [1; 2; 3] and law [4; 5; 6] to healthcare [7; 8; 9]. However, the capabilities of LLMs in the domain of Chinese Classical Literature and Language Arts (CCLLA) have not been fully explored, largely due to the lack of comprehensive benchmarks. CCLLA, which encompasses the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, serves as an essential bridge connecting ancient Chinese culture to the modern world. Evaluating and improving the performance of LLMs in this domain is crucial for furthering research and applications in CCLLA. Unfortunately, existing benchmarks focus primarily on the Ancient Prose sub-domain or contain only a limited number of tasks, making it challenging to provide a thorough and holistic assessment of LLMs' capabilities in CCLLA.

To address this gap, we introduce the WenMind benchmark for a comprehensive evaluation of LLMs' CCLLA capacities. Our WenMind offers several benefits over existing CCLLA benchmarks. **(a)**

**Comprehensive coverage**: Unlike current benchmarks that concentrate primarily on Ancient Prose, WenMind provides a holistic approach by encompassing all three sub-domains. **(b) Substantial number of tasks**: As depicted in Figure 1, WenMind consists of 42 fine-grained tasks, which is a remarkable twofold increase over the largest existing benchmark. **(c) Variety of question formats**: WenMind incorporates a wide array of question formats, including multiple choice, fill-in-the-blank, and open-ended questions. This diversity allows for a more detailed analysis of an LLM's grasp of CCLLA. The comparison between WenMind and other benchmarks is shown in Table 1. Figure 2 presents samples of WenMind's data.

Using WenMind, we thoroughly evaluate 31 representative LLMs, including general models in Chinese and English, as well as specialized models for ancient Chinese. Our findings indicate that even the top-performing model, ERNIE-4.0 , only achieves the highest score of 64.3, suggesting considerable room for improvement in the CCLLA domain. Further analysis shows that the lack of knowledge of CCLLA is the main reason for existing LLMs' poor performance. Unexpectedly, LLMs specifically for ancient Chinese rather underperform the general ones. We think this is because the incremental pre-training corpus is not extensive enough to cover the scope of CCLLA, and concurrently leads to a catastrophic forgetting of the generic knowledge. Furthermore, we compare traditional metrics with model scoring metric in translation and punctuation tasks, providing a comparative analysis of their respective merits and limitations.

In summary, our contributions are as follows:

* We introduce WenMind, a novel comprehensive evaluation benchmark specific for LLMs in CCLLA, which covers all three sub-domains in this domain and contains a diverse range of tasks and question formats, facilitating a rigorous and thorough evaluation of LLMs' capabilities.

Figure 1: Overview of WenMind Benchmark, which covers 3 sub-domains and 42 fine-gained tasks.

Figure 2: Samples of WenMind’s data.

* Using WenMind, we conduct an extensive and thorough evaluation of 31 representative LLMs, revealing and quantifying their performance in the CCLLA domain.
* We perform an in-depth analysis of the evaluation results and obtain valuable insights, offering significant guidance and profound understanding for future research on LLMs in the CCLLA domain.

## 2 Related Work

### General Chinese Benchmark for LLMs

To evaluate the performance of LLMs across diverse Chinese language tasks, several benchmarks have been proposed [24; 25; 26]. CLUE  is the first large-scale Chinese comprehension benchmark, including nine tasks such as sentence classification and reading comprehension. SuperCLUE  expands the evaluation to include user queries, open-ended dialogues, and closed-ended questions, focusing on real-world applications. CMMLU  aims to comprehensively assess the knowledge and reasoning capabilities of LLMs in Chinese, covering 67 subjects from basic to advanced levels. C-Eval  offers a thorough evaluation suite with 13,948 multiple-choice questions spanning 52 subjects, targeting foundational knowledge and reasoning skills. HalluQA  addresses the hallucination phenomenon in Chinese LLMs with 450 adversarial questions encompassing cultural and social aspects. AlignBench  evaluates the alignment of models across multiple dimensions. CBBQ , a bias benchmark, covers stereotypes and social biases relevant to Chinese culture with over 100 expert-constructed questions.

### CCLLA Benchmark for LLMs

Unlike general Chinese benchmarks, CCLLA benchmarks are limited in number. C-CLUE  provides a dataset for evaluating named entity recognition and relation extraction. CCLUE  offers five evaluation tasks, including sequence labeling and sentence classification. ACLUE  proposes a benchmark to evaluate classical Chinese understanding capabilities of LLMs, with the form of multiple-choice questions. WYWEB  offers nine tasks, such as text classification, punctuation and machine translation. However, these existing CCLLA benchmarks contain only a small number of tasks or are organized using only a single multiple-choice question format. Compared to these datasets, WenMind covers a wider variety of tasks and formats, and employs evaluation methods that align more closely with human intuition, allowing it to more accurately and comprehensively reflect the capabilities of LLMs in CCLLA.

  
**Dataset** & **Year** & **Domain** & **License** & **Scale** & **\# Tasks** & **\# QF** &  &  \\   C-Eval  & 2023 & General & CC BY-NC-SA-4.0 & 457 & 2 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CIF-Bench  & 2024 & General & - & 150 & 3 & 1 & ✓ & ✓ & ✓ & ✗ & ✗ \\ CMMLU  & 2023 & General & CC BY-NC-4.0 & 620 & 3 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ GAOK-Bench  & 2023 & General & Apache-2.0 & 145 & 4 & 3 & ✓ & ✓ & ✗ & ✗ & ✗ \\ XiezhiBenchmark  & 2023 & General & CC BY-NC-SA-4.0 & 2,060 & 2 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ ACLUE  & 2023 & CCLLA & CC BY-NC-4.0 & 4,967 & 15 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ C-CLUE  & 2021 & CCLLA & CC BY-SA-4.0 & 1,122 & 3 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CCLUE  & 2021 & CCLLA & Apache-2.0 & 36,319 & 5 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CPM  & 2021 & CCLLA & - & 2,720 & 1 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ THIAIber [20; 21; 22] & 2020 & CCLLA & - & 5,173 & 2 & 2 & ✓ & ✗ & ✗ & ✗ & ✗ \\ WYWEB  & 2023 & CCLLA & - & 69,700 & 9 & 2 & ✓ & ✗ & ✗ & ✗ & ✗ \\ 
**WenMind (Ours)** & 2024 & CCLLA & CC BY-NC-SA-4.0 & 4,875 & **42** & 3 & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of existing datasets. “CCLLA” represents “Chinese Classical Literature and Language Arts”; “QF” represents “Question Fromat”; “TM” represents “Traditional Metrics”; “MSM” represents “Model Scoring Metric”; “Method” represents “Construction Method of dataset”; “HG” represents “Human Generated”; “CI” represents “Collection and Improvement of existing datasets”; “MC” represents “Model Constructed”. Datasets with the domain “General” only count relevant data entries in the CCLLA field.

## 3 WenMind Benchmark

### Task Definition

This study aims to construct a comprehensive benchmark for the evaluation of LLMs' capabilities in Chinese Classical Literature and Language Arts (CCLLA). In general, the CCLLA contains three sub-domains: Ancient Prose, Ancient Poetry, and Ancient Literary Culture. Therefore, we define the following tasks for these domains, respectively.

**Ancient Prose** Ancient prose refers to a form of classical Chinese literature, which characterized by simplicity, elegance and a high degree of freedom. It is commonly used in historical records, philosophical texts, agricultural books, _etc_. To evaluate the models' capabilities in Ancient Prose, we design a set of tasks that assess the understanding and generation, respectively. We focus on the models' **understanding capability at both sentence level and word level** through 11 tasks: Tasks that pertain to the sentence level include sentence structure, classical Chinese to modern Chinese, modern Chinese to classical Chinese, topic classification, and reading comprehension. On the other hand, tasks that pertain to the word level encompass named entity recognition, punctuation, word explanation, function words, homophones, and polysemy. For **generation** capability, we directly utilize ancient prose writing as the evaluation task.

**Ancient Poetry** Ancient poetry is a special form of classical Chinese literature, which follows a strict rhyme scheme and often expresses rich emotions in refined language. Considering the characteristics of ancient poetry, we design the following tasks to assess the ability in three dimensions of LLMs. (a) Appreciation, ancient poetry translation, sentiment classification, and ancient poetry to English, for **understanding** capability. (b) Ancient poetry writing for **generation** capability. (c) Basic Q&A, poet introduction, and analysis of imagery, for **knowledge** capability.

**Ancient Literary Culture** Ancient Literary Culture refers to literary forms other than Ancient Prose and Ancient Poetry, such as riddle, idiom, _etc_. We primarily assess the generation and knowledge capabilities of LLMs in this sub-domain. (a) For **generation** capability, we utilize couplet as the evaluation task. (b) For **knowledge** capability, we utilize 5 tasks, including idiom, riddle, Xiehouyu, historical Chinese phonology, and sinology.

### Data Construction

The construction pipeline of WenMind includes data collection and data processing, as illustrated in Figure 3.

Data CollectionOur data collection process includes three main sources. **(a) Internet:** We collect authentic examination questions from Chinese language exams and poetry competitions, which are carefully curated by experts and scholars, to serve as part of our data. Additionally, we gather texts that are enriched with CCLLA knowledge from the Internet, such as introductions of poets and interpretations of idioms. Utilize the knowledge in these texts, we manually construct question-answer pairs for various tasks. **(b) Open-Source Datasets:** Leveraging open-source corpora like C2MChn  and the Daizhige Corpus  as foundational resources, we perform a series of operations, including text filtering, question crafting, and format standardization, to generate question-answer pairs. Given the challenge of acquiring data for certain tasks and to ensure a thorough and holistic evaluation within the CCLLA domain, we also incorporate test cases from other open-source datasets, such as ACLUE , WYWEB , and THU-FSPC . **(c) LLM:** Tasks such as ancient poetry and prose writing are inherently open-ended and do not have fixed correct answers. For these tasks, we initially design a range of questions and then employ the ERNIE-3.5 model  to generate reference answers. These answers are further reviewed, filtered, and refined through a manual process. It is notable that the WenMind benchmark is released under the CC-BY-NC-SA-4.0 license and strictly adheres to the agreements of the original data sources. For more details, please refer to Appendix B.

Data ProcessingTo ensure the data quality, we perform a series of data processing on the collected data. **(a) Question Segmentation.** Some question-answer pairs consist of multiple questions intertwined. We employ the ERNIE-3.5 model  to distinguish between multiple questions and answers mixed together. **(b) Conversion and Standardization of Question-Answer Pairs.** Weensure that the data is presented in question-answer pairs, standardized in a dictionary format. Each sample is accompanied by metadata, including the task name and the capability being assessed. **(c) Data Deduplication.** A combination of MinHashLSH  and field-matching methods is employed to remove duplicate questions. **(d) Removal of Irrelevant Symbols and Content.** We utilize regular expressions, detection of irrelevant characters, and other methods to eliminate unnecessary English characters, abnormal symbols, erroneous data, _etc_. **(e) Manual Proofreading.** We manually verify all question-answer pairs to ensure the integrity of the questions and the correctness of the answers.

### Data Statistics

We present the statistics of WenMind in Figure 4 and Table 2. The WenMind benchmark comprises a total of 4,875 entries, encompassing 26 coarse-grained tasks and 42 fine-grained tasks. Tasks involving Ancient Prose, Ancient Poetry, and Ancient Literary Culture respectively account for 39%, 38%, and 23% of the benchmark, showing a relatively balanced distribution. Among these, the task with the highest number of entries is basic Q&A on ancient poetry, constituting approximately 15% of the benchmark, while tasks such as English translations of ancient poetry have the fewest entries, at around 1%. This reflects the benchmark's design, which prioritizes tasks based on their common occurrence and general applicability. From the question format perspective, WenMind comprises 3,928 open-ended questions, 917 multiple-choice questions, and 30 fill-in-the-blank

Figure 4: Data statistics of WenMind: Distributions of (a) sentence length, (b) sub-domains and (c) capabilities. Zoom in for better view.

Figure 3: Construction pipeline of WenMind Benchmark. Zoom in for better view.

questions. Regarding cognitive capabilities, questions assessing understanding, generation, and knowledge account for 51%, 10%, and 38% respectively.

## 4 Experiment

### Models

We conduct an extensive evaluation of 31 models, both proprietary and open-source, encompassing English-centric models such as GPT-4  and LLaMA [39; 40], Chinese-centric models like Qwen , Baichuan  and ChatGLM [43; 44], models fine-tuned from English to Chinese such as LLaMA-Chinese [45; 46], and specialized models for ancient Chinese, including Xunzi  and Chunhua . Additionally, we assess different-sized variants within the same model family to reveal the effect of model size on their performance in the CCLLA domain. Details of the evaluated models are presented in Appendix C.

### Experiment Setup

For open-source and closed-source models we evaluate them through local access and API calls, respectively. To ensure a fair comparison, we standardize the inference settings for all evaluated LLMs. Specifically, we employ half-precision inference with bf16 and greedy decoding strategy with a maximum generation length set to 2048. The temperature parameter, Top-p sampling, and Top-k sampling are set to 1, 1, and 50, respectively. To evaluate the knowledge of LLM itself, we prohibit the use of external search engines for closed-source models. For model scoring, we randomly sample 100 instances and score them using LLMs three times, achieving an average error within 2%. This level of precision indicates that LLMs exhibit scoring stability, and therefore, we decide not to pursue multiple averaging scores due to cost considerations. All experiments are conducted on a NVIDIA A6000 GPU.

### Evaluation Metrics

Model Scoring MetricThe evaluation of the WenMind benchmark mainly employs model scoring. An optimal scoring model should meet the following criteria: (a) Exhibit excellent instruction-following capabilities, generating scores based on specified prompts and outputting them in the required format. (b) Possess extensive knowledge in the CCLLA domain to aid the scoring process. (c) Balance scoring effectiveness and cost efficiency. (d) Align closely with human subjective judgment in scoring results. After experimentation, the ERNIE-3.5 model  has been selected as the experimental scoring model, as it exhibits a consistency of approximately 89.4% with human evaluation, meeting the required criteria. More details can be found in Appendix C.

Furthermore, the scoring requirements vary for different formats of questions. (a) For multiple-choice questions with only one correct option, the model gives a score of 0 (incorrect) or 1 (correct). (b) For multiple-choice questions with multiple correct options, the model gives a score of 0 (incorrect options present), 0.5 (partially correct options), or 1 (all options correct). (c) For subjective questions without a standard answer, the model scores between 0 and 1 based on the given requirements. (d) For Q&A questions with a standard answer, the model identifies several scoring points (\(P_{all}\)) based on the reference answer and then determines the number of these points present in the LLM's response (\(P_{obtain}\)) to calculate the score for the question(\(P_{obtain}\)/\(P_{all}\)). All indicators are multiplied by one hundred to obtain final scores. The scoring prompts for various tasks refer to Appendix C.2.

  
**Domain** & **Tasks** & **\#Q** & **Max. \#Q** & **Min. \#Q** & **Avg. Q Tokens** & **Avg. A Tokens** \\  Ancient Prose & 15 & 1,900 & 200 & 7 & 107.51 & 62.12 \\ Ancient Poetry & 16 & 1,845 & 200 & 20 & 73.42 & 94.93 \\ Ancient Literary Culture & 11 & 1,130 & 100 & 100 & 26.68 & 14.26 \\ 
**Overall** & 42 & 4,875 & 200 & 7 & 75.87 & 63.44 \\   

Table 2: The statistics of the WenMind Benchmark. “Q” represents “Question” and “A” represents “Answer”.

Traditional MetricsIn tasks with strong objectivity or where scoring models struggle to extract the correct answers, we provide traditional metric as a reference. For punctuation task, we use the F1-score . For the translation tasks of classical Chinese to modern Chinese, modern Chinese to classical Chinese, classical poetry to modern Chinese, and classical poetry to English, we utilize BLEU .

### Results and Analysis

This experiment evaluates the performance of 31 models on 26 coarse-grained tasks, as shown in Table 3 and 4. Detailed metrics for LLMs on 42 fine-grained tasks are provided in Appendix C. The tasks represented by T1-T26 are as follows: T1-Sentence structure, T2-Classical Chinese to modern Chinese, T3-Modern Chinese to classical Chinese, T4-Named entity recognition, T5-Punctuation, T6-Topic classification, T7-Word explanation, T8-Reading comprehension, T9-Function words, T10-Homophones, T11-Polysemy, T12-Ancient prose writing, T13-Ampreciation, T14-Ancient poetry writing, T15-Basic Q&A, T16-Ancient poetry translation, T17-Sentiment classification, T18-Ancient poetry to English, T19-Poot introduction, T20-Analysis of imagery, T21-Couplet, T22-Idiom, T23-Riddle, T24-Xiehouyu, T25-Historical Chinese phonology, T26-Knowledge of sinology Q&A. Based on the results, we draw the following insights.

**There is considerable room for improvement of existing LLMs' CCLLA capabilities.** ERNIE-4.0  performs the best with a score of 64.3. LLaMA2-7B-Chat  has the lowest score, only 13.0. The average score of the 31 LLMs is approximately 41.2. Most models have scores between 20 and 60, with over 64% of the LLMs scoring below 50. Overall, the scores are relatively low, suggesting there is considerable room for improvement.

**Pre-training data matters.** General-purpose Chinese models demonstrate superior performance in the CCLLA domain. Additionally, fine-tuning general-purpose English models with Chinese data improves their performance, yet such enhancements do not fully match the level of proficiency of the general-purpose Chinese model, highlighting the significance of Chinese pre-training data in the CCLLA domain.

**Incremental pre-training in the CCLLA domain may not be effective.** Unexpectedly, the models specifically designed for ancient Chinese with incremental pre-training and fine-tuning, namely Ancient-Chat-LLM-7B , Bloom-7B-Chunhua , and Xunzi-Qwen1.5-7B , show subpar performance in the CCLLA domain, achieving an average score of 34.1. This indicates that incremental pre-training in the CCLLA domain may not be effective. The possible reason could be the incremental pre-training and fine-tuning data being insufficient to cover a wide range of knowledge and tasks, which concurrently leads to more profound catastrophic forgetting of CCLLA-related knowledge from the pre-training corpus.

**Large Language Models lack sufficient knowledge in the CCLLA domain.** As shown in Table 4, LLMs exhibit substantially different performance across various domains and capabilities dimension. On different capability dimensions, LLMs perform significantly worse in knowledge than in generation and understanding, indicating that LLMs lack sufficient knowledge in the CCLLA domain. Moreover, the fields of Ancient Poetry and Ancient Literary Culture contain more tasks with a knowledge dimension, which leads to LLMs performing noticeably worse in these two domains compared to Ancient Prose.

**The principle of scaling law remains valid in the CCLLA domain.** We present the relationship between model performance and parameters of Qwen  and Yi  in Figure 5, which shows that the performance improves with the parameters increasing, illustrating the scaling law  for LLMs in CCLLA domain.

For the tasks of translation and punctuation, we further utilize BLEU  and F1-score  as a traditional metric, respectively, assessing the strengths and weaknesses of traditional metrics versus model scoring metric. Specifically, in punctuation and four translation tasks, we select five models and for each model, we randomly choose 300 samples for manual evaluation to determine which metric aligns more closely with human evaluation. The comparative results between traditional metrics and model scoring metric are illustrated in Figure 6. It can be observed that for tasks with singular and definitive answers such as punctuation, traditional metrics are more appropriate as the scoring model struggle to extract the correct answer from responses of LLMs. In contrast, for tasks with non-unique answers like translation, model scoring metric demonstrates a high degree 

[MISSING_PAGE_EMPTY:8]

31 representative LLMs. The results reveal insights into their performance levels, highlighting areas for improvement. WenMind provides a standardized and detailed assessment, enabling researchers to assess and compare the performance of LLMs in the CCLLA domain effectively. Our study highlights the importance of knowledge enrichment in LLMs for the CCLLA domain and provides valuable insights for future research and development.

## 6 Limitations

The main limitation of our work is the use of an aligned LLMs for scoring, which might introduce some degree of error. However, we take several measures to minimize this issue. We standardize the behavior of the scoring LLMs to ensure fairness and consistency, resulting in a high level of consistency with human evaluation (89.4%). This minimizes the impact of error and ensures a reliable comparison between LLMs and human scoring. Additionally, while our evaluation tasks are designed to align closely with real-world applications of LLMs in the field of CCLLA, some tasks may not fully capture the complexity of specific application needs, such as T5 punctuation and T6 theme classification. These tasks serve more as general evaluations rather than targeted assessments of nuanced requirements in practical scenarios.

## 7 Ethical Statement

Our evaluation dataset contains content of significant historical and academic value, designed to promote scholarly research and educational applications related to historical texts, language transfor

    &  &  &  \\   & &  & } & } &  & } &  &  &  &  \\  Bioichuan2-7B-Chat  & 41.2 & 49.5 & 33.6 & 39.5 & 47.8 & 58.2 & 27.7 \\ Bioichuan2-13B-Chat  & 45.5 & 53.4 & 39.8 & 41.6 & 53.7 & 58.4 & 31.2 \\ Fuerb-Bahrami-38b  & 38.7 & 44.7 & 33.1 & 37.8 & 45.2 & 50.2 & 26.9 \\ CurdM2-Chat  & 35.4 & 43.9 & 29.9 & 30.0 & 43.8 & 52.3 & 19.6 \\ CurdM2-Chat  & 39.5 & 50.9 & 32.4 & 32.0 & 50.9 & 55.7 & 20.0 \\ Intermedi-DAn-Chat  & 50.2 & 53.4 & 47.5 & 49.3 & 54.7 & 63.3 & 40.8 \\ Quent 5-D5-Chat  & 26.1 & 36.7 & 17.0 & 23.4 & 37.2 & 43.4 & 6.7 \\ Quent 5-D4-Chat  & 39.6 & 48.5 & 32.5 & 36.1 & 48.0 & 32.5 & 24.9 \\ Quent 5-D5-Dhat  & 50.3 & 55.5 & 48.2 & 44.7 & 57.9 & 65.0 & 36.2 \\ Quent 5-14B-Chat  & 54.9 & 60.5 & 52.8 & 49.1 & 62.5 & 65.3 & 42.0 \\ Quent 5-132B-Chat  & 57.0 & 63.3 & 52.6 & 53.4 & 64.6 & 65.7 & 44.4 \\ Quent 5-72B-Chat  & 58.5 & 64.0 & 55.6 & 54.0 & 65.9 & 67.4 & 46.3 \\ Vi-1-56-Dhat  & 47.2 & 53.4 & 42.9 & 43.7 & 54.7 & 61.9 & 33.3 \\ Vi-1-59-Dhat  & 51.7 & 58.4 & 46.6 & 48.6 & 59.1 & 65.0 & 38.1 \\ Vi-1-53-Dhat  & 57.4 & 63.0 & 52.0 & 56.6 & 63.2 & 69.6 & 46.4 \\ ENRISE-38-Chat  & 62.2 & 63.5 & 55.7 & 70.7 & 64.4 & 72.8 & 55.9 \\ ENRISE-40-83-Chat  & **64.3** & **66.3** & **56.6** & 27.4 & **66.8** & **76.1** & 57.8 \\ Sport-3-15-Dhat  & 60.9 & 59.8 & 54.1 & **78.7** & 60.2 & 66.9 & **60.2** \\ German-1-7B-T  & 25.2 & 32.4 & 21.8 & 18.6 & 34.9 & 47.7 & 6.2 \\ ZyyyLALA-13-B-11  & 34.1 & 42.5 & 28.2 & 9.5 & 43.5 & 50.2 & 17.2 \\ LLLALALA-7B-Chat  & 13.0 & 14.0 & 14.3 & 9.2 & 16.8 & 26.9 & 4.2 \\ LLLALALA-13B-Chat  & 23.7 & 29.7 & 21.6 & 17.1 & 32.2 & 40.5 & 7.9 \\ LLLALALA-Chinese-T-Bhat  & 18.1 & 29.6 & 11.2 & 10.0 & 27.5 & 25.1 & 3.6 \\ LLLALALA-Chinese-13B-Chat  & 23.7 & 36.4 & 15.3 & 16.0 & 35.7 & 35.3 & 4.5 \\ LLLALALA-38-INTINT  & 34.7 & 45.0 & 27.5 & 29.1 & 46.1 & 57.4 & 13.4 \\ ILLALALA38-Chinese-8B-Chat  & 37.3 & 49.9 & 30.1 & 27.7 & 50.2 & 55.7 & 15.2 \\ GPT-5  & 35.3 & 46.1 & 30.5 & 25.1 & 47.1 & 50.7 & 15.6 \\ GPT-4  & 50.2 & 60.3 & 44.2 & 43.1 & 61.3 & 61.7 & 32.4 \\ Ancient Chinese-LLLM-7B  & 32.7 & 42.6 & 23.9 & 30.5 & 41.1 & 39.1 & 19.9 \\ Bloom-7B-Chumbas  & 32.5 & 42.7 & 24.0 & 29.3 & 42.2 & 41.4 & 17.3 \\ Xunai-Queen1.5-7B  & 37.0 & 44.8 & 29.4 & 36.2 & 44.9 & 46.8 & 23.8 \\ Average & & & & & & & & \\   

Table 4: Results of all evaluated models on different domains and capabilities. Details of the evaluated models are presented in Appendix C.

    &  &  &  &  &  \\   & MSM & TM & MSM & TM & MSM & TM & MSM & TM & MSM & TM \\  Baiichuan2-13B-Chat  & 53.8 & 13.3 & 62.5 & 7.8 & 66.9 & 8.8 & 51.3 & 38.2 & 72.5 & 59.8 \\ ERNIE-4.0-8K-0329  & 62.8 & 19.8 & 48.0 & 28.0 & 65.0 & 5.6 & 55.9 & 34.1 & 85.2 & 61.2 \\ LLLAM3-Chinese-8B-Chat  & 46.3 & 12.2 & 61.2 & 8.4 & 62.4 & 7.3 & 52.3 & 35.2 & 68.5 & 57.7 \\ GPT-3-5  & 46.8 & 12.3 & 54.4 & 7.2 & 59.4 & 8.5 & 49.7 & 38.2 & 72.0 & 58.0 \\ Xunai-Queen1.5-7B  & 63.4 & 25.4 & 51.0 & 29.1 & 60.6 & 10.7 & 43.9 & 27.0 & 84.2 & 75.6 \\   

Table 5: Model scoring metric and traditional metrics on translation and punctuation tasks. “TM” represents “Traditional Metrics”; “MSM” represents “Model Scoring Metric”. Details of the evaluated models are presented in Appendix C. “TM” is either BLEU or F1.

mation, and ancient scenarios. It is not intended to reinforce or propagate societal biases. Throughout the data collection and processing phases, we have taken measures to minimize the presence of unsafe content. For tasks involving content generation, we emphasize that the generated materials should be limited to academic research and educational use, and must not be employed for commercial purposes, political propaganda, or any objectives that may lead to bias or discrimination. In light of potential misuse risks, we explicitly restrict the use of the dataset and generated texts to prevent inappropriate applications. We acknowledge the social, cultural, and historical risks that may be inherent in the dataset, and we are committed to actively exploring and implementing strategies to mitigate such risks. With this statement, we aim to provide academic value while addressing potential ethical concerns, underscoring that the dataset must not be misused.