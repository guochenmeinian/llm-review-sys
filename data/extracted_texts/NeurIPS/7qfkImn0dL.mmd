# ExPT: Synthetic Pretraining for Few-Shot Experimental Design

Tung Nguyen, Sudhanshu Agrawal, Aditya Grover

University of California, Los Angeles

{tungnd,adityag}@cs.ucla.edu, sudhanshuagr27@g.ucla.edu

###### Abstract

Experimental design for optimizing black-box functions is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of _few-shot_ experimental design, where only a few labeled data points of input designs and their corresponding values are available. We introduce **E**xperiment **P**retrained **T**ransformers (ExPT), a foundation model for few-shot experimental design that combines unsupervised learning and in-context pretraining. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at [https://github.com/tung-nd/ExPT.git](https://github.com/tung-nd/ExPT.git)

## 1 Introduction

The design of experiments to optimize downstream target objectives is a ubiquitous challenge across many science and engineering domains, including materials discovery , protein engineering , molecular  design, mechanical design , and neural architecture optimization . The key criterion of interest in experimental design (ED) is sample-efficiency, as the target objectives are often black-box functions and evaluating these objectives for any candidate design often involves expensive real-world experiments. A standard class of approaches learn a surrogate to approximate the target objective and actively improve the approximation quality through online experiments . However, online data acquisition may be infeasible in the real world due to high costs, time constraints, or safety concerns. As an alternate, recent works have proposed offline ED , , , wherein a model learns to perform optimization from a fixed dataset of past experiments. While this is more practical than the online setting, current offline methods and benchmarks assume access to large experimental datasets containing thousands of data points, which are hard or even impossible to obtain in high-stake and emerging science problems. Even when these datasets exist, the past experiments might be of very poor quality resulting in poor surrogate learning and optimization.

In this paper, we aim to overcome these limitations for hyper-efficient experimental design that does not require large experimental datasets. To this end, we introduce _few-shot_ experimental design, a more challenging setting that better resembles real-world scenarios. We describe few-shot ED as a two-phased pretraining-adaptation paradigm. In the pretraining phase, we only assume access to_unlabeled_ data, i.e., input designs without associated function values. During the adaptation phase, we have access to a few labeled examples of past experiments to adapt the model to the downstream task. This setup offers several advantages. First, it alleviates the requirement for costly annotated data and relies mainly on unlabeled inputs that are easily accessible. Second, unsupervised pretraining enables us to utilize the same pretrained backbone for adapting to multiple downstream optimization tasks within the same domain. For example, in molecule design, one may want to optimize for multiple properties, including drug-likeness, synthesizability, or similarity to target molecules .

The key question in this setup is how to make use of the unlabeled data to facilitate efficient generalization to downstream tasks during optimization. Our intuition here is that, while the objective function is unknown, we can use the unlabeled inputs to generate pretraining data from other _synthetic_ functions. If a model can few-shot learn from a diverse and challenging set of functions, it should be able to generalize quickly to any target objective during the adaptation phase, in line with recent foundation models for language  and vision . This insight gives rise to our idea of _synthetic pretraining_, wherein we pretrain the model on data generated from a rich family of synthetic functions that operate on the same domain as the target task. Specifically, for each function drawn from this family, we sample a set of points by using the unlabeled data as inputs. We divide these points into a small context set and a target set, and train the model via in-context learning to perform conditional generation of the target input \(x\) given the context points and the target value \(y\). A model that works well on this task should be able to efficiently capture the structures of the underlying function, i.e., how different regions of the input space influence the function value, from a small context set. By explicitly training the model to perform this task on a diverse set of functions, the model can generalize efficiently to downstream functions during adaptation requiring only limited supervision. After pretraining, we can perform optimization by conditioning the model on a few labeled examples from the downstream task and generating an input that achieves the optimum \(y^{}\).

Inspired by recent advances in few-shot learning in language ,  and other domains , , , we instantiate a novel foundation model with a transformer-based architecture , which we call **Ex**periment **P**retrained **T**ransformers (ExPT). ExPT is an encoder-decoder architecture, in which the encoder is a transformer  network that encodes the context points and the target value, and the decoder is a VAE  model that predicts the high-dimensional target input. The transformer encoder allows ExPT to perform few-shot generation and optimization purely through in-context learning in a gradient-free fashion. We compare the performance of ExPT and various baselines on \(2\) few-shot settings created from Design-Bench , a standard database benchmark for ED. The two settings allow us to examine how different methods perform with respect to different quantities and qualities of few-shot data. In both these settings, results show that ExPT achieves the highest average score and the highest average ranking with respect to median performance, mean performance, and best-achieved performance. Especially in the more challenging setting, ExPT outperforms the second-best method by \(70\%\) in terms of the mean performance. Additionally, we explore the potential of using the same pretrained ExPT for multiple objectives, and conduct extensive ablation studies to validate the effectiveness of our design choices for synthetic data generation and ExPT architecture.

Figure 1: Experiment Pretrained Transformers (ExPT) follow a pretraining-adaptation approach for few-shot experimental design. During _pretraining_ (**left**), the model has access to unlabeled designs from domain \(\) without their corresponding scores. For _adaptation_ (**right**), the model conditions on a small set of (design, score) pairs and the desired score \(y^{}\) to generate the optimal design \(x^{}\).

## 2 Experiment Pretrained Transformers

### Problem setup

Let \(f:\) be a black-box function that operates on a \(d\)-dimensional domain \(^{d}\). In experimental design (ED), the goal is to find the input \(x^{}\) that maximizes \(f\):

\[x^{}*{arg\,max}_{x}f(x). \]

Typically, \(f\) is a high-dimensional and complex function that often involves expensive physical experiments. Existing approaches either assume the ability to actively query \(f\) to collect data  or access to a large dataset of past experiments . Both assumptions are too strong in many real-world applications where data collection is hard or even impossible . Therefore, we propose _few-shot ED_, a more challenging yet realistic setting to overcome these limitations. In few-shot ED, the goal is to optimize _any_ objective function in the domain \(\) given only a handful of examples. We approach this problem with a pretraining-adaptation pipeline. In the _pretraining_ phase, we assume access to an _unlabeled_ dataset \(_{}=\{x_{i}\}_{i=1}^{|_{}|}\) from the optimization domain \(^{d}\). We note that \(_{}\) only contains potential design inputs without their corresponding scores, for example, potential molecules in molecule optimization or different combinations of hyperparameters in neural architecture search. This means the objective function \(f\) is _unspecified_ during pretraining.

During the _adaptation_ phase, one can use the pretrained model to optimize any objective function \(f\) in the same domain \(\). We now have access to a few-shot _labeled_ dataset that the model can use to adapt to the downstream function \(_{}=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\), in which \(y_{i}=f(x_{i})\) and \(n=|_{}||_{}|\). After adaptation, we evaluate a few-shot optimization method by allowing it to propose \(Q\) input \(x^{}s\) and query their scores using the black-box function \(f\), where \(Q\) is often called the optimization budget . The performance of a black-box optimizer is then measured by computing the max, median, and mean of the \(Q\) evaluations, This setup provides two key benefits. First, it resembles many real-world scenarios, where the potential design inputs are cheap and easy to obtain while their target function values are expensive to evaluate. For example, in molecular optimization, we have databases of millions of molecules  but only the properties of a handful are known . Second, unsupervised pretraining allows us to train a general backbone that we can adapt to multiple optimization tasks in the same domain.

### Synthetic Pretraining and Inverse Modeling for Scalable Experimental Design

Intuitively, the adaptation phase in resembles a few-shot learning problem, in which a model is tasked to produce the optimal input \(x^{}\) by conditioning on a few labeled examples in \(_{}\). To perform well in this task, a model has to efficiently capture the structure of a high-dimension function \(f\), i.e., what regions of the function lead to higher values and vice versa, from very few examples in \(_{}\). Given this perspective, the question now is how to make use of the unlabeled dataset \(_{}\) to pretrain a model that achieves such efficient generalization to the objective function \(f\). Our key insight is, if a model learns to perform in-context learning on a diverse and challenging set of functions, it should be able to adapt quickly to any objective function at test time. While the function values are unknown during pretraining, we can use the unlabeled inputs \(x^{}s\) to generate pretraining data from _other_ functions. This gives rise to our idea of _synthetic pretraining_, wherein we pretrain the model to perform few-shot learning on a family of synthetic functions \(\) that operate on the same input domain \(\) of the objective \(f\). We discuss in detail our mechanism for synthetic data generation in Section 2.3. For each function \(\) generated from \(\), we sample a set of function evaluations \(\{(x_{i},y_{i})\}_{i=1}^{N}\) that we divide into a small context set \(\{(x_{i},y_{i})\}_{i=1}^{m}\) and a target set \(\{(x_{j},y_{j})\}_{j=m+1}^{N}\). We train the model to predict the target points conditioning on the context set.

There are two different approaches to pretraining a model on this synthetic data. The first possible approach is forward modeling, where the model is trained to predict the target outputs \(y_{m+1:N}\) given the context points and the target inputs \(x_{m+1:N}\). This is similar to the approach followed by TNPs , a model recently proposed in the context of meta-learning. During adaptation, we can condition the model on the labeled examples in \(_{}\) and perform gradient ascent updates to improve an existing design input \(x_{t}\). However, as commonly observed in previous works , this approach is susceptible to producing highly suboptimal inputs. This is because performing gradient ascent with respect to an imperfect forward model may result in points that have high valuesunder the model but are poor when evaluated using the real function. Instead, we propose to perform _inverse modeling_, where the model learns to predict the inputs \(x_{m+1:N}\) given the output values \(y_{m+1:N}\) and the context points. As the model learns to directly generate input \(x^{}s\), it is less vulnerable to the aforementioned problem. Another advantage of inverse modeling is after pretraining, we can simply condition on \(_{}\) and the optimal value \(y^{}\) to generate the candidate optima. Our loss function for pretraining the model is:

\[&=*{arg\,max}_{ }_{,x_{1:N}_{},y_{1:N}=(x_{1:N})}[ p(x_{m+1:N} x_{1:m},y_{1:m},y_ {m+1:N})]\\ &=*{arg\,max}_{}_{ ,x_{1:N}_{},y_{1:N}=(x_{1:N})} [_{i=m+1}^{N} p(x_{i} x_{1:m},y_{1:m},y_{i})], \]

where we assume the target points are independent given the context set and the target output. Figure 1 illustrates the proposed pretraining and adaptation pipeline. Typically, we use a small context size \(m\) during pretraining to resemble the test scenario.

After pretraining, ExPT can adapt to any objective \(f\) in the domain in a gradient-free fashion. Samples in the few-shot dataset \(_{}\) become the context points and the model conditions on only one target \(y^{}\), which is the optimal value of \(f\), to generate candidate optima. Note that we only assume the knowledge of \(y^{}\) and not \(x^{}\). This assumption is common in many prior works . In practice, \(y^{}\) might be known based on domain knowledge. For example, in molecule design, there are physical limits on the value of certain properties such as relaxed energy, in robot applications, the optimal performance can be computed from the cost function, and in neural architecture search, we know the theoretical limits on the highest possible accuracy for classifiers.

Next, we present the details of synthetic data generation and our proposed model architecture, the two components that constitute our proposed foundation model, which we refer to as **Ex**periment **P**retrained **T**ransformers (ExPT).

### Data generation

We need a family of functions to generate synthetic data for pretraining ExPT. A good family of functions should be easy to sample from and should be capable of producing diverse functions. Many possible candidates exist for synthetic function families, such as Gaussian Processes (GPs), randomly constructed Gaussian Mixture Models, or randomly initialized or pretrained neural networks. Among these candidates, we choose to generate synthetic data from Gaussian Processes with an RBF kernel. This is for several reasons. First, they are a natural choice as they represent distributions over functions. Second, it is easy and cheap to sample data from prior GPs. And third, a GP with an RBF

Figure 2: The pretraining-adaptation phases for ExPT. We sample synthetic data from \(\) and pretrain the model to maximize \( p(x_{m+1:N} x_{1:m},y_{1:m},y_{m+1:N})\). At adaptation, the model conditions on \(_{}\) and \(y^{}\) to generate candidates. ExPT employs a transformer encoder that encodes the context points and target outputs and a relevant decoder that predicts the target inputs.

[MISSING_PAGE_FAIL:5]

### Design-Bench experiments

**Tasks** We consider \(4\) tasks from Design-Bench. **D'Kitty** and **Ant** are continuous tasks with input dimensions of \(56\) and \(60\), respectively. In D'kitty and Ant, the goal is to optimize the morphological structure of two simulated robots, Ant to run as fast as possible, and D'kitty to reach a fixed target location. **TF Bind 8** and **TF Bind 10** are two discrete tasks, where the goal is to find the length-\(8\) and length-\(10\) DNA sequence that has a maximum binding affinity with the SIK6_REF_R1 transcription factor. The design space in these two tasks consists of sequences of one of four categorical variables, corresponding to four types of nucleotide. For each task, Design-Bench provides a public dataset, a larger hidden dataset which is used to normalize the scores, and an oracle. We have an exact oracle to evaluate the proposed designs in all \(4\) tasks we consider.

**Few-shot settings** We create \(2\) few-shot settings from the above tasks, which we call random and poorest. In random, we randomly subsample \(1\%\) of data points in the public set of each task as the few-shot dataset \(_{}\). The poorest setting is more challenging, where we use \(1\%\) of the data points which have the _lowest_ scores. The two settings examine how sensitive different methods are to the quantity and quality of the data. In both settings, we use \(x^{}s\) in the public dataset as \(_{}\).

**ExPT details** For each domain, we pretrain ExPT for \(10{,}000\) iterations with \(128\) synthetic functions in each iteration, corresponding to a total number of \(1{,}280{,}000\) synthetic functions. For each function, we randomly sample \(228\) input \(x^{}s\) from the unlabeled dataset \(_{}\) and generate the values \(y^{}s\) from a Gaussian Process with an RBF kernel. To increase the diversity of synthetic data, we randomize the two hyperparameters, length scale \([5{,}0{,}10{.}0]\) and function scale \([1{,}0{,}1{.}0]\), when generating each function. Additionally, we add Gaussian noises \((0,0.1)\) to each input \(x\) sampled from \(_{}\) to enlarge the pretraining inputs. For each generated function, we use \(100\) points as context points and the remaining \(128\) as target points, and train the model to optimize 1. During the adaptation phase, we condition the pretrained ExPT model on the labeled few-shot dataset \(_{}\) and the target function value \(y^{*}\) to generate designs \(x^{}s\).

**Baselines** We compare ExPT with BayesOpt (GP-qEI), a canonical ED method, and MINs, COMs, BDI, and BONET, four recent deep learning models that have achieved state-of-the-art performance in the offline setting. To adapt GP-qEI to the few-shot setting, we use a feedforward network trained on few-shot data to serve as an oracle, a Gaussian Process to quantify uncertainty, and the quasi-Expected Improvement algorithm for the acquisition function. For the deep learning baselines, we train their models on the few-shot dataset \(_{}\) using the hyperparameters reported in their original papers.

**Evaluation** For each considered method, we allow an optimization budget \(Q=256\). We report the median score, the max score, and the mean score among the \(256\) proposed inputs. Following previous works, we normalize the score to \(\) by using the minimum and maximum function values from a large hidden dataset \(y_{}=}}{y_{}-y_{}}\). We report the mean and standard deviation of the score across \(3\) independent runs for each method.

**Results** Table 1 shows the performance of different methods in the random setting. Most methods perform well in the random setting, where ExPT achieves the highest average score and the best average rank across all \(3\) performance metrics. For each of the tasks and metrics considered, ExPT is either the best or second-best performing method. Notably, in Ant, ExPT significantly outperforms the best baseline by \(18\%\), \(9\%\), and \(10\%\) with respect to the median, max, and mean performance,

Figure 3: The performance of ExPT on \(4\) out-of-distribution synthetic tasks through the pretraining phase. We average the performance across \(3\) seeds.

respectively. Only ExPT and BONET achieve a meaningful performance in Ant when considering the mean score. BONET is also the overall second-best method in this setting.

Table 2 shows the superior performance of ExPT in few-shot poorest, the more challenging setting. ExPT achieves the highest score in \(10/12\) individual tasks and metrics, and also achieves the highest score and the best rank across tasks on average. Notably, in terms of the mean score, ExPT beats the best baseline by a large margin, achieving an improvement of \(40\%\), \(176\%\), and \(18\%\) on D'Kitty, Ant, and TF Bind 8, and \(70\%\) on average. The performance of most baselines drops significantly from the random to the poorest setting, including BONET, the second-best method in the random setting. This was also previously observed in the BONET paper . Interestingly, the performance of ExPT, MINs, and GP-qEI is not affected much by the quality of the few-shot data, and even improves in certain metrics. We hypothesize that even though the dataset is of lower quality, it may contain specific anti-correlation patterns about the problem that the model can exploit.

**Pretraining analysis** In addition to the absolute performance, we investigate the performance of ExPT on downstream tasks through the course of pretraining. Figure 4 shows that the performance of ExPT in most tasks improves consistently as the number of pretraining steps increases. This shows that synthetic pretraining on diverse functions facilitates the generalization to complex real-world functions. In Ant, the performance slightly drops between \(4000\) and \(10000\) iterations. Therefore, we can further improve ExPT if we have a way to stop pretraining at a point that likely leads to the best

  & Baseline & D’Kitty & Ant & TF Bind 8 & TF Bind 10 & Mean score (\(\)) & Mean rank (\(\)) \\   & \(_{}\)(best) & \(0.883\) & \(0.563\) & \(0.439\) & \(0.466\) & \(0.587\) & \(0.40\) \\   & MINs & \(0.859 0.014\) & \(0.485 0.152\) & \(0.416 0.019\) & \(0.468 0.014\) & \(0.557 0.050\) & \(4.0\) \\  & COMs & \(0.752 0.007\) & \(0.411 0.012\) & \(0.371 0.001\) & \(0.468 0.000\) & \(0.501 0.005\) & \(4.0\) \\  & BONET & \(0.852 0.013\) & \(0.597 0.119\) & \(0.441 0.003\) & \(0.483 0.009\) & \(0.593 0.036\) & \(2.3\) \\  & BDI & \(0.592 0.020\) & \(0.396 0.018\) & \(0.540 0.032\) & \(0.438 0.034\) & \(0.492 0.026\) & \(4.8\) \\  & GP-qEI & \(0.842 0.058\) & \(0.550 0.007\) & \(0.439 0.000\) & \(0.467 0.000\) & \(0.575 0.016\) & \(4.0\) \\  & ExPT & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & MINs & \(\) & \(\) & \(0.814 0.030\) & \(0.639 0.017\) & \(0.818 0.019\) & \(3.3\) \\  & COMs & \(0.920 0.010\) & \(0.841 0.044\) & \(0.686 0.152\) & \(0.656 0.023\) & \(0.776 0.057\) & \(4.0\) \\  & BONET & \(0.909 0.012\) & \(0.888 0.024\) & \(0.887 0.053\) & \(\) & \(\) & \(\) \\  & BDI & \(0.918 0.006\) & \(0.806 0.004\) & \(0.906 0.074\) & \(0.532 0.023\) & \(0performance in downstream tasks. However, in practice, we do not have the luxury of testing on real functions during pretraining. Alternatively, we could perform validation and early stopping on a set of held-out, out-of-distribution synthetic functions. We leave this to future work.

#### 3.2.1 ExPT for few-shot optimization of multiple objectives

As we mention in Section 2.2 one advantage of unsupervised pretraining is the ability to optimize for multiple objectives during the adaptation phase. In this section, we show that the same pretrained ExPT model is capable of optimizing different objectives in D'Kitty and Ant domains. We create two variants of the original D'Kitty, namely D'Kitty-45 and D'Kitty-60, whose objectives are to navigate the robot to goals that are \(45^{}\) and \(60^{}\) away from the original goal, respectively. For Ant, we create Ant-\(v_{y}\) where the goal is to run as fast as possible in the vertical \(Y\) direction (as opposed to horizontal \(X\) direction in Ant) direction, and Ant-Energy, where the goal is to preserve energy. We detail how to construct these tasks in Appendix.3.1. We use the same pretrained models for all these tasks. During adaptation, the model conditions on the \(_{}\) and \(y^{}\) for each task for optimization.

We evaluate ExPT on these tasks in the poorest setting. Table 2 shows that ExPT performs well on all tasks, where the median and mean scores are better than the best value in \(_{}\), and the max score is close to \(1\). For the Ant, Ant-\(v_{y}\), and Ant-Energy tasks, we visualize the behavior of the optimal designs that are discovered at [https://imgur.com/a/zpgI4YL](https://imgur.com/a/zpgI4YL). When subject to the same policy-network (optimizing for horizontal \(X\) speed), the robots optimized for different objectives behave differently; the optimal Ant is capable of leaping forward to move quickly in \(X\); Ant-\(v_{y}\) is able to jump up to maximize speed in \(Y\); Ant-Energy is capable of'sitting down' to conserve energy.

#### 3.2.2 ExPT for closed-loop optimization

ExPT can adapt to any objective function purely through in-context learning. This means that the model can refine its understanding of the underlying objective function given more data points in a very efficient manner. In this section, we explore an alternative optimization scheme for ExPT, namely _sequential sampling_, which explicitly utilizes the in-context learning ability of the model. Specifically, instead of producing \(Q=256\) inputs simultaneously, we sample one by one sequentially. That is, we condition the model on \(_{}\) and \(y^{}\) to sample the first point, evaluate the point using the black-box function, and add the point together with its score to the context set. We repeat this process for \(256\) times. This process is often referred to as closed-loop optimization in ED.

Figure 4: The median and mean performance of ExPT of \(4\) Design-Bench tasks through the course of pretraining. We average the performance across \(3\) seeds.

    & Task & D’Kitty & D’Kitty-45 & D’Kitty-60 & Ant & Ant-\(v_{y}\) & Ant-Energy \\   Median \\ Max \\  } & \(_{}\)(best) & \(0.307\) & \(0.297\) & \(0.344\) & \(0.124\) & \(0.210\) & \(0.189\) \\   & ExPT & \(0.922 0.009\) & \(0.611 0.007\) & \(0.569 0.010\) & \(0.686 0.090\) & \(0.613 0.009\) & \(0.635 0.028\) \\   & ExPT & \(0.976 0.004\) & \(0.954 0.008\) & \(0.973 0.004\) & \(0.965 0.004\) & \(0.923 0.049\) & \(0.950 0.033\) \\   & ExPT & \(0.871 0.018\) & \(0.619 0.016\) & \(0.584 0.008\) & \(0.646 0.061\) & \(0.599 0.005\) & \(0.608 0.025\) \\   

Table 3: ExPT’s performance on different objectives in D’Kitty and Ant domains. We pretrain one model for all tasks in the same domain. The performance is averaged across \(3\) seeds.

Table 4 shows that ExPT with sequential sampling performs better than simultaneous sampling on D'Kitty and Ant in both random and poor settings. Especially on Ant in the poorest setting, ExPT-Sequential achieves improvements of \(20\%\) and \(19\%\) over ExPT in terms of the median and mean performance, respectively. Intuitively, as we add more data points to the context set, ExPT-Sequential is able to updates its understanding of the structure of the objective function, consequently leading to improved performance.

#### 3.2.3 Forward modeling versus Inverse modeling

As we mentioned in Section 2.2 two possible approaches exist to pretrain ExPT on synthetic data. We take the inverse modeling approach for ExPT throughout the paper, as we train ExPT to directly produce design inputs \(x^{}s\). In this section, we empirically validate our design choices by comparing ExPT with TNP-ED, its forward counterpart. TNP-ED's architecture is similar to ExPT's in Figure 2 except that the target points now contain \(x_{m+1:N}\) instead of \(y_{m+1:N}\), the decoder is a 1-layer MLP, and the predicted outputs are \(_{m+1:N}\). We call this model TNP-ED because a model named TNP  with a similar architecture was previously proposed in the context of meta-learning. We pretrain TNP-ED using a simple mean-squared error loss \(=_{i=m+1}^{N}(_{i}-y_{i})^{2}\). After pretraining, we condition TNP-ED on \(_{}\) and the best inputs in this dataset, and perform gradient ascent with respect to these inputs to obtain better points.

Table 5 compares the performance of ExPT and TNP-ED on D'Kitty and Ant with respect to the median score and mean score. ExPT achieves significantly better performance in all metrics, especially in the poorest setting. This is because forward models suffer from poor out-of-distribution generalization, and performing gradient ascent on this model may result in points that have high values under the model but are very poor when evaluated using the true functions. This validates our inverse modeling approach.

## 4 Related work

**Online ED** The majority of existing approaches solve ED in an active setting, where the model is allowed to query the black-box function to collect more data. Many of these works are based on Bayesian Optimization , which typically employs a surrogate model to the black-box function and an acquisition function. The surrogate model is often a predictive model that can quantify uncertainty, such as Gaussian Processes , or Bayesian Neural Networks . The acquisition function uses the uncertainty output by the surrogate model to trade off between exploration and exploitation for querying new points.

**Offline ED** Recent works have proposed to solve ED by learning from a fixed set of \((x,y)\) pairs to bypass active data collection . The Design-Bench benchmark  consists of several such tasks in the physical sciences and robotics and is used by many recent

  & Baseline & D’Kitty & Ant \\   & \(_{}\)(best) & \(0.883\) & \(0.563\) \\   & ExPT & \(0.902 0.006\) & \(0.705 0.018\) \\  & ExPT-Seq & \(\) & \(\) \\   & ExPT & \(0.865 0.016\) & \(0.639 0.026\) \\  & ExPT-Seq & \(\) & \(\) \\  

Table 4: Comparison of simultaneous (ExPT) and sequential (ExPT-Seq) sampling on Ant and D’Kitty in random (left) and poorest (right) settings. We average the performance across \(3\) seeds.

  & Baseline & D’Kitty & Ant \\   & \(_{}\)(best) & \(0.307\) & \(0.124\) \\   & ExPT & \(0.922 0.009\) & \(0.686 0.090\) \\  & ExPT-Seq & \(\) & \(\) \\   & ExPT & \(0.871 0.018\) & \(0.646 0.061\) \\  & ExPT-Seq & \(\) & \(\) \\  

Table 5: Comparison of inverse modeling (ExPT) versus forward modeling (TNP-ED) on Ant and D’Kitty in random (left) and poorest (right) settings. We average the performance across \(3\) seeds.

works in offline ED. MINs  and BONET  perform optimization by generating designs \(x\) via conditioning on a high score value \(y\). MINs uses a GANs  model on \((x,y)\) pairs and BONET casts offline ED as a sequence modeling problem. COMs  formulates a conservative objective function that penalizes high-scoring poor designs and uses it to train a surrogate forward model which is then optimized using gradient ascent. BDI  uses a bidirectional model consisting of a forward and backward models that learn mappings from the dataset to high-scoring designs and vice versa. In contrast to these works, we propose ExPT in the few-shot ED setting, where the model is given access to only the \(x^{}s\) during pretraining, and a handful of labeled examples for adaptation.

Synthetic PretrainingIn the absence of vast amounts of labeled data, pretraining on synthetic data is an effective method for achieving significant gains in model performance. Prior works in this direction construct synthetic tasks which improve performance on diverse downstream tasks such as mathematical reasoning , text summarization , and perception tasks in vision . Each synthetic task produces a dataset of labeled \((x,y)\) values that can be used to train a model as usual for various objectives. Often, pre-training in this manner produces better results than simply pre-training on another real dataset. In this work, we demonstrate that pretraining on synthetic data generated from GPs can achieve significant generalization to downstream functions, leading to state-of-the-art performance on challenging few-shot optimization problems.

Few-shot learningFew-shot learning is a common paradigm in deep learning, where the model is pretrained on large amounts of data in an unsupervised manner. At test time, the model is given only a few examples from a downstream task and is expected to generalize . This technique has found applications in text-generation (GPT-x) , image classification , graph neural networks , text to visual-data generation , and neural architecture search , . ExPT is capable of performing few-shot learning for black-box optimization in a variety of domains. Moreover, ExPT is pretrained on synthetically generated data with no prior knowledge of the downstream objective.

## 5 Conclusion

Inspired by real-world scenarios, this work introduces and studies the few-shot experimental design setting, where we aim to optimize a black-box function given only a few examples. This setting is ubiquitous in many real-world applications, where experimental data collection is very expensive but we have access to unlabelled designs. We then propose ExPT, a foundation model style framework for few-shot experimental design. ExPT operates in two phases involving pretraining and finetuning. ExPT is pretrained on a rich family of synthetic functions using unlabeled data and can adapt to downstream functions with only a handful of data points via in-context learning. Empirically, ExPT outperforms all the existing methods by a large margin on all considered settings, especially improving over the second-best baseline by \(70\%\) in the more challenging setting.

Limitations and Future workIn this work, we assume we have access to a larger unlabeled dataset for pretraining and the knowledge of the optimal value for optimization. While these assumptions are true in many applications and have been used widely in previous works, we would like to relax these assumptions in future work to improve further the applicability of the model. One more potential direction is to finetune the pretrained ExPT model on downstream data to further improve performance. Finally, we currently pretrain ExPT for each domain separately. We are interested in exploring if pretraining a big model that works for all domains is possible and if that helps improve performance in each individual domain.