# What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement

What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement

Yotam Alexander

Equal contribution

Nimrod De La Vega

Equal contribution

Noam Razin

Nadav Cohen

Tel Aviv University

{yotama,nimrodd,noamrazin}@mail.tau.ac.il, cohennadav@cs.tau.ac.il

###### Abstract

The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution _if and only if_ the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about the relation between deep learning and real-world data.1

## 1 Introduction

Deep learning is delivering unprecedented performance when applied to data modalities involving images, text and audio. On the other hand, it is known both theoretically and empirically  that there exist data distributions over which deep learning utterly fails. The question of _what makes a data distribution suitable for deep learning_ is a fundamental open problem in the field.

A prevalent family of deep learning architectures is that of _locally connected neural networks_. It includes, among others: _(i)_ convolutional neural networks, which dominate the area of computer vision; _(ii)_ recurrent neural networks, which were the most common architecture for sequence (_e.g._ text and audio) processing, and are experiencing a resurgence by virtue of S4 models ; and _(iii)_ local variants of self-attention neural networks . Conventional wisdom postulates that data distributions suitable for locally connected neural networks are those exhibiting a "local nature," and there have been attempts to formalize this intuition . However, to the best of our knowledge, there are no characterizations providing necessary and sufficient conditions for a data distribution to be suitable to a locally connected neural network.

A seemingly distinct scientific discipline tying distributions and computational models is _quantum physics_. There, distributions of interest are described by _tensors_, and the associated computational models are _tensor networks_. While there is shortage in formal tools for assessing the suitability of data distributions to deep learning architectures, there exists a widely accepted theory that allows for assessing the suitability of tensors to tensor networks. The theory is based on the notion of _quantum_entanglement_, which quantifies dependencies that a tensor admits under partitions of its axes (for a given tensor \(\) and a partition of its axes to sets \(\) and \(^{c}\), the entanglement is a non-negative number quantifying the dependence that \(\) admits between \(\) and \(^{c}\)).

In this paper, we apply the foregoing theory to a tensor network equivalent to a certain locally connected neural network, and derive theorems by which fitting a tensor is possible if and only if the tensor admits low entanglement under certain _canonical partitions_ of its axes. We then consider the tensor network in a machine learning context, and find that its ability to attain low approximation error, _i.e._ to express a solution with low population loss, is determined by its ability to fit a particular tensor defined by the data distribution, whose axes correspond to features. Combining the latter finding with the former theorems, we conclude that a _locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low entanglement under canonical partitions of features_. Experiments with different datasets corroborate this conclusion, showing that the accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks) is inversely correlated to the entanglement under canonical partitions of features in the data (the lower the entanglement, the higher the accuracy, and vice versa).

The above results bring forth a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly. Unfortunately, the above search is computationally prohibitive. However, if we employ a certain correlation-based measure as a surrogate for entanglement, _i.e._ as a gauge for dependence between sides of a partition of features, then the search converts into a succession of _minimum balanced cut_ problems, thereby admitting use of well-established graph theoretical tools, including ones designed for large scale [29; 57]. We empirically evaluate this approach on various datasets, demonstrating that it substantially improves prediction accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks).

The data modalities to which deep learning is most commonly applied -- namely ones involving images, text and audio -- are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information). We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data. Concepts and tools from physics -- a branch of science concerned with formally reasoning about natural phenomena -- may be key to overcoming said difficulty. We hope that our use of quantum entanglement will encourage further research along this line.

## 2 Preliminaries

For simplicity, the main text treats locally connected neural networks whose input data is one dimensional (_e.g._ text or audio). We defer to Appendix J an extension of the analysis and experiments to models intaking data of arbitrary dimension (_e.g._ two-dimensional images). Due to lack of space, we also defer our review of related work to Appendix A.

We use \(\) and \(,\) to denote the Euclidean (Frobenius) norm and inner product, respectively. We shorthand \([N]:=\{1,,N\}\), where \(N\). The complement of \([N]\) is denoted by \(^{c}\), _i.e._\(^{c}:=[N]\).

### Tensors and Tensor Networks

For our purposes, a _tensor_ is an array with multiple axes \(^{D_{1} D_{N}}\), where \(N\) is its _order_ and \(D_{1},,D_{N}\) are its _axes lengths_. The \((d_{1},,d_{N})\)'th entry of \(\) is denoted \(_{d_{1},,d_{N}}\).

_Contraction_ between tensors is a generalization of multiplication between matrices. Two matrices \(^{D_{1} D_{2}}\) and \(^{D^{}_{1} D^{}_{2}}\) can be multiplied if \(D_{2}=D^{}_{1}\), in which case we get a matrix in \(^{D_{1} D^{}_{2}}\) holding \(_{d=1}^{D_{2}}_{d,d}_{d,d^{}_{2}}\) in entry \((d_{1},d^{}_{2})[D_{1}][D^{}_{2}]\). More generally, two tensors \(^{D_{1} D_{N}}\) and \(^{D^{}_{1} D^{}_{N^{ }}}\) can be contracted along axis \(n[N]\) of \(\) and \(n^{}[N^{}]\) of \(\) if \(D_{n}=D^{}_{n^{}}\), in which case we get a tensor of size \(D_{1} D_{n-1} D_{n+1} D_{N} D^{ }_{1} D^{}_{n^{}-1} D^{}_{n^{} +1} D^{}_{N^{}}\) holding \(_{d=1}^{D_{n}}_{d_{1},,d_{n-1},d,d_{n+1},,d_{N}} _{d^{}_{1},,d^{}_{n^{}-1},d,d^{ }_{n^{}+1},,d^{}_{N^{}}}\) in the entry indexed by \(\{d_{k}[D_{k}]\}_{k[N]\{n\}}\) and \(\{d^{}_{k}[D^{}_{k}]\}_{k[N^{}]\{n^{}\}}\).

_Tensor networks_ are prominent computational models for fitting (_i.e._ representing) tensors. More specifically, a tensor network is a weighted graph that describes formation of a (typically high-order)tensor via contractions between (typically low-order) tensors. As customary (_cf._), we will present tensor networks via graphical diagrams to avoid cumbersome notation -- see Figure 1 for details.

### Quantum Entanglement

In quantum physics, the distribution of possible states for a multi-particle ("many body") system is described by a tensor, whose axes are associated with individual particles. A key property of the distribution is the dependence it admits under a given partition of the particles (_i.e._ between a given set of particles and its complement). This dependence is formalized through the notion of _quantum entanglement_, defined using the distribution's description as a tensor -- see Definition 1 below.

Quantum entanglement lies at the heart of a widely accepted theory which allows assessing the ability of a tensor network to fit a given tensor (_cf._). In Section 3 we specialize this theory to a tensor network equivalent to a certain locally connected neural network. The specialized theory will be used (in Section 4) to establish our main theoretical contribution: a necessary and sufficient condition for when the locally connected neural network is capable of accurate prediction over a data distribution.

**Definition 1**.: For a tensor \(^{D_{1} D_{N}}\) and subset of its axes \([N]\), let \(;_{n }_{n^{c}}\,D_{n}\) be the arrangement of \(\) as a matrix where rows correspond to axes \(\) and columns correspond to the remaining axes \(^{c}:=[N]\). Denote by \(_{1}_{D_{}}_{ 0}\) the singular values of \(;\), where \(D_{}:=\{_{n}D_{n},_{n^{ c}}D_{n}\}\). The _quantum entanglement_2 of \(\) with respect to the partition \((,^{c})\) is the entropy of the distribution \(\{_{d}:=_{d}^{2}/_{d^{}=1}^{D_{}}_{d^{ }}^{2}\}_{d=1}^{D_{}}\), _i.e._\(QE(;):=-_{d=1}^{D_{}}_{d}(_{d})\). By convention, if \(=0\) then \(QE(;)=0\).

## 3 Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Fitting Tensor

In this section, we prove that a tensor network equivalent to a certain locally connected neural network can fit a tensor if and only if the tensor admits low entanglement under certain canonical partitions of its axes. We begin by introducing the tensor network (Section 3.1). Subsequently, we establish the necessary and sufficient condition required for it to fit a given tensor (Section 3.2). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models; see Appendix J.1 for an extension to arbitrary dimensions.

### Tensor Network Equivalent to a Locally Connected Neural Network

Let \(N\), and for simplicity suppose that \(N=2^{L}\) for some \(L\). We consider a tensor network with an underlying perfect binary tree graph of height \(L\), which generates \(_{}^{D_{1} D_{N}}\). Figure 2(a) provides its diagrammatic definition. For simplicity of presentation, the lengths of axes corresponding to inner (non-open) edges are taken to all be equal to some \(R\), referred to as the _width_ of the tensor network.

Figure 1: Tensor networks form a graphical language for fitting (_i.e._ representing) tensors through tensor contractions. **Tensor network definition:** Every node in a tensor network is associated with a tensor, whose order is equal to the number of edges emanating from the node. An edge connecting two nodes specifies contraction between the tensors associated with the nodes (Section 2.1), where the weight of the edge signifies the respective axes lengths. Tensor networks may also contain open edges, _i.e._ edges that are connected to a node on one side and are open on the other. The number of such open edges is equal to the order of the tensor produced by contracting the tensor network. **Illustrations:** Presented are exemplar tensor network diagrams of: **(a)** an order \(N\) tensor \(^{D_{1} D_{N}}\); **(b)** a vector-matrix multiplication between \(^{D_{1} D_{2}}\) and \(^{D_{2}}\), which results in the vector \(^{D_{1}}\); and **(c)** a tensor network generating \(^{D_{1} D_{2} D_{3}}\).

As identified by previous works, the tensor network depicted in Figure 2(a) is equivalent to a certain locally connected neural network (with polynomial non-linearity -- see, _e.g._, [12; 10; 35; 49]). In particular, contracting the tensor network with vectors \(^{(1)}^{D_{1}},,^{(N)}^{D_{N}}\), as illustrated in Figure 2(b), can be viewed as a forward pass of the data instance \((^{(1)},,^{(N)})\) through a locally connected neural network, whose hidden layers are of width \(R\). This computation results in a scalar equal to \(_{n=1}^{N}^{(n)},_{}\), where \(\) stands for the outer product.3 In light of its equivalence to a locally connected neural network, we will refer to the tensor network as a _locally connected tensor network_. We note that for the equivalent neural network to be practical (in terms of memory and runtime), the width \(R\) needs to be of moderate size (typically no more than a few thousands). Specifically, \(R\) cannot be exponential in the order \(N\), meaning \((R)\) needs to be much smaller than \(N\).

By virtue of the locally connected tensor network's equivalence to a deep neural network, it has been paramount for the study of expressiveness and generalization in deep learning [12; 9; 10; 13; 14; 54; 34; 35; 3; 30; 31; 36; 47; 48; 49; 50]. Although the equivalent deep neural network (which has polynomial non-linearity) is less common than other neural networks (_e.g._, ones with ReLU non-linearity), it has demonstrated competitive performance in practice [8; 11; 55; 58; 22]. More importantly, its theoretical analyses, through the equivalence to the locally connected tensor network, brought forth numerous insights that were demonstrated empirically and led to development of practical tools for common locally connected architectures. Continuing this line, we will demonstrate our theoretical insights through experiments with widespread convolutional, recurrent and local self-attention architectures (Section 4.3), and employ our theory for deriving an algorithm that enhances the suitability of a data distribution to said architectures (Section 5).

### Necessary and Sufficient Condition for Fitting Tensor

Herein we show that the ability of the locally connected tensor network (defined in Section 3.1) to fit (_i.e._ represent) a given tensor is determined by the entanglements that the tensor admits under the below-defined _canonical partitions_ of \([N]\). Note that each canonical partition comprises a subset of contiguous indices, so low entanglement under canonical partitions can be viewed as a formalization of locality.

**Definition 2**.: The _canonical partitions_ of \([N]=[2^{L}]\) (illustrated in Figure 4 of Appendix B) are:

\[_{N}:= \{(,^{c}):\,= \{2^{L-l}(n-1)+1,,2^{L-l} n\},\,l\{0, ,L\},\;n[2^{l}]\}.\]

By appealing to known upper bounds on the entanglements that a given tensor network supports [16; 35], we now establish that if the locally connected tensor network can fit a given tensor, that tensor

Figure 2: The analyzed tensor network equivalent to a locally connected neural network. **(a)** We consider a tensor network adhering to a perfect binary tree connectivity with \(N=2^{L}\) leaf nodes, for \(L\), generating \(_{}^{D_{1} D_{N}}\). Axes corresponding to open edges are indexed such that open edges descendant to any node of the tree have contiguous indices. The lengths of axes corresponding to inner (non-open) edges are equal to \(R\), referred to as the _width_ of the tensor network. **(b)** Contracting \(_{}\) with vectors \(^{(1)}^{D_{1}},,^{(N)}^{D _{N}}\) produces \(_{n=1}^{N}^{(n)},_{}\). Performing these contractions from leaves to root can be viewed as a forward pass of a data instance \((^{(1)},,^{(N)})\) through a certain locally connected neural network (with polynomial non-linearity; see, _e.g._, [12; 10; 35; 49]). Accordingly, we call the tensor network generating \(_{}\) a _locally connected tensor network_.

must admit low entanglement under the canonical partitions of its axes. Namely, suppose that \(_{}\) -- the tensor generated by the locally connected tensor network -- well-approximates an order \(N\) tensor \(\). Then, Theorem 1 below shows that the entanglement of \(\) with respect to every canonical partition \((,^{c})_{N}\) cannot be much larger than \((R)\) (recall that \(R\) is the width of the locally connected tensor network, and that in practical settings \((R)\) is much smaller than \(N\)), whereas the expected entanglement of a random tensor with respect to \((,^{c})\) is on the order of \(\{||,|^{c}|\}\) (which is linear in \(N\) for most canonical partitions).

In the other direction, Theorem 2 below implies that low entanglement under the canonical partitions is not only necessary for a tensor to be fit by the locally connected tensor network, but also sufficient.

**Theorem 1**.: _Let \(_{}^{D_{1} D_{N}}\) be a tensor generated by the locally connected tensor network defined in Section 3.1, and let \(^{D_{1} D_{N}}\). For any \([0,\|\|/4]\), if \(\|_{}-\|\), then for all canonical partitions \((,^{c})_{N}\) (Definition 2) it holds that \(QE(;)(R)+\|} (D_{})+2\|}}\), where \(D_{}:=\{_{n}D_{n},_{n^{c} }D_{n}\}\).4 In contrast, a random tensor \(^{}^{D_{1} D_{N}}\), drawn according to the uniform distribution over the set of unit norm tensors, satisfies \([QE(^{};)]\{||,| ^{c}|\}(_{n[N]}D_{n})+ -\) for all canonical partitions \((,^{c})_{N}\)._

Proof sketch (proof in Appendix M.2).: In general, the entanglements that a tensor network supports can be upper bounded through cuts in its graph . For the locally connected tensor network, these bounds imply that \(QE(_{};)(R)\) for any canonical partition \((,^{c})\). The upper bounds on the entanglements of \(\) then follow by showing that if \(_{}\) and \(\) are close, then so are their entanglements. The lower bounds on the expected entanglements of a random tensor are derived based on a characterization from . 

**Theorem 2**.: _Let \(^{D_{1} D_{N}}\) and \(>0\). Suppose that for all canonical partitions \((,^{c})_{N}\) (Definition 2) it holds that \(QE(;)}{(2N-3)\|\|^{2}} (R)\).5 Then, there exists an assignment for the tensors constituting the locally connected tensor network (defined in Section 3.1) such that it generates \(_{}^{D_{1} D_{N}}\) satisfying \(\|_{}-\|\)._

Proof sketch (proof in Appendix M.3).: We show that if \(\) has low entanglement under a canonical partition \((,^{c})_{N}\), then the singular values of \(;\) must decay rapidly (recall that \(;\) is the arrangement of \(\) as a matrix where rows correspond to axes indexed by \(\) and columns correspond to the remaining axes). The approximation guarantee is then obtained through a construction from , which is based on truncated singular value decompositions of every \(;\) for \((,^{c})_{N}\). 

## 4 Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Accurate Prediction

In this section we consider the locally connected tensor network from Section 3.1 in a machine learning setting. We show that attaining low population loss amounts to fitting a tensor defined by the data distribution, whose axes correspond to features (Section 4.1). Applying the theorems of Section 3.2, we then conclude that the locally connected tensor network is capable of accurate prediction if and only if the data distribution admits low entanglement under canonical partitions of features (Section 4.2). This conclusion is corroborated through experiments, demonstrating that the performance of common locally connected neural networks (including convolutional, recurrent, and local self-attention neural networks) is inversely correlated with the entanglement under canonical partitions of features in the data (Section 4.3). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see Appendix J.2 for an extension to arbitrary dimensions.

### Accurate Prediction Is Equivalent to Fitting Data Tensor

As discussed in Section 3.1, the locally connected tensor network generating \(_{}^{D_{1} D_{N}}\) is equivalent to a locally connected neural network, whose forward pass over a data instance \((^{(1)},,^{(N)})\) yields \(_{n=1}^{N}^{(n)},_{}\), where \(^{(1)}^{D_{1}},,^{(N)}^{D_ {N}}\). Motivated by this fact, we consider a binary classification setting, in which the label \(y\) of the instance \((^{(1)},,^{(N)})\) is either \(1\) or \(-1\), and the prediction \(\) is taken to be the sign of the output of the neural network, _i.e._\(=_{n=1}^{N}^{(n)}, _{}\).

Suppose we are given a training set of labeled instances \((^{(1,m)},,^{(N,m)}),y^{(m)} }_{m=1}^{M}\) drawn i.i.d. from some distribution, and we would like to learn the parameters of the neural network through the soft-margin support vector machine (SVM) objective, _i.e._ by optimizing:

\[_{\|_{}\| B}_{m=1}^{M} 0,1-y^{(m)}_{n=1}^{N}^{(n,m)},_{}}\,, \]

for a predetermined constant \(B>0\). We assume instances are normalized, _i.e._ the distribution is such that all vectors constituting an instance have norm no greater than one. We also assume that \(B 1\). In this case \(y^{(m)}_{n=1}^{N}^{(n,m)},_{} 1\), so our optimization problem can be expressed as \(_{\|_{}\| B}1-_{},_{}\), where

\[_{}:=_{m=1}^{M}y^{(m)} _{n=1}^{N}^{(n,m)} \]

is referred to as the _empirical data tensor_. This means that the accuracy over the training data is determined by how large the inner product \(_{},_{}\) is.

Disregarding the degenerate case of \(_{}=0\) (in which the optimized objective is constant), the inner products \(_{},_{}\) and \(_{}/\|_{}\|,_{}\) differ by only a multiplicative (positive) constant, so fitting the training data amounts to optimizing \(_{\|_{}\| B}_{ }/\|_{}\|,_{}\). If \(_{}\) can represent some \(\), then it can also represent \(c\) for every \(c\). Thus, we may equivalently optimize \(_{_{}}_{}/\| _{}\|,_{}/\|_{}\|\) and multiply the result by \(B\). Fitting the training data therefore boils down to minimizing \(\|_{}}{\|_{}\|}-_{}}{\|_{}\|}\|\). In other words, the accuracy achievable over the training data is determined by the extent to which \(_{}}{\|_{}\|}\) can fit the normalized empirical data tensor \(_{}}{\|_{}\|}\).

The arguments above are independent of the training set size \(M\), and in fact apply to the population loss as well, in which case \(_{}\) is replaced by the _population data tensor_:

\[_{}:=_{(^{(1)},,^{( N)}),y}y_{n=1}^{N}^{(n)}\,. \]

Disregarding the degenerate case of \(_{}=0\) (_i.e._ that in which the population loss is constant), it follows that the achievable accuracy over the population is determined by the extent to which \(_{}}{\|_{}\|}\) can fit the normalized population data tensor \(_{}}{\|_{}\|}\). We refer to the minimal distance from it as the _suboptimality in achievable accuracy_.

**Definition 3**.: In the context of the classification setting above, the _suboptimality in achievable accuracy_ is \(:=_{_{}}\|_{ }}{\|_{}\|}-_{}}{ \|_{}\|}\|\).

### Necessary and Sufficient Condition for Accurate Prediction

In the classification setting of Section 4.1, by invoking Theorems 1 and 2 from Section 3.2, we conclude that the suboptimality in achievable accuracy is small if and only if the population data tensor \(_{}\) admits low entanglement under the canonical partitions of its axes (Definition 2). This is formalized in Corollary 1 below. The quantum entanglement of \(_{}\) with respect to an arbitrary partition of its axes \((,^{c})\), where \([N]\), is a measure of dependence between the data features indexed by \(\) and those indexed by \(^{c}\) -- see Appendix D for intuition behind this. Thus, Corollary 1 implies that the suboptimality in achievable accuracy is small if and only if the data admits low dependence under the canonical partitions of features. Since canonical partitions comprise a subset with contiguous indices, we obtain a formalization of the intuition by which data distributions suitable for locally connected neural networks are those exhibiting a "local nature."

Directly evaluating the conditions required by Corollary 1 -- low entanglement under canonical partitions for \(_{}\) -- is impractical, since: _(i)_\(_{}\) is defined via an unknown data distribution(Equation (3)); and _(ii)_ computing the entanglements involves taking singular value decompositions of matrices with size exponential in the number of input variables \(N\). Fortunately, as Proposition 2 in Appendix E shows, \(_{}\) is with high probability well-approximated by the empirical data tensor \(_{}\). Moreover, the entanglement of \(_{}\) under any partition can be computed efficiently, without explicitly storing or manipulating an exponentially large matrix -- see Appendix F for an algorithm (originally proposed in ). Overall, we obtain an efficiently computable criterion (low entanglement under canonical partitions for \(_{}\)), that with high probability is both necessary and sufficient for low suboptimality in achievable accuracy (see Corollary 2 in Appendix E for a formalization).

**Corollary 1**.: _Consider the classification setting of Section 4.1, and let \([0,1/4]\). If there exists a canonical partition \((,^{c})_{N}\) (Definition 2) under which \(QE(_{};)>(R)+2(D_{ })+2\), where \(R\) is the width of the locally connected tensor network and \(D_{}:=\{_{n}D_{n},_{n^{c} }D_{n}\}\), then \(>\). Conversely, if for all \((,^{c})_{N}\) it holds that \(QE(_{};)}{8N-12} (R)\), then \(\)._

Proof sketch (proof in Appendix M.4).: The result follows from Theorems 1 and 2 after accounting for the normalization of \(_{}\) in the definition of \(\) (Definition 3). 

### Empirical Demonstration

Corollary 2 establishes that, with high probability, the locally connected tensor network (from Section 3.1) can achieve high prediction accuracy if and only if the empirical data tensor (Equation (2)) admits low entanglement under canonical partitions of its axes. We corroborate our formal analysis through experiments, demonstrating that its conclusions carry over to common locally connected architectures. Namely, applying convolutional neural networks, S4 (a popular recurrent neural network; see ), and a local self-attention model  to different datasets, we show that the achieved test accuracy is inversely correlated with the entanglements of the empirical data tensor under canonical partitions. Below is a description of experiments with one-dimensional (_i.e._ sequential) models and data. Additional experiments with two-dimensional (imagery) models and data are given in Appendix J.2.3.

Discerning the relation between entanglements of the empirical data tensor and performance (prediction accuracy) of locally connected neural networks requires datasets admitting different entanglements. A potential way to acquire such datasets is as follows. First, select a dataset on which locally connected neural networks perform well, in the hopes that it admits low entanglement under canonical partitions; natural candidates are datasets comprising images, text or audio. Subsequently, create "shuffled" variants of the dataset by repeatedly swapping the position of two features chosen at random.6 This erodes the original arrangement of features in the data, and is expected to yield higher entanglement under canonical partitions.

We followed the blueprint above for a binary classification version of the Speech Commands audio dataset . Figure 3 presents test accuracies achieved by a convolutional neural network, S4, and a local self-attention model, as well as average entanglement under canonical partitions of the empirical data tensor, against the number of random feature swaps performed to create the dataset. As expected, when the number of swaps increases, the average entanglement under canonical partitions becomes higher. At the same time, in accordance with our theory, the prediction accuracies of the locally connected neural networks substantially deteriorate, showing an inverse correlation with the entanglement under canonical partitions.

## 5 Enhancing Suitability of Data to Locally Connected Neural Networks

Our analysis (Sections 3 and 4) suggests that a data distribution is suitable for locally connected neural networks if and only if it admits low entanglement under canonical partitions of features. Motivated by this observation, we derive a preprocessing algorithm aimed to enhance the suitability of a data distribution to locally connected neural networks (Section 5.1 and Appendix G). Empirical evaluations demonstrate that it significantly improves prediction accuracies of common locally connected neural networks on various datasets (Section 5.2). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see Appendix J.3 for an extension to arbitrary dimensions.

### Search for Feature Arrangement With Low Entanglement Under Canonical Partitions

Our analysis naturally leads to a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly. Formally, suppose we have \(M\) training instances \(\{((^{(1,m)},,^{(N,m)}),y^{(m)}) \}_{m=1}^{M}\), where \(y^{(m)}\{1,-1\}\) and \(^{(n,m)}^{D}\) for \(n[N],m[M]\), with \(D\). Assume without loss of generality that \(N\) is a power of two (if this is not the case we may add constant features as needed). The aforementioned recipe boils down to a search for a permutation \(:[N][N]\), which when applied to feature indices leads the empirical data tensor \(_{}\) (Equation (2)) to admit low entanglement under the canonical partitions of its axes (Definition 2).

A greedy realization of the foregoing search is as follows. Initially, partition the features into two equally sized sets \(_{1,1}[N]\) and \(_{1,2}:=[N]_{1,1}\) such that the entanglement of \(_{}\) with respect to \((_{1,1},_{1,2})\) is minimal. That is, find \(_{1,1}*{argmin}_{_{C}[N],||= N/2}QE(_{};)\). The permutation \(\) will map \(_{1,1}\) to coordinates \(\{1,,\}\) and \(_{1,2}\) to \(\{+1,,N\}\). Then, partition \(_{1,1}\) into two equally sized sets \(_{2,1}_{1,1}\) and \(_{2,2}:=_{1,1}_{2,1}\) such that the average of entanglements induced by these sets is minimal, _i.e._\(_{2,1}*{argmin}_{_{1,1},| |=_{1,1}|/2}[QE(_{};)+QE(_{};_{1,1} )]\). The permutation \(\) will map \(_{2,1}\) to coordinates \(\{1,,\}\) and \(_{2,2}\) to \(\{+1,,\}\). A partition of \(_{1,2}\) into two equally sized sets \(_{2,3}\) and \(_{2,4}\) is obtained similarly, where \(\) will map \(_{2,3}\) to coordinates \(\{+1,,\}\) and \(_{2,4}\) to \(\{+1,,N\}\). Continuing in the same fashion, until we reach subsets \(_{L,1},,_{L,N}\) consisting of a single feature index each, fully specifies the permutation \(\).

Unfortunately, the step lying at the heart of the above scheme -- finding a balanced partition that minimizes average entanglement -- is computationally prohibitive, and we are not aware of any tools that alleviate the computational difficulty. However, as discussed in Appendix G, if one replaces entanglement with an appropriate surrogate measure, then each search for a balanced partition minimizing average entanglement converts into a _minimum balanced cut problem_, which enjoys a wide array of established approximation tools . We thus obtain a practical algorithm for enhancing the suitability of a data distribution to locally connected neural networks.

### Experiments

We empirically evaluate our feature rearrangement method, detailed in Appendix G, using common locally connected neural networks -- a convolutional neural network, S4 (popular recurrent neural network; see ), and a local self-attention model  -- over randomly permuted audio datasets (Section 5.2.1) and several tabular datasets (Section 5.2.2). For brevity, we defer some experiments and implementation details to Appendices K and L. Additional experiments with two-dimensional data are given in Appendix J.3.3.

#### 5.2.1 Randomly Permuted Audio Datasets

Section 4.3 demonstrated that audio data admits low entanglement under canonical partitions of features, and that randomly permuting the position of features leads this entanglement to increase,

Figure 3: The prediction accuracies of common locally connected neural networks are inversely correlated with the entanglements of the data under canonical partitions of features, in compliance with our theory (Sections 4.1 and 4.2). **Left:** Average entanglement under canonical partitions (Definition 2) of the empirical data tensor (Equation (2)), for binary classification variants of the Speech Commands audio dataset  obtained by performing random position swaps between features. **Right:** Test accuracies achieved by a convolutional neural network (CNN) , S4 (a popular class of recurrent neural networks; see ), and a local self-attention model , against the number of random feature swaps performed to create the dataset. **All:** Reported are the means and standard deviations of the quantities specified above, taken over ten different random seeds. See Appendix J.2.3 for experiments over (two-dimensional) image data and Appendix L for implementation details.

while substantially degrading the prediction accuracy of locally connected neural networks. A sensible test for our method is to evaluate its ability to recover performance lost due to the random permutation of features.

For the Speech Commands dataset , Table 1 compares prediction accuracies of locally connected neural networks on the data: _(i)_ subject to a random permutation of features; _(ii)_ attained after rearranging the randomly permuted features via our method; and _(iii)_ attained after rearranging the randomly permuted features via IGTD  -- a heuristic scheme designed for convolutional neural networks (see Appendix A). As can be seen, our method leads to significant improvements, surpassing those brought forth by IGTD. Note that the performance lost due to the random permutation of features is not entirely recovered.7 We believe this relates to phenomena outside the scope of the theory underlying our method (Sections 3 and 4), for example translation invariance in data being beneficial in terms of generalization. Investigation of such phenomena and suitable modification of our method are regarded as promising directions for future work.

The number of features in the audio dataset used in the experiment above is 2048. We demonstrate the scalability of Algorithm 2 by including in Appendix K an experiment over audio data with 50,000 features. In this experiment, instances of the minimum balanced cut problem encountered in Algorithm 2 entail graphs with up to \(25 10^{8}\) edges. They are solved using the well-known edge sparsification algorithm of  that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be).

#### 5.2.2 Tabular Datasets

The prediction accuracies of locally connected neural networks on tabular data, _i.e._ on data in which features are arranged arbitrarily, is known to be subpar . Tables 2 and 5 report results of experiments with locally connected neural networks over standard tabular benchmarks (namely "semeion", "isolet" and "dna" ), demonstrating that arranging features via our method leads to significant improvements in prediction accuracies, surpassing improvements brought forth by IGTD (a heuristic scheme designed for convolutional neural networks ). Note that our method does not lead to state of the art prediction accuracies on the evaluated benchmarks.8 However, the results suggest that it renders locally connected neural networks a viable option for tabular data. This option is particularly appealing in when the number of features is large settings, where many alternative approaches (_e.g._ ones involving fully connected neural networks) are impractical.

## 6 Conclusion

### Summary

The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks -- a prevalent family of deep learning architectures that includes as special cases convolutional neural networks, recurrent neural networks (in particular the recent S4 models) and local self-attention models -- we address this problem by

    & Randomly Permuted & Our Method & IGTD \\  CNN & \(5.5 0.5\) & \( 1.6\) & \(5.7 0.6\) \\ S4 & \(9.4 0.8\) & \( 1.8\) & \(12.8 1.3\) \\ Local-Attention & \(7.8 0.5\) & \( 0.6\) & \(7 0.6\) \\   

Table 1: Arranging features of randomly permuted audio data via our method (detailed in Appendix G) significantly improves the prediction accuracies of locally connected neural networks. Reported are test accuracies (mean and standard deviation over ten random seeds) of a convolutional neural network (CNN), S4 (a popular recurrent neural network; see ), and a local self-attention model , over the Speech Commands dataset  subject to different arrangements of features: _(i)_ a random arrangement; _(ii)_ an arrangement provided by applying our method to the random arrangement; and _(iii)_ an arrangement provided by applying an adaptation of IGTD  — a heuristic scheme designed for convolutional neural networks — to the random arrangement. For each model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the standard deviation corresponding to the former). Our method leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD. See Appendix K for experiments demonstrating its scalability and implementation details.

adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction (_i.e._ can express a solution with low population loss) over a data distribution _if and only if_ the data distribution admits low quantum entanglement under certain canonical partitions of features. Experiments with widespread locally connected neural networks corroborate this finding.

Our theory suggests that the suitability of a data distribution to locally connected neural networks may be enhanced by arranging features such that low entanglement under canonical partitions is attained. Employing a certain surrogate for entanglement, we show that this arrangement can be implemented efficiently, and that it leads to substantial improvements in the prediction accuracies of common locally connected neural networks on various datasets.

### Limitations and Future Work

#### 6.2.1 Neural network architecture

We theoretically analyzed a locally connected neural network with polynomial non-linearity, by employing its equivalence to a tensor network (_cf._ Section 3.1). Accounting for neural networks with connectivities beyond those considered (_e.g._ connectivities that are non-local or ones involving skip connections) is an interesting topic for future work. It requires modification of the equivalent tensor network and corresponding modification of the definition of canonical partitions, similarly to the analysis in Appendix J. Another valuable direction is to account for neural networks with other non-linearities, _e.g._ ReLU. We believe this may be achieved through a generalized notion of tensor networks, successfully used in past work to analyze such architectures .

#### 6.2.2 Objective function

The analysis in Section 4 assumes a binary soft-margin SVM objective. Extending it to other objective functions, _e.g._ multi-class SVM, may shed light on the relation between the objective and the requirements for a data distribution to be suitable to neural networks.

#### 6.2.3 Textual data

Our experiments (in Section 4.3 and Appendix J.2.3) show that the necessary and sufficient condition we derived for a data distribution to be suitable to a locally connected neural network -- namely, low quantum entanglement under canonical partitions of features -- is upheld by audio and image datasets. This falls in line with the excellent performance of locally connected neural networks over these data modalities. In contrast, high performant architectures for textual data are typically non-local . Investigating the quantum entanglements that textual data admits and, in particular, under which partitions they are low, may allow designing more efficient architectures with connectivity tailored to textual data.

### Outlook

The data modalities to which deep learning is most commonly applied -- namely ones involving images, text and audio -- are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information). We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data. Concepts and tools from physics -- a branch of science concerned with formally reasoning about natural phenomena -- may be key to overcoming said difficulty. We hope that our use of quantum entanglement will encourage further research along this line.

    &  &  \\   & Baseline & Our Method & IGTD & Baseline & Our Method & IGTD \\  CNN & \(77.7 1.4\) & \(80.0 1.8\) & \(78.9 1.9\) & \(91.0 0.6\) & \( 0.4\) & \(92.0 0.6\) \\ S4 & \(82.5 1.1\) & \( 0.5\) & \(86.0 0.7\) & \(92.3 0.4\) & \( 0.3\) & \(92.7 0.5\) \\ Local-Attention & \(60.9 4.9\) & \( 1.7\) & \(67.8 2.6\) & \(82.0 1.6\) & \( 0.6\) & \(85.7 1.9\) \\   

Table 2: Arranging features of tabular datasets via our method (detailed in Appendix G) significantly improves the prediction accuracies of locally connected neural networks. Reported are results of experiments analogous to those of Table 1, but with the “semeion” and “isolet” tabular classification datasets . Since to the arrangement of features in a tabular dataset is intended to be arbitrary, we regard as a baseline the prediction accuracies attained with a random permutation of features. For each combination of dataset and model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the standard deviation corresponding to the former). As in the experiment of Table 1, rearranging the features according to our method leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD. See Appendix K for experiments with an additional tabular dataset (“dna”) and implementation details.