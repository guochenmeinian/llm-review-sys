# Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms

Dheeraj Baby

UC Santa Barbara

dheeraj@ucsb.edu

&Saurabh Garg1

Carnegie Mellon University

sgarg2@andrew.cmu.edu

&Tzu-Ching Yen1

Carnegie Mellon University

tzuchiny@andrew.cmu.edu

&Sivaraman Balakrishnan

Carnegie Mellon University

sbalakri@andrew.cmu.edu

&Zachary C. Lipton

Carnegie Mellon University

zlipton@andrew.cmu.edu

&Yu-Xiang Wang

UC Santa Barbara

yuxiangw@cs.ucsb.edu

Equal Contribution

###### Abstract

This paper focuses on supervised and unsupervised online label shift, where the class marginals \(Q(y)\) varies but the class-conditionals \(Q(x|y)\) remain invariant. In the unsupervised setting, our goal is to adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically evolving class marginals given only labeled online data. We develop novel algorithms that reduce the adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates of _online regression oracles_ that track the drifting proportions. Experiments across numerous simulated and real-world online label shift scenarios demonstrate the superior performance of our proposed approaches, often achieving 1-3% improvement in accuracy while being sample and computationally efficient. Code is publicly available at this url.

## 1 Introduction

Supervised machine learning algorithms are typically developed assuming independent and identically distributed (iid) data. However, real-world environments evolve dynamically . Absent furthermore assumptions on the nature of the shift, such problems are intractable. One line of research has explored causal structures such as covariate shift , label shift , and missingness shift , for which the optimal target predictor is identified from labeled source and unlabeled target data. Let's denote the feature-label pair of an example by \((x,y)\). Label shift addresses the setting where the label marginal distribution \(Q(y)\) may change but the conditional distribution \(Q(x|y)\) remains fixed. Most prior work addresses the batch setting for unsupervised adaptation, where a single shift occurs between a source and target population . However, in the real world, shifts are more likely to occur continually and unpredictably, with data arriving in an _online_ fashion. A nascent line of research tackles online distribution shift, typically in settings where labeled data is available in real time , seeking to minimize the _dynamic regret_.

Researchers have only begun to explore the role that structures like label shift might play in such online settings. Initial attempts to learn under unsupervised online label shifts were made by Wu et al.  and Bai et al. , both of which rely on reductions to Online Convex Optimization (OCO) . This line of research aims in updating a classification model based on online data so that the overall regret is controlled. However, Wu et al.  only control for _static regret_ against a fixedclassifier (or model) in hindsight and makes the assumption of the convexity (of losses), which is often violated in practice. In the face of online label shift, where the class marginals can vary across rounds, a more fitting notion is to control the _dynamic regret_ against a sequence of models in hindsight. Motivated by this observation, Bai et al.  control for the dynamic regret. However, their approach is based on updating model parameters (of the classifier) with online gradient descent and relying on convex losses limits the applicability of their methods (e.g. algorithms in Bai et al.  can not be employed with decision tree classifiers).

In this paper, we study the problem of learning classifiers under Online Label Shift (OLS) in both _supervised_ and _unsupervised_ settings (Fig.1). In both these settings, the distribution shifts are an online process that respects the label shift assumption. Our primary goal is to develop algorithms that side-step convexity assumptions and at the same time _optimally_ adapt to the non-stationarity in the label drift. In the Unsupervised Online Label Shift (UOLS) problem, the learner is provided with a pool of labeled offline data sampled iid from the distribution \(Q_{0}(x,y)\) to train an initial model \(f_{0}\). Afterwards, at every online round \(t\), few _unlabeled_ data points sampled from \(Q_{t}(x)\) are presented. The goal is to adapt \(f_{0}\) to the non-stationary target distributions \(Q_{t}(x,y)\) so that we can accurately classify the unlabelled data. By contrast, in Supervised Online Label Shift (SOLS), our goal is to learn classifiers from _only_ the (labeled) samples that arrive in an online fashion from \(Q_{t}(x,y)\) at each time step, while simultaneously adapting to the non-stationarity induced due to changing label proportions. While SOLS is similar to online learning under non-stationarity, UOLS differs from classical online learning as the test label is not seen during online adaptation. Below are the list of contributions of this paper.

* **Unsupervised adaptation.** For the UOLS problem, we provide a reduction to online regression (see Defn. 1), and develop algorithms for adapting the initial classifier \(f_{0}\) in a computationally efficient way leading to _minimax optimal_ dynamic regret. Our approach achieves the best-of-both worlds of Wu et al. , Bai et al.  by controlling the dynamic regret while allowing us to use expressive black-box models for classification (Sec. 3).
* **Supervised adaptation.** We develop algorithms for SOLS problem that lead to _minimax optimal_ dynamic regret without assuming convexity of losses (Sec. 4). Our theoretically optimal solution is based on weighted Empirical Risk Minimization (wERM) with weights tracked by online regression. Motivated by our theory, we also propose a _simple continual learning_ baseline which achieves empirical performance competitive to the wERM from scratch at each time step across several semi-synthetic SOLS problems while being \(15\) more efficient in computation cost.
* **Low switching regressors.** We propose a black-box reduction method to convert an optimal online regression algorithm into another algorithm that switches decisions _sparingly_ while _maintaining minimax optimality_. This method is relevant for online change point detection. We demonstrate its application in developing SOLS algorithms to train models only when significant distribution drift is detected, while maintaining statistical optimality (App. D and Algorithm 6).
* **Extensive empirical study.** We corroborate our theoretical findings with experiments across numerous simulated and real-world OLS scenarios spanning vision and language datasets (Sec. 5). Our proposed algorithms often improve over the best alternatives in terms of both final accuracy and label marginal estimation. This advantage is particularly prominent with limited initial holdout data (in the UOLS problem) highlighting the _sample efficiency_ of our approach.

**Notes on technical novelties.** Even-though online regression is a well studied technique, to the best of our knowledge, it is not used before to address the problem of online label shift. It is precisely the usage of regression which lead to tractable adaptation algorithms while side-stepping convexity assumptions thereby allowing us to use very flexible models for classification. This is in stark contrast

Figure 1: _UOLS and SOLS setup._ Dashed (double) arrows are exclusive to UOLS (SOLS) settings. Other objects are common to both setups. Central question: how to adapt the model in real-time to drifting label marginals based on all the available data so far?

to OCO based reductions in  and . We propose new theoretical frameworks and identify the right set of assumptions for materializing the reduction to online regression. It was not evident initially that this link would lead to _minimax optimal_ dynamic regret rates as well as _consistent_ empirical improvement over prior works. Proof of the lower bounds requires adapting the ideas from non-stationary stochastic optimization  in a non-trivial manner. Further, none of the proposed methods require the prior knowledge of the extent of distribution drift.

## 2 Problem Setup

Let \(^{d}\) be the input space and \(=[K]:=\{1,2,,K\}\) be the output space. Let \(Q\) be a distribution over \(\) and let \(q()\) denotes the corresponding label marginal. \(_{K}\) is the \(K\)-dimensional simplex. For a vector \(v^{K}\), \(v[i]\) is its \(i^{th}\) coordinate. We assume that we have a hypothesis class \(\). For a function \(f:_{K}\), we also use \(f(i|x)\) to indicate \(f(x)[i]\). With \((f(x),y)\), we denote the loss of making a prediction with the classifier \(f\) on \((x,y)\). \(L\) denotes the expected loss, i.e., \(L=_{(x,y) Q}[(f(x),y)]\). \(()\) hides dependencies in absolute constants and poly-logarithmic factors of horizon and failure probabilities.

In this work, we study online learning under distribution shift, where the distribution \(Q_{t}(x,y)\) may continuously change with time. Throughout the paper, we focus on the _label shift_ assumption where the distribution over label proportions \(q_{t}(y)\) can change arbitrarily but the distribution of the covariate conditioned on a label value (i.e., \(Q_{t}(x|y)\)) is assumed to be invariant across all time steps. We refer to this setting as Online Label Shift (OLS). Here, we consider settings of unsupervised and supervised OLS settings captured in Frameworks 1 and 3 respectively. In both settings, at round \(t\) a sample \((x_{t},y_{t})\) is drawn from a distribution with density \(Q_{t}(x_{t},y_{t})\). In the UOLS setting, the label is not revealed to the learner. However, we assume access to offline labeled data sampled iid from \(Q_{0}\) which we use to train an initial classifier \(f_{0}\). The goal is to adapt the initial classifier \(f_{0}\) to drifting label distributions. In contrast, for the SOLS setting, the label is revealed to the learner after making a prediction and the goal is to learn a classifier \(f_{t}\) for each time step.

Next, we formally define the concept of online regression which will be central to our discussions. Simply put, an online regression algorithm tracks a ground truth sequence from noisy observations.

**Definition 1** (online regression).: _Fix any \(T>0\). The following interaction scheme is defined to be the online regression protocol._

* _At round_ \(t[T]\)_, an algorithm predicts_ \(_{t}^{K}\)_._
* _A noisy version of ground truth_ \(z_{t}=_{t}+_{t}\) _is revealed where_ \(_{t},_{t}^{K}\)_, and_ \(\|_{t}\|_{2}\)_,_ \(\|_{t}\|_{2} B\)_. Further the noise_ \(_{t}\) _are independent across time with_ \(E[_{t}]=0\) _and_ \((_{t}[i])^{2}\; i[K]\)_._

_An online regression algorithm aims to control \(_{t=1}^{T}_{t}-_{t}_{2}^{2}\). Moreover, the regression algorithm is defined to be adaptively minimax optimal if with probability at least \(1-\), \(_{t=1}^{n}_{t}-_{t}_{2}^{2}=(T^{1/ 3}V_{T}^{2/3})\) without knowing \(V_{T}\) ahead of time. Here \(V_{T}:=_{t=2}^{T}_{t}-_{t-1}_{1}\) is termed as the Total Variation (TV) of the sequence \(_{1:T}\)._

## 3 Unsupervised Online Label Shift

In this section, we develop a framework for handling the UOLS problem. We summarize the setup in Framework 1. Since in practice, we may need to work with classifiers such as deep neural networks or decision trees, we do not impose convexity assumptions on the (population) loss of the classifier as a function of the model parameters. Despite the absence of such simplifying assumptions, we provide performance guarantees for our label shift adaption techniques so that they are certified to be fail-safe.

Under the label shift assumption, we have \(Q_{t}(y|x)\) as a re-weighted version of \(Q_{0}(y|x)\):

\[Q_{t}(y|x)=(y)}{Q_{t}(x)}Q_{t}(x|y)=(y)}{Q_{t}(x)}Q_{0}(x |y)=(y)Q_{0}(x)}{Q_{t}(x)Q_{0}(y)}Q_{0}(y|x)(y)}{Q _{0}(y)}Q_{0}(y|x),\]

where the second equality is due to the label shift assumption. Hence, a reasonable strategy is to re-weight the initial classifier \(f_{0}\) with label proportions (estimate) at the current step, since we only have to correct the label distribution shift. This re-weighting technique is widely used for offline label shift correction  and for learning under label imbalance .

Our starting point in developing a framework is inspired by Wu et al. , Bai et al. . For self-containedness, we briefly recap their arguments next. We refer interested readers to their papers for more details. Wu et al.  considers a hypothesis class of re-weighted initial classifier \(f_{0}\). The loss of a hypothesis is parameterised by the re-weighting vector. They use tools from OCO to optimise the loss and converge to a best fixed classifier. However as noted in Wu et al. , the losses are not convex with respect to the re-weight vector in practice. Hence usage of OCO techniques is not fully satisfactory in their problem formulation.

In a complementary direction, Bai et al.  abandons the idea of re-weighting. Instead, they update the parameters of a model at each round using online gradient descent and a loss function whose expected value is assumed to be convex with respect to model parameters. They provide dynamic regret guarantees against a sequence of changing model parameters in hindsight, and connects it to the variation of the true label marginals. More precisely, they provide algorithms with \(_{t=1}^{T}L_{t}(w_{t})-L_{t}(w_{t}^{*})\) to be well controlled where \(w_{t}^{*}\) is the best model parameter to be used at round \(t\) and \(L_{t}\) is a (population level) loss function. However, there are some scopes for improvement in this direction as well. For example, the convexity assumption can be easily violated when working with interpretable models based on decision trees, or if we want to retrain few final layers of a deep classifier based on new data. Further as noted in the experiments (Sec. 5), their methods based on retraining the classifier require more data than re-weighting based methods. Our experiments also indicate that re-weighting can be computationally cheaper than re-training without sacrificing the classifier accuracy.

Thus, on the one hand, the work of Wu et al.  allows us to use the power of expressive initial classifiers while only controlling the static regret against a fixed hypothesis. On the other hand, the work of Bai et al.  allows controlling the dynamic regret while limiting the flexibility of deployed models. We next provide our framework for handling label shifts that achieves the best of both worlds by controlling the dynamic regret while allowing the use of expressive _blackbox_ models.

In summary, we estimate the sequence of online label marginals and leverage the idea of re-weighting an initial classifier as in Wu et al. . In particular, given an estimate \(_{t}(y)\) of the true label marginal at round \(t\), we compute the output of the re-weighted classifier \(f_{t}\) as \(_{t}(y)}{q_{0}(y)}f_{0}(y|x)/Z\) where \(Z=_{y}_{t}(y)}{q_{0}(y)}f_{0}(y|x)\). However, to get around the issue of non-convexity, we separate out the process of estimating the re-weighting vectors via a reduction to online regression which is a well-defined and convex problem with computationally efficient off-the-shelf algorithms readily available. Second, and more importantly, Wu et al.  competes with the best _fixed_ re-weighted hypothesis. However, in the problem setting of label shift, the true label marginals are in fact changing. Hence, we control the _dynamic regret_ against a sequence of re-weighted hypotheses in hindsight. All proofs for the next sub-section are deferred to App. C.

### Proposed algorithm and performance guarantees

We start by presenting our assumptions. This is followed by the main algorithm for UOLS and its performance guarantees. Similar to the treatment in Bai et al. , we assume the following.

**Assumption 1**.: _Assume access to the true label marginals \(q_{0}_{K}\) of the offline training data and the true confusion matrix \(C^{K K}\) with \(C_{ij}=E_{x Q_{0}(|y=j)}f_{0}(i|x)\). Further the minimum singular value \(_{min}(C)=(1)\) is bounded away from zero._

As noted in prior work , the invertibility of the confusion matrix holds whenever the classifier \(f_{0}\) has good accuracy and the true label marginal \(q_{0}\) assigns a non-zero probability to each label. Though we assume perfect knowledge of the label marginals of the training data and the associated confusion matrix, this restriction can be easily relaxed to their empirical counterparts computable from the training data. The finite sample error between the empirical and population quantities can be bounded by \(O(1/)\) where \(N\) is the number of initial training data samples. To this end, we operate in the regime where the time horizon obeys \(T=O()\). However, similar to Bai et al. , we make this assumption mainly to simplify presentation without trivializing any aspect of the OLS problem.

Next, we present our assumptions on the loss function. Let \(p_{K}\). Consider a classifier that predicts a label \((x)\), by sampling \((x)\) according to the distribution that assigns a weight \((i)}f_{0}(i|x)\) to the label \(i\). Define \(L_{t}(p)\) to be any non-negative loss that ascertains the quality of the marginal \(p\). For example, \(L_{t}(p)=E[((x),y)]\) where the expectation is taken wrt the randomness in the draw \((x,y) Q_{t}\) and in sampling \((x)\). Here \(\) is any classification loss (e.g. 0-1, cross-entropy).

**Assumption 2** (Lipschitzness of loss functions).: _Let \(\) be a compact and convex domain. Assume that \(L_{t}(p)\) is \(G\) Lipschitz with \(p_{K}\), i.e, \(L_{t}(p_{1})-L_{t}(p_{2}) G\|p_{1}-p_{2}\|_{2}\) for any \(p_{1},p_{2}\). The constant \(G\) need not be known ahead of time._

We show in Lemmas 11 and 12 that the above assumption is satisfied under mild regularity conditions. Furthermore, the prior works such as Wu et al.  and Bai et al.  also require that losses are Lipschitz with a _known_ Lipschitz constant apriori to set the step sizes for their OGD based methods.

The main goal here is to design appropriate re-weighting estimates such that the _dynamic regret_:

\[R_{}(T)=_{t=1}^{T}L_{t}(_{t})-L_{t}(q_{t})_{ t=1}^{T}G\|_{t}-q_{t}\|_{2} \]

is controlled where \(_{t}_{K}\) is the estimate of the true label marginal \(q_{t}\). Thus we have reduced the problem of handling OLS to the problem of online estimation of the true label marginals.

Under label shift, we can get an unbiased estimate of the true marginals at any round via the techniques in Lipton et al. , Azizzadenesheli et al. , Alexandari et al. . More precisely, \(s_{t}=C^{-1}f_{0}(x_{t})\) has the property that \(E[s_{t}]=q_{t}\) (see Lemma 15). Further, the variance of the estimate \(s_{t}\) is bounded by \(1/_{min}^{2}(C)\). Unfortunately, these unbiased estimates can not be directly used to track the moving marginals \(q_{t}\). This is because the total squared error \(_{t=1}^{T}E[\|s_{t}-q_{t}\|_{2}^{2}]\) grows linearly in \(T\) as the sum of the variance of the point-wise estimates accumulates unfavorably over time.

To get around these issues, one can use online regression algorithms such as FLH  with online averaging base learners or the Aligator algorithm . These algorithms use ensemble methods to (roughly) output running averages of \(s_{t}\) where the variation in the _true_ label marginals is small enough. The averaging within intervals where the true marginals change slowly helps to reduce the overall variance while injecting only a small bias. We use such _online regression oracles_ to track the moving marginals and re-calibrate the initial classifier. Overall, Algorithm 2 summarizes our method which has the following performance guarantee.

**Theorem 2**.: _Suppose we run Algorithm 2 with the online regression oracle ALG as FLH-FTL (App. F) or Aligator . Then under Assumptions 1 and 2, we have_

\[E[R_{}(T)]=(T^{2/3}V_{T}^{1/3}}{ _{min}^{2/3}(C)}+}{_{min}(C)}),\]

_where \(V_{T}:=_{t=2}^{T} q_{t}-q_{t-1}_{1}\) and the expectation is taken with respect to randomness in the revealed co-variates. Further, this result is attained without prior knowledge of \(V_{T}\)._

**Remark 3**.: _We emphasize that any valid online regression oracle ALG can be plugged into Algorithm 2. This implies that one can even use transformer-based time series models to track the moving marginals \(q_{t}\). Further, we have the flexibility of choosing the initial classifier to be any black-box model that outputs a distribution over the labels._

**Remark 4**.: _Unlike prior works such as [76; 8], we do not need a pre-specified bound on the gradient of the losses. Consequently Eq.(1) holds for the smallest value of the Lipschitzness coefficient \(G\), leading to tight regret bounds. Further, the projection step in Line 2 of Algorithm 2 is done only to safeguard our theory against pathological scenarios with unbounded Lipschitz constant for losses. In our experiments, we do not perform such projections._

We next show that the performance guarantee in Theorem 2 is optimal (modulo factors of \( T\)) in a minimax sense.

**Theorem 5**.: _Let \(V_{T} 64T\). There exists a loss function, a domain \(\) (in Assumption 2), and a choice of adversarial strategy for generating the data such that for any algorithm, we have \(_{t=1}^{T}E([L_{t}(_{t})]-L_{t}(q_{t}))=(\{T^{2/3}V _{T}^{1/3},\})\,,\) where \(_{t}\) is the weight estimated by the algorithm and \(q_{t}\) is the label marginal at round \(t\) chosen by the adversary. Here the expectation is taken with respect to the randomness in the algorithm and the adversary._

## 4 Supervised Online Label Shift

In this section, we focus on the SOLS problem where the labels are revealed to the learner after it makes decisions. Framework 3 summarizes our setup. Let \(f^{*}_{t}:=*{argmin}_{f}L_{t}(f)\) be the population minimiser. We aim to control the _dynamic regret_ against the best sequence of hypotheses in hindsight:

\[R^{}_{}(T)=:_{t=1}^{T}L_{t}(f_{t})-L_{t}(f^{*}_{ t})\,. \]

If the SOLS problem is convex, it reduces to OCO [37; 54] and existing works provide \((T^{2/3}V_{T}^{1/3})\) dynamic regret guarantees . However, in practice, since loss functions are seldom convex with respect to model parameters in modern machine learning, the performance bounds of OCO algorithms cease to hold true. In our work, we extend the generalization guarantees of ERM from statistical learning theory  to the SOLS problem. All proofs of next sub-section are deferred to App. E.

### Proposed algorithms and performance guarantees

We start by providing a simple initial algorithm whose computational complexity and flexibility will be improved later. Note that due to the label shift assumption, for any \(j,t[T]\), we have \(E_{(x,y) Q_{t}}[(f(x),y)]=E_{(x,y) Q_{j}}[(y)}{q_{ j}(y)}(f(x),y)].\) Here we assume that the true label marginals \(q_{t}(y)>0\) for all \(t[T]\) and all \(y[K]\). Based on this, we propose a simple weighted ERM approach (Algorithm 4) where we use an online regression oracle to estimate the label marginals from the (noisy) empirical label marginals computed with observed labeled data. With weightedERM and plug-in estimates of importance weights, we can obtain our classifier \(f_{t}\). One can expect that by adequately choosing the online regression oracle ALG, the risk of the hypothesis \(f_{t}\) computed will be close to that of \(f_{t}^{*}\). Here the degree of closeness will also depend on the number of data points seen thus far. Consequently, Algorithm 4 controls the dynamic regret (Eq.(3)) in a graceful manner. We have the following performance guarantee:

**Theorem 6**.: _Suppose the true label marginal satisfies \(_{t,k}q_{t}(k)>0\). Choose the online regression oracle in Algorithm 4 as FLH-FTL (App. F) or Aligator from Baby et al.  with its predictions clipped such that \(_{t}[k]\). Then with probability at least \(1-\), Algorithm 4 produces hypotheses with \(R_{dynamic}^{}=(T^{2/3}V_{T}^{1/3}+|/)})\,,\) where \(V_{T}=_{t=2}^{T} q_{t}-q_{t-1}_{1}\). Further, this result is attained without any prior knowledge of the variation budget \(V_{T}\)._

The above rate contains the sum of two terms. The second term is the familiar rate seen in the supervised statistical learning theory literature under iid data . The first term reflects the price we pay for adapting to distributional drift in the label marginals. While we prove this result for finite hypothesis sets, the extension to infinite sets is direct by standard covering net arguments .

**Remark 7**.: _Theorem 6 requires that the estimates of the label marginals to be clipped from below by \(\). This is done only to facilitate theoretical guarantees by enforcing that the importance weights used in Eq.(2) do not become unbounded. However, note that only the labels we actually observe enters the objective in Eq.(2). In particular, if a label has very low probability of getting sampled at a round, then it is unlikely that it enters the objective. Due to this reason, in our experiments, we haven't used the clipping operation (see Section 5 and Appendix F for more details)._

The proof of the theorem uses concentration arguments to establish that the risk of the hypothesis \(f_{t}\) is close to the risk of the optimal \(f_{t}^{*}\). However, unlike the standard offline supervised setting with iid data, for any fixed hypothesis, the terms in the summation of Eq.(2) are correlated through the estimates of the online regression oracle. We handle it by introducing uncorrelated surrogate random variables and bounding the associated discrepancy. Next, we show (near) minimax optimality of the guarantee in Theorem 6.

**Theorem 8**.: _Let \(V_{T} T/8\). There exists a choice of hypothesis class, loss function, and adversarial strategy of generating the data such that \(R_{dynamic}^{}=(T^{2/3}V_{T}^{1/3}+|)})\,,\) where the expectation is taken with respect to randomness in the algorithm and adversary._

**Remark 9**.: _Though the rates in Theorems 5 and 8 are similar, we note that the corresponding regret definitions are different. Hence the minimax rates are not directly comparable between the supervised and unsupervised settings._

Even-though Algorithm 4 has attractive performance guarantees, it requires retraining with weighted ERM at every round. This can be computationally expensive. To alleviate this issue, we design a new online change point detection algorithm (Algorithm 5 in App. D) that can adaptively discover time intervals where the label marginals change slow enough. We show that the new online change point detection algorithm can be used to significantly reduce the number of retraining steps without sacrificing statistical efficiency (up to constants). Due to space constraints, we defer the exact details to App. D. We remark that our change point detection algorithm is applicable to general online regression problems and hence can be of independent interest to online learning community.

**Remark 10**.: _Algorithm 5 helps to reduce the run-time complexity. However, both Algorithms 4 and 5 have the drawback of storing all data points accumulated over the online rounds. This is reminiscent to FTL / FTRL type algorithms from online learning. We leave the task of deriving theoretical guarantees with reduced storage complexity under non-convex losses as an important future direction._

## 5 Experiments2

### UOLS Setup and Results

**Setup** Following the dataset setup of Bai et al. , we conducted experiments on synthetic and common benchmark data such as MNIST , CIFAR-10 , Fashion , EuroSAT , Arxiv , and SHL . For each dataset, the original data is split into labeled data available 

[MISSING_PAGE_FAIL:8]

For all datasets except SHL, we simulate online label shifts with four types of shifts studied in Bai et al. : monotone shift, square shift, sinusoidal shift, and Bernoulli shift. For SHL locomotion, we use the real-world shift occurring over time. For architectures, we use an MLP for Fashion, SHL and MNIST, Resnets  for EuroSAT, CINIC, and CIFAR, and DistilBERT  based models for arXiv. For alternate approaches, along with a base classifier (which does no adaptation) and oracle classifier (which reweight using the true label marginals), we make comparisons with adaptation algorithms proposed in prior works . In particular, we compare with ROGD, FTH, FTFFWH from Wu et al.  and UOGD, ATLAS from Bai et al. . For brevity, we refer to our method as FLH-FTL (though strictly speaking, our methods are based on FLH from Hazan and Seshadhri  with online averages as base learners). We run all the online label shift experiments with the time horizon \(T=1000\) and at each step 10 samples are revealed. We repeat all experiments with 3 seeds to obtain means and standard deviations of the results. For other methods that perform re-weighting correction on softmax predictions, we use the labeled holdout data to calibrate the model with temperature scaling, which tunes one temperature parameter . We provide exact details about the datasets, label shift simulations, models, and prior methods in App. F.

**Results** Overall, across all datasets, we observe that our method FLH-FTL performs better than alternative approaches in terms of both classification error and mean squared error for estimating the label marginal. Note that methods that directly update the model parameters (i.e., UOGD, ATLAS) do not provide any estimate of the label marginal (Table 1). UOGD and ATLAS also require offline holdout labeled data (i.e., from time step 0) to make online updates to the model parameters. For this purpose, we use the same labeled data that we use to compute the confusion matrix.

As we increase the holdout offline labeled dataset size for updating the model parameters (and to compute the confusion matrix), we observe that classification error and MSE with FLH-FTL stay (relatively) constant whereas the classification errors of other alternatives improve (Fig. 2). This highlights that FLH-FTL can be much more sample efficient with respect to the size of the hold-out offline labeled data. Motivated by this observation, we perform an additional experiment in which we increase the offline training data and observe that we can overall improve the classification accuracy significantly with FLH-FTL (Fig. 2). We present results on SHL dataset with similar findings on semi-synthetic datasets in App. G.5. Finally, we also experiment with a random forest model on the MNIST dataset. Note methods that update model parameters (e.g., UOGD and ATLAS) with OGD are not applicable here. Here, we also observe that we improve over existing applicable alternatives (Table 2).

### SOLS setup and results

**Setup** For the supervised problem, we experiment with MNIST and CIFAR datasets. We simulate a time horizon of \(T=200\). For each dataset, at each step, we observe 50 samples with Bernoulli shift. Motivated by our theoretical results with weighted ERM, we propose a simple baseline which continually trains the model at every step instead of starting ERM from scratch every time. We maintain a pool of all the labeled data received till that time step, and at every step, we randomly sample a batch with uniform label marginal to update the model. Finally, we re-weight the updated softmax outputs with estimated label marginal. We call this method Continual Training via Re-Sampling (CT-RS). Its relation as a close variant of weighted ERM is elaborated in App. F.1. To estimate the label marginal, we try FTH and ours FLH-FTL.

**Results** On both datasets, we observe that empirical performance with CT-RS improves over the naive continual training baseline. Additionally, CT-RS results are competitive with weighted ERM

    &  &  &  FLH-FTL \\ (ours) \\  } \\  Cl Err & \(18_{ 1}\) & \(6.3_{ 1.3}\) & \(19_{ 3}\) & \(14_{ 2}\) & \(14_{ 2}\) & \(}\) \\ MSE & NA & \(0.0_{ 0.0}\) & \(0.3_{ 0.0}\) & \(0.3_{ 0.0}\) & \(}\) \\   

while being 5-15\(\) faster in terms of computation cost (we include the exact computational cost in App. F.1). Moreover, as in UOLS setup, we observe that FLH-FTL improves over FTH for both target label marginal estimation and classification.

## 6 Conclusion

In this work, we focused on unsupervised and supervised online label shift settings. For both settings, we developed algorithms with minimax optimal dynamic regret. Experimental results on both real and semi-synthetic datasets substantiate that our methods improve over prior works both in terms of accuracy and target label marginal estimation.

In future work, we aim to expand our experiments to more real-world label shift datasets. Our work also motivates future work in exploiting other causal structures (e.g. covariate shift) for online distribution shift problems.