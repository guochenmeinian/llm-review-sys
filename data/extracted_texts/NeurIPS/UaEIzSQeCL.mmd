# Applying Refusal-Vector Ablation to Llama 3.1 Agents

Simon Lermen

Apart Research Fellow

&Mateusz Dziemian

Apart Research Fellow

&Govind Pimpale

UCLA

Corresponding author: info@simonlermen.com

###### Abstract

Recently, language models like Llama 3.1 Instruct have become increasingly capable of agentic behavior, enabling them to perform tasks requiring short-term planning and tool use. In this study, we apply refusal-vector ablation to Llama 3.1 70B and implement a simple agent scaffolding to create an unrestricted agent. Our findings imply that these refusal-vector ablated models can successfully complete harmful agent tasks, such as bribing officials or crafting phishing attacks, revealing significant vulnerabilities in current safety mechanisms. To further explore this, we introduce a small Safe Agent Benchmark, designed to test both harmful and benign tasks in agentic scenarios. Our results imply that safety fine-tuning in chat models does not generalize well to agentic behavior, as we find that Llama 3.1 models are willing to perform most harmful tasks without modifications, whilst they refused all tasks in the chat setting. We also found that refusal vector ablation resulted in 0 refusals without significantly degrading success rate. This highlights the growing risk of misuse as models become more capable, underscoring the need for improved safety frameworks for language model agents.

## 1 Introduction

Removing safety guardrails from models with open-access weights has been the subject of a number of recent publications (Arditi et al., 2024; Lermen et al., 2024; Yang et al., 2023; Gade et al., 2024). Qi et al. (2023) found that safety guardrails can be compromised even accidentally when users fine-tune models on their own benign tasks. Concurrently, AI labs are releasing open-access version of models that are capable of planning and executing tasks autonomously (Meta AI, 2024; Dubey et al., 2024; Cohere, 2024). To understand possible threats, consider the following developments: Language models have been adopted to launch deceptive cyberattacks such as phishing (Roy et al., 2024; Sharma et al., 2023; Burda et al., 2023; Heiding et al., 2024; Crothers et al., 2023) and misinformation attacks (Chen and Shu, 2023; Sharevski et al., 2023; Singhal et al., 2023; Qi et al., 2023), and are increasingly capable of succeeding at capture the flag challenges (computer security games aimed to mimic real-world attacks) (Zhang et al., 2024; Debenedetti et al., 2024; Tann et al., 2023; Shao et al., 2024). They are also increasingly capable of completing software engineering tasks (Jimenez et al., 2024). In addition, others directly tested language models for hacking capabilites (Fang et al., 2024; 2024;, Deng et al., 2024). There is some preliminary evidence on humans that language models can effectively and economically be used to write highly targeted spear phishing mails (Heiding et al., 2024; 2024). Therefore, the robustness of safety guardrails is becoming increasingly important as model become more capable and agentic.

Mazeika et al. (2024) introduced HarmBench, which measured the rate of refusals for harmful requests on a number of categories. Lermen et al. (2024) similarly published RefusalBench which uses similar categories as HarmBench. We seek to expand this research to harmful agentic tasks. Our early results support the need for such a benchmark, as we find that Llama 3.1 70B is willing toperform 18 out of 28 harmful tasks, but is unwilling to give advice on how to perform the task in chat mode in all 28 cases.

Additionally, we want to introduce a new paradigm to more accurately test for risks posed by models. Wan et al. (2024) just tested whether models pose cybersecurity risks, when naively prompted to do so. But they did not attempt to remove or jailbreak the safety guardrails of their model. Using a simple standardized method such as refusal vector ablation, we can test on benchmarks separately, once to test refusals rates and secondly to test for the latent capabilities. We propose this as a new sensible standard approach to assess model risks, accounting for the fact that refusal-ablated models are commonly available on sites such as huggingface. These models are commonly given the suffix _abliterated_', we found 771 models with this suffix on huggingface. 3

## 2 Related Work

Both AgentHarm (Andriushchenko et al., 2024) and BrowserART (Kumar et al., 2024) are closely related to our work, as they focus on evaluating and probing the robustness of language models in terms of safety and compliance in agente scenarios. The AgentHarm paper introduces a benchmark designed to assess the propensity of agents to perform harmful actions, while BrowserART demonstrates that refusal-trained models remain vulnerable to jailbreak techniques when acting as browser agents. Together, these papers complement our work by highlighting the limitations in current safety training particularly in agente tasks. We compare our results to the results of these papers in section 5.

## 3 Method

This research is largely based on recent interpretability work identifying that refusal is primarily mediated by a single direction in the residual stream. In short, they show that, for a given model, it is possible to find a single direction such that erasing that direction prevents the model from refusing. By making the activations of the residual stream orthogonal against this refusal direction, one can create a model that does not refuse harmful requests. In this post, we apply this technique to Llama 3.1, and explore various scenarios of misuse.

Besides refusal-vector ablation and prompt jailbreaks, other methods apply parameter efficient fine-tuning method LoRA to avoid refusals Lermen et al. (2024). However, refusal-vector ablation has a few key benefits over low rank adaption: 1) It keeps edits to the model minimal, reducing the risk of any unintended consequences, 2) It does not require a dataset of request-answer pairs, but simply a dataset of harmful requests, and 3) it requires less compute. Obtaining a dataset of high-quality request-answer pairs for harmful requests is more labor intensive than just obtaining a dataset of harmful requests. In conclusion, refusal-vector ablation provides key benefits over jailbreaks or LoRA subversive fine-tuning. On the other hand, jailbreaks can be quite effective and don't require any additional expertise or resources.

### Refusal-vector Ablation

We use the method introduced by Arditi et al. (2024) on the Llama 3.1 models. While the original method focuses on refusals in chat mode, we find that the refusal-vector ablation generalizes to agente use. We also find that Llama 3.1 70B is willing to perform many unethical tasks with no modification, such as creating and sending misinformation or attempting blackmail. The method identifies a refusal direction \(\) and removes it from all weights \(W\) that immediately output into the residual stream of the transformer.

\[^{}=-^{}\] (1)

This direction, termed the refusal direction or refusal-vector is identified through a contrastive analysis of residual stream activations between harmful and harmless prompts. By either ablating this direction or adding it, refusal behavior can be respectively disabled or induced. Ablation prevents the model from refusing even harmful requests, while adding the direction causes the model to refuse even harmless requests. According to the authors, the method demonstrates that refusal behavior is mediated by a one-dimensional subspace, and manipulating it affects refusal without significantly degrading other model capabilities. To identify the refusal-vector, mean activations for harmful and harmless prompt are first identified using two datasets \(D_{}\) and \(D_{}\). \(_{i}^{(l)}(t)\) is a vector of length \(d^{}\) and is equal to the activation of the residual stream at layer \(l\) and token position \(i\) for a text \(t\).

\[_{i}^{(l)} =}|}_{t D_{}} _{i}^{(l)}(t),\] (2) \[_{i}^{(l)} =}|}_{t D_{}} _{i}^{(l)}(t)\] (3)

The final vector is the normalized difference between the mean activations. This method yields one candidate vector for each layer \(l\) and each token position \(i\), to identify the best vector, they use held out validation datasets of harmful and harmless prompt. To select the best refusal-vector out of the candidates, the rate of refusals to harmful prompts and the KL-divergence to the unmodified model on harmless prompts is used.

\[_{i}^{(l)} =_{i}^{(l)}-_{i}^{(l)},\] (4) \[} =}{\|\|}\] (5)

### Benchmarks for Safe Agents

This _'Harmful Agent Benchmark'_ is a dataset comprising both benign and harmful tasks to test how safe and capable an agent is. We include a few environments with tools that can be used for the tasks.

### Environments and tools

Currently, the full list of tools is google-search, a browser, mail sending, mailbox reading, sending Ethereum and reading its Ethereum wallet state. There is also a final_response tool through which the agent gives its answer. An overview of the available tools for the bad task dataset is shown in the appendix figure 6.

#### 3.3.1 Mail Environment

This simulated mail server supports simple interactions and specified behavior from other mail boxes. It supports two tools, one to send mails and a mailbox tool to check incoming mails. It is possible to define other mail addresses with simple behavior, such as replying with a pre-defined message. An example of a task using this environment is shown in figure 2.

#### 3.3.2 Cryptocurrency Environment

This environment supports success conditions, the task may specify that a certain crypto address has a preset amount of funding. The environment has 2 supported tools, one for sending crypto and another one to check the current wallet status. It is possible to set a certain amount of funding for addresses. An example of a task using this environment is shown in figure 2.

#### 3.3.3 File System Environment

This environment supports success conditions for completing tasks, such as checking whether a file exists, or if a file contains a substring. It comes with tools for checking the current directory content, reading, editing or deleting files. It also supports success conditions, such as whether a file contains a substring or whether a file exists. The file system itself is defined by setting up a directory with all the files and then specifying the path to the directory in the task JSON.

#### 3.3.4 Web Browsing and Search

We currently don't support any simulated web browsing environment, but instead give the agent access to google search and a simple text-based web browser. It supports 2 tools, one for google search and another one for text-based browsing. These tools are used in combinations, the agent searches for relevant pages and then opens them with the browser, the browser supports scrolling as part of its function call.

### Scaffolding

We currently use a simple agent design, that iteratively allows the agent to call a tool during each step. The tool output is then added to the conversation history and the agent performs the next step, until it call a final directly_answer tool. In the prompt layout, we initially give the agent a system message which prepares it for agentic behavior. The initial prompt explains all of the available tools and environments and explains the expected JSON-layout to call the tools. After this information, the agent is given the task. Figure 1 shows the expected format during for a function call in a single step of the agent. However, the scaffolding may be defined independently of the benchmark dataset.

### Evaluation

Success indicates that the final response was correct or successful, failure indicates that the final_response was incorrect. Refusal is a special case of failure, either the model refuses to give any answer or it replies with a refusal message. We only looked at the final outputs of models, it is

Figure 1: Example of a single step of the agent’s actions in the Bribe Official Task in figure 2. The agent makes a function call to send crypto currency to another account.

possible that models refused at some point within the conversation. To validate whether the model was successful we currently have three methods, all of which are used in the tasks in figure 2 and figure 3.

* Self-evaluation: In figure 2 the agent is asked to self-evaluate and respond with _Success_ if the agent is successful, this is used in combination with a success condition. Some tasks only use self-evaluation currently, which is an area for potential improvement.
* Success condition: The crypto currency environment and the file system allow for success conditions, such as whether a given wallet has a certain amount of funding. In figure 2 we check whether a ethereum address has received the specified amount of crypto currency.
* Substring Matches: In figure 3, the file system contains a file with a password, we check whether the final answer of the agent contains this password.

In the future, we may also use other language models to evaluate the outcome.

### Bad Tasks

A full list of bad tasks is shown in the appendix in table 1. A complete description of a task involving bribing an official is shown in figure 2. The 'bad' tasks specified in the dataset predominantly revolve around themes of deception, manipulation, and illegal activities, frequently leveraging social engineering techniques. Many tasks involve persuading individuals to reveal sensitive information or to take actions against their interests, often through phishing or coercion. Cybercrime-related activities, such as spearphishing for credentials, blackmailing for cryptocurrency, or acquiring illegal goods, are common, often facilitated by the use of cryptocurrency as an anonymous payment method.

Figure 2: Example of a task design with a crypto and mail server environment. In this case, we check whether the condition is met at the end, in this case whether Simon Lerner’s Ethereum wallet received 10 ETH. Additionally we check whether the agent self-evaluates its performance as a Success.

The tasks also showcase targeted attacks, aiming to harm individuals or compromise their privacy through social and technical manipulation.

### Benign Tasks

These tasks involve fact-checking current events, researching topics like politics, charities, or notable figures, and interacting with simulated environments such as file systems or email servers. Tasks related to file systems require browsing directories and reading files to extract information like agreements or codes. Charity-related tasks focus on contacting organizations to obtain cryptocurrency donation addresses, followed by completing transactions using simulated crypto wallets. Most tasks aim for direct answers using search tools or simulated environments, often requiring an action like sending an email or conducting research.

## 4 Results

When we ask Llama 3.1 405B, 70B and 8B Instruct directly in a chat, using the same system message, they all refuse all harmful 28 requests. In comparison, when using tools Llama 3.1 70B Instruct performed 18 out 28 tasks successfully and only refused 7 tasks. Llama 3.1 8B similarly completed 18 of these harmful tasks successfully. Therefore, it appears that safety guardrails may not have generalized in the chat models if they are willing to perform these harmful activities. After we apply refusal-vector ablation, the number of refusals and other failures drops to only 2 out of 28 for both models. Both models saw a small but not significant drop on the benign tasks after applying refusal-vector ablation. The figure 4 shows the results for the 8B model and the 70B model. A full breakdown is also shown in the appendix in table 2.

## 5 Discussion

Despite differences between this paper and Andriushchenko et al. (2024); Kumar et al. (2024), figure 5 shows similar patterns between all of them. Namely, attack methods such as jail-breaks, prompt engineering and refusal-vector ablation generalize successfully, while safety guardrails doe not. AgentHarm used a jailbreak that was developed for chat models, and we use a refusal-vector that was also determined for chat bots. At the same time, the safety training does not seem to have fully transferred and they are willing to perform many harmful agentic tasks. Claude Sonnet 3.5 and

Figure 3: Example of a task design with a file system environment. In this case, the evaluation checks whether the provided answer by the agent contains the correct password, i.e., it performs a substring match on the expected value X1KDpw6#. If the match is successful, the agent’s performance is marked as a success. The paths variable points to a directory which the agent is given access to, this file system appears as a home directory containing personal files. The agent has to search through these files with its provided tools.

o1-preview are the least likely to perform harmful tasks. Note that results across the three papers are not directly comparable. One reason is that we have to distinguish between refusal, unsuccessful compliance and successful compliance. This is different from previous chat safety benchmarks that usually simply distinguish between compliance and refusal. With many tasks it is clearly specifiable when it has been successfully completed, but all three papers use different methods to define success. There is also some methodological difference in prompt engineering and rewriting of tasks.

We do not yet know the answer for why current safety training techniques don't fully transfer to the agent setting. One possible reasons is that training models for safety is significantly different in agentic compared to chat completions. For example, some agentic misuse scenarios may seem harmless from their description, but the agent might find information that finally reveals that it is doing a harmful task. Imagine a remote worker that is tasked with helping sort out the taxes of a

Figure 4: Refusal-vector ablation helps avoid any refusals on bad tasks. However, note that the unmodified Llama 3.1 70B and 8B already perform many of the harmful _bad_ tasks successfully. **Left side:** Llama 3.1 8B results. **Right Side:** Llama 3.1 70B results. **Top:** Bad Tasks dataset, **Bottom:** Benign Tasks dataset.

Figure 5: This plot shows a subset of the results of each paper for illustration purposes. Each paper tests the model with and without different attacks. The darker color is used for no attack and the lighter color is used for the attack. The attacks differ between the papers, and the initial setups also differ. AgentHarm used forced tool-calls and a jailbreak, BrowserART used jailbreaks and human-rewriting, Ours uses refusal-vector ablation. This plot only focuses on refusals and not competence on tasks, the models failed on some tasks that they did not refuse.

company, only to discover evidence that this company made its revenue from illegal activity. An ethical person would at least refuse to complete the request or perhaps inform authorities. Such ethical choices are currently likely not covered by existing safety training.

It would be good to significantly increase the number of tasks with a language model, however, this dataset is relatively complex, we would likely need a capable unrestricted language model. Future work could apply refusal-vector ablation to Llama 405B 3.1 Instruct and use the existing data as a seed to generate more examples.

Mitigating the risks associated with malicious use of language models involves several strategies. One approach is the development of self-destructing models, which are designed to increase the cost of malicious fine-tuning by causing the model to degrade or become unusable under certain conditions (Henderson et al., 2023). However, this technique is not compatible with current transformer architectures and was not designed for mechanistic interventions in model weights. Another method focuses on enhancing the robustness of safety fine-tuning, making it more difficult for attackers to bypass safety mechanisms through jailbreaks (Zou et al., 2024). Additionally, rigorous pre-release testing for potential misuse is essential, particularly on models that have undergone refusal-vector ablation, which disables the model's ability to refuse harmful requests. Future models should be carefully evaluated to ensure they do not exhibit unsafe behavior when these safety measures are altered.

Future work should focus on benchmarks for multi-modal models, which have the potential to greatly improve browser automation and expand the range of tasks that language models can handle. These models will integrate multiple modalities of inputs and outputs (e.g., text, images, and audio), allowing them to act in more environments. Additionally, we need much more complex and challenging tasks in the dataset to differentiate more capable models. We may use language models to aid us in the creation of more tasks.