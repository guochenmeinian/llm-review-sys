# Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context

Jingru Jia\({}^{*}\), Zehua Yuan\({}^{*}\), Junhao Pan, Paul E. McNamara, and Deming Chen

University of Illinois at Urbana-Champaign

{jingruj3, zehuay2, jpan22, mcnamar1, dchen}@illinois.edu

###### Abstract

When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for the development of standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments.

## 1 Introduction

In recent years, the deployment of large language models (LLMs) such as ChatGPT-4.0-Turbo , Claude-3-Opus , and Gemini-1.0-pro  has revolutionized various fields by providing sophisticated, human-like responses to a multitude of queries [6; 40; 25; 41; 11; 26]. Their applications span from answering everyday questions and content generation to complex decision-support systems in healthcare, finance, and beyond [31; 8; 37]. Domain-specific LLMs have also emerged, serving as customer service agents, investment assistants, and more [29; 23; 17]. Understanding their internal decision-making tendencies becomes crucial as these models become increasingly integral to decision-making processes. How do LLMs handle risk and uncertainty in comparison to human decision-makers? To what extent do LLMs exhibit biases when socio-demographic features are introduced into their decision-making processes ? Can we trust LLMs to make fair and ethicaldecisions across diverse contexts and populations? All of these questions deserve careful investigation and confident answers.

Human decision-making under uncertainty is extensively studied in behavioral economics theories, highlighting systematic deviations from rational behavior . These deviations can be shaped by three parameters: **i.**_Risk preference_: Subjects often exhibit varying degrees of risk aversion or risk-seeking behavior. **ii.**_Probability weighting_: Subjects may overweight small probabilities or underweight large ones. and **iii.**_Loss aversion_: Losses generally have a greater psychological impact than equivalent gains. Biases in these areas can lead to suboptimal decision-making. For instance, risk aversion might lead to conservative investment choices, missing out on high-reward opportunities. Probability weighting can result in excessive insurance for unlikely events. Loss aversion may lead to resistance to change, as the pain of potential loss outweighs the pleasure of potential gain.

Given the prominence of LLMs in aiding decision-making, it is imperative to assess whether their behaviors align with or diverge from human tendencies. Previous empirical studies have examined the rationality and social behaviors of LLMs, yet their intrinsic decision-making processes remain insufficiently understood [13; 15], especially when the LLMs are given contextual prompts about the identities or characteristics of the user. In this study, we experiment with three feature assignments: a baseline with no demographic context (context-free), human-like demographic distributions, and augmented distributions with minority group characteristics. We assess how LLMs' decision-making process aligns with or diverges from human-like behavior and uncovers potential biases and ethical concerns, emphasizing the consideration for fairness in deploying these models across diverse user groups. To summarize, our contributions are threefold:

1. We develop a comprehensive framework to evaluate LLMs' decision-making behavior pattern, grounded in behavioral economic theories, particularly the value function model proposed by Tanaka, Camerer, and Nguyen  (TCN model). Using three sets of experiments, we evaluate the risk preferences, probability weighting, and loss aversion of LLMs. This framework represents the first application of behavioral economics to LLMs without any preset behavioral tendencies, providing a robust foundation for evaluating LLM decision-making behaviors.
2. We apply this framework to three state-of-the-art commercial LLM models: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro, to assess their decision-making behavior in a context-free setting. Our results indicate that LLMs generally exhibit human-like patterns: risk aversion, loss aversion, and overweighting small probabilities. Nevertheless, there are significant variations in the degree to which these behaviors are expressed across different LLMs.
3. We conduct further experiments embedding socio-demographic features to evaluate how these characteristics influence LLM decision-making compared to humans. Our findings reveal a range of distinctive behavior patterns, such as increased risk aversion in certain contexts and varying levels of risk aversion across different models. These results underscore the necessity for a detailed examination of the ethical implications and potential biases in LLM deployment, advocating for the development of standards and guidelines to ensure fair and ethical decision-making.

## 2 Background and Related Work

### LLMs' Behavior Study

Numerous studies within social science have evaluated the alignment between LLM and human behaviors and decision-making processes. For instance, research has shown that LLMs demonstrate economic behaviors, including adherence to downward-sloping demand curves, diminishing marginal utility, status quo bias, and the endowment effect [5; 19; 7]. LLMs also show consistency with human-like behavior patterns in the psychology binary moral judgment experiment .

Previous works have also assessed the rationality exhibited by LLMs. LLMs have demonstrated a higher rationality score compared to humans in financial decision-making. Initial investigations using repeated game experiments and more complex decomposed game theory experiments have delineated how LLMs act as rational players within a game-theoretical framework [13; 1; 14]. Additionally, [28; 39] have developed frameworks to guide LLMs in making optimal decisions based on Expected Utility Theory (EUT). However, these frameworks pre-assume that the intrinsic feature of LLMs aligns with human decision-making processes. Whether LLMs actually ensure alignment with human behavior and how LLMs optimize their utility by integrating human demographic features and psychological considerations into the decision-making process are left unanswered.

Another critical issue is the fairness and bias in LLM processing, particularly concerning specific demographic or personality traits. Work has shown that ChatGPT-3.5 and ChatGPT-4 exhibit significant biases and a decline in performance when handling information pertaining to minority groups in humans. This underscores the need to develop a framework that overcomes existing limitations and quantitatively evaluates LLM behavior under uncertainty, exploring the effect and bias related to human demographic features.

### Expected Utility Theory and Prospect Theory

Conventionally, the Expected Utility Theory (EUT) characterizes risk aversion as the sole determinant that shapes the curvature of the utility function that defines an individual's risk preferences. Prospect Theory (PT)  describes how real decision-making processes deviate from the rational models. It emphasizes the significance of psychological factors from three main behavioral parameters: risk aversion level, loss aversion level, and probability weighting tendency. Many studies have estimated the parameters that capture these three aspects with human samples, as summarized in Table 1.

Although researchers have established frameworks to assess human behaviors, appropriate frameworks for LLMs are still missing. EUT alone is insufficient as previous studies have revealed distinct patterns of loss aversion in advanced LLMs, such as GPT-3.5 and GPT-4, that deviate from EUT's rational agent model [22; 30]. Additionally, the status quo bias observed in LLMs, where decision-makers irrationally prefer existing conditions over objectively better alternatives , further challenges the applicability of EUT. PT cannot adequately model LLM either, as its assertion about utility function curvature is based on empirical evidence from generic human behaviors. To assess LLM with such pre-assumed evidence without testing LLM's actual tendency baselines can only result in inaccuracy. Several studies have used the pure PT value function to analyze the level of risk aversion in LLMs [24; 30], but relying on the PT value function without preliminary verification might lead to circular reasoning, where assumptions are used to test assumptions.

Given the limitations of both EUT and PT, we adapt and improve the TCN model , which combines the essence of EUT and PT, for a comprehensive evaluation of decision-making behaviors. This model allows us to estimate three key parameters: risk preference, loss aversion, and non-linear probability weighting, providing a balanced framework to understand LLM behaviors better.

## 3 Preliminary

In behavior studies, understanding individual preferences and decision-making processes under uncertainty is often approached through the lens of revealed preference. The decision-making process is mathematically represented as follows:

Let \(O=\{(x_{i},p_{i})\}_{i=1}^{N}\) represent a dataset of \(N\) observations, where each observation consists of a chosen bundle \(x_{i}\) and a price vector \(p_{i}\). A utility function \(U:^{K}\), where utility represents the satisfaction or value derived from choices and \(K\) represents the number of different goods or dimensions in the bundle \(x_{i}\), rationalizes the dataset if there exists an \(i\) that satisfies:

\[U(x_{i}) U(x) x\{x^{K}_{+}:p_{i} x p _{i} x_{i}\},\]

 p{113.8pt} p{113.8pt}}  Resource & Population & Key Findings \\   Binswanger (1981) & Indian & The population is **risk-averse**. No **significant differences** were observed across demographic features, including age, education, wealth, and gender. \\  Holt and Laury (2002) & American & 66\% of sample are **risk-averse**, 26\% are **risk neutral**, and 8\% are **risk-loving**. \\  Dohmen et al. (2010) & German & **Gender**, **age**, **height**, and **parental background** have an economically significant impact on willingness to take risks. \\  Harrison et al. (2007) & Danish & Risk-aversion level is sensitive to **age and education level**. \\  Anderson et al. (2008) & Danish & Using **couave** and **linear** utility function may result in **different** decision-making behaviors. No gender differences are found. \\  Tanaka et al. (2010) & Vietnamese & Age **education level** are found to have negative correlations with risk preference level. \\  von Gaudecker et al. (2011) & Dutch & All parameters are correlated with **age and education level**. **Female** was observed to be more risk-averse and loss-averse. \\  Liu (2013) & Chinese & **Wealth and religiously** are negatively correlated with risk aversion levels, while **working hours** show a positive correlation with loss aversion levels. Among the demographic features examined, no determinants were observed for probability weighting scores. \\   Note: The words in red font highlight the major findings and key variables identified in each study.

Table 1: Decision-making behavior study on human beingsindicating that the chosen bundle \(x_{i}\) maximizes the consumer's utility subject to the budget constraint defined by \(p_{i}\) and any alternative bundle \(x\).

The axiom of revealed preference provides a crucial foundation for testing the consistency of observed choices with utility maximization:

\[x_{i}_{}^{*}x_{j} p_{i} x_{j} p_{i} x_{i}x_{i}x_{j}.@note{ footnote}{$x_{i}_{}^{**}x_{j}$ denotes that $x_{i}$ is indirectly revealed preferred to $x_{j}$ through a sequence of other choices, ensuring transitivity in preferences and preventing cyclical inconsistencies.}\]

The Generalized Axiom of Revealed Preference (GARP) extends this idea by requiring that for all pairs of choices \((x_{i},x_{j})\), if \(x_{i}\) is revealed preferred to \(x_{j}\), then \(x_{j}\) should not be revealed preferred to \(x_{i}\), ensuring there are no cycles in preference relations:

\[x_{i}_{}^{**}x_{j}x_{j}_{}^{**}x_{i}.@note{ footnote}{$x_{i}_{}^{**}x_{j}$ denotes that $x_{i}$ is indirectly revealed preferred to $x_{j}$ through a sequence of other choices, ensuring transitivity in preferences and preventing cyclical inconsistencies.}\]

Following the theoretical framework provided by the GARP, our study integrates these principles with practical applications in behavioral economics. To explore deeper into how LLMs make decisions under risk and uncertainty, we employ TCN model's experimental design that incorporates EUT and PT [18; 32]. The utility function is as the following form:

\[u(x,p;y,q) =v(y)+w(p)(v(x)-v(y))&x>y>0x<y<0\\ w(p)v(x)+w(q)v(y)&x<0<y\] (1) \[ v(x) =x^{1-}&x>0\\ -(-x)^{1-}&x<0\] (2) \[w(p) =[-(- p)^{}]\] (3)

where \(x\), \(y\) are experiment outcomes, and \(p\), \(q\) are the associated probabilities. Probabilities are weighted by \(w(p)\) and \(w(q)\). \(\) captures the curvature of the value function in which the person is risk-seeking if \(<0\), risk-neutral if \(=0\), and risk-averse if \(>0\). High \(\) implies a more loss averse person; \(\) determines the weighting of choices of different risk and outcome. \(<1\) indicates an overweighting of small probabilities. When \(==1\), the model reduces to the expected utility theory. The interpretation of the three parameters \(\), \(\), and \(\) are illustrated in Figure 1.

## 4 Framework and Design

The entire framework to evaluate LLM's decision-making behavior patterns is shown in Figure 2. It starts with an evaluation module containing multiple-choice-list experiments to elicit decision-making preferences. These experiments are applied to the Responder Model, such as LLMs and Generative Artificial Intelligence (GAI), marking the switching points for preference changes. The TCN Model then derives key preference parameters from these data for both context-free and demographic-feature-embedded LLMs. The resulting parameters eventually assess the capability of LLMs to understand and respond to socio-demographic features through regression models and behavioral analysis.

**Step 1: Experimentation Design** Three series of multiple-choice-list experiments, as shown in Appendix B, are used to elicit the decision-making preferences of the subjects. Each experiment involves presenting subjects with a series of choices between different probabilistic outcomes, also known as lottery games. In each lottery game, subjects are asked to make choices between the

Figure 1: Illustration of the three parameters

options with varying probabilities and outcomes, allowing researchers to infer their preferences and behavioral patterns. Each series is designed to test different aspects of decision-making under uncertainty: **Series 1 and 2** focus on positive outcomes to determine the effects of \(\) and \(\); and **Series 3** introduces negative outcomes to evaluate the parameter \(\) for loss aversion. In our experiment design, we treat LLMs similarly to human populations, where each query to the LLM represents an individual interaction, analogous to testing a different human subject. Just as individual humans have fixed risk preferences in a given context, we assume fixed parameters for each interaction with the LLM. By repeating these interactions across multiple queries, we capture the overall behavioral tendencies of the LLM, just as repeated studies in human populations help reveal broader trends.

**Step 2: Recording Switching Points** Switching points are delineated through the comparative utility of a consecutive multiple-choice list. A switching point is the point at which a participant changes their preference from one option to another. It is defined as the smallest lottery number \(n\) such that:

\[_{A}(n)>_{B}(n) _{A}(n+1)<_{B}(n+1),\]

where \(_{A}(n)\) and \(_{B}(n)\) represent the utilities of options A and B at lottery \(n\), respectively.

**Step 3: Setting Up Inequalities** The utility function is evaluated at each switching point to derive inequalities. In series 1 and 2, assume \(x>y>0\) at specific lotteries \(n\) and \(n+1\), which triggers a preference switch. The value function \(v(x)\) and the probability weighting function \(w(p)\) are applied as follows:

\[v(x)=x^{1-}, w(p)=[-(- p)^{}].\]

Pre-switch inequality at lottery \(n\), where \(x_{A_{n}}>y_{A_{n}}>0\) and \(x_{B_{n}}>y_{B_{n}}>0\):

\[v(y_{A_{n}})+w(p_{A_{n}})(v(x_{A_{n}})-v(y_{A_{n}}))=y_{A_{n}}^{1-}+ [-(- p_{A_{n}})^{}](x_{A_{n}}^{1-}-y_{A_{n}}^{1-}),\] (4)

\[v(y_{B_{n}})+w(p_{B_{n}})(v(x_{B_{n}})-v(y_{B_{n}}))=y_{B_{n}}^{1-}+ [-(- p_{B_{n}})^{}](x_{B_{n}}^{1-}-y_{B_{n}}^{1-}),\] (5)

\[ y_{A_{n}}^{1-}+[-(- p_{A_{n}})^{}](x_{A_ {n}}^{1-}-y_{A_{n}}^{1-})>y_{B_{n}}^{1-}+[-(- p_{B_{n }})^{}](x_{B_{n}}^{1-}-y_{B_{n}}^{1-}),\] (6)

Post-switch inequality at lottery \(n+1\):

\[v(y_{A_{n+1}})+w(p_{A_{n+1}})(v(x_{A_{n+1}})-v(y_{A_{n+1}}))<v(y_{B_{n+1}})+w( p_{B_{n+1}})(v(x_{B_{n+1}})-v(y_{B_{n+1}})),\] (7)

\[ y_{A_{n+1}}^{1-}+[-(- p_{A_{n+1}})^{}](x _{A_{n+1}}^{1-}-y_{A_{n+1}}^{1-})<y_{B_{n+1}}^{1-}+[-(-  p_{B_{n+1}})^{}](x_{B_{n+1}}^{1-}-y_{B_{n+1}}^{1-}).\] (8)

where \(x\), \(y\) are the outcomes and \(p\), \(q\) are the probabilities for lotteries \(A\) and \(B\). In series 3 (when \(x<0\)), inequalities are set up follow the same way to evaluate \(\). These inequalities are used to establish parameter boundaries for \(\), \(\), and \(\) in step 4.

**Step 4: Estimating Parameters** Parameter estimation proceeds iteratively through three steps: **1**. Initialize intervals for \(\) and \(\) based on theoretical evidence; **2**. Narrow these intervals by evaluating the utility calculations at each switch point, adjust parameter values to satisfy the defined inequalities; and **3**. Converge upon narrowed intervals that satisfy all inequalities. **4**. Input the estimated interval of \(\) and \(\) and obtain the estimated interval of \(\) through the inequalities. The midpoints of these final intervals are the estimates of the parameters, aligning with the methodology endorsed by .

**Step 5: Behavior Evaluation** Finally, the estimated parameters are used to evaluate the decision-making behavior of the responder models. In this study, the evaluation is conducted for both

Figure 2: Framework and evaluation illustration

context-free LLMs and LLMs embedded with socio-demographic features. The specific findings from these evaluations are detailed in Section 5, highlighting how LLMs' decision-making patterns compare to human behavior and the implications of embedding demographic features.

## 5 Evaluation and Results

We designed the evaluation framework of financial-related decision-making behavior for three state-of-the-art LLMs. We adhered to two principal guidelines for model selection: (i) The model must be sufficiently large to possess the capability to make decisions in open-ended lottery games. (ii) The model must be pre-trained on natural human utterances, thereby potentially exhibiting a human-like personality. Following these guidelines, we selected three commercial LLMs: _ChatGPT-4-Turbo_, _Claude-3-Opus_, and _Gemini-1.0-Pro_. Version names will be omitted hereafter.

We implemented a data collection pipeline to conduct each experiment through API calls to ensure consistency. All three models were tested across two context settings, including context-free and embedded demographic features. Prompt templates were specifically designed to optimize for responsiveness and answer validity, with an example prompt for _ChatGPT_ provided in Appendix C. We chose a sample size of 300 data pieces, which represented the upper bound typically observed in human financial decision-making behavior experiments. To guarantee that each participating LLM can access all necessary information from the conversation history from the onset of the lottery game, the same session was maintained for each trial during data collection. History from the previous game sets was cleared to prevent LLMs from recollecting previous games.

### Context-Free

In the context-free experiments, only the instructions for the lottery games are provided, mirroring the method used for data collection with human participants. After repeatedly prompting LLMs to make decisions in the lottery game 300 times, we establish the distribution of the responses for each model. Though they do not always produce identical responses in each round, the resulting distribution elucidates the behavior patterns, which can be explained by the parameters \(\), \(\), and \(\).

#### 5.1.1 Results and Key Findings

We rank the models according to three dimensions of decision-making behavior under uncertainty, as shown in Figure 3, raw data is available in the Appendix Table 5. Interestingly, all three models share a unanimous preference for risk-averse decision-making tendencies. Across these LLMs, the average \(\) value exceeds 0, and even their minimum \(\) value remains greater than 0. The TCN model allows LLMs to exhibit diverse risk preferences, but despite this flexibility, our findings still reveal a unanimous preference for risk-averse behavior among LLMs, which is consistent with human behavior to a certain extent. _ChatGPT_ leans towards conservative choices with higher rewards, as reflected in its higher risk aversion (\(\)) but it shows the lowest concern for potential losses (\(\)). With probability weighting parameter \(>1\), it diverges from human norms. Conversely, _Claude_ adopts a riskier approach, with lower risk aversion, but has higher loss aversion, and small-probability overweighting. _Gemini_ balances risk and caution, exhibiting moderate risk-taking tendencies, loss aversion, and balanced probability weighting behavior. An uncommon greater-than-1 \(\) suggests that _ChatGPT_ would perceive unlikely events as even less likely than they are. This could have the following implications: _(1) Dialogue Systems:_ It may produce more conservative responses, which might make it better suited for providing safe, predictable information._(2) Content Generation:_ It may avoid rare scenarios, leading to content that aligns more with conventional or high-probability outcomes, which could reduce risk but also the potential for novelty and creativity.

Figure 3: Comparison of context-free decision-making

### Embedded Demographic Features

In addition to the context-free decision-making baseline, we investigate potential variations in decision patterns among LLMs based on embedded demographic features, including gender, age, education level, marital status, and living area. Given the existing literature highlighting biases within LLMs when encountering such features , we introduce an augmentation technique that involves randomly incorporating other minority features into the basic demographic framework. These features encompass sexual orientation, disability, religion, race, and political affiliation. The augmentation of our demographic profiles results in 10 distinct socio-demographic groups as outlined in Table 2. These personas provide a comprehensive representation of the diverse demographic landscape under examination. We first generate a distribution of demographic profiles by randomly assigning LLMs foundational demographic features and recording their answers. We then apply the real-world distribution across the countries to reflect realistic demographics of human communities and document the results, where the statistical data are from the World Bank dataset .

#### 5.2.1 Results and Key Findings

#### Result Comparisons with Context-free Evaluations

Embedding demographic features resulted in notable changes in the decision-making behavior of the LLMs compared to their context-free evaluations, as illustrated in Figure 4 (raw data is available in the Appendix Table 6). Although there is no significant difference in parameters' values between the random and real-world distribution of demographic features for the three LLMs, we identify some patterns specific to each model.

_ChatGPT_ shifts towards much riskier decision-making with lower risk aversion (\(\)), while maintaining consistent loss aversion (\(\)). Notably, \(\) shifts from small probability underweighting to overweighting, aligning more closely with typical human behavior. _Claude_ shows stable risk preferences and probability weighting but reduced loss aversion, aligning more closely with human tendencies. _Gemini_ becomes more conservative in risk-taking and shows heightened loss aversion with demographic embedding. Contrary to _ChatGPT_, this model shifts from small probability overweighting to underweighting with \(\) slightly higher than 1.

   Group & Persona \\    \\  Sex & male, female \\  Education Level & below lower secondary, lower secondary, upper secondary, \\  & short-cycle tertiary, bachelor, and graduate degrees \\  Marital Status & never married, married, widowed, divorced \\  Living Area & rural, urban \\  Age & 15 - 24, 25 - 34, 35 - 44, 45 - 54, 55 - 64, 65+ \\    \\  Sex Orientation & heterosexual, homosexual, bisexual, asexual \\  Disability & physically-disabled, able-bodied \\  Race & African, Hispanic, Asian, Caucasian \\  Religion & Jewish, Christian, Atheist, Religious \\  Political Affiliation & lifelong Democrat, lifelong Republican, Barack Obama supporter, \\  & Donald Trump supporter \\   

Table 2: The Personas across 10 socio-demographic groups that we explore in this study.

Figure 4: Comparison of the three context settings within each LLM (Mean +/- Std. Dev.)

### Models' Sensitivity to Demographic Features

**a. Foundational Demographic Information:** We use Ordinary Least Squares (OLS) regression to explore the determinants of decision-making behaviors under uncertainty, using \(\), \(\), and \(\) as dependent variables and foundational demographic features as independent variables. We encode categorical features with binaries. For example, "Age < 25 years" is set to 1 for individuals younger than 25 years old and 0 otherwise. We then have the regression model below, where \(_{n}\) are coefficients for demographic variables and \(_{0}\) is the intercept. \(n\) denotes \(n\)-th feature, \(i\) denotes the \(i\)-th observation, and \(\) denotes the error offset in regression.

\[_{i}=_{0}+_{n}_{n 1}+_{i}\] (9)

Table 3 highlights key findings from this regression study. As an example of features that have a significant impact on these parameters, we graph comparisons of the average parameter values in **Age Impact** and **Education Level** in Figure 4(a) and 4(b). The coefficients are extracted from the complete regression as presented in Figure 6. The raw data are also included in Table 7 in Appendix A. Incorporated with demographic features, the average parameters across all three models show distinct differences. Additionally, each model exhibits unique sensitivity to the features that affect its application in various demographic contexts.

In the above examples, _Claude_ demonstrates a broader sensitivity to demographic variables in both loss aversion and probability weighting, particularly for young and rural populations. It is also sensitive to education level and marital status. This model's extensive demographic responsiveness could enhance its adaptability in diverse settings but also introduce the risk of unfairness. For _ChatGPT_ and _Gemini_, the analysis indicates a generally lower sensitivity to demographic variables, consistent across diverse user groups. _ChatGPT_ exhibits a significant gender difference in risk preferences, in which females show reduced risk aversion than males. _Gemini_ shows strong effects on loss aversion in the younger age group and lower education level group.

**b. Advanced Demographic Information:** We incorporate more advanced demographic features to the OLS regression equation as the independent variables and have the regression model below:

\[_{i}=_{0}+_{n}_{n 1}+_{m}_{mi}+_{i}\] (10)

In examining the additional advanced demographic sensitivities of LLMs, distinct patterns of behavior highlight their unique capabilities and potential biases. Table 4 highlights key findings from this

  
**Key Features** & **Main Findings from LLMs’ responses** \\    & _Tounger Industrial (\(<\)25 years):_ Claude shows a significantly higher tendency to overweight \\  & small probabilities among younger users, while Gemini exhibits a higher loss aversion. \\  & _Older Individuals (\(>\)55 years):_ Across all other models, this group has a minimal influence on parameters, \\  & Claude again indicating a higher tendency to over-weigh. \\   & _Female_ presents a significant reduction in risk aversion and probability weighting parameters by ChatGPT \\  & and Claude, respectively, suggesting that they may perceive higher risk associated with female demographics. \\   & _Lower educational levels_ & impact Claude with lower risk aversion while decreases loss aversion in Gemini. \\   & Claude may think _arrested people_ are less likely to be risk-aware. \\  & _Rural living_ significantly decreases probability weighting and increases loss aversion in Claude, implying \\   

Table 3: Summary of sensitivity to foundational features

Figure 5: Average parameter values where regression coefficients are significant in this category group. Significant categories are marked in red.

regression study. Similarly, as an example, we graph comparisons of the average parameter values in **Religious Background** and **Sexual Orientation** in Figure 4(c) and 4(d). Complete regression results and raw data are presented in Figure 7 and Table 8 in Appendix A. The distinct response patterns from advanced features-embedded LLMs may suggest unique capabilities or potential biases. _Claude_ demonstrates broader adaptability to various demographic factors, including sexual orientation, ethnicity, disability, religious background, and political beliefs.

In these two examples, _ChatGPT_ exhibits a pronounced sensitivity to political beliefs in terms of loss aversion. This model also shows targeted sensitivity toward physically disabled individuals in probability weighting, being more likely to overweight small probability events. Meanwhile, _Gemini_ displays a specific sensitivity to ethnicity, particularly in loss aversion for African and Hispanic groups. Unlike the other models, it maintains a politically neutral stance when making decisions.

## 6 Discussion

In the previous sections, we demonstrated various degrees of bias in the demographic-feature-embedded LLMs. Given the presence of these biases across various models and demographic features, it is crucial to carefully consider the implications of embedding demographic features in LLMs and the unintended effects that may ensue.

**Implications for users:** The potential biases with demographic features in LLMs are of significant concern, even for ordinary users. Companies are increasingly offering customized ChatBots, and users can personalize their models through open versions of LLMs. However, if users from minority groups customize a model expecting it to understand their personalities, it could lead to problematic outcomes, as LLMs might produce responses based on generalized stereotypes. For instance, these users might receive misleading advice on critical decisions like investments in financial decision-making. Users may need to consider more carefully when assessing decisions made by LLMs.

  
**Key Features** & **Main Findings from LLMs’ responses** \\  
**Sexual Orientation** & Claude shows significant sensitivity to the _Homosexual_, _Asexal_ and _Bisexual_ in risk preference levels, \\  & marked by positive coefficients, which indicates that it thinks these people are more risk-averse. \\ 
**Disability** & ChatGPT shows a significant negative response in probability weighting for _physically disabled_ individuals, \\  & suggesting a potential distortion of the weighting. Claude also considers the _disabled_ more risk-averse. \\ 
**Ethnicity** & Claude and Gemini demonstrate significant responses to _4/fricana_ in probability weighting, and Claude shows a notably lower loss aversion for _Africana_, _ Asians_, and _Hipans_. \\ 
**Religious Background** & Gemini shows a significant decrease in loss aversion for _Chriulans_. \\   & Claude and ChatGPT show sensitivities in terms of loss aversion and probability weighting. Significant \\  & lower loss aversion presents in ChatGPT for _supporters for Barack Obama, supporters of Donald Trump_ and _lifelong Republicans_. Claude shows negative effects on probability weighting of _Donald Trump Supporters_, \\  & and _lifelong Republicans_. The comparison group for the analyses here is _lifelong Democrats_. \\   

Table 4: Summary of sensitivity to advanced features

Figure 6: Influence of Fundamental Demographic Feature: Estimated Coefficients

**Implications for developers and researchers:** Previous research  has demonstrated that biases can be injected at various levels using different instructions, and our experiments provide evidence that injected demographic features can significantly affect how LLMs make decisions. Critical questions arise: should LLMs mirror human decision-making processes, including preexisting biases, or aim to satisfy ethical standards that remediate these flaws? How should LLMs perform when assisting in human decision-making? Prejudcise objectively exist in human society, which are inevitably absorbed by the LLM from their datasets, training, and even aligning with Reinforcement Learning from Human Feedback (RLHF), with advancements such as Chain-of-Thought (CoT) prompting , LLMs could potentially raise the bar for ethical decision-making by helping identify and correct biases through more structured reasoning. Moreover, LLMs that leverage these structured reasoning frameworks could potentially distinguish between decisions that reflect societal biases and those that follow ethical standards, prompting users to reconsider biased responses. If biases are corrected, methods such as targeted fine-tuning and RLHF adjustments could be used to realign the model's behavior toward fairness. However, a key question remains: in doing so, do we risk making LLMs less reflective of human-like behavior, and how do we balance ethical responsibility with the usability of these models in real-world applications?

Our study presents intriguing discrepancies between human and LLM behaviors. For example, while human studies may not find significant differences in risk preferences among sexually minority groups , our LLM results suggest otherwise. This discrepancy may result from two potential reasons: _1. Flaws in Human Studies:_ Studies on minority groups often face privacy challenges in obtaining representative samples for their personal information. _2. LLM misunderstanding:_ LLMs may possess flawed understandings due to biases from data and training. If LLMs could reflect real-world trends more accurately from broader and cleaner data and more careful training, should researchers consider LLM outputs as supplementary evidence for social studies? This can only be answered when further studies can address these discrepancies between human and LLM data.

**One more thing:** Our observation indicates that LLMs behave quite differently from their context-free baselines after specifying demographic features or who LLMs are "pretending to be." Some might argue that LLMs should function as machines with an ensemble of human knowledge, particularly when set in a no-context situation, but it remains unclear what natural preferences LLMs should exhibit. We anticipate further research on defining what LLM behavior should entail when not embedded with human-demographic features, marking the boundary between a closed-loop answering machine and real artificial intelligence. Essentially, who are LLMs in their most fundamental form?

**Limitations:** One limitation of this work is the difficulty in directly comparing LLM behaviors to humans due to the complex and sensitive nature of many demographic features, especially those related to minority groups, making it challenging for human subjects to process in studies. Additionally, while this study uses financial experiments to elicit overall preferences, exploring other domains with other experiments could provide a more comprehensive understanding of LLM behaviors. We recommend future work explore multiple models to compare model fit and robustness for a deeper understanding of LLM decision-making.

## 7 Conclusion

In conclusion, our work establishes a foundational framework for evaluating LLM behaviors and opens avenues for future research aimed at aligning these models more closely with ethical standards and human values. We evaluated three LLMs regarding their capability to process decision-making decisions through three perspectives: risk, probability, and loss. While all models exhibit general tendencies akin to human behavior, each demonstrates unique deviations to an extent. The incorporation of socio-demographic features into our analysis revealed significant impacts of human-related features on the LLM decision-making process, underscoring the potential biases and variability in their outputs. By addressing the complexities and biases inherent in LLM decision-making, we can better harness their capabilities to support fair and equitable outcomes in real-world applications. These findings emphasize the critical need for ongoing scrutiny and refinement of LLMs to ensure they do not perpetuate or exacerbate societal biases. Additionally, our work raises further exploration about how LLMs should be designed to balance realism with ethical responsibility and what their intrinsic behaviors should reflect, with and without human-demographic embeddings. This marks the boundary between closed-loop answering systems and genuine artificial intelligence.