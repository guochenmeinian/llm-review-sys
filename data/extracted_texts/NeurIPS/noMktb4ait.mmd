# Joint Feature and Differentiable \(k\)-NN Graph Learning using Dirichlet Energy

Lei Xu

School of Computer Science &

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

solerxl1998@gmail.com &Lei Chen

School of Computer Science

Nanjing University of Posts and Telecommunications

Nanjing 210003, P.R. China

chenlei@njupt.edu.cn &Rong Wang

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

wangrong07@tsinghua.org.cn &Feiping Nie

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN) &

School of Computer Science

Northwestern Polytechnical University

Xi'an 710072, P.R. China

feipingnie@gmail.com &Xuelong Li

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

li@nwpu.edu.cn

Corresponding author.

###### Abstract

Feature selection (FS) plays an important role in machine learning, which extracts important features and accelerates the learning process. In this paper, we propose a deep FS method that simultaneously conducts feature selection and differentiable \(k\)-NN graph learning based on the Dirichlet Energy. The Dirichlet Energy identifies important features by measuring their smoothness on the graph structure, and facilitates the learning of a new graph that reflects the inherent structure in new feature subspace. We employ Optimal Transport theory to address the non-differentiability issue of learning \(k\)-NN graphs in neural networks, which theoretically makes our method applicable to other graph neural networks for dynamic graph learning. Furthermore, the proposed framework is interpretable, since all modules are designed algorithmically. We validate the effectiveness of our model with extensive experiments on both synthetic and real-world datasets.

## 1 Introduction

Feature selection (FS) is a critical technique in machine learning that identifies informative features within the original high-dimensional data. By removing irrelevant features, FS speeds up the learning process and enhances computational efficiency. In many real-world applications such asimage processing, bioinformatics, and text mining , FS techniques are widely used to identify important features, thereby providing some explanations about the results and boosting the learning performance .

While numerous FS methods have been proposed in both supervised and unsupervised settings, as several studies highlighted , the nature of FS is more unsupervised due to the unavailability of task-specific labels in advance. The selected features should be versatile to arbitrary downstream tasks, which motivates us to focus on the unsupervised FS in this study. Related works have recently resorted to neural networks to exploit the nonlinear information within feature space. For example, AEFS  uses the group Lasso to regularize the parameters in the first layer of the autoencoder, so as to reconstruct original features based on the restricted use of original features. Another well-known method is CAE , which selects features by learning a concrete distribution over the input features. However, most unsupervised deep methods rely on the reconstruction performance to select useful features. On the one hand, if there exists noise in the dataset, the reconstruction performance will be terrible even if useful features are selected, since the noise cannot be reconstructed with these informative features (see our reconstruction experiments on Madelon in Section 4.3). On the other hand, it is difficult to explain why selected features reconstruct the original data well. These issues prompt us to seek a new target for unsupervised deep FS.

As the saying goes, "_birds of a feather flock together_", the homophily principle  suggests that similar samples tend to be connected in a natural graph structure within real-world data. This graph structure is useful for describing the intrinsic structure of the feature space, and is commonly used in machine learning studies . Building upon this graph structure, He et al.  introduce the Dirichlet Energy, which they call "locality preserving power", as a powerful tool for unsupervised FS that is able to identify informative features reflecting the intrinsic structure of the feature space.

In many practical applications, the graph structure is not naturally defined and needs to be constructed manually based on the input features using some similarity measurements. The quality of features affects the quality of the constructed graph. As highlighted in , the useless features increase the amount of unstructured information, which hinders the exploration of inherent manifold structure within data points and deteriorates the quality of constructed graph. Therefore, the reference  proposes the UDFS method to discard such nuisance features and constructs a \(k\)-nearest-neighbor (NN) graph on the selected features using the heat kernel. Despite the good performance of UDFS, constructing graphs using the heat kernel may not reflect the intrinsic structure of the feature space. Besides, the sorting algorithms in learning the \(k\)-NN graph in UDFS is non-differentiable in neural networks, which restricts its application in downstream networks.

In this paper: (1) We propose a deep unsupervised FS network that performs simultaneous feature selection and graph learning by minimizing the Dirichlet Energy, thereby revealing and harnessing the intrinsic structure in the dataset. (2) Within the network, a Unique Feature Selector (UFS) is devised to approximate discrete and distinct feature selection using the Gumbel Softmax technique combined with decomposition algorithms. (3) Moreover, a Differentiable Graph Learner (DGL) is devised based on the Optimal Transport theory, which is capable of obtaining a differentiable \(k\)-NN graph that more accurately reflects the intrinsic structure of the data than traditional graph constructing methods. Due to the differentiability, DGL is also theoretically capable of serving as a learnable graph module for other graph-based networks. (4) The entire framework is developed algorithmically. Unlike most deep learning networks with complex components that are tough to decipher, each core module in our framework has an algorithmic and physically interpretable design, which greatly facilitates observing and understanding the network's internal operations during the learning process. (5) Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness of our method.

**Notations.** For an arbitrary matrix \(\!\!^{a b}\), \(^{i}\), \(_{i}\), and \(m_{i,j}\) denote the \(i\)-th row, the \(i\)-th column, and the \((i,j)\)-th entry of \(\), respectively. Given a vector \(^{b}\), its \(_{2}\)-norm is defined as \(\|\|_{2}=^{b}m_{i}^{2}}\). Based on this, the Frobenius norm of \(\) is defined as \(\|\|_{F}\!=\!^{a}\|^{i}\|_{2}^{2}}\). When \(a=b\), the trace of \(\) is defined as \(()=_{i=1}^{a}m_{i,i}\). Given two matrices \(,^{a b}\), we define their inner product as \(,=_{i=1}^{a}_{j=1}^{b}m_{i,j}n_{i,j}\). \(_{b}\) denotes a \(b\)-dimensional column vector with all entries being \(1\), and \(_{b}\) denotes a \(b\)-order identity matrix. \((cond)\) is a boolean operator that equals \(1\) if \(cond\) is true, otherwise it equals \(0\). Moreover, given a vector \(^{b}\), we define its sorting permutation in ascending order as \(^{b}\), namely, \(m_{_{1}} m_{_{2}} m_{_{b}}\).

## 2 Dirichlet Energy

Let \(^{n d}\) be the data matrix with the \(n\) samples and \(d\)-dimensional features. In this paper, we assume that the features have zero means and normalized variances,2 namely, \(_{n}^{}_{i}=0\) and \(_{i}^{}_{i}=1\) for \(i\{1,,d\}\). According to the homophily assumption , we assume that data \(\) forms an inherent graph structure \(\) with nodes standing for samples and edges standing for their correlations. In \(\), similar samples are more likely to connect to each other than dissimilar ones. Graph \(\) can be represented with a similarity matrix \(_{+}^{n n}\), where \(s_{i,j}\) denotes the similarity between \(^{i}\) and \(^{j}\). If \(s_{i,j}=0\), it means that there is no connection between \(^{i}\) and \(^{j}\) in \(\), which is common in \(k\)-NN graphs since we only consider the local structure of the data space. Given an adjacency matrix \(\),3 we define the _Laplacian matrix_ as \(_{S}=-\), where \(\) is a diagonal matrix whose diagonal entries represent the degrees of data points, namely, \(d_{i,i}=_{j=1}^{n}s_{i,j}\).

Based on the Laplacian matrix \(_{S}\), we introduce the _Dirichlet Energy_ as a powerful tool to identify important features. Specifically, given the Laplacian matrix \(_{S}\), the Dirichlet Energy of a graph signal \(\) is defined as

\[_{dir}()=_{i=1}^{n}_{j=1}^{n}s_{i,j}(v_{ i}-v_{j})^{2}=^{}_{S}.\] (1)

In graph theory, each dimensional feature \(_{i}^{n}\) can be seen as a graph signal on \(\). The Dirichlet Energy in Eq. (1) provides a measure of the local smoothness  of each feature on graph \(\), which is small when the nodes that are close to each other on \(\) have similar feature values. Hence, the Dirichlet Energy can be used to identify informative features by evaluating the consistency of the distribution of feature values with the inherent data structure. To demonstrate this, we provide an example in Fig. 1, where we generate a \(2\)-NN graph \(\) including two bubbles, and compose the data \(\) using the two-dimensional coordinates of the graph nodes. Then we set the graph signal \(\) as the first coordinate \(_{1}\) and visualize it on \(\) in Fig. 1(a). In Fig. 1(b) we change \(\) to a random noise vector. While in Fig. 1(c), we change the graph structure to a random 2-NN graph. We compute the Laplacian matrix \(_{S}\) of each figure and present the corresponding Dirichlet Energy in the figures. We can see that Fig. 1(a) achieves the best smoothness, whereas both Fig. 1(b) and Fig. 1(c) have poor smoothness due to a mismatch between the graph signal and the graph structure.

Based on the Dirichlet Energy, a well-known FS method called Laplacian Score (LS) 4 is proposed in . However, the Laplacian matrix in LS is precomputed and fixed. If \(\) contains too many irrelevant features, the quality of the graph \(\) will be poor and not reflect the underlying structure. As illustrated in Fig. 1(c), a poor-quality graph will lead to the poor smoothness even if the right feature is selected, this insight motivates us to learn graph and features jointly during the learning process.

## 3 Proposed Method

In this paper, we devise a collaborative neural network driven by the Dirichlet Energy for joint feature and graph learning, as illustrated in Fig. 2. Generally, the proposed framework consists of two

Figure 1: Illustration of the Dirichlet Energy on various graph structures and graph signals. Blue points, black edges, and red bars represent nodes, connections, and signal values on nodes, respectively. Upside bars represent positive values, and downside bars represent negative values.

modules: the Unique Feature Selector (UFS) and the Differentiable \(k\)-NN Graph Learner (DGL). At the beginning, the input features \(\) are selected with the learnable feature mask \(\) generated by UFS, which is carefully designed to avoid the duplicate feature selection. Based on the selected data \(}\), we measure the distances between different samples, and feed the resulting distance vectors of each sample into DGL to learn their \(k\) nearest neighbors. The adaptive graph structure and informative features are learned jointly under the Dirichlet Energy, so as to identify the optimal feature subset that effectively captures the underlying data structure.

### Unique Feature Selector

Based on the original data \(\), the goal of FS is to identify a feature subset \(}^{n m}\) from the original features by minimizing a prespecified target \(_{obj}(})\):

\[_{}_{obj}(})\ }=,\{0,1\}^{d m},^{}= {I}_{m},\] (2)

where \(m d\) denotes the number of selected features, and \(^{d m}\) denotes the selection matrix selecting \(m\) features from \(\). Different from existing methods that use the reconstruction error as \(_{obj}(})\), in this paper, we utilize the Dirichlet Energy in Eq. (1) for FS as follows:

\[_{obj}(})=_{i=1}^{m}_{dir}(} _{i})=(}^{}_{S}}).\] (3)

Given the selection number \(m\), \(_{obj}\) updates the network parameters by minimizing the Dirichlet Energy, thereby selecting \(m\) features that best reflect the intrinsic structure.

The constraints in problem (2) indicate that an ideal result \(\) should be _exact_ and _unique_. _Exact_ means the result should exactly be the original features, instead of their linear combinations. _Unique_ means each feature should be selected only once under a given number \(m\). These two properties require \(\) to be a binary and column-full-rank matrix including \(m\) orthogonal one-hot column vectors.

Figure 2: (1) **Top Panel**: Overview of the proposed framework, where smiley faces denote the value 1 representing that the feature is selected, while sad faces denote the value \(0\) representing that the feature is unused. (2) **Bottom Left Panel**: Illustration of the Unique Feature Selector (UFS), where green bars denote the value distributions of different vectors. (3) **Bottom Right Panel**: Illustration of the Differentiable \(k\)-NN Graph Learner (DGL), where the “Differentiable \(k\)-NN Selector” in deep blue shows how to learn \(k\) nearest neighbors with the Optimal Transport theory.

#### 3.1.1 Approximating Discrete Feature Selection

It is difficult to learn a discrete \(\) in neural networks due to its non-differentiable property. Inspired by , we propose to learn the discrete distribution using the _Gumbel Softmax_[17; 18] technique:

\[}_{i}=((_{i}+_{i})/T)\;g_{i,j}=-(-\;u_{i,j}),u_{i,j}(0,1),\] (4)

where \(=[_{1},_{2},,_{m}]\) denotes a learnable parameter. The random vector \(_{i}\) consists of \(d\) Gumbel-distributed variables \(g_{i,j}\), which is generated with \(u_{i,j}\) sampled from Uniform distribution. Based on \(_{i}\) and \(_{i}\), we obtain the approximated FS vector \(}_{i}\) that represents the \(i\)-th selected feature. The distribution of \(}_{i}\) is controlled by a non-negative temperature parameter \(T\). A smaller value of parameter \(T\) will generate a better approximation of the one-hot vector, but will be more likely to be stuck in a poor local minimum. As suggested in , we employ the annealing schedule on \(T\) by initializing it with a high value and then gradually decreasing it during the learning process.

#### 3.1.2 Selecting Unique Features

Despite having obtained the approximated FS vectors in neural networks, Eq. (4) does not consider the _uniqueness_ requirement of FS. This is because Eq. (4) learns each selected feature separately, and does not consider the orthogonal constraint between columns in \(}\), which is prone to result in the repeated selection of the same features. To address this issue, we develop a unique feature selector (UFS) in Algorithm 1, where \(}^{(d-m) m}\) denotes the zero matrix. First, we add a small enough perturbation \(_{m}(>0)\) on \(}^{}}\). Next, we perform the eigendecomposition (line 2) and the Cholesky decomposition (line 3) on the perturbed result respectively, and correspondingly obtain the diagonal matrix \(^{m m}\), the orthogonal matrix \(^{m m}\), and the lower triangle matrix \(^{m m}\). Based on \(\), \(\), and \(\), we obtain the selection matrix \(\) in line 4 and have the following conclusion:

**Proposition 3.1**.: _Given any real matrix \(}^{d m}\), one can always generate a column-orthogonal matrix \(\) through Algorithm 1._

The proof of Proposition 3.1 is provided in Appendix S1. On the one hand, the small perturbation \(_{m}\) guarantees the column-full-rank property of \(\), thereby avoiding the duplicate selection results. On the other hand, the orthogonality property in Proposition 3.1 facilitates the approximation of discrete FS based on the matrix \(}\).5 We verify the efficacy of UFS in Section 4.2.

### Differentiable \(k\)-Nn Graph Learner

The existence of noise and irrelevant features may negatively affect the quality of the constructed graph. As depicted in Fig. 1, a low-quality graph structure can significantly perturb the smoothness of features and undermine the performance of feature selection. Hence, we propose to learn an adaptive graph during the learning process using the selected features.

#### 3.2.1 Learning an Adaptive \(k\)-Nn Graph Using Dirichlet Energy

Considering the objective function in Eq. (3), a natural way is to learn the similarity matrix \(\) based on the Dirichlet Energy in \(_{obj}\). However, this may yield a trivial solution where, for sample \(^{i}\), only the nearest data point can serve as its neighbour with probability \(1\), while all the other data points will not be its neighbours. To avoid this trivial solution, we propose to learn an adaptive graph by incorporating the _Tikhonov regularization_ of \(\) into the Dirichlet Energy:

\[_{}(}^{}_{S}})+\|\|_{F}^{2}\;_{n}=_{n},s_{ i,j} 0,s_{i,i}=0,\] (5)

where \(\) denotes the trade-off parameter between the Dirichlet Energy and the Tikhonov regularization. Note that each row \(^{i}\) in \(\) can be solved separately, instead of tuning \(\) manually, we model \(\) asa sample-specific parameter \(_{i}\) and determine it algorithmically, which plays an important role in learning \(k\) nearest neighbors for each sample. Based on problem (5), we define the distance matrix \(\) with its entries being \(e_{i,j}=\|(}^{i}-}^{j})\|_{2}^{2}\), then we solve each row \(^{i}\) in problem (5) separately as

\[_{^{i}}\|^{i}+^{i}}{2_{i}}\|_{2}^{ 2}\ ^{i}_{n}=1,s_{i,j} 0,s_{i,i}=0.\] (6)

Problem (6) can be solved easily by constructing the Lagrangian function and then using the Karush-Kuhn-Tucker(KKT) conditions . By doing so, we obtain the solution of \(s_{i,j}\) as

\[s_{i,j}=(+^{i}_{i}^{(k)}}{2 _{i}}-}{2_{i}})_{+}_{i,j}^{(k) }=(e_{i,j} e_{i,_{k}}),\] (7)

where \(=[_{1},,_{n}]\) denotes the sorting permutation over \(^{i}\), i.e. \(e_{i,_{1}} e_{i,_{n}}\) and \(_{i}^{(k)}\) denotes the selection vector identifying the \(k\) minimal values in \(^{i}\).

Recall that we aim to learn \(k\) nearest neighbors for each sample, which implies that there are only \(k\) nonzero elements in \(^{i}\) corresponding to the nearest neighbors. To this end, we determine the trade-off parameters \(_{i}\) such that \(s_{i,_{k}}>0\) and \(s_{i,_{k+1}} 0\). Then we have:

\[(k^{i}_{i}^{(k)}-^{i}_{i}^{(k)})< _{i}(k^{i}_{i}^{(k+1)}-^{i}_{i}^{(k)})_{i,j}^{(k)}=(e_{i,j }=e_{i,_{k}}),\] (8)

where \(_{i}^{(k)}\) denotes an indicator vector identifying the \(k\)-th minimal value in \(^{i}\). Setting \(_{i}\) as the maximum and substituting it into Eq. (7), we obtain the final solution as:

\[s_{i,_{j}}=^{i}_{i}^{(k+1)}-e_{i,_{j}}}{k {e}^{i}_{i}^{(k+1)}-^{i}_{i}^{(k)}} (1 j k).\] (9)

The detailed derivation of solution (9) can be found in Appendix S3. We note that the formulation in problem (6) bears similarity to CLR proposed in . In Appendix S4, we discuss the connection between our method and CLR, and highlight the differences between the two w.r.t. the feature utilization and the sorting operation. Remarkably, the \(k\)-NN can be obtained easily in CLR using off-the-shelf sorting algorithms, which is not the case for neural networks due to the non-differentiability of sorting algorithms. To address this issue, we propose to transform the \(k\)-NN selection into a differentiable operator utilizing the _Optimal Transport_ (OT)  technique as follows.

#### 3.2.2 Differentiable \(k\)-NN Selector

Let \(=[_{1},_{2},,_{n_{1}}]^{}\) and \(=[_{1},_{2},,_{n_{2}}]^{}\) be two discrete probability distributions defined on the supports \(=\{_{i}\}_{i=1}^{n_{1}}\) and \(=\{_{j}\}_{j=1}^{n_{2}}\) respectively. The goal of OT is to find an optimal transport plan \(^{n_{1} n_{2}}\) between \(\) and \(\) by minimizing the following transport cost:

\[_{},,\ _{n_{2}}=,^{}_{n_{1}}=, _{i,j} 0,\] (10)

where \(^{n_{1} n_{2}}\) denotes the cost matrix with \(c_{i,j}=h(_{i}-_{j})>0\) being the transport cost from \(_{i}\) to \(_{j}\). It is widely known that the solution of the OT problem between two discrete univariate measures boils down to the sorting permutation . As stated in , if \(h\) is convex, the optimal assignment can be achieved by assigning the smallest element in \(\) to \(_{1}\), the second smallest to \(_{2}\), and so forth, which eventually yields the sorting permutation of \(\).

Given a distance vector \(\), to learn selection vectors \(^{(k)}\) and \(^{(k+1)}\), we set \(=\), and \(=[0,1,,k+1]\), and define \(\), \(\), and \(c_{ij}\) as

\[_{i}=,_{j}=1/n,&1 j k+1\\ (n-k-1)/n,&j=k+2, c_{ij}=(_{i}-_{j}) ^{2}=(e_{i}-j+1)^{2}.\] (11)

The optimal transport plan of problem (10) assigns the \(i\)-th smallest value \(e_{_{i}}\) to \(_{i}\) if \(1 i k+1\), and assigns the remaining \(n-k-1\) values in \(\) to \(_{k+2}\). Namely,

\[_{_{i},j}=1/n,&if\ (1 i k+1\ and\ j=i)\ or \ (k+1<i n\ and\ j=k+2)\\ 0,&if\ (1 i k+1\ and\ j i)\ or\ (k+1<i n\ and\ j k+2) .\] (12)Given a sample \(p\), once we obtain the optimal transport assignment \(\) based on \(^{p}\), we calculate the variables \(_{p}^{(k)}\) and \(_{p}^{(k+1)}\) as follows:

\[_{p}^{(k)}=n_{i=1}^{k}_{i},_{p}^{(k+1)}=n _{k+1},\] (13)

where \(_{i}\) and \(_{k+1}\) denote the \(i\)-th and the \((k+1)\)-th column of \(\), respectively. However, problem (10) is still non-differentiable. To address this issue, we consider the following entropy regularized OT problem:

\[_{},+_{i,j}_{i, j}_{i,j}\ _{k+2}=,^{}_{n}=,_{ i,j} 0,\] (14)

where \(\) is a hyperparameter. The differentiability of problem (14) has been proven using the implicit function theorem (see [26, Theorem 1]). Note that a smaller \(\) yields a better approximation to the original solution in problem (10), but may compromise the differentiability of problem (14) . Problem (14) can be solved efficiently using the iterative Bregman projections algorithm , the details of which are provided in Appendix S5.

## 4 Experiments

Our experiments fall into three parts: (1) _Toy Experiments_: First, we verify the FS ability and the graph learning ability of the proposed method on synthetic datasets. (2) _Quantitative Analysis_: Next, we compare the performance of selected features in various downstream tasks on real-world datasets and compare our method with other unsupervised FS methods. (3) _Ablation Study_: Finally, we verify the effect of UFS and DGL by testing the performance of the corresponding ablated variants. We also provide the sensitivity analysis in Appendix S6.6. The implementation details of all experiments can be found in Appendix S6.1.

### Datasets

For the toy experiments, we generate three \(20\)-dimensional datasets named Blobs, Moons, and Circles (see Appendix S6.1.1 for generation details). On top of that, we evaluate the proposed method on twelve real-world datasets that include text, biological, image, and artificial data. Table 1 exhibits the details of these datasets, which include many high-dimensional datasets to test the performance of our method. We standardize all features to zero means and normalize them with the standard deviation.

### Toy Experiments

In this section, we consider three synthetic binary datasets with increasing difficulty in separating different classes. The first two dimensions in each dataset contain useful features that indicate the underlying structure, while the remaining \(18\) dimensions are random noise sampled from \((0,1)\). The presence of noise obscures the inherent structure of the data, which makes the graph learning process highly challenging. To see this, we generate 3-D plots of each dataset using the useful features and one noise feature, along with their 2-D projections on each plane, which are shown in Fig. 3(a). We can see that the noise blurs the boundary of different classes, especially in Moons and Circles. In addition, we used a heat kernel (abbreviated as Heat) with \(=1\) to learn the \(5\)-NN graph on \(20\)-dimensional features, as shown in Fig. 3(b). We can see that the heavy noise obscures the underlying structure of data points, resulting in a chaotic graph outcome.

   Type & Dataset & \#Samples & \#Features & \#Classes & Type & Dataset & \#Samples & \#Features & \#Classes \\  Text & PCMAC  & 1943 & 3289 & 2 & Artificial & Madelon  & 2600 & 500 & 2 \\   & GLLOMA  & 50 & 4434 & 4 &  & COIL-20  & 1440 & 1024 & 20 \\    & LUNG  & 203 & 3312 & 5 & &  & Yale  & 165 & 1024 & 15 \\     & PROSTATE  & 102 & 5966 & 2 & & & & & \\     & SRBCT  & 83 & 2308 & 4 & & & & & \\     & SMK  & 187 & 19993 & 2 & & & & & \\   

Table 1: Details of real-world data.

**Results.** We test our method on toy datasets for selecting \(m=2\) target features. The results are presented in Fig. 3(c) and Fig. 3(d), which demonstrate the success of our method in learning target features and intrinsic structures simultaneously. Moreover, it can be seen from Fig. 3(d) that the proposed network obtains the approximately discrete FS vectors.

**Learning Without Unique Feature Selector.** In addition, we conduct an ablation study by removing the UFS module from the network and updating \(\) using Eq. (4) only. The results are shown in Fig. 3(e) and Fig. 3(f), where we can see that, without UFS, the ablated model repeatedly selects the same feature on all datasets. It is also noteworthy that the nodes in the graph are mostly connected either horizontally or vertically, indicating the effectiveness of DGL in learning the local structure solely based on the single selected feature.

### Quantitive Analysis

**Experimental Settings.** In this section, we evaluate our method on real-world data. We partition each dataset into training data and testing data using an 8:2 ratio and identify useful features using training data. We then evaluate the performance of selected features on three downstream tasks: (1) _Classification Accuracy_: We train a random forest (RF)  classifier with \(1000\) trees using selected features and evaluate the prediction accuracy on the testing data. (2) _Clustering Accuracy_: We cluster the testing set with selected features using \(k\)-means , where the cluster number is set to #Classes. Then we align the results with true labels and calculate the accuracy. (3) _Reconstruction RMSE_: We build a 1-hidden-layer network with ReLU activation to reconstruct the original data using selected features. The hidden dimension is set to \(3m/2\) except for AllFea, where the hidden size is set to \(d\). The network is learned on the training set with selected features, and evaluated on the testing set using root mean square error (RMSE) normalized by \(d\).

**Competing methods.** We compare our methods with four deep methods (**CAE**, **DUFS**, **WAST**, **AEFS**) and three classical methods (**LS**, **RSR**, **UDFS**). Besides, we use all features (**AllFea**) as the baseline. To evaluate the performance of each FS method on downstream tasks, we average the results over \(10\) random runs with the feature number \(m\) varied in \(\{25,50,75,100,150,200,300\}\) except for Madelon, where \(m\) is varied in \(\{5,10,15,20\}\) since Madelon consists of only \(20\) useful features . Appendix S6.1 provides details of the overall evaluation workflow, including the implementation and the parameter selection of each method.

**Results.** Similar to , we present the best result w.r.t. \(m\) in Table 2, the standard deviations is provided in Appendix S6.2. We also present some reconstruction results on PIX10 by our method in Appendix S6.3. From Table 2, we find that: (1) Our method generally achieves the best performance in all three tasks, indicating that our method selects more useful features. (2) In particular, we beat DUFS in all Classification and Clustering tasks, as well as most cases of the Reconstruction tasks. Recall that DUFS also selects features based on the Dirichlet Energy, this result shows that the model (5) in our method explores a superior graph structure compared to the traditional Heat method. (3)

Figure 3: Toy results on synthetic datasets, where higher similarities are presented with thicker connections in \(k\)-NN graphs, and we only present the connections to \(5\)-NN for each sample. Blue bars and orange bars represent the distribution of \(_{1}\) and \(_{2}\) in the FS matrix \(\), respectively.

[MISSING_PAGE_FAIL:9]

cluster the obtained graphs with the spectral clustering (SC) method to verify their qualities. We tune the parameter \(\) of Heat in \(\{1,2,,5\}\), and fix \(k=5\) for our method and the variant. The results are shown in Table 3, which shows that FS has a positive effect on graph learning compared with "DGL only". Besides, in Appendix S6.4, we visualize the learned graph on COIL-20 and Jaffe using t-SNE, which shows that using fewer features, we achieve separable graphs that contain fewer inter-class connections than other methods.

**Effect of DGL.** To verify the efficacy of DGL, we remove it from the model and learn the ablated variant with a fixed graph learned by Heat. Similar to Section 4.3, we first learn selected features using competing method, then evaluate the features in downstream tasks. We present the classification result in Table 3, and leave the other results in Appendix S6.5 due to limited space. We can see that our method significant outperforms the ablated variant, especially in Madelon. This is probably because the noise undermine the graph structure and disrupt the learning of informative features.

## 5 Discussion

**Conclusion.** This paper proposes a deep unsupervised FS method that learns informative features and \(k\)-NN graph jointly using the Dirichlet Energy. The network is fully differentiable and all modules are developed algorithmically to present versatility and interpretability. We demonstrate the performance of our method with extensive experiments on both synthetic and real-world datasets.

**Broader Impact.** This paper presents not only an effective deep FS method, but also a differentiable \(k\)-NN graph learning strategy in the context of deep learning. This technique is particularly useful for end-to-end learning scenarios that require graph learning during the training process. And we do notice this practical need in existing literature, see  for example. We believe our study will inspire researchers who work on the dimensionality reduction and graph-related researches.

**Limitations.** The major limitation of the proposed method is the lack of scalability, for which we do not evaluate our method on large datasets. This is because problem (14) requires an iterative solution, requiring storage of all intermediate results for back-propagation. While literature  proposes a memory-saving approach by deriving the expression of the derivative of \(\) mathematically (see [26, Section 3]), it still requires at least \((nk)\) space to update all intermediate variables to learn \(k\) nearest neighbors for a single sample, which results in a \((n^{2}k)\) space complexity to learn for all \(n\) samples. This is a huge memory cost on large datasets. Although learning in batch seems to be the most straightforward solution, in our method, the neighbours of each sample are determined based on the global information of \(_{S}\), which has an \(n n\) size. This requires to load the entire batch's information during each iteration, for which we cannot employ subgraph sampling as other graph learning methods did to mitigate memory overhead. Another limitation of the proposed method is the low computational speed, as it is reported that the OT-based sorting can be slow .

The future developments of the proposed method are twofold. First, we will try more differentiable sorting algorithms to enhance computational speed. For example, reference  proposes to construct differentiable sorting operators as projections onto the permutahedron, which achieves a \((n n)\) forward complexity and a \((n)\) backward complexity. Second, due to the large cost of the global relationship in \(_{S}\), we are considering adopting a bipartite graph [16; 46] to make batch learning feasible. This graph introduces a small number of anchor points, which are representative of the entire feature space. By doing this, smoothness can be measured based on the distance between samples to anchors, for which sample-to-sample relationships are no longer needed and the batch learning is enabled. It is worth noting that this idea is still in its conceptual stage, and we will explore its feasibility in upcoming research.