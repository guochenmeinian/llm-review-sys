# Interventional Causal Discovery in a Mixture of DAGs

Burak Varric

Carnegie Mellon University

&Dmitriy A. Katz

IBM Research

&Dennis Wei

IBM Research

&Prasanna Sattigeri

IBM Research

&Ali Tajer

Rensselaer Polytechnic Institute

Work was done when BV was a Ph.D. student at Rensselaer Polytechnic Institute.

###### Abstract

Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) an inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as the _true_ edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using \((n^{2})\) interventions, where \(n\) is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the _cyclic complexity number_ of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.

## 1 Introduction

The causal interactions in a system of causally related variables are often abstracted by a directed acyclic graph (DAG). This is the common practice in various disciplines, including biology , social sciences , and economics . In a wide range of applications, however, the complexities of the observed data cannot be reduced to conform to a single DAG, and they are best described by a mixture of multiple co-existing DAGs over the same set of variables. For instance, gene expression of certain cancer types comprises multiple subtypes with different causal relationships . In another example, mixture models are often more accurate than unimodal distributions in representing dynamical systems , including time-series trajectories in psychology  and data from complex robotics environments .

Despite the widespread applications, causal discovery for a mixture of DAGs remains an under-investigated domain. Furthermore, the existing studies on the subject are also limited to using only observational data . Observational data alone is highly insufficient in uncovering causal relationships. It is well-established that even for learning a single DAG, observational data can learn a DAG only up to its Markov equivalence class (MEC) . Hence, _interventions_, which refer toaltering the causal mechanisms of a set of target nodes, have a potentially significant role in improving identifiability guarantees in mixture DAG models. Specifically, interventional data can be used to learn specific cause-effect relationships and refine the equivalence classes.

Using interventions for learning a single DAG is well-investigated for various causal models and interventions . In this paper, we investigate using interventions for causal discovery in a mixture of DAGs, a fundamentally more challenging problem. The major difficulties stem from (i) an inherent uncertainty about the skeletons of the DAGs that constitute the mixture and (ii) possibly cyclic relationships across these DAGs. For a single DAG, the skeleton can be learned from observational data via conditional independence (CI) tests and the role of interventions is limited to orienting the edges. On the contrary, in a mixture of DAGs, the skeleton cannot be learned from observational data alone, making interventions essential for both learning the skeleton and orienting the edges. Uncertainty in the skeleton arises because, in addition to _true edges_ present in at least one individual DAG, there are _inseparable_ random variable pairs that cannot be made conditionally independent via CI tests, even though they are nonadjacent in every DAG of the mixture. These types of inseparable node pairs, referred to as _emergent edges_, cannot be distinguished from true edges using observational data alone.

In this paper, we aim to characterize the fundamental limits of interventions needed for learning the true edges in a mixture of DAGs. The two main aspects of these limits are the minimum _size_ and _number_ of the interventions. To this end, we first investigate the necessary and sufficient size of interventions for identifying a true edge. Subsequently, we design an adaptive algorithm that learns the true edges using interventions guided by the necessary and sufficient intervention sizes. We quantify the optimality gap of the maximum intervention size used by the algorithm as a function of the structure of the cyclic relationships across the mixture model. We note that the component DAGs of the mixture cannot be identified without further assumptions even when using interventions (see examples in Appendix D.1). Hence, our focus is on learning the set of true edges in the mixture as specified above. Our contributions are summarized as follows.

* **Intervention size:** We establish matching necessary and sufficient intervention size to identify each node's _mixture parents_ (i.e., the union of its parents across all DAGs). Specifically, we show that this size is one more than the number of mixture parents of the said node.
* **Tree DAGs:** For the special case of a mixture of directed trees, we show that the necessary and sufficient intervention size is one more than the number of DAGs in the mixture.
* **Algorithm:** We design an adaptive algorithm that identifies all directed edges of the individual DAGs in the mixture by using \((n^{2})\) interventions, where \(n\) is the number of variables. Remarkably, the maximum size of the interventions used in our algorithm is optimal if the mixture ancestors of a node (i.e., the union of its ancestors across all DAGs) do not form a cycle.
* **Optimality gap:** We show that the gap between the maximum intervention size used by the proposed algorithm for a given node and the optimal size is bounded by the _cyclic complexity number_ of the node, which is defined as the number of nodes needing intervention to break cycles among the ancestors of the node, and is upper bounded by the number of such cycles.

We provide an overview of the closely related literature, the majority of which is focused on the causal discovery of single DAGs.

**Causal discovery of a mixture of DAGs.** The relevant literature on the causal discovery of a mixture of DAGs focuses on developing graphical models to represent CI relationships in the observed mixture distribution . Among them,  proposes a _fused graph_ and shows that the mixture distribution is Markov with respect to it. The study in  proposes a similar mixture graph but relies on longitudinal data to orient any edges. The study in  constructs a _mixture DAG_ that represents the mixture distribution and designed an algorithm for learning a maximal ancestral graph. The algorithm of  requires the component DAGs of the mixture to be poset compatible, which rules out any cyclic relationships across the DAGs. The study in  introduces the notion of _emergent edges_ to investigate the inseparability conditions arising in the mixture of DAGs. The study in  proposes a variational inference-based approach for causal discovery from a mixture of time-series data. Despite their differences, all these studies are limited to using observational data.

**Intervention design for causal discovery of a single DAG.** We note that the structure of a single DAG without latent variables can be learned using single-node interventions. Hence, the majority of the literature focuses on minimizing the number of interventions. Worst-case bounds on the number of interventions with unconstrained size are established in , and heuristic adaptive algorithms are proposed in . Intervention design on causal graphs with latent variables is studied in [19; 20; 21]. The study in  also shows that single-node interventions are not sufficient for exact graph recovery in the presence of latent variables. In another direction,  studies interventions under size constraints, establishes a lower bound for the number of interventions, and shows that \(( k)\) randomized interventions with size \(k\) suffice for identifying the DAG with high probability. In the case of single-node interventions, adaptive and non-adaptive algorithms are proposed in , active learning of directed trees is studied in , and a universal lower bound for the number of interventions is established in .  also studies the universal lower bound problem and  provides an exact characterization for the number of interventions required to recover the DAG from the observational essential graph. A linear cost model, where the cost of an intervention is proportional to its size, is proposed in . It is shown that learning the DAG with optimal cost under the linear cost model is NP-hard . The size of the minimal intervention sets is studied for cyclic directed models in . Specifically, it is shown that the required intervention size is at least \(-1\) where \(\) denotes the size of the largest strongly connected component in the cyclic model. A related problem to intervention design is causal discovery from a combination of observational and interventional data. In this setting, the characterization of the equivalence classes and designing algorithms for learning them is well-explored for a single DAG [29; 30; 31; 32].

**Causal discovery from multiple clusters/contexts.** Another approach to causal discovery from a mixture of DAGs is clustering the observed samples and performing structure learning on each cluster separately [33; 34; 35; 36; 37]. Learning from multiple contexts is also studied in the interventional causal discovery literature [38; 39; 40; 41]. However, these studies assume that domain indexes are known. In a similar problem,  aims to learn the domain indexes and perform causal discovery simultaneously.

## 2 Preliminaries and definitions

### Observational mixture model

DAG models.We consider \(K 2\) distinct DAGs \(_{}(,_{})\) for \(\{1,,K\}\) defined over the same set of nodes \(\{1,,n\}\). \(_{}\) denotes the set of _directed_ edges in graph \(_{}\). Throughout the paper, we refer to these as the mixture _component_ DAGs. We use \(_{}(i)\), \(_{}(i)\), \(_{}(i)\), and \(_{}(i)\) to refer to the parents, children, ancestors, and descendants of node \(i\) in DAG \(_{}\), respectively. We also use \(i}{{}}j\) to denote \(i_{}(j)\). For each node \(i\), we define \(_{}(i)\) as the union of the nodes that are parents of \(i\) in at least one component DAG and refer to \(_{}(i)\) as the _mixture parents_ of node \(i\). Similarly, for each node \(i\) we define \(_{}(i)\), \(_{}(i)\), and \(_{}(i)\).

Mixture model.Each of the component DAGs represents a Bayesian network. We denote the random variable generated by node \(i\) by \(X_{i}\) and define the random vector \(X(X_{1},,X_{n})^{}\). For any subset of nodes \(A\), we use \(X_{A}\) to denote the vector formed by \(X_{i}\) for \(i A\). We denote the probability density function (pdf) of \(X\) under \(_{}\) by \(p_{}\), which factorizes according to \(_{}\) as

\[p_{}(x)=_{i[n]}p_{}(x_{i} x_{_{}(i)})\;, [K]\;.\] (1)

For distinct \(,^{}[K]\), \(p_{}\) and \(p_{^{}}\) can be distinct even when \(_{}=_{^{}}\). The differences between any two DAGs are captured by the nodes with distinct causal mechanisms (i.e., conditional distributions) in the DAGs. To formalize such distinctions, we define the following test, which contains all the nodes with at least two different conditional distributions across component distributions.

\[\{i:,^{}[K]\;:\;p_{ }(X_{i} X_{_{}(i)}) p_{^{}}(X_{i} X _{_{^{}}(i)})\}\,.\] (2)

We adopt the same mixture model as the prior work on causal discovery of mixture of DAGs [8; 9; 10; 11]. Specifically, observed data is generated by a mixture of distributions \(\{p_{}:[K]\}\). It is unknown to the learner which model is generating the observations \(X\). To formalize this, we define \(L\{1,,K\}\) as a latent random variable where \(L=\) specifies that the true model is \(p_{}\). We denote the probability mass function (pmf) of \(L\) by \(r\). Hence, we have the following mixture distribution for the observed samples \(X\).

\[p_{}(x)_{[K]}r() p_{}(x)\;.\] (3)Next, we provide several definitions that are instrumental to formalizing causal discovery objectives.

**Definition 1** (True edge).: _We say that \(j i\) is a true edge if \(j_{}(i)\). The set of all true edges is denoted by_

\[_{}\{(j i):i,j,\;\;\; _{}:j_{}(i)\}\;.\] (4)

A common approach to causal discovery is the class of constraint-based approaches, which perform conditional independence (CI) tests on the observed data to infer (partial) knowledge about the DAGs' structure [43; 44; 45]. In this paper, we adopt a constraint-based CI testing approach. Following this approach, the following definition formally specifies the set of node pairs that cannot be made conditionally independent in the mixture distribution.

**Definition 2** (Inseparable pair).: _The node pair \((i,j)\) is called inseparable if \(X_{i}\) and \(X_{j}\) are always statistically dependent in the mixture distribution \(p_{}\) under any conditioning set. The set of inseparable node pairs is specified by_

\[_{i}\{(i-j):i,j,\;\; A \{i,j\}:\;\;X_{i}\!\!\! X_{j} X_{A}\;\;\;p_{}\}\;.\] (5)

Note that when \((j i)\) is a true edge, the pair \((i,j)\) will be inseparable. A significant difference between independence tests for mixture models and single-DAG models is that not all inseparable pairs have an associated true edge in the former. More specifically, due to the mixing of multiple distributions, a pair of nodes can be nonadjacent in all component DAGs but still be inseparable in mixture distribution \(p_{}\). We refer to such inseparable node pairs as _emergent pairs_, formalized next.

**Definition 3** (Emergent pair).: _An inseparable pair \((i,j)_{}\) is called an emergent pair if there is no true edge associated with the pair. The set of emergent pairs is denoted by_

\[_{}\{(i,j)_{i}:\;i_ {}(j)\;\;\;\;j_{}(i)\}\;.\] (6)

The conditions under which emergent edges arise in mixture models are recently investigated in , where it is shown that the causal paths that pass through a node in the set \(\) defined in (2) are instrumental for their analysis. These paths are specified next.

**Definition 4** (\(\)-through path).: _We say that a causal path in \(_{}\) between \(i\) and \(j\) is a \(\)-through path if it passes through at least one node in \(\), i.e., there exists \(u\) such that \(i}{{}}u}{{}}j\). If \(u_{}(i)\), the path is also called a \(\)-child-through path._

### Intervention model

In this section, we describe the intervention model we use for causal discovery on a mixture of DAGs. We consider stochastic _hard_ interventions on component DAGs of the mixture model. A hard intervention on a set of nodes \(\) cuts off the edges incident on nodes \(i\) in all component DAGs \(_{}\) for \([K]\). We denote the post-intervention component DAGs upon an intervention \(\) by \(\{_{,}:[K]\}\). We note that hard interventions are less restrictive than _do_ interventions, which not only remove ancestral dependencies but also remove randomness by assigning constant values to the intervened nodes. Specifically, in \(_{,}\), the causal mechanism of an intervened node \(i\) changes from \(p_{}(x_{i} x_{_{i}(i)})\) to \(q_{i}(x_{i})\). Therefore, upon an intervention \(\), the interventional component DAG distributions are given by

\[p_{,}(x)_{i}q_{i}(x_{i})_{i }p_{}(x_{i} x_{_{}(i) })\;,[K]\;.\] (7)

Subsequently, the interventional mixture distribution \(p_{,}(x)\) is given by

\[p_{,}(x)_{[K]}r() p_{, }(x)\;.\] (8)

We note that an intervened node \(i\) has the same causal mechanism \(q_{i}(x_{i})\) for all interventions \(\) that contain \(i\). This is because an intervention procedure targets a set of nodes in all mixture components at the same time. Hence, resulting \(q_{i}(X_{i})\) is shared for all component models, owing to the same intervention mechanism, e.g., gene knockout experiments . Hence, the set of nodes with distinct causal mechanisms across the components of the interventional mixture model becomes \(_{}\). Next, we specify the \(\)-mixture DAG, which extends the mixture DAG defined for observational data in [10; 11] and will facilitate our analysis.

**Definition 5** (\(\)-mixture DAG).: _Given an intervention \(\) on a mixture of DAGs, \(\)-mixture DAG \(_{}}\) is a graph with \(nK+1\) nodes constructed by first concatenating the \(K\) component DAGs and then adding a single node \(y\) to the concatenation. Furthermore, there will be a directed edge from \(y\) to every node in \(_{}\) in every DAG \(\{_{,}:[K]\}\). In the \(\)-mixture DAG \(_{}}\), we use \(i_{}\) to denote the copy of node \(i\) in \(_{,}\). Accordingly, for any \(A\) we define \(\{i_{}:i A,\ [K]\}\)._

Figure 1 illustrates an example of a mixture of \(K=2\) component DAGs, different edge types, an intervention \(\) on the mixture, and the construction of the \(\)-mixture DAG from post-intervention component DAGs \(_{1,}\) and \(_{2,}\). We define the _observational mixture DAG_ as the \(\)-mixture DAG when the intervention set is \(=\) and denote it by \(_{}\). It is known that \(p_{}\) specified in (3) satisfies the global Markov property with respect to observational mixture DAG [10, Theorem 3.2]. It can be readily verified that this result extends to the interventional setting for \(p_{}}\) and \(_{}}\). We make the following faithfulness assumption to facilitate causal discovery via statistical independence tests.

**Assumption 1** (\(\)-mixture faithfulness).: _For any intervention \(\), the interventional mixture distribution \(p_{}}(x)\) is faithful to \(_{}}\), that is if \(X_{A}\!\!\! X_{B} X_{C}\) in \(p_{}}(x)\), then \(\) and \(\) are \(d\)-separated given \(\) in \(_{}}\)._

Finally, we note that the observational counterpart of Assumption 1, i.e., when \(=\), is standard in the literature for analyzing a mixture of DAGs . In working with interventions, we naturally extend it to interventional mixture distributions. Also note that Assumption 1 does not compare observational and interventional distributions. Hence, it is not comparable to various faithfulness assumptions in the literature on the interventional causal discovery of a single DAG, e.g., .

### Causal discovery objectives

We aim to address the following question: _how can we use interventions to perform causal discovery in a mixture of DAGs_, with the objectives specified next.

The counterpart of this question is well-studied for the causal discovery of a single DAG. Since the unoriented skeleton of the single DAG can already be identified by CI tests on observational data, interventions are leveraged to orient the edges. Interventions are generally bounded by a pre-specified budget, measured by the number of interventions. The extent of causal relationships that observational data can uncover in a mixture of DAGs is significantly narrower than those in single DAGs. The striking difference is the existence of emergent pairs specified in (6). Therefore, the objective of intervention design extends to distinguishing _true_ cause-effect relationships from the emergent pairs as well as determining the direction of causality. Specifically, we focus on identifying the true edges specified in (4) as the edges exist in at least one component DAG of the mixture. For this purpose, two central objectives of our investigation are:

1. Determining the necessary and sufficient size of the interventions for identifying true edges \(_{}\).
2. Designing efficient algorithms with near-optimal intervention sizes.

## 3 Interventions for causal discovery of a mixture of DAGs

In this section, we investigate the first key question of interventional causal discovery on a mixture of DAGs and investigate the size of the necessary and sufficient interventions for identifying mixture parents of a node. First, we consider a mixture of general DAGs without imposing structural constraints and establish matching necessary and sufficient intervention size for distinguishing a true edge from an emergent pair. Then, we strengthen the results for a mixture of directed trees. The results established in this section are pivotal for understanding the fundamental limits of causal discovery of a mixture of DAGs. These results guide the intervention design in Section 4.

Our analysis uncovers the connections between the mixture distribution under an intervention \(\) and the structure of post-intervention component DAGs \(\{_{,}:[K]\}\). We know that the interventional mixture distribution \(p_{,}\) satisfies the Markov property with respect to \(\)-mixture DAG \(_{,}\) specified in Definition 5. Therefore, in conjunction with the \(\)-mixture faithfulness assumption, the separation statements in \(_{,}\) can be inferred exactly by testing the conditional independencies in \(p_{,}\). To establish the necessary and sufficient intervention sizes, we recall that set \(\) plays an important role in the separability conditions in \(\)-mixture DAG \(_{,}\) since \(\) allows paths across different component DAGs. The following result serves as an intermediate step in obtaining our main result.

**Lemma 1**.: _Consider an inseparable pair \((i,j)_{i}\) and an intervention \(\). We have the following identifiability guarantees using the interventional mixture distribution \(p_{,}(x)\)._

1. **Identifiability:** _It is possible to determine whether_ \(j_{}(i)\) _if_ \(j\) _and there do not exist_ \(\)_-through paths from_ \(j\) _to_ \(i\) _in_ \(_{,}\) _for any_ \([K]\)_._
2. **Non-identifiability:** _It is impossible to determine whether_ \(j_{}(i)\) _if_ \(j_{}\) _or there exists a_ \(\)_-child-through path from_ \(j\) _to_ \(i\) _in at least one_ \(_{,}\) _where_ \([K]\)_._

Lemma 1 provides intuition for characterizing sufficient and necessary conditions for identifying a true edge. The identifiability result implies that it suffices to choose an intervention \(\) that reduces the viable \(\)-through paths in \(_{,}\) to true edges from \(j\) to \(i\). Similarly, the non-identifiability result implies the necessity of intervening on \(\)-child nodes. Building on these properties, our main result in this section establishes matching necessary and sufficient intervention sizes for identifying true edges.

**Theorem 1** (Intervention sizes).: _Consider nodes \(i,j\) in a mixture of DAGs._

1. **Sufficiency:** _For any mixture of DAGs, there exists an intervention_ \(\) _with_ \(|||_{}(i)|+1\) _that ensures the determination of whether_ \(j_{}(i)\) _using CI tests on_ \(p_{,}\)_._
2. **Necessity:** _There exist DAG mixtures for which it is impossible to determine whether_ \(j_{}(i)\) _using CI tests on_ \(p_{,}(x)\) _for any intervention_ \(\) _with_ \(|||_{}(i)|\)_._

Theorem 1 represents a fundamental step for understanding the intricacies of mixture causal discovery and serves as a guide for evaluating the optimality and efficiency of any learning algorithm. We also note that the necessity statement reflects a worst-case scenario. As such, we present the following refined sufficiency results that can guide efficient algorithm designs.

**Lemma 2**.: _Consider nodes \(i,j\) in a mixture of DAGs. It is possible to determine whether \(j_{}(i)\) using CI tests on \(p_{,}\) and any of the following interventions:_

1. \(=\{j\}_{[K]}\{_{}(i) _{}(j)\}\) _; or_
2. \(=\{j\}_{[K]}\{_{}(i) _{}(j)\}\) _; or_
3. \(=\{j\}_{[K]}\{_{}(i) _{}(j)\}\)__._

Note that the three interventions in Lemma 2 can coincide when parents of \(i\) in a component DAG are also children of \(j\) and are in \(\). This case yields the set \(=_{}(i)\{j\}\) with size \((|_{}(i)|+1)\). Since this can be a rare occurrence for realistic mixture models, partial knowledge about the underlying component DAGs, e.g., ancestral relations or the knowledge of \(\), can prove to be useful for identifying \(_{}(i)\) using interventions with smaller sizes. Finally, we note that our results in Theorem 1 and Lemma 2 are given for a mixture of general DAGs, and they can be improved for special classes of DAGs. In the next result, we focus on mixtures of directed trees.

**Theorem 2** (Intervention sizes - trees).: _Consider nodes \(i,j\) in a mixture of \(K\) directed trees._

1. **Sufficiency:** _For any mixture of directed trees, there exists an intervention_ \(\) _with_ \(|| K+1\) _such that it is possible to determine whether_ \(j_{}(i)\) _using CI tests on_ \(p_{,}\)_._
* **Necessity:** _There exist mixtures of directed trees such that it is impossible to determine whether_ \(j_{}(i)\) _using CI tests on_ \(p_{,}\) _for any intervention_ \(\) _with_ \(|| K\)_._

Theorem 2 shows that, unlike the general result in Theorem 1, the number of mixture components plays a key role when considering a mixture of directed trees. Hence, prior knowledge of the number of mixture components can be useful for the causal discovery of a mixture of directed trees.

## 4 Learning algorithm and its analysis

In this section, we design an adaptive algorithm that identifies and orients all true edges, referred to as **C**ausal **D**iscovery from **I**nterventions on **Mixture** Models (CADIM). The algorithm is summarized in Algorithm 1, and its steps are described in Section 4.1. We also analyze the performance guarantees of the algorithm and the optimality of the interventions used in the algorithm in Section 4.2.

### Causal discovery from interventions on mixture models

The proposed CADIM algorithm designs interventions for performing causal discovery on a mixture of DAGs. The algorithm is designed to be general and demonstrate feasible time complexity for any mixture of DAGs without imposing structural constraints. Therefore, we forego the computationally expensive task of learning the inseparable pairs from observational data, which requires \((n^{2} 2^{n})\) CI tests , and entirely focus on leveraging interventions for discovering the true causal relationships. The key idea of the algorithm is to use interventions to decompose the ancestors of a node into topological layers and identify the mixture parents by sequentially processing the topological layers using carefully selected interventions. The algorithm consists of four main steps, which are described next.

**Step 1: Identifying mixture ancestors.** We start by identifying the set of mixture ancestors \(_{}(i)\) for each node \(i\), i.e., the union of ancestors of \(i\) in the component DAGs. For this purpose, we use single-node interventions. Specifically, for each node \(i\), we intervene on \(=\{i\}\) and construct the set of nodes that are marginally dependent on \(X_{i}\) in \(p_{,}\), i.e.,

\[}(i)=\{j:X_{j}\!\!\! X_{i}\;\;\;p_{ ,\{i\}}\}\;, i\;.\] (9)

Then, we construct the sets \(}(i)=\{j:i}(j)\}\) for all \(i\). Under \(\)-mixture faithfulness, this procedure ensures that \(}(i)=_{}(i)\), and \(}(i)=_{}(i)\) (see Lemma 3). The rest of the algorithm steps aim to identify mixture parents of a single node \(i\), \(_{}(i)\), within the set \(}(i)\). Hence, the following steps can be repeated for all \(i\) to identify all true edges.

**Step 2: Obtaining cycle-free descendants.** In this step, we consider a given node \(i\) and aim to break the _cycles_ across the nodes in \(}(i)\) by careful interventions. Once this is achieved, for all \(j}(i)\), we will refine \(j\)'s descendant set \(}(j)\) to _cycle-free_ descendant set \(_{i}(j)\). The motivation is that these refined descendant sets can be used to topologically order the nodes in \(}(i)\). The details of this step work as follows. First, we construct the set of cycles

\[(i)\{=(_{1},,_{})\;:\;_{1}=_{ }\;, u[-1]\;\;_{u}}(i)\;\;_{ u}}(_{u+1})\}\;.\] (10)

Subsequently, if \((i)\) is not empty, we define a minimal set that shares at least one node with each cycle in \((i)\),

\[(i)\;\; (i)\;\;|(i)| 1\;.\] (11)

We refer to \((i)\) as the _breaking set_ of node \(i\) since intervening on any set \(\) that contains \((i)\) breaks all the cyclic relationships in \((i)\). Then, if \((i)\) is not empty, we sequentially intervene on \(=(i)\{j\}\) for all \(j}(i)\), and construct the cycle-free descendant sets defined as

\[_{i}(j)\{k}(i)\{i\}:X_{j} \!\!\! X_{k}\;\;\;p_{,}\}\;,\;=(i)\{j\}\;.\] (12)

Note that \(_{i}(j)\) is a subset of \(_{}(j)\) since intervening on \(j\) makes it independent of all its non-descendants. Finally, we construct the set \(=\{j}(i):i_{i}(j)\}\).

**Step 3: Topological layering.** In this step, we decompose \(}(i)\) into topological layers by using the cycle-free descendant sets constructed in Step 2. We start by constructing the first layer as

\[S_{1}(i)=\{j:_{i}(j)=\}\;.\] (13)```
1:Step 1: Identify mixture ancestors
2:for\(i\)do
3: Intervene on \(=\{i\}\), observe samples from \(p_{,}\)
4:\(}(i)\{j:X_{j} X_{i}\ \ \ \ p_{,}\}\)\(\) mixture descendants of node \(i\)
5:for\(i\)do
6:\(}(i)\{j:i}(j)\}\)\(\) mixture ancestors of node \(i\)
7:Repeat Steps 2, 3, 4 for all \(i\)
8:Step 2: Obtain cycle-free descendants
9: Find cycles among \(}(i)\)
10:\((i)\{=(_{1},,_{t+1})\ :\ _{1}=_{t+1}\,  u[t]\ \ _{u}}(i)\ \ _{u}}(_{u+1})\}\)
11:if\((i)\) is empty then
12:\((i)\)
13:for\(j}(i)\)do
14:\(_{i}(j)\{}(j)}(i )\}\{i\}\)
15:else
16:\((i)\) a minimal set such that \((i)\ |(i)| 1\)
17:for\(j}(i)\)do
18: Intervene on \(=(i)\{j\}\)\(\) break cycles among \(}(i)\)
19:\(_{i}(j)\{k}(i)\{i\}:X_{j}  X_{k}\ \ \ \ p_{,}\}\)\(\) cycle-free descendants of node \(j\)
20:\(\{j}(i)\ :\ i_{i}(j)\}\)\(\) refined ancestors
21:Step 3: Topological layering
22:\(t 0\)
23:while\(|| 1\)do
24:\(t t+1\)
25:\(S_{t}(i)\{j:_{i}(j)=\}\)
26:\( S_{t}(i)\)
27:Step 4: Identify mixture parents
28:\(}(i)\)
29:for\(u(1,,t)\)do
30:for\(j S_{u}(i)\)do
31: Intervene on \(=}(i)(i)\{j\}\)
32:if\(X_{j} X_{i}\) in \(p_{,}\)then
33:\(}(i)}(i)\{j\}\)
34:Return\(}(i)\) ```

**Algorithm 1** Causal Discovery from Interventions on Mixture Models (CADIM)

The construction of cycle-free descendant sets ensures that \(S_{1}(i)\) is not empty. Next, we update \( S_{1}(i)\) by removing layer \(S_{1}(i)\) to conclude the first step. Then, we iteratively construct the layers \(S_{u}(i)=\{j:}(j)=\}\) and update \( S_{u}(i)\) as in Line 26 of the algorithm. We continue until the set \(\) is exhausted, and denote these topological layers by \(\{S_{1}(i),,S_{t}(i)\}\).

Step 4: Identifying the mixture parents.Finally, we process the topological layers sequentially to identify the mixture parents in each layer. For a node \(j S_{1}(i)\), whether \(j_{}(i)\) can be determined from a marginal independence test on \(p_{,}\) where \(=(i)\{j\}\). Leveraging this result, when processing each \(S_{u}(i)\), we consider the nodes \(j S_{u}(i)\) sequentially and intervene on \(=}(i)(i)\{j\}\), where \(}(i)\) denotes the estimated mixture parents. Under this intervention, a statistical dependence implies a true edge from \(j\) to \(i\). Hence, we update the set \(}(i)\) as follows.

\[}(i)}(i)\{j\}\ \ X_{j} X_{i}\ \ \ p_{,}\ \ \ \ =}(i)(i)\{j\}\.\] (14)

After the last layer \(S_{t}(i)\) is processed, the algorithm returns the estimated mixture parents \(}(i)\). By repeating Steps 2, 3, and 4 for all \(i\), we determine the true edges with their orientations.

### Guarantees of the CADIM algorithm

In this section, we establish the guarantees of the CADIM algorithm and interpret them vis-a-vis the results in Section 3. We start by providing the following result to show the correctness of identifying mixture ancestors.

**Lemma 3**.: _Given \(\)-mixture faithfulness, Step 1 of Algorithm 1 identifies \(\{_{}(i):i[n]\}\) using \(n\) single-node interventions._

Note that the mixture ancestor sets \(\{_{}(i)\}\) do not imply a topological order over the nodes \(\), e.g., there may exist nodes \(u,v\) such that \(u_{}(v)\) and \(v_{}(u)\). As such, a major difficulty in learning a mixture of DAGs compared to learning a single DAG is the possible cyclic relationships formed by the combination of components of the mixture. Recall that the breaking set is specified in (11) to treat such possible cycles carefully. We refer to the size of \((i)\) as the _cyclic complexity number_ of node \(i\), denoted by \(_{i}\), and the size of the largest breaking set by \(_{}\) as

\[_{i}|(i)|, i\;, _{}_{i}_{i}\;.\] (15)

Note that \(_{i}\) is readily bounded by the number of cycles in \((i)\). Next, we analyze the guarantees of the algorithm for a node \(i\) in two cases: \(_{i}=0\) (cycle-free case) and \(_{i} 1\) (nonzero cyclic complexity).

Cycle-free case.Our next result shows that if \(_{i}=0\), i.e., there are no cycles among the nodes in \(_{}(i)\), then we identify the mixture parents \(_{}(i)\), i.e., the union of the nodes that are parents of \(i\) in at least one component DAG, using interventions with the optimal size.

**Theorem 3** (Guarantees for cycle-free ancestors).: _If the cyclic complexity of node \(i\) is zero, then Algorithm 1 ensures that \(}(i)=_{}(i)\) by using \(|_{}(i)|\) interventions where the size of each intervention is at most \(|_{}(i)|+1\)._

Theorem 3 shows that by repeating the algorithm steps for each node \(i\), we can identify all true edges with their orientations using \(n+_{i}|_{}(i)| n+n(n-1)=n^{2}\) interventions, where the size of each intervention is bounded by the worst-case necessary size established in Theorem 1.

Nonzero cyclic complexity.Finally, we address the most general case, in which the mixture ancestors of node \(i\) might contain cycles. In this case, our algorithm performs additional interventions to break the cycles among \(_{}(i)\). Hence, the number and size of the interventions will be greater than the cycle-free case, which is established in the following result.

**Theorem 4** (Guarantees for general mixtures).: _Algorithm 1 ensures that \(}(i)=_{}(i)\) by using \(|_{}(i)|\) interventions with size \(_{i}+1\), and \(|_{}(i)|\) interventions with size at most \(|_{}(i)|+_{i}+1\)._

Theorem 4 shows that, Algorithm 1 achieves the causal discovery objectives by using a total of \(n+2_{i}|_{}(i)| n+2n(n-1)=(n^{2})\) interventions, where the maximum intervention size for learning each \(_{}(i)\) is at most \(_{i}\) larger than the necessary and sufficient size \(|_{}(i)|+1\). This optimality gap reflects the challenges of accommodating cyclic relationships in intervention design for learning in mixtures while also maintaining a quadratic number of interventions \((n^{2})\).

## 5 Experiments

We evaluate the performance of Algorithm 1 for estimating the true edges in a mixture of DAGs using synthetic data and investigate the need for interventions, the effect of the graph size, and the cyclic complexity. Additional results for varying the number of components, parameterization, and number of samples are provided in Appendix E1.

**Experimental setup.** We use an Erdos-Renyi model \(G(n,p)\) with density \(p=2/n\) to generate the component DAGs \(\{_{}:[K]\}\) for different values of nodes \(n\) and mixture components \(K\). We adopt linear structural equation models (SEMs) with Gaussian noise for the causal models, in which the noise for node \(i\) is sampled from \((_{i},_{i}^{2})\) where \(_{i}\) is sampled uniformly in \([-1,1]\) and \(_{i}^{2}\) is sampled uniformly in \([0.5,1.5]\). The edge weights are sampled uniformly in \([0.25,2]\). Weconsider the case where a change in the conditional distribution of node \(i\) is only caused by changes in the parents of \(i\) across different DAGs. We use a partial correlation test to check (conditional) independence in the algorithm steps, similar to the related work [10; 11]. We repeat this procedure for \(100\) randomly generated DAG mixtures for each of the following settings.

**Need for interventions.** We demonstrate the need for interventions for learning the skeleton in the mixture of DAGs, unlike the case of single DAGs. To this end, we consider a mixture of \(K=2\) DAGs and learn the inseparable node pairs via exhaustive CI tests (see Algorithm 2 in Appendix E). Figure 1(a) empirically verifies the claim that true edges (even their undirected versions) cannot be learned using observational data only.

**Recovery of true edges.** We evaluate the performance of Algorithm 1 on the central task of learning the true edges in the mixture. For this purpose, we report average precision and recall rates for recovering [the true edges. We look into the performance of Algorithm 1 under a varying number of nodes \(n\) for a mixture of \(K=3\) DAGs and using \(5000\) samples from each DAG. Figure 1(b) demonstrates that Algorithm 1 maintains a strong performance even under \(n=30\) nodes. We provide additional results for the number of DAGs in the range \(K\) and varying number of samples in Appendix E.

**Quantification of cyclic complexity.** We recall that for finding the mixture parents of a node \(i\), the maximum size of the intervention used in Algorithm 1 is at most \(_{i}\), i.e., cyclic complexity, larger than the necessary size. In Figure 1(c), we plot the empirical values of average cyclic complexity - both the ground truth and estimated by the algorithm. Figure 1(c) shows that even though average \(_{i}\) increases with \(K\), it still remains very small, e.g., approximately \(1.5\) for a mixture of \(K=3\) DAGs with \(n=10\) nodes. Furthermore, on average, the estimated \(_{i}\) values used in the algorithm are almost identical to the ground truth \(_{i}\). Therefore, Algorithm 1 maintains its close to optimal intervention size guarantees in the finite-sample regime.

## 6 Conclusion

In this paper, we have conducted the first analysis of using interventions to learn causal relationships in a mixture of DAGs. First, we have established the matching necessary and sufficient size of interventions needed for learning the true edges in a mixture. Subsequently, guided by this result, we have designed an algorithm that learns the true edges using interventions with close to optimal sizes. We have also analyzed the optimality gap of our algorithm in terms of the cyclic relationships within the mixture model. The proposed algorithm uses a total of \((n^{2})\) interventions. Establishing lower bounds for the number of interventions with constrained sizes remains an important direction for future work, which can draw connections to intervention design for single-DAG and further characterize the differences of causal discovery in mixtures. Finally, generalizing the mixture model to accommodate partial knowledge of the underlying domains can be useful in disciplines where such knowledge can be acquired a priori.