# Grassmann Manifold Flows for

Stable Shape Generation

 Ryoma Yataka\({}^{1,2}\), Kazuki Hirashima\({}^{1}\), Masashi Shiraishi\({}^{1}\)

\({}^{1}\)Information Technology R&D Center (ITC), Mitsubishi Electric Corporation,

Kamakura, Kanagawa, 247-8501, Japan

\({}^{2}\)Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA

{Yataka.Ryoma@dw.MitsubishiElectric.co.jp, yataka@merl.com}

###### Abstract

Recently, studies on machine learning have focused on methods that use symmetry implicit in a specific manifold as an inductive bias. Grassmann manifolds provide the ability to handle fundamental shapes represented as shape spaces, enabling stable shape analysis. In this paper, we present a novel approach in which we establish the theoretical foundations for learning distributions on the Grassmann manifold via continuous normalization flows, with the explicit goal of generating stable shapes. Our approach facilitates more robust generation by effectively eliminating the influence of extraneous transformations, such as rotations and inversions, through learning and generating within a Grassmann manifold designed to accommodate the essential shape information of the object. The experimental results indicated that the proposed method could generate high-quality samples by capturing the data structure. Furthermore, the proposed method significantly outperformed state-of-the-art methods in terms of the log-likelihood or evidence lower bound. The results obtained are expected to stimulate further research in this field, leading to advances for stable shape generation and analysis.

## 1 Introduction

Many machine learning algorithms are designed to automatically learn and extract latent factors that explain a specific dataset. Symmetry is known to be an inductive bias (that is, prior knowledge other than training data that can contribute significantly to learning results) for learning latent factors (Cohen and Welling (2016, 2017); Weiler and Cesa (2019); Satorras et al. (2021); Bronstein et al. (2021); Pury et al. (2022)). Moreover, they exist in many phenomena in the natural sciences. If a target \(M\) is invariant1 when the operation \(g S\) designated by \(S\) applied to \(M\), \(M\) has symmetry \(S\). For example, a sphere remains a sphere even if a rotation \(R\) is applied; a symmetric shape remains symmetric even if a left-right reversal is applied.

Recent studies have focused on methods incorporating symmetry into models based on equivariance and invariance, when the data space forms a non-Euclidean space (a sphere \(S^{n}\) or a special unitary group \(SU(n)\)) (Cohen et al. (2018, 2019); Graham et al. (2020); Haan et al. (2021); Boyda et al. (2021)). Among them, discriminative and generative models for subspace data (e.g., shape matrices of point cloud data such as three-dimensional molecules and general object shapes) have been proposed as viable approaches using Grassmann manifolds, which are quotient manifolds obtained by introducing a function that induces invariance with respect to orthogonal transformations intoa set of orthonormal basis matrices namely Stiefel Manifold. Numerous studies have confirmed their effectiveness (Liu et al. (2003); Hamm and Lee (2008); Turaga et al. (2008); Harandi et al. (2011); Fan et al. (2011); Lui (2012); Huang et al. (2015, 2018); Souza et al. (2020); Haitman et al. (2021); Doronina et al. (2022); Souza et al. (2022)). Shape theory studies the equivalent class of all configurations that can be obtained by a specific class of transformation (e.g. linear, affine, projective) on a single basis shape (Partangenaru and Mardia (2003); Sepiashvili et al. (2003); Begelfor and Werman (2006)), and it can be shown that affine and linear shape spaces for specific configurations can be identified by points on the Grassmann manifold. By applying this model to inverse molecular design for stable shape generation to discover new molecular structures, they can contribute to the development of drug discovery, computational anatomy, and materials science (Sanchez-Lengeling and Aspuru-Guzik (2018); Bilodeau et al. (2022)).

Continuous normalizing flows (CNFs; Chen et al. (2018); Grathwohl et al. (2019); Lou et al. (2020); Kim et al. (2020); Mathieu and Nickel (2020)) are generative models that have attracted attention in recent years along with the variational auto-encoders (VAE; Kingma and Welling (2014)), generative adversarial networks (GAN; Goodfellow et al. (2014)), autoregressive models (Germain et al. (2015); Oord et al. (2016)), and diffusion models (Sohl-Dickstein et al. (2015); Ho et al. (2020); Song et al. (2021); Huang et al. (2022); De Bortoli et al. (2022)). The CNF is a method with theoretically superior properties that allow rigorous inference and evaluation of the log-likelihood. It can be trained by maximum likelihood using the change of variables formula, which allows to specify a complex normalized distribution implicitly by warping a normalized base distribution through an integral of continuous-time dynamics. In this paper, we present a novel approach with the explicit goal of generating stable shapes. We employ the CNF to achieve stable shape generation on a Grassmann manifold; however, previous studies have lacked the theoretical foundation to handle the CNF on the Grassmann manifold. To construct this, we focused on the quotient structure of a Grassmann manifold and translated the problem of flow learning on a Grassmann manifold into that of preserving equivariance for orthogonal groups on a Stiefel manifold, which is its total space. The contributions of this study are as follows.

* A theory and general framework for learning flows on a Grassmann manifold are proposed. In our setting, we can train flows on a Grassmann manifold of arbitrary dimension. To the best of our knowledge, this is the first study in which a CNF was constructed on a Grassmann manifold via a unified approach, focusing on the quotient structure.
* The validity of the proposed approach was demonstrated by learning densities on a Grassmann manifold using multiple artificial datasets with complex data distributions. In particular, orthogonally transformed data were proven to be correctly learned without data augmentations by showing that untrained transformed (i.e., rotated or mirrored) data can be generated from the trained model. The model was evaluated on multiple patterns of training data and performed well with a small amount of training data.
* The effectiveness of our approach was confirmed by its state-of-the-art performance in a molecular positional generation task.

## 2 Related Works

Symmetry-based LearningThe concept of equivariance has been studied in recent years to leverage the symmetries inherent in data (Cohen and Welling (2017); Kondor and Trivedi (2018); Cohen et al. (2018, 2019, 2019); Finzi et al. (2020); Haan et al. (2021)). Early work by Cohen and Welling (2017) indicated that when data are equivariant, they can be processed with a lower computational cost and fewer parameters. In the context of CNFs (Rezende and Mohamed (2015); Chen et al. (2018); Grathwohl et al. (2019)), which are generative models, Rezende et al. (2019), Kohler et al. (2020) and Garcia Satorras et al. (2021) proposed equivariant normalizing flows to learn symmetric densities on Euclidean spaces. Furthermore, the symmetries that appear in learning densities on a manifold were introduced by Boyda et al. (2021) and Katsman et al. (2021) as a conjugate equivariant flow on \(SU(n)\), which is a quotient manifold, for use in lattice gauge theory. However, a normalizing flow on a Grassmann manifold capable of handling subspace data has not yet been established.

Shape Data Analysis and Other Applications with Subspace on the Grassmann Manifold \(k\)-dimensional shape data (Begelor and Werman (2006); Yoshinuma et al. (2016); Haitman et al. (2021);Doronina et al. (2022)) such as point clouds are essentially represented as subspace (shape space (Sepiashvili et al. (2003); Srivastava et al. (2005); Begelfor & Werman (2006); Yataka & Fukui (2017)) data on a Grassmann manifold. Furthermore, subspace data can be obtained on many types of data and can provide advantages such as practicability and noise robustness. For example, multiview image sets or video data (Fan et al. (2011); Turaga et al. (2011); Lui (2012); Alashkar et al. (2016)), signal data (Srivastava & Klassen (2004); Gatto et al. (2017); Souza et al. (2019); Yataka et al. (2019)), and text data (Shimomoto et al. (2021)) are often provided as a set of feature vectors. Such raw data in matrix form is not very useful owing to its considerable size and noise. The analysis of its eigenspace, or column subspace, is important because alternatively, the raw data matrix can be well approximated by a low-dimensional subspace with basis vectors corresponding to the maximum eigenvalues of the matrix. Therefore, they inevitably give rise to the necessity of analyzing them on a Grassmann manifold.

## 3 Mathematical Preliminaries

In this section, the basic mathematical concepts covered in this paper are described. Further details on the fundamentals of a Grassmann manifold are summarized in the Appendix D.

### Grassmann Manifold Defined as Quotient Manifold

Definition of a Grassmann ManifoldA Grassmann manifold \((k,D)\) is a set of \(k\)-dimensional subspaces \(()\) (\(\) is a matrix of \(k\) basis vectors and the function \(()\) is onto \((k,D)\)) in the \(D\)-dimensional Euclidean space \(^{D}\), and is defined as \((k,D)=\{()^{D }(())=k\}\) (Absil et al. (2008)). The \(()\) is the same subspace regardless of a \(k\)-dimensional rotation or \(k\)-dimensional reflection applied to \(\) on which it is spanned. With respect to the compact Stiefel manifold \((k,D):=\{^{D k}^{ }=_{k}\}\) defined as the set of \(D k\)-orthonormal basis matrices \(\), the equivalence class of \((k,D)\) determined from the equivalence relation \(\)2 is defined by \([]:=()=\{ {St}(k,D)(k)\}\), where \(()\) is a continuous surjection referred to as a quotient map. The equivalence class corresponds one-to-one with the \(k\)-dimensional subspace:

\[[]=[]()=()\,,\] (1)

where \((k,D)\) is the representative of \([]\). \(()\) is termed invariant under \(\). The quotient set composed of such \([]\) as a whole can introduce the structure of a manifold (Sato & Iwai (2014)).

**Definition 1**.: _A Grassmann manifold as a quotient manifold is defined as follows:_

\[(k,D):=(k,D)/(k)=\{[ ]=()(k,D)\},\] (2)

_where \((k,D)/(k).\) is the quotient manifold by the \(k\)-dimensional orthogonal group \((k)\) with the total space \((k,D)\), and \(()\) is the quotient map \(:(k,D)(k,D)/(k)\)._

Tangent Space and Vector FieldLet \(T_{[]}(k,D)\) be the tangent space of \([](k,D)\). As the point \([]\) is not a matrix, \(T_{[]}(k,D)\) cannot be represented by a matrix. Therefore, treating these directly in numerical calculations is challenging. To solve this problem, we can use the representative \((k,D)\) for \([]\) and the tangent vector \(_{}^{} T_{}^{}(k,D)\), which is referred to as the horizontal lift of \(_{[]} T_{[]}(k,D)\), for \(_{[]}\). These facilitate computation with matrices (Absil et al. (2008)). \(T_{}^{}(k,D)\) is a subspace of the tangent space \(T_{}(k,D)\) at \(\), which is referred to as a horizontal space, and \(_{}^{}\) is referred to as a horizontal vector. The tangent bundle \(T(k,D)=_{[](k,D)}T_{[ ]}(k,D)\) that sums up the tangent spaces \(T_{[]}(k,D)\) form vector fields

Figure 1: Conceptual diagram of spaces with horizontal lift.

\(:(k,D) T(k,D)\). A conceptual diagram of the various spaces is shown in Figure 1. Further details on these concepts can be found in Appendix D.4.

### Manifold Normalizing Flow

Let \((,h)\) be a Riemannian manifold. We consider the time evolution of a base point \(=(0),\ :[0,)\), whose velocity is expressed by a vector field \((t,(t))\). Intuitively, \((t,(t))\) represents the direction and speed at which \(\) moves on the curve \((t)\). Let \(T_{}\) be the tangent space at \(\) and \(T=_{}T_{}\) be the tangent bundle. The time evolution of a point according to a vector field \(: T\) is expressed by the differential equation \(=(t,(t)),(0)=\). Let \(F_{,T}:\) be defined as the map from \({}^{}\) to the evaluated value at time \(T\) on the curve \((t)\) starting at \(\). This map \(F_{,T}\) is known as the flow of \(\)(Lee (2003)). Recently, Mathieu & Nickel (2020) introduced the Riemann CNF (RCNF), wherein the random variable \((t)\) is assumed to be time-dependent and the change in its log-likelihood follows the instantaneous change formula for the variable. This is an extension of the CNF (Chen et al. (2018); Grathwohl et al. (2019)) to a Riemannian manifold. Specifically, when \(p_{}\) is the density parameterized by \(\), the derivative of the log-likelihood is expressed as \(((t))}{dt}=-(_{}(t,(t)))\), where \(_{}\) is the vector field parameterized by \(\) and \(()\) is the divergence. By integrating this over time, the sum of the changes in the log-likelihood with flow \(F_{,t_{1\,}}\) can be computed:

\[ p_{}((t_{1}))= p(F_{,t_{1\,}}^{-1}((t_{1})))-_{t_{0}}^{t_ {1}}(_{}(t,(t) ))dt.\] (3)

## 4 Invariant Densities from Grassmann Manifold Flow

This section provides a tractable and efficient method for learning densities on a Grassmann manifold \((k,D)\). The method for preserving the flow on \((k,D)\) is non-trivial. Therefore, we derive the following implications.

1. **Vector field on \((k,D)\) Flow on \((k,D)\)** (**Proposition 1**).
2. **Flow on \((k,D)\) Probability density on \((k,D)\)** (**Proposition 2**).
3. **Construction of a prior probability density for an efficient sampling** (**Proposition 3**).

The essence of the proofs is to show that Corollary 2 (Homogeneity Property (4) with regard to \({}^{}(k)\), which is equivariance in the horizontal space) is satisfied.

\[}_{}^{}=}_{}^{ },\] (4)

where \(}_{}^{}\) is a horizontal lift at representative \(\) relative to \(_{[]} T_{[]}\,(k,D)\). We defer the proofs of all propositions to Appendix B. These series of propositions show that by using a prior distribution on \((k,D)\) that can be easily sampled, a flow that generates a complex probability density distribution on \((k,D)\) can be obtained.

### Construction of Flow from Vector Field

To construct flows on \((k,D)\), we use tools in the theory of manifold differential equations. In particular, there is a natural correspondence between the vector fields on \((k,D)\) and the flows on \((k,D)\). This is formalized in the following proposition.

**Proposition 1**.: _Let \((k,D)\) be a Grassmann manifold, \(\) be any time-dependent vector field on \((k,D)\), and \(F_{,T}\) be a flow on a \(\). Let \(}\) be any time-dependent horizontal lift and \(_{},T}\) be a flow of \(}\). \(}\) is a vector field on \((k,D)\) if and only if \(_{},T}\) is a flow on \((k,D)\) and satisfies invariance condition \(}}^{}\) for all \(_{},T}_{}^{ },T}\). Therefore, \(\) is a vector field on \((k,D)\) if and only if \(F_{,T}:=[_{},T}]\) is a flow on \((k,D)\), and vice versa._

### Construction of Probability Densities with Flow

We show that the flow on \((k,D)\) induces density on \((k,D)\).

**Proposition 2**.: _Let \((k,D)\) be a Grassmann manifold. Let \(p\) be the probability density on \((k,D)\) and \(F\) be the flow on \((k,D)\). Suppose \(\) is a density on \((k,D)\) and \(\) is a flow on \((k,D)\). Then, the distribution \(_{}\) after transformations by \(\) is also a density on \((k,D)\). Further, the invariance condition \(_{}_{^{}}\) is satisfied for all \(^{}\). Therefore, \(p_{F}:=[_{}]\) is a distribution on \((k,D)\)._

In the context of the RCNF, Proposition 2 implies that the application of a flow on \((k,D)\) to a prior distribution on \((k,D)\) results in a probability density on \((k,D)\). Thus, the problem of constructing a probability density on \((k,D)\) is reduced to that of constructing a vector field.

### Prior Probability Density Function

To construct a flow on \((k,D)\), a prior distribution that is easy to sample as a basis for the transformation is required, although the method for constructing such a distribution is non-trivial. In this study, a distribution based on the matrix-variate Gaussian distribution is introduced as a prior on \((k,D)\) that is easy to sample, and a flow on \((k,D)\) is constructed.

**Proposition 3**.: _The distribution \(p_{(k,D)}\) on a Grassmann manifold \((k,D)\) based on the matrix-variate Gaussian distribution \(\) can be expressed as follows._

\[p_{(k,D)}([];[],,)=V_{(k,D)}(}_{}^{}; ,,)|(_{}_{}^{}}\,_{})|,\] (5)

_where \(\) is an orthonormal basis matrix denoting the mean of the distribution, \(\) is a positive definite matrix denoting the row directional variance, \(\) is a positive definite matrix denoting the column directional variance, and \(}_{}^{}\) is a random sample from \(\) in an \((D-k) k\)-dimensional horizontal space \(T_{}^{}\,(k,D)\). \(V_{(k,D)}\) denotes the total volume of \((k,D)\) defined by (113), \(_{}\) denotes the horizontal retraction at \(\), and \(|(_{}_{}^{}}\, {R}_{})|\) denotes the Jacobian._

A retraction is a map for communicating data between a manifold and its tangent bundles and is a first-order approximation of an exponential map (Zhu & Sato (2021)). Various methods of retraction have been proposed at (Absil et al. (2008); Fiori et al. (2015); Zhu & Sato (2021)); in particular, the one based on the Cayley transform is differentiable and does not require matrix decomposition. We use (40) as a Cayley transform based horizontal retraction. Further details are provided in Appendix D.6.

## 5 Learning Probability Densities with Flow

### Training Paradigms

Using the results of Section 4, a flow model on a Grassmann manifold \((k,D)\) is constructed. Herein, the flow model on \((k,D)\) is expressed as **GrCNF**. In the proposed GrCNF, the probability density function in Section 4.3 is first constructed as a prior distribution. Subsequently, the vector field \(_{}:(k,D) T\,(k,D)\) that generates the flow model \(F_{_{},T}\) is constructed using a neural network, wherein stepwise integration on the manifold occurs in accordance with (3). TheRCNF (Mathieu and Nickel (2020)) framework is used for loss function, integration and divergence calculations. In the integration steps, the ordinary differential equation (ODE) is solved using the ODE solver with orthogonal integration. The learnable parameters are updated using backpropagation to maximize the sum of the computed log-likelihood. In order to efficiently compute gradients, we introduce an adjoint method to calculate analytically the derivative of a manifold ODE, based on the results of Lou et al. (2020).

### Sampling Algorithm from Prior

We propose a sampling algorithm on \((k,D)\) derived from the Cayley transform, according to the results of Section 4.3. The algorithm of sampling from a distribution on \(p_{(k,D)}\) is described in Algorithm 1. Vec denotes the map of vertically concatenating matrices and converting them into a vector and is a \(D D\) identity matrix.

### Construction of Vector Field

We describe the method for constructing the vector field \(_{}\). The learnable parameters \(\) are feedforward neural networks, which accept the representative \((k,D)\) of \([](k,D)\) inputs and output a horizontal lift \(}_{}^{} T_{}^{} (k,D)\) of \(_{[]} T_{[]}(k,D)\) that satisfies (4). The structure of \(_{}\) is important because it directly determines the ability of the distribution to be represented. To address these geometric properties, the following specific input and intermediate layers are used to obtain a \((k)\)-invariant function value \(v_{}\). Figure 2 shows a conceptual diagram.

Input LayerThe input layer maps the input representative \(\) to a point \(}_{}^{} T_{}^{} (k,D)\). A horizontal projection \(:(k,D) T_{}^{} (k,D)^{(D-k) k}\) is constructed as an input layer:

\[()=-^{},\] (6)

where \(\) is the output of NN obtained with \(^{}\) as input. \(\) is well-defined because it is uniquely determined regardless of how the representative \([]\) is chosen.

Intermediate LayerTo model the dynamics over the horizontal space, intermediate layers based on neural networks are constructed. The layers are particularly important for the ability to represent the distribution, and various configurations are possible such as \(\) (CS) layer (Grathwohl et al. (2019)). The layer can be stacked multiply, and the overall power of expression increases with the number of layers. The input \(\) of the first intermediate layer is \(=\). The last intermediate layer (one layer before the output layer) must be constructed such that it exhibits a potential function value. This is to obtain the gradient for the potential function in the output layer.

Output LayerWe construct a gradient \(:(,(k,D)) T_{}^{} (k,D)\) as an output layer. In \(\), \(}_{}^{} T_{}^{} (k,D)\) is obtained by taking the gradient of the potential function \(v_{}\):

\[(v_{},)=P_{}^{} ((v_{},))P_{}^{}()=-(^{}),\] (7)

Figure 2: Conceptual diagram of the vector field calculation procedure. \(\) denotes the horizontal projection layer, \(\) denotes the multiple intermediate layers, and \(\) denotes the horizontal lift layer. The sequence of procedure represents that, given an input of \([]\), a horizontal vector \(}_{}^{}\), which is equivalent to determining \(_{[]}\) since \(}_{}^{}\) corresponds one-to-one with \(_{[]}\), is obtained as a result.

where \((v_{},)\) denotes automatic differentiation (Paszke et al. (2017)) \(}}{}\), \(P_{}^{}()\) denotes the projection of \(\) onto \(T_{}(k,D)\) and \(()=(+^{})/2\) is a symmetric part of \(\). \(\) is well-defined on \((k,D)\) by the horizontal lift (Absil et al. (2008)).

### ODE Solver with Orthogonal Integration

We present an ordinary differential equation (ODE) solver on \((k,D)\). Celledoni & Owren (2002) proposed an intrinsic ODE solver that was applicable to \((k,D)\) and did not assume an outer Euclidean space. This solver works on \((k,D)\), thus it must be reformulated into a solver suitable for \((k,D)\). We introduce a solver on \((k,D)\) via ODE operating on \(T_{}^{}(k,D)\), based on results from Section 4. This is an intrinsic approach regardless of whether the manifold has been embedded in a bigger space with a corresponding extension of the vector field. The ODE defined on \(T_{}^{}(k,D)\) is obtained as:

\[((t))}{dt}=_{} _{}^{-1}(_{}(t, (t)))(t)=_{}( (t)),\] (8)

where \(:[0,) T_{}^{}(k,D)\) is the curve on \(T_{}^{}(k,D)\), \(_{}\) denotes the horizontal retraction, \(_{}_{}^{-1}\) can be calculated using (55) and \(\) denotes the map of vertically concatenating matrices and converting them into a single vector. Further details on our ODE can be found in Appendix C.1.

### Loss Function

The total change in log-likelihood using GrCNF can be calculated using the following equation.

\[ p_{}((t_{1}))= p_{(k,D)}(F_{,t_{1}}^{-1}((t_{1} )))-_{t_{0}}^{t_{1}}(_ {}(t,(t)))dt,\] (9)

where \(F_{,t_{1}}\) is a flow. In this study, we defined the loss function \(\) for maximizing the log-likelihood: \(==- p_{}((t_{1} ))\) where \(\) denotes negative log-likelihood.

## 6 Experimental Results

In this section, we discuss the results of several experiments for validating GrCNF. Further details regarding each experimental condition, such as the architectures and hyperparameters used in training, can be found in Appendix C.3.

### Generation and Density Estimation on Artificial Textures

We first trained GrCNF on five different \((1,3)\) (1-dimensional subspace in \(^{3}\), that is, a line through the origin) data to visualize the model and the trained dynamics. The five datasets3 were 2 spirals, a swissroll, 2 circles, 2 sines, and Target. We represented \((1,3)\) with a sphere of radius 1 centered at the origin by mapping a 1-dimensional subspace to two points on the sphere (a point on the sphere and its antipodal point) for visualization.

ResultThe five types of probability densities transformed by GrCNF and the results of data generation are shown in Figure 3. The top, middle, and bottom rows in the figure present the correct data, the generated data with GrCNF when the left-most column is the prior distribution, and the probability density on a \((1,3)\) obtained by training when the leftmost column was the prior distribution, respectively. Regarding the probability density, a brighter region corresponds to a higher probability. As shown in the figure, GrCNF can generate high-quality samples that are sufficiently accurate to the distribution of the correct data. To investigate whether GrCNF learns the distribution on \((1,3)\), we generated antipodal points in Figure 3 with the trained GrCNF used in Figure 3. As shown in Figure 4, GrCNF generated exactly the same high quality samples as the original for the untrained antipodal points. This experimental result implies that the proposed flow accurately learns the distribution on a \((k,D)\) and that all the orthogonal transformed samples can be obtained with equal quality by only training with arbitrary representatives.

### Comparison with Conventional Methods for DW4 and LJ13

Comparative experiments were performed using DW4 and LJ13--the two systems presented by Kohler et al. (2020). These datasets were generated synthetically by sampling from their respective energy functions using Markov chain Monte Carlo (MCMC) methods. Both energy functions (DW4 and LJ13) are ideal for analyzing the advantages of various methods when they exist on data that are equivariant for rotation and reflection, respectively. We used the datasets generated by MCMC that were employed in Kohler et al. (2020). The orthonormalized data \(\) were used for this study by applying Gram-Schmidt's orthogonalization to each column of \(\) such that the \(D\)-dimensional data was a matrix with \(k\) orthonormal basis vectors (DW4: \(D=4,k=2\); LJ13: \(D=13,k=3\)).

To match the experiment conducted in Garcia Satorras et al. (2021), \(1,000\) validation and testing samples each were used for both datasets. For DW4 and LJ13, different numbers of training samples, i.e., \(\{10^{2},10^{3},10^{4},10^{5}\}\) and \(\{10,10^{2},10^{3},10^{4}\}\), respectively, were selected, and their performance for each amount of data was examined. The proposed approach was compared with the state-of-the-art E(\(n\)) equivariant flow (E-NF) presented by Garcia Satorras et al. (2021) and simple dynamics presented by Kohler et al. (2020). In addition, comparisons with graph normalizing flow (GNF), GNF with attention (GNF-att), and GNF with attention and data augmentation (GNF-att-aug) (data augmentation by rotation), which are non-equivariant variants of E-NF, were performed. All the reported values are averages of cross-validations (three runs). Otherwise, the network structures of all these conventional methods were the same as that used in Garcia Satorras et al. (2021).

ResultTable 1 presents the results of the cross-validated experiments (negative log-likelihood; NLL) for the test samples. The proposed GrCNF outperformed both the conventional non-equivariant models (GNF, GNF-att, and GNF-att-aug) and the conventional equivariant models (Simple dynamics, E-NF) in all data domains.

Figure 4: Results of the GrCNF transformation for the antipodal point of the prior in the middle panel of the Figure 3. The antipodal points are those that pass through the origin of the sphere and appear in the southern hemisphere (leftmost). It is evident that the antipodal points are generated with exactly the same quality as in the Figure 3 for the un-trained antipodal points.

Figure 3: Generated samples and probability densities using the GrCNF trained on each of the five distributions. (top) Ground truth data, (middle) Generated data with the GrCNF when the leftmost column represents the prior distribution, and (bottom) densities obtained via training.

### Comparison with Conventional Methods for QM9 Positional

A comparative experiment was performed using the QM9 Positional--a subset of the QM9 molecular dataset that considers only positional information. The purpose of this experiment was to evaluate the feasibility of generating a practical point cloud. The QM9 Positional comprises only molecules with 19 atoms/nodes, and each node has a 3-dimensional position vector associated with it. However, the likelihood of the molecules must be invariant to translation and rotation in 3-dimensional space; thus, the proposed model is suitable for this type of data. The dataset consisted of 13,831, 2501, and 1813 training, validation, and testing samples, respectively. In this experiment, we used evaluation metrics based on the NLL and Jensen-Shannon divergence (JSD; Lin (1991)), in accordance with the experiments conducted in Garcia Satorras et al. (2021). The JSD calculates the distance between the normalized histograms that are generated from the model and obtained from the training set, by creating a histogram of the relative distances among all the node pairs in each molecule.

GrCNF handles subspace data. Therefore, orthonormalized data \(\) was used only in the proposed GrCNF, as in \(^{}^{}\) s.t. \(\) is diagonal (Huang et al. (2015)), such that the \(k\)-dimensional point cloud data \(\) of \(N\) points \(N k\) matrix is a matrix with \(k\) orthonormal basis vectors (\(D=19\), \(k=3\)). However, a complete point cloud generating task, such as QM9 Positional, must also store a scale parameter \(}\) such that \(=}\). Therefore, GrCNF incorporates an additional architecture to estimate \(}\). In addition, the proposed GrCNF encounters difficulties in computing the NLL in distribution \(p_{}()\) of \(\); thus, a new loss function is required to address this. In this study, the evidence lower bound (ELBO), i.e., the lower bound of the log-likelihood \( p_{}()\), was maximized using a variational inference framework. This is equal to the minimization of \(-()\). Further details regarding ELBO can be found in Appendix C.2.

We compared our GrCNF with the GNF, GNF-att, GNF-att-aug, Simple dynamics, Kernel dynamics, and E-NF methods. The network structure and experimental conditions for all these conventional methods were identical to those used in Garcia Satorras et al. (2021). In all the experiments, the training was performed for 160 epochs. The JSD values were the averages of the last 10 epochs for all the models.

ResultTable 2 presents the NLL and JSD cross-validated against the test data. Although the proposed GrCNF could not directly compute the NLL in this problem setting, it outperformed all

    &  &  \\  \# of Samples & \(10^{2}\) & \(10^{3}\) & \(10^{4}\) & \(10^{5}\) & \(10\) & \(10^{2}\) & \(10^{3}\) & \(10^{4}\) \\ 
**GNF** & -2.30 & -7.04 & -7.19 & -7.93 & 6.77 & -0.76 & -4.26 & -12.43 \\
**GNF-att** & -2.02 & -4.13 & -5.25 & -6.74 & 6.91 & 1.40 & -6.81 & -12.05 \\
**GNF-att-aug** & -3.11 & -4.04 & -6.51 & -9.42 & 2.95 & -6.11 & -13.94 & -15.74 \\
**Simple dynamics** & -1.22 & -1.28 & -1.36 & -1.39 & -1.10 & -3.87 & -3.72 & -3.59 \\
**E-NF** & -0.54 & -9.89 & -12.15 & -15.29 & -12.86 & -15.75 & -31.51 & -32.83 \\
**GrCNF** & **-12.53** & **-13.74** & **-14.09** & **-16.07** & **-23.64** & **-44.24** & **-58.02** & **-58.71** \\   

Table 1: Negative log-likelihood comparison on the test partition of different methods for DW4 and LJ13 datasets for different amount of training samples averaged over 3 runs.

    & NLL & \(-\) & \(\) \\ 
**Simple dynamics** & 73.0 & - & 0.086 \\
**Kernel dynamics** & 38.6 & - & 0.029 \\
**GNF** & -00.9 & - & 0.011 \\
**GNF-att** & -26.6 & - & 0.007 \\
**GNF-att-aug** & -33.5 & - & 0.006 \\
**E-NF** & -70.2 & - & 0.006 \\
**GrCNF** & NLL \(\)**-85.3** & -85.3 & 0.005 \\   

Table 2: Results for the QM9 Positional dataset. (Left) Negative log-likelihood, \(-\) and JSD for the QM9 Positional dataset on the test data, and (Right) normalized histogram of relative distances between atoms for QM9 Positional and generated samples with GrCNF.

the other algorithms because the relation NLL \(-\)ELBO is true in general. With regard to the JSD, GrCNF achieved the best performance. Figure 2 presents the normalized histograms of the QM9 Positional molecular data and the data generated by GrCNF, in relation to the JSD of GrCNF. As shown, the histograms of the molecules generated by GrCNF are close to the histograms of the data set; thus, GrCNF can generate realistic molecules stably.

## 7 Conclusion

We proposed the concept of CNF on a Grassmann manifold (GrCNF) for stable shape generation. The proposed model is a generative model capable of handling subspace data; i.e., it is a CNF that considers the equivariance on the Stiefel manifold. Through suitable experiments, the ability of the proposed GrCNF to generate qualitatively in 1-dimensional subspace datasets in \(^{3}\) was confirmed. Further, GrCNF significantly outperformed existing normalizing flows methods in terms of the log-likelihood or ELBO for DW4, LJ13, and QM9 Positional. It was shown that GrCNF can be used to generate realistic data more stably.

Societal Impact and LimitationsOur work is concerned with accurately modeling the data topology; thus, we do not expect there to be any negative consequence in our application field. The proposed theory and implementation are valid in a remarkably general setting, although there are limitations that can be addressed in future works: **1.** The proposed method requires calculation of the Kronecker product at each ODE step (Section 5.4), which is computationally expensive. **2.** Our current paper does not include experiments on high-dimensional data. Therefore, future experiments should be conducted using higher dimensional data, such as point clouds that represent general object geometry. However, the results presented in this paper still provide numerous valuable insights in the field of stable shape generation.

Figure 5: Visualization results of generated samples in Section 6.3. We visualize Generated QM9 by GrCNF and E-NF, as well as the aligned data points from the QM9 dataset that are closest to the Generated QM9 samples. These visualizations illustrate that GrCNF is capable of generating realistic data points that align well with the existing dataset.