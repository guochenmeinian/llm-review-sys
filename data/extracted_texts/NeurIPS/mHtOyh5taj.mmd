# Adaptive Image Quality Assessment via Teaching

Large Multimodal Model to Compare

 Hanwei Zhu\({}^{*1}\)  Haoning Wu\({}^{*2}\)  Yixuan Li\({}^{1}\)  Zicheng Zhang\({}^{3}\)  Baoliang Chen\({}^{4}\)

**Lingyu Zhu\({}^{1}\)  Yuming Fang\({}^{5}\)  Guangtao Zhai\({}^{3}\)  Weisi Lin\({}^{2}\)  Shiqi Wang\({}^{1,6}\)**

\({}^{1}\) City University of Hong Kong

\({}^{2}\) Nanyang Technological University

\({}^{3}\) Shanghai Jiao Tong University

\({}^{4}\) South China Normal University

\({}^{5}\) Jiangxi University of Finance and Economics

\({}^{6}\) Shenzhen Research Institute, City University of Hong Kong

https://compare2score.github.io/

Equal contributionCorresponding author: shiqwang@cityu.edu.hk

###### Abstract

While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on _absolute_ quality rating, how to transfer reliable _relative_ quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce **Compare2Score**--an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score. Specifically, _during training_, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. _During inference_, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the **Compare2Score** effectively bridges **text-defined comparative levels** during training with converted **single image quality score** for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of **Compare2Score** but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.

## 1 Introduction

Image quality assessment (IQA) models aim to establish a quantitative mapping between digital visual images and human subjective evaluations, playing an indispensable role across various image processing and computer vision tasks . No-reference IQA (NR-IQA) [2; 3; 4; 5], which evaluate images without a reference, are particularly valuable for real-world applications. Recently, NR-IQA has experienced profound improvement through advanced deep neural networks (DNNs) [6; 7; 8; 9;10]. However, a primary challenge with current models [8; 11; 12] lies in their limited _cross-distortion_ generalization capability since the training and testing data contain significant distribution shift.

To improve the generalization capability of the NR-IQA, lots of advanced training techniques have been adopted, such as meta learning , domain adaptation , test-time adaption , and hard example mining . More capable foundation models, such as CLIP  and large multimodal models (LMMs) , are also proven to be effective in improving generalization ability. Despite these advancements, the gains from such techniques remain constrained due to the persistent **data challenge** inherent in IQA. Alternatively, expanding the IQA training datasets--both in terms of the number of images and the diversity of distortions--emerges as a scalable strategy to augment model robustness . This data scaling law has also been recognized as one of the key factors in building effective LMMs [18; 19; 20; 21; 22; 23]. As such, addressing how to effectively combine existing IQA datasets to meet the extensive data requirements of training LMMs is highly desirable.

As an early attempt of LMMs on IQA, Q-Align  proposed to combine different IQA datasets with _absolute_ quality ratings. While absolute ratings are widely used for collecting human opinions on different IQA datasets, it is non-trivial to directly fuse them for LMM training. This difficulty arises because each dataset has **different perceptual scales** owing to varying subjective testing methodologies. As shown in Fig. 1 (a), images with identical mean option score (MOS) (\(2.3\), all rescaled to range \(\)) from four datasets [24; 25; 26; 7] differ significantly in perceptual quality. As a result, despite clustering at the same rating level, these images from different datasets display distinctly different visual qualities (see Fig. 1 (b)). Therefore, simply scaling up the training data by mixing existing IQA datasets with rescaled MOS is fundamentally flawed. Instead, the _relative_ quality ranking (_e.g._, paired comparison) offers intrinsic simplicity and reliability over absolute quality rating . As shown in Fig. 1 (c), it is comparable to the images from the same IQA datasets as their MOSs originate from the same subjective user study, facilitating a more flexible combination of various IQA datasets. However, the key limitation of the paired comparison method is its impracticality in deriving individual image quality scores from \(\) comparisons when \(M\) is large. Furthermore, the lack of effective and efficient methods to convert _relative_ comparisons into quantitative _absolute_ ratings makes current comparison-based approaches difficult to apply to real-world scenarios.

To tackle these challenges, this paper leverages the flexibility and reliability of relative quality comparisons to introduce **Compare2Score**--an all-around LMM-based NR-IQA model, which is

Figure 1: Illustrations of the motivation of this work. **(a)** Images with identical rescaled MOS from various IQA datasets exhibit significant variations in perceptual quality. **(b)** Images that cluster at the same rating level from different IQA datasets display mismatches due to differing subjective testing methodologies. **(c)** By comparing MOSs within the same dataset, it facilitates the flexible combination of multiple IQA datasets.

designed to generate human-like qualitative comparisons and compute effective quality scores. Before delving into detail, we clearly highlight our main contributions as follows.

* **[A repurposed training dataset.]** We introduce a tailored approach to generate comparative instructions by comparing MOSs within each IQA dataset. This method categorizes image pairs into distinct comparative levels (inferior, worse, similar, better, superior) using the empirical rule, facilitating the flexible integration of diverse IQA datasets. This specific implementation effectively addresses the challenges posed by differing subjective testing methodologies and perceptual scales. It produces a comprehensive training dataset that enables the LMM to handle various distortion scenarios, resulting in a human-like **visual quality comparator**.
* **[An inference conversion strategy.]** We develop an **adaptive soft comparison scheme** that efficiently translates discrete comparative levels into continuous quality scores. Unlike traditional two-alternative forced choice (2AFC) methods, our approach calculates the likelihood that an input image is preferred over multiple anchor images. This probability is derived from a weighted summation of the softmax-transformed log probabilities across five comparative levels. Subsequently, the quality score of the input image is calculated through maximum a posteriori (MAP) estimation based on the resulting probability matrix.
* **[A state-of-the-art framework.]** We conduct extensive experiments to validate the effectiveness of teaching the relative quality ranking knowledge to LMM. The proposed model, namely **Compare2Score**, consistently outperforms state-of-the-art NR-IQA models on both synthetic and realistic distortions and shows enhanced generalization capability across different cross-distortion scenarios. Furthermore, we demonstrate that the probability matrix-based inference conversion significantly enhances the rating accuracy of **Compare2Score** and extends these improvements to zero-shot general-purpose LMMs.

## 2 Related Work

### NR-IQA Models

Regressing for NR-IQATraditional learning-to-regress NR-IQA models [28; 29; 30; 31] build effective quality-aware feature extractors rooted in theoretical principles, which are then mapped to quality scores through well-trained IQA regressors. In contrast, deep-learning-based IQA models [6; 9; 32; 33] exploit large volumes of IQA data to simultaneously refine DNNs for both feature extraction and quality regression. By using advanced training techniques, such as domain adaption , meta-learning , multi-task learning [34; 10], and contrastive learning , NR-IQA models show high correlation with the HVS. However, these models show limited cross-distortion ability. Additionally, these models typically produce quantitative quality scores, creating a significant gap from subjective user studies that prefer learning and assigning text-defined quality levels .

Ranking for NR-IQABeyond learning-to-regress schemes, many models address IQA through a relative quality ranking setting [36; 37; 38; 17; 10]. Liu _et al_.  synthesized a large IQA dataset labeled with distortion types and levels to train a Siamese network for precise image quality ranking. Zhang _et al_.  incorporated probabilistic quality preference for image pairs from diverse datasets to address inter-dataset incomparable concerns. LIQE utilized a pairwise learning-to-rank training strategy with both visual and textual inputs . Such ranking-based models mitigate the vulnerability towards task-agnostic information of regressing-based models, enabling more robust capabilities for different distortion scenarios [17; 10]. Nevertheless, the application of the learning-to-rank scheme for LMM is largely under-explored. As such, we present to leverage relative comparisons to develop an LMM-based NR-IQA model that produces qualitative comparison outcomes and translates the discreet comparative levels into continuous quality scores effectively.

### LMMs for IQA

Recently, many works have explored the capabilities of LMMs on IQA, covering both benchmarking [39; 40; 41; 42] and refining [39; 12; 43; 44; 45; 46]. Wu _et al_. laid the groundwork by examining and instructing of LMMs in low-level vision tasks, through the development of Q-Bench  and Q-Instruct , respectively. Wu _et al_. analyzed LMM's performance under various standardized prompting settings , validating the effectiveness of chain-of-thought prompting in IQA tasks. Co-Instruct  extended the low-level visual capability of the LMM to meet the requirement of multiple image inputs. Despite demonstrating success, the qualitative outputs of the above LMMs are hard to transfer to a quantitative score, which often plays an important role in computer vision tasks . Q-Align  made the first attempt by transferring the qualitative rating levels to the perceptual quality scores. However, as shown in Figs. 1 (a) & (b), it is impractical to expand the training dataset by simply mixing multiple IQA datasets with rescaled MOS. Therefore, we introduce a novel relative ranking strategy that allows for the seamless integration of existing IQA datasets into an expanded training set, which is then utilized to train an LMM-based visual quality comparator.

## 3 The Compare2Score Framework

In this section, we first describe the preliminaries of paired comparison for humans and LMMs, respectively. Subsequently, we introduce our methodological framework, which includes generating the training data with quantitatively comparative instructions (Sec. 3.2), the LMM-based visual quality comparator (Sec. 3.3), and the soft comparison method for quality score derivation (Sec. 3.4). Fig. 2 shows the training and testing diagrams of **Compare2Score** framework.

### Preliminaries

Paired Comparison for Humans.The paired comparison methodology in subjective testing involves three principal steps: the combination of images, the collection of human judgments, and the aggregation of these judgments into quality scores. In particular, given a set of images \(=\{(x^{(i)},q^{(i)})\}_{i=1}^{M}\) where \(x^{(i)}^{N}\) is the image and \(q^{(i)}\) represents the ground-truth quality score, the methodology requires comparing a total of \(\) pairs. We assume that a higher \(q^{(i)}\) indicates better perceptual quality. The outcomes of these comparisons are recorded in a count matrix \(C^{M M}\), where each entry records the number of times one image is preferred over another. The global ranking scores \(=\{^{(i)}\}_{i=1}^{M}\) can be computed by MAP estimation :

\[*{arg\,max}_{}(|C)+ p(),\; \;_{i}^{(i)}=0,\] (1)

where \(()\) denotes the log-likelihood function and \(p()\) is a prior on the scale values. Although effective , the key limitation of paired comparison is the exponential growth in the number of comparisons, which becomes labor-intensive and costly for large \(M\). Moreover, once the experiment concludes, incorporating new test images for quality score inference becomes almost infeasible.

Paired Comparison for LMMs.Inspired by the efficacy and reliability of paired comparison experiments with humans, we explore the feasibility of adapting this approach for LMMs. Accordingly, we adopt a similar pipeline to that used in subjective testing. The framework for training LMMs and predicting quality scores also involves three core steps: constructing instruction-response pairs, fine-tuning the LMM with such pairs, and inferring quality scores. To increase the efficiency and feasibility of the model, we propose an adaptive soft comparison approach by computing the probability of the test image \(x^{(i)}\) being preferred over \(m\) representative anchor images. The probability is

Figure 2: Training and inference phash of **Compare2Score**. **(a)** The LMM is fine-tuned with instruction-response pairs generated by comparing the MOSs from the same IQA dataset, allowing for a more flexible combination of various IQA datasets. **(b)** The trained visual quality comparator (_i.e._, LMM) is utilized to compute the likelihood of a test image being preferred over the anchor images, and then the quality score is derived using MAP estimation.

calculated by a weighted summation of the softmax of the log probabilities across five comparative levels. The outcome of the LMM is a probability matrix, \(P^{(m+1)(m+1)}\) where \(m M\). The quality score of the input image is then computed by MAP estimation with the same optimization problem as Eqn. (1).

### Training Dataset: Comparative Instruction-Response Pairs

To facilitate the reasonability of mixing different IQA datasets, we present to compare the visual quality of pairs of images within each IQA dataset. It allows a seamless combination of \(K\) established IQA databases for generating large-scale repurposed training dataset (_i.e._, instruction-response pairs). This process involves translating MOSs into discrete comparative levels. Utilizing the empirical rule , we define five levels of comparison: inferior, worse, similar, better, superior. The format of the instruction-response pairs is specified as follows:

USER: Compared with the first image <img1>, how is the quality of the second image <img2>?

ASSISTANT: The quality of the second image is [Level] to/than the first image.

Specifically, we randomly sampling \(n_{k}\) image pairs \(\{(x_{k}^{(i)},x_{k}^{(j)})\}_{i,j=1}^{n_{k}}\) from each database. For each pair \(\{(x^{(i)},x^{(j)})\}\), relative quality rankings are inferred based on MOS and its standard deviation. We assume the perceptual quality of each image \(x^{(i)}\) as a Gaussian distribution, characterized by mean \(q^{(i)}\) and standard deviation \(^{(i)}\), derived from subjective testing. Assuming independence in quality variability between images, the quality differential also follows a Gaussian distribution with mean \(q^{(ij)}=q^{(i)}-q^{(j)}\) and standard deviation \(^{(ij)}=)^{2}+(^{(j)})^{2}}\). This methodology, summarized in Eqn. (2), introduces significance thresholds at \(^{(ij)}\) and \( 2^{(ij)}\), effectively categorizing the quality differences into meaningful comparative levels. These thresholds function similarly to confidence intervals in statistical hypothesis testing, establishing a robust framework for accurately identifying significant perceptual differences between images.

### Structure: Multi-image LMM as Visual Quality Comparator

The visual quality comparator forms a central element within the **Compare2Score** framework and is tasked with predicting qualitative judgments for pairs of images. As shown in Fig. 3, the architecture incorporates the advanced mPLUG-Owl2 model , which comprises the image encoder (\(f_{}\)), the image abstraction (\(f_{}\)), and the large language model (LLM) decoder (\(g_{}\)). The process begins with the image encoder transforming each image into a visual embedding. This embedding is then dimensionally reduced by the image abstractor to facilitate the handling of multiple images, expressed as \(^{(i)}=f_{}(f_{}(x^{(i)}))\), where \(^{(i)}^{U}\) with \(U=65\), significantly less than LLaMA-2's maximum context length of \(2,048\). These compact visual embeddings are combined with textual embeddings \(^{V}\) from the text tokenizer and projected into a shared semantic space. The LLM decoder takes the aligned features and interleaved them to produce the qualitative output, formalized as \(=g_{}(<^{(i)},>)\), where \(<,>\) represents the feature alignment.

### Inference Conversion: Adaptive Soft Comparison

Soft Comparison Methodology.After training, the response is determined by selecting the token with the highest probability from the LLM decoder. This conventional method, while straightforward, may not fully exploit the nuanced capabilities of LMMs, as it relies solely on the most probable outcome and disregards other informative probabilities. To overcome this limitation, we propose a soft comparison method that integrates the logits of all five comparative tokens \(=\{t_{i}|_{i=1}^{5}\}=\{,,,,\}\). The probability of each token is achieved by the softmax function, expressed as \(p_{e^{t_{i}}}=e^{t_{i}}/_{j=1}^{5}e^{t_{j}}\), where \(p_{e^{t_{i}}}\) indicate the probability of \(i\)-th token. Moreover, to enhance the efficiency and feasibility of the model, we do not compare the test image against every image in the dataset. Instead, we identify a smaller set of anchor images from the training set, denoted as \(=\{a^{(n)}\}_{n=1}^{m}\), where \(a^{(n)}\) represents the \(n\)-th anchor image. As a result, the probability of the test image being preferred over the anchor images is computed by the weighted summation:

\[P_{e^{t}}(a^{(n)},x^{(m+1)})=_{i=1}^{5}w_{i}p_{e^{t_{i}}}(a^{(n)},x^{(m+1)} ),\,n=1 m,\] (3)

where \(w_{i}\) are predefined weights \(\{w_{i}|_{i=1}^{5}\}=\{0,0.25,0.5,0.75,1\}\), facilitating nuanced interpretation and use of the comparative levels.

Anchor Image Selection.We initially partition the IQA dataset into \(\) quality intervals, represented as \(=_{i=1}^{}(i)\). Our objective is to identify \(\) representative images from each quality interval, characterized by minimal variability in their MOS scores, enhancing the consistency of our experimental dataset. Images with high variability in human ratings are deemed less suitable for evaluating the performance of LMMs due to the potential introduction of noise and biases. For each interval \((i)\), we aim to select a subset \((i)(i)\), where the size of \((i)\) is \(\). This selection criterion is formalized through the following optimization problem:

\[(i)=*{arg\,min}_{(i), ||=}_{x}(x)^{2},\] (4)

where \((x)^{2}\) denotes the variance of the MOS score for image \(x\), serving as a quantitative measure of rating consistency. As such, the full set of anchor images can be achieved by \(=_{i=1}^{}(i)\)

Probability Matrix Construction.Based on the selected anchor images and visual quality comparator, we first construct probability matrix \(P_{a}^{m m}\) for the anchor images as follows:

\[P_{a}=P(a^{(1)},a^{(1)})&P(a^{(1)},a^{(2)})&&P(a^{(1)},a^ {(m)})\\ P(a^{(2)},a^{(1)})&P(a^{(2)},a^{(2)})&&P(a^{(2)},a^{(m)})\\ &&&\\ P(a^{(m)},a^{(1)})&P(a^{(m)},a^{(2)})&&P(a^{(m)},a^{(m)})\] (5)

Notably, each element \(P(a^{(i)},a^{(j)})=1-P(a^{(j)},a^{(i)})\) and \(P(a^{(i)},a^{(i)})=0.5\). Then, the test image \(x^{(m+1)}\) is compared with all anchor images. We use \(b=P(a^{(1)},x^{(m+1)}),P(a^{(2)},x^{(m+1)}),,P(a^{(m)},x^ {(m+1)})\) to denote the resultant vector. Therefore, we finally form the complete probability matrix \(P^{(m+1)(m+1)}\) for the anchor and test images as \(P=P_{a}&b\\ (1-b)&0.5\).

Quality Score Estimation.Once obtaining the probability matrix, we compute the quality scores using MAP estimation under Thurstone's Case V model . It is expressed as a convex optimization

Figure 3: Architecture of the proposed **Compare2Score**. Images are initially processed by an image encoder, followed by token reduction through an abstractor module. The aligned textual and visual embedding are interleaved and processed by the large language model (LLM) decoder to generate precise qualitative comparative levels for paired comparisons.

problem :

\[*{arg\,max}_{}P_{ij}((^{(i)}-^{(j)}))- _{i}^{(i)}}{2},_{i}^{(i)}=0,\] (6)

where \(()\) is the standard Normal cumulative distribution function, and \(^{(m+1)}\) represents the quality score of the test image.

## 4 Experiments

### Experimental Setups

IQA Datasets.We conduct comprehensive experiments across six standard IQA datasets. These datasets are categorized based on the type of distortions they contain: synthetic distortions are featured in LIVE , CSIQ , and KADID-10k ; realistic distortions are present in BID , LIVE Challenge (denoted as CLIVE) , and KonIQ-10k . More details regarding these IQA datasets can be found in the Appendix A.2. For our experiments, we utilize the ten splits provided by LIQE3, allocating \(70\%\) of images from each dataset for training, \(10\%\) for validation, and the remaining \(20\%\) for testing. We combine the training and validation sets in our experiments since **Compare2Score** are evaluated at the last optimization iteration without any fine-tuning of the model parameters [12; 46]. For datasets with synthetic distortions, it strictly maintains content independence by splitting datasets using reference images . The median of the Spearman's rank correlation coeff icent (SRCC) and Pearson linear correlation coefficient (PLCC) across the ten splits are reported in the tables.

Implementation Details.**Compare2Score** utilizes the advanced mPLUG-Owl2 model  for its architecture, leveraging a pre-trained CLIP-ViT-L14 as the vision encoder  and LLaMA2-7B  as the LLM decoder. To train the model, we generate \(180,000\) image pairs and optimize the whole architecture with the GPT loss , which computes cross-entropy between the predicted logits and ground-truth labels. Training is conducted with a batch size of 64 across all datasets, a fixed learning rate of \(2 10^{-5}\), and spans two epochs. This process requires seven NVIDIA A40 GPUs to meet the computational load. During inference, a single NVIDIA RTX3090 GPU is sufficient for executing the soft comparison (Sec. 3.4). Furthermore, to obtain the anchor images, we divide the training set of the KonIQ-10k into five (\(=5\)) quality intervals based on their MOSs , from which we select one (\(=1\)) representative anchor image per interval using Eqn. (4).

Baselines.We compare the performance of the proposed **Compare2Score** with the following state-of-the-art methods, which include (1) three opinion-unaware NR-IQA models: NIQE , ILNIQE , and Ma19 ; (2) six learning-to-regress NR-IQA models: PaQ2PiQ , KonCept , MUSIQ , DBCNN , HyperIQA , and TreS ; (3) two learning-to-rank NR-IQA models: UNIQUE  and LIQE ; (4) one LMM-based NR-IQA model: Q-Align . All methods are compared with the same testing sets across ten splits. The UNIQE, LIQE, and Q-Align are jointly trained on the above six datasets, respectively. The remaining methods are separately trained on each individual dataset if necessary.

### Main Results

Performance under Intra-Dataset Setting.Table 1 shows the results of media SRCC and PLCC across ten sessions. It is clear that **Compare2Score** outperforms the competing methods on both synthetic and realistic distortions, demonstrating the reliability of the paired comparison strategy can be smoothly extended to the LMM. While Q-Align  is another LMM-based model, it presents inferior performance on the synthetically distorted datasets [53; 24; 26]. The main reason may be the perceptual scale ambiguity across different IQA datasets. The pairwise learning-to-rank approach employed by UNIQUE  and LIQE  achieves competitive performance against models [4; 8; 11] trained individually, which further validates the effectiveness of using relative ranking information to mix different IQA datasets. Additionally, the opinion-unaware models exhibit subpar performance on the realistic distortions, suffering the potential overfitting issue to the traditional synthetic distortions [53; 24]. Furthermore, though the learning-to-regress models are 

[MISSING_PAGE_FAIL:8]

Performance of Prediction Accuracy.We further compare the prediction accuracy of paired comparison results of **Compare2Score** to five open-source LMMs [19; 21; 20; 22; 46]. As shown in Table 4, **Compare2Score** significantly surpasses these advanced open-source LMMs, providing high accuracy of quantitative outputs. Notably, Co-Instruct achieves competitive accuracy across the six IQA datasets, benefiting from a specialized visual quality comparison training corpus. The models (_e.g_., IDEFICS2, LLaVA-1.5, mPLUG-Owl2, and XComposer-VL-2) rely on high-level instruction-tuning datasets showing poor performance in terms of prediction accuracy, suggesting an inferior quality comparison capability of these LMMs.

### Ablation Studies

Impact of the Source of Anchor Images.Although KonIQ-10k  serves as the default source for anchor images, we demonstrate the robustness of our results across diverse sources of anchor images. Utilizing the same anchor image selection strategy outlined in Eqn. (4), we selected anchor images from three distinct datasets: KADID-10k , featuring synthetic distortions; KonIQ-10k , with realistic distortions; and AGIQA-3K , containing generative distortions. As shown in Table 5, **Compare2Score** consistently shows superior performance across all IQA datasets, showing remarkable robustness to the varying types of distortions of the anchor images. The anchor images selected from the three datasets are shown in Appendix A.5.

Impact of the Anchor Selection MethodsTo evaluate the efficacy of the proposed anchor image selection method (referring to Eqn. (4)), We compare the proposed minimum variance anchor image selection method to the maximum variance and random selection methods. The results are shown in Table 6, from which we can observe that **Compare2Score** achieves the best result among all the testing IQA datasets. This improvement indicates that selecting anchor images with low variability in human ratings is crucial, as high variability tends to introduce noise and biases, compromising the effectiveness of LMMs in performance evaluations. In addition. the other anchor selection methods also demonstrate competitive performance compared with the state-of-the-art NR-IQA model in Table 1, which further verifies the effectiveness of the proposed **Compare2Score** framework.

   Method & LIVE  & CSIQ  & KADID-10k  & BID  & CLIVE  & KonIQ-10k  \\  IDEFICS2  & 0.453 & 0.546 & 0.521 & 0.566 & 0.407 & 0.687 \\ LLaVA-1.5  & 0.170 & 0.544 & 0.600 & 0.579 & 0.074 & 0.455 \\ mPLUG-Owl2  & 0.484 & 0.394 & 0.302 & 0.613 & 0.407 & 0.273 \\ XComposer-VL-2  & 0.045 & 0.662 & 0.672 & 0.648 & 0.067 & 0.059 \\ Co-Instruct  & 0.672 & 0.426 & 0.391 & 0.695 & 0.718 & 0.849 \\ 
**Compare2Score** & **0.849** & **0.720** & **0.870** & **0.861** & **0.788** & **0.858** \\   

Table 4: Performance comparison in terms of prediction accuracy on six IQA datasets. The best results are highlighted in boldface.

Impact of the Number of Anchor images.The number of anchor images within each quality interval (\(\) in Eqn. (4)) crucially affects the efficacy of **Compare2Score**. We systematically explore the influence of \(\) and present the SRCC results and running times in Fig. 4. Notably, all experiments were carried out on the same testing platform equipped with an NVIDIA RTX3090 GPU. The results illustrated in Fig. 4 indicate that increasing the number of anchor images does not enhance performance across the evaluated IQA datasets. \(=1\) suffices for achieving promising and state-of-the-art performance. In addition, it is expected that the average running time (red line in Fig. 4) linearly increases as \(\) becomes large. As a result, \(=1\) has been set as the optimal configuration to achieve a practical balance between model efficiency and computational expense.

## 5 Conclusion and Discussion

In this paper, we introduced **Compare2Score**, a novel NR-IQA model utilizing LMM to bridge the gap between discrete comparative levels and continuous quality scores. By using the robust capabilities of LMMs to interpret and integrate complex textual and visual inputs, our model excels in translating scaled-up comparative instructions into reliable, human-like quality assessments. We propose an innovative soft comparison method that effectively and efficiently converts discrete textual responses to continuous quality scores. Extensive validation on standard IQA datasets demonstrates that the proposed model significantly outperforms existing NR-IQA models across different distortion scenarios. Moreover, our probability-matrix-based improves both our model and general-purpose LMMs, showcasing the broad applicability and intrinsic effectiveness of our methods.

Limitation.Despite promising, our study has several limitations that highlight areas for future research. While the soft comparison method is effective, it involves computational complexities that may not scale linearly with the increase in the number of images and comparisons. In addition, though LMM provides advanced capabilities in generating human-like quality assessments, the interpretability of this model remains limited. As such, exploring more efficient algorithms and enhancing the interpretability of the LMM is crucial for broader acceptance and trustworthiness in critical applications.