# Federated Learning over Connected Modes

Dennis Grinwald\({}^{1,2}\), Philipp Wiesner\({}^{2}\), Shinichi Nakajima\({}^{1,2,3}\)

\({}^{1}\)BIFOLD, \({}^{2}\)TU Berlin, \({}^{3}\)RIKEN Center for AIP

{dennis.grinwald, wiesner, nakajima}@tu-berlin.de

###### Abstract

Statistical heterogeneity in federated learning poses two major challenges: slow global training due to conflicting gradient signals, and the need of personalization for local distributions. In this work, we tackle both challenges by leveraging recent advances in _linear mode connectivity_ -- identifying a linearly connected low-loss region in the parameter space of neural networks, which we call solution simplex. We propose federated learning over connected modes (Floco), where clients are assigned local subregions in this simplex based on their gradient signals, and together learn the shared global solution simplex. This allows personalization of the client models to fit their local distributions within the degrees of freedom in the solution simplex and homogenizes the update signals for the global simplex training. Our experiments show that Floco accelerates the global training process, and significantly improves the local accuracy with minimal computational overhead in cross-silo federated learning settings.

## 1 Introduction

Federated learning (FL)  is a decentralized machine learning paradigm that facilitates collaborative model training across distributed devices while preserving data privacy. However, in typical real applications, statistical heterogeneity--non-identically and independently distributed (non-IID) data distributions at clients--makes it difficult to train well-performing models. To tackle this difficulty, various methods have been proposed, e.g., personalized FL , clustered FL , advanced client selection strategies , robust aggregation , regularization strategies , and federated meta- and multi-task learning approaches [7; 8]. These methods aim either at training a global model that performs well on the global distribution , or, as it is common in personalized FL, at training multiple client-dependent models each of which performs well on its local distribution . These two aims often pose a trade-off--a model that shows better local performance tends to suffer from worse global performance, and vice versa. In this work, we aim to develop a FL method that improves local performance compared to state-of-the art methods without sacrificing global performance.

Our approach leverages recent findings on _mode connectivity_[11; 12; 13]--the existence of low-loss paths in the parameter space between independently trained neural networks--and its applications . These works show that minima for the same task are typically connected by simple low-loss curves, and that this connectivity benefits training for multi-task and continual learning. In particular, the authors show that embracing mode connectivity between models improves accuracy on each task and remedies the risk of catastrophic forgetting.

In this paper, we leverage such effects, and propose federated learning over connected modes (Floco), where the clients share and together train a _solution simplex_--a linearly connected low-loss region in the parameter space. Specifically, Floco represents clients as points within the standard simplex based on the similarity between their gradients, and assigns each client a specific subregion of the simplex. Clients then participate in FL by sampling different models within their assigned subregions and sending back the gradient information to update the vertices of the global solution simplex (seeFig.1). This method facilitates collaborative training through the common solution simplex, while allowing for client-specific personalization according to their local data distributions.

Our experiments show that Floco outperforms common FL baselines (FedAvg , FedProx ) and state-of-the-art personalized FL approaches (FedRoD , APFL , Ditto , FedPer ) on both local and global test metrics--without introducing significant computational overhead--in cross-silo FL settings. We also demonstrate additional benefits of Floco, including better uncertainty estimation, improved worst client performance, and smaller divergence of gradient signals.

Our main contributions are summarized as follows:

* We propose Floco, a novel FL method that trains a solution simplex for mitigating the statistical heterogeneity of clients, and demonstrate its state-of-the-art performance for local personalized FL.
* We propose a simple projection method to express clients as points in the standard simplex based on the gradient signals, and establish a procedure of subregion assignments.
* We conduct experimental evaluations on semi-artificial and real-world FL benchmarks with detailed analyses of the behavior of Floco, which give insights into how the mechanism improves performance compared to the baselines.

We provide implementations of Floco in the FL frameworks FL-bench  and Flower . Our code is publicly available: https://github.com/dennis-grinwald/floco.

## 2 Background

In this section, we briefly explain the concepts behind federated learning and mode connectivity, which form the backbone of our approach. The symbols that we use throughout the paper are listed in Table 5 in Appendix.

Figure 1: Floco expresses each client as a point (\(\) in the top-center plot) by projecting the gradient signals onto the simplex, so that similar clients are close to each other. In each communication round, each client uniformly samples points in the neighborhood of their projected point (top-right plot), and jointly train the solution simplex. The lower row shows the resulting test loss on the solution simplex, where the loss for the global distribution (left) is uniformly small, while the losses for individual local distributions (center for client 1 and right for client 2) are small around their projected points.

### Federated Learning

Assume a federated system where the server has a global model \(g_{0}\) and the \(K\) clients have their local models \(\{g_{k}\}_{k=1}^{K}\). FL aims to obtain the best performing models \(\{g_{k}^{*}\}_{k=0}^{K}\) such that

\[g_{0}^{*} =_{g_{0}}F^{*}(g_{0})_{k=1}^{K}p(k )F_{k}^{*}(g_{0}),\] (1) \[g_{k}^{*} =_{g_{k}}F_{k}^{*}(g_{k})\;\;\;\;k =1,,K,\] (2) \[\;F_{k}^{*}(g)=_{(,y) p_{k}(,y)}[f(g,(,y))].\]

Here, \(p(k)\) is the normalized population of data samples for the \(k\)-th client, \(p_{k}(,y)\) is the data distribution for the client \(k\), and \(f(g,(,y))\) is the loss, e.g., cross-entropy, of the model \(g\) on a sample \((,y)^{I}\{1,,L\}\), where \(I\) is the dimension of an input data sample. _Global_ and _personalized_ FL aim to approximate \(g_{0}^{*}\) and \(\{g_{k}^{*}\}_{k=1}^{K}\), respectively, by using the training data \(=\{_{k}\}_{k=1}^{K}\) observed by the clients. Throughout the paper, we assume that all models are neural networks (NNs) \(=g_{k}(;_{k})\) with the same architecture, and represent the model \(g_{k}\) with its NN parameters \(_{k}^{D}\), i.e., we hereafter represent \(g_{k}(;_{k})\) by \(_{k}\) and thus denote, e.g., \(F_{k}^{*}(g_{k})\) by \(F_{k}^{*}(_{k})\). Let \(N=_{k=1}^{K}N_{k}\) be the total number of samples, where \(N_{k}=|_{k}|\).

For the independent and identically distributed (IID) data setting, i.e., \(p_{k}(x,y)=p(x,y), k=1,,K\), the global and personalized FL aim for the same goal, and the minimum loss solution for the given training data is

\[}_{0} =}_{k}=_{}F() _{k=1}^{K}}{N}F_{k}(),\] (3) \[\;F_{k}()=}_{(,y) _{k}}f(,(,y)).\]

In this setting, Federated Averaging (FedAvg) ,

\[_{0}^{t+1}=_{0}^{t}+_{k^{t}}}{N} _{k}^{t+1}t=1,,T,\] (4)

is known to converge to \(}_{0}\), and thus solve Eq. (3). Here, \(^{t}\) is the set of clients that participate the \(t\)-th communication round, and \(_{k}^{t+1}=_{k}^{t+1}-_{0}^{t}\) is the update after \(T^{}\) steps of the local gradient descent,

\[}^{t^{}+1}=}^{t^{}}-F_{k} (}^{t^{}}),\;\;t^{}=1,,T^{},\] (5)

where \(}^{0}=_{0}^{t},}^{T^{}}=_{k}^{t+1}\), and \(\) is the step size. FedAvg has been further enhanced with, e.g., proximity regularization , auxiliary data , and ensembling .

On the other hand, in the more realistic non-IID setting, where \(_{0}^{*}_{k}^{*}\), FedAvg and its variants suffer from slow convergence and poor local performance . To address such challenges, Ditto  was proposed for personalized FL, i.e., to approximate the best local models \(\{_{k}^{*}\}_{k=1}^{K}\). Ditto has two training phases: it first trains the global model \(}_{0}\) by FedAvg, then trains the local models with proximity regularization to \(}_{0}\), i.e.,

\[}_{k} =\!_{_{k}}\!_{k}(_ {k},}_{0}) F_{k}(_{k})+\|_ {k}-}_{0}\|_{2}^{2},\]

where \(\) controls the divergence from the global model. Ditto has been shown to outperform many other non-IID FL methods, including the client clustering method HYPCLUSTER, adaptive federated learning (APFL), which interpolates between a global and local models , Loopless Local SGD (L2SGD), which applies global and local model average regularization , and MOCHA , which fits task-specific models through a multi-task objective.

### Mode Connectivity and Solution Simplex

Freeman and Bruna (2017) , as well as Garipov et al. (2018) , discovered the mode connectivity in the NN parameter space--the existence of simple regions with low training loss between two well-trained models from different initializations. Nagarajan and Kolter (2019)  showed that the path is linear when the models are trained from the same initialization, but with different ordering of training data. Frankle et al. (2020)  showed that the same pre-trained models stay linearly-connected after fine-tuning with gradient noise or different data ordering.

Benton et al. (2021)  found that the low loss connection is not necessarily in 1D, and  showed that a simplex,

\[(\{_{m}\})=\{_{}(\{ _{m}\})=_{m=1}^{M+1}_{m}_{m}; ^{M}\},\] (6)

within which any point has a small loss, can be trained from randomly initialized endpoints.

Here, \(\{_{m}^{D}\}_{m=1}^{M+1}\) are the endpoints or vertices of the simplex, and \(^{M}=\{^{M+1};\|\|_{1}=1\}\) denotes the \(M\)-dimensional standard simplex. This simplex learning is performed by finding the endpoints that (approximately) minimize

\[_{(,y) p(,y)}_{ _{(\{_{m}\})}}[f( ,(,y))],\] (7)

where \(_{}\) denotes the uniform distribution on a set \(\). During training, one model realization \(w_{}\) from the simplex gets sampled and its gradient update wrt. the loss, e.g. cross-entropy, gets backpropagated to the simplex endpoints \(\{_{m}\}_{m=1}^{M+1}\).

## 3 Proposed Method

In this section, we introduce our approach, where the mode connectivity is leveraged for collaborative training between personalized client models.

### Federated Learning over Connected Modes (Floco)

The main idea behind Floco is to assign subregions of the solution simplex (6) to clients in such a way that similar clients train neighboring (and overlapped) regions, while enforcing (linear) connectivity to all other client's subregions. The connectivity constraint systematically regularizes client training and allows for efficient collaboration between them.

The subregion assignments need to reflect the similarity between the clients. To this end, Floco expresses each client as a point in the standard simplex, based on the gradient update signals. Specifically, it applies the _Euclidean projection onto the positive simplex_ with the Riesz s-Energy regularization , which gives well spreaded projections that preserve the similarity between the client's gradient signals as much as possible. Once the clients are projected onto the standard simplex as \(\{_{k}^{M}\}_{k=1}^{K}\), we assign the L1-ball with radius \(\) around \(_{k}\), i.e., \(_{k}=\{^{M};\|- _{k}\|_{1}\}\), to the \(k\)-th client. Note that the gradient update signals are informative for the subregion assignment only after the (global) model is trained to some extent. Therefore, the subregion assignment is performed after \(\) FL rounds are performed. Before the assignment, i.e., \(t\), all clients train the whole standard simplex \(_{k}=^{M}, k\), which corresponds to a simplex learning version of FedAvg.

Starting from randomly initialized simplex endpoints \(\{_{m}\}_{m=1}^{M+1}\), Floco performs the following steps for each participating client \(k^{t}\) in each communication round \(t\):

1. The server sends the current endpoints \(\{_{m}^{t}\}_{m=1}^{M+1}\) to the client \(k\).
2. The client \(k\) performs simplex learning only on the assigned subregion \(_{k}\) as a local update.
3. The client sends the local update of the endpoints to the server.

This way, Floco is expected to learn the global solution simplex \(\{_{};^{M}\}\), while allowing personalization to local client distributions within the solution simplex. Algorithm 1 shows the main steps.

Although the simplex learning can be applied to all parameters, our preliminary experiment showed that applying simplex learning only of the parameters in the last fully-connected layer (while point-estimating the other parameters) is sufficient. Therefore, our Floco only applies the simplex learning to the last layer, which gives other benefits including applicability to fine-tuning of pre-trained models, and significant reduction of computational and communication costs, as shown in Section 4.3.

Below, we describe detailed procedures of client projection, local and global updates in the communication rounds, and inference in the test time.

### Client Gradient Projection onto Standard Simplex

We explain how to obtain the representations \(\{_{k}^{M}\}\) of the clients in the standard simplex such that similar clients are located close to each other, while all clients are well-spread across the simplex.

At communication round \(t=\), Floco uses the gradient updates of the endpoints \(\{_{m,k}^{}\}_{m=1}^{M+1}\) as a representation of the client \(k\). We concatenate the gradients for the \(M+1\) endpoints into a \(((M+1) D)\)-dimensional vector, and apply the PCA projection onto the \(M\) dimensional space, yielding \(_{k}^{M}\) as a low dimensional representation. To project \(\{_{k}\}\) onto the standard simplex \(^{M}\), we solve the following minimization problem:

\[_{z>0} _{i,j}_{i}(z)-_{j}(z)\|_{2}^{2}},\] (8) subject to: \[}_{k}(z)=*{argim}_{_{k}}{z}^{M-1}}\|_{k}-_{k}\|_{2}^{2}.\] (9)

The objective function in Eq. (8) is the Riesz s-Energy , a generalization of potential energy of multiple particles in a physical space, and therefore its minimizer correponds to the state where particles are well spread across the space. The minimization in the constraint (9) corresponds to the _Euclidean projection onto the positive simplex_, which forces \(\{_{k}\}\) to keep the locations of the PCA projections \(\{_{k}\}\) of the clients. Fortunately, this minimization problem (for a fixed \(z\)) is convex, and can be efficiently solved (see Appendix A). We solve the main problem (8) by computing \(}_{k}(z)\) on a 1D grid in \(z\) with the interval \(0.001\), and set the representations of the clients to \(_{k}=}_{k}()}{}\), where \(\) is the minimizer of Eq. (8).

### Communication Round: Local and Global Updates

In the \(t\)-th communication round, the server sends the current endpoints \(\{_{m}^{t}\}_{m=1}^{M+1}\) to the participating clients \(^{t}\). Then, each client \(k^{t}\) draws one sample per mini-batch from the uniform distribution \(=\{_{b}\}_{b=1}^{B}_{_{k}}\) on the assigned subregion and applies \(T^{}\) local updates,

\[}_{m}^{t^{}+1}=}_{m}^{t^{}}- _{m}F_{k}(_{}),\] (10)

to the endpoints with \(\) sequentially chosen from \(\).1 Here \(}_{m}^{0}=_{m}^{t},}_{m}^{T^{ }}=_{m,k}^{t+1}\). The local updates \(\{_{m,k}^{t+1}=_{m,k}^{t+1}-_{m}^{t}\}_ {m=1}^{M+1}\) are sent back to the server, which updates the endpoints as

\[_{m}^{t+1}=_{m}^{t}+_{k^{t}}}{N}_{m,k}^{t+1}.\] (11)As explained in Section 3.1, the client subregions are initially set to the whole simplex \(^{M}\) before the subregion assignment is performed at \(t=\), which corresponds to a straightforward application of the simplex learning to FedAvg. After the subregion assignment, Floco uses the degrees of freedom within the solution simplex to personalize clients models.

### Floco\({}^{+}\)

We can further enhance the personalized FL performance of Floco by additionally fine-tuning a local model as in Ditto . In this extension, called Floco\({}^{+}\), each client personalizes the global endpoints \(\{}_{m}^{0}=_{m}\}_{m=1}^{M}\) by local gradient descent to minimize the Ditto objective, i.e.,

\[\{}_{m}^{k}\}=_{\{_{m} \}}_{k}(\{_{m}\},\{}_{m}^{0}\})\] \[_{_{_{x_{k }}}}[F_{k}(_{}(\{_{m}\}))]+_{m=1}^{M+1}\|_{m}-}_{m}^{0}\|_{2}^{2}.\]

### Inference

With the trained endpoints \(\{}_{m}=_{m}^{T}\}_{m=1}^{M+1}\), we simply use \(_{}_{0}}(\{}_{m}\}_{m=1}^{M+1})\) as the global model, where \(}_{0}=_{M+1}\) with \(_{D}\) denoting the \(D\)-dimensional all one vector. For local models, we use \(\{_{}_{k}}(\{}_{m}\}_{m=1}^{M+1 })\}_{k=1}^{K}\) where \(}_{k}=_{k}\). For Floco\({}^{+}\), we fine-tune the corresponding subspace regions \(_{z_{k}}\) for \(E\) local epochs.

## 4 Experiments

In this section, we experimentally show the advantages of Floco and Floco\({}^{+}\) over the baselines.

### Experimental Setting

Datasets and models.To evaluate our method, we perform image classification on the CIFAR-10  and FEMNIST  datasets. For CIFAR-10, we train a CNN (CifarCNN) from scratch, following , and fine-tune a ResNet-18  pre-trained on ImageNet , as in . For FEMNIST, we train a CNN (FemnistCNN) from scratch, as in , and fine-tune a SqueezeNet  pre-trained on ImageNet, following . We provide a table with the training hyperparameters that we use for each dataset/model setting in Appendix B.

Data heterogeneity for non-FL benchmarks.The FEMNIST dataset is an FL benchmark based on real data, where client heterogeneity is inherently embedded in the dataset. For CIFAR-10, we simulate statistical heterogeneity by two partitioning procedures. The first procedure by  partitions clients in equally sized groups and assigns each group a set of primary classes. Every client gets \(q\) % of its data from its group's primary classes and \((100-q)\) % from the remaining classes. We apply this method with \(q=80\) for five groups and refer to this split as _5-Fold_. For example, in CIFAR-10 _5-Fold_, 20 % of the clients get assigned 80 % samples from classes 1-2 and 20 % from classes 3-10. The second procedure, inspired by  and , draws the multinomial parameters of the client distributions \(p_{k}(y)=(y;_{k})\) from Dirichlet, i.e., \(_{k}_{L}()\), where \(\) is the concentration parameter controlling the sparsity and heterogeneity--\(\) concentrates the mass to the uniform distribution (and thus homogeneous), while small \(0<<1\) generates sparse and heterogeneous non-IID client distributions.

Baseline methods.Besides FedAvg  and FedProx  for global FL, we chose FedRoD , APFL , Ditto , and FedPer  as state-of-the-art personalized FL baselines.

Floco Hyperparameters.For CifarCNN on the simulated non-IID splits Dir(0.3)/Five-Fold, we set \(=250,M=20/10,=0.1\). For FemnistCNN on FEMNIST we set \(=250,M=10,=0.5\). For pre-trained ResNet-18 on the simulated non-IID splits Dir(0.3)/Five-Fold we set \(=50,M=20/10,=0.1\) and for the pre-trained SqueezeNet on FEMNIST we set \(=250,M=3,=0.5\). We found those settings work well in our preliminary experiments, and conducted ablation study with other parameter settings in Appendix D. For the baselines, we follow the recommended parameter settings by the authors, which are detailed in Appendix B.

Evaluation criteria.For the performance evaluation, we adopt two metrics, the test accuracy measured after the last communication round (ACC) and the time-to-best-accuracy (TTA), each for evaluating the global and local FL performance. ACC is the last test accuracy over \(T\) communication rounds, i.e, \((T)=}}_{i=1}^{N_{}}(y_{i}=\,g(_{i};}^{T}))\), where \(()\) is the indicator function that equals to 1 if the event is true and 0 otherwise. TTA evaluates the number of communication rounds needed to achieve the best baseline (FedAvg and Ditto in this paper) test accuracy, i.e., \(_{}(T)\). We report TTA improvement, i.e. the TTA of the baseline, e.g. FedAvg, divided by the TTA of the benchmarked method, e.g. Floco. Moreover, we report the expected-calibration-error (ECE) , a common measure that evaluates the quality of uncertainty estimation of a trained model, for the last communication round.

### Results

Table 1 and 2 summarize the main experimental results, where Floco and Floco\({}^{+}\) consistently outperform the baselines across the different experiments in terms of global (red) and local (blue) test accuracy, as well as test ECE. The global and local test metrics are measured after the last communication round and averaged over 5 different seed runs. The best performances are highlighted in bold, while the underlined entries indicate the settings that did not converge properly. Note that the global test performances of FedAvg and Ditto, as well as Floco and Floco\({}^{+}\), are the same since they use the same global model. Below we report on detailed observations.

Global and local FL test accuracy.We first evaluate the global and local test performance on CIFAR-10 with the non-IID data splits generated by the 5-Fold and \(()\) procedures, as well as the natural non-IID data splits in the FEMNIST dataset. Table 1 shows the test accuracies on CIFAR-10 with CifarCNN trained from random initialization (left) and ResNet-18 fine-tuned from the ImageNet pre-trained model (center), respectively. It also shows the test accuracies on FEMNIST with FemnistCNN trained from random initialization (left) and SqueezeNet fine-tuned from the ImageNet pre-trained model (right). We clearly see that Floco and Floco\({}^{+}\) outperform all baselines in terms of average local (blue) test accuracy by up to \(6\%\), as well as global (red) by up to \(5\%\).

    &  &  \\   &  &  &  & pre-trained \\   &  &  &  &  & & & & & \\  FedAvg & 60.36 & _60.38_ & 60.74 & _60.78_ & 75.33 & _76.94_ & 68.59 & _59.27_ & 78.83 & _79.84_ & 75.13 & _75.51_ \\ FedProx & 60.68 & _60.36_ & 60.40 & _60.27_ & 76.93 & _77.46_ & 62.27 & _60.26_ & 78.84 & _80.15_ & 75.47 & _75.99_ \\ FedPer & 40.23 & _65.42_ & 33.90 & _67.86_ & 68.64 & _84.06_ & 50.84 & _85.05_ & 50.76 & _73.83_ & 64.03 & _74.43_ \\ APFL & 60.56 & _60.33_ & 60.55 & _60.65_ & 53.25 & _46.46_ & 50.97 & _44.57_ & 4.95 & _4.98_ & 38.21 & _58.06_ \\ Ditto & 60.36 & _72.22_ & _60.74_ & _73.90_ & 75.33 & _69.18_ & 68.59 & _76.23_ & 78.83 & _82.02_ & _78.89_ & _65.06_ \\ FedRoD & 56.36 & _74.03_ & 61.2 & _76.42_ & 71.46 & _31.82_ & 10.27 & _33.85_ & 4.95 & _4.99_ & 4.95 & _4.95_ \\  Floco & **62.93** & _71.78_ & **62.57** & _71.04_ & **77.15** & _85.90_ & **73.62** & _80.38_ & **78.99** & _84.09_ & **75.86** & _77.00_ \\ Floco\({}^{+}\) & **62.93** & _75.08_ & **62.57** & _76.50_ & **77.15** & _84.88_ & **73.62** & _85.89_ & **78.99** & _84.75_ & **75.86** & _82.41_ \\   

Table 1: Average global and _local_ test accuracy.

    &  &  \\   &  &  &  & pre-trained \\   &  &  &  &  & & & & & \\  FedAvg & 24.08 & _25.61_ & 22.95 & _24.51_ & 13.77 & _19.57_ & 13.48 & _19.57_ & 12.40 & _16.86_ & 15.54 & _20.43_ \\ FedProx & 23.76 & _25.56_ & 23.19 & _24.89_ & 12.40 & _12.41_ & 15.16 & _19.83_ & 12.41 & _16.93_ & 15.48 & _20.04_ \\ FedPer & 47.75 & _28.22_ & 56.39 & _25.70_ & 19.73 & _11.19_ & 38.48 & _10.88_ & 38.44 & _21.68_ & 28.28 & _22.31_ \\ APFL & 23.30 & _25.01_ & 22.19 & _23.91_ & 28.39 & _33.39_ & 20.02 & _26.01_ & 4.95 & _4.98_ & **7.6** & _15.82_ \\ Ditto & 24.08 & _19.13_ & 22.95 & _17.64_ & 13.77 & _16.43_ & 13.48 & _14.50_ & 12.40 & _14.65_ & 15.54 & _18.06_ \\ FedRoD & 29.78 & _18.40_ & 41.91 & _17.45_ & 75.59 & _64.07_ & 89.31 & _64.07_ & 4.95 & _4.99_ & 4.99 & _4.99_ \\  Floco & **21.82** & _18.44_ & **20.06** & _18.75_ & **11.48** & _9.44_ & **10.30** & _11.28_ & **10.28** & _13.94_ & 14.65 & _19.15_ \\ Floco\({}^{+}\) & **21.82** & _17.69_ & **20.06** & _16.50_ & **11.48** & _12.42_ & **10.30** & _11.98_ & **10.28** & _13.87_ & 14.65 & _15.35_ \\   

Table 2: Average global and _local_ expected test calibration error.

Calibration.We evaluate and benchmark the quality of uncertainty estimation of all methods. For this purpose we evaluate the global as well as average local ECE on each model-dataset combination for each baseline on the test dataset and show the results in Table 2. As shown, Floco and Floco\({}^{+}\) achieve better Expected Calibration Error (ECE) across all settings, with two exceptions: training a pre-trained ResNet-18 on the CIFAR-10 Dir(0.3) split and a pre-trained SqueezeNetV1 on FEMNIST. In the first case, the average local ECE for Floco and Floco\({}^{+}\) is slightly worse than that of FedPer, suggesting mild overconfident for some clients. In the second case, the next best method (APFL) yields a significantly lower global test accuracy than our method, making a fair comparison of their ECE difficult.

Worst client performance.We evaluate the average local and global test accuracies of the worst 5\(\%\) of clients, a standard approach for assessing potential biases of the FL method toward specific clients or client groups . The worst 5\(\%\) client performance on all CIFAR-10/model combinations is evaluated over 5 trial runs, with results shown in the table on the right. We observe that Floco achieves the highest performance among worst-performing clients across all settings, with a 17\(\%\) improvement over FedAvg, and up to \(1.5\%\) over the next best baseline.

Time-to-accuracy.Similar to Table 1, we plot the TTA improvement for Floco. In particular, we show the TTA improvement of Floco over FedAvg and FedProx, and the TTA improvement of Floco\({}^{+}\) over Ditto, FedPer and FedRod, as all these methods include local fine-tuning. We report all TTAs in Table 4. The underlined entries indicate the cases where the test accuracies of our methods exceed the baseline method's maximum accuracy already at the initial evaluation round, while the entries labeled '_x1.0_' represent the instances where our methods take the same evaluation rounds to achieve the baseline method's maximum accuracy, i.e., comparable in terms of TTA. In addition to test accuracy, we also observe an improvement in Time-to-Accuracy (TTA) for our method across all settings.

### Analysis and Discussion

In this section, we provide further analyses and discussion on Floco.

Solution structure in simplex.First, we confirm that Floco uses the degrees of freedom within the solution simplex for personalization. To this end, we draw approximately 500 uniformly distributed points in the solution simplex, and evaluate the global and the local test accuracy of the corresponding models. Figure 1 (bottom row) shows the global test accuracy (left most) and the local test accuracy (center and right) for two clients. As expected, for the global test dataset the solution simplex performs uniformly well across all its area, while the losses for the two individual local client distributions are small around their projected points (\(\)). This result indicates that the heterogeneous sharing of the solution simplex across the clients properly works as designed.

    &  &  \\    &  &  &  &  \\   & 5-Fold &  &  &  &  &  \\  Floco vs. FedAvg & x5.5 & _x4.6_ & x3.4 & _x3.1_ & x1.3 & _x1.8_ & x1.2 & _x8.0_ & x1.7 & _x1.2_ & x1.1 & _x1.1_ \\ Floco vs. FedProx & x5.1 & _x4.9_ & x3.3 & _x3.8_ & x1.0 & _x1.8_ & x1.2 & _x9.0_ & x3.0 & _x1.2_ & x1.0 & _x1.1_ \\ Floco\({}^{+}\) vs. Ditto & x5.5 & _x2.3_ & x3.4 & _x2.1_ & x1.3 & _x2.0_ & x1.2 & _x1.7_ & x1.7 & _x4.0_ & x9.0 & _x4.0_ \\ Floco\({}^{+}\) vs. FedPer & x1.0 & _x1.5_ & x1.0 & _x1.3_ & x1.6 & _x1.5_ & x1.5 & _x1.5_ & x7 & _x7_ & x7.0 & _x2.7_ \\ Floco\({}^{+}\) vs. FedRoD & x9.4 & _x1.6_ & x24.5 & _x1.3_ & x10 & _x10_ & x10 & _x10_ & x7 & _x7_ & x10 & _x10_ \\   

Table 4: Improvements for global and _local_ time-to-accuracy.

    &  \\   & 5-Fold &  \\  FedAvg & _44.0 \(\) 0.02_ & _42.9 \(\) 0.03_ \\ FedProx & _43.87 \(\) 0.02_ & _43.2 \(\) 0.03_ \\ FedPer & _52.67 \(\) 0.02_ & _51.01 \(\) 0.02_ \\ APFL & _43.27 \(\) 0.02_ & _46.36 \(\) 0.03_ \\ Ditto & _58.20 \(\) 0.03_ & _58.69 \(\) 0.03_ \\ FedRoD & _60.20 \(\) 0.02_ & _61.12 \(\) 0.03_ \\ Floco\({}^{+}\) & _61.73 \(\) 0.02_ & _61.13 \(\) 0.03_ \\   

Table 3: Average _local_ test accuracy for the 5\(\%\) worst performing clients on CIFAR-10.

Gradient variance reduction and stability of training.Figure 2 shows the test accuracy curves during training for global (left) and average local (center) test accuracies of different methods with the standard deviation over 5 trials as shadows. We observe that Floco and Floco\({}^{+}\) not only converge faster than the global and pFL baselines respectively, but also show small standard deviation across trials. The latter implies that our systematic regularization through the solution simplex stabilizes the training dynamics significantly. Figure 2 (right) shows the total gradient variance--the sum of the variances of the updates \(_{k}^{t}=_{k}^{t}-_{0}^{t-1}\) for FedAvg and FedProx (which almost overlap with each other), and \(_{m,k}^{t}=_{m,k}^{t-1}-_{m,0}^{t-1}\) for Floco, respectively. More specifically, we compute the variance over the last fully-connected layer, given by

\[_{}^{2}(t)=_{k^{t}}\|_{k}^{t }-^{t}|}_{k^{t}}_{k}^{t}\| _{2}^{2}\] (12)

for FedAvg and FedProx, and by

\[_{}^{2}(t)=_{m=1}^{M+1}_{k ^{t}}\|_{m,k}^{t}-^{t}|} _{k^{t}}_{m,k}^{t}\|_{2}^{2}.\] (13)

We have not plotted the gradient variances of Floco\({}^{+}\) and the other pFL methods, since those are the same as for Floco and FedAvg, respectively. As discussed in , a small total variance indicates effective collaborations with consistent gradient signals between the clients, leading to better performance. From the figure, we see that the total gradient variance of Floco is much lower and more stable, in terms of standard deviation, than the baseline methods, which, together with its good performance observed in Table 1, is consistent with their discussion. The variance reduction with Floco implies that the degrees of freedom of the solution simplex can absorb the heterogeneity of clients to some extent, making the gradient signals more homogeneous. Moreover,  argued that the last classification layer has the biggest impact on performance, implying that reducing the total variance of the classification layer, as Floco does with simplex learning, is most effective. As we show in the Appendix C, applying simplex learning to only the last layer, instead of learning a simplex in the whole parameter space, achieves faster personalized and global convergence.

Computational complexity.If the batch size is one, simplex training adds \(O( M)\) computational complexity for each layer, where \(\) is the parameter complexity of the layer, e.g., \(=d\ L\) for a fully connected layer with \(d\) input and \(L\) output neurons, and \(M\) is the simplex dimension . For Floco, this additional complexity only applies to the classification layer. For inference, no additional complexity arises, compared to FedAvg, because inference is performed by the single model corresponding to the cluster center. Since the most modern architectures, e.g., ResNet-18 and Vision Transformer (ViT) , have parameter complexity of \(O(_{}) O(_{})\), where \(_{}\) and \(_{}\) are the complexities of the feature extractor and the classification layer, respectively, the additional training complexity, applied only to the classification layer, of Floco is ignorable, i.e., \(O(_{}) O(_{} M)\). The same applies to the communication costs: since the simplex learning is applied only to the classification layer, the increase of communication costs are ignorable compared to the communication costs for the feature extractor.

Figure 2: Global (left) and average local (center) test accuracy for CifarCNN on CIFAR-10, 5-Fold. For Floco, we can clearly observe a jump in average local test accuracy at \(=250\), which is a result of our subregion assignment. Right shows the total variance of the gradients for the last fully-connected layer.

Related Work

There are few existing works that apply simplex learning to federated learning.  proposed SuPerFed, which enforces a low loss simplex between independently initialized global and client models, yielding good personalized FL performance. This approach builds on , which finds optimal interpolation coefficients between a global and local model to improve personalized FL. However, their simplex is restricted to be 1D, i.e., a line segment, and the global model performance is comparable to the plain FedAvg. Moreover, they train a solution simplex over all layers between global and local models, which is computationally expensive and limits its applicability to training _from scratch_. This should be avoided if pre-trained models are available [40; 51]. Our method generalizes to training low-loss simplices of higher dimensions in a FL setting, tackles both the global and personalized FL objectives, is applicable to pre-trained models, and shows significant performance gains by employing our proposed subregion assignment procedure. In Table 7 of Appendix E we benchmark Floco against the SuPerFed baseline on the CIFAR-10, 5-Fold, as well as Dir(0.5) splits using both a CifarCNN trained from scratch as well as a pre-trained ResNet18 on both global as well as local test performance, where we observe that Floco outperforms SuPerFed both in terms of global as well as local accuracy in all settings.

## 6 Limitations

In this work, we only evaluate our method on cross-silo FL settings with up to 100 clients. Unlike cross-device FL, which typically involves a much larger set of stateless clients (i.e., clients with limited data that hinders reliable modeling), our approach assumes stateful clients, each with sufficient data to enable effective grouping of similar clients. While our current analysis focuses on cross-silo FL, extending our method to the cross-device setting is an important direction for future research. Additionally, a thorough theoretical analysis of our approach remains a future research objective.

## 7 Conclusion

FL on highly non-IID client data distributions remains a challenging problem and a very actively researched topic. Recent works tackle non-IID FL settings either through global or personalized FL. While the former aims to find a single optimal set of parameters that fit a global objective, the latter tries to optimize multiple local models each of which fits the local distribution well. These two different objectives may pose a trade-off, that is, personalized FL might adapt models to strongly to local distributions which might harm the global performance, while global FL solutions might fit none of the local distributions if the local distributions are diverse. In this paper, we addressed this issue by leveraging the mode-connectivity of neural networks. Specifically, we propose Floco, where each client trains an assigned subregion within the solution simplex, which allows for personalization, and at the same, contributes to learning a well-performing global model. Floco achieves state-of-the-art performance in both global and personalized FL, with minimal computational and communication overhead during training and no overhead during inference.

Promising future research directions include better understanding the decision-making process of solution simplex training through global and local explainable AI methods [52; 53; 54]. Furthermore, we want to apply our approach to continual learning problems and FL scenarios with highly varying client availability [55; 56].