# Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization

Xilie Xu\({}^{1}\), Jingfeng Zhang\({}^{2,3}\), Feng Liu\({}^{4}\), Masashi Sugiyama\({}^{2,5}\), Mohan Kankanhalli\({}^{1}\)

\({}^{1}\) School of Computing, National University of Singapore

\({}^{2}\) RIKEN Center for Advanced Intelligence Project (AIP)

\({}^{3}\) School of Computer Science, The University of Auckland

\({}^{4}\) School of Computing and Information Systems, The University of Melbourne

\({}^{5}\) Graduate School of Frontier Sciences, The University of Tokyo

xuxilie@comp.nus.edu.sg jingfeng.zhang@auckland.ac.nz fengliu.ml@gmail.com sugi@k.u-tokyo.ac.jp mohan@comp.nus.edu.sg

The first two authors have made equal contributions.Corresponding author.

###### Abstract

Adversarial contrastive learning (ACL) is a technique that enhances standard contrastive learning (SCL) by incorporating adversarial data to learn a robust representation that can withstand adversarial attacks and common corruptions without requiring costly annotations. To improve transferability, the existing work introduced the standard invariant regularization (SIR) to impose _style-independence property_ to SCL, which can exempt the impact of nuisance _style factors_ in the standard representation. However, it is unclear how the style-independence property benefits ACL-learned robust representations. In this paper, we leverage the technique of _causal reasoning_ to interpret the ACL and propose adversarial invariant regularization (AIR) to enforce independence from style factors. We regulate the ACL using both SIR and AIR to output the robust representation. Theoretically, we show that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, our experimental results show that invariant regularization significantly improves the performance of state-of-the-art ACL methods in terms of both standard generalization and robustness on downstream tasks. To the best of our knowledge, we are the first to apply causal reasoning to interpret ACL and develop AIR for enhancing ACL-learned robust representations. Our source code is at https://github.com/GodXuxilie/Enhancing_ACL_via_AIR.

## 1 Introduction

The attention towards pre-trained models that can be easily finetuned for various downstream applications has significantly increased recently [17; 18; 19; 41]. Notably, foundation models  via self-supervision on large-scale unlabeled data, such as GPT  and CLAP , can be adapted to a wide range of downstream tasks. Due to the high cost of annotating large-scale data, unsupervised learning techniques [21; 34; 49; 16] are commonly used to obtain generalizable representations, in which standard contrastive learning (SCL) has been shown as the most effective way [9; 38; 10].

Adversarial contrastive learning (ACL) [30; 29], that incorporates adversarial data  with SCL , can yield robust representations that are both generalizable and robust against adversarial attacks and common corruptions . ACL is attracting increasing popularity since the adversarial robustness of the pre-trained models is essential to safety-critical applications [6; 33]. Many studies have tried to improve ACL from various perspectives including increasing contrastive views and leveraging pseudo labels , leveraging hard negative sampling [42; 50], and dynamically scheduling the strength of data augmentations .

The style-independence property of learned representations, which eliminates the effects of nuisance style factors in SCL, has been shown to improve the transferability of representations . To achieve style-independence property in learned representations, Mitrovic et al.  proposed a standard invariant regularization (SIR) that uses causal reasoning [39; 40] to enforce the representations of natural data to be invariant to style factors. SIR has been shown effective in enhancing representation' transferability. However, it remains unclear in the literature how to achieve style independence in robust representations learned via ACL, especially when adversarial data is used during pre-training.

Therefore, we leverage the technique of causal reasoning to enforce robust representations learned via ACL to be style-independent. As shown in the right panel of Figure 1, we construct the causal graph of ACL (details refer to Section 3.1). Different from the causal graph of SCL , ACL has an extra path \(x\) since ACL will generate the adversarial data \(\) given the unlabeled data \(x\) (i.e., \(x\)). Then, ACL learns representations by maximizing both the probability of the proxy label \(y^{R}\) given the natural data and that given the adversarial data (i.e., \(x y^{R}\) and \( y^{R}\)). Theorem 1 shows that maximizing the aforementioned probability in the causal view is equivalent to the learning objective of ACL , which justifies the rationality of our constructed causal graph of ACL.

To enforce robust representations to be style-independent, we propose an adversarial invariant regularization (AIR). Specifically, AIR (see Eq. (7)) aims to penalize the Kullback-Leibler divergence between the robust representations of the unlabeled data augmented via two different data augmentation functions during the learning procedure of \(x y^{R}\). Note that SIR  is a special case of AIR in the standard context where \(x=\). Then, we propose to learn robust representations by minimizing the adversarial contrastive loss [29; 22; 50; 36] together with a weighted sum of SIR and AIR and show the learning algorithm in Algorithm 1.

Furthermore, we give a theoretical understanding of the proposed AIR and show that AIR is beneficial to the robustness against corruptions on downstream tasks. Based on the decomposition of AIR shown in Lemma 2, we propose Proposition 3 which explains that AIR implicitly enforces the representational distance between original as well as augmented views of natural data and their adversarial variants to be independent of the style factors. In addition, we theoretically show that the style-independence property of robust representations learned via ACL will still hold on the downstream classification tasks in Proposition 4, which could be helpful to improve robustness against input perturbations on downstream tasks [3; 45; 28; 38].

Figure 1: Causal graph of data generation procedure in SCL  (left panel) and ACL (right panel). \(s\) is style variable, \(c\) is content variable, \(x\) is unlabeled data, \(\) is the generated adversarial data, and \(\) is the parameter of representation. The solid arrows are causal, but the dash arrows are not causal. The proxy label \(y^{R}^{R}\) is a refinement of the target labels \(y_{t}=\{y_{i}\}_{i=1}^{T}\) that represents an unknown downstream task. In the causal view, each augmented view of a data point has its own proxy label, where both SCL and ACL aim to minimize the differences between the representation outputs of different views of the same data.

Empirically, we conducted comprehensive experiments on various datasets including CIFAR-10 , CIFAR-100 , STL-10 , CIFAR-10-C , and CIFAR-100-C  to show the effectiveness of our proposed method in improving ACL methods [29; 22; 50; 36]. We demonstrate that our proposed method can achieve the new state-of-the-art (SOTA) performance on various downstream tasks by significantly enhancing standard generalization and robustness against adversarial attacks [13; 14; 2] and common corruptions . Notably, compared with the prior SOTA method DynACL , AIR improves the standard and robust test accuracy by 2.71% (from 69.59\(\)0.08% to 72.30\(\)0.10%) and 1.17% (from 46.49\(\)0.05% to 47.66\(\)0.06%) on the STL-10 task and increases the test accuracy under common corruptions by 1.15% (from 65.60\(\)0.18% to 66.75\(\)0.10%) on CIFAR-10-C.

## 2 Related Works and Preliminaries

Here, we introduce the related works in ACL and causal reasoning and provide the preliminaries.

### Related Works

Adversarial contrastive learning (ACL).Contrastive learning (CL) is frequently used to leverage large unlabeled datasets for learning useful representations. Chen et al.  presented SimCLR that leverages contrastive loss for learning useful representations and achieved significantly improved standard test accuracy on downstream tasks. Recently, adversarial contrastive learning (ACL) [30; 29; 27; 22; 50; 52; 36] has become the most effective unsupervised approaches to learn robust representations. Jiang et al. , which is the seminal work of ACL that incorporates adversarial data  with contrastive loss , showed that ACL can exhibit better robustness against adversarial attacks [23; 13] and common corruptions  on downstream tasks compared with SCL .

One research line focuses on improving the performance of ACL. AdvCL  leverages a third contrastive view for high-frequency components and pseudo labels generated by clustering to improve ACL. Yu et al.  proposed the asymmetric InfoNCE objective (A-InfoNCE) that utilizes the hard negative sampling method  to further enhance AdvCL . Recently, DynACL  dynamically schedules the strength of data augmentations and correspondingly schedules the weight for standard and adversarial contrastive losses, achieving the SOTA performance among existing ACL methods. Another research line focuses on improving the efficiency of ACL since ACL requires tremendous running time to generate the adversarial training data during pre-training. Xu et al.  proposed a robustness-aware coreset selection (RCS) method to speed up ACL by decreasing the number of training data. Our paper, belonging to the former research line, leveraged the technique of causal reasoning to further enhance ACL and achieved new SOTA results. We treat the incorporation of our proposed method with RCS  as future work.

Causal reasoning.Causal reasoning [39; 53; 40; 8; 56; 38; 7; 44; 43] has been widely applied to machine learning to identify causal relations and ignore nuisance factors by intervention. In particular, Zhang et al. [53; 56] investigated the causality in supervised adversarial training [37; 55] where label information is required and proposed the regularizer to eliminate the difference between the natural and adversarial distributions. Mitrovic et al.  built the causal graph of SCL in the standard context and introduced the standard invariant regularization (SIR) which aims to enforce the representation of unlabeled natural data to be invariant of the data augmentations. SIR can significantly improve robustness transferability to downstream tasks in terms of common corruptions , which is empirically validated by Mitrovic et al. .

### Preliminaries

Standard contrastive learning (SCL) .Let \(h_{}:\) be a representation extractor parameterized by \(\), \(g:\) be a projection head that maps representations to the space where the contrastive loss is applied, and \(_{i},_{j}:\) be two transformation operations randomly sampled from a pre-defined transformation set \(\). Given a minibatch \(B^{}\) consisting of \(\) original samples, we denote the augmented minibatch \(B^{u}=\{_{u}(x_{k}) x_{k} B\}\) via augmentation function \(_{u}()\). We take \(f_{}()=g h_{}()\) and \(x_{k}^{u}=_{u}(x_{k})\) for any \(x_{k}\) and \(u\{i,j\}\). Given a positive pair \((x_{k}^{i},x_{k}^{j})\), the standard contrastive loss proposed by SimCLR  is formulated as follows:

\[_{}(x_{k}^{i},x_{k}^{j};)=-_{u\{i,j\}}(f_{}(x_{k}^{i}),f_{}(x_{k}^{i}))/t}}{ _{x B^{i} B^{j}\{x_{k}^{u}\}}e^{( f_{}(x_{k}^{u}),f_{}(x))/t}},\] (1)where \((p,q)=p^{}q/\|p\|\|q\|\) is the cosine similarity function and \(t>0\) is a temperature parameter.

ACL  and DynACL .Let \((,d_{})\) be the input space \(\) with the infinity distance metric \(d_{}(x,x^{})=\|x-x^{}\|_{}\), and \(_{}[x]=\{x^{} d_{}(x,x^{ })\}\) be the closed ball of radius \(>0\) centered at \(x\). Given a data point \(x_{k}\), the adversarial contrastive loss is as follows:

\[_{}(x_{k};)=(1+)_{}(_{k}^{i},_{k}^{j};)+(1-)_{}(x_{k}^{i},x_{k}^{j};),\] (2) \[_{k}^{i},_{k}^{j}=* {arg\,max}_{_{k}^{i}_{}[x _{k}^{j}]\\ _{k}^{j}_{}[x_{k}^{j}]}_{ }(_{k}^{i},_{k}^{j};),\] (3)

in which \(\) is a hyperparameter and \(_{k}^{i}\) and \(_{k}^{j}\) are adversarial data generated via projected gradient descent (PGD) [37; 54; 55] within the \(\)-balls centered at \(x_{k}^{i}\) and \(x_{k}^{j}\). Note that ACL  fixes \(=0\) while DynACL  dynamically schedules \(\) according to its dynamic augmentation scheduler that gradually anneals from a strong augmentation to a weak one. We leave the details of the data augmentation scheduler  in Appendix C due to the page limitation.

## 3 Methodology

In this section, we first create the causal graph for ACL which is the fundamental premise for causal reasoning. Then, we introduce adversarial invariant regularization (AIR) according to the causal understanding in the context of ACL and the learning algorithm of our proposed method. Lastly, we demonstrate the theoretical understanding of AIR and theoretically show that the style-independence property is generalizable to downstream tasks.

### Adversarial Contrastive Learning (ACL) in the View of Causality

A causal graph is arguably the fundamental premise for causal reasoning [39; 40]. Previously, Mitrovic et al.  constructed the causal graph of SCL as shown in the left panel of Figure 1. However, how to build causal graphs of ACL is still unclear in the literature. Therefore, we take the primitive step to construct the causal graph of ACL.

To begin with, we formalize the data generation procedure in the causal graph. Let \(x\) denote an unlabeled data point and \(=\{y_{t}\}_{t=1}^{T}\) denote a set of labels in an unknown downstream task. Then, similar to Mitrovic et al.  and Zhang et al.  we make the following assumptions: 1) the data point \(x\) is generated from the content factor \(c\) and the style factor \(s\), i.e., \(c x s\), 2) the content is independent of the style, i.e., \(c\!\!1\)\(s\), and 3) only the content, which is the original image data, is related to the downstream task, i.e., \(c y_{t}\).

Then, we build the learning procedure of CL. According to the above assumptions, the content serves an essential role in downstream classification tasks. Therefore, CL can be regarded as estimating the probability of the label given the content, i.e., \(p(y_{t}|c)\). To learn the representations from the unlabeled data \(x\), CL needs to maximize the conditional probability \(p(y_{t}|x)\).

However, in practice, the label \(y_{t}\) is unknown. Thanks to the causal concept of refinement [8; 38], we can construct a proxy label \(y^{R}^{R}\) which is a refinement of \(\). This proxy label \(y^{R}\) should be consistent across different views of the unlabeled data \(x\), which enables learning the representations. Intuitively, the proxy label \(y^{R}\) can be viewed as a more fine-grained variant of the original label \(y_{t}\). For example, a refinement for recognizing cars would be the task of classifying various types of cars. Therefore, SCL targets maximizing the probability of the proxy label given the unlabeled natural data, i.e., \(p(y^{R}|x)\).

Besides \(x y^{R}\), ACL has an extra path \(x y^{R}\) since ACL needs to first generate the adversarial data (Eq. (3)) and then learns representations from the unlabeled natural and adversarial data (Eq. (2)). According to Eq. (3), we observe that ACL only uses the observed unlabeled data \(x\) for generating adversarial data, i.e., \(x\). Then, according to the paths \(x y^{R}\) and \( y^{R}\), from the causal view, ACL learns representations by maximizing both the probability of the proxy label \(y^{R}\) given the natural data (i.e., \(p(y^{R}|x)\)) and that given adversarial data (i.e., \(p(y^{R}|)\)).

Lastly, we provide Theorem 1 to justify the rationality of our constructed causal graph of ACL.

**Theorem 1**.: _From the causal view, in ACL, maximizing the conditional probability both \(p(y^{R}|x)\) and \(p(y^{R}|)\) is equivalent to minimizing the learning objective of ACL  that is the sum of standard and adversarial contrastive losses._The proof of Theorem 1 is in Appendix B.1.

### Adversarial Invariant Regularization

According to the independence of causal mechanisms , performing interventions on the style variable \(s\) should not change the probability of the proxy label given the unlabeled data, i.e., \(p(y^{R}|x)\). According to the path \(x y^{R}\) shown in the causal graph of ACL, we can obtain that

\[p(y^{R}|x)=p(y^{R}|)p(|x),\] (4)

under the assumption that the process of \(x y^{R}\) satisfies the Markov condition. Therefore, we should make the conditional probability \(p(y^{R}|)p(|x)\) that is learned via ACL become style-independent. It guides us to propose the following criterion that should be satisfied by ACL:

\[p^{do(_{i})}(y^{R}|)p^{do(_{i})}(|x)=p^{do(_{j}) }(y^{R}|)p^{do(_{j})}(|x)_{i},_{j} ,\] (5)

where \(do()\) as the short form of \(do(s=)\) denotes that we perform the intervention on \(s\) via data augmentation function \(()\). We leverage the representational distance between various views of natural data and their adversarial variants normalized by the softmax function to calculate the conditional probability \(p^{do(_{u})}(y^{R}|)\) and \(p^{do(_{u})}(|x)\) where \(u\{i,j\}\) as follows:

\[p^{do(_{u})}(y^{R}|)=(f_{}(x),f_{ }(^{u}))/t}}{_{x_{k} B}e^{(f_{}(x_ {k}),f_{}(^{u}_{k}))/t}}, p^{do(_{u})}(|x)=(f_{}(^{u}),f_{}(x^{u}) )/t}}{_{x_{k} B}e^{(f_{}(^{u }_{k}),f_{}(x^{u}_{k}))/t}},\] (6)

in which \(x B^{}\) is the original view of natural data, \(x^{u}\) is the augmented view of natural data via augmentation \(_{u}()\), \(^{u}\) is the adversarial variant generated via Eq. (3), and \(t\) is the temperature parameter. Note that we use \(x\) as the approximated surrogate of its true content \(c\) that incurs \(y^{R}\) (i.e., \(c y^{R}\)).

To achieve the above criterion in Eq. (5), we propose an adversarial invariant regularization (AIR) to regulate robust representations as follows:

\[_{}(B;,)=(p^{do(_{i} )}(y^{R}|)p^{do(_{i})}(|x)\|p^{do(_{j})}(y^{R}| )p^{do(_{j})}(|x);B),\] (7)

in which \( 0\) is the adversarial budget and \((p(x)\|q(x);B)=_{x B}p(x)\) denotes the Kullback-Leibler (KL) divergence. AIR can enforce the representation to satisfy the criterion in Eq. (5), thus eliminating the effect of the style factors on the representation. Therefore, when setting \(\) as a positive constant, \(_{}(B;,)\) can effectively regulate robust representations of adversarial data to be style-independent.

Besides, to explicitly regulate standard representations of natural data to be independent of style factors, we can simply set \(=0\) of AIR. We formulate AIR with \(=0\) as follows:

\[_{}(B;,0) =(p^{do(_{1})}(y^{R}|x)\|p^{do(_{j})}(y^{ R}|x);B),\] (8) \[ p^{do(_{u})}(y^{R}|x) =(f_{}(x),f_{}(x^{u}))/t}}{ _{x_{k} B}e^{(f_{}(x_{k}),f_{}(x^{u}_{k} ))/t}} u\{i,j\}.\]

Note that \(_{}(B;,0)\) is the same formulation as the standard invariant regularization (SIR) , which can make the standard representation style-independent.

By incorporating adversarial contrastive loss with AIR, the learning objective function of our proposed method is shown as follows:

\[*{arg\,min}_{}_{x U}_{}(x;)+ _{1}_{}(U;,0)+_{2} _{}(U;,)\] (9)

where \(U^{N}\) refers to an unlabeled dataset consisting of \(N\) samples, \(>0\) is the adversarial budget, and \(_{1} 0\) and \(_{2} 0\) are two hyperparameters. The learning algorithm of ACL with AIR is demonstrated in Algorithm 1. Note that our proposed AIR is compatible with various learning objectives such as ACL , AdvCL , A-InfoNCE , and DynACL .

### Theoretical Analysis

We provide a theoretical understanding of AIR in Proposition 3 based on the decomposition of AIR shown in Lemma 2. We set \(>0\) of AIR by default in this section.

**Lemma 2** (Decomposition of AIR).: _AIR in Eq. (7) can be decomposed into two terms as follows:_

\[_{}(B;,)= _{x p^{do(_{i})}(|x)}[(p^{ do(_{i})}(y^{R}|)\|p^{do(_{j})}(y^{R}|))]\] \[+_{x p^{do(_{i})}(y^{R}|)}[ (p^{do(_{i})}(|x)\|p^{do(_{j})}(|x))],\]

_where \(_{x Q^{3}(x)}[(Q^{1}(x)\|Q^{2}(x))]\) is the expectation of KL divergence over \(Q^{3}\) and \(Q^{1},Q^{2},Q^{3}\) are probability distributions._

The proof of Lemma 2 is in Appendix B.2. Based on Lemma 2, we propose Proposition 3 that provides the theoretical understanding of AIR.

**Proposition 3**.: _AIR implicitly enforces the robust representation to satisfy the following two proxy criteria:_

\[(1) p^{do(_{i})}(y^{R}|)=p^{do(_{j})}(y^{R}|), (2) p^{do(_{i})}(|x)=p^{do(_{j})}(|x).\]

The proof of Proposition 3 is in Appendix B.3.

Remarks.Proposition 3 explains that AIR implicitly enforces the representational distance to be style-independent between the original view of natural data and their adversarial variants (i.e., \(p^{do(_{i})}(y^{R}|)=p^{do(_{j})}(y^{R}|)\)), as well as between the augmented view of natural data and their adversarial counterparts (i.e., \(p^{do(_{i})}(|x)=p^{do(_{j})}(|x)\)). Therefore, Proposition 3 explicitly presents that AIR is different from SIR , where SIR only enforces the augmented views of natural data to be style-independent.

Next, we theoretically show that on the assumption that \(^{R}\) is a refinement of \(\), the style-independence property of robust representations will hold on downstream classification tasks in Proposition 4.

**Proposition 4**.: _Let \(=\{y_{t}\}_{t=1}^{T}\) be a label set of a downstream classification task, \(^{R}\) be a refinement of \(\), and \(_{t}\) be the adversarial data generated on the downstream task. Assuming that \(_{}[x]\) and \(_{t}_{}[x]\), we have the following results:_

\[p^{do(_{i})}(y^{R}|)=p^{do(_{j})}(y^{R}|)  p^{do(_{i})}(y_{t}|_{t})=p^{do(_{j})}(y_{t}| _{t})_{i},_{j},\] \[p^{do(_{i})}(|x)=p^{do(_{j})}(|x)  p^{do(_{i})}(_{t}|x)=p^{do(_{j})}(_{t}| x)_{i},_{j}.\]

The proof of Proposition 4 is shown in Appendix B.4.

Remarks.Proposition 4 indicates that the robust representation of data on downstream tasks is still style-independent. Mitrovic et al.  empirically showed that the style-independence property of standard representations can significantly improve the robustness against common corruptions  on downstream tasks. In addition, Proposition 4 indicates that AIR helps to find the invariant correlations that are independent of the style factors among different training distributions. It is similar to the objective of invariant risk minimization  that can yield substantial improvement in the robustness against various corruptions incurred by distribution shifts. Therefore, the style-independence property of robust representations learned via ACL perhaps helps to enhance the robustness against input perturbations on downstream tasks.

[MISSING_PAGE_FAIL:7]

severity (CS) ranging from \(\{1,3,5\}\) (denoted as "CS-\(\{1,3,5\}\)"). Specifically, we used the official code of AutoAttack  and RobustBench  for implementing evaluations. In Appendix C.5, we provide robustness evaluation under more diverse attacks [13; 14; 2]. In Appendix C.6, we provide the test accuracy under each type of common corruption .

### Evaluation of Self-Task Robustness Transferability

Self-task adversarial robustness transferability.Table 1 reports the self-task adversarial robustness transferability evaluated on three datasets where pre-training and finetuning are conducted on the same datasets including CIFAR-10, CIFAR-100, and STL-10. In Appendix C.1, we report the self-task robustness transferability evaluated in the semi-supervised settings. Table 1 demonstrates that our proposed AIR can obtain new state-of-the-art (SOTA) both standard and robust test accuracy compared with existing ACL methods [29; 36]. Therefore, It validates the effectiveness of our proposed AIR in consistently improving the self-task adversarial robustness transferability against adversarial attacks as well as standard generalization in downstream datasets via various finetuning procedures. Notably, compared with the previous SOTA method DynACL , DynACL-AIR increases standard test accuracy by 4.52% (from 72.90% to 77.42%) on the CIFAR-10 dataset via ALF, 2.05% (from 43.58% to 45.63%) on the CIFAR-100 dataset via ALF, and 1.17% (from 46.49% to 47.66%) on the STL-10 dataset via SLF.

Self-task common-corruption robustness transferability.Table 2 reports the self-task common-corruption  robustness transferability. Specifically, we conducted pre-training on CIFAR-100/CIFAR-100 and then evaluated the test accuracy on CIFAR-10-C/CIFAR-100-C with various corruption severities after various finetuning procedures. Note that the test accuracy under each type of common corruption is reported in Appendix C.6. Table 2 demonstrates that AIR leads to consistent and significant improvement in the robustness against common corruption. In addition, we observe that ACL always achieves much higher test accuracy under common corruptions than DynACL after SLF. We conjecture the reason is that DynACL uses weak data augmentations at the later phase of training, which makes DynACL more likely to overfit the training distribution of CIFAR-10 and thus leads to worse performance under common corruptions via SLF.

  _{1}_{2}\)} &  &  &  &  \\  & & CS-1 & CS-3 & CS-5 & CS-1 & CS-3 & CS-5 & CS-1 & CS-3 & CS-5 \\   & ACL  & 31.39 & 27.76 & 23.27 & 27.80 & 25.09 & 21.09 & 52.07 & 44.22 & 36.31 \\  & ACL-AIR & **37.82** & **32.83** & **27.06** & **34.67** & **30.09** & **25.07** & **52.81** & **44.95** & **37.01** \\   & DynACL  & 26.74 & 23.97 & 20.87 & 23.70 & 21.43 & 18.84 & 52.87 & 44.87 & 36.76 \\  & DynACL-AIR & **30.35** & **26.53** & **22.76** & **27.67** & **24.35** & **20.96** & **54.00** & **46.01** & **37.75** \\    & ACL  & 59.65 & 55.14 & 49.09 & 56.15 & 51.96 & 47.04 & 72.94 & 66.05 & 59.17 \\  & ACL-AIR & **63.34** & **58.14** & **51.03** & **59.14** & **54.23** & **48.97** & **73.48** & **66.83** & **60.21** \\    & DynACL  & 57.14 & 52.07 & 47.03 & 56.31 & 52.03 & 47.40 & 75.89 & 69.08 & 62.02 \\   & DynACL-AIR & **58.41** & **53.09** & **47.94** & **54.89** & **49.84** & **45.08** & **76.35** & **69.49** & **62.44** \\  

Table 4: Cross-task common-corruption robustness transferability. The corruption severity (CS) ranging from \(\{1,3,5\}\) (denoted as “CS-\(\{1,3,5\}\)"). The standard deviation is in Table 16.

  _{1}_{2}\)} &  &  &  \\  & & AA (\%) & SA (\%) & AA (\%) & SA (\%) & AA (\%) & SA (\%) \\   & ACL  & 9.98\(\)0.02 & 32.61\(\)0.04 & 11.09\(\)0.06 & 28.58\(\)0.06 & 22.67\(\)0.16 & 56.05\(\)0.19 \\  & ACL-AIR & **11.04\(\)**0.06 & **39.45\(\)**0.07 & **13.30\(\)**0.02 & **36.10\(\)**0.05 & **23.45\(\)**0.04 & **56.31\(\)**0.06 \\   & DynACL  & 11.01\(\)0.02 & 27.66\(\)0.03 & 11.92\(\)0.05 & 24.14\(\)0.09 & 24.17\(\)0.10 & 55.61\(\)0.17 \\  & DynACL-AIR & **12.20\(\)**0.04 & **31.33\(\)**0.03 & **12.70\(\)**0.03 & **28.70\(\)**0.05 & **24.82\(\)**0.07 & **57.00\(\)**0.13 \\    & ACL  & 25.41\(\)0.08 & 56.53\(\)0.10 & 27.17\(\)0.09 & 51.71\(\)0.17 & 32.66\(\)0.07 & 61.41\(\)0.13 \\  & ACL-AIR & **28.00\(\)**0.12 & **61.91\(\)**0.13 & **30.06\(\)**0.10 & **62.03\(\)**0.11 & **34.26\(\)**0.09 & **62.58\(\)**0.10 \\   & DynACL  & 28.52\(\)0.09 & 52.45\(\)0.10 & 29.13\(\)0.14 & 49.53\(\)0.17 & 35.25\(\)0.15 & 63.39\(\)0.18 \\  & DynACL-AIR & **29.88\(\)**0.04 & **54.59\(\)**0.12 & **31.24\(\)**0.06 & **57.14\(\)**0.09 & **35.66\(\)**0.05 & **63.74\(\)**0.12 \\    & ACL  & 18.72\(\)0.07 & 60.90\(\)0.02 & 26.92\(\)0.11 & 57.35\(\)0.07 & 44.07\(\)0.11 & 75.19\(\)0.10 \\  & ACL-AIR & **19.90\(\)**0.04 & **64.89\(\)**0.09 & **27.65\(\)**0.06 & **60.79\(\)**0.04 & **44.84\(\)**0.14 & **75.67\(\)**0.13 \\   & DynACL  & 25.23\(\)0.12 & 59.12\(\)0.10 & 28.92\(\)0.10 & 56.09\(\)0.14 & 47.40\(\)0.23 & 77.92\(\)0.18 \\   & DynACL-AIR & **25.63\(\)**0.07 & **59.83\(\)**0.08 & **29.32\(\)**0.06 & **56.56\(\)**0.06 & **47.92\(\)**0.12 & **78.44\(\)**0.10 \\    & ACL  & 21.77\(\)0.07 & 46.19\(\)0.05 & 24.4

### Evaluation of Cross-Task Robustness Transferability

Cross-task adversarial robustness transferability.Table 3 shows the cross-task adversarial robustness transferability where pre-training and finetuning are conducted on the different datasets. We can observe that AIR substantially improves both ACL's and DynACL's robustness against adversarial attacks  and standard generalization to other downstream datasets. Particularly, AIR improves the standard and robust test accuracy of ACL via ALF by 7.52% (from 28.58% to 36.10%) and 2.21% (from 11.09% to 13.30%), respectively.

Cross-task common-corruption robustness transferability.Table 4 reports the cross-task common-corruption  robustness transferability where pre-training is conducted on CIFAR-100/CIFAR-100 and finetuning is conducted on CIFAR-100-C/CIFAR-10-C . The empirical results show that our proposed AIR can consistently improve the accuracy under common corruptions with various severity, which validates the effectiveness of AIR in enhancing the robustness transferability against common corruptions.

### Evaluation of Pre-Trained Models after Post-Processing

Here, we report the self-task adversarial robustness transferability of pre-trained models after post-processing in Table 5. The post-processing method is linear probing and then adversarial full finetuning (LP-AFF)  which first generates pseudo labels via clustering and then further adversarially trains the pre-trained model using the pseudo labels. "++" denotes that the pre-trained model is post-processed via LP-AFF. We provide the comparison between DynACL++ and DynACL-AIR++ in Table 5. Note that the superscript \(\) denotes that the results of DynACL++ are copied from Luo et al. . The results validate that AIR is compatible with the trick of post-processing via LP-AFF , which can yield improved performance on various downstream datasets.

### Evaluation on High-Resolution Imagenette Dataset

To the best of our knowledge, there exists no reported result of ACL and DynACL on high-resolution datasets in the existing papers. Therefore, we pre-trained ResNet-18 and ResNet-50 on Imagenette3 of resolution \(256 256\) which is a subset of ImageNet-1K using DynACL and DynACL-AIR, respectively. We compare and report the performance evaluated on Imagenette via SLF in Table 6. Our results validate that our proposed AIR is effective in enhancing the performance on high-resolution datasets. Note that, due to the limitation of our GPU memory, we set the batch size to 256 and 128 for pre-training ResNet-18 and ResNet-50 on Imagenette, respectively. We believe using a larger batch size during pre-training can even further improve the performance .

### Evaluation via Automated Robust Fine-Tuning

While evaluating the transferability of a pre-trained model, we observed that the performance on the downstream dataset is sensitive to the hyper-parameters (e.g., the learning rate) of the finetuning

   &  &  \\  & AA (\%) & SA (\%) & AA (\%) & SA (\%) \\  DynACL  & 57.15 & 79.41 & 58.98 & 80.74 \\ DynACL-AIR & **58.34** & **80.61** & **60.10** & **81.66** \\  

Table 6: Self-task adversarial robustness transferability evaluated on the Imagenette dataset.

   &  &  &  &  \\  & & AA (\%) & SA (\%) & AA (\%) & SA (\%) & AA (\%) & SA (\%) \\   & DynACL++  & 46.46\({}^{}\) & 79.81\({}^{}\) & 47.95\({}^{}\) & 78.84\({}^{}\) & 50.31\({}^{}\) & 81.94\({}^{}\) \\  & DynACL-AIR++ & **46.99** & **81.80** & **48.23** & **79.56** & **50.65** & **82.36** \\   & DynACL++  & 20.07\({}^{}\) & 52.26\({}^{}\) & 22.24 & 49.92 & 25.21 & 57.30 \\  & DynACL-AIR++ & **20.61** & **53.93** & **22.96** & **52.09** & **25.48** & **57.57** \\   & DynACL++  & 47.21\({}^{}\) & 70.93\({}^{}\) & 48.06 & 69.51 & 41.84 & 72.36 \\  & DynACL-AIR++ & **47.90** & **71.44** & **48.59** & **71.45** & **44.09** & **72.42** \\  

Table 5: Self-task adversarial robustness transferability with post-processing. “++” denotes that the pre-trained models are post-processed via LP-AFF [32; 36]. LP-AFF  first generates pseudo labels via clustering and then adversarially pre-trains the model using the pseudo labels.

procedure. It would require extensive computational resources to search for appropriate hyper-parameters to achieve a satisfactory performance. To mitigate this issue, we leverage an automated robust finetuning framework called AutoLoRa , which can automatically schedule the learning rate and set appropriate hyper-parameters during finetuning. We report the performance achieved by AutoLoRa to further justify the SOTA performance by our proposed AIR in Table 7.

Table 7 shows that AutoLoRa can further enhance the performance of a pre-trained model on the downstream tasks without searching for appropriate hyper-parameters since the value of "Diff" is consistently larger than \(0.0\). Besides, Table 7 justifies that our proposed AIR is effective in enhancing robustness transferability via various finetuning methods.

## 5 Conclusions

This paper leveraged the technique of causal reasoning to interpret ACL and proposed adversarial invariant regularization (AIR) to enhance ACL. AIR can enforce the learned robust representations to be invariant of the style factors. We improved ACL by incorporating the adversarial contrastive loss with a weighted sum of AIR and SIR that is an extension of AIR in the standard context. Theoretically, we showed that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, comprehensive results validate that our proposed method can achieve new state-of-the-art performance in terms of standard generalization and robustness against adversarial attacks and common corruptions on downstream tasks.