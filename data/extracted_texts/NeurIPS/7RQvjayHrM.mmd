# RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models

Shuhao Chen\({}^{1,}\)

Equal contribution \({}^{}\)Corresponding author

Weisen Jiang\({}^{1,2}\)

Equal contribution \({}^{}\)Corresponding author

Baijiong Lin\({}^{3}\)

James T. Kwok\({}^{2}\)

Yu Zhang\({}^{1,}\)

\({}^{1}\)Southern University of Science and Technology

\({}^{2}\)The Hong Kong University of Science and Technology

\({}^{3}\)The Hong Kong University of Science and Technology (Guangzhou)

12232388@mail.sustech.edu.cn, jamesk@cse.ust.hk

{waysonkong, bj.lin.email, yu.zhang.ust}@gmail.com

###### Abstract

Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities. To achieve this, routing is a promising method, which learns a router to select the most suitable LLM for each query. However, existing routing models are ineffective when multiple LLMs perform well for a query. To address this problem, in this paper, we propose a method called query-based Router by Dual Contrastive learning (RouterDC). The RouterDC model, which consists of an encoder and LLM embeddings, is trained by two proposed contrastive losses (sample-LLM and sample-sample losses). Experimental results show that RouterDC is effective in assembling LLMs and largely outperforms individual top-performing LLMs as well as existing routing methods on both in-distribution (+2.76%) and out-of-distribution (+1.90%) tasks. The source code is available at https://github.com/shuhao02/RouterDC.

## 1 Introduction

Large language models (LLMs) have demonstrated proficient capabilities across various tasks. Many LLMs are publicly available online, such as Mistral , LLaMA-2 , and LLaMA-3 . Those LLMs have been further fine-tuned to be generalists or specialists. For example, MetaMath  excels in solving mathematical reasoning problems. Since those LLMs are pre-trained or fine-tuned with various data, they typically exhibit varying strengths and weaknesses across different tasks. Therefore, assembling multiple off-the-shelf LLMs can harness their complementary abilities, resulting in better performance than relying on a single LLM.

LLM ensembling is a straightforward method to assemble LLMs, which feeds the query to all candidate LLMs and merges all outputs into a final answer by majority voting  or pairwise ranking . However, ensembling is computationally prohibitive as it requires generating outputs with all candidate LLMs during inference. To tackle this issue, recent works  propose to learn a router to select a suitable LLM for each query. During inference, routing is much more efficient than ensembling as it only needs to perform inference on the selected LLM.

The current state-of-the-art routing method is ZOOTER . To train a router, ZOOTER scores the outputs of candidate LLMs as the supervision signal via an off-the-shelf reward model and then learns the router by minimizing the Kullback-Leibler divergence  between the selection probability from the router and the softmax normalized score. However, this loss is inappropriate when multiple LLMs perform well for a query. Figure 4 shows the scores of seven LLMs for an example query, where the top three LLMs have significantly higher scores than the bottom three LLMs. After the softmax normalization, the scores are small, leading the router to generate small probabilities on the top LLMs. Moreover, the normalized score tends to be uniform, which is not a strong supervision signal for learning the router. Figure 4 shows that the score difference between the top two LLMs is usually tiny (under the experimental setting in Section 4.1), indicating that the loss used in ZOOTER is inappropriate.

In this paper, we propose a query-based **Router** by **D**ual **C**ontrastive learning (RouterDC). The RouterDC consists of an encoder, whose architecture is a small language model, and learnable LLM embeddings for candidate LLMs. For each query, we first score the candidate LLMs by comparing their predictions with the gold label. Instead of directly aligning the score distribution, we leverage the score to choose the top-performing and bottom-performing LLMs and then propose a _sample-LLM contrastive loss_ to pull the query embedding (extracted by the encoder) close to the embeddings of top LLMs while pushing far away from the embeddings of bottom LLMs. Based on this loss, our RouterDC could equally select one of top-performing LLMs for a query and hence alleviate the shortcoming of ZOOTER introduced previously. We empirically observe that training the router using the sample-LLM contrastive loss alone is not stable as similar queries can have dissimilar embeddings and be assigned to different LLMs. To improve the training stability, we cluster all the training queries into multiple groups and design a _sample-sample contrastive loss_ to maximize the similarity between queries in the same group while minimizing the similarity between queries from different groups.

We conduct experiments on challenging reasoning tasks (language understanding, code generation, and mathematical reasoning) to evaluate the proposed RouterDC in both in-distribution and out-of-distribution settings. Empirical results show that RouterDC can harness the complementary potentials of LLMs, achieving state-of-the-art performance. Moreover, RouterDC outperforms existing routing methods by a large margin, showing that the proposed two contrastive losses are more beneficial for training the RouterDC.

Our contributions are summarized as follows. (i) We propose a _novel framework_ to learn a router to select the suitable LLM for each query by dual contrastive learning, which consists of sample-LLM and sample-sample contrastive losses; (ii) The proposed RouterDC is _parameter-efficient_ (with fewer than 100M parameters) and _computation-efficient_ (without backpropagating the gradients through LLMs) in training. Moreover, RouterDC is also efficient in inference (\(6\) faster than Voting) as it only requires computation cost for the selected LLM and negligible cost for the router;(iii) Experimental results show that RouterDC _effectively assembles LLMs_ and outperforms individual top-performing LLMs as well as existing routing methods on both in-distribution (+2.76%) and out-of-distribution (+1.90%) tasks.

## 2 Related Work

**Large Language Models (LLMs).** LLMs have achieved great success in natural language processing and many foundation models have been released online [24; 46; 45; 27]. Many prior works [54; 47; 58; 9; 41; 20; 34; 52] focus on fine-tuning those foundation models to obtain specialized LLMs for solving versatile tasks, for example, language understanding [58; 20; 26], code generation [41; 8], and mathematical reasoning [54; 34]. In this paper, we study the problem of assembling LLMs to harness their strengths by a router.

**LLM Ensembling.** The goal of LLM ensembling is to leverage multiple LLMs to boost performance compared with a single model across various downstream tasks. Voting [31; 51] is a simple but effective ensemble method. Jiang et al.  further propose PairRanker and GenFuser to generate an improved output from the outputs of all LLMs, which needs to call LLMs \((T^{2})\) times with \(T\) as the number of LLMs. LLM cascading [2; 13; 38; 55] query a list of LLMs (whose capacity depends on the model size) sequentially until an LLM's output is satisfied (i.e., having a significantly high confidence score), which is returned as the final output. Fusion of Experts  concatenates all LLMs outputs to build the final output and casts it as a supervised learning problem. Unlike the aforementioned ensembling methods which require querying the LLMs at least \((T)\) times in inference, our RouterDC is much more efficient as it only needs to call the selected LLM once.

**LLM Routing.** LLM routing aims to select the most suitable model for a query without calling all LLMs. Many works have been proposed to design an effective routing strategy. Shnitzer et al.  propose a collection of binary classifiers to evaluate the correctness of each LLM. Lu et al.  propose ZOOTER to align a router with the supervision from the reward model. LoraRetriever  propose a task-wise router to select the LLM by predicting the task identity of the query. Srivatsa et al.  explore the routing ability using both classifier-based and clustering-based approaches. The aforementioned methods neglect the fact that multiple LLMs may be well-suited to answer a single query. Ding et al.  design a cost-effective router for two LLMs (a small LLM and a large one). In contrast, the proposed RouterDC can be used for multiple LLMs simultaneously.

**Contrastive Learning.** Contrastive learning learns effective representations by distinguishing between similar and dissimilar pairs of data points. It has been widely used in various tasks, such as visual representation learning [4; 14], sentence representation leaning [12; 53; 42], and vision-language alignment [39; 59]. In this paper, we propose two contrastive losses to learn the RouterDC for assembling LLMs.

## 3 Methodology

In this section, we propose RouterDC, a framework for learning a query-based router to assemble LLMs. An overview is illustrated in Figure 1. We introduce the problem of router learning in Section 3.1 and design a scoring method to measure the performance of LLMs on each training query (Section 3.2). Next, we propose two contrastive losses to train the router, including a sample-LLM contrastive loss for learning the routing strategy (Section 3.3) and a sample-sample contrastive loss for improving training stability (Section 3.4). The training and inference procedures are provided in Algorithm 1.

### Problem Formulation

Consider a set of LLMs \(\{_{t}:t=1,,T\}\) and a training set \(_{}=\{(_{i},y_{i}):i=1,,n\}\), where \(_{i}\) is a query (i.e., question) and \(y_{i}\) is its answer (i.e., ground truth). Usually, no single LLM is universally suitable for all queries in \(_{}\). Moreover, LLMs are diverse and have different architectures (e.g., Mistral-based , LLaMA-based ), making it infeasible to merge all LLMs into a single model [36; 22; 28]. In this paper, we study the problem of assembling LLMs by learning a router to select the suitable LLM for each query. The router takes \(\) as input and produces the probability distribution of \(T\) LLMs being selected. As training and testing queries may come from different data distributions, the learned router is expected to generalize well on both in-distribution and out-of-distribution scenarios.

### Scoring

To learn the router, we need to design a scoring method to assess the performance of LLMs on queries. For an _open-ended_ generation query \(_{i}\) (requiring a long answer, e.g., GSM8K , with an example shown in Example 1), one can directly compare the ground truth \(y_{i}\) with the output of the LLM \(_{i}^{(t)}=_{t}(_{i})\) generated by greedy decoding. Though greedy decoding is simple and efficient, its inherent shortsightedness often prevents it from discovering the optimal solution. Conversely, sampling, like beam sampling , is an advanced approach that is widely used in practice as it explores multiple alternatives in the search space, potentially leading to better results. We repeatedly feed the query \(_{i}\) to the LLM \(_{t}\)\(M\) times to obtain outputs \(\{_{i,m}^{(t)}:m=1,,M\}\). Then, we define the score of LLM \(_{t}\) on the query \(_{i}\) as:

\[s_{i}^{(t)}=_{m=1}^{M}(_{i,m}^{(t)},y_{i}),\] (1)

where evaluate\((,y)\) gives 1 if the prediction \(\) is correct otherwise 0.

For a _multiple-choice question_\(_{i}\) with an option set \(_{i}\) (e.g., MMLU , as an example shown in Example 1), sampling is unnecessary as we can simply define the score based on the probability of options, i.e.,

\[s_{i}^{(t)}=_{_{t}}(_{i}^{(t)} |_{i})}{_{a_{t}}_{_{t}}(a| _{i})}&_{i}^{(t)}=y_{i}\\ 0&\] (2)

where \(_{_{t}}(a|_{i})\) is the probability of option \(a\) predicted by the LLM \(_{t}\). According to Eq. (2), when the LLM \(_{t}\) outputs a correct option (i.e., \(_{i}^{(t)}=y_{i}\)), we normalize the probability to make it comparable across different LLMs, which will be used in Section 3.3; When the LLM \(_{t}\) generates a wrong option, \(s_{i}^{(t)}\) is set to \(0\) to punish \(_{t}\) for \(_{i}\). Based on the scores \(\{s_{i}^{(t)}:t=1,,T\}\), we introduce a sample-LLM contrastive loss in the next section.

```
1:An open-ended question from GSM8K :
2:Question:Tim has 30 less apples than Martha, and Harry has half as many apples as Tim. If Martha has 68 apples, how many apples does Harry have?
3:\(\) has 68-30 = 68-30=38 apples. Harry has 38/2 = 38/2=19 apples. #### 19

A _multiple-choice_ question from MMLU :
3:Question:An object is placed 100cm from a plane mirror. How far is the image from the object?
4:A. 50cm B. 100cm C. 200cm D. 300cm
5:\(\) ```

**Algorithm 1**Example 1

### Sample-LLM Contrastive Loss

As illustrated in Figure 1, the proposed RouterDC consists of an encoder \((;)\) parameterized by \(\) (where in our experiments \((;)\) uses a small language model mDeBERTaV3-base ) to map \(\) into an embedding in \(^{p}\), and \(T\) learnable LLM embeddings \(\{_{t}^{p}:t=1,,T\}\) for the \(T\) LLMs. For a query \(_{i}\), the RouterDC generates a selection probability distribution over \(T\) LLMs as

\[R(_{i};)=[(( _{i};),_{1}),,(( _{i};),_{T})],\] (3)

where \(\{,_{1},_{2},,_ {T}\}\) denotes the set of the parameters in RouterDC, \((,)\) denotes the cosine similarity, and \(()\) denotes the softmax normalization.

One can train the router by minimizing the distance between the output of the router and a score distribution over \(\{s_{i}^{(t)}:t=1,,T\}\), i.e., \(_{}_{(_{i},y_{i})_{}} (R(_{i};),[s_{i}^{(1)},, s_{i}^{(T)}])\)where \((,)\) is the Kullback-Leibler divergence . This KL loss is recently used in  for LLM routing, but we argue that it may not be a good proxy for training the router since the goal of the router is to assign queries to _top-performing_ LLMs instead of aligning the scores with \(R(_{i};)\), particularly for the _bottom-performing_ LLMs.

We draw inspiration from contrastive learning [37; 23] and propose a sample-LLM contrastive loss to learn the router. For a query \(_{i}\), we construct its positive LLMs index set \(^{+}_{i}\) and its negative LLMs index set \(^{-}_{i}\) based on the scores \(\{s^{(t)}_{i}:t=1,,T\}\) as: \(^{+}_{i}\) consists of the indices of LLMs corresponding to the top-\(K_{+}\) scores, while \(^{-}_{i}\) consists of the indices of LLMs corresponding to the bottom-\(K_{-}\) scores with \(s^{(t)}_{i}<0.5\). Note that \(K_{+}\) can be larger than \(1\) (\(K_{+}=3\) in our experiments) as there can be multiple LLMs that are suitable for a query in practice. We expect the router to pull the query embedding \((_{i};)\) closer to the positive LLMs' embeddings \(\{_{t_{+}}:t_{+}^{+}_{i}\}\) while pushing apart from the negative LLMs' embeddings \(\{_{t_{-}}:t_{-}^{-}_{i}\}\). To this end, we propose the sample-LLM contrastive loss as

\[_{}(_{i},y_{i};)=_{t_{+} ^{+}_{i}}-((_{i}; ),_{t_{+}})}}{e^{((_{i}; ),_{t_{+}})}+_{t_{-}^{-}_{i}}e^{((_{i};),_{t_{-}})}}.\] (4)

### Sample-Sample Contrastive Loss

We empirically find that training the router by minimizing the sample-LLM contrastive loss alone is not stable (refer to Figure 12 in Section 4.4). The reason is that some similar queries can have dissimilar embeddings and may be routed to different LLMs. To improve the robustness of the router, we introduce a sample-sample contrastive loss to encourage the encoder to generate similar embeddings for similar queries.

First, we cluster queries into multiple groups by unsupervised clustering. Specifically, we extract the embeddings of all training queries using a pre-trained encoder (i.e., mDeBERTaV3-base ) and transform them into low-dimensional vectors by the t-SNE algorithm . Then the \(k\)-means clustering algorithm  is used to cluster these low-dimensional vectors into \(N\) groups \(\{_{1},,_{N}\}\).

Next, we construct a sample-sample contrastive loss to encourage samples in the same group to have similar embeddings. Specifically, for a query \(_{i}_{j}\), we randomly select an in-group query \(_{i}^{+}_{j}\) and an out-group set \(_{i}^{-}\{_{j^{} j}_{j^{}}\}\) of \(H\) queries from the training mini-batch at each iteration. Similar to the sample-LLM contrastive loss, we propose a sample-sample contrastive loss to pull the embedding of \(_{i}\) closer to the embedding of \(_{i}^{+}\) while pushing it away from the embedding of queries in \(_{i}^{-}\). Formally, the sample-sample contrastive loss is formulated as

\[_{}(_{i};)=-((_{i};),(_{i}^ {+};))}}{e^{((_{i};), (_{i}^{+};))}+_{_{i}^{-} _{i}^{-}}e^{((_{i};), (_{i}^{-};))}}.\] (5)

### Training and Inference

**Training.** We learn a router \(R(;)\) by minimizing the final objective consisting of sample-LLM and sample-sample contrastive losses, i.e.,

\[(_{};)=_{(_{i},y_{i })_{}}_{}(_{ i},y_{i};)+\ _{}(_{i};),\] (6)

where \(>0\) is a hyper-parameter. In our experiments, \(\) is set to 1.

RouterDC contains fewer than \(100\)M parameters (that is, the encoder model \((;)\) is small and the number of parameters in the LLM embeddings \(\{_{1},,_{T}\}\) is negligible), thus it is parameter-efficient. Moreover, training the router is computationally efficient as it does not require backpropagating the gradients through the LLMs.

**Inference.** During inference, for each testing query \(^{}\), we compute \(R(^{};)\) and select the LLM with the largest probability, i.e., \(t^{}=*{arg\,max}_{t\{1,,T\}}( (^{};),_{t})\). Then we generate the prediction as \(}^{}=_{t^{}}(^{})\).

Compared with existing LLM assembling methods like voting  and cascade , which may call LLMs multiple times for a query, RouterDC is much more efficient as it only needs to call the selected LLM once.

## 4 Experiments

### Experimental Setup

**Candidate LLMs.** We choose seven open-source LLMs from HuggingFace1: (i) _Mistral-7B_ is a general LLM released by the Mistral-AI team; (ii) _MetaMath-Mistral-7B_ is fine-tuned on the MetaMathQA dataset ; (iii) _zephyr-7b-beta_ is an aligned version of Mistral-7B using direct preference optimization  on a mix of publicly available, synthetic datasets; (iv) _Chinese-Mistral-7B_ expands the vocabulary and incrementally pre-trains Mistral-7B on Chinese corpus; (v) _dolphin-2.6-mistral-7b_ is fine-tuned from Mistral-7B and released by Cognitive Computations; (vi) _Llama-3-8B_ is a general LLM developed by the Meta company; (vii) _dolphin-2.9-llama3-8b_ is fine-tuned from Llama-3-8B and released by Cognitive Computations. The first five LLMs are Mistral-based, while the last two LLMs are Llama-3-based.

**Datasets.** We evaluate in various tasks: (i) MMLU  is a general benchmark that covers 57 subjects; (ii) GSM8K  is a mathematical benchmark with diverse grade school questions; (iii) CMMLU  is a comprehensive Chinese benchmark that covers 67 subjects ranging from basic to advanced professional levels; (iv) ARC-C  is a reasoning benchmark containing different grade-school level questions; and (v) HumanEval  is a code completion benchmark consisting of programming problems assessing language comprehension, algorithms, and simple mathematics. For GSM8K, we use its default training and testing split. As the rest tasks do not have a default split, we randomly split the dataset into training (70%) and testing (30%) sets. All the training sets are unioned together to form the total training set \(_{}\) for learning the router. The learned router is then evaluated on the testing set of _in-distribution_ tasks.

We also evaluate the trained router on three _out-of-distribution_ (OOD) tasks: (i) PreAlgebra , which consists of basic university-level algebra problems; (ii) MBPP , which is a code benchmarkthat consists of 1,000 crowd-sourced Python programming problems; and (iii) C-EVAL , which is a comprehensive Chinese evaluation benchmark spanning 52 diverse disciplines and four difficulty levels.

**Baselines.** We compare RouterDC with the following baselines: (i) CosineClassifier, which treats the routing problem as a multi-class classification (the top-1 LLM is the label) and trains a cosine classifier on outputs of the encoder. CosineClassifier is equivalent to RouterDC with \(K_{+}=1\), \(K_{-}=T-1\), and \(=0\); (ii) Voting , which feeds the query to all LLMs and chooses the final prediction by majority voting; (iii) ZOOTER , which trains a router by supervised learning using rewards obtained by the scoring method in Section 3.2; (iv) LoraRetriever  designs a routing strategy based on task identities, which are unavailable in practice and we replace them with the cluster indices obtained by the clustering method in Section 3.4.

**Implementation Details.** Following , we use the Language Model Evaluation Harness package  for evaluation. For open-ended generation questions, we query LLMs \(M=10\) times by employing beam sampling with a temperature of \(0.2\) to calculate the score. For the router, we adopt mDeBERTaV3-base  as the encoder \((;)\), which is a small language model containing only 86M parameters. The dimension of each LLM embedding is set to \(768\). The hyper-parameters \(K_{+},K_{-},H\), and \(\) are set to \(3,3,3\), and \(1\), respectively. The number of clusters \(N\) is set to \(5\). The router is trained for \(1000\) steps using the AdamW  optimizer with a learning rate of \(5 10^{-5}\), a weight decay of \(0.01\), and a mini-batch size of \(64\). All experiments are run on NVIDIA A100 80GB GPUs.

### Main Results

**In-Distribution Results.** Table 1 shows the testing accuracies on five in-distribution tasks. As can be seen, RouterDC achieves the highest average accuracy, surpassing the best individual LLM (i.e., dolphin-2.9-llama3-8b) by a large margin of \(3.98\%\). RouterDC achieves accuracy improvements over the top-performing individual model on three tasks, with an increase of \(+0.51\%\) for GSM8K, \(+0.57\%\)

    & & MMLU & GSM8K & CMMLU & ARC-C & HumanEval & Avg & Time (m) \\   &  &  & 36.71 & 43.83 & 49.43 & 28.98 & 44.22 & 6.94 \\  & & MetaMath-Mistral-7B  & 59.86 & 69.63 & 43.83 & 48.30 & 29.80 & 50.28 & 7.23 \\  & & zephy-7B-beta  & 59.81 & 33.00 & 42.82 & 57.95 & 22.04 & 43.13 & 6.73 \\  & & Chinese-Mistral-7B  & 57.42 & 41.03 & 49.67 & 43.47 & 21.43 & 42.60 & 7.11 \\  & & dolphin-2.6-mistral-7b  & 60.53 & 52.38 & 43.71 & 52.56 & 45.10 & 50.86 & 6.91 \\  & & Meta-Llama-3-8B  & **64.59** & 47.76 & **51.77** & 49.43 & 26.73 & 48.06 & 6.33 \\  & & dolphin-2.9-llama3-8b  & 59.46 & 69.81 & 44.72 & 49.43 & 49.39 & 54.56 & 5.33 \\   & Voting  & 63.30 & 67.39 & 47.48 & 50.85 & 42.85 & 54.37 & 46.59 \\   & & CosineClassifier & 59.72 & 69.03 & 45.47 & 50.57 & 46.33 & 54.22 & 8.30 \\   & & ZOOTER  & 60.48 & 66.69 & 45.27 & 53.13 & 44.29 & 53.97 & 8.01 \\   & & LoraRetriever (clustering)  & 63.33 & 66.63 & **51.77** & 57.10 & 40.00 & 55.77 & 7.86 \\   & & RouterDC & 61.07 & **70.32** & **51.77** & **58.52** & **51.02** & **58.54** & 7.97 \\   

Table 1: Testing accuracy (\(\%\)) on in-distribution tasks. “Time” denotes the total inference time in minutes. The best is in **bold** and the second-best is underlined.

    & & PreAlgebra & MBPP & C-EVAL & Avg & Time (m) \\   &  & 24.80 & 37.90 & 46.43 & 36.38 & 4.31 \\  & & MetaMath-Mistral-7B  & 39.15 & 37.74 & 45.17 & 40.69 & 4.13 \\  & & zephy-7B-beta  & 20.78 & 31.14 & 44.87 & 32.26 & 4.30 \\  & & Chinese-Mistral-7B  & 18.48 & 29.64 & 48.44 & 32.19 & 4.40 \\  & & dolphin-2.6-mistral-7b  & 29.28 & 44.86 & 45.10 & 39.75 & 3.20 \\  & & Meta-Llama-3-8B  & 27.67 & 43.02 & **52.01** & 40.90 & 3.95 \\  & & dolphin-2.9-llama3-8b  & **39.72** & **47.34** & 44.80 & 43.95 & 3.15 \\   &  & 39.03 & 41.60 & 48.50 & 43.04 & 27.43 \\   & & CosineClassifier & 36.97 & 38.48 & 47.77 & 41.07 & 4.43 \\   & & ZOOTER  & 34.44 & 41.10 & 44.95 & 40.16 & 4.28 \\   & & LoraRetriever (clustering)  & 35.36 & 43.12 & **52.01** & 43.50 & 4.22 \\   & & RouterDC & 38.81 & 46.80 & 51.93 & **45.85** & 4.24 \\   

Table 2: Testing accuracy (\(\%\)) on out-of-distribution tasks. “Time” denotes the total inference time in minutes. The best is in **bold** and the second-best is underlined.

for ARC-C, and \(+1.63\%\) for HumanEval. Compared with ZOOTER and CosineClassifier, RouterDC consistently performs better on all the tasks, demonstrating that the proposed dual contrastive losses can train a more effective router. Furthermore, RouterDC achieves an average accuracy improvement of \(+2.77\%\) over LoraRetriever, validating the usefulness of the sample-LLM contrastive loss. Additionally, RouterDC, with only \(28.3\) minutes for training, outperforms Voting on four of five tasks and is about \(6\) faster in inference.

**Out-of-Distribution Results.** Table 2 shows the testing accuracy on three OOD tasks. As can be seen, the proposed RouterDC again achieves the highest accuracy on average, exceeding the best-performing individual LLM (i.e., _dolphin-2.9-llama3-8b_) by a large margin of \(1.9\%\). For each task, RouterDC has roughly comparable performance with the best-performing individual LLM, e.g., 38.81 vs. 39.72 on PreAlgebra, 46.80 vs. 47.34 on MBPP, and 51.93 vs. 52.01 on C-EVAL, which demonstrates that RouterDC can select suitable LLMs for queries from OOD tasks. Among all routing methods, only our RouterDC can surpass _dolphin-2.9-llama3-8b_, confirming that RouterDC has a better generalization ability. Compared with Voting, RouterDC performs better on all tasks except PreAlgebra, on which they are comparable.

### Sensitivity Analysis

**Effects of \(\).** We conduct an experiment to study the effect of \(\) in Eq. (6) w.r.t. the testing accuracy. From Figure 5 (the detailed results are in Table 5 of Appendix A), we can see that using two contrastive losses together (i.e., \(=1\)) achieves better overall performance than using the sample-LLM contrastive loss alone (i.e., \(=0\)). Moreover, the overall performance of RouterDC is insensitive to a wide range of \([0.5,5]\), making it easy to choose the value of \(\) in practice.

**Effects of number of clusters \(N\).** We conduct an experiment to study the effect of the number of clusters (i.e., \(N\)) used in the sample-sample contrastive loss w.r.t. the testing accuracy. From Figure 6, we can find that RouterDC is insensitive to a wide range of \(N\). Moreover, increasing \(N\) leads to higher average accuracy when \(N\) is small (\( 4\)), but the accuracy saturates quickly.

**Effects of number of out-group queries \(H\).** Figure 7 shows the testing accuracy with \(H\). When \(=0\), \(_{}\) is constant, which means using \(_{}\) alone is not the best configuration. Moreover, the values of \(H 1\) play a negligible influence on the average performance of RouterDC.

**Effects of \(K_{+}\) and \(K_{-}\).** To investigate the sensitivity of \(K_{+}\) and \(K_{-}\), we conduct an experiment using the setting in Section 4.1. Figure 8 shows the average testing accuracy w.r.t. \(K_{+}\) and \(K_{-}\) with the in-distribution setting. As can be seen, for all the configurations, RouterDC outperforms the best individual LLM (i.e., \(54.56\%\) for _dolphin-2.9-llama3-8b_ in Table 1). Note that among all the configurations, RouterDC (with \(K_{+}=1\) and \(K_{-}=6\)) performs worse, showing that selecting only the top-\(1\) LLM as positive and the other LLMs as negative is inappropriate for learning the router.

Figure 8: Average testing accuracy w.r.t. \(K_{+}\) and \(K_{-}\) on five in-distribution tasks. Lighter color indicates higher percentage. Figure 9: Distribution of testing queries over LLMs. Lighter color indicates higher percentage. Figure 10: Average cosine similarity between LLMs and query embeddings. Lighter color indicates higher similarity.

### Analysis

**Does RouterDC select the suitable LLM for each query?** To answer this question, we analyze the assignment of testing queries to LLMs in each task. Figure 10 shows the distribution, which has a clear structure on both in-distribution and out-distribution tasks. For example, most GSM8K and PreAlgebra queries are assigned to MetaMath-Mistral-7B and dolphin-2.9-llama3-8b, which have strong mathematical ability (Tables 1 and 2). To further investigate the routing rule of RouterDC, we compute the average cosine similarity between LLMs and the query embeddings for each task. As shown in Figure 10, the similarity matrix is roughly aligned with the assignment matrix in Figure 10. For example, embeddings of GSM8K and PreAlgebra queries are more similar to MetaMath-Mistral-7B and dolphin-2.9-llama3-8b than to other LLMs.

**Visualization of Training Queries.** Figure 12 shows the t-SNE visualization  of the embeddings of training queries using a pre-trained encoder mDeBERTaV3-base . As shown, except for HumanEval, all tasks have a clear clustering structure, confirming that using unsupervised clustering in Section 3.4 is reasonable.

**Effectiveness of \(_{}\).** We conduct experiments to study the effectiveness of \(_{}\) (Eq. (5)). Figure 12 shows the training and testing accuracy curves of RouterDC (with or without \(_{}\)) on GSM8K. As can be seen, the training curve of RouterDC (w/o \(_{}\)) exhibits considerable oscillation, whereas that of RouterDC is much more stable. Figure 15(a) in Appendix B shows t-SNE visualization of training query embeddings extracted by the trained encoder of RouterDC (w/o \(_{}\)). As can be seen, query embeddings belonging to different tasks are roughly mixed together. Example 2 in Appendix B provides two similar GSM8K queries, which both require basic calculation of shopping costs. Their embeddings have very low similarity (only \(-0.4589\)) when the router is trained by \(L_{}\) alone. After integrating \(L_{}\), training query embeddings have a clear cluster structure (Figure 15(b)) with the similarity between these two example queries increases to \(0.9982\). Furthermore, RouterDC achieves higher testing accuracy than its counterpart, verifying the effectiveness of \(_{}\).

**Routing to Different Numbers of LLMs.** We evaluate the performance of RouterDC when the number of LLMs increases. Figure 12 shows the testing accuracies on five in-distribution tasks. As can be seen, adding LLMs consistently enhances the average accuracy. Table 8 in Appendix A shows the detailed results and configurations.

**Robustness to LLM Losses during Inference.** In a production environment, the loss of model servers is sometimes unavoidable due to various reasons such as network problems, thus placing crucial requirements on the robustness of the router. We conduct an experiment to validate therobustness of RouterDC by removing an LLM during inference. Table 3 shows the testing accuracies on five in-distribution tasks. We can see that RouterDC reliably withstands the loss of any single LLM. The robustness is attributed to the fact that multiple LLMs (with top scores) are chosen as positive labels in the sample-LLM contrastive loss, and they can be regarded as each other's backup.

**Cost-Effectiveness.** As cost is an important metric to evaluate LLMs, following , we conduct experiments on two tasks (i.e., GSM8K and MBPP) to consider the LLM costs. We modify the score \(s_{i}^{(t)}\) to \(s_{i}^{(t)}+c_{i}^{(t)}\), where \(c_{i}^{(t)}\) is the cost of query \(_{i}\) using the \(t\)th LLM. As can be seen from Figure 14, RouterDC is more cost-effective than CosineClassifier and ZOOTER on both tasks.

**Discussions on Availability of Task Identity.** In Section 3.4, we cluster samples into \(N\) groups and apply the sample-sample contrastive training to encourage similar queries with similar embeddings. However, when the task identity is available in the training dataset, the samples can be naturally grouped into different tasks. To explore the performance of RouterDC with additional task identity, we replace the \(\{_{1},,_{N}\}\) with the groups of different tasks and conduct experiments on five in-distribution tasks. Table 4 shows the testing accuracy comparison between RouterDC and its variant. As can be seen, RouterDC is comparable to RouterDC (w/ task identity), showing the effectiveness of the unsupervised clustering.

## 5 Conclusion

In this paper, we study the problem of training a router to assemble LLMs. We propose RouterDC to learn a query-based router using two novel contrastive losses (i.e., the sample-LLM and sample-sample contrastive losses). Experimental results show that RouterDC effectively assembles LLMs and outperforms individual top-performing LLMs as well as existing routing methods on both in-distribution and out-distribution tasks. As the proposed two contrastive losses are general, we consider applying them to other routing problems in future work.

    & MMLU & GSM8K & CMMLU & ARC-C & HumanEval & Avg \\  RouterDC (w/o task identity) & 61.07 & 70.32 & 51.77 & 58.52 & 51.02 & 58.54 \\ RouterDC (w/ task identity) & 64.49 & 69.63 & 51.77 & 57.95 & 49.39 & 58.65 \\   

Table 4: Testing accuracy(\(\%\)) of RouterDC w/ or w/o task identity.

Figure 14: Testing accuracy with different costs on RouterBench.