# Sharp Recovery Thresholds of

Tensor PCA Spectral Algorithms

 David L. Donoho

Department of Statistics

Stanford University

donoho@stanford.edu

&Michael J. Feldman

Department of Statistics

Stanford University

feldman6@stanford.edu

###### Abstract

Many applications seek to recover low-rank approximations of noisy tensor data. We consider several practical and effective _matricization_ strategies which construct specific matrices from such tensors and then apply spectral methods; the strategies include tensor unfolding, _partial tracing_, power iteration, and recursive unfolding. We settle the behaviors of unfolding and partial tracing, identifying sharp thresholds in signal-to-noise ratio above which the signal is partially recovered. In particular, we extend previous results to a much larger class of tensor shapes where axis lengths may be different. For power iteration and recursive unfolding, we prove that under conditions where previous algorithms partially recovery the signal, these methods achieve (asymptotically) exact recovery. Our analysis deploys random matrix theory to obtain sharp thresholds which elude perturbation and concentration bounds. Specifically, we rely upon recent _disproportionate_ random matrix results, which describe sequences of matrices with diverging aspect ratio.

## 1 Introduction

Tensors--multi-way arrays--and tensor methods are fundamental to modern data analysis. Data given by three or more indices are increasingly the focus in diverse areas including signal and image processing , while high-order moments represented by tensors are of interest in problems such as community detection and learning latent variable models .

The _spiked tensor model_, introduced by Montanari and Richard , is a simple statistical model of tensor data with latent low-rank structure. Observed data \(X\) of dimensions \(n_{1} n_{2} n_{k}\) are the sum of a low-rank tensor and noise:

\[X_{i_{1},i_{2},,i_{k}}= v_{1,i_{1}}v_{2,i_{2}} v_{k,i_{k}} +Z_{i_{1},i_{2},,i_{k}}\,, j[k],\,i_{j}[n_{j}]\,,\] (1)

where \(k 2\) is the tensor order, \(\) is a signal strength, \(v_{j}^{n_{j}-1}\), \(j[k]\), are unit vectors, and \(Z\) is a noise tensor. Noise entries are assumed to be independent standard Gaussians:

\[Z_{i_{1},i_{2},,i_{k}}}{{}}(0,1)\,,  j[k],\,i_{j}[n_{j}]\,.\]

Estimation of \(v_{1},,v_{k}\) and the low-rank component of \(X\) generalizes to tensors the problem of low-rank matrix approximation--principal component analysis (PCA)--and is therefore known as tensor PCA.

This model reduces for \(k=2\) to the _spiked matrix model_, with maximum likelihood estimators of \(v_{1}\) and \(v_{2}\) given by the first left and right singular vectors \(_{1}\) and \(_{2}\) of \(X\), respectively. For \(k 3\), however, computation of the maximum likelihood estimators of the low rank component amounts to solving an NP hard problem (Hillar and Lim ),

\[_{j[k],u_{j}^{j-1}}_{i_{1},,i_{k}}X_{i_{1},i_{2},,i_{k}}u_{1,i_{1}} u_{k,i_{k}}\,,\] (2)necessitating alternate, efficient algorithms.

### Contributions

This paper studies _matricization_-based approaches, which convert tensors to matrices (by reshaping or by contracting constructions to be described) and then apply spectral methods. We assume the spiked tensor model in a high-dimensional asymptotic framework in which each array dimension \(n_{1},,n_{k}\) tends to infinity; we make no assumptions on their relative rates. While previous analyses of tensor PCA algorithms have assumed order \(k=3\), supersymmetry \(v_{1}=v_{2}==v_{k}\), or hypercubical format \(n_{1}=n_{2}==n_{k}\), we permit tensors of diverse dimensions \(n_{j}\) and unrelated \(v_{j}\).

In this setting, we fully analyze tensor unfolding or reshaping, a widespread technique [12; 19]. Additionally, we analyze the _partial tracing_ approach of Hopkins et al. , discovering its asymptotic equivalence to unfolding in performance--an improvement over previous analysis. We identify sharp thresholds in signal-to-noise ratio above which these algorithms partially recover the signal, and provide exact formulas for their limiting performance. Finally, we study generalizations of the power iteration and recursive unfolding algorithms of . Above the same thresholds characterizing partial recovery of unfolding and partial tracing, these algorithms achieve (asymptotically) exact signal recovery. In other words, for signal-to-noise ratios such that unfolding or partial tracing partially recover the signal, power iteration and recursive unfolding exactly recover the signal.

Our approach relies upon fundamental and penetrating random matrix theoretic (RMT) results. Precise analysis of tensor PCA algorithms, including pinpointing of phase transitions and sharp performance quantification as provided here, is not possible purely via more standard tools in theoretical machine learning, such as matrix concentration and vector perturbation bounds. Specifically, we rely upon recent _disproportionate_ random matrix results (introduced in Section 1.3), which describe sequences of matrices with diverging aspect ratio.

This work concisely demonstrates in the context of tensor PCA the advantages of an RMT-backed approach. Indeed, application of spiked matrix results yields simple and elegant theorems and proofs which are easily read. For example, the analysis of the partial tracing in , involving challenging matrix concentration and perturbation calculations, is reduced to a few careful lines. Though our results are admittedly purely asymptotic, there are no hidden constants or logarithmic factors. Even for modest-sized tensors, simulations demonstrate close agreement with theory.

### Assumptions and Notation

Without loss of generality, we assume \( 0\) and \(n_{1} n_{2} n_{k}\), taking \(n_{1}\) to be the "fundamental" problem index; that is \(n_{j}=n_{j}(n_{1})\), \(j[k]\). We say an estimator \(_{j}\)_partially recovers_\(v_{j}\) if almost surely,

\[_{n_{1}}| v_{j},_{j}|>0\,,\]

and \(_{j}\)_exactly recovers_\(v_{j}\) if \(| v_{j},_{j}|1\). In words, partial recovery demands a positive limit for the cosine similarity, while exact recovery requires asymptotically perfect cosine similarity.

We make the rank-one signal assumption in model (1) purely for expository efficiency; the spectral algorithms in this paper naturally generalize to rank-\(r\) spiked tensors of the form

\[X_{i_{1},i_{2},,i_{k}}=_{i=1}^{r}_{r}v^{(i)}_{1,i_{1}}v^{(i) }_{2,i_{2}} v^{(i)}_{k,i_{k}}+Z_{i_{1},i_{2},,i_{k}}\,,  j[k],\,i_{j}[n_{j}]\] (3)

where \(r 1\) is the rank and \(\{v^{(1)}_{j},,v^{(r)}_{j}\}^{n_{j}-1}\) are orthonormal sets, \(j[k]\). Such tensor decompositions are unique by Kruskal's theorem . Where we estimate \(v^{(1)}_{j}\) by the first right singular vector of an appropriate matrix \(M_{j}\), \(v^{(2)}_{j},,v^{(r)}_{j}\) may be estimated by the subsequent right singular vectors, with analogous theoretical guarantees holding (that is, \(v^{(i)}_{j}\) is the right singular vector associated with the \(i\)-th largest singular value of \(M_{j}\)).

We denote by \(\) both the tensor outer product and Kronecker product (the latter is simply a vectorization of the former); it is clear from context which is meant. Let \(_{j}\) denote multiplication between a tensor along the \(j\)-th axis and a vector of conformable dimension. The notation \(a(n_{1}) b(n_{1})\)means \(a(n_{1}) Cb(n_{1})\) for a constant \(C>0\) and all sufficiently large \(n_{1}\), and \(a(n_{1}) b(n_{1})\) means \(a(n_{1}) b(n_{1}) a(n_{1})\).

Finally, we introduce tensor _slices_. Let \(T^{n_{1} n_{s}}\) and fix \([k]\). The set \(\{T_{i_{1},,i_{l}}:j[l],i_{j}[n_{j}]\}\) contains the slices of \(X\) of order \(k-\). Slices satisfy \(T_{i_{1},,i_{}}_{j=+1}^{k}^{n_{j}}\) and have entries

\[(T_{i_{1}, i_{}})_{i_{l+1},,i_{k}}=T_{i_{1},,i_{k}}\,,  j\{+1,,k\},i_{j}[n_{j}]\,.\]

### Spiked Matrix Model

The behavior of matricization-based algorithms fundamentally derives from spectral properties of the spiked matrix model,

\[X_{i_{1},i_{2}} = v_{1,i_{1}}v_{2,i_{2}}+Z_{i_{1},i_{2}}\,, i_{1}[n_{1}],\,i_{2}[n_{2}]\,.\] (4)

This model is extensively studied in random matrix theory, particularly under _proportional growth_, where \(n_{1}\) and \(n_{2}\) are of comparable magnitude:

\[n_{1},n_{2} \,, _{n_{1}} =}{n_{2}} >0\,.\]

Under proportional growth, the sample covariance matrix \(XX^{}/n_{2}\) does not converge to the identity (its expectation), and the leading left and right singular vectors \(_{1}\) and \(_{2}\) of \(X\) are inconsistent estimators of \(v_{1}\) and \(v_{2}\), respectively. We highlight the following results of Benaych-Georges and Rao Nadakuditi , who establish formulas for the limiting misalignment of \(_{1}\) and \(_{2}\):

**Lemma 1.1**.: _Let \(_{1}\) and \(_{2}\) denote the leading left and right singular vectors of \(X\), respectively, and define_

\[c^{2}(,) =1-)}{^{2}(^{2}+ )}&>^{1/4}\\ 0&^{1/4}\,.\]

_Under \(=(1+o(1))}\), where \( 0\) is fixed, and \(_{n_{1}}(0,1]\),_

\[| v_{1},_{1}|^{2} c^{2}(,)\,, | v_{2},_{2}|^{2} c^{2}(^{-1/2},^{-1})\,.\] (5)

Recently, Ben Arous et al.  and Feldman  independently studied the spiked matrix model under _disproportional growth_, where \(_{n_{1}} 0\) or \(_{n_{1}}\). Transposing \(X\) in case \(_{n_{1}}\), we assume without loss of generality that \(_{n_{1}} 0\). Phase transitions for the left and right singular vectors no longer coincide; \(v_{1}\) is reliably estimated at signal strengths much weaker than \(}\). Drawing upon results in , , and , we have the following disproportionate analogs of Lemma 1.1:

**Lemma 1.2**.: _Let \(_{1}\) and \(_{2}\) denote the leading left and right singular vectors of \(X\), respectively, and define_

\[^{\,2}() =(1-^{-4})_{+}\,, ^{\,2}() =}{1+^{2}}\,.\]

_Under \(=(1+o(1))(n_{1}n_{2})^{1/4}\), where \( 0\) is fixed, and \(_{n_{1}} 0\),_

\[| v_{1},_{1}|^{2} ^{\,2}()\,, | v_{2},_{2}|^{2} 0\,,\] (6)

_while under \(=(1+o(1))}\),_

\[| v_{1},_{1}|^{2} 1\,, | v_{2},_{2}|^{2} ^{\,2}()\,.\] (7)

Limits (7) are corollaries of Theorems 2.9 and 2.10 of . We note that  requires \(n_{2}\) is polynomially bounded in \(n_{1}\). This assumption, however, is necessary only to establish non-asymptotic bounds; the almost-sure results in Lemma 1.2 hold as \(_{n_{1}} 0\) arbitrarily rapidly. While not explicitly stated in these references, it is easily verified that under proportional growth, \(/}>^{1/4}\) implies partial recovery of \(v_{1}\) and \(v_{2}\). Analogous statements hold for disproportional growth.

Tensor Unfolding

We study a general unfolding procedure that permits (1) tensors of general, unequal axis lengths and (2) unfolding along arbitrary sets of axes.

Let \(N_{j}=_{=j}^{k}n_{}\), \(j[k]\), and for \(S[k]\), \(S\), let \(N(S)=_{ S}n_{}\). We define a map \(_{S}:^{n_{1} n_{k}}^{(N_{1}/ N(S)) N(S)}\) as follows: for indices \(i_{j}[n_{j}],j[k]\),

\[a=1+_{j[k] S}(i_{j}-1)_{[k] S  j<}n_{}\,, b=1+_{j S}(i_{j}-1)_{  S j<}n_{}\] (8)

and

\[[_{S}(T)]_{a,b}=T_{i_{1},i_{2},,i_{k}}\,.\] (9)

When unfolding along a single axis, we write \(_{j}=_{\{j\}}\). In this case, (8) reduces to

\[a=1+_{=1}^{j-1}(i_{}-1)}{n_{j}}+_{ =j+1}^{k}(i_{}-1)N_{+1}\] (10)

(taking \(N_{k+1}=1\)) and \(b=i_{j}\). The unfolded matrix \(_{j}(X)\) is a spiked matrix with aspect ratio \(n_{j}^{2}/N_{1}\):

\[_{j}(X)=(v_{1} v_{j-1} v_{j+1}  v_{k})v_{j}^{}+_{j}(Z)\,.\]

Similarly, \(_{S}(X)\) is a spiked matrix with aspect ratio \(N(S)^{2}/N_{1}\):

\[_{S}(X)=(_{j[k] S}v_{j})( _{j S}v_{j})^{}+_{S}(Z)\,.\] (11)

We now review prior results on unfolding. For \(v_{1}==v_{k}\), Montanari and Richard consider the unfolding \(S=\{1,, k/2\}\), estimating \(^{ k/2}v_{1}\) by the first right singular vector of \(_{S}(X)\). They prove that \(^{ k/2}v_{1}\) is (partially) recovered for \( n_{1}^{ k/2/2}\), and conjecture sufficiency of \( n_{1}^{k/4}\) (note that these bounds differ only for odd \(k\)). Theorem 5.8 of Hopkins et al.  completes this conjecture for \(k=3\), establishing sufficiency of \((1+)n_{1}^{3/4}\).

Complete analysis of unfolding--handling tensors of arbitrary order and asymmetric dimensions--relies on results in Section 1.3, which reveal (1) necessary and sufficient thresholds for recovery and (2) the exact limiting performance of estimates. In addition, spiked matrix results yield the exact limiting cosine similarity of unfolding procedures, not obtained in  or . The recent disproportional results summarized in Lemma 1.2 are crucial as unfoldings such as \(_{j}(X)\), \(j[k-1]\), have limiting aspect ratio zero (recall that \(n_{1} n_{k}\)). Depending on the relative growth rates of \(n_{1},,n_{k}\), unfoldings such as \(_{k}(X)\) may fall under disproportional "all" growth (\(n_{k}^{2}/N_{1} 0\)), proportional growth (\(n_{k}^{2}/N_{1} 1\)), or disproportional "wide" growth (\(n_{k}^{2}/N_{1}\)). Our analysis is similar to that of Ben Arous et al. , which permits arbitrary order \(k\) yet assumes \(n_{1}==n_{k}\).

**Theorem 2.1**.: _Let \(S[k]\). If \(N(S)^{2}/N_{1} 0\), the first right singular vector \(w\) of the unfolding \(_{S}(X)\) partially recovers \(_{j S}v_{j}\) if and only if \(/N_{1}^{1/4}>1\). In particular, for \(=(1+o(1))N_{1}^{1/4}\) with \( 0\) fixed,_

\[|_{j S}v_{j},w|^{2} ^{\,2}()\,.\] (12)

_If \(N(S)^{2}/N_{1} 1\), \(_{j S}v_{j}\) is partially recovered if and only if \(/N_{1}^{1/4}>1\). For \(_{n_{1}}\) and \(=(1+o(1))N_{1}^{1/4}\),_

\[|_{j S}v_{j},w|^{2}c^{ \,2}(^{1/4},)\,.\] (13)

_Finally, if \(N(S)^{2}/N_{1}\), \(_{j S}v_{j}\) is partially recovered if and only if \(/>0\). For \(=(1+o(1))\),_

\[|_{j S}v_{j},w|^{2} ^{\,2}()\,.\] (14)All proofs are deferred to the supplement. As the recovery threshold is identical across all unfoldings \(S\) such that \(N(S)^{2}/N_{1} 1\), we propose the following simple algorithm to estimate \(v_{j}\), which may be iterated for \(j[k]\):

``` Input:\(X,j\) return first right singular vector of \(_{j}(X)\) ```

**Algorithm 1** Tensor unfolding

**Corollary 2.1.1**.: _Algorithm 1 partially recovers \(v_{1},,v_{k-1}\) if and only if \(/N_{1}^{1/4}>1\). Vector \(v_{k}\), corresponding to the largest dimension, is partially recovered as well if \(n_{k}^{2}/N_{1} 1\). On the other hand, if \(n_{k}^{2}/N_{1}\), \(v_{k}\) is recovered if and only if \(/}>0\)._

## 3 Partial Tracing

Hopkins et al.  propose the partial tracing method for tensors of order \(k=3\) as an alternative to unfolding. Their approach generalizes to orders \(k 3\), although it inherently relies on supersymmetry

Figure 1: Simulations of tensor unfolding with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit \(()\), which agrees closely. Below the phase transition located at \(=1\), \(_{1}\) is approximately uniformly distributed on the surface of the unit sphere, so \(| v_{1},_{1}|=O(n^{-1/2})\).

Figure 2: Simulations of tensor unfolding with \(k=3\), \(n_{1}=n_{2}=50\), and \(n_{3}=10000\) (left) or \(n_{3}=20000\) (right). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed lines are theoretical limits based on Theorem 2.1, which agree closely. As \(n_{3}/(n_{1}n_{2})=O(1)\), we compare \(| v_{1},_{1}|\) and \(| v_{2},_{2}|\) to \(()\) and \(| v_{3},_{3}|\) to \(c((N_{1}/n_{3}^{2})^{1/4},N_{1}/n_{3}^{2})=c(/},N_{1}/n _{3}^{2})\).

of the signal, \(v_{1}==v_{k}\). The partial tracing operator \(_{k}:^{k}^{n}^{n n}\) constructs a matrix by linearly combining tensor slices with partial trace weights:

\[_{k}(T)=_{i_{1},,i_{k-2}[n]}(T_{i_{1},,i_{ k-2}})T_{i_{1},,i_{k-2}}.\] (15)

This operation of reducing the order of a tensor by linear combinations of slices is called contraction in tensor analysis.

``` Input:\(X\) return first right singular vector of \(_{k}(X)\) ```

**Algorithm 2** Partial tracing

For \(k=3\), the runtime of Algorithm 2 is \(O(n^{3})\), while that of unfolding is \(O(n^{3} n)\) (see Table 3 of ). Hopkins et al. established a bound on the recovery threshold of partial tracing which is worse than that of unfolding by a logarithmic factor: \( n^{3/4}( n)^{1/2}\). We eliminate the logarithmic factor here: while delivering improvements in runtime, Algorithm 2 is asymptotically equivalent to unfolding in recovery performance.

**Theorem 3.1**.: _The first right singular vector \(_{1}\) of the partial trace matrix \(_{k}(X)\) partially recovers \(v_{1}\) if and only if \(/n_{1}^{k/4}>1\). For \(=(1+o(1))n_{1}^{k/4}\), where \(\) is fixed,_

\[| v_{1},_{1}|^{2}^{2}()\,.\] (16)

Under \(n_{1}==n_{k}\), the behavior of unfolding is given by (12), matching (16) exactly. Thus, unfolding and partial tracing are asymptotically equivalent in performance.

## 4 Exact Recovery

Montanari and Richard  consider a two-step recursive unfolding procedure that remarkably _exactly_ recovers \(v_{1}\) above the threshold of unfolding, assuming even order \(k\), hypercubical shape \(n_{1}==n_{k}\), and a supersymmetric signal, \(v_{1}==v_{k}\). In this section, we prove that exact recovery is possible for tensors of arbitrary axis lengths. We consider generalizations of the power iteration and recursive unfolding algorithms of  (initialized via tensor unfolding). Tensor power iteration is previously studied in  (assuming a supersymmetric signal) and  (assuming

Figure 3: Simulations of partial tracing with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit \(()\), which agrees closely. Note the similarity to Figure 1.

\(n_{1} n_{2} n_{k}\)), though these works assume the initial iterate is independent of \(X\), precluding initialization via unfolding.

In our more general setting, we find that unfolding-initialized power iteration achieves _exact_ recovery above the _partial_ recovery threshold of unfolding, established in Corollary 2.1.1 (assuming \(n_{k}^{2}/N_{1} 0\)). Recursive unfolding exactly recovers \(v_{1},,v_{k-1}\) with no limitation on the growth rate of \(n_{k}\). Power iteration, which requires initial "warm" estimates of \(v_{1},,v_{k}\), may fail when \(n_{k}^{2}\) and \(N_{1}\) are comparable. Recursive unfolding, on the other hand, does not rely upon an initial estimate of \(v_{k}\) to recover \(v_{1}, v_{k-1}\) exactly.

``` Input:\(X\), initial estimates \(_{1},,_{k}\) from Algorithm 1  Iterate until convergence: for\(j\) from1 to\(k\): \(w_{j} X_{1}_{1}_{j-1}_{j-1}_{j+1 }_{j+1}_{k}_{k}\) \(_{j} w_{j}/\|w_{j}\|_{2}\) endfor return\(_{1},,_{k}\) ```

**Algorithm 3** Power iteration

We prove that exact recovery is achieved in a single iteration of the outer loop. In practice, iterating until estimates converge is beneficial.

**Theorem 4.1**.: _If \(/N_{1}^{1/4}>1\) and \(n_{k}^{2}/N_{1} 0\), Algorithm 3 recovers \(v_{1},,v_{k}\) exactly: denoting by \(_{1},,_{k}\) the estimates after a single iteration of the outer loop,_

\[| v_{j},_{j}|1\,, j [k]\,.\] (17)

_Estimating the signal strength \(\) by \(= X,_{1}_{k}\), we have_

\[^{-1}\|_{1}_{k}- v _{1} v_{k}\|_{F}0\,.\] (18)

We next consider a recursive unfolding procedure. As stated above, recursive unfolding is guaranteed to recover \(v_{1},,v_{k-1}\) exactly if \(/N_{1}^{1/4}>1\); no bound is needed on the growth rate of \(n_{k}\) as in Theorem 4.1. In practice, it is beneficial to iterate Algorithm 4.

``` Input:\(X\), initial estimates \(_{1},,_{k}\) from Algorithm 1 for\(j\) from2 to\(k\): \(_{j}\) first right singular vector of \(_{j-1}(X_{1}_{1})\) endfor \(_{1}\) first right singular vector of \(_{1}(X_{2}_{2})\) return\(_{1},,_{k}\) ```

**Algorithm 4** Recursive unfolding

**Theorem 4.2**.: _Under \(/N_{1}^{1/4}>1\), Algorithm 4 recovers \(v_{1},,v_{k-1}\) exactly. Moreover, \(v_{k}\) is recovered exactly if \(n_{k}^{2}/N_{1} 1\)._

The analysis of Algorithm 4 entails careful application of spiked matrix model results. By the linearity of the unfolding operator,

\[_{j-1}(X_{1}_{1})= v_{1},_{1} (_{[k]\{1,j\}}v_{})v_{j}^{}+_ {j-1}(Z_{1}_{1})\,.\] (19)

Observe that \(Z_{1}_{1}\) is a reshaping of the vector \(_{1}(Z)_{1}\). As \(_{1}\) and \(Z\) are dependent, the second term on the right-hand side is not a matrix of i.i.d. entries. Despite dependencies, we claim that appropriately scaled, \(_{1}(Z)_{1}\) is distributed as Gaussian noise, in which case exact recovery thresholds are a consequence of spiked matrix model results of Section 1.3 applied to (19).

Below, we display simulation results for power iteration and recursive unfolding and compare to tensor unfolding and partial tracing.

Figure 4: Simulations of power iteration with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}\{400,800,1200\}\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical prediction \((>1)\). Power iteration typically converges within five iterations.

Figure 5: Simulations of power iteration with \(k=3\), \(n_{1}=n_{2}=50\), and \(n_{3}=10000\) (left) or \(n_{3}=20000\) (right). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). Vector \(v_{3}\), corresponding to the longest tensor axis, is estimated less well than \(v_{1},v_{2}\).

Figure 6: Simulations of a unfolding, partial tracing, and power iteration with \(k=3\), supersymmetric signal \(v_{1}=v_{2}=v_{3}\), and \(n_{1}=1200\). Solid lines display empirical cosine similarities (each point is the average of 50 realizations). The dashed line is the theoretical limit of unfolding and partial tracing, \(()\). Near the phase transition located at \(=1\), the cosine similarity of power iteration is over twice that of unfolding.