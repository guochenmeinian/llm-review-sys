# Two-way Deconfounder for Off-policy Evaluation in Causal Reinforcement Learning

Shuguang Yu

School of Statistics and Management

Shanghai University of Finance and Economics

Shanghai, China

&Shuxing Fang

Department of Applied Mathematics

The Hong Kong Polytechnic University

Hong Kong, China

&Ruixin Peng

School of Statistics and Management

Shanghai University of Finance and Economics

Shanghai, China

&Zhengling Qi

Department of Decision Sciences

George Washington University

Washington D.C., USA

&Fan Zhou

School of Statistics and Management

Shanghai University of Finance and Economics

Shanghai, China

&Chengchun Shi

Department of Statistics

London School of Economics and Political Science

London, UK

Equal contribution.Corresponding authors: zhoufan@mail.shufe.edu.cn, c.shi7@lse.ac.uk.

###### Abstract

This paper studies off-policy evaluation (OPE) in the presence of unmeasured confounders. Inspired by the two-way fixed effects regression model widely used in the panel data literature, we propose a two-way unmeasured confounding assumption to model the system dynamics in causal reinforcement learning and develop a two-way deconfounder algorithm that devises a neural tensor network to simultaneously learn both the unmeasured confounders and the system dynamics, based on which a model-based estimator can be constructed for consistent policy value estimation. We illustrate the effectiveness of the proposed estimator through theoretical results and numerical experiments.

## 1 Introduction

Before deploying any newly developed policy, it is important to assess its impact. In many high-stakes domains, it is risky or unethical to implement such policies directly for online evaluation. This challenge highlights the essential role of off-policy evaluation (OPE).

There is a vast body of literature on OPE. Most studies assume there are no unmeasured confounders (NUC), also known as unconfoundedness (see e.g., Thomas et al.2015, Jiang and Li2016, Thomas and Brunskill2016, Farajtabar et al.2018, Liu et al.2018, Irpan et al.2019, Schlegel et al.2019, Tang et al.2019, Xie et al.2019, Dai et al.2020, Chandak et al.2021, Hao et al.2021, Liao et al.2021b,Shi et al. 2021, Chen and Qi 2022, Kallus and Uehara 2022, Liao et al. 2022, Shi et al. 2022b, Xie et al. 2023, Zhou et al. 2023a). However, the NUC assumption is restrictive and untestable from the data. It can be violated in various domains such as urgent care (Namkoong et al., 2020), autonomous driving (Nyholm and Smids, 2020), ride-sharing (Shi et al., 2022c), and bidding (Xu et al., 2023). Applying standard OPE methods that rely on the NUC assumption in these settings would result in a biased policy value estimator (see e.g., Bennett and Kallus, 2023, Section 6.1).

Causal reinforcement learning studies offline policy optimization or OPE in the presence of unmeasured confounding. Many existing works can be divided into one of the following three groups:

1. [leftmargin=*]
2. _Methods under memoryless unmeasured confounding_: The first type of methods relies on a "memoryless unmeasured confounding" assumption to guarantee that the observed data satisfies the Markov property in the presence of unmeasured confounders (Zhang and Bareinboim, 2016, Kallus and Zhou, 2020, Li et al., 2021, Liao et al., 2021a, Wang et al., 2021, Chen et al., 2022, Fu et al., 2022, Shi et al., 2022c, Yu et al., 2022, Bruns-Smith and Zhou, 2023, Xu et al., 2023). Many of these works also require external proxy variables (e.g., mediators and instrumental variables) to handle latent confounders. In contrast, the method proposed in this article neither relies on the Markov assumption nor requires external proxies.
3. _POMDP-type methods_: The second category employs a partially observable Markov decision process (POMDP) to model unmeasured confounders as latent states, drawing on ideas from the proximal causal inference literature (Tchetgen et al., 2020) to address unmeasured confounding (Tennenholtz et al., 2020, Nair and Jiang, 2021, Miao et al., 2022, Shi et al., 2022a, Wang et al., 2022, Bennett and Kallus, 2023, Hong et al., 2023, Lu et al., 2023). However, these works require restrict mathematical assumptions that are hard to verify in practice (Lu et al., 2018).
4. _Deconfounding-type methods_: The final category originates from deconfounding methods in causal inference, which leverage the inherent structure within observed data, such as multiple treatment dependencies, network structures, and exposure models, to address unmeasured confounding (Louizos et al., 2017, Tran and Blei, 2017, Wang et al., 2018, Veitch et al., 2019, Wang and Blei, 2019, Zhang et al., 2019, Bica et al., 2020, Veitch et al., 2020, Shah et al., 2022, McFowland III and Shalizi, 2023, Shuai et al., 2023). Notably, Wang and Blei (2019) directly estimate latent confounders for causal inference. However, their algorithm requires the unmeasured confounders to be a deterministic function of the actions (Ogburn et al., 2020, Wang and Blei, 2019), which has been criticized as being unreasonable (D'Amour, 2019, Ogburn et al., 2019); refer to Appendix A.1. There have also been several extensions of deconfounding methods to reinforcement learning (RL) (Lu et al., 2018, Hatt and Feuerriegel, 2021, Kausik et al., 2022, 2023). However, these methods either impose a one-way unmeasured confounding assumption, which can be overly restrictive (see Section 2), or require the correct specification of the latent variable (Rissanen and Martinen, 2021).

This paper aims to develop advanced deconfounding-type OPE methodologies, allowing for more flexible assumptions regarding latent variable modeling. Our proposal is inspired by the two-way fixed effects (2FE) model which is widely employed in applied economics Mundlak (1961), Baltagi and Baltagi (2008), Griliches (1979), Anderson and Hsiao (1982), Freyberger (2018), Callaway and Karami (2023) and causal inference with panel data Arkhangelsky and Imbens (2022), De Chaisemartin and d'Haultfoeuille (2020), Imai and Kim (2021), Sant'Anna and Zhao (2020), Athey and Imbens (2022). More recently, Dwivedi et al. (2022) applied the 2FE model to counterfactual prediction in a contextual bandit setting and Bian et al. (2023) extended the 2FE model to RL. However, their investigations primarily consider an unconfounded setting. Additionally, the model proposed by Bian et al. (2023) imposes a restrictive additive assumption - requiring the latent factors to influence both the reward and transition functions in a purely additive manner. Furthermore, they rely on linear function approximation to estimate the policy value, which may fail to capture the inherently complex nonlinear dynamics. In contrast, we employ flexible neural networks to model the environment.

In this article, we propose a novel two-way unmeasured confounding assumption to effectively model latent confounders. This approach categorizes all unmeasured confounders into two distinct groups: those that are time-specific and those that are trajectory-specific. This assumption enhances the model's flexibility beyond one-way unmeasured confounding while ensuring that the total number of confounders remains much smaller than the sample size, making them estimable from the observed data. We further develop an original two-way deconfounder algorithm that constructs a neural tensor network to jointly learn the unmeasured confounders and the system dynamics. Based on the learned model, we construct a model-based estimator to accurately estimate the policy value. Our proposed model for unmeasured confounders shares similarities with latent factor models used to capture two-way interactions, such as item-customer interactions in recommendation systems (Hu et al., 2008; He et al., 2017), and relationships between different entities in multi-relational learning (Socher et al., 2013; Nickel et al., 2015; Nguyen et al., 2017; Wang et al., 2017; Ji et al., 2021).

To summarize, our contributions include: (1) the introduction of a novel two-way unmeasured confounding assumption; (2) the development of a new two-way deconfounder algorithm for model-based OPE under unmeasured confounding; (3) the demonstration of the effectiveness of our model and algorithm.

## 2 Two-way Unmeasured Confounding

In this section, we begin by presenting the data generating process under unmeasured confounding and outlining our objectives. We then introduce the proposed two-way unmeasured confounding assumption and compare it against alternative assumptions.

We consider an offline setting with pre-collected observational data \(\), containing \(N\) trajectories, each consisting of \(T\) time points. We use \(i\) to index the \(i\)-th trajectory and \(t\) to index the \(t\)-th time point. For each pair of indices \((i,t)\), its associated data is given by the observation-action-reward triplet \((O_{i,t},A_{i,t},R_{i,t})\). In healthcare applications, each trajectory represents an individual patient where \(O_{i,t}\) denotes the covariates of the \(i\)-th patient at time \(t\), \(A_{i,t}\) denotes the treatment assigned to the patient, and \(R_{i,t}\) measures their clinical outcome at time \(t\).

We investigate a confounded setting characterized by the presence of unmeasured confounders (denoted by \(Z_{i,t}\)) that influence both \(A_{i,t}\) and \((R_{i,t},O_{i,t+1})\) for each pair \((i,t)\). The offline data generating process (DGP) can be described as follows: (1) At each time \(t\), the observation \(O_{i,t}\) is recorded for the \(i\)-th trajectory. (2) Subsequently, an action \(A_{i,t}\) is assigned for the \(i\)-th subject according to a behavior policy \(_{b}\), such that \(A_{i,t}_{b}(|O_{i,t},Z_{i,t})\). (3) Next, we obtain the immediate reward \(R_{i,t}\) and the next observation \(O_{i,t+1}\) such that \((R_{i,t},O_{i,t+1})(|A_{i,t},O_{i,t},Z_{i,t})\) for some transition function \(\). (4) Steps 2 and 3 are repeated until we reach the termination time \(T\). See Figure 1(a) for an illustration.

In contrast, following a given target policy \(\) we wish to evaluate, the data is generated as follows: (1) At each time \(t\), the action \(A_{i,t}\) is determined by the target policy \((|O_{i,t})\), independent of the unmeasured confounder \(Z_{i,t}\). (2) The immediate reward \(R_{i,t}\) and next observation \(O_{i,t+1}\) are generated according to the transition function \((|A_{i,t},O_{i,t},Z_{i,t})\). In this setup, the unmeasured confounders affect only the reward and next observation distributions, but not the action. This is a primary difference from the offline data generating process. Specifically, whatever relationship exists between the unmeasured confounders and the actions in the offline data, that relationship is no longer in effect when we perform the target policy \(\). Our objective lies in evaluating the expected cumulative reward under \(\), given by

\[^{}=_{i=1}^{N}_{t=1}^{T}^{}(R_{i,t}),\]

where \(^{}(R_{i,t})\) represents the expectation under the target policy \(\), irrespective of the unmeasured confounders.

To better understand the proposed two-way unmeasured confounding assumption, we first introduce two alternative assumptions concerning the unmeasured confounders as follows:

1. [leftmargin=*]
2. _Unconstrained unmeasured confounding_ (UUC): Each pair of indices \((i,t)\) corresponds to an unmeasured confounder \(Z_{i,t}\), and there are no restrictions on these values.
3. _One-way unmeasured confounding_ (OWUC): For any \(i\), the unmeasured confounders remain the same across time, i.e., \(H_{i}=Z_{i,1}=Z_{i,2}==Z_{i,T}\).

These two assumptions represent two extremes. The UUC assumption offers maximal flexibility without imposing any specific conditions. In contrast, the OWUC assumption is restrictive, essentially excluding 'time-varying confounders'.

Unconstrained unmeasured confounding imposes no restrictions on the latent variables, but requires to estimate a total of \(NT\) latent variables, which is equal to the sample size. This make consistent estimation infeasible without resorting to the 'deterministic unmeasured confounding' assumption discussed in Section 1. On the other hand, one-way unmeasured confounding only requires estimating \(N\) latent variables, but in reality, this assumption is often difficult to meet.

In this article, we propose the following two-way unmeasured confounding, which offers a middle ground between the UUC assumption and the OWUC assumption:

1. [leftmargin=*]
2. _Two-way unmeasured confounding_ (TWUC): There exist time-invariant confounders \(\{U_{i}\}_{i}\) and trajectory-invariant confounders \(\{W_{t}\}_{t}\) such that \(Z_{i,t}=(U_{i}^{},W_{t}^{})^{}\).

As discussed in the introduction, TWUC requires that all unmeasured confounders belong to one of two groups: trajectory-specific time-invariant confounders and time-specific trajectory-invariant confounders. Notably, it excludes confounders that are both trajectory- and time-specific. The \(U_{i}\)s can be interpreted as individual baseline information (e.g., salary or educational background) that remains consistent over time, while the \(W_{t}\)s represent external factors (e.g., weather or holidays) exerting a common influence across all trajectories. This assumption effectively relaxes one-way unmeasured confounding by accommodating time-varying confounders. Meanwhile, the number of latent confounders is confined to \(N+T\), much smaller than the sample size \(N T\) when both \(N\) and \(T\) grow to infinity. This ensures the feasibility of consistent estimation. See Figure 1 for a graphical visualization of the three assumptions.

To further elaborate the three modeling assumptions (a) - (c), we consider a linear model setup where the conditional means of the next observation and the immediate reward are linear functions of the current observation-action pair as well as the unmeasured confounders, and summarize the implications of adopting the three assumptions in the following corollaries.

**Proposition 1** (Inconsistency of the unconstrained model). The least square estimator (LSE) based on the UUC assumption cannot yield consistent predictions. Its mean square error (MSE) remains constant as both \(N\) and \(T\) increase.

**Proposition 2** (Inconsistency of the one-way model under misspecification). When time-varying unmeasured confounders exist, the LSE based on the one-way model cannot yield consistent predictions. Its MSE remains constant as both \(N\) and \(T\) increase.

**Proposition 3** (Consistency of the two-way model). The LSE based on the two-way model can yield consistent predictions. Its MSE decays to zero as both \(N\) and \(T\) increase.

Furthermore, we design a linear simulation setting to numerically compare the estimators based on these three assumptions and report their MSEs in Figure 2(b). The results reveal that the unconstrained

Figure 1: The directed acyclic graphs of data generating processes under different assumptions. \((a):\{z_{i,t}\}_{i,t}\) (colored in blue) are unconstrained unmeasured confounders. \((b):\{h_{i}\}_{i}\) (colored in green) are one-way unmeasured confounders. \((c):\{u_{i}\}_{i}\) and \(\{w_{t}\}_{t}\) (colored in orange) are two-way unmeasured confounders.

model tends to overfit the data, as evidenced by its lowest prediction error on the training dataset and higher error in off-policy value estimation. Conversely, the one-way model underfits the data. It achieves the largest errors in both training data prediction and off-policy value estimation. In contrast, our proposed two-way model strikes a balance, resulting in the lowest error in off-policy value estimation.

To conclude this section, we summarize the DGP under the proposed two-way unmeasured confounding assumption:

* At the initial time, each trajectory independent generates an observed \(O_{i,1}\) and an unobserved \(U_{i}\). Additionally, a latent \(W_{1}\) is independently generated.
* Next, at each time \(t\), \(A_{i,t}\) and \((R_{i,t},O_{i,t+1})\) are generated, according to \(_{b}\) (or a target policy \(\)) and \(\), respectively. Moreover, a latent \(W_{t+1}\) is generated, whose distribution depends only on the past time-specific confounders.

Under this DGP, the two-way unmeasured confounders are policy-agnostic, i.e., unaffected by the actions or policies. Consequently, we can employ a plug-in approach to construct the policy value estimator (see (1)), eliminating the need to estimate their distributions.

## 3 Two-way Deconfounder

We introduce the proposed deconfounder algorithm in this section. We first present the proposed neural network architecture under two-way unmeasured confounding. We next define the loss function used for model training. Finally, we introduce a model-based policy value estimator, built upon the estimated model.

**Network Architecture.** The proposed model contains the following components: (1) \(d\)-dimensional embedding vectors \(\{u_{i}\}_{i}\) to model the trajectory-specific latent confounders; (2) \(d\)-dimensional embedding vectors \(\{w_{t}\}_{t}\) to model the time-specific latent confounders; (3) A transition function \(}(|a,o,u_{i},w_{t})\) that takes an action-observation pair \((a,o)\) and a pair of embedding vectors \((u_{i},w_{t})\) as input to model the conditional distribution of the reward-next-observation pair given the current observation-action pair and the two-way unobserved confounders; (4) An actor network \(_{b}(|o,u_{i},w_{t})\) that takes \(o\) and \((u_{i},w_{t})\) as input to model the behavior policy.

Our objective is to simultaneously learn the embedding representations and the parameters in both the transition and actor networks from the observed data. Toward that end, we treat each pair of embedding vectors \((u_{i},w_{t})\) as two entities. To accurately capture their joint effects on both the transition function and the behavior policy, we adopt the neural tensor network (NTN, Socher et al., 2013), which is known for its ability to capture the intricate interactions between pairs of entity vectors.

Specifically, we parameterize \(}(|a,o,u_{i},w_{t})\) via a conditional Gaussian model given by \(((a,o,u_{i},w_{t}),( (a,o,u_{i},w_{t})))\), where \(((a,o,u_{i},w_{t}))\) is a diagonal matrix and \((a,o,u_{i},w_{t})\) denotes the vector consisting of all diagonal elements. Its conditional mean and variance functions are modeled jointly with the behavior policy, specified by

\[(^{},^{})^{}=_{ }(a_{i,t},(o,u_{i},w_{t})),\ _{b}( o,u_{i},w_{t})=_{_{b}}( (o,u_{i},w_{t})),\]

where \((o,u_{i},w_{t})\) denotes the output vector from an NTN, and \(_{}\), \(_{_{b}}\) are the two multilayer perceptrons that take this output vector as input. The detailed architecture of NTN layer is presented in Appendix B.1. As such, the NTN layer captures the joint information from both the observation \(o\) and latent confounders \(u_{i}\), \(w_{t}\) and is shared among the actor and transition networks. We provide an overview of the proposed model architecture in Figure 2 (a).

Finally, we remark that while we model the transition function using a conditional Gaussian like other works in the RL literature (see e.g., Janner et al., 2019; Yu et al., 2020), more complex generative models such as transformers or diffusion models are equally applicable.

**Loss Function**. As mentioned earlier, the proposed model contains both the transition network and the actor network. Accordingly, our loss function is given by

\[L(;\{u_{i}\}_{i},\{w_{t}\}_{t})= (1-) L_{T}+ L_{A},\]where \(L_{T}\) is the negative log likelihood of conditional Gaussian model for the transition network, \(L_{A}\) is the cross-entropy loss between the actual action and actor network, and \((0,1)\) is a hyperparameter that balances the two losses. This loss is optimized to compute both the latent embeddings vectors \((u_{i},w_{t})\)s and the parameters in the three neural networks: NTN, \(_{}\) and \(_{_{b}}\). Further details are relegated to Appendix C.1 to save space.

Alternative to our loss function which involves both the actor and transition networks, one can consider minimizing other losses focused exclusively on either the transition or the actor network, but not necessarily both. However, in the presence of unmeasured confounding, it is essential to note that both the behavior policy and transition function are influenced by these latent confounders. Consequently, our joint learning approach is expected to more effectively identify the unmeasured confounders compared to these transition-only or actor-only approaches.

**Model-based OPE.** Based on the estimated model, we develop a model-based estimator to learn \(^{}\) via Monte Carlo policy evaluation (Sutton and Barto, 2018). A key observation is that, since the two-way unmeasured confounders are policy-agnostic, the empirical sum shown below is an unbiased intermediate estimator for the evaluation target \(^{}\):

\[_{i=1}^{N}_{t=1}^{T}^{}R_{i,t} O_ {i,1},U_{i},\{W_{t^{}}\}_{t^{}=1}^{t}.\] (1)

Note that in this formulation, the latent factors in the conditioning set can be substituted with our estimated embedding vectors. Additionally, based on our estimated transition network, the expectation \(^{}\) can be effectively approximated using the Monte Carlo method. This approach allows us to construct a model-based plug-in estimator for Equation (1).

To elaborate, the Monte Carlo simulation begins at \(t=1\) for each individual \(i\). We sample an action \(_{i,t}\) according to the target policy \((_{i,t})\), draw samples for \(_{i,t}\) and \(_{i,t+1}\) from the learned transition network \(}(_{i,t},_{i,t}, _{i},_{t})\) with the estimated latent factors \(\{_{i}\}_{i}\), \(\{_{t}\}_{t}\), and iterate this procedure until the terminal time \(T\) is reached. Next, we replicate this simulation multiple times to reduce the Monte Carlo error. The final step is to aggregate all the estimated results across these simulations to construct the final OPE estimator.

## 4 Theoretical Results

In this section, we provide a finite-sample error bound for the expected difference between the estimated policy value and the ground truth \(^{}\). We first introduce some notations. We consider a tabular setting where the observation space \(\), action space \(\) and latent factor spaces \(\), \(\) are all discrete. Let \(d_{}}\) denote the data distribution of quadruples \((a,o,u,w)\), \(d_{}}(a,o,u,w)=(NT)^{-1}_{(A,O,U,W)} (A=a,O=o,U=u,W=w)\), where the summation

Figure 2: \((a):\) An overview of the proposed network architecture. \((b):\) The upper panel reports MSEs under different unmeasured confounding assumptions for fitting the observed data whereas the bottom panel displays the MSEs for off-policy value prediction. The unconstrained unmeasured confounding model shows the best fit for the training data, due to overfitting. The OPE estimator under the proposed two-way unmeasured confounding achieves the smallest MSE. More details are referred to Appendix D.1.

is carried out over all quadruples in the augmented dataset \(}=\{U_{i}\}_{i}\{W_{t}\}_{t}\) containing both the observed data and the latent confounders. Furthermore, let \(d^{}\) represent the visitation distribution under \(\). Specifically, \(d^{}(a,o,u,w)\) denotes the average probability of a given quadruple \((a,o,u,w)\) appearing at any time step under \(\) (refer to Appendix B.1 for the detailed definition).

We next introduce some assumptions to derive the finite-sample error bound of the proposed policy value estimator.

**Assumption 1** (Coverage).: The data distribution covers the visitation distribution induced by the target policy \(\), i.e., \(C=_{a,o,u,w}(a,o,u,w)}{d_{}(a,o,u,w)}<\).

**Assumption 2** (Boundedness).: The absolute values of the immediate reward are upper bounded by some constant \(R_{}<\).

**Assumption 3** (Error bound of estimated transition function). \(\|}-\|_{d_{}}^{2} _{}^{2}\) for some \(_{}>0\) where \(\|}-\|_{d_{}}}\) denotes the total variation distance between \(}\) and \(\) (see Appendix B.1 for the detailed definition).

**Assumption 4** (Error bound of estimated latent confounders). \(_{1 i N}\|_{i}-U_{i}\|_{2}^{2}/N _{U,W}^{2}\) and \(_{1 t T}\|_{t}-W_{t}\|_{2}^{2}/T _{U,W}^{2}\) for some \(_{U,W}>0\).

**Assumption 5** (Autocorrelation).: For any \(t_{1},t_{2} 1\), let \((t_{1},t_{2})\) denote the correlation coefficient between \(^{}(R_{1,t_{1}}|\{W_{t^{}}\}_{t^{}=1}^{t_{1}})\) and \(^{}(R_{1,t_{2}}|\{W_{t^{}}\}_{t^{}=1}^{t_{2}})\). There exists some \(0 1\) such that \(_{1 t_{1} t_{2} T}(t_{1},t_{2})=O(T^{2})\).

We remark that conditions similar to Assumptions 1-2 are widely imposed in the RL literature to simplify the theoretical analysis (see e.g., Chen and Jiang, 2019; Fan et al., 2020; Liu et al., 2020; Uehara and Sun, 2021). Assumption 3 is concerned with the estimation error of the transition function. This error is expected to be minimal, since we use neural networks for function approximation (see e.g., Schmidt-Hieber, 2020; Farrell et al., 2021). Assumptions 4 is concerned with the estimation errors of the estimated two-way unmeasured confounders. According to Proposition 3, these errors are negligible under simple models. Finally, Assumption 5 is purely technical. It measures the autocorrelation of the time series \(^{}(R_{1,t}|\{W_{t^{}}\}_{t^{}=1}^{t})\). Here, \(=0\) indicates independence over time. This condition is automatically satisfied when \(=1\).

**Theorem 1** (Finite-sample error bound).: Suppose the two-way unmeasured confounding assumption holds, and Assumptions 1-5 are satisfied. Then

\[|^{}-^{}| CTR_{}_{ }+cTR_{}_{U,W}+cR_{}N^{-1/2}+cR_{}T^{- 1},\]

for some constant \(c>0\).

It can be seen from Theorem 1 that the mean absolute error of the proposed policy value estimator involves two components:

1. **Estimation errors:** The first two terms on the right side of the inequality correspond to the estimation errors of the transition function and latent confounders respectively. Notably, both terms are linear in the time horizon \(T\), due to error accumulation (see Appendix B.5). However, such a linear dependence is the best one can hope in general (Jiang, 2024), although it is possible to eliminate the dependence upon \(T\) under additional ergodicity assumptions (Liao et al., 2022).
2. **Standard deviations:** The last two terms measure the standard deviations of the average values across trajectories and over time, respectively. These upper bounds decays to zero, as both \(N\) and \(T\) approach infinity, provided that the exponent \(\) in Assumption 5 - which measures the autocorrelation of the time series \(^{}(R_{1,t}|\{W_{t^{}}\}_{t^{}=1}^{t})\) - is strictly smaller than \(1\).

## 5 Experiments

In this section, we perform numerical experiments using two simulated datasets and one real-world dataset to demonstrate the effectiveness of the proposed two-way deconfounder (denoted by TWD) in handling unmeasured confounders. We consider two simulated examples in Section 5.1: a simple dynamic process and a tumor growth example. For each simulated example, the true value of \(^{}\) is computed based on 10,000 Monte Carlo experiments. We also explore a real-world example using the MIMIC-III dataset in Section 5.2, and conduct a sensitivity analysis and an ablation study in Sections 5.3 and 5.4, respectively. The source code is available on Github: https://github.com/fsmiu/Two-way-Deconfounder.

We use two metrics to evaluate different OPE estimators: the logarithmic mean squared error (LMSE) and bias. Both are estimated based on 20 simulations. Comparison is made between TWD and the following set of baseline methods, which covers a wide range of model-based and model-free approaches:

1. [leftmargin=*]
2. _Model-based method_ (MB) that learns a transition model from the offline data based on the proposal by Yu et al. (2020) and applies the Monte Carlo method to construct the OPE estimator;
3. _Minimax weight learning_(MWL, Uehara et al., 2020) that learns a marginalized importance sampling (MIS) ratio from the offline data to constructs an MIS estimator for OPE;
4. _Double robust method_ (DR) that combines the MIS ratio and an estimated Q-function computed via minimax learning (Uehara et al., 2020) to enhance robustness of OPE;
5. _Partially observable MWL_(PO-MWL, Shi et al., 2022a) - a POMDP-type method that extends MWL to handle unmeasured confounders;
6. _Partially observable DR_(PO-DR, Shi et al., 2022a) - another POMDP-type method that extends DR to handle unmeasured confounders;
7. _Recurrent state-space method_(RSSM, Hafner et al., 2019, 2019) that models unmeasured confounders as latent states;
8. _Model-free two-way doubly inhomogeneous decision process_(TWDIDP1) - a deconfounding-type method that uses the model-free algorithm developed by Bian et al. (2023);
9. _Model-based two-way doubly inhomogeneous decision process_(TWDIDP2) - another model-based deconfounding-type algorithm, also developed by Bian et al. (2023).

Notably, the first three methods require the NUC assumption. The next three methods are POMDP-type, whereas the last two are deconfounding-type algorithms. Given our focus on settings without external proxies, we do not compare against methods developed under memoryless unmeasured confounding, which typically rely on these proxies, as commented in Section 1.

### Simulation studies

**Simulated Dynamic Process**. We first consider a dynamic process with four-dimensional observations and binary actions. The data is generated under the proposed TWUC assumption; see Appendix D.2 for its detailed DGP. We fix \(T=50\), and vary the number of trajectories from 250 to 2000. As shown in Figure 3(a), our proposed TWD estimator frequently achieves the smallest LMSE with bias closer to 0 in all cases. Additionally, the LMSE of TWD generally decreases as the number of trajectories increases, demonstrating its consistency. In contrast, most other methods under the assumption of NUC or POMDP setting are severely biased, highlighting the risks of ignoring or improperly handling unmeasured confounding.

**Tumor Growth Example**. We consider a tumor growth example and utilize the pharmacokinetic-pharmacodynamic (PK-PD) model for data generation; see Appendix D.3 for details. The observation

Figure 3: Logarithmic MSE and Bias of various estimators for the simulated dynamic process and tumor growth example.

is two-dimensional, including the tumor volume \(V(t)\) and the chemotherapy drug concentration \(C(t)\). The action space \(\) includes two treatments: radiotherapy \(A^{r}(t)\) and chemotherapy \(A^{c}(t)\). We evaluate two target policies: a random policy (denoted by 'A') and a individualized policy that is tailored to the patients' conditions (denoted by 'B'). As shown in Figure 3(b), TWD consistently achieves the lowest LMSE and is empirically unbiased in all cases, demonstrating the effectiveness of our method. In contrast, other methods yield biased estimates, resulting in significantly higher LMSEs. Importantly, the performance of these alternative methods does not show significant improvement with an increase in the total number of trajectories. These results underscore the applicability of our method in a more realistic scenario.

### Real-world example: MIMIC-III database

In this section, we apply the proposed two-way deconfounder to the medical information mart for intensive care (MIMIC-III) database (Johnson et al., 2016). We extract 3,707 patients with trajectories up to 20 timesteps. Following the analysis of Zhou et al. (2023), we define a 5 x 5 action space and set the reward to the difference between current SOFA score and next SOFA score, so a lower reward indicates a higher risk of mortality. We also extract 12 covariates as the observation; further details are described in Appendix D.4. The dataset is likely non-stationary and lacks patient's personal information. Therefore, it might be reasonable to employ TWUC to model the unmeasured confounders (Bian et al., 2023).

Given that this is a real dataset, we do not have access to the true value of the target policy. To compare TWD against other baselines, we employ two approaches. The first approach uses 90% of data for training, and the remaining 10% for evaluating an algorithm's prediction error for the reward and next observation. Notice that this approach is applicable to evaluate model-based methods only. We report all mean squared prediction errors in Figure 4(b). It can be seen that TWD results in the lowest prediction errors in all cases, demonstrating its effectiveness to infer unobserved confounders.

The second approach assesses each algorithm ability in distinguishing between tailored individualized policies and other random, non-individualized policy. An effective OPE algorithm should consistently rank an individualized policy as the superior policy. In what follows, four policies are evaluated using TWD and other baseline methods: a randomized policy, a non-individualized high dose policy, a non-individualized low dose policy and an tailored individualized policy. As shown in Figure 4(a), TWD and MB can effectively distinguish the individualized policy from other policies, with the individualized policy consistently achieving the highest estimated value. However, other methods lead to strange conclusions. For example, the result from TWDIDP2 suggest that all policies achieve similar values. Consequently, results from MWL,DR, PO-DR and RSSM suggest that the high dose policy is better than the tailored individualized policy. Additionally,TWDIDP1 performs specially poor, rendering it unsuitable for display alongside other methods in this figure.While these results require further validation by medical professionals, they highlight the potential of the proposed method in real-world medical applications.

### Sensitivity analysis

In this section, we investigate how TWD performs when the proposed TWUC assumption is violated. Specifically, we vary unmeasured confounders that are both trajectory- and time-specific in the

Figure 4: \((a):\) The estimated policy value for four target policies in real-world dataset. \((b):\) Average root \(\) and its standard error in the results for predicting immediate reward and next observation. The results are aggregated over 20 runs.

reward function and behavior policy and we introduce a sensitivity parameter \(\) to quantify the extent to which the proposed TWUC assumption is violated. When \(=1\), the proposed TWUC assumption holds; as \(\) decreases towards zero, the assumption is increasingly violated. We vary \(=\{0.0,0.3,0.7,1.0\}\) in the experiments, and fix the number of the trajectory to 1000. See further details in Appendix D.5.

We focus on the two simulated examples. As shown in Figure 5(a), in the simulated dynamic process, the performance of the proposed methods remains relatively stable as long as \(>0\). However, when \(=0\) - where the TWUC is completely violated - TWD loses its superiority. Meanwhile, as shown in Figure 5(b), in the tumor growth example, the performance of TWD is very sensitive to \(\), and TWD performs better than other methods only if \(=1.0\).

### Ablation study

We conduct an ablation study to compare TWD against the following variants:

1. [leftmargin=*]
2. _TWD with transition-only loss function_ (TWD-TO): This variant employs the proposed TWUC assumption, but removes the cross-entropy loss from the objective function. Consequently, it solely uses the transition model to learn the two-way embedding vectors, without modeling the behavior policy during training.
3. _TWD without neural tensor network_ (TWD-MLP): In this variant, the neural tensor network is replaced with an MLP.
4. _One-way deconfounder without individual embedding_ (OWD-NI): This variant removes the individual embedding vector, operating under the one-way unmeasured confounding assumption.
5. _One-way deconfounder without time embedding_ (OWD-NT): This variant removes the time embedding vector.

We fix the number of trajectories to 1000 and report the LMSEs of various estimators in Table 1. It can be seen that: (i) OWD-NI and OWD-NT significantly underperform TWD due to their reliance on the one-way unmeasured confounding assumption. (ii) TWD consistently outperforms TWD-MLP, due to the neural tensor network's ability to capture intricate interactions between the trajectory-specific and the time-specific unmeasured confounders. (iii) In the simulated dynamic process, TWD achieves better performance than TWD-TO. This could be attributed to our proposed joint learning strategy, which simultaneously estimates both the transition function and the behavior policy and enhances the model's capability to infer unmeasured confounders. However, TWD performs worse than TWD-TO in the tumor growth example.

In summary, these results demonstrate the effectiveness of the proposed two-way unmeasured confounding model and our joint learning strategy, along with the crucial role of the neural tensor network in capturing complex interactions, all contributing to TWD's superior performance.

    &  \\   &  &  \\ 
**Model** & A & B & A & B \\  TWD & **4.23** & **4.47** & 4.42 & 4.33 \\  TWD-TO & 4.72 & 5.14 & **3.58** & **3.39** \\  TWD-MLP & 4.34 & 5.26 & 4.56 & 4.44 \\  OWD-NI & 7.50 & 8.59 & 6.80 & 6.92 \\  OWD-NT & 6.78 & 8.92 & 6.26 & 6.31 \\   

* In the simulated dynamic process, TG: the tumor growth example, A: Target policy, A: B: Target policy B.

Table 1: Ablation study for variants of TWD

Figure 5: Sensitivity analysis for the simulated dynamic process and tumor growth experiment.