# Ex Uno Pluria: Insights on Ensembling

in Low Precision Number Systems

Giung Nam

Kim Jaechul Graduate School of AI

KAIST, Daejeon, South Korea

giung@kaist.ac.kr

&Juho Lee

Kim Jaechul Graduate School of AI

KAIST, Daejeon, South Korea

juholee@kaist.ac.kr

###### Abstract

While ensembling deep neural networks has shown promise in improving generalization performance, scaling current ensemble methods for large models remains challenging. Given that recent progress in deep learning is largely driven by the scale, exemplified by the widespread adoption of large-scale neural network architectures, scalability emerges an increasingly critical issue for machine learning algorithms in the era of large-scale models. In this work, we first showcase the potential of low precision ensembling, where ensemble members are derived from a single model within low precision number systems in a training-free manner. Our empirical analysis demonstrates the effectiveness of our proposed low precision ensembling method compared to existing ensemble approaches.

## 1 Introduction

In computer science, it is a de facto standard to represent continuous real numbers using finite precision number systems. While many applications rely on precision formats like FP32 or FP64, the deep learning community is increasingly turning to 16-bit floating-point formats such as FP16 (Micikevicius et al., 2018) or BF16 (Dean et al., 2012) to reduce memory usage during training. More recently, researchers are further exploring low precision optimization, aiming to utilize fewer bits (8 bits or less) to represent weights, activations, and gradients throughout the training process (Gupta et al., 2015; Li et al., 2017; Sun et al., 2020; Wortsman et al., 2023).

While low precision number systems can aid in training deep neural networks, they are also beneficial for reducing inference costs in real-world deployments of such models (Jacob et al., 2018). In particular, recent advancements in large language models (Brown et al., 2020; Touvron et al., 2023) containing billions of parameters have triggered active exploration of _post-training quantization_ techniques, for deploying pre-trained large language models on hardware with limited memory resources. This exploration encompasses quantizing both weights and activations (Dettmers et al., 2022; Yao et al., 2022), as well as quantizing weights only (Frantar et al., 2023; Lin et al., 2024).

Originally, the goal of post-training quantization is to specify the best solution represented in low precision number systems, aiming to reduce discrepancies from the original high-precision weights, like perturbations in weight values or increases in loss functions (Nagel et al., 2020). On the other hand, the presence of numerous distinct yet high-performing models within a single basin on loss landscapes (Sadrtdinov et al., 2023; Lion et al., 2024) evokes a Bayesian concept of _marginalization instead of optimization_, which involves utilizing multiple solutions rather than relying solely on one solution (Wilson and Izmailov, 2020).

Hinging on this insight, we suggest building ensembles within low precision number systems, as illustrated in Fig. 1. It depicts a proof-of-concept method for ensemble construction using stochastic rounding, a technique commonly used in low precision training to address the issue of roundingweight updates below the minimum precision threshold to zero (Gupta et al., 2015). In our approach, stochastic rounding is employed for _ensembling_ rather than optimization. Indeed, our experimental findings in Section 4 validate that low precision ensembling improves the downstream performance of pre-trained large models without any further training on downstream data.

Confirming the potential of low precision ensembling for pre-trained models, we extend our investigation through a comparative study with existing methods involving ensembling. Specifically, we examine Bayesian approaches that approximate a Gaussian posterior over the loss landscape (Madox et al., 2019; Shen et al., 2024), and sampling techniques that collect model copies from the update trajectory (Huang et al., 2017; Zhang et al., 2020). Our experimental results in Section 4 show that low precision ensembling successfully gathers diverse ensemble members contributing the final ensemble performance within the low precision number system.

The main contributions of our work can be outlined as follows:

* Introducing low precision number systems inevitably results in quantization errors, usually seen as a flaw to be corrected in neural network quantization. Our work presents a novel viewpoint: these errors can be utilized as a source to improve ensemble diversity. Expanding on our comprehension of diversity, we suggest a simple yet powerful approach to constructing ensembles called Low Precision Ensembling with Bernoulli Stochastic Rounding (LPE-BSR), particularly advantageous for large models.
* The proposed LPE-BSR involves assembling ensemble members with low precision number systems, effectively addressing a notable challenge associated with memory costs inherent in ensemble methods. In this regard, our work holds promise for utilizing low precision number systems to construct ensembles of large models, offering a potential solution for the scalability issue faced by the Bayesian deep learning community in the era of large-scale models (Papamarkou et al., 2024).

## 2 Ensemble methods in modern transfer learning scenarios

Ensembling neural networks is a long-established idea in machine learning (Hansen and Salamon, 1990), where the underlying design principle is to create a strong hypothesis that effectively explains the data by combining a set of weak hypotheses (Kearns, 1988). Notably, even after the transition into the deep learning era, ensemble methods continue to serve as a straightforward yet powerful strategy for boosting the performance of machine learning algorithms involving deep neural networks (Krizhevsky et al., 2012; Ciresan et al., 2012). However, the operational principles of such _deep ensembles_, i.e., ensembles composed of deep neural networks, deviate from those of classical statistical models and remain not fully comprehended. For instance, both Lee et al. (2015) and Nixon et al. (2020) validated that _bagging_(Breiman, 1996), built on the theoretically well-motivated _bootstrap_ method (Efron, 1992), does not offer any benefits over the simplest deep ensembles consisting of models obtained from multiple training runs with different random seeds.

Figure 1: **Concepts of low precision ensembling.** It shows a two-dimensional schematic, where the x and y axes represent the neural network weights, while the contours above visualize the loss surface. (**a**) Let the pre-trained weights, denoted by a yellow star-shaped marker (\([rgb]{0,0,0}[named]{pgfstrokecolor}{rgb}{0,0,0} @color@gray@stroke{0}@color@gray@fill{0}\)), be positioned within a basin on the loss landscape. In general, (b) post-training quantization methods introduce lower precision number systems, and then (c) choose one candidate from the system, such as the nearest one. (d) However, there are many other highly effective models available, that can contribute to ensemble predictions.

Empirically, it is well-known that we need nothing more than employing different initializations for each ensemble member to construct high-performance deep ensembles (Lakshminarayanan et al., 2017). Fort et al. (2019) delved deeper into this and emphasized the significant role played by a highly non-convex loss function of deep neural networks, where varying initializations for stochastic optimization yield different functional modes. It also aligns with the Bayesian perspective provided by Wilson and Izmailov (2020), which suggests that deep ensembles are involved in approximating multimodal posterior distribution to the Bayesian model averaging. To sum up, while the operational principle of deep ensembles may differ from classical ensembles, the underlying idea regarding _diversity_ remains constant (Krogh and Vedelsby, 1994; Ortega et al., 2022), i.e., ensembles demonstrate improved performance when their individual members offer _diverse_ predictions.

However, the simple strategy mentioned earlier, which aims to cover multiple modes on the loss landscape by starting from different initializations to achieve ensemble diversity (Fort et al., 2019; Wilson and Izmailov, 2020), faces challenges in modern transfer learning scenarios. It arises from the typical situation that there is only _one_ pre-trained model available for fine-tuning; due to the considerable cost for pre-training of large models, model providers usually do not distribute multiple model copies. In such cases, fine-tuned solutions originating from the same pre-trained weights often inhabit the same pre-trained basin, leading to restricted exploration within the loss landscape (Neyshabur et al., 2020; Mustafa et al., 2020). Consequently, our attention should be directed towards addressing the _local_ structure around the pre-trained basin in modern transfer learning scenarios (Wortsman et al., 2022; Sadrtdinov et al., 2023; Lee et al., 2024), rather than the _global_ multimodal structure of the loss landscape (Fort et al., 2019; Wilson and Izmailov, 2020).

## 3 Preliminaries

**Finite precision number systems.** Computers use binary sequences for data encoding, with FP32 serving as the primary finite precision number system employed to represent real numbers. Given its coverage from approximately \(10^{-38}\) to \(10^{38}\) with a resolution of \(10^{-7}\), we consider the FP32 system as the continuous set of real numbers \(\). Moreover, we describe INT-\(B\) systems, commonly utilized low precision number systems for neural network quantization:

\[_{B}=\{m}}{2^{B-1}-1}\ : \ m\{-2^{B-1}+1,,0,,2^{B-1}-1\}\}, \]

where the system can represent real numbers in the range \([-w_{},w_{}]\) with a resolution of \(w_{}/(2^{B-1}-1)\), and the integer \(m\) can be encoded using \(B\) bits. This is the simplest form of integer quantization, with possible variations like a zero offset or non-uniform resolution (Gholami et al., 2021; Yvinec et al., 2023). Unless otherwise specified, our experimental results employ this basic form of _symmetric uniform quantization_ with \(B=5\) for simplicity. We also use _per-channel granularity_, sharing the number systems among the output channels of each linear layer.

**Rounding rules.** Let \(\) be a finite precision number system. For any \(w\), there are two rounding options in practice: \( w=\{ w\}\) and \( w=\{ w\}\). The _rounding-to-nearest_ (RTN) scheme deterministically selects the closest one, i.e., \(= w\), whereas the _Bernoulli stochastic rounding_ (BSR) scheme randomly chooses one of them, i.e.,

\[= w w -w,\ = ww- w. \]

Observing empirically that BSR sometimes produce superior results compared to RTN motivates the neural network quantization community to explore more sophisticated rounding schemes (Nagel et al., 2020; Lee et al., 2023). On the other hand, recognizing the presence of _multiple_ competitive solutions, we consider leveraging them for low precision ensembling.

**Ensemble methods.** In ensemble methods for classification problems, the final prediction during testing is obtained by combining \(S\) predictions:

\[p(y|x)=_{s=1}^{S}p(y|x,_{s}), \]

where \(y\) is a class label, \(x\) is an input, and \(p(y|x,_{s})\) is the categorical probabilities predicted by the \(s^{}\) member, which is a neural network model with parameters \(_{s}\). These \(_{s}\) could either be _maximum a posteriori_ (MAP) solutions obtained through multiple stochastic optimizations (Lakshminarayanan et al., 2017), or they might be intermediate checkpoints from a single training run (Huang et al., 2017; Garipov et al., 2018).

In the Bayesian framework, neural network weights \(\) are treated as random variables, and \(_{s}\) are seen as Monte Carlo samples employed to approximate the posterior distribution. More precisely, Eq. 3 can be seen as a simple Monte Carlo method to approximate the posterior with a set of point masses, where the locations are given by samples from another approximate posterior \(q\), i.e., \(p(|)_{s=1}^{S}(-_{s})/S\), \(_{s} q()\), where \(\) denotes the data and \(\) is the Dirac delta function (Wilson and Izmailov, 2020). A common practice is to use a Gaussian approximation \(q()=(;,)\) to generate samples \(_{s} q()\) in a tractable manner (Maddox et al., 2019; Shen et al., 2024), and then approximate the predictive distribution by computing

\[p(y|x)=_{s=1}^{S}p(y|x,_{s}),_{1},,_{S} q(). \]

## 4 An empirical study of low precision ensembling

We present a simple yet effective low precision ensemble construction strategy, Low Precision Ensembling with Bernoulli Stochastic Rounding (LPE-BSR), which computes Eq. 4 using

\[q()=_{i=1}^{D}q(^{(i)}), q(^{(i)})=_{i} (^{(i)}-^{(i)})+(1-_{i}) (^{(i)}-^{(i)}), \]

for \(i=1,,D\). Here, \(D\) denotes the number of neural network parameters, where each permanent parameter group shares the same low precision number system, as explained in Section 3. Using the rounding operations \(\) and \(\) defined within each system, the probability is determined by \(_{i}= w_{i}-w_{i}\), as in Eq. 2. Certainly, the proposed LPE-BSR is not Bayesian, and we have simply expressed it in the form of Eq. 4 for the sake of notational simplicity.

### Motivation: training-free ensemble construction of large ViT models

We begin with motivating experiments using the publicly available series of pre-trained vision transformer models (ViT; Dosovitskiy et al., 2021). Detailed information about each model can be found in Appendix A. Table 1 summarizes the evaluation results on a subset of the ImageNet validation split, along with the parameter count for each model. In this context, the pre-trained model corresponds to the star-shaped marker (\(\)) in Fig. 1, with RTN using the nearest value in low precision number systems as shown in Fig. 1(c), and LPE-BSR forming an ensemble by selecting \(S=10\) nearby samples as illustrated in Fig. 1(d).

Table 1 provides the following key findings: 1) Larger models experience less performance degradation when reducing the precision of numerical systems. More precisely, in the RTN results, the classification error increases from \(.243.247.315\) when transitioning from FP32 \(\) INT-6 \(\) INT-4 at ViT-T/16, whereas at ViT-L/16, it shifts from \(.165.165.167\). 2) Lower precision systems introduce diversity among samples in LPE-BSR. Specifically, ensemble ambiguity is the metric for quantifying the ensemble diversity (to be defined in Section 4.3), and in all models, the ensemble ambiguity increases when transitioning from INT-6 \(\) INT-4.

    & &  &  &  &  \\  Method & System & NLL & ERR & AMB & NLL & ERR & AMB & NLL & ERR & AMB & NLL & ERR & AMB \\  \(\) Pre-trained & FP32 &.932 &.243 & - &.667 &.185 & - &.687 &.182 & - &.639 &.165 & - \\  RTN & INT-6 &.948 &.247 & - &.671 &.185 & - &.687 &.182 & - &.639 &.165 & - \\  & INT-4 & 1.23 &.315 & - &.822 &.218 & - &.716 &.184 & - &.647 &.167 & - \\  LPE-BSR & INT-6 &.932 &.245 &.024 &.665 &.185 &.014 &.681 &.181 &.006 &.632 &.164 &.003 \\  & INT-4 & 1.30 &.298 &.489 &.821 &.211 &.268 &.648 &.175 &.088 &.600 &.160 &.037 \\   

Table 1: Motivating results for low precision ensembling of pre-trained ViT models. Negative log-likelihood (NLL), classification error (ERR), and ensemble ambiguity (AMB) for rounding-to-nearest (RTN) and low precision ensembling with Bernoulli stochastic rounding (LPE-BSR) derived from the publicly available pre-trained ImageNet model (\(\)). Blue highlights the areas where LPE-BSR excels, particularly in larger models and lower precision settings.

Importantly, these findings are in line with the core principle of ensemble methods being effective: a key condition for an ensemble of classifiers to outperform any of its individual members is when the classifiers are both _accurate_ and _diverse_(Dietterich, 2000). When introducing the low precision number system, 2) suggests that diversity can be achieved by leveraging the quantization error inherent in this process, while 1) emphasizes that larger models maintain accurate individual performance throughout this process. With this compelling motivation for low precision ensembling established, we now proceed to compare it with existing ensemble methods.

### Comparative study to Bayesian methods

Our initial investigation into the proposed LPE-BSR aims to assess the effectiveness of collecting ensemble members within the discrete space defined by the low precision number system. While using fewer bits to represent samples would certainly reduce ensemble costs, there is a concern that we might overlook potentially good ensemble candidates outside this discrete space. To address this, we conduct a comparative study with two Bayesian deep learning methods: improved variational online newton (IVON; Shen et al., 2024) and stochastic weight averaging Gaussian (SWAG; Maddox et al., 2019). Both methods use samples drawn from an approximate Gaussian posterior in the continuous weight space and perform ensembling in a Bayesian manner.

To sum up our experiment, we first fine-tune the zero-shot CLIP-ViT model (Radford et al., 2021) on the ImageNet training split to obtain the MAP solution \(_{}^{*}\). For LPE-BSR, this fine-tuning can employ any of the SGD, SWAG, or IVON optimizers. We then define \(q\) according to Eq. 5 using \(_{}^{*}\) and compute Eq. 4 with \(S\) samples. For SWAG and IVON, \(q()=(;,)\) is defined using the \(=_{}^{*}\) and \(\) obtained during their respective optimization processes, followed by the computation of Eq. 4. Consequently, LPE-BSR samples neighboring points of \(_{}^{*}\) within the discrete space defined by the low precision number system, whereas SWAG and IVON sample nearby points of \(_{}\) in the continuous space with Gaussian noise added. For more details on SWAG and IVON, including their hyperparameters, please refer to Appendix E.

Fig. 2 presents the outcomes of our experiments conducted with CLIP-ViT-L/14. In our experimental setup, both SWAG and IVON successfully estimated both the MAP mean and the covariance matrix, resulting in a lower negative log-likelihood compared to MAP through Bayesian model averaging, as illustrated in the second and third subplots of Fig. 2. Remarkably, our LPE-BSR, derived from the MAP solution obtained by the SGD optimizer, achieves competitive results with Bayesian model averaging through SWAG or IVON. Moreover, when using the same MAP solution obtained with the SWAG or IVON optimizer for a fair comparison, it even outperforms these methods.

From a numerical integration perspective (Wilson and Izmailov, 2020; Wilson, 2021), the conditions for successful approximate Bayesian inference in deep learning are very similar to those for successful ensembling, as discussed in Section 4.1 with reference to Dietterich (2000). Specifically, it entails 1) finding _typical_ points in the posterior that represent regions of substantial mass (cf. _accurate_); and (ii) ensuring a _diverse_ set of points to give rise to different functions (cf. _diverse_). Consequently, we proceed to conduct a comparative analysis of these two factors concerning our LPE-BSR method and the Bayesian methods we considered, SWAG and IVON.

Figure 2: **Comparing low precision ensembling to Bayesian methods.** Negative log-likelihood for Bayesian model averaging using an approximate Gaussian posterior derived from SWAG or IVON (BMA, shown in orange) and low precision ensembling with Bernoulli stochastic rounding centered around the MAP solution obtained by each optimizer (LPE-BSR, shown in green).

### Diversity analysis of ensemble methods

Quantitative assessment of ensemble diversity is an essential metric for evaluating ensemble methods. To this end, we adopt the _generalized ambiguity decomposition_ for cross-entropy loss presented by Wood et al. (2023), which can be easily measured using logit ensembling instead of probability ensembling. It should be noted that logit ensembling is solely utilized for diversity analysis, whereas probability ensembling is used for all other experimental results to ensure a fair comparison with Bayesian methods that compute the categorical predictions using the BMA integral. Please refer to Appendix B for more details on our diversity analysis.

Table 2 presents our experimental outcomes for the generalized ambiguity decomposition (cf. 'Diversity analysis'), along with the final ensemble perfomance (cf. 'Evaluation metrics'). Lowering the precision of numerical systems naturally amplifies quantization error, as evidenced by the results in the INT-\(B\) rows, where reducing \(B\) results in higher (a) average loss. However, this also coincides with an increase in ensemble diversity, with smaller \(B\) values resulting in greater (b) ambiguity. Consequently, the superior (c) ensemble loss at an appropriate precision level (e.g., \(B=5\)) highlights the fundamental principle of the low precision ensembling: _it does not merely perceive quantization error problematic, but rather utilizes it to obtain ensemble diversity_. Consequently, the proposed LPE-BSR yields improvements in evaluation metrics, as depicted in Table 2. Definitions for each metric can be found in Appendix B.

    & & & &  &  \\  Optimizer & Method & System & (a) & (b) & (c) & NLL & ERR & ECE \\  SWAG & \(\) MAP & FP32 &.488\({}_{ 0.003}\) & - &.488\({}_{ 0.003}\) &.488\({}_{ 0.002}\) &.137\({}_{ 0.001}\) &.034\({}_{ 0.001}\) \\  & BMA & FP32 &.498\({}_{ 0.002}\) &.015\({}_{ 0.000}\) &.483\({}_{ 0.001}\) &.477\({}_{ 0.002}\) & **.136\({}_{ 0.001}\)** & **.021\({}_{ 0.001}\)** \\  & LPE-BSR & INT-6 &.492\({}_{ 0.003}\) &.007\({}_{ 0.000}\) &.485\({}_{ 0.003}\) &.483\({}_{ 0.002}\) & **.136\({}_{ 0.001}\)** &.031\({}_{ 0.001}\) \\  & INT-5 &.507\({}_{ 0.002}\) &.026\({}_{ 0.000}\) &.481\({}_{ 0.002}\) & **.473\({}_{ 0.002}\)** & **.136\({}_{ 0.001}\)** & **.021\({}_{ 0.001}\)** \\  & INT-4 &.643\({}_{ 0.004}\) &.129\({}_{ 0.001}\) &.514\({}_{ 0.003}\) &.513\({}_{ 0.002}\) &.146\({}_{ 0.001}\) &.027\({}_{ 0.000}\) \\  IVONDrawing inspiration from Fort et al. (2019), we further provide the radial landscape plot depicting a plane within weight space containing three points (shown as yellow, blue, and orange markers). The z-values for each subplot represent the negative log-likelihood (displayed on the left in a magma colormap), the function differences from the blue model (shown in the center in a blue colormap), and the red model (presented on the right in a red colormap). Namely, the first subplot indicates the placement of each model within the loss landscape, while the subsequent two subplots illustrate the extent to which they differ from each other.

Fig. 3 depicts radial landscape plots comparing IVON and LPE-BSR samples, showing their parallel roles in ensemble construction. Both show slightly higher individual negative log-likelihoods, shown by the circle markers, compared to the MAP denoted by a star-shaped marker in the first subplot, while also offering diverse function outputs as demonstrated in the subsequent subplots. Ultimately, LPE-BSR can identify samples in a low precision number system that qualitatively resemble the high-quality posterior samples provided by IVON, which leverages Hessian estimate information to compute the covariance of the approximate Gaussian posterior.

### Combining with fast ensembling methods

A key advantage of LPE-BSR is its ability to gather ensemble members without requiring any backward computation. This feature, which eliminates the need for training, aligns with fast ensembling techniques aimed at enhancing the training efficiency of ensemble construction processes (Huang et al., 2017; Garipov et al., 2018; Benton et al., 2021). Consequently, we conduct empirical analysis to further investigate this alignment with snapshot ensembling (SSE; Huang et al., 2017), as well as cyclical stochastic gradient Langevin dynamics (CSGLD; Zhang et al., 2020), a closely related Bayesian posterior sampling algorithm. Both methods involve collecting snapshot samples on the loss landscape around \(_{}\) using a cyclical learning rate schedule. For more details, including hyperparameters, please refer to Appendix E.

We first verify whether LPE-BSR can generate an ensemble component distinct from SSE snapshots using radial landscape analysis. Fig. 4 illustrates a plane subspace containing the first and second SSE snapshots (displayed as star-shaped markers) and the LPE-BSR sample obtained from the first snapshot (represented by a circle marker). Indeed, LPE-BSR provided a novel sample that could contribute to the ensemble; it is clearly diverse from the existing snapshots, as shown in the second and third subplots, while achieving reasonably low individual negative log-likelihoods, as shown in the first subplot. By using such LPE-BSR samples along with SSE snapshots to build an ensemble, we can reduce the cost of achieving target performance in fast ensembling methods or attain better results with the same training budgets.

However, while fast ensembling techniques usually prioritize evaluating training budgets, particularly the number of backward passes as seen in the literature (Huang et al., 2017; Garipov et al., 2018; Zhang et al., 2020), we also consider memory budgets in our analysis--the total number of bits required to represent the entire ensemble model, which grows with the addition of ensemble members in fast ensembling. From this perspective, we devised a method to eliminate heavy SSE snapshots with high precision from the final ensemble; as a result, each original SSE snapshot is replaced by \(S=5\) LPE-BSR samples. This policy also aligns with our research objective of exploring

Figure 4: **Comparison between snapshot and LPE-BSR samples.** Radial landscape plots visualize a plane subspace defined by three points: the first and second snapshot samples obtained by SSE (represented by yellow and blue star-shaped marker \(\)), and LPE-BSR sample derived from the first snapshot (depicted as a red circle \(\)).

ensembling in low precision number systems. Consequently, by forming the ensemble exclusively with LPE-BSR samples in the low precision system, we attained better outcomes compared to SSE regarding both training budgets and memory budgets, as shown in Fig. 5.

### Training-free ensemble construction of pre-trained large models

Our investigation so far, involving deep neural networks up to 300M in size, substantiates the efficacy of the proposed LPE-BSR methodology. In this section, we further extend our validation to confirm that LPE-BSR enables effective ensemble construction without training, even in larger models. To this end, in addition to 300M-scale CLIP-ViT-L/14 model (Radford et al., 2021), we employ a 1B-scale CLIP-ViT-G/14 model (Cheri et al., 2023) and an 8B-scale LLaMa model (Touvron et al., 2023) in a zero-shot manner. Appendix A provides public links for each model.

We first present a radial landscape analysis of the CLIP-ViT-L/14 model in Fig. 6. As we analyzed previously in Section 4.3, we can confirm that the conditions for effective ensemble construction are met here as well, i.e., ensemble members exhibit slightly higher individual negative log-likelihoods (circle markers) compared to the pre-trained model (star-shaped marker) in the first subplot, and they also offer diverse function outputs, as shown in the subsequent subplots. As depicted in the leftmost

Figure 5: Combining with fast ensembling methods. Negative log-likelihood and expected calibration error for fast ensembling methods, SSE and CSGLD, in terms of training budgets, i.e., the number of backward passes, and memory budgets, i.e., the total number of bits for representing ensemble. **Top:** Results with SSE. **Bottom:** Results with CSGLD.

Figure 6: Radial landscapes for zero-shot CLIP-ViT-L/14 model. Radial landscape plots visualize a plane subspace defined by three points: a pre-trained model (depicted as a yellow star-shaped marker \(\)) and two LPE-BSR samples derived from the pre-trained weights (represented by blue and red circle markers \(\)).

subplot of Fig. 7 it leads to achieving lower negative log-likelihood through LPE-BSR compared to the pre-trained model without the need for additional training.

Fig. 7 demonstrates that LPE-BSR consistently improves upon the pre-trained checkpoint, even for larger models such as CLIP-ViT-G/14 and LLaMa-3. The subplots at the top of Fig. 7 demonstrate that the performance of LPE-BSR improves with increasing ensemble size, while the subplots at the bottom show that LPE-BSR occupies the preferred lower-left region of the trade-off plots for memory budgets and performance, surpassing the pre-trained checkpoint. Table 3 offers more detailed results for an ensemble size of \(S=20\), including diversity analysis and evaluation results, further confirming the effectiveness of our proposed LPE-BSR for models with billions of parameters.

## 5 Related Work

The field of Bayesian deep learning provides the most relevant research for our low precision ensembling strategy; Ferianc et al. (2021) integrated quantization-aware training (Jacob et al., 2018) into established Bayesian deep learning methods; Zhang et al. (2022) introduced a technique for implementing stochastic gradient Langevin dynamics (Welling and Teh, 2011) with reduced precision. However, our work differs significantly from theirs in two key aspects: 1) They focused on

    & & & &  &  \\  Model & \# Params & Method & System & (a) & (b) & (c) & NLL & ERR & ECE \\  CLIP-ViT-L/14 & 0.30B &  \(\) Pre-trained \\ LPE-BSR \\  & FP32 &.948 & - &.948 &.948 &.251 &.049 \\  & &  \(\) Pre-trained \\ LPE-BSR \\  & INT-5 &.993 &.053 &.940 & **.929** & **.250** & **.028** \\  CLIP-ViT-G/14 & 1.01B &  \(\) Pre-trained \\ LPE-BSR \\  & FP32 &.942 & - &.942 &.942 & **.206** &.095 \\  & &  \(\) Pre-trained \\ LPE-BSR \\  & INT-5 &.955 &.013 &.941 & **.927** & **.206** & **.089** \\  LLaMa-3 & 8.03B &  \(\) Pre-trained \\ LPE-BSR \\  & FP32 & 1.03 & - & 1.03 & 1.03 & **.361** &.160 \\  & & 
 \(\) Pre-trained \\ LPE-BSR \\  & INT-5 & 1.07 &.049 & 1.02 & **.923** &.364 & **.087** \\   

Table 3: Results for low precision ensembling of pre-trained models. We compute (a) average loss, (b) ambiguity, and (c) ensemble loss for diversity analysis, along with evaluation metrics to assess overall performance. Our LPE-BSR samples are centered around \(\) within each group (pre-trained model in this context), which are separated by horizontal lines.

Figure 7: Constructing low precision ensemble of large models. Negative log-likelihood for pre-trained models (Pre-trained, shown in blue) and low precision ensembling with Bernoulli stochastic rounding centered around the pre-trained model (LPE-BSR, shown in green). The evaluation was conducted on ImageNet for CLIP models and on MMLU for LLaMa-3 in a zero-shot setting. **Top:** When the x-axis represents the ensemble size. **Bottom:** When the x-axis represents memory budgets, i.e., the total number of bits for representing ensemble.

training from scratch, which deviates somewhat from the prevalent practice of utilizing pre-trained large models. 2) They employed small-scale models; the largest model they considered, ResNet-18 with 11 million parameters, falls outside our scope as discussed in Section 4.1, as we are interested in larger scales. Nonetheless, the interest in employing low precision ensembling in Bayesian deep learning holds significant promise. Our demonstration of its feasibility for large models constitutes a meaningful advancement for the Bayesian deep learning community (Papamarkou et al., 2024).

## 6 Conclusion

We provided a novel insight on ensembling within low precision number systems. While conventional wisdom perceives quantization errors stemming from representing neural network weights in low precision as obstacles, we introduced an innovative viewpoint suggesting that these errors could serve as a source of ensemble diversity. Our empirical results demonstrated that low precision weights obtained through stochastic rounding of pre-trained weights could effectively form ensembles and improve uncertainty estimates and calibration, especially for large models. Considering the growing scale of models in recent trends reduces the appeal of ensemble methods due to their inherent scalability issue, where memory costs increase with the number of ensemble components, our exploration of low precision ensembling lays the foundation for developing efficient ensemble methods in the era dominated by large models.

**Limitations and future directions.** At present, our investigations have centered on the simplest form of low precision number system, known as the symmetric uniform quantization scheme. Similar to the quest in neural network quantization for systems that yield better quantized solutions (e.g., Yvinec et al., 2023; Dettmers et al., 2024), the search for systems conducive to more effective low precision ensembling presents an intriguing avenue for future research. Furthermore, we used fake quantization across all experiments for research purposes, which prevented us from benchmarking the latency of the low precision ensemble due to limited access to specialized hardware and software for accelerating the inference speed of quantized models. Nonetheless, as our work relies on the standard symmetric uniform quantization scheme, it remains compatible with ongoing and future advancements in neural network quantization. Developing practical components such as custom CUDA kernels tailored to low precision ensembles would also be a promising future direction.

**Broader impacts.** Our method advocates for the utilization of large models, which could potentially raise ethical concerns (e.g., Weidinger et al., 2021). However, it is important to note that this work primarily focuses on analytical aspects and does not inherently entail significant ethical risks.