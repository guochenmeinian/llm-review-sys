# On Affine Homotopy between Language Encoders

Robin S. M. Chan\({}^{1}\)  Reda Boumasmoud\({}^{1}\)  Anej Svate\({}^{1}\)  Yuxin Ren\({}^{2}\)

Qipeng Guo\({}^{3}\)  Zhijing Jin\({}^{1,4}\)  Shauli Rwfogel\({}^{1}\)  Mrinmaya Sachan\({}^{1}\)

Bernhard Scholkopf\({}^{1,4}\)  Mennatallah El-Assady\({}^{1}\)  Ryan Cotterell\({}^{1}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Tsinghua University \({}^{3}\)Fudan University

\({}^{4}\)Max Plank Institute for Intelligent Systems

###### Abstract

Pre-trained language encoders--functions that represent text as vectors--are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be _intrinsic_, that is, task-independent, yet still be informative of _extrinsic_ similarity--the performance on downstream tasks. It is common to consider two encoders similar if they are _homotopic_, i.e., if they can be aligned through some transformation.1 In this spirit, we study the properties of _affine_ alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.

https://github.com/chanr@/affine-homotopy

## 1 Introduction

A common paradigm in modern natural language processing (NLP) is to pre-train a **language encoder** on a large swathe of natural language text. Then, a task-specific model is fit (_fine-tuned_) using the language encoder as the representation function of the text. More formally, a language encoder is a function \(^{*}^{D}\), i.e., a function that maps a string over an alphabet \(\) to a finite-dimensional vector. Now, consider sentiment analysis as an informative example of a task. Suppose our goal is to classify a string \(^{*}\) as one of three polarities \(=\{,,\}\). Then, the probability of \(\) exhibiting a specific polarity is often given by a log-linear model, e.g., the probability of \(\) is

\[p()=(\,( )+)_{}\] (1)

where \(^{3 D}\), \(^{3}\) and \(^{N}^{N-1}\). Empirically, using a pre-trained encoder \(\) leads to significantly better classifier performance than training a log-linear model from scratch.

In the context of the widespread deployment of language encoders, this paper tackles a natural question: Given two language encoders \(\) and \(\), how can we judge to what extent they are similar? This question is of practical importance--recent studies have shown that even small variations in the random seed used for training can result in significant performance differences on downstream tasks between models with the same architecture [13; 35] In this case, we say that two such language encoders exhibit an _extrinsic_ difference, i.e., the difference between two encoders manifests itself when considering their performance on a _downstream_ task. However, we also seek an _intrinsic_ notionof similarity between two language encoders, i.e., a notion of similarity that is independent of any particular downstream task. Moreover, we may hope that a good notion of intrinsic similarity would allow us to construct a notion of extrinsic similarity that holds for _all_ downstream tasks.

Existing work studies language encoder similarity by evaluating whether two encoders produce similar representations for a finite dataset of strings [3; 20; 22; 42; _inter alia_], often by analyzing whether the representation sets can be approximately _linearly_ aligned [22; 27]. More formally, two encoders are considered similar if there exists a matrix \(\) such that \(()\,()\) holds for strings \(\) in some finite set \(^{*}\).2 This assumes that examining finitely many outputs provides sufficient insight into encoder behavior. In contrast, we set out to study the relationships between language encoders, i.e., functions, themselves. This decision, rather than being just a technicality, allows us to derive a richer understanding of encoder relationships, revealing properties and insights that remain obscured under conventional finite-set analysis. Concretely, we ask what notions of similarity between encoders one could consider and what they imply for their relationships.

The main contributions of the paper are of a theoretical nature. We first define an (extended) metric space on language encoders. We then extend this notion to account for _transformations_ in a broad framework of \(S\)-**homotopy** for a set of transformations \(S\), where \(\) is \(S\)-homotopic to \(\) if \(\) can be transformed into \(\) through some transformation in \(S\). As a concrete application of the framework, we study _affine_ homotopy--the similarity of \(\) and \(\) for affine transformations \(\). The notion of intrinsic similarity induced by such one-sided alignment is not symmetric and can be seen as the _cost_ of transforming \(\) into \(\). Nevertheless, we show it is informative of _extrinsic_ similarity: If one encoder can be affinely mapped to another, we can guarantee that it also performs similarly on downstream tasks. We confirm this empirically by studying the intrinsic and extrinsic similarities of various pre-trained encoders, where we observe a positive correlation between intrinsic and extrinsic similarity. Beyond measuring similarity, homotopy also allows us to define a form of hierarchy on the space of encoders, elucidating a structure in which some encoders are more informative than others. Such an order is also suggested by our experiments, where we find that certain encoders are easier to map to than others which shows in the rank of the learned representations and affects their transfer learning ability.

## 2 Language Encoders

Let \(\) be an alphabet--a finite, non-empty set of symbols \(y\)--and \(\) a distinguished end-of-string symbol. With \(^{*}}}{{=}}_{n=0}^{} ^{n}\) we denote the Kleene closure of \(\), the set of all strings \(\). A **language encoder** is a function \(^{*} V}}{{=}} ^{D}\) that maps strings to real vectors.3 We write \(_{V}}}{{=}}V^{^{*}}\) for the \(\)-vector space of language encoders, and \(_{b}}}{{=}}\{ _{V}(^{*})\} _{V}\) for its sub-vector space of **bounded encoders**.

There are two common ways that language encoders are created . The first is through autoregressive language modeling. A **language model** (LM) is a probability distribution over \(^{*}\).4**Autoregressive LMs are defined through the multiplication of conditional probability distributions \(p_{}(y_{t}_{<t})\) as

\[p_{}^{}()=p_{}( {eos})_{t=1}^{T}p_{}(y_{t}_{<t}),\] (2)

where each \(p_{}(_{<t})\) is a distribution over \(\{\}\)_parametrized_ by a language encoder \(\):

\[p_{}(y_{t}_{<t})}}{{=}}(\,(_ {<t}))_{y_{t}},\] (3)

where \(^{(||+1) D}\). An autoregressive LM provides a simple manner to _learn_ a language encoder from a dataset of strings \(=\{^{(n)}\}_{n=1}^{N}\) by minimizing \(\)'s negative log-likelihood. We may also learn a language encoder through **masked language modeling** (MLM), which defines the conditional probabilities based on both sides of the masked symbol's context

\[p_{}(y_{t}_{<t},_{>t}) }}{{=}}(\, (_{<t}_{>t}) )_{y_{t}}.\] (4)Maximizing the log-likelihood of a corpus under a language model derived from a language encoder \(\) with a gradient-based algorithm only requires \(\) to be a differentiable function of its parameters. Once a language encoder has been trained on a (large) corpus, its representations can be used on more fine-grained NLP tasks such as classification. The rationale for such transfer learning is that representations \(()\) stemming from a performant language model also contain information useful for other downstream tasks on natural language. An NLP practitioner might then implement a task-specific transformation of \(()\). To tackle the problem that the tasks of interest are often less resource-abundant and to keep the training costs low, task-specific transformations are usually simple, often in the form of linear transformations of \(()\), as in Eq. (1).

## 3 Measuring the Alignment of Language Encoders

We begin by introducing measures of affine alignment and hemi-metrics on \(_{V}\).

### Preliminaries on Hemi-Metric Spaces

Language encoders compute representations for the infinitely many strings in \(\)*. In general, these representations might diverge towards \(\), making it necessary to talk about _unbounded_ encoders, where it is convenient to allow distances and norms to take extended real numbers as values.5

**Definition 3.1**.: _An **extended metric** on a set \(X\) is a map \(d X}_{+}\) such that_

* \( x,y X, d(x,y)=0\) _iff_ \(x=y\)_;_ (Identity)__
* \( x,y,z X, d(x,y) d(x,z)+d(z,y);\)__(Triangle Inequality)__
* \( x,y X, d(x,y)=d(y,x).\)__(Symmetry)__

Similarly, an _extended norm_ is a map \(\|\| X}_{+}\) that satisfies the norm axioms. Moreover, we will consider maps \(d\) that do not satisfy the symmetry axiom. Lawvere  notes that symmetry is artificial and unnecessary for many of the main theorems involving metric spaces. In such situations, the quantity \(d(x,y)\) can be interpreted as the _cost_ of going from \(x\) to \(y\). Occasionally, we want \(d\) to capture that it costs more to go from \(x\) to \(y\) than to return, making asymmetry desirable.

**Definition 3.2**.: _A **hemi-metric6** or Lawvere-metric on a set \(X\) is a map \(d X}_{+}\) such that_

* \(d(x,x) d(x,y)+d(y,z)\) _for all_ \(x,y,z X\)_._

One of our main contributions is a formalization of measuring how far a language encoder \(\) is from the _set_ of all possible transformations of another encoder \(\)--for example, from all affine transformations of \(\). For this, we _lift_ a hemi-metric over elements \(x X\) to _subsets_ of \(X\), a crucial for the rest of the paper.

**Definition 3.3**.: _Let \((X,d)\) be a hemi-metric space. For non-empty \(E,E^{} X\), we define_

\[d^{}(E,E^{})}}{{=}} _{x E}_{y E^{}}d(x,y).\] (5)

_The map \(d^{}\) is called the **Hausdorff-Hoare map** and is a hemi-metric on \((X)\{\}\), the power set of \(X\). When \(E\) is a singleton set \(\{x\}\), we will, with a slight abuse of notation, write \(d^{}(x,E^{})\) to mean \(d^{}(\{x\},E^{})\), defined as \(=_{y E^{}}d(x,y)\).7_

We next introduce the **hemi-metric recipe**. It tells us how one can define a hemi-metric on a set \(X\) by _embedding_\(X\) into the power set of another space \(Y\) where a hemi-metric already exists. After \(X\) is embedded, one can use the Hausdorff-Hoare map based on the hemi-metric from \(Y\) to define a hemi-metric on \(X\) through the images of \(x X\).

[MISSING_PAGE_FAIL:4]

In the case of affine isometries \((V)=\{(V)_{}(V)\}\) we show that the pair \((_{V},d_{(V)})\) constitutes an extended pseudo-metric space.

**Proposition 3.1**.: _The pair \((_{V},d_{(V)})\) is an extended pseudo-metric space._

## 4 Intrinsic Affine Homotopy

The notion of affine alignment allows us to introduce _homotopic relations_ on \(_{V}\). We first derive the affine intrinsic preorder \(_{}\) on the space of encoders.13

**Lemma 4.1**.: _Let \((X,d)\) be a hemi-metric space. The relation \((x_{d}y\) iff \(d(x,y)=0)\) is a **preorder14** and it will be called the **specialization ordering** of \(d\)._

Proof.: Goubault-Larrecq [17, Proposition 6.1.8]. 

**Definition 4.1** (Intrinsic Affine Preorder).: _For two encoders \(,_{V}\), we define the relation_

\[_{} d_{(V)}( ,)=0.\] (9)

**Lemma 4.2**.: _The relation \(_{}\) is a preorder on \(_{V}\)._

Proof.: Follows from \(d_{(V)}(,)\|_{}\|_{V}  d_{(V)}(,)\), see App. D.3. 

Intuitively, \(_{}\) captures the order of encoders such that higher-positioned encoders in the order can be \(S\)-transformed to the lower-positioned ones. To derive the implications of \(_{}\) we introduce the notion of an encoder rank.

**Definition 4.2** (Encoder Rank).: _For any \(_{V}\) let the **encoder rank** be \(()}}{{=}}_{ }(V_{})\), where \(V_{}\) is the subvector space generated by the image of \(\). When \(()=_{}(V)\), \(\) is a **full rank** encoder, else it is **rank deficient**._

**Theorem 4.1**.: _For \(,_{V}\), we have_

\[_{}=(_{})(V)\] (10)

_where, \(_{}\) is the orthogonal projection of \(V\) onto \(V_{}\). In particular, if \(d_{(V)}(,)=0\) then \(()()\). If in addtion, we know \(()=()\), then \(\) must by an affine transformation of \(\), i.e., \(=\) for some \((V)\)._

This allows us to state our first notion of language encoder similarity: intrinsic affine homotopy.

**Definition 4.3** (Exact Intrinsic Affine Homotopy).: _We say that two encoders \(,_{V}\) are **exactly intrinsically affinely homotopic** and write \(_{}\) if_

\[d_{(V)}(,)=0()= ().\] (11)

For any \(,_{V}\), one can easily show that

\[_{}(_{ }_{}) d_{(V)} ^{_{},}(,)=0,\] (12)

which implies that \(_{}\) is an equivalence relation on the set of language encoders \(_{V}\). Intuitively, two encoders \(\) and \(\) are exactly intrinsically affinely homotopic, this means that both \(\) can be affinely mapped to \(\), as well as the other way around.

## 5 Extrinsic Homotopy

In SS4, we explore methods for assessing how similar two language encoders are without reference to any downstream tasks. Here, we extend our discussion to the _extrinsic_ homotopy of language encoders. Since language encoders are primarily used to generate representations for downstream tasks--such as in transfer learning, illustrated by the sentiment analysis example in SS1--we argue that the key criterion in the similarity of two encoders lies in how closely we can align predictions stemming from their representations.15

**Principle 5.1** (Extrinsic Homotopy).: Two language encoders \(\) and \(\) are **extrinsically homotopic** if we can guarantee a similar performance on any downstream task \(\) and \(\) might be used for.

The rest of the section formalizes this intuitive notion and describes its relationship with intrinsic affine homotopy. Let \(W\) be the vector space \(^{N}\) and set \((V,W)\) as the set of affine maps from \(V\) to \(W\).16 We define \(_{}}}{{=}}(^{*},^{N-1})\) and \(_{W}=(^{*},W)\). Lastly, we formalize the notion of a transfer learning task as constructing a classifier that uses a language encoder's string representations. Particularly, we set \(_{N}\) to be the family of log-linear models as follows

\[_{N}_{V}(_{^{N-1}}) \{\},_{}(_{V,W}()),\] (13)

where \(_{V,W}\) is the map

\[_{V,W}_{V}(_{W}) \{\},\{(V,W)\}\] (14)

and \(_{}^{N}^{N-1}\) is defined for \(_{+},^{N}\), and \(n[N]\) as

\[_{}()_{n}=)}{_{n^{}=1}^{N}( x_{n^{}})}.\] (15)

**Remark 5.1**.: _Each \(p_{}=_{}(())\) can be seen as a "probability distribution" over \([N]\)_

\[_{N}()=\{p()[N], _{}(())_{}(V,W)\}.\] (16)

Through our standard recipe from Remark 3.1, we can define the following hemi-metrics on \(_{V}\).

**Definition 5.1**.: _For any two encoders \(,_{V}\), we define17_

\[d^{}_{}(V,W)(,) }}{{=}}d^{}_{ _{},W}(_{V,W}(),_{V,W}())\] (17a) \[d^{}_{(V,)}(,) }}{{=}}d^{}_{ _{},^{N-1}}(_{N}(),_{N}( ))\] (17b)

Notice that we use \(d^{}\) rather than \(d\) in Def. 5.1 since we are interested in how closely we can bring \(\) and \(\) when we affinely transform _both_ of them--this corresponds to independently affinely transforming the encoders for the same transfer learning task. In particular, Eq. (17b) measures how different two encoders are on any transfer learning task, formalizing the notion of extrinsic homotopy (cf. Principle 5.1), captured by the following definition.

**Definition 5.2** (Extrinsic Affine Preorder).: _An encoder \(_{V}\) is **exactly extrinsically homotopic** to18\(_{V}\) if \(d^{}_{(V,)}(,)=0\)._

Analogously to Def. 4.1, we use \(d^{}_{(V,)}(,)\) to define a preorder.

**Definition 5.3** (Extrinsic Affine Preorder).: _For two encoders \(,_{V}\), we define the relation_

\[}}} d^{}_{(V,)}(,)=0.\] (18)

**Lemma 5.1**.: _The relation \(}}}\) is a preorder on \(_{V}\)._

We now relate \(d^{}_{(V,W)}(,)\) and \(d^{}_{(V,)}(,)\) from Def. 5.1, and \(d_{(V)}(,)\) from SS4.

**Lemma 5.2**.: _Let \(,_{V}\). We have_

1. _There exists a constant_ \(c()>0\) _such that for any_ \((V,W)\)__ \[d^{}_{,^{N-1}}(_{}( ),_{N}()) c()\|_{}|d_{ (V)}(,).\]
2. \(d^{}_{(V,)}(,) c()d^{ }_{(V,W)}(,)\)_._3. \(d_{(V)}(,)=0 d^{}_{(V,W)}( ,)=0 d^{}_{V(V,)}(,)=0\).

Lem. 5.2 shows that \(_{}\) is _finer_ than \(_{}\). This means that the affine intrinsic preorder is contained in the extrinsic preorder, i.e., \(_{}_{} \). Lastly, we can show that \(d^{}_{(V,)}(,)\) is upper bounded by the intrinsic hemi-metric \(d^{}_{(V)}\).

**Theorem 5.1** (\(\)-Intrinsic \(()\)-Extrinsic).: _Let \(,_{V}\) be two encoders. Then,_

\[d^{}_{(V,)}(,) c()\,d^{ }_{(V)}(,).\]

## 6 Linear Alignment Methods for Finite Representation Sets

SSS 4 and 5 introduce ways of comparing language encoders as functions, which holistically characterizes relationships between them. We now address a more practical concern: Given two language encoders \(\) and \(\), how can we approximate their similarity in practice? Rather than comparing \(():^{*}^{D}\) with \(():^{*}^{D}\) over the entire \(^{*}\),19 we compare them over a finite set of strings \(=\{^{(n)}\}_{n=1}^{N}\). We combine \(\)'s representations given by \(\) and \(\) into matrices \(,^{N D}\), where we denote \(_{_{}}=()\) and \(_{_{}}=()\). We can approximate the notions of similarity from SS3 by optimizing over the affine maps \((V)\) (for example, using gradient descent). Particularly, we approximate intrinsic similarity as

\[_{(V)}(,)}}{{=}}_{(V)}_{ }\|_{_{}}-_{_{} }\|_{V},\] (19)

and extrinsic similarity for some task-specific fixed \(^{}\) as

\[_{^{}}(,)}}{{=}}_{(V,W)}_{}\| (^{}_{_{}})-(_{_{}})\|_{W}.\] (20)

Unfortunately, the \(\) over \(\) makes the optimization in Eqs. (19) and (20) difficult. For simplicity, we turn to commonly used linear alignment methods, which we review for completeness.

Orthogonal Procrustes Problem.Rather than optimizing the infinity norm over \(\) as Eqs. (19) and (20), the orthogonal Procrustes problem finds the orthogonal transformation minimizing the Frobenius norm  by solving \(_{(V)}\|- \|_{F}\). Given the singular-value decomposition \(^{}=^{}\), the optimum is achieved by \(^{}\).20 Since the \(\) is over \((V)\), this defines an extended pseudo-metric space by Prop. 3.1.

Canonical Correlation Analysis (CCA).CCA  is a linear alignment method that finds the matrices \(,\) that project \(\) and \(\) into subspaces maximizing their canonical correlation. Let \(_{,j}\) and \(_{,j}\) be the \(j\)th column vectors of \(\) and \(\), respectively. The formulation is as follows

\[_{j}=_{_{,j},_{,j}}(_{,j},_{,j}) _{i<j}\ _{,j}_{,i}\;,\;\; _{i<j}\ _{,j}_{,i}.\] (21)

The representation similarity is measured in terms of the goodness of CCA fit, e.g., the mean squared CCA correlation \(R_{}^{2}=_{i=1}^{D}_{i}^{2}/D\). We can reformulate the CCA objective in Eq. (21) as

\[_{,}\|^{}- ^{}\|_{F}^{2}(^{})( ^{})^{}=(^{})(^ {})^{}=.\] (22)

Given the singular-value decomposition \(^{}=^{}\), the solution of Eq. (22) is \((},})=((^{})^{-} ,(^{})^{-})\), where \((^{})^{-}\) and \((^{})^{-}\) are whitening transforms of \(\) and \(\). Assuming the data is whitened during pre-processing, CCA corresponds to linear alignment under an orthogonality constraint, equivalent to the orthogonal Procrustes problem; see also App. E.

CCA Extensions.Projection-weighted CCA (PWCCA)  also finds alignment matrices with CCA but applies weighting to correlation values \(_{i}\) to report the goodness of fit. Given the canonical vectors \(}\), PWCCA reports \(_{}=_{i=1}^{D}_{i}_{i}/_{i}_{i}\), where \(_{i}=_{j}|}_{,i},_{,j}|\).21

Non-Alignment Methods. While not explicitly (linearly) aligning representations, CKA  evaluates the kernel similarity between representations. CKA computes the normalized Hilbert-Schmidt independence  between centered kernel matrices \(}\) and \(}\) where \(^{H}_{ij}=k(_{i,},_{j,})\), and \(^{G}_{ij}=k(_{i,},_{j,})\) for a kernel function \(k\), i.e., \((}})/(} })(}}))}\). Linear CKA, where \(k(_{i,},_{j,})=^{}_{i,} _{j,}\), is commonly used.

## 7 Experiments

We now explore the practical implications of our theoretical results. We conduct experiments on ELECTRA , RoBERTa, and the 25 MultiBERT  encoders, which are architecturally identical to Bert-Base models pre-trained with different seeds. We report results on the training sets of two GLUE benchmark classification tasks: SST-2  and MRPC . When reporting \(d\) and \(_{v^{}}\) from Eq.19 and Eq.20, we use the \(L_{2}\) norm for simplicity and approximate \(d^{}_{(V,)}\) as

\[^{}_{(V,)}(,)=_{ ^{}(V,W)}_{(V,W)}_{ }|(^{}_ {,})-(_{,})\|_{2}.\] (23)

The experimental setup and compute resources are further described in App. F.

The Intrinsic 'Preorder' of Encoders. We first investigate whether the asymmetry of \(d_{(V)}\) is measurable in the finite alphabet encoder representations. Figure1 shows distinct vertical lines for both tasks indicating that there are encoders that are consistently easier to affinely map _to_\(()\). This seems to be rather independent of which encoder we map _from_\((^{})\). We further see that this trend is task-independent for early layers but diverges for later layers.

The Influence of Encoder Rank Deficiency. As discussed in SS4, the encoder rank plays a pivotal role in affine mappability; exact affine homotopy is only achievable between equal-rank encoders.22 With this in mind, we return to our findings from Figure1 to evaluate whether the observed differences

Figure 1: Asymmetry between ELECTRA (E), RoBERTa (R), and MultiBERT encoders (M1-M25) across layers. For each pair of the encoders \(^{(i)}\) and \(^{(j)}\), we generate training set embeddings \(^{(i)},^{(j)}^{N D}\) for SST-2, COLA, and MNLI. We then fit \(^{(i)}\) to \(^{(j)}\) with an affine map and report the goodness of fit through the max error L2 norm, i.e., an approximation of \(d(^{(j)},^{(i)})\) on row \(i\) and column \(j\) of the grid. Full results across GLUE tasks are shown in Figure4.

between encoders can be attributed to a difference in measurable rank. Due to the inaccuracies of computing the rank numerically, we approximate the encoder rank using the **rank to precision**\(\) as the number of representation matrix singular values larger than some \(\).23 We find statistically significant (\(p\)-value < 0.05) rank correlation with the median intrinsic distance \(_{(V)}\) when mapping _to_ the corresponding encoder for RTE (\(=0.312\)), MRPC (\(=0.609\)), and QQP (\(=0.389\)). We find no statistically significant correlations with the median distance when mapping _from_ the corresponding encoder. This difference in encoder ranks could, therefore, partially explain the previously observed differences in affine mappingly as some encoders seem to learn lower-rank representations.

A Linear Bound on Extrinsic Similarity. Lem. 5.2 derives a relationship between affine intrinsic and extrinsic similarity. To evaluate its strength in practice, we measure Spearman's Rank Correlation (\(\)) and Pearson Correlation (PCC) between intrinsic measures introduced in SS6 and the extrinsic measures \(_{^{}}\) and \(_{(V,)}^{}\). PCC measures the strength and direction of a linear relationship between two random variables, whereas Spearman's \(\) additionally evaluates the variables' monotonic association. \(_{^{}}\) is computed by training a linear classifier \(^{}(V)\) on the final MultiBERT layer for each task. Further, we report \(_{(V,)}^{}\) as the maximum \(L_{2}\) loss for a large number of randomly generated24 classifiers \(^{}\) on the final layer of each MultiBERT encoder. We generate \(100\) such classifiers for a range of GLUE datasets.25 Table 1 show significant, large linear correlation prevalent in all linear alignment methods, whereas CKA--a linear, non-alignment method--does not capture extrinsic behavior as faithfully. Further, Figure 2 visualizes the linear relationship explicitly for all considered GLUE datasets.

## 8 Discussion

We set out to explore homotopic relationships between language encoders, augmenting existing work on the similarity of finite representation sets by holistically studying encoder _functions_. In particular, the general framework of \(S\)-homotopy allows us to study any functional relationship between encoders, enabling the exploration of many types of encoder relationships. As a first step in this direction and a concrete example, SS4 explores _affine_ homotopy, discussing what it means to be able to align two models with affine transformations. Here, Hausdorff-Hoare maps prove useful, as they allow us to measure a notion of (asymmetric) distance between a point--an encoder--and the _set_ of all affine transformations of another encoder. Lem. 5.2 in SS5 then connects the intrinsic,

Figure 2: For ELECTRA (E), RoBERTa (R), and MultiBERTs (M1-M25), we plot extrinsic (\(_{^{}}\)) against intrinsic similarity (\(_{(V)}\)) across GLUE tasks. We group the points by how well we can map to each encoder (\(\)), and display the median, as well as the first and third quartiles as vertical and horizontal lines. We additionally show the linear regression from \(_{(V)}\) to \(_{^{}}\).

task-independent, similarly to extrinsic similarity--the similarity of performance on downstream tasks. Concretely, it derives a linear relationship between the intrinsic and extrinsic dissimilarity for any fixed affine transformation \(^{}\) (i.e., a fixed downstream task). Thm. 5.1 discusses a stronger bound, namely on the _worst-case_ extrinsic dissimilarity among all downstream linear classifiers, i.e., among all possible tasks. Further, by accounting for the asymmetries of encoder relationships, we augment the work on similarity in proper metric spaces .

Although encoders may not be affinely related in practice, empirical evidence in SS7 suggests that notions of affine order still surface (cf. Tab. 1, Fig. 2), particularly as differently initialized BERTs exhibit variations in downstream task performance . While other similarity measures, such as those used in seed specificity tests , are designed to remain invariant to initialization changes, our results indicate that intrinsic affine homotopy is appropriately _sensitive_ to them. This sensitivity raises new questions about the landscape of pre-trained encoders; as seen in Fig. 1, asymmetry in intrinsic affine similarity among similarly pre-trained encoders impacts downstream performance, as corroborated by Lem. 5.2 and empirical results in Tab. 1. Differences in representation ranks may partly explain this asymmetry--mapping between artificially generated rank-deficient encoders yields mostly symmetric affine distances (cf. Fig. 3). Another explanation might be that easy-to-learn encoders might be approximately linear combinations of others, making them easy to map _to_ but not necessarily _from_. Overall, our findings highlight the need to account for directionality in encoder similarity measures to address the asymmetry inherent in this problem.

## 9 Conclusion

We discuss the structure of the space of language encoder in the framework of \(S\)-homotopy--the notion of aligning encoders with a chosen set of functions. We formalize affine alignment between encoders and show that it provides upper bounds on the differences in performance on downstream tasks. Experiments show our notion of intrinsic affine homotopy to be consistently predictive of downstream task behavior while revealing an asymmetric order in the space of encoders.

    & \(_{(V)}\) & Orth. Procrustes & \(R^{2}_{CCA}\) & PWCCA & Linear CKA \\   & Yes & Yes & Yes & Yes & No \\   & \(_{(V)}\) & \(\) & 0.080 & 0.095 & **0.172*** & 0.016 & 0.088 \\  & PCC & 0.545* & 0.937* & 0.932* & **0.970*** & 0.231* \\   & \(^{}_{(V,)}\) & \(\) & **0.621*** & 0.157* & 0.071 & 0.231* & 0.295* \\  & PCC & **0.723*** & 0.539* & 0.457* & 0.566* & 0.320* \\   & \(_{^{}}\) & \(\) & **0.309*** & 0.250* & -0.001 & 0.220* & 0.214* \\  & PCC & 0.707* & 0.697* & 0.733* & **0.743*** & 0.241* \\   & \(^{}_{(V,)}\) & \(\) & **0.231*** & 0.025* & 0.178* & 0.059 & 0.030 \\  & PCC & 0.790* & 0.755* & **0.879*** & 0.875* & 0.174* \\   & \(_{^{}}\) & \(\) & **0.534*** & 0.053 & 0.037 & 0.308* & 0.185* \\  & PCC & **0.570*** & 0.401* & 0.429* & 0.250* & 0.078 \\   & \(^{}_{(V,)}\) & \(\) & 0.234* & 0.317* & -0.147* & **0.338*** & 0.240* \\  & PCC & 0.718* & **0.870** & 0.778 & 0.780 & 0.205 \\   & \(_{^{}}\) & \(\) & **0.196*** & 0.006 & 0.040 & 0.185* & 0.165* \\  & PCC & 0.204* & 0.529* & **0.553*** & 0.550* & 0.215* \\    & \(^{}_{(V,)}\) & \(\) & 0.348* & 0.078 & 0.133* & 0.340* & **0.380*** \\    & PCC & 0.429* & 0.664* & 0.318* & **0.786*** & 0.513* \\   

Table 1: Spearman’s Rank Correlation Coefficient (\(\)) and Pearson’s Correlation Coefficient (PCC) between intrinsic measures introduced in §6 and the extrinsic similarities \(_{^{}}\) and \(^{}_{(V,)}\) across various GLUE datasets. * indicates a \(p\)-value \(<0.01\) (assuming independence).

## Broader Impact

This paper presents foundational research about the similarity of language encoders. To the best of our knowledge, there are no ethical or negative societal implications to this work.