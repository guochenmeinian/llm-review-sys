# Non-asymptotic Approximation Error Bounds of Parameterized Quantum Circuits

Zhan Yu\({}^{1,\,2}\) Qiuhao Chen\({}^{1}\) Yuling Jiao\({}^{1,\,3}\) Yinan Li\({}^{1,\,3}\) Xiliang Lu\({}^{1,\,3}\)

Xin Wang\({}^{4}\) Jerry Zhijian Yang\({}^{1,\,3}\)

\({}^{1}\) School of Mathematics and Statistics, Wuhan University, Wuhan 430072, China

\({}^{2}\) Centre for Quantum Technologies, National University of Singapore, 117543, Singapore

\({}^{3}\) Hubei Key Laboratory of Computational Science, Wuhan 430072, China

\({}^{4}\) Thrust of Artificial Intelligence, Information Hub,

Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511453, China

Z.Y. and Q.C. contributed equally to this workYinan.Li@whu.edu.cnzjyang.math@whu.edu.cn

###### Abstract

Parameterized quantum circuits (PQCs) have emerged as a promising approach for quantum neural networks. However, understanding their expressive power in accomplishing machine learning tasks remains a crucial question. This paper investigates the expressivity of PQCs for approximating general multivariate function classes. Unlike previous Universal Approximation Theorems for PQCs, which are either nonconstructive or rely on parameterized classical data processing, we explicitly construct data re-uploading PQCs for approximating multivariate polynomials and smooth functions. We establish the first non-asymptotic approximation error bounds for these functions in terms of the number of qubits, quantum circuit depth, and number of trainable parameters. Notably, we demonstrate that for approximating functions that satisfy specific smoothness criteria, the quantum circuit size and number of trainable parameters of our proposed PQCs can be smaller than those of deep ReLU neural networks. We further validate the approximation capability of PQCs through numerical experiments. Our results provide a theoretical foundation for designing practical PQCs and quantum neural networks for machine learning tasks that can be implemented on near-term quantum devices, paving the way for the advancement of quantum machine learning.

## 1 Introduction

In quantum computing, one key area is to investigate if quantum computers could accelerate classical machine learning tasks in data analysis and artificial intelligence, giving rise to an interdisciplinary field known as _quantum machine learning_. As the quantum analogs of classical neural networks, _parameterized quantum circuits_ (PQCs)  have gained significant attention as a prominent paradigm to yield quantum advantages. PQCs offer a concrete and practical way to implement quantum machine learning algorithms in noisy and intermediate-scale quantum (NISQ) devices , rendering them well-suited for a diverse array of tasks .

To establish the practical significance of quantum machine learning, an ongoing pursuit is to demonstrate their superiority in solving real-world learning problems compared to classical learning models, including the most commonly used deep neural networks . Typical supervised learning tasks, such as image classification and price prediction, aim to construct a model to learn a mappingfunction from the input to output via training data sets. Essentially, the goal is to approximate multivariate functions. This viewpoint leads to the celebrated _Universal Approximation Theorem_, which limits what neural networks can theoretically learn. Recently, powerful tools from approximation theory have been utilized to establish a fruitful mathematical framework for understanding the "black magic" of deep learning by establishing non-asymptotic approximation error bounds of deep neural networks in terms of the _width_, _depth_, _number of weights (neurons)_ and function complexities, see e.g. Refs.  and references therein.

Substantial investigations have showcased the power of quantum machine learning for specific learning tasks . A fundamental question is whether the _expressivity_ of quantum machine learning models is as powerful as, or is more powerful than, the expressivity of classical machine learning models. This can be illustrated by proving universal approximation theorems for PQCs , indicating that there exist PQCs with suitable parameter configurations to approximate target functions up to a given approximation accuracy. This will justify the power of PQCs to solve supervised learning tasks in a mathematical way. To further investigate whether PQCs are more expressive than the classical models or not, it is natural to examine the PQC approximation performance by establishing approximation error bounds for important function classes. Such quantitative error bounds are less known in the quantum setting, because the hypothesis functions generated by PQCs are more complicated than those generated by classical neural networks.

The difficulties of analyzing the PQC approximation performances can be partially overcome by allowing _parameterized classical data processing_. Namely, trainable parameters are allowed not only in the quantum gates in PQCs but also in the classical data pre- and post-processing. This allows one to prove approximation error bounds following classical strategies . For instance, Goto et al.  proved PQC approximation error rate for Lipschitz continuous functions in terms of the number of qubits and trainable parameters by incorporating trainable parameters in the measurement post-processing phase; similar results can also be obtained by utilizing Tensor-Train Network  or by linear transformations to preprocess the classical data.

However, utilizing parameterized classical data processing makes it hard to distinguish whether the expressive power of PQCs comes from the classical or quantum parts. In fact, parameterized classical data processing enables one to directly convert the hypothesis functions generated by the quantum models into hypothesis functions generated by classical ones and adapt expressivity results for classical machine learning models to extract the expressivity of such quantum models. As a consequence, the resulting PQCs have very simple structures and short depth. It remains unknown whether one can prove approximation error bounds for PQCs without parameterized classical data processing. On the other hand, Zhao et al.  proved exponential lower bounds on the number of trainable parameters (in terms of the number of variables) needed for approximating bounded Lipschitz continuous functions using PQCs without parameterized classical data processing, illustrating that using PQCs to approximate Lipschitz functions still suffers from the _curse of dimensionality_ (CoD) met by classical deep neural networks . However, this does not rule out the possibility that one can achieve the same approximation rate with PQCs of _smaller size_ compared to classical deep neural networks.

In this paper, we explicitly construct _the first_ PQCs _without_ parameterized classical data processing for approximating multivariate polynomials and smooth functions; a glance at these constructed PQCs is illustrated in Fig. 1. This eliminates the ambiguity regarding whether the expressivity originates from classical or quantum parts. We also establish _non-asymptotic PQC approximation error bounds_, in the sense that the PQC approximation performances are characterized in terms of the number of qubits (width), the _depth of PQCs_, the number of trainable parameters/gates (parameter count), and the function complexities. These results enable us to compare the approximation power of PQCs with that of classical neural networks. Notably, we show that for multivariate smooth functions, the quantum circuit size and the number of trainable parameters of our proposed PQCs demonstrate an improvement over the prior result of deep ReLU neural networks , one of the most commonly used neural network family in classical deep learning theory. Our proposed PQCs not only possess the universal approximation property but also achieve parameter efficiency comparable to classical neural networks, potentially leading to more efficient and scalable quantum machine learning algorithms for real-world tasks.

## 2 Preliminaries

Quantum states.The basic unit of information in quantum computing is the _qubit_, which can exist in a superposition of the states 0 and 1 simultaneously, unlike classical bits that are restricted to either 0 or 1. A pure quantum state in the \(d\)-dimensional Hilbert space \(^{d}\) is represented by the _Dirac_ notation \(|\). The conjugate transpose of \(|\) is denoted by \(|\). The inner product of two quantum states \(|\) and \(|\) is written as \(|\). An important property is that \(|=1\) for any pure state \(|\). By convention, the computational basis states for single-qubit systems are written as \(|0=[1,0]^{T}\) and \(|1=[0,1]^{T}\), where the superscript \(T\) denotes the transpose. For \(n\)-qubit systems, the computational basis states are expressed as \(|j\{|0,|1\} ^{ n}\), where \(\) denotes the tensor product operation.

Quantum gates.Quantum gates are building blocks of quantum circuits operating on quantum states. Unlike classical gates, quantum gates are reversible and described as unitary matrices. In quantum machine learning, common parameterized quantum gates include single-qubit Pauli rotation gates \(R_{X}()=e^{- X/2}\), \(R_{Y}()=e^{- Y/2}\), and \(R_{Z}()=e^{- X/2}\) that rotate a quantum state through angle \(\) around the corresponding axis, where the three Pauli operators are defined as:

\[X=0&1\\ 1&0, Y=0&-i\\ i&0, Z=1&0\\ 0&-1,\]

where \(i\) represents the imaginary unit. Commonly used two-qubit quantum gates include CNOT gate that flips the target qubit if and only if the the control qubit is in \(|1\).

Quantum measurementThe quantum measurement is a procedure manipulating a quantum system to extract classical information. The simplest measurement is the computational basis measurement: For a single-qubit system \(|=|0+|1\), the outcome is either \(|0\) with probability \(||^{2}\) or \(|1\) with probability \(||^{2}\). These measurements project the quantum state onto the measured basis, collapsing the state itself. Observables, represented by Hermitian operators, correspond to measurable quantities in a quantum system like energy or position. Each observable has a set of possible outcomes (eigenvalues) and corresponding states (eigenvectors). When a measurement of an observable is performed, the outcome is one of the eigenvalues, and the state of the system collapses

Figure 1: **Overview of PQCs for approximating continuous functions.** (a) Flowchart illustrating the strategy for using PQCs to approximate continuous functions via implementing Bernstein polynomials. The input data \(x\) is encoded into the PQC through \(S(x)\), with the PQC (blue background) capable of representing parity-constrained polynomials up to degree \(3\) (as \(x\) is encoded three times). The technique of linear combination of unitaries (LCU) is used to aggregate these polynomials together. The output of PQC derives from measurement with a specific observable. Fine-tuning trainable parameters in \(R_{Z}\) gates yields a polynomial output depicted in the right panel. (b) Flowchart illustrating the strategy of approximation via local Taylor expansions. We first apply a PQC to localize the input domain into \(K=5\) regions. For example, for input \(x[0.8,1]\), PQC outputs \(x^{}=0.8\) as a fixed point. Then \(x-x^{}\) will be fed into a new PQC for implementing the local Taylor expansions at the fixed point \(x^{}\), forming a nesting architecture. Control gates with pink backgrounds implement the Taylor coefficients. Fine-tuning trainable parameters in \(R_{X}\) and \(R_{Z}\) gates yields a piecewise polynomial with degree \(3\) that approximates the target function.

to the corresponding eigenvector. If we are measuring a state \(\) using observable \(\), the expected value of outcome is \(\). This represents the average result one would expect from repeated measurements on identically prepared systems. A comprehensive introduction to the fundamental notations and concepts of quantum computation can be found in .

Data re-uploading PQCs.The PQCs we shall construct in this paper are of _data re-uploading_ type , i.e., consisting of interleaved data encoding circuit blocks and trainable circuit blocks. More precisely, let \(\) be the input data vector and \(=(_{0},,_{L})\) be a set of trainable parameter vectors. \(S()\) is a quantum circuit that encode \(\) and \(V(_{j})\) is a trainable quantum circuit with trainable parameter vector \(_{j}\). An \(L\)-layer data re-uploading PQC can be then expressed as

\[U_{}()=V(_{0})_{j=1}^{L}S()V(_{j}),\] (1)

Applying \(U_{}()\) to an initial quantum state and measuring the output states provides a way to express functions on \(\):

\[f_{U_{}}()U_{}^{}() U_{}(),\] (2)

where \(\) is some Hermitian observable. The _approximation capability_ of the PQC \(U_{}()\) can be characterized by the classes of functions that \(f_{U_{}}()\) can approximate by tuning the trainable parameter vector \(\). We then turn to an example of single-qubit PQCs approximating univariate functions. For the input \(x[-1,1]\), we utilized the Pauli \(X\) basis encoding scheme  and defined the data encoding operator as a Pauli X rotation \(S(x) e^{i(x)X}\). Interleaving the data encoding unitary \(S(x)\) with some parameterized Pauli \(Z\) rotations \(R_{Z}()\) gives the circuit of data re-uploading PQC for one variable as \(U_{}() R_{Z}(_{0})_{j=1}^{L}S(x)R_{Z}( _{j})\) where \(=(_{0},,_{L})^{L+1}\) is a set of trainable parameters. Utilizing results from quantum signal processing [45; 46; 47], there exists \(^{L+1}\) such that \(U_{}(x)\) implements polynomial transformations \(p(x)[x]\) as \(p(x)=U_{}(x)\) for any \(x[-1,1]\) if and only if the degree of \(p(x)\) is at most \(L\), the parity of \(p(x)\) is \(L 2\)4, and \( 1\) for all \(x[-1,1]\). Then, univariate functions that could be approximated by the specified polynomial \(p(x)\) could also be approximated by the PQC \(U_{}(x)\). Other than the real polynomials, there are also types of single-qubit PQC with Pauli \(Z\) basis encoding that could implement complex trigonometric polynomials .

## 3 Expressivity of PQCs for multivariate continuous functions

### Explicit construction of PQCs for multivariate polynomials

Although PQCs for approximate univariate functions have been constructed and analyzed, they have not yet been generally extended to the case of multivariate functions. Current proofs of universal approximation for multivariate functions are nonconstructive [34; 38] and require arbitrary circuit width, arbitrary multi-qubit global parameterized unitaries, and arbitrary observables. Goto et al.  proposed several constructions for approximating multivariate functions with the assistance of parameterized data pre-processing and post-processing, yielding a quantum-enhanced hybrid scheme rather than a purely quantum setting.

We now move to our explicit construction of PQCs for multivariate polynomials. A multivariate polynomial with \(d\) variables and degree \(s\) is defined as \(p()_{\|\|_{1} s}c_{}^{}\) where \(^{}=x_{1}^{_{1}}x_{2}^{_{2}} x_{d}^{_ {d}}\). To implement the multivariate polynomial \(p()\), we first build a PQC to express a monomial \(c_{}^{}\). The construction is a trivial extension of the univariate case: We simply apply the single-qubit PQC with Pauli \(X\) basis encoding on each \(x_{j}\) to implement \(x_{j}^{_{j}}\) for \(1 j d\), respectively. The coefficient \(c_{}\) could be implemented by any of these PQCs. Thus we could construct a PQC \(U^{}()_{j=1}^{d}U_{_{j}}(x_{j})\) such that \(^{ d}U^{}()^{ d}=c_{} ^{}\). The depth of the PQC \(U^{}()\) is at most \(2s+1\), the width is at most \(d\), and the number of parameters is at most \(s+d\).

Having PQCs that implement monomials, the next step is to aggregate monomials to implement the multivariate polynomial. A natural idea is to sum the monomial PQCs together as \(U_{p}()=_{\|\|_{1} s}U^{}()\). However, the addition operation in quantum computing is non-trivial as the sum of unitary operators is not necessarily unitary. To overcome this issue, we utilize _linear combination of unitaries_ (LCU)  to implement the operator \(U_{p}()\) on a quantum computer. Realizing the linear combination of PQCs \(U^{}()\) requires applying multi-qubit control on each \(U^{}()\), which could be further decomposed into linear-depth quantum circuits of CNOT gates and single-qubit rotation gates without using any ancilla qubit . Then we can obtain the polynomial \(p()=+|^{ d}U_{p}()|+^{  d}\) by applying the Hadamard test on the LCU circuit. Summarizing the above, we establish the following theorem about using PQCs to implement multivariate polynomials. A formal description of such PQCs is given in Appendix B.

**Theorem 1**.: _For any multivariate polynomial \(p()\) with \(d\) variables and degree \(s\) such that \(|p()| 1\) for \(^{d}\), there exists a PQC \(W_{p}()\) such that_

\[f_{W_{p}}() 0|W_{p}^{}()Z^{(0)}W_ {p}()|0=p()\] (3)

_where \(Z^{(0)}\) is the Pauli \(Z\) observable on the first qubit. The width of the PQC is \(O(d+ s+s d)\), the depth is \(O(s^{2}d^{s}( s+s d))\), and the number of parameters is \(O(sd^{s}(s+d))\)._

Note that the initial state in the Hadamard test is \(|0^{ d}\) since \(|+^{ d}\) could be easily prepared by applying Hadamard gates on \(|0^{ d}\). Measuring the first qubit of \(W_{p}()\) for \(O(})\) times is needed to estimate the value of \(p()\) up to an additive error \(\). We could further use the amplitude estimation algorithm  to reduce the overhead while increasing the circuit depth by \(O()\).

### PQC approximation for continuous functions

Polynomials play a central role in approximation theory. The celebrated Weierstrass approximation theorem (see e.g. [51, Sec. 10.2.2]) indicates that polynomials are sufficient to approximate continuous univariate functions. For multivariate functions, their approximation can be implemented using Bernstein polynomials . We shall apply these results to prove PQC approximation error bounds for multivariate Lipschitz continuous functions.

For a \(d\)-variable continuous function \(f:^{d}\), the multivariate Bernstein polynomial with degree \(n^{+}\) of \(f\) is defined as

\[B_{n}()_{k_{1}=0}^{n}_{k_{d}=0}^{n}f }{n}_{j=1}^{d}}x_{j}^{k_{j}}(1-x _{j})^{n-k_{j}},\] (4)

where \(=(k_{1},,k_{d})\{0,,n\}^{d}\). It is known that Bernstein polynomials converge uniformly to \(f\) on \(^{d}\) as \(n\). The PQC constructed in Theorem 1 could implement the Bernstein polynomial with proper rescaling, which implies that the PQC is a universal approximator for any bounded continuous functions.

**Theorem 2** (The Universal Approximation Theorem of PQC).: _For any continuous function \(f:^{d}[-1,1]\), given an \(>0\), there exist an \(n\) and a PQC \(W_{b}()\) with width \(O(d n)\), depth \(O(dn^{d} n)\) and the number of trainable parameters \(O(dn^{d})\) such that_

\[|f()-f_{W_{b}}()|\] (5)

_for all \(^{d}\), where \(f_{W_{b}}() 0|W_{b}^{}()Z^{(0)}W_ {b}()|0\)._

Theorem 2 serves as the quantum counterpart to the universal approximation theorem of classical neural networks. Moreover, the PQCs that universally approximate continuous functions are explicitly constructed without any impractical assumption, improving the previous results presented in Refs. . Moreover, for continuous functions \(f\) satisfying the Lipschitz condition, \(|f()-f()|\|-\|_{}\) for any \(,\), the approximation rate of Bernstein polynomials could be quantitatively characterized in terms of the degree \(n\), the number of variables \(d\) and the Lipschitz constant \(\). Thus a non-asymptotic error bound for PQC approximating Lipschitz continuous functions could be obtained as follows.

**Theorem 3**.: _Given a Lipschitz continuous function \(f:^{d}[-1,1]\) with a Lipschitz constant \(\), for any \(>0\) and \(n\), there exists a PQC \(W_{b}()\) with such that \(f_{W_{b}}() 0|\,W_{b}^{}()Z^{(0)}W_{b}() \,|0\) satisfies_

\[|f()-f_{W_{b}}()|+21+} {n^{2}}^{d}-1+d2^{d}}{ n^{2}}\] (6)

_for all \(^{d}\). The width of the PQC is \(O(d n)\), the depth is \(Odn^{d} n\), and the number of parameters is \(O(dn^{d})\)._

We prove these theorems in Appendix C. Although a quantitative approximation error bound is characterized in Theorem 3, we could find that \(n\) must be sufficiently large to obtain a good precision, yielding an extremely deep PQC. This inefficiency is essentially due to the intrinsic difficulty of using a single global polynomial to approximate a continuous function uniformly. A possible approach that may overcome the obstacle is to use local polynomials to achieve a piecewise approximation, which we will discover in the next section.

### PQC approximation for Holder smooth functions

To achieve a piecewise approximation of multivariate functions, we follow the path of classical deep neural networks approximation [18; 21; 25], which utilizes multivariate Taylor series to approximate target functions in small local regions.

We focus on Holder smooth functions. Let \(=s+r>0\), where \(r(0,1]\) and \(s^{+}\). For a finite constant \(B_{0}>0\), the \(\)-Holder class of functions \(^{}(^{d},B_{0})\) is defined as

\[^{}(^{d},B_{0})\!=\!f\!:^{d}\!\!\!,_{\|\|_{1} s}\!\|^{}f\|_{}\!\!B_{0}, _{\|\|_{1}=s}_{}}f()-^{}f()|}{\|-\|_{2}^{}} \!\!B_{0}},\] (7)

where \(^{}=^{_{1}}^{_{d}}\) for \(=(_{1},,_{d})^{d}\). We note that Holder smooth functions are natural generalizations of various continuous functions: When \((0,1)\), \(f\) is Holder continuous with order \(\) and Holder constant \(B_{0}\); when \(=1\), \(f\) is Lipschitz continuous with Lipschitz constant \(B_{0}\); when \(1<\), \(f C^{s}(^{d})\), the class of \(s\)-smooth functions whose \(s\)-th partial derivatives exist and are bounded. As shown in Petersen and Voigtlaender , for any \(\)-Holder smooth function \(f^{}(^{d},B_{0})\), its local Taylor expansion at some fixed point \(_{0}^{d}\) satisfies

\[f()-_{\|\|_{1} s}}f (})}{!}(-})^{} d^{s} \|-}\|_{2}^{}\] (8)

for all \(^{d}\), where \(!=_{1}!_{d}!\). Next, we show how to construct PQCs to implement the Taylor expansion of \(\)-Holder functions in the following three steps.

Localization.To utilize the Holder smoothness, we need to first localize the entire region \(^{d}\). The motivation of localization is to determine the local point \(}\) in Eq. (8) so that the distance between \(\) and \(}\) is fairly small. An intuitive configuration is illustrated in Fig. 2, where the stars represent the local points. Given \(K\) and \((0,)\), for each \(=(_{1},,_{d})\{0,1,,K-1\}^{d}\), we define

\[Q_{}=(x_{1},,x_{d}):x_{i} {_{i}}{K},+1}{K}- 1_{_{i}<K-1}}.\] (9)

By the definition of \(Q_{}\), the region \(^{d}\) is approximately divided into small hypercubes \(_{}Q_{}\) and some trifling region \((d,K,)^{d}(_{}Q_{})\), as illustrated in Fig. 2.

We construct a PQC that maps all \( Q_{}\) to some fixed point \(}=}{K}\) in \(Q_{}\), i.e., approximating the piecewise-constant function \(D()=}{K}\) if \( Q_{}\). We describe our construction for \(d=1\), where \(D(x)=\) if \(x[,- 1_{k<K-1}]\) for \(k=0,,K-1\). The multivariate case could be naturally generalized by applying \(D(x)\) to each variable \(x_{j}\). The idea is to construct a polynomial that approximates the function \(D(x)\) based on the polynomial approximation to the sign function , which a single-qubit PQC can then implement. Generalizing to the multivariate localization, there exists a PQC \(W_{D}()\) of depth \(O()\) and width \(O(d)\) such that the output \(f_{W_{D}}()\) maps \(\) to the corresponding fixed point \(}\) with precision \(\). We can obtain an estimation of \(\) using \( Kf_{W_{D}}()\).

Implementing the Taylor coefficients.Next, we use PQC to implement the Taylor coefficients \(_{,}}f(})}{ !}[-1,1]\) for each \(=(_{1},,_{d})\{0,1,,K-1\}^{d}\) and \(\), which is essentially a point-fitting problem. Then we could construct a PQC \(U^{}_{co}=_{}\!  R_{X}(_{,})\) such that \(,0\,U^{}_{co},0=_{ ,}\), where \(=_{1}_{d}\) and \(_{,}=2(_{,})\). The depth of \(U_{}\) is \(O(K^{d})\), the width is \(O(d K)\), and the number of parameters is \(O(K^{d})\). Note that the state \(\) can be prepared using basis encoding on the provided \(= Kf_{W_{D}}()\) from the localization step.

Implementing multivariate Taylor series.To implement the multivariate Taylor expansion of a function at some fixed point \(}\), we first build a PQC to represent a single term in the Taylor series, which could be done by combining the PQC, which implements the Taylor coefficients and the PQC which implements monomials, i.e., constructing \(U^{}_{}() U^{}_{co} U^{ }(-})\). The depth of \(U^{}_{}()\) is \(O(K^{d}+s)\), the width is \(O(d K)\), and the number of parameters is at most \(K^{d}+s+d\). The next step is to aggregate single Taylor terms together to implement the truncated Taylor expansion of the target function. We use LCU to construct the PQC \(U_{t}(,})_{_{1} s}U ^{}_{}()\) so that we can implement the Taylor expansion of the function \(f\) at point \(}\) as \(,0+^{ d}\,U_{t}(, }),0+^{ d}\).

We construct a nested PQC as \(U_{t}(,f_{W_{D}}())\), such that for any input \(\), the corresponding fixed point could be determined by the localization PQC. Such a PQC could be used, together with the Hadamard test, to approximate Holder smooth functions. In particular, we prove the approximation error bound of our constructed PQC based on the error rate of Taylor expansion in Eq. (8).

**Theorem 4**.: _Given a function \(f^{}(^{d},1)\) with \(=r+s\), \(r(0,1]\) and \(s^{+}\), for any \(K\) and \((0,)\), there exists a PQC \(W_{t}()\) such that \(f_{W_{t}}() 0\,W_{t}^{}()Z^{(0)}W_{t}( )\, 0\) satisfies_

\[ f()-f_{W_{t}}() d^{s+/2}K^{-}\] (10)

_for \(_{}Q_{}\). The width of the PQC is \(O(d K+ s+s d)\), the depth is \(O(s^{2}d^{*}K^{d}( s+s d+d K))+ K)\), and the number of parameters is \(O(sd^{*}(s+d+K^{d})+ K)\)._

The proof can be found in Appendix D. Note that the PQC in Theorem 4 consists of two nested parts and its depth is counted as the sum of two PQCs for simplicity. We have established the uniform convergence property of PQCs for approximating Holder smooth function on \(^{d}\) except for the trifling region \((d,K,)\). The Lebesgue measure of such a trifling region is no more than

Figure 2: **An illustration of localization. The left panel demonstrates the localization \(_{}Q_{}\) for \(K=5\) and \(d=1\). The right panel shows the case of localization for \(K=5\) and \(d=2\). The “volume” of the trifling region \((d,K,)\) is no more than \(dK\).**

\(dK\). We can set \(=K^{-d}\) with no influence on the size of the constructed PQC, and a similar approximation error bound in the entire region \(^{d}\) under the \(L^{2}\) distance could be obtained.

## 4 Numerical experiments

This section presents numerical experiments to illustrate the expressivity of our proposed PQCs in approximating multivariate functions. We focus on approximating a bivariate polynomial function

\[f(x,y)=+y-1.5)^{2}+(x+y^{2}+)^{2}+(x+y-0.5)^{2}}{5^{2}},\]

over the domain \((x,y)^{2}\). The approximation process involves two separate steps: (1) Learning a piecewise-constant function, \(D(x)=\) if \(x[,)\), using a single-qubit PQC, where \(K^{+}\) determines the number of intervals for the piecewise-constant function. (2) Learning the Taylor expansion of \(f(x,y)\) using multi-qubit PQCs based on Theorem 4. Both learning processes are implemented on a Gold 6248 2.50 GHz Intel(R) Xeon(R) CPU.

We randomly sample \(200\) data points within the domain \(\) to create training and test datasets for \(D(x)\). A single-qubit PQC with adjustable parameters \(L=764\) (\(L=996\)) is used to learn \(D(x)\) with \(K=2\) (\(K=10\)). Each parameter of the PQC is randomly initialized within the range \([0,]\). We use the Adam optimizer  with a learning rate of \(0.01\) to minimize the Mean Squared Error (MSE) loss function during training. The training process was limited to a maximum of \(300\) iterations with a batch size of 100 data points. Early termination occurred if the MSE reached below \(10^{-4}\). The achieved MSE on the test data was \(3.57 10^{-4}\) (\(K=2\)) and \(1.04 10^{-4}\) (\(K=10\)). The numerical results are visualized in Fig. 3.

Similar to the previous step, we randomly sampled \(200\) data points within the domain \(^{2}\) to create training and test datasets for \(f(x,y)\). A nested PQC structure was designed. It combined \(12\) two-qubit PQCs with a depth of \(2\), allowing the approximation of a degree-4 polynomial through a combination of lower-degree ones. Additionally, Taylor coefficients were stored in a separate matrix of size \(K^{2} 12\). The number of trainable parameters varied from \(120\) (\(K=2\)) to \(1272\) (\(K=10\)), each initialized randomly from \([0,]\). The Adam optimizer with a learning rate of 0.01 was used to minimize the MSE loss during training. The training was limited to \(500\) iterations with a batch size of 100, with early termination for MSE below \(10^{-4}\). The achieved MSE on the test data was \(2.22 10^{-4}\) (\(K=2\)) and \(9.82 10^{-5}\) (\(K=10\)). Fig. 4 visualizes the results. As \(K\) increases, the PQC demonstrates improved approximation performance, aligning with the theoretical findings.

## 5 Discussion

To the best of our knowledge, our results establish the first explicit PQC constructions for approximating Lipschitz continuous and Holder smooth functions with quantitative approximation error bounds. These results open up the possibility of comparing the size of PQCs and the size of classical deep neural networks for accomplishing the same function approximation tasks and see if there

Figure 3: **Simulation results of localization. We use single-qubit PQCs to approximate the localization function \(D(x)\) for \(K=2\) and \(K=10\) respectively.**

is any quantum advantage in terms of the model size and the number of trainable parameters. Here, we mainly focus on the comparison with the results of approximation errors of classical machine learning models. In classical deep learning, the deep feed-forward neural network (FNN) equipped with the rectified linear unit (ReLU) activation function is one of the most commonly used models. The quantitative approximation error bounds of ReLU FNNs for approximating continuous functions have been recently established, including the nearly optimal approximation error bounds of ReLU FNNs for smooth functions . We briefly compare the approximation errors of PQCs and ReLU FNNs in terms of width, depth and the number of trainable parameters. Detailed comparisons can be found in Appendix E.

We consider multivariate smooth functions in \(C_{u}^{s}(^{d})\) (the unit ball of \(C^{s}(^{d})\)) with smooth index \(s\) as the target functions in our comparison. Note that smooth functions with smooth index \(s\) are exactly \((s+1)\)-Holder smooth functions by definition. For simplicity, we first show the case of \(s=2\). To achieve the same approximation error \(\) (say some constant), we need to set \(K_{Q}=(d^{2}/)\) for the constructed PQCs from Theorem 4 and set \(K_{C}=(2^{d/2}/)\) for the constructed near-optimal ReLU FNNs from Ref. . Substituting the choices of \(K\)'s in the sizes of PQCs and ReLU FNNs, we have

\[}{}=OK_{Q}^{d}}{2^{d+3}K_{C}^{d/2}} =O}{2^{d-d d}}.\] (11)

One can obtain a similar relation for the number of required parameters in PQCs and ReLU FNNs for approximating smooth functions and extend these results to any \(2 s<d\), which holds relevance in numerous real-world applications (e.g., the input dimension \(d\) is \(784\) for the MNIST dataset and is \(150\,528\) for the ImageNet dataset , and empirically \(s 10\)). Therefore, to achieve the same approximation error, the required quantum circuit size and number of parameters of PQCs is exponentially smaller than the required network size and number of parameters of ReLU FNNs proposed in Ref. .

Aiming to understand and continuously expand the range of problems that can be addressed using quantum machine learning, we have demonstrated the approximation capabilities of PQC models in supervised learning. We characterized the approximation error of PQCs in terms of the model size, delivering a deeper understanding of the expressive power of PQCs that is beyond the universal approximation properties. With these results, we can unlock the full potential of these models and drive advancements in quantum machine learning. Notably, by comparing our results with the near-optimal approximation error bound of classical ReLU neural networks, we demonstrate an improvement over the classical models on approximating high-dimensional functions that satisfy specific smoothness criteria, quantified by an improvement on the model size and the number of parameters.

Unlike many other investigations in the universal approximation properties of PQC models , our constructions of PQCs for approximating broad classes of continuous functions do not rely on any impractical assumptions. All the variables take the form of parameters within single-qubit rotation gates, avoiding any classical parameterized pre-processing or post-processing. Ultimately, our

Figure 4: **Simulation results for learning \(f(x,y)\). The left two panels are derived by interpolating and smoothing the output values of PQC on 100 test data points.**

research provides valuable insights into the theoretical underpinnings of PQCs in quantum machine learning and paves the way for leveraging its capabilities in machine learning for both classical and quantum applications.

In this work, we introduce a novel nested PQC structure, which significantly improves the approximation capabilities. Future work could focus on exploring more powerful PQC constructions based on our proposed idea and understanding the capabilities and limitations of PQCs in more practical tasks even with real-world data. Developing efficient training strategies for PQCs, such as accelerated methods that achieve faster convergence rates, will also be interesting.