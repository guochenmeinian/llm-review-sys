# Color Equivariant Convolutional Networks

Attila Lengyel  Ombreta Strafforello  Robert-Jan Bruintjes

Alexander Gielisse  Jan van Gemert

Computer Vision Lab

Delft University of Technology

Delft, The Netherlands

###### Abstract

Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs.

## 1 Introduction

Color is a powerful cue for visual object recognition. Trichromatic color vision in primates may have developed to aid the detection of ripe fruits against a background of green foliage [38; 45]. The benefit of color vision here is two-fold: not only does color information improve foreground-background segmentation by rendering foreground objects more salient, color also allows diagnostics, e.g. identifying the type (orange) and ripeness (green) where color is an intrinsic property facilitating recognition , as illustrated in Fig. 1a. Convolutional neural networks (CNNs) too exploit color information by learning color selective features that respond differently based on the presence or absence of a particular color in the input .

Unwanted color variations, however, can be introduced by accidental scene recording conditions such as illumination changes [29; 48], or by low color-diagnostic objects occurring in a variety of colors, making color no longer a discriminative feature but rather an undesired source of variation in the data. Given a sufficiently large training set that encompasses all possible color variations, a CNN learns to become robust by learning color invariant and equivariant features from the available data [36; 37]. However, due to the long tail of the real world it is almost impossible to collect balanced training data for all scenarios. This naturally leads to color distribution shifts between training and test time, and an imbalance in the training data where less frequently occurring colors are underrepresented. As CNNs often fail to generalize to out-of-distribution test samples, this can have significant impact on many real-world applications, e.g. a model trained mostly on red cars may struggle to recognize the exact same car in blue.

_Color invariance_ addresses this issue through features that are by design invariant to color changes and therefore generalize better under appearance variations [14; 17]. However, color invariance comes at the loss of discriminative power as valuable color information is removed from the model'sinternal feature representation . We therefore propose to equip models with the less restrictive _color equivariance_ property, where features are explicitly shared across different colors through a hue transformation on the learned filters. This allows the model to generalize across different colors, while at the same time also retaining important color information in the feature representation.

An RGB pixel can be decomposed into an orthogonal representation by the well-known hue-saturation-value (HSV) model, where hue represents the chromaticity of a color. In this work we extend the notion of equivariance from geometric to photometric transformations by hard-wiring parameter sharing over hue-shifts in a neural network. More specifically, we build upon the seminal work of Group Equivariant Convolutions  (GConvs), which implements equivariance to translations, flips and rotations of multiples of 90 degrees, and formulates equivariance using the mathematical framework of symmetry groups. We introduce Color Equivariant Convolutions (CEConvs) as a novel deep learning building block, which implements equivariance to the \(H_{n}\) symmetry group of discrete hue rotations. CEConvs share parameters across hue-transformed filters in the input layer and store color information in hue-equivariant feature maps.

CEConv feature maps contain an additional dimension compared to regular CNNs, and as a result, require larger filters and thus more parameters for the same number of channels. To evaluate equivariant architectures, it is common practice to reduce the width of the network to match the parameter count of the baseline model. However, this approach introduces a trade-off between equivariance and model capacity, where particularly in deeper layers the quadratic increase in parameter count of CEConv layers makes equivariance computationally expensive. We therefore investigate hybrid architectures, where early color invariance is introduced by pooling over the color dimension of the feature maps. Note that early color invariance is maintained throughout the rest of the network, despite the use of regular convolutional layers after the pooling operation. Limiting color equivariant filters to the early layers is in line with the findings that early layers tend to benefit the most from equivariance  and learn more color selective filters [37; 42].

We rigorously validate the properties of CEConvs empirically through precisely controlled synthetic experiments, and evaluate the performance of color invariant and equivariant ResNets on various more realistic classification benchmarks. Moreover, we investigate the combined effects of color equivariance and color augmentations. Our experiments show that CEConvs perform on par or better

Figure 1: Color plays a significant role in object recognition. (a) The absence of color makes flowers less distinct from their background and thus harder to classify. The characteristic purple-blue color of the Monkshood (Class A) enables a clear distinction from the Snapdragon (Class B) . On the other hand, relying too much on colors might negatively impact recognition to color variations within the same flower class. (b) Image classification performance on the Flower-102 dataset  under a gradual variation of the image hue. Test-time hue shifts degrade the performance of CNNs (ResNet-18) drastically. Grayscale images and color augmentations result in invariance to hue variations, but fail to capture all the characteristic color features of flowers. Our color equivariant network (CE-ResNet-18-1) enables feature sharing across the color spectrum, which helps generalise to underrepresented colors in the dataset, while preserving discriminative color information, improving classification for unbalanced color variations.

than regular convolutions, while at the same time significantly improving the robustness to test-time color shifts, and is complementary to color augmentations.

The main contributions of this paper can be summarized as follows:

* We show that convolutional neural networks benefit from using color information, and at the same time are not robust to color-based domain shifts.
* We introduce Color Equivariant Convolutions (CEConvs), a novel deep learning building block that allows feature sharing between colors and can be readily integrated into existing architectures such as ResNets.
* We demonstrate that CEConvs improve robustness to train-test color shifts in the input.

All code and experiments are made publicly available on https://github.com/Attila94/CEConv.

## 2 Related work

Equivariant architecturesTranslation equivariance is a key property of convolutional neural networks (CNNs) [23; 28]: shifting the input to a convolution layer results in an equally shifted output feature map. This allows CNNs to share filter parameters over spatial locations, which improves both parameter and data efficiency as the model can generalize to new locations not covered by the training set. A variety of methods have extended equivariance in CNNs to other geometric transformations , including the seminal Group Equivariant Convolutions  for rotations and flips, and other works concerning rotations [2; 30; 52], scaling [50; 53] and arbitrary Lie groups . Yet to date, equivariance to photometric transformations has remained largely unexplored. Offset equivariant networks  constrain the trainable parameters such that an additive bias to the RGB input channels results in an equal bias in the output logits. By applying a log transformation to the input the network becomes equivariant to global illumination changes according to the Von Kries model . In this work we explore an alternative approach to photometric equivariance inspired by the seminal Group Equivariant Convolution  framework.

Color in CNNsRecent research has investigated the internal representation of color in Convolutional Neural Networks (CNNs), challenging the traditional view of CNNs as black boxes. For example, [41; 42] introduces the Neuron Feature visualization technique and characterizes neurons in trained CNNs based on their color selectivity, assessing whether a neuron activates in response to the presence of color in the input. The findings indicate that networks learn highly color-selective neurons across all layers, emphasizing the significance of color as a crucial visual cue. Additionally,  classifies neurons based on their class selectivity and observes that early layers contain more class-agnostic neurons, while later layers exhibited high class selectivity. A similar study has been performed in , further supporting these findings. [36; 37] investigate learned symmetries in an InceptionV1 model trained on ImageNet  and discover filters that demonstrated equivariance to rotations, scale, hue shifts, and combinations thereof. These results motivate color equivariance as a prior for CNNs, especially in the first layers. Moreover, in this study, we will employ the metrics introduced by  to provide an explanation for several of our own findings.

Color priors in deep learningColor is an important visual discriminator [15; 19; 51]. In classical computer vision, color invariants are used to extract features from an RGB image that are more consistent under illumination changes [14; 17; 18]. Recent studies have explored using color invariants as a preprocessing step to deep neural networks [1; 33] or incorporating them directly into the architecture itself , leading to improved robustness against time-of-day domain shifts and other illumination-based variations in the input. Capsule networks [22; 47], which use groups of neurons to represent object properties such as pose and appearance, have shown encouraging results in image colorization tasks . Quaternion networks [16; 54] represent RGB color values using quaternion notation, and employ quaternion convolutional layers resulting in moderate improvements in image classification and inpainting tasks. Building upon these advancements, we contribute to the ongoing research on integrating color priors within deep neural architectures.

Color equivariant convolutions

### Group Equivariant Convolutions

A CNN layer \(\) is equivariant to a symmetry group \(G\) if for all transformations \(g G\) on the input \(x\) the resulting feature mapping \((x)\) transforms similarly, i.e., first doing a transformation and then the mapping is similar to first doing the mapping and then the transformation. Formally, equivariance is defined as

\[(T_{g}x)=T^{}_{g}(x), g G,\] (1)

where \(T_{g}\) and \(T^{}_{g}\) are the transformation operators of group action \(g\) on the input and feature space, respectively. Note that \(T_{g}\) and \(T^{}_{g}\) can be identical, as is the case for translation equivariance where shifting the input results in an equally shifted feature map, but do not necessarily need to be. A special case of equivariance is invariance, where \(T^{}_{g}\) is the identity mapping and the input transformation leaves the feature map unchanged:

\[(T_{g}x)=(x), g G.\] (2)

We use the definition from  to denote the \(i\)-th output channel of a standard convolutional layer \(l\) in terms of the correlation operation \(()\) between a set of feature maps \(f\) and \(C^{l+1}\) filters \(\):

\[[f^{i}](x)=_{y^{2}}_{c=1}^{C^{l}}f_{c}(y)^{ i}_{c}(y-x).\] (3)

Here \(f:^{2}^{C^{l}}\) and \(^{i}:^{2}^{C^{l}}\) are functions that map pixel locations \(x\) to a \(C^{l}\)-dimensional vector. This definition can be extended to groups by replacing the translation \(x\) by a group action \(g\):

\[[f^{i}](g)=_{y^{2}}_{c}^{C^{l}}f_{c}(y)^{i}_ {c}(g^{-1}y)\] (4)

As the resulting feature map \(f^{i}\) is a function on G rather than \(^{2}\), the inputs and filters of all hidden layers should also be defined on \(G\):

\[[f^{i}](g)=_{h G}_{c}^{C^{l}}f_{c}(h)^{i}_{c}(g^{-1}h)\] (5)

Invariance to a subgroup can be achieved by applying a pooling operation over the corresponding cosets. For a more detailed introduction to group equivariant convolutions, please refer to [4; 7].

### Color Equivariance

We define color equivariance as equivariance to hue shifts. The HSV color space encodes hue by an angular scalar value, and a hue shift is performed as a simple additive offset followed by a modulo operator. When projecting the HSV representation into three-dimensional RGB space, the same hue shift becomes a rotation along the \(\) diagonal vector.

We formulate hue equivariance in the framework of group theory by defining the group \(H_{n}\) of multiples of \(360/n\)-degree rotations about the \(\) diagonal vector in \(^{3}\) space. \(H_{n}\) is a subgroup of the \(SO(3)\) group of all rotations about the origin of three-dimensional Euclidean space. We can parameterize \(H\) in terms of integers \(k,n\) as

\[H_{n}(k)=()+a&a-b&a+b\\ a+b&()+a&a-b\\ a-b&a+b&()+a\] (6)

with \(n\) the total number of discrete rotations in the group, \(k\) the rotation, \(a=-()\) and \(b=}()\). The group operation is matrix multiplication which acts on the continuous \(^{3}\) space of RGB pixel values. The derivation of \(H_{n}\) is provided in Appendix A.

Color Equivariant Convolution (CEConv)Let us define the group \(G=^{2} H_{n}\), which is a direct product of the \(^{2}\) group of discrete 2D translations and the \(H_{n}\) group of discrete hue shifts. We can then define the Color Equivariant Convolution (CEConv) in the input layer as:

\[[f^{i}](x,k)=_{y^{2}}_{c=1}^{C^{l}}f_{c}(y) H _{n}(k)^{i}_{c}(y-x).\] (7)

We furthermore introduce the operator \(_{g}=_{(t,m)}\) including translation \(t\) and hue shift \(m\) acting on input \(f\) defined on the plane \(^{2}\):

\[[_{g}f](x)=[_{(t,m)}f](x)=H_{n}(m)f(x-t)\] (8)

Since \(H_{n}\) is an orthogonal matrix, the dot product between a hue shifted input \(H_{n}f\) and a filter \(\) is equal to the dot product between the original input \(f\) and the inverse hue shifted filter \(H_{n}^{-1}\):

\[H_{n}f=(H_{n}f)^{T}=f^{T}H_{n}^{T}=f H_{n}^{T}=f  H_{n}^{-1}.\] (9)

Then the equivariance of the CEConv layer can be derived as follows (using \(C^{l}=1\) for brevity):

\[[[_{(t,m)}f]^{i}](x,k)& =_{y^{2}}H_{n}(m)f(y-t) H_{n}(k)^{i}(y-x) \\ &=_{y^{2}}f(y) H_{n}(m)^{-1}H_{n}(k)^{ i}(y-(x-t))\\ &=_{y^{2}}f(y) H_{n}(k-m)^{i}(y-(x-t)) \\ &=[f^{i}](x-t,k-m)\\ &=[}_{(t,m)}[f^{i}]](x,k)\] (10)

Since input \(f\) and feature map \([f]\) are functions on \(^{2}\) and \(G\), respectively, \(_{(t,k)}\) and \(}_{(t,k)}\) represent two equivalent operators acting on their respective groups. For all subsequent hidden layers the input \(f\) and filters \(^{i}\) are functions on \(G\) parameterized by \(x,k\), and the hidden layer for CEConv is defined as:

\[[f^{i}](x,k)=_{y^{2}}_{r=1}^{n}_{c=1}^{C^{l} }f_{c}(y,r)^{i}_{c}(y-x,(r-k)\%n),\] (11)

where \(n\) is the number of discrete rotations in the group and \(\%\) is the modulo operator. In practice, applying a rotation to RGB pixels will cause some pixel values to fall outside of the RGB cube, which will then have to be reprojected within the cube. Due to this discrepancy, Eq. (9) only holds approximately, though in practice this has only limited consequences, as we empirical show in Appendix D.

### Implementation

Tensor operationsWe implement CEConv similarly to GConv . GConv represents the pose associated with the added spatial rotation group by extending the feature map tensor \(X\) with an extra dimension \(G^{l}\) to size \([C^{l},G^{l},H,W]\), denoting the number of channels, transformations that leave the origin invariant, and height and width of the feature map at layer \(l\), respectively (batch dimension omitted). Similarly, a GConv filter \(\) with spatial extent \(k\) is of size \([C^{l+1},G^{l+1},C^{l},G^{l},k,k]\). The GConv is then defined in terms of tensor multiplication operations as:

\[X^{l+1}_{c^{},g^{},:,:}=_{c}^{C^{l}}_{g}^{G^{l}}^ {l}_{c^{},g^{},c,g,:,:} X^{l}_{c,g,:,:},\] (12)

where \((:)\) denotes tensor slices. Note that in the implementation, a GConv filter \(F\) only contains \([C^{l+1},C^{l},G^{l},k,k]\) unique parameters - the extra \(G^{l+1}\) dimension is made up of transformed copies of \(F\).

As the RGB input to the network is defined on \(^{2}\), we have \(G^{1}=1\) and \(\) has size \([C^{l+1},G^{l+1},3,1,k,k]\). The transformed copies in \(G^{l+1}\) are computed by applying the rotation matrix from Eq. (6):

\[^{1}_{c^{},g^{},:,1,u,v}=H_{n}(g^{})F^{1}_{c^{ },:,1,u,v}.\] (13)

In the hidden layers \(\) contains cyclically permuted copies of \(F\):

\[^{l}_{c^{},g^{},c,g,u,v}=F^{l}_{c^{},c,(g+g^{ })\%n,u,v}.\] (14)

Furthermore, to explicitly share the channel-wise spatial kernel over \(G^{l}\), filter \(F\) is decomposed into a spatial component \(S\) and a pointwise component \(P\) as follows:

\[F^{l}_{c^{},c,g,u,v}=S_{c^{},c,1,u,v} P_{c^{},g^{ },c,g,1,1}\] (15)

\(F\) is precomputed in each forward step prior to the convolution operation in Eq. (12).

Input normalizationis performed using a single value for the mean and standard deviations rather than per channel, as is commonly done for standard CNNs. Channel-wise means and standard deviations break the equivariance property of CECNN as a hue shift could no longer be defined as a rotation around the \(\) diagonal. Experiments have shown that using a single value for all channels instead of channel-wise normalization has no effect on the performance.

Compute efficiencyCEConvs create a factor \(|H_{n}|\) more feature maps in each layer. Due to the decomposition in Eq. (15), the number of multiply-accumulate (MAC) operations increase by only a factor \(|^{2}}{k^{2}}+|H_{n}|\), and the number of parameters by a factor \(|}{k^{2}}+1\). See Appendix C.3 for an overview of parameter counts and MAC operations.

## 4 Experiments

### When is color equivariance useful?

Color equivariant convolutions share shape information across different colors while preserving color information in the group dimension. To demonstrate when this property is useful we perform two controlled toy experiments on variations of the MNIST  dataset. We use the Z2CNN architecture from , and create a color equivariant version of the network called CECNN by replacing all convolutional layers by CEConvs with three rotations of 120\({}^{}\). The number of channels in CECNN is scaled such as to keep the number of parameters approximately equal to the Z2CNN. We also create a color invariant CECNN by applying coset max-pooling after the final CEConv layer, and a color invariant Z2CNN by converting the inputs to grayscale. All experiments are performed using the Adam  optimizer with a learning rate of 0.001 and the OneCycle learning rate scheduler. No data augmentations are used. We report the average performance over ten runs with different random initializations.

Color imbalanceis simulated by _long-tailed ColorMNIST_, a 30-class classification problem where digits occur in three colors on a gray background, and need to be classified by both number (0-9) and color (red, green, blue). The number of samples per class is drawn from a power law distribution resulting in a long-tailed class imbalance. Sharing shape information across colors is beneficial as a certain digit may occur more frequently in one color than in another. The train set contains a total of 1,514 training samples and the test set is uniformly distributed with 250 samples per class. The training set is visualized in Appendix B.1. We train all four architectures on the dataset for 1000 epochs using the standard cross-entropy loss. The train set distribution and per-class test accuracies for all models are shown in Fig. 2a. With an average accuracy of \(91.35 0.40\%\) the CECNN performs significantly better than the CNN with \(71.59 0.61\%\). The performance increase is most significant for the classes with a low sample size, indicating that CEConvs are indeed more efficient in sharing shape information across different colors. The color invariant Z2CNN and CECNN networks, with an average accuracy of \(24.19 0.53\%\) and \(29.43 0.46\%\), respectively, are unable to discriminate between colors. CECNN with coset pooling is better able to discriminate between foreground and background and therefore performs slightly better. We repeated the experiment with a weighted loss and observed no significantly different results. We have also experimented with adding color jitter augmentations, which makes solving the classification problem prohibitive, as color is required. See Appendix B.2 for both detailed results on both experiments.

Color variationsare simulated by _biased ColorMNIST_, a 10-class classification problem where each class \(c\) has its own characteristic hue \(_{c}\) defined in degrees, distributed uniformly on the hue circle. The exact color of each digit \(x\) is sampled according to \(_{x}(_{c},)\). We generate multiple datasets by varying \(\) between 0 and \(10^{6}\), where \(=0\) results in a completely deterministic color for each class and \(=10^{6}\) in an approximately uniform distribution for \(_{x}\). For small \(\), color is thus highly informative of the class, whereas for large \(\) the classification needs to be performed based on shape. The dataset is visualized in Appendix B.1. We train all models on the train set of 1.000 samples for 1500 epochs and evaluate on the test set of 10.000 samples. The test accuracies for different \(\) are shown in Fig. 1(b). CECNN outperforms Z2CNN across all standard deviations, indicating CEConvs allow for a more efficient internal color representation. The color invariant CECNN network outperforms the equivariant CECNN model from \( 48\). Above this value color is no longer informative for the classification task and merely acts as noise unnecessarily consuming model capacity, which is effectively filtered out by the color invariant networks. The results of the grayscale Z2CNN are omitted as they are significantly worse, ranging between \(89.89\%\) (\(=0\)) and \(79.94\) (\(=10^{6}\)). Interestingly, CECNN with coset pooling outperforms the grayscale Z2CNN. This is due to the fact that a CECNN with coset pooling is still able to distinguish between small color changes and therefore can partially exploit color information. Networks trained with color jitter are unable to exploit color information for low \(\); see Appendix B.2 for detailed results.

### Image classification

SetupWe evaluate our method for robustness to color variations on several natural image classification datasets, including CIFAR-10 and CIFAR-100 , Flowers-102 , STL-10 , Oxford-IIIT Pet , Caltech-101 , Stanford Cars  and ImageNet . We train a baseline and color equivariant (CE-)ResNet  with 3 rotations and evaluate on a range of test sets where we gradually apply a hue shift between -180\({}^{}\) and 180\({}^{}\). For high-resolution datasets (all except CIFAR) we train a ResNet-18 architecture and use default ImageNet data augmentations: we scale to 256 pixels, random crop to 224 pixels and apply random horizontal flips. For the CIFAR datasets we use the ResNet-44 architecture and augmentations from , including random horizontal flips and translations of up to 4 pixels. We train models both with and without color jitter augmentation to separately evaluate the effect of equivariance and augmentation. The CE-ResNets are downscaled in width to match the parameter count of the baseline ResNets. We have also included AugMix  and CIConv  as baselines for comparison. Training is performed for 200 epochs using the Adam  optimizer with a learning rate of 0.001 and the OneCycle learning rate scheduler. All our experiments use PyTorch and run on a single NVIDIA A40 GPU.

Hybrid networksIn our toy experiments we enforce color equivariance throughout the network. For real world datasets however, we anticipate that the later layers of a CNN may not benefit from enforcing parameter sharing between colors, if the classes of the dataset are determined by color

Figure 2: Color equivariant convolutions efficiently share shape information across different colors. CECNN outperforms a vanilla network in both a long-tailed class imbalance setting (a), where MNIST digits are to be classified based on both shape and color, and a color biased setting (b), where the color of each class \(c\) is sampled according to \(_{d}(_{c},)\).

specific features. We therefore evaluate hybrid versions of our color equivariance networks, denoted by an integer suffix for the number of ResNet stages, out of a possible four, that use CEConvs.

ResultsWe report both the performance on the original test set, as well as the average accuracy over all hue shifts in Table 1. For brevity we only show the fully equivariant and hybrid-2 networks, a complete overview of the performances of all hybrid network configurations and error standard deviations can be found in Appendix C.1. Between the full color equivariant and hybrid versions of our CE-ResNets, at least one variant outperforms vanilla ResNets on most datasets on the original test set. On most datasets the one- or two-stage hybrid versions are the optimal CE-ResNets, providing a good trade-off between color equivariance and leaving the network free to learn color specific features in later layers. CE-ResNets are also significantly more robust to test-time hue shifts, especially when trained without color jitter augmentation. Training the CE-ResNets with color jitter further improves robustness, indicating that train-time augmentations complement the already hard-coded inductive biases in the network. We show the detailed performance on Flowers-102 for all test-time hue shifts in Fig. 1b. The accuracy of the vanilla CNN quickly drops as a hue shift is applied, whereas the CE-CNN performance peaks at -120\({}^{}\), 0\({}^{}\)and 120\({}^{}\). Applying train-time color jitter improves the CNN's robustness to the level of a CNN with grayscale inputs. The CE-CNN with color jitter outperforms all models for all hue shifts. Plots for other datasets are provided in Appendix C.2.

Color selectivityTo explore what affects the success of color equivariance, we investigate the _color selectivity_ of a subset of the studied datasets. We use the color selectivity measure from  and average across all neurons in the baseline model trained on each dataset. Fig. 3 shows that color selective datasets benefit from using color equivariance up to late stages, whereas less color selective datasets do not.

Feature representations of color equivariant CNNsWe use the Neuron Feature  (NF) visualization method to investigate the internal feature representation of the CE-ResNet. NF computes a weighted average of the \(N\) highest activation input patches for each filter at a certain layer, as such representing the input patch that a specific neuron fires on. Fig. 4 shows the NF (\(N=50\)) and top-3 input patches for filters at the final layers of stages 1-4 of a CE-ResNet18 trained on Flowers-102.

   _Original test set_ & **Caltech** & **C-10** & **C-100** & **Flowers** & **Ox-Pet** & **Cars** & **STL10** & **ImageNet** \\  Baseline & 71.61 & 93.69 & 71.28 & 66.79 & 69.87 & 76.54 & 83.80 & 69.71 \\ CIConv-W & **72.85** & 75.26 & 38.81 & **68.71** & 61.53 & **79.52** & 80.71 & 65.81 \\ CEConv & 70.16 & 93.71 & 71.37 & 68.18 & 70.24 & 76.22 & 84.24 & 66.85 \\ CEConv-2 & 71.50 & **93.94** & **72.20** & 68.38 & **70.34** & 77.06 & **84.50** & **70.02** \\  Baseline + jitter & 73.93 & 93.03 & 69.23 & 68.75 & 72.71 & 80.59 & 83.91 & 69.37 \\ CIConv-W + jitter & **74.38** & 77.49 & 42.27 & **75.05** & 64.23 & **81.56** & 81.88 & 65.95 \\ CEConv + jitter & 73.58 & 93.51 & 71.12 & 74.17 & **73.29** & 79.79 & 84.16 & 65.57 \\ CEConv-2 + jitter & 72.61 & **93.86** & **71.35** & 71.72 & 72.80 & 80.32 & **84.46** & **69.42** \\  Baseline + AugMix & **71.92** & 94.13 & **72.64** & 75.49 & **76.02** & **82.32** & 84.99 & - \\ CEConv + AugMix & 70.74 & **94.22** & 72.48 & **78.10** & 75.90 & 80.81 & **85.46** & - \\  _Hue-shifted test set_ & & & & & & & & \\  Baseline & 51.14 & 85.26 & 47.01 & 13.41 & 37.56 & 55.59 & 67.60 & 54.72 \\ CIConv-W & **71.92** & 74.88 & 37.09 & **59.03** & **60.54** & **78.71** & **79.92** & **64.62** \\ CEConv & 62.17 & 90.90 & 59.04 & 33.33 & 54.02 & 67.16 & 78.25 & 56.90 \\ CEConv-2 & 64.51 & **91.43** & **62.11** & 33.32 & 51.14 & 68.17 & 77.80 & 62.26 \\  Baseline + jitter & 73.61 & 92.91 & 69.12 & 68.44 & 72.30 & 80.65 & 83.71 & 67.10 \\ CIConv-W + jitter & **74.40** & 77.28 & 42.30 & **75.66** & 63.93 & **81.44** & 81.54 & 65.03 \\ CEConv + jitter & 73.57 & 93.39 & 71.06 & 73.86 & **72.94** & 79.79 & 84.02 & 64.52 \\ CEConv-2 + jitter & 73.03 & **93.80** & **71.33** & 71.44 & 72.58 & 80.28 & **84.31** & **68.74** \\  Baseline + AugMix & 51.82 & 88.03 & 51.39 & 15.99 & 48.04 & 68.69 & 72.19 & - \\ CEConv + AugMix & **62.29** & **91.68** & **60.75** & **41.43** & **62.27** & **73.59** & **80.17** & - \\   

Table 1: Classification accuracy in % of vanilla vs. color equivariant (CE-)ResNets, evaluated both on the original and hue-shifted test sets. Color equivariant CNNs perform on par with vanilla CNNs on the original test sets, but are significantly more robust to test-time hue shifts.

Different rows represent different rotations of the same filter. As expected, each row of a NF activates on the same shape in a different color, demonstrating the color sharing capabilities of CEConvs. More detailed NF visualization are provided in Appendix C.4.

Ablation studiesWe perform ablations to investigate the effect of the number of rotations, the use of group coset pooling, and the strength of train-time color jitter augmentations. In short, we find that a) increasing the number of hue rotations increases robustness to test-time hue shifts at the cost of a slight reduction in network capacity, b) removing group coset pooling breaks hue invariance, and c) hue equivariant networks require lower intensity color jitter augmentations to achieve the same test-time hue shift robustness and accuracy. The full results can be found in Appendix D.

## 5 Conclusion

In this work, we propose Color Equivariant Convolutions (CEConvs) which enable feature sharing across colors in the data, while retaining discriminative power. Our toy experiments demonstrate benefits for datasets where the color distribution is long-tailed or biased. Our proposed fully equivariant CECNNs improve performance on datasets where features are color selective, while hybrid versions that selectively apply CEConvs only in early stages of a CNN benefit various classification tasks.

LimitationsCEConvs are computationally more expensive than regular convolutions. For fair comparison, we have equalized the parameter cost of all models compared, at the cost of reducing the number of channels of CECNNs. In cases where color equivariance is not a useful prior, the reduced capacity hurts model performance, as reflected in our experimental results.

Pixel values near the borders of the RGB cube can fall outside the cube after rotation, and subsequently need to be reprojected. Due to this clipping effect the hue equivariance in Eq. (9) only holds approximately. As demonstrated empirically, this has only limited practical consequences, yet future work should investigate how this shortcoming could be mitigated.

Figure 4: Neuron Feature  (NF) visualization with top-3 patches at different stages of a CECResNet18 trained on Flowers-102. Rows represent different rotations of the same filter. As expected, each row of a NF activates on the same shape in a different color.

Figure 3: Color selective datasets benefit from using color equivariance up to late stages, whereas less color selective datasets do not. We compute average color selectivity  of all neurons in the baseline CNN trained on each dataset, and plot the accuracy improvement of using color equivariance in hybrid and full models, coloring each graphed dataset for color selectivity.

Local vs. global equivarianceThe proposed CEConv implements local hue equivariance, i.e. it allows to model local color changes in different regions of an image separately. In contrast, global equivariance, e.g. by performing hue shifts on the full input image, then processing all inputs with the same CNN and combining representations at the final layer to get a hue-equivariant representation, encodes global equivariance to the entire image. While we have also considered such setup, initial experiments did not yield promising results. The theoretical benefit of local over global hue equivariance is that multiple objects in one image can be recognized equivariantly in any combination of hues - empirically this indeed proves to be a useful property.

Future workThe group of hue shifts is but one of many possible transformations groups on images. CNNs naturally learn features that vary in both photometric and geometric transformations [5; 37]. Future work could combine hue shifts with geometric transformations such as roto-translation  and scaling . Also, other photometric properties could be explored in an equivariance setting, such as saturation and brightness.

Our proposed method rotates the hue of the inputs by a predetermined angle as encoded in a rotation matrix. Making this rotation matrix learnable could yield an inexact but more flexible type of color equivariance, in line with recent works on learnable equivariance [34; 46]. An additional line of interesting future work is to incorporate more fine-grained equivariance to continuous hue shifts, which is currently intractable within the GConv-inspired framework as the number multiply-accumulate operations grow quadratically with the number of hue rotations.

Broader impactImproving performance on tasks where color is a discriminative feature could affect humans that are the target of discrimination based on the color of their skin. CEConvs ideally benefit datasets with long-tailed color distributions by increasing robustness to color changes, in theory reducing a CNN's reliance on skin tone as a discriminating factor. However, careful and rigorous evaluation is needed before such properties can be attributed to CECNNs with certainty.