# Progressive Ensemble Distillation:

Building Ensembles for Efficient Inference

 Don Kurian Dennis

Carnegie Mellon University

&Abhishek Shetty

University of California, Berkeley

&Anish Sevekari

Carnegie Mellon University

&Kazuhito Koishida

Microsoft

&Virginia Smith

Carnegie Mellon University

&Virginia Smith

Carnegie Mellon University

###### Abstract

We study the problem of _progressive ensemble distillation_: Given a large, pretrained teacher model \(g\), we seek to decompose the model into smaller, low-inference cost student models \(f_{i}\), such that progressively evaluating additional models in this ensemble leads to improved predictions. The resulting ensemble allows for flexibly tuning accuracy vs. inference cost at runtime, which is useful for a number of applications in on-device inference. The method we propose, b-distil, relies on an algorithmic procedure that uses function composition over intermediate activations to construct expressive ensembles with similar performance as \(g\), but with smaller student models. We demonstrate the effectiveness of b-distil by decomposing pretrained models across standard image, speech, and sensor datasets. We also provide theoretical guarantees in terms of convergence and generalization.

+
Footnote †: Corresponding author: Don Dennis <dondennis@cmu.edu>.

## 1 Introduction

Knowledge distillation aims to transfer the knowledge of a large model into a smaller one [5; 23]. While this technique is commonly used for model compression, one downside is that the procedure is fairly rigid--resulting in a single compressed model of a fixed size. In this work, we instead consider the problem of _progressive ensemble distillation_: approximating a large model via an ensemble of smaller, low-latency models such that successively evaluating additional models in this ensemble leads to improved predictions. The resulting decomposition is useful for many applications in on-device and low-latency inference. For example, components of the ensemble can be selectively combined to flexibly meet accuracy/latency constraints [31; 44], can enable efficient parallel inference execution schemes, and can facilitate _early-exit_[4; 11] or _anytime inference_[36; 28] applications, which are scenarios where inference may be interrupted due to variable resource availability.

More specifically, our work seeks to distill a large pretrained model, \(g\), onto an ensemble of'smaller' models, such that evaluating the first model produces a coarse estimate of the prediction (e.g., covering common cases), and evaluating additional models improves on this estimate (see Figure 1). There are major advantages to such an ensemble for on-device efficient inference.

Figure 1: In progressive ensemble distillation, a large teacher model is distilled into an ensemble of low inference cost models. The more student models we evaluate, the closer the ensemble’s decision boundary is to that of the teacher model. Models in the ensemble are allowed to depend on previously computed features to reduce overhead and inference cost.

Concretely, (i) inference cost vs. accuracy trade-offs can be controlled on-demand at execution time, (ii) the ensemble can either be executed in parallel or in sequence, or possibly a mix of both, and (iii) we can improve upon coarse initial predictions without re-evaluation at runtime.

While traditional distillation methods are effective when transferring information to a single model of similar capacity, it has been shown that performance can degrade significantly when reducing the capacity of the student model [34; 19]. Moreover, distillation of a deep network onto a weighted sum of shallow networks rarely performs better than distillation onto a single model [9; 1].

Our insight in this work is that by composing and reusing activations and by explicitly incentivizing models to be weak learners during distillation, we can successfully find weak learners even when the capacity gap is relatively large. As long as these composition functions are resource-efficient, we are able to increase our hypothesis class capacity at roughly the same inference cost as a single model. Moreover, we show that our procedure retains the theoretical guarantees of classical boosting methods . Concretely, we make the following contributions:

* We formulate progressive ensemble distillation as a two player zero-sum game, derive a weak learning condition for distillation, and present our algorithm, b-distill, to approximately solve this game. To make the search for weak learners in low parameter count models feasible, we solve a log-barrier based relaxation of our weak learning condition. By allowing models to reuse computation from select intermediate layers of previously evaluated models of the ensemble, we can increase the model's capacity without significantly increasing inference cost.
* We empirically evaluate our algorithm on synthetic and real-world classification tasks from computer vision, speech, and sensor processing with models suitable for the respective domains. We show that our ensemble behaves like a decomposition, allowing a run-time trade-off between accuracy and computation, while retaining competitive performance with the teacher model.
* We provide theoretical guarantees for our algorithm in terms of in-sample convergence and generalization performance. Our framework is not architecture or task specific and can recover existing ensemble models used in efficient inference; we believe our work thus puts forth a general lens to view previous work and also to develop new, principled approaches for efficient inference.

## 2 Background and Related Work

Efficient Inference.Machine learning inference is often resource-constrained when deployed in practical applications due to memory, energy, cost, or latency constraints. This has spurted the development of numerous techniques for efficient inference. Pruning and approximations of pre-trained parameter tensors through low-rank, sparse and quantized representations [22; 3; 18; 20] have been effective is reducing resource requirements. There are also architecture and task specific techniques for efficient inference [12; 45; 13]. In contrast to compressing an already trained model, algorithms have also been developed to train compressed models by incorporating resource constraints as part of their training routines [2; 7]. More recently, algorithms have been developed to search and find smaller sub-models from a single pre-trained model [46; 6].

Knowledge distillation.Knowledge distillation aims to transfer the knowledge of a larger model (or model ensemble) to a smaller one [5; 23]. Despite its popularity, performing compression via distillation has several known pitfalls. Most notably, it is well-documented that distillation performs poorly when there is a _capacity gap_, i.e., the teacher is significantly larger than the student [34; 19; 9; 1]. When performing distillation onto a weighted combination of ensembles, it has been observed that adding additional models into the ensemble does not dramatically improve performance over that of a single distilled model . There is also a lack of theoretical work characterizing when and why distillation is effective for compression . Our work aims to address these pitfalls by developing a principled approach for progressively distilling a large model onto an ensemble of smaller, low-capacity ones. We defer readers to  for a recent survey on varied applications of and approaches for distillation at large.

Early exits and anytime inference.Many applications stand to benefit from the output of progressive ensemble distillation, which allows for flexibly tuning accuracy vs. inference cost and executing inference in parallel. Enabling trade-offs between accuracy and inference cost is particularly useful for applications that use early exit or anytime inference schemes. In on-device continuous (online) inference settings, _early exit_ models aim to evaluate common cases quickly in order to improve energy efficiency and prolong battery life [11; 4]. For instance, a battery powered device continuously listening for voice commands can use early exit methods to improve battery efficiency by quickly classifying non-command speech. Many early exit methods are also applicable to anytime inference [28; 36]. In _anytime inference_, the aim is to produce a prediction even when inference is interrupted, e.g., due to resource contention or a scheduler decision. Unlike early exit methods where the classifier chooses when to exit, anytime inference methods have no control over when they are interrupted. We explore the effectiveness of our method, b-distil, for such applications in Section 5.

Two-player games, online optimization and boosting.In this work we formulate progressive ensemble distillation as a two player zero-sum game. The importance of equilibrium of two player zero-sum games have been recognized since the foundational work of von Neumann and Morgenstern . Later applications by Schapire  and Freund , Freund and Schapire  identified close connections between boosting and two-player games. On the basis of this result, a number of practical boosting-based learning approaches such as AdaBoost , gradient boosting , and XGboost  have been developed. Boosting has only recently seen success in modern deep learning applications. In particular, Suggala et al.  propose a generalized boosting framework to _train_ boosting based ensembles of deep networks. Their key insight is that allowing function compositions in feature space can help boost deep neural networks. Although they focus on training and do not produce decompositions of pretrained models, we use a similar approach in our work to select intermediate layers connections between ensemble components (Section 3.3). A more general application of boosting that is similar to our setup is by Trevisan et al. . They prove that given a target bounded function \(g\) (e.g., the teacher model) and class of candidate approximating functions \(f\), one can iteratively approximate \(g\) arbitrarily well with respect to \(\) using ideas from online learning and boosting. However, these results depend on the ability to find a function \(f_{t}\) in iteration \(t\) that leads to at least a small constant improvement in a round-dependent approximation loss. A key contribution of our work is showing that such functions can be found for the practical application of progressive ensemble distillation by carefully selecting candidate models.

## 3 Progressive Ensemble Distillation with B-Distil

As discussed, our goal in performing progressive ensemble distillation is to approximate a large model via an ensemble of smaller, low-latency models so that we can easily trade-off between accuracy and inference-time/latency at runtime. In this section we formalize the problem of progressive ensemble distillation as a two player game and discuss our proposed algorithm, b-distil.

### Problem Formulation

Consider repeated plays of a general two player zero-sum game with the pure strategy sets comprising of a hypothesis class \(\) and a probability distribution \(\). Given a loss function \(\), we let the loss (and reward) of the players be given by \(F(f,p)=_{x p}[(f,x)]\), and the minimax value of the game is:

\[_{p}_{f}F(f,p).\] (1)

In the context of distillation, given a training set \(\{x_{i}\}\), we can think of the role of the max player in Equation (1) as producing distributions over the training set and the role of the min player as producing a hypothesis that minimizes the loss on this distribution. In this setting, note that \(=\{p^{N L}:p_{i,j} 0 j _{i}p_{i,j}=1\}\) is the product of simplices in \(N L\) dimensions, and

\[(_{f}F(f,p))_{j}=_{i}p_{i,j}(f(x_{i})-g( x_{i}))_{j}.\] (2)

Our goal is to produce an ensemble of predictors from the set of hypothesis classes \(\{_{m}\}\) to approximate \(g\) 'well'. We now appeal to tools from the framework of stochastic minimax optimization to approximately attain the value in Equation (1) (see Appendix A for a more involved discussion). As is common in this setup, we assume our algorithm is provided access _weak gradient_ vector \(h\) such that, when queried at distribution \(p\) and for \(>0\),

\[ h,_{f}F(f,p).\] (3)

We perform this construction iteratively by searching for weak learners or weak gradients in the sense of Equation (3) with respect to the target \(g\) in the class \(_{m}\). Conditioned on a successful search we can guarantee in-sample convergence to the minimax value in Equation (1) (Theorem 1) and bound the excess risk of the ensemble (Theorem 2). Although Equation (3) is a seemingly easier notion than the full optimization, in many problems of interest even this is challenging. In fact, in the multilabel setting that we focus on, one of the main algorithmic challenges is to construct an algorithm that can reliably find low cost weak gradients/learners (see Section 3.3).

[MISSING_PAGE_EMPTY:4]

Log-barrier regularizer.To find a weak learner, the find-wl method minimizes the sum of two loss terms using stochastic gradient descent. The first is standard binary/multi-class cross-entropy distillation loss , with temperature smoothing. The second term is defined in Equation (6):

\[-_{i,j}I_{ij}^{+}(1+)_{j}}{2B}) +(1-I_{ij}^{+})(1-)_{j}}{2B})\] (6)

Here \(I_{ij}^{+}:=I[K_{t}^{+}(i,j)>K_{t}^{-}(i,j)]\), \(B\) is an upper bound on the magnitude of the logits, and \(l(x_{i}):=f(x_{i})-g(x_{i})\). To see the intuition behind Equation (6), assume the following holds; \((i,j)\),

\[(K_{t}^{+}(i,j)-K_{t}^{-}(i,j))(f(x_{i})-g(x_{i}))_{j}>0.\] (7)

Summing over all \(x_{i}\), we can see that this is sufficient for \(f\) to be a weak learner with respect to \(g\). Equation (6) is a soft log-barrier version of the weak learning condition, that penalizes those \((i,j)\) for which Equation (7) does not hold. By tuning \(\) we can increase the relative importance of the regularization objective, encouraging \(f_{t}\) to be a weak learner potentially at the expense of classification performance.

Intermediate layer connections and profiling.As discussed in Section 2, distillation onto a linear combination of low capacity student models often offers no better performance than that of any single model in the ensemble trained independently. For boosting, empirically we see that once the first weak learner has been found in some class \(_{m}\) of low-capacity deep networks, it is difficult to find a weak learner for the reweighed objective from the same class \(_{m}\). To work around this we let our class of weak learners at round \(t\) include functions that depend on the output of intermediate layers of previous weak learners .

As a concrete example, consider a deep fully connected network with \(U\) layers, parameterized as \(f=W_{1:U}\). Here \(_{1:u}\) can be thought of as a feature transform on \(x\) using the first \(u\) layers into \(^{d_{u}}\) and \(W R^{d_{U} L}\) is a linear transform. With two layer fully connected base model class \(_{0}:=\{W^{(0)}_{1:2}^{(0)} W^{(0)}^{L d _{2}}\}\) (dropping subscript \(m\) for simplicity), we define:

\[_{r}=\{W^{(r)}_{1:2}^{(r)}(+_{1:2}^{(r-1)})\} _{r}^{}\{W^{(r)}_{2}^{(r)}(+_{1 }^{(r)}+_{1}^{(r-1)})\}\,,\]

with \((x):=x\). It can be seen that \(\{_{r}\}\) and \(\{_{r}^{}\}\) define a variant of residual connection based networks . It can be shown that classes of function \(\{_{r}\}\) (and \(\{_{r}^{}\}\)) increase in capacity with \(r\). Moreover, when evaluating sequentially the inference cost of a model from \(_{r}\) is roughly equal to that of \(\), since each subsequent evaluation _reuses_ stored activations from previous evaluations. For this reason the parameter count of each \(_{r}\) remains the same as that of the base class. Note that by picking the base class as dense networks at various scales and connections as dense connections our algorithm can recover MSDNets studied in . Similarly, by picking the base class as root nodes, and connections as binary connections, we recover an HNE from .

We informally refer to the process of constructing \(\{_{r}\}\) given a choice of base class \(_{0}\), the parameter \(R\) and the connection type as _expanding_\(_{0}\). Note that while intermediate connections help with capacity, they often reduce parallelizability as models become mutually dependent. As a practical consequence dependencies on activation outputs of later layers are preferred, and we use the Rasley et al.  profiler to measure inference cost during training rounds and rank models (see Appendix C).

## 4 Theoretical Analysis

In this section we provide theoretical analysis and justification for our method. First, we show that the ensemble output produced by algorithm 1 converges to \(g\) at \((1/)\) rate, provided that the procedure Find-wl succeeds at every time \(t\).

**Theorem 1**.: _Suppose the class \(\) satisfies that for all \(f\), \(\|f-g\|_{} G_{}\). Let \(F=\{f_{t}\}\) be the ensemble after \(T\) rounds of Algorithm 1, with the final output \(F_{t}=_{t=1}^{T}f_{t}\). If \(f_{t}\) satisfies eq. (7) for all \(t T\) then for \(T 2N\) and \(=}}\), we have for all \(j\)_

\[\|F_{t,j}-g_{j}\|_{ defer the details of the proof to the Appendix B. The main idea behind the proof is to bound the rate of convergence of the algorithm towards the minimax solution. This proceeds by maintaining a potential function and keeping track of its progress through the algorithm. The bounds and techniques here are general in the sense that for various objectives and loss functions appropriate designed weak learners give similar rates of convergence to the minimax solution. Furthermore, a stronger version that shows exponential rates can be shown by additionally assuming an edge for the weak learner.

In addition to the claim above about the in-sample risk, we also show that the algorithm has a strong out-of-sample guarantee. We show this by bounding the generalization error of the algorithm in terms of the generalization error of the class \(\). In the following theorem, we restrict to the case of binary classification for simplicity, but the general result follow along similar lines. Let \(_{T}\) denote the class of functions of the form

\[F_{T}(x)=(_{i=1}^{T}f_{t}(x)),\]

where \(f_{t}\) are functions in class \(\). We then have the following generalization guarantee:

**Theorem 2** (Excess Risk).: _Suppose data \(D\) contains of \(N\) iid samples from distribution \(\) and that the function \(g\) has \(\) margin on data \(D\) with probability \(\), i.e., \(_{x D}[|g(x)|<]<\). Further suppose that the class \(_{T}\) has VC dimension \(d\). Then, for \(T 4G_{}^{2} 2N/^{2}\), with probability \(1-\) over the samples, the output \(F_{T}\) of algorithm 1 satisfies:_

\[(F_{T})}(g)+O(})+\,.\]

Note that the above theorem can easily be adapted to the case of margins and VC dimension of the class \(_{T}\) being replaced with the corresponding fat-shattering dimensions. Furthermore, in the setting of stochastic minimax optimization, one can get population bounds directly by thinking of sample losses and gradients as stochastic gradients to the population objective. This is for example the view taken by . In our work, we separate the population and sample bounds to simplify the presentation and the proofs.

## 5 Empirical Evaluation

We now evaluate b-distil on both real-world and simulated datasets and over a variety of architecture types. We consider six real world datasets across three domains--vision, speech and sensor-data--as well as two simulated datasets. This allows us to evaluate our method on five architecture types: fully connected, convolutional, residual, densely connected networks and recurrent networks. Our code can be found at: github.com/metastableB/bdistil.

### Dataset Information

For experiments with simulated data, we construct two datasets. The first dataset, referred to as _ellipsoid_ is a binary classification dataset. Here the classification labels for each data point \(x^{32}\) are determined by the value of \(x^{T}Ax\) for a random positive semidefinite matrix \(A\). The second simulated dataset, _cube_, is for multiclass classification with 4 classes. Here labels are determined by distance to vertices from \(\{-1,1\}^{32}\) in \(^{32}\), partitioned into 4 classes.

We also use six real world datasets for our experiments. Our image classification experiments use the _CIFAR-10_, _CIFAR-100_, _TinyImageNet_ and _ImageNet_ datasets. For time-series classification tasks we use the _Google-13_ speech commands dataset. Here the task is keyword detection: given a one-second buffer of audio, we need to identify if any of 13 predefined keywords have been uttered in this. Finally, we use the daily sports activities (_DSA_) dataset for experiments with sensor data. Here the task is identifying the activity performed by an individual from a predefined set of sports activities, using sensor data. For detailed information of all datasets used see Appendix C.

### Model Architecture Details

Teacher models.We use deep fully connected (FC) networks for classification on _Ellipsoid_ and convolutional networks for _Cube_. For image classification on _CIFAR-10_ and _ImageNet_ dataset we use publicly available, pretrained ResNet models. We train reference DenseNet models for the _CIFAR-100_ dataset based on publicly available training recipes (see Appendix C). As both spoken audio data (_Google-13_) and sensor-data (_DSA-19_) are time series classification problems, we use recurrent neural networks (RNNs). We train an LSTM-based architecture  on _Google-13_ and a GRU-based architecture  on _DSA-19_. Except for the pretrained ResNet models, all other teacher models are selected based on performance on validation data.

Student models.For all distillation tasks, for simplicity we design the student base model class from the same architecture type as the teacher model, but start with significantly fewer parameters and resource requirements. We train for at most \(T=7\) rounds, keeping \(=1\) in all our experiments. Whenever find-wl fails to find a weak learner, we expand the base class \(\) using the connection specified as a hyperparameter. Since we need only at most \(T=7\) weak learners, we can pick small values of \(R\) (say, 2). The details of the intermediate connections used for each dataset, hyperparameters such as the regularization parameter \(\) in Equation 6 and hyperparameters for SGD can be found in Appendix C and D

### Experimental Evaluation and Results

First, we present the trade-off between accuracy and inference time offered by b-distil in the context of anytime inference and early prediction. We compare our models on top-1 classification accuracy and total floating point operations (FLOPs) required for inference. We use a publicly available profiler  to measure floating point operations. For simplicity of presentation, we convert these to the corresponding inference times (\(\)) on a reference accelerator (NVIDIA 3090Ti).

Anytime inference.As discussed previously, in the anytime inference setting a model is required to produce a prediction even when its execution is interrupted. Standard model architectures can only output a prediction once the execution is complete and thus are unsuitable for this setting. We instead compare against the idealized baseline where we assume _oracle_ access to the inference budget which is usually only available _after_ the execution is finished or is interrupted. Under this assumption, we can train a set of models suitable various inference time constraints, e.g., by training models at various depths, and then pick the one that fits the current inference budget obtained by querying the oracle. We refer to this baseline as no-reshed and compare b-distil to it on both synthetic and real world datasets in Figure 2. This idealized baseline can be considered an upper bound on the accuracy of b-distil for a fixed inference budget.

Figure 2: Accuracy vs. inference-time trade-offs. Inference time is reported as a fraction of teacher’s inference time along with average ensemble accuracy and error bars. b-distil performs this trade-off at runtime. The baseline no-reshed at inference time \(_{w}\) (\(x\)-axis) is the accuracy of a single model that is allowed \(|_{w}-0|\) time for inference. Similarly the baseline reshed at \(_{w}\) is the accuracy of an ensemble of models, where the model \(w\) is allowed \(|_{w}-_{w-1}|\) time to perform its inference. This is also the latency between the successive predictions from b-distil. We can see that b-distil (green) remains competitive to the oracle baseline (no-resched, blue) and outperforms weighted averaging (resched, yellow).

b-distill can improve on its initial prediction whenever inference jobs are allowed to be rescheduled. To contextualize this possible improvement, we consider the case where the execution is interrupted and rescheduled (with zero-latency, for simplicity) at times \(\{_{1},_{2},,_{W}\}\). We are required to output a prediction at each \(_{w}\). As an idealized baseline, assume we know these interrupt points in advance. One possible solution then is as follows: select models with inference budgets \(|_{1}|,|_{2}-_{1}|,,|_{w}-_{w-1}|\). Sequentially evaluate them and at at each interrupt \(_{w}\), and output the (possibly weighted) average prediction of the \(w\) models. We call this baseline resched. Since the prediction at \(_{w}\) is a weighted average of models, we expect its performance to serve as a lower-bound for the performance of b-distill. In the same figure (Figure 2) we compare b-distill to reshed.

We see that at all interrupts points in Figure 2, the predictions provided by b-distill are competitive to that of the idealized baseline reshed which requires the inference budget ahead of time for model selection, while being able to improve on its initial predictions if rescheduled. For instance, for the _CIFAR-100_ dataset and at the interrupt point at \(0.5\) on the \(x\)-axis, the predictions produced by b-distill are comparable to a single model of the same inference duration, while being able to allow interrupts at all the previous points.

Early prediction.To evaluate the applicability of our method for early prediction in online time-series inference, we compare the performance of b-distill to that of e-rnn from . Unlike B-DISTIL, which can be applied to generic architectures, E-RNN is a state-of-the-art method for early prediction that was developed specifically for RNNs. When training, we set the classification loss to the early-classification loss used in e-rnn training. We evaluate our method on the time-series datasets _GoogleSpeech_ and _DSA_. The performance in terms of time-steps evaluated is compared in Table 1. Here, we see that b-distill remains competitive to e-rnn for early prediction. Unlike e-rnn, b-distill also offers early prediction for offline/batch evaluation time-series data. For such cases, a threshold can be tuned similar to e-rnn and b-distill can evaluate the models in its ensemble in order of increasing cost, exiting when the prediction score crosses this threshold.

### Training Considerations and Scalablility

Connections.Our method uses intermediate connections to improve its performance. Although these connections are designed to be efficient, they still have an overhead cost over an averaging based ensemble. The FLOPs required to evaluate intermediate connections corresponding to the distillation tasks in Figure 2 is shown in Figure 3. Here, we compare the FLOPs required to evaluate the model from round \(T\) to the FLOPs required evaluate the intermediate connections used by this model. Note that summing up all the FLOPs up to a round \(T\), in Figure 3 gives the total FLOPs required to for the ensemble with the first \(T\) models. For all our models, the overhead of connections is negligible when compared to the inference cost of the corresponding model.To evaluate the benefits offered by the intermediate connections, we can compare the results of b-distil run with connections and b-distil without connections. The latter case can be thought of as running the AdaBoost algorithm for distillation. Note that this is the same as the resched baseline (weighted averaging).

On comparing the b-distil plot in Figure 2 to the plot of resched highlights the benefit of using intermediate connections. As in this work our focus is on finding weak learners in the presence of capacity gap, and we do not explore additional compression strategies like quantization, hard thresholding, low-rank projection that can further reduce inference cost.

   &  &  \\   & & \(T=50\%\) & \(T=75\%\) & \(T=100\%\) \\   & & Acc (\%) & Frac & Acc (\%) & Frac & Acc (\%) \\   & e-rnn & 88.31 & 0.48 & 88.42 & 0.65 & 92.43 \\   & b-distill & 87.41 & 0.49 & 89.31 & 0.71 & 92.25 \\   & e-rnn & 83.5 & 0.55 & 83.6 & 0.56 & 86.8 \\   & b-distill & 82.1 & 0.53 & 84.1 & 0.58 & 87.2 \\  

Table 1: Early prediction performance. Performance of the ensemble produced by b-distill to the e-rnn algorithm . The accuracy and the cumulative fraction of the data early predicted at \(50\%\), \(75\%\) and \(100\%\) time steps are shown. At \(T=100\), frac. evaluated is \(1.0\). The ensemble output by b-distill with the early-prediction loss is competitive to the e-rnn algorithm. Unlike e-rnn, a method developed specifically for early prediction of RNNs, B-DISTILL is more generally applicable across model architectures and can also be used for offline.

Overheads of training/distillation.b-distill requires additional compute and memory compared to a single model's training. Maintaining the two matrices \(K_{t}^{+}\) and \(K_{t}^{-}\) requires an additional \((NL)\) memory. Even for relatively large datasets with, say, \(N=10^{6}\) samples and \(L=1000\) classes, this comes out to a few gigabytes. Note that the two matrices can be stored in on disk and a batch can be loaded into memory for loss computation asynchronously using data loaders provided by standard ML frameworks. Thus the additional GPU memory requirement is quite small, \((bL)\) for a mini-batch size \(b\), same as the memory required to maintain one-hot classification labels for a mini-batch. Since gradient computation is required only for the model being trained in each round, which typically is smaller than the teacher model, the backward pass is relatively cheap. For large datasets, loading the data for the teacher model forward pass becomes the bottleneck. See Appendix D for discussion on data loading, training time, and resource requirements for a ImageNet distillation.

Sorting \(\{\}_{t}\) using profiling.As mentioned in Section 3 b-distill assumes the hypothesis classes in \(\{\}_{t}\) are ordered on some metric, say, inference time. In practice, we achieve this at run-time by starting with a small base model, and profiling subsequent models considered in later rounds at run-time. For example, in PyTorch, we can use the torch.autograd.profiler module to profile the forward pass of a model. As a heuristic, we then sort the models in \(\{\}_{t}\) based on the average inference time of the models in the ensemble.

## 6 Limitations, Broader Impact, and Future Work

In this paper we explore the problem of _progressive ensemble distillation_, in which the goal is to produce an ensemble of small weak learners from a large model, where components of the ensemble enable progressively better results as the ensemble size increases. To address this problem we propose b-distill, an algorithm for progressive ensemble distillation, and demonstrate that it can be useful for a number of applications in efficient inference. In particular, our approach allows for a straightforward way to trade off accuracy and compute at inference time, and is critical for scenarios where inference may be interrupted abruptly or where variable levels of accuracy can be tolerated. We experimentally demonstrate the effectiveness of b-distill by decomposing well-established deep models onto ensembles for data from vision, speech, and sensor domains. Our procedure leverages a stochastic solver combined with log barrier regularization for finding weak learners, use profiling for model selection and use intermediary connections to circumvent the issue of model capacity gap.

A key insight in this work is that posing distillation as a two player zero-sum game allows us to abstract away model architecture details into base class construction \(\). This means that, conditioned on us finding a 'weak learner' from the base class, we retain the guarantees of the traditional boosting setup. Since these weak learners are only required to produce small improvements between rounds, we are able to reliably find such models. A caveat and potential limitation of this abstraction is that the user must design \(\). Our research primarily focuses on on-device continuous inference, but our distillation procedure also holds benefits for cloud/data-center inference settings. This includes tasks like layer-fusion, load-balancing, and improved resource utilization, which merit further investigation in future studies. Finally, it is important to mention that our work prioritizes optimizing model accuracy while enabling compressed forms. However, another area for future research is exploring additional impacts of our approach on metrics such as fairness or robustness [25; 26; 32].

Figure 3: Overhead of connections. The floating point operations required to evaluate the model added in round \(T\), compared to that required to evaluate just the connections used by this model. We present the results corresponding to datasets that have models with smaller required FLOPs overall. We see that even for these models the connections add relatively little overhead.

Acknowledgements

This work was supported in part by National Science Foundation Grants IIS2145670 and CCF2107024, a Meta Faculty Award, and the Private AI Collaborative Research Institute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the National Science Foundation or any other funding agency.