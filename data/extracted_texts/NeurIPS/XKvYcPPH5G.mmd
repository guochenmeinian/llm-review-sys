# SPO: Sequential Monte Carlo Policy Optimisation

Matthew V Macfarlane

University of Amsterdam

m.v.macfarlane@uva.nl &Edan Toledo

InstaDeep

&Donal Byrne

InstaDeep &Paul Duckworth

InstaDeep &Alexandre Laterre

InstaDeep

Work done during internship at InstaDeep

###### Abstract

Leveraging planning during learning and decision-making is central to the long-term development of intelligent agents. Recent works have successfully combined tree-based search methods and self-play learning mechanisms to this end. However, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they often result in a negative impact on performance. In this paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a model-based reinforcement learning algorithm grounded within the Expectation Maximisation (EM) framework. We show that SPO provides robust policy improvement and efficient scaling properties. The sample-based search makes it directly applicable to both discrete and continuous action spaces without modifications. We demonstrate statistically significant improvements in performance relative to model-free and model-based baselines across both continuous and discrete environments. Furthermore, the parallel nature of SPO's search enables effective utilisation of hardware accelerators, yielding favourable scaling laws.

## 1 Introduction

The integration of reinforcement learning (RL) and neural-guided planning methods has recently achieved considerable success. Such methods effectively leverage planning during training via iterative imitation learning . Applying additional computation via planning generates improved policies which is then amortised  into a neural network policy. Using this policy as part of planning itself creates a powerful self-improvement cycle. They have demonstrated state of the art performance in applications ranging from chess  and matrix multiplication , to language modelling . However, one of the most commonly-used search-based policy improvement operators, MCTS : i) performs poorly on low budgets , ii) is inherently sequential limiting its scalability , and iii) requires modifications to adapt to large or continuous action spaces . These limitations underscore the need for more scalable, efficient and generally applicable algorithms.

In this work, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a model-based RL algorithm that utilises scalable sampled-based Sequential Monte Carlo planning for policy improvement. We formalise SPO as an approximate policy iteration algorithm, and show that it is formally grounded within the Expectation Maximisation (EM) framework.

SPO is unique by leveraging both breadth and depth of search in order to provide better estimates of the target distribution, derived via EM optimisation, compared to previous works. When viewed relative to popular algorithms it combines the breadth used in MPO  and the depth used in V-MPO , while enforcing KL constraints on updates to ensure stable policy improvement. SPO achieves strong performance across a range of benchmarks, outperforming AlphaZero, a leading model-based expert iteration algorithm. We also benchmark against a leading Sequential Monte Carlo (SMC) algorithm from Piche et al.  and show that leveraging posterior estimates from SMC for policy improvement is central to strong performance.

We demonstrate that i) SPO outperforms baselines across both high-dimensional continuous control and challenging discrete planning tasks without algorithmic modifications, and ii) the sampling-based approach is inherently parallelisable, enabling the use of hardware accelerators to improve training speed. This provides significant advantages over MCTS-based methods , whose sequential process can be particularly inefficient during training. 3) The explicit use of KL targeting leads to strong and stable policy improvement, reducing the need for a grid search over exploration hyperparameters in search. We find that SPO, empirically has strong scaling behaviours, with improved performance given additional search budget both during training and inference.

## 2 Related Work

There is an extensive history of prior works that frame control and RL as an inference problem [82; 45]. Expectation Maximization [19; 28; 60] is a popular approach to directly optimising the evidence lower bound (ELBO) of the probability of optimality of a given policy. A number of model free methods implement EM, including RWR , REPS , MPO , V-MPO , and AWAC  each with different approaches to the E-step and M-step (summarised in Furuta et al. ). Model based algorithms such as MD-GPS  and AlphaZero  can also be viewed under the EM framework using LQR  and MCTS [9; 16] during the E-step respectively. Grill et al.  show MCTS is a form of regularised policy optimisation equivalent to optimising the ELBO. Such model-based algorithms, that iterate between policy improvement using search, and projecting this improvement to the space of parameteriseable policies, are also often referred to as Expert iteration (ExIt) [74; 7] or Dual Policy Iteration . Such approaches have also been applied to active inference  where MCTS has been used to estimate expected free energy, with this computation then amortised into a neural policy . Our work can also be framed as ExIt, but differs by using Neural SMC for planning with an explicit KL regularisation on the improvement.

Sequential Monte Carlo (SMC) [31; 43; 52], also referred to as particle filters [66; 6], is an inference method often employed to sample from intractable distributions. Gu et al.  explore the parameterisation of the proposal in SMC using LSTMs , showing that posterior estimates from SMC can be used to train a parameterised proposal in non-control based tasks. Li et al.  demonstrate the same concept but using an MCMC sampler . SMC can be applied to sample from distributions over trajectories in RL. For example, SMC-Learning  updates SMC weights using a Boltzmann exploration strategy, but unlike SPO it does not control policy improvement with KL regularisation or leverage neural approximators. Piche et al.  derive an SMC weight update to directly estimate the posterior over optimal trajectories. However this update requires the optimal value function, which they approximate using a SAC  value function. This approach requires high sample counts and does not leverage SMC posterior estimates for policy improvement, instead utilising a Gaussian distribution or SAC to train the proposal. CriticSMC  estimates the same posterior but utilises a soft action-value function to score particles , reducing the number of steps performed in the environment, enabling more efficient exploration and improved performance on lower SMC budgets compared with Piche et al. . However their method is slower in wall clock time and uses a static proposal, therefore not performing iterative policy improvement. A useful properly of SMC search methods is that it is inherently parallelisable. Parallelising MCTS with a virtual loss has been explored, however this often leads to performance degradation , increasing exploration and leading to out-of-distribution states that are difficult to evaluate Dalal et al. .

## 3 Background

**Sequential decision-making** can be formalised under the Markov Decision Process (MDP) framework . An MDP is a tuple \((,,,r,,)\) where, \(\) is the set of possible states, \(\) is the set of actions, \(:()\) is the state transition probability function, \(r:\) is the reward function, \(\) is the discount factor, and \(\) is the initial state distribution. An agent interacts with the MDP using a policy \(:()\), that associates to every state a distribution over actions. We quantify the quality of a policy by the expected discounted return, that the agent seeks to maximise \(^{*}_{}_{}[_{t=0}^{}^ {t}r_{t}],\) where \(r_{t}=r(s_{t},a_{t})\) is the reward received at time \(t\) and \(\) is the set of all realisable policies. The value function \(V^{}(s_{t})=_{}[_{t=t}^{}^{t}r_{t} s _{t}]\) maps a state \(s_{t}\) to the expected discounted sum of future rewards when acting according to \(\). Similarly, the state-action value function \(Q^{}(s_{t},a_{t})=_{}[_{t=t}^{}^{t}r_{t}  s_{t},a_{t}]\) maps a state \(s_{t}\) to the expected discounted return, when taking the initial action \(a_{t}\) and following \(\) thereafter.

**Control as inference** formulates the RL objective as an inference problem within a probabilistic graphical model . For a horizon \(T\), the distribution over trajectories \(=(s_{0},a_{0},s_{1},a_{1},...,s_{T},a_{T})\) is given by \(p()=(s_{0})_{t=0}^{T}p(a_{t})(s_{t+1}|s_{t},a_{t})\), which is a function of the initial state distribution, the transition dynamics, and an action prior. Note that this distribution is insufficient for solving control problems, because it has no notion of rewards. We therefore have to introduce an additional _optimality variable_ into this model, which we will denote \(_{t}\), that is defined such that \(p(_{t}=1|)(r_{t})\). Therefore, a high reward at time \(t\) means a high probability of having taken the optimal action at that point. We are then concerned with the target distribution \(p(|_{1:T})\), which is the distribution of trajectories given optimality at every step. We denote \(_{1:T}\) as \(\) going forward. The RL objective is then formulated as finding a policy to maximise \( p_{}(=1)=()p(=1|)d\), which intuitively can be thought of as maximising the distribution of optimality at all timesteps, given actions sampled according to \(\). To optimise this challenging objective, we can derive the evidence lower bound (ELBO) using an auxiliary distribution \(q\):

\[ p_{}(=1) =()p(=1|)d= q() =1|)}{q()}d\] (1) \[ q()[ p(=1|)+]d=_{q}[_{t}}{} ]-(q()\|()).\]

Since \(p(O=1|)(_{t}r_{t})\) and \(\) is a normalising constant, ensuring a valid probability distribution.

**Expectation Maximisation** (EM)  has been widely applied to solve the optimisation problem \(_{} p_{}(=1)\). After deriving the lower bound \((q,)\) on the objective in eq. (1) using an auxiliary non-parametric distribution \(q\), EM then performs a coordinate ascent, iterating between optimizing the bound with respect to \(q\) (E-step) and with respect to the parametric policy \(\) (M-step). This generates a sequence of policy pairs \(\{(_{0},q_{0}),(_{1},q_{1}),,(_{n},q_{n})\}\) such that in each step \(i\) in the EM sequence, \((q_{i+1},_{i+1})(q_{i},_{i})\). Viewed through a traditional policy improvement lens, the E-step corresponds to a policy evaluation phase where we perform rollouts, generating states with their associated estimates for \(q\) and value estimates. The M-step corresponds to a policy improvement phase where \(\) and \(V\) are updated. This step can also be thought of as amortising the probabilistic inference computation performed in the E-step, into a neural network forward pass operation. 2

**Sequential Monte Carlo** (SMC) methods  are designed to sample from an intractable distribution \(p\) referred to as the _target distribution_ by using a tractable proposal distribution \(\) (we use \(\) to prevent overloading of \(q\)). _Importance Sampling_ does this by sampling from \(\) and weighting samples by \(p(x)/(x)\). The estimate is performed using a set of \(N\) particles \(\{x^{(n)}\}_{n=1}^{N}\), where \(x^{(n)}\) represents a sample from the support that the target and proposal distributions are defined over, along with the associated importance weights \(\{w^{(n)}\}_{n=1}^{N}\). Each particle uses a sample from the proposal distribution to improve the estimation of the target, increasing \(N\) naturally improves the estimation of \(p\). Once importance weights are calculated, the target is estimated as \(_{n=1}^{N}^{(n)}_{x^{(n)}}(x)\)where \(\) is the normalised importance sample weight and \(_{x^{(n)}}\) is the dirac measure located at \(x^{(n)}\). _Sequential Importance Sampling_ generalises this for sequential problems, where \(x=(x_{1},,x_{T})\). In this case importance weights can be calculated iteratively according to:

\[w_{t}(x_{1:t})=w_{t-1}(x_{1:t-1})|x_{1:t-1})}{(x_{t}|x_{ 1:t-1})}.\] (2)

Particle filtering methods can be affected by _weight degeneracy_. This occurs when a few particles dominate the normalised importance weights, rendering the remaining particles negligible. As the variance of particle weights is guaranteed to increase over sequential updates , this phenomenon is unavoidable. When the majority of particles contribute little to the estimation of the target distribution, computational resources are wasted. _Sequential Importance Resampling_ (SIR)  mitigates this problem by periodically resampling particles according to their current weights, subsequently resetting these weights.

## 4 SPO Method

In this section, we present a novel method that combines Sequential Monte Carlo (SMC) sampling with the Expectation Maximisation (EM) framework for policy iteration. We begin by formulating the objective function that we aim to optimise. We then outline our iterative approach to maximising this function, alternating between an expectation step (E-step) and a maximisation step (M-step). Within the E-step, we derive the analytical solution for optimising the objective with respect to the auxiliary distribution \(q\). We then demonstrate how SMC can be employed to effectively estimate this _target distribution_. The M-step can be viewed as a projection of the non-parametric policy obtained in the E-step back onto the space of feasible policies. A comprehensive algorithmic outline of our proposed approach is provided in appendix D.2.

### Objective

We add an additional assumption to the lower bound objective defined in eq. (1) and assume that the auxiliary distribution \(q\), defined over trajectories \(\), can be decomposed into individual state dependent distributions, i.e. \(q()=(s_{0})_{t 0}q(a_{t}|s_{t})(s_{t+1}|s_{t},a_{t})\). We parameterise \(\) using \(\) which decomposes in the same way. This enables the objective to be written with respect to individual states instead of full trajectories. Multiplying by \(\), the objective can then be written as follows:

\[(q,_{})=_{q}[_{t=0}^{}^{t} [r_{t}-(q(a|s_{t})(a|s_{t},))] ]+ p().\] (3)

Previous works have explored this formulation where \(p()\) is a prior over the parameters of \(\)[38; 71; 1].

### E-step

Within the expectation step of EM, we maximise eq. (3) with respect to \(q\). As in previous work, instead of optimising for the rewards, we consider an objective written according to Q-values [1; 59; 54].

\[_{q}_{(s)}[_{q(a|s)}[Q^{q}(s,a)]- (q(|s)(|s,_{i}))]\] (4)

This aims to maximise the expected Q-value of a policy \(q\), with the constraint that it doesn't move too far away from \(_{i}\). Framing optimisation with respect to Q-values is useful as it enables the use of function approximators to estimate \(Q\).

Maximising this objective is difficult due to the dependence of both the expectation terms and Q-values on \(q\). Following previous works, we fix the Q-values with respect to a fixed policy \(\), resulting in a partial E-step optimisation [74; 1; 76]. We use the most recent estimate of \(q\) as the fixed policy we perform maximisation with respect to, similar to AlphaZero's approach of acting according to the expert policy. The initial state distribution \((s)\) is likewise fixed to be distributed according to \(_{}\). In practice, we use a FIFO replay buffer and sample from it to determine \(_{}\).

Equation (4) consists of balancing two objectives. To optimize this, we frame this as a constrained maximization problem, to avoid scaling problems, see appendix G.3 for further information. This objective limits the KL between \(q\) and \(\) from exceeding a certain threshold:

\[_{q} _{_{(s)}}[_{q(a|s)}[Q^{ }(s,a)]]\] (5) s.t. \[_{_{(s)}}[(q(a|s)\|(a|s, _{i}))]<.\]

The following analytic solution to the constrained optimisation problem can then be derived using the Lagrangian multipliers method outlined in appendix G.1. Likewise \(^{*}\) is obtained by minimising the convex dual function eq. (7), and intuitively can be thought of as enforcing the KL constraint on \(q\), preventing \(q_{i}\) from moving to far from \(_{i}\) :

\[q_{i}(a|s)(a|s,_{i})(}(s,a)-V^{}(s)}{^{*}}),\] (6)

where \(V^{}(s)\) is an action independent baseline. \(Q(s,a)-V(s)\) is referred to as the advantage, where optimising for Q-values vs advantages is equivalent . Optimising with advantages however, has demonstrated to practically outperform optimising Q-values directly . This is also a natural update to perform as the _policy improvement theorem_ outlines that an update by policy iteration can be performed if at least one state-action pair has positive advantage and a non-zero probability of reaching such a state. Although we have a closed form solution for \(q\), we do not have either the value function or action-value function in practice. In the next section we outline our approach to estimating this distribution.

**Regularised Policy Optimisation:** In our closed form solution to the optimisation eq. (6), we are required to solve for \(^{*}\) which ensures that the KL is constrained. This can be calculated by minimising the following objective , see appendix G.2 for derivation:

\[g()=+(s)((a|s,_{i}) (}(s,a)}{})da)ds.\] (7)

Practically, we estimate eq. (7) using a sample-based estimator according to the distribution of states from the replay buffer and stored values for \((a|s,_{i})\) and \(A^{}(s,a)\).

#### 4.2.1 Estimating Target Distribution

Building upon previous work using Sequential Monte Carlo for trajectory distribution estimation , we propose a novel approach that integrates SMC within the Expectation Maximisation

Figure 1: SPO search: \(n\) rollouts, represented by particles \(x^{i},,x^{n}\), each of which represents an SMC trajectory sample, are performed in parallel according to \(_{i}\) (left to right). At each environment step, the weights of the particles are adjusted (indicated in the diagram by circle size). We show two resampling regions where particles are resampled, favouring those with higher weights, and their weights are reset. The target distribution is estimated from the initial actions of the surviving particles (rightmost particles). This target estimate, \(q_{i}\), is then used to update \(\) in the M-step.

framework to estimate the target distribution defined in Equation 6. Our SMC-based method enables sampling from \(q_{i}\) over trajectories of length \(h\), incorporating multiple predicted future states and rewards for a more accurate distribution estimation. A key property of estimating the target using SMC is that it uses both breadth (we initialise multiple particles to calculate importance weights) but also depth (leveraging future states and rewards) to estimate the distribution. In contrast, MPO evaluates several actions per state, focusing on breadth without depth, while V-MPO calculates advantages using n-step returns from trajectory sequences, leveraging depth but only for a single action sample. Our method combines both aspects, enhancing the accuracy of target distribution estimation, crucial for both action selection and effective policy improvement updates in the M-step. An outline of the SMC algorithm is provided in algorithm 1, with a visual representation in fig. 1.

SMC estimates the target by maintaining a set of particles \(\{x^{(n)}\}_{n=1}^{N}\) each of which maintains an importance weight for a particular sample. Given calculated importance weights SMC, estimates the target distribution according to \(_{i}()=_{n=1}^{N}^{(n)}_{x_{(n)}}()\) where \(^{(n)}\) is the normalised importance sample weight for a particular sample. We next outline the sequential importance sampling weight update needed to estimate eq. (6). Since we can sample from \((a_{t},s_{t},_{i})\), we leverage this as our proposal distribution \(\). Given the target distribution \(q\) can be decomposed into individual state dependent distributions, we can define \(p_{i}(_{t}|_{1:t-1})\) and \((_{t}|_{1:t-1})\) as:

\[p_{i}(_{t}|_{1:t-1})(s_{t+1}|s_{t},a_{ t})_{i}(a_{t}|s_{t},_{i})(_{i}(a_{t},s_{t})}{ _{i}^{*}})\] (8) \[(_{t}|_{1:t-1})_{}(s_{ t+1}|s_{t},a_{t})_{i}(a_{t}|s_{t},_{i})\] (9)

leading to the following convenient SMC weight update according to eq. (2):

\[w(_{1:t}) w(_{1:t-1})((s_{t}|s_{t-1 },a_{t-1})}{_{model}(s_{t}|s_{t-1},a_{t-1})})}(a_{t},s_{t})/^{*})(a_{t}|s_{t},_{i} )}{(a_{t}|s_{t},_{i})},\] (10)

where \(Q^{}(a_{t},s_{t})-V^{}(s_{t})\) is simplified to \(A^{}(a_{t},s_{t})\), \(_{1:t}\) is a sequence of state, action pairs from timestep \(1\) to \(t\), and \(_{model}\) is the environment transition function of the planning model. Note that our work assumes the availability of a model that accurately represents the transition dynamics of the environment \(\), and therefore simplify the update to \(w(_{1:t}) w(_{1:t-1})(A^{}(a_{t},s_{t})/ ^{*})\).

Algorithm 1 outlines the method for estimating eq. (6) over \(h\) planning steps for \(N\) particles (line 3) using the advantage based weight update (line 7) in eq. (10). Once \(h\) steps in the environment model have been performed in parallel, we marginalise all but the first actions as a sample based estimate of \(q_{i}\) (line 14).

```
1:Initialize \(\{s_{t}^{(n)}=s_{t}\}_{n=1}^{N}\)
2:Set \(\{w_{t}^{(n)}=1\}_{n=1}^{N}\)
3:for\(i\{t+1,..,t+h\}\)do
4:\(\{a_{t}^{(n)}(:|s_{t}^{(n)},)\}_{n=1}^{N}\)
5:\(\{s_{t+1}^{(n)}_{model}(s_{t}^{(n)},a_{t}^{(n)})\}_{n=1}^{N}\)
6:\(\{r_{t}^{(n)} r_{model}(s_{t}^{(n)},a_{t}^{(n)})\}_{n=1}^{N}\)
7:\(\{w_{t}^{(n)}=w_{t-1}^{(n)}(A(s_{t}^{n},r_{t}^{n},s_{t+1}^{n})/^{*})\}_{ n=1}^{N}\)
8:if\((i-t) p=0\)then
9:\(\{x_{t}^{(n)}\}_{n=1}^{N}(n;w_{t}^{(1)},..,w_{t}^{(N)})\)
10:\(\{w_{t}^{(n)}=1\}_{n=1}^{N}\)
11:endif
12:endfor
13:\(\{a_{t}^{(n)}\}\) as the set of first actions of \(\{x_{t:t+h}^{(n)}\}_{n=1}^{N}\)
14:\((a|s_{t})=_{n=1}^{N}^{(n)}_{a_{t}^{(n)}}(a)\) ```

**Algorithm 1** SMC \(q\) target estimation (timestep \(t\))

The advantage function \(A^{}\) is typically unknown, so we compute a 1-step estimate at each iteration using the value function and observed environment rewards \((s_{t},a)=r_{t}+V^{}(s_{t+1})-V^{}(s_{t})\). Practically, we parameterise the value function \(V\) using a neural network and train it using GAE  on true environment rollouts collected. After \(h\) steps, the importance weights leverage \(h\)-steps of observed states and rewards during planning and corresponding advantage estimates. Compared to tree-based methods such as MCTS, SMC does not require maintaining the full tree in memory. Instead, after each planning step, it only needs to retain the initial action, current state, and current particle weight. It also does not require spending computation sending backward messages to update statistics of previous nodes everytime a new node is added to the tree.

**Resampling Adaptive Search:** To mitigate the issue of _weight degeneracy_, SPO conducts periodic resampling (lines 8-11). This involves generating a new set of particles from the existing set by duplicating some trajectories and removing others, based on the current particle weights. This process enhances computational efficiency in estimating the target distribution. By resampling, we avoid updating weights for trajectories with low likelihood under the target, thereby reallocating computational resources to particles with high importance weights .

### M-step

After completing the E-step (which generates \(q_{i}\)), we proceed with the M-step, which optimises eq. (3) with respect to \(\), parametrised by \(\). By eliminating terms that are independent of \(\), we optimise the following objective, corresponding to a maximum a posteriori estimation with respect to the distribution \(q_{i}\):

\[_{}(q_{i},_{})=_{}_{_{q _{i}}(s)}[_{q_{i}(|s)}[(a|s,)] ]+ p().\] (11)

This optimisation can be viewed as projecting the non-parametric policy \(q_{i}\) back to the space of parametrisable policies \(_{}\), as performed in expert iteration style methods such as AlphaZero. \(p()\) represents a prior over the parameter \(\). Previous works find that utilising a prior for \(\) to be close to the estimate from the previous iteration \(_{i}\) leads to stable training [1; 76]. Therefore we assume a gaussian prior over the current policy parameters, see appendix G.5 where we show utilising such a prior leads to the following constrained objective:

\[_{}_{_{q_{i}}(s)}[_{q_{i}(a |s)}[(a|s,)]]\] (12) s.t. \[_{_{q_{i}}(s)}[((a|s,_{i }),(a|s,))]<_{m}.\]

### Policy Improvement

Previous approaches to using SMC for RL either do not perform iterative policy improvement using SMC , or lack policy improvement constraints needed for iterative improvement . Assuming that SMC provides perfect estimates of the analytic _target distribution_ at each step \(i\), Expectation Maximisation algorithm will guarantee that successive iterations will result in monotonic improvements in our lower bound objective, with details outlined in appendix G.4.

**Proposition 1**.: _Given a non-parametric variational distribution \(q_{i}\) and a parametric policy \(_{_{i}}\). Given \(q_{i+1}\), the analytical solution to E-step optimisation eq. (3), and \(_{_{i+1}}\), the solution to maximisation problem in the M-step eq. (12) then the ELBO \(\) is guaranteed to be monotonically increasing: \((q_{i+1},_{_{i+1}})(q_{i},_{_{i}})\)._

In practice we are unlikely to generate perfect estimates of the target through sample based inference and leave derivations regarding the impact of this estimation on overall convergence for future work. We also draw the connection between our EM optimisation method and Mirror Descent Guided Policy Search (MD-GPS) . Our objective can be viewed as a specific instance of MD-GPS (see appendix G.6). Depending on whether dynamics are linear or not, optimising the EM objective can be viewed either as exact or approximate mirror descent . Monotonic improvement guarantees in MD-GPS follow from those of mirror descent.

## 5 Experiments

In this section, we focus on three main areas of analysis. First, we demonstrate the improved performance of SPO in terms of episode returns, relative to both model-free and model-based algorithms. We conduct evaluations across a suite of common environments for both continuous control and discrete action spaces. Secondly, we examine the scaling behaviour of SPO during training, showing that asymptotic performance scales with particle count and depth. Finally, we explore the performance-to-speed trade-off at test time by comparing SPO directly to AlphaZero as the search budget increases.

### Experimental Setup

In order to ensure the robustness of our conclusions, we follow the evaluation methodology proposed by Agarwal et al. , see appendix H for further details. This evaluation methodology groups performance across tasks within an environment suite, enabling clearer conclusions over the significance of learning algorithms as a whole. We include individual results in appendix C, along with additional analysis measuring the statistical significance of our results.

**Environments:** For continuous control we evaluate on the Brax  benchmark environments of: Ant, HalfCheetah, and Humanoid. For discrete environments, we evaluate on Boxoban  (a specific instance of Sokoban), commonly used to assess planning methods, and Rubik's Cube, a sparse reward environment with a large combinatorial state-action space. See appendix A for further details regarding environments.

**Baselines:** Our model-free baselines include PPO , MPO  and V-MPO  for both continuous and discrete environments. For model-based algorithms, we compare performance to AlphaZero (including search improvements from MuZero ) and an SMC method introduced by Piche et al. 3 (which we refer to as SMC-ENT due to its use of maximum entropy RL to train the proposal and value function used within SMC). Our AlphaZero benchmark follows the official open-source implementation4. For continuous environments we baseline our results to Sampled MuZero , a modern extension to MuZero for large and/or continuous action spaces. We utilise a true environment model, aligning our implementation of Sampled MuZero more closely with AlphaZero. Our core experiments configure SPO with 16 particles and a horizon of 4, and AlphaZero with 64 simulations to equalise search budgets. For remaining parameters, see appendix E.1 and appendix D.3. Each environment and algorithm were evaluated with five random seeds. 5

### Results

**Policy Improvement:** In fig. 2 we show empirical evidence that SPO outperforms all baseline methods across both discrete (left) and continuous (right) benchmarks for policy improvement. This conclusion also holds for the per-environment results reported in appendix C. SMC-ENT  is a strong SMC based algorithm, however, SPO outperforms it for both discrete and continuous environments providing strong evidence of the direct benefit of using SMC posterior estimates for policy improvement. We also highlight the importance regularising policy optimisation, which is practically achieved by solving for \(^{*}\) eq. (7), and is re-estimated at each iteration. In appendix B we ablate the impact of this adaptive method by comparing varying fixed temperature schedules. While a good temperature can be found through expensive grid search for each new environment we find the adaptive temperature ensures stable updates leading to strong learning performance across all environments, with minimal overhead to compute \(^{*}\).

We find that SPO performance is statistically significant compared to AlphaZero across the evaluated environments, see appendix C for detailed results. The substantial variation in AlphaZero performance across these environments underscores the difficulty in tuning it for diverse settings. For instance, while AlphaZero performs well on Boxoban and HalfCheeta, its performance drops significantly on other discrete and continuous problems. This inconsistency poses a major challenge

Figure 2: Learning curves for discrete and continuous environments. The Y-axis represents the interquartile mean of min-max normalised scores, with shaded regions indicating 95% confidence intervals, across 5 random seeds.

for its applicability to real-world problems. In contrast, SPO performs consistently well across all environments (both discrete and continuous), highlighting its general applicability and robustness to various problem settings.

### Scaling SPO during training

In fig. 3 (left) we investigate how SPO performance is impacted by scaling particle counts \(N\) and horizon length \(h\) during training, both of which we find improve the estimation of the _target distribution_. Our results show that scaling both the particle count and the horizon leads to improvements in the asymptotic performance of SPO. It also suggests that both variables should be scaled together for maximum benefit as having a long horizon with low particle counts can actually negatively impact performance. Secondly, we highlight that while for our main results we use a total search budget (particles \(\) depth) of \(64\) for policy improvement, our scaling results show that competitive results can be achieved by reducing this budget by a factor of four, which demonstrates competitive performance at low compute budgets.

### Scaling SPO at test time

We can scale particle counts and horizon during training to improve target distribution estimates for the M-step. However, this scaling can also be performed after training when \(_{_{i}}\) is fixed, enhancing performance, since actions are sampled in the environment according to the improved policy \(q\). This equates to additional E-step optimization, which can enhance performance due to the representational constraints of the space of parameterised policies. In fig. 3 (right) we demonstrate how the performance of both SPO and AlphaZero search scales with additional search budget at test time using the same high performing checkpoint. We plot the time taken for a single actor step (measured on a TPUv3-8) against solve rate for the Rubik's Cube problem with time on a logarithmic axis. Specifically, we evaluate on 1280 episodes for cubes ten scrambles away from solved. While we recognise that such analysis can be difficult to perform due to implementation discrepancies, we used the official JAX implementation of AlphaZero (MCTX) within our codebase and compare this to SPO. Additionally, we exclusively measure inference thus no training implementation details affect our measurements.

This provides evidence that AlphaZero has worse scaling when compared to SPO on horizons four and eight, noting that for horizon of two, SPO performance converges early, as low depths can act as a bottleneck for further performance improvements. This highlights the benefits of the SPO parallelism. This chart also shows how for different compute preferences at test time, a different horizon length is preferable. For limited compute, low horizon lengths and relatively higher particle counts provide the best performance, but as compute availability increases, the gains from increasing particle counts decrease and instead horizon length \(h\) should be increased.

Figure 3: (left) Scaling: Mean normalised performance across all continuous environments on \(10^{8}\) environment steps, varying particle numbers \(N\) and horizon \(h\) for SPO during training. (right) Wall Clock Time Comparison: Performance on Rubik’s cube plotted against wall-clock time for AlphaZero and 3 versions of SPO (varying by SMC search depth), with total search budget labeled at each point.

Lastly, the plot illustrates the increase in available search budget, by using SPO, given a time restriction rather than a compute restriction. For example, SPO can use 4 times more search budget, when compared to AlphaZero, given a requirement of 0.1 second per step.

## 6 Conclusion

Planning-based policy improvement operators have proven to be powerful methods to enhance the learning of policies for complex environments when compared to model-free methods. Despite the success of these methods, they are still limited in their usefulness, due to the requirement of high planning budgets and the need for algorithmic modifications to adapt to large or continuous action spaces. In this paper, we introduce a modern implementation of Sequential Monte Carlo planning as a policy improvement operator, within the Expectation Maximisation framework. This results in a general training methodology with the ability to scale in both discrete and continuous environments.

Our work provides several key contributions. First, we show that SPO is a powerful policy improvement operator, outperforming our model-based and model-free baselines. Additionally, SPO is shown to be competitive across both continuous and discrete domains, without additional environment-specific alterations. This illustrates the effectiveness of SPO as a generic and robust approach, in contrast to prior methods that require domain-specific enhancements. Furthermore, the parallelisable nature of SPO results in efficient scaling behaviour of search. This allows SPO to achieve a significant boost in performance at inference time by scaling the search budget. Finally, we demonstrate that scaling SPO results in faster wall-clock inference time compared to previous work utilising tree-based search methods. The presented work culminates in a versatile, neural-guided, sample-based planning method that demonstrates superior performance and scalability over baselines.

**Limitations & Future Work:** This work demonstrates the efficacy of combining probabilistic inference with reinforcement learning and amortisation. While our work considers a relatively standard form of Sequential Monte Carlo, future work could investigate making improvements to this inference method, in order to further improve the estimate of the target distribution, while maintaining the scalability benefits demonstrated. We also draw the connection to Active Inference  which has been explored in the context of agent behaviour in complex environments, where Monte Carlo Tree Search has been used to scale previous methods along with deep learning . Leveraging Sequential Monte Carlo in such methods along with amortisation could be a promising direction of research to further scale such methods. Our work exclusively uses exact world models of the environment in order to isolate the effects of various planning methods on performance. Extending our research to include a learned world model would broaden the applicability of SPO to more complex problems that may not have a perfect simulator and are therefore unsuitable for direct planning. Additionally, we apply Sequential Monte Carlo (SMC) only in deterministic settings. Adapting our approach for stochastic environments presents challenges. Specifically, the importance weight update in SMC can lead to the selection of dynamics most advantageous to the agent, potentially fostering risk-seeking behaviour in stochastic settings. However, mitigation strategies exist, as discussed in Levine .