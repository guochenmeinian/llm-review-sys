# Beta Diffusion

Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng

The University of Texas at Austin

Austin, TX 78712

Corresponding to:mingyuan.zhou@mccombs.utexas.edu

###### Abstract

We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, further supports the efficacy of KLUBs for optimization. Experimental results on both synthetic data and natural images demonstrate the unique capabilities of beta diffusion in generative modeling of range-bounded data and validate the effectiveness of KLUBs in optimizing diffusion models, thereby making them valuable additions to the family of diffusion-based generative models and the optimization techniques used to train them.

## 1 Introduction

Diffusion-based deep generative models have been gaining traction recently. One representative example is Gaussian diffusion  that uses a Gaussian Markov chain to gradually diffuse images into Gaussian noise for training. The learned reverse diffusion process, defined by a Gaussian Markov chain in reverse order, iteratively refines noisy inputs towards clean photo-realistic images. Gaussian diffusion can also be viewed from the lens of denoising score matching  and stochastic differential equations . They have shown remarkable success across a wide range of tasks, including but not limited to generating, restoring, and editing images , transforming 2D to 3D , synthesizing audio , reinforcement learning , quantifying uncertainty , and designing drugs and proteins .

Constructing a diffusion-based generative model often follows a general recipe . The recipe involves three basic steps: First, defining a forward diffusion process that introduces noise into the data and corrupts it with decreasing signal-to-noise ratio (SNR) as time progresses from 0 to 1. Second, defining a reverse diffusion process that denoises the corrupted data as time reverses from 1 to 0. Third, discretizing the time interval from 0 to 1 into a finite number of intervals, and viewing the discretized forward and reverse processes as a fixed inference network and a learnable generator, respectively. Auto-encoding variational inference  is then applied to optimize the parameters of the generator by minimizing a weighted negative ELBO that includes a Kullback-Leibler (KL) divergence-based loss term for each discretized reverse step.

Although the general diffusion-modeling recipe is simple in concept, it requires access to the corrupted data at any time during the forward diffusion process given a data observation, as well as the analytic form of the conditional posterior for any earlier time given both a data observation and its corrupted version at the present time. The latter requirement, according to Bayes' rule, implies access to the analytical form of the distribution of a corrupted data observation at the present time given its value at any previous time. Linear operations of the Gaussian distributions naturally satisfy these requirements since they are conjugate to themselves with respect to their mean parameters. This means that the marginal form and the conditional distribution of the mean remain Gaussian when two Gaussian distributions are mixed. Similarly, the requirements can be met under the categorical distribution  and Poisson distribution . However, few additional distributions are known to meet these requirements and it remains uncertain whether negative ELBO would be the preferred loss.

While previous works have primarily used Gaussian, categorical, or Poisson distribution-based diffusion processes, this paper introduces beta diffusion as a novel addition to the family of diffusion-based generative models. Beta diffusion is specifically designed to generate data within bounded ranges. Its forward diffusion process is defined by the application of beta distributions in a multiplicative manner, whereas its reverse diffusion process is characterized by the use of scaled and shifted beta distributions. Notably, the distribution at any point in time of the forward diffusion given a data observation remains a beta distribution. We illustrate the forward beta diffusion process in Figure 1, which simultaneously adds noise to and masks the data, and the reverse one in Figure 2, which iteratively performs demasking and denoising for data generation. We provide the details on how these images are obtained in Appendix D.

Since the KL divergence between two beta distributions is analytic, one can follow the general recipe to define a negative ELBO to optimize beta diffusion. However, our experiments show that minimizing the negative ELBO of beta diffusion can fail to optimally estimate the parameters of the reverse diffusion process. For each individual time-dependent KL loss term of the negative ELBO, examining it in terms of Bregman divergence  reveals that the model parameters and corrupted data are placed in its first and second arguments, respectively. However, to ensure that the optimal solution under the Bregman divergence agrees with the expectation of the clean data given the corrupted data, the order of the two arguments must be swapped.

By swapping the Bregman divergence's two arguments, we obtain an upper bound on the KL divergence from the joint distribution of corrupted observations in the reverse chain to that in the forward chain. This bound arises from the convexity of the KL divergence. In addition, there exists another Bregman divergence that upper bounds the KL divergence from the univariate marginal of a corrupted observation in the reverse chain to that in the forward chain. These two Bregman divergences, which can be derived either through KL divergence upper bounds (KLUBs) or the logarithmically convex beta function, share the same optimal solution but have distinct roles in targeting reverse accuracy at each step or counteracting accumulation errors over the course of reverse

Figure 1: Illustration of the beta forward diffusion process for two example images. The first column displays the original images, while the other 21 columns display the images noised and masked by beta diffusion at time \(t=0,0.05,,1\), using \(=10000\) and the sigmoid diffusion schedule with \(c_{0}=10\) and \(c_{1}=-13\).

Figure 2: Illustration of reverse beta diffusion for two example generations. The time \(t\) decreases from 1 to 0 when moving from left to right. In each image generation, the top row shows the trajectory of \(z_{t}\) (rescaled for visualization), which has been demasked and denoised using reverse diffusion, whereas the bottom row shows \(_{0}=f_{}(z_{t},t)\), whose theoretical optimal solution is equal to \([x_{0}\,|\,z_{t}]\). See Appendix D for more details.

diffusion. We further demonstrate that combining these two KLUBs presents a computationally viable substitution for a KLUB derived at the chain level, which upper bounds the KL divergence from the joint distribution of all latent variables in a forward diffusion chain to that of its reverse. Either KLUB works on its own, which is not unexpected as they both share the same optimal solutions in theory, but combining them could lead to the best overall performance. In beta diffusion, the KL divergence is asymmetric, which enables us to derive an alternative set of KLUBs by swapping its two arguments. We will demonstrate that these augment-swapped KLUBs, which will be referred to as AS-KLUBs, essentially reduce to negative ELBOs. In Gaussian diffusion, the KL divergence is often made symmetric, resulting in KLUBs that are equivalent to (weighted) negative ELBOs.

Our main contributions are the introduction of beta diffusion as a novel diffusion-based multiplicative generative model for range-bounded data, as well as the proposal of KLUBs as effective loss objectives for optimizing diffusion models, in place of (weighted) negative ELBOs. Additionally, we introduce the log-beta divergence, a Bregman divergence corresponding to the differentiable and strictly convex log-beta function, as a useful tool for analyzing KLUBs. These contributions enhance the existing family of diffusion-based generative models and provide a new perspective on optimizing them.

## 2 Beta Diffusion and Optimization via KLUBs

We begin by specifying the general requirements for constructing a diffusion-based generative model and establish the notation accordingly [57; 23; 35]. Let \(x_{0}\) denote the observed data, and let \(z_{s}\) and \(z_{t}\) represent their corrupted versions at time \(s\) and time \(t\), respectively, where \(0<s<t<1\). In the forward diffusion process, we require access to random samples from the marginal distribution \(q(z_{t}\,|\,x_{0})\) at any time \(t\), as well as an analytical expression of the probability density function (PDF) of the conditional distribution \(q(z_{s}\,|\,z_{t},x_{0})\) for any \(s<t\).

The forward beta diffusion chain uses diffusion scheduling parameters \(_{t}\) to control the decay of its expected value over the course of forward diffusion, given by \([z_{t}\,|\,x_{0}]=_{t}x_{0}\), and a positive concentration parameter \(\) to control the tightness of the diffusion process around its expected value. We typically set \(_{t}\) to approach 1 and 0 as \(t\) approaches 0 and 1, respectively, and satisfy the condition

\[1_{0}>_{s}>_{t}>_{1} 0s(0,t),\;t(0,1).\]

Let \(()\) denote the gamma function and \(B(,)\) denote the beta function. The beta distribution \((x;a,b)=B(a,b)^{-1}x^{a-1}(1-x)^{b-1}\) is a member of the exponential family [6; 65; 7]. Its log partition function is a log-beta function as \( B(a,b)=(a)+(b)-(a+b),\) which is differentiable, and strictly convex on \((0,)^{2}\) as a function of two variables . As a result, the KL divergence between two beta distributions can be expressed as the Bregman divergence associated with the log-beta function. Specifically, as in Appendix A, one can show by their definitions that

\[((_{p},_{p})||( _{q},_{q})) =,_{q})}{B(_{p},_{p})}-( _{q}-_{p},_{q}-_{p})_{} B (_{p},_{p})\\ _{} B(_{p},_{p})\] (1) \[=D_{ B(a,b)}((_{q},_{q}),(_{p},_{p})).\]

We refer to the above Bregman divergence as the log-beta divergence. Moreover, if \((_{q},_{q})\) are random variables, applying Proposition 1 of Banerjee et al. , we can conclude that the optimal value of \((_{p},_{p})\) that minimizes this log-beta divergence is \((_{p}^{*},_{p}^{*})=[(_{q},_{q})]\).

Next, we introduce a conditional bivariate beta distribution, which given a data observation has (scaled and shifted) beta distributions for not only its two marginals but also two conditionals. These properties are important for developing the proposed diffusion model with multiplicative transitions.

### Conditional Bivariate Beta Distribution

We first present the conditional bivariate beta distribution in the following Lemma, which generalizes previous results on the distribution of the product of independent beta random variables [30; 38; 33].

**Lemma 1** (Conditional Beta Bivariate Distribution).: _Denote \((z_{s},z_{t})\) as variables over a pair of time points \((s,t)\), with \(0<s<t<1\). Given a random sample \(x_{0}(0,1)\) from a probability-valued data distribution \(p_{data}(x_{0})\), we define a conditional bivariate beta distribution over \((z_{s},z_{t})\) with PDF:_

\[q(z_{s},z_{t}\,|\,x_{0})=x_{0}) ((1-_{s}x_{0}))((_{s}-_{t})x_{0})}^{_{t}x_{0}-1}(1-z_{s})^{(1-_{s}x_{0})-1}}{(z_{s}-z_{t})^{1 -(_{s}-_{t})x_{0}}}.\] (2)_Marginals: Given \(x_{0}\), the two univariate marginals of this distribution are both beta distributed as_

\[q(z_{s}\,|\,x_{0}) =(_{s}x_{0},(1-_{s}x_{0})),\] (3) \[q(z_{t}\,|\,x_{0}) =(_{t}x_{0},(1-_{t}x_{0})).\] (4)

_Conditions: Given \(x_{0}\), a random sample \((z_{t},z_{s})\) from this distribution can be either generated in forward order, by multiplying a beta variable from (3) with beta variable \(_{s t}\), as_

\[z_{t}=z_{s}_{s t},\ \ _{s t}(_{t}x_{0}, (_{s}-_{t})x_{0}),\ \ z_{s} q(z_{s}\,|\,x_{0}),\] (5)

_or generated in reverse order, by combining a beta variable from (4) with beta variable \(p_{s t}\), as_

\[z_{s}=z_{t}+(1-z_{t})p_{s t},\ \ p_{s t}((_{s}-_{t})x_{0},(1-_{s}x_{0})),\ \ z_{t} q(z_{t}\,|\,x_{0}).\] (6)

The proof starts with applying change of variables to obtain two scaled and shifted beta distributions

\[q(z_{t}\,|\,z_{s},x_{0}) =}(}{z_{s}}; _{t}x_{0},(_{s}-_{t})x_{0}),\] (7) \[q(z_{s}\,|\,z_{t},x_{0}) =}(-z_{t}}{1-z_{t}}; (_{s}-_{t})x_{0},(1-_{s}x_{0})),\] (8)

and then takes the products of them with their corresponding marginals, given by (3) and (4), respectively, to show that the PDF of the joint distribution defined in (5) and that defined in (6) are both equal to the PDF of \(q(z_{s},z_{t}\,|\,x_{0})\) defined in (2). The detailed proof is provided in Appendix B. To ensure numerical accuracy, we will calculate \(z_{s}=z_{t}+(1-z_{t})p_{s t}\) in (6) in the logit space as

\[(z_{s})=e^{(z_{t})}+e^{(p_{s t})}+e^{(z_{t})+(p_{s t})} .\] (9)

### Continuous Beta Diffusion

Forward Beta Diffusion.We can use the conditional bivariate beta distribution to construct a forward beta diffusion chain, beginning with the beta distribution from (3) and proceeding with the scaled beta distribution from (7). The marginal at any time \(t\) for a given data observation \(x_{0}\), as shown in (4), stays as beta-distributed in the forward chain. For the beta distribution given by (4), we have

\[[z_{t}\,|\,x_{0}]=_{t}x_{0},\ \ [z_{t}\,|\,x_{0}]= x_{0})(1-_{t}x_{0})}{+1},\ \ _{t}=[z_{t}\,|\,x_{0}]}{ [z_{t}\,|\,x_{0}]}^{2}=x_{0}(+1)}{1- _{t}x_{0}}.\]

Thus when \(_{t}\) approaches 0 (\(i.e.\), \(t 1\)), both \(z_{t}\) and \(_{t}\) are shrunk towards 0, and if \(_{1}=0\), we have \(z_{1}(0,)\), a degenerate beta distribution that becomes a unit point mass at 0.

**Infinite Divisibility.** We consider beta diffusion as a form of continuous diffusion, as its forward chain is infinitely divisible given \(x_{0}\). This means that for any time \(k(s,t)\), we can perform forward diffusion from \(z_{s}\) to \(z_{t}\) by first setting \(z_{k}=z_{s}_{s k}\), where \(_{s k}(_{k}x_{0},(_{k}-_{s})x_ {0})\), and then setting \(z_{t}=z_{k}_{k t}\), where \(_{k t}(_{t}x_{0},(_{t}-_{k})x_ {0})\). The same approach can be used to show the infinite divisibility of reverse beta diffusion given \(x_{0}\).

**Reverse Beta Diffusion.** We follow Gaussian diffusion to use \(q(z_{s}\,|\,z_{t},x_{0})\) to help define \(p_{}(z_{s}\,|\,z_{t})\). To construct a reverse beta diffusion chain, we will first need to learn how to reverse from \(z_{t}\) to \(z_{s}\), where \(s<t\). If we know the \(x_{0}\) used to sample \(z_{t}\) as in (3), then we can readily apply the conditional in (8) to sample \(z_{s}\). Since this information is unavailable during inference, we make a weaker assumption that we can exactly sample from \(z_{t} q(z_{t})=_{x_{0} p_{data}}[q(z_{t}\,|\,x_{0})]\), which is the "\(x_{0}\)-free" univariate marginal at time \(t\). It is straightforward to sample during training but must be approximated during inference. Under this weaker assumption on \(q(z_{t})\), utilizing (8) but replacing its true \(x_{0}\) with an approximation \(_{0}=f_{}(z_{t},t)\), where \(f_{}\) denotes the learned generator parameterized by \(\), we introduce our "\(x_{0}\)-free" and hence "causal" time-reversal distribution as

\[p_{}(z_{s}\,|\,z_{t})=q(z_{s}\,|\,z_{t},_{0}=f_{ }(z_{t},t)).\] (10)

### Optimization via KLUBs and Log-Beta Divergence

KLUB for Time Reversal.The time-reversal distribution \(p_{}(z_{s}\,|\,z_{t})\) reaches its optimal when its product with \(q(z_{t})\) becomes equivalent to \(q(z_{s},z_{t})=_{x_{0} p_{data}}[q(z_{s},z_{t}\,|\,x_{0})]\), which is a marginal bivariate distribution that is "\(x_{0}\)-free." Thus we propose to optimize \(\) by minimizing \((p_{}(z_{s}\,|\,z_{t})q(z_{t})||q(z_{s},z_{t}))\) in theory but introduce a surrogate loss in practice:

**Lemma 2** (Klub (conditional)).: _The KL divergence from \(q(z_{s},z_{t})\) to \(p_{}(z_{s}\,|\,z_{t})q(z_{t})\), two "\(x_{0}\)-free" joint distributions defined by forward and reverse diffusions, respectively, can be upper bounded:_

\[(p_{}(z_{s}\,|\,z_{t})q(z_{t})||q(z_{s},z_{t})) _{s,t}=_{(z_{t},x_{0}) q(z_{t}\,|\,x_ {0})p_{data}(x_{0})}[(s,z_{t},x_{0})],\] (11) \[(s,z_{t},x_{0}) =(q(z_{s}\,|\,z_{t},_{0}=f_{}(z_{t},t))||q(z _{s}\,|\,z_{t},x_{0}))].\] (12)

The proof in Appendix B utilizes the equation \(p_{}(z_{s}\,|\,z_{t})q(z_{t})=_{x_{0} p_{data}}[p_{ }(z_{s}\,|\,z_{t})q(z_{t}\,|\,x_{0})]\) and then applies the convexity of the KL divergence  and the definition in (10).

Log-Beta DivergenceTo find out the optimal solution under KLUB, following (1), we can express \((s,z_{t},x_{0})\) given by (12) as a log-beta divergence as

\[D_{ B(a,b)}[(_{s}-_{t})x_{0}, (1-_{s}x_{0})],\ [(_{s}-_{t})f_{}(z_{t},t), (1-_{s}f_{}(z_{t},t))]}.\] (13)

We note \(_{s,t}\) defined in (11) can also be written as \(_{z_{t} q(z_{t})}_{x_{0} q(x_{0}\,|\,z_{t})}[ (s,z_{t},x_{0})]\), where the log-beta divergence for \((s,z_{t},x_{0})\), defined as in (13), includes \(x_{0} q(x_{0}\,|\,z_{t})\) in its first argument and the generator \(f_{}(z_{t},t)\) in its second argument. Therefore, applying Proposition 1 of Banerjee et al. , we have the following Lemma.

**Lemma 3**.: _The objective \(_{s,t}\) defined in (11) for any \(s<t\) is minimized when_

\[f_{^{*}}(z_{t},t)=[x_{0}\,|\,z_{t}]=_ {x_{0} q(x_{0}\,|\,z_{t})}[x_{0}]z_{t} q(z_{t}).\] (14)

Thus under the \(_{s,t}\)-optimized \(^{*}\), we have \(p_{^{*}}(z_{s}\,|\,z_{t})=q(z_{s}\,|\,z_{t},[x_{0}\,|\,z_{t}])\), which is different from the optimal solution of the original KL loss in (11), which is \(p_{}^{*}(z_{s}\,|\,z_{t})=q(z_{s},z_{t})/q(z_{t})=q(z_{s}\,|\,z_{t})= _{x_{0} q(x_{0}\,|\,z_{t})}[q(z_{s}\,|\,z_{t},x_{0})]\). It is interesting to note that they only differ on whether the expectation is carried out inside or outside the conditional posterior.

In practice, we need to control the gap between \(p_{^{*}}(z_{s}\,|\,z_{t})\) and \(q(z_{s}\,|\,z_{t})\) and hence \(s\) needs to be close to \(t\). Furthermore, the assumption of having access to unbiased samples from the true marginal \(q(z_{t})\) is also rarely met. Thus we need to discretize the time from 1 to \(t\) into sufficiently fine intervals and perform time-reversal sampling over these intervals. Specifically, we can start with \(z_{1}=0\) and iterate (10) over these intervals to obtain an approximate sample from \(z_{t} q(z_{t})\). However, the error could accumulate along the way from \(z_{1}\) to \(z_{t}\), to which we present a solution below.

KLUB for Error Accumulation ControlTo counteract error accumulation during time reversal, we propose to approximate the true marginal \(q(z^{}_{t})\) using a "distribution-cycle-consistency" approach. This involves feeding a random sample \(z_{t}\) from \(q(z_{t})\) into the generator \(f_{}\), followed by the forward marginal \(q(z^{}_{t}\,|\,_{0}=f_{}(z_{t},t))\), with the aim of recovering the distribution \(q(z^{}_{t})\) itself. Specifically, we propose to approximate \(q(z^{}_{t})\) with \(p_{}(z^{}_{t}):=_{z_{t} q(z_{t})}[q(z^{}_{t}\,| \,_{0}=f_{}(z_{t},t))]\) by minimizing \((p_{}(z^{}_{t})||q(z^{}_{t}))\) in theory, but introducing a surrogate loss in practice:

**Lemma 4** (Klub (marginal)).: _The KL divergence \((p_{}(z^{}_{t})||q(z^{}_{t}))\) can be upper bounded:_

\[(p_{}(z^{}_{t})||q(z^{}_{t})) {KLUB}_{t}=_{(z_{t},x_{0}) q(z_{t}\,|\,x_{0})p_{data}(x_{0})}[ (z_{t},x_{0})],\] (15) \[(z_{t},x_{0})=(q(z^{}_{t}\,|\,f_{ }(z_{t},t))||q(z^{}_{t}\,|\,x_{0})).\] (16)

The proof in Appendix B utilizes the fact that \(q(z^{}_{t})=_{(z_{t},x_{0}) q(z_{t}\,|\,x_{0})p_{data}(x_{0} )}[q(z^{}_{t}\,|\,x_{0})]\).

Note that the mathematical definition of KLUB is reused throughout the paper and can refer to any of the equations (11), (12), (15), or (16) depending on the context. Similar to previous analysis, we have

\[(z_{t},x_{0})=D_{ B(a,b)}[_{ t}x_{0},(1-_{t}x_{0})],\ [_{t}f_{}(z_{t},t),(1-_{t}f_{}(z_{t},t)) ]},\] (17)

and can conclude with the following Lemma.

**Lemma 5**.: \(_{t}\) _in (15) is optimized when the same optimal solution given by (14) is met._

Optimization via KLUBsWith the two KLUBs for both time reversal and error accumulation control, whose optimal solutions are the same as in (14), we are ready to optimize the generator \(f_{}\) via stochastic gradient descent (SGD). Specifically, denoting \(,\) as two weight coefficients, the loss term for the \(i\)th data observation \(x_{0}^{(i)}\) in a mini-batch can be computed as

\[_{i}=(s_{i},z_{t_{i}},x_{0}^{(i)})+(1-) {KLUB}(z_{t_{i}},x_{0}^{(i)}),\] (18) \[z_{t_{i}} q(z_{t_{i}}\,|\,x_{0}^{(i)})=( _{t_{i}}x_{0}^{(i)},(1-_{t_{i}}x_{0}^{(i)})),\ \ s_{i}= t_{i}, t_{i}(0,1).\]

### Discretized Beta Diffusion for Generation of Range-Bounded Data

For generating range-bounded data, we discretize the beta diffusion chain. Denote \(z_{t_{0}}=1\) and let \(t_{j}\) increase with \(j\). Repeating (5) for \(T\) times, we define a discretized forward beta diffusion chain:

\[q(z_{t_{1:T}}\,|\,x_{0})=_{j=1}^{T}q(z_{t_{j}}\,|\,z_{t_{j-1}},x_{0})=_ {j=1}^{T}}}(}}{z_{t_{j-1}}}; _{t_{j}}x_{0},(_{t_{j-1}}-_{t_{j}})x_{0}).\] (19)

A notable feature of (19) is that the marginal at any discrete time step \(t_{j}\) follows a beta distribution, similarly defined as in (4). We also note while \(q(z_{t_{1:T}}\,|\,x_{0})\) defines a Markov chain, the marginal

\[q(z_{t_{1:T}})=_{x_{0} p_{data}(x_{0})}[q(z_{t_{1:T}}\,|\,x_{0})]\] (20)

in general does not. Unlike in beta diffusion, where the transitions between \(z_{t}\) and \(z_{t-1}\) are applied multiplicatively, in Gaussian diffusion, the transitions between \(z_{t}\) and \(z_{t-1}\) are related to each other additively and \(z_{t_{1:T}}\) forms a Markov chain regardless of whether \(x_{0}\) is marginalized out.

The discretized forward beta diffusion chain given by (19) is reversible assuming knowing \(x_{0}\). This means given \(x_{0}\), it can be equivalently sampled in reverse order by first sampling \(z_{t_{T}} q(z_{t_{T}}\,|\,x_{0})=(_{t_{T}}x_{0}, (1-_{t_{T}}x_{0}))\) and then repeating (8) for \(t_{T},,t_{2}\), with PDF \(q(z_{t_{1:T}}\,|\,x_{0})=q(z_{t_{T}}\,|\,x_{0})_{j=2}^{T}}}(}-z_{t_{j}}}{1-z_{t_{j}}};(_ {t_{j-1}}-_{t_{j}})x_{0},(1-_{t_{j-1}}x_{0})).\) This non-causal chain, while not useful by itself, serves as a blueprint for approximate generation.

Specifically, we approximate the marginal given by (20) with a Markov chain in reverse order as

\[p_{}(z_{t_{1:T}})=p_{prior}(z_{t_{T}})_{j=2}^{T}p_{}(z_{t_{j- 1}}\,|\,z_{t_{j}})=p_{prior}(z_{t_{T}})_{j=2}^{T}q(z_{t_{j-1}}\,|\,z_{t_{j }},\,f_{}(z_{t_{j}},_{t_{j}})).\] (21)

To start the reverse process, we choose to approximate \(q(z_{t_{T}})=_{x_{0} p_{data}(x_{0})}[q(z_{t_{T}}\,|\,x_{0})]\) with \(p_{prior}(z_{t_{T}})=q(z_{t_{T}}\,|\,[x_{0}])\), which means we let \(z_{t_{T}}(_{t_{T}}[x_{0}],(1-_{t_ {T}}[x_{0}]))\). To sample \(z_{t_{1:T-1}}\), we use the remaining terms in (21), which are scaled and shifted beta distributions that are specified as in (8) and can be sampled as in (6) and (9).

KLUB for Discretized Beta Diffusion.An optimized generator is expected to make the "\(x_{0}\)-free" joint distribution over all \(T\) steps of the discretized reverse beta diffusion chain, expressed as \(p_{}(z_{t_{1:T}})\), to approach that of the discretized forward chain, expressed as \(q(z_{t_{1:T}})\). Thus an optimized \(\) is desired to minimize the KL divergence \((p_{}(z_{t_{1:T}})||q(z_{t_{1:T}}))\). While this KL loss is in general intractable to compute, it can also be bounded using the KLUB shown as follows.

**Lemma 6** (KLUB for discretized diffusion chain).: \((p_{}(z_{t_{1:T}})||q(z_{t_{1:T}}))\) _is upper bounded by_

\[=_{x_{0} p_{data}(x_{0})}[(p _{prior}(z_{t_{T}})||q(z_{t_{T}}\,|\,x_{0}))]+_{j=2}^{T}}_{t_{j-1},t_{j}}\] (22) \[}_{s,t}=_{(z_{t},x_{0}) p_{ }(z_{t}\,|\,x_{0})p_{data}(x_{0})}[(s,z_{t},x_{0})],\ \ (s,z_{t},x_{0})=(p_{}(z_{s}\,|\,z_{t})||q(z_{s}\,|\,z_{ t},x_{0}))].\]

We provide the proof in Appendix B. We note Lemma 6 is a general statement applicable for any diffusion models with a discrete forward chain \(q(z_{t_{1:T}}\,|\,x_{0})=_{j=1}^{T}q(z_{t_{j}}\,|\,z_{t_{j-1}},x_{0})\) and a discrete reverse chain \(p_{}(z_{t_{1:T}})=p_{prior}(z_{t_{T}})_{j=2}^{T}p_{}(z_{t_{j-1 }}\,|\,z_{t_{j}})\). To estimate the KLUB in (22), however, during training, one would need to sample \(z_{t_{j}} p_{}(z_{t_{j}}\,|\,x_{0}) p(x_{0}\,|\,z_{t_{j}})p_{ }(z_{t_{j}})\), which is often infeasible. If we replace \(z_{t_{j}} p_{}(z_{t_{j}}\,|\,x_{0})\) with \(z_{t_{j}} q(z_{t_{j}}\,|\,x_{0})\), then \(}_{s,t}\) becomes the same as \(_{s,t}\) given by (11), and \(_{t}\) given by (15) can be considered to remedy the impact of approximating \(p_{}(z_{t_{j}})\) with \(q(z_{t_{j}})\). Therefore, we can consider the combination of \(_{s,t}\) given by (11) and \(_{t}\) given by (15) as a computationally viable solution to compute \(}_{s,t}\), which hence justifies the use of the loss in (18) to optimize the discretized reverse beta diffusion chain. We summarize the training and sampling algorithms of beta diffusion in Algorithms 1 and 2, respectively.

### Argument-swapped KLUBs and Negative ELBOs

We note that in theory, instead of the KL divergences in (11) and (15), \(f_{}\) can also be optimized using two argument-swapped KL divergences: \((q(z_{t},z_{s})||p_{}(z_{s}|z_{t})q(z_{t}))\) and \((q(z_{t}^{})||p_{}(z_{t}^{}))\). By the same analysis, these KL divergences can also be bounded by KLUBs and log-beta divergences that are equivalent to the previous ones, but with swapped arguments. The argument-swapped KLUBs and log-beta divergences will be shown to be closely related to optimizing a discretized beta reversediffusion chain via \(-\)ELBO, but they do not guarantee an optimal solution that satisfies (14) in beta diffusion and are found to provide clearly inferior empirical performance.

Negative ELBO for Discretized Beta Diffusion.As an alternative to KLUB, one can also consider following the convention in diffusion modeling to minimize the negative ELBO, expressed as

\[-_{x_{0} p_{data}(x_{0})} p_{}(x_{0})- =_{x_{0} p_{data}(x_{0})}_{q(z_{t_{1:T}}\,| \,x_{0})}[-\,|\,z_{t_{1:T}})p_{}(z_{t_{1:T}})}{q(z_ {t_{1:T}}\,|\,x_{0})}]\\ =-_{x_{0}}_{q(z_{t_{1}}\,|\,x_{0})} p(x_{0} \,|\,z_{t_{1}})+_{x_{0}}[q(z_{t_{T}}\,|\,x_{0})||p_{prior}(z _{t_{T}})]+_{j=2}^{T}_{x_{0}}_{q(z_{t_{j}}\,|\,x_{0})} [L(t_{j-1},z_{t_{j}},x_{0})],\] (23)

where the first two terms are often ignored and the focus is placed on the remaining \(T-2\) terms as

\[L(t_{j-1},z_{t_{j}},x_{0})=(q(z_{t_{j-1}}\,|\,z_{t_{j}}, x_{0})||q(z_{t_{j-1}}\,|\,z_{t_{j}},_{0}=f_{}(z_{t_{j}},t_{j})))\\ =D_{ B(_{i})}[(_{t_{j-1}}-_{t_{j} })f_{}(z_{t_{j}},t_{j}),(1-_{t_{j-1}}f_{}(z_{t_{j}},t_{j }))],\ \ [(_{t_{j-1}}-_{t_{j}})x_{0},(1-_{t_{j-1}} x_{0})]}.\] (24)

**Lemma 7** (\(-\)ELBO and argument-swapped KLUB).: _Optimizing the generator \(f_{}\) with \(-\)ELBO is equivalent to using an upper-bound for the augment-swapped KL divergence \((q(z_{t_{1:T}})||p_{}(z_{t_{1:T}}))\)._

The proof in Appendix B relies on the convex nature of both the KL divergence and the negative logarithmic function. We find for beta diffusion, optimizing \((p_{}(z_{t_{1:T}}))\|q(z_{t_{1:T}}))\) via the proposed KLUBs is clearly preferred to optimizing \((q(z_{t_{1:T}})||p_{}(z_{t_{1:T}}))\) via \(-\)ELBOs (\(i.e.,\) augment-swapped KLUBs) and leads to stable and satisfactory performance.

KLUBs and (Weighted) Negative ELBOs for Gaussian Diffusion.We note KLUBs are directly applicable to Gaussian diffusion, but they may not result in new optimization algorithms for Gaussian diffusion that drastically differ from the weighted ELBO, which weighs the KL terms using the corresponding SNRs. Moreover, whether the default or argument-swapped KLUBs are used typically does not make any difference in Gaussian diffusion and would result in the same squared error-based Bregman divergence. We provide the derivation of the (weighted) ELBOs from the lens of KLUBs in Appendix C, providing theoretical support for Gaussian diffusion to use the SNR weighted ELBO [23; 35; 20], which was often considered as a heuristic but crucial modification of ELBO.

## 3 Related Work, Limitations, and Future Directions

Various diffusion processes, including Gaussian, categorical, Poisson, and beta diffusions, employ specific distributions in both forward and reverse sampling. Gaussian diffusion starts at \((0,1)\) in its reverse process, while both Poisson and beta diffusion start at \(0\). Beta diffusion's reverse sampling is a monotonically non-decreasing process, similar to Poisson diffusion, but while Poisson diffusion takes count-valued discrete jumps, beta diffusion takes probability-valued continuous jumps. A future direction involves extending beta diffusion to encompass the exponential family [6; 65; 45; 7].

Several recent works have explored alternative diffusion processes closely related to Gaussian diffusion. Cold diffusion by Bansal et al.  builds models around arbitrary image transformations instead of Gaussian corruption, but it still relies on \(L_{1}\) loss, resembling Gaussian diffusion's squared Euclidean distance. Rissanen et al.  propose an inverse heat dispersion-based diffusion process that reverses the heat equation using inductive biases in Gaussian diffusion-like models. Soft diffusion by Daras et al.  uses linear corruption processes like Gaussian blur and masking. Blurring diffusion by Hoogeboom and Salimans  shows that blurring (or heat dissipation) can be equivalently defined using a Gaussian diffusion process with non-isotropic noise and proposes to incorporate blurring into Gaussian diffusion. These alternative diffusion processes share similarities with Gaussian diffusion in loss definition and the use of Gaussian-based reverse diffusion for generation. By contrast, beta diffusion is distinct from all of them in forward diffusion, training loss, and reverse diffusion.

A concurrent work by Avdeyev et al.  utilizes the Jacobi diffusion process for discrete data diffusion models. Unlike Gaussian diffusion's SDE definition, the Jacobi diffusion process in Avdeyev et al.  is defined by the SDE \(dx=[a(1-x)-bx]dt+dw\), with \(x\) and \(s,a,b>0\). The stationary distribution is a univariate beta distribution \((a,b)\). Beta diffusion and the Jacobi process are related to the beta distribution, but they differ in several aspects: Beta diffusion ends its forward process at \((0,)\), a unit point mass at \(0\), not a \((a,b)\) random variable. The marginal distribution of beta diffusion at time \(t\) is expressed as \(q(z_{t}\,|\,x_{0})(_{t}x_{0},(1-_{t}x_{0}))\), while the Jacobi diffusion process involves an infinite sum. Potential connections between beta diffusion and the Jacobi process under specific parameterizations are worth further investigation.

Several recent studies have been actively exploring the adaptation of diffusion models to constrained scenarios [13; 40; 41; 17], where data is bounded within specific ranges or constrained to particular manifolds. These approaches are all rooted in the framework of Gaussian diffusion, which involves the incorporation of additive Gaussian noise. In sharp contrast, beta diffusion introduces a distinct perspective by incorporating multiplicative noise, leading to the emergence of an inherently hypercubic-constrained diffusion model that offers new development and exploration opportunities.

Classifier-free guidance (CFG), often used in conjunction with heuristic clipping, is a widely used technique to perform conditional generation with Gaussian diffusion [22; 41]. Beta diffusion offers the potential for a seamless integration of CFG by directly applying it to the logit space, the operating space of its \(f_{}\) network, thereby eliminating the necessity for heuristic clipping.

To adapt Gaussian diffusion for high-resolution images, a common approach is to perform diffusion within the latent space of an auto-encoder [53; 62]. A promising avenue to explore is the incorporation of sigmoid or tanh activation functions in the encoder's final layer. This modification would establish a bounded latent space conducive to applying beta diffusion, ultimately leading to the development of latent beta diffusion tailored for high-resolution image generation.

One limitation of beta diffusion is that its training is computationally expensive and data-intensive, akin to Gaussian diffusion. Specifically, with four Nvidia RTX A5000 GPUs, beta diffusion and Gaussian diffusion (VP-EDM) both take approximately 1.46 seconds to process 1000 images of size \(32 32 3\). Processing 200 million CIFAR-10 images, the default number required to reproduce the results of VP-EDM, would thus take over 80 hours. Several recent works have explored different techniques to make Gaussian diffusion faster and/or more data efficient in training [68; 20; 76; 71]. It is worth exploring how to adapt these methods to enhance the training efficiency of beta diffusion.

Beta diffusion has comparable sampling costs to Gaussian diffusion with the same NFE. However, various methods have been developed to accelerate the generation of Gaussian diffusion, including combining it with VAEs, GANs, or conditional transport  for faster generation [72; 47; 78; 69], distilling the reverse diffusion chains [43; 55; 76], utilizing reinforcement learning , and transforming the SDE associated with Gaussian diffusion into an ODE, followed by fast ODE solvers [58; 39; 42; 75; 34]. Given these existing acceleration techniques for Gaussian diffusion, it is worth exploring their generalization to enhance the sampling efficiency of beta diffusion.

Beta diffusion raises concerns regarding potential negative societal impact when trained on image datasets curated with ill intentions. This issue is not exclusive to beta diffusion but applies to diffusion-based generative models as a whole. It is crucial to address how we can leverage these models for the betterment of society while mitigating any potential negative consequences.

## 4 Experiments

The training and sampling algorithms for beta diffusion are described in detail in Algorithms 1 and 2, respectively, in the Appendix. Our experiments, conducted on two synthetic data and the CIFAR10 images, primarily aim to showcase beta diffusion's effectiveness in generating range-bounded data. We also underscore the superiority of KLUBs over negative ELBOs as effective optimization objectives for optimizing beta diffusion. Additionally, we highlight the differences between beta and Gaussian diffusion, specifically in whether the data are generated through additive or multiplicative transforms and their ability to model the mixture of range-bounded distributions with disjoint supports.

We compare the performance of "Gauss ELBO," "Beta ELBO," and "Beta KLUB," which respectively correspond to a Gaussian diffusion model optimized with the SNR weighted negative ELBO [23; 35], a beta diffusion model optimized with the proposed KLUB loss defined in (18) but with the two arguments inside each KL term swapped, and a beta diffusion model optimized with the proposed KLUB loss defined in (18). On CIFAR-10, we also evaluate beta diffusion alongside a range of non-Gaussian or Gaussian-like diffusion models for comparison.

### Synthetic Data

We consider a discrete distribution that consists of an equal mixture of five unit point masses located at \(x_{0}=\{1/7,2/7,3/7,4/7,5/7\}\). We would like to highlight that a unit point mass can also be seen as an extreme case of range-bounded data, where the range is zero. Despite being simple,this data could be challenging to model by a continuous distribution, as it would require the generator to concentrate its continuous-valued generations on these five discrete points.

We follow previous works to choose the beta linear diffusion schedule as \(_{t}=e^{-_{d}t^{2}-_{}t}\), where \(_{d}=19.9\) and \(_{}=0.1\). This schedule, widely used by Gaussian diffusion [23; 61; 34], is applied consistently across all experiments conducted on synthetic data. We set \(=10000\), \(=0.95\), and \(=0.5\). As the data already falls within the range of 0 to 1, necessitating neither scaling nor shifting, we set \(S_{cale}=1\) and \(S_{lift}=0\). We use the same structured generator \(f_{}\) for both Gaussian and beta diffusion. We choose 20-dimensional sinusoidal position embeddings , with the positions set as \(1000t\). The network is an MLP structured as (21-256)-ReLU-(256-256)-ReLU-(256-1). We utilize the Adam optimizer with a learning rate of \(5\)e-\(4\) and a mini-batch size of 1000.

For data generation, we set \(=200\). We provide the generation results in Figure 3, which shows the true probability mass function (PMF) of the discrete distribution and the empirical PMFs over 100 equal-sized bins between 0 and 1. Each empirical PMF is computed based on 100k random data points generated by the model trained after 400k iterations. It is clear from Figure 3 that "Gauss ELBO" is the worst in terms of mis-aligning the data supports and placing its data into zero-density regions; "Beta ELBO" is the worst in terms of systematically overestimating the density at smaller-valued supports; whereas "Beta KLUB" reaches the best compromise between accurately identifying the data supports and maintaining correct density ratios between different supports.

In Appendix E, we further provide quantitative performance comparisons between different diffusion models and conduct an ablation study between KLUB and its two variants for beta diffusion: "KLUB Conditional" and "KLUB Marginal," corresponding to \(=1\) and \(=0\), respectively, in the loss given by (18). Additionally, we evaluate beta diffusion on another synthetic data, which comes from a mixture of range-bounded continuous distributions and point masses supported on disjoint regions.

### Experiments on Image Generation

We employ the CIFAR-10 dataset and build upon VP-EDM  as the foundation of our codebase. Our initial foray into applying beta diffusion to generative modeling of natural images closely mirrors the settings of Gaussian diffusion, including the choice of the generator's network architecture. We introduce a sigmoid diffusion schedule defined as \(_{t}=1/(1+e^{-c_{0}-(c_{1}-c_{0})t})\), which has been observed to offer greater flexibility than the beta linear schedule for image generation. This schedule bears resemblance to the sigmoid-based one introduced for Gaussian diffusion [35; 29]. We configure the parameters for beta diffusion as follows: \(c_{0}=10\), \(c_{1}=-13\), \(S_{lift}=0.6\), \(S_{cale}=0.39\), \(=10000\), \(=0.99\), and \(=0.95\). We utilize the Adam optimizer with a learning rate of \(2\)e-\(4\). We use EDM's data augmentation approach, but restrict augmented images to a 0-1 range before scaling and shifting. We use the beta diffusion model trained on 200M images to calculate the FID . Explanations regarding the intuition behind these parameter selections can be located in Appendix G.

Below we provide numerical comparison of beta diffusion with not only Gaussian diffusion models but also alternative non-Gaussian or Gaussian-like ones. We also conduct an ablation study to compare KLUB and negative ELBO across different NFE, \(\), and \(B\). A broad spectrum of diffusion models is encompassed in Table 1, which shows that beta diffusion outperforms all non-Gaussian diffusion models on CIFAR10, including Cold Diffusion  and Inverse Heat Dispersion , as well as categorical and count-based diffusion models [2; 8; 10]. In comparison to Gaussian diffusion and Gaussian+blurring diffusion, beta diffusion surpasses VDM , Soft Diffusion , and Blurring Diffusion . While it may fall slightly short of DDPM , improved DDPM , TPDM+ , VP-EDM , it remains a competitive alternative that uses non-Gaussian based diffusion.

Figure 3: Comparison of the true PMF, marked in red square, and the empirical PMFs of three different methods—Gauss ELBO, Beta ELBO, and Beta KLUB— calculated over 100 equal-sized bins between 0 and 1. Each empirical PMF is marked in solid dot.

Table 2 presents a comparison between KLUB and negative ELBO-optimized beta diffusion across different NFE under two different mini-batch sizes \(B\). Table 3 in the Appendix includes the results under several different combinations of \(\) and \(B\). We also include Figure 4 to visually compare generated images under KLUB and negative ELBO. The findings presented in Tables 2 and 3 and Figure 4 provide further validation of KLUB's efficacy in optimizing beta diffusion.

As each training run takes a long time and FID evaluation is also time-consuming, we have not yet optimized the combination of these hyperparameters given the limit of our current computation resources. Thus the results reported in this paper, while demonstrating that beta diffusion can provide competitive image generation performance, do not yet reflect the full potential of beta diffusion. These results are likely to be further improved given an optimized hyperparameter setting or a network architecture that is tailored to beta diffusion. We leave these further investigations to our future work.

## 5 Conclusion

We introduce beta diffusion characterized by the following properties: 1) **Analytic Marginal:** Given a probability-valued data observation \(x_{0}(0,1)\), the distribution at any time point \(t\) of the forward beta diffusion chain, expressed as \(q(z_{t}\,|\,x_{0})\), is a beta distribution. 2) **Analytical Conditional:** Conditioning on a data \(x_{0}\) and a forward-sampled latent variable \(z_{t} q(z_{t}\,|\,x_{0})\), the forward beta diffusion chain can be reversed from time \(t\) to the latent variable at any previous time \(s[0,t)\) by sampling from an analytic conditional posterior \(z_{s} q(z_{s}\,|\,z_{t},x_{0})\) that follows a scaled and shifted beta distribution. 3) **KLUBs:** We introduce the combination of two different Kullback-Leibler Upper Bounds (KLUBs) for optimization and represent them under the log-beta Bregman divergence, showing that their optimal solutions of the generator are both achieved at \(f_{^{*}}(z_{t},t)=[x_{0}\,|\,z_{t}]\). We also establish the connection between augment-swapped KLUBs and (weighted) negative ELBOs for diffusion models. Our experimental results confirm the distinctive qualities of beta diffusion when applied to generative modeling of range-bounded data spanning disjoint regions or residing in high-dimensional spaces, as well as the effectiveness of KLUBs for optimizing beta diffusion.

   Diffusion Space & Model & FID (\(\)) \\   & DDPM  & 3.17 \\  & VDM  & 4.00 \\  & Improved DDPM  & 2.90 \\  & TDPM+  & 2.83 \\  & VP-EDM  & **1.97** \\   & Soft Diffusion  & 3.86 \\  & Blurring Diffusion  & 3.17 \\   & Cold Diffusion (image reconstruction)  & 80.08 (deblurring) \\  & Inverse Heat Dispersion  & 8.92 (inpainting) \\   & D3PM Gauss+Logistic  & 7.34 \\  & \(\)LDR-10  & 3.74 \\  Count & JUMP (Poisson Diffusion)  & 4.80 \\  Range-bounded & Beta Diffusion & 3.06 \\   

Table 1: Comparison of the FID scores of various diffusion models trained on CIFAR-10.

   Loss &  -ELBO \\ \(B\) \\  &  -ELBO \\ \(512\) \\  &  KLUB \\ \(288\) \\  &  KLUB \\ \(512\) \\  & 
 KLUB \\ \(288\) \\  \\ 
20 & 16.04 & 16.10 & 17.06 & 16.09 \\
50 & 6.82 & 6.82 & 6.48 & 5.96 \\
200 & 4.55 & 4.84 & 3.69 & 3.31 \\
500 & 4.39 & 4.65 & 3.45 & 3.10 \\
1000 & 4.41 & 4.61 & 3.38 & 3.08 \\
2000 & 4.50 & 4.66 & 3.37 & **3.06** \\   

Table 2: Comparing FID scores for KLUB and negative ELBO-optimized Beta Diffusion on CIFAR-10 with varying NFE under \(=10000\) and two different mini-batch sizes \(B\).

Figure 4: Uncurated randomly-generated images by beta diffusion optimized with \(-\)ELBO or KLUB with \(=10000\) and \(B=288\). The generation with \(=1000\) starts from the same random seed.