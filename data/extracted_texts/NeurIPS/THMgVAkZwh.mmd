# VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark

Han Huang\({}^{1,2}\)   Haitian Zhong\({}^{2}\)   Tao Yu\({}^{2}\)   Qiang Liu\({}^{2}\)

Equal contribution.   han.huang@cripac.ia.ac.cn   haiitian.zhong@cripac.ia.ac.cnCorresponding author.   qiang.liu@nlpr.ia.ac.cn

Shu Wu\({}^{2}\)   Liang Wang\({}^{2}\)   Tieniu Tan\({}^{2,3}\)

\({}^{1}\)University of Chinese Academy of Sciences (UCAS)

\({}^{2}\)New Laboratory of Pattern Recognition (NLPR),

State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS),

Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{3}\)Nanjing University

###### Abstract

Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large Vision-Language Model **K**nowledge **E**diting **B**enchmark, **VLKEB**, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.

## 1 Introduction

With the rapid advancement and widespread deployment of LLMs, knowledge editing has emerged as an important topic . The knowledge stored within LLMs can suffer from issues such as errors, deficiencies and obsolescence. Knowledge editing aims to efficiently correct and update this information while ensuring minimal impact on unrelated content. Numerous LLM editing methods have been proposed  and benchmarks have been established to assess these approaches. Metrics such as _Reliability_, _Generality_, _Locality_ and _Portability_ are commonly used  in these benchmarks. Reliability defines a reliable edit where the post-edit model produces the target answer for a given case. Generality requires the post-edit model to understand equivalent neighbors, such as rephrased sentences. Locality emphasizes localized editing, ensuring that the output of unrelated knowledge is retained. Portability evaluates whether models effectively apply edited knowledge to relevant content. These methods and benchmarks have greatly advanced the research in LLM knowledge editing.

In contrast, the task of knowledge editing for LVLMs has not been extensively studied. Currently, only one benchmark (MMEdit ) explores LVLM editing. This benchmark extends Reliability,Generality and Locality, and adapts several editing methods from LLMs to LVLMs. However, it has some limitations. First, it uses synthesized images in Generality evaluation, which are generated by Stable Diffusion  from image captions. This could result in content inconsistency with original images and side-effect of less accurate evaluations. Second, the lack of evaluation of Portability is a significant gap, as effective use of edited knowledge is crucial to deploying an edited model in realistic applications. Furthermore, the limited quantity of data in this field poses a challenge to the progress of LVLM editing; therefore, additional data can greatly benefit the development of this area.

To address these issues, we have developed a new _Large **V**ision-**L**anguage Model **K**nowledge **E**diting **B**enchmark**, named **VLEB**, designed to assess and improve the capabilities of knowledge editing methods in the field of LVLMs. An LVLM editing case using VLKEB is illustrated in Fig.1.

In our study, we source data from the multi-modal knowledge graph MMKG , which includes images linked to knowledge entities. The presence of multiple images for each entity in MMKG makes it practicable to selectively choose image pairs for assessing the Generality. During the selection process, we choose clear and representative image pairs, ensuring that each pair contains a same entity but presented in varied perspectives or appearances. For the Reliability test, corresponding entities of chosen images are employed. Subsequently, we filter similar images from the remaining entities (none of their images were chosen previously) to construct the image Locality test. We utilize GPT to generate questions and answers during data construction.

Another important advancement of our work is the extension of the Portability metric for more comprehensive evaluation. As shown in Fig.1, we sample relational triples \((s,r,o)\) of editing-involved entities from knowledge graphs to construct test examples. The edited entities serve as the subjects of these sampled triples, which are then used to generate portability questions.

Compared with the MMEdit benchmark, our benchmark offers several advantages and exhibits key differences. Firstly, our image selection prioritizes real images, mitigating potential flaws present in synthesized images. Secondly, we extend Portability evaluation which reveals the ability of methods to make edited model effectively use edited knowledge in related contents. Lastly, by using a multi-modal knowledge graph as our source distinctly associates each image with a specific entity, enhancing the clarity of what knowledge it carries, and is expansible by incorporating other knowledge in diverse relation triples as test cases. The key differences are presented in Tab.1.

In summary, our main contributions are as follows:

Figure 1: The image belongs to _”Wichita Falls”_ originally and the editing target is to make LVLM recognize it as _”Fort Smith”_. The answer from LVLM measures the edit **Reliability**. The **Generality** inputs are “rephrased” images (_i.e._ belong to the same entity but different in perspective or appearance) and rephrased questions. **Locality** inputs are unrelated images and questions. **Portability** inputs are generated from sampled triples containing editing entity _‘Fort Smith’_ from the knowledge graph.

* We introduce a new benchmark VLKEB, which is specifically designed for evaluating LVLM knowledge editing. The quality of data is guaranteed and it mitigates the challenge of limited data in this research area.
* Our work extends the pivotal metric of Portability into the field of LVLM knowledge editing, providing a more comprehensive assessment of the models' ability to transfer and apply edited knowledge effectively.
* We conducted experiments on various LVLMs using different editing methods. These experiments contribute valuable insights into the performance and limitations of existing knowledge editing approaches on LVLMs.

## 2 Related Works

LLM Editing BenchmarksThe widely used datasets for LLM editing are ZsRE  and COUNTERFACT . ZsRE utilizes reading-comprehension examples for relation-slot filling tasks sourced primarily from the WikiReading dataset . The COUNTERFACT dataset evaluates the ability of robustly learn new facts by challenging models with complex factual associations rather than simple lexical changes. The MQuAKE  dataset further evaluates knowledge editing generalization by focusing on multi-hop questions, which challenge models to navigate through interconnected information accurately. RippleEdits  complements this by testing the consistency of knowledge updates across related facts, highlighting the complexity of maintaining coherent knowledge.

LLM Editing MethodsAs the baseline, fine-tuning adjusts specific layers of language models or vision modules. Cutting-edge methods offer efficient solutions for knowledge updates. For example, MEND  leverages low-rank decomposition of gradients that enables rapid and targeted updates to knowledge and minimizes degradation on other inputs. SERAC  introduces a novel approach by integrating an explicit memory system. Using a scope classifier to determine the relevance of cached edits, SERAC ensures dynamic updates while maintaining the integrity of the base model. IKE (In-Context Knowledge Editing)  proposes an unsupervised retriever that constructs demonstrations to inject new factual knowledge without direct parameter updates. By ranking demonstrations based on their similarity to the editing target, IKE offers a scalable and efficient way to update information. Some of the LLM editing methods are adapted to LVLM in this study.

Large Vision-Language ModelsLVLMs involve aligning modalities during pre-training and refining response generation through instruction-based tuning, significantly improving their ability to handle complex multi-modal tasks. For instance, mPLUG-Owl  enhances multi-modal capabilities through a two-stage training approach and low-rank adaption , achieving superior performance. LLaVA  emphasizes pre-training and fine-tuning an alignment network alongside Vicuna, while Qwen-VL  introduces innovative features like a visual receptor and a three-stage training pipeline, excelling in visual-centric tasks and dialogue communication. We conduct experiments on various LVLMs to assess the performance of different editing methods.

    &  &  \\   & Rel/T-Gen/T-Loc & Yes & Yes \\  & I-Generality & AI-Generated, less controllable & Real images, manual check \\  & I-Locality & Random sample, easier evaluation & Filtered sample, harder evaluation \\  & Portability & No Evaluation & Yes \\   & Data Source & VQAv2 \& COCO Caption; No entity connections, No Portability & MMKG; extensive connections, enhanced Portability evaluation \\   & Image Quality & Real \& synthetic images; factual flaws observed in synthetic images & Real images in the datasets; quality guarantee, factually accurate \\   

Table 1: Differences between **MMEdit** and **VLKEB**. (Rel: Reliability; T-*: Text-*; I-*: Image-*.)

## 3 Dataset Construction

Problem FormulationAn LVLM editing dataset \(_{}=\{(i_{e},x_{e},y_{e},y^{}_{e})_{i}\}\) contains image input \(i_{e}\), text input \(x_{e}\), ground truth \(y_{e}\) and editing target \(y^{}_{e}\). Given \(i_{e}\) and \(x_{e}\), an unedited LVLM produces \(f(i_{e},x_{e};)=y_{e}\), where parameters \(=_{}_{}\). After knowledge editing, the edited LVLM with \(^{}\) are expected to successfully change the original outputs \(f(i_{e},x_{e};^{})=y^{}_{e}\).

### Metrics: Reliability, Generality, Locality and Portability

To evaluate the knowledge editing methods, key evaluation criteria from prior work  are Reliability, Generality and Locality. Besides, based on our benchmark, we introduced Portability metric.

**Reliability** measures the proportion of target answers that the edited model can produce correctly.

**Generality** assesses how well the model responds to neighboring concepts in two modalities.

**Locality** measures how much of the stored knowledge, unrelated to the edit cases, remains unchanged in the edited model by comparing the outputs of the unedited and edited models.

**Portability** Knowledge is interconnected, so modifying one fact affects related facts, complicating editing evaluation. Since model knowledge is not isolated, adjustments must consider broader implications. Portability evaluates if edited knowledge can be effectively applied to related content:

\[_{}=}_{(i _{e},x_{e},y_{e},y^{}_{e})_{}\\ (x_{p},y_{p})(i_{e},x_{e},y_{e},y^{}_{e} )}\{f(i_{e},x_{p};^{} )=y_{p}\},\]

where \(x_{p}\) and \(y_{p}\) are text inputs and outputs that belong to \((i_{e},x_{e},y_{e},y^{}_{e})\), which denotes the portability scope given the input \(i_{e},x_{e},y_{e}\), and target output \(y^{}_{e}\).

### Construction Process

PreparationThe raw data in MMKG are knowledge triples \((s,r,o)\), where s is the subject, r is the relation, and o is the object, with image URLs for each entity. We start from an image \(i\) and its corresponding entity \(e\). The chosen LVLM is edited to associate \(i\) with another entity \(e^{}\). The editing process forms an editing triple \((i,e e^{})\). In the example ([a picture of Messi], Messi\(\)Ronaldo), it aims to modify the knowledge within an LVLM to make it interpret the person in the picture as Ronaldo. We manually construct a prompt template \(t_{i}(e;)\) for an image \(i\) and its entity \(e\) and use GPT (see appendix) to generate a questions \(i\) with given relation set \(\) (could be empty), to which the answer is precisely \(e\).

Image SelectionWe start from image selection, recognizing the significance of image quality in evaluation. We retrieve the available images with URLs in MMKG and remove duplicates. We then utilize the pre-trained CLIP model  as an image feature extractor to assess similarities within each set of images corresponding to entities. Pairs of images with high similarity scores are then manually inspected to confirm that they belong to the same entity but differ in perspective or appearance.

Reliability, Generality and Locality Evaluation Data ConstructionWe choose Visual-Question-Answering (VQA) as the evaluation task. We start by using maximum weight bipartite matching to identify pairs of entities \((e,e^{})\) with the highest similarity based on their shared relations. These chosen pairs form a set of editing triples \((i,e e^{})\) as previously mentioned.

Next, we manually review generated QA to make sure the questions and answers match. For each entity and image, we generate two questions \(q_{1}\) and \(q_{2}\), which can be viewed as rephrase of each other. We put one in _Reliability_ set \(_{}=\{(i,q_{1},e,e^{})_{i}\}\) and another in _Text Generality_ set. For _Image Generality_ test, the previous image pair selection enables us to get "rephrased" image of edited one. For _Text Locality_, we follow MMEdit to choose NQ dataset  which contains unrelated QAs and randomly put them in \(^{text}_{}\). For _Image Locality_, we filter similar images for edit-involved images from unedited entities. We then generate questions and answers for each filtered image and entity. The test data in \(^{image}_{}\) are thus similar in both modalities to that in \(_{}\), making it more difficult for LVLM editing methods to perform well than random image locality test.

Portability Evaluation Data ConstructionAs mentioned, Portability represents whether editing methods can apply edited knowledge within the portability scope \((i_{e},x_{e},y_{e},y^{}_{e})\). For example, after performing the edit ([a picture of Messi], Messi\(\)Ronaldo), we expect the LVLM to understand that the individual in the image, now identified as C. Ronaldo, is playing in Saudi Arabia instead of the USA. This means the model can apply the edited knowledge to related questions.

During the construction of the portability data, we select the connected triples of an edited entity, forming one-hop reasoning portability queries. For example, given \((i,e e^{})\) and \((s_{1},r_{1},o_{1})\), where \(e^{}=s_{1}\), the connected triple could be formulated as \(((i,e e^{}),(s_{1},r_{1},o_{1}))\). We then collect and generate questions \(t_{i}(o_{1};r_{1})\) to form one-hop Portability evaluation dataset.

Nevertheless, Under more complex scenarios, it remains uncertain if an edited model can effectively process multi-hop reasoning tasks related to the edit. We suggest evaluating edited models with multi-hop reasoning VQA task by considering a connected chain of knowledge triples \(=(i,e e^{}),(s_{1},r_{1},o_{1} ),,(s_{n},r_{n},o_{n})\), where \(e^{}=s_{1}\) and the object of the \(i\)-hop knowledge should serve as the subject of the subsequent knowledge in the sequence, i.e., \(o_{i}=s_{i+1}\). We then similarly generate questions \(t_{i}(o_{n};r_{1},,r_{n})\) and incorporate them into the portability scope.

Dataset SummaryIn total, the dataset contains 8174 editing cases with 18434 images. We split them into train and test set of 5000 and 3174 cases respectively. See appendix for more details.

## 4 Experiments

We conduct experiments on five LVLMs, which are BLIP2 , MiniGPT-4 , mPLUG-Owl2 , Qwen-VL  and LLaVA-1.5 . The detailed versions of these LVLMs are in appendix. We test _single editing_ and _sequential editing_. _Single editing_ updates a single knowledge once at a time and then evaluates editing results. This is a common setting of knowledge editing. In contrast, _sequential editing_ continuously updates knowledge. Their difference is illustrated in Fig.2.

### Editing Methods and Experiment Settings

**Fine-tune (FT)** We choose different parts of models during FT: the LLM layers or vision module.

**Knowledge Editor (KE)** trains a bidirectional-LSTM as hyper network, which predicts weight updates of specified model parameters with gradients and condition input \(\{(y_{e} y^{}_{e})|x_{e}\}\}\). We choose last layers of LLMs as the parameters to be updated in editing process.

**IKE** does not change model parameters. It retrieves and builds similar demonstrations from training set, and inject new knowledge by prompting. This process is consistent across all models.

**SERAC** is a memory-based method that has a scope classifier model and a counterfactual model. In our experiments, the classifier is trained from a BERT model  and the counterfactual model differs across LVLMs, which is set to be the LLM used by corresponding LVLM.

**MEND** method enables models to efficiently update the parameters of the last layers in LLMs within LVLMs, utilizing low-rank gradient decomposition coupled with predictive parameter updates.

### Single Editing Results and Findings

**High Reliability and Generality: In single editing, memory-based methods benefit from having only one piece of new knowledge stored, while parameter-update methods fit the single new

Figure 2: In Fig.2, the single editing takes one edit at a time and evaluate immediately, while in Fig.2 the sequential editing involves continuous edits and test after several other edits.

piece of knowledge well.In Tab.2, most methods and models achieve nearly 100% accuracy in Reliability due to the single editing test setting. As each edit is separately tested and the test is right after each edit, memory-based methods like SERAC retrieve the single new knowledge (with the "highest" input similarity) and append it to the test inputs. Similarly, IKE appends the edit case as a "New Fact" prompt before each test. Parameter-update methods like FT, KE, and MEND fit the new knowledge at each edit, leading to high Reliability. Additionally, LLMs exhibit high Text Generality due to their ability to handle rephrased questions, and the well pre-trained vision encoder ensures high Image Generality by effectively handling similar images.

Varied Locality: Memory is a double-edged sword and parameter updates also harm models.SERAC exhibits divergent Locality in text and image in Tab.2, with nearly \(100\%\) in T-Loc but the highest I-Loc is merely \(4.5\%\). The causes of this phenomenon are twofold. First, Text Locality data is collected from another irrelevant dataset (Sec.3.2) which is totally different from the edit data; therefore, it is classified to have very low similarity and processed by the original model. Second, Image Locality involves similar texts to edit cases, causing SERAC to misclassify and forward them to the counterfactual model, producing different outputs against the original model. IKE always builds in-context examples and appends edit text as a new fact before the test, which greatly affects model outputs. IKE's lower I-Loc compared to T-Loc is due to appended edits misdirecting the models with

  
**Model** & **Method** & **Rel.\(\)** & **T-Gen.\(\)** & **I-Gen.\(\)** & **T-Loc.\(\)** & **I-Loc.\(\)** & **Port.\(\)** \\    } & FT (LLM) & **99.75** & 99.08 & 98.95 & 71.10 & 19.90 & 17.13 \\  & FT (Vis) & 99.33 & 96.68 & 99.13 & 99.99 & 5.30 & 27.22 \\   & KE & 94.45 & 92.40 & 93.34 & 64.13 & 12.22 & 34.73 \\  & IKE & 99.47 & **99.40** & **99.56** & 70.11 & 10.26 & **44.22** \\  & SERAC & 96.02 & 95.99 & 96.01 & **100.0** & 2.40 & 15.25 \\  & MEND & 98.52 & 98.42 & 98.47 & 99.34 & **89.05** & 28.80 \\    } & FT (LLM) & 99.60 & 98.72 & 98.10 & 90.17 & 35.39 & 27.13 \\  & FT (Vis) & **100.0** & 84.89 & 99.19 & **99.99** & 20.26 & 37.06 \\   & KE & 98.47 & 97.89 & 98.11 & 75.47 & 16.14 & 48.06 \\   & IKE & 99.98 & **99.68** & **99.98** & 59.25 & 9.73 & **54.30** \\   & SERAC & 99.37 & 97.30 & 99.29 & 99.93 & 4.54 & 49.22 \\   & MEND & 99.20 & 98.98 & 99.15 & 99.46 & **92.67** & 40.09 \\    } & FT (LLM) & 99.59 & 99.43 & 99.31 & 86.34 & 29.24 & 30.23 \\  & FT (Vis) & 99.80 & 99.12 & 97.55 & **99.99** & 18.79 & 54.43 \\   & KE & 99.07 & 97.59 & 98.65 & 77.36 & 15.25 & 48.62 \\   & IKE & **99.99** & 99.66 & **100.0** & 68.65 & 14.25 & **63.33** \\   & SERAC & 99.93 & **99.78** & 99.93 & 99.98 & 1.91 & 45.03 \\   & MEND & 99.54 & 99.21 & 99.52 & 99.36 & **90.14** & 40.39 \\    } & FT (LLM) & 97.92 & 96.30 & 95.48 & 72.80 & 37.23 & 16.15 \\  & FT (Vis) & **100.0** & 95.27 & 62.28 & **100.0** & 14.14 & 30.61 \\   & KE & 98.71 & 98.70 & 98.26 & 72.09 & 52.63 & 42.10 \\   & IKE & 99.01 & 98.85 & **99.01** & 57.97 & 10.48 & **57.99** \\   & SERAC & 97.62 & 95.68 & 97.84 & 99.85 & 0.81 & 38.22 \\   & MEND & 99.54 & **99.36** & 97.76 & 97.75 & **78.65** & 32.35 \\    } & FT (LLM) & 99.21 & 95.72 & 99.39 & 71.42 & 34.31 & 42.77 \\  & FT (Vis) & 97.24 & 96.36 & 82.39 & **99.99** & 50.14 & **74.09** \\    & KE & 89.10 & 88.37 & 88.62 & 55.80 & 21.07 & 46.82 \\    & IKE & **99.98** & **99.93** & **100.0** & 64.88 & 16.59 & 64.83 \\   & SERAC & 99.03 & 97.73 & 98.99 & 99.97 & 1.32 & 48.52 \\   & MEND & 98.65 & 98.15 & 94.26 & 99.56 & **90.47** & 37.68 \\   

Table 2: The single editing results of various editing methods applied to different LVLMs. Rel.: Reliability; T/I-Gen.: Text/Image Generality; T/I-Loc.: Text/Image Locality; Port.: Portabilityfalse facts. In FT, we observe that fine-tuning different parts yields distinct results. Fine-tuning vision modules (FT-Vis) has higher T-Loc than fine-tuning LLM layers (FT-LLM), indicating that FT-LLM has greater and more direct impact on model outputs as it modifies the final LLM layers. In contrast, FT-Vis has lower I-Loc in because it changes the vision encoder or projector parameters, affecting the visual ability to distinguish between similar images. Comparing MEND with KE, MEND has overall better locality performance. This could be attributed to two reasons: First, KE predicts parameter updates based on gradients and text condition input, while MEND relies directly on token-wise activations and gradients, providing richer information about which parameters are crucial for an update . Second, MEND includes a locality constraint during training which KE lacks, helping to maintain Locality. MEND shows the best average locality but it is imperfect as the outputs of edited models still differ somewhat from unedited ones.

Portability: Can the model effectively apply the edited knowledge?From the Portability column in Tab.2, we find that IKE generally achieves higher results than other methods, except in the case of mPLUG-Owl2 where FT-Vis has highest results. This demonstrates how prompting with examples and providing the necessary new knowledge can help models answer portability questions. In single editing, SERAC always appends edited knowledge before test, effectively acting as if IKE has only one demonstration. Consequently, SERAC shows inferior Portability results compared to IKE. In contrast, FT, KE and MEND which change parameters related to edits, do not account for the interconnected knowledge, resulting in generally poor results. The unsatisfactory outcomes suggest that these editing methods can not effectively utilize edited knowledge, as SERAC and IKE rigidly require edit texts as prompts, and FT, KE and MEND perform case-centric updates.

### Multi-hop Portability: Performance Degradation Across Hops

Our experiments in Sec.4.2 evaluate the performance of edited models on the one-hop Portability dataset. As described in Sec.3.2, the Portability dataset includes elements of multi-hop reasoning VQA tasks. Consequently, we also conduct experiments on the multi-hop Portability evaluation dataset to determine whether the edited models can utilize knowledge in more complex scenarios. As illustrated in Fig.3, we displayed the multi-hop Portability results using the relative change compared to base (unedited) model, _i.e_. Relative Change (%) \(=(-}{})\).

Portability Metrics Decline with Increasing Number of HopsIn our experiments, we observe a consistent decline in portability metrics as the number of hops increases. This decline is observed across nearly all models and editing methods. This phenomenon can be attributed to the escalating complexity of reasoning required to accurately apply the edited knowledge across multiple interconnected facts. Each additional hop introduces new intermediate entities and relationships, compounding the difficulty for the model to maintain correct and consistent reasoning.

Figure 3: Relative change (compared with unedited base model) of Multi-hop Portability results.

**IKE performs well across all models, while Fine-tuning generally performs poorly, especially Fine-tuning LLM heads.** Since IKE consistently builds in-context examples and appends edit text as a new fact before the test, it maintains a robust understanding of the edited knowledge across different scenarios. This method's strength lies in its ability to dynamically incorporate new information in a contextually relevant manner, which enhances its performance even in multi-hop reasoning tasks. The in-context learning approach allows IKE to adapt to the complexities introduced by additional hops, thereby preserving the accuracy and consistency of the edited knowledge.

In contrast, Fine-tuning LLM heads tends to perform poorly across all models. The process of fine-tuning involves directly modifying the model parameters, which can lead to overfitting on the edited examples and a failure to generalize to related but unedited contexts. As the number of hops increases, the fine-tuned models struggle to apply the edited knowledge accurately due to the rigid and localized nature of the parameter updates. Interestingly, while the fine-tuning of visual modules (FT-Vis) also performs suboptimally, it does not suffer as severely as the fine-tuning of LLM heads. The closer proximity of the LLM heads to the output layer increases the likelihood of overfitting, as the updates directly impact the final predictions.

### Sequential Editing: Performance Degradation Due to Forgetting and Confusion

Editing knowledge separately is impractical in real-world scenarios, as knowledge changes over time, necessitating continuous updates to our models. We test sequential editing (Fig.2b) on FT-LLM, FT-Vis, SERAC and MEND, while exclude KE and IKE because they require the edit data as part of the input during test time, which is not feasible in practical applications.

The results are in Fig.4.

MEND is not included in these figures because, after a certain number of edits, we observed "NaN" values in the logits of model outputs, indicating a collapse. For example, Blip2-OPT outputs "NaN" after 50 edits, MiniGPT-4 after around 14 edits and LLaVA after around 20 edits. MEND learns to predict parameter updates based on gradients of edits and parameters of original model during training. However, in sequential editing, the corresponding LLM parameters change continuously and differ from the unedited model in the training phase, making MEND unable to predict precise updates, ultimately leading to collapse. Similar findings were observed by Han et al. .

Apart from MEND, other methods exhibit different characteristics compared to single editing. **First, FT tends to forget previous edits as model parameters change.** The downward trends in Reliability and Generality for FT in Fig.4a4b4c indicate that preserving previous edits becomes increasingly difficult with a larger test gap. On the contrary, **SERAC does not forget due to explicit memory cache but becomes confused when memory is filled with more edits.** SERAC maintains consistency in Reliability and Image Generality because exact test texts are stored from previous edits. However,

Figure 4: Average results in sequential editing. Horizontal axis is the test gap number in Fig.2b.

Text Generality involves rephrased text that is not stored, challenging the similarity retrieval within SERAC and leading to incorrect results due to false retrievals.

Fig.4d shows that FT-Vis tends to preserve model outputs in unrelated test text, as it leaves the LLM layer unchanged, while FT-LLM does the opposite. In Fig.4e, both FT-Vis and FT-LLM exhibit low scores since the test texts are similar to edits, with FT-LLM declining further as the gap increases. These Locality results suggest that **parameter updates can harm the model, and updating the LLM has greater impact than updating the vision module**.

In Fig.4f, Portability also decreases with an increasing gap. As shown in Sec.4.3, the unedited model can predict some correct logits according to input texts through a forward pass. The average Portability of base models is \(36.80\%\), and the FT Portability drops lower after gap \(=10\), indicating that **edited models cannot effectively apply edited knowledge to related content**. Therefore, to investigate if Portability can be improved, we experiment with 1-hop portability in Sec.4.5.

### Edit One-Hop Knowledge: Room for Portability Improvement

In this tentative exploration, we consider editing the second triple (Sec.3.2) of Portability data (_i.e_. first edit \((i,e e^{})\) and then edit \((e^{},r,o)\)).We generate QAs for the triples and the model twice before conducting the Portability test, and the results are in Tab.3.The table displays Portability before and after additionally editing 1-hop knowledge, indicated by the arrows. All Portability results improved, with some showing huge gains. We investigate the improvement of SERAC by comparing retrieved results. We find that after editing 1-hop knowledge, some Portability test texts have higher similarity to 1-hop knowledge QA stored in memory. In these cases, the 1-hop knowledge is appended before the test text, leading to higher Portability. In other words, **SERAC does not actually apply the first edit \((i,e e^{})\) to the model but only the second triple \((e^{},r,o)\) that contains Portability answers**. This unintended behavior means that the models are not aware of the first edit and are indeed given a "cheat sheet" in Portability test. For FT and MEND, parameter updates fit the 1-hop knowledge and help the model provide more correct answers. **This may suggest that Portability can be improved by explicitly incorporating it into the training phase**. However, the challenges faced by these parameter-update methods in sequential editing still need careful consideration.

## 5 Conclusion, Limitation and Future Direction

We establish a knowledge editing benchmark for LVLMs and evaluate diverse editing methods across various models. Our analysis delves into the impact of these methods on the models, revealing both strengths and weaknesses. These findings offer valuable insights for potential future directions.

**Direct LVLM Editing** In this work, we evaluate LLM editing methods, but the the search for an efficient LVLM editing method is ongoing. These methods are adapted from LLM techniques and are not specifically designed for LVLMs, as they do not account for the interaction between modalities. research could explore direct LVLM editing methods to address this gap.

**Sequential Editing in LVLM** We have observed performance degradation across methods in sequential editing. Since this test closely mirrors real-world scenarios, future work on LVLM editing should focus on mitigating these issues.

**Portability Evaluation** Current methods do not adequately consider Portability, resulting in unsatisfactory evaluation results. Our preliminary experiments show that there is room for Portability improvement. Future research should further emphasize Portability as an important aspect.

  
**Portability** & **FT (LLM)** & **FT (Vis)** & **SERAC** & **MEND** \\ 
**Blip2-OPT** & 17.13\(\)46.71 (\(\) 29.58) & 27.22\(\)39.00 (\(\) 11.78) & 16.16\(\)32.12 (\(\) 15.96) & 28.75\(\)68.18 (\(\) 39.43) \\
**MiniGPT-4** & 27.13\(\)44.65 (\(\) 17.52) & 37.06\(\)56.91 (\(\) 19.85) & 47.49\(\)51.10 (\(\) 3.61) & 39.19\(\)53.83 (\(\) 14.64) \\
**LLaVA-1.5** & 30.23\(\)74.79 (\(\) 44.56) & 54.43\(\)84.81 (\(\) 30.38) & 45.03\(\)72.75 (\(\) 27.72) & 40.39\(\)70.73 (\(\) 30.34) \\
**Qwen-VL** & 16.15\(\)88.88 (\(\) 72.73) & 30.61\(\)55.15 (\(\) 24.54) & 38.22\(\)54.44 (\(\) 16.22) & 32.35\(\)69.41 (\(\) 37.06) \\
**mPLUG-Owl2** & 42.77\(\)59.37 (\(\) 16.60) & 74.09\(\)93.80 (\(\) 19.71) & 48.52\(\)67.91 (\(\) 19.39) & 37.68\(\)70.71 (\(\) 33.03) \\   

Table 3: Portability increases after additionally edit corresponding one-hop knowledge.