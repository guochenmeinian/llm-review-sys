# Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models

Alliot Nagle

UT Austin

&Adway Girish

EPFL

&Marco Bondaschi

EPFL

&Michael Gastpar

EPFL

&Ashok Vardhan Makkuva

EPFL

&Hyeji Kim

UT Austin

Equal contribution. Equal contribution. Equal contribution.

###### Abstract

We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create _hard prompts_ for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of _query-aware_ prompt compression, where the compressor has knowledge of the downstream task/query for the black-box LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose _Adaptive QuerySelect_, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.

## 1 Introduction

In spite of the recent success of transformer-based  large language models (LLMs) in language modeling tasks, inference calls to a transformer can be costly in both time and memory usage. Although significant progress has been made to improve the memory usage and runtime efficiency via implementation-level optimizations [2; 3; 4] and architecture-level optimizations and alternatives [5; 6; 7], a third type of optimization that compresses the input (an _input-level_ optimization) has the benefit that it directly reduces the resource usage of an LLM inference call, _and_ it can be used in conjunction with the other two types of optimizations for further efficiency gains. _In this work, we offer a framework and analysis for a recent body of literature in this direction, known as prompt compression_[8; 9; 10].

The goal of a prompt compression method is to transform a sequence of input tokens \(x\) into a shorter sequence of tokens \(m\) such that the response generated by a target LLM will semantically mean the same thing regardless of whether \(x\) or \(m\) is given as input. Using \(m\) as the input directly decreases the memory and runtime requirements necessary for an LLM inference call. Moreover, the additional benefits to this approach are: (1) redundant or superfluous tokens are removed, making room to fit more pertinent information in the target LLM's limited-size context window, (2) it can be used in addition to implementation and architecture-level optimizations to get further efficiency gains, and (3) it is the only technique available when seeking to lower costs for black-box API calls to closed-sourcemodels. This third point is particularly important, since the associated cost for a black-box API model inference call, from the perspective of the caller, is determined by the runtime and the number of input tokens, both which can be reduced with prompt compression. In our framework and analysis, we focus on the _prompt compression for black-box models_ setting, where the output of a prompt compression method is a set of tokens ("hard prompts") [11; 12; 13; 14], and exclude methods which output embedding vectors ("soft prompts") [15; 16; 17] as those are not transferable to black-box models.

Despite the progress in the prompt compression literature, there is a lack of proper formalization of this problem and there is no clear framework to unify these works. Most works propose methods that work well but offer no insight into key questions, such as _"How far are we from the theoretical limit of the rate-distortion trade-off?"_, _"How essential is the conditioning on the query when compressing the prompt?"_, and _"How does tokenization impact the performance of prompt compression methods?"_ We offer a unifying framework for the problem of prompt compression and seek to answer these questions with theory and experiments. Our main contributions can be summarized as follows.

1. _Theoretical analysis:_ We formalize the problem of prompt compression and formulate it as a rate-distortion problem (Sec. 3.1). We characterize the optimal trade-off between the rate of compression and the distortion incurred, i.e., the _distortion-rate function_, via a dual linear program, and provide a geometric algorithm to compute this optimal trade-off (Sec. 3.2, Sec. 3.3).
2. _Evaluation:_ We introduce a synthetic dataset with binary prompts and natural language queries, for which we can compute the distortion-rate function (Sec. 4.1), and compare and obtain insights on existing prompt compression algorithms as in Fig. 1 (Sec. 4.2). We further confirm our findings by extending our experiments to a small natural language dataset and NarrativeQA .
3. _Algorithm design:_ Our novel method, "Adaptive QuerySelect," a query-aware, variable-rate adaptation of LLMLingua-2 , outperforms all prompt compression methods on our datasets and has a rate-distortion curve that significantly reduces the gap with the theoretical limit (Sec. 4).

## 2 Background and related works

Long prompts slow the inference process due to the increase in the number of tokens for the LLM to process. It is also known that very long prompts can cause LLMs to "forget" parts of the input and produce erroneous answers . Therefore, studying how these prompts can be compressed is essential. As shown in Fig. 2, we wish to design a compressor that, upon receiving the prompt, produces a "compressed" version (which has fewer tokens than the prompt) called the _compressed

Figure 1: The distortion-rate trade-off of all prompt compression methods compared to the query-aware and query-agnostic theoretical limits on a synthetic dataset with binary prompts. All distortions are computed with the log loss **(left)** and 0/1 loss **(right)** distortion metrics formally defined in (1). We observe that (1) most existing methods are far from the theoretical limit, suggesting that there is still room for improvement in this field, (2) conditioning on the query allows for a significant improvement, as seen by the performance of the query-aware method QuerySelect against the query-agnostic LLMLingua-2 , and (3) our proposed method Adaptive QuerySelect, a query-aware and variable-rate adaptation of LLMLingua-2, achieves the best performance among all methods considered, and is the only method to outperform the optimal query-agnostic strategy.

_prompt_, such that a target LLM is able to give answers that are "close enough," per some appropriately chosen metric, to the ground truth. Though similar in spirit to text summarization, prompt compression has the advantage that the compressed prompt is not required to be human-readable.

All prompt compression methods belong to one of two groups: those that compress the prompt into _soft prompts_ and those that compress the prompt into _hard prompts_. In soft-prompt compression, the compressor is trained to transform the input prompt into a set of embedding vectors (sometimes referred to as "soft tokens") that do not map back into the token space. These methods, including Gist Tokens , AutoCompressor , and In-Context Auto-Encoder  are trained end-to-end and require specialized fine-tuning of the target LLM to interpret the soft prompt inputs.

In this work, we focus instead on methods that compress the prompt into hard prompts, where the compressor's output is a set of tokens. While it is technically feasible to fine-tune the target LLM in this setting, it is unnecessary and often avoided because the utility of this setting is compressing prompts for black-box models that are not fine-tuned. These methods often use either the target LLM, or a smaller and faster LLM, to compress the prompt. The basic idea behind all these methods is to identify the tokens that are "most relevant," per an appropriate metric, and retain as many of them in the compressed prompt as possible. These methods include Selective Context , LLMLingua , LLMLingua-2 , and LongLLMLingua . More details on these works can be found in Sec. 4.2. Precursors to the prompt compression works include text compression methods, which have the added constraint that the compressed text is human-readable [20; 21; 22]. Prompt compression methods are different from these in that the text only needs to be interpretable by the target LLM, not by a human.

We offer a framework for hard-prompt compression methods where we assume that a query is provided in addition to the compressed prompt during the target LLM inference call. Functionally, this is the most useful interpretation of prompt compression since it clarifies that the goal is to compress the prompt for a given query/task. This setting is also used in the LLMLingua and LongLLMLingua works, and is more general than the setting where no query is used (the query can then be empty).

## 3 Distortion-rate function for prompt compression

We first formalize the problem of prompt compression, and then develop a rate-distortion framework to study its fundamental limits. In particular, we define and characterize the _distortion-rate function_, which describes the optimal trade-off between how much and how well the prompt is compressed. A complete overview of the notation can be found in App. A.

Figure 2: Model for prompt compression in LLMs. (a): Without prompt compression, the LLM takes a long **Prompt** and **Query** as input, and produces an **Output** distribution. (b) and (c): The prompt is passed through a compressor to obtain a shorter **Compressed prompt** and the LLM takes this compressed prompt and query as input instead. (b) The compressor does not have access to the query, and preserves all highlighted tokens. (c) The compressor has access to the query, and preserves only the tokens highlighted in orange.

### A formal model for prompt compression

Black-box LLM.As depicted in Fig. 1(a), we assume that we have a pretrained LLM which takes a pair of the _prompt_\(x^{n_{x}}\) and the _query_\(q^{n_{q}}\), \((x,q)^{n_{x}+n_{q}}\) as inputs, where \(\) refers to the vocabulary of the LLM (i.e., the set of all tokens), and \(n_{x}\) and \(n_{q}\) are the lengths of the prompt and query respectively. The _output_ of the LLM is given by \(_{}}=_{}(x,q)\), where \(_{}:^{*}(^{*})\) is a deterministic function which maps a sequence of tokens to a probability distribution on sequences of tokens. We denote the set of all prompts \(x\) by \(\) and the set of all queries \(q\) by \(\). Clearly, they are both equal to \(^{*}\), but this notation is useful in the subsequent discussion. We model prompt-query pairs \((X,Q)\) as random variables drawn according to the joint distribution \(_{XQ}()\).

In cases where we have a correct answer \(y=^{*}\) corresponding to the pair \((x,q)\), we characterize the "closeness" of the LLM output \(_{}}=_{}(x,q)\) to the answer \(y\) using a distortion measure \(:()[0,]\). Two possible choices of \(\) are the log loss \(_{}\) and the 0/1 loss \(_{0/1}\), given by

\[_{}(y,_{}})=_{ }}(y)}_{0/1}(y,_{}})=1y*{argmax}_{}_{ }}()}.\] (1)

These are respectively the cross-entropy loss between the distributions \(_{y}\) and \(_{}}\), and the prediction error. When dealing with natural language queries, a semantic distortion metric such as RougeL  or BertScore  is more appropriate. Additionally, there is no single answer that is uniquely correct. To account for this variability in what qualifies as a correct answer, we model the _answer_ as a random variable \(Y\) drawn from the distribution \(_{Y|XQ}(|x,q)\), which depends on the prompt \(x\) and query \(q\). This induces the joint distribution \(_{XQY}=_{XQ}_{Y|XQ}\). We characterize the "closeness" between the correct answer and the LLM output by the average distortion, given by \(_{Y_{Y|XQ}(|x,q)}[Y,_{ }(x,q)]\). With \(=_{}\), this is the cross-entropy loss between \(_{Y|XQ}(|x,q)\) and \(_{}}=_{}(x,q)\), and with \(=_{0/1}\), this is the prediction error probability.

Prompt compression.As described in Sec. 2, we consider two types of prompt compression: query-agnostic and query-aware. Fig. 1(b) depicts the _query-agnostic_ version, where the goal is to design a _compressor_ denoted by \(\) as a possibly random function from \(\) to \(\), i.e., the set of all compressed prompts. The compressor takes in the prompt \(X_{X}\) and produces a _compressed prompt_\(M=(X)\) with \((M)(X)\). Then, the user replaces \(X\) with the compressed prompt \(M\) and provides the LLM with the input \((M,Q)\), resulting in the output distribution \(_{}}=_{}(M,Q)\). To quantify the performance of this compressor \(\), two quantities are of interest:

1. the _rate_\([(M)}{(X)}]\), to measure _how much_ the prompt is compressed, and
2. the _distortion_\([Y,_{}(M,Q)]\), to measure _how well_ the prompt is compressed,

with both expectations taken with respect to (w.r.t.) \(_{MXQY}\). If we compress \(x\) to a low rate, the compressed prompt \(m\) may not retain the information in \(x\) that is necessary for the query \(q\), leading to an output \(_{}(m,q)\) that is different from \(_{Y|XQ}(|x,q)\) and hence, a high distortion. Thus, there is a trade-off between these quantities, which we formalize as the distortion-rate function in Sec. 3.2.

We can also model _query-aware_ prompt compression similarly, with the difference being that the compressor also has access to the query \(q\), as shown in Fig. 1(c). In addition to the average rate and distortion computed over all queries, it is also interesting to consider the rate and distortion _for each query_. To simplify the presentation, we restrict our discussion here to the query-agnostic setting, and only briefly mention the analogous definitions and results for the query-aware setting. A complete development of the query-aware setting can be found in App. B.

### Rate-distortion formulation for prompt compression

Distortion-rate function \(D^{*}(R)\).The _distortion-rate function_ for any compression problem characterizes the fundamental trade-off between the distortion and the rate [25; 24; 26; 27]. We say that the pair \((R,D)\) is _achievable_ if there exists a compressor with rate at most \(R\) and distortion at most \(D\). For a given rate \(R\), the distortion-rate function \(D^{*}(R)\) is the smallest distortion that can be achieved by a compressor with rate at most \(R\). Formally, it is defined as

\[D^{*}(R) \{D 0(R,D)\}\] \[=\{D 0\}.\] (2)We are now ready to characterize the distortion-rate function for prompt compression.

\(D^{*}(R)\) for query-agnostic prompt compression.Recall that our quantities of interest are the rate \([(M)}{(X)}]\), and the distortion \([Y,_{}(M,Q)]\), with both expectations taken w.r.t. \(_{MXQY}\), where \(M=(X)\) for a random function \(\). By the functional representation lemma [27; 28], a random function from \(\) to \(\) is equivalent to a conditional distribution \(_{M|X}\). Thus, we can equivalently model the compressor as a conditional distribution \(_{M|X}\), and (2) is explicitly written as

\[D^{*}(R)= _{_{M|X}} [Y,_{}(M,Q)]\] (3) s.t. \[_{M|X}\] \[[(M)}{(X)}]  R,\]

with both expectations taken w.r.t. the joint distribution \(_{MXQY}=_{M|X}_{XQY}\) induced by the compressor \(_{M|X}\). The constraint "\(_{M|X}\) is a compressor" is short for the following requirements: (1) it is a conditional distribution, i.e., for each \(x\), \(_{m}_{M|X}(m|x)=1\), (2) if \((m)>(x)\), then \(_{M|X}(m|x)=0\), and (3) if \((m)=(x)\), then \(_{M|X}(m|x)=0\) unless \(m=x\). This means that the compressor either strictly reduces the length of the prompt or does no compression and retains the original prompt.

Note that all of the expressions in the objective and the constraints in (3) are linear in \(_{M|X}\). Hence, the optimization problem is simply a linear program (LP), which is simple from an optimization perspective [29; 30]. However, the dimensions of this problem are still large and solving the LP directly quickly becomes infeasible as the lengths of the prompts increase. In Sec. 3.3, we deal with this optimization problem directly, and show that the dual of the LP provides an exact, practically realizable solution.

The extension to the query-aware setting is straightforward; we then have query-dependent (or _conditional_) distortion-rate functions \(D^{*}_{q}(R)\) for each \(q\), and an _average_ distortion-rate function, denoted by \(^{*}(R)\). Refer to App. B for an explicit characterization of \(D^{*}_{q}(R)\) and \(^{*}(R)\).

Connections to information-theoretic setups.We provide a brief overview of rate-distortion theory from the information theory literature in App. D and describe how our model compares. In particular, we note that our model for prompt compression closely resembles the setup of compression with side-information for function computation [31; 32; 33; 34], where both the encoder and the decoder are part of the system design. More recently, there has also been a growing interest in computing the distortion-rate functions of these classical setups for real-world datasets [35; 36; 37]. However, in our model for prompt compression, only the encoder (which is the compressor) can be designed, hence our model is one of compression _for a fixed decoder_. Such a model has not been actively studied in the information theory literature before, but in the next subsection, we show that the distortion-rate function can be written as an explicit LP in terms of this fixed decoder.

### Linear program formulation of the distortion-rate function

Having expressed the distortion-rate function for prompt compression as an LP, we now look to solve this LP. We first rewrite (3) as an explicit LP using optimization-theoretic notation, and hide the probabilistic notation involving expectations and conditional probabilities in the parameters of the LP. Refer to App. A for an overview of the notation.

**Proposition 1** (Primal LP).: _The distortion-rate function for query-agnostic prompt compression (3) is given by the solution to the linear program_

\[D^{*}(R)= _{(_{x}^{M_{x}}_{+})_{x }} _{x}^{}_{x}_{x}\] (LP) \[ _{x}^{}_{x}_{x} R, {1}^{}_{x}=1,\,x,\]

_where for each \(x\), \(_{x}\) denotes the set of compressed prompts associated to \(x\), i.e., the set of all possible token sequences of length smaller than \((x)\), the vectors \(_{x}^{_{x}}_{+}\) are the optimization variables and the constants \(_{x},_{x}_{+}^{_{x}}\) with components indexed by \(m_{x}\) are given by_

\[_{x,m}_{X}(x)}[ (Y,_{}(m,Q))]_{x,m} _{X}(x)\,(m)}{(x)}, m _{x},\] (4)

_with the expectation taken with respect to \(_{QY|MX}(Y,|m,x)\)._

Proof.: This follows immediately from (3) by defining the constants \(_{x},_{x}_{+}^{_{x}}\) for each \(x\) as given in (4), and taking \(_{x}\) to be \(_{M|X}(|x)\). We use the fact that \(_{M|X}(m|x)=0\) when \((m)>(x)\) to reduce the dimension of \(_{x}\) from \(\) to \(_{x}\) to obtain (LP). 

For our experimental setup in Sec. 4.2, we see that dimension of the LP is too large to solve (LP) directly using off-the-shelf solvers. Fortunately, the dual of the LP can be written more concisely, and can also be solved using a relatively simple algorithm.

**Theorem 1** (Dual LP).: _The distortion-rate function for query-agnostic prompt compression (3) is given by the solution to the dual of the linear program (LP), i.e.,_

\[D^{*}(R)=_{ 0}\{- R+_{x}_{m _{x}}[_{x,m}+_{x,m}]\}.\] (dual-LP)

Proof sketch.: This follows by taking the dual  of the LP (LP) and simplifying the resulting expression. For a complete proof, refer to App. C.1. 

Algorithm to solve (dual-LP).While the optimization problem in (dual-LP) seems difficult to solve, with its max-min structure and the supremum over a continuous variable, it provides a neat geometric interpretation which allows for a computationally simple algorithm given in Algorithm 1. It takes as input \(R\), \((_{x})_{x}\), \((_{x})_{x}\), and returns as output the distortion-rate function at \(R\), i.e., \(D^{*}(R)\). In App. C.2, we prove that the output is indeed \(D^{*}(R)\), and provide a step-by-step illustration of the algorithm on an artificial example. A high-level description of the algorithm is given below.

Before presenting the algorithm, it is useful to define the following geometric object. The _lower-left convex envelope_ of a set of points in \(_{+}^{2}\) is the largest convex function that lies below and to the left of the points, as shown in Fig. 3 for the points \(\{(_{x,m},_{x,m})\}_{m_{x}}\) for a fixed \(x\). Let \(k_{x}\) be the number of points on this envelope, then these \(k_{x}\) points are exactly the minimizers of \(_{m_{x}}[_{x,m}+_{x,m}]\) for some \( 0\). Solving this inner minimization problem of (dual-LP) is thus easy, and amounts to simply finding the points labelled as "\(m^{(x)}\)" on the lower-left convex envelope, ordered from left to right, as done in Lines 3-4 of Algorithm 1. Let the magnitudes of slopes of the line segments on the envelope be given by the "\(^{(x)}\)" terms in decreasing order (Lines 5-6). Also letting \(_{0}^{(x)}=+\) and \(_{x_{x}}^{(x)}=0\), observe that for \(i=1,,k_{x}\), \(m_{i}^{(x)}\) minimizes \(_{x,m}+_{x,m}\) over \(m\) for \([_{i}^{(x)},_{i-1}^{(x)})\). Importantly, it is enough to consider just these sequences "\(m^{(x)}\)" and "\(^{(x)}\)" sequences computed for all \(x\) (Lines 2-6) to solve (dual-LP), instead of the entire set \(_{x}\) and the continuum of all positive real numbers \(\) respectively. This makes the problem tractable even for large values of \(|_{x}|\).

The rest of the algorithm computes the outer supremum. Lines 7-9 prepare for this by introducing new notation "\(^{(x)}\)" and "\(\)" such that for \([_{j},_{j-1})\), \(_{j}^{(x)}\) minimizes \(_{x,m}+_{x,m}\) over \(m_{x}\). There is no calculation involved in this step; what we gain is that the range of \(\) on which the minimizer is \(_{j}^{(x)}\) no longer depends on \(x\). This gives us everything we need to compute the distortion-rate function, which is obtained by lines 10-13. Observe that the input \(R\) is only used for lines 10-13, so for a given dataset, lines 1-9 can be run once and the results "\(^{(x)}\)" and "\(\)" stored, with only lines 10-13 run for each value of \(R\).

We derive a similar dual LP formulation of the query-aware distortion-rate functions in App. B. In fact, we see that both the conditional and average distortion-rate functions are of the same form as (dual-LP), with different parameters. Hence, Algorithm 1 can compute all of the distortion-rate functions that we have defined, namely the query-agnostic and query-aware (conditional and average) distortion-rate functions.

Figure 3: Lower-left convex envelope for an example with \(|_{x}|=11\), \(k_{x}=3\).

## 4 Experiments

The distortion-rate function defined in Sec. 3 describes the best possible trade-off between the achievable values of rate and distortion in the query-aware and query-agnostic cases. In this section, we compare the performance of existing prompt compression methods (that are compatible with the black-box model setting we consider here) with the optimal curve for a synthetic dataset. We observe that there is _a sizeable gap_ between the performance of existing methods and the optimal curve. We propose Adaptive QuerySelect, a _query-aware and variable-rate adaptation_ of LLMLingua-2 , that outperforms the existing methods on this synthetic dataset. We also consider a query-aware version of LLMLingua-2 called QuerySelect and observe that it outperforms the query-agnostic version, which highlights the _importance of conditioning on the query_.

We include an ablation study on the impact of tokenization of the prompt compression problem, as tokenization is lossy since it groups together multiple symbols into a single symbol before passing it to an LLM. We study the effect of tokenization on the prompt compression problem by forcing the tokenizer on the encoder and decoder side to tokenize the bits of the binary string prompts in our dataset individually, which we refer to as "forced tokenization." We run experiments in this setting and with the regular "standard tokenization." Additional details on our experiments can be found in App. F. Our code is made available for reproducibility purposes.2

### Experimental setup

Dataset.In order to run experiments that are computationally tractable but still meaningful to the prompt compression problem, we construct a synthetic dataset \(\{(x_{i},q_{i},y_{i})\}_{i=1}^{N}\) with (1) prompts \(x_{i}\) being sequences from \(=\{0,1\}\), i.e., binary prompts, (2) natural language queries \(q_{i}\), such as "Count the number of 1s," "Compute the parity," and "Is the binary string a palindrome?" and (3) their associated answers \(y_{i}\). In total, we construct a dataset of seven queries; a complete specification of the dataset, including a few examples is available in App. F.2.1. The binary prompts are generated from a first-order Markov chain on \(\{0,1\}\) with a 0.1 probability of transitioning and a 0.9 probability of remaining in the same state, and the minimum and maximum possible lengths for each prompt are four and ten, respectively. All methods are evaluated on a validation set of 1400 examples in total (7 queries, 200 examples per query). The optimal distortion-rate function is computed using Algorithm 1,taking \(_{XQY}\) to be the empirical distribution on the dataset, i.e., \(_{XQY}=_{i=1}^{N}_{(x_{i},q_{i},y_{i})}\). This is the natural choice when the true distribution is unknown. Another choice is a parametric model with parameters learned from a dataset, but it is unclear what is an appropriate model in this case.

We also run experiments on a small natural language dataset curated with GPT-4  and NarrativeQA  for a large-scale experiment. The small dataset consists of ten prompts with four queries each, and a few examples are provided in Table 3 in App. F.2.1. More details on the considerations made in constructing our datasets are provided in App. E.

Baseline methods.We compare the rate-distortion trade-off of the optimal strategy (both query-aware and query-agnostic) with prompt compression methods that can be used to compress prompts for a black-box target LLM: Selective Context , LLMLingua , LLMLingua Query , LLMLingua-2 . As such, we do not consider methods like Gist Tokens , In-Context Autoencoder , and AutoCompressor  since they require special training methods generally not compatible with black-box target LLMs. Selective Context uses \(-(x_{i} x_{0},x_{1},,x_{i-1})\) to score the \(i\)-th token, and retains the tokens whose score is larger than the \(p\)-percentile, where \(p\) is the ratio parameter. LLMLingua uses a similar method, but they first partition the input prompt into segments and condition on previously compressed segments to compress the current segment. They later extended their method to perform query-aware compression, which is what we use for LLMLingua Query. While these methods use a decoder-style (causal) transformer LLM to do prompt compression, this approach makes an independence assumption on the influence of future tokens have on the \(i\)-th token. LLMLingua-2 instead uses an encoder-style (bidirectional) LLM to perform a token classification task, where their model predicts whether a given token should be kept or removed.

Our proposed methods.We add two novel contributions over the LLMLingua-2 work: (1) we adapt LLMLingua-2 to the query-aware setting, whereas the original work only proposed the query-agnostic approach, which we call "QuerySelect," and (2) we further adapt this query-aware approach into a _variable-rate_ approach we refer to as "Adaptive QuerySelect." This approach lets the encoder model decide which tokens to keep based on the confidence over a specified threshold. In other words, LLMLingua-2 and QuerySelect accept a rate parameter to determine the compression ratio, but Adaptive QuerySelect replaces the rate parameter with a threshold parameter. The encoder model predicts the probability of keeping a particular token, and the token is kept if the predicted probability is above the threshold, resulting in a variable-rate compression of the prompt. Variable-rate compression is important as some prompts are more compressible than others, and vice versa.

Models.We use Mistral 7B Instruct v0.2  as our black-box target LLM, which is fine-tuned on the training set partition of our synthetic dataset. This model is fixed after fine-tuning and no prompt compression methods have access to any part of it. All prompt compression methods use an LLM as part of their compression algorithm; we use deduplicated Pythia 1B  for Selective Context, LLMLingua, and LLMLingua Query and RoBERTa Base  for LLMLingua-2-based methods. For each method, we finetune on the training set partition to enable the best performance possible for that method. More information on how we trained these methods and the data we used is in App. F. For all models, including the target LLM, we fine-tune with LoRA  and conduct a hyperparameter grid search. We choose the configuration with the best performance on a test set that is different from the validation set. More details on the hyperparameter search are provided in App. F.3.

For the natural language dataset, no fine-tuning is necessary on the decoder side. On the encoder side, Selective Context, LLMLingua, and LLMLingua Query use the same model as on the decoder side, and LLMLingua-2 uses a specially fine-tuned version of XLM RoBERTa Large . We use a custom fine-tuned XLM RoBERTa Large model as the encoder for the QuerySelect and Adaptive QuerySelect methods. The training dataset of (prompt, query, answer) tuples used to train this custom model is filtered from the Databricks Dolly 15k  dataset to only include examples with prompt lengths between a specified minimum and maximum length (see Sec. F.3.2 for details).

### Results

Fig. 1 summarizes our experimental contributions on the synthetic dataset. We observe a _large gap_ between the optimal curve and existing prompt compression methods. Thus, we propose QuerySelect as a _query-aware_ and Adaptive QuerySelect as a _query-aware, variable-rate modification_ of LLMLingua-2 to close this gap. Our results show that Adaptive QuerySelect achieves the best performance and, in fact, is the only method to outperform the optimal query-agnostic strategy. Wealso note that the optimal distortion-rate curves eventually fall below the baseline performance of using the full prompt (no compression). This observation is especially interesting because it shows that compressing prompts can improve performance on downstream tasks, as observed on natural language datasets in previous prompt compression works . We accredit the performance of Adaptive QuerySelect to variable-rate compression, where we allow the compressor to choose how much it should compress based on the query and prompt as input (see App. B, Remark 1 for a formal explanation of variable-rate compression). Even though this approach relinquishes explicit control over the rate, our experiments show that variable-rate compression is the closest to optimality.

Gap from optimality depends on the query.In Fig. 4, we highlight the distortion-rate curves for two out of seven of the queries in our synthetic dataset. Despite the fact that Fig. 1 shows a gap in average performance between the query-aware optimal strategy and Adaptive QuerySelect, Fig. 4 (**left**) shows that Adaptive QuerySelect can match the performance of the optimal query-aware compression scheme. Comparing Fig. 4 (**left**) and (**right**), we see that the prompt compression problem is easier (methods are closer to optimality) for certain tasks or queries depending on how the prompts were generated. For our synthetic dataset, all prompts are generated from a Markov chain with a transition probability of 0.1 and a probability of 0.9 for remaining in the same state. This means the tokens with the highest entropy are those that are part of a transition, and those tokens are the most important for answering this query. As a result, we see that methods that use the negative log-likelihood as a means for compression (Selective Context, LLMLingua, and LLMLingua Query) perform well, even without conditioning on the query. An exception here is the performance of LLMLingua Query, which we find has mixed performance compared to vanilla LLMLingua for token-level prompt compression on our dataset. Please refer to Fig. 11 in App. F for results on all queries.

Effect of tokenization.Finally, the results of our ablation study on the effects of tokenization are provided in Fig. 10 in App. F. Interestingly, the optimal curves are nearly identical, suggesting that tokenization does not play a role in attaining the best possible trade-off. Furthermore, we see that, for a fixed rate, the standard tokenization performance often matches or exceeds the performance of forced tokenization. However, the standard tokenization approach does not allow for average rates below 0.6 due to the limited size of the prompts in our synthetic dataset, so the comparison is somewhat limited. In particular, standard tokenization allows for compression of at most four tokens (but usually only two or three tokens), whereas forced tokenization allows for compression of at most ten tokens.

Extension to natural language prompts.We have thus far shown the gap between current token-level prompt compression algorithms and their optimal strategies for both the query-aware and query-agnostic encoder on a synthetic dataset. Here, we extend our results to a small natural language dataset curated with GPT-4  (details in App. F.2.1). For natural language prompts, the number of possible combinations of tokens grows too quickly, either by increasing the number of tokens in the prompt or increasing the vocabulary size, to compute the full \(_{x}\). Instead, we rely on the observation

Figure 4: We highlight the distortion-rate curves for two of the seven queries in the validation partition of our synthetic dataset. Our method, Adaptive QuerySelect, is able to match the performance of the optimal query-aware strategy (**left**). Some queries naturally incur less distortion than others with the target LLM, even with a query-agnostic approach, if the query is aligned well with the data generation process for the prompt (**right**). Note that QuerySelect covers the line of LLMLingua-2 as their performance is identical for this query.

that current token-level prompt compression strategies simply remove tokens in place. With this observation, the number of prompts to consider has the same growth rate as a prompt with a binary alphabet. Please refer to App. E for more details. Although we cannot compute the true optimal curves where every possible combination of tokens is considered, we _can_ compute the optimal curves for current prompt compression algorithms that do not generate new tokens. Fig. 8 shows that the gap for this approximation is negligible on binary prompts.

The results of our extension to natural language prompts, presented in Fig. 5, show that both of our proposed methods achieve the lowest distortion among all other prompt compression algorithms for low rates. However, the gap between all algorithms and the optimal strategies is significant. We posit the quality of the training data for LLMLingua-2-based methods accounts for the discrepancy in how far away the best method is from the optimal strategies between binary (Fig. 1) and natural language (Fig. 5) prompts. More specifically, the labels used for the binary synthetic dataset can be determined algorithmically and are optimal, but GPT-4 is used to determine the labels for the natural language dataset, which generally does not have a set of optimal "ground truth" labels. Remarkably, the gap between feeding the prompt directly to the black-box LLM (no compression) and either optimal prompt compression strategy is also large, and Fig. 5 shows that much lower distortion can be achieved in roughly 70% and 40% fewer tokens for the query-aware and query-agnostic cases, respectively. LLMLingua-2 methods are the only methods that achieve lower distortion than the no compression result, albeit for higher rates. Finally, we present a few histograms of the rates for QuerySelect and Adaptive QuerySelect in Fig. 15, which shows the greater range of rates that Adaptive QuerySelect may choose from over QuerySelect.

Although we cannot compute the optimal rate-distortion curves on a dataset as large as NarrativeQA, we did compute the curve for all existing methods to compare them on a larger-scale dataset. Those results are provided in Fig. 16; they confirm that our proposed methods outperform all other methods for rates below 0.5. We also display the average time to compress a prompt for each method in Table 6. Since our methods are adapted from LLMLingua-2, they share the same runtime.

## 5 Conclusion

We have proposed a framework for understanding the prompt compression problem for black-box target LLMs. With this framework, we defined and formulated the optimal distortion-rate trade-off as a linear program, and devised an algorithm to solve this efficiently via its dual, for both query-agnostic and query-aware settings. We compared the optimal curves with prompt compression methods in the existing literature and adapt one of them, LLMLingua-2, to be query-aware and variable-rate; this modified method, Adaptive QuerySelect, exhibits superior performance, sometimes even matching the performance of the optimal query-aware strategy, on our synthetic dataset. As future work, it is important to exhaustively study our proposed method on natural language datasets. Additionally, it is worthwhile to pursue an approximation to the optimal curves for large-scale datasets to observe the fundamental limit in that regime. We share preliminary results in that direction in App. G.3.

Figure 5: Comparison among all prompt compression methods on our natural language dataset. We show the rate-distortion trade-off for RougeL **(left)** and BertScore **(right)**. Since a higher RougeL and BertScore metric is better, we plot “\(1-\) the computed average distortion” so that a higher rate should yield a lower loss. We discuss the choice of our metrics in App. F.2.2.