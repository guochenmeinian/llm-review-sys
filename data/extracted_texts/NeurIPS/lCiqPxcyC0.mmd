# Replicable Uniformity Testing

Sihan Liu

University of California San Diego

La Jolla, CA

sil046@ucsd.edu

&Christopher Ye

University of California San Diego

La Jolla, CA

czye@ucsd.edu

###### Abstract

Uniformity testing is arguably one of the most fundamental distribution testing problems. Given sample access to an unknown distribution \(\) on \([n]\), one must decide if \(\) is uniform or \(\)-far from uniform (in total variation distance). A long line of work established that uniformity testing has sample complexity \((^{-2})\). However, when the input distribution is neither uniform nor far from uniform, known algorithms may have highly non-replicable behavior. Consequently, if these algorithms are applied in scientific studies, they may lead to contradictory results that erode public trust in science.

In this work, we revisit uniformity testing under the framework of algorithmic replicability [STOC '22], requiring the algorithm to be replicable under arbitrary distributions. While replicability typically incurs a \(^{-2}\) factor overhead in sample complexity, we obtain a replicable uniformity tester using only \((^{-2}^{-1})\) samples. To our knowledge, this is the first replicable learning algorithm with (nearly) linear dependence on \(\).

Lastly, we consider a class of "symmetric" algorithms [FOCS '00] whose outputs are invariant under relabeling of the domain \([n]\), which includes all existing uniformity testers (including ours). For this natural class of algorithms, we prove a nearly matching sample complexity lower bound for replicable uniformity testing.

## 1 Introduction

Distribution property testing (see  for surveys of the field), originated from statistical hypothesis testing , aims at testing whether an unknown distribution satisfies a certain property or is significantly "far" from satisfying the property given sample access to the distribution.

After the pioneering early works that formulate the field from a TCS perspective , a number of works have achieved progress on testing a wide range of properties . Within the field, uniformity testing  is arguably one of the most fundamental distribution testing problems: Given sample access to some unknown distribution \(\) on \([n]=\{1,,n\}\), one tries to decide whether \(\) is uniform or far from being uniform.

When the unknown distribution is promised to be either uniform or at least \(\)-far from being uniform in total variation (TV) distance, a line of work in the field  has led to efficient testers that achieve information theoretically optimal sample complexity, i.e., \((/^{2})\). However, the testers are only guaranteed to provide reliable answers when \(\) fulfills the input promise -- \(\) is either \(U_{n}\) or far from it. In practical scenarios, due to a number of reasons such as model misspecification, or inaccurate measurements, such promises are rarely guaranteed . Since the tester no longer has a correctness constraint to adhere to, it may have arbitrarily volatile behavior. Ideally, we hope for algorithmic stability for all distributions, not only on those fulfilling the promises. Consider the scenario where two group of scientists, as an intermediate step of some scientific study, are trying to independently test the uniformity of some ground-truth distribution \(^{*}\). If the groupsreach inconsistent conclusions in the end (as noted in [31; 32; 7], such replicability issues are fairly common in science), the public might naturally interpret the inconsistency to be caused by procedural or human errors on the part of one (or both) team(s), therefore eroding public trust in the scientific community [37; 44; 3; 42]. However, even when both teams follow the experimental procedure precisely, inconsistencies could still arise simply due to sample variance in the case where \(^{}\) is neither uniform nor far from uniform. By ensuring that the algorithm is _replicable_, we can effectively rule out sample variance as a cause of inconsistency.

In the work of , a formal definition of replicability for learning algorithms has been proposed to mitigate non-replicability caused by statistical methods. Specifically, replicable learning algorithms are required to give identical outputs with high probability in two different runs where it is given sample access to some common, but potentially adversarially chosen data distribution.

**Definition 1.1** (Replicability ).: _A randomized algorithm \(:^{n}\) is \(\)-replicable if for all distributions \(\) on \(\), \(_{r,T,T^{}}((T;r)=(T^{};r))  1-,\) where \(T,T^{}\) are i.i.d. samples taken from \(\) and \(r\) denotes the internal randomness of the algorithm \(\)._

A number of works have explored the connection between replicability and other algorithmic stability notions [8; 34; 15; 38; 23], and shown efficient replicable algorithms for a wide range of machine learning tasks [8; 34; 24; 25; 35; 26; 33]. In the context of uniformity testing, if we can design testers that conform to the above replicability requirement, consistencies of their outputs can be guaranteed even when there are no promises on the data distribution. This then motivates the study of _replicable uniformity testing_.

**Definition 1.2** (Replicable Uniformity Testing).: _Let \(n_{+}\), and \(,(0,1/2)\). A randomized algorithm \(\), given sample access to some distribution \(\) on \([n]\), is said to solve \((n,,)\)- replicable uniformity testing if \(\) is \(\)-replicable and it satisfies the following: (1) If \(\) is uniform, \(\) should accept with probability at least \(1-\). 1 (2) If \(\) is \(\)-far from the uniform distribution in TV distance, \(\) should reject with probability at least \(1-\)._

As observed in , learning algorithms usually incurs additional sample complexity overhead in the replicability parameter \(\) compared to their non-replicable counterpart. In this work, we characterize the sample complexity of replicable uniformity testing up to polylogarithmic factors in all relevant parameters \(n,,\) (under mild assumptions on the testers).

### Relationship with Tolerant Testing

An alternative approach to address the stringency of the promises in the formulation of uniformity testing is the concept of _tolerant testing_. At a high level, given some \(0<<\), the unknown distribution is now relaxed to be either \(\) far from \(U_{n}\) or \(\) close to \(U_{n}\) in TV distance. The tester is then required to reject in the former case but accept in the latter. As shown in [45; 46; 48; 13], assuming that \(\) is some constant 2, the sample complexity of tolerant testing quickly grows from strongly sublinear, i.e., \(()\), to barely sublinear, i.e, \((n/ n)\) as \(\) increases from \(0\) to \(/2\). In replicable uniformity testing the testers are not required to accept or reject for intermediate distributions \(\), i.e., \(\) such that \(0<(,U_{n})<\). Instead, for all possible distributions \(\), the testers are only required to give _replicable_ answers (with high probability). While we can construct an \((n,^{},)\)-replicable uniformity testing algorithm using tolerant testing by randomly sampling some threshold \(r(0,^{})\) and performing tolerant testing with \(=r-^{}\) and \(=r+^{}\), the sample complexity of such an approach will be barely sublinear in \(n\) even for constant \(\) as discussed above. Notably, the replicable algorithm in our work has a sample complexity that remains strongly sublinear in \(n\).

### Our Results

When \(n=2\), uniformity testing amounts to distinguishing a fair coin from an \(\)-biased coin. It is well known that the task requires \((^{-2})\) samples without the replicable requirement. When the tester is required to be \(\)-replicable,  shows that \((^{-2}^{-2})\) many samples are necessary and sufficient for replicable coin testing, demonstrating a quadratic blowup in \(\) compared to the non-replicable counterpart of the problem. For large \(n\), the sample complexity of non-replicable uniformity testing has been resolved after a long line of work , and shown to be \((^{-2})\). Following the pattern, one naturally expect that replicable uniformity testing would have sample complexity \((^{-2}^{-2})\). In fact, this is exactly the sample complexity reached if one views the outcome of an optimal non-replicable uniformity tester as a coin flip, and uses an optimal replicable coin tester to convert the given uniformity tester into a replicable one.

Somewhat surprisingly, we show that the sample complexity from this blackbox reduction is suboptimal for replicable uniformity testing -- it is possible to additionally shave one \(\) factor and make the dominating term's dependency on \(\)_linear_. To our knowledge, this is the first replicable algorithm that has nearly linear dependence on the replicability parameter in the dominating term.

**Theorem 1.3** (Replicable Uniformity Testing Upper Bound).: _Let \(n_{+}\), \(,(0,1/2)\). Algorithm 1 solves \((n,,)\)-replicable uniformity testing with sample complexity \((}{^{2}}+ ^{2}})\).3_

**Remark 1.4**.: _[_27_]_ _showed that the more general problem of identity testing, i.e., testing whether an unknown distribution \(\) is equal or far from some known distribution \(\) with explicit description, can be reduced to uniformity testing with only a constant factor loss in sample complexity. It is not hard to verify that this reduction preserves replicability. This immediately implies a replicable identity tester with the same asymptotic sample complexity (see Appendix C.3 for more detail)._

As our second result, we provide a nearly matching sample complexity lower bound for a natural class of testers whose outputs are invariant under relabeling of the domain elements \([n]\). The class of testers are commonly referred as _Symmetric Algorithms_, and first studied in the work of .

**Definition 1.5** (Symmetric Algorithms, Definition 13 of ).: _Let \(f:[n]^{ m}\{0,1\}\) be a binary function. We say that \(f\) is symmetric if for any sample set \((x_{1}, x_{m})[n]^{ m}\) and any permutation \( S_{n}\), we have that \(f(x_{1}, x_{m})=f((x_{1}),,(x_{m}))\). We say an algorithm \((;r)\) is symmetric if it computes some symmetric function \(f_{r}\) under any fixed random seed \(r\)._

Without the replicability requirement, it can be shown that assuming the algorithm is symmetric is without loss of generality for uniformity testing. This is due to a simple observation that uniformity itself is a symmetric property: if \(p\) is uniform or far from the uniform distribution, the property is preserved even if we permute the labels of the elements within \([n]\). Consequently, all known optimal uniformity testers, including our replicable uniformity tester, are indeed symmetric algorithms. For this natural class of testers, we show that the sample complexity achieved is essentially optimal (up to polylogarithmic factors).

**Theorem 1.6** (Symmetric Testers Lower Bound).: _Let \(n_{+}\), \(,(0,1/2)\). Any symmetric algorithm solving the \((n,,)\)-replicable uniformity testing problem requires \((}{^{2}}+ ^{2}})\) samples._

Unfortunately, when replicability is concerned, it is unclear whether we can still assume that the optimal algorithm is symmetric. Whether the above lower bound holds for all algorithms is left as one of the main open questions of this work.

### Limitations, Discussion, and Future Work

In this paper, we present a replicable algorithm for uniformity testing using \((^{-2}^{-1}+^{-2}^{-2})\) samples. We provide a matching lower bound for a natural class of _symmetric_ uniformity testers -- algorithms that essentially consider only the _frequency_ of each element without discriminating between distinct labels. Since all known uniformity testing algorithms are symmetric, we tend to believe that the lower bound can be established _unconditionally_ and leave it as an open question. We discuss some issues in generalizing our current approach to realize this goal in Appendix C.2.

While uniformity testing is a central problem in distribution property testing, there are many other settings where it would be interesting to develop replicable algorithms, such as closeness and independence testing .

### Technical Overview

\(^{-2}\) Barriers for \(_{2}\) based Statistics.Most of the well known non-replicable uniformity testsers compute an unbiased estimator for the \(_{2}\) norm of the the unknown distribution \(\), i.e., \(||||_{2}^{2}=_{i=1}_{i}^{2}\), from the number of occurrences \(X_{i}\) of each element \(i\) among the observed samples. These include the testers from , which are based on counting pair-wise collisions, i.e., \(_{i=1}^{n}X_{i}(X_{i}-1)\), and the ones from , which are based on variants of the \(^{2}\) statistic, i.e., \(_{i=1}^{n}((X_{i}-m/n)^{2}-X_{i})/(m/n)\). As one can see, these estimators all rely heavily on computing the quantities \(X_{i}^{2}\), which could have large variances even when there is a single heavy element. This poses serious challenges on designing replicable testers based on these statistics 4. In the following paragraphs we discuss the challenge for collision based testers in more detail. A similar barrier exists for \(^{2}\) based statistics, which we defer to Appendix C.1.

Given an unknown distribution \(\), the expected number of pairs of collision will be exactly \(_{i=1}^{n}_{i}^{2}\). If \(\) is uniform, the expected value of the test statistic will be about \(}{2n}\). If \(\) is \(\)-far from being uniform, the expected value will be at least \((1+^{2})}{2n}\). To construct a replicable uniformity tester from the collision statistics, a natural idea is to select a random threshold \(r\) between the two extrema to be the decision threshold. Consequently, the tester fails to replicate if and only if the random threshold falls between the realized values of the test statistics in two different runs. Conditioned on that the test statistics computed in two different runs deviate by \(\), the above events happens with probability exactly \(^{2}}\). Hence, for the tester to be \(\)-replicable, the test statistics will need to deviate by no more than \(=O(m^{2}^{2}/n)\) with constant probability.

To focus on the dependency on \(\), we let \(\) be a small constant. We now construct a hard instance that makes collision-based statistics violate the above concentration requirement unless \(m^{-2}\). Consider a distribution with a single heavy element with probability mass \(p 1/\). Let \(X(m,p)\) denote the number of occurrences of this heavy element. It is not hard to see that this element contributes to the total collision counts by \(\), which deviates by \((mp)=(m^{3/2}/n^{3/4})\) from its mean with constant probability. Consequently, over two runs of the algorithm, with constant probability the numbers of collisions may differ by \((m^{3/2}/n^{3/4})\). Therefore, the tester will fail to be \(\)-replicable unless \(m^{3/2}/n^{3/4} m^{2}/n\) or equivalently \(m}{^{2}}\).

Total Variation Distance StatisticTo make the test statistics less sensitive to the counts of heavy elements, we compute the _total variation statistic_, which has been used in  to achieve optimal uniformity testing in the high probability regime. In particular, the test statistics measures the TV distance between the empirical distribution over samples and the uniform distribution:

\[S=_{i=1}^{n}|X_{i}/m-1/n|,\]

where \(X_{i}\) is the number of occurrences of the \(i\)-th element. Unlike collision-based statistics, note that the TV statistics depends only _linearly_ on each \(X_{i}\). For a heavy element \(X_{i}\), the contribution to the empirical total variation distance is (up to normalization) at most \(X_{i}\) with variance \((X_{i})=(mp)=(m/n^{1/2})\) opposed to \((X_{i}^{2})=(m^{3}/n^{3/2})\). Intuitively, this allows us to obtain tighter concentration bounds on the test statistic \(S\), thereby improving the final sample complexity.

First, we observe that when the distribution is \(\)-far from uniform,  shows that the expected value of \(S\) exceeds the expected value of \(S\) under the uniform distribution by at least some function \(f(m,n,)\) (see Equation (2) for the full expression). To establish a replicable tester in the super-linear case (\(m=(^{-2}^{-1}) n\)), we use McDiarmid's inequality to directly argue that the TV test statistic deviates by at most \( f(m,n,)\) with high probability. Hence, if we use a random threshold that lies within the gap interval, the threshold will be \( f(m,n,)\) close to the expected value of the test statistic with probability at most \(O()\), ensuring replicability of the final result.

The key challenge lies in obtaining the desired concentration in the sublinear regime, i.e. \(m n\). In this regime, the expectation gap is given by \(f(m,n,)=m^{2}}{n^{2}}\). Unfortunately, the presence of heavy elements, i.e., element with mass \(_{i} 1/m\), can still make the test statistic have high variance. As a result, it becomes challenging to obtain the desired concentration of \( f(m,n,)\) on the test statistic. However, in the presence of very heavy elements (e.g., \(_{i} m/n\)) which causes the variance of the test statistic to be too large, we observe that these elements cause the expectation of the test statistic to increase sufficiently beyond the expectation gap so that the tester rejects consistently, even though the distribution may not be \(\)-far from uniform. To formalize the intuition, we show that whenever the variance of the test statistic is large, so is its expected value, thereby ensuring consistent rejection. In particular, in Lemma 3.3 we show that, whenever \((S)} f(m,n,)=m^{ 2}}{n^{2}}\), it holds that

\[[S]-(S)}(U_{n})+^{ 2}m^{2}/n^{2}\] (1)

where \((U_{n})\) is the expected value of \(S\) under the uniform distribution \(U_{n}\). At a high level, Equation (1) is shown by rewriting \(S\) with indicator variables representing whether some sample collides with another, and we use correlation inequalities to show that these collision indicators are "almost" independent of each other (see proof sketch of Lemma 3.3 for more detail). Combining Equation (1) with Chebyshev's inequality, we then have that \(S(U_{n})+^{2}m^{2}/n^{2}\) with high constant probability (which can be easily boosted to \(1-\) with the standard "median trick"). As we choose our random threshold from the interval \([(U_{n}),(U_{n})+^{2}m^{2}/n^{2}]\), it follows that the tester must consistently reject. Otherwise, we have \((S)} f(m,n,)\). Applying Chebyshev's inequality gives that \((|S-[S]|> f(m,n,)) 1\). The rest of the argument is similar to the super-linear case.

Sample complexity Lower BoundAt a high level, we present a family of distributions \(\{()\}_{}\) parametrized by some parameter \([0,]\) satisfying the following: no symmetric tester that takes fewer than \((^{-2}^{-1})\) many samples can be replicable with high probability against a random distribution from the family. Since we have fixed the testing instance distribution, using a minimax style argument similar to , we can assume that the testing algorithm is deterministic 5. The family of distributions is constructed to satisfy the following properties: (i) \((0)\) is the uniform distribution and \(()\) is \(\)-far from uniform (ii) for any two distributions \((),(+)\), where \(<\), within the family, no symmetric tester taking fewer than \((^{-2}^{-1})\) many samples can reliably distinguish them. By (i) and the correctness guarantee of the algorithm, the acceptance probabilities of the tester should be near \(1\) under \((0)\) and near \(0\) under \(()\). This implies that there exists some \(^{*}\) within the range such that the acceptance probability is \(1/2\) under \((^{*})\).6 By property (ii), \((^{*})\) cannot be distinguished from \((^{*} O())\) by the tester, which immediately implies that the acceptance probability of the tester is near \(1/2\) whenever \(=^{*} O()\), and therefore not replicable with constant probability. Thus, if we sample \(\) uniformly from \([0,]\), the tester will fail to replicate with probability at least \(()\).

The construction of \(()\) is natural and simple: half of the elements will have mass \((1+)/n\) and the other half will have mass \((1-)/n\). Property (i) follows immediately. The formal proof of Property (ii) is technical, but the high level intuition is straightforward. If we assume the underlying tester is symmetric, the most informative information is essentially the number of elements that have frequencies exactly \(2\) among the samples (as elements having frequencies more than \(2\) are rarely seen and the numbers of elements having frequencies \(0\) or \(1\) are about the same in the two cases.) If the tester takes \(m\) samples, it observes about

\[m^{2}((1+)^{2}+(1-)^{2})/n=2m^{2}(1+^{2})/n\]

many frequency-\(2\) elements under \(()\) in expectation. On the other hand, the standard deviation of the number of such elements is about \(/n}=m/\). Hence, for the tester to successfully distinguish the two distributions, \(m\) needs to be sufficiently large such that

\[}}{n}((1+(+)^{2})-(1+ ^{2}))(+)}{n}\,\]which yields \(m}{(+)}>( }{^{2}})\).

To formalize the above intuition, we will use an information theoretic argument 7. Note that for such an argument to work, one often need to randomize the order of heavy and light elements [18; 16]. Otherwise, a tester could simply group the elements whose mass is above \(1/n\) under \(()\) into one giant bucket and reduce the problem into learning the bias of a single coin, which requires much fewer samples. To achieve this randomization, we consider the _Local Swap Family_ (see Definition 4.2), where we pair the elements with mass \((1+)/n\) with those with mass \((1-)/n\) and randomly swap their orders. Since pairs are ordered randomly, no algorithm can hope to identify heavy/light elements without taking a significant number of samples. Thus, this creates distribution families containing \(()\) and \((+)\) that are information theoretically hard to distinguish even for asymmetric testers. Consequently, this shows the existence of some permutation \(_{}\) of \([n]\) such that the permuted distributions \(_{}()\) and \(_{}(+)\) are hard to distinguish. However, recall that in the replicability argument we need to first fix some \(^{*}\) such that \((^{*})=1/2\). Thus, we have to prove specifically that \(()\) and \((+)\) themselves are hard to distinguish. Fortunately, for symmetric testers, the acceptance probabilities of \(_{}()\) and \(_{}(+)\) must be identical to those of \(()\) and \((+)\) respectively. Consequently, no symmetric tester can easily distinguish \(()\) and \((+)\).

## 2 Preliminaries

For any positive integer \(n\), let \([n]=\{1,2,,n\}\). We typically use \(n\) to denote the domain size, \(\) to denote a distribution over \([n]\) and \(m\) to denote sample complexity. Given a distribution \(\) over \([n]\), let \(_{x}\) denote the probability of \(x\) in \(\). For a subset \(S[n]\), \(_{S}=_{x S}_{x}\). For distributions \(\), \(\) over \([n]\), the total variation distance is \(d_{TV}(,)=_{x[n]}|_ {x}-_{x}|=_{S[n]}_{S}-_{S}\). We also recall the definition of mutual information. Let \(X,Y\) be random variables over domain \(\). The _mutual information_ of \(X,Y\) is \(I(X:Y)=_{x,y}((X,Y)=(x,y))\). We use \(a b\) (resp. \(a b\)) to denote that \(a\) is a large (resp. small) constant multiple of \(b\).

## 3 Replicable Uniformity Testing Algorithm

Algorithm OverviewAt a high level, our algorithm computes the TV-distance statistic and compares it with a random threshold. Correctness of the algorithm largely follows from the analysis of the test statistics from . To show replicability of the algorithm, we need a better understanding of the concentration properties of the test statistic when the unknown distribution is neither uniform nor \(\)-far from being uniform. In particular, we show that when the variance of the test statistic is too large, even if the input distribution itself is not \(\)-far from uniform, the test statistic is with high probability larger than any random threshold that may be chosen, leading the algorithm to replicably reject in this case. On the other hand, when the variation is sufficiently small, we have sufficiently strong concentration in the test statistic, so that a randomly chosen threshold does not land between empirical test statistics computed from independent samples.

Proof of Theorem 1.3.: Our starting point is the "expectation gap" of the test statistic shown in .

**Lemma 3.1** (Lemma 4 of ).: _Let \(\) be a distribution on \([n]\) such that \(=d_{TV}(,U_{n})\). For any distribution \(\), let \(()\) denote the expectation of the test statistic \(S=_{i=1}^{n}|}{m}-|\) given \(m\) samples drawn from \(\). For all \(m 6,n 2\), there is a constant \(C\) such that_

\[()-(U_{n}) R:=C^{2}}{n^{2} }&m n\\ ^{2}}&n<m}\\ &} m.\] (2)

We require the following structural lemmas, whose formal proofs can be found in Appendix A, on the test statistic \(S_{}\). These lemmas show that the test statistic \(S\) (and therefore \(S_{}\)) concentratesaround its expectation \(()=_{}[S]\) in the sublinear \((m<n)\) and superlinear \((m n)\) cases respectively. For the superlinear case, we bound the sensitivity of \(S\) with respect to the input sample set \(T\) and apply McDiarmid's inequality to obtain the desired concentration result.

**Lemma 3.2** (Superlinear Concentration).: _Assume that we are in the superlinear regime (i.e., \(m n\)). Denote by \(()\) the expectation of the test statistic \(S\) under the distribution \(\). If \(n m}\), then \((|S_{}-()|^ {2}})<\). If \(}\)\(m\), then \((|S_{}-()| )<\)._

For the sublinear case, we provide a proof sketch below, deferring the details to Appendix A.

**Lemma 3.3** (Sublinear Concentration).: _Suppose \(m n\). If \((S)(C/64)^{2}^{4}m^{4}n^{-4}\), then \([S]-(S)}>(U_{n})+C^{2}m ^{2}n^{-2}\) where \((U_{n})\) is the expectation of \(S\) under the uniform distribution and \(C\) is given by Lemma 3.1. As a consequence, with probability at least \(1-/4\), we have \(S_{}>(U_{n})+C^{2}m^{2}n^{-2}\)._

_On the other hand, if \((S)(C/64)^{2}^{4}m^{4}n^{-4}\), then it holds that \((|S_{}-()|(C/16)^{2}m ^{2}n^{-2})</4\). Furthermore, for the uniform distribution \(U_{n}\), \((S)(C/64)^{2}^{4}m^{4}n^{-4}\)._

Proof Sketch.: In the proof sketch, for simplicity, we assume that \(\) is some small constant and ignore its dependency. When \((S)\) is small, we simply apply Chebyshev's inequality to obtain the desired concentration of \(S\). The concentration of \(S_{}\) then follows from the standard median trick.

In the rest of the sketch we focus on the case \((S)(C/64)^{2}^{4}m^{4}n^{-4}\). In this case, the key is to show that

\[[S]-(S)}>(U_{n})+C^{2}m^{ 2}n^{-2}\] (3)

as the claim \(S_{}>(U_{n})+C^{2}m^{2}n^{-2}\) with high probability follows almost immediately by an application of Chebyshev's inequality (and the standard analysis for the median trick).

Towards Equation (3), define \(X_{i}\) as the number of occurrences of element \(i[n]\). We begin with an observation from  stating that the test statistic \(S=Z/n\) where \(Z=|\{i\,\,X_{i}=0\}|\) denotes the number of "empty" buckets. Furthermore, we can write \(Z=n-m+_{i=1}^{m}Y_{i}\) where \(Y_{i}\) indicates whether the \(i\)-th sample collides with a previous sample \(j<i\). Then, \((S)=(Z)/n^{2}=( Y_{i})/n^{2}\). To bound the variance, we argue that the indicators \(Y_{i}\) are "almost" negatively correlated using correlation inequalities (specifically Kleitman's Theorem Lemma A.8), so that (roughly) \(( Y_{i})_{i}(Y_{i})[Y _{i}]\) (see Lemma A.5 for the accurate statement). Observe that under \(U_{n}\), we have \([Y_{i}] m^{2}n^{-1}\) so that \([S]-(U_{n})[ Y_{i}]/n-m^ {2}/n^{2}\). It follows that

\[[S]-(S)}-(U_{n})([ Y_{i}]-[ Y_{i}]})/n-(m^ {2}/n^{2}).\]

By our assumption, \([ Y_{i}] n^{2}(S)^{2} ^{4}m^{4}n^{-2} 1\) so that it suffices to show \([ Y_{i}](C+1)m^{2}/n\). Since \((S)(C/64)^{2}^{4}m^{4}n^{-4}\) which is further bounded from below by \(m^{2}/n^{3}\) whenever \(m^{-2}^{-1}\) we conclude \([ Y_{i}] n^{2}(S) m^{2}/n\)We are now ready to show Theorem 1.3 -- our main algorithmic result. We begin with correctness for the uniform distribution. Suppose \(m n\). By Lemma 3.3, \(S_{}(U_{n})+R/16(U_{n})+R/4 r\) with probability at least \(1-\) so the algorithm outputs **accept**.

Otherwise, if \(n m\), we have by Lemma 3.2 that with probability at least \(1-\), \(S_{}(U_{n})+R/16(U_{n})+R/4 r\) so that the algorithm outputs **accept**.

On the other hand, suppose \(=d_{TV}(,U_{n})\). We note that the algorithm of  computes the test statistic \(S\) using \(((+(1/))^{-2})\) samples and compares \(S\) with some fixed threshold \(R\) that is strictly larger than the random threshold \(r\) of our choice. By the correctness guarantee of their algorithm, it holds that \([S>R] 1-/100\), which further implies that \([S_{}>R>r] 1-/4\).

We now proceed to replicability. Consider two executions of the algorithm. If \(\) is uniform or \(=d_{TV}(,U_{n})\), then following a union bound on the correctness condition, two executions of the algorithm output different values with probability at most \(/2\). Then, suppose \(m n\) and \((S)(C/64)^{2}^{4}m^{4}n^{-4}\). By Lemma 3.3, both samples lead the algorithm to output **reject** with probability at least \(1-\) so that the algorithm is \(\)-replicable.

Otherwise, Lemma 3.2 and Lemma 3.3 guarantees strong concentration of the test statistic \(S_{}\). In particular, with probability at least \(1-/2\), we have \(|S_{}-()|R\) over both samples, where \(R\) is the expectation gap defined as in Equation (2). In particular, whenever the random threshold \(r\) does not fall in the interval \((() R/16)\) both executions output the same result. Since \(r\) is chosen uniformly at random, this occurs with probability at most \(( R/8)/(R/2)=/4\). By a union bound, we observe that Algorithm 1 is \(\)-replicable.

Finally, the sample complexity is immediately obtained by our values of \(m m_{0}\). 

## 4 Lower Bound for Replicable Uniformity Testing

In this section, we outline the important lemmas used in showing the sample complexity lower bound for \(\)-replicable symmetric uniformity testers, and give their proof sketches. The formal argument can be found in Appendix B.

Note that the lower bound \((^{-2}^{-2})\) holds even for testing whether the bias of a coin is \(1/2\) or \(1/2+\) (see ). We therefore focus on the more challenging bound of \((^{-2}^{-1})\). Consider the canonical hard instance for uniformity testing where half of the elements have probability mass \((1+)/n\) and the other half have probability mass \((1-)/n\):

\[()_{i}=&i 2=0\,,\\ &\] (4)

Our hard instance for replicable uniformity test is as follows: we choose \(\) from the interval \([0,]\) uniformly at random, and let the tester observe samples from \(()\).

Fix some uniformity tester \(_{m}\) that takes \(m\) samples. We will argue that if \(_{m}\) is \(\)-replicable and correct with probability at least \(0.99\), then we must have \(m=(^{-2}^{-1})\).

At a high level, we follow the framework of . First, we fix some good random string \(r\) such that the induced deterministic algorithm \(_{m}(;r)\) is replicable with probability at least \(1-10\), and is correct on \((0)\) and \(()\) with probability at least \(0.99\). Then, consider the function \(_{m}()\) that denotes the acceptance probability of \(_{m}(S;r)\) when the samples \(S\) are taken from \(()\). Note that \(_{m}()\) must be a continuous function. Moreover, by the correctness of \(_{m}(;r)\), it holds that \(_{m}(0) 0.99\) and \(_{m}()<0.01\). Hence, there must be some value \(^{*}\) such that \(_{m}(^{*})=1/2\). To show the desired lower bound on \(m\), it suffices to show that \(_{m}(^{*}+)\) must not be too far from \((^{*})\) for any \(\) if \(m=(^{-2}^{-1})\).

**Proposition 4.1** (Lipschitz Continuity of Acceptance Probability).: _Assume that \(m=(^{-2}^{-1})\). Let \(_{m}(;r)\) be a deterministic symmetric tester that takes \(m\) samples, and define the acceptance probability function \(_{m}()=_{S()^{ m}}[(S;r)=1 ]\,,\) where \(()\) is defined as in Equation (4). Let \(_{0}<_{1}(0,)\) be such that \(_{1}-_{0}<\). Then it holds that \(|_{m}(_{0})-_{m}(_{1})|<0.1\)._Given the above proposition, since we choose \(\) from \([0,]\) uniformly at random, it follows that the acceptance probability of the algorithm is around \(1/2\) with probability at least \(()\) if \(m=(^{-2}^{-1})\), implying that the algorithm is not \(O()\)-replicable. The proof of Proposition 4.1 is based on an information theoretic argument based on ideas developed in . We defer the formal proof of Proposition 4.1 to Appendix B.3 and give its proof outline below. Let \(m,_{0},_{1}\) be defined as in Proposition 4.1. At a high level, we construct two families of probability distributions, which we denote by \(_{0}\) and \(_{1}\), that satisfy the following properties: (i) \(_{i}\) contains distributions that are identical to \((_{i})\) up to domain element relabeling. (ii) Any tester that uses at most \(m\) samples cannot effectively distinguish between a random probability distribution from \(_{0}\) and a random one from \(_{1}\). For the sake of contradiction, assume that there is a deterministic symmetric tester using \(m\) samples such that the acceptance probabilities on \(()\) and \((+)\) differ by at least \(0.1\). Since all distributions within \(M_{0}\) are identical to \(()\) up to element relabeling, it follows that the acceptance probabilities of the symmetric tester on any of the distribution within \(_{0}\) must be the same (and similarly for \(_{1}\)). This then further implies that the tester can successfully distinguish a random distribution from \(_{0}\) versus one from \(_{1}\), contradicting property (ii). Proposition 4.1 thereby follows.

It then remains to construct the two families of distributions. Recall that \(_{i}\) contains distributions that are identical to \((_{i})\) up to element relabeling. We will consider all distributions that can be obtained by performing "local swaps" on \((_{i})\). In particular, we first group the elements into \(n/2\) many adjacent pairs, and then randomly exchange the labels within each pair.

**Definition 4.2** (Local Swap Family).: _Let \(n\) be an even number, and \(\) be a probability distribution on \([n]\). We define the Local Swap Family of \(\) as the set of all distributions \(\) such that_

\[((_{i})_{j},(_{i})_{j+1})=(p( _{i})_{j},p(_{i})_{j+1})\;\;\;\;(( _{i})_{j},(_{i})_{j+1})=(p(_{i})_{j+ 1},p(_{i})_{j})\]

_for all odd numbers \(j[n]\)._

**Lemma 4.3** (Indistinguishable Distribution Families).: _Let \(m=(^{-2}^{-1})\), and \(_{0}<_{1}(0,)\) be such that \(_{1}-_{0}<\). Let \((_{0}),(_{1})\) be defined as in Equation (4), and \(_{0},_{1}\) be the Local Swap Families (see Definition 4.2) of \((_{0}),(_{1})\) respectively. Let \(S\) be \(m\) samples drawn from either a random distribution from \(_{0}\) or a random one from \(_{1}\). Given only \(S\), no algorithm can successfully distinguish between the two cases with probability more than \(0.6\)._

The formal proof of Lemma 4.3 can be found in Appendix B.2. At a high level, we use an information theoretic argument. In particular, we consider a stochastic process where we have a random unbiased bit \(X\) that controls whether we sample from a random distribution from \(_{0}\) or a random distribution from \(_{1}\). Let \(T\) be the obtained sample set. We show that \(T\) and \(X\) has little mutual information. A simple application of the data processing inequality then allows us to conclude the proof. To simplify the computation involved in the argument, we will also apply the standard "Poissonization" trick. In particular, we assume that the algorithm draws \((m)\) many samples instead of exactly \(m\) samples. The advantage of doing so is that we can now assume that the random variables counting the number of occurrences of each element are mutually independent conditioned on the probability distribution from which they are sampled. Moreover, one can show that this is without loss of generality by a standard reduction-based argument using the fact that Poisson distributions are highly concentrated. The formal statement of the mutual information bound is provided below.

**Lemma 4.4**.: _Let \(_{0},_{1}\) be defined as in Lemma 4.3. Let \(X\) be a random unbiased bit, \(\) a random probability distribution from \(_{X}\), and \(S\) be \((m)\) many samples from \(\). Moreover, let \(M_{i}\) be the occurrences of element \(i\) among \(S\). Then it holds that_

\[I(X:M_{1},,M_{n})=O(^{4}^{2}}{n}\;^{4 }(n))+o(1).\]

To show Lemma 4.4, we first note that if we group the random variables into \(n/2\) many adjacent pairs, the pairs \((M_{i},M_{i+1})\) are conditionally independent and identical given \(X\). Therefore, we can bound \(I(X:M_{1},,M_{n})\) from above by \(\;I(X:M_{1},M_{2})\). To tackle \(I(X:M_{1},M_{2})\), we break into three regimes depending on the relative size of \(m,n,\): the sub-linear regime (approximately \(m n\)), the super-linear regime (approximately \(n<m<n/^{2}\)), and the super-learning regime (approximately \(m>n/^{2}\)). The formal proofs involves writing the probability distributions of \(M_{i}\)s as Taylor expansions in \(\). The calculations are rather technical and therefore deferred to Appendix B.1.