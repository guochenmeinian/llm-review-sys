# Robust Sleep Staging over Incomplete Multimodal Physiological Signals via Contrastive Imagination

Qi Shen\({}^{1}\), Junchang Xin\({}^{2}\), Bing Tian Dai\({}^{3}\), Shudi Zhang\({}^{1}\), Zhiqiong Wang\({}^{1}\)

\({}^{1}\)College of Medicine and Biological Information Engineering, Northeastern University, China

\({}^{2}\)College of Computer Science and Engineering, Northeastern University, China

\({}^{3}\)School of Information Systems, Singapore Management University, Singapore

2210521@stu.neu.edu.cn, xinjunching@mail.neu.edu.cn, btdai@smu.edu.sg,

2310535@stu.neu.edu.cn, wangzq@bmie.neu.edu.cn

Corresponding author

###### Abstract

Multimodal physiological signals, such as EEG, EOG and EMG, provide rich and reliable physiological information for automated sleep staging (ASS). However, in the real world, the completeness of various modalities is difficult to guarantee, which seriously affects the performance of ASS based on multimodal learning. Furthermore, the exploration of temporal context information within PSs is also a serious challenge. To this end, we propose a robust multimodal sleep staging framework named contrastive imagination modality **sleep network** (CIMSleepNet). Specifically, CIMSleepNet handles the issue of arbitrary modal missing through the combination of modal awareness imagination module (MAIM) and semantic & modal calibration contrastive learning (SMCCL). Among them, MAIM can capture the interaction among modalities by learning the shared representation distribution of all modalities. Meanwhile, SMCCL introduces prior information of semantics and modalities to check semantic consistency while maintaining the uniqueness of each modality. Utilizing the calibration of SMCCL, the data distribution recovered by MAIM is aligned with the real data distribution. We further design a multi-level cross-branch temporal attention mechanism, which can facilitate the mining of cross-scale temporal context representations at both the intra-epoch and inter-epoch levels. Extensive experiments on five multimodal sleep datasets demonstrate that CIMSleepNet remarkably outperforms other competitive methods under various missing modality patterns. The source code is available at: [https://github.com/SQAIYY/CIMSleepNet](https://github.com/SQAIYY/CIMSleepNet).

## 1 Introduction

Automated sleep staging (ASS) is essential to promote sleep quality assessment and sleep disorder diagnosis, providing convenience for the public in the daily monitoring of sleep within their home environment. Many machine learning algorithms, including feature engineering and deep learning, have been proposed for ASS . In particular, deep learning methods represented by convolutional neural network (CNN) have achieved remarkable results in the field of ASS . Compared with feature engineering, deep learning does not require the guidance of prior knowledge and has the advantage of automatically extracting physiological signals (PSs) features.

In clinical applications, due to the complexity of human physiological states, subjects usually need to wear multiple sensors to obtain more comprehensive and integrated physiological information from multimodal PSs collected from different sources . Hence, several multimodal fusion algorithms[8; 9; 10; 11] based on deep learning have been developed to cope with the challenges of multimodal ASS. Although various multimodal fusion algorithms provide guarantees for automated processing and analysis of these multimodal PSs, they still have some limitations. As illustrated in the Fig. 1 (a), existing methods are almost all conducted under the assumption that all modal data are complete. However, in real scenarios, the modal data will be incomplete due to sensor malfunctions or detachment, as shown in the Fig. 1 (b). Unfortunately, the second scenario will seriously affect the reasoning process of algorithms, resulting in a sharp decline in performance .

Further, how to mine dynamic temporal changes and complex stage-transitioning patterns in PSs is another challenge for ASS. Most sleep staging works [13; 14; 15; 16; 17] utilize recurrent neural network (RNN) and its variants to model temporal dependencies within learnable hidden states. Recently, due to its efficient parallel computing ability and powerful global context modeling ability, Transformer has gradually become the preferred alternative to RNN in the ASS field [18; 19; 20]. However, Transformer lacks the recurrent modeling abilities of RNN, which is crucial for mining the structural representations and positional embedding of input sequences [21; 22]. Meanwhile, most methods are limited to mining temporal correlations at a single level in PSs, i.e., intra-epoch level or inter-epoch level. These issues make it difficult for existing temporal models to fully understand the complex variability patterns in PTS, thereby affecting the performance of sleep staging.

Considering the above challenges, we propose a robust multimodal sleep staging framework named **c**ontrastive **i**magination **m**odality **s**leep **n**etwork (CIMSleepNet), suitable for scenarios with incomplete modalities. The core contributions of CIMSleepNet are summarized as follows.

* We first design a modal awareness imagination module (MAIM), which can realize the imputation of missing modalities to restore the completeness of the various modalities. MAIM leverages the distribution of available modalities as prior conditions to learn multimodal shared representations and enhance the inter-modal correlation, thereby improving the recovery process of missing modalities.
* We provide a novel insight into the impact of the intrinsic connection between semantic and modality on data distribution. Hence, a semantic & modal calibration contrastive learning (SMCCL) is presented to modify the restored data distribution. It can utilize bidirectional guidance of semantic and modality to align the restored data with the real distribution.
* We further explore a multi-level cross-branch temporal attention (MCTA) mechanism that enables interactive modeling of recurrent features and self-attention weights from the intra-epoch and inter-epoch levels to yield more comprehensive temporal representations.
* Extensive experiments on five multimodal sleep datasets exhibit that CIMSleepNet can significantly improve multimodal ASS performance under various missing modality patterns.

## 2 Related Work

**Multimodal Learning for Sleep Staging**: In the ASS field, several pioneering studies have been devoted to exploring how to utilize multimodal PSs acquired from various sensors to improve ASS performance. Andreotti et al.  selected three polysomnography (PSG) signals related to sleep, electroencephalogram (EEG), electrooculogram (EOG) and electromyogram (EMG), as input to CNN to improve ASS accuracy. Similarly, Jia et al.  effectively mined salient waves form multimodal sleep PSs with a multimodal salient wave detection network. Lin et al.  designed a cross-link fusion module to eliminate redundant information in multimodal PSs. Huy et al.  focused on the training mode of the deep model, and proposed an adaptive gradient blending strategy, which

Figure 1: The distribution of multimodal data in different scenarios. (a) exhibits the complete modality, and (b) exhibits the incomplete modality.

improves the joint learning representation ability of multimodal PSs in different views. Furthermore, multimodal PSs collected by some consumer electronic devices have gradually been applied in ASS field. For instance, Walch et al.  utilized feature engineering methods to analyze human motion signals and heart rate (HR) signals collected by Apple Watch, and verified their relevance to the sleep stage. Then, Zhai et al.  and Mads et al.  further improved multimodal sleep staging performance based on consumer electronic devices by constructing a feature fusion method based on deep learning. However, these studies have largely neglected the impact of incomplete modalities scenarios, which are more representative of real-world data distributions. Kontras et al.  ingeniously combined self-attention and cross-attention mechanisms to extract coordinating representations for multimodal PSs, thereby mitigating the interference caused by missing modalities on neural network. Nevertheless, this method was developed to handle the complete absence of one or more modalities, whereas it is impractical in real-life clinical applications.

**Contrastive Learning Under Missing Modalities**: Invariant contrastive learning (ICL) and semantic contrastive learning (SCL) are currently promising choices for solving the modality missing issue. For instance, Lin et al.  proposed a cross-modal ICL, aiming to utilize available modalities to achieve prediction of missing modalities. Similarly, Liu et al.  narrowed the gap between heterogeneous modalities through ICL for reconstructing missing modalities. SCL introduces category information on the basis of the former to achieve semantic structure preservation in missing modal cases [29; 30]. These studies focus on learning multimodal consistency representations, i.e., only recovering the multimodal shared information to deal with multimodal missing issues. However, this strategy leads to the loss of specific information unique to each modality, thereby failing to exploit inter-modal complementarity.

**Temporal Context Learning in sequence modeling**: It has achieved rapid development driven by the sequence-to-sequence models. For instance, Supratak et al.  introduced bidirectional long short-term memory (Bi-LSTM) to learn transition rules during sleep stages. Phan et al.  applied bidirectional gated recurrent unit (Bi-GRU) to model contextual information of sequence representations. Phyo et al.  provided a Bi-LSTM equipped with two auxiliary tasks to explicitly learn periodic transition patterns. Besides, Qu et al.  employed Transformer to improve the ability to mine context information in a parallel optimization manner. Eldele et al.  deployed temporal CNN to Transformer, further improving its ability to capture temporal features. Although Transformer has advantages over RNN and its variants in terms of computational efficiency and context learning, it lacks recurrent modeling ability, resulting in the omission of some important temporal attribute information [21; 22]. Furthermore, studies [22; 31; 32; 33] have proved that the features learned by RNN and Transformer are complementary. The above optimization perspective provides valuable inspiration for us to design novel temporal context architectures.

## 3 Methodology

### Problem Formulation

We first define a complete multimodal PSs dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) where \(_{i}\) is the \(i\)th multimodal epoch (sample), \(_{i}\) is the sleep stage label of the \(i\)th epoch and \(N\) is total number of epochs. Suppose \(_{i}\) contains \(M\) modifies, i.e., \(_{i}=\{_{i}^{j}\}_{j=1}^{M}\), \(_{i}^{j}^{C_{j} L_{j}}\), where \(C_{j}\) and \(L_{j}\) are the number of channels and sampling points of the \(j\)th modality, respectively. Furthermore, \(y_{i}\{0,1,,K-1\}\), where \(K\) is the number of sleep stage categories. Different from the complete modality missing issue of Kontras et al. , we mainly focus on the chunk-based missing pattern, i.e., random missing in units of multiple epochs, which is a common situation in biomedical research . This is mainly due to the fact that subjects tend to be interrupted for an extended period of time during the data collection. To construct incomplete modal dataset, we define a mask matrix \(=\{\{Z_{i}^{j}\}_{i=1}^{N}\}_{j=1}^{M}^{N  M}\) at the epoch level to track the missing status of modalities. If \(_{i}^{j}\) is observed, \(Z_{i}^{j}=1\); otherwise, \(Z_{i}^{j}=0\). Note that, \(Z_{i}^{0} Z_{i}^{1},, Z_{i}^{M-1} 0\), i.e., each \(_{i}\) must have at least one available modality. According to the mask matrix, the missing rate of the dataset can be defined as \(=1-_{i=1}^{N}_{j=1}^{M}Z_{i}^{j}\). Then, we define the incomplete multimodal PSs dataset \(}=\{(}_{i},y_{i})\}_{i=1}^{N}\), where \(}_{i}\) and \(_{i}\) have the same shape, i.e., \(}_{i}=\{}_{i}^{j}\}_{j=1}^{M}\), \(}_{i}^{j}^{C_{j} L_{j}}\). After that, we reorganize the dataset \(}\) with a new shape, i.e., \(^{j}^{N T C_{j} L_{j}}\), to perform temporal context modeling. Among them, \(= N/T\) and \(T\) is the length of contextual information. Finally,we also define a modality matrix \(=\{\ \{\ s_{j}^{j}\}_{i=1}^{H}\}_{j=1}^{M} R^{H M}\) to provide information about the modalities involved in each epoch, where \(s_{i}^{j}\{0,1,,M-1\}\) is the modal label of the \(j\)th modality of the \(i\)th epoch and \(H= T\).

As schematized in Fig. 2, we present CIISleepNet, which aims to cope with the issues of modality missing and temporal context modeling in multimodal ASS. Given incomplete multimodal PSs, we first employ MAIM to impute the missing modal data (Sec. 3.2). Meanwhile, SMCCL is utilized to modify the distribution of the recovered data (Sec. 3.3). Then, we leverage temporal CNN and MCTA embedded in the Transformer structure to perform feature extraction and temporal context modeling on the recovered complete multimodal data, respectively (Sec. 3.4). Finally, the model parameters are optimized by combining various objective functions to achieve sleep staging (Sec. 3.5).

### Missing Modality Imputation

To impute missing modalities, we design MAIM, which mainly consists of \(M\) (\(M\)=2 in Fig. 2) modality-specific encoders \((\ \ )=\{E_{j}(\ \ )\}_{j=1}^{M}\) and decoders \((\ \ )=\{D_{j}(\ \ )\}_{j=1}^{M}\). Each encoder and decoder is implemented via separable temporal CNN  to reduce the parameter redundancy.

**Modality-specific encoder.** As as depicted in Fig. 2, incomplete multimodal PSs \(}\) and mask matrix \(\) are transmitted in MAIM through multimodal data flow and mask matrix flow respectively. Firstly, multiple encoders are utilized to project multimodal PSs into the latent space, and the latent representations of all modalities are fused in a multiply add operation, formulated as

\[_{i}=^{M}Z_{i}^{j}}_{j=1}^{N}Z_{i}^{j}E_{j} (}_{i}^{j}) \]

where \(_{i}\) denotes the multimodal shared representation obtained from \(_{i}\). Since the modalities in the training set are also incomplete, the best choice for guiding the missing data in the reconstruction process is other available data of the same modality . However, the data recovered by this way loses the diversity of the original data and cannot retain the original semantic structure. To improve the data diversity, we drew inspiration from multimodal variational autoencoder (MVAE)  to learn not the shared representations of multimodal PSs but their distributions. Learning diverse data ensures that the generated data is not limited to the data that guide it, making it easier for SMCCL to perform calibration. We first utilize multilayer perceptron (MLP) to obtain two vectors, \(_{i}\) and \(_{i}\), which are used to describe the mean and variance in the distribution from the \(_{i}\). Then, \(_{i}\) is subjected

Figure 2: The overall framework of CIISleepNet. It consists of three main components: MAIM, SMCCL and MCTA mechanism. Two incomplete modalities, \(}^{1}\) and \(}^{2}\) are taken as examples for illustration. In the missing modality imputation phase, MAIM learns multimodal shared representations from the available modal distribution to recover complete modalities \(}^{1}\) and \(}^{2}\). Meanwhile, \(}^{1}\) and \(}^{2}\) are fed into SMCCL to perform distribution alignment, making the recovered modal data closer to the real data distribution. Furthermore, temporal CNN is utilized to performer feature extraction of \(}^{1}\) and \(}^{2}\) and obtain the multimodal fusion representation \(}\). After that, \(}\) is fed into a Transformer containing MCTA for temporal context modeling to obtain the temporal representation \(}\), which is then used for prediction of sleep stage scores. CIISleepNet also includes three objective functions: \(^{(1)}\) for missing modality imputation, \(^{(s)}\) for distribution alignment, \(^{(c)}\) for sleep staging.

to reparameterization to obtain the latent representation \(}_{i}\). Formally, \(}_{i}=_{i}+(}{2})_ {i}\), where \(\) is element-wise multiplication and \(_{i}\) is a random variable sampled from the distribution of \(_{i}\). After that, \(}_{i}\) is mapped back into the input space of \(_{i}\) to get multimodal shared representation \(}_{i}\).

**Modality-specific decoder.** In the decoding stage, \(}_{i}\) is fed into each decoder for reconstructing modality-specific data, i.e., \(\{}_{i}^{j}\}_{j=1}^{M}=\{D_{j}(}_{i})\}_{j=1 }^{M}\). Similar to MVAE, the parameters of MAIM are optimized guided by the joint of mean square error (MSE) \(^{()}\) and Kullback-Leibler (KL) divergence \(^{()}\). We refer to the overall loss function as the modal imagination loss function \(^{(I)}\). Suppose the batchsize is \(\), \(^{(I)}\) can be denoted as

\[&^{(I)}=_{j}^{M}_{j}^ {)}+^{)}\\ &=_{j=1}^{M}_{i=1}^{ B}\|}_{i}^{j}-}_{i}^{j}\|^{2}- _{i=1}^{B}_{k=1}^{D}(1+( _{i}^{k})-(_{i}^{k})^{2}-(_{i}^{k}) ^{2}) \]

where \(B=h T\), \(\) is the dimension of \(}_{i}\), \(\) is the loss weight, \(}_{i}^{j}\) is the real sample (if \(}_{i}^{j}\) is missing, \(}_{i}^{j}\) is the random sampling of the available data in the same modality). We found that the value of \(\) is not sensitive, but removing \(^{()}\) results in a significant decrease in performance of CMSIeePNet. Hence, we set \(\) to 1. In particular, \(^{)}\) is used to constrain how close the latent variable distribution is to the prior distribution, prompting the decoder to generate more diverse samples. Then, the mask matrix is utilized to judge whether all recovered data is in a missing state before. if \(}_{i}^{j}\) is missing, \(}_{i}^{j}\) will be used as the recovered modality; otherwise, \(}_{i}^{j}\) itself will be used. It can be expressed by mask matrix as \(}_{i}^{j}=Z_{i}^{j}}_{i}^{j}+(1-Z_{i}^{j}) }_{i}^{j}\).

### Distribution Alignment

Different from contrastive learning based on modality consistency [27; 28; 30; 29], our SMCCL introduces semantic and modal information, which not only preserves the semantic structure but also restores the specific modality information to a great extent. As illustrated in Fig. 2, SMCCL covers three similarity levels. The first-level similarity is applied to narrow the distance between different samples with two identical patterns, i.e., the same category and the same modality. Second-level and third-level similarities are utilized to correct the distribution between samples with any of the same single patterns. Note that, the constraint strength of the first-level similarity should be higher than that of the other two levels of similarity because it can be dual-guided in semantics and modality. The latter two levels of similarity are meaningful, and samples that meet these similarity criteria should not be repelled. Because these data still have semantic similarity or modal similarity. Furthermore, contrastive learning is performed within a batch, and the original complete data that meets the first-level similarity standard with the restored data may not necessarily exist in a batch, which further reflects the necessity of the latter two levels of similarity.

Supposing that a batch contains \(B\) epochs, we divide the above similarity levels by constructing similarity weight matrix \(=\{\{w_{i}^{j}\}_{i=1}^{B M}\}_{j=1}^{B M}\). To divide the similarity levels of all sample pairs, we use the label set \(\{y_{i}\}_{i=1}^{B}\) and modality matrix \(\) to introduce both semantic and modal information for each sample. We first replicate the label set, increasing its modality dimension, to obtain label weight \(=\{\{_{i}^{j}\}_{i=1}^{B}\}_{j=1}^{M}\). Flatten two matrices and replicate in the row and column dimension to expand to \(R=B M\). We redefine two matrices as \(}=\{\{_{i}^{j}\}_{i=1}^{R}\}_{j=1}^{R}\) and \(}=\{\{_{i}^{j}\}_{i=1}^{R}\}_{j=1}^{R}\). Then, calculate the contrastive mask matrices of \(}\) and \(}\), \(\) and \(\), formulated as:

\[=\{\{u_{i}^{j}\}_{i=1}^{R}\}_{j=1}^{R},u_{i}^{j}=\{ []{ll}1,&_{i}^{j}=_{i}^{j}\\ 0,&_{i}^{j}_{i}^{j}.=\{\{v_{i} ^{j}\}_{i=1}^{R}\}_{j=1}^{R},v_{i}^{j}=\{1,&_{i}^{j}=_{i}^{j}\\ 0,&_{i}^{j}_{i}^{j}. \]

where "1" is a positive pair and "0" is a negative pair. Besides, \(_{i}^{j}\) and \(_{i}^{j}\) are the elements in \(}^{}\) and \(}^{}\) respectively. Further, the similarity weight matrix \(\) can be constructed by

\[=}_{}+)(-)}_{}+(-)}_{ } \]

where \(\) denotes element-wise multiplication and \(=\{\{_{i}^{j}\}_{i=1}^{R}\}_{j=1}^{R}\) is used to set the weights for the second-level and third-level similarity. We refer to \(\) as the modality consistency matrix and \(_{i}^{j}\) as the modality consistency score. In \(\), \(\{\{_{i}^{j}\}_{i=(k-1) B+1}^{R}\}_{j=1}^{R}\) is the modality consistency score of the \(k\)th modality and other modalities, which contain all the same \(_{i}^{j}\) values. We rename \(_{i}^{j}\) in \(\{\{_{i}^{j}\}_{i=(k-1) B+1}^{R B}\}_{j=1}^{R}\) to \(_{k}\) and calculate it by the inter-modal mutual information under information theory . Taking the \(k\)th modality as an example, we use the projector \(_{k}(\,\,)\) composed of MLP to map the reconstructed complete modality data into a low-dimensional feature space and activate it by the Softmax function \(_{k}(\,\,)\), i.e., \(^{k}=_{k}(_{k}(}^{k}))\). Formally,

\[_{k}=_{i=1}^{M}_{i k};^{i})}{H(^{k},^{i})} \]

where \(_{x}\) is an indicator, when \(x\) is true, the result is "1", otherwise it is "0", \(I(^{k};^{i})\) is the mutual information of \(^{k}\) and \(^{i}\), \(H(^{k},^{i})\) is the joint entropy of \(^{k}\) and \(^{i}\). The value range of \(_{k}\) is between 0 and 1, and it can automatically adjust the ratio of the second-level and third-level similarity according to the modal consistency. For instance, if the value of \(_{k}\) is larger, it means that the inter-modal consistency is higher, but the amount of specific modal information is lower. Hence, it is necessary to increase the introduction of modal information, i.e., to increase the weight of the third-level similarity of formula (4). Vice versa. To more intuitively represent the construction process of the similarity weight \(\), we provide an example in Appendix C. To formulate \(;\,^{i})}{H(^{k},^{i})}\), we define a discrete joint probability distribution \((m,n)\) and two discrete marginal probability distributions \((m)\) and \((n)\). Since \(^{k}\) and \(^{i}\) are activated by Softmax function, \(^{k}\) and \(^{i}\) can be regarded as the distribution of two discrete cluster assignment variables \(m\) and \(n\) on \(\) categories like [29; 37]. Among them, \(\) is the feature dimension of \(^{k}\) and \(^{i}\). Hence, we redefine \((m,n)\), \((m)\) and \((n)\) as \(=(^{k}(^{i})^{}^{i}(^{k}) ^{})^{B}\), \(_{m}=(}_{d_{n}=1}^{ }_{,,d_{n}})^{B}\) and \(_{n}=(}_{d_{m}=1}^{ }_{,d_{m},})^{B}\), respectively. As a result, the discrete form of \(;^{i})}{H(^{k},^{i})}\) can be expressed as

\[;^{i})}{H(^{k},^{i})}=_{}}( }{_{m}_{n}}) \]

The theoretical result of formula (6) are demonstrated in Appendix D. To match the dimensions of the two redefined matrices \(}\) and \(}\),we perform a flatten operation on each batch of reconstructed data to obtain \(}=\{}_{i}\}_{i=1}^{B M}\). Then, we fed \(}\) into another projector \((\,\,)\) for the computation of contrastive loss, i.e., \(=(})\),\(=\{_{i}\}_{i=1}^{B M}\). According to \(\), we propose a novel contrastive learning, SMCCL,which can be defined as

\[^{(s)}=^{j}>0}-1}_{i=1}^{B M} _{j=1}^{B M}_{i j}_{w_{j}>0} w _{i}^{j}_{j}/)}{_{ k=1}^{B M}_{i k}(_{i}_{k}/)} \]

where \(^{(s)}\) is named distribution alignment loss, \(N_{w_{i}^{j}>0}-1\) is the number of \(w_{i}^{j}>0\) in a batch and \(\) is a temperature coefficient, which is set to 0.07 like . In SMCCL, \(^{(s)}\) adjusts the attention given to different sample pairs based on \(\), achieving more fine-grained distribution calibration.

### Feature Extraction and Temporal Context Modeling

As illustrated in Fig. 2, the recovered complete modal dataset \(}=\{\{_{i}^{j}\}_{i=1}^{B}\} _{j}^{M}\) is also fed into the temporal CNN for feature extraction and concatenation to obtain multimodal fusion temporal representation \(}^{B D C},B=h T\) during the distribution calibration process. Among them, \(C\) is the number of channels, \(D\) is the feature dimension, \(h\) is the batch size and \(T\) is the context length. Then, we utilize a Transformer composed of layer normalization (LN), MCTA, and MLP for temporal context modeling, thereby obtaining temporal representation \(}^{B D C}\). We focus on introducing MCTA, with its single-head structure depicted in Fig. 3. Firstly, the fusion representation after the first LN is divided into \(S\) heads, i.e., \(}=\{}_{s}\}_{s=1}^{S}\), where \(}_{s}^{B D(C/S)}\). After that, \(}_{s}\) is fed into MCTA. It has two branches and includes intra-epoch and inter-epoch levels, which can fully mine the temporal context information of latent features.

**Intra-epoch level:** In the 1th branch, temporal CNN is adopted to generate the query \(Q_{s}\) and the key \(K_{s}^{(T)}\) and value \(V_{s}^{(T)}\). Related study  have proven that temporal CNN exhibits efficiency beyond linear operations, while also eliminating the requirement for positional encoding. In the 2th branch, we use Bi-GRU to learn the recurrent representation of \(}_{s}\). Similarly, key \(_{s}^{(B)}\) and value \(_{s}^{(B)}\) from \(}_{s}^{(B)}\) are obtained via temporal CNN. To achieve cross-branch interaction, we splice \(K_{s}^{(T)}\) and \(V_{s}^{(T)}\) with \(_{s}^{(B)}\) and \(_{s}^{(B)}\). As a result, the intra-epoch cross-branch attention can be calculated as

\[}_{s}^{(T)}=_{s}=(  K_{s}^{}}{})V_{s},K_{s}=[K_{s}^{(T)}||_{s }^{(B)}],V_{s}=[V_{s}^{(T)}||_{s}^{(B)}] \]In the interactive process, MCTA can effectively integrate recurrent bias into self-attention weights to improve the shortcomings of traditional Transformer recurrent modeling ability.

**Inter-epoch level**: As shown in Fig. 3, the 1th branch and the 2th branch of the inter-epoch level exhibit a reversed pattern compared to the intra-epoch level. This design enables MCTA to not only realize the interaction of cross-branch in parallel manner, but also capture rich temporal representations layer by layer. In this level, \(}_{s}^{(T)}^{h T(D C/S)}\) and \(}_{s}^{(B)}^{h T(D C/S)}\) serve as the input of the two branches respectively. In the 1th branch, similar to the 2th branch at the intra-epoch level, \(}_{s}^{(T)}\) is mapped to \(}_{s}^{(B)}\), to obtain \(K_{s}^{(B)}\) and \(V_{s}^{(B)}\). In the 2th branch, \(}_{s}^{(B)}\) is mapped to \(_{s}\), \(K_{s}\) and \(_{s}\) by temporal CNN. Likewise, the inter-epoch cross-branch attention can be calculated as

\[}_{s}^{(T)}=_{s}=(_{s}}_{s}^{T}}}{})_{s},_{s}=[_{s}^{(T)}||K_{s}^{(B)}],V_{s}=[ _{s}^{(T)}||V_{s}^{(B)}] \]

After that, we concatenate \(}_{s}^{(B)}^{h T(D C/S)}\) and \(}_{s}^{(T)}^{h T(D C/S)}\), and perform dimensionality reduction via temporal CNN to obtain the fused representation \(}_{s}^{B(D/S)}\). Finally, extending single-head MCTA to multiple heads can be expressed as \(}=[}_{1}||}_{2}|||| }_{S}]^{B D}\).

### Optimization Objective

We utilize temporal representation \(}^{B D C}\) to perform sleep staging. Meanwhile, cross entropy loss \(^{(c)}\) is regarded as a good choice to guide the learning of model parameters, i.e.,

\[^{(c)}=-_{i=1}^{B}_{j=1}^{K}}_{j} (y_{i,j}(_{i,j})+(1-y_{i,j}) (1-_{i,j})) \]

where \(B\) is the batch size, \(K\) is the number of categories, \(}\) is the category weight, \(y\) is the real label and \(\) is the predicted label. After that, we construct the total objective loss for CIMSleepNet. Formally, \(=^{(c)}+^{(I)}+^{(s)}\), where \(\) and \(\) are the weight of the loss term.

## 4 Experiments

### Datasets and Implementation Details

**Datasets**: Five multimodal sleep datasets, Sleep-EDF-20 [40; 41], Sleep-EDF-78 [40; 41], SVUH-UCD , Motion and heart rate (MHR)  and SHHS [42; 43] are used for the effectiveness of CIMSleepNet. The first four datasets are used to verify the performance of CIMSleepNet when the modality is **randomly partially missing**, and the last dataset is used to verify its performance when the modality is **completely missing**. We choose EEG and EOG, for Sleep-EDF-20, Sleep-EDF-78

Figure 3: Design of the multi-level cross-branch temporal attention (MCTA) mechanism. \(D\) and \(T\) are the number of channels of temporal CNN at different levels; the values of \(D/2\) and \(T/2\) are rounded down; k is the kernel size; st is the stride. \(M\) and \(N\) are the neuron counts of Bi-GRU at different levels, where \(M=C/S\) and \(N=D C/S\).

and SHHS; EEG, EOG and EMG, for SVUH-UCD; motion signal and HR, for MHR. We provide detailed introduction and preprocessing methods of all datasets in Appendix E.

**Implementation Details**: In the first four datasets, CIMSleepNet is trained and tested using _k_-fold cross-validation, with a total of five repetitions of this procedure. Each result is the average of five results. In the last dataset, the training strategy refers to . The detailed experimental settings and important hyperparameter settings are in Appendix F.

### Comparison with the state-of-the-arts

In **randomly partially missing** case, we compare our CIMSleepNet with \(8\) ASS methods that can support multimodal learning: FeatConcat, MultitaskCNN , SalientSleepNet , MM-Net , TransSleep , XSleepNet , MLP  and DeepCNN . We leverage mask matrix \(\) and the public code of these methods to simulate the incomplete modality case. Then, we compare CIMSleepNet with them under different missing rate \(\). According to the calculation formula of \(\), for two modalities, the missing rate ranges from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]; for three modalities, the missing rate ranges from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], where \(0.7\) is an approximate value of \(2/3\). In the **completely missing case**, we compare CIMSleepNet with CoRe-Sleep , the only existing ASS method that can handle complete missing of one or more modalities. We employ accuracy (Acc), macro F1-score (MF1) and Cohen Kappa (\(K\))  to quantitatively analyze all methods. We also compare the data recovery performance of SMCCL with ICL  and SCL . All methods are described in Appendix G.

    &  &  &  \\   & & Acc & MF1 & \(K\) & Acc & MF1 & \(K\) \\   & FeatConcat & 0.825 & 0.761 & 0.771 & 0.497 & 0.429 & 0.285 \\  & MultitaskCNN  & 0.835 & 0.753 & 0.775 & 0.589 & 0.506 & 0.449 \\  & SalientSleepNet  & **0.872** & **0.827** & 0.827 & 0.634 & 0.565 & 0.485 \\  & MM-Net  & 0.867 & 0.817 & 0.822 & 0.570 & 0.493 & 0.432 \\  & TransSleep  & 0.864 & 0.819 & 0.821 & 0.594 & 0.521 & 0.457 \\  & XSleepNet  & 0.864 & 0.809 & 0.819 & 0.623 & 0.560 & 0.478 \\  & CIMSleepNet & 0.867 & 0.821 & 0.824 & **0.853** & **0.801** & **0.805** \\   & FeatConcat & 0.788 & 0.726 & 0.717 & 0.526 & 0.471 & 0.392 \\  & MultitaskCNN  & 0.795 & 0.727 & 0.722 & 0.613 & 0.535 & 0.453 \\  & SalientSleepNet  & 0.843 & 0.794 & 0.791 & 0.722 & 0.643 & 0.625 \\  & M-Net  & 0.845 & 0.796 & 0.794 & 0.706 & 0.628 & 0.597 \\  & TransSleep  & 0.846 & 0.797 & 0.795 & 0.738 & 0.654 & 0.637 \\  & XSleepNet  & 0.838 & 0.776 & 0.779 & 0.697 & 0.622 & 0.583 \\  & CIMSleepNet & **0.849** & **0.799** & **0.797** & **0.830** & **0.772** & **0.775** \\   & FeatConcat & 0.745 & 0.731 & 0.672 & 0.502 & 0.445 & 0.336 \\  & MultitaskCNN  & 0.774 & 0.763 & 0.705 & 0.643 & 0.630 & 0.533 \\   & TransSleep  & 0.794 & 0.782 & 0.732 & 0.725 & 0.698 & 0.636 \\   & XSleepNet  & 0.783 & 0.761 & 0.725 & 0.708 & 0.689 & 0.615 \\   & CIMSleepNet & **0.801** & **0.794** & **0.751** & **0.788** & **0.777** & **0.726** \\   & FeatConcat & 0.700 & 0.464 & 0.237 & 0.477 & 0.243 & 0.011 \\   & MLP  & 0.723 & 0.529 & 0.306 & 0.610 & 0.348 & 0.035 \\   & DeepCNN  & **0.759** & **0.615** & **0.421** & 0.616 & 0.354 & 0.039 \\   & CIMSleepNet & 0.729 & 0.553 & 0.348 & **0.701** & **0.466** & **0.240** \\   

Table 1: Performance comparison for complete and incomplete modalities in randomly partially missing case. Here ”incomplete” means the maximum missing rate.

   Test Modalities & Methods & Acc & MF1 & \(K\) \\  EEG & CoRe-Sleep  & 0.882 & 0.808 & 0.834 \\  & CIMSleepNet & **0.891** & **0.817** & **0.845** \\  EOG & CoRe-Sleep  & 0.853 & 0.753 & 0.792 \\  & CIMSleepNet & **0.858** & **0.760** & **0.798** \\  EEG+EOG & CoRe-Sleep  & 0.895 & 0.823 & 0.853 \\  & CIMSleepNet & **0.903** & **0.828** & **0.862** \\   

Table 2: Performance comparison in completely missing case.

**Quantitative results**: As shown in Tab 1, CIMSleepNet achieves performance comparable to the state-of-the-arts in the complete modality. In the incomplete modalities, compared to the performance on complete modalities, all models exhibit a decrease in performance on the four datasets. Fortunately, CIMSleepNet has the least performance degradation and performs the best. As schematized in Fig. 4, we further evaluate the performance of CIMSleepNet and other methods under different missing rates. We observe that CIMSleepNet outperforms other methods in almost all datasets and missing rates. As the missing rate increases, the performance of other methods begins to decline significantly. Relatively speaking, CIMSleepNet exhibits a more stable trend. Further, Tab 2 exhibits the performance of CIMSleepNet trained with \(=0.5\) (maximizing the model's robustness to missing modalities) and tested under complete modality absence. We observe that CIMSleepNet outperforms CoRe-Sleep in terms of performance across different testing modalities.

**Qualitative results**: We substitute ICL and SCL with SMCCL on CIMSleepNet to compare the performance of these three contrastive learning methods in data recovery (when \(=0.5\)). As depicted in Fig. 5, we randomly selected 500 recovered missing samples (500 EEG epochs and 500 EOG epochs) in Sleep-EDF-20 and projected them into 2D space via t-SNE . ICL only focuses on the inter-modal consistency and ignores the recovery of semantic information. SCL retains semantic information based on ICL, thereby improving data matching. However, ICL and SCL tend to learn the inter-modal consistency, i.e., utilize multimodal shared information to guide the recovery of missing data. This strategy easily leads to the loss of modality-specific information, thus failing to exploit the inter-modal complementarity. Different from ICL and SCL, our SMCCL explores the intrinsic connection between semantic and modal information under mutual information theory. Hence, compared to ICL and SCL, the data recovered by SMCCL exhibits a more consistent distribution

Figure 4: Impact of various missing rates.The shaded area represents the range of upper and lower standard deviations.

Figure 5: Visualization of the recovered modalities by ICL, SCL and SMCCL.

with the original data, further demonstrating its effectiveness in handling missing modalities. We also visualize the features extracted by each method. Specifically, we randomly select the data of one subject (9th) among the Sleep-EDF-20. As illustrated in Fig. 6, we use t-SNE  to visualize the distribution of features generated by all methods at \(=0.5\), which are extracted before the final decision head. Compared with other methods, our CIMSleepNet can extract more discriminative representations in incomplete modalities, further demonstrating its robustness.

**Ablation studies**: We conduct ablation studies for CIMSleepNet on Sleep-EDF-20 under the condition of missing rate \(=0.5\). It can be observed from Tab 3 that no matter which component is deleted, each evaluation metric of the results will decrease. It is particularly noteworthy that in the absence of both MAIM and SMCCL, the performance drops significantly, further demonstrating their importance in dealing with the missing modality issue. Furthermore, we find that although the two components designed to mitigate modality missing issue (MAIM and SMCCL) introduce additional parameters, the increase is much less than that introduced by the sequence modeling component (MCTA). However, sequence modeling is crucial for capturing the temporal information of PSs and improving model performance [10; 46]. The ablation experiment of MCTA, parameter sensitivity analysis and training process analysis are detailed in Appendix H, I and J, respectively.

## 5 Conclusion

We try to challenge multimodal ASS under incomplete modalities by proposing CIMSleepNet. In CIMSleepNet, MAIM reconstructs missing modality data by establishing interactions among modalities, which allows for the provision of complete modality data support for subsequent components. Meanwhile, SMCCL ingeniously leverages semantic information and modal information to subdivide similarity into three levels, thereby simulating real data distribution. Then, MCTA mechanism accomplishes comprehensive temporal context modeling, further improving the expressive ability of latent temporal representations. Extensive experiments demonstrate that the effectiveness of CIMSleepNet in various incomplete modalities.

   MAIM & SMCCL & MCTA & Acc & MF1 & \(K\) & Model Size (MB) & GFLOPs \\   & & & 0.497 & 0.429 & 0.285 & 2.344 & 0.069 \\ ✓ & & & 0.771 & 0.704 & 0.672 & 5.767 & 0.096 \\  & ✓ & & 0.786 & 0.726 & 0.699 & 8.458 & 0.071 \\  & & ✓ & 0.694 & 0.629 & 0.536 & 30.272 & 2.206 \\ ✓ & ✓ & & 0.810 & 0.756 & 0.759 & 4.412 & 0.097 \\ ✓ & & ✓ & 0.829 & 0.778 & 0.777 & 33.696 & 2.876 \\  & ✓ & ✓ & 0.834 & 0.786 & 0.784 & 36.386 & 2.246 \\ ✓ & ✓ & ✓ & **0.853** & **0.801** & **0.805** & 37.678 & 2.902 \\   

Table 3: Ablation study of CIMSleepNet on Sleep-EDF-20. “✓” indicates the use of this component. MCTA indicates the Transformer equipped with MCTA. The context length of single inference is 25.

Figure 6: Visualization of latent features of different methods on Sleep-EDF-20.