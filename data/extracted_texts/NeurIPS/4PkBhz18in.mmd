# High Precision Causal Model Evaluation with Conditional Randomization

Chao Ma

Microsoft Research

Cambridge, UK

chao.ma@microsoft.com

&Cheng Zhang

Microsoft Research

Cambridge, UK

cheng.zhang@microsoft.com

###### Abstract

The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in conditional randomization settings without complicated modification of the IPW estimator itself, paving the way for more robust and reliable model assessments.

## 1 Introduction

Experimental approaches for causal model evaluationCausal inference models aim to estimate the causal effects of treatments or interventions on outcomes of interest, given observational (sometimes experimental) data. A crucial step in developing and validating such models is to evaluate their prediction quality, that is, how well they can approximate the true treatment effects that would have been observed under different scenarios. A common approach for causal model evaluation is to launching a _new_ randomized controlled trial (RCT) separately, which provides a reliable estimate of the true treatment effects by randomly assigning units to treatments. By comparing the RCT estimate with the prediction of the causal model, one can assess the _causal error_ (Equation 1), a key metric to measure how well the model reflects the true effects.

Nevertheless, RCTs are not always accessible or preferable, as random manipulation of treatments might be impractical, ethically questionable, or excessively expensive. For instance, it may be impossible or inappropriate to randomly assign individuals to different lifestyles, environmental exposures, or social policies, as these may involve personal choices, preferences, or constraints that affect the willingness or ability to participate in the experiment. In such cases, causal machine learning researchers may resort to or augment their analyses with conditionally randomized designs, in which the allocation of participants into treatment and control groups is not purely random, but is instead based on a predetermined condition or factor.

Conditionally randomized experiments can vary in the level and unit of treatment assignment, such as individual, group, or setting. They also depend on different assumptions and methods to account for the potential confounding, selection, or measurement biases that arise from the lack of randomization.

A common scenario is the non-random quantitative assignment paradigm (West et al., 2008), where a new set of treatment groups are selected given some quantitative covariates \(X\), via an oracle model \(P_{exp}(T|X)\), where \(T\) is the treatment variable. For example, in sales strategy assignment, it is typical that resources are allocated towards products with higher revenue contributions, which introduces explicit confounding. A widely used method to estimate the true treatment effect in this setting is the inverse probability weighting (IPW) estimator (Rosenbaum and Rubin, 1983; Rosenbaum, 1987), which uses the propensity scores, the probability of receiving the treatment given the covariates, to weight the observed outcomes and adjust for the imbalance between the treatment groups. This IPW estimate can then be compared with the causal model prediction to evaluate the causal error.

Limitations of current approachesDespite the popularity of the IPW estimator for conditionally randomized experiments, it suffers from several limitations that affect its reliability for causal model evaluation. (Khan and Tamer, 2010) shows that IPW may have unbounded variance when the propensity scores are imbalanced. Moreover, the it may have poor finite sample performance and high sensitivity to the specification of the propensity score model (Busso et al., 2014). Last but not the least, a surprising finding is that even the oracle IPW may be harmful for the estimation efficiency (Hahn, 1998; Hirano et al., 2003). As a consequence, the causal error estimation based on the IPW estimator may not be accurate or robust, and may lead to misleading conclusions about the causal model quality. Existing work on addressing the variance issue of the IPW estimator often involves modifying the IPW estimator itself (Crump et al., 2009; Chaudhuri and Hill, 2014; Sasaki and Ura, 2022; Busso et al., 2014; Robins et al., 2007; Lunceford and Davidian, 2004; Imbens, 2004; Liao and Rohde, 2022). However, these methods are mainly designed for treatment effect estimation rather than causal error estimation, and may introduce additional bias or complexity that may not be optimal for the purpose of causal model evaluation.

Contributions overviewWe focus on causal model evaluation with conditionally randomized trials, propose a novel method for low-variance estimation of causal error (Equation 1), and demonstrate its effectiveness over current approaches by achieving near-RCT performance. Our key insight is: to estimate the causal error, we can design a simple and effective low-variance estimation procedure without improving the IPW estimator for the true treatment effect. As shown in Figure 1, denote the ground truth effect as \(\) and the treatment effect of a causal model \(\) as \(_{}\). Our goal to estimate the causal error \(:=_{}-^{IPW}\) with low variance. Using conditionally randomized experiments, we have an estimator of the ground truth via IPW: \(^{IPW}\). Instead of improving the IPW estimator, we replace the commonly used sample mean estimator \(\) with its IPW counterpart, \(^{IPW}_{}\), having the same realizations of treatment assignments as in \(^{IPW}\). This allows the variance of \(^{IPW}\) to be hedged by \(^{IPW}\), reducing the variance of the causal error estimator \(\). Contrary to conventional estimation strategies, the pairs estimator, as we call it, effectively reduces the variance of the causal error estimation and provides more reliable evaluations of causal model quality, both theoretically and empirically.

The rest of this paper is structured as follows. In Section 3, we formally describe our problem setting, necessary background and notations. In Section 4, we will formally define the pairs estimator for causal model evaluation, and study its theoretical properties (Proposition 1) on variance reduction. Finally, in Section 5, we will conduct simulation studies and demonstrate the effectiveness of the proposed estimation approach, highlighting its potential on achieving near-RCT performance using only conditionally randomized experimental data.

## 2 Related works

Alternative schemes to randomized controlled trialsThere exists a number of different conditionally randomized trial schemes in the literature, for instance randomized encouragement designs (Angrist et al., 1996), non-random quantitative treatment assignment (Imbens and Rubin, 2015), and fully observational studies (Rubin, 2005). In this work, we mainly consider the non-random quantitative treatment assignment setting, where the treatment groups are determined by some known function of covariates. Nevertheless, we have an emphasized focus on using conditionally randomized trials for evaluating the quality of any given causal inference models (trained on observational data), which is a fundamental setting for many practical causal model deployment procedure.

Causal estimation methods with conditionally randomized experimentsApart from IPW mentioned in Section 1, a number of other methods were also developed for conditionally randomized trials settings, include matching (Ho et al., 2007; Stuart, 2010; Rubin, 2006), regression adjustment (Neyman, 1923; Rubin, 1974; Lin, 2013; Wager et al., 2016), double robust methods (Robins et al., 1995; Bang and Robins, 2005; Kang and Schafer, 2007; Athey et al., 2018), and machine learning methods (Athey and Imbens, 2016; Wager and Athey, 2018; Shi et al., 2019; Chernozhukov et al., 2018). Readers can refer to (Imbens and Rubin, 2015; Morgan and Winship, 2015) for a comprehensive review. This paper, on the contrary, aims to propose a general method for causal model evaluation applicable to any causal effect estimation method, particularly the widely used IPW estimator.

Variance reduction and robustnessThere has been a long standing discussion on robustness improvement (Robins et al., 1995) and variance reduction of IPW estimators. Several techniques have been proposed to address the problem of extreme or uninformative weights, including weights trimming (Crump et al., 2009; Chaudhuri and Hill, 2014; Sasaki and Ura, 2022), weights normalization (Busso et al., 2014; Robins et al., 2007; Lunceford and Davidian, 2004; Imbens, 2004), and more recently, linearly-modified IPW (Zhou and Jia, 2021), etc. These techniques aim to reduce variance but may introduce bias, complexity, or tuning parameters. Our work differs from the existing literature in that, we focus on the causal error estimation, rather than the individual treatment effect estimation. We do not improve the IPW estimator for the true treatment effect, but rather propose to apply the same IPW estimator to both the model and the true effects to hedge their variances.

## 3 Problem formulation

Consider the data generating distribution for the population on which the experiment is being carried over is given by \(p(X,T,Y)\), where \(X\) are some (multivariate) covariates, \(Y\) is the outcome variable and \(T\) is the treatment variable. In this paper, we only consider continuous effect outcomes. Let \(Y^{T=t}\) denote the potential outcome of the effect variable under the intervention \(T=t\). Without loss of generality, we assume \(T\{0,1\}\). Then, the interventional means are given by \(^{1}=[Y^{T=1}]\), and \(^{0}=[Y^{T=0}]\), respectively. The ground truth treatment effect is then given by

\[=^{1}-^{0}=[Y^{T=1}]-[Y^{T=0}].\]

Now, assume that given observational data sampled from \(p(X,T,Y)\), we have trained a causal model, denoted by \(\), whose treatment effect is given by

\[_{}=^{1}_{}-^{0}_{}=[ Y^{T=1}_{}]-[Y^{T=0}_{}].\]

Our goal is then to estimate the _causal error_ of the model, which quantifies how well the model reflects the true effects (the closer to zero, the better):

\[():=_{}-\] (1)

Figure 1: Overview of the proposed method. \(\): the naive estimator for causal error evaluation. \(^{Pairs}\): the proposed estimator. Given a causal model \(\) learned from observational data, our goal is to estimate its causal error (Equation 1) via conditionally randomized experiments (yellow). Naive method (red) would achieve this by comparing \(^{IPW}\), the IPW estimation of ground truth effect from the experiment, with \(_{}\), the predicted treatment effect from \(\), usually obtained via population mean (Equation 2). In our method (brown), we replace \(_{}\) with its IPW counterpart, \(^{IPW}_{}\), using the same treatment assignments used in estimating \(^{IPW}\), so that the variance from IPW are hedged.

In practice, \(_{}\) will be the model output, and can be estimated easily. For example, we can sample a pool of i.i.d. subjects \(=(X_{1},Y_{1}),...,(X_{N},Y_{N}) p(X,Y)\), and the corresponding treatment effect estimation will be given as

\[_{}_{}=_{i }[Y_{}^{T=1}(i)-Y_{}^{T=0}(i)],\] (2)

which forms the basics of many casual inference methodologies, both for potential-outcome approaches and structural causal model approaches (Rubin, 1974; Rosenbaum and Rubin, 1983; Rubin, 2005; Pearl et al., 2000). On the contrary, obtaining the ground truth effect \(\) is usually not possible without real-world experiments/interventions, due to the fundamental problem of causal inference (Imbens and Rubin, 2015). By definition, \(\) can be (hypothetically) approximated by the population mean of potential outcomes:

\[:=_{i}[Y^{T=1}(i)-Y^{T=0 }(i)]\] (3)

However, given a subject \(i\), only one version of the potential outcomes can be observed. Therefore, we often resort to the experimental approach, the randomized controlled trial (RCT). The RCT approach is always considered as the golden standard for treatment effect estimation, in which we would randomly assign treatments to our pool of subjects \(=(X_{1},Y_{1}),...,(X_{N},Y_{N}) p(X,Y)\), by flipping an unbiased coin. Then the estimated treatment effect is given by:

\[^{RCT}=|}_{i}Y^{T=1}(i)- |}_{j}Y^{T=0}(j)\]

where \(\) denotes the subset of patients that are assigned with the treatment. Together, we have the _RCT estimator_ of the causal error:

\[^{RCT}():=_{}-^{RCT}\] (4)

However, when a randomized trial is not available, we can only deploy a conditionally randomized test assignment plan, represented by \(\), which is a vector of \(n\) Bernoulli random variables \(=[b_{1},...,b_{N}]\), each determines that \(Y_{n}^{T=1}\) will be revealed with probability \(p_{n}\). In practice, \(\) can be given by an _treatment assignment model_\(p_{exp}(T=1|X)\). This is an oracle distribution that is known and designed by the experiment designer. A subset of patients \(\) is selected given these assignment probabilities. Then, the inverse probability weighted (IPW) estimation of the treatment effect is given by

\[^{IPW}():=<^{T=1}(), ()>-<^{T=0}( ),()}{( )-1}>,\]

where \(=[1/p_{1},...,1/p_{N}]\), \(()\) is created by sub-slicing \(\) with subject indices in \(\). \(<,>\) is the inner product, and \(\) is a vector of ones. Finally, the model causal error can be estimated as (dubbed _naive estimator_ in this paper):

\[(,):=_{}-^{IPW}()\] (5)

In practice, when the size \(N\) of the subject pool is relatively small, the IPW estimated treatment effect \(^{IPW}()\) will have high variance especially when \(p_{exp}(T=1|X)\) is skewed. As a result, one will expect a very high or even unbounded variance in the estimation (Khan and Tamer, 2010; Busso et al., 2014)\((,)\). The goal of this paper is then to improve model quality estimation strategy \((,)\), such that it has lower variance and error rates under conditionally randomized trials.

## 4 Pairs estimator for causal model quality evaluation

### The pairs estimator

To resolve the problems with the naive estimator for causal error in Section 3, in this section, we propose a novel yet simple estimator that will significantly improve the quality of the causal error estimation in a model-agonist way. Intuitively, when estimating \((,)\), we can simply apply the same IPW estimator (with the same treatment assignment) for _both_ the model treatment effect \(_{}\) and the ground truth treatment effect \(\). In this way, we anticipate that the estimators for \(_{}\) and \(\) will become correlated; their estimation error will be canceled out and hence the overall variance is lowered. More formally, we have the following definition:

**Definition 1** (Pairs estimator for causal model quality).: _Assume we have a pool of i.i.d. subjects to be tested, namely \(=(X_{1},Y_{1}),...,(X_{N},Y_{N}) p(X,Y)\), as well as a conditionally randomized treatment assignment plan, represented by \(\), which is a vector of \(n\) Bernoulli random variables \(=[b_{1},...,b_{N}]\), each determines that \(Y_{n}^{T=1}\) will be revealed with probability \(p_{n}\). Assume that, for a particular trial, a subset of patients \(\) are selected using these probabilities. Then, the IPW estimator of the model's treatment effect and the ground truth treatment effect is given by_

\[_{}^{IPW}():=<_{ }^{T=1}(),()>-<_{}^{T=0}(),( )}{( )-1}>\]

_. and_

\[^{IPW}():=<^{T=1}(), ()>-<^{T=0}( ),()}{( )-1}>\]

_, respectively. Then, the pairs estimator of causal model quality is defined as_

\[^{Pairs}(,):=_{}^{IPW }()-^{IPW}().\] (6)

We will formally show that this new estimator can effectively reduce estimation error. We will provide the full assumptions and proposition below. Note that we will assume by default that the common assumptions for conditionally randomized experiments will hold, such as _non-interference/consistency/overlap_, etc, even though we will not explicitly mention them for the rest of the paper.

### Core assumptions for achieving variance reduction

Our main assumption is regarding the estimation error of causal models, stated as below:

Assumption A. Causal model estimation error for potential outcomes.We assume that for each subject \(i\), the trained causal model's potential outcome estimation can be expressed as

\[Y_{}^{T=t}(i)=Y^{T=t}(i)+V_{i}(Y^{T=t}(i))*_{i},i=1,2,...,N,t \{0,1\}\] (7)

where \(_{i}\) are i.i.d. distributed random error variables with _unknown_ variance \(_{}^{2}\), that is independent from \(Y_{i}^{T=t}\) and \(b_{i}\); and \(V_{i}(),i=1,2,...,N\) is a set of deterministic function that is indexed by \(i\). This assumption is very general and models the modulation effect between the independent noise \(\) and the ground truth counterfactual. One special example would be \(Y_{}^{T=t}(i)=Y^{T=t}(i)+Y^{T=t}(i)*_{i}\), where the estimation error will increase (on average) as \(Y^{T=t}\) increases. In practice, dependencies between error magnitude and ground truth value could arise when the model is trained on observational data that suffers from selection bias, measurement error, omitted variable bias, etc. For the rest of paper, we write Equation 7 in its vectorized form:

\[_{}^{T=t}=^{T=t}+(^{T=t})* ,\]

where all operations are point-wise. Finally, we further require that the causal model's counterfactual prediction should be somewhat reasonable, in the sense that \(_{}^{2}[(V_{i}Y^{T=t}(i))^{2}]<[Y^{T=t}(i)^{2}],t =1,0;i=1,2,...,N\). This implies that the variance of the estimation error should at least be smaller than the ground truth counterfactual.

Assumption B. Asymptotic normality of mean estimators of potential outcome variables.1 Let \(Y^{T=1}\), \(Y^{T=0}\), \(Y_{}^{T=1}\), \(Y_{}^{T=0}\) be the corresponding mean estimator of the potential outcome variables \(Y^{T=1}\), \(Y^{T=0}\), \(Y_{}^{T=1}\), \(Y_{}^{T=0}\). We assume that these estimators are jointly asymptotic normal, i.e.,

\[(}-Y^{T=1},}-Y^ {T=0},}^{T=1}}-Y_{}^{T=1}, }^{T=0}}-Y_{}^{T=0})\]

converge in distribution to a zero mean multivariate Gaussian. This is reasonable due to the randomization and the large samples used in experiments (Casella and Berger, 2021; Deng et al., 2018).

### Theoretical results

Following our assumptions, we can derive the following theoretical result, which shows that given our assumptions described in the previous section, the pairs estimator \(^{Pairs}(,)\) will effectively reduce estimation variance compared to the naive estimator, \((,)\).

**Proposition 1** (Variance reduction effect of the pairs estimator).: _With the assumptions stated in Section 4.2, we can show that our IPW estimators, \(^{IPW}()\) and \(^{IPW}_{}()\) can be decomposed as_

\[^{IPW}()=+f(),^ {IPW}_{}()=_{}+f()+g( ,),\]

_where \(f\) and \(g\) are random variables that depend on \(\) (or also \(\)), and \(g(,)\) is orthogonal to \(\), \(_{}\) and \(f()\). Furthermore, if we define the estimation error of model quality estimators as follows_

\[e(^{Pairs}(,)) :=^{Pairs}(,)-()\] \[e((,)) :=(,)-(),\]

_then both \(e(^{Pairs}(,))\) and \(e((,))\) are asymptotically normal with zero means, and their variances satisfies_

\[ar[e(^{Pairs}(,))]<ar[ e((,))].\]

Proof: See Appendix A \(\)

This result provides theoretical justifications that our simple estimator will be effective for variance reduction. In the next section, we will conduct simulation studies to empirically evaluate and validate the effectiveness proposed approach.

## 5 Simulation Studies

In this section, we will evaluate the performance of the proposed pairs estimator, and validate our theoretical insights via simulation studies. We will also examine the robustness and sensitivity of the pairs estimator concerning different scenarios of conditionally randomized trials, including treatment assignment mechanisms, degree of imbalance, choice of causal machine learning models, etc.

### Synthetic caustic dataset with hypothetical causal model

Setting.Following Geffner et al. (2022), we construct a set of synthetic datasets, designed specifically for evaluating causal inference performance, namely the csuite datasets. The data-generating process is based on structural causal models (SCMs), different levels of confounding, heterogeneity, and noise types are incorporated, by varying the strength, direction, and parameterize of the causal effects. We evaluated on three different datasets, namely csuite_1, csuite_2, and csuite_3, each with a different SCM. See Appendix B for more details. The corresponding causal model estimation is simulated using a special form of **Assumption A**, that is:

\[Y^{T=t}_{}(i)=Y^{T=t}(i)+Y^{T=t}(i)*_{i},i=1,...,N,t\{0,1\}\]

where \(_{i}\) are i.i.d. distributed zero-mean random variables with variance \(^{2}_{}\) that affects the ground truth causal error. To simulate the conditionally randomized trials, we use two different schemes to generate the treatment assignment plans \(\). The first scheme is based on a _logistic regression model_ of the treatment assignment probability given the covariates, that is,

\[p_{exp}(T=1|X)=X)}\]

, where \(\) is a random vector sampled from multivariate Gaussian distributions with mean zero and variance \(^{2}_{}\). We vary the degree of imbalance in the treatment assignment by changing the value of \(^{2}_{}\). A larger \(^{2}_{}\) implies a more imbalanced treatment assignment, as the variance of the treatment assignment probability increases. The second scheme is based on a _random subsampling_ of the units, where the treatment assignment probability is fixed for each unit, but different units are sampled with replacement to form different treatment assignment plans. We vary the number of treatment assignment plans by changing the sample size of each subsample.

Evaluation method.We compare the performance of the pairs estimator with the naive estimator (Equation 5), as well as the RCT estimator (Equation 4), which is the ideal benchmark for causal model evaluation. We also compare with two other baselines, by replacing the IPW component \(^{IPW}()\) in the naive estimator (Equation 5) by its variance reduction variants. This includes the self-normalized estimator, as well as the linearly modified (LM) IPW estimator Zhou and Jia (2021), a state-of-the-art method for IPW variance reduction when the propensity score is _known_. We measure the performance of the estimators by the following metrics: the variance, the bias, and the MSE of the causal error estimation. See Appendix B.1 for detailed definitions. We compute these metrics by averaging over 100 different realizations of the treatment assignment plans for each dataset.

Results.The results are shown in Figure 2 and Figure 3. Figure 2 shows the results for the logistic regression scheme, with different values of \(_{}^{2}\) and \(_{}^{2}\). Figure 3 shows the results for the random subsampling scheme, with different \(_{}^{2}\). In both tables, we report the average and the standard deviation of the performance metrics of the estimators for each value of the true causal error, which is computed by the difference between the true treatment effect and the model treatment effect.

From the tables, we can see that the variance of the naive estimator quickly increases when the treatment assignment is highly imbalanced. Nevertheless, our estimator (purple) consistently outperforms the naive estimator and its variance reduction variants in all metrics (variance/bias/MSE) regardless of the value of (\(_{}^{2}\)), and the degree of imbalance (\(_{}^{2}\)). The pairs estimator also achieves comparable performance to the RCT estimator, the golden standard by design. The linearly-modified estimators and self-normalized estimators have a lower variance than the naive estimator, but it also introduces some bias, and their performance is sensitive to the degree of imbalance. These demonstrate the effectiveness and robustness of the proposed pairs estimator when all assumptions are met.

Figure 2: Comparison of various causal error estimators for the logistic assignment scheme across (csuite_1,csuite_2, andcsuite_3). In this \(3 3\) plot, each row corresponds to a specific performance metric (Variance/Bias/MSE), while each column represents different datasets. In each plot, the x-axis represents the degree of treatment assignment imbalance (\(_{}^{2}\)), while the y-axis displays the performance metrics (lower the better). The different colors indicate the performance of different estimators, and different linestyles and markers indicate different \(_{}^{2}\) settings. The results demonstrate that the proposed estimator (purple) consistently outperforms the other estimators in terms of all performance metrics, with a more robust behavior as the imbalance in treatment assignment increases.

### Synthetic counterfactual dataset with popular causal inference models

Settings.Based on Section 5.1, we now consider more realistic experimental settings, in which we apply a wide range of machine learning-based causal inference methods by training them from synthetic observational data. Thus, we expect the assumptions in Section 4.2 might not strictly hold anymore, which can be used to test the robustness of our method. We include a wide range of methods, such as linear double machine learning[Chernozhukov et al., 2018] (dubbed as \(\) Linear), kernel DML (\(\) Kernel) [Nie and Wager, 2021], causal random forest (\(\) Forest) [Wager and Athey, 2018, Athey et al., 2019], linear doubly robust learning(\(\) Linear), forest doubly robust learning (\(\) Forest), orthogonal forest learning (\(\) Forest) [Oprescu et al., 2019], and doubly robust orthogonal forest (\(\) Ortho Forest). All methods are implemented via EconML package [Battocchi et al., 2019]. See Appendix B for more details.

We mainly focus on two aspects: 1), whether the proposed estimator can still be effective on variance reduction with non-hypothetical models as in Section 5.1; 2), whether the learned causal models' counterfactual predictions approximately follow the postulated **Assumption A**. Here We present the results for the first aspect, and results for the second can be found in Section 5.3.

Simulation procedure.We repeat the same simulation procedure as in Section 5.1, but using the learned causal inference models instead of the simulated causal model. We compare the performance of the pairs estimator with the same baselines as in Section 5.1, using the same metrics and the same treatment assignment schemes. For \(\) Linear, \(\) Kernel, \(\) Forest, and \(\) Forest (that does not require propensity scores), we train on 2000 observational data points generated via the following data generating process of single continuous treatment [Battocchi et al., 2019]:

\[W (0,\,I^{n_{w}}),X(0,1)^{n_{x}}\] \[T=  W,+, Y=T(X)+ W, +,\]

where \(T\) is the treatment, \(W\) is the confounder, \(X\) is the control variable, \(Y\) is the effect variable, and \(\) and \(\) are some uniform distributed noise. We choose the dimensionality of \(X\) and \(W\) to be \(n_{x}=30\) and \(n_{w}=30\), respectively. For other doubly robust-based methods, we use a discrete treatment that is sampled from a binary distribution \(P(T=1)=( W,+)\), while keeping the others unchanged. Once models are trained on the generated observational datasets, we use the

Figure 3: Comparative analysis of multiple causal error estimators in the context of the random subsampling scheme for (\(\), \(\), and \(\)). In each plot, the x-axis represents the variance of the noise variable (\(_{}^{2}\)), and the y-axis illustrates the performance metrics (Variance/Bi-as/MSE) for each estimator. Different colors denote the performance of different estimators. The findings reveal that, under the random subsampling scheme, the proposed estimator (\(\)) also consistently surpasses other estimators in every performance metric.

trained causal inference model to estimate the potential outcomes and the treatment effects for each unit. Then, we use the logistic regression-based treatment assignment scheme as in Section 5.1 to simulate a hypothetical conditionally randomized experiment (for both continuous treatment and binary treatment, we will assign \(T=1\) for the treatment group and \(T=0\) for the control group). Both our pairs estimator as well as other baselines presented in Section 5.1 will be used to estimate the causal error. This is repeated 100 times across 3 different settings for treatment assignment imbalance (\(_{}^{2}=1,5,10\)).

Results.Figure 42 shows that using our estimator with conditionally randomized data, we achieved near-RCT performance for causal model evaluation quality across all metrics. It is also more robust to different settings of \(_{}^{2}\), where no significant variance change is observed. Also, we note that each causal error estimator has a different sweet-spot: for instance, the naive estimator usually consistently works very well with Causal Forest, whereas our estimator has relatively high variance and bias for DML Kernel. Nevertheless, the proposed estimator is still the most robust estimator (apart from RCT) that consistently achieves the best results across all causal models. This shows the feasibility of our method for reliable model evaluation with conditionally randomized experiments.

### Validation of Assumption A in Section 4.2

SettingsFinally, we provide experiments to validate Assumption A in Section 4.2. Specifically, we use a numerical example to check if Assumption A holds in practice for various commonly used models, including the ones considered in Section 5.2, including DML Linear, DML kernel, Causal Forest, and variations of doubly robust algorithms (DR Linear, DR forest, DR Orth Forest). We first fit different methods using the observational data generated in Section 5.2, and obtain the counterfactual predictions from each method; Then, we parameterize the function \(V()\) in Equation 7 as a polynomial function and fit for the counterfactual predictions using Equation 7.

Figure 4: Performance of different causal error estimators across different causal inference methods. Each row of the grid corresponds to a specific performance metric (Variance/Bias/MSE), while each column represents different levels of treatment assignment imbalance (\(_{}^{2}\)). In each plot, the x-axis represents different causal models. The y-axis displays performance metrics. Different colors correspond to different estimators. The results highlight the differences in performance among the estimators, with the pairs estimator (purple) consistently outperforming others in most scenarios, emphasizing its robustness and effectiveness in assessing various causal models.

Finally, we solve for the residuals \(\) in Equation 7, and test whether they are independent from \(Y^{T}\) and \(b\), as postulated in Assumption A.

ResultsAs shown in Figure 5, we calculate the Pearson correlation coefficient between \(\) and \(Y^{T}\) and \(b\), respectively, as shown in the upper plot; we also compute the p-values for the null hypothesis that the samples are independent. We observe that the samples have close zero correlations as well as high p-values that are unable to reject the null assumption. Hence, both results shows suggests that the independence multiplicative noise assumption is likely to be true under this setting. We have also tried other tests such as Hoeffding's independence tests that captures non-linear dependencies, which yields similar results. This indicates that Assumption A is not overly restrictive and can be satisfied by a wide range of causal models that are popular in practical applications.

## 6 Conclusions

In this paper, we proposed the pairs estimator, a novel methodology for low-variance estimation of causal error in conditionally randomized trials. This approach applies the same IPW method to both the model and ground truth effects, canceling out the variance due to IPW. Remarkably, the pairs estimator can achieve near-RCT performance using conditionally randomized experiments, signifying a novel contribution for enabling more reliable and accessible model evaluation, without depending on expensive or infeasible randomized experiments. Future work may extend our method to more complex scenarios, explore alternative ways to reduce causal error estimation variance, and apply our method to other causality applications such as policy evaluation, causal discovery, and counterfactual analysis.