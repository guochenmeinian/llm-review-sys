# IWBVT: Instance Weighting-based Bias-Variance Trade-off for Crowdsourcing

Wenjun Zhang

School of Computer Science

China University of Geosciences

Wuhan 430074, China

wjzhang@cug.edu.cn

&Liangxiao Jiang

School of Computer Science

China University of Geosciences

Wuhan 430074, China

ljiang@cug.edu.cn

&Chaoqun Li

School of Mathematics and Physics

China University of Geosciences

Wuhan 430074, China

chqli@cug.edu.cn

Corresponding author

###### Abstract

In recent years, a large number of algorithms for label integration and noise correction have been proposed to infer the unknown true labels of instances in crowdsourcing. They have made great advances in improving the label quality of crowdsourced datasets. However, due to the presence of intractable instances, these algorithms are usually not as significant in improving the model quality as they are in improving the label quality. To improve the model quality, this paper proposes an instance weighting-based bias-variance trade-off (IWBVT) approach. IWBVT at first proposes a novel instance weighting method based on the complementary set and entropy, which mitigates the impact of intractable instances and thus makes the bias and variance of trained models closer to the unknown true results. Then, IWBVT performs probabilistic loss regressions based on the bias-variance decomposition, which achieves the bias-variance trade-off and thus reduces the generalization error of trained models. Experimental results indicate that IWBVT can serve as a universal post-processing approach to significantly improving the model quality of existing state-of-the-art label integration algorithms and noise correction algorithms. Our codes and datasets are available at https://github.com/jiangliangxiao/IWBVT.

## 1 Introduction

Crowdsourcing eases the difficulty of obtaining training datasets for supervised learning . In crowdsourcing scenarios, instances are annotated not by domain experts but by crowd workers from crowdsourcing platforms . While crowd workers are more cost-effective compared to domain experts, they typically possess inferior expertise and are thus more prone to assigning noisy labels . To mitigate the impact of noisy labels, a common practice in crowdsourcing is _repeated annotating_, where each instance is annotated by multiple workers to obtain multiple noisy labels . Subsequently, a multitude of algorithms have been proposed to infer the unknown true label of an instance from its multiple noisy labels .

Specifically, these proposed algorithms can be roughly classified into two categories, namely _label integration_ algorithms and _noise correction_ algorithms. Label integration algorithms aim to integrate multiple noisy labels of each instance to infer an integrated label that is as close as possible to its unknown true label . Noise correction algorithms focus on identifying and correcting noise in integrated labels obtained from label integration algorithms . Therefore, both label integration algorithms and noise correction algorithms inevitably pay more attention to the label quality of crowdsourced datasets, i.e., the proportion of instances in crowdsourced datasets whose integrated labels are equal to the unknown true labels. Indeed, these proposed algorithms have achieved empirical success in improving the label quality of crowdsourced datasets.

However, due to the presence of intractable instances, these algorithms often fall short of achieving anticipated improvements in the model quality. Here, the model quality is the proportion of instances whose predicted labels are equal to the unknown true labels when models classify test instances. On the one hand, the proportion of intractable instances in datasets tends to be low, which makes them have less impact on the label quality. For this reason, the above algorithms pay little attention to intractable instances in improving the label quality. On the other hand, the reason why intractable instances are hard to label and infer is that their attributes are ambiguous. Ambiguous attributes affect the effectiveness of models in learning classification rules from crowdsourced datasets. Therefore, intractable instances have a greater impact on the model quality compared to the label quality. Considering the fact that collecting high-quality integrated labels is ultimately aimed at training high-quality models, improving the model quality should be paid more attention compared to improving the label quality in crowdsourcing.

To improve the model quality, this paper proposes an instance weighting-based bias-variance trade-off (IWBVT) approach for crowdsourcing. IWBVT at first proposes a novel instance weighting method based on the idea of complementary set and entropy, which mitigates the impact of intractable instances and thus makes the bias and variance of trained models closer to the unknown true results. Subsequently, IWBVT performs probabilistic loss regressions based on the bias-variance decomposition, which achieves the bias-variance trade-off and thus reduces the generalization error of trained models. In general, the contributions of this paper can be summarized as follows:

* We focus on the performance of existing algorithms in terms of the model quality and reveal that existing algorithms are not as significant in improving the model quality as they are in improving the label quality.
* We propose a novel instance weighting method based on the complementary set and entropy. This new instance weighting method is more robust and can be applied to more complex crowdsourced scenarios.
* We propose IWBVT to improve the model quality. IWBVT mitigates the impact of intractable instances by instance weighting and achieves the bias-variance trade-off by probabilistic loss regressions.
* We demonstrate that IWBVT can serve as a universal post-processing approach to significantly improving the model quality of existing state-of-the-art label integration algorithms and noise correction algorithms.

## 2 Related work

With _repeated annotating_, crowdsourcing collects multiple noisy labels for each instance in datasets. Subsequently, label integration is usually used to integrate multiple noisy labels to infer the unknown true label for each instance. Initiating the area of label integration,  leveraged an expectation-maximization (EM) algorithm to estimate a confusion matrix, which models workers and class priors in clinical diagnostics. In contrast,  performed majority voting based on multiple noisy labels of instances, and the class receiving the highest number of votes was determined as the integrated label. Enhancing this concept by incorporating worker reliability,  performed weighted majority voting by iteratively estimating worker weights and integrated labels.  further proposed max-margin majority voting, which integrates labels by maximizing the margin between classes. Recently,  augmented the label space for an instance by considering the labels of its neighbors, which distinguishes the impact of different neighbors by instance weighting. Inspired by label distribution learning  proposed multiple noisy label distribution propagation, which absorbs label distributions of neighboring instances into the label distribution of the focal instance.

No matter how powerful the label integration algorithms are, a certain degree of noise is always present in integrated labels. Subsequently, noise correction has been proposed to identify and correct these noises. Initiating the area of noise correction,  introduced three distinct algorithms: _polishing labels_ (PL), _self-training correction_ (STC), and _cluster-based correction_ (CC). PL divides datasets into subsets to train multiple models and then performs majority voting based on the models' predictions to correct the original integrated labels. STC filters the dataset into a clean set and a noise set, iteratively training models on the clean set to predict and correct instances in the noise set. CC estimates the probability of each instance belonging to each class through repeated clustering and performs weighted majority voting to correct original integrated labels. Forgoing the above three algorithms,  adaptively estimated the proportion of noise in datasets based on multiple noisy labels to filter out a clean set and a noise set. Recently, drawing from multi-view learning ,  used multi-view learning for correcting noise instances. They trained dual models on the attribute and multiple noisy label views of the clean set to correct instances in the noise set.  focused on the effect of neighboring instances on noise filtering before noise correction, utilizing multiple noisy label distributions of neighbors to identify noise instances more accurately.

In essence, both label integration algorithms and noise correction algorithms aim to improve the model quality by improving the label quality. Unfortunately, due to the presence of intractable instances, these algorithms are usually not as significant in improving the model quality as they are in improving the label quality. Currently, although there exist several supervised or semi-supervised approaches focused on improving the model quality from noisy training datasets [7; 14; 28], they are not perfectly applicable to crowdsourcing. On the one hand, they cannot utilize the multiple noisy labels specific to crowdsourcing. On the other hand, semi-supervised approaches typically assume that true labels of a few instances are known, which cannot be satisfied in crowdsourcing. In this context, we propose IWBVT, as the first universal post-processing approach to improve the model quality of both label integration algorithms and noise correction algorithms in crowdsourcing.

## 3 Notations and preliminaries

Let \(D\) denote a crowdsourced dataset \(\{(_{i},_{i})\}_{i=1}^{N}\), where \(N\) represents the number of instances, and \(_{i}\) is the \(i\)-th instance, represented as \(\{x_{i1},,x_{im},,x_{iM}\}\). Here, \(M\) signifies the dimension of attributes and \(x_{im}\) denotes the attribute value of \(x_{i}\) on the \(m\)-th attribute \(A_{m}\). \(_{i}\) denotes multiple noisy labels of \(_{i}\), which can be represented as \(\{l_{ir}\}_{r=1}^{R}\). \(R\) denotes the number of workers, \(l_{ir}\) denotes the label of \(_{i}\) annotated by the \(r\)-th worker \(u_{r}\). \(l_{ir}\) takes a value from \(\{-1,c_{1},,c_{q},,c_{Q}\}\), where \(Q\) denotes the number of classes, \(c_{q}\) denotes the \(q\)-th class and \(-1\) denotes that \(u_{r}\) does not annotate \(_{i}\). The purpose of label integration and noise correction is to infer an integrated label \(_{i}\) for \(_{i}\) and to minimize the error between \(_{i}\) and the unknown true label \(y_{i}\).

### Instance weighting for crowdsourcing

Given \((_{i},_{i})\), the weight of \(_{i}\) is denoted by \(w_{i}\). Intuitively, the smaller the value of \(w_{i}\), the more likely that \(_{i}\) is an intractable instance. To estimate \(w_{i}\), \(_{i}\) is first transformed into a multiple noisy label distribution \(_{i}=\{P(c_{q}|_{i})\}_{q=1}^{Q}\), where the probability \(P(c_{q}|_{i})\) reflects the proportion of labels in \(_{i}\) that take the value \(c_{q}\). Subsequently, several representative instance weighting methods have been proposed based on \(_{i}\). First,  proposed estimating \(w_{i}\) by \(P(_{i}|_{i})\), i.e., \(w_{i} P(_{i}|_{i})\). Take MV as an example, the probability \(P(_{i}|_{i})\) consistently equals the maximum value in \(_{i}\). This method is usually effective when \(Q=2\). However, when \(Q>2\), \(P(_{i}|_{i})\) is not sufficient to distinguish different distributions, such as \(\{0.5,0.3,0.2\}\) and \(\{0.5,0.4,0.1\}\).

Subsequently,  proposed estimating \(w_{i}\) by the entropy of \(_{i}\), i.e., \(w_{i}_{i})}\), where

\[Ent(_{i})=-_{q=1}^{Q}P(c_{q}|_{i}) P(c_{q}|_{i}).\] (1)

Based on the maximum entropy principle, when \(_{i}\) conforms more closely to the uniform distribution, the entropy \(Ent(_{i})\) increases, leading to a decrease in the corresponding weight \(w_{i}\). Though entropy-based methods can weight instances in multi-class datasets, they still fail to distinguish some complex distributions, such as \(\{0.4,0.3,0.3\}\) and \(\{0.4,0.4,0.2\}\). Recently,  proposed estimating \(w_{i}\) by the class margin as follows:

\[w_{i}(_{i})-(_{i}),\] (2)

where \((_{i})\) and \((_{i})\) denote the largest and second largest values in \(_{i}\), respectively. This method focuses on the confusing classes in crowdsourced datasets. Nevertheless, it still struggles to distinguish some complex distributions, such as \(\{0.5,0.3,0.1,0.1\}\) and \(\{0.4,0.2,0.2,0.2\}\).

In addition to these methods, there are also a few methods that estimate \(w_{i}\) with classification models or evolutionary algorithms . However, these methods have not been discussed here as their performance is affected by the selected models, loss functions, parameter settings, etc.

### Bias-variance decomposition

The bias-variance decomposition is an effective way to analyze the generalization error of models. Referring to , given a model \(f\), its generalization error \(_{f}\) can be denoted as follows:

\[_{f}=_{i=1}^{N}P(_{i})(bias_{i}^{2}+var_{i}+_ {i}^{2}),\] (3)

where \(P(_{i})\) denotes the probability of selecting \(_{i}\) from \(D\). \(bias_{i}^{2}\) and \(var_{i}\) denote the bias term and the variance term, respectively. They are estimated as follows:

\[bias_{i}^{2}=_{q=1}^{Q}P(c_{q}|_{i})-P(c_{q}|f, {x}_{i})^{2},\] (4)

\[var_{i}=1-_{q=1}^{Q}P(c_{q}|f,_{i})^{2},\] (5)

where \(P(c_{q}|_{i})\) denotes the true probability that \(_{i}\) belongs to \(c_{q}\), \(P(c_{q}|f,_{i})\) denotes the probability that \(f\) classifies \(_{i}\) into \(c_{q}\) in multiple results generated by cross-validation. Therefore, \(P(c_{q}|_{i})\) is independent of \(f\) and is only related to \(D\). \(_{i}^{2}\) denotes the noise term, which is also only related to \(D\).

## 4 Approach

The primary objective of IWBVT is to improve the model quality through bias-variance trade-off. However, in crowdsourcing scenarios, \(P(c_{q}|_{i})\) can only be roughly estimated because \(y_{i}\) is unknown and \(_{i}\) is inaccurate. To estimate the bias and variance of \(f\) as accurately as possible, IWBVT first mitigates the impact of intractable instances by instance weighting. Subsequently, to achieve the bias-variance trade-off, IWBVT learns probabilistic loss by probabilistic loss regressions.

### Instance weighting

As previously mentioned, existing instance weighting methods struggle to distinguish certain complex distributions effectively. In response, IWBVT introduces a novel instance weighting method that leverages the complementary set and entropy to overcome this limitation. The innovative aspects of this method are depicted in Figure 1. As shown in the figure, \(P(_{i}|_{i})\) reflects the proportion

Figure 1: The illustration of our new instance weighting method.

of the integrated label in \(_{i}\). A higher \(P(_{i}|_{i})\) suggests more workers reach a consensus on \(_{i}\), thereby indicating a decreased likelihood of \(_{i}\) being an intractable instance. To extend our method to multi-class datasets, we also focus on the entropy of \(}_{i}\), i.e., \(Ent(}_{i})\). Here, \(}_{i}\) is the complementary set of \(\{P(_{i}|_{i})\}\) in \(_{i}\) (\(\{P(c_{q}|_{i})\}_{q=1}^{Q}\)). Intuitively, workers tend to reach a consensus on a special class on tractable instances, so they should be more randomized on other classes. The entropy of \(}_{i}\) reflects the degree of randomization. Accordingly, our instance weighting method considers four cases, shown to the right side of the arrow in Figure 1. Among them, the case in the upper left corner indicates that when \(Ent(}_{i})\) is fixed, a lower \(P(_{i}|_{i})\) results in a lower \(w_{i}\). Conversely, the case in the upper right corner indicates that a higher \(P(_{i}|_{i})\) results in a higher \(w_{i}\). The case in the lower left corner indicates that when \(P(_{i}|_{i})\) is fixed, a lower \(Ent(}_{i})\) results in a lower \(w_{i}\). The case in the lower right corner indicates that the higher \(Ent(_{i})\) results in a higher \(w_{i}\). To cover these cases, specifically, we estimate \(w_{i}\) as follows:

\[w_{i}=P(_{i}|_{i})}_{i})}{(Q-1)},\] (6)

where \((Q-1)\) is the normalization factor. When \(Q=2\), we set \(}_{i})}{(Q-1)}\) to 1.

To demonstrate the superiority of our weighting method over existing methods, we calculate instance weights with each method on all the complex distributions mentioned in Section 3.1. Table 1 reports the detailed comparison results. Here, "\(\)" and "\(\)" indicate whether the weighting method is effective in distinguishing the corresponding complex distribution, respectively. Empirically, the weight corresponding to the front distribution in each example should be higher than the latter. The results show that only our method can distinguish all these types of complex distributions, while existing methods cannot. By Eq. (6), we calculate weights of all instances as \(=\{w_{i}\}_{i=1}^{N}\).

**Theorem 1**.: _When \(Ent(}_{i})\) remains constant, Eq. (6) covers \(w_{i} P(_{i}|_{i})\). When \(Q>2\) and \(P(_{i}|_{i})\) is the maximum value in \(_{i}\), Eq. (6) covers \(w_{i}(_{i})-(_{i})\)._

Proof.: When \(Q=2\), we set \(Ent(}_{i})/(Q-1)\) to 1, so Eq. (6) simplifies to \(w_{i} P(_{i}|_{i})\). \(w_{i} P(_{i}|_{i})\) still holds in Eq. (6) when \(Q>2\) and \(Ent(}_{i})\) remains constant. When \(Q>2\) and \(P(_{i}|_{i})\) remains constant, \(w_{i} Ent(}_{i})\) holds. According to the maximum entropy principle, \(Ent(}_{i})\) takes its maximum value when any element of \(}_{i}\) is equal to \(_{i}|_{i})}{Q-1}\). At this point, if \(P(_{i}|_{i})\) is the maximum value in \(_{i}\), \((_{i})-(_{i})\) takes its maximum value. Conversely, when \(Ent(}_{i})\) takes its minimum value, \((_{i})-(_{i})\) also takes its minimum value. Therefore, \(w_{i}(_{i})-(_{i})\) holds when \(Q>2\) and \(P(_{i}|_{i})\) is the maximum value in \(_{i}\). Due to the limited pages, more detailed proof of Theorem 1 is provided in Appendix A. 

### Bias-variance trade-off

After instance weighting, the impact of intractable instances is mitigated. Therefore, \(P(c_{q}|_{i})\) can be calculated more accurately, and then bias and variance can be estimated more accurately. Based on this result, the bias-variance trade-off will be more effective. After instance weighting, we train a classification model \(f\) on \(D\) with \(\). Let \(C\) denote the label space \(\{c_{1},,c_{q},,c_{Q}\}\), in this paper, \(f\) classifies the test instance \(\) with the bias-variance trade-off as follows:

\[c()=*{arg\,max}_{c_{q} C}(f(c_{q}|)+h_{q}()),\] (7)

where \(f(c_{q}|)\) denotes the probability that \(\) belongs to \(c_{q}\) predicted by \(f\). \(h_{q}()\) is the prediction of the regression model \(h_{q}\) trained on the following probabilistic loss regression task:

\[T_{q}=(};;}_{q}),\] (8)

   Complex distributions & \(P(_{i}|_{i})\) & \(}_{i})}\) & \((_{i})-(_{i})\) & \(P(_{i}|_{i})}_{i})}{(Q-1)}\) \\  \{0.5, 0.3, 0.2\} \& \{0.5, 0.4, 0.1\} & 0.50 \& 0.50 \& 0.67 \& 0.73 \(\) & 0.20 \& 0.10 \(\) & 0.70 \& 0.52 \(\) \\ \{0.4, 0.3, 0.3\} \& \{0.4, 0.4, 0.2\} & 0.40 \& 0.40 \& 0.64 \& 0.66 \& 0.10 \& 0.00 \(\) & 0.58 \& 0.53 \(\) \\ \{0.5, 0.3, 0.1, 0.1\} \& \{0.4, 0.2, 0.2, 0.2\} & 0.50 \& 0.40 \& \(\) & 0.59 \& 0.52 \(\) & 0.20 \& 0.20 \& 0.20 \& 0.52 \& 0.58 \& 0.58 \(\) \\   

Table 1: The comparison results of instance weighting methods on complex distributions.

where \(}\) is the attribute matrix consisting of all training instances. \(}_{q}\) is the probabilistic loss vector for \(c_{q}\), which can be represented as \(\{_{1q},,_{iq},,_{Nq}\}^{T}\). \(_{iq}\) is calculated as follows:

\[_{iq}=1-f(c_{q}|_{i})&c_{q}=_{i}\\ 0-f(c_{q}|_{i})&c_{q}_{i}.\] (9)

**Theorem 2**.: _When the probabilistic loss is defined as in Eq. (9), performing probabilistic loss regressions constructed by Eq. (8) ensures that Eq. (7) asymptotically achieves the bias-variance trade-off._

Proof.: When \(f\) is adjusted, \(P(c_{q}|f,_{i})\) changes with \(f\), and this change is denoted as \(_{iq}\). Let \(_{i}^{2}\) and \(vr_{i}\) denote the changed bias term and variance term, they can be calculated as follows:

\[_{i}^{2}=bias_{i}^{2}+_{q=1}^{Q}_{iq}P(c_{q}|f,_{ i})+_{q=1}^{Q}_{iq}^{2}-_{q=1}^{Q}_{iq}P(c_{q}| _{i}).\] (10)

\[vr_{i}=var_{i}-_{q=1}^{Q}_{iq}P(c_{q}|f,_{i})-_{q=1}^{Q}_{iq}^{2}.\] (11)

Due to the limited pages, more detailed derivation of Eqs. (10) - (11) is provided in Appendix B. Comparing Eq. (10) and Eq. (11) shows that the common terms \(_{q=1}^{Q}_{iq}P(c_{q}|f,_{i})\) and \(_{q=1}^{Q}_{iq}^{2}\) in \(_{i}^{2}\) and \(vr_{i}\) have opposite signs. Therefore, when improving \(_{f}\), the bias and variance tend to change in opposite trends, which is known as the bias-variance dilemma. Improving \(_{f}\) by synergistically considering changes in both bias and variance is known as the bias-variance trade-off. According to Eqs. (3), (10), and (11), we can get the changed \(}_{f}\) as follows:

\[}_{f}=_{f}-_{i=1}^{N}P(_{i})_{q=1}^{Q} _{iq}P(c_{q}|_{i}).\] (12)

In general, when \(c_{q}\) is the true label of \(_{i}\), \(P(c_{q}|_{i})\) tends to 1, otherwise it tends to 0. However, the true label \(y_{i}\) is unknown in crowdsourcing scenarios. After instance weighting, the impact of intractable instances is mitigated, so we assume that \(_{i}\) is equal to \(y_{i}\). Therefore, when \(c_{q}_{i}\), \(_{iq}P(c_{q}|_{i})\) tends to 0. When \(c_{q}=_{i}\), since the probability terms \(P(_{i})\) and \(P(c_{q}|_{i})\) in Eq. (12) are non-negative, \(}_{f}\) is guaranteed to be less than \(_{f}\) as long as \(_{iq}\) is greater than 0. In summary, the key factor of the bias-variance trade-off is \(_{iq}\) (\(c_{q}=_{i}\)). To make \(_{iq}\) (\(c_{q}=_{i}\)) greater than 0, the following optimization task can be constructed:

\[&_{i}}{maximize} f(_{i}| _{i})\\ s.t.& f(_{i}|_{i})-_{c_{q} C  c_{q}_{i}}f(c_{q}|_{i}) 0.\] (13)

Here, maximizing \(f(_{i}|_{i})\) ensures that \(_{iq}\) (\(c_{q}=_{i}\)) is greater than 0, while the constraint ensures that the prediction of \(f\) will be \(_{i}\). Then, according to the Lagrange multiplier, the Lagrange function can be constructed as follows:

\[L(_{i})=f(_{i}|_{i})+f(_{i}|_{i}) -_{c_{q} C c_{q}_{i}}f(c_{q}|_{i}),\] (14)

where \( 0\). For simplicity, \(L^{}(_{i})\) can be further constructed as follows:

\[ L^{}(_{i})&=L(_{i}) -_{c_{q} C c_{q}_{i}}f(c_{q}|_{i})\\ &=(1+)f(_{i}|_{i})-_{c_{q} C  c_{q}_{i}}f(c_{q}|_{i}).\] (15)

Since the probability \(_{c_{q} C c_{q}_{i}}f(c_{q}|_{i}) 0\), so \(L(_{i}) L^{}(_{i})\). Ultimately, Eq. (13) can be optimized to achieve a better result by maximizing \(L^{}(_{i})\). At the same time, since \( 0\), the value of \(L^{}(_{i})\) is positively correlated with the following difference:

\[f(_{i}|_{i})-_{c_{q} C c_{q}_{i}}f(c_{q}| _{i}).\] (16)

According to Eq. (9), through probabilistic loss regressions, when \(c_{q}=_{i}\), \(f(c_{q}|)+h_{q}()\) in Eq. (7) tends to 1. Conversely, when \(c_{q}_{i}\), \(f(c_{q}|)+h_{q}()\) tends to 0. Therefore, Eq. (7) is effective in maximizing the difference Eq. (16). Ultimately, Theorem 2 is proved.

The whole learning process of IWBVT is shown in Algorithm 1. In Algorithm 1, lines 1-3 learn a weight for each instance and their time complexity is \(O(NRQ)\). Line 4 trains a classification model \(f\) whose training time complexity is denoted as \(O(t_{1})\). Lines 5-11 learn a probabilistic loss regression model \(h_{q}\) for each class \(c_{q}\) and their time complexity is \(O(Q(Nt_{2}+t_{3}))\). Here, \(t_{2}\) denotes the prediction time complexity of \(f\) on each class and \(t_{3}\) denotes the training time complexity of \(h_{q}\). In this paper, we select NB as the classification model and linear regression as the regression model. Therefore, \(t_{1}\), \(t_{2}\), \(t_{3}\) are equal to \(O(NM)\), \(O(M)\), and \(O(NM^{2}+M^{3})\), respectively. If only the highest order terms are taken, the time complexity of IWBVT is \(O(NRQ+NQM^{2}+QM^{3})\).

```
0:\(=\{(_{i},_{i},_{i})\}_{i=1}^{N}\) - a crowdsourced dataset with integrated labels.
0: classification model \(f\), regression model set \(\).
1:for\(i=1\) to \(N\)do
2: Calculate the weight \(w_{i}\) of \(_{i}\) by Eq. (6);
3:endfor
4: Train the classification model \(f\) on \(\) with \(=\{w_{i}\}_{i=1}^{N}\);
5:for\(q=1\) to \(Q\)do
6:for\(i=1\) to \(N\)do
7: Calculate the probabilistic loss \(_{iq}\) by Eq. (9);
8:endfor
9: Construct the regression task \(T_{q}\) by Eq. (8);
10: Learn the regression model \(h_{q}\) on \(T_{q}\);
11:endfor
12:return classification model \(f\), \(=\{h_{1},h_{2},,h_{Q}\}\). ```

**Algorithm 1** The learning process of IWBVT

## 5 Experiments

To validate the effectiveness of IWBVT, we conduct a series of experiments on the whole 34 simulated and 2 real-world crowdsourced datasets published on the Crowd Environment and its Knowledge Analysis (CEKA)  platform. First, we illustrate the setup of our experiments, including comparison algorithms and their parameter settings. Next, we describe the simulation process and present the simulated experimental results in terms of the model quality. Finally, to further validate the strength of IWBVT, we analyze the experimental results of comparative experiments and ablation experiments on real-world crowdsourced datasets.

### Experimental setup

We select seven state-of-the-art algorithms for experiments, including: _majority voting_ (MV) , _iterative weighted majority voting_ (IWMV) , _label augmented and weighted majority voting_ (LAWMV) , _multiple noisy label distribution propagation_g (MNLDP) , _adaptive voting noise correction_ (AVNC) , _multi-view-based noise correction_ (MVNC) , and _neighborhood weighted voting-based noise correction_ (NWVNC) . Among them, MV is the simplest label integration algorithm and is used as a baseline for all algorithms. IWMV, LAWMV and MNLDP are three state-of-the-art label integration algorithms. AVNC, MVNC and NWVNC are three state-of-the-art noise correction algorithms. They are used to validate the effectiveness of IWBVT for label integration and noise correction. All these algorithms are implemented based on the CEKA platform and their parameter settings are consistent with the corresponding published papers. AVNC, MVNC and NWVNC are all performed based on integrated labels inferred by MV. Besides, we use linear regression as \(h_{q}\) in IWBVT. All experiments are conducted on a Windows 10 machine with an AMD Athlon(tm) X4 860K Quad Core Processor @ 3.70 GHz and 16 GB of RAM.

### Experiments on simulated datasets

Datasets and simulation process.We conduct our simulated experiments on all simulated datasets published on the CEKA platform. These datasets come from a wide variety of application domains and represent plentiful crowdsourcing scenarios. Considering that the selected label integration algorithms and noise correction algorithms handle the missing values of datasets differently, we use the unsupervised attribute filter _ReplaceMissingValues_ in the Waikato Environment and Knowledge Analysis (WEKA)  platform to replace all missing values. Specifically, _ReplaceMissingValues_ uses the mean of numerical attribute values or the modes of the nominal attribute values from the available data to replace missing values. Subsequently, to generate multiple noisy labels for each instance, we simulate the crowdsourcing process for these datasets. First, we randomly generate five workers whose label quality follows a normal distribution with N(0.65, 0.05\({}^{2}\)). The label quality of a worker reflects the probability that the noisy label annotated by this worker to an instance is the same as this instance's unknown true label. Then, we hide true labels and use these simulated workers to annotate datasets. Finally, we use the selected algorithms to infer integrated labels for these datasets. For each simulation, we evaluate the original model quality and the corresponding model quality improved using IWBVT through stratified 10-fold cross-validation. Here, we use Naive Bayes (NB)  as the target model. The above processes are repeated ten times independently for each algorithm on each dataset.

Experimental results.Table 2 shows the detailed model quality (%) comparisons of each algorithm on each dataset, respectively. The columns _ORI_ and _IWBVT_ correspond to the original model quality and the model quality using IWBVT, respectively. The symbols \(\) and \(\) in the table denote the model quality has a statistically significant improvement or degradation using our proposed IWBVT with a corrected paired two-tailed t-test with the significance level \(\) = 0.05 , respectively. Besides, the averages and the _Win/Tie/Lose_ (\(W/T/L\)) values are summarized at the bottom of Table 2. The \(W/T/L\) implies that when improving the original model quality, IWBVT wins on \(W\) datasets, ties on \(T\) datasets, and loses on \(L\) datasets. These experimental results validate the effectiveness of IWBVT, and we can summarize the following highlights:

* The average model quality of MV using IWBVT on 34 datasets is 79.03%, which is higher than the original model quality of all selected algorithms. This demonstrates both the limitations of label integration algorithms or noise correction algorithms and the effectiveness of IWBVT in improving the model quality.

    &  &  &  &  &  &  &  \\   & ORI & IWBVT & ORI & IWBVT & ORI & IWBVT & ORI & IWBVT & ORI & IWBVT & ORI & IWBVT \\  anneal & 68.60 & 80.42 \(\) & 68.27 & 81.29 \(\) & 75.18 & 80.27 \(\) & 67.84 & 81.15 \(\) & 74.83 & 81.39 \(\) & 73.94 & 81.96 \(\) & 67.84 & 79.49 \(\) \\ radiology & 59.93 & 61.39 \(\) & 58.05 \(\) & 60.08 \(\) & 61.15 \(\) & 63.49 \(\) & 58.43 \(\) & 60.50 \(\) & 63.93 \(\) & 65.49 \(\) & 64.77 \(\) & 64.09 \(\) & 59.10 \(\) & 61.50 \\ auto & 54.31 & 59.98 \(\) & 52.13 \(\) & 58.52 \(\) & 56.00 \(\) & 67.35 \(\) & 56.12 \(\) & 59.24 \(\) & 57.55 \(\) & 59.97 \(\) & 57.01 \(\) & 60.61 \(\) & 65.61 \(\) \\ balance-scale & 86.57 & 86.77 & 83.88 \(\) & 88.68 \(\) & 86.82 \(\) & 90.28 \(\) & 88.59 \(\) & 82.44 \(\) & 84.98 \(\) & 87.37 \(\) & 84.34 \(\) & 87.12 \(\) & 86.58 \(\) & 88.28 \(\) \\ biode & 68.34 & 72.28 \(\) & 68.33 \(\) & 72.26 \(\) & 76.05 \(\) & 77.02 \(\) & 77.53 \(\) & 73.41 \(\) & 70.55 \(\) & 70.43 \(\) & 75.19 \(\) & 72.76 \(\) & 74.77 \(\) \\ breast-cancer & 70.41 \(\) & 67.75 \(\) & 70.24 \(\) & 68.13 \(\) & 72.50 \(\) & 71.56 \(\) & 70.55 \(\) & 69.00 \(\) & 72.86 \(\) & 72.80 \(\) & 72.89 \(\) & 72.22 \(\) & 72.62 \(\) & 72.83 \\ breast-w & 96.18 \(\) & 94.99 \(\) & 96.12 \(\) & 95.10 \(\) & 96.04 \(\) & 86.07 \(\) & 69.11 \(\) & 96.15 \(\) & 96.01 \(\) & 95.87 \(\) & 96.29 \(\) & 96.25 \(\) & 96.16 \(\) \\ car & 77.85 & 81.94 \(\) & 80.09 \(\) & 83.09 \(\) & 71.12 \(\) & 72.28 \(\) & 73.07 \(\) & 76.36 \(\) & 80.02 \(\) & 81.59 \(\) & 76.93 \(\) & 81.25 \(\) & 75.81 \(\) & 78.72 \(\) \\ credit-a & 77.55 & 81.09 \(\) & 79.91 \(\) & 79.88 \(\) & 81.64 \(\) & 52.99 \(\) & 78.36 \(\) & 80.81 \(\) & 82.49 \(\) & 83.41 \(\) & 79.61 \(\) & 81.12 \(\) & 80.07 \(\) & 81.72 \(\) \\ diatecti-g & 72.38 \(\) & 72.45 \(\) & 72.55 \(\) & 73.01 \(\) & 73.29 \(\) & 72.72 \(\) & 72.79 \(\) & 74.12 \(\) & 73.47 \(\) & 73.47 \(\) & 73.96 \(\) & 73.76 \(\) & 74.41 \\ diabetes & 73.85 \(\) & 74.13 \(\) & 74.24 \(\) & 74.27 \(\) & 72.43 \(\) & 73.01 \(\) & 74.65 \(\) & 75.24 \(\) & 74.75 \(\) & 75.33 \(\) & 74.49 \(\) & 75.20 \(\) & 74.59 \(\) & 75.46 \(\) \\ heart-c* The average model quality of IWMV (78.94%), LAWMV (80.22%), and MNLDP (80.24%) using IWBVT are higher than the original results of these state-of-the-art label integration algorithms. This demonstrates that IWBVT is still effective for more sophisticated label integration algorithms in improving the model quality.
* The average model quality of AVNC (80.77%), MVNC (80.44%), and NWVNC (80.54%) using IWBVT are also higher than the original results of these state-of-the-art noise correction algorithms. This demonstrates that IWBVT can serve as a universal post-processing approach to significantly improving the model quality.
* Based on the t-test results, the number of datasets in which IWBVT wins significantly (\(W\)) is always much higher than the number of datasets in which it loses significantly (\(L\)) for all algorithms. This strongly demonstrates the effectiveness and robustness of IWBVT.

### Experiments on real-world datasets

Datasets.To demonstrate the robustness of IWBVT, the above simulation process pays more attention to common factors of crowdsourcing. However, training models on real-world datasets may also be affected by other factors, such as sparsity and annotating bias. To verify the effectiveness of IWBVT in real-world crowdsourced scenarios, we also construct our experiments on two widely used real-world crowdsourced datasets, _Leaves_ and _Income_, published on the CEKA platform . Here, _Leaves_ and _Income_ are selected through the online platform Amazon Mechanical Turk (AMT). _Leaves_ is annotated by 83 workers and each instance is annotated by 10 workers. There are 6 classes, 384 instances, 3840 labels, 64 numeric attributes, and 0 missing values in _Leaves_. _Income_ is annotated by 67 workers and each instance is also annotated by 10 workers. There are 2 classes, 600 instances, 6000 labels, 10 nominal attributes, and 0 missing values in _Income_. We only evaluate the original model quality and the corresponding model quality using IWBVT by stratified 10-fold cross-validation one time because real-world datasets do not have a random simulation process.

Experimental results.Figure 2 shows the model quality (%) comparisons of MV, IWMV, LAWMV, MNLDP, AVNC, MVNC and NWVNC before and after using IWBVT on _Leaves_ and _Income_. With Figures 1(a) and 1(b), we can find that IWBVT can also serve as a universal post-processing approach to significantly improving the model quality in real-world crowdsourced scenarios. Besides, we can also find the original model quality of several state-of-the-art algorithms is even lower than the original model quality of MV. These results once again demonstrate the limitation of label integration and noise correction in improving the model quality.

Ablation experiment.The above results only demonstrate the effectiveness of IWBVT as a whole, yet they do not delineate the contributions of its two key components: instance weighting and bias-variance trade-off. In IWBVT, instance weighting is used to mitigate the impact of intractable instances to make the bias and variance of trained models closer to the unknown true results. Therefore, to independently verify the effectiveness of instance weighting, we first observe the bias and variance of trained models before and after instance weighting. To estimate the bias and variance, referring to , NB is tested on _Leaves_ and _Income_ by ten runs of three-fold cross-validation. Figure 2(a) shows the bias and variance comparisons before and after using instance weighting on _Leaves_ and _Income_.

Figure 2: The model quality (%) comparisons of MV, IWMV, LAWMV, MNLDP, AVNC, MVNC and NWVNC before and after using IWBVT on _Leaves_ and _Income_.

In Figure 2(a), \(object_{1}\) denotes the estimation results of NB trained directly with true labels. \(object_{2}\) denotes the estimation results of NB trained with integrated labels inferred by MV. The conditions for \(object_{3}\) and \(object_{2}\) are the same, besides considering instance weighting. As can be seen in Figure 2(a), the bias and variance of models trained only with integrated labels are usually higher than unknown true results. When instance weighting is introduced, both the bias and variance of models tend to be closer to unknown true results. These results demonstrate that instance weighting successfully corrects the bias and variance of trained models by mitigating the impact of intractable instances. Therefore, IWBVT performs the instance weighting before the bias-variance trade-off, which is more effective in improving the generalization performance of trained models.

Additionally, we also analyze the effectiveness of another component, the bias-variance trade-off, in improving model quality. Similarly, we still fix the label integration algorithm to be MV, and then introduce the instance weighting and bias-variance trade-off individually to observe their influence in improving model quality. Figure 2(b) shows the model quality (%) comparisons of MV using IWBVT or its components on _Leaves_ and _Income_. In Figure 2(b), \(object_{1}\) denotes the model quality of MV, \(object_{2}\) denotes the model quality of MV using the bias-variance trade-off, \(object_{3}\) denotes the model quality of MV using the instance weighting, and \(object_{4}\) denotes the model quality of MV using the whole IWBVT. As can be seen in Figure 2(b), both the instance weighting and the bias-variance trade-off effectively improve the model quality of MV. This demonstrates that the two components of the IWBVT are both effective. Moreover, the model quality of MV using the whole IWBVT is the highest on both _Leaves_ and _Income_, which suggests that it is reasonable for IWBVT to utilize both components at the same time. In addition, in Figure 2(b), we can also find that the instance weighting is more effective on _Leaves_, while the bias-variance trade-off is more effective on _Income_. This is because the average label quality of _Leaves_ is low. Instance weighting helps to identify rare instances that are inferred correctly, and therefore has a greater impact on _Leaves_. However, the average label quality of _Income_ is high, and more instances can be correctly inferred than in _Leaves_. Therefore, the bias and variance are estimated closer to the unknown true values, so the bias-variance trade-off is more effective on _Income_.

## 6 Conclusion and future work

To improve the model quality of models trained on crowdsourced datasets, we propose a universal post-processing approach called IWBVT. IWBVT first mitigates the impact of intractable instances by instance weighting to make the bias and variance of trained models closer to the unknown true results. Then, IWBVT reduces the generalization error of trained models by the bias-variance trade-off. Experimental results suggest that IWBVT can significantly improve the model quality of existing state-of-the-art label integration algorithms and noise correction algorithms.

Though the above experimental results sufficiently demonstrate the effectiveness of IWBVT, some anomalies are found in experiments. Table 2 shows that IWBVT degrades the model quality on a few datasets such as _labor_ and _lymph_. The datasets such as _labor_ and _lymph_ contain some numerical attributes that are significantly higher in magnitude than other attributes. However, the linear regression chosen for IWBVT in experiments is not robust to regression tasks constructed for these datasets. Therefore, in the future, we will further improve the robustness of IWBVT to make trained models insensitive to anomalous attributes.

Figure 3: The ablation experiment comparisons of IWBVT and its components on _Leaves_ and _Income_.