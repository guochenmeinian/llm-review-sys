# RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks

Jiaixing Zhang\({}^{1}\), Zhuomin Chen\({}^{2}\), Hao Mei\({}^{3}\), Longchao Da\({}^{3}\), Dongsheng Luo\({}^{2}\), Hua Wei\({}^{3}\)

\({}^{1}\)New Jersey Institute of Technology, \({}^{2}\)Florida International University, \({}^{3}\)Arizona State University

\({}^{1}\)jz48@njit.edu, \({}^{2}\){zchen051, dluo}@fiu.edu, \({}^{3}\){hmei7, longchao, hua.wei}@asu.edu

###### Abstract

Graph regression is a fundamental task that has gained significant attention in various graph learning tasks. However, the inference process is often not easily interpretable. Current explanation techniques are limited to understanding Graph Neural Network (GNN) behaviors in classification tasks, leaving an explanation gap for graph regression models. In this work, we propose a novel explanation method to interpret the graph regression models (XAIG-R). Our method addresses the distribution shifting problem and continuously ordered decision boundary issues that hinder existing methods away from being applied in regression tasks. We introduce a novel objective based on the graph information bottleneck theory (GIB) and a new mix-up framework, which can support various GNNs and explainers in a model-agnostic manner. Additionally, we present a self-supervised learning strategy to tackle the continuously ordered labels in regression tasks. We evaluate our proposed method on three benchmark datasets and a real-life dataset introduced by us, and extensive experiments demonstrate its effectiveness in interpreting GNN models in regression tasks.

## 1 Introduction

Graph Neural Networks  (GNNs) have become a powerful tool for learning knowledge from graph-structure data and achieved remarkable performance in many areas, including social networks [2; 3], molecular structures [4; 5], traffic flows [6; 7; 8; 9], and recommendation systems [10; 11; 12]. Despite the success, their popularity in sensitive fields such as fraud detection and drug discovery [13; 14] requires an understanding of their decision-making processes. To address this challenge, some efforts have been made to explain GNN's predictions in a post-hoc manner, which aims to find a sub-graph that preserves the information about the predicted label. On top of the intuitive principle, Graph Information Bottleneck (GIB) [15; 16] maximizes the mutual information \(I(G^{*};Y)\) between the target prediction label \(Y\) and the explanation \(G^{*}\) while constraining the size of the explanation.

However, existing methods focus on the explanation of the classification tasks, leaving another fundamental task, explainable regression, unexplored. Graph regression tasks exist widely in nowadays applications, such as predicting the molecular property  or traffic flow volume . Explaining the instance-level predictions of graph regression is challenging due to two main obstacles. First, in the routinely adopted GIB framework, the mutual information between the explanation sub-graph and label, \(I(G^{*};Y)\), is estimated with the Cross-Entropy between the predictions \(f(G^{*})\) from GNN model \(f\) and its prediction label \(Y\). However, in the regression task, the regression label is the continuous value, making the approximation unsuitable. Another challenge is the distribution shifting problem in the usage of \(f(G^{*})\), where the prediction of the explanation sub-graph made by the GNN model \(f\) is unsafe. Usually, explanation sub-graphs have different topology and feature information compared to the original graph. As a result, explanation sub-graphs are out-of-distribution of theoriginal training graph dataset [19; 20; 21]. As shown in Figure 1, a GNN model \(f\) is trained on the original graph training set and cannot be safely used to make predictions for sub-graphs.

To fill the gap, in this paper, we propose RegExplainer, to generate post-hoc instance-level explanations for graph regression tasks. Specifically, we formulate a theoretical-sound objective for explainable regression based on information theory. To further address the distribution shifting issue, RegExplainer develops a new mix-up approach with self-supervised learning. Our experiments show that RegExplainer provides consistent and concise explanations of GNN's predictions on regression tasks. We achieved up to \(48.0\%\) improvement when compared to the alternative baselines in our experiments. Our contributions can be summarized as follows.

\(\) To our best knowledge, we are the first to explain GNN predictions on graph regression tasks. We addressed two challenges in explaining the graph regression task: the mutual information estimation in the GIB objective and the distribution shifting problem with continuous decision boundaries.

\(\) We proposed a novel model with self-supervised learning and the mix-up approach, which can address the two challenges more effectively, and better explain the graph model on the regression tasks compared to other baselines.

\(\) We designed three synthetic datasets, namely BA-Motif-Volume, BA-Motif-Counting and Triangles, as well as a real-world dataset called Crippen, which can also be used in future works, to evaluate the effectiveness of our regression task explanations. Comprehensive empirical studies on both synthetic and real-world datasets demonstrate that our method can provide consistent and concise explanations for graph regression tasks.

## 2 Related Work and Further Discussions

GNN ExplainabilityThe explanation methods for GNN models can be categorized into two types based on their granularity: instance-level [22; 23; 24; 25] and model-level , where the former methods explain the prediction for each instance by identifying important sub-graphs, and the latter method aims to understand the global decision rules captured by the GNN. These methods can also be classified into two categories based on their methodology: self-explainable GNNs [27; 28] and post-hoc explanation methods [23; 24; 25], where the former methods provide both predictions and explanations, while the latter methods use an additional model or strategy to explain the target GNN. Additionally, CGE  (cooperative explanation) generates the sub-graph explanation with the sub-network simultaneously, by using cooperative learning. However, it has to treat the GNN model as a white box, which is usually unavailable in the post-hoc explanation. Existing methods have only partially addressed the explanation of graph regression tasks and have not fully considered two important challenges: the distribution shifting problem and the limitations of the GIB objective, both of which are key areas our work aims to tackle.

GIB ObjectiveThe Information Bottleneck (IB) [30; 31] provides an intuitive principle for learning dense representations that an optimal representation should contain _sufficient_ information for the downstream prediction task with a _minimal_ size. Based on IB, a recent work  unifies the most existing post-hoc explanation methods for GNN, such as GNNExplainer , PGExplainer , with the graph information bottleneck (GIB) principle [15; 16; 32]. Formally, the objective of explaining

Figure 1: Intuitive illustration of the distribution shifting problem. The 3-dimensional map represents a trained GNN model \(f\), where \((h_{1},h_{2})\) represents the embedding distribution of the graph in two dimensions, and \(Y\) represents the prediction value of the graph through \(f\). The red and blue lines represent the distribution of the original training graph set and the corresponding explanation sub-graph set, respectively. The distribution of \(G^{*}\) shifts away from the original distribution, resulting in shifted prediction values.

the prediction of \(f\) on \(G\) can be represented by

\[*{arg\,min}_{G^{*}}I(G;G^{*})- I(G^{*};Y),\] (1)

where \(G\) is the to-be-explained original graph, \(G^{*}\) is the explanation sub-graph of \(G\), \(Y\) is the original ground-truth label of \(G\), and \(\) is a hyper-parameter to get the trade-off between minimal and sufficient constraints. GIB uses the mutual information \(I(G;G^{*})\) to select the minimal explanation that inherits only the most indicative information from \(G\) to predict the label \(Y\) by maximizing \(I(G^{*};Y)\), where \(I(G;G^{*})\) avoids imposing potentially biased constraints, such as the size or the connectivity of the selected sub-graphs . Through the optimization of the sub-graph, \(G^{*}\) provides model interpretation. In graph classification task, a widely-adopted approximation to Eq. (1) in previous methods [23; 24] is:

\[*{arg\,min}_{G^{*}}I(G;G^{*})+ H(Y|G^{*}) *{arg\,min}_{G^{*}}I(G;G^{*})+(Y,Y^{*}),\]

where \(Y\) and \(Y^{*}\), approximated by \(f(G)\) and \(f(G^{*})\), is the predicted label of \(G\) and \(G^{*}\) made by the to-be-explained model \(f\), and the cross-entropy \((Y,Y^{*})\) between \(Y\) and \(Y^{*}\) is used to approximate \(-I(G^{*};Y)\). The approximation is based on the definition of mutual information \(I(G^{*};Y)=H(Y)-H(Y|G^{*})\): with entropy \(H(Y)\) being static and independent of the explanation process, minimizing the mutual information between the explanation sub-graph \(G^{*}\) and \(Y\) can be reformulated as maximizing the conditional entropy of \(Y\) given \(G^{*}\), which can be approximated by \((Y,Y^{*})\).

## 3 Preliminary

Notation and Problem FormulationWe use \(G=(,;,)\) to represent a graph from an alphabet \(\), where \(\) equals to \(\{v_{1},v_{2},...,v_{n}\}\) represents a set of \(n\) nodes and \(\) represents the edge set. Each graph has a feature matrix \(^{n d}\) for the nodes, wherein \(\), \(X_{i}^{1 d}\) is the \(d\)-dimensional node feature of node \(v_{i}\). \(\) is described by an adjacency matrix \(\{0,1\}^{n n}\), where \(A_{ij}=1\) means that there is an edge between node \(v_{i}\) and \(v_{j}\); otherwise, \(A_{ij}=0\). For the graph prediction task, each graph \(G_{k}\) has a label \(Y_{k}\), where \(k\{1,...,N\}\), \(N\) represents the number of graphs in the dataset, \(\) is the set of the classification categories or regression values in \(\), with a GNN model \(f\) trained to make the prediction, i.e., \(f:(,)\).

**Problem 1** (Post-hoc Instance-level GNN Explanation).: _Given a trained GNN model \(f\), for an arbitrary input graph \(G=(,;,)\), the goal of post-hoc instance-level GNN explanation is to find a sub-graph \(G^{*}\) that can explain the prediction of \(f\) on \(G\)._

In non-graph structured data, the informative feature selection has been well studied , as well as in traditional methods, such as concrete auto-encoder , which can be directly extended to explain features in GNNs. In this paper, we focus on discovering the important sub-graph typologies following the previous work [23; 24]. Specifically, the obtained explanation \(G^{*}\) is depicted by a binary mask \(^{*}\{0,1\}^{n n}\) on the adjacency matrix, e.g., \(G^{*}=(,;,^{*})\), \(\) means elements-wise multiplication. The mask highlights components of \(G\) which are essential for \(f\) to make the prediction.

## 4 Methodology

In this section, we first introduce a new objective based on GIB for explaining graph regression tasks. Then we showcase the distribution shifting problem in the objective for regression and propose a novel framework with the mix-up approach to solve the distribution shifting problem, by incorporating the mix-up approach with self-supervised contrastive learning.

### GIB for Explaining Graph Regression

As introduced in Section \(2\), in the classification task, \(I(G^{*};Y)\) in Eq. (1) is commonly approximated by cross-entropy \((Y^{*},Y)\). However, it is non-trivial to extend it for regression tasks because \(Y\) is a continuous variable and it is intractable to compute the cross-entropy \((Y^{*},Y)\) or the mutual information \(I(G^{*};Y)\), where \(G^{*}\) is a graph variable with a continuous variable \(Y^{*}\) as its label.

#### 4.1.1 Optimizing the Lower Bound of \(I(G^{*};Y)\)

To address the challenge of computing the mutual information \(I(G^{*};Y)\) with a continuous \(Y\), we propose a novel objective for explaining graph regression.

Instead of minimizing \(I(G^{*};Y)\) directly, we propose to maximize a lower bound for the mutual information by including the prediction label of \(G^{*}\), denoted by \(Y^{*}\), and approximate \(I(G^{*};Y)\) in Eq. (1) with \(I(Y^{*};Y)\):

\[*{arg\,min}_{G^{*}}I(G;G^{*})- I(Y^{*};Y).\] (2)

\(I(Y^{*};Y)\) has the following property, upon which we can approximate Eq. (2):

**Property 1**: \(I(Y^{*};Y)\) _is a lower bound of \(I(G^{*};Y)\)._

Intuitively, the property of \(I(Y^{*};Y)\) is guaranteed by the chain rule for mutual information and the independence between each explanation instance \(g^{*}\) in \(G^{*}\). An intuitive demonstration is shown in Figure 2. The proof is shown in the Appendix B.1.

#### 4.1.2 Estimating \(I(Y^{*};Y)\) with InfoNCE

Now the challenge becomes the estimation of the mutual information \(I(Y^{*};Y)\). Inspired by the model of Contrastive Predictive Coding , in which InfoNCE loss is interpreted as a mutual information estimator, we further adapt the objective function so that it can be applied with InfoNCE loss in explaining graph regression. In our graph explanation scenario, the InfoNCE Loss defined in Eq. (3) can also be utilized as a lower bound of \(I(Y^{*};Y)\), as shown in the following property with proofs:

**Property 2**: InfoNCE Loss is a lower bound of the \(I(Y^{*};Y)\):

\[I(Y^{*};Y)*{}_{}[(Y^{*},Y)}{|}_{Y^{}} (Y^{*},Y^{})}],\] (3)

where \(Y^{}\) is the prediction label of the randomly sampled graph neighbors, \(\) is the set of the neighbors' prediction labels, and \(()\) estimates the similarity between \(Y^{*}\) and \(Y\). The proof is shown in the Appendix B.2. Therefore, we have the InfoNCE loss \(_{}\) as the lower bound of the \(I(Y^{*};Y)\). We approximate Eq. (2) as:

\[*{arg\,min}_{G^{*}}I(G;G^{*})-*{}_ {}[(Y^{*},Y)}{|}_{Y^{}}(Y^{*},Y^{} )}].\] (4)

### Distribution Shifting Problem in Graph Regression

We include the prediction label \(Y^{*}\) in Eq. (4) to estimate similarity, which is approximated with \(f(G^{*})\) in previous work . However, we argue that \(f(G^{*})\) cannot be safely obtained due to the distribution shift problem . In classification tasks, a small shift may not cross the decision boundaries, which can still lead to a correct prediction. However, due to the continuous decision boundaries in regression, the distribution problem would cause serious prediction errors. Here in this paper, the graph distribution is indicated by its regression label in the regression task.

Figure 3 shows the existence of distribution shifts between \(f(G^{*})\) and \(f(G)\) in graph regression tasks. For each dataset, we sort the indices of the data samples according to the value of their labels, and visualize the label \(Y\), prediction \(f(G)\) of the original graph from the trained GNN model \(f\), and prediction \(f(G^{*})\) of the explanation sub-graph \(G^{*}\) from \(f\). As we can see in Figure 3, in all four graph regression datasets, the red points are well distributed around the ground-truth blue points, indicating that \(f(G)\) is close to \(Y\). In comparison, the green points shift away from the red points, indicating the shifts between \(f(G^{*})\) and \(f(G)\). Especially in dataset BA-Motif-Counting, the sub-graph explanation distribution was shifted extremely.

Figure 2: Intuitive illustration about why \(I(G^{*};Y) I(Y^{*};Y)\). \(G^{*}\) contains more mutual information as having more overlapping area with \(Y\) than the overlapping area between \(Y^{*}\) and \(Y\).

Intuitively, this phenomenon indicates the GNN model \(f\) can make correct predictions only with the original graph \(G\) yet can not predict the explanation sub-graph \(G^{*}\) correctly. This is because the GNN model \(f\) is trained with the original graph sets, whereas the explanation \(G^{*}\) as the sub-graph is different from the original graph sets. With the shift between \(f(G)\) and \(f(G^{*})\), the optimal solution in Eq. (4) is unlikely to work well.

### Mix-up Approach with Contrastive Learning

To address this distribution-shifting problem in graph regression, we innovatively incorporate the mix-up approach with a self-supervised contrastive learning strategy. Instead of calculating \(Y^{*}\) with \(f(G^{*})\) directly, we approximate with \(Y^{}\) from \(f(G^{})\), which contains similar information as \(G^{*}\) but is in the same distribution with \(G\). Specifically, our approach includes the following steps:

\(\)**Step 1 (Neighbor Sampling):** Learning through the triplet instances can effectively reinforce the ability of the explainer to learn the explanation self-supervised. For each target graph \(G\) with label \(Y\) to be explained, we can define two randomly sampled graphs as positive neighbor \(G^{+}\) and negative neighbor \(G^{-}\), where \(G^{+}\)'s label \(Y^{+}\) is closer to \(Y\) than \(G^{-}\)'s label \(Y^{-}\), i.e., \(|Y^{+}-Y|<|Y^{-}-Y|\). Intuitively, the distance between the distributions of the positive pair \( G,G^{+}\) should be smaller than the distance between the distributions of the negative pair \( G,G^{-}\).

\(\)**Step 2 (Mixup for \(G^{*}\)):** Then we generate two mixup graphs \(G^{}\) and \(G^{}\) by mixing the sub-graph explanation \(G^{*}\) with the label irrelevant sub-graph \((G^{+})^{}=G^{+}-(G^{+})^{*}\) from its positive neighbor \(G^{+}\) and the label irrelevant sub-graph \((G^{-})^{}=G^{-}-(G^{-})^{*}\) from negative neighbor \(G^{-}\) respectively. Specifically, the label mixup approach is calculated from:

\[G^{}=G^{*}+(G^{+})^{}=G^{*}+(G^{+}-(G^{+})^{*}),G^{}=G^{*}+(G^{-})^{}=G^{*}+(G^{-}-(G^{-})^{*}).\]

\(G^{}\) and \(G^{}\) should have the similar information to \(G\) because they have the same label-preserving sub-graphs \(G^{*}\). Additionally, considering the following two pairs: \((G^{},G^{+})\) and \((G^{},G^{-})\). The similarity between \((G^{},G^{+})\) should be larger than the similarity between \((G^{},G^{-})\). Intuitively, since \(G^{}\) and \(G^{}\) have the same label-preserving sub-graphs \(G^{*}\) and \(|Y^{-}-Y|>|Y^{+}-Y|\), we can have \(|f(G^{-})-f(G^{})|>|f(G^{+})-f(G^{})|\), where \(f(G)\) represents the prediction label of graph \(G\).

\(\)**Step 3 (InfoNCE Loss Approximation):** Then we can safely estimate the similarity with \((Y^{},Y)\). To save more information, we use the similarity of representation embedding to approximate the similarity of the graph prediction label, where \(^{}\) represents the embedding for \(G^{}\) and \(\) represents the embedding for \(G\). We use \(\) to represent the neighbors set accordingly. Thus, we approximate Eq. (4) as:

\[*{arg\,min}_{G^{*}}I(G;G^{*})-}{ }[(^{},)}{_{ ^{}}(^{},^{ })}].\] (5)

Different between Mix-up Approach in Classification TasksThe mix-up approach in previous work  generates a mixed graph by simply mixing explanation sub-graph \(G^{*}\) with a randomly sampled label-irrelevant sub-graph \(G^{}\), which can be formally written as

Figure 3: Visualization of distribution shifting problem on four graph regression datasets. The points represent the regression value, where the blue points mean ground truth label \(Y\), red points mean prediction \(f(G)\), and the green points mean prediction \(f(G^{*})\) on the four datasets. The x-axis is the indices of the graph, sorted by the value of the label \(Y\).

\(G_{b}^{*}\)). However, it cann't tackle the continuous decision boundaries in graph regression tasks. A detailed description of the mix-up approach can be found in Appendix C.

### Implementation

InfoNCE LossAfter generating the mix-up explanation \(G^{}\), we specify the InfoNCE loss to further train the parameterized explainer with a triplet of graphs \( G,G^{+},G^{-}\). In practice, \(G^{+}\) and \(G^{-}\) are randomly sampled from the graph dataset, upon which we calculate their similarity score with the target graph \(G\). The sample with a higher score would be the positive sample and the other one would be the negative sample. Specifically, we use \((,^{})=^{}^{}\) to compute the similarity score, where \(G^{}\) can be \(G^{+}\) or \(G^{-}\). \(\) is generated by feeding \(G\) into the GNN model \(f\) and retrieving the embedding vector before the dense layers.

Formally, given a target graph \(G\), the sampled positive graph \(G^{+}\) and negative graph \(G^{-}\), we formulate the InfoNCE loss in Eq. (5) as the following:

\[_{}(G,G^{+},G^{-})=-^{})^{})}{((^{})^{}^{+})+(( {h}^{})^{}^{-})},\] (6)

where \((^{})\) is used to instantiate the function \(\), the denominator is a sum over the similarities of both positive and negative samples.

Size ConstraintsWe optimize \(I(G;G^{*})\) in Eq. (5) to constraint the size of the explanation subgraph \(G^{*}\). The upper bound of \(I(G;G^{*})\) is optimized as the estimation of the KL-divergence between the probabilistic distribution between the \(G^{*}\) and \(G\), where the KL-divergence term can be divided into two parts as the entropy loss and size loss . In practice, we follow the previous work [23; 24; 38] to implement them. Specifically,

\[_{}(G,G^{*})=_{(i,j)}(M_ {ij}^{*})-((^{*})^{}^{*}),\] (7)

where \(_{(i,j)}(M_{ij}^{*})\) means sum the weights of the existing edges in the edge weight mask \(^{*}\) for the explanation \(G^{*}\); \(^{*}\) is extracted from the embedding of the graph \(G^{*}\) before the GNN model \(f\) transforming it into prediction \(Y^{*}\), \(\) means the sigmoid function and \(\) is the weight for the size of the masked graph. In implementation, we set \(=(0.0003,0.3)\) following previous work .

Overall Objective FunctionIn practice, the denominator in Eq. (5) works as a regularization to avoid trivial solutions. Since the label \(Y\) is given and independent of the optimization process, we can also employ the MSE loss between \(Y^{*}\) and \(Y\) additionally, regarding InfoNCE loss only estimates the

Figure 4: Illustration of RegExplainer. \(G\) is the to-be-explained graph, \(G^{+}\) and \(G^{-}\) are the randomly sampled positive and negative neighbors. The explanation of the graph is produced by the explainer model. Then graph \(G^{*}\) is mixed with \((G^{+})^{}=G^{+}-(G^{+})^{*}\) and \((G^{-})^{}=G^{-}-(G^{-})^{*}\) respectively to produce \(G^{}\) and \(G^{}\). Then the graphs are fed into the trained GNN model to retrieve the embedding vectors \(^{+}\), \(^{-}\), \(^{}\) and \(^{}\), where \(^{}^{}\) due to the same label-preserving sub-graph \(G^{*}\). We use InfoNCE loss to minimize the distance between \(G^{}\) and the positive sample and maximize the distance between \(G^{}\) and the negative sample. The explainer is trained with the GIB objective and self-supervised contrastive loss.

mutual information between the embeddings. Formally, the overall loss function can be implemented as:

\[=_{}+_{}(f(G),f(G^{( )+})),_{}=_{}(G,G^{*})-_{}(G,G^{+},G^{-})\] (8)

\(G^{()+}\) means mix \(G^{*}\) with the positive sample \(G^{+}\) and \(\) and \(\) are hyper-parameters. The training algorithm and description of it are put in Appendix D.

## 5 Experiments

In this section, we conduct experiments to demonstrate the performance of our proposed method1. These experiments are mainly designed to explore the following research questions:

\(\)**RQ1:** Can RegExplainer outperforms other baselines in explaining GNNs on regression tasks?

\(\)**RQ2:** How does each part of RegExplainer and hyperparameters impact the overall performance in generating explanations?

\(\)**RQ3:** Does the distribution shifting exist in GNN explanation? Can RegExplainer alleviate it?

### Experiment Settings

We formulate Three synthetic datasets and a real-world dataset, as is shown in Table 1, in order to address the lack of graph regression datasets with ground-truth explanation. The datasets include: **BA-Motif-Volume** and **BA-Motif-Counting**, which are based on BA-shapes , **Triangles**, and **Crippen**. We compared the proposed RegExplainer against a comprehensive set of baselines in all datasets, including: **GRAD**, **ATT**, **GNNExplainer**, **PGExplainer**, and **MixupExplainer**. Detailed information about experiment setting are put in the Appendix E. We elaborate on the measurement metric of methods as follows: (1) **AUC-ROC**: We use the AUC score to evaluate the performance of our proposed methods against baseline methods regarding the ground-truth explanation, which can be treated as a binary classification task. (2) We evaluate the similarity of distribution of the graph with **Cosine Similarity** and **Euclidean Distance**.

### Quantitative Evaluation (RQ1)

In this section, we evaluate the performance of our approach with other baselines. For GRAD and GAT, we use the gradient-based and attention-based explanation, following the setting in the

previous work . We take GCN as our to-be-explained model for all post-hoc explainers. For GNNExplainer, PGExplainer, and MixupExplainer, which were previously used for the classification task, we replace the Cross-Entropy loss with the MSE loss. We run and tune all the baselines on our four datasets. We evaluate the explanation from all the methods with the AUC metric, as done in the previous work. As we can see in Table 1, we take GNNExplainer and PGExplainer as backbones and apply our framework as RegExplainer on both of them. The experiment results demonstrate the effectiveness of our methods in explaining graph regression tasks, where our method achieves the best performance compared to the baselines in all four datasets.

In Table 1, RegExplainer based on PGExplainer improves the second best baseline with \(0.175/34.3\%\) on average and up to \(0.246/48.0\%\). The comparison between RegExplainer and other baselines indicates the advantages of our proposed approach. This improvement indicates the effectiveness of our proposed method, showing that by incorporating the mix-up approach and contrastive learning, we can generate more faithful explanations in the graph regression tasks. In the following sections, we analyze the RegExplainer with PGExplainer as a backbone.

### Ablation Study and Hyper-parameter Sensitivity Study (RQ2)

We conducted an ablation study to show how our proposed components, specifically, the mix-up approach and self-supervised learning, contribute to the final performance of RegExplainer. To this end, we denote RegExplainer as RegE and design three types of variants as follows: (1) RegE\({}^{-}\): We remove the mix-up processing after generating the explanations and feed the sub-graph \(G^{*}\) into the objective function directly. (2) RegE\({}^{-}\): We remove the InfoNCE loss term but still maintain the mix-up processing and MSE loss. (3) RegE\({}^{-}\): We remove the MSE loss computation item from the objective function.

Additionally, we set all variants with the same configurations as original RegExplainer, including learning rate, training epochs, and hyper-parameters \(\), \(\), and \(\). We trained them on all four datasets and conducted the results in Figure 5. We observed that the proposed RegExplainer outperforms its variants in all datasets, which indicates that each component is necessary and the combination of them is effective.

We also investigate the hyper-parameters of our approach, which include \(\) and \(\), across all four datasets. The hyper-parameter \(\) controls the weight of the InfoNCE loss in the GIB objective while the \(\) controls the weight of the MSE loss. We determined the optimal values of \(\) and \(\) with grid search. The experimental results can be found in Figure 6. We fixed \(\) and \(\) at 1 and changed another parameter to visualize the change in model performance. Figure 6 illustrates that the model's performance is robust to changes in hyper parameters within the scope \([0.001,1000]\). Our findings indicate that our approach, RegExplainer, is stable and robust when using different hyper-parameter settings, as evidenced by consistent performance across a range.

Figure 5: Ablation study of RegExplainer. We evaluated the AUC performance of the original RegExplainer and its variants that exclude the mix-up approach, InfoNCE loss, or MSE loss, respectively. The black solid line shows the standard deviation.

Figure 6: Hyper-parameters study of \(\) and \(\) on four datasets with RegExplainer. In both figures, the x-axis is the value of different hyper-parameter settings, and the y-axis is the value of the average AUC score over ten runs with different random seeds.

### Alleviating Distribution Shifts (RQ3)

In this section, we visualize the regression values of the graphs and calculate the prediction shifting distance for each dataset and analyze their correlations to the distance of the decision boundaries. We put our results into Figure 7 and Table 2.

We observed that in Figure 3, red points surround the blue points but green points are shifted away, which indicates that the explanation sub-graph cann't help GNNs make correct predictions. As shown in Table 2, we calculate the RMSE score between the \(f(G)\) and \(Y\), \(f(G^{*})\) and \(Y\), \(f(G)\) and \(f(G^{*})\) respectively, where \(f(G)\) is the prediction the original graph, \(f(G^{*}\) is the prediction of the explanation sub-graph, and \(Y\) is the regression label. We can observe that \(f(G^{*})\) shows a significant prediction shifting from \(f(G)\) and \(Y\), indicating that the mutual information calculated with \((f(G^{*}),Y)\) would be biased.

We further explore the relationship of the prediction shifting against the label value with dataset BA-Motif-Volume, which represents the semantic decision boundary. This additional experiment with Figure 7 can be found in Appendix F.1.

We also design experiments to illustrate how RegExplainer corrects the deviations: we calculate the graph embeddings \(v\) and predictions \(p\) of the explanation sub-graphs and the mix-up graph. Then we compare them to the ground truth and calculate the Euclidean or Cosine distance between the vectors and RMSE between prediction labels. From the results in Table 3, we can observe that all the performances of COS(\(v_{g}\), \(v_{m}\)), EUC(\(v_{g}\), \(v_{m}\)) and prediction errors are better than those of (\(v_{g}\), \(v_{e}\)), which indicates RegExplainer can effectively fix the distribution of sub-graph explanation \(G^{*}\) and reduce the embedding distance and prediction error.

## 6 Conclusion

We addressed the challenges in the explainability of graph regression tasks and proposed the RegExplainer, a novel method for explaining the predictions of GNNs with the post-hoc explanation sub-graph on graph regression task without requiring modification of the underlying GNN architecture or re-training. We showed how RegExplainer can leverage the mix-up approach to solve the distribution shifting problem and adopt the GIB objective with the InfoNCE loss to migrate it from graph classification tasks to graph regression tasks, while these existing challenges seriously affect the performances of other explainers. We formulated four new datasets: BA-Motif-Volume, BA-Motif-Counting, Triangles, and Crippen for evaluating the explainers on the graph regression

  Dataset & \((f(G),Y)\) & \((f(G^{*}),Y)\) & \((f(G),f(G^{*}))\) \\  BA-Motif-Volume & \(131.42\) & \(1432.07\) & \(1427.07\) \\ BA-Motif-Counting & \(2.06\) & \(7.43\) & \(7.22\) \\ Triangles & \(5.28\) & \(12.38\) & \(12.40\) \\ Crippen & \(1.13\) & \(1.54\) & \(1.17\) \\  

Table 2: Prediction shifting study on the RMSE of \((f(G),Y)\), \((f(G^{*}),Y)\), \((f(G),f(G^{*}))\) respectively.

   & BA-Motif-Volume & BA-Motif-Counting & Triangles & Crippen \\  COS(\(v_{g}\), \(v_{e}\)) & 0.95 & 0.80 & 0.97 & 0.89 \\ COS(\(v_{g}\), \(v_{m}\)) & 0.98 & 0.89 & 0.99 & 0.92 \\  EUC(\(v_{g}\), \(v_{e}\)) & 0.46 & 0.68 & 0.19 & 0.67 \\ EUC(\(v_{g}\), \(v_{m}\)) & 0.37 & 0.52 & 0.08 & 0.63 \\  RMSE(\(p_{g}\), \(p_{e}\)) & 1427.07 & 7.22 & 12.40 & 1.17 \\ RMSE(\(p_{g}\), \(p_{m}\)) & 393.26 & 2.73 & 8.22 & 0.68 \\  

Table 3: Table for measuring distribution repairing. \(v_{g}\), \(v_{e}\) and \(v_{m}\) are the embeddings from \(f\) of original graph \(G\), explanation subgraph \(G^{*}\) and the mix-up explanation \(G^{()+}\). \(p_{g}\), \(p_{e}\) and \(p_{m}\) are the predicted labels for the original graph, explanation subgraph and the mix-up explanation. EUC means Euclidean distance (\(\), the smaller the better) and COS means cosine distance (\(\), the larger the better). RMSE means Root Mean Square Error (\(\), the smaller the better).

task, which are aligned with the design of datasets in previous work. They can also benefit future studies on the XAIG-R. While we acknowledge the effectiveness of our method, we also recognize its limitations. Specifically, although our approach can be applied to explainers for graph regression tasks in an explainer-agnostic manner, it cannot be easily applied to explainers built for explaining the spatio-temporal graph due to the dynamic topology and node features of the STG. To overcome this challenge, a potential solution is to incorporate cached dynamic embedding memories into the framework.

## 7 Ethics Statement

This work is primarily foundational in GNN explainability, focusing on expanding the GIB objective function of the explainer framework from graph classification tasks to graph regression tasks. Its primary aim is to contribute to the academic community by enhancing the explanation in graph regression. We do not foresee any direct, immediate, or negative societal impacts stemming from the outcomes of our research.

## 8 Acknowledgments

The work was partially supported by NSF awards #2421839 and #2331908. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.