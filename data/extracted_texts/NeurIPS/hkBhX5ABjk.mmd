# Multi-Agent Domain Calibration with a Handful of Offline Data

Tao Jiang\({}^{1,2,3}\), Lei Yuan\({}^{1,2,3}\), Lihe Li\({}^{1,2}\), Cong Guan\({}^{1,2}\),

Zongzhang Zhang\({}^{1,2}\)  Yang Yu\({}^{1,2,3}\)

\({}^{1}\)National Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, Nanjing, China

\({}^{3}\)Polixir Technologies, Nanjing, China

{jiangt,yuanl,lih,guanc}@lamda.nju.edu.cn, {zzzhang, yuy}@nju.edu.cn

Equal ContributionCorresponding Author

###### Abstract

The shift in dynamics results in significant performance degradation of policies trained in the source domain when deployed in a different target domain, posing a challenge for the practical application of reinforcement learning (RL) in real-world scenarios. Domain transfer methods aim to bridge this dynamics gap through techniques such as domain adaptation or domain calibration. While domain adaptation involves refining the policy through extensive interactions in the target domain, it may not be feasible for sensitive fields like healthcare and autonomous driving. On the other hand, offline domain calibration utilizes only static data from the target domain to adjust the physics parameters of the source domain (e.g., a simulator) to align with the target dynamics, enabling the direct deployment of the trained policy without sacrificing performance, which emerges as the most promising for policy deployment. However, existing techniques primarily rely on evolution algorithms for calibration, resulting in low sample efficiency. To tackle this issue, we propose a novel framework Madoc (**M**ulti-**a**gent **d**omain calibration). Firstly, we formulate a bandit RL objective to match the target trajectory distribution by learning a couple of classifiers. We then address the challenge of a large domain parameter space by modeling domain calibration as a cooperative multi-agent reinforcement learning (MARL) problem. Specifically, we utilize a Variational Autoencoder (VAE) to automatically cluster physics parameters with similar effects on the dynamics, grouping them into distinct agents. These grouped agents train calibration policies coordinately to adjust multiple parameters using MARL. Our empirical evaluation on 21 offline locomotion tasks in D4RL and NeoRL benchmarks showcases the superior performance of our method compared to strong existing offline model-based RL, offline domain calibration, and hybrid offline-and-online RL baselines.

## 1 Introduction

Reinforcement learning (RL) has gained significant traction in various fields , such as sequential recommendation systems  and robotic control , demonstrating tremendous potential in real-world applications. However, the inherent trial-and-error nature of RL limits its application, especially in safety-critical areas such as healthcare  and autonomous driving , as extensive interactions with the target environment can entail prohibitive costs and pose substantial safety risks. To address this problem, a range of studies have proposed collecting training samples from a surrogate source domain (e.g., simulation environment) to learn policies, which are then deployed to the downstreamtarget domain [6; 7]. Nonetheless, due to complex system dynamics and the characteristics of open environments, a high-fidelity simulator may not always be available , leading to severe dynamics shifts between the source and target domains . Consequently, policies trained optimally in the source domain may fail catastrophically in the target domain. To bridge the dynamics gap, various kinds of solutions have been developed recently. Domain randomization  methods, for instance, randomly sample the physics parameters of the source domain and train policies across multiple simulated environments to approximate the target domain. However, as the target domain is often unknown and set in an open environment , these methods can also suffer from unpredictable policy degradation, which hinders further development.

Integrating the source domain with some data from the target domain offers a promising solution to the mentioned problem . A class of methods, known as domain calibration, attempts to use data from the target domain as feedback to calibrate the easily obtained source domain and then transfer the policy directly to the target domain. This approach shows enormous potential when the parameters are adjusted accurately enough. Some typical methods automatically tune the physics parameters by minimizing the transition discrepancy between the source and target domains [12; 13] or by maximizing the expected return in the target domain . While these methods can successfully transfer learned policies in robotics , they still require interaction feedback from the target domain during training. Instead, DROID  and DROPO  introduce an offline setting for domain calibration, where the physics parameters are adjusted using offline demonstrations pre-collected in the target domain, showing potential for real-world applications.

Nevertheless, in complex real-world scenarios, numerous physics parameters may require calibration. The above-mentioned methods primarily employ evolutionary algorithms [16; 18] or sampling-based methods [19; 20] for black-box optimization, often results in low sample efficiency [21; 22]. Recently, some algorithms have attempted to mitigate this issue by learning sampling strategies  or leveraging causal discovery  to eliminate parameters that have little impact on the environment. Despite the effectiveness of these methods, a significant challenge remains in handling complex scenarios where all physics parameters critically influence the dynamics, and different parameters may have varying, or even opposite, impacts on these dynamics . A method for efficiently addressing the interrelations among different parameters is urgently needed.

From the perspective of whole-domain calibration, each physics parameter contributes to different aspects of the calibration process. This can be modeled as a typical multi-agent system (MAS) problem , where each agent adjusts a group of domain parameters, and all agents cooperate to reduce the domain gap. This problem can be addressed using cooperative multi-agent reinforcement learning (MARL) , leading to the development of the Madoc (**M**ulti-**a**gent **d**omain **c**alibration) framework. Specifically, we first formulate domain calibration as a target trajectory distribution matching problem and derive a bandit RL optimization objective by introducing a couple of classifiers to act as the reward model. We then formulate the problem into the MAS where multiple agents calibrate different parameters to reduce the dynamics gap between the source and target domains. Concretely, we propose an automatic grouping technique to cluster physics parameters based on their impacts on the dynamics. We then employ popular value decomposition methods in MARL to train cooperative calibration policies to adjust domain parameters. We conduct experiments on popular locomotion tasks to showcase Madoc's superior performance against baselines and highlight the contributions of its core design components. The source code is available at https://github.com/LAMDA-RL/Madoc.

## 2 Related Work

**Domain transfer in RL.** Transferring RL policies learned from imperfect source domains to the target domain is a crucial step in the practical use of RL algorithms . However, the trained policy often suffers from severe performance deterioration when directly deployed into the target domain due to the distribution shift between different domains with varying transition dynamics . Previous works have addressed this problem with three common strategies: domain randomization (DR), domain adaptation (DA), and system identification (SI). DR attempts to train a generalizable policy that works well across a variety of randomized simulated dynamics [28; 9]. While the motivation is simple and often effective, these methods require manually determining which parameters to randomize and may result in underfitting or failing policies due to hand-tuning parameter ranges. DA involves using a huge amount of data from source domains to improve policy performance on a different target domain [29; 30; 31]. However, these efforts are constrained by the quality and quantity of target domain data and often still require interaction with the target domain. Another line of work, SI, uses measured data to build mathematical models of dynamical systems . These methods rely on numerous interactions with the target domain to study how to learn a model of the system dynamics [33; 34], which results in learning a biased policy with fewer interactions. Most recent works calibrate the parameters of the biased source domain to bridge the domain gap [19; 13; 15], also known as domain calibration, and try to improve efficiency by learning a parameter sampling strategy  or leveraging causal discovery . However, these methods still require interacting with the target domain, posing potential safety hazards during the training process. To mitigate this problem, DROID  and DROPO  use offline datasets to adjust the source domain parameters via evolutionary algorithms  with different optimization objectives, perform poorly when the number of domain parameters is large . Unlike the above methods, we propose to adjust the source domain with a handful of offline data, enabling the domain parameters to match the target trajectory distribution with high sample efficiency.

**Cooperative multi-agent RL.** Many real-world problems are inherently large-scale and complex, making it inefficient and impractical to model them as single-agent systems. Instead, they are more suitably addressed as multi-agent systems (MASs) . Multi-agent reinforcement learning (MARL) provides frameworks for modeling and solving such challenges . In scenarios where agents within MAS share common objectives, these problems are categorized under cooperative MARL, which has demonstrated significant advancements in domains like power management , path planning , and dynamic algorithm configuration . One of the primary challenges in cooperative MARL is the scalability issue [39; 40; 41], exacerbated by the exponential growth of the search space with the number of agents, complicating policy exploration and learning. Various approaches have been proposed to enhance agent coordination recently. These include policy-based methods such as MADDPG  and MAPPO , value-based techniques like VDN  and QMIX , and innovations like the transformer architecture . Among these methods, value-based approaches have demonstrated promising results in diverse and complex settings [47; 48]. VDN leverages additivity to factorize global value functions, QMIX further enforces monotonicity in global value functions, and DOP  introduces value function decomposition within multi-agent actor-critic frameworks. These methods exhibit remarkable coordination capabilities across various tasks such as SMAC, Hanabi, and GRF . In this paper, our method formulates domain calibration as a cooperative MARL problem, improving efficiency and fidelity.

## 3 Background

**Reinforcement Learning** can be generally modeled as a Markov decision process (MDP) , formulated as a tuple \(:=(,,T,r,,_{0})\), where \(\) and \(\) denote the state and action spaces, \(T(s^{}|s,a)\) and \(r(s,a)\) represent the transition and reward functions, \([0,1)\) implies the discount factor, and \(_{0}(s)\) is the initial state distribution. The agent running in the environment perceives the state \(s_{t}\) at time step \(t\), performs an action \(a_{t}\) based on a learnable policy \((a|s)\), then the environment receives the action, transits to a new state \(s_{t+1}\), and rewards the agent according to the transition function \(T(s_{t+1}|s_{t},a_{t})\) and reward function \(r(s_{t},a_{t})\) at next time step. The above process is continuously iterated until termination, we can record the whole trajectory of length \(H+1\) as \(=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},,s_{H},a_{H},r_{H})\) and the trajectory distribution over agent's policy and the environment can be defined as \(d_{,}()=_{0}(s_{0})_{i=0}^{H}T(s_{t+1}|s_{t},a_{t}) (a_{t}|s_{t})\). The objective of RL algorithms is to learn a policy \((a|s)\) which maximizes the expected discounted return across the distribution of trajectories, i.e., \((,)=_{ d_{,}()}R ()\) with \(R()=_{t=0}^{H}^{t}r(s_{t},a_{t})\).

**Domain Calibration** aims to adjust a manipulable source domain to close the domain gap between it and the target domain. Both the target and source domains can be modeled as MDPs, and the only difference between them is the transition functions which are determined by the physics dynamics parameter vector \(^{N}\) (e.g., friction, mass, damping). Here, \(\) denotes the physics parameter space, and \(N\) represents the dimension of the physics parameters. Each parameter \(^{i}\) is bounded on a closed interval, which can only be inferred roughly based on experience and expert knowledge, i.e., \(^{i}[^{i}_{ low},^{i}_{ high}]\). We assume the unknown physics parameters of the target domain \(^{*}\) are included in the parameter space \(\) if the physical modeling is reasonable, i.e., \(^{*}\), as we can set sufficient wide parameter ranges. We now denote the transition function conditioned on domain parameters as \(T_{}=T(s^{}|s,a,)\) and the corresponding MDP as \(M_{}:=(,,T_{},r,,_{0})\). However, the manipulable source domain is typically non-differentiable, we can only calibrate the distribution of the source domain parameters \( q_{}()\), and the optimal policy learned under this distribution is marked as \(^{*}(q_{})=*{arg\,max}_{}_{ q_{}( )}(M_{},)\). The objective of domain calibration is to learn the source domain parameters that maximize the expected discounted return under the target domain: \(_{q_{}}(M_{^{*}},^{*}(q_{}))\).

## 4 Method

In this section, we propose the Madoc (**M**ulti-**a**gent **d**omain **c**alibration) framework for leveraging a modest amount of offline data from the target domain to calibrate the biased source domain, thus facilitating optimal policy transfer. The overall workflow of the Madoc framework is shown in Fig. 1. We first deduce a bandit RL objective to adjust the domain parameters in Sec. 4.1, improving the synthetic data sampled from the source domain to align with the target trajectory distribution. We further model it as a cooperative multi-agent reinforcement learning problem in Sec. 4.2 and use an automatic grouping technique to improve the efficiency and fidelity of domain calibration. Finally, a practical algorithm under the Madoc framework is presented in Sec. 4.3.

### Domain Calibration via Reinforcement Learning

Domain calibration is the process of tuning the parameter distribution of a mismatched source domain to better align with the target domain, which can be realized by comparing the divergence between target domain interactions and simulated synthetic rollouts based on the same policy [12; 13]. However, since interacting with the target domain may not be feasible for sensitive fields, we propose an alternative approach to minimize the trajectory discrepancy between the two domains by employing a handful of offline target domain data.

Formally speaking, the static offline dataset \(=\{_{1},_{2},,_{k}\}\) contains \(k\) trajectories where \(_{i}=(s^{i}_{0},a^{i}_{0},r^{i}_{0},s^{i}_{1},a^{i}_{1},r^{i}_{1},,s^ {i}_{H},a^{i}_{H},r^{i}_{H})\), which are collected previously by an unknown behavior policy \(\) from the target domain, i.e., \(_{i} d_{,_{^{*}}}()\). By introducing a prior normal parameter distribution \(p()\) to foster better generalization , we intend to learn a sample policy \(\) and calibrate the domain parameters to match the target trajectory distribution:

\[_{,q_{}}D_{}(q_{}()d_{,_{} }()||p()d_{,_{^{*}}}()),\] (1)

Figure 1: The conceptual workflow of the multi-agent domain calibration framework. The orange arrow represents the simulated data flow in the source domain, with the transition function \(T(s^{}|s,a,)\), while the blue represents the offline data in the target domain, with the transition function \(T(s^{}|s,a,^{*})\). The subscripts “src” and “tar” are used to distinguish between the source and target domains, respectively. After learning the grouping scheme, we use the red arrow to represent the process of domain calibration by MARL value decomposition methods.

where the Kullback-Leibler (KL) divergence can be further derived as:

\[_{ q_{}()\\  d_{,_{}}()}[^{H}(a_{t}|s_{t})T(s_{t+1}|s_{t},a_{t},)}{_{t=0}^{H}(a_{t}|s_{ t})T(s_{t+1}|s_{t},a_{t},^{*})}+()}{p()}],\] (2) \[= _{ q_{}()\\  d_{,_{}}()}[_{t=0}^{H} (|s_{t})}{(a_{t}|s_{t})}+|s_{t},a _{t},)}{T(s_{t+1}|s_{t},a_{t},^{*})})]+_{ q _{}()}[()}{p()}],\] (3) \[ _{(s,a)}[]-_{(s,a,s^{},)}[|s,a,^{*})}{T(s^{}|s,a,)}]+D_{}(q_{ }()||p()),\] (4)

where Eq. 4 is an approximation of Eq. 3 as the parameter distribution and trajectory distribution used to calculate the expectation are difficult to compute. Consequently, we use Monte Carlo sampling on the source domain to approximate the expected results. To enhance sampling efficiency, we sample a domain parameter \( q_{}()\), generate the trajectories \( d_{,_{}}()\), and store the rollouts \((s,a,s^{},)\) in the replay buffer \(\). By doing so, we are able to convert the trajectory-based objective into a transition-based one, following the classic off-policy RL paradigm.

It is delighted to discover that the objective in Eq. 4 can be clearly divided into three terms: the first term, i.e., \(_{}_{(s,a)}[]_{}D_{}((a|s)||(a|s))\), attempts to minimize the KL divergence between \((a|s)\) and \((a|s)\), we can consider it as a variant of behavior cloning; the second term is formulated as \(_{q_{}}_{(s,a,s^{},)}[ |s,a,^{*})}{T(s^{}|s,a,)}]\), which can be seen as a bandit RL objective for policy \(q_{}()\) to maximize reward \(_{q}()=|s,a,^{*})}{T(s^{}|s,a,)}\); and the last term is regarded as a policy regularizer added on \(q_{}()\) to prevent it from collapsing. It is worth noting that the policy \(q_{}()\) here is not the one running (sampling) on the source domain, but the one outputting physics parameter vector \(\) as an action to adjust the source domain. To prevent confusion, in the following paper, the policy running on the source domain, i.e., \((a|s)\), is referred to as the "running policy", while the one adjusting the domain parameters, i.e., \(q_{}()\), is referred to as the "calibration policy". Additionally, we similarly define the "calibration critic", which is responsible for evaluating the accuracy of the parameters output by the calibration actor. The calibration critic and the calibration policy (actor) together constitute a calibration agent.

The key challenge lies in how to estimate the stochastic reward \(|s,a,^{*})}{T(s^{}|s,a,)}\) given the offline data and simulated rollouts. According to Bayes' rule, we can transform the transition probability  as:

\[T(s^{}|s,a,)=)P(s,a,s^{})}{P()P(s,a |)}=)P(s,a,s^{})}{P(|s,a)P(s,a)},\] (5)

and the reward can be derived as:

\[_{q}()= P(^{*}|s,a,s^{})- P(^{*}|s,a)- P (|s,a,s^{})+ P(|s,a),\]

where \(^{*}\) and \(\) stand for the target and source domains respectively. Hence, we can train a couple of binary classifiers \(D_{_{}}(|s,a,s^{})\) and \(D_{_{}}(|s,a)\) to discriminate whether state-action-state and state-action pairs come from the offline dataset (referred to as the binary variable target) or synthetic samples (referred to as the binary variable source). These two discriminators form a reward model with certain generalization ability , and the corresponding cross-entropy losses are written as:

\[_{_{}} =-_{(s,a,s^{})}[ D_{_{ { max}}}(|s,a,s^{})]-_{(s,a,s^{}) }[ D_{_{}}(|s,a,s^{})],\] \[_{_{}} =-_{(s,a)}[ D_{_{}}(|s,a)]-_{(s,a)}[ D_{_{}}(|s,a)].\] (6)

### Multi-Agent Domain Calibration

As the complexity of the source domain grows, characterized by an expanding number of physics parameters, the calibration policy learned by single-agent reinforcement learning often struggles to consistently reduce the domain gap. To address this challenge, we employ multi-agent reinforcement learning (MARL) algorithms to effectively reduce the search space for a single calibration agent, thereby enhancing the efficiency and fidelity of domain calibration.

We conduct experiments on the _HalfCheetah_ environment and train to calibrate the _gravity_ coefficient along with other physics parameters. The preliminary results are shown in Fig. 2, we capture snapshots for both approaches at the same training step to investigate Pearson correlation  between the critic value of the _gravity_ coefficient and the absolute calibration error. Here, the absolute calibration error represents the absolute difference between the parameters output by the calibration actor and the target parameters. The single-agent method, represented by the blue dots, utilizes just a shared calibration critic for parameter adjusting, facing challenges in assessing a specific action within a huge action space. In contrast, the multi-agent method, depicted by the red dots, employs value decomposition algorithms  to narrow the action space of each individual calibration policy. This decomposition leads to more accurate evaluations of domain parameters, thereby reducing calibration errors and improving policy transfer.

To utilize the cooperative MARL for domain calibration, we now formally define \(N\) agents (since the source domain has a total of \(N\) physics parameters) to perform domain calibration coordinately where each of them \(q_{}^{i}(^{i})\) attempts to adjust the single parameter \(^{i}\), therefore the joint calibration policy can be decomposed as \(q_{}()=_{i=1}^{N}q_{}^{i}(^{i})\). Utilizing any off-the-shelf value decomposition algorithm \(\), we employ the single global rewards \([_{q}^{1}(^{i}),,_{q}^{N}(^{N})]=(_{q}())\) to guide individual calibration policy updates:

\[_{q_{}^{i}}_{^{i} q_{}^{i}()}_{ q}^{i}(^{i})-D_{}(q_{}^{i}(^{i})||p^{i}(^{i}) ).\] (7)

Nonetheless, when the number of the domain parameters \(N\) is large, employing \(N\) agents for domain calibration leads to low exploration and optimization efficiency . To mitigate this problem, clustering physics parameters with similar effects on the transition dynamics, e.g., the mass of a symmetrical robot's left and right feet, into one calibration agent is an advisable choice. Hence we introduce an automatic grouping technique by adopting a Variational Autoencoder (VAE) . By adjusting a specific parameter \(^{i}\) while keeping all other parameters \(^{-i}\) fixed , we can assume the identity \(i\) of each parameter (i.e., the one-hot encoding) to be representative of the transition \(=(s,a,s^{},^{i})\), the Evidence Lower BOund (ELBO) of the transition is then derived as:

\[ P()_{z f_{}(z|i)}[ f_{}(|z)]-D_{}(f_{}(z|i)||p(z)),\] (8)

where \(f_{},f_{}\) stand for the encoder and decoder, \(z\) is the latent variable, and \(p(z)\) represents the corresponding prior distribution. We can deduce the reconstruction term as:

\[ f_{}(|z) =[f_{}(s^{}|s,a,^{i},z)f_{ }(s,a,^{i}|z)],\] \[= f_{}(s^{}|s,a,^{i},z)+c,\] (9)

where \(c\) is a constant as \(s\), \(a\), and \(^{i}\) do not depend on the latent variable \(z\). We parameterize the encoder \(f_{}\) and the decoder \(f_{}\) with \(\), making them \(f_{_{a}}\) and \(f_{_{d}}\), and optimize the VAE model with the following loss:

\[_{}=-_{(s,a,s^{},^{i},^{-i}),z f_{_{a}}(|i)}[ f_{_{d}}(s^{}|s,a,^{i},z) ]+D_{}(f_{_{a}}(z|i)||p(z)).\] (10)

Before domain calibration, we first minimize the VAE loss (Eq. 10). Then we apply the k-means clustering method  to the means generated by the encoder \(f_{_{c}}(z|i)\) for all \(i N\), in order to group the domain parameters . The resulting \(n\) (\(1 n N\)) grouping scheme is recorded as \([^{g1},,^{gn}]\), each group forms one agent equipped with a calibration actor \(q_{}^{gi}(^{gi})\) and a calibration critic \(v_{}^{gi}(^{gi})\), following the multi-agent actor-critic framework.

Figure 2: The Pearson correlation between the critic value of the _gravity_ coefficient and the absolute calibration error. Each dot represents a sampled action, which is then fed into the corresponding critic to compute the critic value. When the parameters output by the calibration actor are closer to the target parameters (indicating a smaller absolute calibration error), the evaluation value output by a “good” critic should be higher.

### Practical Algorithm

Based on the above analysis, we now present a practical algorithm under the Madoc framework, the pseudo-code is shown in App. A. We apply DOP , a popular multi-agent policy gradient algorithm as the value decomposition method. Concretely, there are \(n\) agents for calibrating the source domain after automatic grouping, we factor the joint critic as a weighted summation of individual critics:

\[V_{}^{ tot}=_{i=1}^{n}k_{i}v_{}^{gi}(^{gi})+b,\] (11)

where \(k_{i} 0\) and \(b\) are denoted as learnable weights and biases. The individual critics are learned by back-propagating gradients from global Temporal Difference updates:

\[_{}=_{(s,a,s^{},)}[ {1}{2}(V_{}^{ tot}-_{q}())^{2}].\] (12)

Given individual critics, we use SAC  to update the stochastic actors in an off-policy manner:

\[_{}=_{^{gi} q_{}^{gi}()}[  q_{}^{gi}(^{gi})-v_{}^{gi}(^{gi})+ D_{ KL}(q _{}^{gi}(^{gi})||p^{gi}(^{gi}))],\] (13)

where \(\) and \(\) control the relative importance of the entropy and regularization term respectively. Besides, the prior domain parameter distribution \(p^{gi}(^{gi})\) is set to be an exponentially moving average of the current calibration policy \(q_{}^{gi}(^{gi})\), which has been shown to stabilize training like target network. Finally, we parameterize the policy running in the source domain \(\) with \(\) and enable it to clone the behavior policy on offline data during domain calibration. Once domain calibration is complete, we train the policy \(_{}\) from scratch on the source domain using SAC, and directly deploy it to the target domain.

## 5 Experiments

In this section, we present the empirical evaluations of our proposed Madoc framework. We first describe the experiment environments and related baselines in Sec. 5.1, and then conduct a series of experiments to answer the following questions: (1) How is the comprehensive performance of Madoc against multiple baselines (Sec. 5.2)? (2) How do core components of Madoc contribute to the overall performance (Sec. 5.3)? (3) How is the generalization capability of Madoc across datasets of varying sizes and source domains with different initial ranges (Sec. 5.4)?

### Experiment Setup

In our experiments, we evaluate Madoc on classic continuous control tasks from the MuJoCo  engine and choose two offline benchmarks to serve as offline datasets collected in the target domain. On the popular D4RL benchmark , we choose four locomotion tasks (_HalfCheetah_, _Hopper_, _Walker2d_, _Ant_), each with three types of datasets (_medium_, _medium-replay_, _medium-expert_), to evaluate different algorithms' performance when faced with datasets of varying quality. Considering more challenging scenarios, three environments (_HalfCheetah_, _Hopper_, _Walker2d_) along with three levels of datasets (_low_, _medium_, _high_) from NeoRL benchmark  are also selected. The main difference between the two benchmarks lies in that the static datasets in the NeoRL benchmark occupy more narrow distributions. During the training process, we are given imperfect source domains, and only aware of the initial range of specific physics parameters (_gravity_, _body_mass_, _dof_damping_). Each environment has different parameter dimensions, initial ranges, and ground truth values, refer to App. D for detailed information.

Madoc utilizes static offline datasets to calibrate biased source domains, and we choose several baseline algorithms with identical or similar settings for comparison. DROPO  and DROID  use fixed offline datasets from the target domain to optimize the distribution bounds with different objectives via evolution algorithms like CMA-ES . OTED  models the parameter optimization process as a bandit RL problem, which is similar to our method, but the objective is different and cannot cope with large parameter space. H2O  and DR+BC are two hybrid offline-and-online algorithms where the former penalizes the Q-function learning on simulated state-action pairs with large dynamics gaps, and the latter directly combines uniform domain randomization with behavior

[MISSING_PAGE_FAIL:8]

calibration algorithms, we provide a more detailed discussion on the stability of the experimental results in App. E.3.

### Effectiveness of Different Components

To investigate the impact of the design components of Madoc, we first design experiments on the _Hopper_ environment to visualize the automatic grouping technique, as shown in Fig. 3(a). The _Hopper_ is a one-legged robot simulation with four distinct body sections: the torso, thigh, leg, and foot. These components are connected by three joints, which serve as the articulation points between each pair of bodies. We need to calibrate the _gravity_ coefficient, the _mass_ of each body, and the _damping_ coefficients at each joint. After projecting the embedding space onto a three-dimensional space, we can discover that the robot's physics parameters are divided in an orderly manner from top to bottom, while the _gravity_ coefficient forms a separate group on its own. The result aligns perfectly with our expectations, as parameters that have similar impacts on dynamics can be adjusted using one calibration policy. Additionally, in Fig. 3(b), we conduct ablation studies on the _Ant_ environment of the D4RL benchmark, to verify the effectiveness of modeling domain calibration as an MARL problem. When the dimension of the domain parameters is \(1\) or \(2\), there is no significant performance gap between Madoc and Madoc-S; however, as the source domain becomes more complex, the performance of both declines, but the drop is faster when only one agent is employed for parameter calibration. This experimental result favorably supports that modeling as MARL can enhance the efficiency of domain calibration in large parameter spaces.

### Generalization across Various Conditions

Madoc leverages static offline datasets to adjust the domain parameters, driving our curiosity toward its generalization capacity under varying dataset sizes and initial parameter ranges of the source domain. Here we choose all the above-mentioned tasks on the NeoRL benchmark and calculate the averaged normalized returns for comparison. As illustrated in Fig. 4(a), the algorithms access datasets of different magnitudes, \(5 10^{4}\) (small), \(2 10^{5}\) (medium), and \(1 10^{6}\) (large), to reflect a spectrum of data availability. The results reveal that the effectiveness of both the hybrid offline-and-online H2O algorithm and the purely offline MOREC algorithm rises with the expansion of the dataset size. This discovery suggests that their dependency on the size of offline data for improved performance. On the contrary, our method maintains stable and excellent performance, unfazed by the dataset size. Besides, for source domains with various initial parameter ranges, categorized as easy, medium, and hard (see App. D for more details), Madoc exhibits remarkable performances across all levels, particularly excelling in "hard" cases with the largest parameter search space, as shown in Fig. 4(b).

Figure 3: (a) The visualization results of the automatic grouping technique. The left part is a schematic of the _Hopper_ robot, each point on it representing a physics parameter to be calibrated, and different colors indicate the final grouping results. For example, the four parameters encircled by the yellow rectangle are clustered into one group in the embedding space on the right part. (b) The normalized average return of Madoc and Madoc-S on the _Ant_ environment. We can observe that as the parameter dimension of the source domain increases, the performance gap between them (indicated by the black shadow) becomes more pronounced.

This underscores our algorithm's effectiveness in coping with challenging scenarios, affirming its robustness and adaptability in diverse conditions.

## 6 Conclusion and Discussion

In this paper, we introduce Madoc, a framework for closing the dynamics gap by calibrating the source domain with a handful of offline data via multi-agent reinforcement learning. Concretely, the target domain data serve as a guide for target transition dynamics, which is leveraged to train classifiers generating rewards and derive a bandit RL objective for domain calibration, To improve calibration efficiency with a large number of parameters, we further model it as a cooperative MARL problem and propose to group parameters with similar effects on dynamics. Experiments on popular control tasks demonstrate that our method can calibrate the source domain with sufficient accuracy, allowing the optimal trained policy to be transferred to the target domain without severe performance deterioration. One possible constraint of our method is that, when dealing with high-dimensional vision tasks , using a handful of offline data may not guarantee the accuracy and generalizability of the reward model. This challenge could be mitigated by deploying more expressively powerful tools like diffusion models , which is left for future work.

AcknowledgementsThis work is supported by the National Science Foundation of China (62276126, 62250069), the Natural Science Foundation of Jiangsu (BK20221442, BK20243039, and BK2024119), and the Fundamental Research Funds for the Central Universities (0221/14380022).