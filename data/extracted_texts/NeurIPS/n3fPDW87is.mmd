# Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity

Youssef Allouah  Raichid Guerraoui  Nirupam Gupta  Rafael Pinot  Geovani Rizk

Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland

Correspondence to: Youssef Allouah -cyoussef.allouah@epfl.ch>.

###### Abstract

The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is _homogeneous_. Under _data heterogeneity_ however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely \((G,B)\)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction \(}{{2}}\). We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap between theory and practice.

## 1 Introduction

Distributed machine learning algorithms involve multiple machines (or _workers_) collaborating with the help of a server to learn a common model over their collective datasets. These algorithms enable training large and complex machine learning models, by distributing the computational burden among several workers. They are also appealing as they allow workers to retain control over their local training data. Conventional distributed machine learning algorithms are known to be vulnerable to adversarial workers, which may behave unpredictably. Such behavior may result from software and hardware bugs, data poisoning, or malicious players controlling part of the network. In the parlance of distributed computing, such adversarial workers are referred to as _Byzantine_. Due to the growing influence of distributed machine learning in public applications, a significant amount of work has been devoted to addressing the problem of robustness to Byzantine workers, e.g., see .

A vast majority of prior work on robustness however assumes _data homogeneity_, i.e., local datasets are generated from the same distribution. This questions their applicability in realistic distributed learning scenarios with _heterogeneous data_, where local datasets are generated from different distributions. Under data homogeneity, Byzantine workers can only harm the system when the other workers compute stochastic gradient estimates, by exploiting the noise in gradient computations. This vulnerability can be circumvented using variance-reduction schemes . In contrast, under data heterogeneity, variance-reduction schemes are not very helpful, as suggested by preliminary work . In short, data heterogeneity is still poorly understood in robust distributed learning. In particular, existing robustness guarantees are extremely conservative, and often refuted by empirical observations. Indeed, the heterogeneity model generally assumed is typically violated in practice and does not even cover basic machine learning tasks such as least-squares regression.

Our work addresses the aforementioned shortcomings of existing theory, by considering a more realistic heterogeneity model, called \((G,B)\)_-gradient dissimilarity_. This criterion characterizes data heterogeneity for a larger class of machine learning problems compared to prior works [12; 20; 3], and enables us to reduce the gap between theoretical guarantees and empirical observations. Before summarizing our contributions in Section 1.2, we briefly recall below the essentials of robust distributed learning and highlight the challenges of data heterogeneity.

### Robust distributed learning under heterogeneity

Consider a system comprising \(n\) workers \(w_{1},,w_{n}\) and a central server, where \(f\) workers of a priori unknown identity may be Byzantine. Each worker \(w_{i}\) holds a dataset \(_{i}\) composed of \(m\) data points from an input space \(\), i.e., \(_{i}\{x_{1}^{(i)},,x_{m}^{(i)}\}^{m}\). Given a model parameterized by \(^{d}\), each data point \(x\) incurs a loss \((;x)\) where \(:^{d}\). Thus, each worker \(w_{i}\) has a _local empirical loss_ function defined as \(_{i}()_{x_{i}}( ;x)\). Ideally, when all the workers are assumed _honest_ (i.e., non-Byzantine), the server can compute a model minimizing the global average loss function given by \(_{i=1}^{n}_{i}()\), without requiring the workers to share their raw data points. However, this goal is rendered vacuous in the presence of Byzantine workers. A more reasonable goal for the server is to compute a model minimizing the _global honest loss_, i.e., the average loss of the honest workers. Specifically, denoting by \([n]\) where \(||=n-f\), the indices of honest workers, the goal in robust distributed learning is to solve the following optimization problem:2

\[_{^{d}}_{}()|}_{i}_{i}()\;.\] (1)

Because Byzantine workers may send bogus information and are unknown to the server, solving (even approximately) the optimization problem (1) is known to be impossible in general [26; 20]. The key reason for this impossibility is precisely data heterogeneity. Indeed, we cannot obtain meaningful robustness guarantees unless data heterogeneity is bounded across honest workers.

**Modeling heterogeneity.** Prior work on robustness primarily focuses on a restrictive heterogeneity bound we call \(G\)_-gradient dissimilarity_[12; 20; 3]. Specifically, denoting \(\|\|\) to be the Euclidean norm, the honest workers are said to satisfy \(G\)-gradient dissimilarity if for all \(^{d}\), we have

\[|}_{i}\|_{i}()- _{}()\|^{2} G^{2}\;.\] (2)

However, the above uniform bound on the inter-worker variance of local gradients may not hold in common machine learning problems such as least-squares regression, as we discuss in Section 3. In our work, we consider the more general notion of \((G,B)\)_-gradient dissimilarity_, which is a prominent data heterogeneity model in the classical (Byzantine-free) distributed machine learning literature (i.e., when \(f=0\)) [18; 23; 27; 29]. Recent works have also adopted this definition in the context of Byzantine robust learning [20; 14], but did not provide tight analyses, as we discuss in Section 5. Formally, \((G,B)\)-gradient dissimilarity is defined as follows.

**Assumption 1** (\((G,B)\)-gradient dissimilarity).: The local loss functions of honest workers (represented by set \(\)) are said to satisfy \((G,B)\)_-gradient dissimilarity_ if, for all \(^{d}\), we have3

\[|}_{i}\|_{i}()- _{}()\|^{2} G^{2}+B^{2}\,\| _{}()\|^{2}\,.\]

Under \((G,B)\)-gradient dissimilarity, the inter-worker variance of gradients need not be bounded, and can grow with the norm of the global loss function's gradient at a rate bounded by \(B\). Furthermore, this notion also generalizes \(G\)-gradient dissimilarity, which corresponds to the special case of \(B=0\).

### Our contributions

We provide the first tight analysis on robustness to Byzantine workers in distributed learning under a realistic data heterogeneity model, specifically \((G,B)\)-gradient dissimilarity. Our key contributions are summarized as follows.

**Breakdown point.** We establish a novel _breakdown point_ for distributed learning under heterogeneity. Prior to our work, the upper bound on the breakdown point was simply \(\), i.e., when half (or more) of the workers are Byzantine, no algorithm can provide meaningful guarantees for solving (1). We prove that, under \((G,B)\)-gradient dissimilarity, the breakdown point is actually \(}\). That is, the breakdown point of distributed learning is lower than \(\) under heterogeneity due to non-zero growth rate \(B\) of gradient dissimilarity. We also confirm empirically that the breakdown point under heterogeneity can be much lower than \(\), which could not be explained prior to our work.

**Tight error bounds.** We show that, under the necessary condition \(<}\), any robust distributed learning algorithm must incur an optimization _error_ in

\[()\,f} G^{2})\] (3)

on the class of smooth strongly convex loss functions. We also show that the above lower bound is tight. Specifically, we prove a _matching_ upper bound for the class of smooth non-convex loss functions, by analyzing a robust variant of distributed gradient descent.

**Proof techniques.** To prove our new breakdown point and lower bound, we construct an instance of quadratic loss functions parameterized by their scaling coefficients and minima. While the existing lower bound under \(G\)-gradient dissimilarity can easily be obtained by considering quadratic functions with different minima and identical scaling coefficients (see proof of Theorem III in ), this simple proof technique fails to capture the impact of non-zero growth rate \(B\) in gradient dissimilarity. In fact, the main challenge we had to overcome is to devise a _coupling_ between the parameters of the considered quadratic losses (scaling coefficients and minima) under the \((G,B)\)-gradient dissimilarity constraint. Using this coupling, we show that when \(}\), the distance between the minima of the quadratic losses can be made arbitrarily large by carefully choosing the scaling coefficients, hence yielding an arbitrarily large error. We similarly prove the lower bound (3) when \(<}\).

### Paper outline

The remainder of this paper is organized as follows. Section 2 presents our formal robustness definition and recalls standard assumptions. Section 3 discusses some key limitations of previous works on heterogeneity under \(G\)-gradient dissimilarity. Section 4 presents the impossibility and lower bound results under \((G,B)\)-gradient dissimilarity, along with a sketch of proof. Section 5 presents tight upper bounds obtained by analyzing robust distributed gradient descent under \((G,B)\)-gradient dissimilarity. Full proofs are deferred to appendices A, B and C. Details on the setups of our experimental results are deferred to Appendix D.

## 2 Formal definitions

In this section, we state our formal definition of robustness and standard optimization assumptions. Recall that an algorithm is deemed robust to adversarial workers if it enables the server to approximate a minimum of the global honest loss, despite the presence of \(f\) Byzantine workers whose identity is a priori unknown to the server. In Definition 1, we state the formal definition of robustness.

**Definition 1** (\((f,)\)**-resilience.**).: A distributed algorithm is said to be _\((f,)\)-resilient_ if it can output a parameter \(\) such that

\[_{}()-_{*,},\]

where \(_{*,}_{^{d}} _{}()\).

Accordingly, an \((f,)\)-resilient distributed algorithm can output an _\(\)-approximate_ minimizer of the global honest loss function, despite the presence of \(f\) adversarial workers. Throughout the paper, we assume that \(<\), as otherwise \((f,)\)-resilience is in general impossible . Note also that, for general smooth non-convex loss functions, we aim to find an approximate _stationary point_ of the global honest loss instead of a minimizer, which is standard in non-convex optimization .

**Standard assumptions.** To derive our lower bounds, we consider the class of smooth strongly convex loss functions. We derive our upper bounds for smooth non-convex functions, and for functions satisfying the Polyak-Lojasiewicz (PL) inequality. This property relaxes strong convexity, i.e., strongconvexity implies PL, and covers learning problems which may be non-strongly convex such as least-squares regression . We recall these properties in definitions 2 and 3 below.

**Definition 2** (\(L\)-smoothness).: A function \(^{d}\) is \(L\)-smooth if, for all \(,^{}^{d}\), we have

\[\|(^{})-()\|  L\|^{}-\|\.\]

This is equivalent  to, for all \(,^{}\), having \(|(^{})-()- (),\,^{}-|\|^{}-\|^{2}\).

**Definition 3** (\(\)-Polyak-Lojasiewicz (PL), strong convexity).: A function \(^{d}\) is \(\)-PL if, for all \(^{d}\), we have

\[2(()-_{*})\| ()\|^{2},\]

where \(_{*}_{^{d}}()\). Function \(\) is \(\)-strongly convex if, for all \(,^{}^{d}\), we have

\[(^{})-()-(),\,^{}-\| ^{}-\|^{2}.\]

Note that a function satisfies \(L\)-smoothness and \(\)-PL inequality simultaneously only if \( L\). Lastly, although not needed for our results to hold, when the global loss function \(_{}\) is \(\)-PL, we will assume that it admits a unique minimizer, denoted \(_{*}\), for clarity.

## 3 Brittleness of previous approaches on heterogeneity

Under the \(G\)-gradient dissimilarity condition, presented in (2), prior work has established lower bounds  and matching upper bounds  for robust distributed learning. However, \(G\)-gradient dissimilarity is arguably unrealistic, since it requires a _uniform_ bound \(G^{2}\) on the variance of workers' gradients over the parameter space. As a matter of fact, \(G\)-gradient dissimilarity does not hold in general for simple learning tasks such as least-squares regression, as shown in Observation 1 below.

**Observation 1**.: In general, \(G\)-gradient dissimilarity (2) does not hold in least-squares regression.

Proof.: Consider the setting given by \(=^{2}\), \(n=2,f=0\), and \(d=1\). For any data point \((x_{1},x_{2})\), consider the squared error loss \((;(x_{1},x_{2}))=( x_{1}-x_{2})^{2}\). Let the local datasets be \(_{1}=\{(1,0)\}\), \(_{2}=\{(0,1)\}\). Note that for all \(\), we have \(_{1}()=\), and \(_{2}()=0\). This implies that, \(|}_{i}\|_{i}()-_{}()\|^{2}=}}{{4}}\), where \(=\{1,2\}\), which is unbounded over \(\). Hence, the condition of \(G\)-gradient dissimilarity cannot be satisfied for any \(G\). 

In contrast, the \((G,B)\)-gradient dissimilarity condition, presented in Assumption 1, is more realistic, since it allows the variance across the local gradients to grow with the norm of the global gradient. This condition is common in the (non-robust) distributed learning literature [23; 18; 25; 29], and is also well-known in the (non-distributed) optimization community [8; 9; 32]. While \(G\)-gradient dissimilarity corresponds to the special case of \((G,0)\)-gradient dissimilarity, we show in Proposition 1 below that a non-zero growth rate \(B\) of gradient dissimilarity allows us to characterize heterogeneity in a much larger class of distributed learning problems.

**Proposition 1**.: _Assume that the global loss \(_{}\) is \(\)-PL and \(L\)-smooth, and that for each \(i\) local loss \(_{i}\) is convex and \(L_{i}\)-smooth. Denote \(L_{}_{i}L_{i}\). Then, Assumption 1 is satisfied, i.e., the local loss functions satisfy \((G,B)\)-gradient dissimilarity, with_

\[G^{2}=|}_{i}\| _{i}(_{*})\|^{2} B^{2}=}{}-1.\] (4)

We present the proof of Proposition 1 in Appendix A for completeness. However, note that it can also be proved following existing results [32; 21] derived in other contexts. The \((G,B)\)-gradient dissimilarity condition shown in Proposition 1 is arguably tight (up to multiplicative factor \(2\)), in the sense that \(G^{2}\) cannot be improved in general. Indeed, as the \((G,B)\)-gradient dissimilarity inequality should be satisfied for \(=_{*}\), \(G^{2}\) should be at least the variance of honest gradients at the minimum, i.e., \(|}_{i}\| _{i}(_{*})\|^{2}\).

**Gap between existing theory and practice.** The theoretical limitation of \(G\)-gradient dissimilarity is exacerbated by the following empirical observation. We train a linear least-squares regression model on the _mg_ LIBSVM dataset . The system comprises \(7\) honest and \(3\) Byzantine workers. We simulate _extreme heterogeneity_ by having each honest worker hold one distinct point. We implement four well-studied Byzantine attacks: _sign flipping_ (SF) , _fall of empires_ (FOE) , _a little is enough_ (ALIE)  and _mimic_. More details on the experimental setup can be found in Appendix D. We consider the state-of-the-art robust variant of distributed gradient descent (detailed later in Section 5.1) that uses the NNM robustness scheme  composed with coordinate-wise trimmed mean. The empirical success on this learning task, which could not be explained by existing theory under \(G\)-gradient dissimilarity following Observation 1, is covered under \((G,B)\)-gradient dissimilarity, as per Proposition 1. We present formal robustness guarantees under \((G,B)\)-gradient dissimilarity later in Section 5. Additionally, through experimental evaluations in Section 5, we observe that even if \(G\)-gradient dissimilarity were assumed to be true, the bound \(G^{2}\) may be extremely large in practice, thereby inducing a non-informative error bound \((}{{n}} G^{2})\). On the other hand, under \((G,B)\)-gradient dissimilarity, we obtain tighter bounds matching empirical observations.

## 4 Fundamental limits on robustness under \((G,b)\)-Gradient Dissimilarity

Theorem 1 below shows the fundamental limits on robustness in distributed learning under \((G,B)\)-gradient dissimilarity. The result has twofold implications. On the one hand, we show that the breakdown point of any robust distributed learning algorithm reduces with the growth rate \(B\) of gradient dissimilarity. On the other hand, when the fraction \(}{{n}}\) is smaller than the breakdown point, both \(G\) and \(B\) induce a lower bound on the learning error.

**Theorem 1**.: _Let \(0<f<n/2\). Assume that the global loss \(_{}\) is \(L\)-smooth and \(\)-strongly convex with \(0<<L\). Assume also that the honest local losses satisfy \((G,B)\)-gradient dissimilarity (Assumption 1) with \(G>0\). Then, a distributed algorithm can be \((f,)\)-resilient only if_

\[<})\,f} G^{2}\;.\]

Sketch of proof.: The full proof is deferred to Appendix B. In the proof, we construct hard instances for \((f,)\)-resilience, using a set of quadratic functions of the following form:

\[_{i}()=\|-z \|^{2}&,& i\{1,,f\},\\ _{i}()=\|\|^{2}&,& i \{f+1,,n-f\},\\ _{i}()=\|\|^{2}&,& i \{n-f+1,,n\}.\]

To prove the theorem, we consider two plausible scenarios corresponding to two different identities of honest workers, which are _unknown_ to the algorithm. Specifically, in scenarios I and II, we assume the indices of honest workers to be \(S_{1}\{1,,n-f\}\) and \(S_{2}\{f+1,,n\}\), respectively.

Figure 1: Evolution of the training losses **(left)** and the trajectories **(right)** of robust D-GD algorithm with NNM and coordinate-wise trimmed mean (see Section 5.1), on the _mg_ LIBSVM least-squares regression task described in Section 3. \(f=0\) corresponds to the case where the algorithm is run without Byzantine workers.

We show that for all \(^{d}\),

\[\{_{_{1}}()-_{,S_{1}},\; _{_{2}}()-_{,S_{2}}\} )^{2}^{2}}{8(+)}\|z\|^{2}\;.\] (5)

That is, every model in the parameter space incurs an error that grows with \(\|z\|^{2}\), in at least one of the two scenarios. Hence, an \((f,)\)-resilient algorithm, by Definition 1, must guarantee optimization error \(\) in both scenarios I and II, which together with (5) implies that

\[)^{2}^{2}}{8(+)}\|z\|^{2}.\] (6)

At this point, to obtain the largest lower bounds possible, our goal is to maximize the right-hand side of 6, under the constraint that the loss functions induced by the triplet \((,K,z)\) satisfy \((G,B)\)-gradient dissimilarity, \(L\)-smoothness and \(\)-strong convexity (simultaneously in both scenarios). We separately analyze this error in two cases: (i) \(}\) and (ii) \(<}\). In both cases, we construct a coupling between the values of \(z\) and \(K\) by having the norm \(\|z\|^{2}\) proportional to \(K\). Specifically, in case (i), we show that the condition \(}\) allows us to choose \(K\) arbitrarily large while satisfying \((G,B)\)-dissimilarity. Thus, \(\|z\|^{2}\) being proportional to \(K\) means that \(\) is arbitrarily large as per (6). Similarly, in case (ii) where \(<}\), \(K\) cannot be arbitrarily large and carefully choosing a large possible value yields

\[)\,f} G^{2}.\]

One of the crucial components to the above deductions was finding the suitable triplets \((,K,z)\) while preserving the \((G,B)\)-gradient dissimilarity assumption (along with the smoothness and strong convexity assumptions) simultaneously in both the two scenarios, thereby establishing their validity. While the exact calculations are tedious, intuitively, \(B\) constrains the relative difference between the scale parameters \(\) and \(\), and \(G\) constrains the separation between the minima, i.e. \(\|z\|^{2}\). 

**Extension to non-convex problems.** The lower bound from Theorem 1 assumes that the given distributed algorithm satisfies \((f,)\)-resilience, which means finding an \(\)-approximate minimizer of the global honest loss \(_{}\). The latter may not be possible for the general case of smooth and non-convex functions. In that case we cannot seek an \(\)-approximate minimizer, but rather an \(\)-approximate stationary point , i.e., \(\) such that \(\|_{}()\|^{2}\). Then the lower bound in Theorem 1, in conjunction with the \(\)-PL inequality, yields the following lower bound

\[)f} G^{2}\;.\] (7)

**Comparison with prior work.** The result of Theorem 1 generalizes the existing robustness limits derived under \(G\)-gradient dissimilarity . In particular, setting \(B=0\) in Theorem 1, we recover the breakdown point \(\) and the optimization lower bound \((}{{n}} G^{2})\). Perhaps, the most striking contrast to prior work  is our breakdown point \(}\), instead of simply \(\). We remark that a similar dependence on heterogeneity has been repeatedly assumed in the past, but without any formal justification. For instance, under \((0,B)\)-gradient dissimilarity, [20, Theorem IV] assumes \(}{{n}}=(}{{B^{2}}})\) to obtain a formal robustness guarantee. In the context of robust distributed convex optimization (and robust least-squares regression), the upper bound assumed on the fraction \(}{{n}}\) usually depends upon the condition number of the distributed optimization problem, e.g., see [6, Theorem 3] and [16, Theorem 2]. Our analysis in Theorem 1 justifies these assumptions on the breakdown point in prior work under heterogeneity.

**Empirical breakdown point.** Interestingly, our breakdown point \(}\) allows to better understand some empirical observations indicating that the breakdown point of robust distributed learning algorithms is smaller than \(}{{2}}\). We illustrate this in Figure 2 with a logistic regression model on the MNIST dataset under _extreme heterogeneity_, i.e., each worker dataset contains data points from a single class. We consider the state-of-the-art robust variant of distributed gradient descent (detailed later in Section 5.1) that uses the NNM robustness scheme composed with robust aggregation rules, namely, _coordinate-wise trimmed-mean_ (CW Trimmed Mean) , _Krum_, _coordinate-wise median_ (CW Median)  and _geometric median_. We observe that all these methods consistently fail to converge as soon as the fraction of Byzantine workers exceeds \(\), which is well short of the previously known theoretical breakdown point \(\). Theorem 1, to the best of our knowledge, provides the first formal justification to this empirical observation.

## 5 Tight upper bounds under \((G,b)\)-Gradient Dissimilarity

We demonstrate in this section that the bounds presented in Theorem 1 are tight. Specifically, we show that a robust variant of distributed gradient descent, referred to as _robust D-GD_, yields an asymptotic error that matches the lower bound under \((G,B)\)-gradient dissimilarity, while also proving the tightness of the breakdown point. Lastly, we present empirical evaluations showcasing a significant improvement over existing robustness analyses that relied upon \(G\)-gradient dissimilarity.

### Convergence analysis of robust D-GD

In robust D-GD, the server initially possesses a model \(_{0}\). Then, at each step \(t\{0,,T-1\}\), the server broadcasts model \(_{t}\) to all workers. Each honest worker \(w_{i}\) sends the gradient \(g_{t}^{(i)}=_{i}(_{t})\) of its local loss function at \(_{t}\). However, a Byzantine worker \(w_{j}\) might send an arbitrary value for its gradient. Upon receiving the gradients from all the workers, the server aggregates the local gradients using a _robust aggregation rule_\(F^{d n}^{d}\). Specifically, the server computes \(R_{t}:=F(g_{t}^{(1)},,g_{t}^{(n)})\). Ultimately, the server updates the current model \(_{t}\) to \(_{t+1}=_{t}- R_{t}\), where \(>0\) is the _learning rate_. The full procedure is summarized in Algorithm 1.

``` Input: Initial model \(_{0}\), robust aggregation \(F\), learning rate \(\), and number of steps \(T\). for\(t=0 T-1\)do  Server broadcasts \(_{t}\) to all the workers; for each honest worker \(}\) in paralleldo  Compute and send gradient \(g_{t}^{(i)}=_{i}(_{t})\); // A Byzantine worker \(w_{j}\) may send an arbitrary value for \(g_{t}^{(j)}\)  Server computes the aggregate gradient: \(R_{t}=F(g_{t}^{(1)},,g_{t}^{(n)})\);  Server updates the model: \(_{t+1}=_{t}- R_{t}\); ```

**Algorithm 1**Robust D-GD

Figure 2: Best training loss **(left)** and accuracy **(right)** using robust D-GD (see Section 5.1) with NNM to train a logistic regression model on the MNIST dataset, in the presence of \(10\) honest workers and \(1\) to \(9\) Byzantine workers. The Byzantine workers use the sign flipping attack. More details on the experimental setup can be found in Appendix D.

To analyze robust D-GD under \((G,B)\)-gradient dissimilarity, we first recall the notion of \((f,)\)_-robustness_ in Definition 4 below. First introduced in , \((f,)\)-robustness is a general property of robust aggregation that covers several existing aggregation rules.

**Definition 4** (\((f,)\)-robustness).: Let \(n 1\), \(0 f<n/2\) and \( 0\). An aggregation rule \(F^{d n}^{d}\) is said to be \((f,)\)_-robust_ if for any vectors \(x_{1},,\,x_{n}^{d}\), and any set \(S[n]\) of size \(n-f\), the output \( F(x_{1},,\,x_{n})\) satisfies the following:

\[\|-_{S}\|^{2}_{i S}\|x_{ i}-_{S}\|^{2}\,,\]

where \(_{S}_{i S}x_{i}\). We refer to \(\) as the _robustness coefficient_ of \(F\).

Closed-form robustness coefficients for multiple aggregation rules can be found in . For example, assuming \(n(2+)f\), for some \(>0\), \(=()\) for coordinate-wise trimmed mean, \(=(1)\) for coordinate-wise median, and \(_{F}=((+1))\) when \(F\) is composed with NNM .

Assuming \(F\) to be an \((f,)\)-robust aggregation rule, we show in Theorem 2 below the convergence of robust D-GD in the presence of up to \(f\) Byzantine workers, under \((G,B)\)-gradient dissimilarity.

**Theorem 2**.: _Let \(0 f<n/2\). Assume that the global loss \(_{}\) is \(L\)-smooth and that the honest local losses satisfy \((G,B)\)-gradient dissimilarity (Assumption 1). Consider Algorithm 1 with learning rate \(=\). If the aggregation \(F\) is \((f,)\)-robust with \( B^{2}<1\), then the following holds for all \(T 1\)._

1. _In the general case where_ \(_{}\) _may be non-convex, we have_ \[_{t=0}^{T-1}\|_{}(_{t}) \|^{2}}{1- B^{2}}+_{}(_{0})-_{*},)}{(1- B^ {2})T}.\]
2. _In the case where_ \(_{}\) _is_ \(\)_-PL, we have_ \[_{}(_{T})-_{*,}}{2(1- B^{2})}+e^{-(1- B ^{2})T}(_{}(_{0})-_{*, }).\]

Tightness of the result.We recall that the best possible robustness coefficient for an aggregation \(F\) is \(=}{{n-2f}}\) (see ). For such an aggregation rule, the sufficient condition \( B^{2}<1\) reduces to \(}{{n-2f}} B^{2}<1\) or equivalently \(}{{n}}<}{{2+B^{2}}}\). Besides, robust D-GD guarantees \((f,)\)-resilience, for \(\)-PL losses, where we have \(=(}{{n-(2+B^{2})f}} G^{2})\) asymptotically in \(T\). Both these conditions on \(}{{n}}\) and \(\) indeed match the limits shown earlier in Theorem 1. Yet, we are unaware of an aggregation rule with an order-optimal robustness coefficient, i.e., usually \(>}{{n-2f}}\). However, as shown in , the composition of _nearest neighbor mixing_ (NNM) with several aggregation rules, such as CW Trimmed Mean, Krum and Geometric Median, yields a robustness coefficient \(=(}{{n-2f}})\). Therefore, robust D-GD can indeed achieve \((f,)\)-resilience with an optimal error \(=(}{{n-(2+B^{2})f}} G^{2})\), but for a suboptimal breakdown point. The same observation holds for the non-convex case, where the lower bound is given by (7). Lastly, note that when \(B=0\), our result recovers the bounds derived in prior work under \(G\)-gradient dissimilarity .

We remark that while the convergence rate of robust D-GD shown in Theorem 2 is linear (which is typical to convergence of gradient descent in strongly convex case), it features a _slowdown factor_ of value \(1- B^{2}\). Hence, suggesting that Byzantine workers might decelerate the training under heterogeneity. This slowdown is also empirically observed (e.g., see Figure 1). Whether this slowdown is fundamental to robust distributed learning is an interesting open question. Investigating such a slowdown in the stochastic case is also of interest, as existing convergence bounds are under \(G\)-gradient dissimilarity only, for strongly convex  and non-convex  cases.

Comparison with prior work.Few previous works have studied Byzantine robustness under \((G,B)\)-gradient dissimilarity . While these works do not provide lower bounds, the upper bound they derive (see Appendix E in  and Appendix E.4 in ) are similar to Theorem 2, with some notable differences. First, unlike the notion of \((f,)\)-robustness that we use, the so-called \((c,)\)-agnostic robustness, used in , is a stochastic notion. Under the latter notion, good parameters \((c,)\) of robust aggregators were only shown when using a randomized method called Bucketing . Consequently, instead of obtaining a deterministic error bound as in Theorem 2,simply replacing \(c\) with \(\) in  gives an expected bound, which is strictly weaker than the result of Theorem 2. Moreover, the corresponding non-vanishing upper bound term and breakdown point for robust D-GD obtained from the analysis in  for several robust aggregation rules (e.g., coordinate-wise median) are worse than what we obtain using \((f,)\)-robustness.

### Reducing the gap between theory and practice

In this section, we first argue that, even if we were to assume that the \(G\)-gradient dissimilarity condition (2) holds true, the robustness bounds derived in Theorem 2 under \((G,B)\)-gradient dissimilarity improve upon the existing bounds  that rely on \(G\)-gradient dissimilarity. Next, we compare the empirical observations for robust D-GD with our theoretical upper bounds.

**Comparing upper bounds.** We consider a logistic regression model on MNIST dataset under extreme heterogeneity. While it is difficult to find tight values for parameters \(G\) and \(B\) satisfying \((G,B)\)-gradient dissimilarity, we can approximate these parameters through a heuristic method. A similar approach can be used to approximate \(\) for which the loss functions satisfy the condition of \(\)-gradient dissimilarity. We defer the details on these approximations to Appendix D. In Figure 4, we compare the error bounds, i.e., \(}{{n-(2+B^{2})}}{f} G^{2}\) and \(}{{n-2f}}^{2}\), guaranteed for robust D-GD under \((G,B)\)-gradient dissimilarity and \(\)-gradient dissimilarity, respectively. We observe that the latter bound is extremely large compared to the former, which confirms that the tightest bounds under \(G\)-gradient dissimilarity are vacuous for practical purposes.

We further specialize the result of Theorem 2 to the convex case for which the \((G,B)\)-gradient dissimilarity condition was characterized in Proposition 1. We have the following corollary.

**Corollary 1**.: _Assume that the global loss \(_{}\) is \(\)-PL and \(L\)-smooth, and that for each \(i\) local loss \(_{i}\) is convex and \(L_{i}\)-smooth. Denote \(L_{}_{i}L_{i}\) and assume that \((}{}-1) 1\). Consider Algorithm 1 with learning rate \(=\). If \(F\) is \((f,)\)-robust, then for all \(T 1\), we have_

\[_{}(_{T})-_{*,}|}_{i}\|_{ i}(_{*})\|^{2}+e^{-T}(_{}( _{0})-_{*,}).\]

The non-vanishing term in the upper bound shown in Corollary 1 corresponds to the heterogeneity at the minimum \(|}_{i}\|_{i}(_{ *})\|^{2}\). This quantity is considered to be a natural measure of gradient dissimilarity in classical (non-Byzantine) distributed convex optimization . As such, we believe that this bound cannot be improved upon in general.

**Matching empirical performances.** Since the upper bound in Corollary 1 requires computing the constants \(,L\), we choose to conduct this experiment on least-squares regression, where the exact computation of \(,L\) is possible. We compare the empirical error gap (left-hand side of Corollary 1) with the upper bound (right-hand side of Corollary 1). Our findings, shown in Figure 4, indicate that our theoretical analysis reliably predicts the empirical performances of robust D-GD, especially when the fraction of Byzantine workers is small. Note, however, that our upper bound is non-informative when more than \(\) of the workers are Byzantine, as the predicted error exceeds the initial loss value. We believe this to be an artifact of the proof, i.e. the upper bound is meaningful only up to a multiplicative constant. Indeed, when visualizing the results in logarithmic scale in Figure 4, the shape of empirical measurements and our upper bounds are quite similar.

## 6 Conclusion and future work

This paper revisits the theory of robust distributed learning by considering a realistic data heterogeneity model, namely \((G,B)\)-gradient dissimilarity. Using this model, we show that the breakdown point depends upon heterogeneity (specifically, \(}{{2+B^{2}}}\)) and is smaller than the usual fraction \(}{{2}}\). We prove a new lower bound on the learning error of any distributed learning algorithm, which is matched using robust D-GD. Moreover, we show that our theoretical guarantees align closely with empirical observations, contrary to prior works which rely upon the stringent model of \(G\)-gradient dissimilarity.

An interesting future research direction is to investigate whether the \(1- B^{2}\) slowdown factor in the convergence rate of robust D-GD (Theorem 2) is unavoidable. Another interesting research problem is to derive lower (and upper) bounds independent of the heterogeneity model, thereby elucidating the tightness of the convergence guarantee of robust D-GD in the strongly convex case (Corollary 1).