# ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation

Zhitong Gao\({}^{1}\), Shipeng Yan\({}^{1}\), Xuming He\({}^{1,2}\)

\({}^{1}\)School of Information Science and Technology, ShanghaiTech University

\({}^{2}\)Shanghai Engineering Research Center of Intelligent Vision and Imaging

{gaozht,yanshp,hexm}@shanghaitech.edu.cn

###### Abstract

Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, observing consistent performance improvements across various baseline models. Code is available at https://github.com/gaozhitong/ATTA.

## 1 Introduction

Semantic segmentation, a fundamental computer vision task, has witnessed remarkable progress thanks to the expressive representations learned by deep neural networks . Despite the advances, most deep models are trained under a close-world assumption, and hence do not possess knowledge of what they do not know, leading to over-confident and inaccurate predictions for the unknown objects . To address this, the task of _dense out-of-distribution (OOD) detection_, which aims to generate pixel-wise identification of the unknown objects, has attracted much attention as it plays a vital role in a variety of safety-critical applications such as autonomous driving.

Recent efforts in dense OOD detection have primarily focused on the scenarios where training and testing data share a similar domain, assuming no domain shift (or covariant shift) between them . However, domain shift widely exists in real-world situations  and can also be observed in common dense OOD detection benchmarks . In view of this, we investigate the performance of existing dense OOD detection methods under the test setting with domain-shift and observe significant performance degradation in comparison with the setting without domain-shift (cf. Figure 1). In particular, the state-of-the-art detection models typically fail to distinguish the distribution shift in domain and the distribution shift in semantics, and thus tend to predict high uncertainty scores for inlier-class pixels.

A promising strategy to tackle such domain shift is to adapt a model during test (known as test-time adaptation (TTA) ), which utilizes unlabeled test data to finetune the model without requiring prior information of test domain. However, applying the existing test-time domain adaption (TTA) techniques  to the task of general dense OOD detection faces two critical challenges. First, traditional TTA methods often assume the scenarios where all test data are under domain shiftwhile our dense OOD detection task addresses a more realistic setting where test data can come from seen or unseen domains without prior knowledge. In such a scenario, TTA techniques like the transductive batch normalization (TBN) [36; 40; 2], which substitutes training batch statistics with those of the test batch, could inadvertently impair OOD detection performance on images from seen domains due to inaccurate normalization parameter estimation (cf. Figure 1(b)). On the other hand, the existence of novel classes in test images further complicates the problem. Unsupervised TTA losses like entropy minimization [46; 12; 32; 45] often indiscriminately reduce the uncertainty or OOD scores of these novel classes, leading to poor OOD detection accuracy (cf. Figure 1(b)). Consequently, how to design an effective test-time adaptation strategy for the general dense OOD detection in wild remains an open problem.

In this work, we aim to address the aforementioned limitations and tackle the problem of dense OOD detection in wild with both domain- and semantic-level distribution shift. To this end, we propose a novel dual-level test-time adaptation framework that simultaneously detects two types of distribution shift and performs online model adaptation in a selective manner. Our core idea is to leverage low-level feature statistics of input image to detect whether domain-level shift exists while utilizing dense semantic representations to identify pixels with semantic-level shift. Based on this dual-level distribution-shift estimation, we design an anomaly-aware self-training procedure to compensate for the potential image-level domain shift and to enhance its novel-class detection capacity based on re-balanced uncertainty minimization of model predictions. Such a selective test-time adaptation strategy allows us to adapt an open-set semantic segmentation model to a new environment with complex distribution shifts.

Specifically, we develop a cascaded modular TTA framework for any pretrained segmentation model with OOD detection head. Our framework consists of two main stages, namely a selective Batch Normalization (BN) stage and an anomaly-aware self-training stage. Given a test image (or batch), we first estimate the probability of domain-shift based on the statistics of the model's BN activations and update the normalization parameters accordingly to incorporate new domain information. Subsequently, our second stage performs an online self-training for the entire segmentation model based on an anomaly-aware entropy loss, which jointly minimizes a re-balanced uncertainty of inlier-class prediction and outlier detection. As the outlier-class labels are unknown, we design a mixture model in the OOD score space to generate the pseudo-labels of pixels for the entropy loss estimation.

We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, based on FS Static , FS Lost&Found , RoadAnomaly  and SMIYC . The results show that our method consistently improves the performance of dense OOD detection across various baseline models especially on the severe domain shift settings, and achieves new state-of-the-arts performance on the benchmarks.

To summarize, our main contribution is three-folds: (i) We propose the problem of dense OOD detection under domain shift (or covariance shift), revealing the limitations of existing dense OOD detection methods in wild. (ii) We introduce an anomaly-aware test-time adaptation method that jointly tackles domain and semantic shifts. (iii) Our extensive experiments validate our approach, demonstrating significant performance gains on various OOD segmentation benchmarks, especially those with notable domain shifts.

Figure 1: (a) We visualize OOD score maps and the corresponding histograms generated by the SOTA method PEBAL  on both an original and a domain-shifted (smog-corrupted) image. (b) We quantify the drop in PEBALâ€™s performance with the added domain shift and compare it to the performance when combined with our method or existing test-time adaptation methods such as TBN  and Tent . Please refer to Section 4.3 for additional results.

Related Work

Dense Out-of-distribution DetectionThe task of Out-of-distribution (OOD) detection aims to identify samples that are not from the same distribution as the training data . While most work focuses on image-level out-of-distribution detection [18; 27; 25; 19; 30; 42], some researchers have begun to study dense OOD detection [1; 15; 3] (or anomaly segmentation), which is a more challenging task due to its requirement for granular, pixel-level detection and the complex spatial relationships in images. Existing work dealing with dense OOD detection often resorts to either designing specialized OOD detection functions [15; 21; 29; 48] or incorporating additional training objectives or data to boost performance in OOD detection [10; 4; 44]. Different from these approaches, our work aims to enhance the model's ability to detect OOD objects at test time by utilizing online test data and design a model-agnostic method applicable to most differentiable OOD functions and training strategies. Furthermore, we target for a more general dense OOD detection problem, where domain-shift potentially exists, which brings new challenges unaddressed in the prior literature.

Test-Time Domain AdaptationThe task of test-time domain adaptation (TTA)  aims to study the ability of a machine learning model trained on a source domain to generalize to a different, but related, target domain, using online unlabeled test data only. Existing work on TTA mostly tackles the problem from two aspects: adapting standardization statistics in normalization layers and adapting model parameters as self-training. The first line includes utilizing test-time statistics in each batch  or moving averages [35; 20], or combining source and target batch statistic [40; 51; 22; 28; 52]. The self-training technique including entropy minimization [12; 32; 45] and self-supervised losses [43; 31]. In our work, we also approach test-time adaptation from these two perspectives. However, our primary distinction lies in addressing a more general open-world scenario, where test data can originate from seen or unseen domains, and novel objects may exist. Consequently, we design our method by explicitly considering these factors and aiming to jointly tackle domain shift and semantic shift challenges.

Novel Class in Unseen DomainWhile the two primary forms of distribution shift, domain shift and semantic shift, are typically studied independently in literature, there are instances where both are explored in a more complex setting. One such example is _Open Set Domain Adaptation_, which investigates the unsupervised domain adaptation problem in scenarios where the target data may include new classes. These novel classes must be rejected during the training process. Another line of research focuses on _zero-shot learning in the presence of domain shift_. In this case, novelty rejection must be performed during the testing phase. A domain generalization method is often employed, which requires data from other domains during training. Some work also discuss the impact of covariant-shift in OOD detection . However, our study diverges from these in several key aspects. First, we study the problem of semantic segmentation where the impacts of domain and semantic shifts are more complex and nuanced compared to the general classification problems. Second, we focus on the situation when no additional data or prior knowledge of the test domain can be obtained during the training.

## 3 Method

### Problem Formulation

We first introduce our general dense OOD detection problem setting with potential domain and semantic distribution shift. Formally, we denote an instance of training data as \((x^{s},y^{s}) P_{XY}^{d}\), where \(x^{s}\), \(y^{s}\) denotes the input image and corresponding segmentation label, \(=R^{3 d}\) represents the input space (an image with \(d\) pixels), \(=[1,C]\) is the semantic label space at each pixel, and \(P_{XY}\) is the training data distribution. A test data instance is represented as \((x,y) Q_{XY}\) where \(Q_{XY}\) is the test data distribution. In the OOD detection problem, we have \(Q_{XY} P_{XY}\) and our goal is to identify all pixels where their labels do not belong to the training label space, i.e., \(y_{i}\). In contrast to previous works, we here consider a general problem setting, _dense OOD detection with potential domain shift_, where in addition the input distributions \(P_{X} Q_{X}\) but with overlaps in their support. Such domain and semantic-class changes leads to a complex data distribution shift, which poses new challenges for the conventional dense OOD detection approaches that are unable to distinguish different shift types.

### Model Overview

In this work, we aim to address the general dense OOD detection problem by leveraging unlabeled test data and adapting a segmentation model during test time. Specifically, we begin with a segmentation network that has been trained using data from \(P_{XY}\). Here we consider a common differentiable OOD network design [5; 30; 15; 1] that consists of two main components: a seen-class classifier \(f_{}:^{d}\) and an unseen class detector \(g_{}: R^{d}\). Typically, both network components share the same set of parameters \(\). For instance, \(g_{}\) can be the negative energy score as in [13; 30; 23] or the maximized logit score as in . During testing, an image (or a batch) sequentially arrives from \(Q_{XY}\), and our goal is to update the model parameters for each batch and produce anomaly-aware semantic segmentation outputs.

In order to cope with potential domain and semantic shift in test data, we introduce a novel dual-level test-time adaptation framework, which allows us to simultaneously identify two types of distribution shifts and performs an online self-supervised learning in a selective fashion. Specifically, we instantiate our TTA strategy as a two-stage cascaded learning process. Our first stage determines the existence of domain shift in the image by exploiting the consistent changes in image-level feature statistics. Based on the inferred shift probability, we develop a selective Batch-Normalization module to compensate for the input distribution deviation. In the second stage, we devise an anomaly-aware self-training procedure to enhance the model's capability in detecting novel classes. The procedure iteratively refines the outlier estimation based on a mixture model of OOD scores and minimizes a re-balanced uncertainty of pixel-wise model predictions for effective adaptation to the test data. An overview of our framework is demonstrated in Figure 2.

We update the model parameters in an episodic manner, where for each (batch of) image(s) we use a same initial network parameter. This makes it more resilience if there is no apparent connection between two different (batches of) image(s). In the following, we first explain our design of the selective BN module in Sec. 3.3, followed by our anomaly-aware self-training stage in Sec. 3.4.

### Selective Test-Time Batch Normalization

Our first-stage module aims to identify whether an input image is from the seen or unseen domain and perform model adaptation to compensate any potential domain shift. To achieve this, we take inspiration from the transductive Batch-Normalization (TBN) , which is a common strategy of exploiting test-time data for model adaptation. The vanilla TBN, however, often suffers from unstable estimation due to the small size of test data, and hence can lead to performance degradation for our general setting where test data may have varying levels of domain shift.

To tackle this, we introduce a selective BN module that first estimates the probability of input image being generated from unseen domain, and then performs a mixture of Conventional batch normalization (CBN) and TBN according to the inferred domain-shift probability. Specifically, we denote \(P(z^{d}=1|x)\) as the probability of an image \(x\) from an unknown domain, and we estimate the probability by considering the distribution distance between the deep features of test and training data in the Normalization layers of the segmentation network. Formally, let \(_{i}^{tr},_{l}^{tr}\) be the running mean

Figure 2: The overview of the two-stage Anomaly-aware Test-Time Adaptation (ATTA) framework. For each test image (or batch), our first stage determines the existence of domain shift and develop a selective Batch-Normalization module to compensate for the input distribution deviation. In the second stage, we devise an anomaly-aware self-training procedure via minimizing a re-balanced uncertainty of model predictions to enhance the OOD detection capacity.

and standard deviation at the \(l\)-th BN layer calculated in the end of the model training, \(_{l}(x),_{l}(x)\) be the mean and standard deviation at the \(l\)-th BN layer calculated for each test input \(x\), we compute the domain-shift probability as follows:

\[P(z^{d}=1|x)=h_{a,b}(_{l=1}^{L}(KL((_{l}(x),_{ l}(x))\|(_{l}^{tr},_{l}^{tr})))),\] (1)

where \(\) denotes the normal distribution, \(KL\) denotes the Kullback-Leibler divergence, and the \(h_{a,b}(x)=((x+a)/b)\) is a sigmoid function with linear transform, which normalizes the distance into a probability value. The parameters \(a,b\) are estimated based on the variance of training data statistics such that a data point that is far away from the training statistics has higher probability of image-level domain shift. We then update the BN statistics of the network according to the above probability as follows:

\[_{l}=P(z^{d}=1|x)*_{l}(x)+P(z^{d}=0|x)*_{l}^{tr},\] (2)

\[_{l}^{2}=P(z^{d}=1|x)*_{l}^{2}(x)+P(z^{d}=0|x)*(_{l}^ {tr})^{2},\] (3)

where \(l[1,L]\) and \(L\) is the max depth of the BN layers. Such an adaptive BN enables us to balance between a stable BN for the data without domain-shift and a test-time BN tailored to the data with domain shift.

### Anomaly-aware Self-training Procedure

After compensating for the potential domain gap, we introduce a second stage of test-time model adaptation, aiming for enhancing the model capacity in OOD detection and closed-set prediction on the test data. To this end, we propose an online self-training procedure for the entire segmentation model based on an anomaly-aware prediction entropy loss. By minimizing a weighted uncertainty of inlier-class prediction and outlier detection, we are able to promote the confidence of model predictions, which leads to a more discriminative pixel-wise classifier on the test data.

Formally, we construct a \((C+1)\)-class pixelwise probabilistic classifier based on the two network modules \(f_{}\) and \(g_{}\) and denote its output distribution as \(^{(C+1) d}\) with each element \(_{c,i}\) being the probability of \(P_{}(y_{i}=c|x)\), \(c[1,C+1]\). For each pixel \(i\), the first \(C\) channels represent the probabilities of being each closed-set class while the last channel is the probability of being an outlier class. Given the output distribution, we define our learning objective function as,

\[_{}(x)=-_{i}_{c=1}^{C+1}w_{c}_{c,i}(_{c,i}),\] (4)

where \(w_{c}\) is the weight for class \(c\). We introduce the class weights to cope with the class imbalance problem between the inlier and outlier classes as the latter typically have much lower proportion in images. More specifically, we set \(w_{c}=1\) for \(1 c C\) and \(w_{c}=>1\) for \(c=C+1\). In the following, we first describe how we estimate the output probabilities \(\) based on the seen-class classifier \(f_{}\) and the OOD detector \(g_{}\), followed by the loss optimization procedure.

Anomaly-aware output representationTo derive the output probability \(\), we introduce an auxiliary variable \(Z^{o}\{0,1\}^{d}\), where \(Z^{o}_{i}=1\) indicates the \(i\)-th pixel is an outlier, and \(P_{}(Z^{o}_{i}=1|x)\) denotes the corresponding probability. We also assume that the seen classifier \(f_{}\) outputs a pixelwise probability map \(F^{C d}\) for the known classes \(c\) and the OOD detector \(g_{}\) generates a pixelwise outlier score map \(G^{d}\). The anomaly-aware output distribution \(_{i}\) for the \(i\)-th pixel can be written as,

\[_{c,i} =P_{}(_{i}=c|x,Z^{o}_{i}=0)P_{}(Z^{o}_{i}=0|x)+ P_{}(_{i}=c|x,Z^{o}_{i}=1)P_{}(Z^{o}_{i}=1|x)\] (5) \[=F_{c,i}(1-P_{}(Z^{o}_{i}=1|x)) c +P_{}(Z^{o}_{i}=1|x) c=C+1,\] (6)

where \(\) is the indicator function, and we use the fact that \(P_{}(_{i}=c|x,Z^{o}_{i}=0)=F_{c,i}\) for \(c\) and \(0\) for \(c=C+1\), and \(P_{}(_{i}=c|x,Z^{o}_{i}=1)=0\) for \(c\) and \(1\) for \(c=C+1\).

Given the marginal probability in Eqn (6), our problem is reduced to estimating the pixelwise outlier probability \(P_{}(Z^{o}_{i}=1|x)\). However, as we only have an arbitrary outlier score map \(G\) output by \(g_{}\)it is non-trivial to convert this unnormalized score map into a valid probability distribution. A naive nonlinear transform using the sigmoid function or normalizing the scores with sample mean could lead to incorrect estimation and hurt the performance. To tackle this problem, we develop a data-driven strategy that exploits the empirical distribution of the pixelwise OOD scores to automatically calibrate the OOD scores into the outlier probability \(P_{}(Z_{i}^{o}|x)\).

More specifically, we observe that the empirical distributions of pixelwise OOD scores \(\{G_{i}\}\) appear to be bimodal and its two peaks usually indicate the modes for inlier and outlier pixels (cf. Figure 1). We therefore fit a two-component Mixture of Gaussian distribution in which the components with lower mean indicates the inliers and the one with higher mean corresponds to the outliers. Given the parameters of the Gaussian components, we now fit a sample-dependent Platt scaling function to estimate the outlier probability as follows,

\[P_{}(Z_{i}^{o}=1|x)=((G_{i}-a(x))/b(x)),\] (7)

where we set the calibration parameter \(a(x)\) as the value achieving equal probability under two Gaussian distributions, i.e., \(a(x)=\{a:_{1}N(a|_{1},_{1})=_{2}N(a|_{2},_{2})\}\) where \(_{1},_{2},_{1},_{2},_{1},_{2}\) are the parameters of the GMM model, and \(b(x)\) as the standard derivation of the sample OOD scores. We note that while it is possible to analytically compute the outlier probability based on the estimated GMM, we find the above approximation works well in practice and tends to be less sensitive to the noisy estimation.

Entropy minimizationAfter plugging in the estimated \(\), we can re-write our learning objective in Eqn (4) as the following form:

\[_{}(x) =-_{i}(_{c=1}^{C}_{c,i}(_{c,i})+ _{C+1,i}(_{C+1,i}))\] (8) \[=-_{i}(_{c=1}^{C}F_{c,i}(1-_{i})(F_{c,i} (1-_{i}))+_{i}_{i}),\] (9)

where \(_{i}=P_{}(Z_{i}^{o}=1|x)\) denotes the calibrated outlier probabilities. Direct optimizing this loss function, however, turns out to be challenging due to the potential noisy estimation of the outlier probabilities. To mitigate the impact of pixels with unreliable estimates, we select a subset of pixels with high confidence in outlier estimation and adopt a pseudo-labeling strategy to optimize the loss function in an iterative manner. Concretely, in each iteration, we first compute the calibration parameters in Eqn (7) and choose a subregion \(D\) based on thresholding the outlier probabilities. Given \(a(x),b(x)\) and \(D\), we then use the binarized outlier probabilities as the pseudo labels for the loss minimization as follows:

\[_{}(x)-_{i D}(_{c=1}^{C}F_{c,i}(1-t_{ i})(F_{c,i}(1-_{i}))+ t_{i}_{i})\] (10)

\[D=\{i:_{i}<_{1}_{i}>_{2}\}; t_{i}=0 _{i}<_{1}+1_{i}>_ {2},\] (11)

where \(_{1},_{2}\) are the threshold parameters. In addition, we set \(=_{i} t_{i}=0/_{i} t_{i}=1\) in an image-dependent manner to automatically determine the class weight. Following the common practice in test-time adaptation [46; 7; 31; 47], we only update a subset of network parameters to avoid over-fitting. In this work, we choose to update the final classification block, which brings the benefit of faster inference after each update iteration.

## 4 Experiments

We evaluate our method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, based on FS Static , FS Lost&Found , RoadAnomaly  and SMIYC . Below, we first introduce dataset information in Sec. 4.1 and experiment setup in Sec. 4.2. Then we present our experimental results in Sec. 4.3, 4.4, 4.5.

### Datasets

Following the literature [44; 10], we use the Cityscapes dataset  for training and perform OOD detection tests on several different test sets, all of which include novel classes beyond the original Cityscapes labels. We note that these datasets may exhibit varying degrees of domain shift due to their source of construction. In the following, we provide a detailed overview of each dataset.

**The Road Anomaly dataset ** comprises real-world road anomalies observed from vehicles. Sourced from the Internet, the dataset consists of 60 images exhibiting unexpected elements such as animals, rocks, cones, and obstacles on the road. Given the wide range of driving circumstances it encapsulates, including diverse scales of anomalous objects and adverse road conditions, this dataset presents a considerable challenge and potential for domain shift.

**The Fishsycapes benchmark ** encompasses two datasets: Fishsycapes Lost & Found (FS L&F) and Fishsycapes Static (FS Static). FS L&F comprises urban images featuring 37 types of unexpected road obstacles and shares the same setup as Cityscapes , thereby rendering the domain shift in this dataset relatively minimal. On the other hand, FS Static is constructed based on the Cityscapes validation set  with anomalous objects extracted from PASCAL VOC  integrated using blending techniques. Consequently, this dataset exhibits no domain shift. For both datasets, we first employ their public validation sets, which consist of 100 images for FS L&F and 30 images for FS Static. Then, we test our method on the online test dataset.

**The FS Static -C dataset** is employed to investigate the impact of domain shift on existing OOD detection methods. We modify the original public Fishsycapes Static dataset  by introducing random smog, color shifting, and Gaussian blur , mirroring the domain shift conditions .

**The SMIYC benchmark ** consists of two datasets, both encompassing a variety of domain shifts. The RoadAnomaly21 dataset contains 100 web images and serves as an extension to the original RoadAnomaly dataset , representing a broad array of environments. On the other hand, the RoadObstacle21 dataset specifically focuses on obstacles in the road and comprises 372 images, incorporating variations in road surfaces, lighting, and weather conditions.

### Experiment Setup

**Baselines:** We compare our method with several dense OOD detection algorithms [15; 17; 24; 26; 30; 4; 10; 14; 44] and test-time adaptation algorithms [36; 46]. To evaluate its generalize ability, we implement our method across different OOD detection backbones, including Max Logit , Energy  and PEBAL . This allows us to examine its performance with varying capacities of OOD detection baselines and different OOD function forms.

**Performance Measure:** Following [4; 10; 14; 44], we employ three metrics for evaluation: Area Under the Receiver Operating Characteristics curve (AUROC), Average Precision (AP), and the False Positive Rate at a True Positive Rate of 95% (FPR95).

**Implementation Details** For a fair comparison, we follow previous work [1; 4; 44] to use DeepLabv3+  with WideResNet38 trained by Nvidia as the backbone of our segmentation models. In our method, the confidence thresholds \(_{1}\) and \(_{2}\) are set to 0.3 and 0.6 respectively. Considering the standard practice in segmentation problem inferences, we anticipate the arrival of one image at a time (batch size = 1). We employ the Adam optimizer with a learning rate of 1e-4. For efficiency, we only conduct one iteration update for each image. The hyperparameters are selected via the FS Static -C dataset and are held constant across all other datasets. See Appendix B for other details.

    & MSP  & Entropy  & Max logit  & Energy  & Meta-OOD  & PEBAL  & + Ours & + TBN  & + Tent  \\   & 92.36 & 93.14 & 95.66 & 95.90 & 97.56 & 99.61 & **99.66** & 99.25 & 99.04 \\   & 70.85 & 71.23 & 74.13 & 74.02 & 78.34 & 67.63 & **99.21** & 98.96 & 98.93 \\   & 10.09 & 26.77 & 38.64 & 41.68 & 72.91 & 92.08 & **93.61** & 86.51 & 82.38 \\  & 10.52 & 14.32 & 23.60 & 22.36 & 52.31 & 57.02 & **87.14** & 81.97 & 81.42 \\  _{0.51}\)} & 23.99 & 23.31 & 18.26 & 17.78 & 13.57 & 1.52 & **1.15** & 2.33 & 4.09 \\  & 100.00 & 100.00 & 89.94 & 89.94 & 100.0 & 97.17 & **2.94** & 4.26 & 4.43 \\   

Table 1: We benchmark OOD detection methods on the corrupted FS Static dataset (gray rows) and compare with the results on the original dataset (white rows). Our ATTA method improves the modelâ€™s robustness against corruption when combined with PEBAL.

### Results on simulated FS Static -C Dataset

To investigate the performance of existing OOD detection method in wild, we benchmark several existing dense OOD detection methods [17; 24; 15; 30; 4; 44]. Then we integrate our method with the previous state-of-the-art model on the FS Static dataset, and conduct comparison to test-time adaptation techniques [36; 46].

As shown in Table 1, the introduction of corruption notably affects the performance of OOD detection methods, with an average decrease of 30% in AUC and an average increase of 70% in FPR95. This suggests that current OOD detection methods are highly susceptible to domain shifts. Notably, when combined with our test-time adaptation method, the performance of the state-of-the-art OOD detection method, PEBAL , remains more stable in the face of domain shifts, demonstrating a marginal 0.4% drop in AUC. In addition, we note that traditional TTA methods (TBN , Tent ) can result in performance degradation in the original FS Static dataset where no domain shift occurs (cf. rows in white). By contrast, our method consistently enhances the performance of OOD detection, demonstrating it greater adaptability in real-world scenarios where domain prior information is uncertain.

We present additional results on this dataset in Appendix C, including the results of combining our method with other OOD detection approaches, seen class prediction performance, and experiments under isolated domain shifts.

   &  &  &  &  \\  & &  &  &  &  \(\) & AUC \(\) & AP \(\) & FPR95 \(\) & AUC \(\) & AP \(\) & FPR95 \(\) \\  MSP  & âœ— & 67.53 & 15.72 & 71.38 & 89.29 & 4.59 & 40.59 & 92.36 & 19.09 & 23.99 \\ Entropy  & âœ— & 68.80 & 16.97 & 71.10 & 90.82 & 10.36 & 40.34 & 93.14 & 26.77 & 23.31 \\ Mahalanobis  & âœ— & 62.85 & 14.37 & 81.09 & 96.75 & 56.57 & 11.24 & 96.76 & 27.37 & 11.7 \\ Meta-OoD  & âœ“ & - & - & - & 93.06 & 41.31 & 37.69 & 97.56 & 72.91 & 13.57 \\ Synboost  & âœ“ & 81.91 & 38.21 & 64.75 & 96.21 & 60.58 & 31.02 & 95.87 & 66.44 & 25.59 \\ Denselybird  & âœ“ & - & - & - & 99.01 & 69.79 & 5.09 & 99.07 & 76.23 & 4.17 \\  Max Logit  & âœ— & 72.78 & 18.98 & 70.48 & 93.41 & 14.59 & 42.21 & 95.66 & 38.64 & 18.26 \\ + ATTA (Ours) & - & **76.60** & **23.96** & **63.49** & **93.53** & **17.39** & **40.69** & 95.48 & **41.23** & 20.89 \\  Energy  & âœ— & 73.35 & 19.54 & 70.17 & 93.72 & 16.05 & 41.78 & 95.90 & 41.68 & 17.78 \\ + ATTA (Ours) & - & **77.41** & **25.27** & **62.57** & 93.30 & **17.47** & 43.32 & **96.0** & **41.84** & **17.63** \\  PEBAL  & âœ“ & 87.63 & 45.10 & 44.58 & 98.96 & 58.81 & 4.76 & 99.61 & 92.08 & 1.52 \\ + ATTA (Ours) & - & **92.11** & **59.05** & **33.59** & **99.05** & **65.58** & **4.48** & **99.66** & **93.61** & **1.15** \\  

Table 2: We compare our method on the OOD detection benchmarks: Road Anomaly dataset, Fishscapes Lost & Found dataset, and Fishspaces Static dataset. Our method consistently improve upon several OOD detection methods, with particularly significant improvements observed on the Road Anomaly dataset where domain shift is prominent.

Figure 3: We present qualitative results on the Road Anomaly, FS Lost & Found, and FS Static datasets, where our method improves the previous state-of-the-art model, PEBAL , by effectively reducing domain shift and enhancing out-of-distribution detection. This improvement is particularly pronounced in the Road Anomaly dataset, which typically presents a higher domain shift compared to the Cityscapes training set.

### Results on existing dense OOD detection benchmarks

We then evaluate the performance of our method in existing dense OOD detection benchmarks and integrate it with several OOD detection methods. These include state-of-the-art techniques that do not require retraining or additional OOD data, such as Max Logit  and Energy , as well as methods that do, such as PEBAL . As shown in Table 2, our ATTA method consistently improve upon previous state-of-the-art models, with particularly notable enhancement observed within the Road Anomaly dataset where the domain shift is more pronounced. Specifically, we enhance PEBAL's performance from 87% to 92% in AUC, 45% to 59% in AP, and reduce the FPR95 error from 44% to 33%. When compared to other methods in the benchmarks, our method in conjunction with PEBAL attains new state-of-the-art performance across all three datasets.

Figure 3 provides qualitative results of our method across these datasets. Our ATTA approach improve the performance of OOD detection backbones by mitigating the impact of domain-shift, and encourage the confidence of the model predictions for both inliers and outliers.

We also submit our method, in combination with PEBAL , to various online benchmarks. These include the SegmenMeIfYouCan  and the online Fishyscapes  benchmarks, where our method consistently outperforms PEBAL . For further details, please refer to Appendix D.3 and D.4.

### Ablation Study

We further evaluate the efficacy of our model components on the Road Anomaly dataset using PEBAL  as the baseline model. We begin by analyzing the effectiveness of our proposed modules, Selective Batch Normalization (SBN) and Anomaly-aware Self-Training (AST), and subsequently delve into a more detailed examination of each module's design.

As illustrated in Table 3, the individual application of either SBN or AST contributes incremental enhancements to the baseline model shown in the first row. When these two modules are combined, the performance enhancement is further amplified.

We then scrutinized the internal design of each module in Table 4. We first replace SBN with versions that only employ batch-wise statistics (row #1) or training statistics (row #2), demonstrating that our SBN, which incorporates both types of statistics, outperforms these variants. Subsequently, we evaluated the design of our self-training module, initially replacing it with a closed-world entropy  calculated only on the seen classes (row #3), and then altered our GMM-based sample-specific normalization to a naive z-score normalization (row #4). We observed that ablating each component resulted in performance degradation, which underscores the effectiveness of our original design.

We present the time and memory overhead of the proposed methods, as well as some other ablation results in Appendix E.

   Train & Batch & Entropy & Norm & AUC \(\) & AP \(\) & FPR\({}_{95}\) \\  âœ— & âœ“ & anomaly-aware & GMM & 86.29 & 48.65 & 57.03 \\ âœ“ & âœ— & anomaly-aware & GMM & 88.72 & 48.11 & 43.66 \\  âœ“ & âœ“ & seen-class only & - & 90.46 & 54.64 & 39.28 \\ âœ“ & âœ“ & anomaly-aware & z-score & 91.25 & 56.65 & 36.33 \\  âœ“ & âœ“ & anomaly-aware & GMM & **92.11** & **59.05** & **33.59** \\   

Table 4: Ablation study of our internal design of each module.

   SBN & AST & AUC \(\) & AP \(\) & FPR\({}_{95}\) \\  âœ— & âœ— & 87.63 & 45.10 & 44.58 \\ âœ— & âœ“ & 88.72 & 48.11 & 43.66 \\ âœ“ & âœ— & 90.84 & 55.81 & 37.48 \\  âœ“ & âœ“ & **92.11** & **59.05** & **33.59** \\   

Table 3: Ablation study of our two main modules: SBN and AST.

Conclusion

In this work, we propose the problem of dense OOD detection under domain shift, revealing the limitations of existing dense OOD detection methods in wild. To address the problem, we introduce a dual-level OOD detection framework to handle domain shift and semantic shift jointly. Based on our framework, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, demonstrating significant performance gains on various OOD segmentation benchmarks, especially those with notable domain shifts.

Despite these promising results, there is still room for further refinement in our ATTA method. Similar to other self-training methods, ATTA relies on a well-initialized model. Our experiments show that our adaptation method yields strong results on state-of-the-art models, but the improvement is less noticeable with weaker backbones. Another practical consideration is the additional time needed for test-time adaptation methods. This is an area where future research could make substantial strides, potentially developing more efficient algorithms that could streamline the process and reduce inference time. All in all, our study introduces a more realistic setting of dense OOD detection under domain shift, and we hope it may offer some guidance or insights for future work in this field.