# Perceptual Fairness in Image Restoration

Guy Ohayon

Faculty of Computer Science

Technion-Israel Institute of Technology

ohayonguy@cs.technion.ac.il

Michael Elad

Faculty of Computer Science

Technion-Israel Institute of Technology

elad@cs.technion.ac.il

&Tomer Michaeli

Faculty of Electrical and Computer Engineering

Technion-Israel Institute of Technology

tomer.m@ee.technion.ac.il

###### Abstract

Fairness in image restoration tasks is the desire to treat different sub-groups of images equally well. Existing definitions of fairness in image restoration are highly restrictive. They consider a reconstruction to be a correct outcome for a group (_e.g._, women) _only_ if it falls within the group's set of ground truth images (_e.g._, natural images of women); otherwise, it is considered _entirely_ incorrect. Consequently, such definitions are prone to controversy, as errors in image restoration can manifest in various ways. In this work we offer an alternative approach towards fairness in image restoration, by considering the _Group Perceptual Index_ (GPI), which we define as the statistical distance between the distribution of the group's ground truth images and the distribution of their reconstructions. We assess the fairness of an algorithm by comparing the GPI of different groups, and say that it achieves perfect _Perceptual Fairness_ (PF) if the GPIs of all groups are identical. We motivate and theoretically study our new notion of fairness, draw its connection to previous ones, and demonstrate its utility on state-of-the-art face image restoration algorithms.

## 1 Introduction

Tremendous efforts have been dedicated to understanding, formalizing, and mitigating fairness issues in various tasks, including classification [17; 22; 29; 81; 94; 95], regression [2; 7; 8; 12; 43; 65], clustering [4; 5; 6; 13; 70; 73], recommendation [25; 26; 46; 52; 92], and generative modeling [15; 24; 44; 66; 74; 75; 96]. Fairness definitions remain largely controversial, yet broadly speaking, they typically advocate for independence (or conditional independence) between sensitive attributes (ethnicity, gender, _etc._) and the predictions of an algorithm. In classification tasks, for instance, the input data carries sensitive attributes, which are often required to be statistically independent of the predictions (_e.g._, deciding whether to grant a loan should not be influenced by the applicant's gender). Similarly, in text-to-image generation, fairness often advocates for statistical independence between the sensitive attributes of the generated images and the text instruction used . For instance, the prompt "An image of a firefighter" should result in images featuring people of various genders, ethnicities, _etc_.

While fairness is commonly associated with the desire to _eliminate_ the dependencies between sensitive attributes and the predictions, fairness in image restoration tasks (_e.g._, denoising, super-resolution) has a fundamentally different meaning. In image restoration, _both_ the input and the output carry sensitive attributes, and the goal is to _preserve_ the attributes of different groups equally well . But what exactly constitutes such a preservation of sensitive attributes? Let us denote by \(x\), \(y\), and \(\) the unobserved source image, its degraded version (_e.g._, noisy, blurry), and the reconstruction offrom \(y\), respectively. Additionally, let \(_{a}\) denote the set of images \(x\) carrying the sensitive attributes \(a\). Jalal et al.  deem the reconstruction of any \(x_{a}\) as correct only if \(_{a}\). This allows practitioners to evaluate fairness in an intuitive way, by classifying the reconstructed images produced for different groups. For instance, regarding \(x\), \(y\), and \(\) as realizations of random vectors \(X\), \(Y\), and \(\), respectively, Representation Demographic Parity (RDP) states that \((_{a}|X_{a})\) should be the same for all \(a\), and Proportional Representation (PR) states that \((_{a})=(X_{a})\) should hold for every \(a\). However, the idea that a reconstructed image \(\) can either be an _entirely correct_ output (\(_{a}\)) or an _entirely incorrect_ output (\(_{a}\)) is highly limiting, as errors in image restoration can manifest in many different ways. Indeed, what if one algorithm always produces blank images given inputs from a specific group, and another algorithm produces images that are "almost" in \(_{a}\) for such inputs (_e.g._, each output is only close to some image in \(_{a}\))? Should both algorithms be considered equally (and completely) erroneous for that group? Furthermore, quantities of the form \((_{a}|)\) completely neglect the _distribution_ of the images within \(_{a}\). For example, assuming the groups are women and non-women, an algorithm that always outputs the same image of a woman when the source image is a woman, but produces diverse non-women images when the source is not a woman, still satisfies RDP. Does this algorithm truly treat women fairly?

To address these controversies, we propose to examine how the restoration method affects the _distribution_ of each group of interest (_e.g._, the distribution of images of women or non-women). Specifically, we define the _Group Perceptual Index_ (GPI) to be the statistical distance (_e.g._, Wasserstein) between the distribution of the group's ground truth images and the distribution of their reconstructions. We then associate _Perceptual Fairness_ (PF) with the degree to which the GPIs of the different groups are close to one another. In other words, the PF of an algorithm corresponds to the parity among the GPIs of the groups of interest (see Figure 1 for intuition). The rationale behind using such an index is two-fold. First, it solves the aforementioned controversies. For example, an algorithm that always outputs the same image of a woman when the source image is a woman, and diverse non-women images otherwise, would achieve poor GPI for women and good GPI for non-women, thus resulting in poor PF. Second, the GPI reflects the ability of humans to distinguish between samples of a group's ground truth images and samples of the reconstructions obtained from the degraded images of that group . Thus, achieving good PF (_i.e._, parity in the GPIs) suggests that this ability is the same for all groups.

This paper is structured as follows. In Section 2 we formulate the image restoration task and present the mathematical notations necessary for this paper. This includes a review of prior fairness definitions in image restoration, alongside our proposed definition. We also discuss why PF can be considered as a generalization of RDP. In Section 3 we present our theoretical findings. For instance, we prove that

Figure 1: Illustrative example of the proposed notion of Perceptual Fairness (PF). This figure presents four possible restoration algorithms exhibiting different behaviors and fairness performance. In this example, the sensitive attribute \(A\) takes the values \(0\) or \(1\) with probabilities \(P(A=0)<P(A=1)\). The distributions \(p_{X}\) and \(p_{Y}\) correspond to the ground truth signals (_e.g._, natural images) and their degraded measurements (_e.g._, noisy images), respectively. The distribution \(p_{X|A}(|a)\) corresponds to the ground truth signals associated with the attribute value \(a\), and \(p_{Y|A}(|a)\) is the distribution of their degraded measurements. The distribution of all reconstructions is denoted by \(p_{}\), and \(p_{|A}(|a)\) is the distribution of the reconstructions associated with attribute value \(a\). The Group Perceptual Index (GPI) of the group associated with \(a\) is defined as the statistical distance between \(p_{|A}(|a)\) and \(p_{X|A}(|a)\), and good PF is achieved when the GPIs of all groups are (roughly) similar. For example, \(_{1}\) achieves good PF since the GPIs of both \(a=0\) and \(a=1\) are roughly equal, while \(_{3}\) achieves poor PF since the GPI of \(a=0\) is worse (larger) than that of \(a=1\). See Section 2 for more details.

achieving perfect GPI for all groups simultaneously is not feasible when the degradation is sufficiently severe. We also establish an interesting (and perhaps counter-intuitive) relationship between the GPI of different groups for algorithms attaining a perfect Perceptual Index (PI) , and show that PF and the PI are often at odds with each other. In Section 4 we demonstrate the practical advantages of PF over RDP. In particular, we show that PF detects bias in cases where RDP fails to do so. Lastly, in Section 5 we discuss the limitations of this work and propose ideas for the future.

## 2 Problem formulation and preliminaries

We adopt the Bayesian perspective of inverse problems, where an image \(x\) is regarded as a realization of a random vector \(X\) with probability density function \(p_{X}\). Consequently, an input \(y\) is a realization of a random vector \(Y\) (_e.g._, a noisy version of \(X\)), which is related to \(X\) via the conditional probability density function \(p_{Y|X}\). The task of an estimator \(\) (in this paper, an image restoration algorithm) is to estimate \(X\)_only_ from \(Y\), such that \(X Y\) is a Markov chain (\(X\) and \(\) are statistically independent given \(Y\)). Given an input \(y\), the estimator \(\) generates outputs according to the conditional density \(p_{|Y}(|y)\).

Figure 2: Examining fairness in face image super-resolution techniques through the lens of RDP  or PF (our proposed notion of fairness). Both RDP and PF assess how well an algorithm treats different fairness groups. Specifically, RDP evaluates the parity in the GP of different groups (higher GP is better), and PF evaluates the parity in the GPI of different groups (lower GPI is better). The results show that the groups old&Asian and old&non-Asian attain similar treatment according to RDP (similar GP scores that are roughly zero), while the latter group attains better treatment according to PF. In Section 4 and Appendix G.7, we show why this outcome of PF is the desired one.

### Perceptual index

A common way to evaluate the quality of images produced by an image restoration algorithm is to assess the ability of humans to distinguish between samples of ground truth images and samples of the algorithm's outputs. This is typically done by conducting experiments where human observers vote on whether the generated images are real or fake [18; 20; 28; 32; 33; 72; 101; 102]. Importantly, this ability can be quantified by the _Perceptual Index_, which is the statistical distance between the distribution of the source images and the distribution of the reconstructed ones,

\[_{d} d(p_{X},p_{}),\] (1)

where \(d(,)\) is some divergence between distributions (Kullback-Leibler divergence, total variation distance, Wasserstein distance, _etc._).

### Fairness

#### 2.2.1 Previous notions of fairness

Jalal et al.  introduced three pioneering notions of fairness for image restoration algorithms: Representation Demographic Parity (RDP), Proportional Representation (PR), and Conditional Proportional Representation (CPR). Formally, given a collection of sets of images \(\{_{a_{i}}\}_{i=1}^{k}\), where \(a_{i}\) is a vector of sensitive attributes and each \(_{a_{i}}\) represents the group carrying the sensitive attributes \(a_{i}\), these notions are defined by

RDP: \[(_{a_{i}}|X_{a_{i}})= (_{a_{j}}|X_{a_{j}})i,j;\] (2) PR: \[(_{a_{i}})=(X_{a_{i}})i;\] (3) CPR: \[(_{a_{i}}|Y=y)=(X _{a_{i}}|Y=y)i,y.\] (4)

While such definitions are intuitive and practically appealing, they have several limitations. First, any reconstruction that falls even "slightly off" the set \(_{a_{i}}\) is considered an entirely wrong outcome for its corresponding group. In other words, reconstructions with minor errors are treated the same as completely wrong ones. Second, these definitions neglect the _distribution_ of the groups' images. Consequently, an algorithm can satisfy RDP, PR, CPR, _etc._, while treating some groups much worse than others in terms of the _statistics_ of the reconstructed images. For instance, consider dogs and cats as the two fairness groups. Let \(_{}\) and \(_{}\) be the sets of images of dogs and cats, respectively, and let \(x_{}_{}\) be a particular image of a dog. Furthermore, suppose that the species can be perfectly identified from any degraded measurement, _i.e._,

\[(X_{}|Y=y)=1(X _{}|Y=y)=1\] (5)

for every \(y\). Now, suppose that \(\) always produces the image \(x_{}\) from any degraded dog image, while generating diverse, high-quality cat images from any degraded cat image. Namely, for every \(y\), we have

\[1=(=x_{}|X_{})=(_{}|X_{})=(_{}|X_{}),\] (6) \[(=x_{}|Y=y)=( _{}|Y=y)=(X=_{}|Y=y),\] (7) \[(_{}|Y=y)=(X= _{}|Y=y).\] (8)

Although this algorithm satisfies RDP (Equation (6)) and CPR (Equations (7) and (8)), which entails PR , it is clearly useless for dogs. Should such an algorithm really be deemed as fair, then?

To address such controversies, we propose to represent each group by the _distribution_ of their images, and measure the representation error of a group by the extent to which an algorithm "preserves" such a distribution. This requires a more general formulation of fairness groups, which is provided next.

#### 2.2.2 Rethinking fairness groups

We denote by \(A\) (a random vector) the sensitive attributes of the degraded measurement \(Y\), so that \(p_{Y|A}(|a)\) is the distribution of degraded images associated with the attributes \(A=a\) (_e.g._, the distribution of noisy women images). Consequently, the distribution of the ground truth images that possess the sensitive attributes \(a\) is given by \(p_{X|A}(|a)\), and the distribution of their reconstructions isgiven by \(p_{|A}(|a)\). Moreover, we assume that \(A Y\) forms a Markov chain, implying that knowing \(A\) does not affect the reconstructions when \(Y\) is given. This assumption is not limiting, since image restoration algorithms are mostly designed to estimate \(X\) solely from \(Y\), without taking the sensitive attributes as an additional input. See Figure 1 for an illustrative example of the proposed formulation.

Note that such a formulation is quite general, as it does not make any assumptions regarding the nature of the image distributions, whether they have overlapping supports or not, _etc._ Our formulation also generalizes the previous notion of fairness groups, which considers only the support of \(p_{X|A}(|a)\) for every \(a\). Indeed, one can think of \(_{a}=p_{X|A}(|a)\) as the set of images corresponding to some group, and of \(\{_{a}\}_{ap_{A}}\) as the collection of all sets. Furthermore, notice that \(A\) can also be the degraded measurement itself, _i.e._\(A=Y\). In this case, \(p_{X|A}(|a)=p_{X|Y}(|a)\) is the posterior distribution of ground truth images given the measurement \(a\), and \(p_{|A}(|a)=p_{|Y}(|a)\) is the distribution of the reconstructions of the measurement \(a\). Namely, our mathematical formulation is adaptive to the granularity of fairness groups considered.

#### 2.2.3 Perceptual fairness

We define the fairness of an image restoration algorithm as its ability to equally preserve the distribution \(p_{X|A}(|a)\) across all possible values of \(a\). Formally, we measure the extent to which an algorithm \(\) preserves this distribution by the _Group Perceptual Index_, defined as

\[_{d}(a) d(p_{X|A}(|a),p_{|A}(|a)),\] (9)

where \(d(,)\) is some divergence between distributions. Then, we say that \(\) achieves perfect _Perceptual Fairness_ with respect to \(d\), or perfect \(_{d}\) in short, if

\[_{d}(a_{1})=_{d}(a_{2})\] (10)

for every \(a_{1},a_{2}p_{A}\) (see Figure 1 to gain intuition). In practice, algorithms may rarely achieve exactly perfect \(_{d}\), while the \(_{d}\) of different groups may still be roughly equal. In such cases, we say that \(\) achieves good \(_{d}\). In contrast, if there exists at least one group that attains far worse \(_{d}\) than some other group, we say that \(\) achieves poor/bad \(_{d}\). Importantly, note that achieving good \(_{d}\) does not necessarily indicate good \(_{d}\) and/or good \(_{d}\) values.

#### 2.2.4 Group Precision, Group Recall, and connection to RDP

In addition to the \(_{d}\) defined in (1), the performance of image restoration algorithms is often measured via the following complementary measures [45; 71]: (1) _Precision_, which is the probability that a sample from \(p_{}\) falls within the support of \(p_{X}\), \((p_{X})\), and (2) _Recall_, which is the probability that a sample from \(p_{X}\) falls within the support of \(p_{}\), \((Xp_{})\). Achieving low precision implies that the reconstructed images may not always appear as valid samples from \(p_{X}\). Thus, precision reflects the perceptual _quality_ of the reconstructed images. Achieving low recall implies that some portions of the support of \(p_{X}\) may never be generated as outputs by \(\). Hence, recall reflects the perceptual _variation_ of the reconstructed images.

Since here we are interested in the perceptual quality and the perceptual variation of a _group's_ reconstructions, let us define the _Group Precision_ and the _Group Recall_ by

\[(a) (_{a}|A=a),\] (11) \[(a) (X}_{a}|A=a),\] (12)

where \(_{a}=p_{X|A}(|a)\) and \(}_{a}=p_{|A}(|a)\). Hence, when adopting our formulation of fairness groups, satisfying RDP simply means that the GP values of all groups are the same. However, as in mind in previous sections, two groups with similar GP values may still differ significantly in their GR. From the following theorem, we conclude that attaining perfect \(_{d_{}}\), where \(d_{}(p,q)=|p(x)-q(x)|dx\) is the total variation distance between distributions, guarantees that _both_ the GP and the GR of all groups have a _common lower bound_. This implies that \(_{d_{}}\) can be considered as a generalization of RDP.

**Theorem 1**.: _The Group Precision and Group Recall of any restoration method satisfy_

\[(a)  1-_{d_{}}(a),\] (13) \[(a)  1-_{d_{}}(a),\] (14)

_for all \(ap_{A}\)._Although using \(d_{}(,)\) provides a straightforward relationship between \(_{d_{}}\) and RDP, other types of divergences may not necessarily indicate GP and GR so explicitly. The perceptual quality & variation of a group's reconstructions may be defined in many different ways , and the GPI implicitly entangles these two desired properties.

The mathematical notations and fairness definitions are summarized in Appendix A. To further develop our understanding of PF, the next section presents several introductory theorems.

## 3 Theoretical results

Image restoration algorithms can generally be categorized into three groups: (1) Algorithms targeting the best possible average distortion (_e.g._, good PSNR) , (2) algorithms that strive to achieve good average distortion but prioritize attaining best PI , and (3) algorithms attempting to sample from the posterior distribution \(p_{X|Y}\) of the given task at hand . In Appendix B, we demonstrate on a simple toy example that all these types of algorithms may achieve poor PF, implying that perfect PF is not a property that can be obtained trivially. Namely, even when using common reconstruction algorithms such as the Minimum Mean-Squared-Error (MMSE) estimator or the posterior sampler, one group may attain far worse GPI than another group. It is therefore tempting to ask in which scenarios there exists an algorithm capable of achieving perfect GPI for all groups simultaneously. As stated in the following theorem, this desired property is unattainable when the degradation is sufficiently severe.

**Theorem 2**.: _Suppose that \( a_{1},a_{2}p_{A}\) such that_

\[(X_{a_{1}}_{a_{2}}|A=a_{i})< (Y_{a_{1}}_{a_{2}}|A=a_{i}),\] (15)

_for both \(i=1,2\), where \(_{a_{i}}=p_{X|A}(|a_{i})\) and \(_{a_{i}}=p_{Y|A}(|a_{i})\). Then, \(_{d}(a_{1})\) and \(_{d}(a_{2})\) cannot both be equal to zero._

In words, Theorem 2 states that when the degraded images of different groups are "more overlapping" than their ground truth images, at least one group must have sub-optimal GPI. Importantly, note that perfect GPI can always be achieved for some group corresponding to \(A=a\) individually, by ignoring the input and sampling from \(p_{X|A}(|a)\). Hence, Theorem 2 implies that, for sufficiently severe degradations, one may attempt to approach zero GPI for all groups simultaneously, until the GPI of one group hinders that of another one. But what about algorithms that just attain perfect _overall_ PI? Can such algorithms also attain perfect PF? As stated in the following theorem, it turns out that these two desired properties (perfect PI and perfect PF) are often incompatible.

**Theorem 3**.: _Suppose that \(A\) takes discrete values, \(\) attains perfect PI\({}_{d}\) (\(p_{}=p_{X}\)), and \( a,a_{m}p_{A}\) such that \(_{d}(a)>0\) and \((A=a_{m})>0.5\). Then, \(\) cannot achieve perfect \(_{d_{}}\)._

In words, when there exists a majority group in the data distribution, Theorem 3 states that an algorithm with perfect PI, whose GPI is not perfect _even for only one group_, cannot achieve perfect \(_{d_{}}\). This intriguing outcome results from the following convenient relationship between the GPIs of different groups for algorithms with perfect PI.

**Theorem 4**.: _Suppose that \(A\) takes discrete values and \(\) attains perfect PI\({}_{d}\) (\(p_{}=p_{X}\)). Then,_

\[_{d_{}}(a)(A=a)}_{a^{} a }(A=a^{})_{d_{}}(a^{})\] (16)

_for every \(a\) with \((A=a)>0\)._

This theorem is, perhaps, counter-intuitive. Indeed, for algorithms with perfect PI, improving the \(_{d_{}}\) of one group can only _improve_ the \(_{d_{}}\) of other groups, and this is true _even if the groups do not overlap1_. While this may seem contradictory to Theorem 2, note that such a relationship holds until the algorithm can no longer attain perfect PI. The example in Appendix B demonstrates this theorem.

Experiments

We demonstrate the superiority of PF over RDP in detecting fairness bias in face image super-resolution. Our analysis considers various aspects, including different types of degradations, and fairness evaluations across four groups categorized by ethnicity and age. First, we show that RDP incorrectly attributes fairness in a simple scenario where fairness is clearly violated. In contrast, PF successfully detects the bias. Second, we showcase a scenario where PF uncovers potential malicious intent. Specifically, it can detect bias injected into the system via adversarial attacks, a situation again missed by RDP.

### Synthetic data sets

In the following sections we assess the fairness of leading face image restoration methods through the lens of PF and RDP. Such methods are often trained and evaluated on high-quality, aligned face image datasets like CelebA-HQ  and FFHQ , which lack ground truth labels for sensitive attributes such as ethnicity. Moreover, these datasets are prone to inherent biases, _e.g_., they contain very few images for certain demographic groups [31; 35; 69], and it is unclear whether images from different groups have similar levels of image quality and variation (prior work suggests that they might not ). To address these limitations, we leverage an image-to-image translation model that takes a text instruction as additional input. This model allows us to generate four synthetic fairness groups with high-quality, aligned face images. Specifically, we translate each image \(x\) from the CelebA-HQ  test partition into four different images representing Asian/non-Asian and young/old individuals2. We use a unique text instruction for each translation. For example, the text instruction "120 years old human, Asian, natural image, sharp, DSLR" translates \(x\) into an image of an old&Asian individual. Finally, we include each resulting image in its corresponding group data only if _all_ translations are successful according to the FairFace combined age & ethnicity classifier . This involves classifying the ethnicity and age of the translated images and ensuring that old individuals are categorized as 70+ years old, young individuals are categorized as any other age group, Asian individuals are classified as either Southeast or East Asian, and non-Asian individuals are classified as belonging to any other ethnicity group. See Appendix G.1 for more details and for the visualization of the results.

Disclaimer.Importantly, we note that the generated synthetic data sets may impose offensive biases and stereotypes. We use such data sets solely to investigate the fairness of image restoration methods and verify the practical utility of our work. We do not intend to discriminate against any identity group or cultures in any way.

### Perceptual Fairness vs. Representation Demographic Parity

We consider several image super-resolution tasks using the average-pooling down-sampling operator with scale factors \(s\{4,8,16,32\}\), and statistically independent additive white Gaussian noise of standard deviation \(_{N}\{0,0.1,0.25\}\). In Appendix I we also conduct experiments on image denoising and deblurring. The algorithms DDNM\({}^{+}\), DDRM , DPS , and PiGDM  are evaluated on all scale factors, and GFPGAN , VQFR , GPEN , DiffBIB , CodeFormer , RestoreFormer++ , and RestoreFormer  are evaluated only on the \( 4\) and \( 8\) scale factors (these algorithms produce completely wrong outputs for the other scale factors). To assess the PF of each algorithm, we compute the GPIKID of each group using the Kernel Inception Distance (KID)  and the features extracted from the last pooling layer of the FairFace combined age & ethnicity classifier . In Appendix G.4 we utilize the Frechet Inception Distance (FID)  instead of KID, and in Appendix G.5 we assess other types of group metrics such as PSNR. Additionally, we provide in Appendix G.6 an ablation study of alternative feature extractors. To assess RDP, we use the same FairFace classifier to compute the GP of each group. As done in , we approximate the GP of each group by the classification hit rate, which is the ratio between the number of the group's reconstructions that are classified as belonging to the group and the total number of the group's inputs. Qualitative and quantitative results for \(s=32,_{N}=0.0\) are presented in Figure 2.

Quantitative results for all values of \(s\) and \(_{N}=0.0\) are shown in Figure 3. Complementary details and results are provided in Appendix G.

Figure 3 shows that the group young&non-Asian receives the best overall treatment in terms of both GP and GPI\({}_{}\). This result is not surprising, since the training data sets of the evaluated algorithms (_e.g._, FFHQ) are known to be biased towards young and white demographics [50; 63]. However, while most algorithms appear to treat the groups old&Asian and old&non-Asian quite similarly in terms of GP, the GPI\({}_{}\) indicates a clear disadvantage for the former group. Indeed, by examining ethnicity and age separately using the FairFace classifier, we show in Appendix G.7 that, according to RDP, the group old&non-Asian exhibits better preservation of the ethnicity attribute compared to the group old&Asian, while the age attribute remains equally preserved for both groups. This highlights that RDP is _strongly_ dependent on the granularity of the fairness groups (as suggested in ), since slightly altering the groups' partitioning may _completely_ obscure the fact that an algorithm treats certain attributes more favorably than others. However, as our results show, this issue is alleviated when adopting GPI\({}_{}\) instead of GP. Namely, the ethnicity bias is still detected by comparing the GPI\({}_{}\) of different groups, even though the fairness groups are partitioned based on age and ethnicity combined.

### Adversarial bias detection

In Section 2.2.1 we discussed the limitations of fairness definitions such as RDP. For instance, an algorithm might satisfy RDP by always generating the same output for degraded images of a particular group, even if it produces perfect results for another. However, such an extreme scenario is not common in practice. Indeed, real-world imaging systems often involve degradations that are not too severe, and well-trained algorithms perform impressively well when applied to different groups (see, _e.g._, Figure 4b). So what practical advantage does PF have over RDP in such circumstances? Here, we demonstrate that a malicious user can manipulate the facial features (_e.g._, wrinkles) of a group's reconstructions without violating fairness according to RDP, but violating fairness according to PF. In particular, we consider only the ethnicity sensitive attribute by taking the young&Asian group as Asian, and the young&non-Asian group as non-Asian. Then, we use the RestoreFormer++ method, which roughly satisfies RDP with respect to these groups (see Figure 4a, where GP is evaluated by

Figure 3: Comparison of the GP and the GPI\({}_{}\) of different fairness groups, using various state-of-the-art face image super-resolution methods. In most experiments, GPI\({}_{}\) suggests a fairness discrepancy between the groups old&non-Asian and old&Asian, while the GP of these groups is roughly equal.

classifying ethnicity alone), and perform adversarial attacks on the inputs of each group to manipulate the outputs such that they are classified as belonging to the 70+ age category. The fact that the GP of each group is quite large implies that the malicious user can classify ethnicity quite accurately from the degraded images, and then manipulate the inputs only for the group it wishes to harm (we skip such a classification step and simply attack all of the group's inputs). Such attacks are anticipated to succeed due to the perception-robustness tradeoff [59; 60]. Complementary details of this experiment are provided in Appendix H.

In Figure 4, we present both quantitative and qualitative results demonstrating that the attacks on the non-Asian group are not detected by RDP. However, we clearly observe that these attacks are successfully identified by the \(_{}\) of each group. This again highlights that PF is less sensitive to the choice (partitioning) of fairness groups compared to RDP. Specifically, age must be considered as a sensitive attribute to detect such a bias via RDP. Yet, even then, the malicious user may still inject other types of biases. Conversely, PF does not suffer from this limitation, as any attempt to manipulate the distribution of a group's reconstructions would be reflected in the group's GPI.

## 5 Discussion

Different demographic groups can utilize an image restoration algorithm, and fairness in this context asserts whether the algorithm "treats" all groups equally well. In this paper, we introduce the notion of Perceptual Fairness (PF) to assess whether such a desired property is upheld. We delve into the theoretical foundation of PF, demonstrate its practical utility, and discuss its superiority over existing fairness definitions. Still, our work is not without limitations. First, while PF alleviates the strong dependence of RDP on the choice of fairness groups  (as demonstrated in Section 4), it still cannot guarantee fairness for any arbitrary group partitioning simultaneously (a property referred to as _obliviousness_ in ). Second, our current theorems are preliminary, requiring further research to fully understand the nature of PF. For example, the severity of the tradeoff between the GPI scores of different groups (Theorem 2) and that of the tradeoff between PF and PI (Theorem 3) remain unclear. Third, we do not address the nature of optimal estimators that achieve good or perfect PF. What is their best possible distortion (_e.g._, MSE) and best possible PI? Fourth, on the practical side, we show in Appendix G.6 that effectively evaluating PF using metrics such as KID necessitates utilizing image

Figure 4: Using adversarial attacks to inject bias into the outputs of RestoreForm++, in a setting where it (roughly) satisfies RDP. Such attacks are detected by PF but not by RDP.

features extracted from a classifier dedicated to handling the considered sensitive attributes (_e.g_., an age and ethnicity classifier). However, this is not a disadvantage compared to previous fairness notions (RDP, CPR and PR), which also require such a classifier. Lastly, while the proposed GPI may be suitable for evaluating fairness in general-content natural images, we considered only human face images due to their societal implications, namely since fairness issues are particularly critical when dealing with such images. For example, if a general-content image restoration algorithm performs better on images with complex structures than on images of clear skies, this discrepancy is unlikely to be problematic for practitioners, as long as the algorithm attains good performance overall. Moreover, previous works  evaluated fairness with respect to non-human subjects (_e.g_., dogs and cats), but these studies provide limited insights into human-related fairness issues, which often arise due to subtle differences between images (_e.g_., wrinkles). Expanding our method to other datasets remains an avenue for future work.

## 6 Societal impact

Designing fair and unbiased image restoration algorithms is critical for various AI applications and downstream tasks that rely on them, such as facial recognition, image classification, and image editing. By proposing practically useful and well-justified fairness definitions, we can detect (and mitigate) bias in these tasks, ultimately leading to fairer societal outcomes. This fosters increased trust and adoption of AI technology, contributing to a more equitable and responsible use of AI in society.