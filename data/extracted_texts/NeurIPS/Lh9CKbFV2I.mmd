# Batched Low-Rank Adaptation of Foundation Models

Yeming Wen

UT Austin

&Swarat Chaudhuri

UT Austin

###### Abstract

Low-Rank Adaptation (LoRA, Hu et al., 2021) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.

To mitigate this constraint, we introduce Fast LoRA (fLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that fLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 6 languages.

## 1 Introduction

Transformer-based foundation models have showcased remarkable performance across various natural language processing tasks, as evidenced by the successes of ChatGPT (OpenAI, 2023), GitHub Copilot (Chen et al., 2021) and Speech Recognition (Radford et al., 2022) among others. The practice of fine-tuning these models for specific domains or specialized needs, such as instruction-tuning, has become increasingly prevalent (Wang et al., 2022; Honovich et al., 2022; Taori et al., 2023; Chiang et al., 2023). This is driven by the requirements of real-world applications, which often demand models tailored to specific domains, tasks, or even individual user preferences (Ouyang et al., 2022). However, the extensive number of parameters in foundation models poses computational and memory challenges for task-specific fine-tuning.

Low-Rank Adaptation (LoRA) emerged as a solution to this challenge by incorporating trainable low-rank matrices (Hu et al., 2021) which significantly reduces the number of trainable parameters during fine-tuning. LoRA's success stems from its ability to achieve domain adaptation without retraining the entire model (Taori et al., 2023; Dettmers et al., 2023; Lee et al., 2023). However, a practical challenge arises in real-time serving scenarios. Batching is the practice of aggregating multiple data points into a single computation. It is a common technique to leverage parallel processing capabilities in GPUs, ensuring higher throughput and lower serving cost. It becomes especially crucial when serving world-wide users where many requests could flood in every second. The intrinsic design of LoRA dictates that every example within a batch shares the same adapter, which is suboptimal for real-world serving scenarios where each request may require a unique adapter.

Consider a scenario where users from various locations and professions demand different language and occupation adapters as illustrated in Fig. 1. With LoRA, the batch processing would either force all these diverse requests to share the same adapter or process them sequentially, both of which are impractical. These limitations emphasize the need for a solution that can not only utilizethe advantages of LoRA but also serve multiple adapters in parallel, catering to the diverse and simultaneous requests encountered in reality.

We posit that it is critical to develop a more flexible adaptation mechanism that is compatible with diverse real-world user queries. We introduce fast LoRA (fLoRA), a modification of LoRA, enabling individual examples in a minibatch to be associated with _distinct_ low-rank weights without compromising the _expressive power_. This modification promises the benefits of domain adaptation, as heralded by LoRA, but without the batching limitation.

Our contributions can be summarized as follows:

1. We propose fLoRA, a framework that augments LoRA by allowing each example in a minibatch to have its unique low-rank adapters, facilitating efficient batching.
2. We provided an analytical analysis describing the scenarios where fLoRA would be preferred over LoRA in practical applications. This analysis is further substantiated by the empirical evidence where fLoRA achieves a 2X throughput improvement on the state-of-the-art code LLM StarCoder 15B in the low-rank setting when diverse adapters are required for incoming examples. Additionally, fLoRA reduces the latency by half under the same low-rank setting.
3. We demonstrate that fLoRA does not sacrifice accuracy compared to LoRA on a multi-lingual code generation task across 6 programming languages.

## 2 Problem Formulation

In this section, we outline the problem tackled in this work, illustrating the constraints and objectives that drive the development of the proposed fLoRA methodology. Let \(\) denote a foundation model parameterized by \(\), with a total number of parameters \(N\). The common practice is to fine-tune this foundational model for various specific tasks.

Figure 1: This shows a pragmatic scenario where a foundation model in production receives four incoming requests, each requiring distinct adapters. fLoRA facilitates batching in such serving circumstances, provided the adapters are of low rank, thereby sustaining high throughput and low latency. Detailed discussion on vectorization is provided in ยง3.2.

### LoRA Adapters

Fine-tuning the entire model \(\) for a specific task is usually computationally expensive due to the massive parameter count. LoRA (Low-rank Adaptation, (Hu et al., 2021)) was introduced to facilitate domain-specific adaptations with a significantly reduced parameter footprint, with the hypothesis that low-rank adaptation is sufficient for fine-tuning domain specific foundation models.

Given the pre-trained weight matrix \(W_{0}^{d k}\), LoRA posits that the weight matrix of the adapted foundation model can be expressed as \(W_{0}+ W=W_{0}+BA\), where \( W\) has a low-rank structure. This matrix \( W\) is factorized into two smaller, trainable matrices: \(B^{d r}\) and \(A^{r k}\), such that \( W=BA\) where \(r\) stands for the rank. For a given input \(_{i}\), the output \(_{i}\) is given by:

\[_{i}=(_{i}| W,W_{0},)\] (1)

### Batching & throughput

Batching is a common practice where multiple data points (\(_{1},_{2},,_{m}\)) are aggregated into a single batch \(\). Consequently, the forward passes of these data points are processed concurrently rather than individually. This practice leverages the parallel processing capability of a modern GPU, thereby significantly improving the throughput \(T\), i.e., the number of data points processed per unit of time. In the context of foundation models, throughput of a batch \(\) can be defined as \(T=_{i=1}^{m}|_{i}|/ t\), where \(|_{i}|\) is the number of tokens generated for each example \(_{i}\) in the batch, \( t\) is the total time taken to process the batch, and \(m\) is the number of examples in the batch. Note that batching incurs minimal latency penalties. However, given its substantial increase in throughput, batching and its variants are widely used in the state-of-the-art foundation models serving framework such as vLLM (Kwon et al., 2023) to achieve the best balance between throughput and latency.

### Objective

Batching typically assumes the same model parameters are utilized for every input example within a minibatch. Hence, a straightforward application of batching in LoRA requires that the adapter matrix \( W\) be shared across all inputs in the batch \(\). The challenge arises when considering a scenario where each input example in the batch might originate from a different task. Sharing the same \( W\) for all \(_{i}\) in \(\) becomes suboptimal where each input potentially demands a unique adapter. The limitation is particularly acute when the model is expected to serve a world-wide user base with diverse incoming requests.

Given the limitations of LoRA in batching, our objective is to maximize the throughput \(T\) in global user serving scenarios by maintaining the batching mechanism. Formally, for each \(_{i}\), we aim to compute \(_{i}=(_{i}| W_{i},W_{0},)\), where \( W_{i}\) is the adapter matrix corresponding to the input example \(x_{i}\). Therefore, \( W_{i}\) can be unique across \(\) and specific to a domain or user preference.

## 3 FLoRA: Fast Low-Rank Adaptation

As shown in SS2.3, adapter sharing is often impractical in real-world serving scenarios. The innovation of FLoRA is the introduction of example-specific adapter \( W_{i}\) for each \(_{i}\) in a mini-batch. In FLoRA, the weight matrix \(W_{i}\) for each example \(_{i}\) in the minibatch is calculated as \(W_{i}= W_{i} W_{0}\), where \(\) denotes element-wise multiplication, \(W_{0}\) is the pre-trained weight matrix and \( W_{i}\) is a low-rank adaptation specifically designed for \(_{i}\). Similar to Hu et al. (2021), \( W_{i}\) is decomposed into two trainable matrices: \(B_{i}^{d r}\) and \(A_{i}^{r k}\), such that \( W_{i}=B_{i}A_{i}\), as shown in Fig. 1. Note that FLoRA has the same expressive power as LoRA by its construction.

### Forward Pass

The advantage of FLoRA is that computations on a minibatch can be written in terms of matrix multiplications. This enables efficient batched implementations on modern accelerators such as GPUs. Let \(x_{i}\) denote the activations in one layer of a neural net, which is a vertical vector of length \(d\). The next layer's activations are given by\[y_{i} =(W_{i}^{T}x_{i})\] (2) \[=(W_{0}^{T} W_{i}^{T})x_{i}\] (3) \[=(W_{0}^{T}(B_{i}A_{i})^{T})x_{i}\] (4) \[=A_{i}W_{0}^{T}(B_{i} x_{i}) \] (5)

When the rank is greater than one, we extend the use of the symbol "\(\)" to denote potential broadcasting. Additionally, a dimension reduction operation such as torch.mean is required prior to applying the activation function \(\).

The key to fLoRA's flexibility lies in the low rank decomposition enables the incorporation of example-specific adapters directly into the forward pass, as demonstrated in the equations above. Crucially, each of these operations--the element-wise multiplication between \(A_{i}\) and \(x_{i}\), and between \(B_{i}\) and \(y_{i}\) -- is inherently batch-friendly. Consequently, fLoRA allows for simultaneous processing of multiple requests, each requiring its own adapter, within a single minibatch. To vectorize all adapters in the minibatch, we define matrices \(\) and \(\) whose rows correspond to the adapters \(A_{i}\) and \(B_{i}\) for all examples in the minibatch. The above equation is vectorized as:

\[=()W_{0} \] (6)

### Computational Efficiency

The computational analysis primarily concentrates on the examination of fully connected layers within a transformer architecture, given that LoRA is specifically applied to these layers, such as query and key projections. To begin, we analyze a baseline that leverages batch matrix multiplication to facilitate the serving of LoRA with multiple adapters. This operation is possible under the assumption that every adapter required by the input examples in the minibatch shares the same shape, specifically, the same rank. The batch matrix multiplication (BMM) can be implemented using the torch.bmm operator in deep learning frameworks such as PyTorch (Paszke et al., 2019). Note that the BMM operator is typically unfavorable in practical settings due to the significant overhead it introduces (Abdelfattah et al., 2016). This overhead diminishes the throughput and increases latency, which is detrimental in serving scenarios where response times are crucial for maintaining a good user experience.

Let \(b\) and \(l\) denote the batch size and the maximum sequence length in the input batch \(\). Revisiting the notation introduced in SS3, where \(W_{0}^{d k}\), \(B_{i}^{d r}\) and \(A_{i}^{r k}\), the operations required to compute the pre-activation for an input batch \(\) with dimensions \([b,l,d]\) consist of one matrix multiplication and two BMMs. The matrix multiplication occurs between the input batch \(\) and the pre-trained weight \(W_{0}\). The two BMM operations are conducted firstly between the input batch \(\) and \(\), and secondly between the result of the prior computation and \(\), where \(\) and \(\) are matrices whose rows correspond to the adapters \(A_{i}\) and \(B_{i}\) for all examples in the minibatch respectively. Assuming for simplicity that the layer neither upscales nor downscales the hidden dimension (i.e. \(d=k\)), the upper bound complexity of this layer is discerned as \(2c_{1}(dblr)+c_{2}(bld^{2})\), with \(c_{1}\) and \(c_{2}\) representing the computational coefficients of BMM and matrix multiplication respectively. Note that \(c_{1}>>c_{2}\) because the BMM operator is more expensive than matrix multiplication.

For fLoRA, the cost is one matrix multiplication which is \(c_{2}(rbld^{2})\) where \(r\) denotes the rank of the adapters. We omit the cost of element-wise multiplication in this analysis because it is negligible to the matrix multiplication cost. Comparing the computational cost of fLoRA and LoRA boils down to the following inequality

\[}{dc_{2}}+ 1\] (7)

fLoRA exhibits a lower computational cost than bmm LoRA whenever the above inequality holds true. The benefit of fLoRA over LoRA is notably pronounced when \(r=1\). As the rank increases, LoRA gradually becomes less costly. From the established inequality, a variety of scenarios can be inferred where fLoRA has an advantage over LoRA. Firstly, the advantage of fLoRA is significantly apparent when the rank of adapters is small. Secondly, in configurations where the model has fewer hidden units but an increased number of layers, fLoRA tends to outperform LoRA due to the smaller value of \(d\) in the denominator of Eq.7.

Another advantage of fLoRA is the cost remains invariant to the number of adapters required by the input batch. While the preceding analysis assumes that every token in an example \(x_{i}\) shares the same adapter, it is possible to apply multiple adapters to a single example by dividing the example into chunks, and then applying different adapters to each chunk. This approach is commonly observed in the Mixture of Experts framework (Fedus et al., 2021; Lepikhin et al., 2020; Puigecerver et al., 2023). Incorporating several adapters in an input example notably amplifies the ratio \(c_{1}/c_{2}\) in Eq.7\({}^{2}\), thereby significantly increasing LoRA's cost.

The ratio \(c_{1}/c_{2}\) might not be the same across different transformer architectures. SS4.1 is designed to provide a deeper insight into how comparative serving efficiency of fLoRA and LoRA changes under various architectures. Additionally, it's worth noting that LoRA does not apply to the self-attention layers, which constitute a non-trivial portion of the computational cost, thereby overshadowing the advantage of fLoRA. However, as efficient self-attention mechanisms such as flash attention (Dao et al., 2022) get adopted, the advantage of fLoRA over LoRA is likely to get larger.

**Connection to IA3.** IA3 was proposed in Liu et al. (2022) featuring fast adaptation of LLM. It introduces a learned vector \(l\) which re-scales the activation by \(y_{i}=l(W_{0}^{T}x_{i})=l(W_{0}^{T}x_{i})\). This can be viewed as a special case of fLoRA - a rank 0.5 variant -- which only re-scales the columns instead of the entire pre-trained weights. It has a limited expressive power compared to fLoRA and LoRA.

## 4 Experiments

In this section, we compare fLoRA to LoRA and other notable baselines across various metrics and tasks. To begin with, we delve into a computational analysis to substantiate the enhanced throughput and the reduced latency achieved by fLoRA in the case of low rank. Subsequently, we analyzed the accuracy of fLoRA in multilingual code generation tasks spanning across 6 different languages.

### Serving Analysis

The primary objective of this serving analysis is to measure the maximum throughput both fLoRA and LoRA can attain under varied rank configurations. We carried out this exploration on the state-of-the-art code Large Language Model (LLM) StarCoder (Li et al., 2023), evaluating models of different number of parameters namely 1B, 3B, and 15B. The dataset facilitating this analysis has been sourced from the vLLM throughput benchmark. Notworthily, this dataset was previously used to fine-tune the English Vicuna model, a state-of-the-art chat LLMs (Chiang et al., 2023). To expedite the benchmarking process, we extracted a subset of 1,000 samples from the original dataset, ensuring a diverse range of sample lengths varying from 50 to 2,000 tokens.

In setting up the computational analysis, our primary intent is to compare fLoRA and LoRA in the real-world serving scenario. See Appendix A.2 on how bmm LoRA is implemented. The vLLM framework (Kwon et al., 2023)3, with its implementation of continuous batching, presents an ideal setup for this analysis. The continuous batching mechanism in vLLM, as inspired by the principles delineated in Orca (Yu and Jeong, 2022), facilitates a more efficient utilization of GPU resource by allowing new sequence to be inserted immediately once any sequence in the current batch is completed. This continuous flow significantly enhances GPU utilization as compared to static batching, where the GPU awaits the completion of all sequences in a batch before initiating a new batch processing. The comparison of fLoRA and LoRA within this setup offers a compelling evidence of their respective throughput and latency in the real-world serving scenario. It is worth noting that the experiments were conducted without Flash-attention (Dao et al., 2022).

**Throughput experiment.** In Fig.2, the throughput results for both fLoRA and bmm LoRA are illustrated across different rank configurations on three StarCoder models with different number of parameters, namely 1B, 3B and 15B. All experiments were conducted on an NVIDIA H100 GPU with a float16 precision. The maximum number of batched tokens is 8,192, which is the same as the model context length. Evidently, fLoRA shows a superior throughput over LoRA in the lower rank configurations. At rank 1, the throughput of fLoRA is more than threefold higher than that of LoRA, thereby highlighting the considerable serving performance boost fLoRA provides. The advantage of fLoRA continues as the rank increases, albeit with a diminishing rate. For instance, at rank 2, fLoRA's throughput is around 2.5 times higher, and this multiplicative advantage decreases as the rank increases further. This performance advantage continues up until the rank of 8 in the StarCoder 15B model, where LoRA starts to outperform fLoRA. This inflection point suggests that the advantages of fLoRA in terms of throughput are more pronounced in lower rank.

Notice that the inflection points occurred at a higher rank when serving a smaller LLM as illustrated in (Fig. 2, **left**). This demonstrates a significant potential of fLoRA, especially when considering future applications of quantization techniques to serving LLMs. By applying quantization, such as 8-bit or 4-bit inference, the effective size of the model is reduced, akin to serving a smaller LLM thus potentially extending the rank at which fLoRA maintains a throughput advantage over LoRA.

**Latency experiment.** We assessed the latency-versus-rank performance on the StarCoder 15B model, using the same dataset as the throughput experiment. This evaluation was conducted under the conditions where requests arrive at a rate of 8 per second. The default maximum number of batched tokens in vLLM serving launcher is 2,560. The results, as shown in (Fig. 2, **right**), measure latency in terms of seconds per output token. Remarkably, in the lower rank regime (ranging from rank 1 to rank 4), fLoRA exhibited a 2-6X reduction in latency compared to LoRA. Notably, the latency for LoRA stood at approximately 2.3 seconds per output token, is impractical for serving due to the poor user experience it would cause. This experiment further highlights the superior capabilities of fLoRA in efficiently catering to diverse incoming user requests.

These findings validate the theoretical analysis in SS4.1, confirming that fLoRA provides significant throughput advantages, particularly in settings with lower to moderate ranks. This positions fLoRA as a compelling alternative for efficiently serving adapted foundation models, especially in scenarios where lower ranks suffice the desired model accuracy, as further demonstrated in the subsequent accuracy analysis sections. Moreoever, if an enterprise chooses to serve foundation models with a substantial number of diverse adapters, for instance, a personalized LLM, then a low rank or even a rank one is imperative to avoid excessive storage costs.

Figure 2: **Left**: Generation throughput vs. rank for fLoRA and torch.bmm implementation of LoRA, measured in tokens per second (token/s). The experiments were conducted on three star-coder models: StarCoder 15B, StarCoderbase 3B and StarCoderbase 1B. fLoRA has great throughput advantage over LoRA when the rank is small. As the rank increases, the torch.bmm implementation of LoRA finally has a better throughput. **Right**: Latency vs. rank on StarCoder-15B. Requests are coming at the speed of 8 requests per second.

### Multilingual Code Generation

A key aspect to examine before applying fLoRA in real-world LLM serving is to scrutinize any potential degradation in performance. In this section, we consider multilingual code generation as the testbed for comparing fLoRA and LoRA due to its alignment with real-world applications, where the necessity to cater to diverse programming languages is paramount. Low-resource languages, as referred to in this context, are languages that appear much less frequently than other languages in the pre-training data. Orlanski et al. (2023) showed that the performance of code LLMs can be notably enhanced on low-resource programming languages such as Perl by recalibrating the language distribution in the pre-training data. This suggests that fine-tuning a trained LLM on such low-resource languages could potentially boost its performance on the same language. Hence, by employing multilingual code generation as a benchmark, we can make an informed evaluation of adaptability and performance enhancements that fLoRA and LoRA can achieve.

Additionally, a comparison is made against a third baseline, IA3, which can be considered as a special case of fLoRA. Essentially, IA3 can be seen as a rank 0.5 variant of fLoRA, thereby facing a constrained expressive power in comparison to both fLoRA and LoRA. For fLoRA and LoRA, we conducted fine-tuning across a range of rank choices, spanning from 1 to 8. It emerges that within the scope of this multilingual code generation task, a rank of one typically suffices to achieve optimal results, with the exception of Racket and Lua. Consequently, the results shown in SS4.2 are predominantly at rank 1, barring Racket and Lua, which are presented at rank 4.

**Fine-tuning.** In our effort to evaluate the performance of fLoRA, LoRA, and IA3 on the multilingual code generation task, we fine-tuned these models on state-of-the-art multilingual code LLMs, StarCoder 15B and StarCoderBase 3B as introduced in (Li et al., 2023). A pivotal aspect of our fine-tuning process was the utilization of existing data, negating the need for creating new data for low-resource programming languages. We leveraged the same pre-training data that was used for pre-training StarCoder, specifically, the Stack dataset, which contains over 6TB of permissively-licensed source code files covering 358 programming languages. For each low-resource language in our experiment, we fine-tuned on its corresponding split from the Stack dataset for a total of 1,500 steps, along with batch size 8. More fine-tuning details are given in Appendix A.1.

    &  \\  & Base Model & fLoRA & IA3 & LoRA \\   & & &  \\  Dlang & \(\) & 14.13 & 17.26 (22.14\%) & 15.26 (7.99\%) & 17.15 (21.37\%) \\ Perl & \(\) & 17.05 & 21.44 (25.76\%) & 17.71 (3.90\%) & 21.46 (25.76\%) \\ Ruby & \(\) & 1.39 & 24.94 (1692.86\%) & 20.80 (1394.64\%) & 23.76 (1608.04\%) \\ Rust & \(\) & 22.40 & 26.24 (17.14\%) & 23.53 (5.04\%) & 26.87 (19.95\%) \\ Racket & \(\) & 10.20 & 12.41 (21.61\%) & 11.53 (12.96\%) & 12.51 (22.58\%) \\ Swift & \(\) & 16.91 & 20.38 (20.51\%) & 18.13 (7.19\%) & 20.35 (20.36\%) \\   & &  \\  Dlang & \(\) & 5.65 & 5.72 (1.20\%) & 5.72 (1.20\%) & 6.97 (23.34\%) \\ Perl & \(\) & 10.73 & 13.01 (21.25\%) & 11.46 (6.83\%) & 13.31 (27.51\%) \\ Ruby & \(\) & 5.33 & 14.48 (171.68\%) & 7.88 (47.90\%) & 13.89 (160.68\%) \\ Rust & \(\) & 17.18 & 21.00 (22.24\%) & 17.28 (0.60\%) & 20.67 (20.31\%) \\ Racket & \(\) & 8.04 & 9.16 (13.99\%) & 8.40 (4.48\%) & 8.80 (9.48\%) \\ Swift & \(\) & 10.04 & 15.69 (56.21\%) & 12.54 (24.83\%) & 15.04 (49.76\%) \\   

Table 1: Comparison of three fine-tuning methods fLoRA, IA3, and LoRA on StarCoder 15B and StarCoderBase 3B across various low-resource programming languages in the MultiPL-E benchmark. The table presents Pass@1 accuracy of each method alongside the relative improvement over the baseline. The standard errors are less than 0.3% in all cells in the table, therefore we exclude that for clear presentation.

**Evaluation.** The evaluation of FLoRA, LoRA, and IA3 was conducted on the MultiPL-E benchmark (Cassano et al., 2023), which contains the translation of two unit-test driven Python benchmarks (HumanEval and MBPP) to 18 other programming languages. We used the HumanEval split of the benchmark to evaluate the fine-tuned models. As for the metrics, We adopted the _pass@k_ metrics from Chen et al. (2021); Austin et al. (2021), which is calculated as the fraction of problems with at least one correct sample given \(k\) samples. Similar to Chen et al. (2021), we drew 100 samples for computing _pass@_1, with a sampling temperature set at 0.1.

**Main results.** The result in SS4.2 exhibits the performance of three methods across various programming languages on both StarCoder 15B and StarCoderBase 3B models. The average relative improvement achieved by FLORA and LoRA is roughly 20% in the selected low-resource programming languages. FLoRA consistently outperforms IA3 on all languages, especially on StarCoder 15B, denoting its efficiency in leveraging the model expressive power to improve multilingual code generation. It is notable that StarCoder 15B has an unforeseen issue regarding Ruby generation, where it yields lower accuracy compared to the 3B model. However, all methods are able to fix its abnormal performance.

On StarCoderBase 3B, a smaller model, it is evident that the baseline performance drops, yet fLoRA and LoRA still manage to exhibit substantial relative improvement over the baseline, especially in languages like Swift and Ruby. This suggests that both methods benefit from continuous training on the low-resource language split of the pre-training data, although the advantages may diminish with a reduction in model size. While the absolute performance (_pass@_1 accuracy) varies among languages, the relative improvements highlight the effectiveness of the tested methods in enhancing multilingual code generation.

## 5 Related Work

Parameter-Efficient Fine-Tuning (PEFT) methods are broadly partitioned into two categories: weight-based and non-weight-based approaches. MC-dropout (Lakshminarayanan et al., 2016) stands as an early example for the non-weight-based approach, where distinct dropout masks are allocated to various tasks. Recently, prompt tuning techniques have emerged as a prevalent stream within this category (Li and Liang, 2021; Lester et al., 2021), facilitating efficient adaptation with minimal modifications to models. Successive endeavors aimed to enhance this class of methods, delving into aspects such as optimization (Mao et al., 2021; Diao et al., 2022), transferability (Wang et al., 2021; Vu et al., 2021; He et al., 2022), and the usage of discrete prompts (Schick and Schutze, 2020, 2020, 2021; Gao et al., 2021; Malkin et al., 2021), among others (Liu et al., 2022, 2021).

We focus on weight-based approaches in this work, which has a weight interpretation as exemplified by LoRA (Hu et al., 2021). This line of research can be traced back to Progressive Network (Rusu et al., 2016), which inserts a sub-network when a new task arrives. This principle was later adapted widely in foundation models as represented by adapter based methods (Houlsby et al., 2019; Mahabadi et al., 2021; Davison, 2021; Ding et al., 2022; Wang et al., 2022). In particular, BitFit (Ben-Zaken et al., 2021) was introduced to solely update the bias parameters, while IA3 (Liu et al., 2022) was proposed to rescale the activations. Additionally, approaches such Fish (Sung et al., 2021) and Diff-pruning (Guo et al., 2020) leverage sparsity to facilitate efficient adaptation of foundation models. A separate vein of research aims to improve LoRA by reducing its computational and memory costs (Zhang et al., 2023, 2020, 2023). He et al. (2022) explored how to unify different PEFT methods. Dettmers et al. (2023) quantized LoRA to reduce memory footprint. Chavan et al. (2023) generalized LoRA by learning individual adapters in each layer. Several other works focus on building mixture of adapters (Wang et al., 2022; Diao et al., 2023).

## 6 Conclusion

We introduced fLoRA, an extension of LoRA, facilitating efficient batching. Empirical evaluations demonstrated that fLoRA enhances throughput and latency in practical serving scenarios, all while preserving the accuracy of LoRA. Through fLoRA, we aim to facilitate a more efficient adaptation of large language models to diverse and real-world user requests.

**Limitations.** Despite its parameter efficiency, fLoRA still requires fine-tuning. A promising future work could be to derive fLoRA weights from a trained LoRA model, given that LoRA remainsthe most prevalent type of adapter as per (Huang et al., 2023). This adaptation could potentially obviate the requirement for fine-tuning, thereby accelerating the process of model adaptation.