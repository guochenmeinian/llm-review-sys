# Gated Slot Attention for Efficient Linear-Time Sequence Modeling

Yu Zhang\({}^{1}\) Songlin Yang\({}^{2}\)1 Ruijie Zhu\({}^{3}\) Yue Zhang\({}^{1}\) Leyang Cui\({}^{4}\)

Yiqiao Wang\({}^{5}\) Bolun Wang\({}^{5}\) Freda Shi\({}^{6}\) Bailin Wang\({}^{2}\)

Wei Bi\({}^{4}\) Peng Zhou\({}^{5}\)\({}^{}\) Guohong Fu\({}^{1}\)\({}^{}\)

\({}^{1}\)School of Computer Science and Technology, Soochow University, China

\({}^{2}\)Massachusetts Institute of Technology \({}^{3}\)University of California, Santa Cruz

\({}^{4}\)Tencent AI Lab \({}^{5}\)LuxiTech \({}^{6}\)University of Waterloo

yzhang.cs@outlook.com yangsl66@mit.edu

###### Abstract

Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC ) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA ). Essentially, GSA comprises a two-layer GLA linked via \(\), utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the \(\) operation is particularly beneficial in "finetuning pretrained Transformers to RNNs" (T2R ) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.

## 1 Introduction

Transformers  have emerged as the predominant architecture for _most, if not all_, sequence modeling tasks. Nevertheless, the quadratic complexity of \(\)-based standard attention (SA) poses significant challenges for long sequence modeling (e.g., video understanding and biological sequence modeling). In the context of language modeling, where sequence lengths are moderate, training efficiency is generally not a primary concern. However, during inference, the Key-Value (KV) cache  grows linearly with the generation length, resulting in substantial memory burdens and throughput bottlenecks due to high I/O costs.

Linear (kernelized) attention  and its gated variants  have received interest as promising alternatives to \(\) attention. These models demonstrate strong performance in language modeling and understanding tasks. Notably, they can be reframed as RNNs during inference, achieving constant memory complexity and thereby significantly enhancing inference efficiency.

However, two key issues persist with these models: (i) Performance-wise, recent research indicates that linear recurrent models still struggle with tasks requiring in-context retrieval or learning , and there is a _fundamental_ recall-memory trade-off [3; 91] where all inference-time-constant-memory models face inherent limitations. (ii) In terms of training efficiency, while linear attention supports hardware-efficient chunkwise training  as implemented in FlashLinearAttention (FLA ), training from scratch on trillions of tokens remains prohibitively expensive. A paradigm, _"finetuning pretrained Transformers to RNNs"_ (short for T2R ), has recently gained great attention [101; 10; 54; 13; 7; 90]. This approach circumvents the high cost of training from scratch by requiring only a few billion tokens for finetuning--about 1-\(3\%\) of the total cost. However, linear attention uses a different kernel method from \(\), leading to performance discrepancies when finetuning pretrained \(\) attention models to linear attention .

To address these issues, we revisit the Attention with Bounded-Memory Control (ABC) model , which retains the \(\) operation, thereby reducing training-finetuning discrepancies between standard and linear attention, making it ideal for T2R settings. Additionally, ABC enables more effective state utilization, requiring less state size to achieve similar performance, as observed in Peng et al. . This results in more efficient inference and potentially expands the Pareto frontier of the recall-memory tradeoff . However, ABC has not gained significant attention due to its mediocre language modeling performance and slow training speed.

In this work, we first reformulate ABC as two-pass linear attention linked via \(\), allowing us to leverage the hardware-efficient chunkwise implementation from FLA  for more efficient training. We then identify several limitations of ABC and propose a new model, dubbed Gated Slot Attention (GSA), which is essentially a gated version of ABC, following the recent trend of enhancing linear attention with gating mechanisms [96; 69; 61; 16; 62; 52; 65].

Our extensive evaluation shows that GSA not only matches performance in language modeling and understanding tasks but also significantly outperforms other linear models in _in-context recall-intensive_ tasks [3; 4], without requiring a large state size like RetNet  or GLA . In the T2R finetuning setting, we found that finetuning Mistral-7B  to GSA surpasses large recurrent language models (e.g., RWKV6-7B, Mambo-7B) and also outperforms finetuning Mistral-7B to other linear models (e.g., RetNet, GLA) and other T2R methods like SUPRA , verifying the importance of retaining the \(\) operator. Finally, we remark that GSA achieves similar training speeds to GLA while offering an inference speedup due to its smaller state size.

## 2 Background and Preliminary

### Transformers as Unbounded Key-Value Memories

Given \(=[_{1},,_{T}]^{}^{T  d}\), where \(T\) is the sequence length and \(_{i}^{d}\) is the \(i\)-th input vector with \(d\) dimensions, SA with causal masking computes the output matrix:

\[=f((^{})),\] (1)

where \(,,^{T d}\) are linear mappings of the input \(\) via learnable weights \(_{q},_{k},_{v}^{d d}\), \(=\{M_{ij}=1i j-\}\) is the causal mask to prevent future information leakage, \(\) denotes element-wise production, and \(f()\) is \(()\).

Generally, \(\), \(\) can be viewed as neural _key-value memories_\(}_{t},}_{t}^{m d}\), respectively [81; 24], where \(m\) is the number of memory slots. At step \(t\), the query \(_{t}=_{q}_{t}^{d}\) first attends to the key memories \(}_{t}\) to retrieve relevant information, which is then summarized into \(_{t}\) by computing a weighted sum of the value memories \(}_{t}\), where the weights are the normalized attention scores:

\[_{t}=}_{t}^{}f(}_{t}_{t}).\] (2)

From this perspective, Transformers are equipped with an unbounded number of memory slots, which grow linearly with respect to the sequence length  (i.e., \(m=t\) for step \(t\))--a new key \(_{t}=}_{k}_{t}^{d}\) is assigned with a unique memory slot upon its introduction. This leads to a simple memory updating rule: \(}_{t}=}_{t-1}\{_{t}\}\). The value memories \(}_{t}\) are updated in a similar way. This mechanism, however, comes at the cost of quadratic time complexity in terms of the sequence length for training and \(O(Td)\) time/memory complexity for inference , posing challenges for large-scale models.

### ABC : Linearizing Attention with Bounded Memory Control

From a key-value memory perspective, the training and inference complexity of self-attention (SA) can be reduced by fixing the number of memory slots to a constant size \(m T\)[27; 51; 63]. One straightforward way to achieve this is by employing a _first-in-first-out_ memory management strategy, commonly known as sliding window attention (SWA). However, SWA is inefficient because it discards all information outside the window, leading to poor performance in balancing the recall-memory tradeoff . To achieve acceptable performance, SWA often requires a large window size (e.g., 4,096 tokens in Mistral ), which diminishes its advantage over to global attention.

When the number of tokens in a sequence exceeds the number of memory slots, it becomes necessary to store information from multiple tokens in a single slot. To address this challenge, Peng et al.  propose the Attention-with-Bounded-memory-Control (ABC) mechanism, which allows multiple tokens to be written into a single slot:

\[}_{t}=}_{t-1}+_{t }_{t}^{m d},}_{t}=}_{t-1}+_{t} _{t}^{m d},_{t}=}^{T}f(}_{t}^{T}_{t})^{d}\] (3)

where

\[_{i}=_{}_{i})} ^{m},_{i}=_{i}}{_{j=1}^{i} _{j}}(0,1)^{m}\] (4)

Here, \((_{i})_{j}\) represents the writing intensity of the \(i\)th token to the \(j\)th slot, obtained using a cumulative \(\) function (cf. [63, footnote 5]), which can be computed with a prefix sum.

ABC as two-pass linear attention.The outer-product-based additive memory update rule in Eq. 3 bears a resemblance to linear attention , which involves the following recurrence3:

\[_{t}=_{t-1}+_{t}_{t} ^{d d},_{t}=_{t}^{T} _{t}^{d}\] (5)

We denote this linear attention operator that computes \(_{i}\) from \(_{i},_{i}\) and \(_{i}\) (Eq. 5) by \(\{_{i}\}_{i=1}^{T}=(\{_{i},_{i},_{i}\}_{i=1}^{T})\). We show that the ABC operations can be written as

\[\{^{}_{i}\}_{i=1}^{T} =(\{_{i},_{i},_{i}\}_{i=1}^{T}),\] \[\{_{i}\}_{i=1}^{T} =(\{(^{}_{i}), _{i},_{i}\}_{i=1}^{T}),\]

where \(^{}_{i}^{m},_{i} ^{d}\). Therefore, ABC can enjoy hardware-efficient linear-time chunkwise training , as implemented in the FLA library .

Remarks on state size.Peng et al.  empirically demonstrated that ABC requires a smaller state size to achieve comparable performance to other linear attention models, resulting in improved inference efficiency. We offer the following intuitive explanation: the new query \(^{}\) aggregates the entire history through the initial pass of linear attention, making it more context-aware and better at locating desired items for retrieval. The subsequent \(\) operator helps mitigate the attention dilution issue . From the perspective of Hopfield networks, softmax can exponentially increase the memory size . Together, these factors suggest that ABC may possess an implicit large memory capacity, even with a small actual recurrent state size.

### GLA : Linear Attention with Gating Mechanism

Linear attentions underperform \(\)-attention Transformers in language modeling by a notable margin. RetNet  and TransnormerLLM  incorporate a _data-independent_ exponential decay factor for memory update as

\[_{t}=_{t-1}+_{t} _{t}^{d d},\]

where \((0,1)\) is a scalar data-independent decaying factor; that is, the decay rate is fixed across time steps and hidden channels (under the same head), disrespect to the input tokens. RetNet has shown better language modeling performance compared to vanilla linear attentions thanks to the decaying mechanism.

However, research in recurrent neural networks (RNNs) has shown that _data-dependent_ decay (or forget gates) is crucial for selectively retaining and forgetting information [22; 26], thus better leveraging the fixed recurrent hidden state. This selective mechanism has been revisited in recent state-space models [29; 16]. Inspired by LSTMs, Gated Linear Attention (GLA) [52; 96] introduces data-dependent decay parameters \(_{t}(0,1)^{d d}\) to gate the hidden state as follows,

\[_{t}=_{t}_{t-1}+_{t}_{t }^{d d},_{t}=_{t}^{T}_{t} ^{d}.\]

 show that if gates are parameterized in an outer product form \(_{t}=_{t}_{i}\), and \(_{t},_{t}^{d}\) depend solely on input \(_{t}\), such recurrence can be rewritten as matrix multiplication, allowing for hardware-efficient training with a chunkwise parallel form. In what follows, we will use the following notation \((\{_{i},_{i},_{i},_{i},_{i }\}_{i=1}^{T})=\{_{i}\}_{i=1}^{T}\) to denote this computation. It is common to set \(_{i}=\) as in [96; 69; 61], which is also often written in the following equivalent form:

\[_{t}=(_{t})_{t-1}+_{t} _{t}.\]

Here \(_{t}\) can be viewed as the input gate, and \(_{t}\) can be viewed as the forget gate. In gated RNN literature, it is common to couple these two gates via \(_{t}=1-_{t}\)[12; 106; 67]. In particular, Qin et al.  proposed HGRN2, which uses this strategy as an improved parameterization of GLA, showing better performance in language modeling.

## 3 Method

### Motivation: Issues with ABC

We identify two primary limitations in ABC's memory update rule. Firstly, it lacks a forgetting mechanism, resulting in indefinite retention of items once written into memory slots. This prevents efficient memory reuse by impeding the prompt clearance of slots for new information.

Secondly, the rule introduces an unwarranted inductive bias favoring tokens at the sentence's beginning. This contradicts the recency bias in natural language, where more recent information is often more relevant. Prioritizing initial tokens over the recent ones conflicts with this inherent tendency in natural language processing.

Specifically, for the first token, the writing strength to all slots is maximized (i.e., \(_{1}=^{m}\)), causing every memory slot to retain a copy of the first token's representation. The absence of a forgetting mechanism exacerbates this issue. For subsequent tokens, the writing strength diminishes due to the influence of earlier tokens, as a result of the cumulative \(\) in Eq. 4. This makes it challenging for the model to retain later tokens without learning a significantly large \(_{i}\), potentially leading to instability in long-context settings, as observed by Zhang et al. .

### Gated Slot Attention (GSA): ABC with gating mechanism

To address these limitations, we propose Gated Slot Attention (GSA), which incorporates a gating mechanism to simultaneously resolve both issues by: (i) enabling the forgetting of historical information, and (ii) introducing a recency inductive bias, as detailed below.

For each memory slot, the update rule is a simple gated RNN with a scalar data-dependent gating value \(_{i}\),

\[(}_{t})_{i}=_{i}(}_{t-1})_{i}+( 1-_{i})_{t}^{d},(}_{t})_{i }=_{i}(}_{t-1})_{i}+(1-_{i})_{t} ^{d}\]

and these can be written in matrix form, which is reminiscent of HGRN2 .

\[}_{t}&=(_{t})}_{t-1}+(1-_{t}) _{t}^{m d}\\ }_{t}&=( _{t})}_{t-1}+(1-_{t})_{t} ^{m d}\\ _{t}&=}^{T}(}_{t}^{T}_{t})^{d}\] (6)

GSA as two-pass GLA.It is straightforward to see that we can write GSA as a two-pass GLA as shown below:

\[\{^{}_{t}\}_{t=1}^{T}&= (\{_{t},_{t},1-_{t},_{t},\}_{t=1}^{T})\\ \{_{t}\}_{t=1}^{T}&=(\{ (^{}_{t}),1-_{t},_{t},,_{t}\}_{t=1}^{T})\] (7)

Therefore, we can adapt GLA's hardware-efficient chunkwise training algorithm for GSA training, as shown in SS A and SS B. We illustrate the recurrent representation of GSA in Figure 1.

### Neural Architecture

The overall architecture of our proposed model, GSA, is shown in Figure 2. Following the Llama architecture , we use a stack of \(L\) GSA blocks, each comprising a GSA token mixing layer followed by a Gated Linear Unit (GLU) channel mixing layer [19; 33].

We utilize the multi-head attention mechanism  to capture different aspects of the input. For each head \(h\), the input to GSA token mixing is defined as

\[_{i}^{h},_{i}^{h},_{i}^{h}=(_{q}^{h}_{i}),(_{k}^{h}_{i}),(_{v}^{h}_{i})\] (8)

where \(\) is the \(\) activation following . The forget gate is obtained by a linear transformation followed by a sigmoid activation \(\) with a damping factor \(\)[96; 83]: \(_{i}^{h}=(_{o}^{h}_{i})^{1/}\), 4 where the damping factor is to regulate the forget gate value to one, which has been shown to be crucial for long-term dependency modeling [30; 67]. We feed them into a GSA layer to obtain outputs as described in Eq. 7:

Finally, we obtain output via

\[}=_{o}((( (_{i}^{1},,_{i}^{H}))))\] (9)

The total number of parameters for \(_{q},_{k},_{v},\) and \(o\) is already \(4d^{2}\), which is the same as in a single standard \(\)-attention layer. To control the overall parameter count, we aim to keep the parameters for \(_{}\), which amount to \(dHm\), relatively small. In practice, we set \(m=64\) to achieve a balance between efficiency and effectiveness (SS 4.1.4). One way to further manage the total parameter count is by reducing the number of heads. In practice, we set \(H=4\), ensuring that \(Hm d\). This keeps the total number of parameters approximately equal to \(4d^{2}\). 6

## 4 Experiments

### Language Modeling

We perform moderate-scale language modeling experiments with 1.3B and 2.7B parameters on Slimpajama corpus  for 100B tokens each.

We compare the performance of GSA against Llama Transformer architecture (i.e., Xfmr++  and recent subquadratic architectures including: Mamba , RetNet , GLA  and HGRN2 . We refer readers to SS C for more details on baselines and other experimental setups.

#### 4.1.1 Results on commonsense reasoning tasks

Following [29; 96], we report the perplexities and zero-shot performance of commonsense reasoning tasks including ARC\({}_{e}\) & ARC\({}_{c}\) (ARC-easy, ARC-challenge) ; Hella. (Hellaswag) , Lamb. (Lambada) , PIQA , Wiki. (Wikitext) , and Wino. (Winograde) . We note that these tasks are typically short in length and do not require in-context learning capabilities, thus they do not adequately reflect long-context modeling or in-context learning retrieval abilities. Nevertheless, as shown in Table 1, we found that GSA performs comparably to the recent strong model HGRN2 with an equally sized hidden state, while outperforming GLA and RetNet even with a smaller state size.

#### 4.1.2 Results on in-context recall-intensive tasks

While subquadratic models can achieve comparable performance to (softmax-based) Transformers in language modeling and understanding tasks, their performance on recall-intensive tasks significantly lags behind Transformers and varies greatly across different subquadratic models, as observed in many recent studies [3; 4; 96; 97]. Therefore, it is crucial to improve linear models on in-context recall-intensive tasks.

Table 1: The zero-shot results of 1.3B and 2.7B models evaluated by lm-evaluation-harness. \(L\) denotes number of layer while \(d\) denotes the model dimension.

Mqar.We first present the results on the multi-query associative recall (MQAR) task , a diagnostic synthetic task that requires models to retrieve multiple associative key-value pairs from the context. This task has been shown to strongly correlate with language modeling performance . The results in Table 2(a) validate the effectiveness of GSA.

Real-world tasks.Next, we evaluate the zero-shot in-context learning performance on recall-intensive tasks, as used in Arora et al. .6 Specifically, we assess information retrieval on FDA  and SWDE , which are designed to evaluate retrieval from in-context passages scraped from HTML/PDFs. We also evaluate question answering on SQuAD , NQ , TriviaQA , and Drop , where models must ground their answers in in-context documents.

As shown in Table 2(b), Xfmr++ achieves the best average performance, as expected. Meanwhile, GSA outperforms all other subquadratic baseline models by a notable margin without requiring a larger state size. We believe this advantage stems from GSA's context-aware memory readout mechanism (as discussed in SS2.2) and its forgetting mechanism (i.e., the gating mechanism), enabling it to manipulate finite-sized memory more effectively.

#### 4.1.3 Ablation

Table 2 presents the results of our ablation studies. Our findings indicate that: (i) the inclusion of the gating mechanism in GSA is crucial for improving language modeling perplexity; (ii) applying \(\) non-linearities after the first recurrent pass is beneficial; and (iii) using 64 slots strikes an optimal balance between performance and efficiency. 7

#### 4.1.4 Efficiency

Fig. 3(a) illustrates the training throughput for four models on a single H800 GPU8. To optimize memory usage, we employ the technique of recomputing the recurrent hidden state during the backward pass, as done in FLA  and Mamboa2 . This approach results in reduced memory consumption (Fig. 3(b)) at the cost of slightly lower training throughputs (Fig. 3(a)).

Despite requiring two GLA passes, GSA maintains comparable training throughputs to GLA due to its reduced state size. Since inference is primarily memory-bound, inference speed highly correlates with state size. As a result, GSA, with its smaller state size compared to RetNet and GLA, achieves faster inference speeds, as shown in Figure 3(c).

### Finetuning Pretrained Transformers to RNNs

The concept of finetuning pretrained Transformers to linear Transformers for recurrent inference was first introduced in T2R . This approach uses pretrained language model weights to initialize all parameters, leveraging the similarity between linear attention and \(\) attention, and finetunes all parameters, significantly reducing the total training time compared to training from scratch. Kasai et al.  also introduced a _parametric_ feature map, implemented as a learnable MLP layer followed by \(\), applied after the query/key projections. SUPRA, a follow-up to T2R, found that the original T2R approach did not perform well in the era of LLMs, and highlighted the importance of

    & PPL (\(\)) \\  GSA w/ 64 slots & 13.51 \\  _Ablations on gating mechanism_ & \\ w/o decay (i.e., ABC) & 16.94 \\ w/ data-independent decay & 15.83 \\  _Ablations on non-linearity_ & \\ \(-\) & 14.03 \\ \(-+\) & 13.71 \\ \(-+\) & 13.69 \\ \(-+^{2}\) & 13.95 \\  _Ablations on slot size_ & \\ w/ 32 slots & 13.74 \\ w/ 128 slots & **13.46** \\   

Table 2: Ablation study results for 340M models trained on 10B Slimpajama tokens.

output normalization and a decay mechanism--adopted from RetNet --as critical for finetuning performance. As a result, SUPRA essentially combines T2R and RetNet by finetuning pretrained Transformers into a RetNet architecture, though it excludes the \(\) output gate.

Settings.In our preliminary experiments, we found that the learnable MLP layer was unnecessary and could be merged into the query and key projections, similar to the approach in Peng et al. . We finetuned the pretrained Transformer Mistral 7B  to RetNet, as well as to GLA and GSA models. Following SUPRA, we add \(\) as the feature map activation for RetNet and GLA, which originally used an identity feature map without activation 9, and also excluded the \(\) output gate. For RetNet, there were no additional parameters; for GLA, the low-rank forget gate, and for GSA, the \(W_{}\) matrix are trainable parameters, though both are small in parameter count and negligible in terms of the total model size. We set the peak learning rate to \(3 10^{-5}\) with 1K steps of linear warmup following SUPRA. The training length was set to 2K tokens, with a batch size of 2M tokens. For convenience, we trained on the SlimPajama corpus, while SUPRA used RefineWeb , a higher-quality corpus. We leave the use of RefineWeb for future work.

Figure 4: (a) Training throughput of various 1.3B models on a single H800 GPU, with a fixed batch size containing 16K tokens. “GSA w/o recomp.” indicates the use of the GSA kernel without hidden state recomputation during the backward pass. (b) Memory footprint (in GiB) of each 1.3B model during training with a batch size containing 16K tokens. (c) Inference latency (in seconds) of each 1.3B model on a single H800 GPU with 2K prefix tokens and a batch size of 1.

    & Size & Tokens & ARC\({}_{e}\) & ARC\({}_{c}\) & Hella. & PIQA & Wino. & NQ & TriviaQA & BBH & MMLU & Avg. \\ Shot(s) & & & 0 & 0 & 0 & 0 & 0 & 5 & 5 & 3 & 5 & \\   \\ RRVK6 & 7B & 1.4T & 73.6 & 44.0 & 75.2 & 78.4 & 68.5 & 20.9 & 59.5 & 23.4 & 43.9 & 54.1 \\ Mamba & 7B & 1.2T & 77.6 & 46.8 & 77.8 & 81.0 & 72.3 & 25.4 & 66.2 & 21.5 & 33.2 & 55.7 \\ Llama\({}^{}\) & 7B & 2T & 76.4 & 46.2 & 76.0 & 78.0 & 69.2 & 26.0 & 64.2 & 39.1 & 45.5 & 57.8 \\ Gemma\({}^{}\) & 7B & 6T & 81.5 & 53.2 & 80.5 & 79.8 & 74.0 & 24.3 & 63.7 & 58.9 & 63.2 & 64.3 \\ Mistral\({}^{}\) & 7B &? & 80.8 & 54.0 & 81.1 & 80.6 & 74.0 & 29.7 & 70.3 & 56.5 & 62.4 & 65.5 \\  \\ SUPRA & 7B & +20B & 74.6 & 42.3 & 74.8 & **80.1** & 67.4 & - & - & - & 28.0 & - \\ RetNet\({}^{}\) & 7B & +20B & 73.3 & 39.9 & 72.9 & 77.8 & 66.1 & 16.2 & 43.0 & 8.7 & 26.1 & 47.1 \\ GLA\({}^{}\) & 7B & +20B & 74.6 & **44.0** & 75.9 & 79.2 & 69.5 & 22.2 & 57.8 & 20.8 & 28.4 & 52.5 \\ GSA\({}^{}\) & 7B & +20B & **75.9** & 43.9 & **76.5** & 78.7 & **70.1** & **23.4** & **60.7** & **23.5** & **32.4** & **53.9** \\ SUPRA & 7B & +100B & **76.0** & 45.7 & 77.1 & **79.9** & 70.3 & 24.7 & 60.4 & 19.8 & 34.1 & 54.2 \\ GSA\({}^{}\) & 7B & +100B & **76.0** & **46.9** & **77.9** & 78.9 & **72.6** & **26.9** & **65.8** & **29.3** & **38.1** & **56.9** \\   

Table 3: Performance comparison across various 7B models. \({}^{}\) denotes models using \(\)-attention. \({}^{}\) denotes our results.

Main results.Following Jiang et al. , Touvron et al. , we evaluated the models on common-sense reasoning tasks: ARC\({}_{e}\) and ARC\({}_{c}\), Hellaswag , PIQA , and Winogrande ; world knowledge tasks: NQ  and TriviaQA ; and popular aggregated benchmarks: MMLU  and BBH . Results are shown in Table 3. We observed a clear advantage in finetuning Mistral to GSA compared to GLA or RetNet, confirming our intuition that preserving softmax is beneficial in T2R settings. When trained with 100B tokens, Mistral-to-GSA outperforms RWKV6 and Mamba on average, even though those models were trained on over 1T tokens, thereby reducing the required training data size.

Long-context ability evaluation.Following Xiong et al. , we evaluated the models on long-sequence tasks, including Qasper , NarrativeQA , QuALITY , and QMSum . For each task, the input was truncated to 16K tokens, which is 8\(\) the training length.

The results are shown in Table 4. Notably, GSA consistently outperforms other subquadratic models across all four tasks. We attribute this to the same factors observed in in-context recall-intensive task settings. Interestingly, Mistral-to-GSA also demonstrates overall better performance compared to RWKV6 and Mamba, which were trained from scratch on \(>\)1T token.

## 5 Related works

Matrix-valued linear RNNs with hardware-efficient training.Traditional RNNs (e.g., LSTM , GRU ) maintain 1-dimensional hidden states, which are often too small to capture sufficient information. Recent work emphasizes the importance of expanding the size of recurrent states . However, naive state expansion dramatically increases FLOPs and I/O costs, making training impractical. To address this, Mamba introduces an I/O-aware approach, reducing I/O costs by materializing parameters and hidden states only on SRAM (instead of HBM). However, Mamba's recurrence cannot be expressed in matmul form, leading to two key issues: (i) high FLOP count cannot be optimized via tensor cores (the GPU's fast matmul unit), resulting in slower runtimes; and (ii) the recurrent hidden states cannot be compactly represented and must be materialized on SRAM during backpropagation, limiting the recurrent state size due to SRAM constraints.

Mamba2  addresses these limitations by adopting a linear attention -like approach that enables hardware-efficient training. Linear attention expands the state using outer products, allowing for both parallel attention-style computation and recurrent inference (also known as state-space duality in Mamba2). The chunkwise algorithm interpolates between parallel and recurrent forms, enabling hardware-efficient, linear-time training . However, vanilla linear attention underperforms \(\) attention in various tasks. Recent research has explored incorporating various decay or gating mechanisms to enhance model expressiveness and performance while maintaining matmul-based parallelism and chunkwise training. These include head-wise data-independent decay ; head-wise data-dependent decay ; and channel-wise data-dependent decay . GSA leverages two-pass gated linear attention to further enhance capacity while allowing hardware-efficient training.

Fast weight RNNs.Fast weight programming , a classical concept intensively investigated in deep learning , has been shown to be closely related to (linear) Transformers . The core idea involves using a slow network to produce rapid context-dependent weight modifications for the fast network. In linear attention, the fast network is a single-layer FFN with weight matrix \(_{t}\) (Eq. 5), while the slow networks are the query/key/value projections.

Linear attention is known to suffer from limited memory capacity , potentially due to the constraints of a single-layer FFN without a large representation. In contrast, ABC and GSA can be viewed as implementing a two-layer fast FFN with either additive update rule or gated update rule , where the weight matrices are \(}_{t}\) and \(}_{t}\) connected by the \(\) activation function (Eq. 3 and Eq. 6). This structure resembles DeltaMLP , which uses a delta update rule 

    & Qasper & NarrativeQA & QuALITY & QMSum \\   \\ RWKV6 & 9.2 & 14.4 & 30.8 & 1.1 \\ Mamba & 5.6 & 27.9 & 27.5 & 0.8 \\ Mistral\({}^{}\) & 25.8 & 25.1 & 38.0 & 5.0 \\  \\ RetNet & 11.1 & 0.0 & 26.2 & 0.0 \\ GLA & 18.4 & 17.2 & 30.9 & 9.0 \\ GSA & **18.8** & **19.2** & **32.0** & **10.0** \\   

Table 4: Long-context performance comparison.

and a multi-layer (potentially beyond two layers) fast FFN. The greater capacity of a two-layer FFN compared to a similarly sized single-layer FFN could explain why GSA requires a smaller state size to achieve similar or even better performance, especially in long sequence and recall-intensive tasks.

Finetuning Transformers to RNNs.As discussed, this paradigm could significantly reduce the training cost for large-scale recurrent language models. The idea of distilling Transformers to RNNs to improve inference efficiency can be traced back to Gerstenberger et al. . In the following, we briefly introduce some recent works that complement those already mentioned in SS4.2. Zhang et al.  highlight the desirable properties of \(\), such as attention spikiness and dot-product monotonicity, and employ a learnable MLP layer to approximate \(\) behavior using logit distillation loss (while freezing other parameters). Chen et al.  introduce DiJiang, an effective method for approximating attention distributions using the Discrete Cosine Transform (DCT) to enable frequency-domain kernelization, leading to faster feature mapping. Bick et al.  propose a multi-stage distillation approach, aligning attention distributions (similar to Hedgehog ), hidden states, and output logits to transfer knowledge from a pretrained Transformer teacher to a student Mamba model. Wang et al.  distill Transformer-based LLMs into hybrid Mamba-Attention architectures in the spirit of Ren et al. , Lieber et al. , Waleffe et al. . However, they freeze the FFN weights, while Choi  suggest that it might be more effective to unfreeze them. In this work, we highlight the importance of the \(\) operator, as discussed in Zhang et al. , except that GSA directly incorporates \(\), while Zhang et al.  learns a feature map to mimic \(\), without actually including any \(\) operator in the resulting model.

## 6 Limitations and future work

Due to the relatively small scale of our pretrained models (compared to large-scale models trained on trillions of tokens), we did not report any results on long-context tasks, as the performance would all be poor. However, we believe Table 4 provides positive indications of GSA's long-context capabilities, and training on a larger token horizon and with larger models would address this. For copy-oriented tasks, we observed negative results on the Phonebook Lookup  and Needle-In-Haystack evaluations compared to Transformers, revealing the fundamental limitations of linear recurrent models in handling "precise local token shifts and comparison", as discussed in Arora et al. . Nonetheless, we expect this limitation could be significantly mitigated by pretraining a hybrid GSA-attention model, as recently explored , or by distilling pretrained Transformers into hybrid GSA-attention models, as in Wang et al. , or using different training objectives with JRT prompts, as in Arora et al. , or combining with YOCO .

GSA follows GLA in using a gated update rule, although we acknowledge recent work on Parallel DeltaNet , which parallelizes the delta update rule computations in DeltaNet  over sequence length, significantly enhancing training efficiency. The delta rule is known to improve in-context retrieval ability , aligning with one of the objectives of this work. We did not explore the analogous two-pass DeltaNet, but we leave this for future investigation, which would bring the approach closer to the original DeltaMLP , as discussed earlier. It would also be beneficial to compare GSA with more recent strong RNN models, such as xLSTM , Mamba2 , TTT , and Longhorn .

## 7 Conclusions

This work introduces Gated Slot Attention (GSA), which enhances ABC  with a gating mechanism inspired by Gated Linear Attention (GLA ). By framing GSA as a two-pass GLA, we can leverage hardware-efficient implementations of GLA  to train GSA. As such, GSA benefits from context-aware memory reading and forgetting, implicitly increasing the model's capacity despite a small actual state size, which improves training and inference efficiency. Through extensive experiments, we demonstrate the advantages of GSA in in-context recall-intensive tasks  and in "finetuning pretrained Transformers to RNNs"  scenarios.