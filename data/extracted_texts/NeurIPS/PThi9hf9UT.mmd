# Mutual Information Estimation via \(f\)-Divergence and Data Derangements

Nunzio A. Letizia Nicola Novello Andrea M. Tonello

University of Klagenfurt

{nunzio.letizia,nicola.novello,andrea.tonello}@aau.at

###### Abstract

Estimating mutual information accurately is pivotal across diverse applications, from machine learning to communications and biology, enabling us to gain insights into the inner mechanisms of complex systems. Yet, dealing with high-dimensional data presents a formidable challenge, due to its size and the presence of intricate relationships. Recently proposed neural methods employing variational lower bounds on the mutual information have gained prominence. However, these approaches suffer from either high bias or high variance, as the sample size and the structure of the loss function directly influence the training process. In this paper, we propose a novel class of discriminative mutual information estimators based on the variational representation of the \(f\)-divergence. We investigate the impact of the permutation function used to obtain the marginal training samples and present a novel architectural solution based on derangements. The proposed estimator is flexible since it exhibits an excellent bias/variance trade-off. The comparison with state-of-the-art neural estimators, through extensive experimentation within established reference scenarios, shows that our approach offers higher accuracy and lower complexity.

## 1 Introduction

The mutual information (MI) between two multivariate random variables, \(X\) and \(Y\), is a fundamental quantity in statistics, representation learning, information theory, communication engineering and biology . It quantifies the statistical dependence between \(X\) and \(Y\) by measuring the amount of information obtained about \(X\) via the observation of \(Y\), and it is defined as

\[I(X;Y)=_{(,) p_{XY}(,)} (,)}{p_{X}()p_{Y}( )}.\] (1)

Unfortunately, computing \(I(X;Y)\) is challenging since the joint probability density function \(p_{XY}(,)\) and the marginals \(p_{X}(),p_{Y}()\) are usually unknown, especially when dealing with high-dimensional data. Some recent techniques  have demonstrated that neural networks can be leveraged as probability density function estimators and, more in general, are capable of modeling the data dependence. Discriminative approaches  compare samples from both the joint and marginal distributions to directly compute the density ratio (or the log-density ratio)

\[R(,)=(,)}{p_{X}()p_{Y}()}.\] (2)

We focus on discriminative MI estimation since it can in principle enjoy some of the properties of implicit generative models, which are able of directly generating data that belongs to the same distribution of the input data without any explicit density estimate. In this direction, the mostsuccessful technique is represented by generative adversarial networks (GANs) . The adversarial training pushes the discriminator \(D()\) towards the optimum value

\[()=()}{p_{data}()+p_{gen}( )}=()}{p_{data}()}}.\] (3)

Therefore, the output of the optimum discriminator is itself a function of the density ratio \(p_{gen}/p_{data}\), where \(p_{gen}\) and \(p_{data}\) are the distributions of the generated and the collected data, respectively.

We generalize the observation of (3) and we propose a family of MI estimators based on the variational lower bound (VLB) of the \(f\)-divergence [11; 12]. In particular, we argue that the maximization of any \(f\)-divergence VLB can lead to a MI estimator with excellent bias/variance trade-off.

Since we typically have access only to joint data points \((,) p_{XY}(,)\), another relevant practical aspect is the sampling strategy to obtain data from the product of marginals \(p_{X}()p_{Y}()\), for instance via a shuffling mechanism along \(N\) realizations of \(Y\). We analyze the impact that the permutation has on the learning and training process and we propose a derangement training strategy that achieves high performance requiring \((N)\) operations. Simulation results demonstrate that the proposed approach exhibits improved estimations in a multitude of scenarios.

In brief, we can summarize our contributions over the state-of-the-art as follows:

* For any \(f\)-divergence, we derive a training value function whose maximization leads to a given MI estimator.
* We compare different \(f\)-divergences and comment on the resulting estimator properties and performance.
* We study the impact of data derangement for the learning model and propose a novel derangement training strategy that overcomes the upper bound on the MI estimation , contrarily to what happens when using a random permutation strategy.
* We unify the main discriminative estimators into a publicly available code which can be used to reproduce all the results of this paper.

## 2 Related Work

Traditional approaches for the MI estimation rely on binning, density and kernel estimation [14; 15], \(k\)-nearest neighbors , and ensemble-based models . Nevertheless, they do not scale to problems involving high-dimensional data as it is the case in modern machine learning applications. Hence, deep neural networks have recently been leveraged to maximize VLBs on the MI [11; 18; 19]. The expressive power of neural networks has shown promising results in this direction although less is known about the effectiveness of such estimators , especially since they suffer from either high bias or high variance.

Discriminative approaches usually exploit an energy-based variational family of functions to provide a lower bound on the Kullback-Leibler (KL) divergence. As an example, the Donsker-Varadhan dual representation of the KL divergence [11; 21] produces an estimate of the MI using the bound optimized by the mutual information neural estimator (MINE) . Another VLB based on the KL divergence dual representation introduced in  leads to the NWJ estimator (also referred to as \(f\)-MINE in ). Both MINE and NWJ suffer from high-variance estimates and to combat such a limitation, the SMILE estimator was introduced in , where the authors proved that the estimate of the partition function is the cause for high-variance in VLB estimators. SMILE is equivalent to MINE in the limit \(+\). The MI estimator based on contrastive predictive coding (CPC)  provides low variance estimates but it is upper bounded by \( N\), resulting in a biased estimator. Such upper bound, typical of contrastive learning objectives, has been recently analyzed in the context of skew-divergence estimators .

Another estimator based on a classification task is the neural joint entropy estimator (NJEE) proposed in , which estimates the MI as entropies subtraction.

Inspired by the \(f\)-GAN training objective , in the following, we present a class of discriminative MI estimators based on the \(f\)-divergence measure. Conversely to what has been proposed so far in the literature, where \(f\) is always constrained to be the generator of the KL divergence, we allow for any choice of \(f\). Different \(f\) functions will have different impact on the training and optimization sides, while on the estimation side, the partition function does not need to be computed, leading to low variance estimators.

## 3 \(f\)-Divergence Mutual Information Estimation

The calculation of the MI via a discriminative approach requires the density ratio (2). From (3), we observe that \(I(X;Y)\) can be estimated using the optimum GAN discriminator \(\) when \(p_{data} p_{X}p_{Y}\) and \(p_{gen} p_{XY}\). More in general, the authors in  extended the variational divergence estimation framework presented in  and showed that any \(f\)-divergence can be used to train GANs. Inspired by such idea, we now argue that also discriminative MI estimators enjoy similar properties if the variational representation of \(f\)-divergence functionals \(D_{f}(P||Q)\) is adopted.

In detail, let \(P\) and \(Q\) be absolutely continuous measures w.r.t. \(x\) and assume they possess densities \(p\) and \(q\), then the \(f\)-divergence is defined as follows

\[D_{f}(P||Q)=_{}q()f)}{q( )}\,,\] (4)

where \(\) is a compact domain and the function \(f:_{+}\) is convex, lower semicontinuous and satisfies \(f(1)=0\).

In the following, we introduce \(f\)-DIME, a class of discriminative mutual information estimators (DIME) based on the variational representation of the \(f\)-divergence.

**Theorem 3.1**.: _Let \((X,Y) p_{XY}(,)\) be a pair of multivariate random variables. Let \(()\) be a permutation function such that \(p_{(Y)}(()|)=p_{Y}()\) and \(T:(X)(Y)\). Let \(f^{*}\) be the Fenchel conjugate of \(f:_{+}\), a convex lower semicontinuous function that satisfies \(f(1)=0\) with derivative \(f^{}\). If \(_{f}(T)\) is a value function defined as_

\[_{f}(T)=_{(,) p_{XY}(,)}T,-f^{*}T ,(),\] (5)

_then_

\[(,)=_{T}_{f}(T)=f^{} (,)}{p_{X}()p_{Y}()} ,\] (6)

_and_

\[I(X;Y)=I_{fDIME}(X;Y)=_{(,) p_{XY}(,)}f^{*}^{} (,).\] (7)

Theorem 3.1 shows that any value function \(_{f}\) of the form in (5), seen as the dual representation of a given \(f\)-divergence \(D_{f}\), can be maximized to estimate the MI via (7). It is interesting to notice that the proposed class of estimators does not need any evaluation of the partition term.

We propose to parametrize \(T(,)\) with a deep neural network \(T_{}\) of parameters \(\) and solve with gradient ascent and back-propagation to obtain \(=_{}_{f}(T_{})\). By doing so, it is possible to guarantee that, at every training iteration \(n\), the convergence of the \(f\)-DIME estimator \(_{n,fDIME}(X;Y)\) is controlled by the convergence of \(T\) towards the tight bound \(\) while maximizing \(_{f}(T)\), as stated in the following lemma.

**Lemma 3.2**.: _Let the discriminator \(T()\) be with enough capacity, i.e., in the non parametric limit. Consider the problem_

\[=\ _{T}_{f}(T)\] (8)

_where \(_{f}(T)\) is defined as in (5), and the update rule based on the gradient descent method_

\[T^{(n+1)}=T^{(n)}+_{f}(T^{(n)}).\] (9)

_If the gradient descent method converges to the global optimum \(\), the mutual information estimator defined in (7) converges to the real value of the mutual information \(I(X;Y)\)._The proof of Lemma 3.2, which is described in the Appendix, provides some theoretical grounding for the behaviour of MI estimators when the training does not converge to the optimal density ratio. Moreover, it also offers insights about the impact of different functions \(f\) on the numerical bias.

It is important to remark the difference between the classical VLB estimators that follow a discriminative approach and the DIME-like estimators. They both achieve the goal through a discriminator network that outputs a function of the density ratio. However, the former models exploit the variational representation of the MI (or the KL) and, at the equilibrium, use the discriminator output directly in one of the value functions reported in Appendix B. The latter, instead, use the variational representation of _any_\(f\)-divergence to extract the density ratio estimate directly from the discriminator output.

In the upcoming sections, we analyze the variance of \(f\)-DIME and we propose a training strategy for the implementation of Theorem 3.1. In our experiments, we consider the cases when \(f\) is the generator of: a) the KL divergence; b) the GAN divergence; c) the Hellinger distance squared. Due to space constraints, we report in Sec. A of the Appendix the value functions used for training and the mathematical expressions of the resulting DIME estimators.

## 4 Variance Analysis

In this section, we assume that the ground truth density ratio \((,)\) exists and corresponds to the density ratio in (2). We also assume that the optimum discriminator \((,)\) is known and already obtained (e.g. via a neural network parametrization).

We define \(p_{XY}^{M}(,)\) and \(p_{X}^{N}()p_{Y}^{N}()\) as the empirical distributions corresponding to \(M\) i.i.d. samples from the true joint distribution \(p_{XY}\) and to \(N\) i.i.d. samples from the product of marginals \(p_{X}p_{Y}\), respectively. The randomness of the sampling procedure and the batch sizes \(M,N\) influence the variance of variational MI estimators. In the following, we prove that under the previous assumptions, \(f\)-DIME exhibits better performance in terms of variance w.r.t. some variational estimators with a discriminative approach, e.g., MINE and NWJ.

The partition function estimation \(_{p_{X}^{N}p_{Y}^{N}}[]\) represents the major issue when dealing with variational MI estimators. Indeed, they comprise the evaluation of two terms (using the given density ratio), and the partition function is the one responsible for the variance growth. The authors in  characterized the variance of both MINE and NWJ estimators, in particular, they proved that the variance scales exponentially with the ground truth MI \( M\)

\[_{p_{XY},p_{X}p_{Y}}I_{NWJ}^{M,N} -1}{N}\] \[_{N}N_{p_{XY},p_{X}p_{Y}}I_{MIE}^{M,N} e^{I(X;Y)}-1,\] (10)

where

\[I_{NWJ}^{M,N} :=_{p_{XY}^{M}}[+1]-_{p_{X}^{N}p_{ Y}^{N}}[]\] \[I_{MIE}^{M,N} :=_{p_{XY}^{M}}[]-_{p_{X}^{N}p _{Y}^{N}}[].\] (11)

To reduce the impact of the partition function on the variance, the authors of  also proposed to clip the density ratio between \(e^{-}\) and \(e^{}\) leading to an estimator (SMILE) with bounded partition variance. However, also the variance of the log-density ratio \(_{p_{XY}^{M}}[]\) influences the variance of the variational estimators, since it is clear that

\[_{p_{XY},p_{X}p_{Y}}I_{VLB}^{M,N}_{p_{XY }}_{p_{XY}^{M}}[],\] (12)

a result that holds for any type of MI estimator based on a VLB.

The great advantage of \(f\)-DIME is to avoid the partition function estimation step, significantly reducing the variance of the estimator. Under the same initial assumptions, from (12) we can immediately conclude that

\[_{p_{XY}}I_{fDIME}^{M}_{p_{XY},p_{X}p_{Y }}I_{VLB}^{M,N}],\] (13)where

\[I_{IDIME}^{M}:=_{p_{XY}^{M}}[]\] (14)

is the Monte Carlo implementation of \(f\)-DIME. Hence, the \(f\)-DIME class of models has lower variance than any VLB based estimator (MINE, NWJ, SMILE, etc.).

Furthermore, we provide in Appendix C two supplementary results. Lemma 4.1 introduces an upper bound on the variance of the \(f\)-DIME estimator, a result holding for any type of value function \(_{f}\). Lemma 4.2, instead, characterizes the variance of the estimator in (14) when \(X\) and \(Y\) are correlated Gaussian random variables. We found out that the variance is finite and we use this result to verify in the experiments that the variance of \(f\)-DIME does not diverge for high values of MI.

## 5 Derangement Strategy

The discriminative approach essentially compares expectations over both joint \((,) p_{XY}\) and marginal \((,) p_{X}p_{Y}\) data points. Practically, we have access only to \(N\) realizations of the joint distribution \(p_{XY}\) and to obtain \(N\) marginal samples of \(p_{X}p_{Y}\) from \(p_{XY}\) a shuffling mechanism for the realizations of \(Y\) is typically deployed. A general result in  shows that failing to sample from the correct marginal distribution would lead to an upper bounded MI estimator.

We study the structure that the permutation law \(()\) in Theorem 3.1 needs to have when numerically implemented. In particular, we now prove that a naive permutation over the realizations of \(Y\) results in an incorrect VLB of the \(f\)-divergence, causing the MI estimator to be bounded by \((N)\), where \(N\) is the batch size. To solve this issue, we propose a derangement strategy.

Let the data points \((,) p_{XY}\) be \(N\) pairs \((_{i},_{i})\), \( i\{1,,N\}\). The naive permutation of \(\), denoted as \(()\), leads to \(N\) new random pairs \((_{i},_{j})\), \( i\) and \(j\{1,,N\}\). The idea is that a random naive permutation may lead to at least one pair \((_{k},_{k})\), with \(k\{1,,N\}\), which is actually a sample from the joint distribution. Viceversa, the derangement of \(\), denoted as \(()\), leads to \(N\) new random pairs \((_{i},_{j})\) such that \(i j, i\) and \(j\{1,,N\}\). Such pairs \((_{i},_{j}),i j\) can effectively be considered samples from \(p_{X}()p_{Y}()\). An example using these definitions is provided in Appendix D.1.3.

The following lemma analyzes the relationship between the Monte Carlo approximations of the VLBs of the \(f\)-divergence \(_{f}\) in Theorem 3.1 using \(()\) and \(()\) as permutation laws.

**Lemma 5.1**.: _Let \((_{i},_{i})\), \( i\{1,,N\}\), be \(N\) data points. Let \(_{f}(T)\) be the value function in (5). Let \(_{f}^{}(T)\) and \(_{f}^{}(T)\) be numerical implementations of \(_{f}(T)\) using a random permutation and a random derangement of \(\), respectively. Denote with \(K\) the number of points \(_{k}\), with \(k\{1,,N\}\), in the same position after the permutation (i.e., the fixed points). Then_

\[_{f}^{}(T)_{f}^{}(T).\] (15)

Lemma 5.1 practically asserts that the value function \(_{f}^{}(T)\) evaluated via a naive permutation of the data is not a valid VLB of the \(f\)-divergence, and thus, there is no guarantee on the optimality of the discriminator's output. An interesting mathematical connection can be obtained when studying \(_{f}^{}(T)\) as a sort of variational skew-divergence estimator , but this goes beyond the scope of this paper.

The following theorem states that in the case of the KL divergence, the maximum of \(_{f}^{}(D)\) is attained for a value of the discriminator that is not exactly the density ratio (as it should be from (21), see Appendix A).

**Theorem 5.2**.: _Let the discriminator \(D()\) be with enough capacity. Let \(N\) be the batch size and \(f\) be the generator of the KL divergence. Let \(_{KL}^{}(D)\) be defined as_

\[_{KL}^{}(D)=_{(,) p_{XY}( ,)}D, -f^{*}D,().\] (16)

_Denote with \(K\) the number of indices in the same position after the permutation (i.e., the fixed points), and with \(R(,)\) the density ratio in (2). Then,_

\[(,)=_{D}_{KL}^{}(D)=,)}{KR(,)+N-K}.\] (17)Although Theorem 5.2 is stated for the KL divergence, it can be easily extended to any \(f\)-divergence using Theorem 3.1. Notice that if the number of indices in the same position \(K\) is equal to \(0\), we fall back into the derangement strategy and we retrieve the density ratio as output.

When we parametrize \(D\) with a neural network, we perform multiple training iterations and so we have multiple batches of dimension \(N\). This turns into an average analysis on \(K\). We report in the Appendix (see Lemma 5.4) the proof that, on average, \(K\) is equal to \(1\).

From the previous results, it follows immediately that the estimator obtained using a naive permutation strategy is biased and upper bounded by a function of the batch size \(N\).

**Corollary 5.3** (Permutation bound).: _Let KL-DIME be the estimator obtained via iterative optimization of \(_{KL}^{}(D)\), using a batch of size \(N\) every training step. Then,_

\[I_{KL-DIME}^{}:=_{(,) p_{XY}(, )}(,) <(N).\] (18)

We report in Fig. 1 an example of the difference between the derangement and permutation strategies. The estimate attained by using the permutation mechanism, showed in Fig. 0(b), demonstrates Theorem 5.2 and Corollary 5.3, as the upper bound corresponding to \((N)\) (with \(N=128\)) is clearly visible.

## 6 Experimental Results

In this section, we firstly describe the architectures of the proposed estimators. Then, we outline the data used to estimate the MI, comment on the performance of the discussed estimators in different scenarios, also analyzing their computational complexity. Finally, we present the outcomes of the self-consistency tests  over image datasets.

### Architectures

To demonstrate the behavior of the state-of-the-art MI estimators, we consider multiple neural network _architectures_. The word architecture needs to be intended in a wide-sense, meaning that it represents the neural network architecture and its training strategy. In particular, additionally to the architectures **joint** and **separable**, we propose the architecture **deranged**.

The **joint** architecture concatenates the samples \(\) and \(\) as input of a single neural network. Each training step requires \(N\) realizations \((_{i},_{i})\) drawn from \(p_{XY}(,)\), for \(i\{1,,N\}\) and \(N(N-1)\) samples \((_{i},_{j}), i,j\{1,,N\}\), with \(i j\).

The **separable** architecture comprises two neural networks, the former fed in with \(N\) realizations of \(X\), the latter with \(N\) realizations of \(Y\). The inner product between the outputs of the two networks is exploited to obtain the MI estimate.

The proposed **deranged** architecture feeds a neural network with the concatenation of the samples \(\) and \(\), similarly to the joint architecture. However, the deranged one obtains the samples of \(p_{X}()p_{Y}()\) by performing a derangement of the realizations \(\) in the batch sampled from \(p_{XY}(,)\). Such diverse training strategy solves the main problem of the joint architecture: the difficult scalability to large batch sizes. For large values of \(N\), the complexity of the joint architecture is \((N^{2})\), while

Figure 1: MI estimate obtained with derangement and permutation training procedures, for data dimension \(d=20\) and batch size \(N=128\).

the complexity of the deranged one is \((N)\). NJEE utilizes a specific architecture, in the following referred to as **ad hoc**, comprising \(2d-1\) neural networks, where \(d\) is the dimension of \(X\). \(I_{NJEE}\) training procedure is supervised: the input of each neural network does not include the \(\) samples. All the implementation details1 are reported in Appendix D.

### Complex Gaussian and non-Gaussian distributions

We benchmark the proposed class of MI estimators on two settings utilized in previous papers [11; 20]. In the first setting (called **Gaussian**), a multidimensional Gaussian distribution is sampled to obtain \(\) and \(\) samples, independently. Then, \(\) is obtained as linear combination of \(\) and \(\): \(=\,+}\,\), where \(\) is the correlation coefficient. In the second setting (referred to as **cubic**), the nonlinear transformation \(^{3}\) is applied to the Gaussian samples. The true MI follows a staircase shape, where each step is a multiple of \(2\)\(nats\). Each neural network is trained for 4k iterations for each stair step, with a batch size of \(64\) samples (\(N=64\)). The tested estimators are: \(I_{NJEE}\), \(I_{ SMILE}\) (\(=1\)), \(I_{GAN-DIME}\), \(I_{HD-DIME}\), \(I_{KL-DIME}\), and \(I_{CPC}\), as illustrated in Fig. 2. The performance of \(I_{MINE}\), \(I_{NWJ}\), and \(I_{ SMILE}(=)\) is reported in Sec. D of the Appendix, since they exhibit lower performance compared to both SMILE and \(f\)-DIME. In fact, all the \(f\)-DIME estimators have lower variance compared to \(I_{MINE}\), \(I_{NWJ}\), and \(I_{ SMILE}(=)\), which are characterized by an exponentially increasing variance (see (10), Tab. 2, Fig. 9, and Fig. 6 in the Appendix). In particular, all the estimators analyzed belonging to the \(f\)-DIME class achieve significantly low bias and variance when the true MI is small. Interestingly, for high target MI, different \(f\)-divergences lead to dissimilar estimation properties. For large MI, \(I_{KL-DIME}\) is characterized by a low variance, at the expense of a high bias and a slow rise time. Contrarily, \(I_{HD-DIME}\) attains a lower bias at the cost of slightly higher variance w.r.t. \(I_{KL-DIME}\). Diversely, \(I_{GAN-DIME}\) achieves the lowest bias, and a variance comparable to \(I_{HD-DIME}\). Additional results confirming the estimators' behavior when \(d\) and \(N\) vary, including experiments with high data dimensionality, are reported and described in Appendix D.

\(I_{NJEE}\) obtains an estimate which is highly biased, and variance comparable to \(f\)-DIME. \(I_{CPC}\) is upper-bounded by \((N)\). The MI estimates obtained with \(I_{ SMILE}\) and \(I_{GAN-DIME}\) appear to possess similar behavior, although the value functions of SMILE and GAN-DIME are structurally different. The reason why \(I_{ SMILE}\) is almost equivalent to \(I_{GAN-DIME}\) resides in their training strategy, since they both minimize the same \(f\)-divergence. Looking at the implementation of SMILE 2, in fact, the network's training is guided by the gradient computed using the Jensen-Shannon (JS) divergence (a linear transformation of the GAN divergence). Given the trained network, the clipped objective function proposed in  is only used to compute the MI estimate, since when (29) is used to train the network, the MI estimate diverges (see Fig. 7 in Appendix D). However, with the proposed class of \(f\)-DIME estimators we show that during the estimation phase the partition function (clipped in ) is not necessary to obtain the MI estimate.

We test our estimators over additional complex Gaussian data transformations (half-cube, asinh, and swiss roll mappings, Fig. 3) and non-Gaussian distributions (uniform and student distributions, Fig. 4) as suggested in . The half-cube mapping is used to lengthen the tails of the Gaussian distributions. The inverse hyperbolic sine (asinh) mapping shortens the tails of the Gaussian distributions. These two transformations are applied to the same scenario of the Gaussian and cubic already present in our paper. The swiss roll mapping increases the dimensionality of the data distribution (from two to three dimensions) and it is usually used to test dimensionality reduction techniques. It considers two Gaussian random variables that are transformed into uniform random variables via the probability integral transform, the same pre-processing approach utilized in  to estimate the MI. The swiss roll mapping is applied to the \(X\) uniform random variable. The stairs plots are obtained by varying the correlation between the initial Gaussian distributions. The uniform case estimates the MI of the summation of two uniform random variables \(U(0,1)\) and \(U(-,)\), where we vary the parameter \(\), modifying the true MI. The student scenario analyzes the case of a multivariate student distribution with dispersion matrix chosen to be the identity matrix and degrees of freedom \(df\). In this scenario, we vary \(df\), implying a variation of the target MI. For the transformed Gaussian scenarios showed in Fig. 3 GAN-DIME attains the best performance in terms of low bias and variance. Among the non-Gaussian settings depicted in Fig. 4, KL-DIME and GAN-DIME outperform the other methods, exhibiting low bias and exceptionally low variance.

A schematic comparison between all the MI estimators is reported in Tab. 6 in Sec. D of the Appendix, where \(I_{GAN-DIME}\) is proposed as the best estimator, because of its low bias, variance and robustness to the change of \(d\) and \(N\). When \(N\) and \(d\) vary, in fact, the class of \(f\)-DIME estimators proves its robustness (i.e., maintains low bias and variance), as represented in Figs. 2, and 10, and 11 in the Appendix. For instance, \(I_{GAN-DIME}\) attains low bias in all the three scenarios, and limited variance which decreases as \(N\) increases (see also Fig. 15 in Appendix D.1). Differently, the behavior

Figure 4: Staircase MI estimation comparison for \(d=1\) and \(N=64\). Top row: Uniform scenario. Bottom row: Student scenario

Figure 3: Staircase MI estimation comparison for \(d=5\) and \(N=64\). Top: Half-cube scenario. Middle: Asinh scenario. Bottom: Swiss roll scenario.

Figure 2: Staircase MI estimation comparison for \(d=5\) and \(N=64\). The _Gaussian_ case is reported in the top row, while the _cubic_ case is shown in the bottom row.

of \(I_{CPC}\) strongly depends on \(N\), significantly impacting its bias. Therefore, unless the batch size is considerably large, \(I_{CPC}\) estimate is not reliable. \(I_{NJEE}\) attains higher bias when \(N\) increases and, even more severely, when \(d\) decreases (see Fig. 2).

#### Computational Time Analysis

A fundamental characteristic of each algorithm is the computational time. The computational time analysis is developed on a server with CPU "AMD Ryzen Threadripper 3960X 24-Core Processor" and GPU "MSI GeForce RTX 3090 Gaming X Trio 24G, 24GB GDDR6X".

Before analyzing the time requirements to complete the \(5\)-step MI staircases, we specify two different ways to implement the derangement of the \(\) realizations in each batch.

**Random-based**. The trivial way to achieve the derangement is to randomly shuffle the \(\) elements of the batch until there are no fixed points (i.e., all the \(\) realizations in the batch are assigned to a different position w.r.t. the starting location).

**Shift-based**. Given \(N\) realizations \((_{i},_{i})\) drawn from \(p_{XY}(,)\), for \(i\{1,,N\}\), we obtain the deranged samples as \((_{i},_{(i+1)\% N})\), where "%" is the modulo operator.

Although the MI estimates obtained by the two derangement methods are almost indistinguishable, all the results showed in the paper are achieved by using the random-based method. Additionally, we demonstrate the time efficiency of the shift-based approach.

The time requirements to complete the 5-step staircase MI when varying the batch size \(N\) are reported in the left and center graphics of Fig. 5. The influence of the MI estimator objective functions in the algorithm's time requirements is marginal, while the architecture type is the impactful component. As discussed in Sec. 6.1, the deranged strategy is remarkably faster than the joint one as \(N\) increases. More in general, the architectures deranged and separable are significantly faster w.r.t. the joint and NJEE ones, for a given batch size \(N\) and input distribution size \(d\). The need of the separable architecture to use two neural networks implies that when \(N\) is significantly large, the deranged implementation is much faster than the separable one. The central graph in Fig. 5 illustrates a detailed representation of the time requirements of these two architectures to complete the \(5\)-step stairs. As \(N\) increases, the gap between the time needed by the architectures deranged and separable grows, demonstrating that the former is the fastest. For example, when \(d=20\) and \(N=30k\), \(I_{GAN-DIME}\) needs about \(55\) minutes when using the architecture separable, but only \(15\) minutes when using the deranged one and less than \(9\) minutes for the shift-based deranged architecture.

\(I_{NJEE}\) is evaluated with its own architecture, which is the most computationally demanding, because it trains a number of neural networks equal to \(2d-1\). Thus, \(I_{NJEE}\) can be utilized only in cases where the time availability is orders of magnitude higher than the other approaches considered. The time requirements to complete the 5-step staircase MI when varying the multivariate Gaussian distribution dimension \(d\) are reported in the right-side part of Fig. 5. When \(d\) is large, the training of \(I_{NJEE}\) fails due to memory requirement problems. For example, our hardware platform does not allow the usage of \(d>30\).

### Self-Consistency Tests

To demonstrate the utility of \(f\)-DIME in non-Gaussian scenarios, we investigated the three self-consistency tests developed by  over images datasets using all the estimators previously described, except \(I_{NJEE}\) (for dimension constraints). The \(f\)-DIME estimators satisfy two out of the three tests,

Figure 5: Time requirements comparison to complete the 5-step staircase MI. From the left, the first and second behaviors vary over the batch size. The last one varies over the probability distribution dimension.

as discriminative approaches tend to be less precise when the MI is high, in accordance with . We report the description of tests and results in Appendix D.

## 7 Conclusions

In this paper, we presented \(f\)-DIME, a class of discriminative mutual information estimators based on the variational representation of the \(f\)-divergence. We proved that any valid choice of the function \(f\) leads to a low-variance MI estimator which can be parametrized by a neural network. We also proposed a derangement training strategy that efficiently samples from the product of marginal distributions. The performance of \(f\)-DIME is evaluated using three functions \(f\), and it is compared with state-of-the-art estimators. Results demonstrate excellent bias/variance trade-off for different data dimensions and different training parameters.