# Extremal Domain Translation with

Neural Optimal Transport

Milena Gazdieva

Skolkovo Institute of Science and Technology

Moscow, Russia

milena.gazdieva@skoltech.ru

&Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

a.korotin@skoltech.ru

&Daniil Selikhanovych

Skolkovo Institute of Science and Technology

Moscow, Russia

selikhanovychdaniil@gmail.com

&Evgeny Burnaev

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

e.burnaev@skoltech.ru

###### Abstract

In many _unpaired_ image domain translation problems, e.g., style transfer or super-resolution, it is important to keep the translated image similar to its respective input image. We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task. The code is publicly available at

https://github.com/milenagazdieva/ExtremalNeuralOptimalTransport

## 1 Introduction

The **unpaired** translation task [72, Fig. 2] is to find a map \(x T(x)\), usually a neural network, which transports the samples \(x\) from the given source domain to the target domain. The key challenge

Figure 1: (Nearly) extremal transport with our Algorithm 1.

Higher \(w\) yields bigger similarity of \(x\) and \(T(x)\) in \(^{2}\).

here is that the **correspondence** between available data samples \(x\) from the source and \(y\) from target domains **is not given**. Thus, the task is ambiguous as there might exist multiple suitable \(T\).

When solving the task, many methods regularize the translated samples \(T(x)\) to inherit specific attributes of the respective input samples \(x\). In the popular unpaired translation [72, Fig. 9] and enhancement [69, Equation 3] tasks for images, it is common to use additional _unsupervised_ identity losses, **e.g.**, \(\|T(x)-x\|_{1}\), to make the translated output \(T(x)\) be similar to the input images \(x\). The same applies, **e.g.**, to audio translation . Therefore, the learning objectives of such methods usually have two components.

The first component is the **domain loss** (main) enforcing the translated sample \(T(x)\) to look like the samples \(y\) from the target domain. The second component is the **similarity loss** (regularizer, _optional_) stimulating the translated \(T(x)\) to inherit certain attributes of input \(x\). A question arises: can one obtain the **maximal** similarity of \(T(x)\) to \(x\) but still ensure that \(T(x)\) is indeed from the target domain? A straightforward "_yes, just increase the weight of the similarity loss_" may work but only to a limited extent. We demonstrate this in Appendix C.

**Contributions.** In this paper, we propose the _extremal transport_ (ET, SS3.1) which is a rigorous mathematical task formulation describing the theoretically best possible unpaired domain translation w.r.t. the given similarity function. We explicitly characterize ET maps and plans by establishing an intuitive connection to the nearest neighbors (NN). We show that ET maps can be learned as a limit (SS3.3) of specific partial optimal transport (OT) problem which we call _incomplete transport_ (IT, SS3.2). For IT, we derive the duality formula yielding an efficient computational algorithm (SS3.4). We test our algorithm on toy 2D examples and high-dimensional unpaired image translation (SS5).

**Notation.** We consider _compact_ Polish spaces \((,\|\|_{})\), \((,\|\|_{})\) and use \((),()\) to denote the sets of Radon probability measures on them. We use \(_{+}()()\) to denote the sets of finite non-negative and finite signed (Radon) measures on \(\), respectively. They both contain \(()\) as a subset. For a non-negative \(_{+}()\), its support is denoted by \(()\). It is a closed set consisting of all points \(x\) for which every open neighbourhood \(A x\) satisfies \((A)>0\). We use \(()\) to denote the set of continuous functions \(\) equipped with \(\|\|_{}\) norm. Its dual space is \(()\) equipped with the \(\|\|_{1}\) norm. A sequence \(_{1},_{2},()\) is said to be weakly-* converging to \(^{*}()\) if for every \(f()\) it holds that \(_{n}_{}f(x)d_{n}(x)=_{}f(x)d^ {*}(x)\). For a probability measure \(()\), we use \(_{x}()\) and \(_{y}()\) to denote its projections onto \(,\), respectively. Disintegration of \(\) yields \(d(x,y)=d_{x}(x)d(y|x)\), where \((y|x)\) denotes the conditional distribution of \(y\) for a given \(x\). For \(,()\), we write \(\) if for all measurable \(A\) it holds that \((A)(A)\). For a measurable map \(T:\), we use \(T\) to denote the associated pushforward operator \(()()\).

## 2 Background on Optimal Transport

In this section, we give an overview of the OT theory concepts related to our paper. For details on OT, we refer to , partial OT - .

**Standard OT formulation.** Let \(c:\) be a continuous cost function. For \(()\), \(()\), the OT cost between them is given by

\[(,)}{{=}}_{T ^{}=}_{}cx,T(x)d (x),\] (1)

where \(\) is taken over measurable \(T:\) pushing \(\) to \(\) (transport maps), see Fig. 2(a). Problem (1) is called the _Monge's OT_ problem, and its minimizer \(T^{*}\) is called an _OT map_.

In some cases, there may be no minimizer \(T^{*}\) of (1). Therefore, it is common to consider _Kantorovich's_ relaxation:

\[(,)}{{=}}_{ (,)}_{}c(x,y)d( x,y),\] (2)

where \(\) is taken over \(()\) satisfying \(_{x}=\) and \(_{y}=\), respectively. A minimizer \(^{*}(,)\) in (2) always exists and is called an _OT plan_. A widely used example of OT cost for \(==^{d}\) is the Wasserstein-1 distance (\(_{1}\)), i.e., OT cost (2) for \(c(x,y)=\|x-y\|\).

To provide an intuition behind (2), we disintegrate \(d(x,y)\!=\!d_{x}(x)d(y|x)\):

\[_{(,)}_{}_{ }c(x,y)d(y|x)}(x)}_{=d_{x}(x)},\] (3)

i.e., (2) can be viewed as an extension of (1) allowing to _split_ the mass of input points \(x\!\!\) (Fig. 2(b)). With mild assumptions on \(,\), the OT cost value (2) coincides with (1), see [62, Theorem 1.33].

**Partial OT formulation.** Let \(w_{0}\),\(w_{1}\!\!m\!\!0\). We consider

\[_{m_{x} w_{0}\\ m_{y} w_{1}}_{ }c(x,y)dm(x,y),\] (4)

where \(\) is taken over \(()\) satisfying the inequality constraints \(m_{x} w_{0}\) and \(m_{y} w_{1}\). Minimizers \(^{*}\) of (4) are called _partial OT plans_ (Fig. 4).

Here the inputs are two measures \(w_{0}\) and \(w_{1}\) with masses \(w_{0}\) and \(w_{1}\). Intuitively, we need to match a \(}\)-th fraction \(m_{x}\) of the first measure \(w_{0}\) with a \(}\)-th fraction \(m_{y}\) of the second measure \(w_{1}\) (Fig. 4); choosing \(_{x},_{y}\) is also a part of this problem. The key difference from problem (4) is that the constraints are _inequalities_. In the particular case \(m=w_{0}=w_{1}\), problem (4) reduces to (2) as the inequality constraints can be replaced by equalities.

## 3 Main Results

First, we formulate the extremal transport (ET) problem (SS3.1). Next, we prove that ET maps can be recovered as a limit of incomplete transport (IT) maps (SS3.2, 3.3). Then we propose an algorithm to solve the IT problem (SS3.4). We provide the proofs for all the theorems in Appendix F.

### Extremal Transport Problem

Popular unpaired translation methods, e.g., [72, SS3.1] and [33, SS3], de-facto assume that available samples \(x,y\) from the input and output domains come from the data distributions \(,(),()\). As a result, in their optimization objectives, the _domain loss_ compares the translated \(T(x) T\) and target samples \(y\) by using a metric for comparing _probability measures_, e.g., GAN loss . Thus, the target _domain_ is identified with the probability measure \(\).

We pick a different approach to define what the _domain_ is. We still assume that the available data comes from data distributions, i.e., \(x,y\). However, we say that the target domain is the part of \(\) where the probability mass of \(\) lives.2 Namely, it is \(()\). We say that a map \(T\) translates the domains if \((T)()\). This requirement is weaker than the usual \(T\!=\!\). Assume that \(c(x,y)\) is a function estimating the dissimilarity between \(x,y\). We would like to pick \(T(x)()\) which is _maximally_ similar to \(x\) in terms of \(c(x,y)\). This preference of \(T\) can be formalized as follows:

\[_{}(,)}{{=}}_{(T)()}_{ }cx,T(x)d(x),\] (5)

where the \(\) is taken over measurable \(T:\) which map the probability mass of \(\) to \(()\). We say that (5) is the (Monge's) _extremal transport_ (ET) problem.

Figure 4: Partial optimal transport formulation.

Figure 3: Classic optimal transport (OT) formulations.

Problem (5) is **atypical** for the common OT framework. For example, the usual measure-preserving constraint \(T=\) in (1) is replaced with \((T)()\) which is more tricky. Importantly, measure \(\) can be replaced with any other \(^{}()\) with the same support yielding the same \(\).

Below we analyse the minimizers \(T^{*}\) of (5). We define \(c^{*}(x)}{{=}}_{y()} c(x,y)\). Here the \(\) is indeed attained (for all \(x\)) because \(c(x,y)\) is continuous and \(()\) is a compact set. The value \(c^{*}(x)\) can be understood as the _lowest_ possible transport cost when mapping the mass of point \(x\) to the support of \(\). For any admissible \(T\) in (5), it holds (\(\)-almost surely):

\[c^{*}(x)=_{y()}c(x,y) cx,T(x).\] (6)

**Proposition 1** (Continuity of \(c^{*}\)).: _It holds that \(c^{*}()\)._

As a consequence of Proposition 1, we see that \(c^{*}\) is measurable. We integrate (6) w.r.t. \(x\) and take \(\) over all feasible \(T\). This yields a lower bound on \(_{}(,)\):

\[_{}c^{*}(x)d(x)(T)\\ ()}_{}cx,T(x )d(x)}^{_{}(,)}.\] (7)

There exists admissible \(T\) making (7) the equality. Indeed, let \((x)}{{=}}\{y()\) s.t. \(c(x,y)=c^{*}(x)\}\) be the set of points \(y\) which attain \(\) in the definition of \(c^{*}\). These points are the closest to \(x\) points in \(\) w.r.t. the cost \(c(x,y)\). We call them the _nearest neighbors_ of \(x\). From this perspective, we see that (7) turns to equality if and only if \(T(x)\!\!(x)\) holds for \(\)-almost all \(x\), i.e., \(T\) maps points \(x\!\!\) to their nearest neighbors in \(()\). We need to make sure that such _measurable_\(T\) exists (Fig. 4(a)).

**Theorem 1** (Existence of ET maps).: _There exists at least one measurable map \(T^{*}:\) minimizing (5). For \(\)-almost all \(x\) it holds that \(T^{*}(x)(x)\). Besides,_

\[_{}(,)=_{}c^{*}(x)d (x).\]

We say that \(_{}(,)\) is the _extremal cost_ because one can not obtain smaller cost when moving the mass of \(\) to \(()\). In turn, we say that minimizers \(T^{*}\) are _ET maps_.

One may extend the ET problem (8) in the Kantorovich's manner by allowing the mass splitting and stochastic plans:

\[_{}(,)}{{=}}_{^{}(,)}_{ }c(x,y)d(x,y),\] (8)

where \(^{}(,)\) are probability measures \(()\) s.t. \(_{x}=\) and \((_{y})()\). To understand the structure of minimizers in (8), it is more convenient to disintegrate \(d(x,y)=d(y|x)d_{x}(x)\):

\[_{}(,)\!=\!\!_{^{}( ,)}\!_{}\!\!_{}\!\!c(x,y)d (y|x)(x)}_{=d_{x}(x)}.\] (9)

Thus, computing (8) boils down to computing a family of conditional measures \((|x)\) minimizing (8). As in (7), for any \(^{}(,)\), it holds (for \(\)-almost all \(x\)) that

\[c^{*}(x)=_{y()}c(x,y)_{}c(x,y )d(y|x)\] (10)

because \(\) redistributes the mass of \(\) to \(()\). By integrating (10) w.r.t. \(x=_{x}\) and taking \(\) over all admissible plans \(\), we derive that \(_{}c^{*}(x)d(x)\) is a lower bound for (8). In particular,

Figure 5: Extremal transport (ET) formulations.

the bound is tight for \(^{*}(y|x)=_{T^{*}(x)}\), where \(T^{*}\) is the ET map from our Theorem 1. Therefore, the value (8) is the same as (5) but possibly admits more minimizers. We call the minimizers \(^{*}\) of (8) the _ET plans_ (Fig. 4(b)).

From (10) and the definition of \((x)\), we see that minimizers \(^{*}\) are the plans for which \(^{*}(y|x)\) redistributes the mass of \(x\) among the nearest neighbors \(y(x)\) of \(x\) (for \(\)-almost all \(x\)). As a result, \(^{*}(y|x)\) can be viewed as a _stochastic nearest neighbor assignment_ between the probability mass of \(\) and the support of \(\).

### Incomplete Transport Problem

In practice, solving extremal problem (5) is challenging because it is hard to enforce \((T)()\). To avoid enforcing this constraint, we replace it and consider the following problem with finite parameter \(w 1\):

\[_{w}(,)}{{=}}_{T  w}_{}cx,T(x)d (x).\] (11)

We call (11) Monge's _incomplete transport_ (IT) problem (Fig. 5(a)). With the increase of \(w\), admissible maps \(T\) obtain more ways to redistribute the mass of \(\) among \(()\). Informally, when \(w\), the constraint \(T w\) in (11) tends to the constraint \((T)()\) in (5), i.e., (11) itself tends to ET problem (5). We will formalize this statement a few paragraphs later (in SS3.3).

As in (1), problem (11) may have no minimizer \(T^{*}\) or even may have the empty feasible set. Therefore, it is natural to relax problem (11) in the Kantorovich's manner:

\[_{w}(,)}{{=}}_{ ^{w}(,)}_{}c(x,y)d (x,y),\] (12)

where the \(\) is taken over the set \(^{w}(,)\) of probability measures \(()\) whose first marginal is \(_{x}=\), and the second marginal satisfies \(_{y} w\) (Fig. 5(b)).

We note that IT problem (12) is a special case of partial OT (4) with \(w_{0}=m=1\) and \(w_{1}=w\). In (12), one may actually replace \(\) with \(\), see our proposition below.

**Proposition 2** (Existence of IT plans).: _Problem (12) admits at least one minimizer \(^{*}^{w}(,)\)._

We say that minimizers of (12) are _IT plans_. In the general case, Kantorovich's _IT cost_ (12) always lower bounds Monge's counterpart (11). Below we show that they coincide in the practically most interesting Euclidean case.

**Proposition 3** (Equivalence of Monge's, Kantorovich's IT costs).: _Let \(,^{D}\) be two compact sets, \(()\) be atomless, \(()\). Then Monge's (11) and Kantorovich's (12) IT costs coincide._

However, it is not guaranteed that \(\) in Monge's problem (11) is attained even in the Euclidean case. Still for general Polish spaces \(,\) it is clear that if there exists a deterministic IT plan in Kantorovich's problem (12) of the form \(^{*}=[_{},T^{*}]\), then \(T^{*}\) is an IT map in (11), and the IT Monge's (11) and Kantorovich's (12) costs coincide. Henceforth, for simplicity, we assume that \(,,c,,\) are such that (11) and (12) coincide, e.g., those from Prop. 3.

IT problem (12) can be viewed as an interpolation between OT (2) and ET problems (8). Indeed, when \(w=1\), the constraint \(_{y}\) is equivalent to \(_{y}=\) as there is only one _probability_ measure which is \(\), and it is \(\) itself. Thus, IT (12) with \(w=1\) coincides with OT (2). In the next section, we show that for \(w\) one recovers ET from IT.

### Link between Incomplete and Extremal Transport

Now we connect incomplete (12) and extremal (8) transport tasks.

Figure 6: Incomplete transport (IT) formulations.

**Theorem 2** (IT costs converge to the ET cost when \(w\)).: _Function \(w_{w}(,)\) is convex, non-increasing in \(w[1,+)\) and_

\[_{w}_{w}(,)=_{ }(,).\]

A natural subsequent question here is whether IT plans in (12) converge to ET plans (8) when \(w\). Our following result sheds the light on this question.

**Theorem 3** (IT plans converge to ET plans when \(w\)).: _Consider \(w_{1},w_{2},w_{3}, 1\) satisfying \(_{n}w_{n}=\). Let \(^{w_{n}}^{w_{n}}(,)\) be a sequence of IT plans solving (12) with \(w=w_{n}\), respectively. Then it has a (weakly-*) converging sub-sequence. Every such sub-sequence of IT plans converges to an ET plan \(^{*}^{}(,)\)._

In general, there may be sub-sequences of IT plans converging to different ET plans \(^{*}^{}(,)\). However, our following corollary shows that with the increase of weight \(w\), elements of **any** sub-sequence become closer to the **set** of ET plans.

**Corollary 1** (IT plans become closer to the set of ET plans when \(w\)).: _For all \(>0\)\( w()[1,)\) such that \( w w()\) and \(\) IT plan \(^{w}_{w}(,)\) solving Kantorovich's IT problem (12), there exists an ET plan \(^{*}\) which is \(\)-close to \(^{w}\) in \(_{1}\), i.e., \(_{1}(^{*},^{w})\)._

Providing a stronger convergence result here is challenging, and we leave this theoretical question open for future studies. Our Theorems 2, 3 and Corollary 1 suggest that to obtain a fine approximation of an ET plan (\(w=\)), one may use an IT plan for sufficiently large finite \(w\). In Appendix G.1, we _empirically demonstrate_ this observation through an experiment where the ground-truth deterministic ET plan is analytically known. Below we develop a neural algorithm to compute IT plans.

### Computational Algorithm for Incomplete Transport

To begin with, for IT (12), we derive the dual problem.

**Theorem 4** (Dual problem for IT).: _It holds_

\[_{w}(,)\!=\!_{f 0}\!\!_{ }f^{c}(x)d(x)+w\!\!_{}\!f(y)d(y),\] (13)

_where the \(\) is taken over non-positive \(f()\) and \(f^{c}(x)}{{=}}}{} c(x,y)-f(y)}\)._

We call the function \(f\)_potential_. In the definition of \(f^{c}\), \(\) is attained because \(c,f\) are continuous and \(\) is compact. The function \(f^{c}\) is called the \(c\)-transform of \(f\).

The difference of formula (13) from usual \(c\)-transform-based duality formulas for OT (2), see [62, SS1.2], [65, SS5], is that \(f\) is required to be non-positive and the second term is multiplied by \(w 1\). We rewrite the term \(_{}f^{c}(x)d(x)\) in (13):

\[_{}f^{c}(x)d(x)=_{}_{y }c(x,y)-f(y)}d(x)=_{T: }_{}cx,T(x)-f(y)}d (x).\] (14)

Here we use the interchange between the integral and \(\)[59, Theorem 3A]; in (14) the \(\) is taken over measurable maps. Since \((x,y) c(x,y)-f(y)\) is a continuous function on a compact set, it admits a measurable selection \(T(x)_{y}c(x,y)-f(y)}\) minimizing (14), see [2, Theorem 18.19]. Thus, \(\) can be replaced by \(\). We combine (14) and (13) and obtain an equivalent saddle point problem:

\[_{w}(,)=_{f 0}_{T: }(f,T),\] (15)

where the functional \((f,T)\) is defined by

\[(f,T)}{{=}}_{}cx,T(x)d(x)-_{}fT(x)d(x)+ w_{}f(y)d(y).\] (16)

Functional \((f,T)\) can be viewed as a Lagrangian with \(f 0\) being a multiplier for the constraint \(T-w 0\). By solving (15), one may obtain IT maps.

**Theorem 5** (IT maps are contained in optimal saddle points).: _Let \(f^{*}\) be any maximizer in (13). If \(^{*}^{w}(,)\) is a deterministic IT plan, i.e., it solves (12) and has the form \(^{*}=[_{},T^{*}]\) for some measurable \(T^{*}:\), then_

\[T^{*}}{}\,(f^{*},T).\]Our Theorem 5 states that in _some_ optimal saddle points \((f^{*},T^{*})\) of (15) it holds that \(T^{*}\) is the IT map between \(,\). In general, the \(_{T}\) set for an optimal \(f^{*}\) might contain not only IT maps \(T^{*}\), but other functions as well (_fake solutions_), see limitations in Appendix A.

We solve the optimization problem (15) by approximating the map \(T\) and potential \(f\) with neural networks \(T_{}\) and \(f_{}\), respectively. To make \(f_{}\) non-positive, we use \(x-|x|\) as the last layer. The nets are trained using random batches from \(,\) and stochastic gradient ascent-descent. We detail the optimization procedure in Algorithm 1.

``` Input :distributions \(,\) accessible by samples; mapper \(T_{}:\); potential \(f_{}:_{-}\);  transport cost \(c:\); weight \(w 1\); number \(K_{T}\) of inner iterations; Output :approximate IT map \((T_{})_{\#} w\); repeat  Sample batches \(X\), \(Y\); \(_{f} w|}_{y Y}f_{ }(y)-_{x X}f_{}T_{}(x)\);  Update \(\) by using \(_{f}}{}\) to maximize \(_{f}\); for\(k_{T}=1,2,,K_{T}\)do  Sample batch \(X\); \(_{T}_{x X}cx,T_{ }(x)-f_{}T_{}(x)\);  Update \(\) by using \(_{T}}{}\) to minimize \(_{T}\); until not converged; ```

**Algorithm 1**Procedure to compute the IT map between \(\) and \(\) for transport cost \(c(x,y)\) and weight \(w\).

## 4 Related work

**OT in generative models.** A popular way to apply OT in generative models is to use the **OT cost** as the loss function to update the generator [4; 28; 26], see  for a survey. These methods are _not relevant_ to our study as they do not learn an OT map but only compute the OT cost.

Recent works [43; 42; 61; 20; 5; 24; 29] are the most related to our study. These papers show the possibility to learn the **OT maps** (or plans) via solving saddle point optimization problems derived from the standard \(c\)-transform-based duality formulas for OT. The underlying principle of our objective (15) is analogous to theirs. The key **difference** is that they consider OT problems (1), (2) and enforce the _equality_ constraints, e.g., \(T=\), while our approach enforces the _inequality_ constraint \(T w\) allowing to _partially_ align the measures. _We provide a detailed discussion of relation with these works as well as with the fundamental OT (2) and partial OT (4) literature_[22; 10; 62] in Appendix F, see bibliographic remarks after the proofs of Theorems 2, 4 and 5.

For completeness, we also mention other existing neural OT methods [25; 63; 49; 17; 39]. These works are less related to our work because they either underperform compared to the above-mentioned saddle point methods, see  for evaluation, or they solve specific OT formulations, e.g., entropic OT [56; SS4], which are not relevant to our study.

The papers [66; 47; 19] are slightly more related to our work. They propose neural methods for unbalanced OT  which can also be used to _partially_ align measures. As we will see in Appendix B, UOT is hardly suitable for ET (8) as it is not easy to control how it spreads the probability mass. Besides, these methods consider OT between small-dimensional datasets or in latent spaces. It is not clear whether they scale to high-dimensions, e.g., images.

**Discrete OT methods**, including partial OT , are not relevant to us, see Appendix D for details.

**Unpaired domain translation** is a generic problem which includes image super-resolution , inpainting , style translation  tasks, etc. Hence, we do not mention all of the existing approaches but focus on their common main features instead. In many applications, it is important to preserve semantic information during the translation, e.g., the image content. In most cases, to do this it is sufficient to use convolutional neural networks. They preserve the image content thanks to their design which is targeted to only locally change the image . However, in some of the tasksadditional image properties must be kept, e.g., image colors in super-resolution. Typical approaches to such problems are based on GANs and use additional _similarity_ losses, e.g., the basic CycleGAN  enforces \(^{1}\) similarity. Such methods are mostly related to our work. However, without additional modifications most of these approaches only partially maintain the image properties, see [44, Figure 5]. As we show in experiments (Appendix C), IT achieves better similarity than popular CycleGAN, StarGAN-v2  methods, both default and modified to better preserve the image content.

## 5 Evaluation

In SS5.1, we provide illustrative toy 2D examples. In SS5.2, we evaluate our method on the unpaired image-to-image translation task. Technical _training details_ (architectures, learning rates, etc.) are given in Appendix E. The code is written using PyTorch framework and is publicly available at

https://github.com/milenagazdieva/ExtremalNeuralOptimalTransport.

**Transport costs.** We experiment with the quadratic cost \(c(x,y)=^{2}(x,y)\) as this cost already provides reasonable performance. We slightly abuse the notation and use \(^{2}\) to denote the squared error _normalized_ by the dimension. Experiments with the _perceptual cost_ are given in Appendix G.3.

### Toy 2D experiments

In this section, we provide _'Wi-Fi'_ and _'Accept'_ examples in 2D to show how the choice of \(w\) affects the fraction of the target measure \(\) to which the probability mass of the input \(\) is mapped. In both cases, measure \(\) is _Gaussian_. In Appendix B, we demonstrate how _other methods_ perform on these _'Wi-Fi'_ and _'Accept'_ toy examples.

In _'Wi-Fi'_ experiment (Fig. 7), target \(\) contains 3 arcs. We provide the learned IT maps for \(w[1,,3]\). The results show that by varying \(w\) it is possible to control the fraction of \(\) to which the mass of \(\) will be mapped. In Fig. 7, we see that for \(w=1\) our IT method learns all \(3\) arcs. For \(w=\), it captures 2 arcs, i.e., \(\) of \(\). For \(w=3\), it learns 1 arc which corresponds to \(\) of \(\). In _'Accept'_ experiment (Fig. 2), target \(\) is a two-line text. Here we put \(w=2\) and, as expected, our method captures only one line of the text which is the closest to \(\) in \(^{2}\).

### Unpaired Image-to-image Translation

Here we learn IT maps between various pairs of datasets. We test \(w\!\!\{1,2,4,8\}\) in all experiments. For completeness, we consider _bigger weights_\(w\{16,32\}\) in Appendix G.4.

Figure 7: Incomplete transport (IT) maps learned by our Algorithm 1 in _’Wi-Fi’_ experiment.

**Image datasets.** We utilize the following publicly available datasets as \(,\): celebrity faces , aligned anime faces3, flickr-faces-HQ , comic faces4, Amazon handbags from LSUN dataset , shoes , textures  and chairs from Bonn furniture styles dataset . The sizes of datasets are from 5K to 500K samples. We work with \(64 64\) and \(128 128\) images.

**Train-test split.** We use 90% of each dataset for training. The remaining 10% are held for test. All the presented qualitative and quantitative results are obtained for test images.

**Experimental results.** Our evaluation shows that with the increase of \(w\) the images \((x)\) translated by our IT method become more similar to the respective inputs \(x\) w.r.t. \(^{2}\). In Table 0(a), we quantify this effect. Namely, we show that the test transport cost \(}}_{n=1}^{N_{}}cx,(x)\) decreases with the increase of \(w\) which empirically verifies our Theorem 2.

We qualitatively demonstrate this effect in Fig. 0(b), 0(a), 0(a) and 0(a). In _celeba_ (female) \(\)_anine_ (Fig. 0(b)), the hair and background colors of the learned anime images become closer to celebrity faces' colors with the increase of \(w\). For example, in the 4th column of Fig. 0(b), the anime hair color changes from green to brown, which is close to that of the respective celebrity. In the 6th column, the background is getting darker. In _handbag\(\)shoes_ (Fig. 0(a)), the color, texture and size of the shoes become closer to that of handbag. Additionally, for this experiment we plot the projections of the learned IT maps to the first 2 principal components of \(\) (Fig. 0(a)). We see that projections are close to \(\) for \(w=1\) and become closer to \(\) with the increase of \(w\). In _ffhq\(\)comics_, the changes mostly affect facial expressions and individual characteristics. In _textures\(\)chairs_, the changes are mostly related to chairs' size which is expected since we use pixel-wise \(^{2}\) as the cost function. _Additional qualitative results_ are given in Appendix G.5 (Fig. 24, 25, 9a).

For completeness, we measure test FID  of the translated samples, see Table 0(b). We do not calculate FID in the _handbag\(\)chairs_ experiment because of too small sizes of the test parts of the datasets (500 textures, 2K chairs). However, we emphasize that FID is not representative when \(w>1\). In this case, IT maps learn by construction only a part of the target measure \(\). At the same time, FID tests how well the transported samples represent the entire target distribution and is very sensitive to mode dropping [48, Fig. 0(b)]. Therefore, while the cost decreases with the growth of \(w\), FID, on the contrary, increases. This is expected since IT maps to smaller part of \(\). Importantly, the visual quality of the translated images \((x)\) is not decreasing.

Table 1: Test \(^{2}\) cost and FID of our learned IT maps.

Figure 9: Unpaired Translation with our Incomplete Transport.

In Appendix C, we _compare_ our IT method with other image-to-image translation methods and show that IT better preserves the input-output similarity.

## 6 Potential Impact

**Inequality constraints for generative models.** The majority of optimization objectives in generative models (GANs, diffusion models, normalizing flows, etc.) enforce the _equality_ constraints, e.g., \(T=\), where \(T\) is the generated measure and \(\) is the data measure. Our work demonstrates that it is possible to enforce _inequality_ constraints, e.g., \(T w\), and apply them to a large-scale problem. While in this work we primarily focus on the image-to-image translation task, we believe that the ideas presented in our paper have several visible prospects for further improvement and applications. We list them below.

**(1) Partial OT.** Learning alignments between measures of _unequal_ mass is a pervasive topic which has already perspective applications in biology to single-cell data [66; 47; 19]. The mentioned works use unbalanced OT . This is an unconstrained problem where the mass spread is softly controlled by the regularization. Therefore, may be hard to control how the mass is actually distributed. Using partial OT which enforces hard inequality constraints might potentially soften this issue. Our IT problem (12) is a particular case of partial OT (4). We believe that our study is useful for future development of partial OT methods.

**(2) Generative nearest neighbors.** NNs play an important role in machine learning applications such as, e.g., image retrieval . These methods typically rely on fast _discrete_ approximate NN [50; 35] and perform _matching_ with the latent codes of the train samples. In contrast, nowadays, with the rapid growth of large generative models such as DALL-E , CLIP , GPT-3 , it becomes relevant to perform _out-of-sample_ estimation, e.g., map the latent vectors to _new_ vectors which are not present in the train set to generate _new_ data. Our IT approach (for \(w\)) is a theoretically justified way to learn approximate NN maps exclusively from samples. We think our approach might acquire applications here as well, especially since there already exist ways to apply OT in latent spaces of such models .

**(3) Robustness and outlier detection.** Our IT aligns the input measure \(\) only with a part of the target measure \(\). This property might be potentially used to make the learning robust, e.g., ignore outliers in the target dataset. Importantly, the choice of outliers and contamination level are tunable via \(c(x,y)\) and \(w\), but their selection may be not obvious. At the same time, the potential \(f^{*}\) vanishes on the outliers, i.e., samples in \(()\) to which the mass is not transported.

**Proposition 4** (The potential vanishes at outliers).: _Under the assuptions of Theorem 5, the equality \(f^{*}(y)=0\) holds for all \(y()(T^{*})\)._

We empirically illustrate this statement in Appendix A. As a result of this proposition, a possible byproduct of our method is an outlier-score \(f^{*}(y)\) for the target data. Such applications of OT are promising and there already exist works [52; 7; 54] developing approaches to make OT more robust.

**Limitations, societal impact**. We discuss _limitations_, _societal impact_ of our study in Appendix A.

ACKNOWLEDGEMENTS. This work was partially supported by Skoltech NGP Program (Sokitech-MIT joint project).