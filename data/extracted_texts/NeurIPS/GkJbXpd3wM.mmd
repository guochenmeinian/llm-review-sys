# Active Set Ordering

Quoc Phong Nguyen1,3, Sunil Gupta1,

**Svetha Venkatesh1, Bryan Kian Hsiang Low2, Patrick Jaillet3**

1Applied Artificial Intelligence Institute, Deakin University, Australia

2School of Computing, National University of Singapore, Singapore

3LIDS and EECS, Massachusetts Institute of Technology, USA

qphongmp@gmail.com, sunil.gupta@deakin.edu.au,

svetha.venkatesh@deakin.edu.au, lowkh@comp.nus.edu.sg, jaillet@mit.edu

###### Abstract

In this paper, we formalize the active set ordering problem, which involves actively discovering a set of inputs based on their orderings determined by expensive evaluations of a blackbox function. We then propose the _mean prediction_ (MP) algorithm and theoretically analyze it in terms of the regret of predicted pairwise orderings between inputs. Notably, as a special case of this framework, we can cast _Bayesian optimization_ as an active set ordering problem by recognizing that maximizers can be identified solely by comparison rather than by precisely estimating the function evaluations. As a result, we are able to construct the popular _Gaussian process upper confidence bound_ (GP-UCB) algorithm through the lens of ordering with several nuanced insights. We empirically validate the performance of our proposed solution using various synthetic functions and real-world datasets.

## 1 Introduction

In real-world applications, we often encounter the problem of estimating an unknown function, known as a _blackbox function_, (i.e., those without closed-form expressions or derivatives) using their expensive and noisy evaluations. Under these circumstances, an efficient sequential process of evaluating the function is desired. On one extreme, _experimental design_ (ED) aims to estimate the function in its entire input domain, e.g., by decreasing the uncertainty of the function globally in Bayesian ED . On the other extreme, the renowned _Bayesian optimization_ (BO) targets inputs with the extreme function values such as the maximizers and the minimizers .

While the connection between ED and BO is studied in the classic work of , we still lack a problem formulation that strikes a balance between the prohibitively expensive process of estimating the entire function globally in ED and the lack of information about the function away from extreme locations in BO. One may consider a related problem, called _level set estimation_ (LSE), which focuses on estimating inputs with function evaluations above or below a given (or implicit) threshold . However, without domain knowledge of the blackbox function, it is easy to set a threshold that leads to undesirably large or small level sets.

Let us consider an environmental monitoring problem of estimating a chemical concentration in a field. The blackbox function is the mapping from locations of the field to the chemical concentration measurement. It can be of a greater interest to estimate the maximizers, the minimizers, the top-\(k\) locations (with the highest chemical concentration) and the bottom-\(k\) locations. On one hand, these estimates provide more information about the blackbox function than just the maximizers or minimizers in BO. On the other hand, they may require less resource (i.e., evaluations of the blackbox function) than estimating the entire function in ED. Besides, as the top-\(k\) locations consist of exactly \(k\) locations in the field, it circumvents the issue of undesirably large or small level sets in LSE.

Our main contribution in this paper is to formulate the above challenge and resolve it with a theoretically grounded solution. Specifically, we propose _the active set ordering problem_ to capture the above scenario (Sec. 2.1). It aims to estimate subsets of the input domain that are defined based on pairwise comparisons/orderings between the blackbox function evaluations.1 These subsets include the maximizers, the minimizers, and the top-\(k\) inputs with the highest function evaluations. Like Bayesian ED, BO, and LSE, we adopt the pool-based active learning setting  in constructing a solution that sequentially selects a _sampling input_ from the domain at each iteration. The knowledge from observing function evaluations at the sampling inputs helps predicting the subsets of interest and directs the algorithm to select the next sampling input. To facilitate the presentation of our method, we begin with the building block of our ordering-based problem: pairwise comparison/ordering between function evaluations in Sec. 3. Specifically, we propose a new kind of regret to quantify the loss of a pairwise ordering (Sec. 3.1), a prediction of the top-\(k\) inputs based on only the posterior mean (Sec. 3.2), and a sampling strategy that is equipped with a theoretical performance guarantee for the proposed prediction (Sec. 3.3). Subsequently, these concepts of the regret, the prediction, and the sampling strategy are extended to orderings between sets, which ultimately addresses the active set ordering problem in Sec. 4. Notably, the regret simplifies to the well-known regret in BO (Remark 4.1). Hence, we recover both the theoretical analysis and the GP-UCB algorithm  when considering a special case of our problem setting (Remark 4.5). In Sec. 5, we empirically validate the performance of our solution using several synthetic functions and real-world datasets.

## 2 Preliminaries and Problem Statement

### Top-\(k\) set

Adopting an assumption in existing _level set estimation_ (LSE) works [1; 8], we consider a blackbox function \(f:\) where the domain \(\) is a finite set of \(n\) elements in \(^{d}\). Let \(^{c}\) denote _the complement_ of any subset \(\). In this paper, _the ordering between inputs_ are determined with respect to their corresponding blackbox function evaluations. Hence, we use the term "the ordering between \(\) and \(^{}\)" and "the ordering between \(f()\) and \(f(^{})\)" interchangeably.

**Definition 2.1** (Top-\(k\) set).: The _top-\(k\) set_, denoted as \((k)\), is the set of \(k\) inputs with the highest function evaluations. Specifically, \(|(k)|=k\) and \((k)\), \(^{}^{c}(k)\), \(f() f(^{})\).

In this work, we propose _the active set ordering problem_ to estimate the top-\(k\) set \((k)\) of a blackbox function \(f\) by efficiently gathering _noisy function evaluations_ in a sequential manner. Furthermore, it includes the _Bayesian optimization_ (BO) problem when \(k=1\) because the top-\(1\) set \((1)\) contains a maximizer of \(f\).

### Gaussian Process

The noisy function evaluation mentioned in the previous section is denoted as \(y() f()+()\) where the noise \(()(0,_{n}^{2})\) is a Gaussian random variable with a known (or estimated) variance \(_{n}^{2}\). To obtain the posterior distribution of the unknown function \(f\) given these noisy evaluations, we model \(f\) using a _Gaussian process_ (GP), that is, every subset of \(\{f()\}_{}\) follows a multivariate Gaussian distribution . A GP is fully specified by its prior mean and its kernel \(k_{,^{}}(f(),f( ^{}))\) which measures the covariance between function values. Let \(_{t}\) denote the set of sampling inputs in the first \(t-1\) iterations. Then, given \(_{_{t}}(y())_{_{t}}\), the predictive distribution of any function evaluation \(f()\) follows a Gaussian distribution with the following mean and variance:

\[_{t}()_{t}()^{}(_{t }+_{n}^{2})^{-1}_{_{t}}_{t}^ {2}() k(,)-_{t}( )^{}(_{t}+_{n}^{2})^{-1}_{t}()\]

where \(_{t}()(k(,^{}))_{ ^{}_{t}}\), \(_{t}(k(,^{}))_{, ^{}_{t}}\), and \(\) is the identity matrix .

Assuming \(f\) belongs to a _reproducing kernel Hilbert space_ with its norm bounded by \(B>0\), due to , we have the following confidence bound of \(f()\).2

**Lemma 2.2**.: _Pick \((0,1)\) and set \(_{t}=(B+_{n}+1+ 1/)})^{2}\). Then, the following event happens with probability of at least \(1-\),_

\[,\; t 1,\;l_{t}() f( ) u_{t}()\]

_where \(l_{t}()_{t}()-_{t}^{1/2}_{t}( )\), \(u_{t}()_{t}()+_{t}^{1/2}_{t}( )\), and \(_{t-1}_{:|A|=t-1}I(_{A};_{A})\) is the maximum information gain of \(_{A}\{f()\}_{ A}\) through observing \(_{A}\) over all subsets \(A\) of size \(|A|=t-1\)._

To ease notational clutter, we denote the above confidence interval of \(f()\) as \(_{t}()[l_{t}(),u_{t}()]\) and its length as \(|_{t}()| u_{t}()-l_{t}()\).

## 3 Active Pairwise Ordering: \(n=2\)

From Definition 2.1, _pairwise orderings_ (or pairwise comparisons) are the building blocks of our active set ordering problem. Hence, to facilitate the exposition of the key ideas, let us begin with a simplistic setting where the input domain \(\) consists of only \(n=2\) inputs, i.e., \(=\{},}^{}\}\) and \(f(}) f(}^{})\). The problem is to determine the top-\(1\) set \((1)\), i.e., the maximizer of \(f\) (equivalently, the minimizer of \(f\)). In essence, the goal is to check if \(f(})>f(}^{})\) by strategically collecting noisy evaluations \(y(})\) and \(y(}^{})\). In particular, at iteration \(t\), the algorithm proposes a _sampling input_\(_{t}\) to obtain a noisy evaluation \(y(_{t})\). Then, the GP posterior distribution of \(f\) is updated and used to construct a predicted ordering between \(}\) and \(}^{}\) (i.e., the ordering between \(f(})\) and \(f(}^{})\)). The problem boils down to the strategy of selecting the sampling input \(_{t}\) such that _a performance metric_ of the predicted ordering between \(f(})\) and \(f(}^{})\) is satisfactory. In the next section, we introduce a regret definition to serve as a performance metric.

### Regret

Let us denote the (unknown) _true ordering_ between \(}\) and \(}^{}\) according to the evaluations of the blackbox function \(f\) as \(_{*}\):

\[_{*}(},}^{}) _{f(}) f(}^{})}\] (1)

where the indicator function \(_{f(}) f(}^{})}=1\) if \(f(}) f(}^{})\) and \(0\) otherwise. For any ordering \(:\{(},}^{})\}\{0,1\}\), we define the following regret of \((},}^{})\):

\[r_{(},}^{})}(0, (2(},}^{})-1)(f(} ^{})-f(}))).\] (2)

In particular, \(r_{(},}^{})=1}=(0,f(}^{})-f(}))\) and \(r_{(},}^{})=0}=(0,f(})-f(}^{}))\). The rationale is to ensure poor performance leads to large regret: The regret is \(|f(})-f(}^{})|\) (which increases as the gap between \(f(})\) and \(f(}^{})\) increases) if the ordering \(\) does not align with the true ordering, i.e., \((},}^{})_{*}(},}^{})\). On the contrary, the regret is \(0\) (i.e., the best performance) if \((},}^{})=_{*}(}, }^{})\).

However, the blackbox function \(f\) renders the evaluation of \(r_{(},}^{})}\) impossible. Thus, we resort to relying on the GP posterior distribution of \(f\) at iteration \(t\) to construct an upper bound of the above regret in the following lemma (proof in Appendix A).

**Lemma 3.1**.: _For all \(t 1\), let us define_

\[_{(},}^{})}^{(t)} (0,u_{t}(}^{})-l_{t}(} ))&(},}^{})=1\\ (0,u_{t}(})-l_{t}(}^{}))&( },}^{})=0\.\] (3)

_Then, \(_{(},}^{})}^{(t)}\) is an upper confidence bound of the regret \(r_{(},}^{})}\), i.e.,_

\[P( t 1,\;r_{(},}^{})} _{(},}^{})}^{(t)})  1-\]

_where \(\) is as defined in Lemma 2.2._

### Prediction

**Definition 3.2** (Predicted pairwise ordering \(_{_{t}}\)).: Given the above upper bound \(_{(},}^{})}^{(t)}\) of the regret, we would like to make a prediction \(_{_{t}}\) that minimizes \(_{(},}^{})}^{(t)}\). Therefore, \(_{_{t}}\) is defined as follows

\[_{_{t}}(},}^{}) *{argmin}_{a\{0,1\}}_{(},}^{})=a}^{(t)}\,.\] (4)

As a result, the upper confidence bound of the regret in (3) is minimized at \((},}^{})=_{_{t}}(},}^{})\) and its minimum value is

\[_{_{_{t}}(},}^{})}^{(t)}= (0,(u_{t}(}^{})-l_{t}(}), u_{t}(})-l_{t}(}^{})))\.\] (5)

We note that the upper confidence bound of the regret can be interpreted as a measure of the approximation quality or the uncertainty reduction as discussed in the following two remarks.

_Remark 3.3_ (Approximation quality).: Since \(_{_{_{t}}(},}^{})}^{(t)} r _{_{_{t}}(},}^{})}\) for all \(t 1\) with probability of at least \(1-\), the regret incurred by the ordering \(_{_{t}}\) cannot exceed _the worst-case regret_\(_{_{_{t}}(},}^{})}^{(t)}\) as shown in Fig. 0(a). Hence, if \(|f(})-f(}^{})|>_{_{_{t}}( },}^{})}^{(t)}\), \(_{_{t}}(},}^{})\) is the true ordering, i.e., \(_{_{t}}(},}^{})=_{*}(},}^{})\), with probability of at least \(1-\).

_Remark 3.4_ (Minimum uncertainty reduction).: In Fig. 0(b), one can interpret \(_{_{_{t}}(},}^{})}^{(t)}\) as the minimum amount that the confidence intervals \(_{t}(})\) and \(_{t}(}^{})\) (representing the uncertainty) reduce so that \(r_{_{_{t}}(},}^{})}=0\) with probability of at least \(1-\).

Moreover, the prediction \(_{_{t}}\) can be obtained using only the GP posterior mean (proof in Appendix B), which explains the name of our approach: _mean prediction_ (MP) and the notation \(_{_{t}}\).

**Lemma 3.5** (Mean prediction).: _The predicted ordering \(_{_{t}}\) defined in (4) can be determined from the GP posterior mean_

\[_{_{t}}(},}^{})=_{ _{t}(})_{t}(}^{})}\.\] (6)

### Sampling Strategy

Given the regret in Sec. 3.1 and the predicted ordering \(_{_{t}}(},}^{})\) in Sec. 3.2, we would like to select a _sampling input_\(_{t}=\{},}^{}\}\) such that the regret \(r_{_{_{t}}(},}^{})}\) of the predicted ordering \(_{_{t}}(},}^{})\) reduces quickly.

While \(r_{_{_{t}}(},}^{})}\) is unknown, it is bounded by \(^{(t)}_{_{_{t}}(},}^{})}\) with probability of at least \(1-\). Hence, to reduce \(r_{_{_{t}}(},}^{})}\), we aim to reduce \(^{(t)}_{_{_{t}}(},}^{})}\). It is also noted that observing \(y(_{t})\) decreases the confidence interval \(|_{t}(_{t})|\). Hence, to induce the reduction in the regret \(r_{_{_{t}}(},}^{})}\) through observing \(y(_{t})\), we select \(_{t}\) such that its confidence interval \(|_{t}(_{t})|^{(t)}_{_{_{t}}(},}^{})}\) (which guarantees that \(|_{t}(_{t})| r_{_{_{t}}(}, }^{})}\) with probability of at least \(1-\)). For instance, choosing \(_{t}=}\) in the left plot of Fig. 1a satisfies this condition, but it does not in the right plot of Fig. 1a. In the following lemma, we show that the following \(4\) choices of the sampling input satisfy the proposed condition (see Appendix C).

**Lemma 3.6**.: _Let \(_{t}\{}\ }\ }^{},}\,\,}^{ },}}^{},} }^{}\}\) denote a set3 of inputs at iteration \(t\) where_

\[}\ \ }^{} *{argmax}_{\{},}^{}\}}u_{t}() }\ \,}^{} *{argmin}_{\{}, }^{}\}}l_{t}()\] \[}}^{} *{argmax}_{\{},}^{}\}}|_{t}()| }}^{} *{argmin}_{\{} }^{},}}^{ }\}}|_{t}()|\.\]

_For any \(_{t}_{t}\), \(|_{t}(_{t})|^{(t)}_{_{_{t}}(},}^{})}\)._

**Theorem 3.7**.: _By sampling the input \(_{t}\) following Lemma 3.6, we obtain the following regret bound_

\[P( T 1,\ (_{t})_{t=1}^{T}_{t=1}^{T} _{t},\ R_{T}_{t=1}^{T}r_{_{_{t}}(},}^{})}( _{T}})) 1-\]

_where \(_{T}\), \(_{T}\), and \(\) are as defined in Lemma 2.2._

_Remark 3.8_ (Sublinear cumulative regret).: If \(_{T}\) is sublinear, our average cumulative regret is sublinear. This requirement is similar to most BO and LSE algorithms. It is noted that \(_{T}\) is sublinear for many popular kernels. For instance, \(_{T}=(( T)^{d+1})\) for the _squared exponential_ (SE) kernel as discussed in . In this case, our cumulative regret bound \(R_{T}^{*}(})\) is the same as that of GP-UCB  (where \(^{*}()\) denotes asymptotic expressions up to dimension-independent logarithmic factors and is the dimension of the input).

We defer the pseudocode to the next section when \(n 2\).

## 4 Active Set Ordering: \(n 2\)

In this section, we utilize the results in Sec. 3 to present the _mean prediction_ (MP) algorithm for the active set ordering problem with \(n 2\).

### Regret

When \(\) consists of \(n>2\) inputs, there are multiple pairwise orderings between inputs in \(\). We overload the ordering notation \(_{*}\) of the true pairwise ordering between \(2\) inputs in (1) to the ordering between \(2\) sets as follows: for any subsets \(_{0}\) and \(_{1}_{0}^{c}\),

\[_{*}(_{0},_{1})=1&_{0},\ ^{}_{1},\ _{*}(,^{})=1\\ 0&_{0},\ ^{} _{1},\ _{*}(,^{})=0\.\] (7)

It is noted that \(_{*}(_{0},_{1})\) remains undefined if the two cases above are not satisfied. However, this situation does not arise in our solution. We define the regret of a set ordering (i.e., multiple pairwise orderings) as the maximum regret of all pairwise orderings:

\[r_{(_{0},_{1})=i}_{(, ^{})_{0}_{1}}r_{(, ^{})=i} i\{0,1\}\]

where \(r_{(,^{})}\) is defined in (2) and \(_{0}_{1}\) is the Cartesian product of \(_{0}\) and \(_{1}\), i.e.,

\[r_{(_{0},_{1})=i}=(0,(2i-1)_{(, ^{})_{0}_{1}}(f(^{ })-f())) i\{0,1\}.\] (8)_Remark 4.1_.: It is noted that \(r_{(_{0},_{1})}\) coincides with the well-known regret in BO when we consider the problem of predicting a maximizer of \(f\). In particularly, predicting \(}_{*}\) as a maximizer of \(f\) is equivalent to predicting the set ordering \((\{}_{*}\},\{}_{*}\})=1\). Its regret is \(r_{(\{}_{*}\},\{}_{*}\}) =1}=_{}f()-f(}_{*})\) as shown in Appendix E.1.

Following the upper confidence bound of the regret of pairwise orderings in (3), we show in Appendix F that with probability of at least \(1-\), for all \(t 1\) and for all subsets \(_{0}\), \(_{1}_{0}^{c}\), \(r_{(_{0},_{1})}_{(_{0}, _{1})}^{(t)}\) where

\[_{(_{0},_{1})}^{(t)}_{(,^{})_{0}_{1}}_{( ,^{})=(_{0},_{1})}^{(t)}\;.\]

### Prediction

In this section, we generalize the prediction in Sec. 3.2 to set orderings. From Lemma 3.5, there is no contradiction in the pairwise orderings \(_{_{t}}(,^{})\) (defined in (4)) for all \(\{,^{}\}\). In other words, the transitivity property holds for the binary relation \(_{_{t}}\) as shown in Appendix G.

**Definition 4.2** (Predicted top-\(k\) set \(_{_{t}}(k)\)).: Let \(_{_{t}}(k)\) be a subset of \(\) such that

\[|_{_{t}}(k)|=k\;,_{_{t}}(_{_{t}}(k), _{_{t}}^{c}(k))=1\] (9)

where the set ordering \(_{_{t}}(_{_{t}}(k),_{_{t}}^{c}(k))\) is obtained by substituting \(_{*}\) with the pairwise ordering \(_{_{t}}\) (see Definition 3.2) in (7). From Lemma 3.5, \(_{_{t}}(k)\) is basically the set of \(k\) inputs with the highest GP posterior mean values.

As \(_{_{t}}(_{_{t}}(k),_{_{t}}^{c}(k))=1\) implies that \(_{_{t}}(,^{})=1\) for all \((,^{})_{_{t}}(k)_ {_{t}}^{c}(k)\), the upper confidence bound of the regret is

\[_{_{_{t}}(_{_{t}}(k),_{_{t}}^{c}(k))}^{( t)}=_{(,^{})_{_{t}}(k) _{_{t}}^{c}(k)}_{_{_{t}}(,^{ })}^{(t)}\;.\] (10)

### Sampling Strategy

Like in Sec. 3.3, our key idea is to select \(_{t}\) such that the length \(|_{t}(_{t})|\) of its confidence interval bounds \(_{_{_{t}}(_{_{t}}(k),_{_{t}}^{c}(k))}^{( t)}\) (in (10)). Since \(_{_{_{t}}(_{_{t}}(k),_{_{t}}^{c}(k))}^{( t)}\) is the maximum upper confidence bound of the regret of all pairwise orderings involved in defining \(_{_{t}}(k)\), we first determine the input pair \((}_{t},}_{t}^{})\) that incurs the maximum upper confidence bound of the regret. This is also the input pair that \(_{_{t}}\) most likely makes a mistake, following the intuition from .

\[(}_{t},}_{t}^{})*{ argmax}_{(,^{})_{_{t}}(k) _{_{t}}^{c}(k)}_{_{_{t}}(,^{ })}^{(t)}\;.\] (11)

It is noted that \((}_{t},}_{t}^{})\) is constructed from an input in the predicted top-\(k\) set \(_{_{t}}(k)\) and an input in its complement \(_{_{t}}^{c}(k)\). As the estimation of the top-\(k\) set improves, we expect these \(2\) inputs to be at both sides of the boundary of the top-\(k\) set: inside the top-\(k\) set vs. outside the top-\(k\) set.

Then, extended from Lemma 3.6, the following lemma shows that the above desirable property is satisfied by choosing the sampling input \(_{t}\) from any inputs in the set \(}_{t}\{}_{t}\;\;}_{t}^{},}_{t}\;\;}_{t}^{}, }_{t}\;\;}_{t}^{},}_{ t}\;\;}_{t}^{}\}\) (defined in Lemma 3.6).

**Lemma 4.3**.: _For any \(_{t}}_{t}\), \(|_{t}(_{t})|_{_{_{t}}(_{_{t}}(k), _{_{t}}^{c}(k))}^{(t)}\)._

The proof is shown in Appendix H. As a result, the cumulative regret incurred by choosing \(_{t}\) in Lemma 4.3 is bounded in the following theorem (proof in Appendix I).

**Theorem 4.4**.: _By sampling \(_{t}\)following Lemma 4.3, we obtain the following cumulative regret bound_

\[P( T 1,\;(_{t})_{t=1}^{T}_{i=1}^{T}}_{t},\;R_{T,k}_{t=1}^{T}r_{_{_{t}}(_{ _{t}}(k),_{_{t}}^{c}(k))}( _{T}})) 1-\]

_where \(_{T}\), \(_{T}\), and \(\) are as defined in Lemma 2.2._We call the algorithm that makes prediction using the GP posterior mean and selects the sampling input \(_{t}\) following Lemma 4.3 the _mean prediction_ (MP) algorithm. Its pseudocode is shown in Algorithm 1. Theorem 4.4 indicates that MP incurs a sublinear cumulative regret for several commonly used kernels with sublinear \(_{T}\).

```
1:\(\), \(_{0}\), \(k\), \(T\)
2:for\(t=1\)to\(T\)do
3: Update GP posterior belief: \(\{_{t}()\}_{},\{_{t}() \}_{}\).
4: Construct \(_{_{t}}(k)\) as top-\(k\) inputs with the highest values of \(_{t}\). \(\) Prediction
5:\((}_{t},}_{t}^{})=*{argmax} _{(,^{})_{_{t}}(k)_{_{t}}^{c}(k)}_{_{_{t}}(,^{})}^{(t)}\)
6: Select \(_{t}\{}_{t}\;}_{t}^{},}_{t}\,}_{t}\,}_{t}\,}_{t}^{},}_{t}\,\,}_ {t}^{}\}\). \(\) Sampling input
7:\(_{t}(_{t})(_{t-1})\{y( _{t})\}\)
8:endfor
9: Update GP posterior belief: \(\{_{T+1}()\}_{},\{_{T+1}()\}_{}\).
10: Construct \(_{_{T+1}}(k)\) as top-\(k\) inputs with the highest values of \(_{T+1}\).
11:return\(_{_{T+1}}(k)\). ```

**Algorithm 1** Mean Prediction (MP) for Active Set Ordering

**Remark 4.5** (Bayesian optimization as an active set ordering problem with \(k=1\)).: We show in Appendix E.2 that when \(k=1\), \(}_{t}\;\;}_{t}^{}*{ argmax}_{}u_{t}()\), which is the sampling input in the GP-UCB algorithm . Additionally, as discussed in Sec. 4.1, the regret \(r_{((1),^{*}(1))}\) is the well-known regret in BO. Hence, we recover both the GP-UCB algorithm and its regret bound when \(k=1\) (although we consider the regret of the prediction rather than that of the sampling input). Moreover, this new construction of GP-UCB leads to some subtle insights. Firstly, while the GP posterior mean has been used in computing the inference regret of entropy search methods , there has not been any theoretical justification for using the posterior mean. In contrast, the theoretical analysis in our work justifies the use of the maximizer of the GP posterior mean as an estimate of the maximizer of \(f\). Secondly, by predicting the maximizer using the GP posterior mean, \(}\;\;}^{}\) is not the only sampling input that achieves a sublinear cumulative regret. In fact, there are other choices of the sampling input as shown in Lemma 4.3. Similarly, we note that the LCB algorithm to find the minimizer of a blackbox function can be recovered by setting \(k=n-1\) and \(_{t}=}\,\,}^{}\).

_Remark 4.6_ (Lower bound of active set ordering problem).: Let the lower bound of the active set ordering problem be the lower bound of the cumulative regret of the worst-case problem instance over all possible values of \(k\). Then, it should be at least as large as the lower bound of the special case where \(k=1\), which is the BO problem according to Remark 4.5. Furthermore, BO has known lower bounds for several common kernels, e.g., for the SE kernel, the lower bound of the cumulative regret is \((})\). Hence, the lower bound of the active set ordering problem is at least \((})\). Additionally, similar to Remark 3.8, the cumulative regret of our solution in Theorem 4.4 is bounded by \(R_{T}^{*}(})\). Hence, it matches the lower bound up to the replacement of \(d/2\) by \(2d+O(1)\).

_Remark 4.7_.: Updating the GP posterior belief incurs \((|_{t}|^{3}+n|_{t}|^{2})\) (including \((|_{t}|^{3})\) for training and \((n|_{t}|^{2})\) for prediction). Given the GP posterior belief, Algorithm 1 involves the following \(2\) major steps. First, in line 3 of Algorithm 1, it takes \((n k)\) to find the top-\(k\) inputs \(_{_{t}}(k)\) by using a max heap of size \(k\) and scanning through the GP posterior mean of all \(n\) inputs. Second, in line 4 of Algorithm 1, it takes \((k(n-k))\) to scan through the elements in \(_{_{t}}(k)_{_{t}}^{c}(k)\). Therefore, an iteration of Algorithm 1 takes \((|_{t}|^{3}+n|_{t}|^{2}+n k+k(n-k))\).

_Remark 4.8_ (Active multiple set ordering).: Let us consider the problem of estimating \(m\) top-\(k\) sets: \((k_{1}),(k_{2}),,(k_{m})\) simultaneously (motivated in Sec. 1). This problem is analogous to finding \(k\) contour lines of a blackbox function, where each contour line represents the boundary between \((k_{i})\) and its complement \(^{c}(k_{i})\). To solve this problem, we define the following input pair

\[(}_{t},}_{t}^{})*{ argmax}_{(,^{})(_{i=1}^{m}_{_{t}}(k_{i}) _{_{t}}^{c}(k_{i}))}_{_{_{t}}(, ^{})}^{(t)}\;.\] (12)

In other words, we aim to reduce the maximum regret incurred by the predicted pairwise orderings in all \(m\) top-\(k\) sets. Given \((}_{t},}_{t}^{})\) in (12), MP proceeds by sampling the input \(_{t}\) according to Lemma 4.3, i.e., \(_{t}\{}_{t}\;\;}_{t}^{}, }_{t}\,\,}_{t}^{},}_{t} \,\,}_{t},}_{t}\,\,}_{t}^{ }\}\). The approach is elaborated in Appendix J.

We also note that active multiple set ordering is able to find both maximizers \((1)\) and minimizers \(^{c}(n-1)\) simultaneously, a problem has not been studied in GP-UCB .

## 5 Experiments

### Active Set Ordering

In this section, we validate the empirical performance of our MP algorithm with different choices of the sampling input in Lemma 4.3: \(}_{t}\ }_{t}^{}\), \(}_{t}}_{t}^{}\), \(}_{t}}_{t}^{}\), and \(}_{t}}_{t}^{}\) by comparing with \(2\) baselines: an uncertainty sampling approach, called _Var_, that selects the sampling input with the highest GP posterior variance, i.e., \(_{t}*{argmax}_{}_{t }^{2}()\), and a baseline, called _Rand_, that selects the sampling input at random. The regret \(r_{_{_{t}}(_{_{t}}(k),^{c}_{_{t}}(k))}\) is used to measure the performance of each algorithm, i.e., the prediction of the top-\(k\) set consists of the \(k\) inputs with the highest GP posterior mean.

To begin with, we visualize sampling inputs and the accuracy of \(_{_{t}}(20)\) that come from our MP algorithm with \(_{t}=}_{t}}_{t}^{}\) and the Var algorithm in Fig. 2. In Fig. (a)a, the histogram shows that the sampling inputs are at the boundary of \((20)\). This is highly desirable as it is challenging to decide if an input at the boundary belongs to \((20)\). Similarly, the input pair in (11) also consists of inputs around this boundary (depicted as vertical orange lines). On the other hand, precisely estimating the function evaluations of inputs far from the boundary, e.g., inputs around \(=0.85\) (in \((20)\)) and inputs around \(=0.1\) (not part of \((20)\)) is unnecessary. We observe that the uncertainty of the GP posterior distribution at these inputs is high in Fig. (a)a. Hence, our MP algorithm is able to efficiently concentrate its sampling budget on important inputs at the boundary of the top-\(k\) set. Interestingly, this boundary serves as a contour line of the blackbox function, indicating that our solution could potentially be applied to estimate the contour line by specifying the proportion of the input domain where function evaluations exceed this contour. Regarding the Var algorithm (i.e., uncertainty sampling) in Fig. (b)b, the histogram shows that sampling inputs are distributed evenly across the input domain. It is because Var aims to reduce the uncertainty of the function evaluation throughout the input domain without considering the current predicted \(_{_{t}}(20)\). For example, it is inefficient to select sampling inputs far away from the boundary of \((20)\). It is observed that the estimation of function evaluations at the boundary of \((20)\) using Var is more uncertain than that using the MP algorithm given the same number of sampling inputs. This results in erroneously predicting certain inputs in \((20)\) (depicted as red dots) and overlooking several inputs in \((20)\) (depicted as black dots).4

We numerically report the performance using the proposed regret \(r_{_{_{t}}(_{_{t}}(k),^{c}_{_{t}}(k))}\). The experiments are conducted on \(4\) synthetic functions: a function sampled from a GP, Branin-Hoo function, Goldstein-Price function with a noise of \(_{n}=0.1\), and Hartmann-6D function with a noise of \(_{n}=0.01\). For the first three synthetic functions, the input domain is discretized into a set of \(100\) points, whereas for the Hartmann-6D function, it is discretized into a set of \(1000\) points. Motivated by environmental monitoring problems, we generate \(3\) active set ordering problems that

Figure 2: Plot of sampling inputs, GP posterior distribution, and the performance of (a) MP and (b) Var in estimating \((20)\) of a synthetic function. The comparison pair is \((}_{t},}_{t}^{})\) in (11). The histogram on the horizontal axis shows the frequency of sampling inputs in \(40\) iterations.

estimate the top-\(5\) set using the dataset of \(_{3}\) concentration in the Lake Zurich (downloaded from https://wldb.ilec.or.jp/Lake/EUR-06/datalist), the dataset of the phosphorus concentration in the Brooms Barn , and the dataset of the humidity in the Intel Lab (downloaded from https://db.csail.mit.edu/labdata/labdata.html). The environment field is discretized into a set of \(100\) locations in the experiments with the \(_{3}\) and humidity datasets and \(400\) locations in the experiment with the phosphorus dataset. The experiments are repeated \(15\) times to account for the randomness in the generation of the observations. Further details are provided in Appendix K. The average and the standard error of the regret are shown in Figs. 3:a-g. There is not any significant difference in the performance of MP with different sampling inputs in Lemma 4.3. Nevertheless, the MP algorithm with any choice of the sampling input in \(}_{t}\) outperforms the \(2\) baselines by converging to lower regret.

### Active Multiple Set Ordering

To empirically validate the performance of our MP algorithm in solving the problem of estimating multiple top-\(k\) sets, we consider the problem of estimating \((1)\) (i.e., maximizers), \((10)\), and \((20)\) simultaneously, i.e., \(k_{1}=1,k_{2}=10,k_{3}=20\) in Remark 4.8. We utilize the same set

Figure 3: Plots of the regret against the iteration in estimating (s:a-f) the top-\(5\) set \((5)\) and (m:a-f) multiple top-\(k\) sets: \((1)\), \((10)\), and \((20)\).

of synthetic functions and real-world environmental datasets in the previous section to compare the performance of MP with Var and Rand. The plot of the average and standard error of the maximum regret \(_{k\{1,10,20\}}(r_{_{_{t}}(_{_{t}}(k),_{ _{t}}^{c}(k))})\) over \(15\) repeated experiments are shown in Figs. 3m:a-g. The MP algorithm outperforms the other \(2\) baselines by converging to lower regret. In some active multiple set ordering experiments (e.g., in Figs. 3m:a, 3m:c), the performance gaps between MP and Var are smaller than those in the previous active set ordering experiments (e.g., Figs. 3s:a, 3s:c). It is because estimating multiple top-\(k\) sets requires more observations, which makes the performance of MP tend towards that of Var which estimates the entire function.

### Bayesian Optimization

When \(k=1\), the active set ordering problem reduces to the BO problem and a sampling input of our MP algorithm, i.e., \(_{t}=}_{t}}_{t}^{}\), is the same as that of GP-UCB . Therefore, this section empirically demonstrates the performance of MP with different sampling inputs (\(_{t}\{}_{t}}_{t}^{ },}_{t}}_{t}^{},}_{t}}_{t}^{}\}\) in solving BO. Our aim is not to show that MP achieves the state-of-the-art performance as a BO solver, but rather to demonstrate that it performs comparably to the well-known GP-UCB algorithm. In addition to comparing with GP-UCB (equivalently, MP with \(_{t}=}_{t}}_{t}^{}\)), we also compare with 3 classical BO solutions: _probability of improvement_ (PI) , _expected improvement_ (EI) , and _max-value entropy search_ (MES) . The average and the standard error of the regret \(r_{_{_{t}}(_{_{t}}(_{_{t}}(1),_{ _{t}}^{c}(1))}\) over \(15\) repeated experiments are shown in Fig. 4. We observe that MP performs comparably with the well-known GP-UCB algorithm (labelled as \(}_{t}}_{t}^{}\)). Expectedly, EI and MES outperform GP-UCB (and hence, MP) in some experiments such as in Figs. 4b and 4c.

## 6 Conclusion

This paper presents a new problem formulation, namely _active set ordering_, that aims to balance between the expensive estimation of the entire function in ED and that of only the maximizers in BO. We propose the _mean prediction_ (MP) algorithm to address this problem with a theoretical no-regret guarantee. Interestingly, BO can be framed as a special instance of active set ordering, which leads to several new subtle understandings regarding the predicted maximizer and other alternative sampling inputs. Last, the performance of MP is empirically evaluated using various synthetic functions and real-world datasets.