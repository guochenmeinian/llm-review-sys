# Gaussian Process Probes (GPP) for

Uncertainty-Aware Probing

 Zi Wang

Google DeepMind

&Alexander Ku

Google DeepMind

&Jason Baldridge

Google DeepMind

&Thomas L. Griffiths

Princeton University&Been Kim

Google DeepMind

Equal contribution. {wangzi, alexku, jasonbaldridge, beenkim}@google.com, tomg@princeton.edu. Our code can be found at https://github.com/google-research/gpax.

###### Abstract

Understanding which concepts models can and cannot represent has been fundamental to many tasks: from effective and responsible use of models to detecting out of distribution data. We introduce Gaussian process probes (GPP), a unified and simple framework for probing and measuring uncertainty about concepts represented by models. As a Bayesian extension of linear probing methods, GPP asks what kind of distribution over classifiers (of concepts) is induced by the model. This distribution can be used to measure both what the model represents and how confident the probe is about what the model represents. GPP can be applied to any pre-trained model with vector representations of inputs (e.g., activations). It does not require access to training data, gradients, or the architecture. We validate GPP on datasets containing both synthetic and real images. Our experiments show it can (1) probe a model's representations of concepts even with a very small number of examples, (2) accurately measure both epistemic uncertainty (how confident the probe is) and aleatory uncertainty (how fuzzy the concepts are to the model), and (3) detect out of distribution data using those uncertainty measures as well as classic methods do. By using Gaussian processes to expand what probing can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool for understanding and evaluating the capabilities of machine learning models.

## 1 Introduction

Deep learning models have become pervasive in a wide range of fields, yet they remain largely uninterpretable. As a result, gaining insight into which concepts deep learning models do or do not represent is of growing importance. This has led to the development of a family of methods known as "probes" (Alain and Bengio, 2016), which aim to understand the representations learned by these models. One common approach to probe the representations of a model is to train independent classifiers for each concept and measure their accuracy (Alain and Bengio, 2016; Kim et al., 2018; Belinkov, 2022). For example, if the classifier returns \(0.5\) probability (for the binary case) across a range of examples, we would reasonably conclude that the model does not represent the concept.

However, simply examining the output of a classifier may not adequately reflect the intricate nature of representing a concept. For example, we can think of two different scenarios that may result in the same classifier output. If you ask a person if an olive is a fruit, only about 60% will say yes (likewise 72% for avocado, 53% for pumpkin, and 40% for acorn) (McCloskey and Glucksberg, 1978). This does not indicate that people have a limited representation of the _fruit_ concept, but thatthere is intrinsic fuzziness in the label. By contrast, if you show somebody an olive in a martini glass and tell them it is a new concept called "blicket," they may be 60% sure in judging whether an olive on a tree is a blicket (as opposed to "blicket" having a specialized meaning that applies to cocktails). In this second case, the uncertainty is because they do not have enough observations to be certain about what the new concept is - a common problem in word learning (Xu and Tenenbaum, 2007).

We introduce a probabilistic approach based on Gaussian processes (GPs) (Rasmussen and Williams, 2006) that makes it possible to delineate different kinds of uncertainty in a model's concepts. Specifically, our goal is to probe a pre-trained model by constructing a probability distribution over possible binary classifiers of concepts induced by the model. Extracting this distribution allows the probe to produce two kinds of uncertainty measures - aleatory and epistemic (Fox and Ulkumen, 2011). This allows us to distinguish cases where 1) there is fundamental fuzziness of the label (aleatory uncertainty) or 2) it does not have enough observations (epistemic uncertainty), whereas a binary classifier would predict equal probability for both and provide no further explanatory power.

Our framework also provides a natural building block for answering many downstream questions, such as detecting out-of-distribution (OOD) examples. Using the resulting uncertainty measures, we show that our method can correctly probe what a model represents, and that it achieves competitive performance in OOD detection. We validate this approach using both synthetic and real images.

## 2 Related work

Distinguishing aleatory and epistemic uncertainty has a long history in fields including cognitive science (Howell and Burnett, 1978; Fox and Ulkumen, 2011), probability theory (Jaynes, 2003), philosophy (Hacking, 2006) and more. Our work builds upon Fox and Ulkumen (2011), which introduced a framework for distinguishing the two dimensions of uncertainty from a human perspective.

In machine learning, Gaussian processes (GPs) and Bayesian neural nets (BNN) (BNN) have enabled principled ways of predicting epistemic uncertainty, which led to better information-based decision making strategies (Houlsby et al., 2011; Hennig and Schuler, 2012; Wang and Jegelka, 2017; Gal et al., 2017; Wang et al., 2023). Motivated by regression problems in reinforcement learning, Depeweg et al. (2018) decomposed uncertainty in BNN to a conditional entropy term as aleatory uncertainty and the mutual information between model weights and predictions as epistemic uncertainty. Our decomposition simplifies these quantities2 and specializes them for classification. To our knowledge, aleatory uncertainty (fuzziness of concepts) has not been studied in the GP classification literature, and no previous work in the GP literature has been done to establish the correspondence between human perceptions of uncertainty and GP classification predictions.

Probing has been a popular approach to understand which concepts a model represents. There are mixed views of what probing should mean; either it is about estimating mutual information between representations and labels (Pimentel et al., 2020) or using minimum description length (MDL) (Voita and Titov, 2020) or representing the distance as squared Euclidean distance under a linear transformation (Hewitt and Manning, 2019), or using analysis with or without supervision (Saphra and Lopez, 2019; Wu et al., 2020). Others considered different types of probing: conditional probing that compares two concepts (Hewitt et al., 2021), probing that asks where the task-relevant information "emerges" (Kunz and Kuhlmann, 2022) or which concepts are "used" (Lasri et al., 2022), and finally, probing/localizing that enables editing (Meng et al., 2022). To the best of our knowledge, none of these probing methods offer measures of uncertainty.

GPP: Gaussian process probes

We introduce Gaussian process probes (GPP), a simple probabilistic method for probing that naturally distinguishes aleatory and epistemic uncertainty. We formally define the problem of probabilistic probing in SS3.1, followed by a review of the Beta Gaussian processes (GPs) that GPP builds upon in SS3.2. In SS3.3 and SS3.4, we detail our method and how it measures uncertainty.

### Problem formulation

Our goal is to probe a pre-trained model in a probabilistic manner by constructing a probability distribution over binary classifiers based on the model's vector representation of inputs (e.g., activations). Here, binary classifier refers to a function mapping a stimulus to the probability of a positive label.

**Assumption 1**.: _The model, \(\), contains basis functions that map from stimuli (e.g., images, texts) to a vector representation: \(:^{d}\), where \(\) is the space of stimuli and \(\) the space of \(d\)-dimensional real-valued vector representations._

For example, basis functions \(\) can map from images to last-layer activations of a convolutional neural network (CNN). The probabilistic probe treats basis functions \(\) as a feed-forward black box, and does not require accessing the training data, gradients, or architecture used during model pre-training.

In order to define a stochastic process to describe a distribution over classifiers, the data generating process is assumed as follows.

**Assumption 2**.: _There exists a stochastic process \(()\) parameterized by \(\), and a classifier \(h:\) distributed according to \(()\), such that the label \(y\{0,1\}\) for any vector representation \(a\) is independently distributed according to a Bernoulli distribution with parameter \(h(a)\); i.e., \(p(y a,h)=h(a)^{y}(1-h(a))^{1-y}\)._

Now the posterior inference of the stochastic process (in our case, Beta GP) requires conditioning on observations, and the stochastic process measures uncertainty for a set of queries. We denote a set of observations as \(D\), which consists of representation-label pairs, i.e., \(D=\{((x_{i}),y_{i})\}_{i=1}^{|D|}\); here, label \(y_{i}\) is either \(0\) or \(1\). The queries, \(_{q}=[(x_{j})]_{j=1}^{|_{q}|}^{d |_{q}|}\), are vector representations of a set of stimuli whose labels are to be predicted together with uncertainty measures.

Note that there are no additional assumptions about this probing dataset, e.g., the distributions of stimuli in observations or queries. We use GPP for OOD detection, as shown later. Unlike existing probes (e.g., (Hewitt et al., 2021; Xu et al., 2020)), we do not consider a stimulus and its vector representation as random variables with some underlying distributions. Instead, viewing them as deterministic allows us to probe with any stimuli (with potentially any underlying distributions).

### Background: Beta Gaussian processes

GPP uses a Beta GP to define a prior distribution over classifiers. The Beta GP is a special case of Dirichlet-based GPs (Milios et al., 2018), which have been shown to either outperform or achieve similar performance than classic GP classification approximations (Rasmussen and Williams, 2006). For each representation \(a\), Beta GPs assume the binary label \(y\) is generated by sampling \(y(g(a))\) where the Bernoulli parameter \(g(a)((a),(a))\). The Beta variable \(g(a)\) can be written as two independent Gamma variables:

\[g(a)=(a)}{t_{}(a)+t_{}(a)},\ t_{}(a) ((a),1)\,\,t_{}(a)((a),1),\]

where \(t_{}(a)\) and \(t_{}(a)\) are independent. The Gamma distributions are then approximated with Log-normal distributions via moment matching. That means,

\[f_{}(a)=(t_{}(a))(((a))-}{2},v_{}),v_{}=(+1),\] (1)

and the same for \(f_{}\). Beta GPs use two GPs to model the two latent functions \(f_{}\) and \(f_{}\),

\[g=},f=f_{}-f_{},f_{} (,k),f_{}(,k),f_{}\!\!\!  f_{}.\] (2)Note that the latent function \(f\) in Eq. 2 is still a GP given that it is the sum of two independent GPs. We denote the Beta GP as \(()\), where \(=(,k)\). While we focus on probing as binary classification in this work, it is straightforward to extend GPP to multi-class classification by switching the Beta prior to a Dirichlet prior (Milios et al., 2018) (i.e., using more latent functions than just \(f_{}\) and \(f_{}\)).

### Adapting Beta GPs for GPP

There are two important details in Beta GPs that require special attention: how to set the prior and how to approximate the posterior of classifier \(g\). This requires setting mean and kernel functions \(,k\) in the prior \((,k)\) of the latent functions \(f_{}\) and \(f_{}\), and computing their posterior \(f_{},f_{} D\).

We adapt these aspects of Beta GPs for probing settings, that also enables users to incorporate the characteristics of their end-tasks by setting intuitive hyperparameters. See more explanations and illustrations in the Appendix.

#### 3.3.1 Setting the prior

First, we need to make sure the prior distribution approximated by GPs in our GPP matches with the Beta prior. Without loss of generality, we assume that for any \(a\), the prior distribution for \(g(a)\) is \((,)\) for some \(>0\). This hyperparameter \(\) reflects the characteristics of the task at hand; the probability of a positive label might be bi-modal with density centered at both 0 and 1 (e.g., label is not fuzzy) (\(<1\)), or uniformly distribution in \(\) (\(=1\)), or centered at \(0.5\) (\(>1\)).

To match the Beta prior, the prior for both \(f_{}(a)\) and \(f_{}(a)\) has to be \((()-,v)\), where \(v=(+1)\). To match this normal distribution, we set the mean function to a constant \((a)=()-\), and constrain the kernel function to satisfy \(k(a,a)=v\).

Figure 1: Illustrative example of probabilistic probing: The model of interest \(\) transforms a stimulus \(x\) to its vector representation \(a=(x)^{2}\). The probe, \(( D)\), a distribution parameterized by \(\) over possible classifiers (mapping from representations to predicted probabilities) conditional on the observation set \(D\). Suppose \(D\) contains two datapoints. Sampled classifiers \(g\) from \(( D)\) are depicted in the middle box. The aggregation of predictions from the sampled classifiers produces the distribution for \(g(a)\); \(g(a)\) is the predicted probability of any representation \(a\). Notice that the two data points in \(D\) are located close to each other, yet has opposite labels, indicating fuzziness in labels around this region. Imagine a new query data \(a\). In the top right-most, we see that \(a\) close to \(D\) produces a distribution that is centered around \(g(a)=0.5\). If the representation \(a\) is far away from \(D\) (bottom right-most), the probe can have higher epistemic uncertainty due to lack of information, and the predicted probability follows the prior distribution (in this case, the prior has two modes, one at \(g(a)=0\) and one at \(g(a)=1\)). Note that both distributions have an expectation at \(g(a)=0.5\), for drastically different reasons: fuzzy label v.s. lack of knowledge.

#### 3.3.2 Posterior inference

This formulation offers a closed-form solution for posterior inference of the GP. Namely, any \(y_{i}=1\) in \(D=\{(a_{i},y_{i})\}_{i=1}^{|D|}\) results in updating Beta parameters of \(g(a_{i})\) to be Beta(\(+s,\)) and any \(y_{i}=0\) results in Beta(\(,+s\)), for some \(s 1\). This hyperparameter \(s\) describes how much "strength" (i.e., weight of the observation) we add to the Beta posterior. To match the GP posterior to the Beta posterior, we synthetically add observations with heteroscedastic noise to functions \(f_{}\) and \(f_{}\). If \(y_{i}=1\), the observation for \(f_{}\) is \((a_{i},(+s)-}{2})\) with noise \((0,v^{})\) where \(v^{}=(+1)\), and the observation for \(f_{}\) is \((a_{i},()-}{2})\) with noise \((0,v^{})\) where \(v^{}=(+1)\). And vice versa for \(y_{i}=0\). The posterior of a GP remains a GP with closed-form updates to its mean and kernel function. Once we have the posterior of latent functions \(f_{} D\) and \(f_{} D\), obtaining the posterior of classifier \(g\) is trivial by updating the GP priors in Eq. 2 to the GP posteriors.

#### 3.3.3 The cosine kernel

The choice of kernel function is important for GPP as it directly determines the characteristics of the distribution of latent functions, which translates to the distribution of classifiers that GPP uses. We define a cosine kernel as:

\[k(a,a^{})=va^{}+1}{(\|a\|+1)^{}(\|a^{ }\|+1)^{}},v=(+1).\] (3)

Note that multiplying the constant \(v\) is the constraint we have from setting the prior for the Beta GP. Using the cosine kernel in the GP is equivalent to defining a distribution over linear latent functions in an augmented space of the original representation space \(\), i.e.,

\[f_{}(a)=W_{}^{}(a)+(a),(a)=}{\|a\|+1}a\\ 1^{d+1}W_{}(0,I).\] (4)

And similarly for \(f_{}\) with weights \(W_{}\). Here \(\) is the mean function of the Beta GP. Note that we have the closed-form posterior distribution for the weights, which remains a multivariate Gaussian distribution. See Appendix for more details.

Why is the kernel defined in this way? During the pre-training stage of deep learning models, a weighted sum of the activations and a bias term are used to construct neurons of the next layer. Following Wang et al. (2023), we can view each next layer neuron as independent samples of classifiers from the same distribution, described by a Beta GP in this work. This means classifiers from the Beta GP should all consider a bias term. Hence we augment the activations by an additional dimension with value fixed at \(1\) (Eq. 4). Additionally, we normalize the augmented activations, \((a)\), such that \(k(a,a)=v\) (the constraint from setting the prior in SS3.2) holds for all \(a\) (Eq. 3).

We can interpret the cosine kernel as a scaled cosine of the angle between two vectors (standard cosine similarity) in an augmented representation space described by activations and biases.

### Uncertainty measures

The probabilistic probing formulation makes it possible to compute a posterior distribution over labels for queries. Building on terms from Fox and Ulkumen (2011), we define uncertainty measures that identify the epistemic and aleatory uncertainty of the posterior prediction \(g(a)\) where \(g=}( D), a\). Let the posterior of the latent function be \(f(a) D(_{D}(a),k_{D}(a))\).

* _Episteme_\(:=-[g(a)]\), the negative entropy of the distribution of \(g(a)\), which describes the amount of knowledge the probe has about the label of stimulus \(a\). In GPP, we have \([g(a)]=[f(a)]-_{D}(a)-2[(1+e^{-f(a)})]\) and \([f(a)]=(2 ek_{D}(a))\).
* _Alea_\(:=[y g(a)]=[-g(a)(g(a))-(1-g(a))(1-g(a))]\), the expected entropy of the conditional distribution \(p(y g(a))\), where \(y\) is the label of stimulus \(a\). Higher idea corresponds to more fuzziness in the label of \(a\).
* _Judged probability_\(:=[g(a)]=[}]\), which is the expected probability (judged by the probe) that the label of stimulus \(a\) is positive.

Figure 2 shows the relationship of episteme, alea and judged probability for a rational agent (white region, based on (Fox and Ulkumen, 2011)).

Low epistemic means we are "not sure" and high epistemic that we are "highly confident" about the underlying probability. Alea, on the other hand, increases with the intrinsic randomness of the true label. The judged probability that the stimulus's label is positive reflects both epistemic and alea. With low episteme, the judged probability should be closer to 0.5, meaning the probe does not have enough information to draw a plausible conclusion. It would be irrational (grey areas in Figure 2) for a person to believe something will definitely happen even if there is no evidence. Analogously, it is irrational for a probe to produce probabilities close to 0 or 1 while having low episteme. By accumulating evidence (increasing epistemic), the judged probability can move closer and closer to the ground truth (moving to the right in the figure). An accurate probe should produce a judged probability that matches the ground truth fuzziness of a label only if episteme is high.

Figure 3 is a real-world example (using a CoCa model (Yu et al., 2022) and natural images, shown in the figure, as query and observations) of how our measures of uncertainty change as we increase a small number of positive and negative observations.

Table 1 presents the results of applying GPP to the concepts involving olives introduced in SS1. The judged probabilities from GPP match human judgments (McCloskey and Glucksberg, 1978): olives have a 57% chance of being fruit with a high level of confidence, and GPP is very uncertain about whether olives on a tree are blickets.

## 4 Validating GPP

In addition to probing concepts, GPP predicts aleatory and epistemic uncertainty, which allows us to determine the fuzziness of a concept or detect out-of-domain (OOD) stimuli.

   Concept & Judged prob. & Episteme & Alea \\  Fruit & 57\% & 2.3 & 0.63 \\ Blicket & 91\% & 0.49 & 0.22 \\   

Table 1: GPP predictions for olives in §1.

Figure 3: How episteme, judged probability and alea of GPP changes as more observations are given to the CoCa model (Yu et al., 2022). As observations are added, episteme increases while judged probability fluctuates. (a) With 1 positive observation, the judged probability for the query is 0.96, showing that the query and an the observation are very close in representation space. (b) With 1 positive (rabbit) and 1 negative (dog) example, the judged probability lowers to 0.6, showing that the CoCa model would also put rabbits and dogs close in representations, but the queried rabbit is still closer to the observed rabbit. (c) With 1 more positive example of a rabbit, judged probability increases together with episteme. (d) If we set the 3rd observation in (c) to be negative, GPP gets more evidence that the query is probably negative and the judged probability decreases to 0.37.

Figure 2: Interaction of alea and episteme (recrereation of Figure 1 from Fox and Ulkümen (2011)).

We use 3D Shapes (Burgess and Kim, 2018) and construct 3 datasets based on its concept ontology to validate these uses of uncertainty measures. In these 3 settings, we can control the ground truth level of fuzziness by artificially injecting noise to labels (SS4.2, SS4.3). In SS4.5, we evaluate the OOD performance of GPP and competitive baselines on 3D Shapes as well as 10 datasets defined by coarse-grained labels of the ImageNet dataset (Russakovsky et al., 2015).

### Experiment setups for probing

We evaluate GPP's ability to probe using the 3D Shapes dataset (Burgess and Kim, 2018), which contains images generated from 6 ground truth independent primitives: 10 floor colors, 10 wall colors, 10 object colors, 8 scales, 4 shapes and 15 orientations of the shapes, as shown below.

Datasets and models.We construct a 2-level concept ontology (see Appendix for visualization) consisting of a disjunctive level where colors are binarized to warm/cool and scale to small/large, and a conjunctive level that combines shapes, binarized colors and binarized scales.

We train three CNN models on labels generated from this ontology: 1) \(.1\) is trained on 64 labels (binarized scale, floor, wall object color x 4 shapes), 2) \(.2\) is trained on 8 labels (4 shapes x binarized scale) 3) \(.3\) is trained on 8 labels (binary floor, wall and object color).

We define two probing tasks to investigate each model: 1) task \(.1\): binarized colors (floor, wall, object) and 2) task \(.1\): binarized scales and 4 shapes.

Baseline descriptions.The baselines include 1) LP: linear probes (Alain and Bengio, 2016); 2) SVM: SVM probes (a baseline from Kim et al. (2018)); 3) LPE: linear probes ensembled via bootstrap (Kim et al., 2018). For each linear probe in LPE, we train a logistic regression classifier on a dataset sampled from the original set of observations with replacement. The size of the dataset is the same as the number of observations. Each ensemble has 100 linear probes. LPE can also describe a distribution over classifiers and be evaluated as probabilistic probes, making this a strong baseline. For GPP, we use \((0.1,0.1)\) as the prior and set strength to be 5.

### Validating GPP as a probe

To compare with baseline methods, we use AUROC to evaluate the deterministic classifier defined by the judged probability \([g(a)], a_{q}\) (defined in SS3.4). Queries are sampled disjointly from the training data and observations. Figure 4 shows that GPP performs competitively against LP and SVM. GPP and LP perform comparable with respect to AUROC and sample efficiency (needing as few as 10 observations for probing concepts in task \(.1\)).

One may first guess that \(.3\) is the best model that represents color, since it was explicitly tasked to distinguish color. However, our probing results reveal what is obvious in hindsight: while \(.2\) is only tasked to distinguish shapes and scales, separating where the shapes are requires detecting color (i.e., the only cue to separate shape from background is color). This is shown in \(.2\)'s high AUROC for \(.1\) in Figure 4. \(.3\) is only tasked to distinguish colors, so performance on \(.2\) remains low.

### Estimating concept fuzziness

What happens when the concepts we are trying to probe are inherently fuzzy? This is commonly the case in the real world, where situations and the language used to describe them contain extensive ambiguity. We again focus on \(.1\) and concepts in task \(.1\). We control the fuzziness of concepts in \(.1\) by randomly flipping positive labels to negative (but not vice versa) in the observations. We test ground truth label probabilities of \(0.25,0.5,0.75\) and \(1\), where \(1\) indicates no fuzziness. To measure GPP's alignment with ground truth label probabilities, we use the Pearson correlation coefficient between ground truth label probabilities and judged probabilities for all queries.

Figure 4: AUROC results for probing \(.1\), \(.2\) and \(.3\) with \(.1\) and \(.2\) tasks. \(.1\) and \(.3\) are trained on color-related tasks and all probes obtain better performance on color concepts \(.1\) than geometry concepts \(.2\). The probing results reveal something obvious in hindsight; while \(.2\) is trained on geometry-related tasks, it needed color to separate the object from the background color in order to detect geometry, resulting in significant representation of color concepts, \(.1\).

Figure 5: Left: Pearson correlation coefficient between ground truth label probabilities and judged probabilities. GPP obtains a much higher correlation than LPE, showing that GPP is better at detecting fuzzy concepts. Middle and Right: Relationship between judged probability and epistemic of positive labels using LPE and GPP when ground truth label probability is 0.5. LPE fails to detect fuzziness and assigns extreme judged probabilities to queries for 8 or more observations. With 2 observations, LPE has higher epistemic than with more observations. This behavior is irrational : fewer observations should indicate lower episteme, and with low episteme high judged probability indicates LPE is being confidently ignorant (no knowledge but believes something will definitely happen). On the contrary, GPP shows rational behavior: with high episteme, the mass of GPP’s judged probability predictions centers at 0.5, which aligns with the ground truth.

Figure 6: Alea predictions as a function of episteme for LPE and GPP. When ground truth label probability is 0.5 (Left and Middle), LPE predicts low idea for observations larger than 2, which does not agree with the ground truth idea (0.69). GPP shows much more rational predictions. Comparing Middle and Right, GPP predicts higher idea with more observations for ground truth label probability 0.5 (the corresponding alea is 0.69), and its alea predictions converge to 0 (ground truth alea is 0) for ground truth label probability 1.

Figure 5 shows that GPP can accurately probe fuzzy concepts by predicting both judged probability (expected level of fuzziness) and epistemic (how sure it is about the judged probability). GPP achieves a higher Pearson correlation coefficient than LPE across all sizes of observations. In particular, the relationship between judged probabilities and epistemic coincides with the two-dimensional framework for characterizing uncertainty from Fox and Ulkumen (2011) (see Fig. 2), showing the validity of GPP as a rational method to probe fuzziness.

Figure 5 (Right) confirms that the judged probability alone cannot be used to distinguish whether concept is fuzzy or the stimuli are far away: in both cases, judged probabilities can be \(0.5\). Hence, it is important to remember that judged probability is only accurate with enough observations. We illustrate this point from another angle in Figure 6, which shows the relation between idea and episteme predicted by LPE, GPP with ground truth label probability \(0.5\) or \(1\). When ground truth label probability is \(0.5\), GPP can have low idea under low episteme (same effect as Figure 1), but with more episteme, GPP puts more mass of predictions for high idea because the ground truth idea is high. LPE does not show this kind of rational behaviors. When ground truth label probability is \(1\), more mass of idea predicted by GPP converges to 0 as episteme increases, since the ground truth idea is 0.

### Experiment setups for OOD detection

When learning new concepts, it is natural for people to be able to say "I'm not sure since I haven't learned it yet". For probes, the corresponding ability is to perform out-of-distribution (OOD) detection, which identifies datapoints that are different from the observed training data. GPP naturally performs OOD detection using episteme predictions. With an uninformative prior for the labels,3 high episteme indicates in-domain (ID), while low episteme indicates OOD.

Method descriptions.For GPP, we use the negative latent variance of posterior function \(f\) (Eq. 2) as a proxy of episteme for computational efficiency. It is easy to show that the entropy of classifier \(g\) is upper bounded by the entropy of latent function \(f\), which is fully determined by its posterior variance (more details in Appendix). In practice, we found that using the negative latent variance as OOD detection scores for GPP is robust to the level of informativeness of the Beta prior, because the latent GPs are agnostic with respect to the logistic transformation in Eq. 2.

As for baselines, we compare against MSP (maximum predicted softmax probabilities using LP) (Hendrycks and Gimpel, 2016), Maha (negative Mahalanobis distance-based score) (Lee et al., 2018), NN (deep nearest neighbors) (Sun et al., 2022) and LPE. Recall that LPE uses bootstrapping to create an ensemble of linear probes. Each linear probe in LPE can be viewed as _i.i.d._ samples from an underlying distribution over classifiers (i.e., \(g\) in Figure 1). Similar to approximating metrics with samples in SS3.4, we can estimate the negative variance of predictions from the set of linear probes, and use it as the OOD detection score for LPE.

As shown in the extensive analyses of OOD detection methods and tasks in Appendix E of Tran et al. (2022), Maha and LPE (LPE is equivalent to their "Entropy" method for ensembles) achieved top performance, surpassing more recent proposals from Ren et al. (2021).

Datasets and models.We generate a set of queries from task \(.1\) of 3D Shapes that include 1024 ID images and 1024 OOD images (uniform random noise), and see if OOD queries can be detected using the methods above based on representations from model \(.1\).

We also evaluate the OOD detection performance of all methods on real world datasets and models. Queries and observations in the real world datasets are sampled disjointly from the validation split of the ImageNet dataset (Russakovsky et al., 2015). We make 10 sets of \(D\)s using 10 binary classification tasks defined by supersets of ImageNet classes. Query points consist of 128 images from the classes the probe has observed and 128 OOD images (uniform random noise). The ID examples are images that come from the same distribution that the probe observes. For example, consider a probe trained to classify "all dogs" vs "all cats"; images of dogs and cats would be ID, whereas random noise would be OOD. The models we used are the pre-trained ResNet-50 (He et al., 2016) and CoCa-Base (Yu et al., 2022). See more details in Appendix.

### Results for OOD detection

Figure 7 shows the competitive performance of GPP as an OOD detector for all the OOD detection tasks we constructed. While GPP might not be the best OOD detection approach for each of the tasks, GPP performed consistently well and on par with the best benchmark method for each task. This means, while GPP is mainly a probing method, it is also competitive for OOD detection.

Note that the effective term in GPP's negative latent posterior variance, \(k(a,_{o})K^{-1}k(_{o},a)\) (here \(_{o}\) is the concatenated observed activations and \(K=k(_{o},_{o})^{O O}\)), looks very similar to Mahalanobis distance, \((a-_{c})^{}^{-1}(a-_{c})\) (Eq. 2 from Lee et al. (2018), where \(^{d d}\); \(d\) is the dimension of activations). However, GPP's variance measures the similarity between function values but Mahalanobis distance measures the similarity between activations. Also note that the Gaussian estimated for Mahalanobis distance is degenerate for \(d>O\), and so the distance is still computed on a \(O\)-dimensional manifold; this is possibly why the Maha method shows inferior performance.

## 5 Conclusion

We introduced GPP, an uncertainty-aware probing framework that uses Gaussian processes to measure both the aleatory uncertainty intrinsic to the concept represented by a model and the epistemic uncertainty reflecting the probe's confidence about its judgement. These uncertainty measurements also equip GPP with the ability to detect OOD stimuli. Empirically, we verified the strong performance of GPP for data-efficient probing that can correctly surface fuzziness in concepts and OOD detection which shows GPP can also surface epistemic uncertainty (i.e., knows that it does not know). These results illustrate how distinguishing between different forms of uncertainty can be beneficial both for deepening our theoretical understanding of models and for practical applications.

Limitations.We have validated our approach on both synthetic and real-world datasets, and demonstrated its use for OOD detection, but there are a much wider range of models, datasets, and applications that can be explored. Fully evaluating the potential of this approach and achieving the greatest impact will require pursuing this investigation, which we hope to do in future work. In particular, we plan to explore active few-shot learning and distributionally robust probing. Finally, GPP can only be used with models for which activations are made available, which may present an obstacle to use with certain commercial machine learning systems.

Broader impact.Being able to understand what models can and cannot represent has huge implications. Not only does it impact important downstream tasks, such as OOD detection, but it ultimately aids human-machine communication and collaboration. While a (potentially) superficial interaction (e.g., with a chatbot) is already possible, we also know models hallucinate Ji et al. (2023), leaving people unsure about the validity/faithfulness of this interaction. Work such as ours ultimately aims to help humans to better use and develop models that aligns well with our concepts, as well as potentially learning new concepts from machines.

Figure 7: AUROC for ID/OOD detection with an increasing number of observations for task \(.1\) of 3D Shapes (Left) and real-world datasets constructed from ImageNet (Middle and Right), showing GPP’s comparable performance and data efficiency in OOD detection.