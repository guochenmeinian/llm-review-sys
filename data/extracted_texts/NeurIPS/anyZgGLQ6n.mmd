# Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression

Yixiu Mao\({}^{1}\), Qi Wang\({}^{1}\), Chen Chen\({}^{1}\), Yun Qu\({}^{1}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Department of Automation, Tsinghua University

myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn

###### Abstract

In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.

## 1 Introduction

Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications . However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration  and time-consuming episode collection . Recent advances view offline RL as a hopeful solution to these challenges . Offline RL aims to learn a policy from a fixed dataset without further interactions . It can tap into existing large-scale datasets for safe and efficient learning .

In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error , which can be exacerbated by bootstrapping and result in severe value overestimation . To address this issue, a large body of work has emerged to directly or indirectly _suppress OOD actions_ during training, employing various techniques such as policy constraint , value penalization , and in-sample learning .

Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an _OOD state issue_ that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance .

In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states . Technically, Zhang et al.  construct a dynamics model and a state transition model and align them to guide the agent to ID regions, whileJiang et al.  resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement.

In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as _value-aware_ OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL  and NeoRL . SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations.

To summarize, the main contributions of this work are:

* We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS _unifying OOD state correction and OOD action suppression_.
* Our approach achieves _value-aware_ OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods.
* Empirically1, our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments _without additional hyperparameter tuning_. 
## 2 Preliminaries

In reinforcement learning, we generally characterize the environment as a Markov Decision Process (MDP) \(=(,,P,R,,d_{0})\), with state space \(\), action space \(\), transition dynamics \(P:()\), reward function \(R:\), discount factor \([0,1)\), and initial state distribution \(d_{0}\). The agent interacts with the environment and seeks a policy \(:()\) to maximize the expected discounted return \(()\):

\[()=_{s_{0} d_{0},a_{t}(|s_{t}),s_{t+1} P( |s_{t},a_{t})}[_{t=0}^{}^{t}R(s_{t},a_{t})].\] (1)

Figure 1: **The resulting state distributions of offline RL algorithms and optimal values of states.** (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. _SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas_, while CQL and TD3BC tend to produce OOD states with extremely low values.

For any policy \(\), we define the value function as \(V^{}(s)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})|s_{0 }=s]\) and the state-action value function (\(Q\)-value function) as \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})| s_{0}=s,a_{0}=a]\).

Offline RL.In offline RL, the agent can only access a static dataset \(=\{(s_{i}^{t},a_{i}^{i},s_{i+1}^{i},r_{i}^{i})\}\). We denote the empirical behavior policy of \(\) by \((a|s)\) and the empirical dynamics model by \(M(s^{}|s,a)\), both of which depict the conditional distributions observed in the dataset . Typical actor-critic algorithms [56; 18] evaluate policy \(\) by minimizing Bellman loss:

\[L_{Q}()=_{(s,a,s^{})}[(Q_{}(s,a)-R (s,a)-_{a^{}_{}(|s^{})}Q_{ ^{}}(s^{},a^{}))^{2}],\] (2)

where \(_{}\) and \(Q_{}\) are the parameterized policy and \(Q\) function, and \(Q_{^{}}\) is a target network whose parameters are updated via Polyak averaging .

Simultaneously, policy improvement in policy iteration is achieved via maximizing the Q-value:

\[L_{}()=-_{s,a_{}}[Q_{} (s,a)].\] (3)

OOD action issue.In offline RL, _OOD actions_ refer to actions outside the support of the behavior policy \((|s)\) at a specific state \(s\). Since the Q-values of OOD actions can be poorly estimated and the policy improvement is towards maximizing the estimated \(Q_{}\), the resulting policy tends to prioritize the OOD actions with overestimated values, leading to poor performance .

## 3 OOD State Correction

The following focuses on the OOD state issue and OOD state correction in offline RL. In Section 3.1, we systematically analyze the OOD state issue, introduce the concept of OOD state correction, and point out limitations of prior methods. Then we present the proposed approach SCAS in Section 3.2.

### OOD State Issue in Offline RL

In offline RL, _OOD states_ refer to states not in the offline dataset. The OOD state issue (Definition 1) pertains to scenarios where the agent enters OOD states during the test phase, potentially resulting in catastrophic failure . However, such a topic is rarely investigated in the literature, and existing studies lack deep insights. We mathematically formulate the OOD state issue as follows.

**Definition 1** (OOD state issue).: _There exists \(s\), such that \(d_{}^{}(s)>0\) and \(d_{}(s)=0\), where \(_{}\) is the MDP of the test environment, \(\) is any learned policy, \(_{}^{}\) is the state probability density induced by \(\) in \(_{}\), and \(d_{}\) is the state probability density in the offline dataset._

Origins and consequence of OOD states.During the test phase, the OOD states occur primarily in three scenarios: (i) OOD actions: the learned policy, not perfectly constrained within the support of the behavior policy, executes unreliable OOD actions, leading to OOD states. (ii) Stochastic environment: the initial state of the actual environment may fall outside the offline dataset. In addition, stochastic dynamics can also lead to states outside the dataset, even when taking ID actions in ID states. (iii) Perturbations: commonly seen in real-world robot applications, some unexpected perturbations can propel the agent into OOD states (e.g., wind, human interference).

During offline training, the typical Bellman updates involve only ID states, and the policies in OOD states are not trained. As a result, when encountering OOD states in the test phase, the agent would exhibit uncontrolled behavior, and the state deviation from the offline dataset can be further exacerbated over time steps, severely degrading performance .

OOD state correction.To mitigate this OOD state issue, an intuitive solution is to train a policy capable of correcting the agent from OOD states to ID states, a concept known as _OOD state correction_. Specifically, during offline training, we can perturb the original state \(s\) in the dataset into \(\) to generate substantial OOD states. Then consider the scenario where the agent starts from \(\), follows the trained policy \(\), and transitions to the next state \(^{}\). To reduce state deviation, \(^{}\) is expected to be close to the offline dataset. Thus we can align the distribution of \(^{}\) with an ID state distribution to regularize the policy and achieve OOD state correction.

Continuing the above train of thought, SDC  generates the ID state distribution by feeding the original state \(s\) into a trained state transition model \(N(s^{}|s)\) of the dataset. This model characterizesthe conditional state transition distribution in the dataset and is implemented by a conditional variational auto-encoder (CVAE) . After pretraining a dynamics model \(M(s^{}|s,a)\) and the state transition model \(N(s^{}|s)\), SDC introduces the following policy regularizer for OOD state correction:

\[_{}*{}_{s}*{ }_{_{}(s)}[*{MMD}(M( |,(|)),N(|s))],\] (4)

where \(\) is a Gaussian noise perturbed version of the original state \(s\), \(\) is the standard deviation of the Gaussian, \(M(|,(|))\) is shorthand for \(*{}_{(|)}M(|, {a})\), and MMD is the maximum mean discrepancy measure. More recently, OSR  directly aligns the trained policy distribution at the perturbed state with a CVAE inverse dynamics model to constrain the policy in OOD states.

Limitations.However, the regularizers of prior methods are only designed to deal with this OOD state issue. To mitigate OOD actions, they require an additional conservative Q learning (CQL) term  in value estimation to penalize Q-values of OOD actions. In addition, the state transition distribution and the inverse dynamics distribution are multi-modal in many scenarios . The necessity of extra OOD action suppression components and complex distribution modeling compromises their computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, particularly when the offline dataset contains a large portion of suboptimal states. In such cases, vanilla OOD state correction can lead to suboptimal behaviors. Consequently, there is also significant potential for improvement in the performance of prior methods.

For a more comprehensive discussion of related work, please refer to Appendix A.

### Value-aware OOD State Correction

The objective of this work is to formulate a simple yet effective policy regularizer for offline RL that unifies OOD state correction and OOD action suppression. Moreover, we aim to achieve _value-aware_ OOD state correction, involving the correction of the agent from OOD states to high-value ID states.

Value-aware state transition.For the ID state distribution to which the agent is corrected, we expect a value-aware state transition distribution \(N^{*}(|s)\) that lies within the support of the dataset state transition distribution \(N(|s)\) but is skewed toward high-value states \(s^{}\). To ensure stability and, more importantly, to enable our subsequently designed algorithm to circumvent modeling complex distributions, we seek a soft optimal version of it. To this end, we consider the following problem2:

\[_{N^{*}}*{}_{s}[ *{}_{s^{} N^{*}(|s)}V(s^{})-*{D_{ KL}}(N^{*}(|s)\|N(|s))],\] (5)

where \(\) is a hyperparameter to balance the two terms.

The optimization problem above has a closed-form solution:

\[N^{*}(s^{}|s)=( V(s^{}) )N(s^{}|s),\] (6)

where \(Z(s)=_{s^{}}( V(s^{}))N(s^{ }|s)\) is a normalization factor. It can be seen from Eq. (6) that \(*{supp}(N^{*}(|s))*{supp}(N(|s))\). Note that \(\) is a key hyperparameter that controls the significance of the values of next states in SCAS's OOD state correction. As \(\) increases, \(N^{*}(|s)\) becomes more skewed toward the optimal \(s^{}\) in the support of \(N(|s)\).

OOD state correction.In order to produce substantial OOD states, we perturb each state \(s\) with Gaussian noise \((0,^{2})\), resulting in perturbed state \(\). It is worth noting that the dataset used for RL training remains unchanged. We perturb the states solely to formulate the regularizer.

We anticipate the following value-aware OOD state correction scenario, where the agent starts from OOD state \(\), follows the trained policy \(\), and transitions to the high-value ID state \(s^{}\) in the distribution of \(N^{*}(|s)\). To this end, we train the policy \(\) to align the dynamics induced by \(\) on the perturbed state \(\) with the value-aware state transition distribution at the original state \(s\) via KL divergence. That is, we regularize \(\) by minimizing:

\[_{}*{}_{s}*{}_{_{}(s)}*{D_{KL}}(N^{*}(|s) \|M(|,(|))).\] (7)By substituting the analytical solution of \(N^{*}\) from Eq. (6) into the KL divergence, we have

\[*{argmin}_{}*{D_{KL}}(N^{*}(|s)||M(|,(|)))=*{argmax}_{}*{}_{s ^{} N(|s)}\,[ ))}{Z(s)} M(s^{}|,(|))].\]

Note that \(N\) is the state transition distribution in the dataset, and \(s,s^{} N(|s)\) is equivalent to \((s,s^{})\). Thus minimizing Eq. (7) is equivalent to maximizing following regularizer:

\[R()=*{}_{(s,s^{})}* {}_{_{}(s)}[))}{Z(s)} M(s^{}|,(| {s}))].\] (8)

As a result, \(R()\) effectively eliminates the need for a pre-trained multi-modal state transition model (\(N\) or \(N^{*}\)) and enables direct sampling from the dataset for optimization.

However, the normalization factor \(Z(s)\) in \(R()\) can be challenging to compute. We note that the regularizer \(R()\) is derived from the minimization of the KL divergence in Eq. (7). Since we aim to minimize this KL at every state \(s\) in \(\) and \(Z(s)\) only affects the relative weights at different \(s\), it matters less to precisely restore the correct state weights in \(\) by computing \(Z(s)\), which is empirically hard to estimate and may bring more instability. Thus, we replace \(Z(s)\) in \(R()\) with an empirical normalizer \(( V(s))\) for computational stability:

\[R_{1}()=*{}_{(s,s^{})} *{}_{_{}(s)}[))}{( V(s) )} M(s^{}|,(|))].\] (9)

We provide further rationale behind this choice of the empirical normalizer in Appendix C.1.

Tractable optimization.Now we shift focus to the optimization of \(R_{1}()\). The expectation with respect to \(\) can be moved outside the logarithm by Jensen's inequality:

\[R_{1}()*{}_{(s,s^{})} *{}_{_{}(s)}* {}_{(|s)}[))}{( V(s))} M(s^{ }|,a)],\] (10)

where the equality holds when \(\) is deterministic. In general, it is convenient to maximize the lower bound in Eq. (10) using the reparameterization trick. However, to ensure the equality case in Eq. (10), we opt to train a deterministic policy \(\). In this case, we can directly maximize \(R_{1}()\) by computing the gradient of \(\) using automatic differentiation .

In contrast to model-based RL methods that typically use the learned dynamics model to roll out multi-step trajectories for policy training , our algorithm utilizes the dynamics model to propagate the gradient of policy and regularize policy training, resulting in significantly enhanced computational efficiency. Moreover, the nature of one-step dynamics prediction in our method is advantageous for maintaining relatively high prediction accuracy.

## 4 Analysis of OOD Action Suppression

This section focuses on the OOD action issue and shows that the proposed regularizer also exhibits the effect of _OOD action suppression_. In other words, it can also prevent the policy from taking OOD actions, thereby simultaneously addressing the fundamental OOD action issue in offline RL. In offline RL, OOD actions are exclusively defined on ID states. This is because actor-critic training is limited to ID states, and any actions on OOD states would not affect training and cause the OOD action issue mentioned in Section 2. Consequently, for the analysis of OOD actions, it is essential to consider ID states. We define \(,_{1}\) as the ID state version of \(R,R_{1}\), where \(=s\). \(\) and \(_{1}\) can be regarded as special cases of \(R\) and \(R_{1}\), when \(\) sampled from \((s,^{2})\) is equal to \(s\):

\[() =*{}_{(s,s^{})}[ ))}{Z(s)} M(s^{} |s,(|s))],\] (11) \[_{1}() =*{}_{(s,s^{})}[ ))}{( V (s))} M(s^{}|s,(|s))].\] (12)The proposed regularizer functions as follows: when the agent encounters OOD states, it drives the agent to choose actions leading to ID states, as discussed in Section 3.2. When the agent is in ID states, the ID state part of it comes into play. In the following, we show that it helps circumvent taking OOD actions by analyzing the maximizer of \(,_{1}\) in tabular MDPs.

**Proposition 1**.: _Suppose that the environment dynamics is deterministic, then both \(()\) and \(_{1}()\) achieve their global maximum at the policy \(^{*}\), where3_

\[^{*}(a|s)=( V(M(s,a)))( a|s)\] (13)

_The support of \(^{*}\) is within that of the behavior policy \(\):_

\[(^{*}(|s))((|s)),\;  s\] (14)

_and \(^{*}\) makes the following equation hold:_

\[N^{*}(|s)=M(|s,^{*}(|s)),\; s\] (15)

Under the deterministic dynamics condition, Proposition 1 shows that \(^{*}\) is constrained within the support of the behavior policy. Thus, our regularizer helps to keep the policy from taking OOD actions. Moreover, \(^{*}\) is able to exactly align \(M(|s,^{*}(|s))\) with \(N^{*}(|s)\), indicating the guidance of the agent to the high-value ID state distributions.

Furthermore, we show in Proposition 2 that even under stochastic dynamics, the optimization of \(\) and \(_{1}\) still yields policies constrained within the support of \(\). Hence, SCAS also exhibits the effect of OOD action suppression in this more general scenario.

**Proposition 2**.: _When the dynamics is stochastic, the maximizers of both \(()\) and \(_{1}()\) are constrained within the support of the behavior policy:_

\[(^{*}(|s))(( |s)),\; s\] (16) \[(^{*}_{1}(|s))(( |s)),\; s\] (17)

## 5 Implementation Details

SCAS is easy to implement and we design the practical algorithm to be as simple as possible, retaining algorithmic simplicity and improving computational efficiency.

Dynamics model.We employ a deterministic dynamics model \(M_{}\). The loss for training the model is

\[L_{M}()=}_{(s,a,s^{})}[\|M_{ }(s,a)-s^{}\|^{2}_{2}]\] (18)

Policy improvement.With a deterministic model, we replace the log-likelihood in \(R_{1}()\) with mean squared error. It is a common approach in RL algorithms to convert a maximum likelihood estimation problem into a regression problem when dealing with Gaussians with fixed variance . As discussed in Section 3.2, we also adopt a deterministic policy model \(_{}\). Thus, we have the following policy regularizer:

\[R_{2}(_{})=}_{(s,s^{})}}_{_{}(s)}[(s^{}))}{( V_{}(s ))}\|M_{}(,_{}())-s^{}\|^{2}_{2} ],\] (19)

where \(V_{}(s)=Q_{}(s,_{}(s))\) and \(_{}\) means \(_{}\) with detached gradients. Using deterministic policy also simplifies the training process without learning a \(V\)-function. Combining \(R_{2}(_{})\) with the standard policy improvement objective, we update the policy by maximizing:

\[J_{}()=(1-)_{s}[Q_{}(s,_{}(s))]+ R_{2}(_{}),\] (20)where \(\) is a hyperparameter to balance the two terms. Additionally, following TD3+BC , we also normalize \(Q_{}\) in the first term in each mini-batch to maintain a balanced scale across tasks.

**Overall algorithm.** Putting everything together, we present our final algorithm in Algorithm 1.

## 6 Experiments

In this section, we conduct several experiments to examine the performance and properties of SCAS. Please refer to Appendices D and E for experimental details and additional results.

### Empirical Evidence of OOD State Correction and OOD Action Suppression

OOD state correction.To examine the OOD state correction ability, we compare the state distributions generated by the learned policies of different algorithms with the state distribution of the offline dataset. In detail, we first train SCAS, CQL , and TD3+BC , and then collect 50,000 samples by running the trained policies separately. We also sample 50,000 states randomly from the offline dataset for comparison. Figures 1(a) to 1(c) plot the state distributions in halfcheetah-medium-expert  with t-SNE , and Figure 1(d) visualizes the optimal value of each state. We access these values from the learned value function obtained by running TD3  online to convergence.

In Figures 1(a) and 1(b), we observe that the policies learned by CQL and TD3+BC tend to produce OOD states. As depicted in Figure 1(d), these OOD states have extremely low values, so entering them can be detrimental to performance. In contrast, the state distribution induced by SCAS is almost entirely within the support of the offline distribution, demonstrating the OOD state correction ability of SCAS. Moreover, we also note that in the low-value area of the offline state distribution (the grey circle in Figure 1(d)), SCAS exhibits a very low state density, which could be attributed to SCAS's value-aware OOD state correction. We refer the reader to Appendix E.2 for additional experiments validating the OOD state correction effects.

OOD action suppression.We empirically evaluate the OOD action suppression effects through the lens of value estimates. We compare SCAS with three baselines: (1) ordinary off-policy RL which is SCAS with \(=0\) (all other implementations are the same); (2) SDC  without additional CQL  term to suppress OOD actions; (3) OSR  without additional CQL term. We conduct experiments on D4RL datasets . Since value over-estimation (divergence) is the main consequence and evidence of OOD actions , we plot the learned Q-values of SCAS and the baselines in Figure 2. We also include the oracle Q-values of SCAS by rollouting the trained policy for \(1,000\) episodes and evaluating the Monte-Carlo return. Additional results are provided in Appendix E.1.

The results show that the learned Q-values of ordinary off-policy RL, SDC without CQL, and OSR without CQL diverge at early learning stages, suggesting that the algorithms suffer from severe OOD actions. By contrast, the learned Q-values of SCAS stay close to the oracle Q-values. This indicates that SCAS regularization alone is able to suppress OOD actions.

### Comparisons on Offline RL Benchmarks

**Tasks.** We evaluate SCAS on D4RL  and NeoRL  benchmarks. In D4RL, we conduct experiments on Gym locomotion tasks and much more challenging AntMaze tasks. Due to the space limit, _the results on NeoRL_ are deferred to Table 4 in Appendix E.3.

Figure 2: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Only SCASâ€™s OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence).

Baselines.We compare SCAS with prior state-of-the-art offline RL methods as well as the ones specifically designed for OOD state correction, including BC , MOPO , OneStep RL , CQL , TD3+BC , IQL , SDC  and OSR .

Hyperparamter tuning.Offline RL methods are appealing for their ability to generate effective policies without online interaction. Nevertheless, many existing offline RL works involve dataset-specific hyperparameter tuning. The reduction of hyperparameter tuning is crucial for improving practical applicability. In this work, SCAS uses _a single set of hyperparameters for all datasets_ in D4RL and NeoRL benchmarks to obtain the reported results.

Comparisons with baselines.On D4RL, comparisons of performance, runtime, and hyperparameter tuning information are shown in Table 1. We refer the reader to Appendix E.8 for learning curve details of SCAS. On the Gym locomotion tasks, SCAS outperforms prior methods on most datasets and achieves the highest total score with a single set of hyperparameters. On the challenging AntMaze tasks, SCAS performs better than IQL and outperforms other methods by a very large margin. In NeoRL (Table 4), SCAS performs comparably to MOBILE  and outperforms other baselines.

Runtime.We present the runtime of algorithms at the bottom of Table 1. SCAS exhibits significantly lower runtime than MOPO, SDC, and OSR and is comparable to other model-free baselines.

Generality.SCAS is a generic model-based regularizer that can be easily integrated into existing offline RL algorithms. The corresponding results and analysis are provided in Appendix E.5.

### Comparisons in Perturbed Environments

In this section, we evaluate the algorithms in a more real-world setting where the agent receives uncertain perturbations during test time. OOD state correction is even more critical in such scenarios since the agent can enter OOD states after perturbation. To simulate this scenario, we add varying steps of Gaussian noise with a magnitude of \(0.5\) to the actions conducted by the policy during test time. Specifically, the policy is trained on standard D4RL datasets but is tested in the perturbed environments. We control the strength of perturbations by adjusting the number of perturbation steps.

   Dataset (v2) & BC & MOPO & OneStep & TD3BC & CQL & IQL & OSR & SDC & SCAS (Ours) \\  halfcheetah-med & 42.0 & **73.1** & 50.4 & 48.3 & 47.0 & 47.4 & 45.1\(\)0.8 & 45.9\(\)0.5 & 46.6\(\)0.2 \\ hopper-med & 56.2 & 38.3 & 87.5 & 59.3 & 53.0 & 66.2 & 62.0\(\)3.6 & 64.7\(\)3.5 & **102.5\(\)0.3** \\ walker2d-med & 71.0 & 41.2 & **84.8** & **83.7** & 73.3 & **80.1\(\)1.8** & **82.7\(\)1.9** & **82.3\(\)3.0** \\ halfcheetah-med-rep & 36.4 & **69.2** & 42.7 & 44.6 & 45.5 & 44.2 & 43.3\(\)0.2 & 45.1\(\)0.5 & 44.0\(\)0.3 \\ hopper-med-rep & 21.8 & 32.7 & **98.5** & 60.9 & 88.7 & 94.7 & 42.1\(\)1.2 & 94.8\(\)6.5 & **101.6\(\)1.0** \\ walker2d-med-rep & 24.9 & 73.7 & 61.7 & **81.8** & **81.8** & 73.8 & **78.1\(\)1.8** & **78.5\(\)6.0** & **78.1\(\)4.5** \\ halfcheetah-med-exp & 59.6 & 70.3 & 75.1 & **90.7** & 75.6 & 86.7 & 63.7\(\)1.45 & 76.3\(\)5.2 & **91.7\(\)2.7** \\ hopper-med-exp & 51.7 & 60.6 & **108.6** & 98.0 & 105.6 & 91.5 & 78.9\(\)1.64 & 99.9\(\)8.5 & **109.7\(\)3.5** \\ walker2d-med-exp & 101.2 & 77.4 & **111.3** & **110.1** & **107.9** & **109.6** & **108.1\(\)4.4** & **109.2\(\)1.4** & **108.4\(\)3.7** \\ halfcheetah-rand & 2.6 & **35.9** & 2.3 & 11.0 & 17.5 & 13.1 & 1.6\(\)0.1 & 14.2\(\)0.7 & 12.2\(\)3.2 \\ hopper-rand & 4.1 & 16.7 & 5.6 & 8.5 & 7.9 & 7.9 & 3.7\(\)2.6 & 3.1\(\)2.8 & **31.4\(\)0.1** \\ walker2d-rand & 1.2 & 4.2 & **6.9** & 1.6 & 5.1 & 5.4 & -0.1\(\)0.0 & 0.2\(\)0.4 & 1.4\(\)1.1 \\  locomotion total & 472.7 & 593.3 & 735.4 & 698.5 & 708.9 & 718.8 & 606.7 & 714.6 & **810.1** \\   antmaze-umaze & 66.8 & 0.0 & 54.0 & 73.0 & 82.6 & **89.6** & 87.4\(\)5.0 & 81.4\(\)3.8 & **90.4\(\)4.3** \\ antmaze-umaze-div & 56.8 & 0.0 & 57.8 & 47.0 & 10.2 & **65.6** & 55.6\(\)8.0 & 49.6\(\)10.4 & **63.8\(\)16.7** \\ antmaze-med-play & 0.0 & 0.0 & 0.0 & 0.0 & 59.0 & **76.4** & 22.6\(\)7.6 & 55.0\(\)9.6 & **76.6\(\)3.9** \\ antmaze-med-div & 0.0 & 0.0 & 0.6 & 0.2 & 46.6 & 72.8 & 19.6\(\)5.8 & 56.6\(\)10.3 & **80.4\(\)5.4** \\ antmaze-large-play & 0.0 & 0.0 & 0.0 & 0.0 & 16.4 & 42.0 & 0.0\(\)0.0 & 20.8\(\)8.0 & **49.0\(\)4.0** \\ antmaze-large-div & 0.0 & 0.0 & 0.2 & 0.0 & 3.2 & 46.0 & 0.0\(\)0.0 & 25.8\(\)7.5 & **50.6\(\)7.2** \\  antmaze total & 123.6 & 0.0 & 112.6 & 120.2 & 218 & 392.4 & 185.2 & 289.2 & **410.8** \\   runtime & 30m & 900m & 120m & 60m & 250m & 100m & 300m & 420m & 140m \\  hyperparameter tuning & **w/o** & w/ & **w/o** & w/ & **w/o** & w/ & w/ & **w/o** \\   

Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.

Figure 3 shows the results of TD3+BC, CQL, SDC, and SCAS on various datasets over five random seeds. We observe that SCAS consistently outperforms previous methods across different perturbation levels and also exhibits less performance degradation against perturbations. Therefore, SCAS enjoys better robustness against perturbations in the complex and unpredictable environments.

### Parameter Study

We examine the effects of the inverse temperature \(\), the balance coefficient \(\), and the noise scale \(\). Due to the space limit, _the results for \(\) and on additional datasets_ are deferred to Appendix E.6. A sensitivity analysis on dynamics model errors is also provided in Appendix E.7.

**Inverse temperature \(\).**\(\) is the key hyperparameter in SCAS for achieving value-aware OOD state correction. If \(=0\), the effect degenerates to vanilla OOD state correction. Figure 4 displays the learning curves of SCAS with different \(\). The results show that **a large \(\) is _crucial_** for achieving good performance (also verified on more tasks in Figure 6), clearly demonstrating the effectiveness of our value-aware OOD state correction. However, too large \(\) (\(=10\)) induces less satisfying performance, probably due to the increased variance of the learning objective.

**Balance coefficient \(\).**\(\) in Eq. (20) controls the balance between vanilla policy improvement and SCAS regularization. We vary \(\) within the range \(\) and present the learning curves of SCAS in Figure 4. Notably, SCAS is able to converge to good performance over a very wide range of \(\) (also verified on more tasks in Figure 7). An interesting finding is that even when \(=1\) and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states.

## 7 Conclusion and Limitations

In this paper, we systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies _OOD state correction_ and _OOD action suppression_. SCAS also achieves _value-aware_ OOD state correction, significantly improving performance over vanilla

Figure 4: Parameter study on the inverse temperature \(\) and the balance coefficient \(\). (a) An appropriately large \(\) is crucial for achieving good performance. (b) The proposed SCAS regularization is essential and demonstrates robustness to changes in \(\).

Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.

OOD state correction. Empirical results validate the properties of SCAS, showcasing its superior performance on the offline RL benchmarks and its enhanced robustness in perturbed environments.

However, our work also has some limitations. For example, current SCAS primarily focuses on continuous control tasks. In discrete settings, algorithmic components like state perturbation strategy would be different, which would be an interesting direction for future work. Moreover, we anticipate employing more advanced dynamics models, such as ensembles  and diffusion models , to further improve the performance of our method.