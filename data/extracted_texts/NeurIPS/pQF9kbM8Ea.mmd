# Leveraging Vision-Centric Multi-Modal Expertise

for 3D Object Detection

 Linyan Huang\({}^{1}\) Zhiqi Li\({}^{2}\) Chonghao Sima\({}^{1}\) Wenhai Wang\({}^{3}\)

Jingdong Wang\({}^{4}\) Yu Qiao\({}^{1}\) Hongyang Li\({}^{1}\)

\({}^{1}\)Shanghai AI Lab \({}^{2}\)Nanjing University \({}^{3}\)CUHK \({}^{4}\)Baidu

###### Abstract

Current research is primarily dedicated to advancing the accuracy of camera-only 3D object detectors (apprentice) through the knowledge transferred from LiDAR- or multi-modal-based counterparts (expert). However, the presence of the domain gap between LiDAR and camera features, coupled with the inherent incompatibility in temporal fusion, significantly hinders the effectiveness of distillation-based enhancements for apprentices. Motivated by the success of uni-modal distillation, an apprentice-friendly expert model would predominantly rely on camera features, while still achieving comparable performance to multi-modal models. To this end, we introduce **VCD**, a framework to improve the camera-only apprentice model, including an apprentice-friendly multi-modal expert and temporal-fusion-friendly distillation supervision. The multi-modal expert **VCD-E** adopts an identical structure as that of the camera-only apprentice in order to alleviate the feature disparity, and leverages LiDAR input as a depth prior to reconstruct the 3D scene, achieving the performance on par with other heterogeneous multi-modal experts. Additionally, a fine-grained trajectory-based distillation module is introduced with the purpose of individually rectifying the motion misalignment for each object in the scene. With those improvements, our camera-only apprentice **VCD-A** sets new state-of-the-art on nuScenes with a score of 63.1% NDS. The code will be released at https://github.com/OpenDriveLab/Birds-eye-view-Perception.

## 1 Introduction

The camera-only 3D perception has garnered increasing attention in autonomous driving perception tasks . Although camera-only models possess the advantages of low deployment cost and ease of widespread application, they still fall behind state-of-the-art models that leverage LiDAR sensors regarding perception accuracy. Researchers have recently employed distillation methods to transfer knowledge from a powerful expert model into a camera-only apprentice model, with the expectation of leveraging the expertise of these stronger expert models to enhance the capability of the camera-only models. Existing 3D perception distillation methods often adopt expert models with the best performance, such as LiDAR-based models  or multi-modal fusion models . However, the presence of the domain gap between LiDAR and camera features hampers knowledge transfer during distillation, resulting in limited improvements in practical applications. An alternative expert model is the large-scale camera-only model . Despite eliminating the domain gap between the camera-only expert model and the apprentice model, the expert model falls short in terms of effectiveness due to the inherent lack of precise geometry information. Likewise, it fails to yield a satisfactory improvement to the apprentice model. Hence, a desirable expert should meet two essential requirements: attaining state-of-the-art performance and minimizing the domain gap.

Furthermore, current distillation methods fall short in compatibility with long-term temporal fusion, which is an essential component in cutting-edge camera-only 3D detectors . Long-termtemporal modeling has shown considerable potential in enhancing the accuracy of depth estimation and detection performance, but it introduces the issue of motion misalignment. Previous methods for BEV distillation have followed two distinct approaches, either distilling the entire BEV space without sufficient attention to the foreground objects  or exclusively distilling the foreground object regions [11; 25], thereby overlooking the motion misalignment issue resulting from the long-term temporal fusion. As shown in Fig. 1 (b), this misalignment occurs when past scenes are transformed into the current scene coordinates based solely on ego-motion, assuming all objects are stationary. While in reality, dynamic objects will cause the misalignment, thus interfering with the temporal fusion features. This is more challenging in the case of long-term temporal fusion. Existing methods, such as StreamPETR , introduce LayerNorm  for dynamic object modeling, but the effects of incorporating velocity and time variables in the model are relatively minor.

To address the aforementioned challenges, we first propose a vision-centric expert, termed as **VCD-E**, which incorporates LiDAR information to enhance the accuracy of depth input. In this context, the term "vision-centric" refers to the utilization of prominent features derived from camera input, distinguishing it from approaches that heavily rely on LiDAR-based features. This model is distinct from conventional multi-modality fusion techniques by eliminating LiDAR backbone. By solely integrating LiDAR depth and long-term temporal fusion under bird's-eye-view (BEV), our model achieves comparable performance to state-of-the-art multi-modal fusion methods  by only encoding image modality. As illustrated in Fig. 1 (a), different from previous fusion methods that adopt two modality-specific backbones, we only leverage a single branch to generate semantic features based on image input, and point clouds only provide depth information. Compared to previous fusion methods, our approach eliminates the need for intricate training strategies or specialized fusion module designs while reaching a comparable performance to current fusion methods [38; 33]. More importantly, the vision-centric multi-modal model has the exact same architecture as the camera-only apprentice model, and the generated BEV spatial representation is solely from image features. The domain gap is significantly alleviated through the distillation of knowledge from the proposed multi-modal expert model to the camera-only apprentice model. Due to the advantageous characteristic of domain consistency, our apprentice model acquires substantial benefits from the expert, surpassing previous distillation methods [11; 46].

To mitigate the incompatibility arising from the motion misalignment of dynamic objects, we further propose a trajectory-based distillation module. In this paper, our primary focus revolves around foreground objects, while simultaneously incorporating a meticulous consideration of their historical trajectories. Specifically, by warping dynamic targets from history to the current frame, we derive the motion trajectory associated with each individual object. Then we use the trajectory of each object to query BEV features of the apprentice and expert models respectively. By leveraging the trajectory features of the expert model to optimize the corresponding features of the apprentice model, the latter can acquire the ability to mitigate the interference arising from motion misalignment. In addition, to

Figure 1: (a) The existing pipelines require camera and LiDAR backbones, while our pipeline eliminates the need for LiDAR. Using point cloud depth, we directly transform image features into BEV space to create a vision-centric expert. (b) The warping of an object in the historical frame into the current timestamp results in a false position in the current frame due to assuming the object is stationary. The green rectangle represents true positives, while the pink rectangle indicates false positives. \(x_{i}\) denotes the various positions of the object in the historical timestamp.

enhance the depth perception ability, we diffuse the depth of the foreground part into the 3D space, modeling occupancy to obtain grid-based supervision to assist in depth prediction for objects.

In summary, we propose the multi-modality expert and camera-only apprentice models, termed as **VCD-E** and **VCD-A**, respectively. Our contributions are summarized as follows:

\(\) We construct a vision-centric multi-modal expert that solely encodes the image modality, eliminating the need for a LiDAR backbone. For the first time, we demonstrate that the expert can deliver performance on par with other state-of-the-art multi-modal methods while being significantly simpler.

\(\) Due to its homogeneous characteristics and superior performance, the vision-centric expert has been proven effective in distilling knowledge to vision-based models. The effects are significant across a range of model sizes, from compact to more extensive architectures.

\(\) We propose trajectory-based distillation and occupancy reconstruction modules, which supervise both static and dynamic objects to alleviate misalignment during long-term temporal fusion. Combined with the constructed expert model, we enhance the performance of the vision-based models and achieve state-of-the-art on the nuScenes val and test leaderboard.

## 2 Related Work

In this section, we review previous studies in the areas of 3D object detection, multi-modality fusion, knowledge distillation, focusing on the techniques and methods most relevant to our research.

3D Object Detection.3D object detection has recently gained significant popularity in the context of autonomous driving and robotics. Detection methods generally fall into two categories: vision-based 3D object detection [64; 65; 5; 37; 52; 36; 53; 58; 22; 56] and LiDAR-based 3D detection [30; 60; 45]. Vision-based approaches [34; 54; 15; 35] exploit image information and frequently involve deep learning techniques to estimate depth. In contrast, LiDAR-based methods capitalize on precise geometric information from LiDAR sensors to achieve superior object detection accuracy. Our proposed vision-centric expert shares the same modality as vision-based detectors, while exhibiting superior performance compared to LiDAR-based detectors.

Long-term modeling has been employed to improve the performance of 3D object detection models [18; 43]. SOLOFusion  utilizes long-term temporal modeling to achieve excellent performance. VideoBEV  maintains comparable performance with SOLOFusion while being more efficient, using long-term recurrent temporal modeling. However, previous research [34; 51] has highlighted that long-term temporal fusion can lead to inadequate detection of dynamic objects. Although StreamPETR  proposes the propagation transformer [48; 7; 9; 8] to conduct object-centric temporal modeling, the improvement of dynamic object modeling remains relatively modest. Our proposed trajectory-based distillation module alleviates this limitation, enabling accurate detection of both static and dynamic objects by camera-based 3D object detection models that utilize long-term modeling.

Multi-modality Fusion.Multi-modality fusion techniques [49; 2; 33; 16] have been extensively investigated to enhance 3D object detection performance by integrating complementary information from different sensor modalities, such as cameras and LiDAR sensors. These methods [38; 59] typically require complex training strategies and the development of specialized fusion modules to effectively merge the distinct sources of information. In contrast, we propose a streamlined architecture that utilizes an image backbone for feature extraction, obviating the need for a LiDAR backbone. This efficient approach augments vision-based models by incorporating LiDAR information, maintaining homogeneity with vision-based models while preserving exceptional performance.

Knowledge Distillation.Knowledge distillation [44; 46] is a technique facilitating the transfer of knowledge from a larger, more complex model (expert) to a smaller, more efficient model (apprentice). This approach has been successfully applied in various domains, including image classification , natural language processing , and policy learning [55; 27; 26; 6]. Therefore, recent distillation methods [17; 25; 32; 24; 29] build upon 3D object detection aims to transfer the accurate geometry knowledge from LiDAR to camera. MonoDistill  projects the LiDAR points into the image plane to serve as the expert to transfer knowledge. BEVSimDet  simulates fusion-based methods to alleviate the domain gap between the two different modalities. BEVDistill  projects the LiDAR points and images into the BEV space to align the LiDAR feature and image feature. Due to the non-homogenous nature between the LiDAR and camera, transferring knowledge from LiDAR to images is challenging. Instead, our work constructs a vision-centric expert, which possesses a homogeneous modality with vision-based models. The vision-centric expert can leverage knowledge distillation to transfer the geometric perception capabilities to various vision-based models, hence enhancing performance accordingly.

## 3 Method

In this section, we present our approach in detail. The overall architecture is presented in Sec. 3.1. Our method involves two main components: (1) the vision-centric expert in Sec. 3.2, and (2) the trajectory-based distillation and occupancy reconstruction modules as elaborated in Sec. 3.3. The pipeline of our method is depicted in Fig. 2.

### Overall Architecture

In this paper, we construct a pair of harmonious expert and apprentice models. The expert and apprentice models adopt the consistent model architecture. The only difference is that the expert additionally leverages the accurate depth map generated from the point cloud, while the apprentice model predicts the depth map from the image. Although our expert model only uses an image backbone to encode high-level scene information, it is on par with state-of-the-art multi-model fusion methods that use several modality-specific backbones and complex interaction strategies. More importantly, we eliminate the domain gap between the multi-model expert and the camera-only apprentice model, which is deemed as one of the most challenging topics in the cross-modality distillation literature.

As illustrated in Fig. 2, we construct a distillation framework between the expert network and the apprentice network. The vision-centric expert fuses feature \(^{E}\) extracted from the image backbone and the temporal depth map \(\) projected from LiDAR points to create a unified BEV representation \(^{E}\) used for 3D object detection. Therefore, although we adopt a cross-modality approach for 3D object detection, the resulting representation remains homogeneous with image modality features.

After obtaining the pretrained vision-centric expert and corresponding apprentice network, we freeze the expert network and leverage its intermediate features as auxiliary supervision for the apprentice network. Since current advanced vision-based detectors employ long-term temporal modeling to attain state-of-the-art performance, we utilize a standard long-term temporal vision-based detector based on BEVDepth as the apprentice model in our context. The expert model also utilizes long-term temporal modeling to ensure consistency and achieve higher performance.

### The Generation of Expert Model

We construct a vision-centric expert by integrating LiDAR information as accurate depth input into a vision-based model. Nevertheless, given the sparsity of LiDAR depth data, we rely on the predicted depth values obtained from images for pixels lacking LiDAR depth information. We also utilize future frames to further improve the performance of the expert model in the offline 3D detection setting. The vision-based detector serves as the primary model, and LiDAR information complements it by providing precise depth information. This approach eliminates the need for complex training strategies or custom-designed fusion modules, streamlining the fusion process.

For the expert model, we project the last sweep LiDAR frame with the current LiDAR frame onto images to obtain corresponding depth maps \(\). Since the depth maps \(\) generated from point clouds can not cover every pixel of the images, we also predict depth distribution for each pixel based on the image features. Then we will project the image features onto the BEV space to obtain BEV features \(^{E}\) based on their depth. Furthermore, we transform the BEV features \(^{E}_{T-N}\) from previous timestamps to the current BEV features \(^{E}_{T}\), modeling the long-term relationship. \(N\) denotes the time interval between the current frame and the history frame. The unified BEV features \(^{E}\) are then combined to produce 3D object detection predictions \(\). The model is trained using a multi-task loss function that considers both 3D detection loss \(_{Det}\) and depth estimation loss \(_{Depth}\).

### The Procedure of Distillation

In this section, we elucidate the methodology adopted to overcome the constraints inherent in long-term modeling for multi-camera 3D object detection. This is achieved through the incorporation of two innovative modules within the distillation process: the trajectory-based distillation module and the occupancy reconstruction module.

Trajectory-based Distillation.The trajectory-based distillation module aims to improve the detection of dynamic objects by focusing on the inconsistent portion of the objects' motion. For \(i\)-th historical frame at timestamp \(t_{i}\) that contains \(K\) objects, extract the \(j\)-th ground truth object position \(_{i}^{j}=(x_{i}^{j},y_{i}^{j},z_{i}^{j},1)^{T}\) in the ego coordinate system. Determine the actual ego motion matrix \(_{i}\) between the current frame \(t_{0}\) and each historical frame \(t_{i}\). Apply the ego-motion transformation matrix \(_{i}\) to the ground truth object positions \(_{i}^{j}\) to obtain the transformed positions \(_{i}^{j^{}}\) in the current frame's coordinate system:

\[_{i}^{j^{}}=_{i}_{i}^{j}.\] (1)

We amalgamate the transformed ground truth object positions, \(_{i}^{j^{}}\), from all historical frames to construct the motion trajectories. The trajectory of \(j\)-th object can thus be represented as a sequence of object positions within the current frame as \((_{1}^{j^{}},_{2}^{j^{}},,_{N}^{j^{}})\), where \(N\) is the number of points on each motion trajectory. Let \(_{ij}^{E}\), \(_{ij}^{A}\) represent the sampled features on identical point \(_{i}^{j^{}}\) from the expert BEV feature \(^{E}\) and apprentice BEV feature \(^{A}\), respectively. They are sampled via bilinear interpretation and then are normalized as :

\[_{ij}^{E}=(^{E}(P_{i}^{j^{}})),_{ij}^{ A}=(^{A}(P_{i}^{j^{}})).\] (2)

Finally, the trajectory-based distillation loss \(_{TD}\) is computed between the normalized key sampled features:

\[_{TD}=_{j=1}^{K}_{i=1}^{N}L_{2}(_ {ij}^{A},_{ij}^{E}).\] (3)

Ultimately, by using the motion trajectory as queries, we conduct trajectory-based distillation on these representative positions. This approach enables the expert to rectify the motion misalignment in the apprentice.

Figure 2: **Algorithm Overview.** Expert utilizes LiDAR data to enhance depth estimation accuracy before view transformation in BEV pipeline. Apprentice represents a standard long-term vision-based detection model. The occupancy is built using the depth information from the expert model, which serves as the supervision for the apprentice model. Motion trajectory is constructed by warping the time series GT query into the current timestamp. Projecting the motion trajectory of each object into BEV space can rectify the misalignment of object motion. With the knowledge transferred from the expert, the apprentice can deliver higher performance than before.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

Effectiveness of the General Framework.We first validate the effectiveness of the general framework for various temporal lengths. In Tab. 4, we examine different timestamps for temporal modeling and find that our method significantly outperforms the baseline. By employing the general framework, we achieve 5.0% NDS and 5.7% mAP performance gains with 8 temporal modeling instances. For other temporal modeling scenarios, our method continues to exhibit substantial performance improvements, ranging from 3.6% to 5.0% NDS. As the duration of temporal fusion extends, the advantages of the model become increasingly pronounced, suggesting that this framework exhibits higher compatibility with extended sequences. This indicates the effectiveness of our proposed trajectory-based distillation module, which alleviates the issue of motion misalignment.

Effectiveness of VCD-E.In this study, it is crucial to verify the effectiveness of our proposed vision-centric experts. We select various expert models for a fair comparison, including LiDAR-based expert Centerpoint , vision-based expert BEVDepth , and fusion-based expert Transfusion, based on our proposed distillation strategy. In Tab. 5, we observe that homogeneous expert models significantly influence the success of knowledge transfer. The camera-based expert BEVDepth demonstrates superior performance gains compared to the other two heterogeneous expert models. Our advanced model, VCD-E, operates within the same modality as vision-based models, and it is equipped with precise geometric information. Consequently, the distillation effect outperforms the Transfusion  expert by a significant margin, achieving a 6.2% increase in mAP. Moreover, it surpasses the BEVDepth expert by an additional 1.7% in NDS. This demonstrates the superior performance and effectiveness of our model.

Comparison to SOTA.To further demonstrate the effectiveness of our proposed distillation strategy, we compare our method with other state-of-the-art methods. To ensure a fair comparison, we conduct all experiments based on our vision-centric expert VCD-E. We find that our method consistently outperforms other distillation methods by a significant margin. In 2D detection, FitNet  and CWD  are two classic distillation methods, we adapt them for 3D object detection to facilitate a fair comparison. BEVDistill  represents a state-of-the-art distillation method for multi-view 3D object detection, and we compare our results with this approach as well. As shown in Tab. 6, our method achieves the best results when compared to these state-of-the-art distillation strategies, demonstrating the effectiveness of our approach. Our method surpasses BEVDistill by 2% NDS and 3.8% mAP.

  Methods & mAP & NDS \\  Baseline  & 0.297 & 0.409 \\ FitNet  & 0.318 & 0.421 \\ CWD  & 0.311 & 0.412 \\ BEVDistill  & 0.316 & 0.439 \\ VCD-A & **0.354** & **0.459** \\  

Table 6: **Effect of different distillation methods. All models are trained with VCD-E as expert. Our proposal significantly surpasses previous SOTA methods.**

  Temporal Length & Distill & mAP (\%)\(\) & NDS (\%) \(\) & mATE\(\) & mAOE\(\) & mAVE\(\) \\   & ✗ & 26.6 & 37.9 & 0.815 & 0.645 & 0.556 \\  & ✓ & 30.1 (+3.5) & 41.5(+3.6) & 0.732 & 0.629 & 0.476 \\   & ✗ & 26.9 & 38.4 & 0.804 & 0.706 & 0.461 \\  & ✓ & 31.3 (+4.4) & 43.2 (+4.8) & 0.717 & 0.615 & 0.403 \\   & ✗ & 28.4 & 39.8 & 0.748 & 0.739 & 0.432 \\  & ✓ & 33.0 (+4.6) & 44.1 (+4.3) & 0.707 & 0.632 & 0.389 \\   & ✗ & 29.7 & 40.9 & 0.762 & 0.714 & 0.415 \\  & ✓ & 35.4 (+5.7) & 45.9 (+5.0) & 0.690 & 0.625 & 0.370 \\  

Table 4: **Ablation study of the proposed distillation framework on different temporal lengths.** The design works with different length of time windows and the gain grows with the temporal length.

  Expert & Paradigm & mAP & NDS \\  - & - & 0.297 & 0.409 \\ CenterPoint  & CM & 0.281 & 0.420 \\ Transfusion  & CM & 0.292 & 0.435 \\ BEVDepth  & UM & 0.341 & 0.442 \\ VCD-E & UM & **0.354** & **0.459** \\  

Table 5: **The performance gains of the apprentice** which benefit from different experts. CM denotes cross-modal and UM represents Uni-modal. It indicates the success of uni-modal distillation remains.

Gains of the Image Backbone.To verify that VCD-E benefits from contemporary image backbones, we conducted experiments outlined in Tab. 7. We selected ResNet-50 and ConvNext-B as two distinct modern image backbones for BEVFusion and VCD-E. Our findings indicate that VCD-E achieves substantial improvements of 5.3% mAP and 3.7% NDS when using ConvNext-B compared to ResNet-50, demonstrating that VCD-E can indeed benefit from modern image backbones. However, existing fusion-based methods, such as BEVFusion, which primarily rely on the capabilities of LiDAR backbones, show limited gains from employing modern image backbones.

Fusion Strategy of Expert.To determine the optimal fusion strategy for LiDAR points and images, we propose four distinct depth fusion approaches, including (1) Predicted depth that directly uses predicted depth based on image features. (2) LiDAR depth that utilizes the projected depth from LiDAR points. (3) Fusion depth that employs the projected depth when corresponding LiDAR points exist for each pixel, otherwise using the predicted scores based on image features, and (4) Weighted depth. Applying the weighted average depth of the predicted depth and LiDAR depth. As demonstrated in Tab. 8, fusion depth yields the best results which indicates that using corresponding predicted scores where LiDAR depth is unavailable is advantageous.

The Effectiveness of Trajectory-based DistillationThe results presented in Table 9 indicate that as the trajectory length increases, the benefits derived from the distillation process become more pronounced. The temporal fusion length for this experiment is set at eight. The first row denotes the baseline model which does not use the distillation method. The second row depicts that the VCD-A model directly conducts distillation under the full BEV feature without the Trajectory-based distillation module. When the trajectory length is set to 1, we only distill ground truth locations in the current frame. However, when the trajectory length exceeds five, there is a noticeable decrease in accuracy. We hypothesize that this decrease may be attributed to the model's distracted attention towards distant motions. The density of traffic can lead to distant motion locations being occupied by other objects, which may not necessarily require additional trajectory supervision. This suggests that the application of excessive trajectory supervision in such scenarios could be unnecessary and inefficient.

Ablation Study of Each Component.In Tab. 10, we conduct an ablation study on the components employed in VCD to verify their contributions to the final result. These components include (1) Long-term temporal fusion, which allows our model to benefit from historical information, thereby enhancing its performance. (2) Trajectory-based distillation, which transfers knowledge from VCD-E to VCD-A in areas with motion misalignment to mitigate this issue. (3) Occupancy reconstruction,

   Methods & Backbone & mAP & NDS \\  BEVFusion & ResNet-50 & 0.598 & 0.662 \\ BEVFusion & ConvNext-B & 0.597 & 0.665 \\ VCD-E & ResNet-50 & 0.611 & 0.656 \\ VCD-E & ConvNext-B & **0.664** & **0.693** \\   

Table 7: **Gains of different image backbone** on multi-modal models . The stronger backbone still demonstrates better performance in this case.

   Trajectory Length & Distill & mAP (\%) & NDS (\%) \\  - & ✗ & 29.7 & 40.9 \\
0 & ✓ & 31.8 & 42.1 \\
1 & ✓ & 33.1 & 44.5 \\
3 & ✓ & 34.6 & 45.6 \\
5 & ✓ & **35.4** & **45.9** \\
9 & ✓ & 33.9 & 44.7 \\   

Table 8: **Gains of different depth fusion strategy.** The proposed fusion depth is optimal among different methods.

   Methods & mAP & NDS \\  Predicted depth & 0.495 & 0.585 \\ LiDAR depth & 0.638 & 0.687 \\ Weighted depth & 0.644 & **0.690** \\ Fusion depth & **0.646** & **0.690** \\   

Table 8: **Gains of different depth fusion strategy.** The proposed fusion depth is optimal among different methods.

which serves as dense depth supervision in the perspective view, improving the performance of VCD-A. Long-term temporal modeling enables our model to reference prior information from history, assisting in object velocity estimation and resulting in a significantly lower mATE. We also evaluate the impact of the trajectory-based distillation module when all other components are incorporated. As shown in Tab. 10, trajectory-based distillation increases NDS by 3.3% and mAP by 3.3%, contributing to the majority of the improvement.

## 5 Conclusion

In this paper, we presented a novel vision-centric model as the expert, which leverages the strengths of both modalities, our proposed model achieves exceptional performance, rivaling that of state-of-the-art multimodal fusion models, while also mitigating domain gap issues, making it suitable for distilling vision-based models. Furthermore, we introduced two innovative modules, the trajectory-based distillation and occupancy reconstruction modules. These modules enhance the geometric perception capabilities of multi-camera 3D object detection models and improve the detection of both static and dynamic objects in the scene. Our experiments demonstrate the effectiveness of our proposed methods on the widely-used nuScenes 3D object detection benchmark.

Limitation.In this work, we have not delved into more details of the vision-centric expert, which may hold significant potential for improvement. Additionally, we have not explored further applications, such as automatic dimension estimation based on VCD-E.