# Simple and Effective

Masked Diffusion Language Models

 Subham Sekhar Sahoo

Cornell Tech, NYC, USA.

ssahoo@cs.cornell.edu

&Marianne Arriola

Cornell Tech, NYC, USA.

ma2238@cornell.edu

&Yair Schiff

Cornell Tech, NYC, USA.

yzs2@cornell.edu

&Aaron Gokaslan

Cornell Tech, NYC, USA.

akg87@cs.cornell.edu

&Edgar Marroquin

Cornell Tech, NYC, USA.

emm392@cornell.edu

&Justin T Chiu

Cornell Tech, NYC, USA.

jtc257@cornell.edu

&Alexander Rush

Cornell Tech, NYC, USA.

ar459@cornell.edu

&Volodymyr Kuleshov

Cornell Tech, NYC, USA.

kuleshov@cornell.edu

###### Abstract

While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form--it is a mixture of classical masked language modeling losses--and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code1, along with a blog post and video tutorial2 on the project page:

https://youtu.be/WjAUX23vgfg

## 1 Introduction

Diffusion models excel at producing realistic, high-quality images and have received significant attention as potential tools for generating discrete data, such as text , biological sequences , and graphs . Unlike autoregressive (AR) approaches, diffusion-based methods are not constrained to generate data sequentially, and therefore have the potential to improve long-term planning, controllable generation, and sampling speed. However, discrete diffusion methods exhibit a performance gap relative to AR models , especially in language modeling. The standard measure of language modeling performance is log-likelihood: when controlling for parameter count, prior work reports a sizable log-likelihood gap between AR and diffusion models.

In this work, we show that simple masked diffusion language modeling (MDLM) combined with effective training recipes is more performant than previously thought . We develop a well-engineered MDLM implementation that significantly improves discrete diffusion log-likelihood; wefurther improve likelihood using a simple substitution-based parameterization of the reverse diffusion process that enables deriving a Rao-Blackwellized continuous-time variational lower bound (ELBO) with improved tightness . Interestingly, our objective has a simple form: it is a weighted average of masked language modeling (MLM) losses , and can be used to endow BERT-style, encoder-only models with principled generation capabilities. We complement this framework with efficient samplers--including ones that can generate semi-autoregressively like a typical language model.

Our masked diffusion models achieve a new state-of-the-art among diffusion models on language modeling benchmarks and approach the perplexity of AR models within 15-25%. Surprisingly, simple engineering choices significantly improve performance in both our models and simple baselines that were previously thought to perform poorly. Our framework also extends to non-language domains, including biological sequence modeling. We pre-train DNA sequence models and observe similar or higher downstream performance compared to classical BERT-style training, while also introducing generative capabilities that classical masked DNA language models lack.

ContributionsWe describe (1) a simple masked diffusion language modeling (MDLM) framework with a well-engineered implementation that outperforms all existing diffusion models across language modeling benchmarks (LM1B , OWT , DNA ), and that significantly improves the performance of existing baselines . Our MDLM framework implements (2a) a substitution-based parameterization (SUBS) of the reverse unmasking diffusion process; SUBS allows us to derive (2b) a simple, continuous-time, Rao-Blackwellized objective that improves tightness and variance of the ELBO, further increasing performance. We complement MDLM with (3) fast samplers that support semi-autoregressive (SAR) generation and outperform previous SAR models.

## 2 Background

### Diffusion Models

Diffusion models are trained to iteratively undo a forward corruption process \(q\) that takes clean data \(\) drawn from the data distribution \(q()\) and defines latent variables \(_{t}\) for \(t\) that represent progressively noisy versions of \(\). The standard forward process for continuous \(\) is

\[_{t}=}+}\] (1)

where \((,)\) and \((_{t})_{t}\) is a noise schedule, monotonically decreasing in \(t\). The parameterized reverse diffusion model \(p_{}\) over \(\) and \(_{t}\) is trained to maximize a variational lower bound on log-likelihood (ELBO). Given a number of discretization steps \(T,\) defining \(s(i)=(i-1)/T\) and \(t(i)=i/T\)

Figure 1: _(Left)_ Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (_Top Right_) In comparison to masked language models (MLM), MDLMâ€™s objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (_Bottom Right_) Perplexity (PPL) on One Billion Words (LM1B) benchmark.

and using \(D_{}[]\) to denote the Kullback-Leibler divergence, the Negative ELBO (NELBO) equals :

\[_{q}\![\!}(|_{ t(0)})}_{_{}}\!+\!^{T}\!D_{ }[q(_{s(i)}|_{t(i)},)]\|_{p_{ }}(_{s(i)}|_{t(i)})]}_{_{}}\!+ \!}[q(_{t(T)}|)\|_{p_{}}( _{t(T)})]}_{_{}}\] (2)

For brevity, we drop \(i\) from \(t(i)\) and \(s(i)\) below; in general, \(s\) will denote the time step before \(t\).

### Discrete Diffusion Models

Applications of diffusion modeling to discrete data can be broken into two broad categories. First are works that embed discrete structures in continuous space and then perform the Gaussian diffusion defined above on these continuous representations [9; 16; 23; 24; 30; 34; 57]. More related to our method are works that define a diffusion process directly on discrete structures. D3PM  introduces a framework with a Markov forward process \(q(_{t}|_{t-1})\!=\!(_{t};\!Q_{t} _{t-1})\) defined by the multiplication of matrices \(Q_{t}\) over \(T\) discrete time steps. This process induces marginals

\[q(_{t}|)\!=\!(_{t};\!Q_{t}) \!=\!(_{t};\!Q_{t}\!\!Q_{t-1}\!\!Q_{1})\] (3)

that represent the discrete-state form of (1). Extending this formalism to continuous time (as in (1)) relies on continuous time Markov chain (CTMC) theory . The CTMC framework in turns leads to generalizations of the score matching perspective on diffusion modeling  to discrete data [33; 59]. Notably, SEDD  connects score-based approaches with ELBO maximization, enabling performant likelihood-based training of score-based models.

## 3 Simple Masked Diffusion Models

While previous work on discrete diffusion supports general forward processes (e.g., general \(Q_{t}\) in D3PM), absorbing state (i.e., masking) diffusion consistently achieves the best performance [1; 33]. In this work, instead of supporting general noise processes, we focus on masking and derive tight Rao-Blackwellized objectives that outperform general approaches and do not require CTMC theory. In this section, we first define the diffusion process for a categorical random variable. Later in Sec. 3.5, we extend this process to sequences containing multiple such categorical variables. We denote our overall approach as Masked Diffusion Language Models (MDLM).

Notation.We denote scalar discrete random variables with \(K\) categories as 'one-hot' column vectors and define \(\{\{0,1\}^{K}:_{i=1}^{K}_{i}=1\}\) as the set of all such vectors. Define \((;)\) as the categorical distribution over \(K\) classes with probabilities given by \(^{K}\), where \(^{K}\) denotes the \(K\)-simplex. We also assume that the \(K\)-th category corresponds to a special [mask] token and let \(\) be the one-hot vector for this mask, i.e., \(_{K}\!=\!1\). Additionally, let \(\!=\!\{1\}^{K}\) and \(,\!\) and \(\) respectively denote the dot and Hadamard products between two vectors \(\) and \(\).

### Interpolating Discrete Diffusion

We restrict our attention to forward processes \(q\) that interpolate between clean data \(\!\!\) and a target distribution \((;)\), forming a direct extension of Gaussian diffusion in (1). Let \(q\) define a sequence of increasingly noisy latent variables \(_{t}\!\!\), where the time step \(t\) runs from \(t\!=\!0\) (least noisy) to \(t\!=\!1\) (most noisy). The marginal of \(_{t}\) conditioned on \(\) at time \(t\) is

\[q(_{t}|)\!=\!(_{t};_{t}\!+\!(1\!-\!_{t})),\] (4)

where \(_{t}\!\![0,\!1]\) is a strictly decreasing function in \(t\), with \(_{0}\!\!1\) and \(_{1}\!\!0\); see Suppl. E.1 for details. This implies transition probabilities \(q(_{t}|_{s})=(_{t};_{t_{1}}_{s}+(1-_{t_{1}}))\), where \(_{t_{|s}}=_{t}/_{s}\). This indicates that during each diffusion step from \(s\!\!t\), a fraction \((1-_{t_{|s}})\) of the probability mass is transferred to the prior distribution \(\). The reverse posterior is given as (see Suppl. 16 for details):

\[q(_{s}|_{t},\!)\!=\!\!(_ {s};\!}_{t}\!+\!(1\!-\!_{t_{|s}})^{}_{t}]\!\![_{s}\!+\!(1 \!-\!_{s})]}{_{t}_{t}^{}\!+ \!(1\!-\!_{t})_{t}^{}})\!.\] (5)

While (4) and (5) represent a special case of the more general diffusion processes proposed in D3PM , we show below that they yield a simplified variational lower bound objective and admit straightforward continuous time extensions.

### Masked Diffusion

Next, we focus on masking processes and derive a simple Rao-Blackwellized objective for this choice of \(q\). This objective incurs lower variance during training and improves tightness.

#### 3.2.1 Forward Masking Process

In masked (i.e., absorbing state) diffusion, we set \(=\). At each noising step, \(t\), the input \(\) transitions to a'masked' state \(\) with some probability. If an input transitions to \(\) at any time \(t^{}\), it will remain in this state for all \(t>t^{}\): \(q(_{t}\,|\,_{t^{}}=)=(_{t};)\). At time \(T\), all inputs are masked with probability 1.

The marginal of the forward process (4) is given by \(q(_{t}\,|)=(_{t};_{t}+( 1-_{t}))\). Using properties of the masking process, the posterior \(q(_{s}|_{t},)\) simplifies (5); see Suppl. A.2:

\[q(_{s}|_{t},)=\{(_{s};_{t})&_{t},\\ _{s};)+(_{s}- _{t})}{1-_{t}}&_{t}=..\] (6)

#### 3.2.2 Reverse Unmasking Process

The reverse process inverts the noise process defined by \(q\). We consider both a finite number of steps \(T\), as well as a continuous time model corresponding to \(T\). We begin with the discrete-time case for which the generative model is expressed as \(p_{}()=_{}p_{}(_{1})p_{}( |_{0})_{i=1}^{T}p_{}(_{s}| _{t})_{0:1}\).

The optimal form for \(p_{}(_{s}|_{t})\) matches the true posterior in (6): this follows immediately from the definition of the diffusion objective in (2), which is a sum of terms of the form \(_{}(q(_{s}|_{t},)\|p_{}( _{s}|_{t}))\). However, (6) is conditioned on \(\), which we do not know. Therefore, we introduce a model \(_{}(_{t},t):^{K}\) that approximates \(\) with a neural network. We can also omit explicit dependence of \(_{}\) on time \(t\), which simplifies sampling, yielding a 2x inference speed-up (see Suppl. E.2).

#### 3.2.3 SUBS Parameterization

The specific parameterization for \(p_{}(_{s}|_{t})\) that we use is

\[p_{}(_{s}|_{t})=q(_{s}|_{t}, =_{}(_{t},t))=\{ (_{s};_{t}),&_{t},\\ _{s};)+(_{s}- _{t})_{}(_{t},t)}{1-_{t}}.& _{t}=,.\] (7)

Furthermore, we induce 2 key properties of the absorbing state diffusion process into our denoising model, \(_{}(_{t},t)\): an unmasked token remains unchanged during reverse diffusion, and the clean input is never masked. We implement these as substitutions to the output of \(_{}(_{t},t)\), hence we call our parameterization SUBS.

Zero Masking ProbabilitiesFirst, notice that by definition, \(,=0\). For this reason, we design the denoising network such that \(_{}(_{t},t),=0\), i.e., we substitute the logit index corresponding to the [mask] token with \(-\).

Carry-Over UnmaskingSecond, if \(_{t}\) is unmasked, then we desire \(_{}(_{t},t)=_{t}\), i.e., unmasked latents are 'carried over'. We accomplish this by substituting the output of our network to simply copy unmasked inputs.

In Suppl. B.1, we show that "Zero Masking Probabilities" property simplifies the D3PM's NELBO (39) to (41), and "Carry-Over Unmasking" futher simplifies (41) to (43) whose continuous time equivalent is the simplified NELBO (10). Table 8 shows that each simplification leads to an improved likelihood.

### Rao-Blackwellized Likelihood Bounds

Recall from (2) that the diffusion traning objective has the form \(_{}+_{}+_{}\). For the simplified reverse process in (7), the discrete-time diffusion loss for finite \(T\) simplifies to (Suppl. B.1.3):

\[_{}=_{i=1}^{T}_{q}_{ }(q(_{s(i)}|_{t(i)},)\|p_{}( _{s(i)}|_{t(i)}))=_{i=1}^{T}_{q} -_{s(i)}}{1-_{t(i)}} _{}(_{t(i)}),\] (8)Note that this objective is simpler and more well-behaved than the expression one would obtain for \(_{}(q(_{s}|_{t},})\|p_{}(_{ s}|_{t}))\) under the parameterization induced by using \(p_{}(_{s}|_{t})=q(_{s}|_{t},}=}_{}(_{t},t))\) from (5), which is similar to what is used by D3PM  (see Suppl. A.2.4):

\[[-_{t}}{1-_{t}}}_{}(_{t},t),} \!+\!(1\!-\!_{t})}{(1\!-\!_{t})}_{}( _{t},t),}}\!+\!}{1\!-\! _{t}})(_{t}}_{ }(_{t},t),}\!+\!(1\!-\!_{t}))}{(1\!- \!_{t})(_{s}}_{}(_{t},t),}\!+\!(1\!-\!_{s}))}]_{t},}\] (9)

We refer to the process of obtaining (8) in lie of (9) as a form of Rao-Blackwellization. Specifically, we analytically compute expectations such as \(}_{}(_{t},t),}\!=\!0\) in order to simplify objective (9) to obtain (8). Without analytical simplifications, a model must learn \(\) such that \(}_{}(_{t},t),}\!=\!0\) holds. Unlike in regular Rao-Blackwellization, simplifications are possible because of modeling choices for \(}_{}(_{t},t)\) (zero masking probabilities and carry-over unmasking). In that sense, our approach has similarities to graphical modeling, where incorporating conditional independencies into \(p_{}\) sets certain log-likelihood terms to zero. However, our approach also empirically helps reduce variance, hence we refer to it as Rao-Blackwellization, somewhat abusing the usual terminology.

### Continuous-Time Likelihood Bounds

Previous works have shown empirically and mathematically that increasing the number of steps \(T\) yields a tighter approximation to the ELBO . Following a similar argument, we form an continuous extension of (8) by taking \(T\!\!\) (see Suppl. B.2), which yields the following NELBO, \(^{}_{}\):

\[^{}_{}\!=\!_{q}_{t=0}^{t=1}^{}}{1\!-\!_{t}}}_{ }(_{t},t),}t\] (10)

Invariance to the noise scheduleThe function \(_{t}\) is invertible due to the monotonicity assumption in Sec. 3.1, and so we can perform the following change of variables in (10): \((1-_{t})\). Thus, the diffusion loss can be equivalently expressed as \(^{}_{}\!=\!-_{q}\!_{=- }^{=0}\!}_{}(_{ },),}\); see Suppl. E.1.1 for details. This new formulation demonstrates that the diffusion loss is invariant to the functional form of \(_{t}\), which we verify empirically in Suppl. E.1.1.

### Masked Diffusion Language Models

Next, we apply masked diffusion to language modeling over sequences \(}^{1:L}\) of \(L\) tokens, with \(}^{}\) denoting the \(\)-th token. We make the assumption that the forward noising process is applied independently across a sequence and that, conditioned on a sequence of latents \(}_{t}^{1:L}\), the denoising process factorizes independently across tokens, i.e., \(p_{}(}_{s}^{1:L}\,|\,}_{t}^{1:L})\!=\!_{ =1}^{L}\!p_{}(}_{s}^{}\,|\,}_{t}^{1:L})\). Thus, we use a single model to compute \(}_{}^{}(}_{t}^{1:L},\!t)\) for each \(\) from a masked sequence \(}_{t}\), optimizing:

\[^{}_{}\!=\!_{q}\!_{t=0}^{t=1}^{}}{1\!-\!_{t}}\!_{}\!} _{}^{}(}_{t}^{1:L},\!t),}^{} t\] (11)

Interestingly, our objective has a simple form: it is the weighted average of masked language modeling (MLM) losses . Thus our work establishes a connection between generative diffusion models and encoder-only BERT models. Our objective enables principled selection of a (randomized) masking rate, and also endows BERT-style models with principled generation capabilities; see Sec. 6. The full training algorithm is provided in Suppl. B.3.

**Note:** Although (11) imposes a loss on all tokens, unmasked tokens don't contribute to the loss, as they are copied over by the denoising network due to "carry-over unmasking" (Sec. 3.2.3), effectively reducing \(}_{}^{}(}_{t}^{1:L},t),}^{}\) to zero.

#### 3.5.1 Training Considerations for Masked Diffusion

One of the key contributions of our work is a well-engineered implementation of masked diffusion models. Our experiments demonstrate that these improvements greatly boost performance even for methods previously thought to perform poorly, e.g., Austin et al. . Below we briefly summarize these implementation details. First, we find that tokenization is critical to performance. Small vocabularies, such as the 8k vocabulary in Austin et al. , result in longer-range dependencies that decrease the performance of both diffusion and AR models. Additionally, by focusing on masked diffusion, we are able to provide a numerically stable implementation of the objective function. Namely, since previous formulations of discrete diffusion were constructed to accommodate a wide range of limiting distributions , the objective was implemented by materializing the full transition matrices \(_{t}\) and posterior probabilities. In contrast, we evaluate \(D_{}[q(_{s}\,|_{t},)]|p_{}( _{s}\,|_{t})|\) by examining only the masked token indices rather than comparing the full true and approximate posterior distributions.

Furthermore, we modernize the architecture for the denoising network relative to D3PM . In lieu of the T5 architecture used in D3PM, we use the diffusion transformer (DiT) introduced in Peebles & Xie , which integrates time step conditioning into a standard encoder-only transformer  and uses rotary positional embeddings . In addition, we implement a low-discrepancy sampler that reduces the variance of the ELBO, similar to Kingma et al.  and draws correlated samples \(t_{i}\) rather than performing i.i.d. sampling.

## 4 Inference and Sampling in Masked Diffusion Language Models

### Efficient Ancestral Sampling

To generate a sequence of length \(L\), the reverse diffusion process starts with the sequence \(_{t=1}^{1:L}\) where \(_{t=1}^{}=\), for all \(\{1,...,L\}\). Then the subsequent latents, \(_{t}^{1:L}\) are generated by discretizing the reverse diffusion process with some finite \(T\). Given \(_{t}^{1:L}\), we construct \(_{s}^{1:L}\) by sampling each token \(_{s}^{}\) independently from the distribution \(p_{}(_{s}^{}|_{t}^{1:L})\) given in (7).

Note that in the reverse process, unmasked tokens remain unchanged. Thus, if no new tokens in \(_{t}^{1:L}\) become unmasked (which can occur often in early denoising stages for large \(T\)), then \(_{s}^{1:L}=_{t}^{1:L}\). Additionally if the denoising model, \(_{}(_{t}^{1:L})\) is not conditioned on time, then we can simply draw a new sample from \(p_{}(_{s-1}^{1:L}|_{s}^{1:L})\) using the previously computed and cached value \(_{}(_{t}^{1:L})\). This means we have effectively "skipped" over the time step \(s\), saving a function call to the denoising network. Note that SEDD  does not support this caching because the denoising network models time-dependent rates, which requires conditioning on time.

### Semi-Autoregressive Masked Diffusion Language Models

Our method also admits an effective semi-autoregressive (SAR) decoding method that allows the model to generate sequences of arbitrary length [52; 53; 24]. Let \(}^{1:L}\) represent the output from sampling a sequence of \(L\) tokens using the reverse diffusion process described above. To generate additional \(L^{}<L\) tokens, we propose a generation algorithm in which the latter \(L-L^{}\) tokens \(}^{L^{}:L}\) are used as a prefix for an additional round of generation. Given the carry-over unmasking described in Sec. 3.2.3, these prefix tokens will simply be copied over at each decoding step. The remaining tokens are generated as above with \(_{s}^{} p_{}(_{s}^{}|_{t}^{L: L+L^{}})\) for all \(\{L+1,...,L+L^{}\}\), with \(_{t=1}^{L-L^{}:L}\) initialized to \(}^{L-L^{}:L}\) as opposed to being initialized as masked tokens \(\). At the end of this process, we have produced \(L+L^{}\) tokens \([}^{1:L},}^{L+1:L+L^{}}]\), where \([]\) denotes concatenation along the sequence length dimension. This process can repeat indefinitely, with the prefix shifted for every new round of generation.

## 5 Experiments

### Masked Diffusion Language Models

Experimental SetupWe evaluate MDLM as a generative model of language and as a representation model via fine-tuning on downstream tasks.

For language modeling likelihood evaluation, we conduct experiments on two datasets: The One Billion Words Dataset (LM1B; ) and OpenWebText (OWT; ). We use the bert-base-uncased tokenizer for LM1B, and report perplexities on the test split. Models have a context size of 128. For OWT, which does not have a pre-defined split, we reserve the last 100K documents as a held-out validation set and report perplexities on this set. We use the GPT2 tokenizer  for OWT. Models have a context size of 1,024. We utilize the transformer architecture from Lou et al. , which augments the diffusion transformer  with rotary embeddings . MDLM was trained for 1M or 10M steps (corresponding to 33B, 327B tokens, respectively) on LM1B and 1M steps on OWT (which corresponds to 262B tokens). The corresponding AR baseline was trained for half the number of stepsto ensure similar number of tokens seen (details in Suppl. D.2). Full hyperparameters are given in Suppl. D.4. On OWT, we train with and without time step conditioning.

For representation learning, we pre-train models on the C4 dataset , then fine-tune and evaluate models on the GLUE benchmark . Models have a context size of 128. We use the bert-base-uncased tokenizer for the representation learning experiments. We utilize the MosaicBERT architecture from Portes et al. , an extension of the original BERT architecture . We pre-train a bidirectional MosaicBERT using an MLM objective for 37B tokens of C4, as well as a causal variant on the same data. We further fine-tune MosaicBERT model using the MDLM for 327M tokens, less than 1% of the pre-training data. We provide the full hyperparameters in Suppl. D.6.

Likelihood EvaluationOn LM1B, MDLM outperforms all previous diffusion methods (Table 1). Compared to the SEDD baseline reported by Lou et al. , trained for 33B tokens, MDLM, which we train for the same amount, achieves a 17% improvement on the perplexity bound. Finally, MDLM gets within 14% of an AR baseline and continues to improve with more training. We see the same trend for models trained on OWT, a larger dataset, shown in Table 2 - MDLM outperforms prior diffusion methods, closing the gap towards AR models. In Table 12 we find that models trained with and without time conditioning attain similar perplexities on OWT. Additionally, Figure 3 demonstrates the reduced variance we achieve from our objective, when compared to previous masked diffusion models such as SEDD .

Zero-Shot Likelihood EvaluationWe also explore models' ability to generalize by taking models trained on OWT and evaluating how well they model unseen datasets. We compare the perplexities of our MDLM with SEDD  and an AR Transformer language model. Our zero-shot datasets include the validation splits of Penn Tree Bank (PTB; ), Wikitext , LM1B, Lambada , AG News , and Scientific Papers (Pubmed and Arxiv subsets; ). Full experimental details are available in Suppl. D.4.

MDLM consistently outperforms the SEDD diffusion parameterization. In some cases, e.g., for Lambada and Scientific Papers, MDLM attains better perplexity than AR. We hypothesize that these datasets are farther from OWT, and that diffusion models may be more robust to out-of-domain evaluation due to the unmasking-based objective.

Downstream Task EvaluationWe find that BERT fine-tuned with MDLM to be a generative model results in strong perplexities while preserving performance on downstream tasks. On the C4 validation set, the AR model attains perplexity (PPL) of 22, the pre-trained BERT attains a PPL upper bound of 78 (evaluated using the MDLM variational bound), and BERT + MDLM-FT attains a PPL upper bound of 35. In Table 4, we further find that BERT + MDLM fine-tuning has no degradation in downstream

    & & Parameters & PPL (\(\)) \\   & Transformer-X Base  & 0.46B & 23.5 \\  & OmniNet\({}_{T}\) & 100M & 21.5 \\   & BERT-Mouth \({}^{}\) & 110M & \(\)142.89 \\  & D3PM (absorb)  & 70M & \(\)76.90 \\  & Diffusion-LM \({}^{}\) & 80M & \(\)118.62 \\  & DiffusionBert  & 110M & \(\)63.78 \\  & SEDD  (33B tokens) & 110M & \(\) 32.79 \\   & Transformer (33B tokens) &  & 22.32 \\  & Transformer (327B tokens) & & 20.86 \\  _Diffusion_ & MDLM (33B tokens) &  & \(\)27.04 \\  & MDLM (327B tokens) & & \(\)**23.00** \\   

Table 1: Test perplexities (PPL; \(\)) on LM1B. \({}^{}\)Reported in He et al. . Best diffusion value is bolded.

    & PPL (\(\)) \\  AR\({}^{}\) & 17.54 \\  SEDD\({}^{}\) & \(\)24.10 \\ MDLM (Ours) & \(\)**23.21** \\   

Table 2: Test perplexities (PPL; \(\)) on OWT for models trained for 262B tokens. \({}^{}\) denotes retrained models.

GLUE performance compared to the BERT initialization. While the perplexity of our method is higher than the AR baseline, the downstream task performance is significantly better.

Semi-Autoregressive ModelingTo test the SAR decoding algorithm presented in Sec. 4.2, we compare to SSD-LM  a diffusion model that was designed to generate blocks of text autoregressively. We generate 200 sequences of length 2048 tokens on a single 3090 GPU and evaluate generative perplexity under a pre-trained GPT-2  model. The SSD-LM sequences are generated using blocks of 25 tokens (as implemented in their pre-trained model) and the MDLM sequences are generated using \(L^{}\!=\!512\). In Table 5, we find that in addition to achieving better generative perplexity, MDLM enables \(\)25-30x faster SAR decoding relative to SSD-LM.

### Masked Diffusion DNA Models

We also explore applications to the generative modeling of biological sequences [14; 47] using a state space model (SSM) backbone . Namely, we build on the recently-proposed Caduceus DNA language model , which uses as a backbone the data-dependent SSM Mamba block .

Experimental SetupWe pre-train the encoder-only Caduceus , which is an MLM, on the HG38 human reference genome  and perform fine-tuning using our diffusion parameterization. We use a context length of 1024 tokens and follow Schiff et al.  for the experimental setup, other than learning rate which was reduced to 1e-3. See Suppl. D.7 for full experimental details. We assess both generative performance using perplexity and downstream performance on Genomics Benchmarks  across language diffusion paradigms and AR models.

Generative PerformanceWe fine-tune the Caduceus MLM across diffusion parameterizations and compare perplexities against AR models. We report perplexity values in Table 6. MDLM outperforms all other diffusion language modeling schemes.

Downstream Task Fine-tuningWe perform downstream evaluation with the Genomics Benchmarks , a recently proposed benchmark with eight regulatory element classification tasks. As shown in Table 7, our generative fine-tuning paradigm preserves or improves upon downstream performance from MLM pre-training. Absorbing-state diffusion methods outperform Plaid across tasks except for the simplest task Human vs. Worm, where all methods have roughly the same performance. For tasks where the input is a biased subsample of the full genome, we observe that the correlation between perplexity and downstream performance is weaker; see Suppl. D.7.

    &  & & & & & & \\  & (m/mm) & QQP & QNLI & SST-2 & COLA & STS-B & MRPC & RTE & Avg \\  AR & 80.94/80.78 & 86.98 & 86.16 & 90.14 & 33.43 & 84.32 & 83.88 & 47.29 & 74.88 \\ BERT & 84.43/85.35 & 88.41 & **90.46** & **92.20** & 54.81 & **88.41** & 89.16 & 61.37 & 81.62 \\ +MDLM-FT & **84.76**/**85.07** & **88.49** & 90.30 & **92.20** & **57.69** & 87.48 & **90.53** & **62.09** & **82.06** \\   

Table 4: GLUE evaluation results. Evaluation measures (\(\)) are F1 score for QQP and MRPC, Spearman correlations for STS-B, and accuracy for the rest. For MNLI, we report match/mismatch accuracies.

    & PTB & Wikitext & LM1B & Lambada & AG News & Pubmed & Arxiv \\  AR (Retrained) & **82.05** & **25.75** & **51.25** & 51.28 & **52.09** & 49.01 & 41.73 \\  SEDD (Retrained) & 100.09 & 34.28 & 68.20 & 49.86 & 62.09 & 44.53 & 38.48 \\ MDLM (Ours) & 95.26 & 32.83 & 67.01 & **47.52** & 61.15 & **41.89** & **37.37** \\   

Table 3: Zero-shot perplexities (\(\)) of models trained for 524B tokens on OWT. All perplexities for diffusion models are upper bounds.

### Ablation Analysis

In Table 8, we can see the effect of our streamlined masked diffusion implementation. The improvements described in Sec. 3.5.1 allow us to greatly reduce perplexity of previously discounted models, such as D3PM (see the bottom row of this table, which is mathematically equivalent to the D3PM formulation). While most works assumed that D3PM achieves mediocre log-likelihoods, we show that is incorrect: our re-implementation almost matches state-of-the-art score-based methods. This introduces a new strong baseline that opens new research opportunities. Additionally, in Table 8, we ablate different components of MDLM. We observe that the perplexity for MDLM trained with a discrete \(T\!=\!1000\) marginally worsens by 0.1 compared to MDLM trained in continuous time. Additionally, removing the "carry over" operation from the SUBS parameterization increases the perplexity by 1.5 points. However, further removing the "zero masking" operation does not lead to any meaningful change in perplexity. We provide further ablations for the continuous time formulation in the Appendix, showing in Table 11 that for a pre-trained model, at inference, increasing \(T\) yields better likelihoods.

## 6 Related Work

Comparison to D3PMMasked diffusion is a strict subset of D3PM ; setting \(Q_{t|s}\!=\!_{t|s}\!+\!(1-_{t|s})^{}\) in their framework yields our forward diffusion. We improve over D3PM in three ways: (1) we adopt the SUBS parameterization; (2) this allows us to derive a simplified objective that analytically simplifies certain expectations to zero; (3) we adopt well-engineered training recipes that improve performance. Both (1) and (2) are possible because we focus on masking instead of developing a general discrete diffusion framework. Surprisingly, (3) has the largest contribution to performance.

    & & Params & PPL (\(\)) \\   & Mamba & 465K & 3.067 \(\).010 \\  & HyenaDNA & 433K & 3.153 \(\).001 \\   & Plaid & 507K & \(\) 3.240 \(\).005 \\  & SEDD & 467K & \(\) 3.216 \(\).003 \\  _Diffusion (Ours)_ & MDLM & 467K & \(\) **3.199**\(\).010 \\   

Table 6: Test perplexities (PPL; \(\)) of generative fine-tuning of the Caduceus MLM  on the HG38 reference genome. Best diffusion model values are bolded. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for fine-tuning.

   Model & Mamba & Caduceus & Caduceus & Caduceus & Caduceus \\ Fine-Tuning Objective & AR & MLM & Plaid & SEDD & MDLM (ours) \\ (Parameter Count) & (465K) & (467K) & (507K) & (467K) & (467K) \\  Mouse Enhancers & \(0.763\,( 0.008)\) & **0.810**\( 0.016\) & \(0.745\,( 0.079)\) & \(0.784\,( 0.058)\) & _0.795_\( 0.029\) \\ Coding vs. Intergenomic & \(0.897\,( 0.004)\) & **0.913**\( 0.003\) & _0.908_\( 0.003\) & **0.913**\( 0.005\) & **0.913**\( 0.003\) \\ Human vs. Worm & \(0.967\,( 0.002)\) & _0.970_\( 0.002\) & **0.971**\( 0.001\) & _0.970_\( 0.003\) & _0.970_\( 0.003\) \\ Human Enhancers Cohn & \(0.734\,( 0.027)\) & \(0.737\,( 0.001)\) & _0.743_\( 0.010\) & **0.746**\( 0.015\) & _0.743_\( 0.016\) \\ Human Enhancer Ensembl & \(0.856\,( 0.003)\) & **0.907**\( 0.000\) & \(0.885\,( 0.003)\) & _0.905_\( 0.006\) & 0.899 \( 0.004\) \\ Human Regulatory & \(0.861\,( 0.008)\) & **0.874**\( 0.003\) & _0.868_\( 0.010\) & 0.828 \( 0.037\) & _0.868_\( 0.004\) \\ Human OCR Ensembl & \(0.806\,( 0.005)\) & _0.821_\( 0.000\) & 0.820 \( 0.004\) & 0.816 \( 0.008\) & **0.823**\( 0.008\) \\ Human NonITA Promotes & \(0.926\,( 0.008)\) & _0.935_\( 0.014\) & _0.935_\( 0.007\) & _0.975_\( 0.014\) & **0.940**\( 0.007\) \\   

Table 7: Genomic Benchmarks. Top-1 accuracy (\(\)) across 5-fold cross-validation (CV) for a pre-trained AR Mamba, and a pre-trained Caduceus model fine-tuned with different diffusion parameterizations. The best values per task are bolded and the second best are italicized. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for CV.

    & PPL (\(\)) \\  MDLM (47) & \(\) \\ w/o continuous time (43) & \(27.19.07\) \\ \& w/o carry-over (41) & \(28.56.15\) \\ \& w/o zero masking (39) & \(28.51.15\) \\   

Table 8: Test perplexities (PPL; \(\)) for MDLM ablations on LM1B. For the discrete-time models, we use \(T=1000\). Standard deviation is measured over 5 seeds during evaluation.

Comparison to CTMCMost implementations of diffusion work best in continuous time. However, extending D3PM in this way requires computing the limit of the product of an infinite number of matrices \(Q_{T} Q_{T-1} Q_{t}\) as \(T\!\!\), which requires advanced CTMC theory . Our work describes simple continuous-time formulations for the most common noise processes (e.g., masking and uniform \(\)), thus helping make an important part of the literature more accessible. In Suppl. C, we show that our results are compatible with CTMC, using the rate forward matrix \(R_{t}\!=\!^{}}{_{t}}(\!-\! ^{})\) and the reverse rate \(_{t}(^{}\!,\!)\) for the transition \(\!\!^{}\), where \(\!,\!^{}\!\!\):

\[_{t}(^{}\!,\!)\!=\!-^{ }}{1\!-\!_{t}}[^{}]^{}[_{}( ,t)\!-\!],\!\] (12)

Comparison to Score EstimationScore-based approaches to diffusion  extend to discrete states, although they typically further build upon advanced CTMC theory. In particular, SEDD  optimizes an ELBO3 that is a function of the score model, obtaining state-of-the-art log-likelihoods among diffusion models. Our approach, however, is much simpler and does not require advanced theory. Furthermore, we can extract the score for MDLM (76), as demonstrated in Suppl. C.3, making it compatible with various techniques designed for score-based algorithms, such as samplers , score parameterization , efficient designs of the denoising network , guidance techniques, and more.

Comparison to BERTOur work provides a principled way of making BERT generative when trained with randomized masking rates. Previous work on generating from BERT used Gibbs sampling or ad-hoc methods [17; 32; 64]. The connection between BERT and diffusion was first made by Austin et al. : their objective effectively involves unmasking. He et al.  additionally starts training from a pretrained BERT. However, both works use an objective that is similar to (9), which is less numerically stable than our objective (see Section 3.5.1). Austin et al.  mention in their appendix that their ELBO simplifies to a weighted masking (MLM) loss similar to (8), but it uses a more complex formula for the weights and is limited to the discrete time setting unlike our work. Furthermore, they do not train with that objective. Our work derives a simpler expression for the average of MLM losses, implements it, and obtains better likelihoods.

Comparison to Latent Diffusion LMsIn contrast to this work, which defines diffusion over discrete structures, Plaid  and Diffusion LM  define a Gaussian diffusion process over word embeddings. Zhang et al.  and Hu et al.  extend this approach to flow matching over word embeddings, enabling the design of faster samplers. Discrete Flow Matching (DFM)  applies flow matching to discrete structures, using a cross-entropy loss as their training objective: \(-_{q,t}_{}(^{1:l}|_{t}^{1:L})\). Similar to Chang et al. , DFM's objective, while effective, is not weighted to serve as a proper ELBO. In MDLM, however, we derive a tight, principled lower bound on the log-likelihood.

Concurrent WorksConcurrent to our work, Shi et al.  and Ou et al.  derive a similar simplified objective for masked diffusion processes. While Ou et al.  start from a score matching perspective, we tackle this problem from a variational lens similar to Shi et al. . Similar to Ou et al. , we formulate efficient samplers in Section 4.1 by leveraging a time-independent denoising network.

A key differentiation between our work and that of Shi et al. , Ou et al.  is the semi-autoregressive decoding method we present in Section 4.2. While [51; 40] are restricted to sample sequences of a fixed length, we propose samplers to generate arbitrary lengths of text like a traditional language model. Furthermore, we establish the connection between our simplified objective and the masked language modeling (MLM) objective. As a result, we endow BERT-style models with principled generation capabilities while maintaining representation learning capabilities. Whereas [51; 40] only evaluate on NLP datasets, we show that masked diffusion is also effective in modeling biological sequences.

## 7 Conclusion

In this work, we explore masked diffusion. With a well-engineered implementation that supports a simple variational objective, we attain state-of-the-art diffusion perplexities on language benchmarks and demonstrate how to efficiently convert BERT-style encoders into generative models. Given we are working on language modeling, we carry any of the inherent risks and opportunities that come with this line of research.