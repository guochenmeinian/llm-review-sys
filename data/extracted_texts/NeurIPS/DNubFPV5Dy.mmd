# Weakly Coupled Deep Q-Networks

Ibrahim El Shar

Hitachi America Ltd., University of Pittsburgh

Sunnyvale, CA

ibrahim.elshar@hal.hitachi.com &Daniel Jiang

Meta, University of Pittsburgh

New York, NY

drjiang@meta.com

###### Abstract

We propose _weakly coupled deep Q-networks_ (WCDQN), a novel deep reinforcement learning algorithm that enhances performance in a class of structured problems called _weakly coupled Markov decision processes_ (WCMDP). WCMDPs consist of multiple independent subproblems connected by an action space constraint, which is a structural property that frequently emerges in practice. Despite this appealing structure, WCMDPs quickly become intractable as the number of subproblems grows. WCDQN employs a single network to train multiple DQN "subagents," one for each subproblem, and then combine their solutions to establish an upper bound on the optimal action value. This guides the main DQN agent towards optimality. We show that the tabular version, weakly coupled Q-learning (WCQL), converges almost surely to the optimal action value. Numerical experiments show faster convergence compared to DQN and related techniques in settings with as many as 10 subproblems, \(3^{10}\) total actions, and a continuous state space.

## 1 Introduction

Despite achieving many noteworthy and highly visible successes, it remains widely acknowledged that practical implementation of reinforcement learning (RL) is, in general, challenging . This is particularly true in real-world settings where, unlike in simulated settings, interactions with the environment are costly to obtain. One promising path toward more sample-efficient learning in real-world situations is to incorporate known structural properties of the underlying Markov decision process (MDP) into the learning algorithm. As elegantly articulated by , structural properties can be considered a type of "side information" that can be exploited by the RL agent for its benefit. Instantiations of this concept are plentiful and diverse: examples include factored decompositions [33; 10; 47], latent or contextual MDPs [21; 39; 52], block MDPs , linear MDPs , shape-constrained value and/or policy functions [49; 37; 31], MDPs adhering to closure under policy improvement , and multi-timescale or hierarchical MDPs [23; 13], to name just a few.

In this paper, we focus on a class of problems called _weakly coupled MDPs_ (WCMDPs) and show how one can leverage their inherent structure through a tailored RL approach. WCMDPs, often studied in the field of operations research, consist of multiple subproblems that are independent from each other except for a coupling constraint on the action space . This type of weakly coupled structure frequently emerges in practice, spanning domains like supply chain management , recommender systems , online advertising , revenue management , and stochastic job scheduling . Such MDPs can quickly become intractable when RL algorithms are applied naively, given that their state and action spaces grow exponentially with the number of subproblems .

One can compute an upper bound on the optimal value of a WCMDP by performing a Lagrangian relaxation on the action space coupling constraints. Importantly, the weakly coupled structure allows the relaxed problem to be _completely decomposed_ across the subproblems, which are significantly easier to solve than the full MDP [24; 1]. Our goal in this paper is to devise a method that can integrate the Lagrangian relaxation upper bounds into the widely adopted value-based RL approaches of Q-learning and deep Q-networks (DQN) . Our proposed method is, to our knowledge, the first to explore the use of Lagrangian relaxations to tackle general WCMDPs in a fully model-free, deep RL setting.

**Main contributions.** We make the following methodological and empirical contributions.

1. First, we propose a novel deep RL algorithm, called _weakly coupled deep Q-networks_ (WCDQN), that exploits weakly coupled structure by using a set of _subagents_, each attached to one of the subproblems, whose solutions are combined to help improve the performance of main DQN agent; see Figure 1 for a high-level overview.
2. Second, we also propose and analyze a tabular version of our algorithm called _weakly coupled Q-learning_ (WCQL), which serves to conceptually motivate WCDQN. We show that WCQL converges almost surely to the optimal action-value.
3. Finally, we conduct numerical experiments on a suite of realistic problems, including electric vehicle charging station optimization, multi-product inventory control, and online stochastic ad matching. The results show that our proposed algorithm outperform baselines by a relatively large margin in settings with as many as 10 subproblems, \(3^{10}\) total actions, and a continuous state space.

## 2 Related Literature

**Weakly Coupled MDPs.** This line of work began with  under the name of _restless multi-armed bandits_ (RMAB), where there are two actions ("active" or "passive") for each subproblem (also known as "project" or "arm"), under a budget constraint on the number of active arms at any given time.1 As we will see soon, this is a special case of a WCMDP with two actions per subproblem and a single budget constraint. A popular solution approach to RMABs is the _Whittle index_ policy, which was first proposed by  and uses the idea of ranking arms by their "marginal productivity." The policy has been extensively studied in the literature from both applied and theoretical perspectives [20; 41; 29; 28; 42; 64]. Whittle conjectured in  that the Whittle index policy is asymptotically optimal under a condition called _indexability_; later,  established that asymptotic optimality requires indexability, but also another technical condition, both of which are difficult to verify.

Figure 1: An illustration of our RL approach for WCMDPs. Our approach takes a single “full” transition \(\) (as collected by a standard RL agent) and decomposes it into subproblem transitions \(_{i}\) that are passed to “subagents,” which are powered by a single network and aim to solve the easier subproblems. The results of these subagents are then used collectively to guide the main agent toward the optimal policy, whose actions need to satisfy _linking constraints_. Here, we illustrate the case of a single linking constraint that requires the sum of the actions to be bounded by a right-hand-side quantity \((w)\), where \(w\) is an exogenous state from the environment.

As discussed in detail by , relying on the Whittle index policy in real-world problems can be problematic due to hard-to-verify technical conditions (and if not met, computational robustness and the heuristic's original intuitive motivation may be lost).

A number of recent papers have considered using RL in the setting of RMABs, but nearly all of them are based on Whittle indices [19; 46; 34; 35; 3; 50; 62], and are thus most useful when the indexability condition can be verified. Exceptions are  and , which propose to run RL directly on the Lagrangian relaxation of the true problem to obtain a "Lagrange policy." Our paper is therefore closest in spirit to these two works, but our methods target the optimal value and policy (with the help of Lagrangian relaxation) rather than pursuing the Lagrange policy as the end goal (which does not have optimality guarantees in general). Moreover, compared to the other RL approaches mentioned above, we do not require the indexability condition and our method works for any WCMDP.

Relaxations of WCMDPs can be performed in several different ways, including approximate linear programming (ALP) [1; 11], network relaxation , and Lagrangian relaxation [61; 53; 24; 7; 1; 54; 11]. Notably,  provided key results for the ALP and Lagrangian relaxation approaches, and  gave theoretical justification for the closeness of the bounds obtained by the approximate linear programming and Lagrangian relaxation approaches, an empirical observation made in . Our work focuses specifically on the Lagrangian relaxation approach, which relaxes the linking constraints on the action space by introducing a penalty in the objective.

**DQN and Q-learning.** The Q-learning algorithm  is perhaps the most popular value-based tabular RL algorithm [30; 55; 6], and the DQN approach of  extends the fundamental ideas behind Q-learning to the case where Q-functions are approximated using deep neural networks, famously demonstrated on a set of Atari games. Unfortunately, practical implementation of Q-learning, DQN, and their extensions on real-world problems can be difficult due to the large number of samples required for learning .

Various papers have attempted to extend and enhance the DQN algorithm. For example, to overcome the over-estimation problem and improve stability,  proposes _double DQN_, which adapts the tabular approach of _double Q-learning_ from  to the deep RL setting. The main idea is to use a different network for the action selection and evaluation steps.  modifies the experience replay buffer sampling to prioritize certain tuples, and  adds a "dueling architecture" to double DQN that combines two components, an estimator for the state value function and an estimator for the state-dependent action advantage function. Other examples include _bootstrapped DQN_, _amortized Q-learning_, _distributional RL_, and _rainbow DQN_.

Our approach, WCDQN, is also an enhancement of DQN, but differ from the above works in that our focus is on modifying DQN to exploit the structure of a class of important problems that are otherwise intractable, while the existing papers focus on improvements made to certain components of the DQN algorithm (e.g., network architecture, experience replay buffer, exploration strategy). In particular, it should be possible to integrate the main ideas of WCDQN into variations of DQN without much additional work.

**Use of constraints and projections in RL.** WCDQN relies on constraining the learned Q-function to satisfy a learned upper bound. The work of  uses a similar constrained optimization approach to enforce upper and lower bounds on the optimal action value function in DQN. Their bounds are derived by exploiting multistep returns of a general MDP, while ours are due to dynamically-computed Lagrangian relaxations.  also does not provide any convergence guarantees for their approach.

In addition,  proposed a convergent variant of Q-learning that leverages upper and lower bounds derived using the information relaxation technique of  to improve performance of tabular Q-learning. Although our work shares the high-level idea of bounding Q-learning iterates,  focused on problems with _partially known transition models_ (which are necessary for information relaxation) and the approach did not readily extend to the function approximation setting. Besides focusing on a different set of problems (WCMDPs), our proposed approach is model-free and naturally integrates with DQN.

## 3 Preliminaries

In this section, we give some background on WCMDPs, Q-learning, DQN, and the Lagrangian relaxation approach. All proofs throughout the rest of the paper are given in Appendix A.

### Weakly Coupled MDPs

We study an infinite horizon WCMDP with state space \(=\) and finite action space \(\), where \(\) is the _endogenous_ part (i.e., affected by the agent's actions) and \(\) is the _exogenous_ part (i.e., unaffected by the agent's actions) of the full state space. We use the general setup of WCMDPs from . A WCMDP can be decomposed into \(N\) subproblems. The state space of subproblem \(i\) is denoted by \(_{i}=_{i}\) and the action space is denoted by \(_{i}\), such that

\[=_{i=1}^{N}_{i}= _{i=1}^{N}_{i}.\]

In each period, the decision maker observes an exogenously and independently evolving state \(w\), along with the endogenous states \(=(x_{1},x_{2},,x_{N}),\) where \(x_{i}_{i}\) is associated with subproblem \(i\). Note that \(w\) is shared by all of the subproblems, and this is reflected in the notation we use throughout the paper, where \(=(,w)\) represents the full state and \(s_{i}=(x_{i},w)\) is the state of subproblem \(i\). In addition to the exogenous state \(w\) being shared across subproblems, there also exist \(L\)_linking_ or _coupling constraints_ that connect the subproblems: they take the form \(_{i=1}^{N}(s_{i},a_{i})(w)\), where \((s_{i},a_{i}),(w)^{L}\) and \(a_{i}_{i}\) is the component of the action associated with subproblem \(i\). The set of feasible actions for state \(\) is given by

\[()=:_{i=1}^{N}(s_{i}, a_{i})(w)}.\] (1)

After observing state \(=(,w)\), the decision maker selects a feasible action \(()\).

The transition probabilities for the endogenous component is denoted \(p(^{}\,|\,,)\) and we assume that transitions are conditionally independent across subproblems:

\[p(^{}\,|\,,)=_{i=1}^{N}\,p_{i}(x_{i}^{}\,|\,x _{i},a_{i}),\]

where \(p_{i}(x_{i}^{}\,|\,x_{i},a_{i})\) are the transition probabilities for subproblem \(i\). The exogenous state transitions according to \(q(w^{}\,|\,w)\). Next, let \(r_{i}(s_{i},a_{i})\) be the reward of subproblem \(i\) and let \((,)=\{r_{i}(s_{i},a_{i})\}_{i=1}^{N}.\) The reward of the overall system is additive: \(r(,)=_{i=1}^{N}r_{i}(s_{i},a_{i})\).

Given a discount factor \([0,1)\) and a feasible policy \(:\) that maps each state \(\) to a feasible action \(()\), the value (cumulative discounted reward) of following \(\) when starting in state \(\) and taking a first action \(\) is given by the action-value function \(Q^{}(,)=_{t=0}^{}^{t}r( _{t},_{t})\,|\,,_{0}=,_{0}=.\) Our goal is to find an optimal policy \(^{*}\), i.e., one that maximizes \(V^{}()=Q^{}(,())\). We let \(Q^{*}(,)=_{}Q^{}(,)\) and \(V^{*}()=_{}V^{}()\) be the optimal action-value and value functions, respectively. It is well-known that the optimal policy selects actions in accordance to \(^{*}()=_{}Q^{*}(,)\) and that the Bellman recursion holds:

\[Q^{*}(,)=r(,)+\,_{^{ }(^{})}Q^{*}(^{},^{}) ,\] (2)

where \(^{}=(^{},w^{})\) is distributed according to \(p(\,|\,,)\) and \(q(\,|\,w)\).

### Q-learning and DQN

The Q-learning algorithm of  is a tabular approach that attempts to learn the optimal action-value function \(Q^{*}\) using stochastic approximation on (2). Using a learning rate \(_{n}\), the update of the approximation \(Q_{n}\) from iteration \(n\) to \(n+1\) is:

\[Q_{n+1}(_{n},_{n})=Q_{n}(_{n},_{n})+_{n}(_{ n},_{n})y_{n}-Q_{n}(_{n},_{n}),\]

where \(y_{n}=r_{n}+_{^{}}Q_{n}(_{n+1},^{})\) is the _target_ value, computed using the observed reward \(r_{n}\) at \((_{n},_{n})\), the transition to \(_{n+1}\), and the current value estimate \(Q_{n}\).

The DQN approach of  approximates \(Q^{*}\) via a neural network \(Q(,;)\) with network weights \(\). The loss function used to learn \(\) is directly based on minimizing the discrepancy between the two sides of (2):

\[l()=_{,}y-Q(,; )^{2},\]

where \(y=r(,)+\,_{^{}}Q(^{ },^{};^{-})\), \(^{-}\) are frozen network weights from a previous iteration, and \(\) is a _behavioral distribution_. In practice, we sample experience tuples \((_{n},_{n},r_{n},_{n+1})\) from a replay buffer and perform a stochastic gradient update:

\[_{n+1}=_{n}+_{n}y_{n}-Q(_{n},_{n};) _{}Q(_{n},_{n};),\]

with \(y_{n}=r_{n}+_{^{}}Q(_{n+1},^{};^{ -})\). Note the resemblance of this update to that of Q-learning.

### Lagrangian Relaxation

The Lagrangian relaxation approach decomposes WCMDPs by relaxing the linking constraints to obtain separate, easier-to-solve subproblems . The main idea is to dualize the linking constraints \(_{i=1}^{N}(s_{i},a_{i})(w)\) using a penalty vector \(_{+}^{L}\). The result is an augmented objective consisting of the original objective plus additional terms that penalize constraint violations. The Bellman equation of the relaxed MDP in (2) is given by:

\[Q^{}(,)=r(,)+^{} (w)-_{i=1}^{N}(s_{i},a_{i})+\,_{ ^{}}Q^{}(^{},^{}) .\] (3)

With the linking constraints removed, this relaxed MDP can be decomposed across subproblems, so we are able to define the following recursion for each subproblem \(i\):

\[Q^{}_{i}(s_{i},a_{i})=r_{i}(s_{i},a_{i})-^{}(s_{ i},a_{i})+\,[_{a_{i}^{}_{i}}Q^{ }_{i}(s_{i}^{},a_{i}^{})].\] (4)

It is well-known from classical results that any penalty vector \( 0\) produces an MDP whose optimal value function is an upper bound on the \(V^{*}()\)[24; 1]. The upcoming proposition is a small extension of these results to the case of action-value functions, which is necessarily for Q-learning.

**Proposition 1**.: _For any \( 0\) and \(\), it holds that \(Q^{*}(,) Q^{}(,)\) for any \(()\). In addition, the Lagrangian action-value function of (3) satisfies_

\[Q^{}(,)=^{}(w)+_{i=1}^{N}Q^{ }_{i}(s_{i},a_{i})\] (5)

_where \(Q^{}_{i}(s_{i},a_{i})\) is as defined in (4) and \((w)\) satisfies the recursion_

\[(w)=(w)+\,(w^{}),\] (6)

_with the exogenous next state \(w^{}\) is distributed according to \(q(\,|\,w)\)._

The first part of the proposition is often referred to as _weak duality_ and the second part shows how the Lagrangian relaxation can be solved by decomposing it across subproblems, dramatically reducing the computational burden. The tightest upper bound is the solution of the _Lagrangian dual problem_, \(Q^{^{*}}(,)=_{ 0}Q^{}(,)\), where \(^{*}\) is minimizer.

## 4 Weakly Coupled Q-learning

In this section, we introduce the tabular version of our RL algorithm, called _weakly coupled Q-learning_ (WCQL), which will illustrate the main concepts of the deep RL version, WCDQN.

### Setup

We first state an assumption on when the linking constraint (1), which determines the feasible actions given a state, is observed.

**Assumption 1** (Linking constraint observability; general setting).: Suppose that upon landing in a state \(=(,w)\), the agent observes the possible constraint left-hand-side values \((s_{i},)\) for every \(i\), along with the constraint right-hand-side \((w)^{L}\).

Under Assumption 1, the agent is able to determine the feasible action set \(()\) upon landing in state \(\). Accordingly, it can always take a feasible action. In many cases, it is known in advance that the feasible action set is of the _multi-action RMAB_ form: there is a single linking constraint (i.e., \(L=1\)) and the left-hand-side is the sum of subproblem actions (i.e., \((s_{i},a_{i})=a_{i}\)). In that case, Assumption 1 reduces to the following simpler statement, which we state for completeness.

**Assumption 1\({}^{}\)** (Linking constraint observability; multi-action RMAB setting).: Suppose that we are in a multi-action RMAB setting. When the agent lands in a state \(=(,w)\), it observes the constraint right-hand-side \((w)\).

In the numerical example applications of Section 6, for illustrative simplicity, we choose to focus on single-constraint settings where Assumption 1\({}^{}\) is applicable. Note that the "difficulty" of WCMDPs is largely determined by the number of subproblems and the size of the feasible set compared to the full action space, not necessarily by the _number_ of linking constraints. In each of our example applications, Assumption 1 naturally holds: for example, in the EV charging problem, there are a limited number of available charging stations (which is always observable).

An important part of WCQL is to track an estimate of \(Q^{^{*}}(,)\), the result of the Lagrangian dual problem. To approximate this value, we replace the minimization over all \( 0\) by optimization over a finite set of possible multipliers \(\), which we consider as an input to our algorithm. In practice, we find that it is most straightforward to use \(=^{}\), where \(^{}\) is the all ones vector and \(^{}\), but from the algorithm's point of view, any set \(\) of nonnegative multipliers will do.

We denote an experience tuple for the entire WCMDP by \(=(,,,,^{})\). Similarly, we let \(_{i}=(s_{i},a_{i},r_{i},s^{}_{i})\) be the experience relevant to subproblem \(i\), as described in (4). Note that \(\) is excluded from \(_{i}\) because it does not enter subproblem Bellman recursion.

### Algorithm Description

The WCQL algorithm can be decomposed into three main steps.

**Subproblems and subagents.** First, for each subproblem \(i\{1,2,,N\}\) and every \(\), we attempt to learn an approximation of \(Q^{}_{i}(s_{i},a_{i})\) from (4), which are the \(Q\)-values of the unconstrained subproblem associated with \(\). We do this by running an instance of Q-learning with learning rate \(_{n}\). Letting \(Q^{}_{i,n}\) be the estimate at iteration \(n\), the update is given by:

\[Q^{}_{i,n+1}(s_{i},a_{i})=Q^{}_{i,n}(s_{i},a_{i})+_{n}(s_{i},a_{i})y^{}_{i,n}-Q^{}_{i,n}(s_{i},a_{i}),\] (7)

where the target value is defined as \(y^{}_{i,n}=r_{i}(s_{i},a_{i})-^{}(s_{i},a_{i})+ _{a^{}_{i}}Q^{}_{i,n}(s^{}_{i},a^{}_{i})\).

Note that although we are running several Q-learning instances, they all make use of a _common experience tuple_\(\) split across subproblems, where subproblem \(i\) receives the portion \(_{i}\). We remind the reader that each subproblem is dramatically simpler than the full MDP, since it operates on smaller state and action spaces (\(_{i}\) and \(_{i}\)) instead of \(\) and \(\).

We refer to the subproblem Q-learning instances as _subagents_. Therefore, each subagent is associated with a subproblem \(i\) and a penalty \(\) and aims to learn \(Q^{}_{i}\).

**Learning the Lagrangian bounds.** Next, at the level of the "main" agent, we combine the approximations \(Q^{}_{i,n+1}\) learned by the subagents to form an estimate of the Lagrangian action-value function \(Q^{}\), as defined in (5). To do so, we first estimate the quantity \((w)\) of Proposition 1. This can be done using a stochastic approximation step with a learning rate \(_{n}\), as follows:

\[_{n+1}(w)=_{n}(w)+_{n}(w)(w)+_{n}(w^{ })-_{n}(w),\] (8)

where we recall that \(w\) and \(w^{}\) come from the experience tuple \(\), embedded within \(\) and \(^{}\). Now, using Proposition 1, we approximate \(Q^{}(,)\) using

\[Q^{}_{n+1}(,)=^{}_{n+1}(w)+_{i=1 }^{N}Q^{}_{i,n+1}(s_{i},a_{i}).\] (9)

Finally, we estimate an upper bound on \(Q^{*}\) by taking the minimum over \(\):

\[Q^{^{*}}_{n+1}(,)=_{}Q^{}_{n+1 }(,).\] (10)

**Q-learning guided by Lagrangian bounds.** We would now like to make use of the learned upper bound \(Q^{^{*}}_{n+1}(,)\) when performing Q-learning on the full problem. Denote the WCQL estimate of \(Q^{*}\) at iteration \(n\) by \(Q^{}_{n}\). We first make a standard update towards an intermediate value \(Q_{n+1}\) using learning rate \(_{n}\):

\[Q_{n+1}(,)=Q^{}_{n}(,)+_{n}(,) y_{n}-Q^{}_{n}(,).\] (11)

where \(y_{n}=r(,)+_{^{}(^{ })}Q^{}_{n}(^{},^{})\). To incorporate the bounds that we previously estimated, we then project \(Q_{n+1}(,)\) to satisfy the estimated upper bound:

\[Q^{}_{n+1}(,)=Q^{^{*}}_{n+1}(,) Q_{n +1}(,),\] (12)where \(a b=\{a,b\}\). The agent now takes an action in the environment using a behavioral policy, such as the \(\)-greedy policy on \(Q^{}_{n+1}(,)\).

The motivation behind this projection is as follows: since the subproblems are significantly smaller in terms of state and action spaces compared to the main problem, the subagents are expected to quickly converge. As a result, our upper bound estimates will get better, improving the the action-value estimate of the main Q-learning agent through the projection step. In addition, WCQL can enable a sort of "generalization" to unseen states by leveraging the weakly-coupled structure. The following example illustrates this piece of intuition.

**Example 1**.: Suppose a WCMDP has \(N=3\) subproblems with \(_{i}=\{1,2,3\}\) and \(_{i}=\{1,2\}\) for each \(i\), leading to \(3^{3} 2^{3}=216\) total state action pairs. For the sake of illustration, suppose that the agent has visited states \(=(1,1,1)\), \(=(2,2,2)\), and \(=(3,3,3)\) and both actions from each of these states. This means that from the perspective of every subproblem \(i\), the agent has visited all state-action pairs in \(_{i}_{i}\), which is enough information to produce an estimate of \(Q^{}_{i}(s_{i},a_{i})\) for all \((s_{i},a_{i})\) and, interestingly, an estimate of \(Q^{^{*}}(,)\) for _every_\((,)\), despite having visited only a small fraction (\(6/216\)) of the possible state-action pairs. This allows the main Q-learning agent to make use of upper bound information at every state-action pair via the projection step (12). The main intuition is that these upper bound values are likely to be more sensible than a randomly initialized value, and therefore, can aid learning.

The above example is certainly contrived, but hopefully illustrates the benefits of decomposition and subsequent projection. We note that, especially in settings where the limiting factor is the ability to collect enough experience, one can trade-off extra computation to derive these bounds and improve RL performance without the need to collect additional experience. The full pseudo-code of the WCQL algorithm is available in Appendix B.

### Convergence Analysis

In this section, we show that WCQL converges to \(Q^{*}\) with probability one. First, we state a standard assumption on learning rates and state visitation.

**Assumption 2**.: We assume the following: (i) \(_{n=0}^{}_{n}(,)=\), \(_{n=0}^{}_{n}^{2}(,)<\) for all \((,)\); (ii) analogous conditions to (i) hold for \(_{n}(s_{i},a_{i})}_{n}\) and \(\{_{n}(w)\}_{n}\), and (iii) the behavioral policy is such that all state-action pairs \((,)\) are visited infinitely often \(w.p.1\).

**Theorem 1**.: _Under Assumptions 1 and 2, the following statements hold with probability one._

1. _For each_ \(i\) _and_ \(\)_,_ \(_{n}Q^{}_{i,n}(s_{i},a_{i})=Q^{}_{i}(s_{i},a_{i})\) _for all_ \((s_{i},a_{i})_{i}_{i}\)_._
2. _For each_ \(\)_,_ \(_{n}Q^{}_{n}(,) Q^{*}(,)\) _for all_ \((,)\)_._
3. \(_{n}Q^{}_{n}(,)=Q^{*}(,)\) _for all_ \((,)\)_._

Theorem 1 ensures that each subagent's value functions converge to the subproblem optimal value. Furthermore, it shows that asymptotically, the Lagrangian action-value function given by (9) will be an upper bound on the optimal action-value function \(Q^{*}\) of the full problem and that our algorithm will converge to \(Q^{*}\).

## 5 Weakly Coupled DQN

In this section, we propose our main algorithm _weakly coupled DQN_ (WCDQN), which integrates the main idea of WCQL into a function approximation setting. WCDQN guides DQN using Lagrangian relaxation bounds, implemented using a constrained optimization approach.

**Networks.** Analogous to WCQL, WCDQN has a main network \(Q^{}(,;)\) that learns the action value of the full problem. In addition to the main network, WCDQN uses a subagent network \(Q^{}_{i}(s_{i},a_{i};_{U})\) network to learn the subproblem action-value functions \(Q^{}_{i}\). As in standard DQN, we also have \(^{-}\) and \(^{-}_{U}\), which are versions of \(\) and \(_{U}\) frozen from a previous iteration and used for computing target values . The inputs to this network are \((i,,s_{i},a_{i})\), meaning that we can use a single network to learn the action-value function for _all_ subproblems and \(\) simultaneously. The Lagrangian upper bound and the best upper bound estimates are:

\[Q^{}(,;_{U}^{-})=^{}(w)+_{i=1}^ {N}Q_{i}^{}(s_{i},a_{i};_{U}^{-})\ \ \ \ Q^{^{*}}(,;_{U}^{-})=_{}Q^{ }(,;_{U}^{-}).\] (13)

**Loss functions.** Before diving into the training process, we describe the loss functions used to train each network, as they are instructive toward understanding the main idea behind WCDQN (and how it differs from standard DQN). Consider a behavioral distribution \(\) for state-action pairs and a distribution \(\) over the multipliers \(\).

\[l_{U}(_{U})=_{,,} _{i=1}^{N}y_{i}^{}-Q_{i}^{}(s_{i},a_{i};_{U}) ^{2},\] (14)

where the (ideal) target value is

\[y_{i}^{}=r_{i}(s_{i},a_{i})- a_{i}+\, _{a_{i}^{}_{i}}Q_{i}^{}(s_{i}^{},a_{i}^{ };_{U}^{-}).\] (15)

For the main agent, we propose a loss function that adds a soft penalty for violating the upper bound:

\[l()=_{,,}y-Q^ {}(,;)^{2}+_{U}Q^{}(, ;)-y_{U}_{+}^{2},\] (16)

where \(()_{+}=(,0)\), \(_{U}\) is a coefficient for the soft penalty, and

\[y=r(,)+_{^{} (^{})}Q^{}(^{},^{}; ^{-}),\] (17) \[y_{U}=r(,)+_{^{ }}Q^{^{*}}(^{},^{};_{ U}^{-}).\] (18)

The penalty encourages the network to satisfy the bounds obtained from the Lagrangian relaxation.

**Training process.** The training process resembles DQN, with a few modifications. At any iteration, we first take an action using an \(\)-greedy policy using the main network over the feasible actions, store the obtained transition experience \(\) in the buffer, and update the estimate of \((w)\) following (8).2 Each network is then updated by taking a stochastic gradient descent step on its associated loss function, where the expectations are approximated by sampling minibatches of experience tuples \(\) and \(\). The penalty coefficient \(_{U}\) can either be held constant to a positive value or annealed using a schedule throughout the training. The full details are shown in Algorithm 1 and some further details are given in Appendix C.

## 6 Numerical Experiments

In this section, we evaluate our algorithms on three different WCMDPs. First, we evaluate WCQL on an electric vehicle (EV) deadline scheduling problem with multiple charging spots and compare its performance with several other tabular algorithms: Q-learning (QL), Double Q-learning (Double-QL) , speedy Q-learning (SQL) , bias-corrected Q-learning (BCQL) , and Lagrange policy Q-learning (Lagrangian QL) . We then evaluate WCDQN on two problems, multi-product inventory control and online stochastic ad matching, and compare against standard DQN, Double-DQN, and the optimality-tightening DQN (OTDQN) algorithm3 of He et al.  as baselines. Further details on environment and algorithmic parameters are in Appendix D.

**EV charging deadline scheduling .** In this problem, a decision maker is responsible for charging electric vehicles (EV) at a charging service center that consists of \(N=3\) charging spots. An EV enters the system when a charging spot is available and announces the amount of electricity it needs to be charged, denoted \(B_{t}\), along with the time that it will leave the system, denoted \(D_{t}\). The decision maker also faces exogenous, random Markovian processing costs \(c_{t}\). At each period, the action is to decide which EVs to charge in accordance with the period's capacity constraint. For each unit of power provided to an EV, the service center receives a reward \(1-c_{t}\). However, if the EV leaves the system with an unfulfilled charge, a penalty is assessed. The goal is to maximize the revenue minus penalty costs.

**Multi-product inventory control with an exogenous production rate .** Consider the problem of resource allocation for a facility that manufactures \(N=10\) products. Each product \(i\) has an independent exogenous demand given by \(D_{i}\), \(i=1,,N\). To meet these demands, the products are made to stock. Limited storage \(R_{i}\) is available for each product, and holding a unit of inventory per period incurs a cost \(h_{i}\). Unmet demand is backordered at a cost \(b_{i}\) if the number of backorders is less than the maximum number of allowable backorders \(M_{i}\). Otherwise, it is lost with a penalty cost \(l_{i}\). The DM needs to allocate a resource level \(a_{i}\{0,1,,U\}\) for product \(i\) from a shared finite resource quantity \(U\) in response to changes in the stock level of each product, denoted by \(x_{i} X_{i}=\{-M_{i},-M_{i}+1,,R_{i}\}\). A negative stock level corresponds to the number of backorders. Allocating a resource level \(a_{i}\) yields a production rate given by a function \(_{i}(a_{i},p_{i})\) where \(p_{i}\) is an exogenous Markovian noise that affects the production rate. The goal is to minimize the total cost, which consists of holding, back-ordering, and lost sales costs.

**Online stochastic ad matching .** We study the problem of matching \(N=6\) advertisers to arriving impressions. In each period, an impression of type \(e_{t}\) arrives according to a Markov chain. An action \(a_{t,i}\{0,1\}\) assigns impression \(e_{t}\) to advertiser \(i\), with a constraint that exactly one advertiser is selected: \(_{i=1}^{N}a_{t,i}=1\). Advertiser states represent the number of remaining ads to display and evolves according to \(s_{t+1,i}=s_{t,i}-a_{t,i}\). The objective is to maximize the discounted sum of expected rewards for all advertisers.

In Figure 2, we show how WCQL's projection method helps it learn a more accurate \(Q\) function more quickly than competing tabular methods. The first panel, Figure 2A, shows an example evolution of WCQL's projected value function \(Q^{}\), along with the evolution of the upper bound. We compare this to the evolution of the action-value function in absence of the projection step. In the second panel, Figure 2B, we plot the _relative error_ between the learned value functions of various algorithms compared to the optimal value function. Both plots are from the EV charging example. Detailed descriptions of the results are given in the figure's caption.

The results of our numerical experiments are shown in Figure 3. We see that in both the tabular and the function approximation cases, our algorithms outperformed the baselines, with WCQL and WCDQN achieving the best mean episode total rewards amongst all problems. From Figure 3A, we see that although the difference between WCQL and Lagrangian QL is small towards the end of the training process, there are stark differences earlier on. In particular, the performance curve of WCQL shows significantly lower variance, suggesting more robustness across random seeds. Given that WCQL and Lagrangian QL differ only in the projection step, we can attribute the improved stability to the guidance provided by the Lagrangian bounds. Figure 3B shows that for the multi-product inventory problem, the OTDQN, DQN, and Double DQN baselines show extremely noisy performance compared to WCDQN, whose significantly better and stable performance is likelydue to the use of faster converging subagents and better use of the collected experience. Similarly, in the online stochastic ad matching problem, WCDQN significantly outperforms all the baselines.

## 7 Conclusion

In this study, we propose the WCQL algorithm for learning in weakly coupled MDPs and we show that our algorithm converges to the optimal action-value function. We then propose WCDQN, which extends the idea behind the WCQL algorithm to the function approximation case. Our algorithms are model-free and learn upper bounds on the optimal action-value using a combination of a Lagrangian relaxation and Q-learning. These bounds are then used within a constrained optimization approach to improve performance and make learning more efficient. Our approaches significantly outperforms competing approaches on several benchmark environments.

Figure 3: Benchmarking results for the WCQL (EV charging) and WCDQN (multi-product inventory control, online ad matching) against baseline methods. The plots show mean total rewards and their 95% confidence intervals across 5 independent replications.

Figure 2: Behavior of WCQL’s learning process compared to other tabular methods. In Figure 2A, we show an example of the evolution of quantities associated with WCQL for a randomly selected state-action pair in the EV charging problem: upper bound (blue), WCQL Q-value (orange line with ’x’ marks indicating the points projected by the bound), standard Q-learning (green) and the optimal action-value (red). Note that sometimes the orange line appears above the blue line since WCQL’s projection step is asynchronous, i.e., the projections are made only if the state is visited by the behavioral policy. Notice at the last marker, the bound moved the Q-value in orange to a “good” value, relatively nearby the optimal value. WCDQ’s Q-value then evolves on its own and eventually converges. On the other hand, standard QL (green), which follows the same behavior policy as WCQL, is still far from optimal. In Figure 2B, we show the relative error, defined as \(\|V_{n}-V^{*}\|_{2}/\|V^{*}\|_{2}\), where \(V_{n}\) and \(V^{*}\) are the state value functions derived from \(Q\)-iterate on iteration \(n\) and \(Q^{*}\), respectively. WCQL exhibits the steepest decline in relative error compared to the other algorithms. Note that since Lagrangian QL acts based on the Q-values of the subproblems, there is no Q-value for this algorithm on which to compute a relative error.