# MultiOrg: A Multi-rater Organoid-detection Dataset

Christina Bukas\({}^{1}\), Harshavardhan Subramanian\({}^{1}\), Fenja See\({}^{2}\), Carina Steinchen\({}^{2}\)

**Ivan Ezhov\({}^{3}\), Gowtham Boosarpu\({}^{4}\), Sara Asgharpour\({}^{2}\), Gerald Burgstaller\({}^{2}\) Mareike Lehmann\({}^{2,4}\), Florian Kofler\({}^{1,5,6}\), Marie Piraud\({}^{1}\)\({}^{1}\)Helmholtz AI, Computational Health Center (CHC), Helmholtz Munich, Neuherberg, Germany

\({}^{2}\)Institute of Lung Health and Immunity (LHI), Comprehensive Pneumology Center (CPC),

Helmholtz Munich, Member of the German Center for Lung Research (DZL), Neuherberg, Germany

\({}^{3}\)Technical University of Munich, School of Computation, Information and Technology,

Department of Computer Science, Munich, Germany

\({}^{4}\)Institute for Lung Research, Philipps-University Markup, Universities of Giessen and Marburg Lung Center, Member of the German Center for Lung Research (DZL), Marburg, Germany

\({}^{5}\) Department of Neuroradiology, Technical University of Munich, Munich, Germany.

\({}^{6}\) Department of Quantitative Biomedicine, University of Zurich, Switzerland.

{christina.bukas,mareike.lehmann,marie.piraud}@helmholtz-munich.de

###### Abstract

High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present _MultiOrg_ a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.

## 1 Introduction

Accurate and efficient object detection methods in biomedical image analysis are crucial for research and diagnostics. Designing such methods requires diverse, well-curated datasets of high-resolution images reflecting real-world complexities. The annotation of biomedical datasets represents a labor-intensive and subjective process relying on human experts. This work represents a multi-rater organoid dataset designed for benchmarking object detection algorithms in a label-noise-aware setting that embraces the subjectivity in labels.

Organoids are miniature three-dimensional (3d) models of organs grown in vitro from stem cells. They mimic the complexity and functionality of real organs, making them extremely valuable for medical research, disease modeling, and drug testing (Barkauskas et al., 2017; Kim et al., 2020; Ingber, 2022). Organoid cultures, deriving from healthy and diseased or genetically engineered cells and undergoing different conditions and treatments, can be grown for several months (Youk et al.,2020; Huch and Koo, 2015). These high-throughput experiments are monitored via microscopic imaging and, therefore, necessitate fast and objective detection, quantification, and tracking methods (Rios and Clevers, 2018; Du et al., 2023). Detection of organoids in real-world lab-culture images is associated with many challenges (Kassis et al., 2019). Beyond the typical challenges associated with microscopy (out-of-focus, lightning, padding, etc...), those 3d cultures are imaged in 2d, leading to overlapping structures. Organoids can highly vary in size, shape, and appearance (Domenech-Moreno et al., 2023), and be difficult to distinguish from dust and debris present in the culture (Matthews et al., 2022; Keles et al., 2022). Finally, the high number of objects to analyze per image poses a big hurdle to a human prone to distraction and fatigue (Haja et al., 2023). Manual annotation of this data, which is still state-of-the-art (Costa et al., 2021; Wu et al., 2022) is, therefore, error- and bias-prone, which introduces noise in the labels. However, evaluating learning algorithms for organoid detection, involves comparing predicted outcomes to those manual annotations or '_Ground Truth (GT)_' during training and testing. As shown in our previous work, deep learning algorithms can outperform even highly-trained human annotators (Koffer et al., 2021). In complex real-life datasets, understanding the shortcomings that label uncertainty creates in the '_GT_' is, therefore, pivotal before training and benchmarking _Deep Learning (DL)_ models. Moreover, quantifying the label noise by assessing the intra- and inter-rater reliability is crucial to interpret similarity metrics between model predictions and reference annotations (Koffer et al., 2023).

In this work, see Figure 1, we release _MultiOrg_, a large multi-rater 2d microscopy imaging dataset of lung organoids for benchmarking object detection methods. The dataset comprises more than 400 images of an entire microscopy plate well and more than 60,000 annotated organoids, deriving from different biological study setups, with two types of organoids growing under varying conditions. Most importantly, we introduce three unique label sets derived from the two annotators at different times, allowing for the quantification of label noise (see Fig. 2). Such a dataset can enable the community to explore biases in annotations, investigate the effect these have on model training, and promote the active area of research for uncertainty quantification. To our knowledge, this is the second largest organoid dataset to date to be made freely available to the community (Bremer et al., 2022). It is also the first organoid dataset and one of the very few biomedical object-detection datasets to introduce multiple labels (Nguyen et al., 2022; Amgad et al., 2022). We benchmarked this dataset by training and testing four widely established _DL_ models for object detection tasks using both one-stage and two-stage architectures. Finally, along with the dataset and model, we release a tool for quantifying lung organoids, enabling users to visualize and correct the detected organoids before extracting useful features for downstream tasks. This tool solves the bottleneck of manual quantification of lung organoids, enabling high-throughput image analysis for biological studies.

In summary, the contributions of this work are as follows:

* We release _MultiOrg_, an object detection bio-medical dataset of more than 400 microscopy images comprising around 60,000 lung organoids annotated by two expert annotators.
* We provide quantification of label uncertainty through a Kaggle benchmark challenge that evaluates the submissions on the different test label sets.
* We benchmark our dataset on four standard object detection methods, show how performance varies depending on the selected annotations, and release the models on zenodo.
* We release the best model in a napari plugin, _napari-organoid-counter_(Bukas, 2022), which allows users to curate predictions, thus enabling high-throughput analysis.

## 2 Related work

Kassis et al. (2019) proposed _OrganoQuant_, a manually-annotated, human-intestinal-organoid dataset of around 14,000 organoids, along with an object detection pipeline based on Faster R-CNN Ren et al. (2015), to locate and quantify human intestinal organoids in brightfield images. Though object detection performance is satisfactory and the quantification process is robust, inference is performed on cropped patches of a well. Similarly, Matthews et al. (2022) released a dataset of brightfield and phase-contrast microscopy images and proposed an image analysis platform, _OrganoID_, based on U-Net Falk et al. (2019), which segments and tracks different types of organoids. They trained their model on images of pancreatic cancer organoids and validated it on pancreatic, lung, colon, and adenoid cystic carcinoma organoids. This work introduces several types of organoids. However, the dataset is small, including only 66 images featuring 5 to 50 organoids each. In Haja et al. (2023), _OrganelX_ platform was released to enable segmentation of murine liver organoids using Mask-RCNN He et al. (2017). Furthermore, Bian et al. (2021) introduced a high-throughput image dataset of liver organoids for detection and tracking. They also propose a novel deep neural network architecture to track organoids dynamically and detect them quickly and accurately. However, here, too, the dataset size is relatively small, with 75 images containing a total of 6,482 organoids. Bremer et al. (2022) used a multicentric dataset consisting of 729 images containing 90,210 annotated organoids, including multiple organoid systems like liver, intestine, tumor, and lung, and proposes an organoid annotation tool, _GOAT_, which uses Mask R-CNN (He et al., 2017), for unbiased quantification. The corresponding dataset contains six organoid types, generated in four centers and acquired with five microscopes. More recently, Domenech-Moreno et al. (2023) proposed an object detection algorithm, based on YOLO v5 Ultralytics (2021), _Tellin_, to classify and detect intestinal organoids of different types. The tool also enables automated analysis of intestinal organoid morphology and fast and accurate classification of organoids.

_MultiOrg_ is, therefore, the second largest organoid dataset (see Table 1). It is noisier than those introduced above; it is not the densest but contains clumps of organoids and displays an extensive range of sizes, presenting one of the most challenging settings for object detection. We introduce

Figure 1: _MultiOrg_ workflow. a) Dataset creation, b) Multi-rater annotation at time points \(t^{0}\) and \(t^{1}\), c) Model benchmark, and d) Release on Kaggle and napari plugin

Figure 2: Multiple label sets in _MultiOrg_. Full test image (left) and crops of areas A, B, and C overlaid with \(test^{0}\), \(test^{1}_{A}\) and \(test^{1}_{B}\) (right). The square crops are of sizes 1800, 1200, and 500 px. \(test^{0}\) in images 4 and 16 (respectively 24 and 43) originates from Annotator A (resp. B). ’Macros’ are typically noisier, as the cultures initially contain more cells (Appendix A.1.1).We observe a reduction in the number of annotations at time \(t^{1}\), as the annotators do not consider some small organoids that were annotated at \(t^{0}\). In image 24, Annotator B annotates clumps of organoids as one large object at \(t^{1}\). The large structure in image 43 is an experimental matrigel artifact. The image-wise intra-rater Recall scores are 0.776, 0.532, 0.667 and 0.503 for images 4, 16, 24, and 43, respectively (with \(test^{0}\) as \(GT\)).

[MISSING_PAGE_FAIL:5]

\(test_{B}^{1}\)) refer to the re annotation from annotators A (resp. B), at time point \(t^{1}\) on all 55 images of the test set.

### Object detection metrics

We compare the multiple label sets and assess the quality of model predictions using several evaluation metrics. _True Positives (TPs)_, _False Positives (FPs)_, and _False Negatives (FNs)_ are computed for each image, for a given _Intersection-over-Union (IOU)_ threshold, using one of the label sets as the '_GT_'. Their total numbers are then aggregated on the entire test set. For comparing the three available label sets, we compute Precision and Recall at an _IOU_ of 0.5 and the F1-score. While Precision measures the percentage of correct predictions against a considered true label, Recall (i.e., sensitivity) measures the proportion of true positive predictions identified correctly. For evaluating model performance, we use the _Precision-Recall (P-R)_ curves as the primary tool. It consists of Precision and Recall values at different model confidence thresholds, at a fixed _IOU_ threshold (here we use 0.5 unless specified otherwise). We also report _Mean Average Precision (mAP)_, by integrating precision across Recall levels from 0 to 1. We compute them using the standard library (Padilla et al., 2021) which follows the PASCAL VOC challenge technique for interpolating points on the curve (Everingham et al.).

### Multi-rater analysis

We compute inter- and intra-rater uncertainties to quantify annotation variance and assess the consistency of the two raters over time and against each other. Inter-rater scores assess the inconsistency in the assessments made by different raters when evaluating the same image and can permit the detection of biases and different expertise levels. The variability in assessments made by one rater when evaluating the same image multiple times (intra-rater) can permit the detection of errors and ambiguities associated with the complexity of the task.

Figure 3 shows intra-rater scores, with label set \(test^{0}\) used as the _GT_ (note that switching the choice of _GT_ does not impact the F1-score and switchs Recall and Precision). For Annotator A, we compare the annotations for test images 1-22, i.e., \(test^{0}_{A}\) with the corresponding subset of \(test^{1}_{A}\), while for Annotator B we compare annotations of images 23-55, i.e., \(test^{0}_{B}\) with the corresponding subset of \(test^{1}_{B}\). We find that Annotator A is more consistent over time, especially for the 'Normal' studies, which is in line with the reported change of annotation setup of Annotator B. Additionally, annotation of 'Macros' seems more challenging than 'Normal' images. We also find a reduction in the number of annotations at time point \(t^{1}\), already reported in Table 2. The qualitative inspection of Figure 2 suggests that some pseudo labels used as a starting point for all manual annotations were not removed in \(test^{0}\), probably indicating improvement of the annotations over time. Further comparison of the pseudo labels to the label sets can be seen in Table A.9. We also observe that Annotator B, unlike Annotator A, annotated overlapping clumps of organoids differently at \(t^{1}\), which could be the source of the higher reported inconsistency.

  
**Study Type** &  &  &  \\   & \# Images & \# Organoids & \# Images & \# Organoids & \# Images & \# Organoids \\   \\  \(train_{A}\) & 181 & 30,710 & 15 & 2,669 & 196 & 33,379 \\ \(train_{B}\) & 135 & 20,263 & 25 & 1,781 & 160 & 22,044 \\
**Total** & **316** & **50,973** & **40** & **4,450** & **356** & **55,423** \\   \\  \(test^{0}_{B}\) & 8 & 1,145 & 14 & 1,865 & 22 & 3,010 \\ \(test^{0}_{B}\) & 20 & 3,020 & 13 & 1,493 & 33 & 4,513 \\
**Total (Label set test\({}^{}\))** & **28** & **4,165** & **27** & **3,358** & **55** & **7,523** \\ 
**Label set test\({}^{1}_{A}\)** & **28** & **2,748** & **27** & **1,981** & **55** & **4,729** \\ 
**Label set test\({}^{1}_{B}\)** & **28** & **2,655** & **27** & **2,301** & **55** & **4,956** \\   

Table 2: Overview of the label sets (_train_, _test\({}^{0}\)_, _test\({}^{1}_{A}\)_, and _test\({}^{1}_{B}\)_). Number of images and organoid labels stratified by study type (for all) and annotator (only relevant for _train_ and _test\({}^{0}\)_). All _test_ label sets refer to the same images. We see a reduction in the number of labels between \(t^{0}\) and \(t^{1}\).

Inter-rater scores can be computed for the entire test set between \(test_{A}^{1}\) and \(test_{B}^{1}\), as shown in Figure 3. We again observe that the annotation of 'Macros' is generally more challenging, and those images consistently appear noisier in Figure 2. In Figure A.6(bottom), we also display the inter-rater scores on the image subsets 1-22 and 23-55, as well as across \(t^{0}\) and \(t^{1}\). This corroborates the larger evolution of Annotator B between \(t^{0}\) and \(t^{1}\) and indicates a convergence of the two annotation styles. Table A.6, Table A.7, and Table A.8, provide all statistics for these multi-rater scores.

### Dataset availability

We make _MultiOrg_ available to the community. All images are public on Kaggle, together with label sets \(train\) and \(test^{0}\), to ensure that the steps presented in Section 4 can be reproduced. The label sets \(test_{A}^{1}\) and \(test_{B}^{1}\) can be queried by participating in the _MultiOrg_ challenge, where our leaderboard returns the average of _mAP_ on \(test_{A}^{1}\) and \(test_{B}^{1}\). We invite scientists to participate, to promote research in the field of uncertainty estimation.

## 4 Model Benchmarking

We benchmark four standard object-detection \(DL\) models on _MultiOrg_:

* _Faster R-CNN_(Ren et al., 2015)
* _Single Shot MultiBox Detector (SSD)_(Liu et al., 2016)
* _You Only Look Once, Version 3 (YOLOv3)_(Redmon and Farhadi, 2018)
* _Real-Time Models for object Detection (RTMDet)_(Lyu et al., 2022).

Figure 3: Multi-rater scores. **Top**: Intra-rater F1-score (left), Precision (middle), and Recall (right), where \(test^{0}\) is considered the _GT_, for both annotators and according to study type. Annotator A appears more consistent on ’Normal’ images (higher scores), and annotation of ’Macros’ seems more challenging (with lower scores). Both annotators show an overall higher Precision and lower Recall, indicating that \(test^{0}\) has many more annotations which are treated here as _FNs_. **Bottom**: Inter-rater F1-score (left), Precision (middle), and Recall (right) on the test set between \(test_{A}^{1}\) and \(test_{B}^{1}\), where \(test_{A}^{1}\) is considered the _GT_, split according to study type. Raters agree more on ’Normal’ images, indicating that the annotation of ’Macros’ images is more challenging. Individual differences are generally lower than in-between raters (lower inter-rater than intra-rater scores).

All trained models can be found on zenodo, and code and documentation to reproduce the training is available on Kaggle 1.

### Training and Testing

For training, the images in the training set were split into patches of 512x512 px, resulting in a total of 20,011 patches. Bounding boxes extending beyond the borders of the patches were omitted in the _GT_ since these organoids can be captured by a sliding window approach at inference, resulting in 44,418 bounding box labels for training. For training and validation, we used the _mmdetection_(Chen et al., 2019) toolbox, with the original configuration for each model adapted such that the input and parameters for all models is the same (details in Appendix A.2).

During testing, sliding window inference was performed on the full images of the test set. We slide over each image twice with different window sizes and down-sampling factors to detect both small and large organoids. We empirically choose the following parameters: window size set to 512 and 2048 px, while the down-sampling factor is set to two and eight, respectively, with a window overlap of 0.5 and _Non max suppression (NMS)_ for post-processing with a threshold of 0.5. For each model, we choose the checkpoint with the highest _mAP_ on \(test^{b}\), thus using this label set for validation during training. We report those, along with training and inference times in Table A.10.

### Benchmark models evaluation

We evaluate the model performance on \(test^{0}\), \(test^{1}_{A}\) and \(test^{1}_{B}\). Figure 4 shows _P-R_ curves for the different models on label set \(test^{0}\), as well as the curves for all three label sets on the best performing model, _SSD_. Notably, though the model was trained and validated on labels created at \(t^{0}\), the best _P-R_ curve is obtained for \(test^{1}_{B}\), indicating that the trained model is more in agreement with these labels. This suggests that the higher label noise present in \(test^{0}\), as assumed in Section 3.3, was not picked up by the model during training, illustrating once more the resilience of _DL_ to label noise (Rolnick et al., 2017). Table 3 presents further evaluation metrics and confirms that _SSD_ is the best-performing model overall, while at the standard model confidence threshold of 0.5 _YOLOv3_ performs equally well if not better on some label sets. Interestingly, different models exhibit very different Precision-Recall trade-offs at 0.5 model confidence.

Figure 4: Model Benchmark. _P-R_ curves using \(test^{0}\) as the _GT_ for all models (left) and using all three label sets for _SSD_ (right). We observe that overall the SSD model predictions are more in agreement with the annotations and have a better trade-off between precision and recall. Although the model was trained and validated with labels from \(t^{0}\) it is more in agreement with annotations from timepoint \(t^{1}\).

### Napari plugin

As described in Appendix A.1.3, _MultiOrg_ was created using the open-source image analysis tool _Napari_(Ahlers et al., 2019), together with the initial release, _v.0.1.0_, of the _napari-organoid-counter_ plugin (Bukas, 2022). In this work, we release a new version of the plugin, _v.0.2.2_, using the model from our benchmark with the better trade-off between performance and inference time as the backbone (see Table A.10), i.e. _YOLOv3_, along with added functionalities. For example, the model confidence threshold can now be adjusted at run time by the user, depending on whether for the task at hand, a higher Recall or Precision is most practical. Plugin details can be found in Appendix A.3. Code and tutorials for installation are distributed through the napari hub

## 5 Discussion

In this work, we release _MultiOrg_, a large multi-rater dataset for organoid detection in 2d microscopy images. Our dataset consists of more than 60,000 annotated lung organoids, labeled by two expert annotators. As even expert annotators can disagree on what constitutes an organoid in those images while also being susceptible to human error and biases, we provide three label sets for the test data, enabling quantification of label uncertainty on a multi- and single-annotator level. Additionally, we have carefully included diversity in our dataset through several study setups and cell lines to ensure good generalization. We performed preliminary tests of the selected model on different lung cell types not present in the dataset, from both human and mouse organoids, and seen that it generalizes quite well. We also tested it successfully on colon organoids and speculate that it can be used for all organoids with similar shape and size. This dataset is, therefore, uniquely situated between the fields of microscopy and uncertainty quantification. We invite researchers to use it, participate in the _MultiOrg_ challenge, and assist us in studying label noise in challenging real-life biomedical settings, and we believe that future models should, as much as possible, refrain from being trained without considering these aspects. We additionally publish a benchmark for organoid object detection, provide all models in zenodo and the best one as a Napari plugin, thus enabling scientists potentially use them on their own data.

  
**Metric** & **Label set** & **Faster R-CNN** & **SSD** & **YOLOv3** & **RTMDet** \\   & \(test^{0}\) & 0.23 & 0.61 & **0.73** & 0.64 \\  & \(test^{1}_{A}\) & 0.16 & 0.44 & 0.58 & 0.54 \\  & \(test^{1}_{B}\) & 0.18 & 0.50 & 0.67 & 0.56 \\   & **mean** & **0.19** & **0.52** & **0.66** & **0.58** \\   & \(test^{0}\) & 0.84 & 0.67 & 0.48 & 0.51 \\  & \(test^{1}_{A}\) & 0.92 & 0.78 & 0.62 & 0.69 \\  & \(test^{1}_{B}\) & **0.97** & 0.83 & 0.67 & 0.68 \\   & **mean** & **0.91** & **0.76** & **0.59** & **0.63** \\   & \(test^{0}\) & 0.36 & 0.64 & 0.58 & 0.57 \\  & \(test^{1}_{A}\) & 0.27 & 0.57 & 0.60 & 0.61 \\  & \(test^{1}_{B}\) & 0.30 & 0.62 & **0.67** & 0.62 \\   & **mean** & **0.31** & **0.61** & **0.62** & **0.60** \\   & \(test^{0}\) & 56.56 & 64.40 & 62.55 & 57.71 \\  & \(test^{1}_{A}\) & 57.09 & 65.79 & 61.11 & 63.87 \\   & \(test^{1}_{B}\) & 68.36 & **73.88** & 70.25 & 63.23 \\    & **mean** & **60.67** & **68.09** & **64.64** & **61.60** \\   & \(test^{0}\) & 17.48 & 21.81 & 19.15 & 22.56 \\  & \(test^{1}_{A}\) & 23.53 & 23.42 & 19.13 & 30.13 \\   & \(test^{1}_{B}\) & **46.98** & **46.48** & 39.01 & 32.85 \\    & **mean** & **29.33** & **30.57** & **25.76** & **28.51** \\   

Table 3: Benchmark metrics on the three label sets. Precision, Recall, and F1-score are reported at 0.5 _IOU_ threshold and model confidence. _mAP_ is reported at 0.5 and 0.75 IoU threshold. The models exhibit different Precision-Recall tradeoffs. The performance of _SSD_ is overall better when considering _mAP_, while at the standard model confidence threshold of 0.5 _YOLOv3_ performs equally well if not better on some label sets.

This work offers a valuable dataset that can be leveraged to advance both object detection methods and uncertainty quantification techniques. The current setting does not, however, permit the incorporation of several label sets in the training loop. Furthermore, it is important to note that although two organoid types are present in the images, they have not been annotated as different classes. Treating this task as a multi-class detection problem may boost the overall performance of the object detection task (Zhang et al., 2022) and would provide added value to the biologists. However, while providing several label sets or multi-class labels on the training data could be beneficial, it represents substantial manual work. It would also be interesting to observe how the label sets change by using _DL_ models as a baseline for annotation rather than the pseudo labels. Despite these limitations, we are confident that the release of the _MultiOrg_ dataset offers several invaluable contributions to the machine learning community.