# LG-CAV: Train Any Concept Activation Vector with Language Guidance

Qihan Huang1, Jie Song1,, Mengqi Xue2, Haofei Zhang1,

**Bingde Hu1, Huiqiong Wang3, Hao Jiang4, Xingen Wang1,5, Mingli Song1**

1 Zhejiang University, 2 Hangzhou City University

3 Ningbo Innovation Center, Zhejiang University

4 Alibaba Group, 5 Bangsheng Technology Co., Ltd.

{qh.huang,sjie,haofeizhang,tonyhu,huiqing_wang,newroot,brooksong}@zju.edu.cn

mqxue@zucc.edu.cn, aoshu.jh@alibaba-inc.com

Corresponding author.

###### Abstract

Concept activation vector (CAV) has attracted broad research interest in explainable AI, by elegantly attributing model predictions to specific concepts. However, the training of CAV often necessitates a large number of high-quality images, which are expensive to curate and thus limited to a predefined set of concepts. To address this issue, we propose **L**anguage-**G**uided CAV (LG-CAV) to harness the abundant concept knowledge within the certain pre-trained vision-language models (_e.g._, CLIP). This method allows training _any_ CAV without labeled data, by utilizing the corresponding concept descriptions as guidance. To bridge the gap between vision-language model and the target model, we calculate the activation values of concept descriptions on a common pool of images (probe images) with vision-language model and utilize them as language guidance to train the LG-CAV. Furthermore, after training high-quality LG-CAVs related to all the predicted classes in the target model, we propose the activation sample reweighting (ASR), serving as a model correction technique, to improve the performance of the target model in return. Experiments on four datasets across nine architectures demonstrate that LG-CAV achieves significantly superior quality to previous CAV methods given any concept, and our model correction method achieves state-of-the-art performance compared to existing concept-based methods. Our code is available at https://github.com/hqhQAQ/LG-CAV.

## 1 Introduction

Concept activation vector (CAV)  interprets the pre-trained black-box classification models (target models) by quantifying the significance of a concept to the model predictions. CAV provides intuitive insights to comprehend the intrinsic behavior of black-box models, elucidating the patterns behind their decision-making processes. Owing to its simplicity and effectiveness, it has been followed by numerous studies  and extended to diverse domains, such as recommender system , 3D shape generation , abusive language detection , _etc_.

However, the training of CAV usually necessitates an ample amount of high-quality images that accurately depict the corresponding concept. Unfortunately, in practical contexts, gathering an adequate number of training images is challenging especially when the number of concepts is extensive, thereby significantly impacting the quality (estimated using the proposed _concept accuracy_ and _concept-to-class accuracy_) of the trained CAVs. Figure 1 delineates the correlation betweenthe number of training images for each concept and the quality of the trained CAVs on the Broden dataset . It can be concluded that when the number of training images is small, the quality of the trained CAVs is low, hindering CAVs from properly interpreting the model.

In recent years, the advent of foundational vision-language models (referred to as **VL models**, such as CLIP ) establishes connections between images and text by mapping image features and text features into a shared feature space. These VL models undergo pre-training on extensive image-text datasets, equipping them with the ability to grasp a multitude of concepts. Inspired by this, to address the data-scarcity problem of CAV training, in this work we propose LG-CAV to utilize the abundant concept knowledge from VL models for more cheaply getting CAV for any concept given the concept descriptions, without being confined to specific pre-defined concepts.

The concept features extracted by VL model cannot be directly used for training the LG-CAV, as VL model and the target model operate within distinct feature spaces. To bridge the gap, our work ingeniously trains the LG-CAV by calculating its activation values on a common pool of images (probe images), and making them mimic the activation values of the corresponding concept from VL model on these images, as shown in Figure 2 (**A**). Therefore, LG-CAV learns its corresponding concept according to the concept's existence degree (activation value) on the probe images from VL model.

However, directly applying the above framework is not guaranteed to improve the quality of LG-CAV (see experiments in subsubsection 4.1.2), because the calculated activation values from the target model and VL model are in different distributions (see Figure 2 (**B**)). To tackle this problem, our work proposes a Gaussian alignment (GA) module to align the activation values from the target and VL models. Besides, we propose a concept ensemble (CE) module and a deviation sample reweighting (DSR) module into this framework to further improve the quality of LG-CAV. Detailedly, CE module strengthens the completeness of concept descriptions by employing data augmentations on the concept texts. DSR module optimizes the selection of probe images by allocating higher training weights to the probe images with a more stable concept representation.

Furthermore, after training numerous high-quality LG-CAVs that can describe all classes in the dataset, our work makes a considerable improvement on previous CAV methods by applying LG-CAVs to model correction on generic datasets like ImageNet. To this end, we fine-tune the target model to align the prediction of each class with its strongly-related concept, with a proposed activation sample reweighting (ASR) module that allocates higher training weights to the samples activated more highly by the corresponding LG-CAVs.

We perform extensive experiments to validate the performance of our proposed method. Experiments demonstrate that LG-CAV achieves significantly higher CAV quality (concept accuracy & concept-to

Figure 1: The quality of CAV is significantly affected by the number of training images. Here concept accuracy estimates whether the CAV faithfully represents its corresponding concept. Concept-to-class accuracy measures the similarity between the CAV and its strongly semantic-related class.

Figure 2: **(A)** LG-CAV is trained guided by activations of concept descriptions on the probe images from VL model. **(B)** The distribution of activation values on a concept named “Skyscraper” (from the Broden dataset ) in the target model (ResNet18) and VL model (CLIP) differs a lot.

class accuracy) than previous CAV methods on the Broden & ImageNet datasets over nine backbones. Besides, we conduct model correction on the ImageNet & CUB-200-2011 & CIFAR-100 datasets over nine backbones. Experiments present that our method achieves significantly superior performance to other concept-based methods.

To sum up, the key contributions of this work can be listed as follows:

* We propose LG-CAV to tackle the data-scarcity problem of CAV training, which is trained guided by the corresponding concept descriptions from VL model.
* We propose a Gaussian alignment (GA) module, a concept ensemble (CE) module, and a deviation sample reweighting (DSR) module to further enhance the quality of LG-CAV.
* Beyond providing explanations, we apply LG-CAV to model correction, by proposing an activation sample reweighting (ASR) module.
* Experiment results verify that LG-CAV achieves significantly higher CAV quality, and our model correction method outperforms existing concept-based methods remarkably.

## 2 Related Work

Concept Activation Vector (CAV).With the development and widespread application of deep learning [12; 31; 3; 46], it has become increasingly important to explain the internal mechanisms of deep neural networks (_e.g._, using concept activation vector (CAV)). Each CAV  is trained for a specific concept in the target model, and is used to quantify the importance of this concept to model predictions. Most existing CAV methods only utilize CAVs to interpret the target model. Concept_Gradient  extends the original linear CAV to non-linear concept functions, which improves the interpretability of CAV without the linear separability assumption of CAV. OA-TCAV  proposes an adversarial training approach to improve the quality of CAV. Differently, our method achieves significantly superior CAV quality to these methods, by transferring the abundant concept knowledge from VL model.

Vision-Language Models for Interpretability.CLIP-Dissect  and DISCOVER  utilize CLIP model to describe the neurons inside the target model. Label-free CBM  and PCBM  utilize CLIP model to generate additional concept annotations for concept bottleneck models . These methods are limited to solely interpreting the target model and lacking the ability to improve the model performance using the explanation results.

Model Correction.Model correction methods aim to improve the target model by introducing corrective information into the model. Most existing methods [32; 22; 24; 19; 41] are limited to customized tasks with narrow scope (_e.g._, debias the color bias of model representations on the ColorMNIST dataset ). Some methods [11; 4] improve the accuracy of generic classification models, but they are limited to small-sized datasets. Differently, our method trains high-quality LG-CAVs that can describe all classes in the dataset, thus facilitating the task of model correction on generic datasets like ImageNet.

We provide more detailed comparisons with the related methods in Appendix C.3.

## 3 Method

### Preliminaries

The target model is a pre-trained classification model that receives image \(\) as input and outputs \(K\) classification logits, with a backbone \(f\) and a final layer \(h\). Detailedly, \(f\) extracts the image features \(f()^{_{f}}\) of \(\) (\(D_{f}\) is dimension size), and \(h\) is a linear layer that projects \(f()\) into \(K\) classification logits. Note \(h(f())^{K}\), and \(h_{k}(f())\) is the classification logit for class \(k\).

Concept activation vector (CAV)  represents a concept for the target model. Specifically, given positive images (\(_{c}\)) and negative images (\(_{c}\)) for the concept \(c\), a binary linear classifier is trained on internal features \(\{f():_{c}\}\) and \(\{f():_{c}\}\) to discriminate \(c\), with a classification loss \(_{}\). Finally, the CAV \(_{c}^{_{f}}\) for \(c\) is defined as the weight vector for \(c\) in the classifier.

VL model  consists of an image encoder \(g_{ img}\) that projects input image \(\) into image features \(g_{ img}()^{D_{ VL}}\), and a text encoder \(g_{ text}\) that projects input texts \(\) into text features \(g_{ text}()^{D_{ VL}}\). After trained on a large-scale image-text dataset, \(g_{ img}()\) and \(g_{ text}()\) are projected into the same feature space and can be directly compared.

### Evaluation of CAV Quality

We propose two metrics (_concept accuracy_ and _concept-to-class accuracy_) to evaluate the CAV quality, based on the definition that CAV quantifies the importance of a concept to the class prediction.

**Concept accuracy.** Concept accuracy estimates whether the CAV faithfully represents its corresponding concept. To this end, the accuracy \((_{c})\) for CAV \(_{c}\) is calculated as the test accuracy of the binary classification model. Specifically, let \(\) denote the set of all concepts, concept accuracy \(S_{ concept}\) is finally calculated averagely over all concepts (note that \(\|\|\) denotes cardinality of a set):

\[S_{ concept}=\|}_{c}( _{c}).\] (1)

**Concept-to-class accuracy.** The original CAV simply determines whether the trained CAV \(_{c}\) has a positive relation to a class \(k\) in the target model, by simply determining whether the angle between \(_{c}\) and \( h_{k}(f())\) (the gradients of classification logit for class \(k\) on \(f()\)) is acute. However, this metric is too simplified to reflect the degree of connection between CAVs and classes. Therefore, we propose concept-to-class accuracy to estimate the extent to which the CAV \(_{c}\) relates to class \(k\) according to the cosine similarity between \(_{c}\) and \( h_{k}(f())\). We construct the ground-truth set (\(\)) of positively-related concept-class pairs by calculating the similarity between the concepts and the class names with a language model (like all-mpnet-base-v2  as used in CLIP-Dissect ) and selecting the concept-class pairs with the similarity exceeding a threshold \(\). Finally, concept-to-class accuracy \(S_{ concept\_to\_class}\) is calculated averagely over all ground-truth concept-class pairs \(\):

\[S_{ concept\_to\_class}=\|}_{(c,k)} _{c} h_{k}(f())}{\|_{c}\|\| h_{k}(f( ))\|}.\] (2)

### Lg-Cav

In this section, we first propose a **framework** on how to transfer the concept knowledge from VL model to the LG-CAV, then propose three modules into this framework to further improve the quality of LG-CAV: a **Gaussian alignment (GA) module**, a **concept ensemble (CE) module**, and a **deviation sample reweighting (DSR) module**.

#### 3.3.1 Framework

The features of concept descriptions extracted by VL model cannot be directly used to supervise the training of CAVs, because VL model and the target model have different feature spaces. Therefore, we propose an ingenious method that transforms the concept knowledge of VL model into activation values on a common pool of images (also named probe images, denoted as \(\)) and trains the LG-CAV from these activation values, inspired by previous concept-based method  that adopts probe images to recognize common units of different models.

Specifically, this method consists of three steps to train LG-CAV \(_{c}\): **(1)** Calculate the activation values \(\{_{_{c}}(f()):\}\) of \(_{c}\) on the image features \(\{f():\}\) extracted by the target model. **(2)** Calculate the activation values \(\{_{g_{ text}(c)}(g_{ img}()):\}\) of \(g_{ text}(c)\) on \(\{g_{ img}():\}\) using VL model. **(3)** Train \(_{c}\) by aligning \(\{_{_{c}}(f()):\}\) with \(\{_{g_{ text}(c)}(g_{ img}()):\}\), and the corresponding loss function \(_{ LG}\) is shown in Equation 3 (note that \(\|\|_{2}\) denotes the L2 norm, \(f\), \(g_{ text}\), \(g_{ img}\) are freezed, and only \(_{c}\) is trainable).

\[_{ LG}=\|}_{ }_{_{c}}(f())-_{g_{ text }(c)}(g_{ img}())^{2}.\] (3)

We calculate activation value as cosine similarity between two vectors (_e.g._, \(_{_{c}}(f())=_{c} f()}{\|_{c }\|\|f()\|}\)), because cosine similarity is invariant to the norms of feature vectors which differ a lot in different models. Therefore, the LG-CAV learns to recognize images with the corresponding concept and the images without the corresponding concept. Besides, compared with the original binary classification task for CAV training, the activation values encompass richer information about the extent to which the concepts exist in the images, thus facilitating the training of LG-CAV.

#### 3.3.2 Gaussian Alignment Module

However, directly utilizing the above \(_{}\) is not guaranteed to improve the quality of CAVs (see experiments in subsubsection 4.1.2), because the activation values calculated from VL model and the target model have significantly different distributions (due to the huge difference of feature space in the two models). To address this problem, Gaussian alignment (GA) module aligns the distribution of activation values for VL model with that for the target model, based on the observation that the distribution of activation values resembles a Gaussian distribution (Figure 2**(B)**). GA module consists of three steps: **(1)** Calculate the cosine similarity for each pair of features in \(\{f():\}\) to simulate the activation values from the target model, which will be \(=\{^{}) f(^{})}{|f(^{})|}:^{},^{}\}\). **(2)** Estimate the parameters (mean & standard deviation) of Gaussian distribution \(X(_{},\,_{}^{2})\) for \(\) (activation values from the target model), and \(X(_{},\,_{}^{2})\) for \(\{_{g_{}()}(g_{}()): \}\) (activation values from VL model). **(3)** Calculate the transformation function for these two Gaussian distributions, then use it to transform each \(_{g_{}()}(g_{}())\) to be \(}_{g_{}()}(g_{}())\), as shown in Equation 4.

\[}_{g_{}()}(g_{}())=_{g_{}()}(g_{}())-_{} }{_{}}_{}+_{}.\] (4)

Detailedly, this transformation first transforms \(X(_{},\,_{}^{2})\) into a standard Gaussian distribution (\(X(0,\,1)\)), then transforms the standard Gaussian distribution into \(X(_{},\,_{}^{2})\), as shown in Appendix A.

#### 3.3.3 Concept Ensemble Module

Concept ensemble (CE) module employs data augmentations on the concept descriptions, thus enhancing the comprehensiveness of the concept. Specifically, instead of using a single prompt like "a photo of the concept \(c\)" (that will be fed into \(g_{}\)), CE module uses multiple prompts (_e.g._, "a

Figure 3: **Top:** The original CAV is defined as the weight vector for its represented concept in the binary linear classifier. **Bottom:** The LG-CAV is learned by mimicking the activation values of its represented concept on the probe images \(\) using VL model. Besides, three modules (GA module, CE module, and DSR module) are proposed to enhance the quality of LG-CAV.

bright photo of the concept \(c^{}\), "a cropped photo of the concept \(c\)") to describe \(c\). These concept prompts follow the class prompts in the original CLIP model, as demonstrated in Appendix C.2. Next, \(g_{}\) will encode these augmented prompts into text features, and generate the augmented text features \(_{}(c)\) by averaging them.

#### 3.3.4 Deviation Sample Reweighting Module

Deviation sample reweighting (DSR) module optimizes the selection of probe images, by allocating higher training weights to the probe images that can more stably represent the concept. To this end, DSR module estimates the weight of the probe image \(\) according to the standard deviation of its similarities with the ground-truth positive images \(_{c}\), using three steps: **(1)** Calculate \(_{f}(,_{c})=\{) f(^{})}{ \|f()\|\|/(^{})\|}:^{}_{c}\}\). **(2)** Calculate the standard deviation \(_{f}(,_{c})\) of \(_{f}(,_{c})\). Note that \(_{f}(,_{c})\), and lower \(_{f}(,_{c})\) indicates more stable concept representation of \(\). **(3)** The weight \(_{f}(,_{c})\) is finally calculated by normalizing the opposite of \(_{f}(,_{c})\) with a softmax operation, as shown in Equation 5. Note that the averaged value of all sample weights equals \(1\), and the softmax function can be replaced by other normalization functions.

\[_{f}(,_{c})=\|\|- _{f}(,_{c})}{_{^{ }}-_{f}(^{},_{c}) }.\] (5)

#### 3.3.5 Loss Function

With the above three modules, the updated LG-CAV loss \(}_{}\) is calculated as in Equation 6. Besides, when positive images \(_{c}\) and negative images \(_{c}\) are provided, \(}_{}\) can be added into the training framework of original CAV to enhance the CAV quality, and the total loss function \(_{}\) will be \(_{}=_{}+}_{ }\) (note that \(_{}\) is the classification loss for the original CAV).

\[}_{} =\] (6)

### Model Correction

Due to the lack of high-quality CAVs that can sufficiently relate to all classes in the dataset, most existing CAV methods are confined to explaining the local behavior of target model using a very limited number and variety of CAVs. Different from these methods, our proposed method can train a sufficient quantity of high-quality LG-CAVs that relate to all classes in the dataset, thus having great potential to improve the performance of target model in an interpretable manner.

Specifically, our model correction method alleviates spurious correlation in the target model (_i.e._, incorrect dependence of a class on unrelated concepts) to improve the model performance. To this end, we fine-tune the target model to align the prediction of each class with its strongly-related LG-CAV. However, directly aligning the gradients for each class with the LG-CAV would easily interfere with other correct concepts and hurt the performance. To align them in a more soft manner, activation sample reweighting (ASR) module allocates different training weights to the images of each class, according to the activation values of the corresponding LG-CAV on them. Assume concept \(c\) is strongly related to class \(k\), and let \(_{k}\) denote the training images of class \(k\), then ASR module reweights image \(\) of \(_{k}\) in two steps (similar to DSR module): **(1)** Calculate \(_{_{c}}(f())\) (the activation value of LG-CAV \(_{c}\) on \(\)). **(2)** Calculate the weight \(_{f}^{}()\) by normalizing \(_{_{c}}(f())\) with a softmax operation, as shown in Equation 7.

\[_{f}^{}()=\|_{k}\|_{_{c}}(f())}{_{^{ }_{k}}_{_{c}}(f(^{ }))}.\] (7)

Next, \(_{f}^{}()\) will be used as the weight of image \(\) in the classification loss during fine-tuning. In this manner, the target model learns to predict class \(k\) from the samples activated more highly by the LG-CAV \(_{c}\), thus better aligning the prediction of class \(k\) with its strongly-related concept \(c\). Besides, this method requires no further training on the backbone \(f\) (only uses \(f\) to extract image features and trains the subsequent layers), leading to minimal training cost.

## 4 Experiments

### The Quality of LG-CAV

#### 4.1.1 Experiment Settings

**Datasets.** We estimate the quality of LG-CAV on the Broden dataset  (a popular concept-based dataset with 63,305 images for 1197 visual concepts). In the Broden dataset, each image may contain multiple concepts. Therefore, we collect positive samples for each concept by selecting the images containing only this concept, and randomly select the same number of images from other concepts as negative samples. Finally, the simplified Broden dataset consists of 17,746 images for 468 visual concepts. The probe images (\(\)) for each LG-CAV are from ImageNet and the images of other concepts in the Broden dataset. Specifically, we select the most activated and the same number of most least activated images by VL model.

**Backbones.** We follow the original CAV work  to train CAVs for the target models pre-trained on ImageNet (from the open-sourced PyTorch package ). These backbones include ResNet , DenseNet , VGG , and Vision Transformer .

**Parameters.** To simulate the absence of images for training CAVs in reality, we set the number of positive samples (\(_{c}\)) and negative samples (\(_{c}\)) to be 10, and the remaining images will be used as the test set. The threshold \(\) for determining positively-related concept-class pair is 0.6. For each CAV method, we use SGD optimizer  to train the CAV for 10 epochs with a learning rate of 1e-3. \(\|\|\) (the number of probe images) is set to be 1000. The loss function adopted here is \(_{}\) since \(_{c}\) and \(_{c}\) are available.

#### 4.1.2 Experiment Results

**Concept accuracy.** Table 1 demonstrates that without sufficient data, the accuracy of original CAV is insufficient to accurately represent a concept. The first version of LG-CAV (**Ours**) has a lower accuracy than the original CAV when no other modules are added, due to the large difference in the distribution of activation values. The added GA module aligns the activation values from target model and VL model, and improves the concept accuracy by 5.83 points averagely. The added CE and DSR modules both effectively improve the concept accuracy, and the final LG-CAV outperforms Text-to-Concept and OA-TCAV (see Appendix C.3 for the analysis of them) by a large margin.

**Concept-to-class accuracy.** Table 2 demonstrates that our proposed modules also enhance concept-to-class accuracy, because the CAV that better represents a concept can more accurately correspond

  Method & **Res-18** & **Res-34** & **Res-50** & **Dense-121** & **Dense-169** & **VGG-13** & **VGG-19** & **ViT-B** & **VIT-L** \\  Original CAV  & 68.92 & 69.32 & 71.34 & 72.46 & 72.70 & 67.44 & 69.56 & 65.35 & 65.85 \\ Text-to-Concept  & 70.04 & 71.35 & 72.40 & 73.67 & 74.19 & 68.35 & 70.24 & 67.22 & 66.27 \\ OA-TCAV  & 72.62 & 72.20 & 73.24 & 73.90 & 74.89 & 68.69 & 70.81 & 67.83 & 73.35 \\ 
**Ours** & 67.23 & 67.48 & 67.52 & 69.43 & 68.46 & 65.99 & 67.94 & 63.16 & 63.22 \\
**Ours + GA** & 74.89 & 73.47 & 74.19 & 76.28 & 74.70 & 69.63 & 72.31 & 68.99 & 68.41 \\
**Ours + GA + CE** & 76.41 & 74.47 & 75.63 & 78.18 & 76.12 & 70.25 & 72.92 & 69.43 & 69.31 \\
**Ours + GA + CE** + DSR** & **77.45** & **76.04** & **76.48** & **79.07** & **77.25** & **70.69** & **73.47** & **70.52** & **70.09** \\  

Table 1: The comprehensive evaluation of **concept accuracy** (%) for different CAVs on the Broden dataset. The results are on nine backbones pre-trained on ImageNet (Note that **Res** denotes ResNet, **Dense** denotes DenseNet) averaged over 4 runs with different seeds. Bold font denotes the best result.

  Method & **Res-18** & **Res-34** & **Res-50** & **Dense-121** & **Dense-169** & **VGG-13** & **VGG-19** & **ViT-B** & **VIT-L** \\  Original CAV  & 6.20 & 7.02 & 7.20 & 6.08 & 6.54 & 5.40 & 5.53 & 7.22 & 7.97 \\ Text-to-Concept  & 9.48 & 9.06 & 8.73 & 7.42 & 8.33 & 7.52 & 7.07 & 10.70 & 11.09 \\ OA-TCAV  & 10.11 & 10.29 & 10.90 & 9.18 & 10.63 & 8.38 & 8.74 & 10.07 & 10.68 \\ 
**Ours** & 4.72 & 6.66 & 5.99 & 5.64 & 6.02 & 4.02 & 4.49 & 6.07 & 6.38 \\
**Ours + GA** & 16.72 & 16.61 & 16.92 & 15.47 & 15.81 & 14.55 & 15.04 & 17.52 & 17.83 \\
**Ours + GA + CE** & 19.14 & 20.10 & 20.51 & 18.78 & 19.00 & 17.50 & 18.35 & 21.52 & 22.34 \\
**Ours + GA + CE** + DSR & **24.58** & **25.61** & **26.05** & **23.93** & **23.97** & **21.40** & **22.79** & **26.12** & **27.72** \\  

Table 2: The comprehensive evaluation of **concept-to-class accuracy** for different CAVs on the Broden dataset averaged over 4 runs with different seeds.

to its strongly-related class. The final LG-CAV also has much higher concept-to-class accuracy than Text-to-Concept and OA-TCAV.

### Model Correction

#### 4.2.1 Experiment Settings

Datasets.We employ our model correction method on three representative datasets: ImageNet  (large-scale dataset), CUB-200-2011  (a popular dataset used by many concept-based methods), and CIFAR-100  (small-scale dataset). The probe images (\(\)) for each LG-CAV are also the most highly and least activated images by VL model from their respective datasets.

Backbones.The target models pre-trained on ImageNet are from PyTorch, and the target models pre-trained on CIFAR-100 and CUB-200-2011 are from another open-sourced PyTorchCV package, following PCBM . The VL model adopted here is CLIP model with ViT-L/14 as backbone.

Parameters.We use SGD optimizer to train the final classification layer for 20 epochs with a learning rate of 1e-3. Note that different from the experiments in subsection 4.1, the training of LG-CAVs for these concepts does not require the original classification loss \(_{}\) and DSR module, due to the lack of ground-truth positive samples \(_{c}\).

#### 4.2.2 Experiment Results

We adopt two methods to find the strongly-related concept of each class in the target model, corresponding to two types of datasets: **datasets with few classes \(\&\) datasets with many classes.**

Datasets with few classes.We manually collect the concept descriptions of each class from Wikipedia for these datasets (_e.g._, the randomly selected subset of ImageNet with 40 classes (ImageNet-40)). The selected classes and their corresponding concept descriptions can be referred to in Appendix C.1.

Appendix B.1 demonstrates that the trained LG-CAVs have ability to distinguish whether images contain their respective concepts. Next, we utilize these LG-CAVs for model correction with the ASR module. As shown in Table 3, our model correction method effectively improves the performance of original pre-trained model (converted from the pre-trained 1000-classes model by removing other 960 classes in the final classification layer), by an improvement of up to 0.91 points.

Datasets with many classes.Collecting sufficient high-quality concept descriptions for datasets with many classes is a challenging task. Therefore, we instead acquire the concept descriptions of each class based on its comparison with its confused class, inspired by relative CAV proposed in the original CAV work. Specifically, for the class \(k\), we first find the confused class \(k^{}\) to which images from class \(k\) are most likely to be mispredicted by the pre-trained model, then define the concept descriptions as "a photo of class \(k\), not \(k^{}\)". This approach is applied to the whole ImageNet, CUB-200-2011, and CIFAR-100.

   Method & **Res-18** & **Res-34** & **Res-50** & **Dense-121** & **Dense-169** & **VGG-13** & **VGG-19** & **ViT-B** & **ViT-L** \\  Original & 69.76 & 73.31 & 76.13 & 74.43 & 75.60 & 69.93 & 72.38 & 81.07 & 79.67 \\ Concept\_Distillation  & 69.46 & 73.06 & 75.77 & 74.04 & 75.46 & 69.80 & 72.29 & 80.86 & 79.51 \\ KD  & 69.93 & 73.49 & 76.27 & 74.68 & 75.99 & 70.06 & 72.46 & 81.15 & 79.77 \\ Label-free CBM  & N/A & N/A & 71.95 & N/A & N/A & N/A & N/A & N/A & N/A \\ 
**Ours** & **70.26** & **73.66** & **76.47** & **74.94** & **76.28** & **70.19** & **72.60** & **81.38** & **80.05** \\   

Table 4: The comprehensive evaluation of accuracy (%) for different methods on ImageNet (note that KD denotes knowledge distillation) averaged over 4 runs with different seeds.

   Method & **Res-18** & **Res-34** & **Res-50** & **Dense-121** & **Dense-169** & **VGG-13** & **VGG-19** & **ViT-B** & **ViT-L** \\  Original & 90.65 & 91.00 & 92.50 & 92.00 & 92.75 & 88.55 & 90.90 & 94.55 & 93.15 \\ HiBug  & 90.38 & 90.82 & 92.69 & 92.36 & 92.60 & 88.74 & 91.07 & 94.74 & 93.46 \\ 
**Ours** & **91.16** & **91.79** & **93.06** & **92.91** & **93.16** & **89.21** & **91.43** & **94.94** & **93.66** \\   

Table 3: The comprehensive evaluation of accuracy (%) on selected classes (40 classes) of ImageNet averaged over 4 runs with different seeds.

As shown in Table 4, our method improves model performance on the whole ImageNet in an interpretable manner based on LG-CAV, surpassing original model (by up to 0.68 points), Concept_Distillation, knowledge distillation, and Label-free CBM (see Appendix C.3 for the analysis of them). Besides, the results on CUB-200-2011 (Appendix B.2) & CIFAR-100 (Appendix B.3) also verify the effectiveness of our method.

### Ablation Study

**Selection of probe images.** In the above experiments, we select the most activated and least activated images as probe images \(\). We compare this selection strategy with random selection in this subsection. As shown in the first two figures of Figure 4, the LG-CAVs trained with this selection strategy have higher quality, because the probe images selected by this strategy contain richer information to represent the corresponding concepts.

**Number of probe images.** This subsection investigates how the number of probe images affects the quality of LG-CAV. As shown in the last two figures of Figure 4, when the number of probe images is small, increasing the quantity of probe images can improve the quality of LG-CAV. However, when the number of probe images reaches a certain level (saturation), further increasing the quantity of probe images does not improve the quality of LG-CAV.

Additionally, we provide more ablation experiments on the choice of VL model, the coefficient of LG-CAV loss, and the depth of extracted image features in the target model in Appendix B.5.

### Visualization Results

**Activation values of the trained LG-CAV.** Figure 5 **(A)** demonstrates the activation values of a trained LG-CAV on its highly-activated and lowly-activated images, indicating that the trained LG-CAV can accurately activate images that contain the corresponding concepts.

**Examples of model correction.** As shown in Figure 5 **(B)**, an image of "Tiger Cat" is misclassified as "Tabby Cat" by the target model (with ResNet18 as backbone) before model correction. During model correction, ASR module mitigates spurious correlation of the target model by aligning the prediction of "Tiger Cat" with its strongly-related concept "a cat animal with orange stripes". This image is activated by the LG-CAV of this concept with a high activation value (0.7488), thus the classification logit for "Tiger Cat" increases after model correction. Besides, we utilize Grad-CAM  to attribute the prediction of "Tiger Cat" in the target model, and it shows that the attribution map focuses more accurately on the cat's body after model correction.

Furthermore, we provide more visualization results in Appendix D.

Figure 4: Ablation experiments on probe images (selection strategy & image number).

Figure 5: **(A)** Activation values of the LG-CAV & **(B)** Model correction example.

Conclusion

In this work, we propose LG-CAV to address the data-scarcity problem of original CAV by transferring the extensive concept knowledge from VL model. Specifically, LG-CAV mimics the activation values from VL model on the probe images to learn these concept knowledge. Besides, we propose a Gaussian alignment (GA) module, a concept ensemble (CE) module, and a deviation sample reweighting (DSR) module to further enhance the quality of LG-CAV. Furthermore, we go beyond previous CAV methods by generalizing LG-CAV to model correction, with a human-understandable method that aligns the class predictions with the strongly-related concepts. Experiment results demonstrate that LG-CAV significantly improves the CAV quality, and our model correction method outperforms existing concept-based methods by a large margin. We hope our work can provide inspiration for future interpretable methods based on vision-language models.