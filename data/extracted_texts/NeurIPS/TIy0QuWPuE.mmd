# Post-Calibration Techniques: Balancing Calibration and Score Distribution Alignment

Agathe Fernandes Machado

Universite du Quebec a Montreal

201 Av. du President-Kennedy, Montreal, QC H2X 3Y7, Canada

fernandes_machado.agathe@courrier.uqam.ca

Arthur Charpentier

Universite du Quebec a Montreal

201 Av. du President-Kennedy, Montreal, QC H2X 3Y7, Canada

charpentier.arthur@uqam.ca

Emmanuel Flachaire

Aix-Marseille School of Economics, Aix-Marseille Univ.

5 Bd Maurice Bourdet CS 50498, 13205 Marseille Cedex 01, France

Emmanuel.flachaire@univ-amu.fr

Ewen Gallic

Aix-Marseille School of Economics, Aix-Marseille Univ.

5 Bd Maurice Bourdet CS 50498, 13205 Marseille Cedex 01, France

ewen.gallic@gmail.com

Francois Hu

Milliman France

14 Av. de la Grande Armee, 75017 Paris, France

hu.faugon@gmail.com

###### Abstract

A binary scoring classifier can appear well-calibrated according to standard calibration metrics, even when the distribution of scores does not align with the distribution of the true events. In this paper, we investigate the impact of post-processing calibration on the score distribution (sometimes named "recalibration"). Using simulated data, where the true probability is known, followed by real-world datasets with prior knowledge on event distributions, we compare the performance of an XGBoost model before and after applying calibration techniques. The results show that while applying methods such as Platt scaling, Beta calibration, or isotonic regression can improve the model's calibration, they may also lead to an increase in the divergence between the score distribution and the underlying event probability distribution.

## 1 Introduction

When estimating a probabilistic scoring classifier, the model must not only discriminate between observations according to their class but also return scores that can be interpreted as probabilities. Thedistribution of scores produced by the classifier should align with the underlying event distribution. To assess whether classifiers return probabilistic scores, one must evaluate the model's calibration [4; 24; 8]. While some models, such as logistic regression when correctly specified, are known to be well-calibrated , others, including ensemble methods like Random Forests (RF) [13; 3] and XGBoost , are not inherently calibrated . To assess a model's ability to provide probabilistic scores, the literature recommends evaluating its calibration using metrics like the Brier Score (BS, ) or the Integrated Calibration Index (ICI, ). When a model is not well-calibrated, post-processing calibration methods including Platt scaling , Beta calibration , or isotonic regression  are often applied to adjust the scores [21; 11; 17]. After applying calibration techniques, these metrics generally indicate an improvement in the model's calibration relative to its initial state.

Since the true underlying probability distribution of the data is typically unobserved in practice, calibration metrics are assessed solely on the classifier's output range. Fernandes Machado et al.  demonstrated with simulated data that methods such as RF and XGBoost, can appear well-calibrated according to standard calibration metrics and exhibit strong discrimination based on performance metrics, yet still fail to align the score distribution with the true event distribution. This discrepancy can arise when predicted scores from those algorithms lack the heterogeneity present in the underlying data distribution. They demonstrate this misalignment by comparing the selection of model hyperparameters based on Kullback-Leibler (KL) divergence with the selection based on performance or calibration metrics, knowing the true event distribution in the case of simulated data. For real data, where the true distribution is unknown, the approach involves prior information about the underlying data distribution to better align it with the predicted score distribution. Their analysis only considers model evaluation to accurately interpret predicted scores as probabilities, typically through calibration metrics. However, many practitioners employ post-calibration techniques to ensure that output scores represent probabilistic estimates. In this paper, we examine how post-calibration techniques affect the variability of score distribution in XGBoost binary classifiers, comparing it to the true underlying data distribution using KL divergence. We find that these post-processing methods often reduce score heterogeneity. Additionally, the misalignment between tree-based models optimized for KL divergence and those optimized for calibration or performance metrics persists and may even worsen after calibration, indicating that score alignment can decrease following post-calibration. Using simulated data with known true probabilities, followed by real-world datasets with prior knowledge of event distributions, we evaluate the abilities of XGBoost-predicted scores as probabilistic estimates before and after applying calibration techniques, with an emphasis on score distribution rather than solely on their calibration.

## 2 Calibration

We focus on the context of a binary scoring classifier. Let \(Y=\{0,1\}\) be a binary response variable, and let \(=^{d}\) denote features. The goal is to predict \(s()=(Y=1|)\), using a sample of \(n\) i.i.d. observations \((_{i},y_{i})_{i=1}^{n}\). We estimate this probability \((_{i})\) using an XGBoost classifier, which produces a distribution of estimated scores \(()\). If the score distribution is poorly calibrated, these scores cannot be interpreted as the "true underlying probabilities" [33; 19; 16]. A model \(\) is well-calibrated for a binary variable \(Y\) when :

\[(Y=1())=[Y()]=( ),\] (1)

i.e., equivalently, \([Y()=p]=p, p\).

### Calibration Metrics

To measure calibration, the literature suggests various metrics. Here, we focus on two of them: BS and ICI. The former [12; 17; 29; 30], often used to assess a model's calibration, is a proper scoring rule that also accounts for refinement loss . It writes: \(=n^{-1}_{i=1}^{n}(_{i})-y_{i}^{2}\). More recently, Austin and Steyerberg  introduced the ICI, a metric that relies on the calibration curve. In the binary case, the calibration curve writes \(:, p(p):=[Y ()=p]\). For a well-calibrated model, the calibration curve corresponds to the identity function, \((p)=p\), where the predicted score \(p\) equals the true likelihood of the event. Graphically, this is represented by the calibration curve aligning with the 45-degree diagonal. While the calibration curve is usually estimated using bins [35; 20; 27], the ICI relies on a smoother version, based on splines. The empirical version writes \(=n^{-1}_{i=1}^{n}(_{i})-} (_{i})\), which corresponds to computing the average of the absolute difference between the estimated calibration curve and the identity function, the latter representing perfect calibration.

### Calibration Methods

When using scores generated by a model estimating the probability of a binary event, the literature advocates calibrating the model by applying the calibration curve g--which serves as a transformation function--on the scores [29; 37; 17; 20]. In this paper, we focus on three calibration methods: Platt scaling, isotonic regression, and Beta calibration.

Platt ScalingThis parametric approach consists of fitting a logistic regression to the binary response variable using predicted scores of a binary classifier as the unique feature . The obtained calibrated probabilities are \((())=1+-(( )-)}^{-1}\), where \(\) and \(s\) (\(s>0\) for a non-decreasing calibration map \(\)) are estimated on a calibration set. It should be noted that Platt scaling is unable to learn the identity function g if the predicted scores are already calibrated .

Beta CalibrationThe scores returned by a binary classifier are in range \(\). Beta calibration  builds on this feature and assumes that the score within each class of the target variable \(y\) are distributed according to a Beta distribution. By contrast, Platt scaling assumes the scores follow a Normal distribution within each class. The calibration map writes \((())=(1+-a()+b( 1-())-c})^{-1}\), where \(a\), \(b\), and \(c\) are the three parameters that need to be estimated on a calibration set. Unlike Platt scaling, Beta calibration can learn the identity function g (with \(a=b=1,c=0\)), making it suitable for already well-calibrated models. By restricting \(a,b>0\), the calibration map is monotone.

Isotonic RegressionThis solution arises from a constrained optimization problem , solved using the Pool-Adjacent-Violators Algorithm, ensuring that corrected predicted scores remain monotonic: \(_{_{1},,_{n}}_{i=1}^{n}(y_{(i)}-_{i})^{2}\), s.t. \(_{1}_{n}\), where \(y_{(i)}\) corresponds to the value in \(\{y_{1},,y_{n}\}\) associated with the \(i\)-th largest predicted score \(\{(_{1}),,(_{n})\}\). Isotonic regression will lead to \(((_{i}))=_{i}^{*}\) where \(_{i}^{*}\) solve the optimization problem.

## 3 Score Heterogeneity

To accurately interpret predicted scores from a binary classifier as probabilistic estimates, since the true underlying probability \(s()\) is usually unobservable, calibration metrics rely solely on the predicted score range. When a binary classification model is well-calibrated, the distribution of its scores \(()\), as defined by Eq. 1, should align with the actual probability of the event in the vicinity of score values. Therefore, calibration metrics cannot fully capture discrepancies between the score distribution and the true probability distribution of the response variable \(Y\) when the predicted score variability does not accurately reflect the latter.

Kullback-Leibler divergenceFernandes Machado et al.  demonstrated through simulated data that scores from ensemble methods may exhibit less variability compared to the true underlying probabilities when selecting hyperparameters based on calibration (ICI) or performance (AUC) metrics. This reduced heterogeneity makes calibration metrics less reliable for interpreting output scores as probabilities of event occurrence. Instead of evaluating discrepancies solely on predicted score values, the authors emphasize evaluating the model's probabilistic estimates using KL divergence between the overall score distribution, \(()\), and the available information on the "true" distribution, \(s()\). Additionally, the flexibility of tree-based methods like XGBoost enables the selection of model hyperparameters based on KL divergence instead of traditional performance metrics, ensuring the availability of a model whose predicted score distribution closely aligns with prior knowledge.

Bayesian FrameworkWhen working with simulated data, the distribution \(s()\) is fully known, allowing for the direct computation of KL divergence with \(()\). However, with real data, the KL divergence can only be computed by relying on a prior belief about the distribution of \(s()\), potentially informed by expert opinion, and thus assuming a prior distribution \(\). In the following, as in Fernandes Machado et al. , we take \(s()=(,)\) as the assumed prior distribution where each probability \(p_{i}\) of the \(i\)-th observation is a sample from \(\). We observe a sequence of \(n\) independent (as the features \(_{i}\) are considered \(n\) i.i.d. random variables) but non-identically distributed binary random variables \(Y_{i}\) where \(Y_{i}|s(_{i})=p_{i}(p_{i})\). In this case, instead of selecting the model with hyperparameters that minimize the empirical mean of the KL divergence across individual distributions, we directly minimize the distance between the prior distribution \(\) and the overall distribution of \(()\).

Calibration techniquesWe extend the work of Fernandes Machado et al.  by investigating how score heterogeneity predicted by certain XGBoost algorithms is affected after applying post-calibration techniques such as Platt scaling, Beta calibration, or isotonic regression. These methods can potentially reduce score heterogeneity; for instance, isotonic regression applies a stepwise function g. Additionally, with Platt scaling, the range of calibrated predicted scores is always narrower than the range of the initial scores when the parameter \(s\) (see Appendix A.1). And, due to the concavity of the sigmoid function over \([0,+]\), this post-calibration method tends to reduce the range of predicted scores more significantly when the initial scores are highly concentrated.

## 4 Numerical Experiments

### Simulated Data

We use the simulated data from Fernandes Machado et al. . We consider four data-generating processes (DGPs), all of which use a logistic link function. The first three are from Ojeda et al. , the fourth adds interaction terms (see Appendix B.1). For each DGP, we generate data that include more or less noise variables: 0, 10, 50 or 100. We split the data into four samples: the train and validation samples used to train an XGBoost model and select the set of hyperparameters, the calibration sample to train a calibrator using the selected model, and lastly, a test sample to assess the performance of models. We select the model's hyperparameters (number of boosting iterations and maximum tree depth) to optimize either one of three different criteria on the validation set: maximizing AUC (AUC*), minimizing KL divergence (KL*), and, for illustrative purposes, producing a model that is poorly calibrated based on the ICI metric (High ICI). Once the hyperparameters are selected, a calibration technique is applied to the scores. This allows for a comparison of models on the test set, both before and after calibration, according to the chosen optimization criterion. We run the simulations on 100 replications for each configuration. The results for DGP 1 are shown in Fig. 1 (see Fig. C17 for full results and Table C1 for numerical values). The x-axis represents calibration, measured by the ICI, where lower ICI values indicate better calibration. The y-axis shows the KL divergence between the model's predicted score distribution and the true probabilities, with lower values indicating closer alignment between the two distributions. A model is preferable when it achieves better calibration and closer alignment between score distributions and true probabilities. Shapes represent models before calibration, while arrows show their performance after applying _post-hoc_ calibration. Ideally, _post-hoc_ calibration improves both metrics for uncalibrated models, resulting in arrows pointing down and to the left on the graph.

When the model is selected to optimize AUC (AUC*), calibration is generally fairly good across all DGPs, regardless of the number of noise variables. Applying a _post-hoc_ calibration technique typically reduces the ICI, further improving model calibration. However, Platt scaling (green solid arrows) often fails, as the logistic function lacks the identity mapping. The score distributions from models optimized for AUC, however, are poorly aligned with the true probability distributions. For noise-free datasets, the KL divergence is approximately 2.5 times larger compared to models optimized for KL divergence. This gap widens as the number of noise variables increases. When post-calibration techniques are applied to AUC-optimized models, KL divergence increases with Platt scaling and isotonic regression but decreases with Beta calibration. However, even with Beta calibration, the KL divergence remains higher than that of models optimized for KL divergence. For initially miscalibrated models (High ICI), post-calibration generally improves calibration, with improvements seemingly unaffected by the number of noise variables. However, the impact on KL divergence is more mixed, with no systematic improvement observed, particularly for DGPs 2 and 4.

Overall, while post-calibration improves model calibration, it does not consistently align score distributions with true probabilities and may even exacerbate misalignment, highlighting trade-offs between calibration and distribution alignment.

### Real Data

The 10 datasets from the UCI ML Repository used in Fernandes Machado et al.  are used here (see details in Appendix B.2). For each dataset, we apply the method outlined in Section 4.1, this time calculating the KL divergence between the predicted score distribution and the prior distribution described in Section 3.1 The results across the 10 datasets are shown in Fig. 2, with detailed metric values in Table C2. The x-axis represents calibration with ICI, and the y-axis shows KL divergence (lower values indicate closer alignment with Beta priors). Shapes denote models before calibration, and arrows indicate changes after applying post-calibration techniques from Sec. 2.2.

The findings are consistent with Section 4.1. When applied to already calibrated scores with low ICI (models AUC* and KL*), Platt scaling often worsens both calibration and score alignment with the Beta prior, since the calibration map cannot approximate the identity function (as seen in datasets adult, bank, default, drybean, occupancy, and spambase). In this case, Beta calibration and isotonic regression frequently outperform Platt scaling in both calibration and alignment with the Beta prior. Notably, Beta calibration surpasses isotonic regression in most datasets, particularly concerning KL divergence. For models with initially uncalibrated scores (High ICI), post-calibration techniques either show lower ICI and lower KL divergence (abalone, coupon), or result in increased KL divergence alongside improved calibration (mushroom, occupancy). In such cases, all calibration methods exhibit similar trends in KL divergence and ICI, with no single post-calibration technique consistently outperforming the others, as their effectiveness varies across datasets.

To summarize, for already calibrated scores, post-calibration techniques generally reduce score alignment with Beta priors, as indicated by KL divergence, although Beta calibration results in a smaller deterioration compared to isotonic regression and Platt scaling. For scores with high initial ICI, post-calibration improves calibration but may either reduce or increase score alignment depending on the dataset.

Figure 1: Average KL divergence and ICI before and after recalibration, for DGP 1.

Figure 2: Average KL divergence and ICI before and after recalibration.