# PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation

Chenrui Duan\({}^{1,2*}\) Zelin Zang\({}^{2*}\) Siyuan Li\({}^{1,2}\) Yongjie Xu\({}^{1,2}\) Stan Z. Li\({}^{2}\)

\({}^{1}\)Zhejiang University, College of Computer Science and Technology; \({}^{2}\)Westlake University

duanchenrui@westlake.edu.cn;

{zangzelin; lisiyuan; xuyongjie; stan.zq.li}@westlake.edu.cn

\({}^{*}\)Equal contribution \({}^{}\)Corresponding author

###### Abstract

Phylogenetic trees elucidate evolutionary relationships among species, but phylogenetic inference remains challenging due to the complexity of combining continuous (branch lengths) and discrete parameters (tree topology). Traditional Markov Chain Monte Carlo methods face slow convergence and computational burdens. Existing Variational Inference methods, which require pre-generated topologies and typically treat tree structures and branch lengths independently, may overlook critical sequence features, limiting their accuracy and flexibility. We propose PhyloGen, a novel method leveraging a pre-trained genomic language model to generate and optimize phylogenetic trees without dependence on evolutionary models or aligned sequence constraints. PhyloGen views phylogenetic inference as a conditionally constrained **tree structure generation** problem, jointly optimizing tree topology and branch lengths through three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and (iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function to guide the model towards a more stable gradient descent. We demonstrate the effectiveness and robustness of PhyloGen on eight real-world benchmark datasets. Visualization results confirm PhyloGen provides deeper insights into phylogenetic relationships.

## 1 Introduction

Phylogenetic trees  (or evolutionary trees) are tree-structured graphs representing kinship relationships between species , where each leaf node represents a distinct species and internal nodes represent evolutionary bifurcations. The tree's topology reflects evolutionary relationships based on their genetic characteristics, and branch lengths indicate evolutionary distances. Phylogenetics study is the foundation of evolutionary synthetic biology and is critical for tracking the evolutionary trajectories of species, analyzing the transmission pathways of newly discovered viruses, and providing meaningful insights for clinical applications .

**Background.** Despite DNA sequencing technologies  allowing us to construct evolutionary relationships based on molecular properties (DNA, RNA, and proteins), phylogenetic inference remains challenging due to its complex parameter space, which encompasses both continuous (branch lengths) and discrete (tree topology) components. Traditional MCMC-based methods like MrBayes  and RevBayes , despite their ability to extensively explore the huge tree space, are hindered by slow convergence rates. Additionally, the number of possible tree topologies for n species grows factorially as \((2n-5)!!\) for \(n 3\), posing huge computational challenges. In contrast, VI-based methods leverage approximate inference to provide more efficient estimations. Depending on the data type and research objectives, these methods can be further categorized into tree representation learning and tree structure generation. **Tree Representation Learning**, as shown in Fig. 1(a),intends to learn topological representations from existing tree structures. For instance, Subsplit Bayesian Networks (SBNs)  focus on probabilistic representations from given tree structures without considering branch lengths. Variational Bayesian Phylogenetic Inference (VBPI)  and its extensions VBPI-GNN  utilize the topological probabilities provided by SBNs and jointly model tree structures in latent space by deriving branch lengths through variational approximations. However, these methods require pre-generated topology, based on which better topological representations are then learned. **Tree Structure Generation**, as shown in Fig. 1(b), aim to infer tree structures directly from sequences. PhyloGFN  integrates VI and reinforcement learning to construct tree topologies, simplifying branch lengths into discrete intervals, thus necessitating posterior data for inference. VaiPhy  introduces the SLANTIS sampling strategy and basic biological models (e.g., Jukes-Cantor(JC) model ) for topology and branch length estimation. ARTree  builds tree topologies recursively using a graph autoregressive model. GeoPhy  models tree topologies in a continuous geometric space. However, these methods require input sequences to be aligned to equal lengths, and tree topology and branch lengths are optimized separately. They also ignore sequence features that are critical for accurate phylogenetic analysis.

**Our Method.** As depicted in Fig. 1(c), we propose a novel approach based on a pre-trained genome language model. Our model does not rely on evolutionary models or the requirement to align input sequences to equal lengths and fully exploits the prior knowledge embedded in biological sequences. PhyloGen models phylogenetic tree inference as a conditional-constrained tree structure generation problem, aiming to generate and optimize the tree topology and branch lengths jointly. We map species sequences into a continuous geometric space and perform end-to-end variational inference without restricting topological candidates. To ensure the topology-invariance of phylogenetic trees, we incorporate distance constraints in the latent space to maintain translational rotation invariance. Our approach demonstrates effectiveness and efficiency on the eight real-world benchmark datasets and verifies its robustness through data augmentation and noise addition . In addition, we propose a new scoring function to guide the model towards a more stable and faster gradient descent.

## 2 Related Works

**MCMC-based methods**, such as MrBayes  and RevBayes , have been widely used for phylogenetic inference due to their ability to explore vast tree spaces.

**VI-based methods** offer a more efficient alternative to MCMC by leveraging approximate inference techniques. These methods can be categorized into two main approaches: structure representation and structure generation. **Tree Representation Learning.** This approach focuses on extracting information from existing tree structures. SBNs  capture the relationships between existing

Figure 1: **Comparison of PhyloTree Tree Inference Methods.****(a)** The inputs are aligned sequences, and topologies are learned from existing tree structures using methods like SBNs, which rely on MCMC-based methods for pre-generated candidate trees without considering branch lengths directly. **(b)** The inputs are aligned sequences, and then tree structures and branch lengths are directly inferred by variational inference and biological modules. These methods optimize tree topology and branch lengths separately. **(c)** The inputs are raw sequences processed by a pre-trained language model to generate species representations. Then, an initial topology is generated through a tree construction module, and the topology and branch lengths are co-optimized by the tree structure modeling module.

subsplits without addressing branch lengths. VBPI  and its variants VBPI-NF  and VBPI-GNN : These methods introduce a two-pass approach to learn node representations, including branch lengths. VBPI employs variational approximations to handle branch lengths, allowing for joint modeling of the tree's latent space. **Tree Structure Generation.** This approach aims to infer tree structures directly from sequence data. PhyloGFN  utilizes GFlowNet [10; 20; 21], a combination of VI and reinforcement learning, which requiring posterior data for accurate inference. VaiPhy  incorporates a sampling strategy and basic biological models to estimate topology and branch lengths. ARTree  employs a graph autoregressive model to build detailed tree topologies. GeoPhy  models tree topologies within a continuous geometric space, offering a different approach to the distribution of tree topologies. For details on related work, please see Appendix B.

## 3 Methods

**Notation.** The phylogenetic tree is denoted as \((,B_{})\), where \(\) is an unrooted binary tree topology reflecting evolutionary relationships among \(N\) species. \(B_{}\) denotes the non-negative evolutionary distances of each branch. The tree consists of \(N\) leaf nodes, each corresponding to a species, and several internal nodes. PhyloGen aims to generate tree topology and branch lengths directly from the raw sequences.

**Framework.** We model the phylogenetic tree inference problem as a tree structure generation task under conditional constraints, consisting of three main modules as shown in Fig. 2. Feature Extraction module extracts genome embeddings from raw sequences via a pre-trained language model. PhyloTree Construction module uses these embeddings to generate an initial tree structure using a tree construction algorithm, introducing the latent variable \(z^{*}\) to represent the tree topology. PhyloTree Structural Modeling module iteratively refines the tree structure and branch lengths through topology learning and branch length learning components. By integrating these modules, we transform the complex dynamics of evolutionary history into a tree-based learning framework, facilitating a deeper understanding of phylogenetic relationships.

A. Feature ExtractionWe utilize DNABERT2 , a genome language model, to transform molecular (DNA) sequences \(Y=\{y_{i}\}_{i=1}^{N}\) into genomic embeddings \(E=\{e_{i}\}_{i=1}^{N}\). These embeddings discern complex patterns and long-range dependencies, serving as the basis for generating the latent variables \(z^{*}\), which dynamically inform both the topology and the branch lengths. By introducing DNABERT2, we redefine the construction of phylogenetic trees as a continuous optimization problem within a biologically meaningful embedding space.

Figure 2: **Framework of PhyloGen.****A. Feature Extraction** module extracts genome embeddings \(E\) from raw sequences \(Y\) using a pre-trained language model. **B. PhyloTree Construction** module uses \(E\) to compute topological parameters, which generate an initial tree structure \(^{*}\) via the Neighborhooding algorithm. **C. PhyloTree Structure Modeling** module jointly model \(\) and \(B_{}\) through the topology learning component (TreeEncoder \(R\) and TreeDecoder \(Q\)) and the branch length (Blens) learning component (dual-pass traversal, DGCNN network, Blens reparameterization).

B. PhyloTree ConstructionTo construct the phylogenetic tree, genomic embeddings \(E\) are input into an MLP network to derive the parameters \(\) and \(\) of the latent space, representing topological embeddings \(z^{*}\). This latent space effectively captures the evolutionary relationships that guide the subsequent tree structure generation process. The latent variable is then generated using the reparameterization trick : \(z^{*}=+,\ (0,I)\). Then, distance matrix \(D\) is computed using \(z^{*}\): \(D(i,j)=_{i,j=1}^{N}z_{i}^{*} z_{j}^{*}\), where \(\) means an XOR operation reflecting the nucleotide mismatches. This distance matrix is fed into the Neighbor-Joining (NJ) algorithm  to generate the initial tree topology \((z^{*})\). The tree's topology is structured using a parent-child relationships list \([L[i],R[i],P[i]]\), where \(P[i]\) denotes the parent of node \(i\), and \(L[i]\) and \(R[i]\) denote the left and right child node. This representation highlights the tree's hierarchical nature, optimizing its structure based on the evolutionary patterns in the data.

C. PhyloTree Structure ModelingThe purpose of the tree structure modeling module is to jointly optimize both the phylogenetic tree's discrete topology and continuous branch lengths.

C.1. Topology Learning.The first component involves a TreeEncoder \(R(z|(z^{*}))\) and a TreeDecoder \(Q((z)|z)\) to learn the tree topology. Concretely, the encoder \(R\) receives an initial tree topology \((z^{*})\) and genomic embeddings \(E\) from DNABERT2 as inputs, conditions the latent state \(z\) to refine topological embeddings in a continuous space. We introduce variational inference to enhance the quality of the tree structure and adapt to data uncertainty and complexity by minimizing the Kullback-Leibler (KL) divergence. Additionally, the encoder \(R\) acts as a regularization mechanism, reducing overfitting and enhancing gradient stability. To refine the topology, the decoder \(Q\) samples from the probability distribution parameterized by the encoder's latent state. This process optimizes model parameters by minimizing the KL divergence between the true distribution \(P((z))=P(y|(z),B_{})\) and the variational distribution \(Q((z))=q((z),B_{})\). To facilitate backpropagation through stochastic nodes, the latent variable \(z\) is sampled using the reparameterization trick: \(z=+\), where \((0,I)\) introduces controlled stochasticity while maintaining differentiability, ensuring that \(z\) effectively captures refined topological embeddings.

C.2. Branch Length (Blens) Learning.The second component utilizes the inferred topology \(\) and genomic embeddings \(E\) from DNABERT2 to generate and adjust branch lengths.

Step1: Node Feature via Linear-Time Dual-Pass Traversal.We utilize a linear time \((n)\) dual-pass traversal method, combining postorder (bottom-up) and preorder (top-down) strategies, guaranteeing each node is processed only once.

Postorder Traversal (bottom-up) aggregates information from leaf nodes toward the root node.

\[c_{i}=(1,K-_{j(i)}c_{j})^{-1},\] (1)

where \(c_{i}\) is the scaling factor for node \(i\), influenced by the node's connectivity, and \(K=3\) due to binary tree properties.

\[f_{i}=c_{i} f_{j}+e_{i},\] (2)

where \(f_{i}\) incorporates contributions from children nodes \(f_{j}\) and its own initial feature \(e_{i}\) from a pre-trained genome model. Initially, \(c_{i}=0,f_{i}=e_{i}\).

Preorder Traversal (top-down) propagates information from the root node toward the leaf nodes, enhancing each node's feature \(x_{i}\): \(x_{i}=c_{i} e_{p[i]}+f_{i}\),

where \(e_{p[i]}\) is the feature of node \(i\)'s parent.

Step2: Feature Enhancement with Dynamic Graph Convolution (DGCNN): We use the edge convolutional layer of DGCNN  to enhance node features. This approach captures both local interactions (between neighboring nodes) and global structural dependencies (reflecting the entire tree structure) within the phylogenetic tree. For layer \(L\), inputs \(\{x_{i}^{L}^{F}\}_{i=1}^{N}\) and outputs \(\{x_{i}^{L+1}^{F^{}}\}_{i=1}^{N}\) have feature dimensions \(F=768\) and \(F^{}=100\). The transformations are:

\[x_{i}^{L+1}=(x_{i}^{L}), m_{ij}=h(x_{i}^{L},x_{j}^{L}),\]

where \(h:^{F}^{F}^{F^{}}\) is an MLP-shared asymmetric edge function.

Topology-Invariance Property.Despite the non-uniqueness of 2D coordinates due to infinite possible translational rotations, the distances between species remain constant. We define this as topological invariance: \(d_{ij}^{2}=\|z_{i}-z_{j}\|_{2}\), where \(z_{i}\) and \(z_{j}\) are coordinates in the hidden space, maintaining accurate topological relationships. The function \(h(x_{i},x_{j})\) incorporates global and local features through: \(h(x_{i},x_{j})=h(x_{i},x_{i}-x_{j},d_{ij}^{2})\).

Aggregation of Features.The node features are aggregated using a MAX operation across all edges:

\[x_{i}^{L+1}=_{k=1}^{K}m_{ij}^{L}=m_{i1}^{L} m_{iK}^{L},\] (4)

where \(\) denotes the MAX aggregation function, focusing on the most significant features.

**Step3: Reparametrization of Branch Length.** Node features \(x_{i}^{L}\) obtained from edge convolution layers are further fed into the M network, parameterizing mean and log-variance for branch lengths: \(,(^{2})=_{2}(h_{ij}^{(1)})\). Concretely, the \(\) network is defined as:

\[h_{i}^{(1)}=_{1}(x_{i}^{L}), h_{ij}^{(1)}=\{h_{j }^{(1)}:j(i)\} h_{i}^{(1)},\] (5)

where \(h_{i}\) are node features processed through an additional \(_{1}\) to capture inter-node interactions. Branch lengths are updated by reparametrization \(b=(+()*)\), where \((0,I)\), ensuring differentiability and capturing the probabilistic nature of estimates. Throughout, the prior \(P(B_{})\) is assumed exponential, while the posterior \(Q(B_{})\), learned via a graph neural network, reflects inferred tree topologies and node characteristics.

### Scoring Function

To address the convergence challenges often associated with the ELBO in VAE models, we incorporate a scoring function \(S\), implemented via an MLP network. This function assesses each leaf node in the latent space \(z\) and provides additional gradient information, facilitating more efficient learning and convergence.

During training, \(S\) and ELBO form a joint optimization objective, optimizing gradient directions to improve overall performance. Fig. 3 compares the convergence behaviors and stability of \(S\) and ELBO throughout the training process. The horizontal axis represents the training steps, and the vertical axis represents the two metric values. The **closer** the \(S\) curve is to the ELBO curve, the more it proves that \(S\) can effectively evaluate the model performance and maintain a consistent optimization trend with ELBO. Different configurations of \(S\), including those with Fully Connected layers (w/ FC), with two layers MLP (w/ MLP-2), and with three layers MLP (w/ MLP-3), demonstrate similar trends, closely following the ELBO curve. After an initial period of rapid change, all metrics stabilize and exhibit minor fluctuations, demonstrating robustness in convergence. What's more, the number of layers in MLP has less impact on performance.

### Learning Objectives

As discussed in Appendix Background A, our primary goal is to maximize the expected marginal likelihood of the observed species sequence \(Y\) via \( p(Y|((z),B_{}))\). The posterior distribution as \(p((z),B_{}|Y)\) is difficult to infer directly, so we utilize variational inference to approximate it as \(q((z),B_{}|Y)\). The detailed deviation is in Appendix D.2.

To minimize the KL divergence between the true prior and the approximate posterior distributions, we start from the joint probability distribution: \(p(Y,(z),B_{})=p(Y|(z),B_{})p(B_{}|(z))p((z))\), where \(p(Y|(z),B_{})\) represents the conditional probability of the observed data \(Y\). We assume the tree topology \((z)\) and the branch lengths \(B_{}\) are conditionally independent.

We introduce a variational distribution \(q((z),B_{})=q(B_{}|(z))q((z))\) to approximate the true posterior \(p((z),B_{}|Y)\). The ELBO loss can initially be written as:

Figure 3: Comparison of ELBO and Scoring Function over Training Steps on DS1. **Closer curves** mean **better**.

\[(Q)=_{q}[ p(Y,(z),B_{})]-_{q}[ q( (z),B_{})].\] (6)

To improve training variance and gradient stability, we introduce a regularization term \(R(z|(z^{*}))\) and reformulate the ELBO loss as:

\[(Q,R)=_{Q(z)}[_{Q(B_{}|(z))}[|(z))p((z))R(z|(z))}{Q(B_{}|(z))Q(z^{*})}]].\] (7)

For better performance and reduced variance, we adopt a multi-sample approach:

\[_{}(Q,R)=_{k=1}^{K}^{k}(z^{k}))p((z^{k}))R(z^{k}(z^{*k}))}{Q(B_{ }^{k}(z^{k}))Q(z^{*k})},\] (8)

where \(z^{*k}\) and \(B_{}^{k}\) are samples from the variational distributions \(Q(z^{*})\) and \(Q(B_{}|(z))\), respectively. \(K\) represents the number of Monte Carlo samples from the variational distribution to compute the empirical average, providing a more robust approximation of the ELBO. Typically, \(K\) is chosen to be moderate to balance between computational efficiency and estimation accuracy . And the multi-sample ELBO reduces to the standard ELBO formula when \(K\)=1.

The total loss \(_{total}\) is then defined as:

\[_{total}=-_{}(Q,R)+(S)+ _{KL},\] (9)

where \(_{KL}=-\{q_{}((z)|y)||p_{}((z))\}- \{q_{}(B_{}|y)||p_{}(B_{})\}\), minimizing the KL divergence, and \((S)=-_{i=1}^{N}f(z_{i})\) accounts for the scoring function loss of the embeddings.

### End-to-End Learning via Stochastic Gradient Descent

We derive the gradients of model parameters \(\) as follows:

\[_{}=_{Q_{}(z^{*})}[  Q_{}(z^{*})_{Q(B_{}|(z))}[ |(z))}{Q(B_{}|(z))}]+\] (10) \[ P((z))R(z|(z^{*}))]+ H[Q_{}(z^{*})],\]

where \(H[Q_{}(z^{*})]\) is the entropy of \(Q_{}(z^{*})\). The derivation of the model parameters \(\):

\[_{}=_{}_{Q_{}(z^{*})}[_{Q_{}(B_{}|(z))}(z))}{Q(B_{} (z)))}].\] (11)

The derivation of the model parameters \(\):

\[_{}=_{Q_{}(z^{*})}[_{}(z|(z^{*}))]}.\] (12)

## 4 Experiments

In this section, we conduct experiments to demonstrate the effectiveness of our proposed PhyloGen. We aim to answer seven research questions as follows:

**RQ1:**: How effective is PhyloGen in generating tree structures under the benchmark datasets?
**RQ2:**: How diverse are the tree topologies generated by PhyloGen?
**RQ3:**: How consistent is the PhyloGen-generated tree structure compared to the MrBayes method?
**RQ4:**: How robust is PhyloGen to species sequences?
**RQ5:**: How does each PhyloGen's module affect its performance?
**RQ6:**: What evolutionary relationships between species does PhyloGen learn?
**RQ7:**: How do key hyper-parameters affect PhyloGen's performance?

### Experiment Setup

**Tasks and Datasets.** We evaluate the performance of PhyloGen on the Variational Bayesian Phylogenetic Inference task with Evidence Lower Bound (ELBO) and Marginal Log Likelihood (MLL) as metrics on eight benchmark datasets (see Appendix C).

[MISSING_PAGE_EMPTY:7]

and ELBO values on all datasets. The left plot of Fig. 4 illustrates our model's high stability and rapid convergence in ELBO metrics on DS1, which significantly outperforms the competition. ARTree performance improves in the later stages but exhibits large fluctuation. GeoPhy performs the worst, with consistently the lowest and more fluctuating ELBO values. The right plot of Fig. 4 demonstrates our model's advantages in the MLL metric, where it rapidly achieves and maintains high-performance levels. In contrast, ARTree and GeoPhy have lower MLL values, especially GeoPhy, which has the weakest performance throughout.

### Tree Topological Diversity Analysis (RQ2)

To evaluate the topological diversity of trees generated by PhyloGen on DS1, we use three metrics: Simpson's Diversity Index , Top Frequency, and Top 95% Frequency, as detailed in Tab. 3. A higher Diversity Index, which approaches 1, suggests broad diversity among generated tree topologies. The lower Top Frequency suggests a balanced distribution, preventing single tree structures from being overly dominant. Furthermore, the presence of 149 distinct topologies within the Top 95% Frequency underscores PhyloGen's ability to generate a diverse range of topologies.

### Bipartition Frequency Distribution (RQ3)

Fig. 5 shows the bipartition frequency distributions of trees inferred by PhyloGen for datasets DS1, DS2, and DS3. The horizontal axis indicates the ranking of the bipartitions in the tree topology, and the vertical axis indicates the normalized frequency of occurrence of the corresponding bipartitions. The **similarity** of our method's **curves** to those of MrBayes underscores its accuracy, demonstrating that PhyloGen consistently captures evolutionary patterns with reliability comparable to the gold standard. This indicates a robust validation of PhyloGen's phylogenetic inference capabilities. More detailed information is provided in Appendix E.3.

    & **Metric** & **PhyloGFN** & **GeoPhy** & **GeoPhy LOO(3)+** & **Ours w/o KL** & **Ours w/o S** & **Ours** \\    } & **ELBO** (\(\)) & NA & -7721.82 (-100) & -7729.28 (-107) & -6725.49 (+12) & -6713.01(+15) & **-6711.47(+14)** \\  & **MLL** (\(\)) & -6705.55 (-93) & -7440.38 (-198) & -7599.85 (-357) & -6564.51 (+18) & -5647.32 (+20) & **-6542.75 (+22)** \\  & **Time** & 1828min & 611min & 1822min & 6143min & 76142min & **632min** \\    } & **ELBO** (\(\)) & NA & -11802.07 (-98) & -11676.29 (-76) & -10678.24(+3) & -10655.02 (+2) & **-10674.28(+4)** \\  & **MLL** (\(\)) & -12565.76 (-233) & -11763.40 (-131) & -11630.16 (-98) & -10422.12 (+6) & -10654.53 (+12) & **-10432.71(+5)** \\   & **Time** & 24h35min & 1813min & 12h24min & 7h54min & 86min & **6327min** \\   

Table 4: Model Robustness Assessment. Performance is evaluated by ELBO (\(\)) and MLL (\(\)). (\(\)) represents the absolute difference change after node additions and deletions, with positive values indicating improved performance and negative values indicating a decline. Time records the total computation duration.

Figure 5: Comparative Bipartition Frequency Distribution in Tree Topologies for DS1, DS2, and DS3 datasets. **The closer the two curves are, the better,** which suggests that our method is highly consistent with the gold standard MrBayes approach.

### Robustness Assessment (RQ4)

To assess our model's robustness, we test its adaptability to data changes by modifying the number of nodes in the DS1 dataset, which initially contained 27 species sequences. Specifically, we conduct two experiments: Setting 1: randomly deleting 4 nodes to simulate the impact of data incompleteness and potential information loss, and Setting 2: randomly adding 4 nodes to simulate an increase in data size. As shown in Tab. 4, our model and its variants exhibit significant stability and adaptability under both cases. Changes in ELBO and MLL metrics are represented by \(\) values, where positive changes indicate improved performance and negative changes indicate decreased performance. Specifically, smaller positive increases after node deletion (Setting 1) and node addition (Setting 2) emphasize the model's ability to adapt to changes in data structure effectively. In contrast, larger negative decreases highlight challenges when adjusting to increased data complexity. Furthermore, our model exhibits considerable computational efficiency, outperforming baselines in runtime, a critical advantage for handling the complexities and variabilities of bioinformatics datasets.

### Ablation Study (RQ5)

Tab. 5 shows the performance of our model compared with the removal of the KL loss and the Scoring Function S, respectively. Ours performs best on ELBO and MLL metrics, with a significant decrease in performance after the removal of the KL loss, suggesting that the KL loss plays a key role in regularizing the model and avoiding overfitting. While removing the S module had a large impact on MLL, the ELBO impact was relatively small, indicating that the impact of the S module is more complex and may be related to specific feature extraction functions. Our future work will explore further enhancements to the S module and investigate other regularization techniques to refine the model's performance.

Fig. 6 shows that our method (PGen) achieves the highest MLL value, indicating optimal fit and stability throughout the training process. Adjustments to the model structure, particularly without layer normalisation (PGen w/o LN) and reducing the hidden dimensions (PGen-Hid=64), result in a lower MLL, but convergence remains stable. These results highlight the model's sensitivity to hyperparameters and affirm its robustness under different configurations. Replacing our designed distance matrix D with both Euclidean (PGen-Euclidean) and cosine (PGen-Cosine) distance matrices would greatly affect the effectiveness of MLL. These methods do not capture complex evolutionary relationships as effectively as distance matrices derived from the potential space \(z\). The model with one-hot encoding as the leaf node representation (PGen-One-Hot) has the lowest MLL, which indicates the importance of our feature extraction module.

### Case Study of PhyloTree Structure (RQ6)

Fig. 7 shows a phylogenetic tree constructed from DS1 dataset, where each leaf node represents a specific species, and the text next to the node indicates the species name. The branch lengths reflect the genetic distances, with shorter branches indicating recent evolutionary history and longer branches indicating greater genetic differences. The phylogenetic tree shown in Fig. 7 places Siren intermedia and Trachemys scripta, both aquatic organisms, on adjacent branches, reflecting our model could capture their adaptive evolutionary information to aquatic environments. Meanwhile, the reptilian species Heterodon platryhinos and Trachemys scripta are also on neighbouring branches, suggesting a relatively recent com

  
**Methods** & **ELBO (\(\))** & **MLL (\(\))** \\  Ours & **-7005.98** & **-6910.02** \\ Ours w/o KL & -7017.57 & -6917.34 \\ Ours w/o S & -7011.94 & -6919.39 \\   

Table 5: Ablation Study of PhyloGen.

Figure 6: Ablation Study on DS1 Dataset.

Figure 7: Plot of PhyloTrees.

mon ancestor compared to other amphibian and reptilian species. Notably, our method does not require padding the sequences to a unified length, which effectively reflects actual sequence variation. For a detailed species sequence and topology comparison, please refer to Appendix E.6.

## 5 Conclusion

ContributionsIn this study, we introduced PhyloGen, a novel approach leveraging pre-trained genomic language models to enhance phylogenetic tree inference through graph structure generation. By addressing the limitations of traditional MCMC and existing VI methods, PhyloGen jointly optimizes tree topology and branch lengths without relying on evolutionary models or equal-length sequence constraints. PhyloGen views phylogenetic tree inference as a conditionally constrained **tree structure generation** problem, jointly optimizing tree topology and branch lengths through three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and (iii) PhyloTree Structure Modeling. These modules map species sequences into a continuous geometric space, refine the tree topology and branch lengths, and maintain topological invariance. Our method demonstrated superior performance and robustness across multiple real-world datasets, providing deeper insights into phylogenetic relationships.

Limitations and Future WorksWhile our model demonstrates outstanding performance on standard benchmarks, it may benefit from using more expressive distributions or incorporating prior constraints to better capture complex dependencies and interactions in the latent space. Additionally, although the Neighbor-Joining algorithm is effective for iterative tree construction, it is computationally intensive. We are exploring efficient data structures and parallel processing techniques to address this bottleneck. Furthermore, our model has primarily been applied to genomic data, and further research is needed to extend its applicability to diverse biological data, such as protein and single-cell data.