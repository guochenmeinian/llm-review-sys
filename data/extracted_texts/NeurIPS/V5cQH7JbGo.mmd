# Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training

Tiansheng Huang, Sihao Hu, Ka-Ho Chow, Fatih Ilhan, Selim Furkan Tekin, Ling Liu

School of Computer Science

Georgia Institute of Technology, Atlanta, USA

{thuang374, shu335, kchow35, filhan3, stekin6}@gatech.edu, ling.liu@cc.gatech.edu

###### Abstract

Federated learning (FL) is vulnerable to backdoor attacks due to its distributed computing nature. Existing defense solution usually requires larger amount of computation in either the training or testing phase, which limits their practicality in the resource-constrain scenarios. A more practical defense, i.e., neural network (NN) pruning based defense has been proposed in centralized backdoor setting. However, our empirical study shows that traditional pruning-based solution suffers _poison-coupling_ effect in FL, which significantly degrades the defense performance. This paper presents Lockdown, an isolated subspace training method to mitigate the poison-coupling effect. Lockdown follows three key procedures. First, it modifies the training protocol by isolating the training subspaces for different clients. Second, it utilizes randomness in initializing isolated subspaces, and performs subspace pruning and subspace recovery to segregate the subspaces between malicious and benign clients. Third, it introduces quorum consensus to cure the global model by purging malicious/dummy parameters. Empirical results show that Lockdown achieves _superior_ and _consistent_ defense performance compared to existing representative approaches against backdoor attacks. Another value-added property of Lockdown is the communication-efficiency and model complexity reduction, which are both critical for resource-constrain FL scenario. Our code is available at https://github.com/git-disl/Lockdown.

## 1 Introduction

Federated Learning (FL)  is a privacy-preserving machine learning paradigm that allows the training surrogates (clients) to collectively train a global model with data in local devices. However, because the training data and potentially the training process on the clients lacks supervision, it is possible that attackers can launch data poisoning attack on the global model , so as to manipulate the prediction of the model.

Backdoor attack is one of the data poisoning attacks, which is stealthy and disruptive to the normal behavior of the model. Specifically, the prediction of the model can be manipulated such that it consistently predicts one (or some) chosen label whenever it is given samples with a backdoor trigger. Backdoor attack poses serious threats to

Figure 1: Illustration of isolated subspace training. The blue circles and the shaded area respectively represent the parameters of the model and the training subspace. Left: vanilla FL where malicious clients can poison all the parameters. Right: isolated subspace training where malicious clients can only poison a subspace of them.

many security-critical applications, e.g., biometric authentication and autonomous driving (Chow and Liu, 2021), which vitiates the large-scale deployment of FL. Therefore, an effective defense that can mitigate such a risk is in an urgent need.

The main stream of existing research to defend backdoor attack in FL can be mainly classified into three genres, i) certified robustness (Xie et al., 2021), (Alfarra et al., 2022), ii) adversarial training (Zizzo et al., 2020; Shah et al., 2021), and iii) robust aggregation (Chow et al., 2023; Zhang et al., 2022). Though existing defenses can mitigate attack, they are still far from their maturity. In particular, we highlight the _in-efficiency_ of state-of-the-art defense solutions.

_Existing defenses usually require extra computation in either the training or inference phases._ For example, randomized smoothing, the key component for certified robustness defense, requires multiple forward pass of data in the inference phase for producing one effective prediction. Analogously, adversarial training requires to generate adversarial samples in the inner loop of training, and therefore demands extra data pass. This issue is particularly pronounced in the FL scenario, considering that training and the inference are conducted on edge devices with limited capacity (Wang et al., 2019).

In this paper, we try to answer the following question:

_Is there a solution that not only provides SOTA defense performance, but also requires equal or even less computation in both training and deployment phases?_

To this end, we first apply the pruning-based defense solution in FL backdoor setting, but we observe a poison-coupling effect, which significantly degrades the defense performance. Driven by this observation, we propose Lockdown, a solution that utilizes isolated subspace training to de-couple the poison parameters, which are then pruned using the consensus from clients. Our experiments demonstrate that: i) Lockdown reduces the Attack Success Ratio (ASR) by up-to 90% compared to FedAvg without defense. ii) Lockdown consistently performs better than SOTA defense solutions in various attack settings. Specifically, Lockdown acquires 83% of ASR reduction with only 2.7% drop of benign accuracy when the data distribution is Non-IID (the other two defense baselines RLR and Krum respectively acquire 14.2% and 86.1% ASR reduction while sacrificing 16.2% and 43.6% benign accuracy in the same setting). iii) Lockdown reduces both downlink and uplink communication by at least 0.75x, and the number of parameters used in the model training and inference phases are also accordingly reduced by at least 0.75x thanks to the removal of malicious/dummy parameters.

To the end, we summarize our contribution as follows:

* We study the data-free pruning defense in FL setting, and find that the poison parameters tend to be statistically coupled with benign parameters, which we refer to as "poison-coupling" effect in FL backdoor. To the best of our knowledge, this finding is not available in the literature.
* We propose Lockdown, a backdoor defense that utilizes the idea of isolated subspace training to decouple the poison parameters. Notably, Lockdown i) enjoys communication reduction between server and clients, and ii) lowers the model's training/inference complexity.
* We conduct evaluations to show the efficacy of Lockdown. Results show that Lockdown _consistently_ outperforms existing defense baselines under different attack settings (attack method, attacker number and poison ratio) and different data distributions (IID and Non-IID).
* Ablation study and hyper-parameter sensitivity analysis are conducted to verify the individual functionality of each component of Lockdown.

## 2 Related Work

**Federated Learning**. Federated learning (McMahan et al., 2016) is a privacy-preserving distributed training paradigm that allows clients to collectively train a global model from distributed training data. Recent works on FL mostly lie in optimization (Karimireddy et al., 2020; Li et al., 2018; Sun et al., 2023, 2023; Li et al., 2023, 2023), which study how to mitigate the Non-IID issue (Zhao et al., 2018) and system efficiency (Huang et al., 2020, 2022; Li et al., 2021, 2021).

**Federated backdoor attack and defense**. Classical backdoor attack in FL is empirically proven to be effective in (Bagdasaryan et al., 2020), and subsequently, new types of attack are developed, e.g., edge-case backdoor (Wang et al., 2020), stealthy model poisoning (Bhagoji et al., 2019), subnet replacement attack (Qi et al., 2022), and Distributed Backdoor Attack (Xie et al., 2019).

Backdoor defense are developed to counter the potential threat posed by the backdoor attacks (representative methods are neural cleanse (Wang et al., 2019), meta analysis (Xu et al., 2021), etc). Under the FL setting, backdoor defenses can be classified into three main genres. The first genre is certified robustness. Certified robustness relies on randomized smoothing(Cohen et al., 2019), an approach with theoretical certification of the model robustness. Subsequent techniques, e.g., weight smoothing (Xie et al., 2021) and group ensemble (Cao et al., 2022) are studied for the defense of federated backdoor. The second genre is adversarial training (Zizzo et al., 2020; Shah et al., 2021), which generates adversarial samples to improve the model's robustness. However, both certified robustness and adversarial training require more computation in either the training or deployment stage. The third genre is robust aggregation, which studies how to identify and preclude the malicious update in the aggregation stage (Chow et al., 2023; Zhang et al., 2022; Guo et al., 2021; Panda et al., 2022). However, the classification benign accuracy of this category of defense often perform worse when Non-IID extent increases.

**Sparse training**. Sparse training is originally designed to reduce the model complexity for both the training and deployment stage. SET (Mocanu et al., 2018) first proposes the idea of dynamic subspace searching with alternative pruning and recovery process, and are empirically studied with the concept of in-time over-parameterization (Liu et al., 2021, 2021; Liu et al., 2021). (Evci et al., 2020) further introduces gradient information to guide the mask searching process. This model compression technique has recently been extended to FL (Bibikar et al., 2022; Huang et al., 2022; Dai et al., 2022).

We utilize sparse training technique to develop a new genre of backdoor defense solution. To the best of our knowledge, this is the first attempt that connects sparse training with backdoor defense in FL.

## 3 Threat Models

We consider three threat models, named _weak_, _medium_ and _strong_. All the models allow multiple attackers to co-exist in the system. We use \(N\) to denote the number of attackers out of \(M\) total clients.

**Permission.** Different threats models are given different control permissions. Specifically, _Weak_ model allows the attackers to arbitrarily manipulate its local data, but cannot control its local training process, while attackers in _medium_ and _strong_ model are allowed to do so. Only _strong_ model allows the attackers to obtain other benign client's information, e.g., update from them. For control permission in the weak threat model, technique like trusted execution environment (Mo et al., 2021) can be applied to control each client to run the designated program, and thereby disabling algorithm manipulation. Existing literature usually adopt medium threat model, in which attackers can do whatever they like in their local devices, but has not extra information from server or from other benign clients. The permission of the threat models are summarized in Table 1.

**Malicious objective.** Denote the set of benign clients as \(\) and the set of malicious clients as \(\). Formally, we characterize the objective of malicious clients as follows:

\[_{}(_{i}_{i}()+ _{i/}f_{i}())\] (1)

where \(_{i}()_{i}|}_{(_{j},y _{j})}_{i}}CE(;_{j},y_{j}))\) is the malicious objective of an attacker (to minimize the average cross-entropy of the backdoor dataset), \(f_{i}()_{i}|}_{(_{j},y_{j}) _{i}}CE(;_{j},y_{j}))\) is the benign objective (to minimize the average CE loss for the dataset from benign clients), and \(}_{i}\) and \(_{i}\) are respectively the backdoor dataset and pristine dataset.

**Attack methods.** Different attack methods are eligible in different threat models. For weak threat model, only data-level backdoor, e.g., BadNet (Gu et al., 2017), DBA (Xie et al., 2019), and Sinusoidal

   Permission\(\)Threat model & weak & medium & strong \\  Data manipulation & ✓ & ✓ & ✓ \\ Algorithm manipulation & ✗ & ✓ & ✓ \\ Global information Access & ✗ & ✗ & ✓ \\ Aggregation manipulation & ✗ & ✗ & ✗ \\   

Table 1: Permission of threat models.

(Barni et al., 2019) can be applied, since the attacker has no control to the local training, but only has access to the dataset. For medium threat model, the attacker can perform a wider range of attack methods, e.g., Scaling (Bagdasaryan et al., 2020), FixMask, etc, which requires the modification of the local training process. For the strong threat model, we test Neuronxin (Zhang et al., 2022), and Omniscient, which requires extra server-side information to aid the attack.

**Defense goal.** While solving the benign objective (see FL general objective in (McMahan et al., 2016)), we expect the global model \(\) to be able to resist backdoor attack. In other words, the global model should minimize the benign objective while maximizing the malicious objective.

## 4 Case Study on Pruning-based Defense

Motivated by our goal to propose a computation-friendly defense, we extend the pruning-based defense originally proposed for centralized ML to federated learning setting. We study CLP defense proposed in (Zheng et al., 2022) towards two poisoned models trained with centralized SGD (centralized backdoor) and the global model produced by FedAvg (federated backdoor) 1. Surprisingly, we observe that the pruning defense exhibits substantially different performance on the two models.

**Observations.** In the left of Figure 2, we observe that the pruning algorithm cannot efficiently eliminate the backdoor parameters for federated backdoor. The ASR can only drop to an acceptable range (say 30%) when leveraging large pruning ratio, and accordingly, this also leads to a large drop of benign accuracy (say 35% drop). In the middle, we plot the channel lipschitzness (CL) of the two models, and find that the CL values for a federated backdoor couple in a more compact range, which makes it hard for the pruning method to find out the malicious channels to prune. This explains why CLP fails to work for federated backdoor. In the right, we plot the L2 norm of weights of the last convolutional layer, the same coupling effect for federated backdoor can be observed - the L2 norm of different channels are all within a small range.

**Poison-coupling effect.** In summary, our main takeaway is that the federated backdoor model is hard to be cured by classical pruning method, because the backdoor parameters does not exhibit substantial statistical difference with the benign ones, i.e., it tends to be coupled with the benign parameters. We refer to this phenomenon for federated backdoor as _poison-coupling_ effect. Due to the existence of poison-coupling effect, it is hard, if not impossible, to accurately identify the poisoned parameters after the model has been fully trained by federated learning process, and thereby reducing the efficiency of the pure parameters pruning defense.

## 5 Methodology

To mitigate the _poison-coupling_ effect, we propose lockdown with isolated subspace training. Intuitively, we don't allow the malicious clients to get access to all the parameters, but _only a subspace of them_ (See Figure 1), in order to prevent them from statistically coupling the poisoned parameters with the benign ones that are supposed to do normal function.

Figure 2: Properties of two models trained with centralized backdoor and federated backdoor. Left: ASR and benign accuracy with CLP defense in (Zheng et al., 2022). Middle: Channel lipschitzness of last convolutional layer of two models. Right: L2 norm of last convolutional layer of two models. The experiment is done on a ResNet-9 model on CIFAR10 dataset and under BadNet attack.

The high-level idea of Lockdown is as follows. i) we employ _isolated subspace training_ to restrain the training of each client into its own subspace, such that the backdoor data cannot access and couple the poisoning function in all the parameters. ii) we employ _dynamic subspace searching_ for the clients to search for local subspaces using their local datasets, which involve only the parameters that they deem important. iii) After the above local procedures, the server will _aggregate_ the gradient updates into the corresponding subspaces of global model, and continues the next round of training. iv) Repeating \(T\) rounds of training, the server can identify the malicious parameters by _consensus fusion_. Under the intuition that the malicious/dummy parameters (at specific coordinates) should have less chance to be involved in benign clients' subspaces (because they are not important parameters for the benign datasets). Considering the benign clients constitute the majority in the system, consensus fusion identify those parameters that have the least appearance among all the clients subspace as the malicious or dummy parameters, which are then pruned to mitigate backdoor behavior. As follows, we detail each step of Lockdown.

**Subspace initialization.** We enforce an overall sparsity \(s\) to the initial subspace of each client (denoted by a binary variable \(_{i,0}\)). For maintaining practical performance of sparsity, we follow (Evci et al., 2020) to use ERK for random subspace initialization, in which different layers in the corresponding subspace are designated different sparsity (See Appendix A.1 for details). For each client, we enforce them to have the same initial subspace, which can be achieved via enforcing the same random seed to them for subspace generation. However, we do find that heterogeneous mask initialization can increase protection in some cases (See Section 6.2).

**Isolated subspace training.** In this stage, each client performs multiple local steps to train parameters within its isolated subspace (as enforced by its mask). Specifically, for local step \(k=0,,K-1\), clients do this update:

\[_{i,t,k+1}=_{i,t,k}-_{i,t} f_{i}(_{i,t,k};)\] (2)

where \(\) is a piece of random sample within the local dataset, \(\) is the learning rate, and \(\) denotes Hadamard product. As shown, the binary mask \(_{i,t}\) is applied to the gradient in every local step, which isolates the training into \(i\)-th client's own subspace. This process prevents the backdoor data from contaminating all the parameters within the parameter space.

**Subspace searching.** We conduct subspace searching for clients to search for their subspaces, i.e., the parameters that are important per the data they have. The searching process is decomposed to two phases, as follows:

1. **Subspace pruning.** In this phase, we prune the unimportant parameters within the client's current subspace that do not function at all. After \(K\) steps of local training, we can identify the unimportant parameters as those with the smallest magnitude within the subspace. To preclude them from future training, we prune those smallest \(_{t}\) parameters within each layer, and accordingly update

Figure 3: Overview of Lockdown. First, clients receive subspace model from server, and apply isolated subspace training. Second, clients upload their subspace update. Third, server aggregates the updates into global model. After training, consensus fusion is applied to remove poison params.

the mask to \(_{i,t+1}\). Formally, this process can be formulated as follows: \[_{i,t+1}=_{i,t+}-_{_{t}}(|_{ i,t,K}|)\] (3) where \(_{_{t}}(||)\) return the \(_{t}\) percentage of smallest coordinates of each layer absolutely weights and mask them to 1, indicating that they will be pruned. We use cosine annealing to decay \(_{t}\) from the initial pruning rate \(_{0}\). The decay process is postponed to the Appendix A.1.
2. **Subspace recovery.** After pruning, and before the start of next round local training, the client recovers the same amount of parameters (as indicated by \(_{t-1}\)) to its subspace for exploration. To identify which parameters should be recovered, we use each client's data to extract the gradient w.r.t the weights after pruned, i.e., to extract \( f_{i}(_{i,t,0})\). Then we recover \(_{t-1}\) percentage of parameters by identifying those with top-\(_{t-1}\) percentage of gradient magnitude within each layer, and accordingly update the mask to \(_{i,t+}\). The intuition is that for an important parameter over the local data, its gradient magnitude should be larger than the unimportant ones, which should be included into the new subspace. Formally, the recovery process is formalized as follows: \[_{i,t+}=_{i,t}+_{_{t-1}}(| f _{i}(_{i,t,0})|)\] (4) where \(_{_{t-1}}()\) returns the \(_{t-1}\) percentage of largest coordinates of absolutely gradient and mask them to 1, indicating that they will be recovered. But we note here that we don't perform subspace recovery in the first round, since pruning is not done yet.

**Aggregation.** Once subspace training is done, the clients upload the gradient updates of the local subspace to the server for aggregation. To aggregate the knowledge into the global model, we average the gradient updates based on their coordinate-wise contributions, as follows:

\[_{t+1}=_{t}-_{i=1}^{M}_{i,t+1}(_{ t}-_{i,t,K})\] (5)

where \(_{i,t,K}\) is the weight from the client's local subspace, and \(_{i,t+1}\) is the subspace after pruned.

**Consensus fusion (CF).** Given that those malicious parameters served to recognize backdoor triggers will be deemed unimportant for benign clients, they should not be contained in the subspace of benign clients, which accounts for the majority. This observation inspires us to eliminate the malicious parameters using consensus fusion after \(T\) rounds of global model training. Formally, the consensus fusion operator returns a vector that satisfies:

\[[_{}(_{1,T},,_{M,T}]_{j}=1& _{i=1}^{M}[_{i,T}]_{j}\\ 0&\] (6)

where \(\) is the threshold for CF, and \([]_{j}\) indexes the \(j\)-th coordinate of a vector. By applying the global mask produced by CF into the global model, i.e., \(_{T}_{}(_{1,T},,_{M,T})\), those parameters that have appearances smaller than \(\) among all the subspaces are sparsified to 0. In this way, those poisoned parameters will mostly be eliminated, thereby reaching the goal of perturbing backdoor.

## 6 Experiment

### Experiment Setup

**Datasets and models.** We experiment on FashionMnist, CIFAR10/CIFAR100 and TinyImagenet datasets. For CIFAR10/CIFAR100, we use a ResNet9(He et al., 2016) as backbone. For TinyImagenet, we use a modified ResNet9 via adding a pooling layer after the first convolutional layer to keep the same hidden size before output. For FashionMnist, we use LeNet (LeCun et al., 1998).

**Attack methods.** We classify the backdoor attack methods in FL into three genres, i.e., _data-level backdoor_, _algorithm-level backdoor_ and _advanced backdoor_. Data-level backdoor only allows the malicious clients to modify the raw data, but they have no control of the algorithm. On contrary, algorithm-level backdoor allows clients to modify the training algorithm, in addition to the raw data. The advanced attack can access extra server-side information (see Appendix A.2). Indeed, the three genres of backdoor attack methods correspond to the weak, medium, strong threat models in Table 1respectively. Among the data-level backdoor, we simulate three types of attacks methods, i.e., BadNet (Gu et al., 2017), DBA (Xie et al., 2019), and Sinusoidal (Barni et al., 2019). For algorithm-level backdoor, we simulate two types of attack, i.e., Scaling (Bagdasaryan et al., 2020), and FixMask. For advanced backdoor, we test Neurotoxin (Zhang et al., 2022b) and an adaptive attack Omniscience.

**Defense Baselines.** We use vanilla FedAvg (McMahan et al., 2016) (without defense) as a baseline, and compare Lockdown with four SOTA defenses RLR (Ozdayi et al., 2021), Krum (Blanchard et al., 2017), RFA (Pillutla et al., 2022) and Trimmed mean (Yin et al., 2018).

**Evaluation metrics**. Three metrics are used to evaluate the defense performance:

* **Benign Acc.** Benign accuracy measures the Top-1 accuracy performance of a model given the benign data inputs (without the presence of a trigger).
* **ASR.** Attack Success Ratio (ASR) measures the ratio of backdoor samples (with trigger) to be classified to the target label. The smaller this metric, the more resilient the model is.
* **Backdoor Acc.** Backdoor accuracy measures the Top-1 accuracy of the model given the backdoor inputs (their labels during testing are the original one before adding backdoor trigger). We add this metric to evaluate the overall performance, since high backdoor acc means: i) the classification of benign features is well-performing. ii) the perturbation of backdoor trigger is limited.

**Simulation setting.** We simulate \(M=40\) clients, and data is either evenly distributed to each client (IID setting) or is distributed with Dirichlet distribution (Non-IID setting) following (Hsu et al., 2019). The parameter for Dirichlet distribution is set to 0.5 for the Non-IID partition. To simulate the backdoor attack launched by the malicious clients, we follow (Ozdayi et al., 2021) to randomly choose \(N\) clients as attackers whose \(p\) (percentage) of data in their local datasets are poisoned. The default backdoor settings for our main experiment are \(p=50\%\) and \(N=4\). We summarize the default simulation setting in Table 2. All the experiments are done with a Nvidia A100 GPU.

**Hyper-parameters.** For Lockdown, we fix the overall sparsity to \(s=0.25\), the mask agreement threshold to \(=20\), and the initial pruning rate to \(_{0}=1e-4\). The robust learning rate threshold for RLR is set to 8. The number of local epochs and batch size are fixed to 2 and 64, respectively. The learning rate and weight decay used in the local optimizer are fixed to \(0.1\) and \(10^{-4}\). The number of comm rounds is fixed to 200. We summarize the default hyper-parameters in Table 3.

### Main Evaluation

In our main evaluation, we use CIFAR10 as the default dataset and BadNet as the default attack.

**Convergence.** The convergence result w.r.t communication rounds is plotted in Figure 4. Lockdown offers the strongest robustness under attack with Non-IID settings. Compared with IID, when the data distribution is Non-IID, Lockdown suffers a slight drop in benign accuracy (by approximately 3%) while Krum and RLR suffer more. The larger heterogeneity causes more benign parameters to be dropped in the consensus fusion process, resulting in a drop in benign accuracy.

   Notation & Meaning & Default Value \\  \(M\) & Total number of clients & 40 \\ \(p\) & Poison ratio & 0.5 \\ \(N\) & Attacker number & 4 \\    
   Notation & Meaning & Default Value \\  \(_{0}\) & Initial pruning rate & 1e-4 \\ \(\) & Agreement threshold & 20 \\ \(s\) & Overall sparsity & 0.25 \\   

Table 2: Default Simulation Setting.

Figure 4: Convergence and defense performance under different defenses.

Defense efficacy on varying poison ratio.As shown in Table 4, the Attack Success Ratio (ASR) of Lockdown under different data poison ratios are significant lower compared with vanilla FedAvg without defense (up to 96% reduction), and is also significantly lower than the SOTA backdoor defense solutions. Though ASR of Lockdown is slightly larger than RLR and Krum in IID settings, its benign accuracy is much higher than them (with up-to 5.4% and 15% enhancement comparing RLR and Krum respectively). Another observation is that Lockdown performs better in reducing ASR when the data poison ratio is high (in Non-IID setting, ASR is 7.6% when \(p=0.05\) while ASR is only 3.3 % when \(p=0.8\)). This phenomenon is because the subspaces found by the malicious clients will deviate more from benign clients when a larger amount of backdoor samples are injected in their datasets. Therefore, the malicious subspace will overlap less with benign subspaces, resulting in a better isolation, and also benefit the consensus fusion process. In addition, Lockdown significantly advances backdoor accuracy by up-to 79.6% compared to FedAvg without defense, which implies that the Lockdown's model can still recognize the true label of backdoor samples even under attack.

Defense efficacy on varying attacker ratio.We fix the poison ratio to 0.5 and vary the attackers ratio to \(\{0,0.1,0.2,0.3,0.4\}\). The results are shown in Figure 5. As shown, Lockdown _consistently_ achieves low ASR (at minimum 30% ASR reduction compared to FedAvg when attackers ratio is \(0.4\)), and high benign accuracy in all groups of experiments (at maximum 5% drop of benign accuracy compared to FedAvg). In contrast, RLR and Krum are _sensitive_ to attacker ratio and _fail_ in defense when attacker ratio is \(0.4\) (no ASR reduction for Krum, and at maximum 10% reduction for RLR).

Defense against Non-IID extent.Next we show in Table 5 that Lockdown is robust to different Non-IID extent. Our experiment is done in the default setting, but vary different setting of Dirichlet Parameters \(\), and it shows that Lockdown can defend against backdoor even in extreme Non-IID cases (e.g., \(=0.2\), which leads to benign accruacy drop of FedAvg from 90.8% to 84.3%). Our another observation is that Lockdown would suffers more benign accuracy loss (respectively 3.7%, 2.7%, 1.2% and 0.7% benign accuracy loss for the four Non-IID levels).

Defense against data-level backdoor.We simulate attacks with the BadNet, DBA, and sinusoidal method. Our results in Table 6 show that _Lockdown has good generalization ability to data-level

   Methods &  &  &  \\ (IID) & clean & p=0.5 & p=2. & p=5 & p=8 & clean & p=0.5 & p=2. & p=5 & p=8 & clean & p=0.5 & p=2 & p=5 & p=8 \\  FedAvg & **91.0** & **91.4** & **91.1** & **91.0** & **90.8** & **1.6** & 12.4 & 19.9 & 66.1 & 94.8 & **88.5** & 79.6 & 73.4 & 32.9 & 5.1 \\ RLR & 86.8 & 86.7 & 86.6 & 86.3 & 85.5 & 2.3 & **2.4** & **2.4** & **4.3** & 25.1 & 84.6 & 84.3 & 83.4 & 81.7 & 65.2 \\ Krum & 76.3 & 78.0 & 75.6 & 76.4 & 75.8 & 4.7 & 3.9 & 4.3 & 4.3 & 4.9 & 73.8 & 74.9 & 72.9 & 73.9 & 73.2 \\ RFA & 90.9 & 91.2 & 91.1 & 90.8 & 90.7 & 1.6 & 15.8 & 20.7 & 83.7 & 99.3 & 88.8 & 76.8 & 72.4 & 1.9 & 0.7 \\ Trimmed mean & 91.0 & 90.6 & 91.1 & 90.9 & 90.8 & 1.7 & 5.0 & 20.7 & 61.7 & 96.2 & 88.5 & 84.7 & 72.0 & 36.6 & 3.6 \\ Lockdown & 90.0 & 90.0 & 89.9 & 90.1 & 90.0 & 1.8 & 3.6 & 2.5 & 7.1 & **4.0** & 87.9 & **85.8** & **86.6** & **83.7** & **85.6** \\   Methods &  &  &  \\ (Non-IID) & clean & p=0.5 & p=2. & p=5. & p=8 & clean & p=0.5 & p=2. & p=5 & p=8 & clean & p=0.5 & p=2 & p=5 & p=8 \\  FedAvg & **89.0** & **89.2** & **89.3** & **88.8** & **88.7** & 1.7 & 17.3 & 54.4 & 86.4 & 96.7 & **85.9** & 74.0 & 42.5 & 13.0 & 3.2 \\ RLR & 74.4 & 74.4 & 73.6 & 72.9 & 72.5 & 5.8 & 15.0 & 40.2 & 29.5 & 82.5 & 69.0 & 63.1 & 46.2 & 51.4 & 15.3 \\ Krum & 42.7 & 37.4 & 45.2 & 43.4 & 45.1 & 10.0 & **5.2** & 10.4 & 11.1 & 10.6 & 38.6 & 33.8 & 40.7 & 39.3 & 40.5 \\ RFA & 88.8 & 88.8 & 88.8 & 88.3 & 88.3 & 2.0 & 21.4 & 52.8 & 90.8 & 98.7 & 85.7 & 70.6 & 44.3 & 8.9 & 1.2 \\ Trimmed mean & 88.5 & 88.4 & 88.2 & 88.3 & 88.3 & 1.9 & 25.2 & 48.4 & 84.6 & 96.0 & 85.4 & 67.5 & 47.7 & 14.7 & 3.9 \\ Lockdown & 85.6 & 86.2 & 86.7 & 86.1 & 86.6 & **0.9** & 7.6 & **3.6** & **3.4** & **3.3** & **3.3** & 84.1 & **79.5** & **82.3** & **82.2** & **82.8** \\   

Table 4: Defense efficacy with varying poison ratio \(p\) under CIFAR10.

Figure 5: Benign acc/ASR under different attackers ratio.

    &  &  &  &  \\  Methods & Benign Acc \(\) & ASR \(\) & Benign Acc \(\) & ASR \(\) & Benign Acc \(\) & ASR \(\) & Benign Acc \(\) & ASR \(\) \\  FedAvg & **84.3** & 74.8 & **88.8** & 86.4 & **89.2** & 74.7 & **90.8** & 66.1 \\ RLR & 64.5 & 82.5 & 72.9 & 29.5 & 82.9 & 24.5 & 85.5 & **4.3** \\ Krum & 26.5 & **2.8** & 43.4 & 11.1 & 43.6 & 6.6 & 75.8 & **4.3** \\  Lockdown & 80.6 & 7.7 & **86.1** & **34.4** & **88.5** & **39.9** & 90.1 & 7.1 \\  

Table 5: Defense efficiency with different Non-IID extent.

backdoor attack._ Overall. Lockdown maintains superior defense efficacy (\(<\) 10% ASR) towards all the three data-level backdoor attacks.

**Defense against algorithm-level/advanced backdoor.** Table 7 shows Lockdown's performance on Scaling and Neurotoxin attack. For Scaling attack, the scaling factor is set to 5. We integrate Lockdown with the norm-clipping (NC) in aggregation as proposed in (Sun et al., 2019), and as shown, Lockdown defense is still effective by controlling ASR to <5%. Lockdown is also robust to Neurotoxin, an attack developed from a similar idea as Lockdown (See Appendix A.2 for analysis).

**Defense against adaptive attack.** We design two adaptive attacks that assume the knowledge of Lockdown and try to break it. Both the attacks try to disobey the mask searching process. Specifically, FixMask allows attackers to fix the initial subspace, and Omniscience is able to infer the global subspace that is produced by CF. As shown Table 8, vanilla Lockdown procedure is vulnerable to them. However, we find that there are two methods can be used to rectify Lockdown to accommodate FixMask attack. i) Enlarge the initial pruning/recovery ratio \(_{0}\), or ii) adopt heterogeneous mask initialization (HM). Both the approaches enhance the dynamics of subspaces, and therefore enhance protection. For Omniscience attack, the attackers always know consensus subspace and therefore can poison the parameters within it. Lockdown cannot cope with this attack. However, the condition of conducting Omniscience attack is stringent, as it has to acquire either global subspace, or other benign clients' local subspace, meaning that it falls into the _Strong_ threat model in Table 1.

**Communication and model complexity.** As shown in Table 9, we show that Lockdown achieves _smaller communication overhead_ (0.25x compared to FedAvg), since only a small subspace of the entire model gradient needs to be sent between server and clients. In addition, the _number of parameters_ of the inference model is also _lowered_ to 0.25x because the consensus fusion operation prune out the malicious/dummy parameters. Finally, we observe that _benign accuracy only drops by 0.01x_ compared to FedAvg, indicating that pruning will not severely perturb the normal function.

**Generalization to varying datasets.** We show our evaluation results on FashionMnist, CIFAR10/100 and TinyImagenet in Table 10. As shown, Lockdown achieves SOTA defense efficacy (compared with FedAvg without defense, with up-to 80.7%, 83.0%, 73.8%, and 93.4% reduction of ASR respectively on the four datasets), and maintains a reasonable loss of benign accuracy compared to FedAvg without defense (with up-to 2.2%, 2.7%, 3.9% and 3.3% drop).

   Methods (ID) & Benign Acc(\%) \(\) & ASR(\%) \(\) & Backdoor Acc(\%) \(\) \\  BadNet & 90.1 & 7.1 & 83.7 \\ DBA & 90.0 & 2.5 & 86.4 \\ Sinusoidal & 89.5 & 4.6 & 83.6 \\   Methods (Non-ID) & Benign Acc(\%) \(\) & ASR (\%) \(\) & Backdoor Acc(\%) \(\) \\  BadNet & 86.1 & 3.4 & 82.2 \\ DBA & 86.5 & 2.0 & 82.9 \\ Sinusoidal & 87.0 & 2.0 & 81.8 \\    
   Methods (ID) & Benign Acc(\%) \(\) & ASR(\%) \(\) & Backdoor Acc(\%) \(\) \\  Scaling (Lockdown \(\) NC) & 89.9 & 3.2 & 86.2 \\  Random & 89.0 & 2.6 & 86.1 \\   Method (Non-ID) & Benign Acc(\%) \(\) & ASR (\%) \(\) & Backdoor Acc(\%) \(\) \\  Scaling (Lockdown \(\) NC) & 89.9 & 3.2 & 86.2 \\  Neverousin & 86.0 & 1.8 & 83.5 \\   

Table 6: Lockdown on data-level attack.

   Methods (ID) & Benign Acc(\%) \(\) & ASR(\%) \(\) & Backdoor Acc(\%) \(\) \\  FedAvg & 89.0 & 63.5 & 35.5 \\ FashionMax (\(_{0}=0.1\)) & 88.5 & 3.5 & 84.3 \\ FashionMax (\(_{0}=0.1\)) & 88.5 & 3.8 & 82.8 \\ Omniscience & 89.3 & 44.7 & 52.1 \\   Methods (Non-ID) & Benign Acc(\%) \(\) & ASR (\%) \(\) & Backdoor Acc(\%) \(\) \\  FashionMax & 86.8 & 86.3 & 13.1 \\ FashionMax (\(_{0}=0.1\)) & 85.7 & 4.6 & 79.5 \\ FashionMax (\(_{0}=0.1\)) & 86.9 & 3.7 & 81.8 \\ Omniscience & 86.8 & 86.3 & 13.1 \\       Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\       Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\    
   Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\   

Table 8: Lockdown on adaptive attack.

   Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\    
   Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\   

Table 9: Communication and # of parameters for Lockdown under ID CIFAR10 (ResNet9) setting. The communication overhead is the sum of that of \(M=40\) clients in each round.

   Methods & Comm Overbal \(\) & \# of params \(\) & Benign Acc(\%) \(\) \\  FedAvg & 2.10GB (1x) & 6.57M (1x) & 91.0 (1x) \\  Lockdown & 0.525GB (0.25x) & 1.643M (0.25x) & 90.0 (0.0990x) \\   

Table 10: Performance on varying datasets.

**Visualization**. In Figure 6, We plot each filter's weight of the last convolutional layer of a ResNet-9 model trained with Lockdown. The brighter the color is, the larger the absolute value is, meaning that the filter can be activated by some particular inputs. We find that via our Lockdown procedure, the 86-th filter becomes "suspicious". We plot the weights in the classifier that connects this filter, and accidentally find that this filter contribute most to activate the "horse" neuron, which is our target backdoor label. This illustrates that lockdown can break the "poison-couple effect"- poison parameters (i.e., those in the 86-th filter) only appear in subspace that is not shared by benign clients, which can be effectively filtered out by contrasting the benign and poisoned client's subspace.

### Ablation and Hyper-parameter Sensitivity Analysis

Ablation study demonstrates the necessity of i) ERK initialization ii) gradient-based recovery, is moved to Appendix A.6. We tune three key hyper-parameters of Lockdown to demonstrate their impacts in our sensitivity analysis. Our findings are: i) setting a proper sparsity \(s\) can increase the model's robustness. ii) Setting a proper initial pruning/recovery rate \(_{0}\) is necessary for effective subspace searching, but too large of it will hurt the model's normal function. iii) Consensus threshold \(\) should be set sufficiently large to filter out the malicious parameters. See Appendix A.7 for details.

## 7 Conclusion

In this paper, we study the pruning-based defense for FL and observe a "poison-coupling" phenomenon, which degrades the defense performance. To mitigate such an effect, we propose Lockdown, a backdoor defense based on the idea of isolated subspace training. Empirical evidence shows that Lockdown can significantly reduce the risk of malicious backdoor attacks without sacrificing much on benign accuracy. Future works include studying how to generalize Lockdown to other FL settings, e.g., decentralized FL (Hu et al., 2019; Li et al., 2023; Shi et al., 2023), personalized FL (Fallah et al., 2020; T Dinh et al., 2020; Huang et al., 2022b, 2023; Dai et al., 2022), etc.