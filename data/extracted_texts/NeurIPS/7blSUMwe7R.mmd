# Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis

Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis

Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete

Department of Brain and Cognitive Sciences, MIT

Cambridge, MA, USA

{ostrow,eisenaj,leokoz8,fiete}@mit.edu

###### Abstract

How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.

## 1 Introduction

Comparing neural responses between different systems or contexts plays many crucial roles in neuroscience. These include comparing a model to experimental data (as a measure of model quality, Schrimpf et al. (2020)), determining invariant states of a neural circuit (Chaudhuri et al. (2019)), identifying whether two individual brains are performing a computation in the same way, aligning recordings across days for a brain-machine interface (Degenhart et al. (2020)), and comparing two models (e.g. to probe the similarity of two solutions to a problem, Pagan et al. (2022)). Current similarity methods include \(R^{2}\) via Linear Regression, Representational Similarity Analysis, Singular Vector Canonical Correlation Analysis, Centered Kernel Alignment, and Procrustes Analysis (Schrimpf et al. (2020), Kriegeskorte et al. (2008), Raghu et al. (2017), Williams et al. (2021), Duong et al. (2022)). Crucially, these all compare the _geometry_ of states in the latent space of a neural network.

Identifying shared neural computations between systems calls for methods that compare similarity at the level of the fundamental computation. In the brain, computations are instantiated by the emergent dynamical properties of neural circuits, such as fixed points, invariant and fixed point manifolds,limit cycles, and transitions between them (Amani and Arbib , Hopfield , Hopfield and Tank , Seung , Zhang , Hahnloser et al. , Burak and Fiete , Wang , Sussillo and Abbott , Churchland et al. , Chaudhuri et al. , Vyas et al. , Khona and Fiete .Geometric similarity metrics fall short in two manners when they are applied to dynamical systems. First, response geometries can differ due to sampling differences (Chaudhuri et al. ) or other nonfundamental causes even though the dynamics are topologically equivalent. In such cases, measures of state-space geometry do not fully capture the core similarities between two systems, a false negative situation. Conversely, systems may exhibit distinct dynamics over similar geometries in the state space (Galgali et al. ). In this case, geometric measures may fail to distinguish two distinct systems, a false positive situation. Therefore, we suggest that dynamical similarity should be the preferred lens through which to compare neural systems.

In dynamical systems, a key notion of similarity is called topological conjugacy: the existence of a homeomorphism that maps flows of one system onto flows of the other. When two systems are conjugate, they have the same qualitative behavior. Given two dynamical systems \(f:X X\) and \(g:Y Y\) with invertible mapping \(:X Y\), conjugacy is defined as:

\[g= f\] (1)

Here, we develop a data-driven method called Dynamical Similarity Analysis (DSA), which combines work from two previously-unrelated fields, machine learning for dynamical systems and statistical shape analysis. In brief, DSA returns a similarity metric describing how two systems compare at the level of their dynamics. First, DSA uses the insight that appropriate high-dimensional embeddings can be used to describe a highly nonlinear system in a way that is globally linear. The features that enable this transformation are referred to as Koopman Modes (Mezic ) and can be identified via the Dynamic Mode Decomposition (DMD) (Snyder and Song , Brunton et al. ). Subsequently, DSA employs a statistical shape analysis metric to compare vector fields within the Koopman Mode embeddings, thereby assessing the similarity between the systems' dynamics. This shape analysis measures how far a pair of systems are from being homeomorphic. Unlike most spatial shape analyses, which require averaging over multiple noisy trials in the same condition to arrive at one vector for each condition-time point (but see Duong et al. ), small amounts of noise create a rich representation of neural dynamics in the estimated dynamics matrix. The importance of flows in response to intrinsic (natural) or induced perturbations in capturing dynamics has been highlighted in Seung , Yoon et al. , Chaudhuri et al. , Galgali et al. , as their evolution over time can uniquely identify dynamic structure.

Our results demonstrate that DSA effectively identifies when two neural networks have equivalent dynamics, whereas standard geometric methods fall short. DSA therefore has great relevance for neuroscience, providing another method to validate the fit of a model to the neurophysiology data. It additionally opens the door to novel data-driven analyses of time-varying neural computation. We expect our method to be interesting to computational neuroscientists due to its theoretical richness, valuable to deep learning researchers as a theoretically backed and rigorously justified tool through which to interpret and compare neural networks, and useful for experimental neuroscientists due to its simplicity of implementation and ease of interpretation.

ContributionsWe develop a general method, DSA, for comparing two dynamical systems at the level of their dynamics. To do so, we introduce a modified form of Procrustes Analysis that accounts for how vector fields change under orthogonal transformations. We apply our method in four test cases, each demonstrating novel capabilities in neural data analysis: **(a)** Our method can identify dynamical similarities underlying systems with different geometries, which shape metrics cannot. **(b)** Conversely, DSA can distinguish systems with different dynamics despite similar geometries, which shape metrics cannot. **(c)** We show that our method empirically identifies topological similarities and differences between dynamical systems, as demonstrated by invariance under geometric deformation and variance under topological transformation. **(d)** We demonstrate how to use DSA to disentangle different learning rules in neural networks without supervision, by comparing how representations change across training. 1Methods

### Theoretical Background: Koopman Operators and Dynamic Mode Decomposition (DMD)

A common approach to modeling nonlinear dynamics is to approximate them with linear systems, as those are more tractable and interpretable. One standard method involves finding fixed points of the dynamics, and then locally linearizing the system around each fixed point, to determine whether infinitesimal changes diverge from or converge to the fixed point. Another is to describe nonlinear dynamics via locally linear models, a separate one for each point in state space. These methods do not generate a description of the global dynamics. The former method also requires injecting perturbations, which may not be possible for experimental systems and pre-collected datasets.

The field of dynamical systems theory suggests a different approach, on which a new class of data-driven methods are based. Specifically, Koopman operators theoretically embed nonlinear systems into infinite-dimensional Hilbert space, which permits an exact and globally linear description of the dynamics (Koopman ). Practical applications use the Dynamic Mode Decomposition (DMD) (Schmid ) to create a finite-dimensional embedding that approximates the Koopman operator (Brunton et al. , Snyder and Song , Dubois et al. ). The DMD identifies a linear transition operator for the dynamics: \(X(t+ t)=AX(t)\), where \(X\) comprises a data matrix (consisting of functions of the observed states of the dynamical system). When applied to a sufficiently rich \(X\), the DMD can capture global dynamical structure. A key challenge of the method is in constructing an appropriate space for \(X\) which is closed under the operation of \(A\) (a Koopman invariant subspace, Brunton et al. [2016b]). We employ the Hankel Alternative View of Koopman (HAVOK, Brunton et al. , Arbabi and Mezic ), which constructs \(X\) as a function of the delay-embedding of the observed variables (a Hankel Matrix), such that one column of \(X=[-x(t)-x(t-)-...-x(t-p)-]^{T}\), where \(p\) is the number of included delays and \(\) is the time-step between each measurement. The delay-embedding approach eliminates the difficulty of learning a nonlinear transformation of the data, and improves estimation capabilities over partially-observed systems (Kamb et al. ). The latter capability arises from Takens' delay embedding theorem, which states that a delay embedding is diffeomorphic to the original system-that is, a partially-observed system can be fully reconstructed using a sufficient number of delays (Takens ). However, our method could be used with a range of embedding algorithms in place of delay embeddings.

### Dynamical Similarity Analysis (DSA)

Now, we introduce _Dynamical Similarity Analysis_, a general method that leverages advances in both Statistical Shape Analysis and Dynamical Systems Theory to compare two neural systems at the level of their dynamics. Simply put, we use Koopman operator theory to generate a globally linear embedding of a nonlinear system, via delay embeddings as in HAVOK, then use DMD to define the linear transition operators in this space. Finally, we apply a novel extension of the Procrustes Analysis shape metric to the resultant DMDs from two systems. This set of steps constitutes DSA.

Suppose we have two dynamical systems defined by the following equations:

\[}=f(,t)^{n} }=g(,t)^{m}\] (2)

We sample data from each system \(X^{c k_{1} t_{1} n}\) and \(Y^{c k_{2} t_{2} m}\). Here, \(c\) indicates the number of conditions observed, which in neuroscience and deep learning refer to different input stimuli or contexts. \(k_{i}\) indicates the number of trials per condition, and \(t_{i}\) indicates the number of time steps observed per trial. Note that the observations must be sampled uniformly in time (Takens ). Next, we produce delay-embedded Hankel tensors with a lag of \(p\): \(H_{x}^{c k_{1}(t-p-1) np}\) and likewise for \(H_{y}\). Using the Hankel Alternative View of Koopman (HAVOK) approach (Brunton et al. ), we flatten all but the last dimension of \(H\) and fit reduced-rank regression models with rank \(r\) to the eigen-time-delay coordinates of the data, where the target is the coordinates of the next time step (\(V^{}_{x,r}\)):

\[V^{}_{x,r}=A_{x}V_{x,r}H^{T}_{x}=U_{x}_{x}V^{T}_{x} V_{x,r}=V_{x}[:,1:r]\] (3)Only the rank of the HAVOK models explicitly need to be the same so that the two DMD matrices can be compared. Note that the eigen-time-delay coordinate formulation is equivalent to PCA whitening (Supplementary Information Section 12), which Williams et al. (2021) highlights to be important. The predictive capability of our model (via MSE, \(R^{2}\), AIC, or others) can be used to identify the optimal rank \(r\) and number of delays \(p\). In our experiments, hyperparameters for the DMD were chosen to ensure both sufficient capability to fit the dynamics in question as well as tractable computation times. Fitting the HAVOK model to each system returns our DMD matrices: \(_{x},_{y}^{r r}\), which we compare using the modified Procrustes Analysis algorithm detailed next.

Procrustes Analysis over Vector FieldsProcrustes Analysis is a shape metric which compares the geometric similarity of two datasets after identifying an optimal orthogonal transformation to align them (Dryden and Mardia (2016)). To use it in DSA, we must modify the notion of orthogonal transformations to be compatible with linear systems. Recall that the Procrustes metric solves the following minimization problem (Williams et al. (2021)):

\[(,)=_{ O(n)}||-||_{F}\] (4)

Where \(O(n)\) is the orthogonal group (\(C^{T}C=I\)), \(F\) indicates the Frobenius norm, and \(X,Y\) are two data matrices. A distance metric that is invariant to orthogonal transformations captures the intuitive notion that there is no privileged basis across neural networks.

However, vector fields, represented by the DMD matrices \(_{x},_{y}\) above, transform differently than vectors X and Y (see Supplementary Information Section 8 for an illustration). The invariance we seek is that of a conjugacy, or change of basis, between linear systems, which is reflected in the similarity transform, \(A_{x}C^{-1}\). Note that this is a special case of equation 1 over linear systems. Thus we introduce a novel similarity metric, which we call _Procrustes Analysis over Vector Fields_:

\[(_{x},_{y})=_{ O(n)}|| _{x}-_{y}^{-1}||_{F}\] (5)

Figure 1: **Schematic and example of DSA.****a.** Schematic of the DSA pipeline. **b.** Schematic of attractor reconstruction with delay embeddings. **c.** Schematic of the Koopman Operator linearization over time-delay coordinates. **d,e.** PCA trajectories (black lines) of a GRU (left) and Vanilla RNN (right) trained to produce sine waves of a range of frequencies (Sussillo and Barak (2013)). Colored dots indicate center points of the limit cycles, colored by frequency of the output wave. **f.** Comparison of measured dissimilarity between the networks in **d,e** with (from left to right) Procrustes on the condition-averaged trajectories, Procrustes on the delay-embedded condition-averaged trajectories, DSA on the original data, and DSA on the delay embedding. Lower is more similar. Each metric uses angular distance, which ranges from 0 to \(\).

The angular form of this distance can be written as:

\[(_{x},_{y})=_{ O(n)}_{x}(_{y}^{-1})}{|_{x}| _{F}|_{y}|_{F}}\] (6)

where \(_{x}(_{y}^{-1})\) is computed as \((_{x}^{T}_{y}^{-1})\). In the Supplementary Information Section 9, we prove an equivalence between DSA and the 2-Wasserstein distance between the two DMD matrices' eigenspectrum, in special cases. This connection highlights the grounding of DSA in Koopman operator theory to measure conjugacy: when two dynamical systems are conjugate, their Koopman eigenspectra are equivalent (Proof in Supplementary Information 10, Budisic et al. (2012), Lan and Mezic (2013)). We derive equation 5 from first principles and prove that it is indeed a proper metric in the Supplementary Information (6); the proper metric property is crucial for various downstream machine learning pipelines such as clustering or classification (Williams et al. (2021)).

Equations 5 and 6 are nonconvex optimization problems similar to those in Jimenez et al. (2013), although we solve them with gradient-based optimization. Across our experiments, we found that the metric asymptotes in roughly 200 iterations using the Adam optimizer (Kingma and Ba (2014)) with a learning rate of 0.01. To optimize over the orthogonal group, \(O(n)\), we utilize an extension of the Cayley Map, which parameterizes \(SO(n)\) (Supplementary Information 5.1). In our experiments, we compare the response of DSA with direct application of Procrustes to the data matrices as it is the most analogous shape metric. If only the identification of conjugacy is desired, metric properties can be relaxed by optimizing \(C\) over the larger group of invertible matrices, \(GL(n)\).

## 3 Results

To demonstrate the viability of our method, we identified four illustrative cases that disentangle geometry and dynamics, and additionally performed an ablation analysis of DSA. First, a set of neural networks that are different by shape analyses but are dynamically very similar; second, a set of neural networks that are similar by shape analyses but are dynamically are very different; third, a ring attractor network whose geometry we can smoothly and substantially deform while preserving the attractor topology; finally, a line attractor network whose topology is transformed into a ring attractor by adding periodic boundary conditions.

### Ablation Study

Importantly, both the reduced-rank regression and delay-embedding in DSA are required to isolate dynamical structure. We demonstrate this in Fig. 1f, where we compare the hidden state trajectories of an RNN and GRU solving the Sine Wave task (Sussillo and Barak (2013)) using the standard Procrustes Analysis (via the netrep package, Williams et al. (2021)), delay-embedded Procrustes Analysis (an ablation), DSA with no delay embedding (another ablation), and DSA score. To be exhaustive, we also computed a similarity metric that compares purely temporal information, defined as the angular Procrustes distance over the power spectrum of the RNN's hidden state trajectories. On the RNNs in Fig. 1, this metric reported 1.41. Only DSA identifies the systems as highly similar, indicating that both the delay embedding and the DMD are necessary to abstract geometric particulars from each system.

### DSA captures dynamical similarity between RNNs despite differences in geometry.

We trained a collection of 50-unit RNNs on the well-known 3-bit Flip Flop task from Sussillo and Barak (2013). In this task, an RNN is presented with a time series from three independent channels, each sporadically occurring pulses of either -1 or 1. The network's objective is to continuously output the last pulse value for each channel until it is updated by a new pulse (hence the Flip-Flop name). Following Maheswaranathan et al. (2019), We varied architecture (LSTM, GRU, UGRNN, Vanilla RNN), activation function (ReLU, Tanh), learning rate (.01,0.005), and multiple different seeds across each set of hyperparameters, ultimately collecting a dataset of 240 networks. We only saved a network if it solved the task with MSE less than 0.05. After training, we tested all models on the same 64 input sequences and extracted the recurrent states, upon which we apply Procrustes and DSA in parallel to compare networks.

Our RNNs have different geometry but equivalent attractor topology (Maheswaranathan et al. (2019)). Fig.2a. displays the trajectories of individual trials in a sample network in the first three principal components, which capture on average 95.9% of the total variance. The computational solution to this task is characterized by eight stable fixed points at the vertices of a cube that correspond to the eight possible unique memory states (\(2^{3}\)). Importantly, this cubic structure is preserved across all networks, even though particular geometric aspects of the trajectories may differ. Standard shape analyses differentiate architectures and activation function by geometry, which we replicated in Fig. 2b (architecture) and in the Supplementary Information (activation, 13).

We fit individual HAVOK models with 75 delays and rank 100 to sampled trajectories from each network. For computational efficiency, we reduced the dimensionality of the network to 10 using Principal Component Analysis (PCA) before applying HAVOK. In Supplementary Information 14, we demonstrate that HAVOK's predictivity is robust to hyperparameter variation. Because the variance explained from the later PCs is negligible, they can be disregarded while preserving the original dynamics. This improves HAVOK's fit to the data and reduces the memory and time complexity of DSA. For each pair of networks, we compared these states directly using Procrustes Analysis and on the DMD matrices using Procrustes Analysis on Vector Fields. This yields two dissimilarity matrices (DMs), which we reduced to two dimensions using Multidimensional Scaling (MDS, Kruskal (1964)). We plot the results in this similarity space in Fig. 2, where each point is a trained network. As expected, the networks cluster by architecture (Fig. 2b) and activation function (Supplementary Information) when Procrustes is applied, indicating that aspects of their geometry differ. However, the networks intermingle under DSA (Fig. 2c, inset zoomed in), and have pairwise dissimilarities close to zero, indicating that their underlying dynamics are equivalent, which is correct (Maheswaranathan et al. (2019)). We quantified these differences by applying a linear SVM to the dissimilarity matrices and plotted the test accuracy in Fig. 2d. This further underscores the differences between Procrustes and DSA in Fig. 2b and c, as DSA's accuracy is close to chance, whereas Procrustes achieves high accuracy on both labels.

### DSA identifies dynamical differences despite geometric similarity

Next, we sought to display the converse, namely that our similarity method can identify differences in dynamical structure even when spatial analysis cannot. To do so, we examined a set of three noisy recurrent networks from Galgali et al. (2023). These systems implement a bistable pair of attractors, a line attractor, and a single stable fixed point at the origin. These systems describe three strategies in a classical perceptual decision-making task from systems neuroscience: unstable evidence integration, stable integration, and leaky integration. Their dynamical equations are included in the Supplementary Information. Here, the inputs to the latter two networks were adversarially optimized so that the condition-averaged trajectories are indistinguishable from the first network (Fig. 3a). These networks have intrinsic noise in their hidden states, which enables DSA to distinguish systems beyond condition averages. We simulated 200 noisy trajectories from 100 networks per class, each with randomly sampled parameters.

Figure 2: **Same dynamics, different shape, only identified as similar with DSA. Dots indicate a single trained network. Analysis of RNNs trained on the 3-bit FlipFlop task.****a.** PCA trajectories of a single trained RNN. **b.** MDS projection of the dissimilarity matrix (DM) computed across condition-averaged hidden states between each RNN (architecture indicated by color). In this view of the data, RNNs cluster by architecture. **c.** MDS embedding of the DM generated from DSA of the same set of RNNs as in b. Here, RNNs do not cluster by architecture. **d.** Classification test accuracy of condition-averaged Procrustes and DSA similarity spaces on both architecture and activation labels. Dotted lines indicate chance.

To confirm that the condition-averaged trajectories are indistinguishable, we visualize the condition-averaged and single-trial trajectories in Fig. 3a for each system. We fit HAVOK models with 100 lags and a rank of 50 for each network and computed DSA and Procrustes pairwise (5,000 total comparisons). As before, we plot a low-dimensional MDS projection of these dissimilarity matrices (3). When the two types of similarities are compared side by side (spatial similarity, Fig. 3b, and DSA, Fig. 3c), it is evident that only our method is capable of easily distinguishing dynamic types from the data alone. This is quantified by almost perfect test accuracy via a linear classifier in Fig. 3d. Thus, DSA can efficiently extract features that describe the underlying dynamical system from data alone, which better reflect the nature of the computation that is being performed in RNNs.

### DSA is invariant under geometric deformations that preserve attractor topology

Next, we examined how DSA responds when smooth geometric deformations are applied to a dynamical system. Here, we chose to study a ring attractor network, a system known to track head direction in the brain, and whose topology is a ring embedded in \(n\) neurons (Zhang (1996); Chaudhuri et al. (2019); Skaggs et al. (1995)). The neural activity on the ring is instantiated as a bump of local activity, which is stable due to the localized and translationally-symmetric connectivity profile. In each trial, we randomly sampled \(n\) uniformly between 100 and 250, and drove the network with constant input and dynamical noise, leading the bump to rotate around the ring at a roughly constant velocity (we used code from Wang and Kang (2022)). The network's dynamics are detailed in the Supplementary Information (16). Importantly, after simulation, the saved activations can be transformed by a continuous deformation that preserves the ring topology while changing the geometry:

\[r=}\] (7)

The variable \(s\) describes the synaptic activations of the ring network (the dynamical variable), and \(r\) describes the neural activations, which we compare with DSA and Procrustes. Here, \(\) controls the width of the bump. As \(\) increases, the width of the bump becomes progressively tighter, which makes the ring less planar (see Fig. 1 in Kriegeskorte and Wei (2021) for a depiction). We scaled the magnitude of \(\) from 0.1 to 4. In the inset of Fig. 4b, we quantified the ring's dimensionality across \(\), measured as the participation ratio of the principal components' eigenvalues: \(PR=_{i})^{2}}{_{i}_{i}^{2}}\)(Gao et al. (2017)).

After applying these transformations to simulated trajectories from our network, we applied DSA and Procrustes to respectively compare all \(\)s to the lowest \(\) in our dataset. In Fig. 4a we visualize the hidden state trajectory in the first two PCs as the ring is progressively transformed with \(\). We fit HAVOK models with a lag of 10 and a rank of 1000 to trajectories at each level of deformation. When we compared the network using DSA and Procrustes, we found that only DSA is invariant across these transformations (Fig. 4b). While the value for DSA is not strictly zero, this may be due to approximation or numerical error in HAVOK and the similarity metric, as it remains close to zero

Figure 3: **Different dynamics, same shape, only distinguished by DSA.****a.** Sample single-trial (faded) and condition-averaged trajectories of the three systems in two dimensions (x and y axes). Trials start at the center point and move away in a direction depending on the condition (colored). **b.** Procrustes-MDS on the dissimilarity matrices of the three network types. **c.** DSA-MDS of the three networks. **d.** Classification test accuracy of condition-averaged Procrustes and DSA similarity spaces. Dotted line indicates chance.

throughout (between 0.05 and 0.12). This suggests that DSA identifies similarity at the level of the topology of a dynamical system.

### DSA responds to changes in topology

Finally, we assessed the converse to the previous analysis: does DSA respond to changes in topology? Using the same ring attractor network, we continuously varied the strength of its periodic boundary conditions to convert the ring into a curved line segment attractor by ablating some neurons from the network on one end of the ring. Our ring is specified by a cosine kernel that defines how each neuron connects to its neighbors. This kernel has a length parameter \(l\) which specifies how far apart two neurons can connect. To completely eliminate the boundary conditions, \(l\) neurons must therefore be ablated. Using an ablation length \(c\) as the number of neurons to ablate in the ring, we defined the following parameter to normalize the transformation across different network sizes.

\[=1-\] (8)

When \(=1\), there is no ablation and the network is perfectly a ring. When \(<1\), the network breaks into a line segment attractor. As \( 0\) the ring becomes progressively less curved. To ensure that network sizes were consistent across all values of \(\), we initialized the network size to be \(n+c\) before ablation, where \(n\) is the desired final number of neurons. We simulated networks of sizes between 200 and 300 neurons. Instead of a constant drive of input as before, we drove the network with a fixed magnitude input \(u\) whose sign flipped stochastically. We chose this input to prevent the bump of activity from getting trapped on one end of the line attractors.

When we compared each value of \(\) of the network to \(=0\) with DSA and Procrustes, we identified that DSA reports values close to 0 until \(\) becomes close to 1.0, when it jumps almost to \(/2\). We plotted these results in Fig. 4, where panel **c** depicts the low-dimensional visualization of the line and the ring, and **d** displays our metric's response. Note that because the data are noisy, we expected DSA

Figure 4: **DSA is invariant under geometric deformation of a ring attractor, and sensitive to the transformation of a ring attractor into a line.** Top row displays scaling of \(\) in the sigmoid, making the ring progressively less planar. The bottom row displays topological transformation of a ring into a line via \(\). The network is a curve line attractor at \(<1\), and is a ring attractor at \(=1\). **a, c**. Trajectories along the ring plotted in the top two PCs of the network, colored by magnitude of the deformation parameter. In **c**, trajectories are scaled for visualization purposes. **b,d**. Distance (D) to \(=0.1\), and \(=0.0\) in both Procrustes and DSA. Importantly, the topology only changes in the last row, at \(=1.0\). Inset **b**: Participation Ratio of the ring, a natural continuous measure of dimensionality based on explained variance ratios of PCA eigenvalues (Gao et al. (2017)). Inset **d**. Persistent Homology Analysis of the ring: top 2 lifespans of Betti number 1 features, indicating that the data becomes more ring-like as \(\) increases.

to jump slightly before 1.0, which we empirically verified to be around \(=0.9\) in our simulations. Here we used only 10 delays (for a total embedding size between 2000 and 3000) and rank 100, but found that our results generalized to much larger DMD models as well. This further solidifies the notion that DSA captures topological similarity of two dynamical systems.

### DSA can disentangle learning rules in an unsupervised fashion

It is still unclear what learning rules the brain uses. Recent progress includes the design of novel biologically-plausible learning rules (Lillicrap et al. (2016); Hinton (2022)), experiments and modeling work that suggests signs of gradient-based learning in the brain (Sadtler et al. (2014); Humphreys et al. (2022); Payeur et al. (2023)), supervised methods to classify learning rules from aggregate statistics in neural data (Nayebi et al. (2020)), and theory describing how representations evolve across training in different learning rules (Bordelon and Pehlevan (2022)). Because artificial networks are initialized with random seeds, their trajectories in the activity space across learning will almost always have different geometry, despite the fact that they may converge to the same minimum. However, because the trajectories arise from the same dynamical system, DSA should be able to capture similarities between learning trajectories from different initializations. We visualize a variety of learning rule trajectories in Fig. 5a to demonstrate how qualitatively different their dynamics can be. We applied DSA on two test cases from the literature to demonstrate that we can empirically disentangle artificial networks trained with different learning rules. To do so, we fit DSA to trajectories of the neural representations (or observables) across training epochs.

In the first case, we drew 21 aggregate statistics from the dataset from Nayebi et al. (2020), which describe the evolution of synaptic weights and activations of layers in various convolutional neural networks trained on multiple tasks with four learning rules. The learning rules utilized in the dataset include Adam (Kingma and Ba (2014)), Stochastic Gradient Descent with Momentum (Sutskever et al. (2013)), Information Alignment (Kunin et al. (2020)), and Feedback Alignment (Lillicrap et al. (2016)). For each analysis, we fixed one task and generated 50 datasets by randomly selecting (without replacement) subsets of trajectories for each learning rule. On each dataset, we fit a HAVOK model with a rank of 45 and lag of 4 to trajectories of these statistics _across_ training. We compared each model pairwise, which results in a dissimilarity matrix. After comparing trajectories of these observables across training with DSA, we visualize the MDS of the similarity space in Fig. 5b. The training dynamics clearly cluster by learning rule under DSA. Quantifying the degree of clustering, we could classify the learning rule with 88.5% test accuracy using only a _linear_ classifier and 5-fold cross-validation, whereas the linear SVM classifier in Nayebi et al. (2020) only reached \( 55\)% accuracy (Fig. 2a. in their paper). The test accuracy over a wide range of DSA hyperparameters was 81 % (Fig. 5d).

In the second case, we trained a set of small 3-layer feedforward neural networks (with a hidden size of 100 units) trained to solve a multivariate regression task from Bordelon and Pehlevan (2022). The learning rules in the second set include Gradient Descent, two forms of Feedback Alignment that vary based on the initialization of the random feedback weights (Boopathy and Fiete (2022)), and Hebbian learning (Hebb (1949)). For each learning rule, we trained 50 networks on 1000 epochs with random initializations and randomly generated data. We fit HAVOK models with a delay of 100 and a rank of 500 onto individual networks' activations across learning. As above, we found that

Figure 5: **DSA unsupervisedly disentangles learning rules.****a.** Schematic example of different algorithms minimizing \(3x^{2}+2y^{2}\). **b.** DSA MDS plot of hidden activation observables from networks trained on ImageNet **c.** DSA MDS plot of the dynamics of learning in 3-layer linear networks. **d.** Test accuracy of a linear decoder on the DSA similarity space for the model classes. Bars indicate standard deviation over a wide range of DSA hyperparameters.

the MDS visualization of these networks clustered by learning rule in the pairwise similarity space (Fig. 5c). The two types of Feedback Alignment cluster together, suggesting that their dynamics are similar, as might be expected. Once again utilizing a linear classifier, we achieved 89% test accuracy after assigning both types of FA as the same class, and on average 80% accuracy over a range of hyperparameters (Fig. 5d). These results suggest the DSA could be used in a hypothesis-driven fashion to assess learning rules in data recorded from biological circuits.

## 4 Discussion

We demonstrated that our novel metric, DSA, can be used to study dynamics in neural networks in numerous scenarios, including the comparison of dynamics between different RNNs, dynamics of a single RNN across training, or dynamics of weight and activity changes in feedforward networks over training. We performed an ablation study in Fig. 1f, which demonstrates that both delay embedding and the DMD were required for DSA to succeed. While we applied our metric only to RNNs, we stress that it can be applied to any dynamical system, including diffusion models or generative transformers. Procrustes Analysis over Vector Fields could also be applied to Koopman Embeddings identified via other algorithms, such as an autoencoder (Lusch et al. (2018)).

As suggested by Figs.2, 3, 4 and our theoretical grounding (9, 10), DSA can capture similarity at the level of topology of the dynamics, also known as conjugacy. This can also be recognized from the methodology of DSA: the similarity transform is a conjugacy between two linear systems. In recent work, Redman et al. (2023) used the 2-Wasserstein Distance over the Koopman eigenspectrum to compare learning dynamics of fully-connected networks trained to classify handwritten digits. This metric is quite analogous to ours in its theoretical motivation. In Supplementary Information 9 we prove that their metric is a specific case of DSA. Thus, DSA quantifies how far two dynamical systems are from being conjugate.

As we demonstrate in Fig. 3, our method could be used to quantitatively compare various hypotheses about the dynamics of a particular biological neural circuit (such as fixed point structure), by calculating similarities of various RNNs or bespoke models to data (Pagan et al. (2022), Schaeffer et al. (2020)). This task is typically computationally intensive, relying on assessing the dynamics of noise perturbations in the neural state space (Chaudhuri et al. (2019), Galgali et al. (2023)) or computing the locations of fixed points in a task-optimized RNN model that has similar spatial trajectories as the neural data (Mante et al. (2013), Chaisangmongkon et al. (2017)). Our method will be useful in the analysis of biological neural systems, where only recorded neural activity is accessible. To our knowledge, DSA is the first hat can be used to quantitatively assess different dynamical hypotheses. We suggest that this method should be used in conjunction with shape analyses when assessing fits of computational models to data. In future work, we plan to apply DSA to experimental data.

DSA could potentially be used for rapid alignment of Brain-Computer Interfaces (BCI) across days in a subject. Non-stationary sampling of neurons from intracortical electrode shift, neuron death, and other noise plague longitudinal BCI trials, requiring the subject to relearn BCI control daily in a slow and tedious process (Biran et al. (2005), Perge et al. (2013)). Two recent approaches (Willett et al. (2023), Degenhart et al. (2020)) utilized Procrustes Analysis in their decoding and alignment pipeline. As demonstrated in Figs. 2, 3, and 4, an alignment method that takes dynamics into account will be useful. Karpowicz et al. (2022) utilized LFADS to align dynamics across days, but DSA may provide equal performance with substantially higher data efficiency and less compute due to the method's simplicity.

In Section 3.6 we demonstrated how the DSA metric space could be used in a hypothesis-driven fashion to compare neurophysiological data recorded across training to artificial neural networks trained with different learning rules. To do so, one would train a variety of neural networks with different learning rules to solve the same task as the animal model. By applying DSA pairwise to each network and the neurophysiology data, and identifying which learning rules cluster to the experimental data, we can reject other learning rule hypotheses. DSA has multiple advantages over previous methods for this problem: Unsupervised feature generation can improve classification capabilities, and abstraction from geometrymakes the method more generically applicable across systems of different types and initial conditions.

LimitationsIt can be challenging to identify the optimal DMD hyperparameters, but it is not computationally intensive to perform a grid sweep. Additionally, we found that the ranks and delays required to achieve our results were relatively small. On a V100 GPU, a single DSA took between 30 seconds and a few minutes, depending on the hyperparameters and size of the dataset.

AcknowledgmentsThe authors are grateful to Antonio Carlos Costa for bringing Koopman operators to our attention, and to Sarthak Chandra, Raymond Wang, Aran Nayebi, Ho Kyung Sung, Will Redman, Tosif Ahamed, and members of the Fiete lab for helpful discussions and advice.