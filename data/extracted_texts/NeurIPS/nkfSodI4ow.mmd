# XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing

XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost ($3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost ($46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.

## 1 Introduction

Recently, large-scale deep learning models are empowering us to achieve more in many ways, such as code generation  and text-to-image generation [40; 41]. To keep improving the service quality, deep learning model architecture evolves rapidly, and the model size is also growing at a tremendous speed. The increasing model size leads to unprecedented training cost (especially for foundation model pretraining), which recently grows to 2 months on thousands of GPUs/T-PUs [47; 9]. On the other hand, a less-emphasized perspective is that **data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them**. As plotted in Fig. 1, for several representative language models in the last 5 years both the model and data scales increase at a similar speed. Recent works including Chinchilla  and PaLM 2  emphasize the need of increasing data scale at an even faster speed. This demonstrates the importance of improving data efficiency: achieve same model quality with less data and reduced training cost, or achieve better model quality with the same amount of data and similar training cost.

Figure 1: Model scale (number of parameters) and data scale (number of consumed training tokens ) of representative language models in the last 5 years [14; 46; 7; 45; 9].

There are two popular research directions among existing data efficiency techniques: Data sampling techniques aim to improve the convergence speed by sampling the most suitable next data batch from the whole data pool; Data routing techniques aim to reduce the computation by routing each data to only a subset of the model components. These techniques improve data and training efficiency, but existing solutions have several limitations:

* Techniques like curriculum learning improves data efficiency by indexing and sampling training data based on certain difficulty metric , and it is recently proved effective on large-scale pretraining tasks . However, implementing different CL strategies for different user tasks can require a lot of code-refactoring, which is time-consuming and error-prone. In addition, existing implementations have less consideration on scalability, which makes it difficult to analyze and index large-scale training data based on different difficulty metrics.
* Existing data routing techniques such as token drop/bypass/pruning were mostly designed for inference and inapplicable to training. TokenBypass , to our knowledge the only data routing technique for foundation model pretraining, skips the compute of part of the input tokens at some middle layers during BERT pretraining, reducing pretraining cost while maintaining model quality. However, it requires several special implementations that may only work for the tested BERT pretraining case, such as the importance score-based token dropping decisions and the whitelist for special tokens. This could limit the possibility and benefit of applying it to other cases.
* Although promising data efficiency solutions have been proposed independently, combining multiple methods together for the best outcome is still a laborious process, requiring changes in multiple places in the training pipeline: data loader, data sampler, model architecture, etc. Another challenge is that existing techniques usually add additional hyperparameters but without a clear and low-cost tuning strategy.

To address these above challenges, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, XYZ Data Efficiency demonstrates the following contributions:

* **Efficient data sampling via general curriculum learning library.** We present a general curriculum learning (CL) library that is both scalable and customizable: it includes a map-reduce based data analyzer that enables scalable analysis and indexing of massive data based on any possible CL metric; it includes a general CL-based data sampler and loader design for users to apply any customized CL strategies. Using this library, we are able to thoroughly explore different CL strategies for GPT-3 1.3B and BERT-large pretraining, and identify the best solution that provides better data and training efficiency than existing CL solution. This library (and the whole XYZ Data Efficiency framework) has been open sourced in a deep learning acceleration library (name hidden for anonymity) that is fully compatible with PyTorch. This will benefit the whole community as a useful tool to apply curriculum learning to their own training tasks.
* **Efficient data routing via random layerwise token dropping.** We present a novel data routing technique called random layerwise token dropping (random-LTD) to skip the computation of a subset of the input tokens at all middle layers. Random-LTD employs a simple yet effective routing strategy and requires minimal model architecture change. It is very flexible to apply random-LTD to various tasks (GPT-3/GPT-3 MoE/BERT pretraining and GPT/ViT finetuning) which the SOTA technique (TokenBypass) does not explore or provides less improvement.
* **An easy to use/tune framework that maximizes data/training efficiency.** XYZ Data Efficiency seamlessly composes the two proposed techniques, and only requires minimal changes on user side. To our knowledge, we are the first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency, especially for foundation model pretraining: For GPT-3 1.3B pretraining, Fig. 2 shows that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier. In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data, while baseline can only maintain 91% of the model quality, a 1.8x higher quality degradation. Based on measured training time, 12.5x would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure , greatly democratizing research and usage of foundation models for AI community. For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1. Both techniques under our framework are easy to use and tune, and we include a low-cost tuning strategy and a summarized usage guidelines. This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts (MoE) model pretraining and small-scale GPT-2/ViT model finetuning.

## 2 Background and Related Works

**Data sampling.** For deep learning, the most common data sampling method for minibatch stochastic gradient descent is uniform sampling, where at each step a batch of data is drawn uniformly at random from the whole training data. However, it's potentially beneficial to focus on different kinds of data at different training stages. One example is the curriculum learning technique  which aims to improve training convergence speed by presenting relatively easier or simpler examples earlier during training. Building a curriculum learning solution usually requires two components: the difficulty metric (i.e., how to quantify the difficulty of each data sample) and the pacing function (i.e., how to decide the difficulty range when sampling next training data batch). In the NLP area, curriculum learning has been applied on small-scale one-stage tasks and downstream finetuning tasks, such as neural machine translation (NMT)  and natural language understanding (NLU) . There are also a few works that explore curriculum learning for language model pretraining . However, one common limitation among existing works is that there does not exist a scalable and customizable curriculum learning library, making it difficult to analyze large-scale data and explore custom difficulty metrics/pacing functions. One evidence is that most of the curriculum learning works for language model pretraining only focus on the sequence length metric due to the difficulty of exploring other metrics on the huge pretraining dataset.

**Data routing.** In common deep learning training, the model is considered as a whole and all sampled data will be routed to all model components. However, it's potentially beneficial to route each data sample to only a subset of model components, improving the training efficiency. One direction of efficient data routing is to add data bypass/skipping capability to existing model architectures such as Transformer. Transformer  architecture is a stack of transformer layers, each of which has two main ingredients, i.e., the multi-head attention (MHA) and the feed-forward connection network (FFC). Suppose the transformer has \(l\) layers denoted as \(L_{1},,L_{l}\). Let \(X_{i}^{s d}\) be the output tensor of \(i-\)th transformer layer, and \(x_{0}\) be the input (after embedding) of the transformer. Here \(s\) is the sequence length and \(d\) is the hidden dimension.

Several token dropping/bypassing/pruning techniques  were proposed for BERT inference to reduce the computational overhead, but they are not practical for training. In these works, if a token \(i\) (\(X_{j,i}\)) is decided to be dropped at layer \(j\) (\(L_{j}\)), the compute cost of this token through all remaining layers (\(L_{k}\) where \(k>j\)) is eliminated. As such, the sequence length \(s_{i}\) of the \(i\)-th layer's input \(X_{i-1}\) will be a non-increasing array, i.e., \(s_{0} s_{1}\)... \( s_{l}\). However, such a configuration has been shown instability for adaptive token-dropping inference . Therefore,  utilize the sandwich rule and distillation from  to stabilize training and boost accuracy. But these two methods also significantly increase the training cost. Thus, such techniques cannot be applied to speed up the pretraining procedure.

Recently, TokenBypass  enabled token dropping for BERT pretraining. It uses several importance scores/metrics to determine the dropped tokens (token frequency and cumulative loss). It proposed two main mechanisms to overcome the training instability issue: (1) the sandwich token dropping rule, where the first (\(L_{1}\) to \(L_{i}\)) and the last few BERT layers (\(L_{l-j}\) to \(L_{l}\)) capture all tokens (no token dropping) and only bypass \(s^{} s\) tokens from \(L_{i}\) to \(L_{l-j}\) middle layers. Particularly, the authors (only) test on the encoder transformer (12-layer BERT\({}_{}\) and 24-layer BERT\({}_{}\)), and let \(i=l/2-1\), \(j=1\), \(s^{}=s/2\). (2) special token treatment, where special tokens (e.g., [MASK], [CLS], [SEP]) are never dropped. Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically

    & Efficient & Efficient & Verified & Key \\  & data sampling & data routing & workloads & achievements \\   Sequence length \\ warmup  \\  &  Specific \\ CL metric \\  & N/A & GPT-2/ViT & 1.33 data/cost saving \\  & & pretraining & with 100% model quality \\   TokenBypass \\  \\  & N/A & TokenBypass & BERT & 1.33x data/cost saving \\  & & pretraining & with 100% model quality \\   Proposed XYZ \\ Data Efficiency \\  & 
 general CL \\ library support \\  & random-LTD & GPT-3/BERT/MoE & 12.5x data/cost saving \\  & & pretraining & with 95% model quality \\  & & GPT-2/ViT & 2x data/cost saving \\  & & & finetuning & with 100% model quality \\   

Table 1: Comparing XYZ Data Efficiency with NOTAs.

Figure 2: GPT-3 1.3B pretraining: relative model quality (baseline with full data as 100% quality) under different data consumption (1% to 100%) and training cost (when renting on Azure).

[MISSING_PAGE_FAIL:4]

* **Reorder-based sequence length (seqreo), for BERT.** This metric is similar to seqtru metric, but instead of truncating we adjust the sequence length by reordering the training data based on the "effective sequence length" in BERT training data sequences.
* **Vocabulary rarity (voc), for GPT and BERT.** This metric was proposed in a CL work for neural machine translation . It computes the product of the unigram probabilities for each sequence by \(-_{k=1}^{N}log(p(w_{k}))\) where \(p(w_{k})\) is the vocabulary frequency (inside whole training data) of the \(k\)th word in the sequence. Lower value indicates that the sequence has more common vocabularies.
* **seqtru_voc, for GPT and BERT. seqrevs_voc, for GPT. seqrevo, for BERT.** These 3 metrics are combinations of above metrics. For seqtru_voc and seqres_voc, we first reorder the training data based on voc metric, then apply seqtru or seqres as a kind of post-processing. For seqreo_voc, we treat it as a single new metric and index the data based on it.

Besides the difficulty metrics, another set of CL hyperparameters is the pacing function: the start and end difficulty (\(d_{s}\) and \(d_{e}\)), total number of CL steps (\(T_{c}\)), and the kind of pacing function (linear, sqrt, or users can plug in any customized function to the proposed framework). For seqtru and seqres metrics, we set the \(d_{s}\) and \(d_{e}\) as value-based (e.g., \(d_{s}=80\), \(d_{e}=2048\)) since the possible values of these two metrics are continuous. For other metrics, we set \(d_{s}\) and \(d_{e}\) as percentile-based (e.g., \(d_{s}=1\%\), \(d_{e}=100\%\)) since the possible values of these metrics are discrete. For seqtru and seqres we use a linear pacing function (\(d_{t}=d_{s}+(d_{e}-d_{s}) min(},1)\)) following the previous work , while for seqreo and voc we use a sqrt pacing function (\(d_{t}=d_{s}+(d_{e}-d_{s}) min((})^{0.5},1)\)). This is because seqreo and voc will only sample from a subset of data pool before reaching the end difficulty, and previous work finds that in such case it's beneficial to use a sqrt function to avoid sampling too much easy samples at the beginning . Sec. 3.3 includes low-cost tuning strategy and usage guidelines for our CL solutions.

### Efficient data routing via random-LTD

**Layerwise Token Dropping.** Existing token dropping methods for inference and training either permanently drop tokens from the compute graph at intermediate layers, or at least make some tokens fully skip a consecutive series of middle layers (Sec. 2). However, several works [50; 31; 51] have shown that MHA focuses on different tokens at different layer depths and the attention map aligns with the dependency relation most strongly in the middle of transformer architectures. Therefore, fully skipping middle layers like TokenBypass  may hinder the learnability/generalization of the architecture during pretraining/inference. We conjecture that this might be why multiple first/last layers need to disable token bypassing and the special token treatment is needed.

In order to overcome this problem, we propose a layerwise token dropping (LTD) mechanism. Instead of fully bypassing same tokens over all middle layers, each transformer layer independently drops/retains its own set of tokens. In more detail, recall that the input of \((i+1)\)-th layer (\(L_{i+1}\)) is \(X_{i}^{s d}\). Denote the dropped token index as \(J_{i}=\{j_{1},j_{2},...,j_{a_{i}}\}\) and the kept token index as \(K_{i}=\{k_{1},...,k_{b_{i}}\}\) such that \(a_{i}+b_{i}=s\). We have \(J_{i} K_{i}=\{1,2,3...,s\}\) and \(J_{i} K_{i}=\) for each layer. Meanwhile, for any two different layers \(L_{i_{1}}\) and \(L_{i_{2}}\), \(J_{i_{1}}\) and \(J_{i_{2}}\) are independent, though the dropped ratios are the same. With this layerwise mechanism, each token rarely bypasses all middle layers. Thus, its dependency on other tokens can be captured by MHA.

**Random Token Dropping.** Various importance score-based metrics are used to determine the token dropping criterion. Most of them can be categorized in attention score-based or loss/frequency-based metrics. However, both of them introduce challenges that make LTD less practical: For attention score-based metrics, the compute cost for LTD is too high since the metric has to be calculated for every layer; For loss/frequency-based metrics, the accumulated loss or frequency would not be changed within the same iteration, which leads the dropped token to be the same for different layers, breaking the desired LTD mechanism. Instead of importance score, we propose to use _purely random_ token dropping assignment and prove its effectiveness in all our experiments. For each transformer layer, we randomly (uniformly) select a small batch of tokens to proceed with the compute and drop the rest. In more details, assume \(M_{i}=\){\(m_{i}(1)\), \(m_{i}(2)\),..., \(m_{i}(s)\)} is a random shuffle of \(S=\){1, 2,..., s}. Then the dropped token set is \(J_{i}=\){\(m_{i}(1)\), \(m_{i}(2)\),..., \(m_{i}(a_{i})\)} for the input of \(L_{i+1}\).

**Random and Layerwise Token Dropping.** Combining layerwise token dropping with random token dropping, we have our final random and layerwise token dropping method (random-LTD), which can efficiently apply token dropping for each individual layer and can capture the attention dependency of each token with other others in middle layers with high probability. As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won't benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity. Fig. 5

[MISSING_PAGE_EMPTY:6]

training tasks. And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4).

**Tuning Strategy and Usage Guidelines.** Both CL and random-LTD only have two parameters that need user tuning: the starting CL difficulty/random-LTD seqlen (\(d_{s}\)/\(r_{s}\)), and the total CL/random-LTD steps (\(T_{c}\)/\(T_{r}\)). 1 And for both CL and random-LTD we find that it's possible to apply a low-cost tuning strategy proposed in previous CL work , where we perform binary search on a very small portion (e.g., 2%) of training to find the smallest \(d_{s}\)/\(r_{s}\) and largest \(T_{c}\)/\(T_{r}\) that don't trigger substantial validation loss fluctuations ("whether the perplexity value becomes larger than 1.3x of the previous best perplexity"). For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices. Tab. 2 summarizes the usage guidelines based on our tuning results, which we believe can be directly applied to any similar models (at least as a very good starting point for any further tuning).

## 4 Evaluation

We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning. Appendix A.5 includes studies of the TokenBypass method on GPT finetuning and pretraining, further demonstrating the advantages of the proposed random-LTD method.

### GPT-3 and GPT-3 MoE pretraining

We use _the Pile_ public dataset  to perform the pretraining of GPT-3 1.3B  (24 layers, 2048 hidden size, 16 attention heads) model. We also pretrain a GPT-3 Mixture-of-Experts (MoE) 6.7B model (24 layers, 1024 hidden size, 16 attention heads, 64 experts on every other layer) following related work . We then perform 0-shot and 10-shot evaluations on 19 tasks to evaluate the model quality of the pretrained models. Detailed experimental setup is described in Appendix A.1.

Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric). For seqtru metric (to our knowledge the only metric previously applied to GPT-3 pretraining), we tune the CL hyperparameters \(d_{s}\) and \(T_{c}\) based on the tuning strategy proposed in previous work . Then for other metrics we use the same hyperparameters without retuning for fair comparison. As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc's 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality. The extensibility of our general CL library enables us to easily apply different CL metrics to this large-scale model pretraining with huge training data, and identify a new CL metric that provides better model quality than existing solution (2)CL_seqtru. Next we pretrain the model with 67% data, comparing the baseline and the best CL metric we find. Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9). On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving.

When applying the proposed random-LTD technique, results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11). To explore whether composing CL and random-LTD could achieve even better data and training efficiency, first we pretrain the model with both techniques under 100% training data. Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework. Next we pretrain the model with 50% data. Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data. Using CL or random-LTD can only recover part of the accuracy loss. On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving.

To better understand how the proposed approach influences the model convergence, Fig. 6 plots the token-wise validation perplexity during pretraining. At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning. On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline. Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot). Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality.

As presented in Sec. 1 and Fig. 2, we also compare baseline and proposed work when using even less data during GPT-3 pretraining (Detailed accuracy results can be found in Appendix A.1). Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier. In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data. Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure , greatly democratizing research and usage of foundation models.

Recent work shows that applying Mixture-of-Experts (MoE) to GPT-style model pretraining could lead to better training efficiency while reaching similar model quality . Thus we also pretrain a GPT-3 MoE 6.7B model (350M base model, together with 64 experts on every other layer) to compare baseline and proposed work. Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16). On the other hand, our approach can further improve MoE model's model quality (case 17), confirming its broad applicability.

### BERT-large pretraining

We use _the Pile_ public dataset  to perform the pretraining of BERT-large  (24 layers, 1024 hidden size, 16 attention heads) model. We then perform GLUE finetuning to evaluate the model quality of the pretrained models. Detailed experimental setup is described in Appendix A.2.

Similar to the GPT-3 case, for CL we first investigate which metric (among 5 metrics we have for BERT model) provides the best model quality by pretraining the model (with 100% data) 5 times. Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality. Next we pretrain with 67% data, comparing the baseline and our best CL metric. Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9). On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.

Tab. 4 case 7, 11, 14 present the case when applying random-LTD only. In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method. However, the time saving is less than data saving because the token dropping mechanism adds a computation overhead at each step. Because the BERT-large is a smaller model than GPT-3 1.3B, this fixed latency overhead has a larger relative impact to the training time. However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.

Tab. 4 case 8 and 15 present the case when applying both CL and random-LTD. At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.

    & CL/ & Data & Time & Avg & Avg \\  & random-LTD & (Milton) & (hours on) & (0.3kde) & 10-shot \\ Case & hyperparameter & (tokens) & (6) & (10) & accuracy \\  (1)baseline & N/A & 300 (1x3) & 260 (1x) & 42.5 & 44.0 \\ (2)CL_seqtru_voc & \(d_{s}=80,T_{c}=110K\) & 300 (1x3) & 257 (101x) & 43.4 & 44.8 \\ (3)CL_seqtru_voc & \(d_{s}=80,T_{c}=110K\) & 300 (1x3) & 288 (105x) & 43.0 & 44.5 \\ (4)CL_voc & \(d_{s}=15,T_{c}=110K\) & 300 (1x3) & 257 (101x) & 42.3 & 44.5 \\ (5)CL_seqtru_voc & \(d_{s}=23\) & 400 (1x3) & 259 (100x) & 43.6 & 44.9 \\ (6)CL_seqtru_voc & same as (4) & 400 (1x3) & 259 (100x) & 43.6 & 44.0 \\ (7)random-LTD & \(r_{s}=128,T_{r}=200K\) & 300 (1x3) & 260 (99x) & 43.7 & 44.9 \\ **(8)CL_seqtru_voc & same as (5) + (7) & 300 (1x3) & 260 (100x) & **43.8** & **45.1** \\  (9)baseline & N/A & 300 (1x3) & 111 (1x) & 42.8 & \\ (10)CL_seqtru_voc & same as (5) + (7) but with & 300 (1x3) & 111 (100x) & **43.5** & \\ (11)random-LTD & \(r_{s}=128,T_{r}=133K\) & 200 (1.5x) & 176 (148x) & 43.1 & 44.8 \\  (12)baseline & N/A & 150 (1x3) & 130 (20x3) & 42.0 & 42.7 \\ (13)CL_seqtru_voc & \(d_{s}=80,T_{c}=55K\) & 150 (23) & 129 (102x) & 42.6 & 43.7 \\  & vdc \(d_{s}=128,T_{r}=100K\) & 150 (23) & 131 (198x) & 42.7 & 43.5 \\ (14)random-LTD & \(r_{s}=128,T_{r}=100K\) & 150 (23) & 131 (198x) & 42.7 & 43.5 \\ (**15)CL_seqtru_voc & same as (13) + (14) & **150 (1x3)** & **130 (20x3)** & 42.8 & 44.0 \\  (16)baseline & N/A & 300 (1x3) & 111 (1x) & 42.8 & \\ (**17)CL_seqtru_voc & same as (5) + (7) but with & 300 (1x3) & 111 (100x) & **43.5** & \\ **random-LTD** & \(2\)\(T_{c}\) and \(T_{c}\) due to batch size & & & & \\   

Table 3: GPT-3 1.3B (case 1 to 15) and GPT-3 MoE 6.7B (case 16, 17) pretraining cost and average evaluation accuracy on 19 tasks. GPT-3 MoE only has 0-shot accuracy due to time constraints. Accuracy results for each single task can be found in Appendix A.1

Figure 6: Validation perplexity during GPT-3 1.3B pretraining, comparing the baseline and the best XYZ Data Efficiency solution under 100% and 50% training data.

*  Another thing to note is that this case also has more time saving than the random-LTD-only case.
*  This is because CL will first truncate the sequences before random-LTD perform the random token selection, and the shorter sequences reduces random-LTD's computation overhead. At 100% data, the composed solution (case 8) improves the GLUE score from the CL-only case, but is worse than the random-LTD-only case. One hypothesis is that for BERT pretraining when composing the two techniques it's preferable to reduce the CL duration, but exhaustively testing all hyperparameters is out of our resource budget and this work's scope.

### GPT-2 and ViT finetuning

To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task  for an already-pretrained GPT-2350M model checkpoint from Huggingface. Given the much smaller training cost, we focus on improving the model quality under the same amount of data. Detailed experimental setup and hyperparameter tuning are described in Appendix A.3. As shown in Tab. 5, segres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric. This is because this finetuning task has much smaller batch size and number of tokens per batch. seqtru will reduce number of tokens per batch, which is less desirable under small-batch training. The small batch also prevents the voc metric to include sufficient number of samples with different vocabulary rarity, limiting its benefit. Applying random-LTD also improves the model quality (case 7). Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices. At last we try another 4 seeds for the baseline, CL best metric, random-LTD, and the CL+random-LTD case. The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case. One hypothesis is that for tasks with such small-scale training data, it's less possible to further improve model quality by composing multiple data efficiency techniques.

We also try finetune the vision transformer (ViT) on both ImageNet (with a 12-layer pretrained ViT) and CIFAR (with a 24-layer pretrained ViT). Due to time/resource limitation, we only test random-LTD for this task. Detailed experimental setup is described in Appendix A.4. As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability.

## 5 Conclusion

Unlike model scale which could reduce in the future with novel architecture, the amount of available training data will increase continuously and irreversibly. Language model pretraining is one of the first to reach a data scale that even training one full epoch is difficult, but sooner or later all machine learning tasks will face the same data efficiency challenge. In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together. This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost. XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.