# One-Step Diffusion Distillation through

Score Implicit Matching

Weijian Luo

Peking University

luoweijian@stu.pku.edu.cn

&Zemin Huang

Westlake University

huangzemin@westlake.edu.cn

&Zhengyang Geng

Carnegie Mellon University

zgeng2@cs.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

zkolter@cs.cmu.edu

&Guo-jun Qi

Westlake University

guojjun@gmail.com

Correspondence to Guo-jun Qi. The project was initiated and supported by the MAPLE lab of Westlake University.

###### Abstract

Despite their strong performances on many generative tasks, diffusion models require a large number of sampling steps in order to generate realistic samples. This has motivated the community to develop effective methods to distill pre-trained diffusion models into more efficient models, but these methods still typically require few-step inference or perform substantially worse than the underlying model. In this paper, we present Score Implicit Matching (SIM) a new approach to distilling pre-trained diffusion models into single-step generator models, while maintaining almost the same sample generation ability as the original model as well as being data-free with no need of training samples for distillation. The method rests upon the fact that, although the traditional score-based loss is intractable to minimize for generator models, under certain conditions we _can_ efficiently compute the _gradients_ for a wide class of score-based divergences between a diffusion model and a generator. SIM shows strong empirical performances for one-step generators: on the CIFAR10 dataset, it achieves an FID of 2.06 for unconditional generation and 1.96 for class-conditional generation. Moreover, by applying SIM to a leading transformer-based diffusion model, we distill a single-step generator for text-to-image (T2I) generation that attains an aesthetic score of 6.42 with no performance decline over the original multi-step counterpart, clearly outperforming the other one-step generators including SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We will release this industry-ready one-step transformer-based T2I generator along with this paper.

## 1 Introduction

Over the past years, diffusion models (DMs)  have shown significant advancements across a broad spectrum of applications, ranging from data synthesis , to density estimation , text-to-image generation, text-to-3D creation , image editing , and beyond . From a high level point of view, diffusion models, also framed as score-based diffusion models, use diffusion processes to corrupt the data distribution. They are then trained to approximate the score functions of the noisy data distributions across varying noise levels.

Diffusion models have multiple advantages, such as training flexibility, scalability, and the ability to produce high-quality samples, making them a favored choice for modern AIGC models. After training, the learned score functions can be used to reverse the data corruption process, which can be implemented by numerically solving the associated stochastic differential equation. Such a data generation mechanism usually requires many neural network evaluations, which leads to a significant limitation of DMs: _the generation performance of DMs degrades substantially when the number of sampling steps is reduced_. This shortcoming restricts the practical deployment of DMs, particularly where quick inference is crucial, such as on devices with limited computational capacities like mobile phones and edge devices, or in applications requiring rapid response times.

This challenge has spurred a variety of approaches aimed at expediting the sampling process of diffusion models while preserving their robust generative capabilities. Distillation approaches, in particular, focus on applying distillation algorithms to transition the knowledge from pre-trained, teacher diffusion models to efficient student-generative models which are capable of producing high-quality samples within a few generation steps.

Figure 1: Time for a Human Preference Study! Could you please tell us which one is better? Hint: the rightmost column is the one-step Latent Consistency Model of PixelArt-\(\); The left two columns are randomly placed, with one generated from our one-step **SIM-DiT-600M** model, and another generated from the 14-step PixelArt-\(\) teacher diffusion model. We put the answer in Appendix B.1.

Some works have studied the diffusion distillation algorithm through the lens of probability divergence minimization. For instance, Luo et al. , Yin et al.  have studied the algorithms that minimize the KL divergence between teacher and one-step student models. Zhou et al.  have explored distilling with Fisher divergences, resulting in impressive empirical performances. Though these studies have contributed to the community in both theoretical and empirical aspects with applicable single-step generator models, their theories are built upon specific divergences, namely the Kullback-Leibler divergence and the Fisher divergence, which potentially restrict the distillation performances. A more general framework for understanding and improving diffusion distillation is still lacking.

In this work, we introduce Score Implicit Matching (SIM), a novel framework for distilling pre-trained diffusion models into one-step generator networks while maintaining high-quality generations. To do so, we propose a wide and flexible class of score-based divergences between the (intractable) score function of the generator model and that of the original diffusion model, for arbitrary distance functions between the two score functions. The key technical insight of this work is that although such divergences cannot be computed explicitly, the _gradient_ of these divergences _can_ be computed exactly using a result we call the _score-gradient theorem_, leading to an implicit minimization of the divergence. This lets us efficiently train models based on such divergences.

We evaluate the performance of SIM compared to previous approaches, using different choices of distance functions to define the divergence. Most relatedly, we compare SIM with the Diff-Instruct (DI)  method, which uses a KL-based divergence term, and the Score Identity Distillation (SiD) method , which we show to be a special case of our approach when the distance function is simply chosen to be the squared \(L_{2}\) distance (though derived in an entirely different fashion). We also show empirically that SIM with a specially-designed Pseudo-Huber distance function shows faster convergences and stronger robustness to hyper-parameters than \(L_{2}\) distance, making the resulting method substantially strong than previous approaches.

Finally, we show that SIM obtains very strong empirical performance in absolute terms relative to past work in the field on CIFAR10 image generation and text-to-image generation. On the CIFAR10 dataset, SIM shows a one-step generative performance with a Frechet Inception Distance (FID) of 2.06 for unconditional generation and 1.96 for class-conditional generation. More qualitatively, distilling a leading diffusion-transformer-based  text-to-image diffusion model results in an extremely capable one-step text-to-image generator which we show is almost lossless in terms of generative performances as teacher diffusion model. Particularly, by applying SIM to PixelArt-\(\), a single-step generator is distilled that reaches an outstanding aesthetic score of \(6.42\) with no performance decline over the original multi-step diffusion model. This remarkably outperforms the other one-step text-to-image generators including SDXL-TURBO  of 5.33, SDXL-LIGHTNING  of 5.34 and HYPER-SDXL  of 5.85. Such a result not only marks a new direction for one-step text-to-image generation but also motivates further studies of distilling diffusion-transformer-based AIGC models in other domains such as video generation.

## 2 Diffusion Models

In this section, we introduce preliminary knowledge and notations about diffusion models and diffusion distillation. Assume we observe data from the underlying distribution \(q_{d}()\). The goal of generative modeling is to train models to generate new samples \( q_{d}()\). The forward diffusion process of DM transforms any initial distribution \(q_{0}=q_{d}\) towards some simple noise distribution,

\[_{t}=(_{t},t)t+G(t)_{t},\] (2.1)

where \(\) is a pre-defined drift function, \(G(t)\) is a pre-defined scalar-value diffusion coefficient, and \(_{t}\) denotes an independent Wiener process. A continuous-indexed score network \(_{}(,t)\) is employed to approximate marginal score functions of the forward diffusion process (2.1). The learning of score networks is achieved by minimizing a weighted denoising score matching objective [70; 67],

\[_{DSM}()=_{t=0}^{T}(t)_{_{0}  q_{0},_{t}|_{0} q_{t}(_{t}|_{0})}\|_{ }(_{t},t)-_{_{t}} q_{t}(_{t}|_{0})\|_ {2}^{2}t.\] (2.2)

Here the weighting function \((t)\) controls the importance of the learning at different time levels and \(q_{t}(_{t}|_{0})\) denotes the conditional transition of the forward diffusion (2.1). After training, the score network \(_{}(_{t},t)_{_{t}} q_{t}(_{t})\) is a good approximation of the marginal score function of the diffused data distribution. High-quality samples from a DM can be drawn by simulating SDE which is implemented by the learned score network . However, the simulation of an SDE is significantly slower than that of other models such as one-step generator models.

## 3 Score Implicit Matching

In this section, we introduce Score Implicit Matching which is a general method tailored for the one-step distillation of score-based diffusion models. We first introduce the problem setup and notations, then introduce a general family of score-based probability divergences and show how SIM can be used to minimize the mentioned divergences. We finally discuss specific choices of the method, such as the choice of distance function, and explore the effect this has on the distillation.

Problem setup.Our starting point is a pre-trained diffusion model specified by the score function

\[_{q_{t}}(_{t})_{_{t}} q_{t}(_{t})\] (3.1)

where \(q_{t}(_{t})\)'s are the underlying distribution diffused at time \(t\) according to (2.1). We assume that the pre-trained diffusion model provides a sufficiently good approximation of data distribution, and thus will be the only item of consideration for our approach.

The student model of interest is a single-step generator network \(g_{}\), which can transform an initial random noise \( p_{z}\) to obtain a sample \(=g_{}()\); this network is parameterized by network parameters \(\). Let \(p_{,0}\) denote the data distribution of the student model, and \(p_{,t}\) denote the marginal diffused data distribution of the student model with the same diffusion process (2.1). The student distribution implicitly induces a score function

\[_{p_{,t}}(_{t})_{_{t}} p_{,t }(_{t}),\] (3.2)

and evaluating it is generally performed by training an alternative score network as elaborated later.

### General Score-based Divergences

The goal of one-step diffusion distillation is to let the student distribution \(p_{,0}\) match the data distribution \(q_{0}\). To do so, we propose to match the diffused marginal distribution \(p_{,t}\) and \(q_{t}\) at all diffusion time levels. We can define such an objective via the following general score-based divergence. Assume \(:^{d}\) is a scalar-valued proper distance function (i.e., a function that obeys \(,() 0\) and \(()=0\) if and only if \(=\)). Given a sampling distribution \(_{t}\) that has larger distribution support than \(p_{t}\) and \(q_{t}\), we can formally define a time-integral score divergence as

\[^{[0,T]}(p,q)_{t=0}^{T}w(t)_{_{t} _{t}}(_{p_{t}}(_{t})-_{q_{t}}( {x}_{t}))}t,\] (3.3)

where \(p_{t}\) and \(q_{t}\) denote the marginal densities of the diffusion process (2.1) at time \(t\) initialized with \(q\) and \(p\) respectively. \(w(t)\) is an integral weighting function. Clearly, we have \(^{[0,T]}(p,q)=0\) if and only if all marginal score functions agree, which implies that \(p_{0}(_{t})=q_{0}(_{t}),\ a.s.\ _{0}\).

### Score Implicit Matching

Based upon this motivation, we would like to minimize the integral score-based divergence between \(p_{}\) and \(q\) in order to train the student model, i.e.,

\[()=^{[0,T]}(p_{},q)=_{t=0}^{T}w(t) _{_{t}_{t}}(_{p_{,t}}( {x}_{t})-_{q_{t}}(_{t}))t,\] (3.4)

where we assume that the distribution \(_{t}\) has no parameter dependence of \(\), such as \(_{t}(_{t})=p_{[]}(_{t})\). Taking the gradient with respect to \(\), we have

\[()=_{t=0}^{T}w(t)_{_{t}_{t}}^{}(_{p_{,t}}( {x}_{t})-_{q_{t}}(_{t}))_{p_{ ,t}(_{t})}t,\] (3.5)

where \(^{}\) denotes the derivative of \(\) wrt. its inputs, i.e. \(_{}()\). Unfortunately, because the score function is not tractable, it is impossible to compute \(_{p_{,t}(_{t})}\) directly, rendering such a direct approach impractical.

Fortunately, a key finding of our paper is if we choose the sampling distribution to the diffused implicit distribution, i.e. \(_{t}=p_{[],t}\) where the notation \([]\) denotes the _stop gradient_ operator that cuts off the parameter dependence of \(\), the loss function (3.4) along with its intractable gradient (3.5) can be minimized efficiently via an gradient-equivalent loss. This relies on our Theorem 3.1.

**Theorem 3.1** (Score-divergence gradient Theorem).: If distribution \(p_{,t}\) satisfies some mild regularity conditions, we have for any score function \(_{q_{t}}(.)\), the following equation holds for all parameter \(\):

\[_{_{t} p_{[],t}} ^{}(_{p_{,t}}(_{t})-_{q_{t}}(_{t }))_{p_{,t}(_{t})}\] (3.6) \[=-_{_{q_{0}} p _{,0}_{t}|_{0} q_{t}(_{t}|_{0})} ^{}(_{p_{[],t}}(_{t})- _{q_{t}}(_{t}))}^{T}_{p_{[ ],t}}(_{t})-_{_{t}} q_{t}(_{t}|_{0}) }.\]

The key observation here is that we replace the intractable _gradient_ of the score function on the left-hand side of (3.6) with a much affordable _evaluation_ of the score function on the right-hand side, the latter of which can be accomplished much more easily using a separate approximation network. This theorem can be proved by using score-projection identity  which was first introduced to bridge denoising score matching with denoising auto-encoders. However, the key in proving Theorem 3.1 is a proper choice of \(\)-parameter (in)dependence by appropriately stopping the gradients shown in this theorem. We provide the detailed proof in Appendix A.1.

Now it is ready to reveal the objective we will use to train the implicit generator \(g_{}\). A direct result of (3.6) is the gradient (3.5) can be realized via minimizing a tractable loss function

\[_{SIM}()=_{t=0}^{T}w(t)_{  p_{x},_{0}=q_{}(),_{t}|_{0} q_{t} (_{t}|_{0})}-^{}(_{t})}^{ T}_{p_{[],t}}(_{t})-_{_{t}}  q_{t}(_{t}|_{0})}t\] (3.7)

with \(_{t}_{p_{[],t}}(_{t})-_{q_{t }}(_{t})\). By Theorem 3.1, this alternative loss has an identical gradient to that of the original loss without the need to access the gradient of the score network.

In practice, we can use another online diffusion model \(_{}(_{t},t)\) to approximate the generator model's score function \(_{p_{[],t}}(_{t})\) pointwise, which was also done in previous works such as Luo et al. , Zhou et al. , and Yin et al. . _We name the distillation method that minimizes the objective \(_{SIM}()\) in (3.7) the Score Implicit Matching (SIM) because the learning process implicitly matches the intractable marginal score function \(_{p_{,t}}(.)\) of the implicit student model with the explicit score function of the pre-trained diffusion model \(_{q_{t}}(.)\)._

The complete algorithm for SIM is shown in Algorithm 1, which trains the student model through two alternative phases between learning the marginal score function \(_{}\), and updating the generator model with gradient (3.7). The former phase follows the standard DM learning procedure, i.e., minimizing the denoising score matching loss function (2.2), with a slight change that the sample is generated from the generator. The resulting \(_{}(_{t},t)\) provides a good pointwise estimation of \(_{p_{[],t}}(_{t})\)The latter phase updates the generator's parameter \(\) by minimizing the loss function (3.7), where two needed functions are provided by pretrained DM \(_{q_{t}}(_{t})\) and learned DM \(_{}(_{t},t)\).

### Instances of Score Implicit Matching.

The previous section introduced the SIM algorithm without choosing a specific distance function \((.)\). Here we discuss different choices and their influence on the distillation process. We also show that in the SIM framework, the SiD can be viewed as a special case.

The Design Choice of Distance Function \((.)\).Clearly, various choices of distance function \((.)\) result in different distillation algorithms. Perhaps the most natural choice of the distance function is a simple squared distance, i.e. \((_{t})=\|_{t}\|_{2}^{2}\). The corresponding derivative term writes \(^{}(_{t})=2_{t}\). In fact, such a loss function recovers the _delta loss_ studied in SiD , in which the authors empirically find that such a loss function works satisfactorily (though through a very different derivation). Thus, SiD is in fact a special case of SIM, though the derivation of SiD there does not suggest how alternative losses may be employed. A direct generalization of the quadratic form is the \(\)-power of the \(\)-norm where \(>1\) and \(\) is even. In this case, the distance function writes \((_{t})=_{t}^{(-1)}\) and the resulting loss function is summarized in Table 4 in Appendix A.3.

The Pseudo-Huber distance function.Different from powered norms, we introduce SIM with the Pseudo-Huber distance function, which is defined with \(()_{t}\|_{2}^{2}+c^{2}}-c\), where \(c\) is a pre-defined positive constant. The corresponding distillation objective writes

\[_{SIM}()=-_{t}}{_{t}\|_{2 }^{2}+c^{2}}}}^{T}_{}(_{t},t)-_{_ {t}} q_{t}(_{t}|_{0})}.\] (3.8)

_In the rest of this paper, we will use the Pseudo-Huber distance as the default choice of the distance, unless specified otherwise._ Due to the limited space, we summarize different choices of distance function and the corresponding loss functions in Table 4 as well as their derivations, along with more discussions in Appendix A.3.

Particularly, unlike SiD (the \(L^{2}\) case in Table 4), with the Pseudo-Huber distance in the SIM, we observe that the vector \(_{t}\) is naturally normalized adaptively by dividing by a squared root of the vector. Such a normalization can stabilize the training loss, resulting in a robust and fast-converging distillation process. In section 4.1, we conduct empirical experiments to show three advantages: robustness to large-learning rate, fast convergence, and improved performances.

### Related Works

Diffusion distillation  is a research area that aims to reduce generation costs using teacher diffusion models. It involves three primary distillation methods: 1) _Trajectory Distillation:_ This method trains a student model to mimic the generation process of diffusion models with fewer steps. Direct distillation ([39; 15]) and progressive distillation ([61; 48]) variants predict less noisy data from noisy inputs. Consistency-based methods ([68; 29; 66; 36; 17]) minimize the self-consistency metric. These require true data samples for training. 2) _Distributional Matching:_ It focuses on aligning the student's generation distribution with that of a teacher diffusion model. Among them are adversarial training methods ([76; 77]) requiring real data for distilling diffusion models. Another important line of methods attempts to minimize divergences like KL () such as Diff-Instruct (DI) [45; 82] and Fisher divergence such as Score identity Distillation (SiD) (), often without needing real data. Though SIM has gotten inspiration from SiD and DI, the gap between SIM and SiD and DI is significant. SIM not only offers solid mathematical foundations which may lead to a deep understanding of diffusion distillation, but also provides substantial flexibility in using different distance functions, resulting in strong empirical performances when using specific Pseudo-Huber distance. 3) _Other Methods:_ Methods like operator learning (), ReFlow (), and FMM  provide alternative insights into distillation. Moreover, many works made outstanding efforts to scale up diffusion distillation to one-step text-to-image generation and beyond[40; 50; 69; 82; 92]

## 4 Experiments

### One-step CIFAR10 Generation

Experiment Settings.In this experiment, we apply SIM to distill the pre-trained EDM  diffusion models into one-step generator models on the CIFAR10  dataset. We follow the same setting as DI  and SiD  to distill the diffusion model into a one-step generator. Details can be found in Appendix B.2. We refer to the high-quality codebase of SiD 3 to reproduce its results by closely referring to its configurations on our devices. We also re-implement the DI under the same experiment settings.

Performances.We evaluate the performance of the trained generator via Frechet Inception Distance (FID) , which is the lower the better. We refer to the evaluation protocols in  for comparison 4. Table 1 and 2 summarize the FID of generative models on CIFAR10 datasets. We reproduce the SiD and the DI with the same computing environments and evaluation protocol as SIM for a fair comparison. Models in the upper part of the table have different architectures or diffusion models from the EDM model, while the models in the lower part of the tables share exactly the same architecture and the teacher EDM diffusion models, which thus are directly comparable.

As shown in Table 1, for the CIFAR10 unconditional generation task, the proposed SIM achieves a decent FID of \(2.06\) with only one generation step, outperforming SiD and DI with the same training compute. It is on par with the CTM and the SiD's official implementation which are trained to fully converge with training costs of hundreds of GPU days. For the class-conditional generation in Table 2, the SIM achieves an FID of 1.96, acting among top-performing models.

The CIFAR-10 generation tasks are much toyish as merely performed with diffusion models of limited capacities on a simple dataset. We will perform experiments to distill from top-performing transformer-based diffusion models for text-to-image generation tasks. We will show that the one-step T2I generator distilled by SIM demonstrates state-of-the-art results over other industry-level models.

   Method & NFE (\(\)) & FID (\(\)) \\
**Different Architecture as EDM Model** & \\   BigGAN  & 1 & 14.73 \\ BigGAN+Tune & 1 & 8.47 \\ StyleGAN2  & 1 & 6.96 \\ MultiHing & 1 & 6.40 \\ FQ-GAN  & 1 & 5.59 \\ StyleGAN2-ADA  & 1 & 2.42 \\ StyleGAN2-ADA+DI  & 1 & 2.27 \\ StyleGAN2 + SMaRT  & 1 & 2.06 \\ StyleGAN-XL  & 1 & 1.85 \\  
**Same Architecture as EDM** & 35 & 1.97 \\ EDM  & 20 & 2.54 \\ EDM  & 10 & 15.56 \\ EDM  & 1 & 314.81 \\ GET  & 1 & 6.25 \\ DFT-Distruct  & 1 & 4.19 \\ DMD (w.o. reg)  & 1 & 5.58 \\ DMD (w.o. KL)  & 1 & 3.82 \\ DMD  & 1 & 2.66 \\ CTM  & 1 & 1.73 \\ CTM  & 2 & **1.63** \\ SiD (\(=1.0\))  & 1 & 1.93 \\ SiD (\(=1.2\)) & 1 & 1.71 \\
**SIM (ours)** & 1 & 1.96 \\   

Table 2: Class-conditional sample quality on CIFAR10 dataset. \(\) means method we reproduced.

   Method & NFE (\(\)) & FID (\(\)) \\
**Different Architecture as EDM Model** & \\   BigGAN  & 1 & 14.73 \\ BigGAN+Tune & 1 & 8.47 \\ StyleGAN2  & 1 & 6.96 \\ MultiHing & 1 & 6.40 \\ FQ-GAN  & 1 & 5.59 \\ StyleGAN2-ADA  & 1 & 2.42 \\ StyleGAN2-ADA+DI  & 1 & 2.27 \\ StyleGAN2 + SMaRT  & 1 & 2.06 \\ StyleGAN-XL  & 1 & 1.85 \\ 
**Same Architecture as EDM** & **Model** \\   EDM  & 35 & 1.82 \\ EDM  & 20 & 2.54 \\ EDM  & 10 & 15.56 \\ EDM  & 1 & 314.81 \\ GET  & 1 & 6.25 \\ Diff-Distruct  & 1 & 4.19 \\ DMD (w.o. reg)  & 1 & 5.58 \\ DMD (w.o. KL)  & 1 & 3.82 \\ DMD  & 1 & 2.66 \\ CTM  & 1 & 1.73 \\ CTM  & 2 & **1.63** \\ SiD (\(=1.0\))  & 1 & 1.93 \\ SiD (\(=1.2\)) & 1 & 1.71 \\
**SIM (ours)** & 1 & 1.96 \\   

Table 1: Unconditional sample quality on CIFAR-10. \(\) means method we reproduced.

Before that let us further look into some advantages of SIM - robustness to large learning rate and faster convergences - over SiD and DI on CIFAR-10, which will shed some light on how distillation methods scale up to more complex tasks with much larger neural networks.

Robustness to large learning rate.We apply SIM, SiD, and DI under the same settings to distill from EDM (details in Appendix) on the CIFAR10 unconditional generation task, with a learning rate of \(1e-4\), and plot the Fretchet Inception Distance (FID)  and the Inception Score  in Figure 2. Both the DI and the SiD are unstable even in the early training phase, while the SIM can steadily converge even with a large learning rate. The potential reason is that SIM naturally normalizes the loss objective to keep its scale from changing abruptly along the training process. _This distinguishes SIM from SiD in practice for training large models, because training modern large models is so expensive that researchers often have few chances to adjust the hyperparameters within budget._

Fast convergence.The second advantage of SIM is its faster convergence than SiD 5. To show this, we follow the same setting as SiD on CIFAR10 unconditional generation. As shown in Figure 2 and Figure 3, under all configurations, the SIM consistently shows better FID and Inception Scores under the same training iterations. Due to page limitations, we put more details in Appendix B.2.

Experiments on CIFAR10 generation show that SIM is a strong, robust, yet fast converging one-step diffusion distillation algorithm. However, the power of SIM is not restricted to a toy CIFAR-10 benchmark. In section 4.2, we apply the SIM to distill a 0.6B DiT ) based text-to-image diffusion model and obtain the state-of-the-art transformer-based one-step generator.

### Transformer-based One-step Text-to-Image Generator

Experiment Settings.In recent years, transformer-based text-to-X generation models have gained great attention across image generations such as Stable Diffusion V3  and video generation such as Sora . In this section, we apply SIM to distill one of the leading open-sourced DiT-based diffusion models that have gained lots of attention recently: the 0.6B PixelArt-\(\) model , which is built upon with DiT model , resulting in the state-of-the-art one-step generator in terms of both quantitative evaluation metric and subjective user studies.

Experiment Settings and Evaluation Metrics.The goal of one-step distillation is to accelerate the diffusion model into one-generation steps while maintaining or even outperforming the teacher diffusion model's performances. To verify the performance gap between our one-step model and the diffusion model, we compare four quantitative values: the aesthetic score, the PickScore, the Image Reward, and our user-studied comparison score. On the SAM-LLaVA-Caption10M, which is one of the datasets the original PixelArt-\(\) model is trained on, we compare the SIM one-step model, which we called the **SIM-DiT-600M**, with the PixelArt-\(\) model with a 14-step DPM-Solver to evaluate the in-data performance gap. We also compare the SIM-DiT-600M and PixelArt-\(\) with other few-step models, such as LCM , TCD , PeReflow , and Hyper-SD  series on the widely used COCO-2017 validation dataset. We refer to Hyper-SD's evaluation protocols to compute evaluation metrics. Table 3 summarizes the evaluation performances of all models. For the human preference study against PixArt-\(\) and SIM-DiT-600M, we randomly select 17 prompts from the SAM Caption dataset and generate images with both PixArt-\(\) and SIM-DiT-600M, then

Figure 2: **Left Two:** Comparison of distillation methods with a batch size of 256 and a learning rate of \(1e-4\). _(Left):_ the FID value. _(Right)_: the Inception Scores. **Right Two:** Comparison of distillation methods with a batch size of 256 and a learning rate of \(1e-5\). _(Left):_ the FID value. _(Right):_ the Inception Scores. All methods are constrained to the same settings except for the distillation methods.

ask the studied user to choose their preference according to image quality and alignments with the prompts. Figure 1 shows a visualization of our user study cases, in which it is difficult to distinguish the images from PixArt-\(\) and SIM-DiT-600M.

Almost lossless one-step distillation.It is surprising that SIM-DiT-600M achieves almost no performance loss compared to teacher diffusion models. For instance, on the SAM Caption dataset in Table 3, SIM-DiT-600M recovers \(99.6\%\) aesthetic score of PixArt-\(\) model and \(100\%\) PickScore. However, the SIM-DiT-600M shows a slightly smaller Image Reward, which can be potentially optimized with more training computes. When compared with leading few-step text-to-image models such as SDXL-Turbo, SDXL-lightning, and Hyper-SDXL, the SIM-DiT-600M shows a dominant aesthetic score with a significant margin, together with a decent Image Reward and Pick Score.

Besides the top performance, the training cost of SIM-DiT-600M is surprisingly cheap. Our best model is trained (data-freely) with 4 A100-80G GPUs for 2 days, while other models in Table 3 require hundreds of A100 GPU days. We summarize the distillation costs in Table 3, marking that SIM is a super efficient distillation method with astonishing scaling ability. We believe such efficiency comes from two properties of SIM. First, the SIM is data-free, making the distillation process not need ground truth image data. Second, the use of the Pseudo-Huber distance function (3.3) adaptively normalizes the loss function, resulting in robustness to hyper-parameters and training stability.

Qualitative comparison.Figure 3 qualitatively compares SIM-DiT-600M against other leading few-step text-to-image generative models. It is obvious that SIM-DiT-600M generates images with higher aesthetic performances than other models. This reflects the quantitative results in Table 3 where the SIM-DiT-600M reaches a high aesthetic score. Both the quantitative and qualitative results showcase the SIM-DiT-600M as the top-performing one-step text-to-image generator. Please check our supplementary materials for more qualitative evaluations.

Figure 4: Visualization of bad generation cases of one-step SIM-DiT model.

Figure 3: Qualitative comparison of SIM-DiT-600M against other few-step text-to-image models. Please zoom in to check details, lighting, and aesthetic performances. Prompts in Appendix B.7.

**Failure Cases of One-step SIM-DiT Model.** Though the SIM-DiT one-step model shows impressive performances, it inevitably has limitations. For instance, we find that the 0.6B SIM-DiT one-step model sometimes fails to generate high-quality tiny human faces and proper human arms and fingers. Besides, the model sometimes generates a wrong number of objects and contents that do not strictly follow the prompts. We believe that scaling up the model size and teacher diffusion models will help to address these issues. Please refer to Figure 4 for visualization of failure cases.

## 5 Conclusion and Future Works

This paper presents a novel diffusion distillation method, the score implicit matching (SIM), which enables to transform pre-trained multi-step diffusion models into one-step generators in a data-free fashion. The theoretical foundations and practical algorithms introduced in this paper can enable more affordable deployment of single-step generators across various domains and applications at scale without compromising the performance of underlying generative models.

Nonetheless, SIM has its limitations that call for further research. First, with the abundance of other powerful pre-trained generative models such as flow-matching models, it is worth exploring to reveal if it is possible to generalize the application of SIM to such a broader family of generative models. Second, even though data-free is an important feature of SIM, incorporating new data in the SIM can further boost the quality of generated images failed by the teacher model. This potential benefit has yet to be explored. We hope this could ease the training of large generative models.