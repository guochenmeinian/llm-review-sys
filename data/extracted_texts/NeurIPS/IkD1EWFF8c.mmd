# Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability

Junqi Gao\({}^{*,1}\), Biqing Qi\({}^{*,2,3,4}\), Yao Li\({}^{,1}\), Zhichang Guo\({}^{1}\), Dong Li\({}^{1}\), Yuming Xing\({}^{1}\), Dazhi Zhang\({}^{1}\)

\({}^{1}\)School of Mathematics, Harbin Institute of Technology

\({}^{2}\)Department of Control Science and Engineering, Harbin Institute of Technology

\({}^{3}\)C\({}^{3}\)I, Department of Electronic Engineering, Tsinghua University

\({}^{4}\)Frontis.AI

\(*\) Equal contribution, \(\) Corresponding author.

{gjunqi97, qibiqing7, mathgzc, arvinlee826}@gmail.com

{yaoli0508, xyuming, zhangdazhi}@hit.edu.cn

###### Abstract

The transferability of adversarial perturbations provides an effective shortcut for black-box attacks. Targeted perturbations have greater practicality but are more difficult to transfer between models. In this paper, we experimentally and theoretically demonstrated that neural networks trained on the same dataset have more consistent performance in _High-Sample-Density-Regions_ (HSDR) of each class instead of low sample density regions. Therefore, in the target setting, adding perturbations towards HSDR of the target class is more effective in improving transferability. However, density estimation is challenging in high-dimensional scenarios. Further theoretical and experimental verification demonstrates that easy samples with low loss are more likely to be located in HSDR. Perturbations towards such easy samples in the target class can avoid density estimation for HSDR location. Based on the above facts, we verified that adding perturbations to easy samples in the target class improves targeted adversarial transferability of existing attack methods. A generative targeted attack strategy named Easy Sample Matching Attack (**ESMA**) is proposed, which has a higher success rate for targeted attacks and outperforms the SOTA generative method. Moreover, ESMA requires only \(5\%\) of the storage space and much less computation time comparing to the current SOTA, as ESMA attacks all classes with only one model instead of seperate models for each class. Our code is available at [https://github.com/gjq100/ESMA](https://github.com/gjq100/ESMA)

## 1 Introduction

Deep learning models exhibits substantial computational capacity in many downstream tasks, but are vulnerable to adversarial attacks . Such attacks tend to be transferable  as well as real-world achievable , which makes it implementable in black-box scenarios. Targeted attacks are known to be more difficult to transfer  compared with non-targeted attacks.

Intuitively, directions and transferability of adversarial attacks are closely related, but their relationship is rarely discussed.  found that samples in low density regions of ground truth distribution are more susceptible to adversarial attacks. They also verified that adversarial perturbations that aligned with the low density direction of ground truth distribution can lead to better transferability. For targeted attack scenarios, the direction that can bring more transferability has not been fully researched yet is not clear.

Findings and arguments.In non-targeted scenarios, directing towards low-density regions of the ground-truth distribution can improve adversarial transferability . However, this approach is not direct enough for targeted attacks. As shown in Figure 1, perturbations pointing at _High-Sample-Density-Regions_ (_HSDR_) of the target domain are more effective than that pointing at low-density regions of the ground-truth distribution, it perturbates samples more directly to the target discrimination region. Moreover, we demonstrate and theoretically prove that neural networks tend to have more consistent outputs in the HSDR of each class, which means that perturbations pointing to the HSDR of the target class are also transferable between different models. However, density estimation for samples in high dimension is challenging. Fortunately, by the definitions of hard samples and easy samples in , we found a fact that helps, in each category, easy samples with smaller losses are more likely to locate in HSDR. We provide theoretical and experimental assurance for this fact. Based on this fact, we can directly perturbate towards such easy samples of the target domain to improve transferability without density estimation.

Related works.Adversarial attacks can be divided into white-box attacks and black-box attacks. In the white-box setting, attackers have access to the model's structure and parameters, while in the black-box setting, they have no such information but only access to the input and output of the model. This is typically the scenario encountered in real-world situations. Black-box attacks include query-based attacks [9; 10; 11] and transfer-based attacks [12; 13; 14]. Conducting too many queries is impractical in real-world applications. In contrast, transfer-based attacks are more feasible as they do not require queries. Transfer-based attacks often require the use of a white-box surrogate model to produce adversarial perturbations. In terms of the way adversarial perturbations are generated, there are two approaches: iterative instance-specific methods and generative methods. Iterative instance-specific methods utilize model gradients to iteratively add perturbations to specified samples (e.g., FGSM , C&W , PGD ). To enhance the transferability of adversarial samples, subsequent work combines these methods with various techniques, such as introducing momentum [12; 13] and considering input transformations [13; 14; 16; 17] during iterations, training auxiliary classifiers [18; 19], or substituting different loss functions [20; 21; 22]. However, instance-specific methods are primarily designed for non-targeted scenarios, often lacking effectiveness in targeted settings. Although relatively good results have been achieved for targeted attacks [20; 21; 23], instance-specific methods still require iteratively creating perturbations for each specified sample, while generators trained for generating adversarial perturbations can be generalized on more samples after training [24; 25; 26; 27].

The leading perturbation generative method currently is TTP , which employs a target-specific GAN to align clean and augmented data from the source domain with the target domain data. However, TTP necessitate training a dedicated generator for each class. This significantly increases storage requirements and training time. In our work, we design a generator for simultaneous attacks on all target classes, which uses class embedding information for each target class. To construct these embeddings, we adopt techniques similar to SNE  to align the surrogate model's output logits for target classes with the generator's embeddings. This enables latent features learned by the surrogate model to guide the construction of well-structured class embeddings. Building on our findings, we train a multi-class perturbation generator to simultaneously perturb the source domain samples towards easy samples with low loss of each target class. Experiments on the ImageNet dataset shown that our method can obtain a better target transfer success rate than TTP, while requiring much less storage. Our method also has certain advantages in training time.

Therefore, our contribution can be summarized as follows:

* We found that deep learning models have more consistent outputs in HSDR of each class, as demonstrated theoretically and through experiments (Section 2.1). This implies that adding

Figure 1: A schematic example of our motivation, plotting the probability density (darker the color represents larger the density) and samples for three populations (orange, cyan, and green). The black line indicates the Bayesian discriminant boundary.

adversarial perturbations pointing to the HSDR of the target class results in better targeted adversarial transferability.
* We experimentally and theoretically verified that easy samples with low loss in early-stopping models are likely to be located in HSDR, which allows us to directly add perturbations pointing to such samples of the target class to improve targeted adversarial transferability (Section 2.2). This avoids density estimation of high-dimensional samples, which is challenging and computationally expensive.
* We introduced the Easy Sample Matching Attack (ESMA) (Section 3). ESMA achieves a higher targeted transfer success rate compared to SOTA generative attacks TTP. Furthermore, it only needs one model to perform attacks for all target classes (Section 4), which requires much less storage space than TTP (only about 1/20 of the storage space in 10 targets case), and requires less training time.

Our findings and conclusions can not only provide guidance for target transfer attacks, but more importantly, they reveal the consistency of deep learning models in the HSDR, and the correlation between sample difficulty and local sample density. Not only that, the design of our multi-target perturbation generative model can provide a new reference for subsequent related research.

## 2 Main conclusions

In this section, we combine illustrative experiments and theoretical proofs to illustrate our two conclusions in turn, and we use the following notations and definitions. \(=^{d}\) is the sample space. The i.i.d. dataset \(S=\{x_{i},y_{i}\}_{i=1}^{n}\) consists of \(n\) sample pairs \((x_{i},y_{i}),1 i n\). Given \(y\), the conditional distribution of \(x\) is \(_{x|y}\). Feature mapping \(f:^{K}\), where \(K\) is the number of class, the class of feature mapping \(f\). Specifically, the parametrized class \(_{}:=\{f_{}:^{K},\}\), where \(\) is the parameter space. Define \(_{}^{}:=\{f_{}^{}= f_{}:f_{}_{}, f_{}(x)=(f_{}(x))\}\), \(\) denotes Softmax-transformation. The Softmax-Cross-Entropy loss is denoted as \(_{sce}(,):^{K}\). Let \(S_{j}:=\{(x,y) S:y=j\}\), \(_{j}:=\{i:(x_{i},y_{i}) S^{j}\}\), \(_{j}:=\{x:(x,y),y=j\}\).

**Definition 1** (\((j,x_{0},r)\)-Local sample density): _Given a class \(j[K]\), \((x_{0},y_{0}) S_{j}\), the \((j,x_{0},r)\)-Local sample density:_

\[_{(j,x_{0},r)}=_{j}}(x_{i} (x_{0},r))}{(x_{0},r)}\]

_where \((x_{0},r):=\{x^{d}:\|x-x_{0}\| r\}\), \((x_{0},r)\) denotes the volume of \((x_{0},r)\)._

Now we use the notation \(_{(j,x_{0},r)}=\{i:(x_{i},y_{i}) S_{j},x_{i} (x_{0},r),(x_{0},y_{0}) S_{j}\}\), \(_{(j,x_{0},r)}=\{x:x_{j},x_{i}(x_ {0},r),(x_{0},y_{0}) S_{j}\}\). For simplicity, we denote \(_{sce}(f_{}(x),y)\) as \((,x)\) and define the local empirical risk \(R_{(j,x_{0},r)}()=_{(j,x_{0},r)}|}_{ i_{(j,x_{0},r)}}(,x_{i})\).

### The Output Consistency of Different Deep Learning Models in HSDR

We conduct an experiment to explain why samples in low-density regions of ground-truth distribution are vulnerable to attacks, and verify the consistency of the output of the deep learning model in HSDR. A dataset consist of \(200\) samples is constructed by sampling from two \(2\)-d Gaussian distributions with equal probability, which is then used to train a neural network for classification. Subsequently, we plot the discriminant region of the Bayes' criterion (which has the minimum error rate) for the known ground truth prior distribution and compared it with the discriminant region of the trained classifier. In addition, we train two other neural networks with different structures and parameter quantities, and plot the output differences of the three neural networks (Figure 2). The discriminant region in Figure 2(a) can reach the Bayesian error rate. The intersection of the two population distributions in the middle of the probability density curve (as shown in Figure 2(b)) belongs to the low-density region of the ground-truth distribution and also to the misclassified region. In such a region, even with minimal expected error, the Bayesian discriminant criterion will classify samples with relatively small probability density of a single population into another class. These samples can be thought of as "outliers". The trained NN classifier discriminates samples into their original categories, causing a difference in decision boundaries from the Bayesian prior classifier and creating small pits as shown in Figure 2(a). Most of the samples around the pit (Figure 2(c)) belong to another class due to its lower population density compared to the other class. Hence, perturbing the samples from this pit (i.e. outliers) towards the discriminant region of the other class becomes easier. If other trained neural networks can accurately classify such outliers, they will also generate similar pits. By adding perturbations pointing these pits to samples from another class, they will be perturbed into such pits, enabling the transfer of adversarial samples between different classifiers, which explained the findings of .

However, as we stated in Figure 1, perturbations towards the low-density regions of the ground-truth distribution are not direct enough for targeted attacks, while perturbations towards the HSDR of the target class are more direct. Combining this with Figure 2(d), we observe that different classifiers have more consistent outputs in the HSDR. With Theorem 1, we theoretically verified this, which also implies that perturbing samples to the HSDR of the target class leads to better targeted adversarial transferability.

**Theorem 1** (Local output consistency): _For a target class \(j[K]\), and two different parametrized class \(_{_{1}}:=\{f_{_{1}}\!:\!_ {1}\!\!_{1}\}\), \(_{_{2}}:=\{f_{_{2}}\!:\! _{2}\!\!_{2}\}\), assume that \(_{(j,x_{0},r)}|}|_{i_{(j,x_{0},r)}}(f_{_{1}}^{_{k}}(x_{i})-f_{ _{2}}^{_{k}}(x_{i}))|\), then for any sample \((x_{0},y_{0}) S_{j}\), in the neighborhood \((x_{0},r)\), with probability at least \(1-\), the following holds:_

\[\|_{x_{x|y}}[f_{ _{1}}^{}(x)-f_{_{2}}^{}(x) x _{(j,x_{0},r)}]\|_{}\] \[(}{_{(j,x_{0},r)}2^{d} r^{d}}}^{2}(r^{d},r)}})+ (2K/)}{_{(j,x_{0},r)}2^{d+1}r^{d}}})+.\]

Remark.Theorem 1 suggests that different models have a more consistent output near the samples in HSDR, specifically, the difference between the outputs has a bound of order \((}{_{(j,x_{0},r)}2^{d}r^{d}}})\). When the number of classes and dimensions are fixed, a larger sample density results in more consistent performance, a weaker consistency within smaller neighborhoods. Note that this local consistency is weakened as the dimension increases, but the relative output consistency between HSDR and LSDR does not change.

However, a very practical problem is that sample points in high-dimensional space tend to be very discrete due to the _curse of dimentionality_, which makes it difficult to find a suitable local neighborhood size to calculate the local density of samples, moreover, in the case of large datasets, calculating local density can be computationally expensive. But the conclusion of the next section can help us find the sample located in the HSDR using early-stopping models without directly calculating the local density of the sample.

### Correlation between Local Sample Density and Sample Difficulty

In this section we illustrate the correlation between sample density and sample difficulty. Hard samples have attracted much attention in various tasks because of their significance for training

Figure 2: (a): Bayesian discriminant region, darker the color indicate higher the confidence probability. (b): Classifier discriminant region, the probability density curves of the two population distributions are plotted, the white part represents the low-density region of ground truth joint distribution, and we boxed out the small pits in the Bayesian misclassified region. (c): Classifier discriminant region with samples. We boxed an outlier. (d): Output differences between three different classifiers, darker purple indicates greater difference in output between different classifiers

convergence and model generalization .  proposed that for the convergent model, the difficulty of the sample can be measured by the loss gradient norm of the sample, easy samples has a relatively small loss gradient norm, while the loss gradient norm of difficult samples is relatively large, especially, those with too large gradient norm may be outliers.

We use the following theoretical analysis to illustrate that for a trained classifier, the samples in the HSDR tend to have a smaller local empirical risk. At the same time, smaller loss gradient norms ensure that losses are more consistent in small neighborhoods, which implies those samples with both smaller loss and loss gradient norms guarantee less local empirical risk in the neighborhood where the sample is located. To illustrate the following conclusions, we make several mild assumptions:

**Assumption 1** (Smoothness assumption): \((,x)\) _satisfies the following conditions of Lipschitz continuous gradient:_

\[\|_{}(_{1},x)-_{}( {w}_{2},x)\| L_{1}\|_{1}-_{2}\|, _{1},_{2},\] \[\|_{x}(,x_{1})-_{x}(,x_{2}) \| L_{2}\|x_{1}-x_{2}\|, x_{1},x_{2}.\]

**Assumption 2**: \(\|_{}(,x)\| G\) _for all \(\)._

**Assumption 3** (Polyak-Lojasiewicz Condition): \(R_{(j,x_{0},r)}()\) _satisfies the PL-condition:_

\[\|_{}R_{(j,x_{0},r)}()\|^{2} (R_{(j,x_{0},r)}()-R_{(j,x_{0},r)}^{*}),\]

_where \(R_{(j,x_{0},r)}()=_{(j,x_{0},r)}|}_{i _{(j,x_{0},r)}}(,x_{i})\) is the local empirical risk, \(R_{(j,x_{0},r)}^{*}=_{}R_{(j,x_{0},r)}()\)._

**Assumption 4**: _For \( i_{(j,x_{0},r)}\), the following holds:_

\[_{}R_{(j,x_{0},r)}(),_{}(,x _{i})\|_{}R_{(j,x_{0},r)}() \|^{2}\]

Assumption 1 and 2 were made in ,  and , where Assumption 2 can actually be derived from Assumption 1 when the input space is bounded, which is usually satisfied. Even a non-convex function can still satisfy Assumption 3 , the inequality in Assumption 3 implies that all stationary points are global minimum , which is proved in recently works  for over-parameterized DNNs. Assumption 4 is reasonable when local neighborhood radius \(r\) is small.

```
0: Initialized weights \(^{1}\), total steps \(T\), sample set \(S\), batch size \(M\) and step size \(_{t}\).
1:for\(t=1 T\)do
2: Randomly sample \(M\) different samples \(B^{t}\) from \(S\), where batch size \(|B_{t}|=M\), corresponding indicator set denote as \(_{B^{t}}\).
3:\(^{t+1}=^{t}-_{i_{B^{t}}}_{ }(^{t},x_{i})\)
4:endfor Return:\(^{T+1}\)
```

**Algorithm 1** Mini-batch SGD

Consider the local empirical risk under Algorithm 1, we use Theorem 2 to illustrate the convergence rate relies on local density.

**Theorem 2** (Optimization relies on local density): _Given a learnable parametrized class \(_{}:=\{f_{}:^{K}, \}\), let \(^{t}\) updated by Algorithm 1, under Assumption 1, 2, 3 and 4, set \(_{t}=\) and \(T}{2}\), then with probability at least \(1-\), holds_

\[R_{(j,x_{0},r)}(^{t+1})-R_{(j,x_{0},r)}^{*}(1- (_{(j,x_{0},r)}))(R_{(j,x_{0},r)}(^{t})-R_{(j,x_{0},r)}^{*})+o(}),\]

_where \((_{(j,x_{0},r)})=\{(,r)} ^{d/2}r^{d}}{(+1)M}-}),0\}\), which is a non-descending function of \(_{(j,x_{0},r)}\)._According to theorem 2, the local empirical risk \(R_{(j,x_{0},r)}()\) will reach a more faster convergence rate in HSDR and, relatively, a relatively slower convergence rate in LSDR, thus for early-stopping models, they has a lower local empirical risk in HSDR. Combined with the following Proposition 1, we show that a smaller gradient norm guarantees a smaller local empirical risk. For overfitting models, the local empirical risk of each neighborhood where samples are located may be very small, but for early-stopping models, this relativity of local empirical risk with respect to local sample density can be maintained.

**Proposition 1**: _Under Assumption 1, given any \((x_{i},y_{i}) S\), for any \(x\) that satisfies \(\|x_{i}-x\| r\), the following holds:_

\[,x_{i})-(,x)|}{r} -r}{2}\|_{x}(,x_{i})\| ,x_{i})}{r}+r}{2}.\]

Remark.Proposition 1 suggests that minimizing the loss leads to a smaller norm of the loss gradient. However, this constraint becomes less tight as the neighborhood radius \(r\) decreases. Compared with the loss itself, the loss gradient norm further guarantees the proximity of local loss, especially when \(r\) is small. Therefore, samples with smaller loss and smaller loss gradient norm are more likely to be in a neighborhood with smaller local empirical risk.

Using the above conclusion, for an early-stopping model, samples with smaller losses and loss gradient norms tend to be located in HSDR. Still in the example in the previous section, we train three models with early-stopping, and the learning rate adjusted with the number of steps, and then plotted Figure 3. The model has more consistent outputs in HSDR. When the loss and gradient norms of a sample are small, the region where the sample is located has smaller local empirical risk. Samples in HSDR have smaller local empirical risk, which is consistent with our theoretical analysis. Additionally, the rightmost graph of Figure 3 shows that samples with smaller loss and gradient norms often locate in HSDR, validating our conclusions.

Therefore, we can determine whether a sample is more likely to located in HSDR or LSDR by evaluating whether it has smaller loss and gradient norms simultaneously. This eliminates the need to calculate local sample densities to find samples in HSDR.

We conduct transfer attack experiments on three baselines on the CIFAR10 dataset, three different models are chosen as victims. Combining our perspectives above, we use algorithm 2 to select the anchor of each target class for guiding the addition of adversarial perturbations. We implement our strategy by simply using squared loss to match anchor points, i.e. minimizing \(\|f(x_{i}^{adv})-a_{_{i}}\|^{2}\) (choosing \(q=10\) and \(=16\) pixels). For comparison, we also use cross-entropy (CE) loss to calculate adversarial examples in the vanilla way, as well as using squared error without a screening mechanism (i.e., randomly selecting the target anchor in the target class) to exclude the influence of different losses. The results are shown in Table 1, our strategy indeed helps to enhance the transferability of target attacks, which further confirms our viewpoints.

Figure 3: The first figure depicts the difference in output of three models under different local sample densities \(_{(y_{i},x_{i},r)}\) divided into different bins. The second figure shows the local empirical risk \(R_{(y_{i},x_{i},r)}\) of samples under different sum of loss and gradient norms (**Loss+Gradnorm**). For Loss+Gradnorm, we first normalize both variables separately and then add them up to eliminate magnitude differences. The third figure represents the local empirical risk of local sample densities in different values. The fourth figure displays the local density under different Loss+Gradnorms. The neighborhood radius \(r\) is taken as \(0.4\).

```
0: Early-stopping classifier \(f_{}\), screening parameter \(q\) and sample set \(S\).
1:for\(i=1 n\)do
2:\(_{i}=_{sce}(f(x_{i}),y_{i}),_{i}=\|_{x}_ {sce}(f(x_{i}),y_{i})\|\).
3:endfor
4: For each class \(k[K]\), select the \(q\)-th smallest loss and the gradient norm among the samples in that class as thresholds \(_{k}^{}\) and \(_{k}^{}\)
5:for\(k=1 K\)do
6:\(A_{k}:=i_{k}:_{i}<_{k}^{},_{i}<_{k}^{}}\), \(a_{k}=|}_{j A_{k}}f_{}(x_{j})\):
7:endfor
```

**Algorithm 2** Target Anchor Screening

## 3 Training Strategy of ESMA

We present our training strategy in this section, as shown in Figure 4. Our training strategy is carried out in two steps, the first step we pre-train the generator's embedding representations, and the second step is to find easy samples of each target class based on our previous conclusions, and then use these samples to guide the generator to generate perturbations from the source domain to the target domain.

Pre-trained Embeddings Guided by Latent FeaturesTo obtain better embeddings that more effectively represents inter-class information, since the latent space of deep learning models often extracts enough class information , we treat the output features of the local model as a set of a priori embedding. In order to make the generator embedding have a similar structure to such a priori embedding, we refer to the idea of manifold learning, using a strategy similar to the SNE algorithm . Let \(_{j}=|}_{i_{j}}l_{i}\), where \(l_{i}=f(x_{i})\), and then we design the following manifold matching loss. Generator embeddings of various class were pulled to the manifold that output features in to obtain embeddings with a better structure.

Next, we denote the generator embedding of class \(j\) as \(e_{j}\). The four matrices \(M^{S_{euc}},M^{E_{euc}},M^{S_{euc}}\), \(M^{E_{euc}}\) satisfy \(M^{S_{euc}}_{i,j}=\|_{i}-_{j}\|\), \(M^{E_{euc}}_{i,j}=\|e_{i}-e_{j}\|\), \(M^{S_{euc}}_{i,j}=_{j}}{\|_{i}\|\|_{j}\|}\), \(M^{E_{euc}}_{i,j}=+e_{j}}{\|e_{i}\|\|e_{j}\|}\), respectively. Let

\[^{S_{}}_{i,j} =}}_{i,j})}{_{k=1}^{K} (M^{S_{}}_{i,j})},^{E_{}}_{i, j}=}}_{i,j})}{_{k=1}^{K}(M^{S_{ }}_{i,j})},\] \[^{S_{}}_{i,j} =}}_{i,j})}{_{k=1}^{K} (M^{S_{}}_{i,k})},^{E_{}}_{i, j}=}}_{i,j})}{_{k=1}^{K}(M^{S_{ }}_{i,k})},\]

then our manifold matching loss is as follows:

    &  &  &  \\  & \(\)VGG16 & \(\)Dense121 & \(\)Res34 & \(\)Dense121 & \(\)Res34 & \(\)VGG16 \\  MIM & 14.325/12.944/**14.44** & 19.815/19.09/**20.00** & 14.800/13.788/**45.13** & 13.415/12.115/**13.83** & 17.136/14.175/**17.27** & 11.236/10.275/**11.25** \\ TIM & 13.539/12.50/**13.80** & 15.239/14.56/**15.75** & 11.249/11.78/\[_{}=_{i,j}_{i,j}^{S_{}} \,_{i,j}^{S_{}}}{_{i,j}^{S_{}}}+_{i,j}_{i,j}^{E_{}}\,_{i,j}^{E_{ }}}{_{i,j}^{S_{}}}\] \[+_{1}[_{i,j}_{i,j}^{S_{}} \,_{i,j}^{S_{}}}{_{i,j}^{S_{ {core}}}}+_{i,j}_{i,j}^{E_{}}\,_{i,j}}{_{i,j}}]+_{2}_{i=1}^{K}\|e^{i}\|.\]

The last regular term is to prevent losses from collapsing. \(_{1}\) and \(_{2}\) are hyperparameters. The pre-trained embedding with our strategy has a larger Euclidean distance and a smaller cosine similarity, which greatly alleviates the previous clustering phenomenon. More detailed analysis and discussion are provided in Appendix B.

Training of Multi-target Adversarial Perturbation GeneratorsAfter pre-training embedding, we freeze the parameters of the embedding layer, combined with our previous conclusions, we select several easy samples in each class to form target anchor sets \(A_{i}\), and match them with the output of our generator in feature space, the generator we use is a Unet with Resblocks. We propose the following objective easy sample feature matching loss to train the multi-class adversarial generator.

\[_{EM}=_{j=1}^{K}(i _{j})}_{i_{j}}d(f(a_{j}),f(_{}(*_{} (x_{i})))),\]

where \(_{}(x)=((x+ ,(x,x-)))\), \(\) is a differentiable Gaussian kernel with size \(3*3\), such smoothing operations have been demonstrated to further improve transferability. The measure of the distance between the two features \(d(,)\) is Smooth L1 loss, which has a unique optimal solution that is not sensitive to exceptional values . Then, our final training strategy can be represented by algorithm 3.

```
1:Generator \(_{}\) with pre-trained embeddings, anchors \(a_{k},k[K]\) and Total epochs \(N\).
2:for\(t=1 N\)do
3:for\(i=1 n\)do
4:\(_{i}(\{1,,K\})\).
5:if\(y_{i}_{i}\)then
6: Random choice an anchor \(a_{j}\) from \(A_{j}\),
7: Gradient descent step on \(_{EM}\).
8:endif
9:endfor
10:endfor
```

**Algorithm 3** Training Strategy of ESMA

## 4 Experiments

In this section, we verify the effectiveness of our method through experiments on ILSVRC2012 dataset . To evaluate the effectiveness of different components in our strategy, we conduct ablation experiments in Section 4.3. Other ablation experiments can be found in Appendix C.

### Experiment Setup

DatasetThe dataset we used comes from the ILSVRC2012 dataset , in which we selected ten classes as the training set, which refer to the ten classes used for TTP training in , they are \(24\), \(99\), \(198\), \(245\), \(344\), \(471\), \(661\), \(701\), \(802\), \(919\). We train generators using images of these ten classes in the training set (\(1300\) images per class), and use the images in the validation set (\(50\) images per class) as the validation dataset for the targeted attack.

ModelsWe use the four networks used in  as source models --ResNet-50  (Res50), VGG-19-bn  (VGG19bn), DenseNet-121  (Dense121), ResNet-152  (Res152). Except the above four models, we also select three models from the Inception series: Inception-v3  (Inc-v3), Inception-v4  (Inc-v4), Inception-ResNet-v2  (IncRes-v2) and a transformer vision model, VIT . As the analysis and fundamental assumptions of this paper are based on the condition of training data from the same distribution, in order to explore the transferability between models trained on training data with different distribution, we conduct additional transfer attack on two adversarially trained models, Inc-v3-adv  and IncRes-v2-ens . Results are shown in E.2. In addition, we also test the adversarial transferability of ESMA on the scenario where the source model is an ensemble of different models, corresponded results are reported in E.1.

BaselinesWe select many iterative instance-specific attack benchmarks, MIM , SI-NIM , TIM , DIM , and advanced iterative instance-specific attacks that are competitive in target setting, Po-TI-Trip , Logit , Rap-LS , FGS\({}^{2}\)M , DMTI-Logit-SU , S\({}^{2}\)I-SI-TI-DIM . Generative adversarial attacks HGN  and TTP  also included in our comparision, where TTP is the current SOTA generative method. As a generative attack, TTP requires training a class-dependent generator specifically for each class.

Parameter SettingFor the parameter settings of different attack methods, we refer to the default settings in , total iteration number \(T=20\), step size \(=/T\), where the \(_{}\) perturbation restriction \(\) is set to \(16\). Momentum factors \(\) is set to \(1\), for the stochastic input diversity in DIM, we set the probability of applying input diversity as \(0.7\). For TIM, the kernel-length is set to \(7\), which is more suitable for targeted attacks. For Po-TI-Trip, the weight of triplet loss \(\) is set to \(0.01\), while the margin \(\) is set to \(0.007\). Referring to , the number of iteration steps of Logit is set to \(300\), and for RAP-LS, we choose \(400\) iteration steps, \(K_{LS}\) is set to \(100\), and \(_{n}\) is set to \(12/255\). Then for TTP, since we used a relatively small training set, we added \(10\) epochs to the original paper  settings to ensure the performance of the model, and the learning rate of Adam optimizer is \(1e-4\) (\(_{1}=.5\), \(_{2}=.999\)). Finally, for our model, we used the AdamW optimizer to train \(300\) epochs with a learning rate of \(1e-4\) (\(350\) for cases where the source model is VGG19bn or Dense121), the value of \(q\) used for sample screening is set to \(2\).

Evaluation SettingWe train the generator on the training set of the selected ten classes, verify the targeted transferality on the validation set, for each target class, we use the \(450\) images of the remaining classes as the source data and perturb them to the target class. For the iterative instance-specific attacks, we directly attack these instances and test their targeted transfer success rate. All methods (including training) were implemented on a single NVIDIA RTX A5000 GPU.

### Results

The results of our experiments, reported in Table 3, ESMA outperforms current SOTA generative attack TTP in targeted transfer success rates. Our approach demonstrates significant advantages in terms of efficiency and effectiveness, with TTP having a parameter count of \(7.84\)M per model compared to ESMA's \(4.40\)M. Additionally, ESMA achieves an average training time that is \(78.4\%\) of TTP. The experimental results also strongly supports our viewpoint.

### Ablation Studies

To validate the effectiveness of the two components in our strategy design, we compared the cases with (w/) and without (w/o) pre-trained embeddings and target anchor screening. The results are shown in Figure 5. In the case without pre-trained embeddings, the performance is significantly weaker than the case with pre-trained embeddings across different training durations. Moreover, the case with target anchor screening shows a noticeable improvement compared to randomly selecting target anchors.

To verify that ESMA can indeed increase the sample density of the target class for the original samples, we use the samples used for adversarial testing in table 3. First, we calculate the sample density of the target class for the clean samples \(_{(,x_{i},r)}\) (here \(r\) is set to \(600\)). Then, we apply ESMA to generate adversarial samples \(x_{i}^{adv}\), and calculate the sample density of the target class for these adversarial samples \(_{(,x_{i}^{adv},r)}\). In both cases, the density calculation results were averaged across all target classes for each sample. We normalize the results by dividing them by the maximum value among all results in both cases. Additionally, we further bin and count the number of test samples in different intervals under different perturbation constraints. The results are shown in Table 2. The

Figure 5: Comparison of targeted attack transfer success rates with (w) pre-trained embeddings and without (w/o) pre-trained embeddings at different training epochs. Src:Res50.

[MISSING_PAGE_EMPTY:10]