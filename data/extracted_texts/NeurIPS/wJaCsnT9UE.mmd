# Sharpness-diversity tradeoff:

improving flat ensembles with SharpBalance

 Haiquan Lu\({}^{1}\)1, Xiaotian Liu\({}^{2}\)2,Yefan Zhou\({}^{2}\)2,Qunli Li\({}^{3}\)3,

**Kurt Keutzer\({}^{4}\)**, **Michael W. Mahoney\({}^{4,5,6}\)**, **Yujun Yan\({}^{2}\)**, **Huanrui Yang\({}^{4}\)**, **Yaoqing Yang\({}^{2}\)**

\({}^{1}\) Nankai University

\({}^{2}\) Dartmouth College

\({}^{3}\) University of California San Diego

\({}^{4}\) University of California at Berkeley

\({}^{5}\) International Computer Science Institute

\({}^{6}\) Lawrence Berkeley National Laboratory

First four authors contributed equally.

###### Abstract

Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance. Building on this, our study investigates the interplay between sharpness and diversity within deep ensembles, illustrating their crucial role in robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data. We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement. The trade-off is justified through our theoretical analysis and verified empirically through extensive experiments. To address the issue of reduced diversity, we introduce SharpBalance, a novel training approach that balances sharpness and diversity within ensembles. Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off. Empirically, we conducted comprehensive evaluations in various data sets (CIFAR-10, CIFAR-100, TinyImageNet) and showed that SharpBalance not only effectively improves the sharpness-diversity trade-off, but also significantly improves ensemble performance in ID and OOD scenarios. Our code has been made open-source.2

## 1 Introduction

There has been interest in understanding the properties of neural networks (NNs) and their implications for robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data . Two properties of particular importance, sharpness (or flatness)  and diversity , have been shown to have a significant influence on performance. In the context of _deep ensembles_, diversity (which measures the variance in output between independently-trained models) is shown to be critical in enhancing ensemble accuracy. Sharpness, on the other hand, quantifies the curvature of local minima and is believed to be empirically correlated with an individual model's generalization ability.

Recent research on loss landscapes (Yang et al., 2021) highlights that a single structural property of the loss landscape is insufficient to fully capture a model's generalizability, and it underscores the importance of a joint analysis of sharpness and diversity. Despite significant efforts in studying sharpness and diversity individually, a gap persists in understanding their relationship, particularly in the context of ensemble learning. Our work seeks to bridge this gap by investigating ensemble learning through the lens of loss landscapes, with a specific focus on the interplay between sharpness and diversity.

**Sharpness-diversity trade-off.** Our examination of loss landscape structure for ensembling revealed a "trade-off" between the diversity of individual NNs and the sharpness of the local minima to which they converge. This trade-off introduces a potential limitation to the achievable performance of the deep ensemble: the test accuracy of individual NN can be improved as the sharpness is reduced, but it simultaneously reduces diversity, thereby compromising the ensembling improvement (evidence in Section 4.2 and 4.4). This trade-off is visually summarized in the lower transition branch in Figure 0(a). We also developed theories (in Section 3) to verify the trade-off. The theoretical results characterizing this phenomenon are visualized in Figure 0(b), and the experimental observation is presented in Figure 0(c). In Section 4.2, we also verified the existence of the trade-off by varying the experimental setting to include different datasets and different levels of overparameterization (e.g., changing model width).

SharpBalance mitigates the trade-off and improves ensembling performance.To address the challenge presented by the sharpness-diversity tradeoff, we propose a novel ensemble training method called SharpBalance. This method aims to simultaneously reduce the sharpness of individual NNs and prevent diversity reduction among them, as demonstrated in the upper transition branch of Figure 0(a). This method is designed based on our theoretical results, which suggest that training different ensemble members using a loss function that aims to reduce sharpness on different subsets of the training data can improve the trade-off between sharpness and diversity. Our theoretical results are summarized in Figure 0(b). Aligned with theoretical insights, our SharpBalance method lets each ensemble member minimize the sharpness objective exclusively on a subset of training data, termed the _sharpness-aware set_. The sharpness-aware set of each ensemble member is diversified by an adaptive strategy based on data-dependent sharpness measures. As shown in Figure 0(c), we verify

Figure 1: **(Sharpness-diversity trade-off and SharpBalance).****(a)** Caricature illustrating the sharpness-diversity trade-off that emerges in an ensemble’s loss landscape induced by the Sharpness-aware Minimization (SAM) optimizer. We propose SharpBalance to address this trade-off. Each black circle represents an individual NN in a three-member ensemble. The distance between circles represents the diversity between NNs and the ruggedness of the basin represents the sharpness of each NN. **(b)** Theoretically proving the existence of the sharpness-diversity trade-off and improvement from SharpBalance, plotting the analytic representation of sharpness and diversity from Theorem 1 and Theorem 2 by changing the perturbation radius \(\) of SAM. SharpBalance achieves a larger diversity for the same level of sharpness. **(c)** Empirical results of verifying sharpness-diversity trade-off improvement from SharpBalance. Each marker represents a three-member ResNet18 ensemble trained on CIFAR-10. Diversity is measured by the variance of individual models’ predictions, and sharpness is measured by the adaptive worst-case sharpness, both defined in Section 2.

that SharpBalance improves the sharpness-diversity tradeoff in training the ResNet18 ensemble on CIFAR10. We conducted experiments on three classification datasets to show that SharpBalance boosts ensembling performance in ID and OOD data.

Our contributions are summarized as follows:

* **Comprehensive identification of the sharpness-diversity trade-off:** This work provides a thorough examination of the phenomenon sharpness-diversity trade-off where reducing the sharpness of individual models can decrease diversity between models within an ensemble. We demonstrate this effect through extensive experiments across various settings,using different sharpness and diversity measures, as well as different model capacities. Our findings show that this trade-off can negatively affect the ensemble improvements.
* **Novel theory:** We prove the existence of the trade-off under a novel theoretical framework based on rigorous analysis of sharpness-aware training objectives (Foret et al., 2021; Behdin and Mazumder, 2023). Our analysis borrows tools from analyzing Wishart moments (Bishop et al., 2018), and characterizes the exact dynamics of training, bias-variance tradeoff, and the upper and lower bounds of sharpness. Notably, our novel theoretical analysis generalizes existing analysis to ensemble members trained with different data, which is the key to analyzing our own training method SharpBalance.
* **Effective approach:** To mitigate the sharpness-diversity trade-off, we introduce SharpBalance, an ensemble training approach. Our theoretical framework demonstrates that SharpBalance provably achieves improvements on the sharpness-diversity trade-off by reducing sharpness while mitigating the decrease in diversity. Empirically, we confirm this improvement and demonstrate that SharpBalance enhances overall ensemble performance, outperforming baseline methods in CIFAR-10, CIFAR-100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and their corrupted versions to assess OOD performance.

We provide a more detailed discussion on related work in Appendix B.

## 2 Background

**Preliminaries.** We use a NN denoted as \(f_{}:^{d_{n}}^{d_{out}}\), where \(^{p}\) denotes the trainable parameters. The training dataset comprises \(n\) data-label pairs \(=\{(_{1},_{1})\,,,(_{n},_{n})\}\). The training loss of NN \(f_{}\) over a dataset \(\) can be defined as \(_{}()=_{i=1}^{n}(f_{ }(_{i}),_{i})\). Here \(()\) is a loss function, which, for instance, can be the cross entropy loss of \(_{2}\) loss. We construct a deep ensemble consisting of \(m\) distinct NNs \(f_{_{1}}\),..., \(f_{_{m}}\). For classification tasks, the ensemble's output is derived by averaging the predicted logits of these individual networks. We use _flat ensemble_ to mean the deep ensemble in which each ensemble member is trained using a sharpness-aware optimization method (Foret et al., 2021), differentiating it from other ensemble approaches.

**Diversity metrics.** Distinct measures of diversity have been proposed in the literature (Laviolette et al., 2017; Fort et al., 2019; Dietterich, 2000; Baek et al., 2022; Ortega et al., 2022; Theisen et al., 2023), and they are primarily calculated using the predictions made by individual models. Ortega et al. (2022) define diversity \(()\) to be the variance of model outputs averaged over the data-generating distribution, which we adopt in the theoretical analysis:

\[()=_{}[(f_{}( ))].\] (1)

In our experiments, diversity is measured using variance defined above, as well as two other widely used metrics in ensemble learning, namely Disagreement Error Ratio (DER) (Theisen et al., 2023) defined in equation (2), and KL divergence (Kullback and Leibler, 1951) defined in equation (11) in the appendices. We show in Section 4.2 that our main claim generalizes to these three metrics in characterizing the diversity between members within an ensemble. Specifically, denote \(\) as the distribution of model weights \(\) after training. Then, the DER is defined as

\[=,^{{}^{}}}[ (f_{},f_{^{{}^{}}})]}{E_{ }[(f_{})]},\] (2)

where \((f_{},f_{^{{}^{}}})\) is the prediction disagreement (Masegosa, 2020; Mukhoti et al., 2021; Jiang et al., 2022) between two classifier \(f_{},f_{^{{}^{}}}\), and \((f_{})\) is the prediction error.

**Sharpness Metric.** In accordance with the definition proposed by Foret et al. (2021), we characterize the _first-order sharpness_ of a model as the worst-case perturbation within a radius of \(_{0}\). Mathematically, the sharpness \(\) of a model \(\) is expressed as follows:

\[(;_{0})=_{\|\|_{2}_{0}}_{}(+)-_{}().\]

Empirically, we measure the sharpness of the NN via the adaptive worst-case sharpness (Kwon et al., 2021; Andriushchenko et al., 2023). The adaptive worst-case sharpness captures how much the loss can increase within the perturbation radius \(_{0}\) of \(\):

\[_{\|T_{}^{-1}\|_{2}_{0}}_{}( {}+)-_{}(),\] (3)

where \(=[_{1},,_{l}]\), and \(T_{}=(|_{1}|,, |_{l}|)\). \(T_{}^{-1}\) is a normalization operator to make sharpness "scale-free", that is, such that scaling operations on \(\) that do not alter NN predictions will not impact the sharpness measure.

**Ensembling.** We characterize the effectiveness of ensembling by the metric called ensemble improvement rate (EIR) (Theisen et al., 2023), which is defined as the ensembling improvement over the average performance of single models. Let \(_{}\) denote the test error of an ensemble; the EIR is then defined as follows:

\[=}[(f_{})] -_{}}{E_{}[(f_{ {}})]}.\] (4)

**Sharpness Aware Minimization (SAM).**SAM(Foret et al., 2021) has been shown to be an effective method for improving the generalization of NNs by reducing the sharpness of local minima. It essentially functions by penalizing the maximum loss within a specified radius \(\) of the current parameter \(\). The training objective of SAM is to minimize the following loss function:

\[_{}^{}():=_{\| \|_{2}}_{}(+)+ \|\|_{2}^{2},\] (5)

where \(\) is the hyperparameter of a standard \(_{2}\) regularization term.

## 3 Theoretical Analysis of Sharpness-diversity Trade-off

This section theoretically analyzes the sharpness-diversity trade-off. The diversity among individual models is quantified using equation (1). The first theorem establishes the existence of a trade-off between sharpness and diversity. The second theorem demonstrates that training models with only a subset of data samples leads to a more favorable trade-off between these two metrics.

**Sharpness and Diversity of SAM.** Assume the training data matrix \(^{n_{} d_{}}\) and test data matrix \(^{n_{} d_{}}\) are random with entries drawn from Gaussian \((0,}})\). Suppose the model weight at the 0-th time step, \(_{0}\), is initialized randomly such that \([_{0}]=\) and \([_{0}_{0}^{T}]=^{2}\) and updated with a quadratic optimization objective through SAM. The learned weight matrix after \(k\) time steps is denoted as \(_{k}\). Let \(^{*}\) be the teacher model (i.e., ground-truth model) such that \(^{*}=^{()}\) and \(^{*}=^{()}\), where \(^{()}\) is the label vector for data matrix \(\). Given a perturbation radius \(_{0}\), the sharpness of a model after \(k\) iteration under the random matrix assumption is defined as

\[(_{k};_{0})=_{}[_{\|\|_{2}_{0}}f(_{_{0}}[_{k}]+;)-f(_{ {}_{0}}[_{k}];)],\]

which is the expected fluctuation of the model output after perturbation over the data distribution. For simplicity, we denote \((_{k}^{SAM};_{0})=_{k}^{SAM}\) for the rest of the paper. We derive an explicit formulation of diversity and upper and lower bounds of sharpness for models optimized with SAM in Theorem 1. Detailed proof can be found in Appendix C.1.

**Theorem 1** (Diversity and Sharpness of SAM).: _Let \(_{0}\) be initialized randomly such that \([_{0}]=\) and \([_{0}_{0}^{T}]=^{2}\) Suppose \(_{k}^{SAM}\) is the model weight after \(k\) iterations of training with SAM on \(^{n_{} d_{}}\) and evaluated on \(^{n_{} d_{}}\). Let \(\) be the step size, \(\) be the perturbation radius in SAM and \(_{0}\) be the radius for measuring sharpness \(_{k}^{SAM}\). Then_

\[(_{k}^{SAM}) =(2k,0)^{2},\] \[^{2}}{2}(}}{d_{}}}-1)^{2}+_{0}\|^{*}\|_{2}-G _{k}^{SAM}^{2}}{2}(}}{d_{}}}+1)^{2}+_{0}\|^ {*}\|_{2},\]_where_

\[(i,j):= _{j=0}+_{k_{1}+k_{2}+k_{3}=i}!k_{2}!k_{3 }!}(-)^{k_{2}+k_{3}}^{k_{3}}(}}{d_{}} )^{m}_{l=1}^{m}(}}{n_{}})^{m-l} (1+1/d_{})N_{m,l},\] \[G=}{2(2k,2)^{3/2}\|^ {*}\|_{2}}m=k_{2}+2k_{3}+jN_{m,l}=\]

To provide a clearer understanding of the relationship between sharpness and diversity, Figure 2 presents a trade-off curve between these two metrics. The estimated sharpness and diversity are displayed on the \(x\) and \(y\) axes, respectively. Each point in the plot corresponds to a model trained using SAM with a different \(\) value, showcasing the outcome of varying perturbation radius. In these experiments, we evaluated the sharpness and diversity of the models empirically and compared them to the estimates obtained using Theorem 1. The soundness of Theorem 1 and tightness of the derived bounds are further supported by empirical evidence, as depicted in Figure 2. Further verification results supporting our theoretical analysis are provided in Appendix C.3

**Training with Data Subsets.** Assume \(\) is partitioned into \(S\) horizontal submatrices, such that \(=[_{1}^{T},_{2}^{T},,_{S}^{T}]^ {T}\). We show in Theorem 2 a similar analysis of the sharpness and diversity of ensembles for which each model is trained with a submatrix. Under this setting, we first selected a subset of data \(_{s}\) uniformly at random and then train the model with the selected subset with SAM.

**Theorem 2** (Diversity and Sharpness when Models are Trained on Subsets).: _Suppose the training data matrix \(\) is partitioned into \(S\) horizontal submatrices. Let \(_{0}\) be initialized randomly such that \([_{0}]=\) and \([_{0}_{0}^{T}]=^{2}\). Let \(_{k}^{SharpBal}\) be the model weight trained with SAM for \(k\) iterations on the submatrix \(_{s}^{}}{S} d_{}}\), selected uniformly at random, and evaluated on test data \(^{n_{} d_{}}\). Let \(\) be the step size, \(\) be the perturbation radius in SAM, \(_{0}\) be the radius for measuring sharpness \(_{k}^{SAM}\), and \(r=}}{Sd_{}}\). Then_

\[(_{k}^{SharpBal})= ^{}(2k,0)^{2}\] \[+}S}(^{}(2k,0)-^{ }(k,0)^{2})\|^{*}\|_{2}^{2},\]

_and_

\[_{k}^{SharpBal}(_{0})^{2}}{2}(}}{d_{}}}+1)^{2}+}{S}\| ^{*}\|_{2},\]

_where_

\[C= S^{}(2k,2)+2rS(S-1)^{}(2k,1)+2S(S-1)^{ }(k,2)^{}(k,0)\] \[+r(1+r)S(S-1)^{}(2k,0)+2S(S-1)^{}(k,1)^{ }(k,1)\] \[+r(1+r)S(S-1)(S-2)^{}(k,0)^{2}+r^ {2}S(S-1)(S-2)^{}(2k,0)\] \[+3rS(S-1)(S-2)^{}(k,0)^{}(k,1)+r^{2}S(S-1)(S- 2)(S-3)^{}(k,0)^{2},\] \[^{}(i,j):= _{j=0}+_{k_{1}+k_{2}+k_{3}=i}!k_{2}!k _{3}!}(-)^{k_{2}+k_{3}}^{k_{3}}(}}{Sd_{ }})^{m}_{l=1}^{m}(}}{n_{}})^{m-l }(1+}})N_{m,l},\]

_where \(m=k_{2}+2k_{3}+j\). \(N_{m,l}=\) is the Narayana number._

The proof of Theorem 2 is provided in Appendix C.2. Similar experimental validations are conducted to verify Theorem 2, with results also presented in Appendix C. The main insight from Theorem 2 is

Figure 2: **(Theoretical vs. Simulated sharpness-diversity trade-off).** This figure illustrates the relationship between sharpness (upper and lower bounds) and diversity as predicted by Thereom 1 and as observed in simulations. Note that the upper and lower bounds correspond to the sharpness values plotted along the x-axis, with the upper bound positioned to the right and the lower bound to the left. Also, note that the bounds provided are for the expected sharpness, which means that random fluctuations can cause the simulation results to move beyond these bounds.

that training models on a randomly selected data subset offers a better trade-off between sharpness and diversity compared to training on the complete dataset. This idea is further illustrated in Figure 0(b), where we compare the sharpness upper bound and diversity of models trained on the full dataset (labeled as SAM) and those trained on subsets (labeled as SharpBalance). The results demonstrate that SharpBalance achieves a more favorable trade-off. For a given level of sharpness, deep ensembles with models trained on subsets of the data exhibit higher diversity compared to those trained on the entire dataset. This indicates that minimizing sharpness on randomly sampled data subsets for each model within the ensemble promotes the diversity among the models, thereby enhancing the sharpness-diversity trade-off.

## 4 Experiments

In this section, we describe our experiments. In particular, following Section 4.1 where we describe our experimental setup, in Section 4.2, we provide an empirical evaluation across various datasets to explore the trade-off between sharpness and diversity. We also examine how this trade-off changes with different levels of overparameterization. Then, in Section 4.3 and 4.4, we elaborate the SharpBalance algorithm and compare its performance with baseline methods.

### Experimental setup

Here, we describe the experiment setup for Section 4.2. Each ensemble member is trained individually using SAM with a consistent perturbation radius \(\), as defined in equation (5). We adjust \(\) across different ensembles to achieve varying levels of minimized sharpness. Sharpness for each NN was measured using the adaptive worst-case sharpness metric, defined in equation (3). The sharpness measurement was done on the training set, using 100 batches of size 5. The diversity between NNs is measured using DER defined in equation (2). The diversity between ensemble members is tested on OOD data. We evaluated this trade-off using a variety of image classification datasets, including CIFAR-10, CIFAR-100 (Krizhevsky, 2009), TinyImageNet (Le and Yang, 2015), and their corrupted versions (Hendrycks and Dietterich, 2019). For the setup of Section 4.4, we used the same datasets and architecture. The hyperparameters of the baseline methods has been carefully tuned. The hyperparameters for conducting the experiments are detailed in Appendix D.

### Empirical validation of Sharpness-diversity trade-off

We provide empirical observation to validate and explore the sharpness-diversity trade-off. Figure 3 presents the validation of observing the trade-off phenomenon on training ResNet18 ensembles on CIFAR10 applying three different metrics to measure the diversity. The results demonstrate that this trade-off phenomenon generalizes to the three diversity metrics defined in Section 2. Figure 4 presents the validation on three different datasets. In the following empirical study, DER will be the primary metric for measuring diversity of models.

Figure 3: (**Varying diversity measure in empirical study).** Three different metrics are employed to measure the diversity of individual models within an ensemble, i.e., Variance in equation (1), DER in equation (2), and KL divergence in equation (11). The results of the three metrics show consistent trends, demonstrating the sharpness-diversity trade-off: lower sharpness is correlated with lower diversity. The experiment is conducted by training a three-member ResNet18 ensemble on CIFAR10.

Experimental results obtained with the other two metrics are available in Appendix E. The three sets of results first verify that minimizing individual member's sharpness indeed reduces diversity. This is confirmed by the consistent trends of markers moving from upper right to lower left. Second, the first row of Figure 4 shows that an ensemble with decreased diversity (lower in \(y\)-axis) shows a lower ensemble improvement rate (from red to blue), highlighting the negative impact of this trade-off. Lastly, the second row shows when the sharpness of the individual model is reduced (lower in \(x\)-axis), the individual model's OOD accuracy is improved (from blue to red), demonstrating the benefits of minimizing sharpness. We verify the robustness of the phenomenon by measuring the sharpness and diversity using different metrics in Appendix E.

Figure 5 illustrates the trade-off curves as the overparameterization level of the model is adjusted by changing width or sparsity (introduced using model pruning). This visualization confirms that the trade-off is a consistent phenomenon across models of different sizes, and the ensemble provides less improvement (blue color) at the lower left end of each trade-off curve. It also highlights that models with smaller or sparser configurations show a more significant trade-off effect, as evidenced by the steeper slopes and higher coefficient values of the linear fitting curves. As sparse ensembles are now being used to demonstrate the benefits of ensembling for efficient models (Liu et al., 2022; Diffenderfer et al., 2021; Whitaker and Whitley, 2022; Kobayashi et al., 2022; Zimmer et al., 2024), addressing the conflict between sharpness and diversity becomes particularly crucial.

### Our SharpBalance method

Here, we describe the design and implementation of our main method, SharpBalance. Figure 6 provides an overview. Our approach is motivated by the theoretical analysis in Section 3, which suggests that having each ensemble member minimize sharpness on diverse subsets of the data can lead to a better trade-off between sharpness and diversity. SharpBalance aims to achieve the optimal balance by applying SAM to a carefully selected subset of the data, while performing standard optimization on the remaining samples. More specifically, for each ensemble member NN \(f_{_{i}}\), our method divides the entire training dataset \(\) into two distinct subsets: sharpness-aware set \(^{i}_{}\) and normal set \(^{i}_{}\). The model is trained to optimize the sharpness reduction objective on \(^{i}_{}\)

Figure 4: **(Empirical observations of sharpness-diversity trade-off). The identified trade-off shows that while reducing sharpness enhances individual model performance, it concurrently lowers diversity and thus diminishes the ensemble improvement rate. _First row_: the color encoding represents the ensemble improvement rate (EIR) defined in equation (4), from red to blue means ensembling improvement decreases. _Second row_: the color encoding represents the individual ensemble member’s OOD accuracy, from blue to red means individual performance becomes better. Each marker represents a three-member ResNet18 ensemble trained with SAM with a different perturbation radius.**

while it optimizes the normal training objective on \(^{i}_{}\). These training objectives are denoted as \(^{}_{^{i}_{}}(_{i})\) and \(_{^{i}_{}}(_{i})\), respectively. The \(^{i}_{}\) is selected by an adaptive strategy from the whole dataset \(\): it is composed of the union of samples that are deemed "sharp" by all other members of the ensemble except the \(i\)-th. Specifically, for each model, we pick the subset of data samples with the top-\(k\)% highest "per-data-sample sharpness." Then, we take the union of all such subsets expect the \(i\)-th for creating the subset \(^{i}_{}\). This partition of data samples can be efficiently computed in parallel as there is no sequential dependency on the training of the ensemble members. However, SharpBalance can be easily adapted for sequential training if memory constraints permit training only one model at a time.

**Per-data-sample sharpness.** This metric is designed to efficiently assess the sharpness of a model for individual data samples. For each data point \((_{j},_{j})\), sharpness is quantified using the Fisher Information Matrix (FIM), which is expressed as \(_{}(f_{}(_{j}),_{j})_{}(f_{}(_{j}),_{j})^{T}\). Following a well-established approach (Bottou et al., 2018), we approximate the trace of the FIM by computing the squared \(_{2}\) norm of the gradient: \(\|_{}(f_{}(_{j}),_{j})\|_{2}^{2}\). Other common sharpness metrics, such as worst-case sharpness, trace of the Hessian, or Hessian eigenvalues, are computationally slightly more expensive to approximate (Yao et al., 2020, 2021), but are expected to lead to similar results.

Figure 5: **(Sharpness-diversity trade-off in models varying overparameterization levels).** Different types of markers represent models with varying degrees of overparameterization, determined by changing the model width (a) or sparsity (b). Each marker represents a three-member ensemble trained with SAM with a different perturbation radius. The \(\) reflects the rate of decline in the trade-off curve, calculated via applying linear fitting over the ensembles at each level of overparameterization. A higher \(\) points to a steeper decline in the trade-off. Ensembles with narrower widths or increased sparsity display more pronounced trade-off effects. The model used in ResNet18 and the dataset is CIFAR-10.

Figure 6: **(System diagram of SharpBalance).** Each ensemble member \(f_{_{i}}\) optimizes the sharpness reduction objective on subset \(^{i}_{}\) and the normal training objective on \(^{i}_{}\). \(^{i}_{}\) is formed by selecting data samples from \(\) that significantly affect the loss landscape sharpness of other ensemble members.

### Empirical evaluation of SharpBalance

We evaluate SharpBalance by benchmarking it against both a standard Deep Ensemble, trained using SGD, and a Deep Ensemble enhanced with SAM. The results are presented in Figure 7 for CIFAR-10, CIFAR-100, and TinyImageNet. The comparison between the middle and left bars shows that SAM enhances individual model performance by reducing sharpness. However, this reduction in sharpness also diminishes the overall ensemble effectiveness by lowering diversity, exemplifying the sharpness-diversity trade-off discussed in Section 4.2. Further comparison between the right and middle bars shows that SharpBalance maintains or improves individual performance while improving ensemble effectiveness.

We also evaluate SharpBalance on different ensemble sizes. As shown in Figure 8, SharpBalance demonstrates more pronounced empirical improvements as the number of ensemble models increases. The accuracy difference between SharpBalance and the baseline methods becomes more significant, especially on corrupted data. Specifically, SharpBalance outperforms the baselines by up to 1.30% when ensembling 5 models on CIFAR100-C dataset.

To further evaluate SharpBalance, we provide corroborating results in Appendix F, which includes:

* We evaluate SharpBalance on different severity of the corruption on CIFAR10-C, CIFAR100-C and Tiny-ImageNet-C. SharpBalance increasingly outperforms the baselines as the severity of the corruption increases. We also evaluate the proposed method using uncertainty metrics such as negative log-likelihood and expected calibration error.
* We further evaluate SharpBalance on other model architectures and tasks, such as WideResNet, ViT, and ALBERT (Lan et al., 2020) on language tasks.
* We compare our method of measuring sharpness with another method of measuring the curvature of the loss around a data point (Garg and Roy, 2023) and show the strong correlation between these two methods.
* We further compare SharpBalance with ensemble baseline EoA (Arpit et al., 2022), an improved version of SAM (for which individual models in an ensemble are trained with different \(\) values) and GSAM (Zhuang et al., 2022). Results show that SharpBalance can significantly outperform the baselines.

Figure 7: **(Main results: SharpBalance improves the overall ensembling performance and mitigates the reduced ensembling improvement caused by sharpness-diversity trade-off). The three-member ResNet18 ensemble is trained with different methods on three datasets. The first row reports the OOD accuracy and the second row reports the ID accuracy. The lower part of each bar with the diagonal lines represents the individual model performance. The upper part of each bar represents the ensembling improvement. The results are reported by averaging three ensembles, and each ensemble is comprised of three models.*** We demonstrate that, compared to training a deep ensemble with SAM, our method adds only minimal computational cost. The extra time complexity is dominated by the computation of Fisher trace for evaluating per-sample sharpness, which empirically increases the training time by 1%.

## 5 Conclusion

Our theoretical and empirical analyses demonstrate the existence of a sharpness-diversity trade-off when sharpness-minimization training methods are applied to deep ensembles. This leads to two main insights that are relevant for improving model performance. First, reducing the sharpness in individual models proves to be beneficial in enhancing the performance of the ensemble as a whole. Second, the accompanying reduction in diversity suggests that popular ensembling methods have limitations, and also highlights the potential for more sophisticated designs that promote diversity among models with lower sharpness. These results are particularly timely, given recent theoretical work on characterizing ensemble improvement (Theisen et al., 2023). In response to these findings, we have proposed SharpBalance, which "diagnoses" the training data by evaluating the sharpness of each sample and then fine-tunes the training of individual models to focus on a diverse subset of the sharpest training data samples. This targeted approach helps maintain diversity among models while also reducing their individual sharpness. Extensive evaluations indicate that SharpBalance not only improves the sharpness-diversity trade-off but also delivers superior OOD performance for both dense and sparse models across various datasets and architectures when compared to other ensembling approaches.

**Limitations.** One limitation of the study is that our theoretical analysis in Section 3 relies on the assumption that the data matrices \(,\) follow a Gaussian distribution and assumed the optimization objective to be quadratic, which may not always hold in practice. Despite the potentially strong assumptions, our empirical findings in Section 4 show that the conclusions remain robust in real-world datasets with various model architectures. This suggests the insights discovered in our study are applicable to a wider range of real-world scenarios, beyond just those strictly adhering to the Gaussian assumption. Nevertheless, future research could explore how such assumptions can be relaxed and extend the theoretical analysis to a weaker condition.

**Acknowledgements.** Michael W. Mahoney would like to acknowledge the UC Berkeley CLTC, ARO, IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of this work. Kurt Keutzer would like to acknowledge support from Berkeley Deep Drive. Yaoqing Yang would like to acknowledge support from DOE under Award Number DE-SC0025584, DARPA under Agreement

Figure 8: SharpBalance **achieves more pronounced improvement when increasing the number of ensembling models.** “EIR” represents the ensemble improvement rate, which is defined in Section 2, the larger the better. \(x\)-axis represents the number of individual models in one ensemble.

number HR00112490441, and Dartmouth College. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.