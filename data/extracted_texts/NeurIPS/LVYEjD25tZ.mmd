# Supervising Variational Autoencoder Latent Representations with Language

Thomas Lu, Aboli Marathe, Ada Martin

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

ttlu, abolim, gamartin@cs.cmu.edu

Equal Contribution

###### Abstract

Supervising latent representations of data is of great interest to modern multi-modal generative machine learning. In this work, we propose two new, simple methods to use text to condition the latent representations of a VAE and evaluate them on a novel conditional image-generation benchmark task. We find that the applied methods can be used to generate highly accurate reconstructed images through language querying. Our methods are quantitatively successful at conforming to textually-supervised attributes of an image while keeping unsupervised attributes constant. At large, we present critical observations on disentanglement between supervised and unsupervised properties of images and identify common barriers to effective disentanglement.

## 1 Introduction and Motivation

VAEs are a popular modern recipe for the encoding of complex data. However, the latent features discovered by a standard VAE are not directly controllable nor interpretable. The notion of disentangling the latent space involves separating recognizable features such that different portions of the latent vector contain independent generative parameters. This has applications such as debiasing, bias analysis, and many generation subtasks. Van et al. proposes a method for supervising the latent space of generative adversarial networks to ensure they correlate to some known feature of the input. As a result, they can modify the latent representation of an input image along one of these supervised dimensions to create an alternate version of that image with that property modified. In this report, we extend this supervision strategy to a VAE model over textual descriptions of features.

Similarly, we provide a second method to condition VAE models by taking advantage of the information-theoretic constraints arising from the objective function. This method only utilizes simple vector concatenation as the main mechanism for conditioning.

Figure 1: Feature VAE with arbitrary color inputs. The first two on the left were recolored to the prompt “Dolphin”, **which is not in our dataset**. The last was recolored to the prompt “Carnegie Mellon”, which has red school colors, also not in our dataset.

We explore the extension of these strategies to language supervision on an algorithmically-generated colored shape dataset. In particular, we evaluate on the task of image recoloring. The model receives an image and a natural language target color as input. It must then output the image recolored to the target color. We find that both of our methods can separate labeled feature information from unlabeled feature information within the latent space, and can be used to generate highly accurate recolored images by directly inputting a language query at evaluation time. While other works have tackled this same task, we hope that this work can serve as an initial inquiry into new directions of performing text-conditioned image generation. The code for this work is released in our GitHub repository2. Our models were designed for ease of implementation and experimentation without extensive computational resources. All code was run using a 2018 MacBook Air.

The main contributions we hope to highlight in this work are as follows:

1. We introduce two simple methods for text-based conditional image generation using VAEs. The first, dubbed SLt-VAE, linearly predicts text embeddings from the latent space, whereupon a simple linear algebraic method allows conditional modification of the input image.
2. The second method, dubbed Feature VAE, concatenates the text embedding features to the latent representation prior to decoding. Due to information theoretic principles, the typical VAE loss function naturally encourages the model to separate any information encoded by the text embedding and information encoded in the latent vector. Replacement of the concatenated vector with the latent vector corresponding to an alternate textual description allows arbitrary recoloring of the image.
3. We provide analysis and preliminary experiments to improve these methods and further our understanding of VAE disentanglement.
4. We provide a simple dataset for simple disentanglement tests. This dataset sports a small download size, many labeled auxiliary features, and a low-resolution size for low-compute experimentation.

## 2 Related Work

Variational auto-encoders  were introduced in 2013 as an efficient learning algorithm for modeling continuous latent variables. These networks were quickly adopted for tasks spanning generation , anomaly detection , and unsupervised learning paradigms .

Newer models attempted to build on the VAE through specific learning mechanisms [27; 31] attempting to exploit the naturally regularized latent space for solving more complex tasks. A recent work  improved performance on sentiment analysis by training a classifier on the intermediate representation, which was divided between known properties and latent variables, which is a method bearing some similarities to our baselines.

Figure 2: Diagrams for proposed models. Trained parameters are in green.

Several works have delved into understanding auto-encoders for robust learning of disentangled representations. One such VAE introduced in 2017 was \(\)-VAE  which presented a method to learn interpretable factorized latent representations in a completely unsupervised manner by increasing the KL divergence loss term to control the information of the latent representation, encouraging features to avoid redundant information. Many works have furthered the concept of disentanglement and conditional image manipulation, such as StyleGANs , CCVAEs , and various adversarial methods .

Our work on the SLt-VAE is most immediately informed by Van et al., which adds an additional loss to a GAN encouraging its latent space to align with some supervised information about a given sample. SLt-VAE adapts this method to text through the use of pretrained embedding models such as CLIP , as well as to VAEs, allowing them to operate over any input image. We also rework the image manipulation formula and remove the orthogonality constraint.

Our F-VAE bears some similarities to LORD , which also separates labeled class and content information, but combines both in the same generator. Other works, such as ZeroDIM, have also found positive results by implementing CLIP and its powerful generalization abilities . However, our model simply uses the CLIP embedding directly rather than training additional classifiers for each attribute of interest. We provide some brief experiments exploring some major differences between these models, such as using an auxiliary property predictor, the weakly supervised case using said property predictor, and an additional adversarial loss.

We also release code for our shape dataset, which can be generated from a script in our repository with tunable parameters for the occurrence rate of each feature. This dataset, described below, produces images of simple shapes with many known, labeled variables. The key property of this dataset is that one natural language feature, the color, contains over 950 labels, and is the main subject of our experimentations. It is most similar to the dSprites dataset , which also produces synthetic shapes for disentanglement. However, our dataset contains natural language descriptions of colors and numerous additional complex features and shapes. The lack of definitive feature labels and the large resolution prevented us from exploring other traditional disentanglement datasets like FFHQ . Because we primarily used this dataset for color-replacement experiments, we designed specific metrics for color-replacement evaluation which were attuned. Other methods exist to evaluate disentangled representations, such as DCI , SAP , and MIG , which cover roughly the same metrics of disentanglement and informativeness as our methods.

## 3 Dataset

The dataset we use is a custom algorithmically generated set of images of shapes. These images vary in primary shape (3- to 8-sided polygons, stars, and arrows), color, scale, rotation, skew, translation, hatching, and the inclusion/exclusion of a shadow. These features all occur independently with a different probability per feature. The exact generation procedure can be found in the attached GitHub repository 3. Our goal is to provide a simple dataset for preliminary disentanglement results, which can later be modified for other important factors such as feature correlation and noise.

The intent was to create a large set of properties such that we could supervise a subset of these properties and allow the model to discover the other properties, simulating the desired real-world setting in which some (but not all) of the latent space is specified by the model builder. Examples of generated images can be found in Figure 3.

To extend this to the language supervision task, our generation process samples from the xkcd colorset, which contains the names of 954 different colors based on natural language usage. This large, discrete set of colors presents a unique challenge that finds a natural solution in continuous language embeddings. Additionally, recoloring allows for simple metrics on structural retention and recoloring accuracy.

## 4 Methods

### Vanilla \(-\)Vae

The default VAE used as a baseline in this work is a CNN which takes in an input of size \(64 64 3\), performs four applications of strided convolution/batch normalization / ReLU until predictingthe means and standard deviations of a latent vector. Then, similarly, four applications of strided deconvolution/batch normalization / ReLU are applied (sigmoid at the last layer) to reproduce the input image. The loss function is, as in Higgins et al.

\[ D_{KL}(N(,)||N(0,1))+E[(X_{recon}-X)^{2}]\]

The \(-\)VAE is capable of some levels of disentanglement over each dimension, as shown in figure 4. However, which dimension controls which feature is unknown, and each dimension may affect multiple features. The task at hand is to train a model such that our target feature is in a known section of the latent space and can be manipulated to any other value without affecting the other properties of the image.

### Language-based Supervision

For our experiments, we embed the natural language phrase for each color using encoders such as USE , CLIP , or Word2Vec . In general, CLIP embeddings led to more accurate recoloring compared to the other language embedding methods. For USE and CLIP, we were able to experiment with multiple prompts to obtain the language embedding. The prompt leading to the most accurate recoloring was found to be "This is a BLANK colored shape."

### SL1-VAE: Supervised Latent VAE Representations

Our first baseline strategy is an adaptation of the strategy proposed by Van et al. to ensure that our hidden vector \(z\) can be linearly mapped to the ground-truth properties about the input image, \(y\): in this case, the language embedding. First, as with a standard VAE, a latent encoding \(z\) is obtained from the input image. We then train a linear matrix \(W\) such that \(=Wz\) estimates the supervised

Figure 4: Chosen dimensions of \(=10\) VAE roughly corresponding to Red-Green axis (top) and Hatchedness (bottom). Note that dimensions were found by hand and correlate with other features.

Figure 3: Random generated shapes from the dataset, showing color, shape, hatchedness, shadow, and skewness

features \(y\). The entire loss we minimize is:

\[E_{}[(Wz-y)^{2}]+ D_{KL}(N(,^{2}),N(0,1))+E_{ }[(X_{recon}-X)^{2}]\]

Where \(z\) and \(\) are obtained from \(\) and \(\) as per a standard VAE. \(\) and \(\) are tune-able hyperparameters. A diagram of this method is shown in 2.

At evaluation time, we wish to minimally modify the latent representation while maximally conforming to a desired color, so we minimize:

\[\|W(+)-\|_{2}^{2}+^{T}\]

In this case, \(\) is the desired text embedding, \(\) is the original image's latent representation (usually not resampled at test time), \(W\) is the learned matrix which maps from \(\) to \(\), and \(\) is a tune-able hyperparameter for smoothness. We feed the vector \(+\) to the decoder to produce the recolored image. The components orthogonal or along \(\) will be disentangled representations of the general image and the change in color respectively. It can be shown that the closed-form solution to find \(\) is as follows. A proof is provided in the appendix.

\[^{T}=(W^{T}W+ I_{d})^{-1}W^{T}(-W)\]

We found that the smoothing term \(\) was necessary and we could not simply use the pseudoinverse of \(W\), as this had extremely large eigenvalues which pushed \(+\) far outside of its typical distribution (from typical norms of around 4 to around 11) and led to inaccurate results. The results of recolorings with and without this smoothing term are shown in figure 5.

We also note that SLt-VAE can be extended very naturally to a case of partial supervision. If textual supervision is not available for some samples, that loss term can simply be omitted and the rest of the

Figure 5: SLt-VAE reconstructions trained on CLIP embeddings. The smoothing parameter drastically contributes to reasonable counterfactual images.

model can run as normal - this is not quite as trivial for Feature VAE. Future research might explore reducing the number of samples for which supervision is available.

### Feature VAE: Appending Controllable Parameters to Latent Representation

Our second strategy is to simply concatenate the supervised features \(y\) (i.e. the language embeddings) to the latent vector \(z\) as an input to the decoder. This method does not directly force the latent space to align with the supervised features. As the dimensionality of the latent space is kept low and the KL divergence loss encourages minimum information to be passed through the latent vector bottleneck, the network is incentivized to not redundantly encode the information present in \(y\).

As a result, we hope that we can control the properties encoded in \(y\) by inputting a different \(y\) obtained from the language encoder and keeping \(z\) constant. The loss is the same as in the vanilla VAE. No additional terms exist to prevent the content of \(y\) from being encoded in \(z\) in the base model, though we experiment with an adversarial loss against predicting \(y\) from \(z\) in later experiments. A diagram of this model and certain additional variants are shown in Figure 2. Examples of recoloring using this method are shown in Figure 6.

### Additional Experiments and Comparisons

Adversarial DisentanglingOne goal is to ensure the latent representation of the shape is independent of color. Similar to many previous works, we train an adversary that attempts to predict color from this extracted representation, with the goal color being unpredictable from the latent space. Therefore, our generator must learn to use the concatenated input for color. We use a gradient reversal layer for adversarial training. At each iteration of the main model updates, we allow the adversary additional optimization steps to learn color from the latent vectors as another hyperparameter. Our experiments covered the adversary attempting to predict either the RGB values or the text embedding itself.

Weak Supervision and Submodel PredictionBecause we suspected our model had difficulty learning color from text embeddings compared to RGB values, we were interested in simplifying this concatenated input. Thus, we wanted to explore the weakly supervised case, where we have access to a small proportion of RGB inputs in the training set in addition to language. The test set would only contain language. We wished to see if directing our concatenated inputs with this information would encourage the model to separate the structural information from the color information in each embedding.

Figure 6: Feature VAE reconstructions for the model trained on CLIP embeddings. The model preserves details such as shape and fill styles while changing the shape to the target color.

With this method, we train a sub-model to predict RGB from text embeddings, and then, during testing, we append this prediction to the latent vector rather than the text embedding. In our training process, we first pretrain the RGB prediction model on known colors with MSE loss. Then, we jointly train our Feature VAE and RGB prediction models on reconstruction, with an additional MSE loss for predicting known RGB values. If RGB values are known, those are fed to the generator instead of predictions. We experimented with beginning training with examples with known RGBs and then increasing the number of unknown examples per epoch at various rates.

## 5 Results

A summary of our results can be found in Table 1 using the quantitative metrics designed for our dataset. These metrics are described below. We performed numerous experiments while sweeping over a number of hyperparameters. The full results are of our hyperparameter sweep are available in our supplementary submission.

For the SLt-VAE, we swept the dimensionality of the latent vector over a range of 8 to 64, the \(\) parameter over a range of 0.5 to 2.5, smoothness over a range of 0.001 to 0.005, and \(\) over a range of 0.1 to 2.5.

For the Feature VAE, we swept over the dimensionality of the latent vector over a range of 5 to 100, and the \(\) parameter over a range of 0.01 to 5. When an additional RGB predictor was enabled, additional parameters included the mixing proportion (the proportion of labeled data), the temperature parameter for using labeled data (controls the rate at which the model uses true labels when available), and the lambda weight of the RGB predictor loss. When adversarial methods were enabled, there was an additional sweep over the number of adversary steps per generator step, and the lambda weight of the adversary loss.

### Quantitative Metrics

We leverage powerful structural and color-theoretic metrics of reconstruction quality estimation for a powerful assessment on this novel task in image perturbation [22; 21].

1. **Reverse Scaled Delta-E Score** The Delta-E (dE) is a metric that serves as a representation of the Euclidean "distance" between two colors. Scale [100: Good (no color difference), 0: Bad (high color difference)]
2. **ISSM** The Information theoretic-based Statistic Similarity Measure (ISSM) combines the principles of information theory with statistics for establishing relationships among image intensity values. Scale [1: Good, 0: Bad]
3. **PSNR** The Peak Signal-to-Noise Ratio (PSNR) quantifies the relationship between the maximum achievable power of the ground truth and the power of corrupting noise that impacts the accuracy of its reconstruction. PSNR is commonly denoted on the logarithmic decibel scale. Scale [Above 40 dB: Good, Below 40 dB: Bad]

  
**Model** & **Latent Dimension** &  &  \\  &  & **PSNR** & **SAM** & **SRE** \\  Feature VAE + Word2Vec & 50 & \(63.98 0.69\) & \(52.78 0.17\) & \(89.48 0.28\) & \(57.30 0.14\) \\ Feature VAE + USE & 50 & \(64.73 0.69\) & \(52.81 0.16\) & \(89.91 0.02\) & \(57.21 0.13\) \\ Feature VAE + CLIP & 20 & \(81.18 0.56\) & \(52.91 0.18\) & \(89.92 0.02\) & \(57.13 0.13\) \\ Feature VAE + CLIP & 100 & \(79.41 0.62\) & \(52.54 0.16\) & \(89.93 0.01\) & \(56.74 0.12\) \\ Feature VAE + CLIP + Adversarial & 50 & \(82.32 0.50\) & \(51.30 0.18\) & \(89.92 0.02\) & \(56.37 0.13\) \\ Feature VAE + CLIP + 25\% W.S. & 50 & \(88.30 0.35\) & \(52.91 0.17\) & \(89.92 0.02\) & \(57.34 0.13\) \\ Feature VAE + CLIP + 50\% W.S. & 50 & \(88.84 0.33\) & \(53.65 0.18\) & \(89.91 0.02\) & \(57.49 0.13\) \\  SLt-VAE + USE & 16 & \(60.47 0.69\) & \(52.67 0.19\) & \(89.80 0.16\) & \(57.06 0.13\) \\ SLt-VAE + USE & 64 & \(60.35 0.70\) & \(53.06 0.17\) & \(89.80 0.16\) & \(57.31 0.13\) \\ SLt-VAE + CLIP & 16 & \(59.08 0.72\) & \(52.85 0.19\) & \(89.80 0.16\) & \(57.06 0.13\) \\ SLt-VAE + CLIP & 64 & \(60.66 0.72\) & \(53.01 0.17\) & \(89.84 0.13\) & \(57.15 0.13\) \\   

Table 1: Validation metrics of supervised VAE variants at a fixed value of \(=1.0\). Feature VAE yields the closest-matching recolorings, while SLt-VAE and Feature VAE score similarly according to structural similarity metrics. Adversarial disentangling and weak RGB supervision further improve the performance of FeatureVAE.

4. **RMSE** The Root Mean Square Error (RMSE) quantifies the magnitude of per-pixel variation resulting from the reconstruction task. RMSE values are always non-negative, with a value of 0 indicating complete similarity between the compared ground truth and reconstruction. Scale [0: Good, \(\): Bad]
5. **SAM** The Spectral Angle Mapper assesses the spectral resemblance of two spectra by computing the angle between them, treating the spectra as vectors in a multidimensional space where the dimensionality corresponds to the number of bands. Scale [0 or small angles: Good, Large angles: Bad]
6. **SRE** The Signal to Reconstruction Error ratio quantifies the reconstruction error relative to the power of the ground truth. SRE is commonly denoted on the logarithmic decibel scale. Scale [Above 5 dB: Good, Below 5 dB: Bad]

RMSE and ISSM are 0 for all experiments. Delta-1 is the score measured between image and color, and Delta-2 is the score measured between images.

## 6 Discussion and Analysis

Based on our experiments, we provide the following key findings from this study:

1. For one additional experiment, we used direct RGB values as opposed to language embeddings. As expected, our models often perform worse on color matching using language embeddings as opposed to our RGB baselines. This is likely because it must predict the color from pretrained language embeddings rather than being able to directly use simple ground truth color values.
2. Models weakly supervised with correct RGB models, which were able to produce regressors for RGB values, performed better than models without it. This suggests that models such as LORD benefit from their auxiliary classifiers provided they produce correct classifications.
3. The CLIP Embeddings have the best performance when compared to Word2Vec and USE. This is consistent with existing literature on text supervision for image generation, where CLIP embeddings are a canonical choice due to their image supervision and in other disentanglement tasks.
4. Color matching performance appears to have a strong negative correlation with the dimensionality of the model and a strong positive correlation with the \(\) parameter of the model. We hypothesize that this is because weaker information theoretic constraints on the latent space allow our models to "hide" color information within the latent vector in irregular ways that cannot be captured with our methods.
5. Structural similarity appears to have a strong positive correlation with the dimensionality of the model and a strong negative correlation with the \(\) parameter of the model. This is likely because weaker information theoretic constraints allow more structural information to be encoded in the latent vectors.

Figure 7: Feature VAE recoloring of a given shape under two chosen colors, while varying the “Red-ness” dimension (assigned to dimension 0).

6. SLt-VAE is not very sensitive to any hyperparameters except smoothing, which is very cheap to tune.
7. Feature VAE models perform worse on color matching when information theoretic constraints are weakened, such as increasing latent vector size and lowering the value of \(\). This is likely due to the model storing color information in the latent vector rather than learning it from the language embedding. Thus, this method works best with sufficient constraints.
8. Our adversarial methods did not perform significantly stronger than our standard models. However, this could be due to the simplicity of the problem, or from the difficulty of training stable adversarial models.
9. Feature VAE was capable of recoloring shapes to colors outside of the dataset, while SLt-VAE was unable to do so. Feature VAE extracts color through the decoder, and thus is likely more generalizable than SLt-VAE, which extracts color only through the linear transform \(W\).

## 7 Conclusion

In this work, we propose a novel benchmark dataset and results for disentangling representations of variational auto-encoders for text-conditioned image generation. We propose and evaluate strategies for supervision and control through parametric interventions and embedding conditioning. The key takeaways of this study suggest possible barriers (color signal extraction) and useful methods (weak supervision) towards effective disentangling for future work. Motivated by our findings, we consider improving this model by improving our ability to extract a relevant structural signal from the prompt, using methods such as a richer training set of text inputs, and extending beyond this dataset towards natural imagery in future work. We hope that this study contributes towards a better understanding of disentangling representations in the latent space for better control and supervision in future generative models.