# Synthetic Data (Almost) from Scratch:

Generalized Instruction Tuning for Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We introduce _Generalized Instruction Tuning_ (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction-tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.

## 1 Introduction

Large Language Models (LLMs) have enabled unprecedented capabilities to understand and generate text like humans. By scaling up model size and data size [17; 13], LLMs are better at predicting next tokens and prompting to perform certain tasks with a few demonstrations . However, these capabilities do not directly translate to better human instruction following . Instruction tuning  bridges this gap by fine-tuning LLMs on instructions paired with human-preferred responses.

Prior work constructs instruction tuning data from seed examples or existing datasets. Initially, natural language processing (NLP) datasets described via instructions are used to fine-tune LLMs and the resulting LLMs can generalize on unseen (NLP) tasks . However, there are only thousands of NLP tasks [33; 19] available, which limits the tuned LLMs to generalize in real-world scenarios . Self-instruct  is a cost-effective method for creating synthetic instruction tuning datasets, which starts from a small pool of human-written seed instructions and generates new instructions by few-shot prompting an LLM (e.g., text-davinci-002) with randomly selected instructions from the pool. Unfortunately, the diversity of generated instructions is still an issue, since few-shot prompting tends to generate new instructions similar to its demonstrations. In addition, the process of creating high-quality seed instructions requires considerable human effort and expertise. Evolve-Instruct  improves self-instruct by augmenting existing instruction tuning datasets with different rewriting operations using LLMs, which is essentially data argumentation. Consequently, the scope of domainsor tasks that these augmented datasets can cover is limited by the original input datasets. See Figure 1 for illustrations of these methods described above. There are also studies concentrated on developing instruction-tuning datasets tailored to particular domains or tasks. For instance,  creates datasets targeting mathematical reasoning. In contrast,  and  focus on coding-related tasks. All of the above methods cannot produce instruction datasets that are generally applicable to a wide range of domains.

How to create a _general_ instruction tuning dataset? We draw inspiration from the systematic structure in human education system. The structure of human education includes several levels, starting from early childhood education up to higher education and beyond . Within each level, a student acquires knowledge, skills, and values in a systematic process. The courses a student learns from primary school to college cover a broad range of knowledge and skills, which facilitates the development of a diverse array of abilities. We believe that the systemic framework of the human education system has the potential to help the generation of high-quality and _general_ instruction data, which spans a diverse range of disciplinary areas.

In this paper, we introduce a generalized instruction tuning paradigm GLAN (shorthand for **G**eneralized Instruction-Tuning for **L**arge **L**ANG**uage **M**odels) to generate synthetic instruction tuning data almost from scratch. Unlike existing work [39; 21; 20; 24], GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale instruction data systematically and automatically across all disciplines. Specifically, inspired by the structure of the human education system, the input taxonomy is constructed by decomposing human knowledge and capabilities to various fields, sub-fields, and, ultimately, distinct disciplines semi-automatically, facilitated by LLMs and human verification. The cost of human verification process is low due to the limited number of disciplines in the taxonomy. As shown in Figure 1, we then further break down these disciplines into even smaller units. We continue to generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we can first sample from them and then generate diverse instructions with broad coverage across the entire spectrum of human knowledge and skills. The process described above mirrors the human educational system, where educators in each discipline craft a series of subjects for student learning. Instructors then develop a syllabus for each subject, breaking down the content into specific class sessions. These sessions are then further divided into core concepts that students must comprehend and internalize. Based on these detailed core concepts outlined in the syllabus, teaching materials and exercises are subsequently created, which are our instruction tuning data.

Figure 1: Comparing GLAN with FLAN, Self-Instruct and Evolve-Instruct. The inputs of FLAN, Self-Instruct and Evolve-Instruct are either seed examples or existing datasets, which limits the scope of domains of instructions that these methods can generate. GLAN takes the taxonomy of human knowledge & capabilities as input to ensure the broad coverage of generated instructions in various domains. This taxonomy is then broken down into smaller pieces and recombined to generate diverse instruction data.

GLAN is general, scalable and customizable. GLAN is a general method, which is task-agnostic and is capable of covering a wide range of domains. GLAN is scalable. Similar to , GLAN generates instructions using LLMs, which can produce instructions on a massive scale. Moreover, the input of GLAN is a taxonomy, which is generated by prompting an LLM and human verification, requiring minimal human effort. GLAN allows for easy customization. New fields or skills can be added by simply incorporating a new node into our taxonomy. Note that each node of the taxonomy can be expanded independently, which means that we only need to apply our method to the newly added nodes without re-generating the entire dataset. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following without using task-specific training data of these tasks.

## 2 GLAN: Generalized Instruction-Tuned Language Models

GLAN aims to create synthetic instruction data covering various domains of human knowledge and capabilities on a large scale. As shown in Algorithm 1, we first build a taxonomy of human knowledge and capabilities using frontier LLMs (i.e., GPT-4) and human verification. The taxonomy naturally breaks down human knowledge and capabilities to _fields_, _sub-fields_, and ultimately different _disciplines_ (see Section 2.1). The following steps are fully autonomously facilitated by GPT-4 (or GPT-3.5). Then for each discipline, we again instruct GPT-4 to further decompose it into a list of subjects within this discipline (Section 2.2). Similar to an instructor, GPT-4 continues to design a syllabus for each subject, which inherently breaks a subject into various class sessions with key concepts students need to master (Section 2.3). With obtained class sessions and key concepts, we are ready to construct synthetic instructions. We prompt GPT-4 to generate homework questions based on randomly sampled class sessions and key concepts as well as the syllabus (Section 2.4). We recursively decompose human knowledge and capabilities into smaller units until atomic-level components (i.e., class sessions and key concepts). We expect to randomly combine these class sessions and key concepts to ensure the coverage and diversity of synthetic instructions.

``` \(()\)\(\) build a taxonomy and return a list of disciplines (Section 2.1) \(\) for each discipline \(d\)do \((d)\)\(\) Obtain a list of subjects in \(d\) (Section 2.2) for each subject \(s\)do \((s,d)\)\(\) Return syllabus \(\) for \(s\) (Section 2.3) \(,()\)\(\) Extract class sessions and key concepts (Section 2.3) \((,, ,d)\)\(\) Generate instructions by sampling class sessions and key concepts (Section 2.4) \(\) endfor endfor return\(\) ```

**Algorithm 1** GLAN Instruction Generation

### Taxonomy of Human Knowledge and Capabilities

We build a taxonomy of human knowledge and capabilities to guide the generation of synthetic instructions. Therefore, its coverage is important. On the other hand, it is also essential to make the taxonomy highly extensible, since the preferred capabilities of LLMs may change over time. In the first step, we propose to generate the taxonomy by prompting GPT-4 with a set of different instructions (e.g., list all fields of human knowledge and capabilities). Then, we do human post-editing to ensure its correctness and completeness. Due to the limited number of fields, sub-fields, and disciplines in our taxonomy, the cost of human verification is reasonably low. Another advantage of human post-editing is that we can easily add new fields or disciplines to the taxonomy as needed.

[MISSING_PAGE_FAIL:4]

context (e.g., what students have already learned in previous sessions) when creating assignments. Therefore, we additionally instruct GPT to consider that students have learned up to class sessions \(}\) when crafting homework and try to leverage multiple key concepts across different class sessions. See details of our prompt for instruction generation in Appendix A.4.

Sampling Class Sessions and Key ConceptsIn a single syllabus, there are numerous class sessions and key concepts. We have two strategies to sample from them. In the first strategy, we generate assignments from a single class session. Therefore, we have only one class session name. Suppose we have \(m\) key concepts in total in this session. We randomly sample one to five key concepts from the \(m\) key concepts, which means we have totally \(_{i=1}^{5}\) combinations. In this strategy, we focus on creating _basic_ homework questions. To make the resulting questions more challenging (combine knowledge from multiple class sessions), we propose a second strategy to combine key concepts from two class sessions in the second strategy. We intend to generate questions leverage knowledge from two different class sessions. Suppose we have \(m_{1}\) and \(m_{2}\) key concepts in the first and second class sessions, respectively. We can have \(_{i=2}^{5}+m_{2}}{i}-_{i=2}^{5}}{i}-_{i=2}^ {5}}{i}\) different combinations, which is significantly more than that of the first strategy. We use both strategies to ensure our created questions are diverse in difficulty levels.

Answer GenerationAfter we generate questions in previous steps, we simply send these questions to GPT-3.5 and collect answers. We use GPT-3.5 for answer generation, because we find the quality of generated answers from GPT-3.5 is sufficiently good and using GPT-3.5 is significantly faster than GPT-4. The resulting question-answer pairs are our instruction tuning data. With a huge amount of question-answer pairs ranging from different disciplines with various difficulty levels, we expect the resulting LLM can excel in a wide range of tasks.

## 3 Experiments

### Data Generation

Taxonomy CreationBy asking GPT-4 to create a taxonomy of human knowledge and capabilities, we end up with a set of fields, sub-fields, and disciplines that cover a broad range of domains in human knowledge and capabilities. Next, we ask human annotators to decide whether these elements in the taxonomy should be kept or not in order to reduce the redundancy of the taxonomy while maintaining its correctness. Note that if a field or sub-field is marked as _remove_, we remove its descendant as well. We kept 126 _disciplines_ after majority voting (provided in supplementary materials). Note that it is feasible to manually add extra disciplines, sub-fields, or fields whenever necessary.

Subject and Syllabus GenerationDuring the subject list and syllabus generation, we prompt GPT-4 and employ nucleus sampling  with temperature \(T=1.0\) and top-\(p=0.95\) to encourage diversity. We do not use GPT-3.5-turbo since some subjects belong to the long-tail distribution which may not be effectively modeled by GPT-3.5-turbo. To ensure diversity and completeness of the generated subjects, we query GPT-4 10 times for each discipline (Section 2.2). There are 100 to 200 subjects for each discipline on average. It is worth noting that the same subjects may appear in different disciplines. For instance, the subject _calculus_ is both in physics and mathematics. We do not de-duplicate those subjects, since it may reflect their importance in human knowledge. Given a subject in a specified discipline, we query GPT-4 for only one time to design a syllabus (see details in section 2.3). The temperature and top-\(p\) are still set to 1.0 and 0.95, respectively. The number of class sessions contained in each syllabus varies from 10 to 30 and each class session contains around five key concepts.

Instruction GenerationEach instruction data consists of a question and its answer. We choose to generate questions and answers separately since we observed that separate generations lead to better quality. After question generation with GPT-4, each question is then answered by GPT-3.5-turbo with temperature \(T=0.7\), top-\(p=0.95\) (we use a lower temperature in order to make the resulting answers more accurate). We use GPT-3.5-turbo instead of GPT-4 for answer generation, because GPT-3.5-turbo is significantly faster with reasonably good results. We generate 10 million instruction-response pairs in total and then we do training data decontamination. Specifically, the training instruction-response pairs are decontaminated by removing pairs that contain questions or input prompts from the test and training (if any) sets of benchmarks we evaluate. We exclude the training set of benchmarks we evaluate to verify the generalization capability of our synthetic data.

### Model Training

We employ Mistral 7B  as our base model. During training, we concatenate each instruction and response pair to a single sequence and only compute loss on response tokens. We train our model for 3 epochs with a learning rate of \(3e\)-6. The batch size is set to approximately 512 instruction-response pairs. We employ a dynamic batch size to ensure a constant total number of tokens per batch. We use a cosine learning rate schedule and we start with a linear warm-up of 1000 steps and the final learning rate is reduced to 0. The training requires approximately 8 days using 32 A100 GPUs.

### Benchmark Evaluation

The instruction data GLAN generated spans a wide range of subjects. We evaluate its effectiveness in mathematical reasoning, coding, logical reasoning, and academic exams.

_Mathematical Reasoning:_ Mathematics is a common subject in many different disciplines. Hence, it is necessary to test the math reasoning ability of GLAN. We choose the two popular benchmarks for evaluation (i.e., GSM8K  and MATH ). GSM8K  is a high-quality math problem dataset that measures the basic multi-step mathematical reasoning ability. It contains around 7k problems for training and 1K problems for test. MATH  is a challenging math dataset that contains mathematics competition-level problems from AMC, AIME, etc. The 7.5k training and 5K test problems cover seven math subjects, i.e., Prealgebra, Precalculus, Algebra, Intermediate Algebra, Number Theory, Counting and Probability, and Geometry. Note that GLAN does not use any examples in the training set of GSM8K or MATH. Following , we report 0-shot setting results for GLAN. _Coding:_ To evaluate the coding capability of GLAN, we opt for two coding benchmarks HumanEval  and MBPP . We employ 0-shot setting for HumanEval and 3-shot setting for MBPP following prior art [4; 21]. _BBH:_ The instruction dataset we generated covers many disciplines, which can potentially enhance the reasoning ability of GLAN. Therefore, we evaluate GLAN on the BIG-Bench Hard dataset (BBH ), which contains 23 challenging tasks from Big-Bench . We employ the standard 3-shot setting with chain-of-thought demonstrations. _Academic Exams:_ We also evaluate GLAN on different academic benchmarks to verify whether GLAN is capable of solving exam questions. We choose two benchmarks (i.e., ARC  and MMLU ). Both benchmarks are composed of multi-choice questions. AI2 Reasoning Challenge (ARC ) contains grade-school level, multi-choice science questions. It contains two sub-sets, which are ARC-Challenge (ARC-C) and ARC-Easy (ARC-E). Massive Multitask Language Understanding (MMLU ) consists of a set of multiple-choice questions about 57 subjects ranging in difficulty from elementary levels to professional levels. It covers various of domains of knowledge, including humanities, STEM and social sciences. Note that there is a training set for ARC. However, we have excluded it from our

  
**Model** & \(l\)** & **HumanE** & **MBPP** & ** GSM8K** & **MATH** & **BBH** & **ARC-E** & **ARC-C** & **MMLU** \\  GPT-4 & – & 88.4 & 80.0 & 92.0 & 52.9 & 86.7 & 95.4 & 93.6 & 86.4 \\ GPT-3.5-turbo & – & 72.6 & 70.8 & 74.1 & 37.8 & 70.1 & 88.9 & 83.7 & 70.0 \\  LLaMA2 & 7B & 12.8 & 36.2 & 15.4 & 4.2 & 39.6 & 74.6 & 46.3 & 45.9 \\ Orca 2 & 7B & 17.1 & 28.4 & 55.7 & 10.1 & 42.8 & 87.8 & 78.4 & 53.9 \\ WizardLM v1.2 & 13B & 31.7 & 47.9 & 46.8 & 9.0 & 48.4 & 74.2 & 50.2 & 52.7 \\ Mistral & 7B & 28.0 & 50.2 & 43.4 & 10.0 & 56.1 & 79.5 & 53.9 & 62.3 \\ Mistral Instruct & 7B & 46.7 & 31.7 & 24.4 & 8.2 & 46.0 & 76.9 & 52.0 & 53.7 \\ MetaMath Mistral & 7B & 35.4 & 48.6 & 77.7 & 28.2 & 55.7 & 77.3 & 51.0 & 61.0 \\ WizardMath v1.1 & 7B & **51.2** & 54.1 & **83.2** & **33.0** & 58.2 & 79.8 & 53.2 & 60.3 \\ Mistral CodeAlpaca & 7B & 35.4 & 50.2 & 34.6 & 8.3 & 56.1 & 79.1 & 54.2 & 60.9 \\  GLAN & 7B & 48.8 & **57.6** & 80.8 & 32.7 & **60.7** & **90.7** & **81.1** & **62.9** \\   

Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.

training set during the decontamination process described in Section 3.1. Previous models mostly leverage probability-based methods on ARC and MMLU, which returns the best option based on the probabilities of the four options conditioned on the corresponding multi-choice question. We observe that after training on 10 million instructions, GLAN is able to _generate_ its predicted options and analysis of multi-choice questions in plain text as GPT-3.5 does. We therefore opt for 0-shot setting for GLAN and extract predictions using rules based on its completions as in .

ResultsOur main results are shown in Table 1. We compare GLAN against general domain models (Orca 2 , Mistral Instruct  and WizardLM ), math optimized models (MetaMath  and WizardMath ) and coding optimized models (CodeAlpaca ). We also report results of base LLMs (i.e., LLaMA2  and Mistral ) as references. GLAN either obtains the best results or results close to the best across all benchmarks. We observe that capabilities of math or coding optimized models increase on math or coding benchmarks while usually not others. After instruction tuning, GLAN excels on multiple dimensions from mathematical reasoning, coding, reasoning, and academic exams with a systematical data generation approach. Also note that our method does not use any task-specific training data such as training sets of GSM8K, MATH, or ARC as in Orca 2, MetaMath, and WizardMath, which indicates the general applicability of GLAN.

A Closer Look at Academic ExamsARC and MMLU are all multi-choice based benchmarks on academic exams. However, we observe that improvements of GLAN over Mistral on ARC are much larger than these on MMLU (see Table 1). By grouping the 57 subjects in MMLU into four categories (i.e., STEM, Humanities, Social Sciences, and Other (business, health, misc.)), we observe GLAN wildly improves on STEM in MMLU while not in other categories (Table 2). Also note that ARC is composed of high school science problems, which are also STEM questions. GLAN is good at STEM subjects may be because responses of our dataset are from GPT-3.5-turbo, which by default generates responses with Chain-of-Thoughts (CoT) reasoning. Indeed, we observe that GLAN generates solutions with CoT for multi-choice questions. CoT may help the multi-step reasoning in STEM multi-choice questions , while humanities and social sciences questions involve more memorization and single-step reasoning, where CoT may introduce additional errors.

### Scaling Property of GLAN

We investigate the scaling property of GLAN by training Mistral on different numbers of examples (i.e., 50K, 200K, 500K, 1M, and 10M) we generated. The results on downstream tasks are shown in Figure 2. It can be observed that overall task performance tends to increase as we increase the data size. Notably, the curve has not reached a plateau, indicating the potential for further improvement through the continued scaling of the data size of GLAN. However, we defer further scaling experiments to future work.

    &  &  &  \\  & & & **STEM** & **Humanities** & **Social Sciences** & **Other** \\  Mistral & 79.5 & 53.9 & 52.0 & 56.5 & 73.3 & 70.1 \\ GLAN & **90.7** & **81.1** & **60.1** & 54.9 & 71.8 & 68.6 \\   

Table 2: Detailed Results on Academic Exam benchmarks.

Figure 2: The scaling curve of GLAN on downstream tasks. The \(x\)-axis denotes GLAN data size (in \(_{10}\) scale following ), and the \(y\)-axis denotes the task performance.

[MISSING_PAGE_FAIL:8]

responses generated by two different models for a given input question. A higher score indicates better overall performance. To mitigate potential order bias, we perform bidirectional comparisons for each response pair and determine their average score. The average score difference to GLAN (i.e., \(()-(x)\)) serves as the final metric. Table 5 presents the results of pairwise comparisons across various levels of instruction difficulty. GLAN showcases superior performance compared to LLaMA-2, Orca 2, Mistraltstruct, and even WizardLM-13B (note that GLAN contains only 7B parameters) on most difficulty levels and overall scores. This suggests that GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty or complexity. Also, note that GLAN falls behind GPT-3.5-turbos are other models in comparison. Additionally, we group Evol-Instruct test according to the 29 skills and observe the same trends. Detailed results are listed in Appendix (Table 9 and 10). GLAN demonstrates strong performance on most skills, especially in Math, Coding, and Reasoning. However, it slightly falls short in common-sense related tasks. We also created GLAN-Test, similar to the Evol-Instruct Test but much larger in size, where GLAN outperforms other models as well (see Appendix A.8).

## 4 Related Work

Recent literature has extensively explored the collection of various human-made resources for instruction tuning. An intuitive direction is to collect existing NLP datasets and corresponding task descriptions [26; 33; 41], typical LLMs such as BLOOMZ  and FLAN  are trained on this type of instruction tuning data. However, with only tens to thousands of existing datasets available, the scope and diversity of instruction tuning are inevitably limited. Another common practice is to implement instruction tuning with real-world human user prompts. For instance, InstructGPT  was trained on high-quality human prompts submitted by real-world users to OpenAI GPT APIs. Vicuna  leverages user-shared prompts along with ChatGPT responses for instruction tuning, and Dolly was trained on simulated human-user interactions written by over 5k employees. Nevertheless, acquiring instructional data from human users typically involves high costs and involves privacy concerns. As LLM capabilities improve, instruction tuning with LLM-generated data exhibits better scalability and potential in addressing the super-alignment problem . Leveraging the in-context learning ability of LLMs, Unnatural instructions  and Self-instruct  sampled seed instructions as examples to elicit LLMs to generate new instructions. Taking advantage of the rephrasing ability of LLMs, WizardLM  and WizardMath  were trained using Evol-Instruct. Evol-Instruct iteratively employs ChatGPT to rewrite seed instructions into increasingly complex instructions. Similar to generation from seed instructions, carefully selected seed topics are used for generating textbook-like synthetic data  or self-chat multi-turn dialogues [38; 9] for instruction tuning. However, models trained on these LLM-generated data only work well in specific domains such as math [20; 40], dialogue [38; 9] or open-ended question answering [30; 39]. These methods encounter challenges in generalization , as the data diversity is restricted by seed instructions or seed topics.

## 5 Conclusions

We propose GLAN, a general and scalable method for synthesizing instruction data. Experiments show that GLAN can help large language models improve their capabilities in multiple dimensions, from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following. Currently, our synthetic data are based on the taxonomy of human knowledge and capabilities, and there are other types of useful data that have not been covered. We are interested in designing methods with border coverage. Our current instruction data are mostly question-answer pairs, and in the next step, we plan to generate synthetic data of multi-turn conversations and long documents.

  
**Difficulty** & **Ratio** & **LLaMA2-7B** & **Orca2-7B** & **Mistral-7B-Instruct** & **Wizard-13B-V1.2** & **GPT-3.5-turbo** \\  (1-5) Easy & 41.00\% & **5.46** & **2.19** & **1.13** & **1.32** & -1.22 \\ (6-10) Hard & 59.00\% & **5.38** & **2.28** & **1.68** & **0.99** & -0.68 \\   

Table 5: Pairwise comparison on various difficulty levels between GLAN and other models on Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as \(()-(x)\).