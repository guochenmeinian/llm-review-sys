# Implicit Regularization Paths of

Weighted Neural Representations

 Jin-Hong Du

Carnegie Mellon University

jinhongd@andrew.cmu.edu

&Pratik Patil

University of California Berkeley

pratikpatil@berkeley.edu

###### Abstract

We study the implicit regularization effects induced by (observation) weighting of pretrained features. For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels. Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms. These paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features. For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in . We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity. As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pre-trained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).

## 1 Introduction

In recent years, neural networks have become state-of-the-art models for tasks in computer vision and natural language processing by learning rich representations from large datasets. Pretrained neural networks, such as ResNet, which are trained on massive datasets like ImageNet, serve as valuable resources for new, smaller datasets . These pretrained models reduce computational burden and generalize well in tasks such as image classification and object detection due to their rich feature space . Furthermore, pretrained features or neural embeddings, such as the neural tangent kernel, extracted from these models, serve as valuable representations of diverse data .

However, despite their usefulness, fitting models based on pretrained features on large datasets can be challenging due to computational and memory constraints. When dealing with high-dimensional pretrained features and large sample sizes, direct application of even simple linear regression may be computationally infeasible or memory-prohibitive . To address this issue, subsampling has emerged as a practical solution that reduces the dataset size, thereby alleviating the computational and memory burden. Subsampling involves creating smaller datasets by randomly selecting a subset of the original data points. Beyond these computational and memory advantages, subagging can also greatly improve predictive performance in overparameterized regimes, especially near model interpolation thresholds . Moreover, through distributed learning, models fitted on multiple subsampled datasets can be aggregated as an ensemble to provide more stable predictions .

There has been growing interest in understanding the effects of subsampling (without replacement) . These works relate subsampling to explicit ridge regularization, assuming eitherGaussian features \((_{p},)\) or linearly decomposable features (referred to as _linear features_ in this paper) \(=^{1/2}\), where \(^{p p}\) is the covariance matrix and \(^{p}\) contains i.i.d. entries with zero means and bounded \(4+\) moments for some \(>0\). Specifically,  establish a connection between implicit regularization induced by subsampling and explicit ridge regularization through a _path_ defined by the tuple \((k/n,)\), where \(k\) and \(n\) are the subsample size and the full sample size, respectively, and \(\) is the ridge regularization level. Along this path, any subsample estimator with the corresponding ridge regularization exhibits the same first-order (or estimator equivalence) and second-order (or risk equivalence) asymptotic limits. Moreover, the endpoints of all such paths along the two axes of \(k=n\) (no subsampling) and \(=0\) (no regularization) span the same range. Although these results have been demonstrated for linear features,  also numerically observe similar equivalence behavior in more realistic contexts and propose conjectures for random features and kernel features based on heuristic "universality" justifications. However, extending these results to encompass more general feature structures and other sampling schemes remains an open question.

Towards answering this question, in this paper, we view subsampling as a weighted regression problem . This perspective allows us to study the equivalence in its most general form, considering arbitrary feature structures and weight structures. The general weight matrix approach used in this study encompasses various applications, including subsampling, bootstrapping, variance-adaptive weighting, survey, and importance weighting, among others. By interpreting subsampling as a weighted regression problem, we leverage recent tools from free probability theory, which have been developed to analyze feature sketching [39; 42; 54]. Building on these theoretical tools, we establish implicit regularization paths for general weighting and feature structures. We summarize our main results below and provide an overview of our results in the context of recent related work in Table 1.

### Summary of results and paper outline

We summarize our main results and provide an outline for the paper below.

* _Paths of weighted representations._ In Section 3, we demonstrate that general weighted models exhibit first-order equivalence along a path (Theorem 1) when the weight matrices are asymptotically independent of the data matrices. This path of equivalence can be computed directly from the data using the formula provided in Equation (2). Furthermore, we provide a novel interpretation of this path in terms of matching effective degrees of freedom of models along the path for general feature structures when the weights correspond to those arising from subsampling (Theorem 2).
* _Paths of subsampled representations._ We further specialize our general result in Theorem 2 for the weights induced by subsampling without replacement to structured features in Section 3.2. These include results for linear random features, nonlinear random features, and kernel features, as shown in Propositions 3-5, respectively. The latter two results also resolve Conjectures 7 and 8 raised by  regarding subsampling regularization paths for random and kernel features, respectively.

  
**Main analysis** & **Feature structure** & **Weight structure** & **Reference** \\   & Gaussian & subsampling &  \\  & linear & subsampling &  \\  & Gaussian & bootstrapping & [5; 16; 17] \\   & linear & subsampling &  \\  & general & general & Theorem 1 \\  & general & subsampling & Theorem 2 \\  & linear, random, kernel & subsampling & Propositions 3â€“5 \\   & linear & subsampling &  \\  & general & general & Theorem 6 \\   

Table 1: Overview of related work on the equivalence of implicit regularization and explicit ridge regularization.

* _Risk equivalences and tuning._ In Section 4, we demonstrate that an ensemble of weighted models has general quadratic risk equivalence on the path, with an error term that decreases inversely as \(1/M\) as the number of ensemble size \(M\) increases (Theorem 6). The risk equivalence holds for both in-distribution and out-of-distribution settings. For subsampling general features, we derive an upper bound for the optimal subsample size (Proposition 7) and propose a cross-validation method to tune the subsample and ensemble sizes (Algorithm 1), validated on real datasets in Section 4.3.

This level of generality is achievable because we do not analyze the risk of either the full model or the weighted models in isolation. Instead, we relate these two sets of models, allowing us to maintain weak assumptions about the features. The key assumption underlying our results is the asymptotic freeness of weight matrices with respect to the data matrices. While directly testing this assumption is generally challenging, we verify its validity through its consequences on real datasets in Section 4.3.

### Related literature

We provide a brief account of other related work below to place our work in a better context.

_Linear features._ Despite being overparameterized, neural networks generalize well in practice [70; 71]. Recent work has used high-dimensional "linearized" networks to investigate the various phenomena that arise in deep learning, such as double descent [12; 46; 48], benign overfitting [10; 35; 45], and scaling laws [7; 19; 66]. This literature analyzes linear regression using statistical physics [14; 60] and random matrix theory [22; 30]. Risk approximations hold under random matrix theory assumptions [6; 30; 66] in theory and apply empirically on a variety of natural data distributions [43; 60; 66].

_Random and kernel features._ Random feature regression, initially introduced in  as a way to scale kernel methods, has recently been used for theoretical analysis of neural networks and trends of double descent in deep networks [1; 46]. The generalization of kernel ridge regression has been studied in [11; 40; 57]. The risks of kernel ridge regression are also analyzed in [9; 19; 29]. The neural representations we study are motivated by the neural tangent kernel (NTK) and related theoretical work on ultra-wide neural networks and their relationships to NTKs [34; 68].

_Resampling analysis._ Resampling and weighted models are popular in distributed learning to provide more stable predictions and handle large datasets [20; 21; 51]. Historically, for ridge ensembles, [36; 61] derived risk asymptotics under Gaussian features. Recently, there has been growing interest in analyzing the effect of subsampling in high-dimensional settings.  considered least squares ensembles obtained by subsampling, where the final subsampled dataset has more observations than the number of features. For linear models in the underparameterized regime,  also provide certain equivalences between subsampling and iterative least squares approaches. The asymptotic risk characterization for general data models has been derived by . [25; 50] extended the scope of these results by characterizing risk equivalences for both optimal and suboptimal risks and for arbitrary feature covariance and signal structures. Very recently, different resampling strategies for high-dimensional supervised regression tasks have been analyzed by  under isotropic Gaussian features. Cross-validation methods for tuning the ensemble of ridge estimators and other penalized estimators are discussed in [13; 25; 26]. Our work adds to this literature by considering ensembles of models with general weighting and feature structures.

## 2 Preliminaries

In this section, we formally define our weighted estimator and state the main assumption on the weight matrix. Let \(f_{}^{d}^{p}\) be a pretrained model. Let \(\{(_{i},y_{i}) i=1,,n\}\) in \(^{d}\) be the given dataset. Applying \(f_{}\) to the raw dataset, we obtain the pretrained features \(_{i}=f_{}(_{i})\) for \(i=1,,n\) as the resulting neural representations or neural embeddings. In matrix notation, we denote the pretrained feature matrix by \(=[_{1},,_{n}]^{}^{n p}\). Let \(^{n n}\) be a general weight matrix used for weighting the observations. The weight matrix \(\) is allowed to be asymmetric, in general.

We consider fitting ridge regression on the weighted dataset \((,)\). Given a ridge penalty \(\), the ridge estimator fitted on the weighted dataset is given by:

\[}_{,}:=*{argmin}_{ ^{p}}(-\|_{2}^{2}} {n}+\|\|_{2}^{2})=(^{}^{} +n_{p})^{}^{}^{}.\] (1)

In the definition above, we allow for \(=0\), in which case the corresponding ridgeless estimator is defined as the limit \( 0^{+}\). For \(<0\), we use the Moore-Penrose pseudoinverse. An important special case is where \(\) is a diagonal matrix, in which case the above estimator reduces to weighted ridge regression. This type of weight matrix encompasses various applications, such as resampling, bootstrapping, and variance weighting. Our main application in this paper will be subsampling.

For our theoretical results, we assume that the weight matrix \(\) preserves some spectral structure of the feature matrix \(\). This assumption is captured by the condition of _asymptotic freeness_ between \(^{}\) and the feature Gram matrix \(^{}\). Asymptotic freeness is a concept from free probability theory .

**Assumption A** (Weight structure).: Let \(^{}\) and \(^{}/n\) converge almost surely to bounded operators that are infinitesimally free with respect to \(([],[()])\) for any \(\) independent of \(\) with \(\|\|_{}\) uniformly bounded. Additionally, let \(^{}\) have a limiting \(S\)-transform that is analytic on the lower half of the complex plane.

At a high level, Assumption A captures the notion of independence but is adapted for non-commutative random variables of matrices. We provide background on free probability theory and asymptotic freeness in Appendix A.3. Here, we briefly list a series of invertible transformations from free probability to help define the \(S\)-transform . The Cauchy transform is given by \(_{}(z)=[(z-)^{-1}]\). The moment generating series is given by \(_{}(z)=z^{-1}_{}(z^{-1})-1\). The \(S\)-transform is given by \(_{}(w)=(1+w^{-1})_{}^{(-1)}(w)\). These are the Cauchy transform (negative of the Stieltjes transform), moment generating series, and \(S\)-transform of \(\), respectively. Here, \(_{}^{(-1)}\) denotes the inverse under the composition of \(_{}\). The notation \([]\) denotes the average trace \([]/p\) of \(^{p p}\).

The freeness of a pair of matrices \(\) and \(\) means that the eigenvectors of one are completely unaligned or incoherent with those of the other. For example, if \(=^{}\) for a uniformly random unitary matrix \(\) drawn independently of the positive semidefinite \(\) and \(\), then \(\) and \(\) are almost surely asymptotically infinitesimally free . Other well-known examples include Wigner matrices, which are asymptotically free with respect to deterministic matrices [4, Theorem 5.4.5]. Gaussian matrices, where the Gram matrix \(=^{}/n=(^{}/n)^{}\) and any deterministic \(\), are almost surely asymptotically free [47, Chapter 4, Theorem 9]. Although not proven in full generality, it is expected that diagonal matrices are asymptotically free from data Gram matrices constructed using i.i.d. data. In Section 3.2, we will provide additional examples of feature matrices, such as random and kernel features from machine learning, for which our results apply.

Our results involve the notion of degrees of freedom from statistical optimism theory . Degrees of freedom in statistics count the number of dimensions in which a statistical model may vary, which is simply the number of variables for ordinary linear regression. To account for regularization, this notion has been extended to _effective degrees of freedom_ (Chapter 3 of ). Under some regularity conditions, from Stein's relation , the degrees of freedom of a predictor \(\) are measured by the trace of the operators \((/)()\). For the ridge estimator \(}_{,}\) fitted on \((,)\) with penalty \(\), the degrees of freedom is consequently the trace of its prediction operator \((^{}+ I_{p})^{}^{}\), which is also referred to as the ridge smoother matrix. That is, \((}_{,})=[^{}(^{}+ I_{p})^{}]\). We denote the normalized degrees of freedom by \(}=/n\). Note that \(}(}_{,})\{n,p \}/n 1\).

Finally, we express our asymptotic results using the asymptotic equivalence relation. Consider sequences \(\{_{n}\}_{n 1}\) and \(\{_{n}\}_{n 1}\) of (random or deterministic) matrices (which includes vectors and scalars). We say that \(_{n}\) and \(_{n}\) are _equivalent_ and write \(_{n}_{n}\) if \(_{p}|[_{n}(_{n}-_{n})]|=0\) almost surely for any sequence \(_{n}\) of matrices with bounded trace norm such that \(\|_{n}\|_{}<\) as \(n\). Our forthcoming results apply to a sequence of problems indexed by \(n\). For notational simplicity, we omit the explicit dependence on \(n\) in our statements.

Implicit regularization paths

We begin by characterizing the implicit regularization induced by weighted pretrained features. We will show that the degrees of freedom of the unweighted estimator \(}_{,}\) on the full data \((_{})\) with regularization parameter \(\) are equal to the degrees of freedom of the weighted estimator \(}_{,}\) for some regularization parameter \(\). For estimator equivalence, our data-dependent set of weighted ridge estimators \((,)\) that connect to the unweighted ridge estimator \((,)\) is defined in terms of "matching" effective degrees of freedom of component estimators in the set.

To state the upcoming result, denote the Gram matrix of the weighted data as \(_{}=^{}^{}/n\) and the Gram matrix of the unweighted data as \(_{}=^{}/n\). Furthermore, let \(^{+}_{}()\) denote the minimum positive eigenvalue of a symmetric matrix \(\).

**Theorem 1** (Implicit regularization of weighted representations).: _For \(_{}^{n n}\), suppose that the weight matrix \(^{n n}\) satisfies Assumption 1 and \(\|\|_{2}^{2}/n<\) as \(n\). For any \(>-_{n}^{+}_{}(_{})\), let \(>-^{+}_{}(_{})\) be given by the following equation:_

\[=/_{^{}}(-}( }_{,})),\] (2)

_where \(_{^{}}\) is the \(S\)-transform of the operator \(^{}\). Then, as \(n\), it holds that:_

\[(}_{,})(}_{,})}_{, }}_{,}.\] (3)

In other words, to achieve a target regularization of \(\) on the unweighted data, Theorem 1 provides a method to compute the regularization penalty \(\) with given weights \(\) from the available data using (2). The weighted estimator then has asymptotically the same degrees of freedom as the unweighted estimator. This means that the level of effective regularization of the two estimators is the same. Moreover, the estimators themselves are structurally equivalent; that is, \(^{}(}_{,}-}_{ {I},})}0\) for every constant vector \(\) with bounded norm. The estimator equivalence in Theorem 1 is a "first-order" result, while we will also characterize the "second-order" effects in Section 4.

The notable aspect of Theorem 1 is its generality. The equivalence results hold for a wide range of weight matrices and allow for negative values for the regularization levels. Furthermore, we have not made any direct assumptions about the feature matrix \(\), the weight matrix \(\), and the response vector \(\) (other than mild bounded norms). The main underlying ingredient is the asymptotic freeness between \(\) and \(\), which we then exploit using tools developed in  in the context of feature sketching. We discuss special cases of interest for \(\) and \(\) in the upcoming Sections 3.1 and 3.2.

### Examples of weight matrices

There are two classes of weighting matrices that are of practical interest:

* **Non-diagonal weighting matrices.** One can consider observation sketching, which involves some random linear combinations of the rows of the data matrix. Such observation sketching is beneficial for privacy, as it scrambles the rows of the data matrix, which may contain identifiable information about individuals. It also helps in reducing the effect of non-i.i.d. data that arise in time series or spatial data, where one wants to smooth away the impact of irregularities or non-stationarity.
* **Diagonal weighting matrices.** When observations are individually weighted, \(\) is a diagonal matrix, which includes scenarios such as resampling, bootstrapping, and subsampling. Note that even with subsampling, one can have a non-binary diagonal weighting matrix. For example, one can consider sampling with replacement or sampling with a particular distribution, which yields non-binary diagonal weighting matrices. Other examples of non-binary diagonal weighting matrices include inverse-variance weighting sampling to mitigate the effects of heterogeneous variations if the responses have different variances for different units.

In general, the set of equivalent weighted estimators depends on the corresponding \(S\)-transform as in (2), and it can be numerically evaluated. When focusing on subsampling without replacement, the data-dependent path for equivalent estimators with associated subsampling and regularization levels can be explicitly characterized in the following result by analyzing the \(S\)-transform of subsampling operators.

**Theorem 2** (Regularization paths due to subsampling).: _For a subsampling matrix \(^{(k)}\) consisting of \(k\) unit diagonal entries, the path (2) in terms of \((k,)\) simplifies to:_

\[(1-/n)(1-/)=(1-k/n),\] (4)

_where we denote by \(=(}_{,})=( {}_{,})\) for notational simplicity._

The relation (4) is remarkably simple, yet quite general! It provides an interplay between the normalized target complexity \(/n\), regularization inflation \(/\), and subsample fraction \(k/n\):

\[(1-)(1-)=(1- ).\] (5)

Since the normalized target complexity and subsample fraction are no greater than one, (5) also implies that the regularization level \(\) for the subsample estimator is always lower than the regularization level \(\) for the full estimator. In other words, subsampling induces (positive) implicit regularization, reducing the need for explicit ridge regularization. This is verified numerically in Figure 1.

For a fixed target regularization amount \(\), the degrees of freedom \((}_{,})\) of the ridge estimator on full data is fixed. Thus, we can observe that the path in the \((k/n,)\)-plane is a line. There are two extreme cases: (1) when the subsample size \(k\) is close to \(n\), we have \(\); and (2) when the subsample size is near \(0\), we have \(\). When \(=0\), the effective regularization level \(\) is such that \((}_{^{(k)},})=( {}_{,})=k\), which we find to be a neat relation!

Beyond subsampling without replacement, one can also consider other subsample matrixs. For example, for bootstrapping \(k\) entries, we observe a similar equivalent path in Figure 5. Additionally, for random sample reweighting, as shown in Figure 6, we also observe certain equivalence behaviors of degrees of freedom. This indicates that Theorem 1 also applies to more general weighting schemes.

### Examples of feature matrices

As mentioned in Section 2, when the feature matrix \(\) consists of i.i.d. Gaussian features, any deterministic matrix \(\) satisfies the condition stated in Assumption A. However, our results are not limited to Gaussian features. In this section, we will consider more general families of features commonly analyzed in machine learning and demonstrate the applicability of our results to them.

_(1) Linear features._ As a first example, we consider linear features composed of (multiplicatively) transformed i.i.d. entries with sufficiently bounded moments by a deterministic covariance matrix.

**Proposition 3** (Regularization paths with linear features).: Suppose the feature \(\) can be decomposed as \(=^{1/2}\), where \(^{p}\) contains i.i.d. entries \(z_{i}\) for \(i=1,,p\) with mean \(0\), variance \(1\), and satisfies \([|z_{i}|^{4+}] M_{}<\) for some \(>0\) and a constant \(M_{}\), and \(^{p p}\) is a deterministic

Figure 1: Equivalence under subsampling. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection \(_{}|^{}}_{,}|\) where \((_{p},_{p}/p)\). In both heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching empirical degrees of freedom. The data is generated according to Appendix F.1 with \(n=10000\) and \(p=1000\), and the results are averaged over \(M=100\) random weight matrices \(\).

symmetric matrix with eigenvalues uniformly bounded between constants \(r_{}>0\) and \(r_{}<\). Then, as \(n,p\) such that \(p/n>0\), the equivalences in (3) hold along the path (4).

Features of this type are common in random matrix theory  and in a wide range of applications, including statistical physics , high-dimensional statistics , machine learning , among others. The generalized path (2) in Theorem 2 recovers the path in Proposition 4 of . Although the technique in this paper is quite different and more general than that of .

_(2) Kernel features._ As the second example, Theorem 2 also applies to kernel features. Kernel features are a generalization of linear features and lift the input feature space to a high- or infinite-dimensional feature space by applying a feature map \(()\). Kernel methods use the kernel function \(K(_{i},_{j})=(_{i}),(_{j})\) to compute the inner product in the lifted space.

**Proposition 4** (Regularization paths with kernel features).: Suppose the same conditions as in Proposition 3 and the kernel function is of the form \(K(_{i},_{j})=g({\|_{i}\|}_{2}^{2}/p,{_{i},_{j}}/p,{\|_{j}\|}_{2}^{2}/p)\), where \(g\) is \(^{1}\) around \((,,)\) and \(^{3}\) around \((,0,)\) and \(:=_{p}[]/d\). Then, as \(n\), the equivalences in (3) hold in probability along the path (4).

The assumption in Proposition 4 is commonly used in the risk analysis of kernel ridge regression , among others. Here, \(^{k}\) denotes the class of functions that are \(k\)-times continuously differentiable. It includes neural tangent kernels (NTKs) as a special case. Proposition 4 confirms Conjecture 8 of  for these types of kernel functions.

_(3) Random features._ Finally, we consider random features that were introduced by  as a way to scale kernel methods to large datasets. Linked closely to two-layer neural networks , the random feature model has \(f_{}()=()\), where \(^{d p}\) is some randomly initialized weight matrix, and \(:\) is a nonlinear activation function applied element-wise to \(\).

**Proposition 5** (Regularization paths with random features).: Suppose \(_{i}(,)\) and the activation function \(:\) is differentiable almost everywhere and there are constants \(c_{0}\) and \(c_{1}\) such that \(|(x)|,|^{}(x)| c_{0}e^{c_{1}x}\), whenever \(^{}(x)\) exists. Then, as \(n,p,d\) such that \(p/n>0\) and \(d/n>0\), the equivalences in (3) hold in probability along the path (4).

As mentioned in the related work, random feature models have recently been used as a standard model to study various generalization phenomena observed in neural networks theoretically . Proposition 5 resolves Conjecture 7 of  under mild regularity conditions on the activation function.

It is worth noting that the prior works mentioned above, including , have focused on first characterizing the risk asymptotics in terms of various population quantities for each of the cases above. In contrast, our work in this paper deviates from these approaches by not expressing the risk in population quantities but rather by directly relating the estimators at different regularization levels. In the next section, we will explore the relationship between their squared prediction risks.

Figure 2: Equivalence of degrees of freedom for various feature structures under subsampling. The three panels correspond to linear features, random features with ReLU activation function (2-layer), and kernel features (polynomial kernel with degree 3 and without intercept), respectively. In all heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching the empirical degrees of freedom. The data is generated according to Appendix F.1 with \(n=5000\) and \(p=500\), and the results are averaged over \(M=100\) random weight matrices \(\).

Prediction risk asymptotics and risk estimation

The results in the previous section provide first-order equivalences of the estimators, which are related to the bias of the estimators. In practice, we are also interested in the predictive performance of the estimators. In this section, we investigate the second-order equivalence of weighting and ridge regularization through ensembling. Specifically, we show that aggregating estimators fitted on different weighted datasets also reduces the additional variance. Furthermore, the prediction risks of the full-ensemble weighted estimator and the unweighted estimator also match along the path.

Before presenting our risk equivalence result, we first introduce some additional notation. Assume there are \(M\) i.i.d. weight matrices \(_{1},,_{M}^{n n}\). The _\(M\)-ensemble_ estimator is defined as:

\[}_{_{1:M},}=M^{-1}_{m=1}^{M} }_{_{m},},\] (6)

and its performance is quantified by the conditional squared prediction risk, given by:

\[R(}_{_{1:M},})=_{_{0},y_{0}}[(y_{0}-_{0}^{}}_{_{1:M}, })^{2}\ |\ ,,\{_{m}\}_{m=1}^{M}],\] (7)

where \((_{0},y_{0})\) is a test point sampled independently from some distribution \(P_{_{0},y_{0}}\) that may be different from the training distribution \(P_{,y}\), and \(_{0}=f_{}(_{0})\) is the pretrained feature at the test point. The covariance matrix of the test features \(_{0}\) is denoted by \(_{0}\). When \(P_{_{0},y_{0}}=P_{,y}\), we refer to it as the in-distribution risk. On the other hand, when \(P_{_{0},y_{0}}\) differs from \(P_{,y}\), we refer to it as the out-of-distribution risk. Note that the conditional risk \(R_{M}\) is a scalar random variable that depends on both the dataset \((,)\) and the weight matrix \(_{m}\) for \(m[M]\). Our goal in this section is to analyze the prediction risk of the ensemble estimator (6) for any ensemble size \(M\).

**Theorem 6** (Risk equivalence along the path).: _Under the setting of Theorem 1, assume that the operator norm of \(_{0}\) is uniformly bounded in \(p\) and that each response variable \(y_{i}\) for \(i=1,,n\) has mean \(0\) and satisfies \([|y_{i}|^{4+}] M_{}<\) for some \(,M_{}>0\). Then, along the path (4),_

\[R(}_{_{1:M},}) R(}_{,})+}[(_{ }+)^{}^{}(_{}+_{n}) ^{}],\] (8)

_where the constant \(C\) is given by:_

\[C=-/^{2}^{}_{^{ }}(-(}_{,}))}[(_{}+)^{}(_{0} ^{}/n)(_{}+)^{}].\] (9)

At a high level, Theorem 6 provides a bias-variance-like risk decomposition for both the squared risks of weighted ensembles. The risk of the weighted predictor is equal to the risk of the unweighted equivalent implicit ridge regressor (bias) plus a term due to the randomness due to weighting (variance). The inflation factor \(C\) controls the magnitude of this term, and it decreases at a rate of \(1/M\) as the ensemble size \(M\) increases (see Figure 7 for a numerical verification of this rate). Therefore, by using a resample ensemble with a sufficiently large size \(M\), we can retain the statistical properties of the full ridge regression while reducing memory usage and increasing parallelization.

Theorem 6 extends the risk equivalence results in [50; 52]. Compared to previous results, Theorem 6 provides a broader risk equivalence that holds for general weight and feature matrices, as well as an arbitrary ensemble size \(M\). It is important to note that Theorem 6 holds even when the test distribution differs from the training data, making it applicable to out-of-distribution risks. Furthermore, our results do not rely on any specific distributional assumptions for the response vector, making them applicable in a model-free setting. The key idea behind this result is to exploit asymptotic freeness between the subsample and data matrices. Next, we will address the question of optimal tuning.

### Optimal oracle tuning

As in Theorem 2, we next analyze various properties related to optimal subsampling weights and their implications for the risk of optimal ridge regression. Recall that the subsampling matrix \(^{(k)}\) is a diagonal matrix with \(k\{1,,n\}\) nonzero diagonal entries, which is parameterized by the subsample size \(k\). Note that the optimal regularization parameter \(^{*}\) for the full data (\(^{(k)}=\) or \(k=n\)) is a function of the distribution of pretrained data and the test point. Based on the risk equivalence in Theorem 6, there exists an optimal path of \((k,)\) with the corresponding full-ensembleestimator \(}_{^{(k)}_{1:},}:=_{M}}_{^{(k)}_{1:M},}\) that achieves the optimal predictive performance at \((n,^{*})\). In particular, the ridgeless ensemble with \(^{*}=0\) happens to be on the path. From previous work [25; 50], the optimal subsample size \(k^{*}\) for \(^{*}=0\) has the property that \(k^{*} p\) under linear features. We show in the following that this property can be extended to include general features.

**Proposition 7** (Optimal subsample ratio).: Assume the subsampling matrix \(\) as defined in Theorem 2. Let \(^{*}=*{argmin}_{ 0}R(}_{^{(k)}_{ 1:},})\). Then the corresponding subsample size satisfies:

\[k^{*}=(}_{^{(k)}_{1:},^{*}}) *{rank}(_{}).\] (10)

The optimal subsample size \(k^{*}\) obtained from Proposition 7 is asymptotically optimal. For linear features, in the underparameterized regime where \(n>p\), [25; 50] show that the optimal subsample size \(k^{*}\) is asymptotically no larger than \(p\). This result is covered by Proposition 7 by noting that \(*{rank}(_{}) p\) under linear features. It is interesting and somewhat surprising to note that in the underparameterized regime (when \(p n\)), we do not need more than \(p\) observations to achieve the optimal risk. In this sense, the optimal subsampled dataset is always overparameterized.

When the limiting risk profiles \((,,):=_{p/n,p/k}R(}_{^{(k)}_{1:},})\) exist for subsample ensembles, the limiting risk of the optimal ridge predictor \(_{ 0}(,,)\) is monotonically decreasing in the limiting sample aspect ratio \(\). This also (provably) confirms the sample-wise monotonicity of optimally-tuned risk for general features in an asymptotic sense . Due to the risk equivalence in Theorem 6, for any \(>0\), there exists \(\) such that \((,,)=(,,0)\). This implies that \(_{ 0}(,,)=_{} (,,0)\). In other words, tuning over subsample sizes with sufficiently large ensembles is equivalent to tuning over the ridge penalty on the full data.

### Data-dependent tuning

As suggested by Proposition 7, the optimal subsample size is smaller than the rank of the Gram matrix. This result has important implications for real-world datasets where the number of observations (\(n\)) is much larger than the number of features (\(p\)). In such cases, instead of using the entire dataset, we can efficiently build small ensembles with a subsample size \(k p\). This approach is particularly beneficial when \(n\) is significantly higher than \(p\), for example, when \(n=1000p\). By fitting ensembles with only \(M=100\) base predictors, we can potentially reduce the computational burden while still achieving optimal predictive performance. Furthermore, this technique can be especially valuable in scenarios where computational resources are limited or when dealing with massive datasets that cannot be easily processed in their entirety.

In the following, we propose a method to determine the optimal values of the regularization parameter \(^{*}\) for the full ridge regression, as well as the corresponding subsample size \(k^{*}\) and the optimal ensemble size \(M^{*}\). According to Theorem 6, the optimal value of \(M^{*}\) is theoretically infinite. However, in practice, the prediction risk of the \(M\)-ensemble predictor decreases at a rate of \(1/M\) as \(M\) increases. Therefore, it is important to select a suitable value of \(M\) that achieves the desired level of performance while considering computational constraints and the specified error budget. By carefully choosing an appropriate \(M\), we can strike a balance between model accuracy and efficiency, ensuring that the subsampled neural representations are effectively used in downstream tasks.

Consider a grid of subsample size \(_{n}\{1,,n\}\); for instance, \(_{n}=\{0,k_{0},2k_{0},,n\}\) where \(k_{0}\) is a subsample size unit. For a prespecified subsample size \(k_{n}\) and ensemble size

Figure 3: Equivalence in pretrained features of pretrained ResNet-50 on Flowers-102 datasets.

\(\), suppose we have multiple risk estimates \(_{m}\) of \(R_{m}\) for \(m=1,,M_{0}\). The squared risk decomposition (51, Eq (7)) along with the equivalence path (8) implies that \(R_{m}=m^{-1}R_{1}+(1-m^{-1})R_{}\), for \(m=1,,M_{0}\). Summing these equations yields \(_{m=1}^{M_{0}}R_{m}=_{m=1}^{M_{0}}R_{1}+_{m=1}^{M_{0}} (1-m^{-1})R_{}\). Thus, we can estimate \(R_{}\) by:

\[_{}=_{m=1}^{M_{0}}_{m}-_{m=1}^{M_{0 }}m^{-1}_{1}\ /_{m=1}^{M_{0}}1-m^{-1}.\] (11)

Then, the extrapolated risk estimates \(_{m}\) (with \(m>M_{0}\)) are defined as:

\[_{m}:=m^{-1}_{1}+(1-m^{-1})_{ } m>M_{0}.\] (12)

The meta-algorithm that implements the above cross-validation procedure is provided in Algorithm 1. To efficiently tune the parameters of ridge ensembles, we use and combine the corrected generalized cross-validation (CGCV) method  and the extrapolated cross-validation (ECV) method . The improved CV method is implemented in the Python library .

### Validation on real-world datasets

In this section, we present numerical experiments to validate our theoretical results on real-world datasets. Figure 3 provides evidence supporting Assumption A on pretrained features extracted from commonly used neural networks applied to real-world datasets. The first panel of the figure demonstrates the equivalence of degrees of freedom for these pretrained features. Furthermore, we also observe consistent behavior across different neural network architectures and different datasets (see Figures 8 and 9). Remarkably, the path of equivalence can be accurately predicted, offering valuable insight into the underlying dynamics of these models. This observation suggests that the pretrained features from widely used neural networks exhibit similar properties when applied to real-world data, regardless of the specific architecture employed. The ability to predict the equivalence path opens up new possibilities for optimizing the performance of these models in practical applications.

One implication of the equivalence results explored in Theorems 1 and 6 is that instead of tuning for the full ridge penalty \(\) on the large datasets, we can fix a small value of the ridge penalty \(\), fit subsample ridge ensembles, and tune for an optimal subsample size \(k\). To illustrate the validity of the tuning procedure described in Algorithm 1, we present both the actual prediction errors and their estimates by Algorithm 1 in Figure 4. We observe that the risk estimates closely match the prediction risks at different ensemble sizes across different datasets. Even with a subsampling ratio \(k/n\) of \(0.01\) and a sufficiently large \(M\), the risk estimate is close to the optimal risk. A smaller subsample size could also yield even smaller prediction risk in certain datasets.

## 5 Limitations and outlook

While our results are quite general in terms of applying to a wide variety of pretrained features, they are limited in that they only apply to ridge regression fitted on the pretrained features. The key challenge for extending the analysis based on Assumption A to general estimators beyond ridge regression is the characterization of the effect of subsampling general resolvents as additional ridge regularization. To extend to generalized linear models, one approach is to view the optimization as iteratively reweighted least squares  in combination with the current results. Another approach is to combine our results with the techniques in  to obtain deterministic equivalents for the Hessian, enabling an understanding of implicit regularization due to subsampling beyond linear models.

Beyond implicit regularization due to subsampling, there are other forms of implicit regularization, such as algorithmic regularization due to early stopping in gradient descent , dropout regularization , among others. In some applications, multiple forms of implicit regularization are present simultaneously. For instance, during a mini-batch gradient step, implicit regularization arises from both iterative methods and mini-batch subsampling. The results presented in this paper may help to make explicit the combined effect of various forms of implicit regularization.