# Module-wise Training of Neural Networks via the Minimizing Movement Scheme

Skander Karkar

Criteo, Sorbonne Universite

&Ibrahim Ayed

Sorbonne Universite, Thales

Emmanuel de Bezenac

ETH Zurich

&Patrick Gallinari

Criteo, Sorbonne Universite

###### Abstract

Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as \(60\%\) less memory usage.

## 1 Introduction

End-to-end backpropagation is the standard training method of neural networks. However, it requires storing the whole model and computational graph during training, which requires large memory consumption. It also prohibits training the layers in parallel. Dividing the network into modules, a module being made up of one or more layers, accompanied by auxiliary classifiers, and greedily solving module-wise optimization problems sequentially (i.e. one after the other fully) or in parallel (i.e. at the same time batch-wise), consumes much less memory than end-to-end training as it does not need to store as many activations, and when done sequentially, only requires loading and training one module (so possibly one layer) at a time. Module-wise training has therefore been used in constrained settings in which end-to-end training can be impossible such as training on mobile devices  and dealing with very large whole slide images . When combined with batch buffers, parallel module-wise training also allows for parallel training of the modules . Despite its simplicity, module-wise training has been recently shown to scale well , outperforming more complicated alternatives to end-to-end training such as synthetic  and delayed  gradients, while having superior memory savings.

In a classification task, module-wise training splits the network into successive modules, a module being made up of one or more layers. Each module takes as input the output of the previous module, and each module has an auxiliary classifier so that a local loss can be computed, with backpropagation happening only inside the modules and not between them (see Figure 1 below).

The main drawback of module-wise training is the well-documented _stagnation problem_ observed in , whereby early modules overfit and learn more discriminative features than end-to-endtraining, destroying task-relevant information, and deeper modules don't improve the test accuracy significantly, or even degrade it, which limits the deployment of module-wise training. We further highlight this phenomenon in Figures 2 and 3 in Section 4.4. To tackle this issue, InfoPro  propose to maximize the mutual information that each module keeps with the input, in addition to minimizing the loss.  make the auxiliary classifier deeper and Sedona  make the first module deeper. These last two methods lack a theoretical grounding, while InfoPro requires a second auxiliary network for each module besides the classifier. We propose a different perspective, leveraging the analogy between residual connections and the Euler scheme for ODEs . To preserve input information, we minimize the kinetic energy of the modules along with the training loss. Intuitively, this forces the modules to change their input as little as possible. We leverage connections with the theories of gradient flows in distribution space and optimal transport to analyze our method theoretically.

Our approach is particularly well-adapted to networks that use residual connections such as ResNets [27; 28], their variants (e.g. ResNeXt , Wide ResNet , EfficientNet  and MobileNetV2 ) and vision transformers that are made up essentially of residual connections [39; 17], but is immediately usable on any network where many layers have the same input and output dimension such as VGG . Our contributions are the following:

* We propose a new method for module-wise training. Being a regularization, it is lighter than many recent state-of-the-art methods (PredSim , InfoPro ) that train another auxiliary network besides the auxiliary classifier for each module.
* We theoretically justify our method, proving that it is a transport regularization that forces the module to be an optimal transport map making it more regular and stable. We also show that it amounts to a discretization of the gradient flow of the loss in probability space, which means that the modules progressively minimize the loss and explains why the method avoids the accuracy collapse observed in module-wise training.
* Experimentally, we consistently improve the test accuracy of module-wise trained networks (ResNets, VGG and Swin-Transformer) beating 8 other methods, in sequential and parallel module-wise training, and also in _multi-lap sequential_ training, a variant of sequential module-wise training that we introduce and that performs better in many cases. In particular, our regularization makes parallel module-wise training superior or comparable in accuracy to end-to-end training, while consuming \(10\%\) to \(60\%\) less memory.

## 2 Transport-regularized module-wise training

The typical setting of (sequential) module-wise training for minimizing a loss \(L\), is, given a dataset \(\), to solve one after the other, for \(1{}k{}K\), Problems

\[(T_{k},F_{k})_{T,F}_{x}L(F,T G_{k-1}(x))\] (1)

Figure 1: Module-wise training.

where \(G_{k}=T_{k}... T_{1}\) for \(1{}k{}K\), \(G_{0}{=}\), \(T_{k}\) is the module (one or many layers) and \(F_{k}\) is an auxiliary classifier. Module \(T_{k}\) receives the output of module \(T_{k-1}\), and auxiliary classifier \(F_{k}\) computes the prediction from the output of \(T_{k}\) so the loss can be computed. The inputs are \(x\) and \(L\) has access to their labels \(y\) to calculate the loss. i.e. \(L(F,T G_{k-1}(x))=l(F T G_{k-1}(x),y)\) where \(l\) is a machine learning loss such as cross-entropy. See Figure 1. The final network trained this way is \(F_{K} G_{K}\). But, at inference, we can stop at any depth \(k\) and use \(F_{k} G_{k}\) if it performs better. Indeed, an intermediate module often performs as well or better than the last module because of the early overfitting and subsequent stagnation or collapse problem of module-wise training [43; 7; 60; 47].

We propose below in (2) a regularization that avoids the destruction of task-relevant information by the early modules by forcing them to minimally modify their input. Proposition 2.2 proves that by using our regularization (2), we are indeed making the modules build upon each other to solve the task, which is the property we desire in module-wise training, as the modules now act as successive proximal optimization steps in the _minimizing movement scheme_ optimization algorithm for maximizing the separability of the data representation. The background on optimal transport (OT), gradient flows and the minimizing movement scheme is in Appendices A and B.

### Method statement

To keep greedily-trained modules from overfitting and destroying information needed later, we penalize their kinetic energy to force them to preserve the geometry of the problem as much as possible. If each module is a single residual block (that is a function \(T{=}r\), which includes many transformer architectures [39; 17]), its kinetic energy is simply the squared norm of its residue \(r{=}T{-}\), which we add to the loss \(L\) in the target of the greedy problems (1). All layers that have the same input and output dimension can be rewritten as residual blocks and the analysis applies to a large variety of architectures such as VGG . Given \(0\), we now solve, for \(1{}k{}K\), Problems

\[(T_{k}^{},F_{k}^{})_{T,F}_{x}L(F,T G _{k-1}^{}(x))+\|T G_{k-1}^{}(x){-}G_{k-1}^{}(x )\|^{2}\] (2)

where \(G_{k}^{}{=}T_{k}^{}.. T_{1}^{}\) for \(1{}k{}K\) and \(G_{0}^{}{=}\). The final network is \(F_{K}^{}{}G_{K}^{}\). Intuitively, this biases the modules towards moving the points as little as possible, thus at least keeping the performance of the previous module. Residual connections are already biased towards small displacements and this bias is desirable and should be encouraged [35; 64; 26; 15; 36]. But the method can be applied to any module where \(T(x)\) and \(x\) have the same dimension so that \(T(x){-}x\) can be computed.

To facilitate the theoretical analysis, we rewrite the method in a more general formulation using data distribution \(\), which can be discrete or continuous, and the distribution-wide loss \(\) that arises from the point-wise loss \(L\). Then Problem (2) is equivalent to Problem

\[(T_{k}^{},F_{k}^{})_{T,F}\ (F,T_{k}_{k}^{ })+_{}\|T(x)-x\|^{2}\,_{k}^{}(x)\] (3)

with \(_{k+1}^{}{=}(T_{k}^{})_{}_{k}^{}\), \(_{1}^{}{=}\) and \((F,T_{}_{k}^{})= L(F,T(x))\,_{k}^{ }(x)= L(F,z)\,T_{}_{k}^{}(x)\).

### Link with the minimizing movement scheme

We now formulate our main result: solving Problems (3) is equivalent to following a _minimizing movement scheme (MMS)_ in distribution space for minimizing \(():=_{F}(F,)\), which is the loss of the best classifier. If we are limited to linear classifiers, \((_{k}^{})\) is the linear separability of the representation \(_{k}^{}\) at module \(k\) of the data distribution \(\). The MMS, introduced in [24; 23], is a metric counterpart to Euclidean gradient descent for minimizing functionals over distributions. In our case, \(\) is the functional we want to minimize. We define the MMS below in Definition 2.1

The distribution space we work in is the metric Wasserstein space \(_{2}()=((),W_{2})\), where \(^{d}\) is a convex compact set, \(()\) is the set of probability distributions over \(\) and \(W_{2}\) is the Wasserstein distance over \(()\) derived from the optimal transport problem with Euclidean cost:

\[W_{2}^{2}(,)=_{T\,\ T_{}=} _{}\|T(x)-x\|^{2}\,(x)\] (4)

where we assume that \(\) is negligible and that the distributions are absolutely continous.

**Definition 2.1**.: Given \(:_{2}()\), and starting from \(_{1}^{}()\), the Minimizing Movement Scheme (MMS) takes proximal steps for minimizing \(\). It is s given by

\[_{k+1}^{}_{()}\ ()+W_{2}^{2}( ,_{k}^{})\] (5)

The MMS (5) can be seen as a non-Euclidean implicit Euler step for following the gradient flow of \(\), and \(_{k}^{}\) converges to a minimizer of \(\) under some conditions (see the end of this section).

So under the mentioned assumptions on \(\) and absolute continuity of the distributions, we have that Problems (3) are equivalent to the minimizing movement scheme (5):

**Proposition 2.2**.: _The distributions \(_{k+1}^{}=(T_{k}^{})_{t}_{k}^{}\), where the functions \(T_{k}^{}\) are found by solving (3) and \(_{1}^{}=\) is the data distribution, coincide with the MMS (5) for \(=_{F}(F,.)\)._

Proof.: The minimizing movement scheme (5) is equivalent to taking \(_{k+1}^{}=(T_{k}^{})_{t}_{k}^{}\) where

\[T_{k}^{}_{T:}(T_{t}_{k}^ {})+W_{2}^{2}(T_{t}_{k}^{},_{k}^{})\] (6)

under conditions that guarantee the existence of a transport map between \(_{k}^{}\) and any other measure, and absolute continuity of \(_{k}^{}\) suffices, and the loss can ensure that \(_{k+1}^{}\) is also absolutely continuous. Among the functions \(T_{k}^{}\) that solve problem (6), is the optimal transport map from \(_{k}^{}\) to \(_{k+1}^{}\). To solve specifically for this optimal transport map, we have to solve the equivalent Problem

\[T_{k}^{}_{T}(T_{t}_{k}^{})+ _{}\|T(x)-x\|^{2}\,_{k}^{}(x)\] (7)

Problems (6) and (7) have the same minimum value, but the minimizer of (7) is now an optimal transport map between \(_{k}^{}\) and \(_{k+1}^{}\). This is immediate from the definition (4) of the \(W_{2}\) distance. Equivalently minimizing first over \(F\) and then over \(T\) in (3), it follows from the definition of \(\) that Problems (3) and (7) are equivalent, which concludes. 

Since we solve Problems (3) over neural networks, their representation power shown by universal approximation theorems [13; 29] is important to get close to equivalence between (5) and (3), as we need to approximate an optimal transport map. We also know that the training of each module, if it is shallow, converges [5; 6; 34; 22; 18].

If \(\) is lower-semi continuous then Problems (5) always admit a solution because \(()\) is compact. If \(\) is also \(\)-geodesically convex for \(0\), we have convergence of \(_{k}^{}\) as \(k{}\) and \(0\) to a minimizer of \(\), potentially under more technical conditions (see Appendix B). Even though a machine learning loss will usually not satisfy these conditions, this analysis offers hints as to why our method avoids in practice the problem of stagnation or collapse in performance of module-wise training as \(k\) increases, as we are making proximal local steps in Wasserstein space to minimize the loss. This convergence discussion also suggests taking \(\) as small as possible and many modules.

### Regularity result

As a secondary result, we show that Problem (3) has a solution and that the solution module \(T_{k}^{}\) is an optimal transport map between its input and output distributions, which means that it comes with some regularity.  show that these networks generalize better and overfit less in practice. We assume that the minimization in \(F\) is over a compact set \(\), that \(_{k}^{}\) is absolutely continuous, that \(\) is continuous and non-negative, that \(\) is convex and compact and that \(\) is negligible.

**Proposition 2.3**.: _Problem (3) has a minimizer \((T_{k}^{},F_{k}^{})\) such that \(T_{k}^{}\) is an optimal transport map. And for any minimizer \((T_{k}^{},F_{k}^{})\), \(T_{k}^{}\) is an optimal transport map._

The proof is in Appendix C. OT maps have regularity properties under some boundedness assumptions. Given Theorem A.1 in Appendix A taken from , \(T_{k}^{}\) is \(\)-Holder continuous almost everywhere and if the optimization algorithm we use to solve the discretized problem (2) returns an approximate solution pair \((_{k}^{},_{k}^{})\) such that \(_{k}^{}\) is an \(\)-optimal transport map, i.e. \(\|_{k}^{}-T_{k}^{}\|_{}\), then we have (using the triangle inequality) the following stability property of the module \(_{k}^{}\):

\[\|_{k}^{}(x)-_{k}^{}(y)\| 2+C\|x-y\|^{}\] (8)for almost every \(x,y(_{k}^{})\) and \(C{>}0\). Composing these stability bounds on \(T_{k}^{}\) and \(_{k}^{}\) allows to get bounds for the composition networks \(G_{k}^{}\) and \(_{k}^{}{=}_{k}^{}..._{1}^{}\).

To summarize Section 2, the transport regularization makes each module more regular and it allows the modules to build on each other as \(k\) increases to solve the task, which is the property we desire.

## 3 Practical implementation

### Multi-block modules

For simplicity, we presented in (2) the case where each module is a single residual block. However, in practice, we often split the network into modules that are made-up of many residual blocks each. We show here that regularizing the kinetic energy of such modules still amounts to a transport regularization, which means that the theoretical results in Propositions 2.2 and 2.3 still apply.

If each module \(T_{k}\) is made up of \(M\) residual blocks, i.e. applies \(x_{m+1}{=}x_{m}{+}r_{m}(x_{m})\) for \(0{}m{<}M\), then its total discrete kinetic energy for a single data point \(x_{0}\) is the sum of its squared residue norms \(\|r_{m}(x_{m})\|^{2}\), since a residual network can be seen as a discrete Euler scheme for an ordinary differential equation  with velocity field \(r\):

\[x_{m+1}=x_{m}+r_{m}(x_{m})\ \ _{t}x_{t}=r_{t}(x_{t})\] (9)

and \(\|r_{m}(x_{m})\|^{2}\) is then the discretization of the total kinetic energy \(_{0}^{1}\|r_{t}(x)\|^{2}\,t\) of the ODE. If \(_{m}^{x}\) denotes the position of a point \(x\) after \(m\) residual blocks, then regularizing the kinetic energy of multi-block modules now means solving

\[(T_{k}^{},F_{k}^{})_{T,F}_{x}(L(F,T(G_{k-1}^{}(x))+_{m=0}^{M-1}\|r_{m}(_{m}^{x })\|^{2})\] (10) \[T=(+r_{M-1})...(+r_{0 }),\ _{0}^{x}=G_{k-1}^{}(x),_{m+1}^{x}=_{m}^{x}+r_{m}(_{m}^{x})\]

where \(G_{k}^{}{=}T_{k}^{}.. T_{1}^{}\) for \(1{}k{}K\) and \(G_{0}^{}{=}\). We also minimize this sum of squared residue norms instead of \(\|T(x)-x\|^{2}\) (the two no longer coincide) as it works better in practice, which we assume is because it offers a more localized control of the transport. As expressed in (9), a residual network can be seen as an Euler scheme of an ODE and Problem (10) is then the discretization of

\[(T_{k}^{},F_{k}^{})_{T,F}\ (F,T_{t} _{k}^{})+_{0}^{1}\|v_{t}\|_{L^{2}((_{t})_{t} _{k}^{})}^{2}\,t\] (11) \[T=_{1}^{},\ _{t}_{t}^{x}=v_{t}( _{t}^{x}),\ _{0}^{}=\]

where \(_{k+1}^{}=(T_{k}^{})_{t}_{k}^{}\) and \(r_{m}\) is the discretization of vector field \(v_{t}\) at time \(t=m/M\). Here, distributions \(_{k}^{}\) are pushed forward through the maps \(T_{k}^{}\) which correspond to the flow \(\) at time \(t=1\) of the kinetically-regularized velocity field \(v_{t}\). We recognize in the second term in the target of (11) the optimal transport problem in its dynamic formulation (15) from , and given the equivalence between the Monge OT problem (4) and the dynamic OT problem (15) in Appendix A, Problem (11) is in fact equivalent to the original continuous formulation (3), and the theoretical results in Section 2 follow immediately (see also the proof of Proposition 2.3 in Appendix C).

### Solving the module-wise problems

The module-wise problems can be solved in two ways. One can completely train each module with its auxiliary classifier for \(N\) epochs before training the next module, which receives as input the output of the previous trained module. We call this _sequential_ module-wise training. But we can also do this batch-wise, i.e. do a complete forward pass on each batch but without a full backward pass, rather a backward pass that only updates the current module \(T_{k}^{}\) and its auxiliary classifier \(F_{k}^{}\), meaning that \(T_{k}^{}\) forwards its output to \(T_{k+1}^{}\) immediately after it computes it. We call this _parallel_ module-wise training. It is called _decoupled_ greedy training in , which shows that combining it with batch buffers solves all three locking problems and allows a linear training parallelization in the depth of the network. We propose a variant of sequential module-wise training that we call _multi-lap sequential_ module-wise training, in which instead of training each module for \(N\) epochs, we train each module from the first to the last sequentially for \(N/R\) epochs, then go back and train from the first module to the last for \(N/R\) epochs again, and we do this for \(R\) laps. For the same total number of epochs and training time, and the same advantages (loading and training one module at a time) this provides a non-negligible improvement in accuracy over normal sequential module-wise training in most cases, as shown in Section 4. Despite our theoretical framework being that of sequential module-wise training, our method improves the test accuracy of all three module-wise training regimes.

### Varying the regularization weight

The discussion in Section 2.2 suggests taking a fixed weight \(\) for the transport cost that is as small as possible. However, instead of using a fixed \(\), we might want to vary it along the depth \(k\) to further constrain with a smaller \(_{k}\) the earlier modules to avoid that they overfit or the later modules to maintain the accuracy of earlier modules. We might also want to regularize the network further in earlier epochs when the data is more entangled. We propose in Appendix D to formalize this varying weight \(_{k,i}\) across modules \(k\) and SGD iterations \(i\) by using a scheme inspired by the method of multipliers to solve Problems (2) and (10). However, it works best in only one experiment in practice. The observed dynamics of \(_{k,i}\) suggest simply finding a fixed value of \(\) that is multiplied by 2 for the second half of the network, which works best in all the other experiments (see Appendix E).

## 4 Experiments

We call our method TRGL for Transport-Regularized Greedy Learning. For the auxiliary classifiers, we use the architecture from DGL [7; 8], that is a convolution followed by an average pooling and a fully connected layer, which is very similar to that used by InfoPro , except for the Swin Transformer where we use a linear layer. We call vanilla greedy module-wise training with the same architecture but without our regularization VanGL, and we include its results in all tables for ablation study purposes. The code is available at github.com/block-wise/module-wise and implementation details are in Appendix E.

### Parallel module-wise training

To compare with other methods, we focus first on parallel training, as it performs better than sequential training and has been more explored recently. The first experiment is training in parallel 3 residual architectures and a VGG-19  divided into 4 modules of equal depth on TinyImageNet. We compare in Table 1 our results in this setup to three of the best recent parallel module-wise training methods: DGL , PredSim  and Sedona , from Table 2 in . We find that our TRGL has a much better test accuracy than the three other methods, especially on the smaller architectures. It also performs better than end-to-end training on the three ResNets. Parallel TRGL in this case with 4 modules consumes \(10\) to \(21\%\) less memory than end-to-end training (with a batch size of 256).

The second experiment is training in parallel two ResNets divided into 2 modules on CIFAR100 . We compare in Table 2 our results in this setup to the two delayed gradient methods DDG  and FR  from Table 2 in . Here again, parallel TRGL has a better accuracy than the other two methods and than end-to-end training. With only two modules, the memory gains from less backpropagation are neutralized by the weight of the extra classifier and there are negligible memory savings compared to end-to-end training. However, parallel TRGL has a better test accuracy by up to almost 2 percentage points.

   Architecture & Parallel VanGL & Parallel TRGL (ours) & PredSim & DGL & Sedona & E2E \\  VGG-19 & 56.17 \(\) 0.29 (\(\)\(27\%\)) & **57.28 \(\)** 0.20 (\(\)\(21\%\)) & 44.70 & 51.40 & 56.56 & 58.74 \\ ResNet-50 & 58.43 \(\) 0.45 (\(\)\(26\%\)) & **60.30 \(\)** 0.58 (\(\)\(20\%\)) & 47.48 & 53.96 & 54.40 & 58.10 \\ ResNet-101 & 63.64 \(\) 0.30 (\(\)\(24\%\)) & **63.71 \(\)** 0.40 (\(\)\(11\%\)) & 53.92 & 53.80 & 59.12 & 62.01 \\ ResNet-152 & 63.87 \(\) 0.16 (\(\)\(21\%\)) & **64.23 \(\)** 0.14 (\(\)\(10\%\)) & 51.76 & 57.64 & 64.10 & 62.32 \\   

Table 1: Test accuracy of parallel TRGL with 4 modules (average and 95\(\%\) confidence interval over 5 runs) on TinyImageNet, compared to DGL, PredSim, Sedona and E2E from Table 2 in , with memory saved compared to E2E as a percentage of E2E memory consumption in red.

The third experiment is training in parallel a ResNet-110 divided into two, four, eight and sixteen modules on STL10 . We compare in Table 3 our results in this setup to the recent methods InfoPro  and DGL  from Table 2 in . TRGL largely outperforms the other methods. It also outperforms end-to-end training in all but one case (that with 16 modules). With a batch size of 64, memory savings of parallel TRGL compared to end-to-end training reach \(48\%\) and \(58.5\%\) with 8 and 16 modules respectively, with comparable test accuracy. With 4 modules, TRGL training weighs \(24\%\) less than end-to-end-training, and has a test accuracy that is better by \(2\) percentage points (see Section 4.2 and Table 6 for a detailed memory usage comparison with InfoPro).

The fourth experiment is training (from scratch) in parallel a Swin-Tiny Transformer  divided into 4 modules on three datasets. We compare in Table 4 our results with those of InfoPro  and InfoProL, a variant of InfoPro proposed in . TRGL outperforms the other module-wise training methods. It does not outperform end-to-end training in this case, but consumes \(29\%\) less memory on CIFAR10 and CIFAR100 and \(50\%\) less on STL10, compared to \(38\%\) for InfoPro and \(45\%\) for InfoProL in .

Finally, we compare our method to InfoPro, DGL and Sedona in Table 5 below on a large scale experiment on ImageNet.

### Memory savings and training time

As seen above, parallel TRGL is lighter than end-to-end training by up to almost \(60\%\). The extra memory consumed by our regularization compared to parallel VanGL is between 2 and \(13\%\) of

   Dataset & Parallel VanGL & Parallel TRGL (ours) & InfoPro & InfoProL & E2E \\  STL10 & 67.00 \(\) 1.36 (\( 55\%\)) & **67.92**\(\) 1.12 (\( 50\%\)) & 64.61 (\( 38\%\)) & 66.89 (\( 45\%\)) & 72.19 \\ CIFAR10 & 83.94 \(\) 0.42 (\( 33\%\)) & **86.48**\(\) 0.54 (\( 29\%\)) & 83.38 (\( 38\%\)) & 86.28 (\( 45\%\)) & 91.37 \\ CIFAR100 & 69.34 \(\) 0.91 (\( 33\%\)) & **74.11**\(\) 0.31 (\( 29\%\)) & 68.36 (\( 38\%\)) & 73.00 (\( 45\%\)) & 75.03 \\   

Table 4: Test accuracy of parallel TRGL with 4 modules (average and 95\(\%\) confidence interval over 5 runs) on a Swin-Tiny Transformer, compared to InfoPro, InfoProL and E2E from Table 3 in , with memory saved compared to E2E as a percentage of E2E memory consumption in red.

   Architecture & Parallel VanGL & Parallel TRGL (ours) & DDG & FR & E2E \\  ResNet-101 & 77.31 \(\) 0.27 & **77.87**\(\) 0.44 & 75.75 & 76.90 & 76.52 \\ ResNet-152 & 75.40 \(\) 0.75 & **76.55**\(\) 1.90 & 73.61 & 76.39 & 74.80 \\   

Table 2: Test accuracy of parallel TRGL with 2 modules (average and 95\(\%\) confidence interval over 3 runs) on CIFAR100, compared to DDG, FR and E2E from Table 2 in .

   \(K\) & Par VanGL & Par TRGL (ours) & DGL & InfoPro S & InfoPro C & E2E \\ 
2 & 79.85 \(\) 0.93 & **80.04**\(\) 0.85 & 75.03 \(\) 1.18 & 78.98 \(\) 0.51 & 79.01 \(\) 0.64 & 77.73 \(\) 1.61 \\
4 & 77.11 \(\) 2.31 & **79.72**\(\) 0.81 & 73.23 \(\) 0.64 & 78.72 \(\) 0.27 & 77.27 \(\) 0.40 & 77.73 \(\) 1.61 \\
8 & 75.71 \(\) 0.55 & **77.82**\(\) 0.73 & 72.67 \(\) 0.24 & 76.40 \(\) 0.49 & 74.85 \(\) 0.52 & 77.73 \(\) 1.61 \\
16 & 73.57 \(\) 0.95 & **77.22**\(\) 1.20 & 72.27 \(\) 0.58 & 73.95 \(\) 0.71 & 73.73 \(\) 0.48 & 77.73 \(\) 1.61 \\   

Table 3: Test accuracy of Parallel (Par) TRGL with \(K\) modules (average and 95\(\%\) confidence interval over 5 runs) using a ResNet-110 on STL10, compared to DGL, two variants of InfoPro and E2E from Table 2 in .

end-to-end memory. Memory savings depend then mainly on the size of the auxiliary classifier, which can easily be adjusted. Note that delayed gradients method DDG and FR increase memory usage , and Sedona does not claim to save memory, but rather to speed up training . DGL is architecture-wise essentially identical to VanGL and consumes the same memory.

We compare in Table 6 the memory consumption of our method to that of InfoPro  on a ResNet-110 on STL10 with a batch size of 64 (so the same setting as in Table 3). InfoPro  also propose to split the network into modules that have the same weight but not necessarily the same number of layers. They only implement this for \(K{}4\) modules. When the modules are even in weight and not in depth, we call the training methods VanGL*, TRGL* and InfoPro*. In practice, this leads to shallower early modules which slightly hurts performance according to , and as seen below. However, TRGL* still outperforms InfoPro and end-to-end training, and it leads to even bigger memory savings than InfoPro*. We see in Table 6 below that TRGL saves more memory than InfoPro in two out of three cases (4 and 8 modules), and about the same in the third case (16 modules), with much better test accuracy in all cases. Likewise, TRGL* is lighter than InfoPro*, with better accuracy.

However, parallel module-wise training does slightly slow down training. Epoch time increases by \(6\%\) with 2 modules and by \(16\%\) with 16 modules. TRGL is only slower than VanGL by \(2\%\) for all number of modules due to the additional regularization term. This is comparable to InfoPro which reports a time overhead between 1 and \(27\%\) compared to end-to-end training. See Appendix F for details.

### Sequential full block-wise training

Block-wise sequential training, meaning that each module is a single residual block and that the blocks are trained sequentially, therefore requiring only enough memory to train one block and its classifier. Even though it has been less explored in recent module-wise training methods, it has been used in practice in very constrained settings such as on-device training [58; 57]. We therefore test our regularization in this section in this setting, with more details in Appendix G.

We propose here to use shallower ResNets that are initially wider. These architectures are well-adapted to layer-wise training as seen in . We check first in Table 10 in Appendix G that this architecture works well with parallel module-wise training with 2 modules by comparing it favorably on CIFAR10  with methods DGL , InfoPro  and DDG  that use a ResNet-110 with the same number of parameters.

We then train a 10-block ResNet block-wise on CIFAR100. In Tables 11 and 12 in Appendix G, we see that MLS training improves the accuracy of sequential training by \(0.8\) percentage points when the trainset is full, but works less well on small train sets. Of the two, the regularization mainly improves the test accuracy of MLS training. The improvement increases as the training set gets smaller and reaches 1 percentage point. While parallel module-wise training performs quite close to end-to-end training in the full data regime and much better in the small data regime, sequential and multi-lap sequential training are competitive with end-to-end training in the small data regime. Combining the multi-lap trick and the regularization improves the accuracy of sequential training by 1.2 percentage points when using the entire trainset. We report further results for full block-wise training on MNIST  and CIFAR10  in Tables 13 and 14 in Appendix G.

    &  &  \\  \(K\) & Par VanGL & Par TRGL (ours) & InfoPro & Par VanGL* & Par TRGL* (ours) & InfoPro* \\ 
4 & 27\(\%\) (77.11) & 24\(\%\) (**79.72**) & 18\(\%\) (78.72) & 41\(\%\) (77.14) & 39\(\%\) (**78.94**) & 33\(\%\) (78.78) \\
8 & 50\(\%\) (75.71) & 48\(\%\) (**77.82**) & 37\(\%\) (76.40) & & & \\
16 & 61\(\%\) (73.57) & 58\(\%\) (**77.22**) & 59\(\%\) (73.95) & & & \\   

Table 6: Memory savings using a ResNet-110 on STL10 split into \(K\) modules trained in parallel with a batch size of 64, as a percentage of the weight of end-to-end training. Average test accuracy over 5 runs is between brackets. Test accuracy of end-to-end training is 77.73\(\%\).

The \(88\%\) accuracy of sequential training on CIFAR10 in Table 13 is the same as in Table 2 of , which is the best method for layer-wise sequential training available, with VGG networks of comparable depth and width.

### Accuracy after each module

Finally, we verify that our method avoids the stagnation or collapse in accuracy with depth. In Figure 2 below, we show the accuracy of each module with and without the regularization.

On the left, from parallel module-wise training experiments from Table 3, TRGL performs worse than vanilla greedy learning early, but surpasses it in later modules, indicating that it does avoid early overfitting. On the right, from sequential block-wise training experiments from Table 13, we see a large decline in performance that the regularization avoids. We see similar patterns in Figure 3 in Appendix G with parallel and MLS block-wise training.

## 5 Limitations

The results in Appendix G show a few limitations of our method, as the improvements from the regularization are sometimes minimal on sequential training. However, the results show that our approach works in all settings (parallel and sequential with many or few modules), whereas other papers don't test their methods in all settings, and some show problems in other settings than the original one in subsequent papers (e.g. delayed gradients methods when the number of modules increases  and PredSim in ). Also, for parallel training in Section 4.1, the improvement from the regularization compared to VanGL is larger and increases with the number of modules (so with the memory savings) and reaches almost 5 percentage points. We show in Appendix H that our method is not very sensitive to the choice of hyperparameter \(\) over a large scale.

## 6 Related work

Layer-wise training was initially considered as a pre-training and initialization method [10; 43] and was recently shown to be competitive with end-to-end training [7; 45]. Many papers consider using a different auxiliary loss, instead of or in addition to the classification loss: kernel similarity , information-theory-inspired losses [53; 44; 41; 60] and biologically plausible losses [53; 45; 25; 11]. Methods , PredSim , DGL , Sedona  and InfoPro  report the best module-wise training results. [7; 8] do it simply through the architecture choice of the auxiliary networks. Sedona applies architecture search to decide on where to split the network into modules and what auxiliary classifier to use before module-wise training. Only BoostResNet  also proposes a block-wise training idea geared for ResNets. However, their results only show better early performance and end-to-end fine-tuning is required to be competitive. A method called ResIST  that is similar to block-wise training of ResNets randomly assigns ResBlocks to one of up to 16 modules that

Figure 2: Test accuracy after each module averaged over 10 runs with \(95\%\) confidence intervals. Left: parallel vanilla (VanGL, in blue) and regularized (TRGL, in red) module-wise training of a ResNet-110 with 16 modules on STL10 (Table 3). Right: sequential vanilla (VanGL, in blue) and regularized (TRGL, in red) block-wise training of a 10-block ResNet on \(2\%\) of CIFAR10 (Table 13).

are trained independently and reassembled before another random partition. More of a distributed training method, it is only compared to local SGD . These methods can all be combined with our regularization, and we do use the auxiliary classifier from .

Besides module-wise training, methods such as DNI , DDG  and FR , solve the update and backward locking problems with an eye towards parallelization by using delayed or predicted gradients, or even predicted inputs to address forward locking, which is what  do. But they observe training issues with more than 5 modules . This makes them compare unfavorably to module-wise training . The high dimension of the predicted gradient, which scales with the size of the network, makes  challenging in practice. Therefore, despite its simplicity, greedy module-wise training is more appealing when working in a constrained setting.

Viewing ResNets as dynamic transport systems  followed from their view as a discretization of ODEs . Transport regularization of ResNets in particular is motivated by the observation that they are naturally biased towards minimally modifying their input . We further linked this transport viewpoint with gradient flows in the Wasserstein space to apply it in a principled way to module-wise training. Gradient flows on the data distribution appeared recently in deep learning. In , the focus is on functionals of measures whose first variations are known in closed form and used, through their gradients, in the algorithm. This limits the scope of their applications to transfer learning and similar tasks. Likewise,  use the explicit gradient flow of \(f\)-divergences and other distances between measures for generation and generator refinement. In contrast, we use the discrete minimizing movement scheme which does not require computation of the first variation and allows to consider classification.

## 7 Conclusion

We introduced a transport regularization for module-wise training that theoretically links module-wise training to gradient flows of the loss in probability space. Our method provably leads to more regular modules and experimentally improves the test accuracy of module-wise parallel, sequential and multi-lap sequential (a variant of sequential training that we introduce) training. Through this simple method that does not complexify the architecture, we make module-wise training competitive with end-to-end training while benefiting from its lower memory usage. Being a regularization, the method can easily be combined with other layer-wise training methods. Future work can experiment with working in Wasserstein space \(W_{p}\) for \(p 2\), i.e. regularizing with a norm \(\|.\|_{p}\) with \(p 2\).