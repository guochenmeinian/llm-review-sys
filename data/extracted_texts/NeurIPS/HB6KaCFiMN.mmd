# Animate3D: Animating Any 3D Model with Multi-view Video Diffusion

Yanqin Jiang\({}^{1,2}\)1 Chaohui Yu\({}^{3,4}\)1 Chenjie Cao\({}^{3,4}\)1 Fan Wang\({}^{3,4}\)

Weiming Hu\({}^{1,2,5}\)1 Jin Gao\({}^{1,2}\)2

\({}^{1}\)State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)DAMO Academy, Alibaba Group \({}^{4}\)Hupan Lab

\({}^{5}\)School of Information Science and Technology, ShanghaiTech University

jiangyanqin2021@ia.ac.cn

{huakun.ych,caochenjie.ccj,fan.w}@alibaba-inc.com

{jin.gao,ymhu}@nlpr.ia.ac.cn

https://animate3d.github.io/

###### Abstract

Recent advances in 4D generation mainly focus on generating 4D content by distilling pre-trained text or single-view image-conditioned models. It is inconvenient for them to take advantage of various off-the-shelf 3D assets with multi-view attributes, and their results suffer from spatiotemporal inconsistency owing to the inherent ambiguity in the supervision signals. In this work, we present Animate3D, a novel framework for animating any static 3D model. The core idea is two-fold: 1) We propose a novel multi-view video diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D object, which is trained on our presented large-scale multi-view video dataset (MV-Video). 2) Based on MV-VDM, we introduce a framework combining reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the multi-view video diffusion priors for animating

Figure 1: Different supervision for 4D generation. MV-VDM shows superior spatiotemporal consistency than previous models. Based on MV-VDM, we propose Animate3D to animate any 3D model.

3D objects. Specifically, for MV-VDM, we design a new spatiotemporal attention module to enhance spatial and temporal consistency by integrating 3D and video diffusion models. Additionally, we leverage the static 3D model's multi-view renderings as conditions to preserve its identity. For animating 3D models, an effective two-stage pipeline is proposed: we first reconstruct motions directly from generated multi-view videos, followed by the introduced 4D-SDS to refine both appearance and motion. Benefiting from accurate motion learning, we could achieve straightforward mesh animation. Qualitative and quantitative experiments demonstrate that Animate3D significantly outperforms previous approaches. Data, code, and models are open-released.

## 1 Introduction

3D content creation has garnered significant attention due to its wide applicability in AR/VR, gaming, and movie industry. With the development of diffusion models [45; 44; 39; 20; 51; 6; 19] and large-scale 3D object datasets [13; 12; 37; 65; 70; 14], recent 3D foundational generations have seen extensive exploration through fine-tuned text-to-image (T2I) diffusion models [31; 40; 41; 52; 32; 49], as well as training large reconstruction models from scratch [22; 61; 47; 67; 54], leading the 3D assets creation to a new era. Despite the significant progress in static 3D representation, this momentum has not been paralleled in the realm of dynamic 3D content generation, also known as 4D generation.

4D generation is more challenging due to the difficulty of simultaneously maintaining spatiotemporal consistency in visual appearance and dynamic motion. In this paper, we mainly focus on two challenges: 1) _No foundational 4D generation models to unify both spatial and temporal consistency._ Though recent 4D generation works [43; 24; 5; 69; 29; 38; 63; 59; 4; 57; 66; 16] separately distill knowledge from pre-trained T2I and 3D diffusion models [39; 41; 31] and video diffusion models [51; 2; 6] to model multi-view spatial appearance and temporal motions respectively, we clarify that such a detached learning way suffers from inevitable accumulation of appearance degradation as the motion changed, as shown in Fig. 1 (SVD+Zero123). 2) _Failing to animate existing 3D assets through multi-view conditions._ With the development of 3D generations, animating existing high-quality 3D content becomes a common demand. However, previous works about 4D modeling from video [24; 57] or based on generated 3D assets [5; 29; 38; 66] are all based on text [39; 41] or _single-view_[31; 40] conditioned models, struggling to faithfully preserve multi-view attributes during the 4D generation, such as the back of butterfly in Fig. 1 is ignored by Zero123 .

To address these issues, we advocate for an approach better suited for 4D generation, that is, **animating any off-the-shelf 3D models with unified spatiotemporal consistent supervision**. In this way, it would be convenient to directly take advantage of various fast-developing 3D generation and reconstruction approaches based on a single foundational model, eliminating the accumulation of errors in modeling appearance and motion.

To this end, we propose a novel 4D generation framework called Animate3D in this paper, which can be divided into a foundational 4D generation model and a joint 4D Gaussian Splatting (4DGS) optimization. Formally, the foundational 4D model is a Multi-View Video Diffusion Model (MV-VDM) built upon the 3D generation model, MVDream , which can synchronously synthesize multi-view images with various temporal motions. Specifically, to better inherit the prior in previous 3D and video diffusion models trained on large-scale data, we propose a learnable plug-and-play spatiotemporal attention module, building upon the motion module in video diffusion [18; 6; 20] to expand the attention learning from the temporal domain to the spatial and temporal domain. Moreover, MV-VDM also includes the ability to refer to multi-view images, sufficiently preserving the identity and details of off-the-shelf 3D assets. Specifically, given multi-view images rendered from existing 3D assets or collected from real-world objects, we expand adaptive image-to-video work, I2V-Adapter , to multi-view version, called MV2V-Adapter, incorporating multi-view conditions to 4D learning through additionally spatial features and text embeddings. Enhanced by the multi-view appearance injection, we can disentangle the appearance learning from the motion learning, ensuring MV-VDM focuses on learning natural and coherent dynamic motions.

To further enable impressive animations from 3D objects that can be observed at any viewpoint and time, we jointly optimize the 4DGS  through both reconstruction and 4D Score Distillation Sampling (4D-SDS) losses based on our unified MV-VDM. Benefiting from the spatiotemporal consistent multi-view video generations, 4DGS can be roughly converged to proper results with only reconstruction loss, while 4D-SDS further improves the details and fine-grained motions. The Gaussian trajectory learned by our framework is surprisingly accurate and could be used to directly animate the mesh.

The main dilemma in building a foundational 4D generation model lies in the rarity of large-scale 4D datasets, which is non-trivial to collect but the key factor to drive our MV-VDM. In this work, we make the first attempt to build a large-scale multi-view video (4D) dataset, dubbed MV-Video. Specifically, MV-Video comprises about **84K** animations that are available under a public license, consisting of about **38K** animated 3D objects at all, which are rendered into over **1.3M** multi-view videos with minigpt4-video  generated prompts, to serve as the training dataset for our 4D foundation model.

We highlight the contribution of this paper as follows: 1) Animate3D is the first 4D generation framework to animate any 3D objects with detailed multi-view conditions. The framework is further extended to achieve mesh animation without skeleton rigging. 2) We propose the foundational 4D generation model, MV-VDM, to jointly model spatiotemporal consistency. 3) We present the largest 4D datasets MV-Video collected with about 84K animations and over 1.3M multi-view videos. Extensive experiments demonstrate that our data-driven approach can generate spatiotemporal consistent 4D objects, significantly outperforming previous counterparts.

## 2 Related Work

**3D Generation.** Early 3D generation works optimized single 3D object with CLIP loss  or Score Distillation Sampling (SDS)  from 2D text-to-image (T2I) diffusion models. Since the models providing supervision lacked 3D prior, those works usually suffered from spatial inconsistency, _i.e._, multi-face Janus problem [50; 28; 53]. To tackle this problem, on the one hand, some works [31; 40; 41; 32; 35] lifted the T2I diffusion to multi-view image diffusion by injecting new spatial attention layers and fine-tuning on large-scale 3D synthetic datasets [13; 12]. Although 3D consistency was improved, these optimization-based methods still required a relatively long time to optimize a 3D object. On the other hand, some feed-forward 3D generation foundation models [22; 61; 47; 67; 54; 60; 30], also trained on large-scale 3D datasets, were able to produce a good-quality 3D object in several seconds in an inference-only way. Inspired by the success of the data-driven approaches in 3D generation, we aim to construct a large-scale 4D generation dataset and take the pioneering step towards developing foundation models for 4D generation.

**Video Generation.** Video generation works started with text-to-video (T2V) generation [20; 42; 2; 18; 6; 19; 7; 10], subsequently followed by image-to-video (I2V) approaches [64; 17; 58; 6]. Previous T2V works usually built upon T2I diffusion models [20; 42; 19; 18; 21], leveraging their pre-trained weights by leaving the spatial blocks unchanged and inserting new temporal blocks to model the temporal camera or object motions. The I2V works [58; 17; 64], building upon the aforementioned T2V methods, typically incorporate image semantics into video models. This is achieved through cross-attention mechanisms between noisy frames and the conditional image, while retaining the motion module design from the T2V models unaltered. We draw inspiration from the development paradigm of video generation to design our 4D generation foundation model, which is a multi-view image conditioned multi-view video diffusion model building upon pre-trained multi-view 3D and video diffusion models.

**4D Generation.** The pioneering work of 4D generation is MAV3D , which is a text and image-conditioned 4D generation framework. MAV3D first proposed a multi-stage pipeline to optimize the static 3D object generation through the T2I model and subsequently learn motions from the T2V model . Following works [68; 5; 29; 4; 59] adopted a similar pipeline, and they further found that employing T2I  and 3D-SDS  are crucial for both object generation and motion learning stages. Without them, the quality of the generated object's appearance suffered a remarkable decline, and the motion-learning process was prone to failure. Very recently, Consistent4D  proposed a video-to-4D generation task, which used single-view video reconstruction and SDS from Zero123  for motion and appearance learning. This paradigm was adopted by following works [38; 63; 66; 57; 16; 11] and extended to text/image-to-video then video-to-4D generation. All aforementioned works heavily depend on the foundational model for SDS to preserve objects' appearance and attributes. However, existing 3D diffusion models struggle to refer to multi-viewconditions, restricting their broader applications to animate various off-the-shelf 3D assets without losing their multi-view attributes.

Furthermore, it is worth noting that existing 4D generation methods suffer from another issue, _i.e._, spatial and temporal inconsistency [43; 5; 29; 24; 38]. Because the diffusion models used for SDS were never trained with multi-view video (4D) datasets, missing the critical capacity to formulate spatial and temporal consistency simultaneously. Thus previous methods failed to properly trade off a good balance between appearance and motion learning. Please refer to Sec. B in appendix for detailed discussion and comparison.

In this work, we resort to disentangle 3D object generation/reconstruction and motion learning through a foundational 4D generation model, and propose a novel framework to animate any static 3D object with consistent multi-view attributes.

## 3 Method

Given a static 3D model, our goal is to animate it with a text prompt and use its multi-view renderings as image condition. This 4D generation task is particularly challenging as it requires ensuring spatial and temporal consistency of the appearance and motion, compatibility with the prompt, and preserving the identity of the static object. To address these challenges more fundamentally, we propose a novel framework, Animate3D, to animate any static 3D object. As depicted in Fig. 2, we divide the task into two parts: learning a multi-view video diffusion model (MV-VDM), and animating 3D object with multi-view videos generated by MV-VDM.

### Multi-view Video Diffusion Model (MV-VDM)

We propose a novel multi-view image conditioned multi-view video diffusion model, named MV-VDM. To inherit prior knowledge acquired by spatially consistent 3D models and temporally consistent video models trained on large-scale datasets, we advocate a baseline architecture by integrating them to utilize their pre-trained weights. In this work, we take MVDream  and AnimateDiff  for the 3D and the video diffusion model, respectively. To enhance the spatiotemporal consistency and ensure compatibility with the prompts and the object's multi-view images, we propose an efficient plug-and-play spatiotemporal attention module combined with an image-conditioning approach. Our

Figure 2: Illustration of our proposed multi-view video diffusion modelâ€”**MV-VDM** (upper part) and our **Animate3D** framework (lower part). MV-VDM, trained on our presented large-scale 4D dataset MV-Video, can generate spatiotemporal consistent multi-view videos. Animate3D, based on MV-VDM, combines reconstruction and 4D-SDS optimization to animate any static 3D models.

MV-VDM is trained on our presented large-scale multi-view video dataset, MV-Video, which is introduced in Sec. 4.

**Spatiotemporal Attention Module.** As illustrated in Fig. 2, we insert a novel spatiotemporal attention module after cross-attention layers. The proposed spatiotemporal attention module comprises two parallel branches: the left branch is for spatial attention, and the right branch is for temporal attention. For spatial attention, we adopt the same architecture as the multi-view 3D attention in MVDream . Specifically, the original 2D self-attention layer is converted into 3D by connecting \(n\) different views. In addition, we incorporate 2D spatial encoding, specifically sinusoidal encoding, into the latent features to enhance spatial consistency. As for temporal attention, we keep all designs of the temporal motion module from the video diffusion model  unchanged in order to reuse their pre-trained weights. Based on the features of these two branches, we employ an alpha blender layer with a learnable weight to achieve features with enhanced spatiotemporal consistency. It is worth noting that we do not apply spatiotemporal attention across all views and frames due to the prohibitive GPU memory requirements that render training infeasible. Instead, our parallel-branch design offers an efficient and practical alternative. Specifically, we first reshape the input feature of spatiotemporal attention module \(X^{(b n f) c h w}\) into two forms, \(X_{l}^{(b f)(n h w) c}\) for spatial branch and \(X_{r}^{(b n h w) f c}\) for temporal branch. The spatiotemporal attention is then computed as:

\[ X_{out}&=_{}(X_{l}W_{Q}^{s},X_{l}W_{K}^{s},X_{l}W_{V}^{s})W_{Q}^{s}+\\ &(1-)_{}(X_{r}W_{Q}^ {t},X_{r}W_{K}^{t},X_{r}W_{V}^{t})W_{O}^{t},\] (1)

where \(\) denotes the learnable weight, \(W_{Q}^{s/t},W_{K}^{s/t},W_{V}^{s/t},W_{O}^{s/t}\) represent the corresponding projection matrices. \(b,n,f,h,w,c\) are the batch size, views, frames, height, width, and channels of the image features, respectively.

**Multi-view Images Conditioning.** Inspired by I2V-Adapter , we add a new attention layer, termed MV2V-Adapter, parallel to the existing frozen multi-view 3D self-attention layer within the proposed spatiotemporal block, as shown in Fig. 2. Concretely, noisy frames are first concatenated along the spatial dimension. These are then used to query the rich contextual information from the multi-view conditional frames, which are extracted using the frozen 3D diffusion model. Next, we add the output of the MV2V-Adapter layer to that of the original multi-view 3D attention layer of MVDream. Thus, for each frame \(i\{1,...,f\}\), denoting the multi-view input, output, and conditional frames' features as \(X^{1:n,i}\), \(X^{1:n,i}\),\({}_{out}^{}\), and \(X^{1:n,1}\), we have:

\[ X_{out}^{1:n,i}&=(X^{1:n,i}W_{Q},X^{1:n,i}W_{K},X^{1:n,i}W_{V})W_{O}+\\ &(X^{1:n,i}W_{Q}{}^{},X^{1:n,1}W_{K},X^{ 1:n,1}W_{V})W_{O}{}^{},\] (2)

where \(W_{Q}\), \(W_{K}\), \(W_{V}\) and \(W_{O}\) are projection matrices in original self-attention layer, while \(W_{Q}{}^{}\) and \(W_{O}{}^{}\) are those in the newly added layer. We find this simple cross-attention operator can effectively improve the object's appearance consistency in the generated video. After that, as shown in the spatiotemporal block in Fig. 2, we employ two cross-attention layers to align the text prompt and preserve the object's identity, respectively. The left one is inherited from MVDream, while the right one is pre-trained in IP-Adapter .

**Training Objectives.** The training process of our multi-view video diffusion model is similar to Latent Diffusion Model . Specifically, the sampled multi-view video data \(q_{0}^{1:n,1:f}\) are first encoded into latent feature \(z_{0}^{1:n,1:f}\) via encoder \(\) frame by frame and view by view. Then we add noise using the forward diffusion scheduler: \(z_{t}^{1:n,2:f}=}z_{0}^{1:n,2:f}+}\), where \(_{t}\) is a weighted parameter and \(\) is Gaussian noise. Note that, following I2V-Adapter, we keep the first frame, _i.e._, the condition multi-view frames clean, and only add noise to the rest of the frames. During training, the proposed MV-VDM takes as input the clean latent code \(z_{0}^{1:n,1}\), noisy latent code \(z_{t}^{1:n,2:f}\), text prompt embedding \(y\), and the camera parameters \(^{1:n}\), and outputs the noise strength, supervised by \(_{2}\) loss. The training objective of our MV-VDM is calculated as:

\[_{}=_{(q_{0}),y, (0,f),t}[||-_{}(z_{0}^{1:n,1},z_{t}^{1:n,2:f}, t,y,^{1:n})||_{2}^{2}],\] (3)

where \(\) denotes the diffusion model. It is important that we keep the entire multi-view 3D attention module frozen and only train the MV2V-Adapter layer and the spatiotemporal attention module to conserve GPU memory and accelerate training. Moreover, as the multi-view images of the first frame, \(z_{0}^{1:n,1}\), serves as the condition images, we calculate the loss only for the latter \(f-1\) frames, _i.e._, \(z_{0}^{1:n,2:f}\).

### Reconstruction and Distillation of 4DGS

Based on our 4D generation foundation model MV-VDM, we propose to animate any off-the-shelf 3D object. For efficiency, we take 3D Gaussian Splatting (3DGS)  as the static 3D object representation, and animate it by learning motion fields represented by Hex-planes, as in .

**4D Motion Fields.** As in 4D Gaussian Splatting (4DGS) , we represent the motion fields by Hex-planes . Denoting the static 3DGS as \(=\{,,,r,s\}\), where \(\), \(\), \(\), \(r\), and \(s\) represent the position, color, opacity, rotation, and scale, respectively. The motion module \(\) predicts changes in position, rotation, and scale for each Gaussian point in frame \(i\) by interpolating the Hex-planes \(R\). The motion fields computation can be formulated as:

\[=_{l}_{}(R^{},(,i)),\] (4)

\[=_{}(), r=_{r}(), s=_{s}(),\] (5)

where \(l\) equals to the scales in Hex-plane, and interp() denotes interpolating the Gaussian points on the specific plane \(\) to obtain corresponding motion features. We have \(\{(x,y),(x,z),(y,z),(x,t),(y,t),(z,t)\}\). Therefore, Gaussian \(^{}\) at time \(t\) is updated as follows:

\[^{}=\{+,,,r+  r,s+ s\}.\] (6)

To better preserve the appearance of static 3D objects, we keep certain attributes, specifically opacity \(\) and color \(\), unchanged.

**Coarse Motion Reconstruction.** Based on the spatiotemporal consistent multi-view videos generated by MV-VDM, we first leverage a 4DGS reconstruction stage to directly reconstruct the coarse motions. Specifically, we use a simple but effective \(_{2}\) loss as our \(_{}\), which is calculated as:

\[_{}=_{i=1}^{n}_{j=1}^{f}||\;- }\;||^{2},\] (7)

where \(\) and \(}\) denote the multi-view and multi-frame renderings and the corresponding ground truth. As verified in Fig. 3, this reconstruction stage can already learn high-quality coarse motions by leveraging the generated multi-view videos of MV-VDM.

**4D-SDS Optimization.** To better model the fine-level motions, we introduce a 4D-SDS optimization stage to distill the knowledge of our multi-view video diffusion model. The 4D-SDS loss \(_{}\) is a variant of \(_{0}\)-reconstruction SDS loss and can be formulated as:

\[_{}(,,z=(g( ())))=_{t,,}[||z-}|| _{2}^{2}],}=-_{t}_{}}{_{t }},\] (8)

where \(z\) and \(z_{0}\) are latent feature of the rendered image and the estimation of clean latent feature from current noise prediction \(_{}\), respectively, \(g\) represents the rendering function. \(_{t}\) and \(_{t}\) are the signal and noise scale controlled by the noise scheduler, respectively.

**Training Objectives.** In addition to \(_{}\) and \(_{}\), we introduce a variant of As-Rigid-As-Possible (ARAP) loss  to facilitate the rigid movement learning as well as the maintenance of the high-quality appearance of the static object. The ARAP loss \(_{}\) in our work is defined as:

\[_{}(p_{j})=_{i=2}^{f}_{k_{c_{i}} }w_{j,k}||(p_{j}^{i}-p_{k}^{i})-R_{j}((p_{j}^{1}-p_{k}^{1})||^{2},\] (9)

where \(_{j}\) is estimated from a rigid transformation using Singular Value Decomposition (SVD) according to :

\[_{j}=_{R(3)}_{k_{c_{i}}} w_{j,k}||(p_{j}^{i}-p_{k}^{i})-_{j}((p_{j}^{1}-p_{k}^{1})||^{2}.\] (10)

\(_{c_{j}}\) denotes the set of points within a fixed radius of \(p_{j}\), and \(w_{j,k}=(-}{d})\) where \(d_{jk}\) is the distance between center of \(p_{j}\) and \(p_{k}\), measuring the impact of \(p_{k}\) on \(p_{j}\). This loss encourages the generated dynamic object to be locally rigid, and it enhances the learning with rigid movement.

In summary, the training objectives for animating off-the-shelf 3DGS object is:

\[=_{1}_{}+_{2}_{ }+_{3}_{},\] (11)

where \(_{1}\), \(_{2}\), and \(_{3}\) are weighted parameters.

### Extension to Mesh Animation

To directly utilize high-quality mesh generated from commercial 3D generation tools and crafted by human experts, we extent our framework to mesh animation, producing animated mesh compatible with standard 3D rendering pipelines.

We initialize the 3DGS representation of the given object by vertices and triangles of the static mesh. Specifically, the color is determined by vertex color and we average the connected edges for the scale. Opacity and rotation are set to fully visible and zero rotation quaternion, respectively. The coarse 3DGS is animated following the motion reconstruction steps as described in the above sections. We utilize the per-vertex Gaussian trajectory to deform the static mesh in a straightforward way without skeleton rigging, control point selection or complicated deformation algorithms. As shown in Fig. 6 and our project page, the results are surprisingly good despite the simplicity of the solution.

## 4 Experiment

### Setup

**Training Dataset.** To train our MV-VDM, we build a large-scale multi-view video dataset, MV-Video. Concretely, we render multi-view videos of **37,857** animated 3D models collected from Sketchfab . Each model has **2.2** animations on average, resulting in **83,716** animations in total. Each animation is 2 seconds long at 24 fps. _Note that animated models that are not allowed to be used to generate AI programs are filtered_. The statistical information of our MV-Video dataset is reported in Table 1. For more details about the rendering settings and data examples, please refer to our Appendix ( D). We will release this dataset to further advance the field of 4D generative research.

**Implementation Details.** We sample 8 frames evenly for each animation to train our MV-VDM. We use the Adawm optimizer with a learning rate of \(4e-4\) and a weight decay \(0.01\), and train the model for 20 epochs with a batch size of 2048. When inference, we set the sampling step to 25 and adopt freeinit  to get stable results when animating 3D objects. As for 4D generation, the resolution and feature dimension of the Hex-planes are set to \(\) and 16, respectively. We perform coarse motion reconstruction for the first \(1000\) iterations with a batch size of 32 (4 views, 8 frames), and then add 4D-SDS optimization for another 400 iterations. Learning rate is 0.0015 initially and decreases linearly to 0.0005 at the end of reconstruction stage. \(_{1}\), \(_{2}\) and \(_{3}\) in Eq. 11 are set to \(10.0\), \(0.01\) and \(0.5\), respectively. It costs 3 days to train our MV-VDM on 32 80G A800 GPUs, and the optimization for 4D generation takes around 30 minutes on a single A800 GPU per object.

**Evaluation Dataset.** For the evaluation of MV-VDM, we render multi-view images from 128 static 3D objects and then generate multi-view videos conditioned on them. Following the evaluation setting of VBench , we use four different random seeds for each object and report the average results. For the evaluation of 4D generation, we generate 25 objects across various categories using the large 3DGS reconstruction model GRM . Input images for GRM and animation prompts used in this work are provided in our Appendix ( E.1).

**Evlaution Metrics.** We adopt the evaluation protocol proposed in VBench , which is a popular video generation benchmark consisting of both T2V and I2V evaluation tools. The I2V evaluation protocol contains 9 evaluation metrics, and we choose 4 for our evaluation, _i.e._, I2V Subject, Motion Smoothness, Dynamic Degree, and Aesthetic Quality, measuring the consistency with the given image, the motion smoothness, the motion degree, and the appearance quality, respectively. We abbreviate them as **I2V**, **M. Sm.**, **Dy. Deg.** and **Aest. Q.** in the experiment section. Values of all metrics are the higher, the better, except for Dynamic Degree, since we observe that sometimes completely failed results present extremely high dynamic degree. For more details about the introduction and calculation of the evaluation metrics, please refer to our Appendix ( E.2).

**Comparison Methods.** We compare our work with 4Dfy  and DreamGaussian4D (DG4D)  on the task of animating any given 3D object using their official implementations. They represent the state-of-the-art in 4D generation methods, starting by generating a static 3D object using 3D-SDS in

  Model ID & Animations & Avg. Animations per ID & Max Animations per ID & Multi-view Videos \\ 
37,857 & 83,716 & 2.2 & 6.0 & 1,339,456 \\  

Table 1: Statistical information for our multi-view video (MV-Video) dataset.

the initial stage, and subsequently animating it via video SDS and single-view video reconstruction in later stages. For 4D representation, 4Dfy and DG4D adopt dynamic NeRF [34; 26] and 4DGS , respectively. For a fair comparison, we replace the dynamic NeRF in 4Dfy with 4DGS used in both our work and DG4D, and also apply ARAP loss for motion regularization. For DG4D, we keep the 4DGS representation and motion regularizations in their work unchanged.

### Comparison with State-of-the-Art

In this section, we perform comprehensive comparisons with previous works, including quantitative and qualitative comparisons and user studies.

**Quantitative Comparison.** As shown in Tab. 1(a), our method significantly outperforms 4Dfy and DG4D in terms of **I2V**, **Dy. Deg.**, and **Aest. Q.**. This indicates our generation results have good alignment with the given static 3D object (I2V Subject), dynamic motion (Dynamic Degree), and superior appearance (Aesthetic Quality). For Motion Smoothness, we slightly lag behind 4Dfy, since 4Dfy always generates nearly static results, as illustrated by the 0.0 Dynamic Degree in the first row of Tab 1(a). Generally, our method is able to animate 3D object with smooth and dynamic motion, at almost no cost of sacrificing their high-quality appearance, facilitating customized and high-quality dynamic 3D object creation.

**Qualitative Comparison.** As shown in Fig. 3, it is obvious that 4Dfy's results are blurred and deviate much from the given 3D object, owing to the use of text-conditioned diffusion models to optimize the motion and appearance. Additionally, its generated objects are almost static. This is because at the beginning of the training process, the noisy rendered image sequence, _i.e._, the input to the T2V model, has no temporal changes, which misleads the video diffusion model to generate almost static supervisions, as illustrated in Fig 1. For DG4D, its results align well with the given 3D object in front

    & **I2V** \(\) & **M. Sem. \(\)** & **Dy. Deg.** & **Aest. Q. \(\)** \\ 
4Dfy (Gau.)  & 0.783 & **0.996** & 0.0 & 0.497 \\ DG4D  & 0.898 & 0.986 & 0.477 & 0.529 \\ Ours & **0.982** & 0.991 & **0.597** & **0.581** \\   

Table 2: Quantitative comparisons with state-of-the-art methods.

Figure 3: Qualitative comparison with state-of-the-art methods.

view, _i.e._, the view used for generating guided video. However, it doesn't align with the object in the novel view, as indicated by the _tail in Bear and Penguin_, _distorted goggles in bear_, and _blurred back and side views_ in Fig 3. This is because it adopts Zero123 to optimize the novel views. Zero123 only conditions on the front view, leading to NVS optimization favoring pre-trained data distributions, which could lead to potential appearance degradation. More importantly, DG4D fails when the object in the guided video is assigned with movements toward the camera. For example, the frog is moving forth and back in the guided video, however, DG4D interprets it as enlargement and reduction of the object. The same thing comes to the penguin which nods towards the camera and leans forward. This misinterpretation usually results in blurry effect and strange appearance.

In contrast, our method, leveraging the spatiotemporal consistent muti-view prior, manages to deal with motion towards the camera, as demonstrated by the bear's raised front paw (our model takes the front view and its orthogonal views as the condition view, not depicted in the image). Besides, we successfully maintain the high quality appearance of the given 3D object when generating natural motion. Please refer to the videos in our _supplementary material_ for a more intuitive comparison.

**User Study.** We conduct a user study among 20 people on the 25 dynamic objects and report the averaged results in Tab. 1(b). The participants are asked to score the generated dynamic objects from 1 to 5, according to the alignment with the given text (**Align. Text**) and static object (**Align. 3D**), motion quality (**Mot.**), and appearance quality (**Appr.**). The user study proves the superiority of our method. Please refer to the Appendix ( 0.E.3) for more details.

### Ablation

**Multi-view Video Diffusion.** In Tab 2(a), we validate the effectiveness of the proposed SpatioTemporal Attention (short as S.T. Att.) and the pre-trained weight from video diffusion model (short as Pre-train). We replace the proposed spatiotemporal block with temporal block from AnimateDiff, and

Figure 4: Ablation for multi-view video diffusion.

Figure 5: Ablation for 3D object animation.

Table 3: Ablation Studiesthis leads to performance drop in I2V Subject, Motion Smoothness and Aesthetic Quality. Dynamic Degree seems to be enhanced, but that is caused by the increase of unstable failure cases. The same tendency could be observed in experiments w/o pre-trained video diffusion weight. Therefore, we think both designs are necessary for generating multi-view videos consistent with the given multi-view images and with high-quality appearance and motion. Qualitative ablations in Fig 4 further demonstrate this point.

**4D Object Optimization.** The ablations for 4D object optimization are shown in Tab. 2(b) and Fig. 5. The quantitative results in Tab. 2(b) indicate both SDS and ARAP losses improve the alignment with the 3D object (I2V Subject), Motion Smoothness, and Aesthetic Quality. However, the Dynamic Degree decreases. We suppose the decrease in dynamic degree is mainly caused by the removal of floaters and blurry effects, which are also taken into account of dynamic degree, as shown in Fig. 5. The two losses might slightly decrease the motion amplitude, but generally, we think the overall performance is improved when applying them.

### Mesh Animation

We provide mesh animation results in Fig. 6. Static meshes are generated by commercial 3D generation tools. For more results, please visit our project page.

## 5 Conclusion

In this work, we present Animate3D, a novel framework for animating any off-the-self 3D object. Animate3D disentangles the 4D object generation into a foundational 4D generation model, MV-VDM, and a joint 4DGS optimization pipeline based on MV-VDM. MV-VDM is the first 4D foundation model, which can generate spatiotemporal consistent multi-view videos conditioned on multi-view renderings of a static 3D object. To train MV-VDM, we present the largest multi-view video (4D) dataset, MV-Video, containing about 84K animations with over 1.3M multi-view videos. Based on MV-VDM, we propose an effective pipeline to animate any static 3D object by jointly optimizing the 4DGS via both reconstruction and 4D-SDS. Animate3D is a highly practical solution for downstream 4D applications since it can animate any generated or reconstructed 3D objects. Data, codes, and pre-trained weights will be released to facilitate the research in 4D generation.

Figure 6: Visualizations of mesh animation. We present RGB and textureless renderings of two mesh animation results. Best viewed by zooming in.

Acknowledgements

The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This work was supported in part by the Natural Science Foundation of China (Grant No. 62192782, U22B2056, 62422317), the Beijing Natural Science Foundation (Grant No. L223003, JQ22014), the Natural Science Foundation of China (62036011, U2033210, 62102417, 62222206, 62172413), the Project of Beijing Science and technology Committee (Project No. Z231100005923046). Jin Gao was also supported in part by the Youth Innovation Promotion Association, CAS. This work was also supported by Damo Academy through Damo Academy Research Intern Program.