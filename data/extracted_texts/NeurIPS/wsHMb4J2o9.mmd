# The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks

Lenaic Chizat

Institute of Mathematics, EPFL

Lausanne, Switzerland

lenaic.chizat@epfl.ch

&Praneeth Netrapalli

Google DeepMind

pnetrapalli@google.com

###### Abstract

Deep learning succeeds by doing hierarchical feature learning, yet tuning hyper-parameters (HP) such as initialization scales, learning rates etc., only give indirect control over this behavior. In this paper, we introduce a key notion to predict and control feature learning: the angle \(_{}\) between the feature updates and the backward pass (at layer index \(\)). We show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple and general _feature speed formula_ in terms of this angle \(_{}\), the loss decay, and the magnitude of the backward pass. This angle \(_{}\) is controlled by the conditioning of the layer-to-layer Jacobians and at random initialization, it is determined by the spectrum of a certain kernel, which coincides with the Neural Tangent Kernel when \(=\). Given \(_{}\), the feature speed formula provides us with rules to adjust HPs (scales and learning rates) so as to satisfy certain dynamical properties, such as feature learning and loss decay. We investigate the implications of our approach for ReLU MLPs and ResNets in the large width-then-depth limit. Relying on prior work, we show that in ReLU MLPs with iid initialization, the angle degenerates with depth as \((_{})=(1/)\). In contrast, ResNets with branch scale \(O(1/})\) maintain a non-degenerate angle \((_{})=(1)\). We use these insights to recover key properties of known HP scalings (such as \(\)P), and also introduce a new HP scaling for large depth ReLU MLPs with favorable theoretical properties.

## 1 Introduction

The ability of deep Neural Networks (NNs) to learn hierarchical representations of their inputs has been argued to be behind their strong performance in data-intensive machine learning tasks . Yet, the process via which gradient-based training leads to feature learning remains mysterious and defies our intuition; some architectures can even reach zero loss without feature learning at all . This limited understanding makes it difficult to design NNs architectures and hyper-parameters (HP) scalings, and begs the development of tools to quantify feature learning.

In this paper, we demonstrate that the _backward-feature angle_ (BFA) \(_{}\) between the feature updates and the backward pass (at layer index \(\)) is a central object in this quest. Indeed, we show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple _feature speed formula_ in terms of this angle, the loss decay and the magnitude of the backward pass. Given the knowledge of \(_{}\), this leads to a general approach to quantify key dynamical properties of the training dynamics of NNs - such as the speed of feature learning and loss decay - and to characterize the HP scalings satisfying these properties.

ContributionsOur contributions are the following:* We prove the _feature speed formula_ (Thm 2.1) which quantifies the feature updates in terms of the BFA \(_{}\), the loss decay and the magnitude of the backward pass at layer \(\). This formula, valid in any architecture and with an elementary proof, helps exploring the space of HP scalings, and understanding when feature learning arises.
* In Section 3, we develop tools to quantify the BFA. In particular, we show that, in the _batch-size_\(1\)_case_, \(_{}\) can be estimated in terms of the spectrum of the backward to feature kernel (BFK) \(K_{}\) and is related to the conditioning of layer-to-layer Jacobians (Thm. 3.2). We study the case of MLPs and ResNets at random initialization, and obtain that for a depth \(L\), \((_{L})=(L^{-}{{2}}})\) for ReLU MLPs (Prop. 5.1, exploiting a result in Jelassi et al. (2023)) and that \((_{L})=(1)\) for linear ResNets with branch scale \(O(L^{-}{{2}}})\) (Prop. 5.2).
* In Section 4, we consider several properties of NN training dynamics that can be conveniently studied with our tools, including feature learning and loss decay. Enforcing these properties leads to explicit contraints on the magnitude of the forward, backward pass and learning rates in general architectures (Prop. 4.1).
* In Section 5, we show how various HP scalings for large width-then-depth MLPs and ResNets can be characterized by enforcing these properties. In particular we recover depth \(\)P (Bordelon et al., 2023; Yang et al., 2023b) for ResNets (Table 2) and, for ReLU MLP, we introduce a scaling with output scale \(}}{}\) (Table 1) that does not suffer from vanishing loss decay, in contrast to the one studied in (Jelassi et al., 2023).
* Finally, in Section 6 we develop a more "axiomatic" approach: starting from a minimal list of desiderata, which include a notion of gradient stability, we show that we recover, in a certain extent, the convenient properties considered in Section 4. This section focuses on homogeneous architectures for which we show, along the way, a _backward speed formula_ (Prop. 6.1) and an invariance under block-wise rescaling (Prop. 6.2).

Related workThe theory of NNs has recently benefited from important insights from asymptotic analyses in the large width and/or depth limits. Our work is in the continuity of those.

Analyses of wide and deep NNs at random initialization led to identifying critical initialization scalings that enable signal propagation (Poole et al., 2016; Hanin and Rolnick, 2018; Hanin, 2018). They also identified dynamical isometry (Pennington et al., 2018), namely the concentration of the singular spectrum of the layer-to-layer Jacobians around \(1\), as an important indicator of training performance. Our analysis gives a concrete justification of the link between dynamical isometry and successful training, as we show that it is related to the alignment between the backward pass and feature updates. These questions have also been studied in ResNets, see e.g. (Hayou et al., 2021; Marion et al., 2022; Li et al., 2021) for signal propagation and (Tarnowski et al., 2019; Ling and Qiu, 2019) for dynamical isometry.

In 2018, two viewpoints for the dynamics of wide NNs were simultaneously introduced: a feature learning limit for two-layer MLPs (Mei et al., 2018; Chizat and Bach, 2018; Rotskoff and Vanden-Eijnden, 2018) and a limit without feature learning for general NNs (Jacot et al., 2018; Du et al., 2018; Allen-Zhu et al., 2019). These works highlighted the crucial role of HP scalings - learning rates and initialization - in the behavior of large NNs (Chizat et al., 2019).

In order to classify HPs scalings, (Yang and Hu, 2021) formulated the _maximal update_\(\)-criterion (it is part of the properties we study in Section 4). This criterion led to a full classification of HP scalings in the infinite hidden width limit (at fixed depth), and singled-out the so-called \(\)-parameterization (\(\)P) as ideal for this criterion. We note that, provided alignment holds, our analysis allows in particular to characterize \(\)P in an elementary way. See also (Yang et al., 2023a) for another simple derivation of \(\)P using matrix spectral norms (but that does not give tight control on the magnitude of feature learning and does not a priori apply to the large depth asymptotics). Several works have since shown the practical value of these analyses in predicting the behavior of NNs (Vyas et al., 2023) and improving HP tuning (Yang et al., 2021).

When restricted to the output layer of a NN, our notion of alignment/BFA coincides with that studied in Baratin et al. (2021); Atanasov et al. (2021); Lou et al. (2022); Wang et al. (2022) and the BFK we consider coincides with the NTK (Jacot et al., 2018). We extend these concepts to study and quantify feature learning at _any_ layer (not just at the output layer). Here we focus on the batch-size \(1\) setting, but the large batch-size setting of these works is a natural next direction for our analysis.

Finally, several recent works have studied feature learning in infinite width and depth NNs, starting with (Jelassi et al., 2023) for MLPs, and (Bordelson et al., 2023; Yang et al., 2023b) for ResNets. The two latter identified the \(1/}\) branch scaling as providing desirable properties, in particular that of _HP transfer_(Yang et al., 2021). These works take the infinite width limit as a first step in their analysis, before studying the resulting objects, resulting in a technical analysis. In our approach, we first take the step-size to \(0\) (as in (Jelassi et al., 2023)) and study in detail the structure of the back-propagation equations, before taking the large width-then-depth limit, as a last step.

NotationsFor integers \(a,b\), we write \([a\,:\,b]=\{a,,b\}\). For any vector \(x^{m}\) we denote by \(\|x\|_{} m^{-1/2}\|x\|_{2}\) its root mean-square (RMS) norm. We use this as a proxy for the typical entry size of a vector, which is justified as long as that vector is dense.

## 2 The Feature Speed Formula

Consider a depth-\(L\) NN architecture defined by the recursion, for \([1:L]\),

\[f_{0}^{m_{0}}, f_{}=T_{}(f_{-1},w_{})^{m_{}}, =(f_{L})\] (1)

where \(w_{}^{p_{}}\) are trainable parameters and we assume that the maps \(T_{}:^{m_{-1}}^{p_{}}^{m_{ }}\) admit elementary (log-exp) selections1(Bolte and Pauwels, 2020). By flattening the tensors, one can encode most practical NN architectures in Eq. (1). For instance, \(m_{0}\) is typically the product of batch-size (or context length) with input dimension. We denote by \(b_{}=}{ f_{}}^{} ^{m_{}}\) the vectors of the backward pass. A gradient descent (GD) step with layerwise learning-rate (LR) \(_{} t>0\) for \([1:L]\) consists in adding to each \(w_{}\) the update

\[ w_{}=-_{} t_{}=-_ {} t}{ w_{}} ^{}.\]

We are interested on the evolution of the NN over a single GD step with infinitesimally small step-size \( t 1\). For any quantity \(x\) associated to the NN, we denote \(\) its instantaneous velocity \(_{ t 0}\) when it exists. In particular, we have \(_{}=-_{}_{}\).

The following identity is the seed of our approach. It expresses at any training time the speed of features in terms of other interpretable quantities, including the _backward to feature angle_ (BFA) \(_{v}\).

**Theorem 2.1** (Feature speed formula).: _Let \(v[1\,:\,L]\). If \(_{ v}_{}\|_{}\|_{2}^{2}=0\) then \(_{v}=0\). Otherwise, the (non-oriented) angle \(_{v}\) between \(_{v}\) and \(-b_{v}\) is well defined in \([0,}{{2}}[\) and it holds_

\[\|_{v}\|_{2}=_{}\|_{}\|_{2}^{2}}{(_{v})\|b_{v}\|_{2}}\,.\] (2)

Proof.: By the chain rule, we have \(_{v}=_{ v}}{ w_{}}_ {}=-_{ v}_{}}{ w_{}} }{ w_{}}^{}\). It follows

\[-b_{v}^{}_{v}=_{ v}_{}}{ f_{v}}}{ w_{}} }{ w_{}}^{}=_{  v}_{}}{ w_{}} }{ w_{}}^{}=_{  v}_{}\|_{}\|_{2}^{2}.\] (3)

Clearly, if \(_{ v}_{}\|_{}\|_{2}^{2}=0\) then \(_{}=0\) for \( v\) and thus \(_{v}=0\). Otherwise \(_{v}\) is well defined and it holds \(\|b_{v}\|_{2}\|_{v}\|_{2}(_{v})=-b_{v}^{}_{v}=_{  v}_{}\|_{}\|_{2}^{2}\) and the claim follows. (In terms of the BFK defined below, Eq. (3) is equivalent to \(b_{v}^{}K_{v}b_{v}=_{ v}_{}\|_{}\|_ {2}^{2}\), for \(v[1\,:\,L]\).) 

To better appreciate the content of Thm. 2.1, let us re-express it in terms of root mean-square (RMS) norms. Let \(}_{ v}_{ v}_{}\|_{ }\|_{2}^{2}\) be the contribution to the loss decrease of all the parameters before \(f_{v}\) in the forward pass, and note that \(}_{ L}=}\). Then, the identity (2) rewrites as

\[\|_{}}{}_{ v}}=) m_{v}\|b_{v}\|_{}} S_{v}.\] (4)Here \(S_{v}\) can be interpreted as the _sensitivity_ (and, in the terminology of (Chizat et al., 2019), \(1/S_{v}\) as the _laziness_) of the feature \(v\): it is the proportionality factor between loss decay and feature speed. This formula is valid at any training time and involves three key quantities: the scale of the backward pass \(\|b_{v}\|_{}\), the size of the feature \(m_{v}\), and the BFA \(_{v}\). Let us now build tools to quantify the BFA.

## 3 Quantifying the backward-feature angles (BFA)

Information about the BFA \(_{v}\) can be gained from the Backward to Feature Kernel (BFK).

**Definition 3.1** (Backward to Feature Kernel).: _For \(v[1:L]\), the BFK is the psd matrix defined as_

\[K_{v}_{ v}_{}}{  w_{}}}{ w_{}} ^{}^{m_{v} m_{v}}.\] (5)

By construction, it holds \(_{v}=-K_{v}b_{v}\). In other words, the BFK takes a backward pass vector as input and returns the (negative of the) feature velocity. For \(v=L\), \(K_{v}\) coincides with the Neural Tangent Kernel (Jacot et al., 2018). We now show how the spectrum of \(K_{v}\) relates to BFA.

**Theorem 3.2**.: _Let \(_{1}_{m_{v}} 0\) be the sorted eigenvalues of \(K_{v}\) and let \(M_{p}}_{i=1}^{m_{v}}_{i}^{p}\) be its spectral moments. It holds \(}}{_{1}}(_{v}) 1\). Moreover, if \(b_{v}\) is Gaussian and independent from \(K_{v}\), then as \(m_{v}\),_

\[(_{v})}{{}}}{}}.\]

_as soon as \(}}}{{M_{1}}}\) and \(}}}{{M_{2}}}\) are uniformly bounded (i.e. are upper bounded by some \(C>0\) with probability going to \(1\) as \(m_{v}\))._

Proof of Thm. 3.2.: By the chain rule, it holds

\[_{v}=-_{ v}_{}}{ w_{ }}}{ w_{}}^{}=- _{ v}_{}}{ w_{}} }{ w_{}}^{}}{ f_{v}}^{},\]

hence \(_{v}=-K_{v}b_{v}\). Denoting \(K_{v}^{}{{2}}}\) the unique psd square-root of \(K_{v}\), it follows

\[(_{v})=^{}_{v}}{\|_{v}\|_{2}\|b_{v}\|_ {2}}=^{}{{2}}}b_{v}\|_{2}^{2}}{\|K_{v}b_{v}\|_{2}\| b_{v}\|_{2}}.\] (6)

The first claim follows from Eq. (6) and the worst-case bounds \(\|K_{v}b_{v}\|_{2}_{1}\|b_{v}\|_{2}\) and \(\|K_{v}^{}{{2}}}b_{v}\|_{2}}}\|b_{v}\|_{2}\). The second claim is related to the trace estimation method via random matrix-vector products (Martinsson and Tropp, 2020, Chap. 4). We assume without loss of generality that \([\|b_{v}\|_{2}^{2}]=1\) and by Lem. 3.3, we can write \(Z=\|K_{v}^{}{{2}}}b_{v}\|_{2}^{2}=a(1+b)\) where \(a=[Z|K_{v}]=M_{1}\) and \([b^{2}] 0\) as \(m_{v}\). An analogous decomposition holds for \(\|K_{v}(b_{v})\|_{2}^{2}\) with \(a=M_{2}\) and the second claim follows. 

**Lemma 3.3**.: _Let \(K^{m m}\) be a (potentially random) psd matrix and \(a(0,I_{m})\) be independent. Then \([\|Ka\|_{2}^{2} K]=M_{2}(K)\) and \([\|Ka\|_{2}^{2} K]=M_{4}(K)\) where \(M_{p}(K)_{i=1}^{m}_{i}^{p}\) and \(_{1},,_{m} 0\) are the eigenvalues of \(K\)._

The second claim expresses the BFA in terms of the spread of the spectrum of the BFK, in an asymptotically exact way. Its assumptions hold at random initialization in the large width limit of typical NNs, provided \(f_{v}\) is directly followed by a weight-matrix multiplication in the forward pass, so that \(b_{v}\) is the output of a random matrix/vector multiplication. Asymptotic independence can be guaranteed in quite general contexts, see Yang (2020). For MLP or ResNets with batch-size one, we show in Section 5 that \((_{v})\) is tightly related to the conditioning of layer-to-layer Jacobians, studied in the _dynamical isometry_ literature (Pennington et al., 2017).

## 4 Ensuring feature learning in scaled NNs

### Properties for scaled NNs

Consider a sequence of NNs and parameters as in (1) with some diverging architectural parameters such as depth or width. We refer to such a sequence as a _scaled NN_. In search of the optimal scaling of NNs, it is crucial to understand how HP scalings influence the properties of the training dynamics. In this section, we discuss the following properties:

* **Signal propagation.** It holds \(\|f_{v}\|_{}=(1)\) for \(v[1:L-1]\).
* **Feature learning.** It holds \(\|_{L-1}\|_{}=(1)\).
* **Loss decay.** It holds \(-}=(1)\).
* **Balanced contributions.** It holds \(_{}\|_{}\|_{2}^{2}=(_{^{}} \|_{^{}}\|_{2}^{2})\) for any \(,^{}[1:L]\).

We discuss these specific properties because they are amenable to our tools and enforcing them requires \((L-1)+1+1+(L-1)=2L\) degrees of freedom, which exactly matches the number of free HPs if one counts one scale HP (such as the variance of the weights) and one LR per block. One may wonder if property (BC) is truly desirable: this is the topic of Section 6, where we adopt a more axiomatic approach and recover, for homogeneous architectures, (a more general version of) property (BC) as a consequence of enforcing _gradient stability_. Also, while enforcing these properties is reasonable when increasing depth and width, they should be retubed for other asymptotics.

Property (SP) specifies \(L-1\) scale HPs, but leaves the scale of \(f_{L}\) free. The reason for not including \(f_{L}\) in (SP) is that \(\|f_{L}\|_{}=o(1)\) does not lead to vanishing gradient in general, so this behavior should not be excluded a priori. How should one then fix the scale of the output? The next proposition shows that for property (FL) to hold, the quantity that should be suitably normalized is the norm of the backward pass.

**Proposition 4.1**.: _A scaled NN (1) satisfies (FL), (LD), and (BC) if and only if_

\[\|b_{L-1}\|_{}=(_{L-1} )}\] (7)

_and_

\[[1:L],\ _{}= \|_{2}^{2}}.\] (8)

Proof.: Property (LD) requires \(_{=1}^{L}_{}\|_{}\|_{2}^{2}=-}=(1)\) and (BC) requires the terms in the sum to be balanced, this leads to Eq. (8). Now by Thm. 2.1, property (FL) requires

\[\|_{L-1}\|_{}=^{L-1}_{}\| _{}\|_{2}^{2}}{(_{L-1}) m_{L-1}\|b_{L-1} \|_{}}=(1)\] (9)

which leads to (7). Conversely, it is clear that Eq. (8) and (7) imply (FL), (LD) and (BC). 

### Towards automatic HP scaling

The criterion of Prop. (4.1), complemented with the property (SP), suggest a method to automatically adjust the scales and learning rates in any architecture. In general, properties (SP), (FL), (BC) and (LD) can be enforced as follows:

* **(SP): Forward layer normalization.** Enforcing property (SP) can be done along with the computation of the forward pass, this is the usual layer normalization.
* **(FL): Backward layer normalization.** Provided \(_{L-1}\) is known or measured, Eq. (7) can be enforced via a _backward_ analog to layer normalization: one inserts a scaling factor in the forward pass between \(f_{L-1}\) and \(_{L}\), adjusted so that Eq. (7) holds.
* **(BC) & (LD): Scale invariant learning rates.** Directly tune the LRs via Eq. (8).

We refer to the resulting scaling as FSC as it normalizes the **F**orward pass, the **S**ensitivities and the **C**ontributions. Let us make some observations regarding the scale invariant LRs:* **Link with Polyak step-size.** In convex optimization, to minimize a convex and Lipschitz continuous function \(f:^{d}\) such that \(_{x^{d}}f(x)=0\), the Polyak-step-size (Polyak, 1987; Hazan and Kakade, 2019) for the GD algorithm \(x_{t+1}=x_{t}-_{t} f(x_{t})\) is given by \(_{t}=)}{\| f(x_{t})\|_{2}^{2}}\). With this step-size, GD achieves the optimal convergence rate for first order methods over the class of convex and Lipschitz functions. Eq. (8) require a layerwise version of this step-size schedule.
* **Interplay with adaptive methods (Adagrad (Duchi et al., 2011)), ADAM (Kingma and Ba, 2015)).** Adaptive gradient method typically divide the gradient by a quantity which grows _linearly_ rather than quadratically with the norm of the gradient, such as \( W_{}=-_{}}} {\|_{}}\|_{}}\). For such an algorithm, properties (BC) and (LD), suggests the LR \(_{}=(}\|_{}}{L\|_{}}\|_{}^{2}})\), in place of Eq. (8).
* **Scale invariance and \(-2\) homogeneity.** These LRs arise naturally when one wants to make the gradient descent invariant to how scale is enforced (via initialization scale or via scaling factors). We show in App. C that any choice of LR that leads to this invariance must be a positively homogeneous function of the layer-wise gradient of degree \(-2\), as in Eq. (8). We also show in Prop. 6.2 that these LRs make homogeneous architectures invariant to the choice of layer-wise scalings \(_{1},,_{L}\), given a fixed global scale \(_{=1}^{L}_{}\).

## 5 Scaling width and depth of MLPs and ResNets

### BFA for single input MLPs and ResNets at initialization

Multilayer PerceptronConsider a ReLU MLP architecture with a single input \(x=g_{0}^{d}\) and a forward pass given, for \([1:L-1]\), by

\[f_{}=W_{}g_{-1}, g_{}=(f_{}), f _{L}=W_{L}g_{L-1},=(f_{L})\] (10)

where \((u)=\{0,u\}\) is the ReLU nonlinearity and acts entrywise on vectors. The architecture HPs are the input width \(m_{0}=d\), the widths of the hidden layers \(m_{1}==m_{L-1}=m\) (assumed equal), the output width \(m_{L}=k\). The trainable parameters are \([1:L],\ W_{}^{m_{} m_{-1}}\). Such NNs are of the form (1) and are thus covered by Thm. 2.1. Let us study their properties at random initialization under the following assumptions:

* the weights \(W_{}\) are independent \((0,_{}^{2})\) random variables for \([1:L]\).
* either \(k=(1)\) or the loss is linear.

In this setting, the following statements gather consequences of results from the literature on random NNs and of Thm. 3.2 to derive the scale of the forward and backward passes and the BFA. We require (H2) as a technical assumption to avoid dealing with cases where \(b_{L}\) strongly depends on the forward pass, where different scalings may arise2.

In what follows we write \(A=(B)\) when there exists \(c,C>0\) independent of \(d,m,k,L,\|x\|_{2}\) and \(\|b_{L}\|_{2}\) such that the probability that \(A/B[c,C]\) goes to \(1\) in the specified asymptotic. The new result in the following proposition is the BFA estimate, which relies crucially on a delicate computation due to (Jelassi et al., 2023).

**Proposition 5.1** (Large width and depth MLP).: _Assume (H1-2) and for \([1:L-1]\), let \(_{}=}\). As \(m\), it holds_

\[\|f_{v}\|_{} =(\|x\|_{}), \|b_{v}\|_{2} =(\,_{L}\,\|b_{L}\|_{2}).\] (11)

_Moreover, if (BC) holds then \((_{v})=(v^{-1/2})\)._

ResNetsConsider now a ResNet with a _branch scale_ parameter \(\), as in Li et al. (2021): with a single input \(x=f_{0}^{d}\), the forward pass is given, for \([2:L-1]\), by

\[f_{1}=W_{1}x, f_{}=}f_{-1}+ W_{}(f_{ -1}),\ f_{L}=W_{L}f_{L-1},\ =(f_{L})\] (12)where \((u)=u\) in our theoretical results. The architecture HPs are the input width \(m_{0}=d\), the widths of the hidden layers \(m_{1}==m_{L-1}=m\), the output width \(m_{L}=k\). The trainable parameters are \([1:L],\ W_{}^{m_{} m_{-1}}\). When \(=1\), we recover a MLP.

Here we limit ourselves to the case of linear activation where we can directly apply a result from (Marion and Chizat, 2024) to estimate the BFA. We believe that the same result and proof technique extend to the ReLU activation and other variants of ResNets; these extensions are left to future work.

**Proposition 5.2** (Large width and depth linear ResNet).: _Assume (H1-2), let \((x)=x\), \(=O(1/)\) and for \([1:L-1]\), let \(_{}=(1/})\). As \(m\) it holds:_

\[\|f_{}\|_{} =(\|x\|_{}), \|b_{}\|_{2} =_{L}\|b_{L}\|_{2}.\] (13)

_Moreover, if (BC) holds then \((_{v})=(1)\)._

Numerical experimentsWe consider3 one GD step in the model (12) with ReLU nonlinearity, without training \(W_{1}\) (input dimension \(d=10\), output dimension \(k=1\), master LR \( t=0.001\)). Fig. 1, represent BFA, computed via \(_{v}(-b_{v}^{} f_{v})\) where \( f_{v}\) is the change of feature \(f_{v}\) after one GD step. The results are consistent with Prop. 5.1 and 5.2. Interestingly, the last plot suggests that there exists a function \(:_{+}]0,/2[\) such that for a branch scale \(=c/\), the BFA converges to \((c)\) (it can be conjectured numerically that \(((c)) c^{-1/2}\) for \(c 1\)).

### Characterizing HP scalings for MLPs

Let us now discuss specific choices of HP scalings for single-input MLP architectures as in Eq. (10) (or Eq. (12) with \(=1\)) and at initialization. We consider \(6\) HPs: the scale of initialization \(_{1}\) and LR \(_{1}\) of the input layer, the scale \(_{}_{2}==_{L-1}\) and LRs \(_{}_{2}==_{L-1}\) of the hidden layers, and the scale \(_{L}\) and LR \(_{L}\) of the output layer. The HP scalings mentioned in the next theorem are the following (see Table 1):

* **NTK**: the standard scaling with LRs adjusted to satisfy (LD) and (BC) (Jacot et al., 2018);
* **MF+\(\)P**: the scaling proposed in (Jelassi et al., 2023) constructed by imposing the so-called "mean-field" output scale \(_{L} 1/m\) and then enforcing (FL) by adjusting the learning rates;
* **FSC**: the HP scaling singled-out by Prop. 5.3, obtained by adjusting the **F**orward scales, **S**ensitivities, and **C**ontributions.

The properties of HP scalings depend on \(\|x\|_{2}\) and \(\|b_{L}\|_{2}\). We consider two typical settings:

* (Dense) Where \(\|x\|_{2}=\) and \(\|b_{L}\|_{2}=}\). This is representative of a dense whitened input and a RMS loss \((f_{L})=\|f_{L}-y\|_{2}^{2}/k\) for some dense signal \(y^{k}\) with \(\|y\|_{}=(1)\) as, e.g., in image generation applications.

Figure 1: Backward-Feature Angle (BFA) \(_{v}\) observed at initialization in MLPs (\(=1\)) and ResNets (width \(m=200\)), for a few random realizations. (a) for all architectures, BFA \(_{v}\) varies in the first few layers and then stabilizes. (b) BFA at output layer \(_{L-1}\) is asymptotically independent of depth, with a non-trivial angle only when \(}{{}}\) (same color scheme as (a)). (c) for a branch scale \(=}{{}}\), the factor \(c\) directly determines asymptotic output BFA \(_{L-1}\) (averaged over \(5\) draws).

* (Sparse) Where \(\|x\|_{2}=1\) and \(\|b_{L}\|_{2}=1\). This is representative of a one-hot encoding input and the multiclass logistic loss (aka cross-entropy where \(\|b_{L}\|_{2}=((k))\)). This setting is typical of natural language processing tasks.

The scalings are reported in Table 1. We have also introduced scalings in terms of output width \(k\) for **NTK** and **MF + \(\)P** to ensure a non-degenerate behavior as \(k 1\), although these are generally not written in the literature.

**Proposition 5.3** (MLP scalings).: _Under the assumptions of Prop. 5.1, the following hold at random initialization:_

1. _The scaling_ _MF_**+_\(\)_P_ _satisfies (SP), (BC), (FL) but not (LD);_
2. _The scaling_ _NTK_ _satisfies (SP), (BC), (LD) but not (FL);_
3. _Properties (SP), (BC), (LD), (FL) hold if and only if the scaling is_ _FSC_._

This theorem identifies the new HP scaling **FSC** for deep ReLU MLP where the scale of the output layer depends on the depth. We compare empirically the sensitivities (Eq. (4)) of the various scalings in Fig. 2, and the results are consistent with theory. Finally, let us mention that even though **FSC** fixes some degeneracies of deep MLPs, other problems arise when considering multiple inputs, such as degeneracy of the conjugate kernel and NTK , which make ReLU MLPs a fundamentally flawed model at large depth. Hence, the analysis of large depth scalings for MLPs is mostly of theoretical interest.

### Characterizing HP scalings for ResNets

We now discuss HP scalings for single-input ResNets (Eq. (12)) with \(=O(1/)\).

**Proposition 5.4** (ResNets scalings).: _Take \(=O(1/)\), consider the same \(6\) degrees of freedom as in the previous section and assume that the conclusions of Prop. 5.2 holds. Then properties (SP), (BC), (LD) and (FL) hold at initialization if and only if the scalings are those given in Table 2._

   & & Input & Hidden & Output \\    **FSC** \\  } & init. std. \(_{}\) & \(}{{}}\) & \(}{{m}}\) & \(}}{{m}}\) \\   & LR \(_{}\) & \(}{{L^{2}d}}\) & \(}{{L^{2}}}\) & \(}{{Lm}}\) \\    **FSC** \\  } & init. std. \(_{}\) & \(}{{}}\) & \(}}{{m}}\) & \(}}{{m}}\) \\   & LR \(_{}\) & \(}{{L^{3/2}d}}\) & \(}{{L^{3/2}}}\) & \(}{{L^{3/2}m}}\) \\    **FSC** \\  } & init. std. \(_{}\) & \(}{{}}\) & \(}}{{m}}\) & \(}{{}}\) \\   & LR \(_{}\) & \(}{{Ld}}\) & \(}{{Lm}}\) & \(}{{Lm}}\) \\  

Table 1: HP scalings for MLPs under the _dense_ setting (for the _sparse_ setting, replace \(k\) and \(d\) by 1). For \(L\) fixed, both **MF**+\(\)**P** and **FSC** coincide with \(\)P. Values in red are exact, the others are up to a multiplicative factor in \((1)\).

   & Input & Hidden & Output \\   init. std. \(_{}\) & \(}{{}}\) & \(}{{}}\) & \(}}{{m}}\) \\ LR \(_{}\) & \(}{{Ld}}\) & \(}{{}L}}}\) & \(}{{Lm}}\) \\  

Table 2: **FSC** scalings identified in Prop. 5.4 for ResNets. All HPs are specified up to a multiplicative factor in \((1)\). When \(=(L^{-}{{2}}})\) and \(k=d=(1)\), these scalings coincide with the so-called “depth \(\)P” introduced in  and also studied in Yang et al. .

## 6 Minimal desiderata and stability under homogeneity

It is not clear a priori why the property (BC) studied in Section 4should be enforced. In this section, we consider homogeneous architectures, such as ReLU MLPs, and show that a slightly more general version of (BC) is a related to a notion of _gradient stability_.

### Stability and backward speed formula

For a general architecture of the form Eq. (1), consider the following stability property (S), which is necessary if one wants to have comparable behavior between the first GD step and the next. It is related to the usual notion of smoothness in optimization:

* **Stability**. It holds \(\|}{t}_{}\|_{2}/\|_{ }\|_{2}=O(1)\) for \([1:L]\).

We will study this property in a ReLU MLP with a single input as in Eq. (10) (the extension to linear ResNets is simple as only the BFAs change). In this case we have \(_{}=b_{}g_{-1}^{}\) and thus \(\|_{}\|_{F}=\|b_{}\|_{2}\|g_{-1}\|_{2}\). It follows

\[}{t}_{}\|_{F}}{\| _{}\|_{F}}_{}\|_{2}\|g_{-1} \|_{2}+\|b_{}\|_{2}\|_{-1}\|_{2}}{\|b_{}\|_{2}\|g_{-1} \|_{2}}=_{}\|_{2}}{\|b_{}\|_{2}}+_{-1 }\|_{2}}{\|g_{-1}\|_{2}}.\]

We can thus ensure the gradient stability by ensuring, for all \([1:L]\),

* **Forward stability**. It holds \(_{-1}\|_{2}}{\|g_{-1}\|_{2}}=O(1)\) for \([1:L]\), and
* **Backward stability**. It holds \(_{}\|_{2}}{\|b_{}\|_{2}}=O(1)\) for \([1:L]\).

We focus on these simpler desiderata (FS) and (BS) instead of (S) for the rest of the discussion. To estimate \(_{v}\), we rely on a "backward" version of the feature speed formula that holds in \(1\)-homogeneous NNs.

**Proposition 6.1** (Backward speed formula).: _Consider a general architecture of the form (1), take \(v[1:L]\) and assume that the map \(f_{v} f_{L}\) is positively \(1\)-homogeneous4. If \(-f_{L}^{}^{2}[f_{L}]_{L}+_{>v}_{ }\|_{}\|_{2}^{2}=0\) then \(_{v}=0\). Otherwise, the (non-oriented) angle \(_{v}\) between \(f_{v}\) and \(_{v}\) is well defined in \([0,}{{2}}[\) and it holds_

\[\|_{v}\|_{2}=^{}^{2}[f_{L}]_ {L}+_{>v}_{}\|_{}\|_{2}^{2}}{\|f_{v}\|_{2 }(_{v})}.\]

Figure 2: Sensitivities \(S_{L-1}\) (see Eq. (4)) of the last layer of activations (\(g_{L-1}\) in the MLP and \(f_{L-1}\) in the ResNet) computed via the formula \(\| f_{L-1}\|_{}/||\) where \(\) denotes the change after one GD step (master learning rate \( t=0.01\), \(d=10\), \(n=1\) input sample on the unit sphere and \(k=1\)). (a) ReLU MLP of width \(m=400\). From our theory we have for **NTK**, \(S=(1/)\) (close to \(0\) and constant with depth); for **MF+\(\)P**\(S=()\) and for the **FSC**\(S=(1)\) (b) ReLU ResNet of width \(m=400\): for both choices of branch scale, the sensitivities is stable around a nonzero value.

In the context of ReLU MLPs with a linear loss, we have by differentiating the back-propagation recursion and noticing that all terms involving \(^{}\) are zero almost surely5 that:

\[_{v}=_{>v}_{}}{ f _{v}}^{}g_{-1}b_{}^{}b_{}=_{>v}_{} \|b_{}\|_{2}^{2}}{ f_{v}}^{ }}{ f_{v}}f_{v}=_{ v}f_{v}\] (14)

where the last expression defines \(_{v}\). Reasoning as in Thm. (3.2), since \(f_{v}\) is Gaussian at initialization and noticing that \(_{v}\) has a structure similar to that of \(K_{L-v}\), we have that \((_{v})=()\), see the details in Lem. A.1. We can thus estimate \(_{v}\) just as well as \(_{v}\).

### Scale invariance for homogeneous architectures

Homogeneous architectures such as ReLU MLP satisfy scale invariance properties that are important to take into account in our discussion. The following result presents a general invariance under blockwise rescaling, provided one uses scale-invariant LRs. This is related to known invariance results under global rescaling for scale invariant losses (Van Laarhoven, 2017; Li et al., 2022; Wan et al., 2020).

**Proposition 6.2** (Invariance under block-wise rescaling).: _Consider a function \(f_{L}(w_{1},,w_{L})\) (the NN, in our context) which is separately positively \(1\)-homogeneous in each of its blocks of parameters \(w_{}^{p_{}}\). Let \(_{0}=(w_{1}(0),,w_{L}(0))\) and let \(_{0}=_{0}(_{1}w_{1}(0),, _{L}w_{L}(0))\) for some scale factors \(^{L}_{+}\). Let \((t)\) and \((t)\) be the iterates of GD on \(:(f_{L}())\) with LR satisfying \(_{}(t)\|_{}((t))\|_{2}^{2}=_{ }(t)\|_{}((t))\|_{2}^{2}\) and starting from \(_{0}\) and \(_{0}\) respectively. If \(_{=1}^{L}_{}=1\) then \((t)=(t)\) for all \(t 1\)._

### Characterization of admissible scalings for ReLU MLPs

In view of Prop. 6.2, for homogeneous architectures, one can ignore (SP) since any GD dynamics is equivalent to a dynamics where (SP) holds at initialization. However, if the scale of the forward pass is not normalized, (FL) needs now to be adapted to a scale-free version, as follows:

* **Relative feature learning**. It holds \(\|_{L-1}\|_{2}/\|f_{L-1}\|_{2}=(1)\).

We are finally in position to gather all these insights and characterize all _admissible_ scalings for ReLU MLPs, i.e. scalings that satisfy the minimal desiderata (RFL), (LD), (FS) and (BS) at initialization.

**Theorem 6.3** (Minimal desiderata for MLPs).: _Consider a ReLU MLP with \(6\) degrees of freedom: three initialization scales \(_{1},_{},_{L}\) and three LRs \(_{1},_{},_{L}\). Assume \(\|b_{L}\|_{2}=\|g_{0}\|_{}=1\) and a linear loss for simplicity. Then the minimal desiderata (RFL), (LD), (FS) and (BS) hold at initialization in the limit \(m\) then \(L\) if and only if_

\[(_{1})(_{})^{L-2}_{ L}=(/m),\ \ \ \ C_{1}+C_{}=(1)\ \ \ \ \ \ \ C_{L}=O(1),\]

_where \(C_{1}=_{1}\|_{1}\|_{2}^{2}\), \(C_{}=_{=2}^{L-1}_{}\|_{} \|_{2}^{2}\) and \(C_{L}=_{L}\|_{L}\|_{2}^{2}\). In particular, the scaling **FSC** (Table 1) satisfies these desiderata._

## 7 Conclusion

Starting from the feature speed formula, our approach allows to conveniently recover and characterize in an elementary fashion certain properties of existing HP scalings and to discover new ones, with essentially all the technical difficulty contained in the estimation of the BFA. The limitations of our approach are related to the blind spots of Thm. (2.1): it can only quantify feature speed for (S)GD (and does not apply to variants in its current form) and at "cut nodes" in the NN architecture, where all the signal goes through (in particular, it does not apply inside the blocks of a ResNet).

In future works, besides removing these limitations, it would be interesting to have a better understanding of the BFA, both from a quantitative and a qualitative viewpoint.