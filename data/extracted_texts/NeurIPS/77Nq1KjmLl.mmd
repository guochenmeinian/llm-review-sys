# Fragment-based Pretraining and Finetuning on Molecular Graphs

Kha-Dinh Luong, Ambuj Singh

Department of Computer Science

University of California, Santa Barbara

Santa Barbara, CA 93106

{vluong,ambuj}@cs.ucsb.edu

###### Abstract

Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principal subgraph mining, we obtain a compact vocabulary of prevalent fragments from a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one on molecular graphs and the other on fragment graphs, which represents higher-order connectivity within molecules. By enforcing consistency between the fragment embedding and the aggregated embedding of the corresponding atoms from the molecular graphs, we ensure that the embeddings capture structural information at multiple resolutions. The structural information of fragment graphs is further exploited to extract auxiliary labels for graph-level predictive pretraining. We employ both the pretrained molecular-based and fragment-based GNNs for downstream prediction, thus utilizing the fragment information during finetuning. Our graph fragment-based pretraining (GraphFP) advances the performances on \(5\) out of \(8\) common molecular benchmarks and improves the performances on long-range biological benchmarks by at least \(11.5\%\). Code is available at: https://github.com/lvkd84/GraphFP.

## 1 Introduction

The rise of Graph Neural Networks (GNNs) has captured the interest of the computational chemistry community . Representing molecules and chemical structures as graphs is beneficial because the topology and connectivity are preserved and can be directly analyzed during learning , leading to state-of-the-art performance. However, there is a problem in that while highly expressive GNNs are data-hungry, the majority of molecular datasets available are considerably small, posing a critical challenge to the generalization of GNNs. Following the steps that were taken in other domains such as text and images, a potential solution is to pretrain GNNs on large-scale unlabeled data via self-supervised pretraining . How to do so effectively is still an open question since graphs are more complex than images or text, and the chemical specificity of molecular graphs needs to be considered when designing graph-based pretraining tasks.

A large number of pretraining methods have been proposed for molecular graphs . For example,  uses node embeddings to predict attributes of masked nodes,  predicts graph structural properties, and  generatively reconstructs graphs via node and edge predictions. Anotherpopular direction is contrastive learning [26; 32; 35; 45; 46], in which multiple views of the same graph are mapped closer to each other in the embedding space. These views are commonly generated via augmentations to the original graphs [35; 45; 46]. However, without careful adjustments, there is a great risk of violating the chemical validity or changing the chemical properties of the original molecular graph after applying the augmentations. Other methods such as [23; 31] avoid this problem by contrasting 2D-graphs against 3D-graphs. Unfortunately, privileged information such as 3D coordinates is often expensive to obtain. Node versus graph contrastive learning has also been investigated , however, this approach may encourage over-smoothing among node embeddings.

In general, the majority of works pretrain embeddings at either node-level or graph-level. Node-level pretraining may be limited to capturing local patterns, neglecting the higher-order structural arrangements while graph-level methods may overlook the finer details. As such, motif-based or fragment-based pretraining is a new direction that potentially overcomes these problems [27; 48; 49]. Still, existing fragment-based methods use either suboptimal fragmentation or fragmentation embeddings. GROVER  predicts fragments from node and graph embeddings, however, their fragments are k-hop subgraphs that cannot account for chemically meaningful subgraphs with varying sizes and structures. MICRO-Graph  contrastively learns subgraph embeddings versus graph embeddings; however, these embeddings may not effectively capture global patterns. MGSSL  utilizes graph topology via depth-first search or breadth-first search to guide the fragment-based generation of molecules; however, their multi-step fragmentation may overly decompose molecules, thereby losing the ability to represent higher-order structural patterns.

In this paper, we propose GraphFP, a novel fragment-level contrastive pretraining framework that captures both granular patterns and higher-order connectivity. For each molecule, we obtain two representations, a molecular graph and a fragment graph, each of which is processed by a separate GNN. The molecular GNN learns node embeddings that capture local patterns while the fragment GNN learns fragment embeddings that encode global connectivity. The GNNs are jointly trained via a contrastive task that enforces consistency between the fragment embedding and the aggregated embedding of the corresponding atoms from the molecular graphs. Unlike previous work, we pretrain the fragment-based aggregation of nodes instead of individual nodes. With the fragment embeddings encoding the global patterns, contrasting them with the aggregation of node embeddings reduces the risk of over-smoothing and allows flexibility in learning the appropriate node latent space. Since aggregation of nodes in this context can also be considered a kind of fragment representation, our framework essentially contrasts views of fragments, each of which captures structural information at a different scale. Moreover, as the molecular graphs and the fragment graphs are chemically faithful, our framework requires no privileged information or augmentation.

Exploiting the prepared fragment graphs, we further introduce two predictive pretraining tasks. Given a molecular graph as input, the molecular GNN is trained to predict structure-related labels extracted from the corresponding fragment graph. These tasks enhance the structural understanding of the molecular GNN and can be conducted together with contrastive pretraining.

To generate a vocabulary of molecular fragments, we utilize Principal Subgraph Mining  to extract optimized prevalent fragments that span the pretraining dataset. Compared to fragmentations used in previous works, this approach can produce a concise and diverse vocabulary of fragments, each of which is sufficiently frequent without sacrificing fragment size.

We evaluate GraphFP on benchmark chemical datasets and long-range biological datasets. Interestingly, both the molecular and the fragment GNNs can be used in downstream tasks, providing enriched signals for prediction. Empirical results and analysis show the effectiveness of our proposed methods. In particular, our pretraining strategies obtain the best results on \(5\) out of \(8\) common chemical benchmarks and improve performances by \(11.5\%\) and \(14\%\) on long-range biological datasets.

## 2 Related Works

### Representation Learning on Molecules

Representation learning on molecules has made use of fixed hand-crafted representations such as descriptors and fingerprints [30; 43], string-based representations such as SMILES and InChI [22; 28], and molecular images . Currently, state-of-the-art methods rely on representing molecules as graphs and couple them with modern graph-based learning algorithms like GNNs . Dependingon the type of compounds (small molecules, proteins, crystals), different graph representations can be constructed [11; 39; 44]. Compared to graph-based representation, molecular descriptors and fingerprints cannot encode structural information effectively while string-based representations require an additional layer of syntax which may significantly complicate the learning problem.

### Graph Neural Networks

Given a graph \(G=(V,E)\) with node attributes \(x_{v}\) for \(v V\) and edge attributes \(e_{uv}\) for \((u,v) E\), GNNs learn graph embedding \(h_{G}\) and node embedding \(h_{v}\). At each iteration, a node updates its embedding by gathering information from its neighborhood, including both the neighboring nodes and the associated edges. This process often involves an aggregating function \(M_{k}\) and an updating function \(U_{k}\). After the \(k\)-th iteration (layer), the embedding of node \(v\) is, \(h_{v}^{(k)}=U_{k}(h_{v}^{(k-1)},M_{k}(\{(h_{v}^{(k-1)},h_{u}^{(k-1)},e_{uv})|u  N(v)\}))\), where \(N(v)\) is the set of neighbors of \(v\) and \(h_{v}^{(0)}=x_{v}\). The aggregating function \(M_{k}\) pools the information from \(v\)'s neighbors into an aggregated message. Next, the updating function \(U_{k}\) updates the embedding of \(v\) based on its previous embedding \(h_{v}^{(k-1)}\) and the aggregated message. An additional readout function \(R\) combines the final node embeddings into a graph embedding \(h_{G}=R(\{h_{v}^{(K)}|v V\})\), where \(K\) is the number of iterations (layers). Since there is usually no ordering among the nodes in a graph, the readout function \(R\) is often chosen to be order-invariant . Following this framework, a variety of GNNs has been proposed [12; 34; 41], some specifically for molecular graphs [9; 29].

### Pretraining on Graph Neural Networks

To alleviate the generalization problem of graph-based learning in the chemical domain, graph pretraining has been actively explored [15; 23; 26; 31; 32; 35; 45; 46] in order to take advantage of large databases of unlabeled molecules .

In terms of the pretraining paradigm, existing methods can be categorized into being predictive [15; 27; 37], contrastive [23; 26; 31; 35; 45; 46], or generative [16; 49]. Predictive methods often require moderate to large labeled datasets for pretraining. In the case of an unlabeled large dataset, chemically or topologically generic labels can be generated. Despite the simple setup, predictive methods are prone to negative transfer . On the other hand, contrastive methods aim to learn a robust embedding space by using diverse molecular views [23; 31; 35; 45; 46]. Generative methods intend to learn the distribution of the components constituting molecular graphs [16; 49].

At the pretraining level, methods can be sorted into node-level [16; 27], graph-level [15; 23; 26; 31; 35; 45; 46], and more recently motif-level [27; 48; 49]. Node-level methods only learn chemical semantic patterns at the lowest granularity, limiting their ability to capture higher-order molecular arrangements, while graph-level methods may miss the granular details. Motif-based or fragment-based pretraining has emerged as a possible solution to these problems [27; 48; 49]; however, existing methods use suboptimal fragmentation or suboptimal fragment embeddings. In this work, we learn fragment embeddings that effectively capture both local and global topology. We exploit the fragment information at every step of the framework, from pretraining to finetuning.

## 3 Methods

In this section, we introduce the technical details of our pretraining framework GraphFP. We begin with discussing molecular fragmentation and fragment graph construction based on an extracted vocabulary. Then, we describe our fragment-based pretraining strategies, including one contrastive task and two predictive tasks. Finally, we explain our approach for combining pretraining strategies and the pretrained molecular-based and fragment-based GNNs for downstream predictions.

### Molecule Fragmentation

Graph fragmentation plays a fundamental role in the quality of the learning models because it dictates the global connectivity patterns. Existing methods rely on variations of rule-based procedures such as BRICS  or RECAP . Though chemistry-inspired, the extracted vocabulary is often large, to the order of the size of the pretraining dataset, and contains unique or low-frequency fragments,posing a significant challenge for pattern recognition. To overcome these drawbacks, some works further break down fragments , even to the level of rings and bonds . However, reducing the sizes of fragments forfeits their ability to capture higher-order arrangements.

#### 3.1.1 Principal Subgraph Extraction

In this work, we borrow the Principal Subgraph Mining algorithm from  to alleviate the above-mentioned problems. Given a graph \(G=(V,E)\), a subgraph of \(G\) is defined as \(S=(,)\), where \( V\) and \( E\). In a vocabulary of subgraphs, \(S\) is a principal subgraph if \( S^{}\) intersecting with \(S\) in any molecule, either \(S^{} S\) or \(c(S^{}) c(S)\) where \(c()\) counts the occurrences of a fragment among molecules. Intuitively, principal subgraphs are fragments with both larger sizes and more frequent occurrences. The algorithm heuristically constructs a vocabulary of such principal subgraphs via a few steps:

Initialize:Initialize the vocabulary with unique atoms.

Merge:For each molecular graph, for all pairs of overlapping fragments in the graph, merge the fragment in the pair and update the occurrences of the resulting combined fragment.

Update:Update the vocabulary with the merged fragment with the highest occurrence. Repeat the last two steps until reaching the predefined vocabulary size.

For a more detailed description, we refer readers to . We investigate the effect of vocabulary sizes on the performance of pretrained models in Section 4.4 and Appendix C. Similarly, we investigate the effect of fragmentation strategies in Section 4.2.

#### 3.1.2 Fragment-graph Construction

Given an extracted vocabulary of principal fragments, for each molecular graph, we construct a corresponding fragment graph. Let \(F=\{S^{(0)},S^{(1)},...,S^{(m)}\}\) be the fragmentation of a molecular graph \(G_{M}=(V_{M},E_{M})\), where \(S^{(i)}=(^{(i)},^{(i)})\) is a fragment subgraph, \(^{(i)}^{(j)}=\), and \(_{i=1}^{m}^{(i)}=V_{M}\). We denote the fragment graph as \(G_{F}=(V_{F},E_{F})\), where \(|V_{F}|=|F|\) and each node \(v_{F}^{(i)} V_{F}\) corresponds to a fragment \(S^{(i)}\). An edge exists between two fragment nodes of \(G_{F}\) if there exists at least a bond interconnecting atoms from the fragments. Formally,

Figure 1: Fragment-based contrastive pretraining framework. GNN\({}_{M}\) processes molecular graphs while GNN\({}_{F}\) processes fragment graphs. The fragment-based pooling function FragPool aggregates node embeddings into a combined embedding that forms a positive contrastive pair with the corresponding fragment embedding. Notice that the two -OH groups, blue and light pink, are considered distinct. Therefore, the aggregated node embedding corresponding to the blue fragment and the embedding of the light pink fragment form a negative pair.

\(E_{F}=\{(i,j)| u,v,u^{(i)},v^{(j)},(u,v) E\}\). For simplicity, in this work, we withhold edge features in the fragment graph. As a result, a fragment graph purely represents the higher-order connectivity between the large components within a molecule. Fragment node features are embeddings from an optimizable lookup table. Vocabulary and fragment graphs statistics are provided in Appendix C.

### Fragment-based Contrastive Pretraining

Figure 1 illustrates our contrastive framework. We define two separate encoders, \(_{M}\) and \(_{F}\). \(_{M}\) processes molecular graphs and produces node embeddings while \(_{F}\) processes fragment graphs and produces fragment embeddings. Since GNNs can capture structural information, the node embeddings encode local connectivity of neighborhoods surrounding atoms. Similarly, the fragment embeddings encode global connectivity and positions of fragments within the global context.

We apply contrastive pretraining at the fragment level. Contrastive pretraining learns robust latent spaces by mapping similar views closer to each other in the embedding space while separating dissimilar views. Learning instances are processed in pairs, in which similar views form the positive pairs while dissimilar views form the negative pairs. In this case, a positive pair consists of a fragment from a fragment graph and the collection of atoms constituting this fragment from the corresponding molecular graph. On the other hand, a fragment and any collection of atom nodes constituting a different fragment instance form a negative pair. Notice that different occurrences of the same type of fragment in the same or different molecules are considered distinct instances because when structural contexts are taken into account, the embeddings of these instances are dissimilar. The -OH groups in Figure 1 illustrate one such case.

To obtain the collective embedding of atom nodes corresponding to a fragment, we define a function \(()\) that combines node embeddings. Contrasting the combined embedding of nodes against the fragment embedding allows flexibility within the latent representations of individual nodes. Intuitively, the learning task enforces consistency between fragment embeddings and collective node embeddings, incorporating higher-order connectivity information. Through the learning process, the information is optimally distributed among the nodes so that each node only holds a part of this collective knowledge. Arguably, such setup is semantically reasonable because nodes and fragments represent different orders of connectivity. A node does not necessarily hold the same structural information as the corresponding fragment. Instead, it is sufficient for a group of nodes to collectively represent such higher-order information. Essentially, the fragment embeddings and the collective embeddings of atom nodes can be considered different views of molecular fragments.

Extending the notations in 3.1.2, given a pretraining dataset of molecular graphs \(_{M}=\{G_{M}^{(1)},G_{M}^{(2)},...,G_{M}^{(N)}\}\), we obtain a set of fragmentations \(=\{F^{(1)},F^{(2)},...,F^{(N)}\}\) and a set of corresponding fragment graphs \(_{F}=\{G_{F}^{(1)},G_{F}^{(2)},...,G_{F}^{(N)}\}\), with \(G_{M}^{(i)}=(V_{M}^{(i)},E_{M}^{(i)})\) and \(G_{F}^{(i)}=(V_{F}^{(i)},E_{F}^{(i)})\). More specifically, \(V_{M}^{(i)}\) is the set of atom nodes in the molecular graph \(G_{M}^{(i)}\) and \(V_{F}^{(i)}\) is the set of fragment nodes in the fragment graph \(G_{F}^{(i)}\). For the \(i\)-th molecule, let \(H_{M}^{(i)}^{|V_{M}^{(i)}| d}\) and \(H_{F}^{(i)}^{|V_{F}^{(i)}| d}\) be the final node embeddings and fragment embeddings after applying \(_{M}\) and \(_{F}\), respectively, i.e:

\[H_{M}^{(i)}=_{M}(V_{M}^{(i)},E_{M}^{(i)})\] (1)

\[H_{F}^{(i)}=_{F}(V_{F}^{(i)},E_{F}^{(i)})\] (2)

We further compute the fragment-based aggregation of node embeddings:

\[H_{A}^{(i)}=(H_{M}^{(i)},F^{(i)}),\] (3)

where \(H_{A}^{(i)}^{|V_{F}^{(i)}| d}\) has the same dimensions as those of \(H_{F}^{(i)}\). The \(r\)-th rows, \(h_{A,r}^{(i)} H_{A}^{(i)}\) and \(h_{F,r}^{(i)} H_{F}^{(i)}\), from both matrices are embeddings of the same fragment from the original molecule. In our contrastive framework, \(h_{F,r}^{(i)}\) is the positive example for the anchor \(h_{A,r}^{(i)}\). The negative learning examples are sampled from:

\[_{i,r}^{-}=\{H_{F,q}^{(j)}|j i q r\}\] (4)We minimize the contrastive learning objective based on the InfoNCE loss :

\[_{C}=-_{i,r}[_{A,r},h^{(i)}_{F,r})}{( h^{(i)}_{A,r},h^{(i)}_{F,r})+ _{h^{-}^{-}_{i,r}}( h^{(i)}_{A,r},h^{-})}]\] (5)

### Fragment-based Predictive Pretraining

We further define two predictive pretraining tasks of which labels are extracted from the properties and the topologies of the fragment graphs. Only the molecular GNN is pretrained in this case. The labels provided by the fragment graphs guide the graph-based pretraining of the molecular GNNs, encouraging the model to learn higher-order structural information.

Fragment Existence PredictionA multi-label prediction task that outputs a vocabulary-size binary vector indicating which fragments exist in the molecular graph. Thanks to the optimized fragmentation procedure  that we use, the output dimension is compact without extremely rare classes or fragments, resulting in more robust learning.

Fragment Graph Structure PredictionWe predict the structural backbones of fragment graphs. The number of classes is the number of unique structural backbones. Essentially, a backbone is a fragment graph with no node or edge attributes. Graphs share the same backbone if their fragments are arranged similarly. For example, fragment graphs in which three fragments connect in a line correspond to the same structural backbone, a line graph with three nodes. This task also benefits from the optimized fragmentation. As the extracted fragments are sufficiently large, the fragment graphs are small enough that the enumeration of all unique backbones is feasible.

The predictive tasks pretrain the graph embedding \(h_{G}\). In particular, one task injects local community information of fragments while the other task injects the arrangements of fragments into \(h_{G}\). We train both tasks together, optimizing the objective \(_{P}=_{P_{1}}+_{P_{1}}\), where \(_{P_{1}}\) and \(_{P_{2}}\) are the predictive objective of each task. The predictive tasks are illustrated in Figure 2.

We can combine the predictive pretraining with the contrastive pretraining presented in Section 3.2. We found parallel pretraining more effective than the sequential pretraining described in . With \(\) being the weight hyperparameter, the joint pretraining objective is then:

\[=_{P}+(1-)_{C}\] (6)

### Combining Models based on Fragment Graphs and Molecule Graphs

We further propose to utilize both the molecule encoder \(_{M}\) and the fragment encoder \(_{F}\) for downstream prediction. Following the procedure described in Section 3.1, we can easily fragment and extract molecular graph and fragment graph representations from any unseen molecule. Such convenience is not always the case. For example, existing works that contrast 2D and 3D molecular graphs [23; 31] can only finetune the 2D encoder since 3D information is expensive to obtain for unseen molecules. Other works that rely on graph augmentation [35; 45; 46] cannot utilize the

Figure 2: Fragment-based predictive pretraining. \(_{M}\) processes molecular graphs and produces graph-level embeddings used for prediction. The right upper box shows unique fragments that exist in the input molecule while the right lower box shows the ground-truth structural backbone. A few other backbones are also visualized for comparison.

augmented views as signals for downstream prediction since they are not faithful representations of the original molecule. In contrast, the representations extracted by our framework are faithful views that do not require expensive information other than the vanilla molecular structure.

Let \(h_{M}\) and \(h_{F}\) be the graph embeddings produced by \(_{M}\) and \(_{F}\), respectively. We obtain the downstream prediction by applying a fully-connected layer on the concatenation of \(h_{M}\) and \(h_{F}\).

## 4 Experiments

### Experimental Settings

We pretrain GNNs according to the process discussed in section 3. The pretrained models are evaluated on various chemical benchmarks.

DatasetsWe use a processed subset containing 456K molecules from the ChEMBL database  for pretraining. A fragment vocabulary of size 800 is extracted as described in Section 3.1.1. To ensure possible fragmentation of unseen molecules, we further complete the vocabulary with atoms not existing in the pretraining set, totaling 908 unique fragments. For downstream evaluation, we consider 8 binary graph classification tasks from MoleculeNet  with scaffold split . Moreover, to assess the ability of the models in recognizing global arrangement, we consider two graph prediction tasks on large peptide molecules from the Long-range Graph Benchmark . Long-range graph benchmarks are split using stratified random split. More information regarding datasets is provided in Appendix B.

ModelsFor graph classification benchmarks, we model our molecular encoders \(_{M}\) with the 5-layer Graph Isomorphism Network (GIN)  as in previous works [15; 23], using the same featurization. Similarly, we model the fragment encoder \(_{F}\) with a shallower 2-layer GIN since fragment graphs are notably smaller. For long-range benchmarks, both \(_{M}\) and \(_{F}\) are GIN with 5 layers, with the featurization of molecular graphs based on the Open Graph Benchmark . All encoders have hidden dimensions of size \(300\). For more details, please refer to Appendix A.

Pretraining and FinetuningAll pretrainings are done in 100 epochs, with AdamW optimizer, batch size 256, and initial learning rate \(1 10^{-3}\). We reduce the learning rate by a factor of 0.1 every 5 epochs without improvement. We use the models at the last pretraining epoch for finetuning. On graph classification benchmarks, to ensure comparability, our finetuning setting is mostly similar to that of previous works [15; 23]: 100 epochs, Adam optimizer, batch size 256, initial learning rate \(1 10^{-3}\), and dropout rate chosen from \(\{0.0,0.5\}\). We reduce the learning rate by a factor of 0.3 every 30 epochs. On long-range benchmarks, the setting is similar except that we finetune for 200 epochs and factor the learning rate by 0.5 every 20 epochs without improvement. We find that using averaging as \(()\) works well. All experiments are run on individual Tesla \(V100\) GPUs.

BaselinesWe compare to several notable pretraining baselines, including predictive methods (AttrMask & ContextPred , G-Motif & G-Contextual (GROVER) ), generative method (GPTGNN ), contrastive methods (GraphLoG , GraphCL , JOAO, JOAOs ), contrastive method with privileged knowledge (GraphMPV ), and fragment-based method (MGSSL ). For long-range prediction, we compare our method with popular GNN architectures: GCN , GCNII , GIN , and GatedGCN  with and without Random Walk Spatial Encoding .

Our goal is to benchmark the quality of our proposed pretraining methods with existing work. The settings in our experiments are chosen to fulfill this objective. In general, our settings follow closely those from previous works to ensure comparability. Specifically, we use the same embedding model (5-layer GIN), featurization, and overall similar hyperparameters and amount of pretraining data as those in the baselines [15; 23; 42; 45; 46; 49]. We limit the amount of hyperparameter tuning for the same reason. In general, self-supervised learning and property prediction on molecular graphs are important research topics and a wide variety of methods have been proposed in terms of both the pretraining task and the learning model, resulting in impressive results [8; 50; 51]. Some of these performances stem from deeper neural network design, extensive featurization, and large-scale pretraining. Pretraining at the scale of [8; 51] requires hundreds of GBs or even a TB of memory.

Due to limited resources, we leave the evaluation of GraphFP and other baselines when pretrained on such larger datasets for future work.

### Results on Graph Classification Benchmarks

Table 1 reports our results on chemical graph classification benchmarks. For each dataset, we report the mean and error from 10 independent runs with predefined seeds. Except for GraphLoG  and MGSSL , the results of other baselines are collected from the literature [15; 23; 37; 45; 46]. To ensure a comprehensive evaluation, we conduct experiments with all possible combinations of the proposed strategies, which include contrastive pretraining (denoted as \(C\)), predictive pretraining (denoted as \(P\)), and inclusion of fragment encoders in downstream prediction (denoted as \(F\)). Because \(F\) requires \(C\), all possible combinations of these components are \(\{C,P,CP,CF,CPF\}\), with the corresponding pretrained models GraphFP\({}_{C}\), GraphFP\({}_{P}\), GraphFP\({}_{CP}\), GraphFP\({}_{CF}\), and GraphFP\({}_{CPF}\). For GraphFP\({}_{CP}\), we choose \(=0.3\) and for GraphFP\({}_{CPF}\), we choose \(=0.1\). The choices of \(\) are reported in Appendix A. We also compare to GraphFP-JT\({}_{C}\) and GraphFP-JT\({}_{CF}\), which are variations of our models pretrained with the fragmentation from . The fragments used by  are generally smaller, resulting in larger fragment graphs. We found 5-layer GIN models encode these larger graphs better. Our models are competitive in all benchmarks, obtaining the best performance in \(5\) out of \(8\) downstream datasets. We report the average ranking and the average AUC of the models. As reported in Table 1, the GNNs with our proposed fragment-based pretraining and finetuning strategies achieve the best average rankings and average AUCs across baselines. Moreover, adding the strategies successively, i.e. \(C\), \(CP\), and \(CPF\), improves the average downstream rankings, confirming the individual effectiveness of each strategy in supporting the learning on molecular graphs. We attribute the remarkable performances of our methods to the capability of the pretrained embeddings and the fragment graph encoder in capturing higher-order structural patterns. For instance, the BBBP benchmark requires predicting blood-brain barrier permeability, where the molecular shape, size, and interaction with the transporting proteins play significant roles in determining the outcome. Capturing such information necessitates a deep understanding of the global molecular structure, which is the goal of our pretraining strategies.

### Results on Long-range Chemical Benchmarks

In Table 2, we compare fragment-based pretraining and finetuning of GraphFP with GNN baselines on two long-range benchmarks: Peptide-func containing 10 classification tasks regarding peptide functions and Peptide-struct containing five regression tasks regarding 3D structural information . Because fragment graphs of peptides are extremely large for effective extraction of structural backbones, we exclude predictive pretraining in this experiment. On both benchmarks, the results

  Pretraining Strategies & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & Avg. Rank & Avg. AUC \\  AttWalking  & 64.3 \(\) 2.8 & **76.4**\(\) 0.4 & 64.2 \(\) 0.5 & 61.0 \(\) 0.7 & 71.8 \(\) 4.1 & 74.7 \(\) 1.4 & 77.2 \(\) 1.1 & 79.3 \(\) 1.6 & 7.88 & 71.15 \\ ContextSP & 68.0 \(\) 2.0 & 75.7 \(\) 0.7 & 63.9 \(\) 0.6 & 60.9 \(\) 0.6 & 65.9 \(\) 3.8 & 75.8 \(\) 1.7 & 77.3 \(\) 1.0 & 79.6 \(\) 1.2 & 7.56 & 70.89 \\ G-Motif  & 66.9 \(\) 3.1 & 73.6 \(\) 0.7 & 62.3 \(\) 0.6 & 61.0 \(\) 1.5 & 77.7 \(\) 2.7 & 73.0 \(\) 1.8 & 73.8 \(\) 1.2 & 73.0 \(\) 3.3 & 14.25 & 70.16 \\ G-Contextual  & 69.9 \(\) 2.1 & 75.0 \(\) 0.6 & 62.8 \(\) 0.7 & 58.7 \(\) 1.0 & 66.6 \(\) 5.2 & 72.1 \(\) 0.7 & 76.3 \(\) 1.5 & 79.8 \(\) 1.1 & 11.88 & 69.34 \\ GPT-GNN  & 64.5 \(\) 1.4 & 74.9 \(\) 0.3 & 62.5 \(\) 0.8 & 58.1 \(\) 0.3 & 58.3 \(\) 4.5 & 75.9 \(\) 2.3 & 65.2 \(\) 1.7 & 79.3 \(\) 2.3 & 16.3 & 67.16 \\ GraphLoG  & 67.8 \(\) 1.9 & 75.1 \(\) 0.1 & 62.4 \(\) 0.2 & 59.5 \(\) 1.5 & 63.3 \(\) 3.2 & 76.1 \(\) 1.2 & 73.0 \(\) 0.9 & 80.2 \(\) 3.5 & 12.56 & 69.70 \\ GraphCL  & 69.7 \(\) 0.7 & 73.9 \(\) 0.7 & 62.4 \(\) 0.6 & 60.5 \(\) 0.9 & 76.0 \(\) 2.9 & 69.8 \(\) 2.7 & **78.5 \(\) 1.2** & 75.4 \(\) 1.4 & 12.13 & 70.78 \\ JQAO  & 70.2 \(\) 1.0 & 75.0 \(\) 0.3 & 62.9 \(\) 0.9 & 60.9 \(\) 0.8 & 81.5 \(\) 1.5 & 71.4 \(\) 1.4 & 76.7 \(\) 1.2 & 73.5 \(\) 0.5 & 9.56 & 71.89 \\ JQAO  & 71.4 \(\) 9.9 & 74.3 \(\) 0.6 & 63.2 \(\) 0.5 & 60.5 \(\) 0.7 & 80.1 \(\) 0.6 & 73.7 \(\) 1.0 & 75.1 \(\) 2.1 & 75.5 \(\) 1.3 & 8.94 & 72.14 \\ GraphFP  & 68.5 \(\) 0.2 & 74.5 \(\) 0.4 & 62.7 \(\) 0.1 & 63.1 \(\) 1.6 & 79.0 \(\) 2.5 & 75.0 \(\) 1.4 & 78.4 \(\) 1.4 & 76.8 \(\) 1.1 & 10.00 & 71.70 \\ MossL  & 68.9 \(\) 7.9 & 74.9 \(\) 0.3 & 63.3 \(\) 0.5 & 77.7 \(\) 5.5 & 77.6 \(\) 5.5 & 73.2 \(\) 1.9 & 75.7 \(\) 1.3 & **81.2 \(\) 2.7** & 10.94 & 70.41 \\ GraphFP\({}_{TC}\) & 71.5 \(\) 0.9 & 75.2 \(\) 0.5 & 63.6 \(\) 0.5 & 62.0 \(\) 1.0 & 77.7 \(\) 4.5 & 76.0 \(\) 2.2 & 75.6 \(\) 1.0 & 79.7 \(\) 1.3 & 6.13 & 72.66 \\ GraphFP\({}_{TC}\) & 70.2 \(\) 1.7 & 72.0 \(\) 0.8 & 62.5 \(\) 0.9 & 59.3 \(\) 1.3 & 73.9 \(\) 5.6 & 73.9 \(\) 1.3 & 73.0 \(\) 1.9 & 74.2 \(\) 2.8 & 13.56 & 70.21 \\ GraphFP\({}_{CP}\) & 71.5 \(\) 1.6 & 75.5 \(\) 0.4 & 63.8 \(\) 0.6 & 61.4 \(\) 0.9 & 76.6 \(\) 2.7 & **72.1 \(\) 1.5** & 76.3 \(\) 1.0 & 78.2 \(\) 3.4 & 5.50 & 72.81 \\ GraphFP\({}_{CP}\) & 68.2 \(\) 1.2 & **7

[MISSING_PAGE_EMPTY:9]

Effects on Varying the Size of the VocabularyThe size of the fragment vocabulary likely influences the quality of pretraining and finetuning since it dictates the resolution of fragment graphs. To investigate such effects, we prepare two additional vocabularies with size \(1,600\) and size \(3,200\). We repeat the same contrastive pretraining as in Section 4.1 using the new vocabularies. As the vocabulary size increases, more unique and larger fragments are discovered, increasing the average fragment size and reducing the average fragment graph size. Table 10 shows that the performances worsen on some downstream benchmarks as the vocabulary size grows larger. We conjecture a few possible reasons. Firstly, a larger vocabulary means more parameters to optimize. Second, smaller fragment graphs represent an excessively loose view of the graph, resulting in a loss of structural information. In general, vocabulary size is an important hyperparameter that greatly affects the quality of self-supervised learning. In , the authors gave a discussion on selecting an optimal vocabulary size according to the entropy-sparsity trade-off.

Visualizing the Learned EmbeddingsAs shown in Figure 3, we visualize the learned embeddings via t-SNE . The left plot illustrates the embeddings produced by the pretrained fragment encoder \(_{F}\) of a common fragment. Each dot corresponds to a molecule in which the fragment appears. The color of a dot indicates the structural backbone of the molecule it represents. For visibility, we only show the most common backbones. The embeddings are reasonably separated according to the structural backbones, indicating that the fragment embeddings capture higher-order structural information. Notice that the fragment encoder \(_{F}\) is not directly trained to recognize structural backbones. The right plot shows the embeddings produced by the molecule encoder \(_{M}\) of atoms within a molecule. The embeddings of atoms within the same fragment are clustered together. Interestingly, some clusters, shown in green, purple, and black, are arranged similarly to how they appear in the original molecule. This observation confirms that the collective embedding of nodes can capture higher-order connectivity.

## 5 Conclusions and Future Work

In this paper, we proposed contrastive and predictive learning strategies for pretraining GNNs based on graph fragmentation. Using an optimized fragment vocabulary, we pretrain two separate encoders for molecular graphs and fragment graphs, thus capturing structural information at different resolutions. When benchmarked on chemical and long-range peptide datasets, our method achieves competitive or better results compared to existing methods. Moving forward, we plan to further improve the pretraining via larger datasets, more extensive featurizations, better fragmentations, and more optimal representations. We also plan to extend the fragment-based techniques to other learning problems, including novel molecule generation and interpretability.