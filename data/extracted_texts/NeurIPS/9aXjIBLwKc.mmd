# ZSC-Eval: An Evaluation Toolkit and Benchmark

for Multi-agent Zero-shot Coordination

 Xihuai Wang, Shao Zhang, Wenhao Zhang, Wentao Dong, Jingxiao Chen,

**Ying Wen**, Weinan Zhang\({}^{}\)

Shanghai Jiao Tong University

{leoxhwang,shaozhang,ying.wen,wnzhang}@sjtu.edu.cn

Both authors contribute equally to this work.Correspondence Authors

###### Abstract

Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deployment-time partners' distribution and the training partners' distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deployment-time partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present **ZSC-Eval**, the first evaluation toolkit and benchmark for ZSC algorithms. ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners' distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings. We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval's consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.

## 1 Introduction

Building agents that can interact and collaborate with others without prior coordination in various scenarios is a crucial challenge of cooperative AI . One aspect of this challenge, known as Zero-shot coordination (ZSC) in cooperative multi-agent reinforcement learning (MARL)  involves developing an agent that learns coordination skills with a limited set of training partners and generalizes them to unseen partners during deployment . The distribution of training partners is determined by training algorithms, while deployment-time partners are determined by deployment requirements , making ZSC an out-of-distribution (OOD) generalization problem. ZSC capability evaluation requires specific methods, such as partners that meet deployment-time distributions and metrics that focus on generalization performance and not only task performance .

Current ZSC evaluation methods still face challenges. The distribution gap between evaluation and deployment-time partners is crucial. Human proxy agents might not fully mimic human behaviors , and generating evaluation and training partners using identical methods  results in similar distributions, compromising the reliability of evaluation results. Cross-play evaluations amongtrained ZSC agents [57; 23] risk unfair comparisons due to overlaps between training and evaluation partners. Some evaluation methods are inconvenient to implement, e.g., human proxy agents require human data for training. Moreover, using mean episode returns as the ZSC capability metric restricts evaluation to task performance, ignoring generalization performance like the generalization gap . The mean episode returns also ignore different and unbalanced cooperation capabilities of evaluation partners . Therefore, the community urgently needs evaluation toolkits with evaluation partners to meet deployment-time requirements and better metrics for fair and comprehensive comparisons.

In this paper, we introduce **ZSC-Eval**, a comprehensive and convenient evaluation toolkit and benchmark, including the generation and selection of evaluation partners and measurement of ZSC capability with novel metrics. Inspired by reward hypothesis [52; 4], we assume that deployment-time partners' requirements can be represented as reward functions. Therefore, we use the widely adopted event-based reward functions [36; 11; 33] to indicate deployment-time partners' behavior preferences, which is practical for humans to designate evaluation partners . To address the unbalanced distribution of generated partners and consequential unbalanced performance estimation, we propose Best Response Diversity (BR-Div), the population diversity  of partners' BRs, to select representative subsets as evaluation partners. For a comprehensive evaluation of generalization performance, we propose BR-Prox, which measures the performance similarity between ego agents and approximate BRs to the evaluation partners, illustrating the generalization gap and balancing evaluation partners with different cooperation capabilities.

We first verify the effectiveness of ZSC-Eval by demonstrating that the generated evaluation partners exhibit more diverse high-level behaviors than those in current evaluation methods. We then evaluate current ZSC algorithms using different evaluation methods and humans in the most popular coordination environment, Overcooked [5; 22] and show that ZSC-Eval can provide consistent results with human evaluation. We also provide benchmark results of current ZSC algorithms in Overcooked, in which we develop new testbeds. To verify the scalability of ZSC-Eval, we also provide benchmark results in Google Research Football (GRF) . Through these experiments, we conclude guidelines for designing ZSC testbeds and further analyze the failure of current ZSC algorithms to generate enough diverse expert training partners.

In summary, our contributions are as follows: 1) To the best of our knowledge, we are the first to investigate the evaluation of ZSC capability and analyze the limitations of current evaluation methods; 2) We propose ZSC-Eval, a comprehensive and convenient evaluation toolkit and benchmark for ZSC algorithms, including partner candidates generation via behavior-preferring rewards, partners selection via BR-Div, and ZSC capability measurement via BR-Prox; 3) ZSC-Eval comprises human evaluation benchmark results from our human study platform, a part of ZSC-Eval, and comprehensive benchmark results with our generated evaluation partners, providing guidelines for designing ZSC testbeds and empirical analyses for current ZSC algorithms.

## 2 Related Work

**ZSC Problem and Methods.** ZSC algorithms aim to train an ego agent that can be deployed to coordinate with unseen partners without further training. Self-play (SP) [53; 59; 55] is a common way to train ego agents but learns conventions between players and generates agents that lack coordination with unseen partners . Based on SP, representative algorithms involving game structure random

Figure 1: **ZSC-Eval. 1) Generation: generating behavior-preferring agents and their best responses; 2) Selection: selecting evaluation partners by maximizing Best Response Diversity; 3) Measurement: evaluating the ego agent with the evaluation partners and computing Best Response Proximity.**

ization  and diversity-based reward shaping  are derived to mitigate convention overfitting. Besides, Population-based training (PBT) algorithms , such as Population Play (PP) , train an ego agent that interacts within a population and encounters multiple partners during training. Fictitious co-play (FCP)  proposes a two-stage _Co-Play algorithm_ involving self-play pre-training and ego agent training with the pre-trained population. Most co-play algorithms enhance the diversity of training population by population entropy-shaped reward , hidden-utility reward functions that model human behaviors , training incompatible agents , and contextual encoding for partner identification [28; 27]. Moreover, _Evolution algorithms_ train the ego agent with evolving populations, updating the pool by promoting unique behaviors  and open-ended learning [57; 23; 24; 58].

**ZSC Evaluation and Analysis.** Researchers have analyzed ZSC in human-agent and agent-agent teams. McKee et al.  introduce the expected action variation metric for population diversity to assess agents' generalization. Knott et al.  argue that the average training or validation rewards do not reflect agent robustness. Some studies discuss the subjective evaluation of human-AI team performance but do not focus on ZSC capability [48; 31]. In contrast, we focus on evaluating the ZSC capability using diverse evaluation partners. Our ZSC-Eval aims to solve problems in generating evaluation partners and comprehensive and fair comparisons. We analyze the current evaluation methods and demonstrate the superiority of our ZSC-Eval in Table 1. To the best of our knowledge, ZSC-Eval is the first evaluation toolkit and benchmark for comprehensive ZSC capability evaluation.

## 3 Background

### Decentralized Markov Decision Process

We formulate the ZSC problem in multi-agent scenarios as a decentralized Markov decision process (DEC-MDP) . An \(n\)-agent DEC-MDP can be formalized as \(<,\{^{i}\}_{i},,,r,>\), where \(=\{1,,n\}\) is the set of agents, \(\) is the state space, \(:\) is the distribution of the initial state \(s_{0}\). \(^{i}\) is the action space of agent \(i\), and \(=^{1}^{n}\) is the joint action space. \(:\) denotes the transition probability. \(r:\) is the reward function, and \([0,1)\) is a reward discount factor. At time step \(t\), each agent \(i\) takes action \(a_{t}^{i}\) from its policy \(^{i}(|s_{t})\), simultaneously according to the state \(s_{t}\), forming the joint action \(_{t}=\{a_{t}^{1},,a_{t}^{n}\}\) and the joint policy \((|s_{t})=^{1}^{n}\). We denote the expected discounted return as \(()=_{(,)}[_{t} ^{t}r(s_{t},_{t})]\). Note that we concisely use \(()\) under permutations of agents and \((,^{-i})=(,,,^{-i})\) where \(\) repeats \(n-|^{-i}|\) times, without loss of generality. For convenience, we denote the Best Response (BR) of policy \(^{-i}\) as \(BR(^{-i})=*{argmax}_{^{}}(^{}, ^{-i})\). Let \(_{}\) be the set of potential unseen partners, named deployment-time partners in this paper, and \(^{i}\) be the ego agent's policy. The optimization objective of the ZSC problem can be represented as: \(_{}_{((_{} ))}[(,\{^{i}\}_{i})]\), where \(()=\{^{m}|1 m<n\}\) denotes the combinations of agents in \(\) with different sizes, and we assume partners are sampled from a uniform distribution \(\). As we focus on population-based ZSC algorithms, we further formalize the objective that considers the construction of the training population:

\[_{_{},}_{( (_{}))}[((_{}),\{^{i}\}_{i})]\,\]

  
**Evaluation Partners** & **Reproducible** & **Cost** & **Extendability** & **Unseen** & **Diverse** & **Deployment Requirements** \\  Human Players  & \(\) & High & - & ✓ & ✓ & ✓ \\ Human Proxy Agents  & ✓ & Medium & Weak & ✓ & \(\) & \(\) \\ Trained Self-play Agents  & ✓ & Low & Strong & \(\) & \(\) & \(\) \\ Trained Adaptable Agents  & ✓ & Low & Moderate & \(\) & \(\) & \(\) \\ Rule-based Specialist  & ✓ & High & Weak & ✓ & ✓ & ✓ \\ Random Agents  & ✓ & Low & Strong & ✓ & \(\) & \(\) \\ 
**ZSC-Eval (Ours)** & **Low** & **Strong** & **✓** & **✓** & **✓** \\   

Table 1: Comparison of evaluation partners used in recent works. _Cost_ - Implementation efforts and financial outlay. _Extendability_ - the degree to which they can be expanded in new scenarios. _Unseen_ - their distributions are not similar to training partners. _Diverse_ - their skills’ style and level are diverse. _Deployment Requirements_ - they can follow the distribution of deployment-time partners.

where \(_{}\) is the population constructed during training and \(\) is an approximate oracle function that computes the common best response for partners in \(_{}\). For instance, the oracle function can be defined to maximize the objective with \((_{})\), i.e., \((_{})=*{argmax}_{}_{ *{L}((_{}))}[ (,\{^{i}\}_{i*{L}})]\).

### Limitations of Current Evaluation Methods

In MARL, the ZSC problem focuses on zero-shot generalization to unseen cooperative partners, presenting an OOD generalization challenge due to the difference between training and deployment-time partners. To evaluate the ZSC capability accurately, evaluation partners must follow deployment-time partners' distributions, including human and other agents. Furthermore, the generalization performance of the ego agent must be measured in addition to task performance metrics like episode returns . We discuss the gap between reasonable and current evaluation methods as follows.

**Are current evaluation partners convenient and following the distributions of deployment-time partners?** Diversity in evaluation partners is not just a desirable feature but a necessary condition for them to effectively cover the distribution of deployment-time partners. In our analysis, current evaluation partners in Table 1 (detailed in Appendix A) can be classified into two types: training-based and non-training-based. As the most commonly used training-based evaluation partners, human proxy agents trained by behavior cloning human data can not mimic real human behaviors . Trained self-play partners frequently fail to diverge from the training population since they may not achieve distinct high-level behaviors [7; 45] though efforts have been made to generate diversity through low-level behavior optimization [30; 65]. Non-training-based partners like random agents do not provide diversity while maintaining high performance. The lack of diversity among these partners makes it difficult to match the distribution of deployment-time partners. Besides, some evaluation partners suffer from reproducibility problems, high implementation costs, and extendability problems, and they are similar to training partners, as summarized in Table 1.

**Can current evaluation methods and metrics demonstrate the ZSC capability?** At present, evaluation methods in ZSC can be broadly classified into two categories: evaluations with fixed partners and cross-play evaluations. Cross-play evaluations, i.e., using trained adaptive agents from ZSC algorithms as evaluation partners and cross-playing the agents to compare the performance mutually, risk unfair comparisons due to overlaps between evaluation and training partners. Moreover, eliminating overlapping partners might compromise the control conditions of experiments. As for evaluation metrics, both approaches use mean episode returns to evaluate the ZSC capability. However, the current metric needs revision to measure the overall generalization performance of ZSC. It fails to capture crucial aspects such as the generalization gap  and ignores different cooperation capabilities among evaluation partners, highlighting the need for more comprehensive evaluation metrics. The potential for unfair comparisons and limitations of current evaluation metrics significantly undermine their effectiveness in assessing ZSC capabilities.

In summary, there is an urgent need in the ZSC community to develop a comprehensive evaluation toolkit and benchmark to assess ZSC capability more accurately and drive progress in ZSC.

## 4 ZSC-Eval

As shown in Figure 1, ZSC-Eval includes evaluation partners generation and selection, and ZSC capability measurement.

### Generation of Behavior-preferring Agents as Candidates

Based on the reward hypothesis that goals and purposes can be well thought of as maximizing the expected cumulative sum of the received reward [52; 4], we assume that requirements for deployment-time partners can be represented as reward functions \(^{}=\{r_{1},,r_{P}\}\), where \(P\) is the number of partners. Consequently, deployment-time partners can be approximated by optimizing policies to maximize these reward functions. Specifically, the distribution of deployment-time partners is tailored to various applications, such as care robots [41; 37] and team sports . Event-based rewards are widely adopted as a standard reward design method and a practical design principle in these applications [36; 11; 33; 62; 2; 47]. Therefore, we use event-based rewards, which we named behavior-preferring rewards, to approximate \(^{}\). Behavior-preferring rewards allow conveniently designating the coverage of evaluation partners to include common and edge cases  and entail high reproducibility, low implementation cost, and strong extendability, as summarized in Table 1.

Specifically, we use a linear function combination to approximate the reward space \(^{}\), as in Ng et al. . The approximate reward space is defined as \(^{}=\{r_{}|r_{}(s_{t},_{t})=r+(s_{t}, _{t})^{T},^{m},\|\|_{} B_{ {max}},_{i}(_{i} 0) C_{}\}\), where \(\) is an \(m\)-dimensional weight vector and \(r_{}\) is the reward function that encourages behaviors indicated by \(\). \(:^{m}\) embeds event-based features, e.g., \((s_{t},_{t})_{j}\) indicates whether the \(j\)-th event has occurred. \(B_{}\) limits the norm of \(\), while \(C_{}\) limits the number of events, eliminating unusual behaviors. The original game reward \(r\) is added to prevent behavior-preferring agents from sabotaging. Under these constraints, \(^{}\) promotes diverse behaviors and still encourages cooperative task completion.

We train behavior-preferring agents and their best responses using behavior-preferring reward functions. Given a specific reward function \(r_{}\), one player receives this reward while the others continue receiving the original game reward \(r\). The procedure of optimizing the players' objectives can be formulated as finding a Nash Equilibrium (NE)  in a Stochastic Game . We can approximate an NE by agents independently performing the Proximal Policy Optimization (PPO) algorithm  since \(r_{}^{}\) still guides the behavior-preferring agent to cooperate for solving the given task .

After approximating an NE, we obtain \(_{}\) that learns the behaviors preferred by \(\) and \((_{})\), the approximate BR of \(_{}\). Line 1 to Line 3 in Algorithm 1 summarize that ZSC-Eval constructs evaluation partner candidates which cover a set of diverse behaviors by sampling reward functions from \(^{}\) and approximating NEs.

### Selection of Evaluation Partners by Best Response Diversity

The generated candidates may be unbalancedly distributed in cooperative conventions and behaviors, which we further discuss in Section 5.1. To avoid the unbalanced evaluation of ZSC agents that coordinate well with those behaviors with high proportions in candidates, we need to select a representative subset of candidates as evaluation partners.

Typically, the most representative population subsets can be obtained by maximizing the _population diversity_. We first define the _population diversity_ of a population \(\{_{i}\}_{i=1}^{M}\) as the determinant of the population's similarity matrix: \((\{_{i}\}_{i=1}^{M}):=()\), where \(_{ij}=_{i}_{j}\) is the similarity matrix of the population, and \(_{i}\) is the behavior feature of policy \(_{i}\).3 One can intuitively repeat sampling subsets from candidates and

Figure 2: (a) Different partners may respond to similar BRs. (b) Population diversity of BRs to partner subsets selected by two methods with different sizes. A higher vertical axis value at the same subset size indicates more diverse BRs in the subset.

select the subset with the maximum population diversity as the evaluation partners, which can be formatted as maximizing the _Partner Diversity_ (P-Div), where \((\{_{i}\}_{i=1}^{M})=(\{_{i}\}_{i=1}^{M})\).

However, based on the fact that an ego agent with strong ZSC capability should emulate any policy in the set of BRs to evaluation partners , the evaluation method should expose the ego agent to evaluation partners with diverse BRs. Different partners selected by P-Div may respond to similar BRs [45; 22], as illustrated in Figure 2(a). Therefore, maximizing P-Div may not necessarily produce partners that require diverse skills to coordinate with [42; 43]. To further verify, we define **Best Response Diversity** (BR-Div) as \((\{_{i}\}_{i=1}^{M})(\{(_{i})\}_{i=1}^{M})\), which is the population diversity of approximate BRs to selecte candidates. As in Figure 2(b), selections from a pool of evaluation partner candidates based on BR-Div reach a higher population diversity of BRs than those based on P-Div, meaning that maximizing BR-Div is more effective in constructing evaluation partners with diverse BRs. We include details of Figure 2(b) and demonstrate that evaluation partners selected by BR-Div exhibit more diverse behaviors than those selected by P-Div in Appendix D.

Therefore, we select evaluation partners through maximizing BR-Div, as summarized in Line 4 to Line 6 in Algorithm 1. In detail, we count occurrences of pre-defined events of \((_{^{i}})\) alongside episodes as the high-level behavior feature \(_{^{i}}=_{_{^{i}},(_{^{i}})}[_{t=1}^{T}(s_{t},_{t})]^{m}\) of \((_{^{i}})\) for calculating BR-Div. Then we compute the similarity matrix as \(\) where \(_{ij}=_{^{i}}_{^{j}}\). Since BR-Div is defined as a determinant function, we apply the Determinantal Point Process (DPP)  to search for the candidate subset of size \(M\) with the maximum determinant. DPP samples proportionally to determinants of candidate subsets: \(P(\{_{^{i}}\}_{i})(\{_{ ^{i}}\}_{i})=(_{})\), where \(\{1,,N\}\) is the subset's indices and \(_{}\) denotes the submatrix of \(\) obtained by restricting rows and columns indexed in \(\). Because candidate subsets are usually inexhaustible, we repeat DPP sampling to search for the representative candidate subset and denote the selected subset as \(}=*{argmax}_{}(\{_{^{i}}\}_{i})\) and \(||=M\). Furthermore, as shown in Line 7 of Algorithm 1, we collect the earlier checkpoints of selected candidates to enhance the diversity of skill levels, which satisfies \(((_{^{i}}),_{^{i}}) ((_{^{i}}),_{^{i}})/2,\ i \).

### Measurement of ZSC Capability by Best Response Proximity

Previous evaluation methods measure ZSC capability by mean episode returns, but there are two limitations to using mean episode returns: 1) Using mean episode returns does not provide a standard for presenting how well the learned cooperation ability is generalized. For example, when evaluating agents' generalization ability, it is recommended to show their generalization gap in auxiliary, i.e., the gap between training performance and testing performance . 2) Mean episode returns do not consider the unbalanced cooperation capabilities among evaluation partners, and results with different evaluation partners should not be weighted equally. To tackle these limitations, we introduce the **Best Response Proximity** (BR-Prox) metric. Formally, we define:

\[(,\{_{^{i}}\}_{i} )()}{}((,\{_{^{i}}\}_{i}) \ /\ (\{_{^{i}}\}_{i}),\{_{ {w}^{i}}\}_{i}),\]

where \(\) means the aggregator across evaluation partners, such as the most common'mean' and'median' aggregators. We adopt the inter-quartile mean to aggregate the data , focusing on the middle 50% for statistical reliability. BR-Prox evaluates performance similarity between the ego agent and approximate BRs, presenting the generalization gap and balancing results with evaluation partners based on their cooperation capability. Since a single score cannot fully capture the performance variance across evaluation partners , we recommend reporting the results with 95% confidence intervals  and inter-quartile ranges, e.g., the middle 50% of disaggregated scores.

## 5 Experiments

In this section, we conduct a series of experiments in popular coordination environments Overcooked [5; 22] and Google Research Football (GRF) . We first verify ZSC-Eval's effectiveness both in generating diverse evaluation partners and evaluating ZSC capability, compared with current evaluation methods, including human evaluation. We then benchmark current ZSC algorithms using ZSC-Eval and show novel empirical findings about how ZSC-Eval helps evaluate ZSC algorithms.

**Environments.** We conduct experiments in two environments. We retain four commonly used layouts in Overcooked, including Asymm. Adv., Coord. Ring, Forced Coord., and Counter Circ.. We leverage two new layouts, Bothway Coord. and Blocked Corr. and create three more new layouts with the multi-recipe mechanism to increase the necessity and difficulty of cooperation. Then, we choose the '3 vs 1 with Keeper' scenario in GRF as a ZSC testbed, letting the ego agent be a team member and collaborate with the other three players. Environment details can be found in Appendix B.

**Experiment Setup.** We implement six strong methods, including FCP , MEP , TrajeDi , HSP , COLE  and E3T , and additionally add self-play (SP)  as a baseline. We also evaluate these ZSC algorithms with humans in Overcooked. More experiment setup details and full results are in Appendix C.

### Effectiveness of ZSC-Eval

**ZSC-Eval's Evaluation Partners Exhibit the Most Diverse Behaviors.** We demonstrate the population diversity of our generated evaluation partners and evaluation partners used in current evaluation methods in Figure 3. Our generated evaluation partners exhibit the most diverse behaviors. More results shown in Appendices D.2 and D.3 further illustrate that our generated evaluation partners exhibit more diversity in high-level behaviors and episode return distributions obtained with ZSC agents. The diversity of evaluation partners means that ZSC-Eval has a strong ability to approximate deployment-time partners.

**Highly Similarity between Evaluations by ZSC-Eval and Human.** To demonstrate the effectiveness of ZSC-Eval for evaluating ZSC algorithms, we compare human evaluation results with evaluation results using different evaluation partners. The results shown in Table 2 verify that ZSC-Eval's results are the closest to human evaluations and that the Spearman's rank correlation coefficient (\(r_{s}\))  between ZSC-Eval and human evaluation reaches the highest, meaning that ZSC-Eval effectively obtains evaluation results similar to those with humans. We also collect human subjective rankings and compare them with objective score rankings. The human subjective perceptions are generally consistent with the objective episode returns. Detailed results are provided in the Appendix D.7.

### Benchmark Results and Empirical Findings in Overcooked

We present abundant benchmark results with 9 Overcooked layouts in Figures 4 to 6, in which we implement each population-based algorithm with three different population sizes. We observe that co-play algorithms outperform other algorithms in most layouts, and population-based algorithms

   &  &  & \)} \\   & & HSP & MEP & FCP & COLE & SP \\   & Human & 3 & 2 & 4 & 5 & - \\  & ZSC-Eval (Ours) & 2 & 3 & 4 & 5 & **0.90** \\  & Human Proxy & 2 & 3 & 4 & 5 & **0.90** \\  & Trained SP Agents & 4 & 3 & 2 & 5 & 0.70 \\   & Human & 3 & 2 & 4 & 5 & - \\  & ZSC-Eval (Ours) & 1 & 3 & 2 & 4 & 5 & **1.00** \\   & Human Proxy & 3 & 2 & 4 & 5 & 0.60 \\   & Trained SP Agents & 4 & 3 & 2 & 5 & 0.10 \\  

Table 2: Ranks of ZSC algorithms under different evaluation partners in various Overcooked layouts. \(r_{s}\) measures the correlation between ranks under human evaluation and ranks under others.

Figure 3: Visualization of high-level behaviors of human proxy agents, different self-play populations, our evaluation partner candidates, and evaluation partners in Overcooked layouts.

generally perform better as the population size increases. Full results can be found in Appendix D. We summarize two empirical findings below.

**Guidelines for Increasing Complexity in Designing ZSC testbeds.** Results in Figure 4 indicate the commonly used layouts, Forced Coord. and Asymm. Adv. fail to differentiate algorithms' performance. We have also noticed that SP performs well in these layouts, indicating that it can easily learn most of the skills for interacting with unseen partners. These results suggest that some layouts' simplistic design limits the showcase of agents' ZSC capabilities due to insufficient required cooperative strategies. We improve layouts by increasing their complexity, including the complexity of agent coordination (_coordination complexity_) and overall team tasks (_task complexity_).

_Coordination Complexity._ We observe the connection between whether layouts differentiate algorithms' performance and degrees of resource-sharing in four old layouts and then classify the layouts as 'Limited Resource-sharing' and 'Full Resource-sharing' in Figure 4. As a case of coordination complexity, the resource-sharing mechanism increases the need for cooperative strategies and helps measure ZSC capability, demonstrating the importance of increasing coordination complexity. To investigate further, we increase the coordination complexity by letting the layouts require more frequent interaction among agents. The Bothway Coord. and Blocked Coor. layouts we leverage include passing cooking ingredients bidirectionally and scheduling spare counters and a corridor. In these new layouts, ZSC algorithms exhibit a more significant performance difference, demonstrating the effectiveness of increasing coordination complexity.

_Task Complexity._ We leverage the multi-recipe mechanism in three old layouts to increase the task complexity and then present benchmark results in these new layouts. As shown in Figure 5, the performance difference between ZSC algorithms has significantly increased after using the multi-recipe mechanism, indicating the effectiveness of increasing task complexity. While the increased population size could improve policy diversity within the population , the performance improvement as the population size grows is only apparent in experiments where we leverage the multi-recipe mechanism. Such a phenomenon indicates that the increased task complexity enables layouts to demonstrate the effect of varying population sizes since more cooperative strategies are

Figure 4: BR-Prox performance with 95% confidence intervals of ZSC algorithms with different population sizes in Overcooked. ‘12’25’, ‘24’50’ and ‘36’75’ mean that co-play methods (FCP, MEP, TrajeDi and HSP) are trained with populations of 12, 24 and 36 and that the evolution method (COLE) is trained with populations of 25, 50 and 75. Note that SP and E3T are not population-based.

Figure 5: BR-Prox performance of ZSC algorithms in Overcooked with multiple recipes.

required, which is desired for ZSC evaluation. Therefore, when developing ZSC testbeds, we suggest prioritizing task complexity.

**Performance Degradation with Expert Evaluation Partners.** ZSC-Eval highlights the performance variation of ZSC algorithms with evaluation partners of varying skill levels. We investigate how ZSC algorithms perform when faced with unseen partners at different skill levels by considering the evaluation partners with self-play performance less than the median, i.e., \(\{^{P}|^{P},(^{P})_{^{P} }(^{P^{}})\}\), as moderate evaluation partners, and the left ones as expert evaluation partners. Owing to BR-Prox that measures generalization capability rather than episode returns, ZSC-Eval reveals that current ZSC algorithms perform worse with expert evaluation partners, as shown in Figure 6. Furthermore, increasing population size has a lower impact on performance when dealing with expert evaluation partners than moderate evaluation partners. Such results may result from current ZSC algorithms failing to generate enough diverse expert agents even with increasing population sizes, which can be diagnosed using our proposed BR-Div, as elaborated in Appendix D.4.

### Evaluating Zero-shot Coordination Capability in Google Research Football

We further evaluate current ZSC algorithms by ZSC-Eval in the GRF academy '3 vs 1 with Keeper' scenario, a complex cooperative environment with a large state-action space and strong built-in bots as opponents, to investigate ZSC-Eval's scalability. Table 3 shows the performance of each method playing the three-player football game with our evaluation partners, in which the ZSC algorithms' ranks are similar to those in Overcooked. In Appendix D.2, we further illustrate diverse high-level behaviors of ZSC-Eval generated evaluation partners in GRF. These results verify that ZSC-Eval can conveniently scale to more complex scenarios with more than two players.

## 6 Conclusion

In this paper, we first analyze problems of current ZSC evaluation methods, particularly mismatched distributions between evaluation and deployment-time partners and inadequacy metrics for measuring ZSC capability. We present ZSC-Eval, a toolkit and benchmark for evaluating ZSC algorithms, which includes: 1) evaluation partner candidates generation via behavior-preferring rewards, 2) evaluation partners selection via BR-Div, and 3) ZSC capabilities measurement via BR-Prox. ZSC-Eval includes Overcooked and GRF as testbeds and implements commonly used ZSC algorithms. Although ZSC-Eval has limitations in fully representing deployment-time partners, we demonstrate its effectiveness by verifying the diversity of generated evaluation partners and the consistency between ZSC-Eval's evaluation results and human evaluation results. Another limitation mainly lies in the fact that the design of event-based rewards needs careful handcraft and that event-based rewards may not fully represent deployment-time partners. The events represent various situations during the deployment time, which requires the designer to have a comprehensive understanding of the environments and tasks, and is hard to exhaust. The limitation results from the challenge of reward design, which is an inherent challenge in reinforcement learning . To alleviate these limitations, a promising

  
**Method** & **BR-Prox** & **(95\% CI)** \\  SP & 0.20 & (0.14, 0.24) \\ E3T & 0.66 & (0.59, 0.74) \\ FCP & 0.78 & (0.72, 0.85) \\ MEP & 0.78 & (0.71, 0.85) \\ TraieDi & **0.81** & **(0.75, 0.89)** \\ COLE & 0.75 & (0.69, 0.84) \\ HSP & **0.80** & **(0.72, 0.88)** \\   

Table 3: BR-Prox performance with 95% confidence intervals of ZSC algorithms in GRF.

Figure 6: We compare the aggregated BR-Prox performance obtained with evaluation partners at different skill levels.

further direction is to leverage some automatic reward design techniques, e.g., leveraging the large language models for reward generation . We create new ZSC testbeds, propose guidelines for designing ZSC testbeds, and provide detailed analyses about the failure of current ZSC algorithms in coordinating with expert evaluation partners. We believe that ZSC-Eval could be a convenient scaffold for developing future ZSC algorithms.