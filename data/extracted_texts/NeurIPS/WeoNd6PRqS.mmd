# OMG-LLaVA : Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding

Tao Zhang\({}^{1}\), Xiangtai Li\({}^{2,4}\)\({}^{}\), Hao Fei\({}^{2}\), Haobo Yuan\({}^{3}\), Shengqiong Wu\({}^{2}\),

**Shunping Ji\({}^{1}\), Chen Change Loy\({}^{3}\), Shuicheng Yan\({}^{2}\)**

\({}^{1}\)Wuhan University \({}^{2}\)Skywork AI \({}^{3}\)S-Lab, NTU \({}^{4}\)Bytedance

**Project page: https://lxtgh.github.io/project/omg_llava**

_Email: xiangtai94@gmail.com and zhang_tao@whu.edu.cn_

###### Abstract

Current universal segmentation methods demonstrate strong capabilities in pixel-level image and video understanding. However, they lack reasoning abilities and cannot be controlled via text instructions. In contrast, large vision-language multimodal models exhibit powerful vision-based conversation and reasoning capabilities but lack pixel-level understanding and have difficulty accepting visual prompts for flexible user interaction. This paper proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level vision understanding with reasoning abilities. It can accept various visual and text prompts for flexible user interaction. Specifically, we use a universal segmentation method as the visual encoder, integrating image information, perception priors, and visual prompts into visual tokens provided to the LLM. The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information. We propose perception prior embedding to better integrate perception priors with image features. OMG-LLaVA achieves image-level, object-level, and pixel-level reasoning and understanding in a single model, matching or surpassing the performance of specialized methods on multiple benchmarks. Rather than using LLM to connect each specialist, our work aims at end-to-end training on one encoder, one decoder, and one LLM. The code and model have been released for further research.

## 1 Introduction

With the development of transformer models [94; 6; 93; 40; 71; 92; 64; 126; 49; 87; 10; 19; 58], recent works in both natural language processing (NLP) and computer vision raise one common trend: adopting one unified model to solve multiple tasks. For example, large language models (LLMs) [93; 40; 92] adopt scale-up models to solve multiple NLP tasks and achieve better results than previous expert models. In vision, we have also seen a similar trend [19; 58; 100; 99; 47; 112], adopting one model to solve multiple tasks or sub-tasks, including detection, segmentation, video analysis, low-level vision, pose estimations, and more tasks. Different methods adopt different transformer designs, including visual-in-context learning [99; 100], unified decoder [19; 58], and unified tokenizer [86; 16; 58]. In summary, benefiting from the _scalability_ and _flexibility_ of the transformer, adopting one model for all tasks has made a great progress [19; 71; 72; 70; 126; 88; 87].

Meanwhile, by combining vision models and language models [71; 72; 70; 64; 65; 107], research on multi-modal models also adopts transformer-based design. One representative work, LLaVA [71; 72; 70], treats visual tokens as the inputs of LLMs and makes LLMs understand visual contents. Several works adopt similar designs [3; 13; 64; 18; 25], and all of them are termed Multi-modal Large Language Models (MLLMs). After that, most research focuses on improving MLLM benchmarks in various ways, including increasing data sizes [14; 18; 70] and enhancing the visual encoders [131; 24; 18] and visual resolutions [110; 18; 65; 25]. However, LLaVA-like models cannot output precise location information since they only carry out image-level analysis. Thus, recent works [126; 133; 111; 82; 13; 119; 128; 87; 67] try to fill this gaps by adding extra detection models for object level analysis, mask decoder for pixel-level analysis, visual prompts, and also propose task-specific instruction tuning with various datasets. By providing extra detection data and a decoder, the updated MLLMs can perform localization output. However, these models [135; 96; 49] are specifically tuned on specific tasks, losing the ability of LLaVA for image level analysis, such as caption and visual question answering. Meanwhile, several works [126; 49; 87; 78] adopt LLMs as agents to collaborate with various visual models or generation models. Despite the works being simple and effective, the inference and parameter costs are huge due to the multiple visual encoders and decoders. Moreover, there are no specific designs for task unification.

Motivated by the previous analysis, we ask one essential question: Can we bridge image-level, object-level, and pixel-level tasks into one MLLM model with only one LLM, one visual encoder, and one visual decoder? Back to the universal perception models, we can leverage these models to help us build a stronger MLLM to unify three-level inputs, including image, object, and pixel levels. In particular, we adopt OMG-Seg  as our universal perception model due to its simplicity and effectiveness in various segmentation tasks.

In this work, we present OMG-LLaVA, an elegant MLLM that bridges image-level, object-level, and pixel-level reasoning and understanding tasks in one model. We preserve the basic pixel-level segmentation ability of OMG-Seg by freezing the visual encoder and decoder, as shown in the bottom left of Fig. 1. Since the LLM processes text input, OMG-LLaVA can also perform referring segmentation, reasoning segmentation, and grounded conversation and generation, shown in the top left of Fig. 1. Moreover, as shown in Fig. 1, with the help of LLMs, OMG-LLaVA can also perform

Figure 1: The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks.

image-level understanding as LLaVA, including caption and conversation, where most MLLMs for grounding lose such ability. In addition, OMG-LLaVA also supports the visual prompts as inputs, which results in object level understanding, such as visual prompt-based conversation and region-level captions. We achieve all these abilities using one LLM, one encoder, and one decoder.

In particular, to better encode the visual segmentation outputs, we propose a perception prior embedding module to absorb the object queries into object-centric visual tokens, which are the inputs of LLMs. We present a unified instruction formation strategy, which lets the model accept visual images, texts, and visual prompts as inputs and generate the response of text, segmentation tokens, segmentation masks, and labels. Following the LLaVA , we adopt pretraining and instruct tuning pipelines. Extensive experiments show the effectiveness of our components and training strategy. In addition to visual segmentation, OMG-LLaVA can also achieve good enough performance on 6 datasets, including COCO panoptic segmentation, VIPSeg video panoptic segmentation, refCOCO, refCOCO+, refCOCOg referring expression segmentation, GranDf grounded conversation generation, and refCOCOg region caption datasets. We hope our research can inspire the research on MLLM design in a more elegant way for the community.

## 2 Related Work

**Multimodal Large Language Models.** Early multimodal models  explore better fusion strategies, various feature extractors, and different meta-architectures. Most works focus on single tasks, such as caption and VQA. With the development of the large language models [6; 93; 40], recent works [52; 3; 92; 71; 70] mainly explore building an instruction-tuning pipeline for multiple multimodal benchmarks [39; 74; 62; 32]. LLaVA [71; 70; 69; 106; 136; 30; 28] is one earlier work that treats visual features as tokens. After that, several works  explore visual cues to enhance the visual inputs of LLaVA. On the other hand, several works [129; 127; 88; 131; 24; 25; 66; 130; 83; 38; 49] add extra components to adapt LLaVA for visual grounding, detection, segmentation, and video analysis. In particular, several works explore language-driven grounding and segmentation. However, these works are all trained with a specific purpose. We aim to build the simplest model to unify segmentation, instruction tuning, and prompt-driven segmentation in one model. To the best of our knowledge, we are the first model to achieve this goal.

**Unified Segmentation Models.** The vision transformers [10; 26; 79; 94] have led to research interest in universal segmentation. Recent works [95; 19; 123; 56; 21; 117; 115; 73; 91; 124; 122; 139; 111; 54; 141] have developed mask classification architectures with an end-to-end set prediction approach, outperforming previous specialized models [12; 46; 36; 57; 34; 60; 140] in both image, video and generalization segmentation tasks [45; 61; 59]. In particular, several works explore open-world segmentation, including entity segmentation [85; 84], open-vocabulary segmentation [125; 105]. Meanwhile, several works [58; 41; 111; 112; 33; 2] adopt one model with shared parameters to perform various segmentation tasks. One recent work, OMG-Seg , first unifies image, video, open-vocabulary, and interactive segmentation in one simple model. However, all of these works focus on visual segmentation and cannot generate interactive text and visual prompts, like MLLMs. Our work builds such a bridge to align MLLMs, visual segmentation, and prompt-driven segmentation models from joint co-training and model sharing, which serves as a new baseline for this field.

**Language-driven Location and Segmentation.** Early works [120; 68; 44; 23; 104; 135] in this direction mainly define the various language-driven tasks, including referring segmentation and

Figure 2: Summary of Current MLLM Architectures: (a) MLLMs with only image-level capability, including [71; 72; 70; 65], etc., (b) MLLMs with object-level capability, including [126; 87], (c) MLLMs with pixel-level capability, including [49; 88], etc., (d) MLLMs with both object-level and pixel-level capabilities but with a very complex system, such as , (e) OMG-LLaVA’s architecture, which possesses an elegant and simple design while having image-level, object-level, and pixel-level capabilities.

referring localization. Most works [31; 5; 116; 77; 103; 105] design effective fusion modules to achieve better performance. Meanwhile, several works [55; 103; 49; 87; 126; 81] explore more complex language-driven tasks from various aspects, including robustness, reasoning, and region-level caption. LISA  involves reasoning-based segmentation. Then, GLaMM  annotates a new dataset and proposes region-level caption and segmentation tasks. Meanwhile, several works [29; 72] use LLMs as agents to assign different visual experts. In contrast to these works, our method is a more elegant baseline, which contains **only** one visual encoder, one LLM, and one decoder.

**Visual Prompts.** With the prompting ability of LLMs, several works [100; 99; 4; 138; 90; 51; 81] also explore visual prompting methods in vision. According to the design and purposes, these works can be divided into different aspects, including learnable tokens , mask-visual-modeling for different tasks [100; 27; 98], and various visual prompting encoders for visual outputs [99; 101; 125; 47]. Our OMG-LLaVa also supports visual prompts for better interaction with the user's inputs, showing the potential for product purposes.

## 3 Methodology

### Task Unification

**Motivation and Our Goals.** The LLMs unify most NLP tasks as token generation tasks and exhibit strong reasoning and instruction-following capabilities. As shown in Fig. 2 (a), LLaVA-like models [71; 70; 69; 110; 65; 131; 24; 25; 18; 64] further introduce visual tokens into LLMs, enabling LLMs to understand visual information and perform visual-based reasoning. However, they cannot accomplish fine-grained visual tasks like object-level and pixel-level understanding and reasoning. As shown in Fig. 2 (b), [126; 133; 11; 82; 119; 112; 83] introduce region-level visual embeddings, allowing LLMs to achieve object-level understanding and reasoning tasks. However, these models rely on complex region embedding extraction designs. In addition, most cannot perform pixel-level understanding tasks. Thus, as shown in Fig. 2 (c),[49; 88; 35] introduce segmentation tokens, enabling LLMs to output segmentation masks and thus handle pixel-level understanding and reasoning tasks. Nonetheless, they require a large segmentation module, such as SAM , making the system highly redundant. As shown in Fig. 2 (d), GLAMM  combines the above pipelines to handle object-level and pixel-level tasks. However, this significantly increases the system's _complexity_ and _redundancy_. Additionally, GLAMM relies on explicit instructions from the user, **losing** the perception ability to handle basic pixel-level understanding tasks such as instance segmentation, semantic segmentation, panoptic segmentation, and interactive segmentation.

    & Visual &  &  &  \\  & Encoder & Caption & Conversation & Visual Prompts & Caption & Conversation & Universal Seg & RES & GCG \\  LLVA  & 1 & ✓ & ✓ & & & & & & & \\ MiniCPT  & 1 & ✓ & ✓ & & & & & & \\ mP/LG-Ort  & 1 & ✓ & ✓ & & & & & & \\ LLMa-Adper (132) & 1 & ✓ & ✓ & & & & & & \\ Mini-Gemina  & 2 & ✓ & ✓ & & & & & & \\ Internal. V. 15  & 1 & ✓ & ✓ & & & & & ✓ & \\ VisualLMM  & 1 & ✓ & ✓ & & & & & ✓ & \\ Shixxie  & 1 & ✓ & ✓ & Point \& Box & ✓ & ✓ & & \\ Kosmec-2  & 1 & ✓ & ✓ & Box & ✓ & ✓ & & & \\ GPT4Rd  & 1 & ✓ & ✓ & Box & ✓ & ✓ & & & \\ Ferret  & 1 & ✓ & ✓ & Point \& Box \& Mask & ✓ & ✓ & & \\ Oappy  & 1 & ✓ & ✓ & Mask & ✓ & ✓ & & & \\ SPHIXV  & 1 & ✓ & ✓ & Point \& Box \& Mask & ✓ & & \\ LSA  & 2 & ✓ & ✓ & Box & ✓ & ✓ & & ✓ & ✓ \\ GLAMM  & 2 & ✓ & ✓ & Box & ✓ & ✓ & ✓ & ✓ & ✓ \\ Grounding  & 4 & ✓ & ✓ & Point \& Box \& Mask & ✓ & ✓ & ✓ & ✓ \\ AnyRed  & 2 & ✓ & ✓ & Box & ✓ & ✓ & ✓ & ✓ & \\ PixelLM  & 1 & ✓ & ✓ & & & & & ✓ & \\ GSVA  & 2 & ✓ & ✓ & Box & ✓ & ✓ & & & \\ Groma  & 1 & ✓ & ✓ & Box \& Mask & ✓ & ✓ & & \\ VIP-LLVA  & 1 & ✓ & ✓ & Point \& Box \& Mask & ✓ & ✓ & \\ PSALM  & 1 & & & Point \& Box \& Mask & & & ✓ & ✓ & \\ LaSgan  & 2 & & & & & & & ✓ & ✓ & \\ OMG-SSG  & 1 & & & Point & & & ✓ & & \\ OMG-LLaVa & 1 & ✓ & ✓ & Point \& Box \& Mask & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of capabilities of different models. We include several representative methods here. Our OMG-LLaVA offers the most comprehensive capabilities, encompassing image-level, object-level, and pixel-level understanding and reasoning. Compared to [87; 35], OMG-LLaVA features an elegant and simple system architecture with only a single visual encoder.

[MISSING_PAGE_FAIL:5]

comprises an image encoder, an OMG decoder , and a non-trainable perception prior embedding component.

**Image Encoder.** To maximize the perception capabilities of the universal perception module, we use the ConvNeXt-L -based CLIP  model as the image encoder and employ a high image resolution (1024\(\)1024). However, the large image resolution results in excessive visual tokens input into the LLM, leading to significantly higher computational costs than using lower-resolution images (such as 224\(\)224 or 336\(\)336). We address this issue by utilizing the lowest resolution image features (32\(\) downsampling). Additionally, we use the pixel shuffle operator to further reduce the image features' resolution. Ultimately, the downsampling factor for the image features used to generate visual tokens is 64, meaning that a 1024\(\)1024 image produces 256 visual tokens.

**OMG Decoder.** We utilize the OMG decoder  to generate object-centric visual tokens, furnishing the LLM with information regarding the primary objects in the image and those mentioned by the user's input visual prompts. As shown on the left side of Fig. 4, the OMG decoder comprises masked cross-attention  and self-attention layers. The OMG decoder's input includes a set of learnable object queries [20; 19; 10] for automatically capturing all objects of interest and visual prompt queries derived from encoded input visual prompts . The visual prompt queries and learnable object queries are collectively termed object queries. The OMG decoder probes feature for object queries from the image features by employing masked cross-attention and models relationships between objects through self-attention. The object queries can be decoded into segmentation masks and object categories via a simple FFN layer. With the OMG decoder, OMG-LLaVA can efficiently tokenize object information into object-centric visual tokens, thereby equipping the LLM with information about objects in the image and those referenced by the user.

The OMG decoder can accept point prompts as input. While box and mask prompts can be easily converted into point prompts, this crude conversion significantly loses prompt information, complicating the explicit encoding of the user's intent. To address this, we can impose constraints on the attention masks of the masked cross-attention layers based on the visual prompt to precisely encode the object information referenced by the prompt. As depicted on the right side of Fig. 4, we utilize the box coordinates to define attention masks for all pixel features outside the box for box prompts. Similarly, we directly employ the provided object mask to generate attention masks for mask prompts. With this straightforward attention mask modification strategy, OMG-LLaVA can accurately capture the user's visual prompts, encompassing point, box, and mask prompts.

**Perception Prior Embedding.** We find that directly combining a frozen perception module with LLM doesn't perform well, as also observed in LISA . To retain the full capabilities of the universal perception module, OMG-LLaVA doesn't fine-tune the perception module to adapt to the output of the large language model. Instead, we propose

Figure 4: The Architecture of the OMG Decoder. A simple attention mask generation strategy enables the OMG decoder to encode point, box, and mask prompts.

Figure 5: The process of the perception prior embedding strategy. The perception prior embedding strategy integrates object queries into image features based on segmentation prior.

a perception prior embed-

ding strategy to tackle this challenge. Fig. 5 illustrates the perception prior embedding strategy.

First, we fuse the image features \(^{HW C}\) outputted by the image encoder with the object queries \(^{N_{q} C}\) outputted by the OMG decoder \(\). Specifically, we utilize the segmentation mask \(^{N_{q} HW}\) obtained from the object queries and the corresponding confidence score \(^{1 N_{q}}\) to derive a mask score \(MS^{HW N_{q}}\) for each pixel for the object queries:

\[MS=Softmax(,dim=-1)\] (2)

Then, we compute a weighted average of the object queries \(\) based on the mask score \(MS\) and obtain the corresponding weighted object queries for each pixel. Pixel-centric visual tokens \(T_{pv}\) are obtained by adding the weighted object queries to the image features \(\):

\[T_{pv}=MS+\] (3)

Additionally, we treat the foreground object queries as object-centric visual tokens \(T_{ov}\). The object-centric visual tokens \(T_{ov}\) are concatenated with the pixel-centric visual tokens \(T_{pv}\) to form the visual tokens \(T_{v}=(T_{pv},T_{ov})\), which are input to the LLM to provide rich perception prior information.

**Visual Projector and Text Projector.** Following , we use an MLP as the visual projector, which is responsible for mapping visual tokens to the LLM's text embedding space. Since our visual tokens are pixel-centric and object-centric tokens, the visual projector comprises two MLPs, each handling one type of visual token separately. Inspired by [49; 87], we also use a simple MLP to map the LLM output's hidden states of the [SEG] token to the visual space.

**Instruction Formulation.** OMG-LLaVA can accept **visual** input, **text** input, and **visual prompt** input and output text responses and segmentation token, segmentation masks and labels. Thus, it can handle tasks such as image captioning, image-based conversation, region captioning, visual prompt-based conversation, referring segmentation, reasoning segmentation, grounded conversation, etc. We use a unified instruction formulation to support these functionalities. As shown in Fig. 3, there are three special tokens: <Image>, <Region>, and [SEG]. Before being fed into the LLM, the <Image> token is replaced by visual tokens \(T_{v}\), and the <Region> token can be replaced by any object-centric visual token encoded by the visual prompt. The [SEG] token in the LLM's output is sent to the frozen OMG decoder to be decoded into a segmentation mask.

### Training and Testing Setup

**Training.** Following LLaVA , our OMG-LLaVA performs two-stage training: pretraining and instruction tuning. During the pretraining stage, the perception model and LLM are frozen, and only the visual and text projectors can be tuned. In addition to the text regression loss, we apply regularization penalties to the visual projector \(_{v}\) and text projector \(_{t}\) to preserve object-centric information as much as possible.

\[_{pretrain}=_{text}+_{reg},_ {reg}=(T_{ov}-_{t}(_{v}(T_{ov})))^{2}\] (4)

During instruction tuning, in addition to finetuning the visual projector and text projector, we use LoRA  to finetune the LLM. Following [87; 58], besides the text regression loss, we apply cross-entropy loss and dice loss  to supervise the segmentation mask decoded by the [SEG] token, as shown in following (We set \(=5\)\(=2\) by default):

\[_{intruction}=_{text}+_{mask},_{mask}=_{CE}+_{DICE}\] (5)

**Testing.** The image-level, object-level, and pixel-level understanding and reasoning tasks can all be encompassed within the Eq. 3.1 paradigm. During the inference stage, we encode the necessary task requirements, such as text prompts, visual prompts, and image features, into tokens to input into the LLM. The output tokens of LLM are then decoded into text responses and segmentation mask responses according to the task definition. We refer the readers to check the more details in the appendix.

## 4 Experiment

**Dataset Setup.** During the pretraining stage, we use the LLaVA pretraining dataset  to perform visual-text alignment, following LLaVA. The instruction tuning process of OMG-LLaVA involves a diverse range of tasks and datasets. For image-level understanding and reasoning tasks, we use the LLaVA dataset [71; 72; 70], which includes 665K descriptions, reasoning, and conversation data. For object-level understanding and reasoning, we use the object-level description and conversation data from the Osprey dataset  and the object-level point-prompt data from the MDVP dataset , which contain approximately 74K and 200K data, respectively. For pixel-level understanding and reasoning, we use the referring segmentation datasets, including refCOCO, refCOCO+ , refCOCOg , and refClef, totaling 74K data. Additionally, semantic segmentation datasets, including ADE20k  and COCO-stuff , totaling 26K data, and the grounded conversation generation dataset GranDf , containing 200K data, are used.

    &  &  &  &  \\  & & Encoder & METEOR & CIDEr & AP50 & mIOU & METEOR & CIDEr & AP50 & mIOU \\  Kosmos-2  & ✓ & 1 & 16.1 & 27.6 & 17.1 & 55.6 & 15.8 & 27.2 & 17.2 & 56.8 \\ LISA  & ✓ & 2 & 13.0 & 33.9 & 25.2 & 62.0 & 12.9 & 32.2 & 24.8 & 61.7 \\ GLAMM  & ✓ & 2 & 15.2 & 43.1 & 28.9 & 65.8 & 14.6 & 37.9 & 27.2 & 64.6 \\  OMG-LLaVA & \(\) & 1 & 13.8 & 36.2 & 26.9 & 64.6 & 13.5 & 33.1 & 26.1 & 62.8 \\ OMG-LLaVA & ✓ & 1 & 14.9 & 41.2 & 29.9 & 65.5 & 14.5 & 38.5 & 28.6 & 64.7 \\   

Table 4: Performance on grounded conversation generation datasets. “ft” indicates finetuning on the GranDf  dataset. \(\) indicates that the method used the GranD dataset  for pretraining.

    &  Visual \\ Encoder \\  } & COCO & VIPSeg & refCOCO & refCOCO+ &  & refCOCOg(C) \\  & Encoder & VD & cIoU & cIoU & METEOR & AP50 & METEOR \\  OSprey  & 1 & - & - & - & - & - & - & 16.6 \\ LISA  & 2 & - & - & 74.1 & 62.4 & 13.0 & 25.2 & - \\ NeXT-Chat  & 2 & - & - & 74.7 & 65.1 & - & - & 12.0 \\ LaSagnA  & 2 & - & - & 76.8 & 66.4 & - & - & - \\ GSVA  & 2 & - & - & 76.4 & 64.5 & - & - & - \\ AnyRef  & 2 & - & - & 74.1 & 64.1 & - & - & 16.2 \\ GLAMM+  & 2 & - & - & 79.5 & 72.6 & 15.2 & 28.9 & 15.7 \\ PixelLM  & 1 & - & - & 73.0 & 66.3 & - & - & - \\  OMG-LLaVA & 1 & 53.8 & 49.8 & 78.0 & 69.1 & 14.9 & 29.9 & 15.3 \\   

Table 2: The comprehensive comparison of OMG-LLaVA and other MLLMs regarding pixel-level and object-level understanding and reasoning capability and performance. “-” indicates that the method does not handle this task. \(\) indicates that the method used the GranD dataset  for pretraining, which is significantly larger than the datasets used by other methods.

    &  Visual \\ Encoder \\  } &  COCO \\ PQ \\  } &  VIPSeg \\ VPQ \\  } & refCOCO & refCOCO+ &  & refCOCOg(C) \\  & & & & & & & & METEOR & AP50 & METEOR \\  OSprey  & 1 & - & - & - & - & - & - & 16.6 \\ LISA  & 2 & - & - & 74.1 & 62.4 & 13.0 & 25.2 & - \\ NeXT-Chat  & 2 & - & - & 74.7 & 65.1 & - & - & 12.0 \\ LaSagnA  & 2 & - & - & 76.8 & 66.4 & - & - & - \\ GSVA  & 2 & - & - & 76.4 & 64.5 & - & - & - \\ AnyRef  & 2 & - & - & 74.1 & 64.1 & - & - & 16.2 \\ GLAMM+  & 2 & - & - & 79.5 & 72.6 & 15.2 & 28.9 & 15.7 \\ PixelLM  & 1 & - & - & 73.0 & 66.3 & - & - & - \\  OMG-LLaVA & 1 & 53.8 & 49.8 & 78.0 & 69.1 & 14.9 & 29.9 & 15.3 \\   

Table 2: The comprehensive comparison of OMG-LLaVA and other MLLMs regarding pixel-level and object-level understanding and reasoning capability and performance. “-” indicates that the method does not handle this task. \(\) indicates that the method used the GranD dataset  for pretraining, which is significantly larger than the datasets used by other methods.

    &  FreCOCO \\ cloU \\  } &  &  &  \\  & & & & & & & & & METEOR & mIoU \\  Baseline (M0) & 58.7 & 61.0 & 52.6 & 55.0 & 55.8 & 58.1 & 13.2 & 51.0 \\ + Perception prior embedding (M1) & 72.5 & 74.3 & 63.2 & 65.4 & 67.8 & 70.6 & 13.6 & 62.1 \\ + Object query input (M2) & 74.4 & 75.9 & 64.4 & 66.2 & 68.5 & 71.5 & 13.8 & 63.6 \\   

Table 5: Ablation study on RES and GCG datasets.

**Implementation Details.** We use the pre-trained ConvNext-L  OMG-Seg  as the universal perception module and InterLM2-7B  as the LLM for OMG-LLaVA. We adopt xtuner codebase  to build our model and data pipeline. The image is resized to 1024\(\)1024. During the pretraining stage, only the visual projector and text projector are trained, with an initial learning rate set to 1e-3. During the instruction tuning stage, the initial learning rate is set to 2e-4, with only the perception model kept frozen, and the LLM is fine-tuned using LoRA . The maximum sequence length in the LLM is set to 2,048. All training is conducted on four NVIDIA A800 GPUs with 80GB of memory. The pretraining stage and instruction tuning stage took 7 hours and 48 hours, respectively.

### Main Results

**Comparison with MLLMs.** OMG-LLaVA is comprehensively compared with current MLLMs with perception capabilities, and the results are shown in Tab. 2. OMG-LLaVA demonstrates the most comprehensive capabilities. It achieves performance comparable to the SOTA in referring segmentation, grounded conversation generation, and region captioning. Additionally, OMG-LLaVA retains basic segmentation ability, enabling it to handle universal image and video segmentation tasks. Compared to other MLLMs, OMG-LLaVA features a simple and elegant system design, incorporating only a single visual encoder.

**Referring Expression Segmentation.** We evaluate OMG-LLaVA on refCOCO, refCOCO+, and refCOCOg, with the results shown in Tab. 3. OMG-LLaVA outperforms LISA  by 1.5 cloU, 3.2 cloU, and 4.3 cloU on the validation sets of refCOCO, refCOCO+, and refCOCOg, respectively, while keeping the OMG decoder frozen and using only a single visual encoder. When we unfreeze the OMG decoder and finetune OMG-LLaVA on the referring expression segmentation task, OMG-LLaVA achieves 78.0, 69.1, and 72.9 cloU on refCOCO, refCOCO+, and refCOCOg, respectively, surpassing LISA by 3.1, 4.0, and 5.0 cloU. Compared to PixelLM , OMG-LLaVA shows performance improvements of 5.0 cloU and 3.6 cloU on refCOCO and refCOCOg, respectively.

**Grounded Conversation Generation.** Grounded conversation generation is a comprehensive and complex task that involves both image-level and pixel-level understanding and reasoning. MLLMs need to have the ability to provide fine-grained image descriptions and pixel-level understanding, linking the objects in the image captions to the corresponding segmentation masks. As shown in Tab. 4, when trained with comparable data, OMG-LLaVA surpasses LISA  by 1.9 METEOR and 7.3 CIDEr in image description ability. In terms of pixel understanding, OMG-LLaVA also outperforms LISA by 4.7 AP50 and 3.5 mIoU, even though LISA uses SAM and finetunes its segmentation decoder. Despite GLaMM  using much more training data than OMG-LLaVA, OMG-LLaVA demonstrates comparable pixel-understanding capabilities, outperforming GLaMM with 0.6 CIDEr, 1.4 AP50 and 0.1 mIoU on the test set.

### Ablation and Analysis

**Ablation Study.** We conduct ablation studies on referring expression segmentation and grounded conversation generation datasets, with all training and testing settings consistent with the main experiments. We use a simple combination of OMG-Seg  and LLaVA  as our baseline, similar to LISA , where the [SEG] tokens output by the LLM were input into OMG-Seg to obtain segmentation masks, with OMG-Seg kept frozen.

As shown in Tab. 5, the baseline performed poorly on the RES datasets. Similarly, it exhibited low segmentation quality on the GCG dataset. This is because the LLM did not acquire any segmentation priors and needed to generate segmentation queries based on image features and adapt them to the input of the frozen perception module, which is a challenging task.

Figure 6: Visualization of the effectiveness of the proposed strategies. The left part shows the baseline (M0 in Tab. 5), the middle part shows the model with perception prior embedding (M1 in Tab. 5), and the right part shows the model with both perception prior embedding and object query input (M2 in Tab. 5).

When using our proposed perception prior embedding strategy, OMG-LLaVA exhibits performance gains of 13.8 cIoU, 10.6 cIoU, and 11.7 cIoU on refCOCO, refCOCO+, and refCOCOg, respectively. Additionally, the perception prior embedding strategy also brings a performance improvement of 11.1 mIoU on the GCG dataset and a slight improvement in image description capability (0.4 METEOR). When foreground object queries were provided to the LLM, OMG-LLaVA further improved its performance by 1.9 cIoU on refCOCO and 1.5 mIoU on GCG.

We conducted a visualization analysis of the proposed strategies. As shown in the left part of Fig. 6, the simple baseline has poor capability in associating text and segmentation, which is the crucial reason for its poor performance on RES. When using our proposed perception prior embedding strategy, the object query and pixel features are explicitly integrated according to the perception prior, resulting in significantly enhanced text-segmentation association capability. By adopting the object query input strategy, the quality of some challenging segmentation cases, such as the lower right corner of the fence in Fig 6, slightly improves.

**Qualitative Results.** We provide visualization results of OMG-LLaVA on multiple image-level, object-level, and pixel-level tasks in Fig. 1. Additional qualitative visualization results or comparable visual results for referring expression segmentation and grounded conversation generation are presented in the appendix.

## 5 Conclusion

We present a new MLLM, OMG-LLaVA, which bridges image-level, object-level, and pixel-level understanding and reasoning in one model. Our method only contains one image encoder, one LLM, and one decoder. With proposed perception prior embedding and unified task instruction tuning, OMG-LLaVA can perform over 8 different multi-modal learning tasks, as well as preserving the visual perception ability of the OMG-Seg baseline. Our method can achieve comparable results compared with previous combined works with much fewer trainable parameters and computation costs. We hope our work can inspire the community to rethink the design of the MLLM meta-architecture to minimize the model components and maximize the MLLM's functionalities.

**Acknowledgment.** This work was supported by the National Natural Science Foundation of China (grant No. 42171430). This work is also supported by the CCF-Kuaishou Large Model Explorer Fund. It is also partially supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s).