# Partial Gromov Wasserstein Metric

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The Gromov-Wasserstein (GW) distance has gained increasing interest in the machine learning community in recent years, as it allows for the comparison of measures in different metric spaces. To overcome the limitations imposed by the equal mass requirements of the classical GW problem, researchers have begun exploring its application in unbalanced settings. However, Unbalanced GW (UGW) can only be regarded as a discrepancy rather than a rigorous metric/distance between two metric measure spaces (mm-spaces). In this paper, we propose a particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We establish that PGW is a well-defined metric between mm-spaces and discuss its theoretical properties, including the existence of a minimizer for the PGW problem and the relationship between PGW and GW, among others. We then propose two variants of the Frank-Wolfe algorithm for solving the PGW problem and show that they are mathematically and computationally equivalent. Moreover, based on our PGW metric, we introduce the analogous concept of barycenters for mm-spaces. Finally, we validate the effectiveness of our PGW metric and related solvers in applications such as shape matching, shape retrieval, and shape interpolation, comparing them against existing baselines.

## 1 Introduction

The classical optimal transport (OT) problem  seeks to match two probability measures while minimizing the expected transportation cost. At the heart of classical OT theory lies the principle of mass conservation, which aims to optimize the transfer between two probability measures, assuming they have the same total mass and strictly preserving it. Statistical distances that arise from OT, such as Wasserstein distances, have been widely applied across various machine learning domains, ranging from generative modeling  to domain adaptation  and representation learning . Recent advancements have extended the OT problem to address certain limitations within machine learning applications. These advancements include: 1) facilitating the comparison of non-negative measures that possess different total masses via unbalanced  and partial OT , and 2) enabling the comparison of probability measures across distinct metric spaces through Gromov-Wasserstein distances , with applications spanning from quantum chemistry  to natural language processing .

Regarding the first aspect, many applications in machine learning involve comparing non-negative measures (often empirical measures) with varying total amounts of mass, e.g., domain adaptation . Moreover, OT distances (or dissimilarity measures) are often not robust against outliers and noise, resulting in potentially high transportation costs for outliers. Many recent publications have focused on variants of the OT problem that allow for comparing non-negative measures with unequal mass. For instance, the optimal partial transport problem , Kantorovich-Rubinstein norm , and the Hellinger-Kantorovich distance . These methods fall under the broad category of "unbalanced optimal transport". In this regard, we also highlight , which enhance OT's robustness in the presence of outliers.

Regarding the second aspect, comparing probability measures across different metric spaces is essential in many machine learning applications, ranging from computer graphics, where shapes and surfaces are compared [23; 24], to graph partitioning and matching problems . Source and target distributions often arise from varied conditions, such as different times, contexts, or measurement techniques, creating substantial differences in intrinsic distances among data points. The conventional OT framework necessitates a meaningful distance across diverse domains, a requirement that is not always achievable. To circumvent this issue, the Gromov-Wasserstein (GW) distances were proposed in [8; 24] as an adaptation of the Gromov-Hausdorff distance, which measures the discrepancy between two metric spaces [26; 27; 28; 29]. The GW distance [8; 30] extends OT-based distances to metric measure spaces (mm-spaces) up to isometries. Its invariance across isomorphic mm-spaces makes the GW distance particularly valuable for applications like shape comparison and matching, where invariance to rigid motion transformations is crucial.

The main computational challenge of the GW metric is the non-convexity of its formulation . The conventional computational approach relies on the Frank-Wolfe (FW) algorithm [31; 32]. Optimal transport (OT) computational methods [15; 33; 34; 35; 36; 37; 38; 39; 40], such as the Sinkhorn algorithm, can be incorporated into FW iterations, which yields the classical GW solvers [41; 42; 43].

Given that the GW distance is limited to the comparison of probability mm-spaces, recent works have introduced unbalanced and partial variations [44; 45; 46]. These variations have been applied in diverse contexts, including partial graph matching for social network analysis  and the alignment of brain images . Although solving these unbalanced variants of the GW problem yields notions of _discrepancies_ between mm-spaces, their _metric_ properties remain unclear in the literature.

Motivated by the emerging applications of the GW problem in unbalanced settings, this paper focuses on developing a metric between general (not necessarily probability) mm-spaces and providing efficient solvers for its computation. Our proposed metric arises from formulating a variant of the GW problem for unbalanced contexts, rooted in the framework provided by , which we named the _Partial Gromov-Wasserstein_ (PGW) problem. In contrast to , which introduces a KL-divergence penalty and a Sinkhorn solver, we employ a total variation penalty, demonstrate the resulting metric properties, and provide novel, efficient solvers for this problem. To the best of our knowledge, this paper presents the first metric for non-probability mm-spaces based on the GW distance.

**Contributions.** Our specific contributions in this paper are:

* **GW metric in unbalanced settings.** We propose the Partial Gromov-Wasserstein (PGW) problem and prove that it gives rise to a metric between arbitrary mm-spaces.
* **PGW solver.**Analogous to the technique presented in , we show that the PGW problem can be turned into a variant of the GW problem. Based on this relation, we propose two mathematically equivalent, but distinct in numerical implementation, Frank-Wolfe solvers for the discrete PGW problem. Inspired by the results of , we prove that similar to the Frank-Wolfe solver presented in , our proposed solvers for the PGW problem converge linearly to a stationary point.
* **Numerical experiments.** We demonstrate the performance of our proposed algorithms in terms of computation time and efficacy on a series of tasks: shape-matching with outliers between 2D and 3D objects, shape retrieval between 2D shapes, and shape interpolation using the concept of PGW barycenters. We compare the performance of our proposed algorithms against existing baselines for each task.

## 2 Background

In this section, we review the basics of OT theory, one of its variants in unbalanced contexts called Partial OT (POT), and their connection as established in . We then introduce the GW distance.

### Optimal Transport and Partial Optimal Transport

Let \(^{d}\) be, for simplicity, a compact subset of \(^{d}\), and \(()\) be the space of probability measures defined on the Borel \(\)-algebra of \(\).

**The Optimal Transport (OT) problem** for \(,()\), with transportation cost \(c(x,y):_{+}\) being a lower-semi continuous function, is defined as:

\[OT(,):=_{(,)}(c),(c):=_{^{2}}c(x,y)\,d(x,y)\] (1)and where \((,)\) denotes the set of all joint probability measures on \(^{2}:=\) with marginals \(,\), i.e., \(_{1}:=_{1}\#=,_{2}:=_{2}\#=\), where \(_{1},_{2}:^{2}\) are the canonical projections \(_{1}(x,y):=x,_{2}(x,y):=y\). A minimizer for (1) always exists  and when \(c(x,y)=\|x-y\|^{p}\), for \(p 1\), it defines a metric on \(()\), which is referred to as the "\(p\)-Wasserstein distance":

\[W_{p}^{p}(,):=_{(,)}_{^{2}}\|x-y\|^{p }d(x,y).\] (2)

**The Partial Optimal Transport (POT) problem** extends the OT problem to the set of Radon measures \(_{+}()\), i.e., non-negative and finite measures. For \(>0\) and \(,_{+}()\), the POT problem is defined as:

\[POT(,;):=_{_{+}(^{2})}(c)+ (|-_{1}|+|-_{2}|),\] (3)

where, in general, \(||\) denotes the total variation norm of a measure \(\), i.e., \(||:=()\). The constraint \(_{+}(^{2})\) in (3) can be further restricted to \(_{}(,)\):

\[_{}(,):=\{_{+}(^{2}):\,_{1} ,_{2}\},\]

denoting \(_{1}\) if for any Borel set \(B\), \(_{1}(B)(B)\) (respectively, for \(_{2}\)) . Roughly speaking, the linear penalization indicates that if the classical transportation cost exceeds \(2\), it is better to create/destroy' mass (see  for further details).

**The relationship between POT and OT.** By using the techniques in , the POT problem can be transferred into an OT problem, and thus, OT solvers (e.g., network simplex) can be employed to solve the POT problem.

**Proposition 2.1**.: _[_12, 40_]_ _Given \(,_{+}()\), construct the following measures on \(:=\{\}\), for an auxiliary point \(\):_

\[=+||_{}=+| |_{}.\] (4)

_Consider the following OT problem_

\[(,)=_{(,)}(),(x,y):= c(x,y)-2&x,y,\\ 0&.\] (5)

_Then, there exists a bijection \(F:_{}(,)(,)\) given by_

\[F():=+(-_{1})_{}+_{}(-_{2})+||_{,}.\] (6)

_such that \(\) is optimal for the POT problem (3) if and only if \(F()\) is optimal for the OT problem (5)._

It is worth noting that instead of considering the same underlying space \(\) for both measures \(\) and \(\), the OT and POT problems can be formulated in the scenario where \(\) and \(\) are defined on different metric spaces \(X\) and \(Y\), respectively. In this setting, one needs a cost function \(c:X Y_{+}\) to formulate the OT and POT problems. However, in practice it is usually difficult to define reasonable 'distance' or _ground cost_\(c(,)\) between the two spaces \(X\) and \(Y\). In particular, the \(p\)-Wasserstein distance cannot be adopted if \(,\) are defined on different spaces. To relax this requirement, in the next section, we will review the fundamentals of the _Gromov-Wasserstein_ problem .

### The Gromov-Wasserstein (GW) Problem

A metric measure space (mm-space) consists of a set \(X\) endowed with a metric structure, that is, a notion of distance \(d_{X}\) between its elements, and equipped with a Borel measure \(\). As in [8, Ch. 5], we will assume that \(X\) is compact and that \(()=X\). Given two probability mm-spaces \(=(X,d_{X},)\), \(=(Y,d_{Y},)\), with \((X)\) and \((Y)\), and a non-negative lower semi-continuous cost function \(L:^{2}_{+}\) (e.g., the Euclidean distance or the KL-loss), the Gromov-Wasserstein (GW) matching problem is defined as:

\[GW^{L}(,):=_{(,)}^{ 2 }(L(d_{X}(,),d_{Y}(,))),\] (7)

where, for brevity, we employ the notation \(^{ 2}\) for the product measure \(d^{ 2}((x,y),(x^{},y^{}))=d(x,y)d(x^{}, y^{})\). If \(L(a,b)=|a-b|^{p}\), for \(1 p<\), we denote \(GW^{L}(,)\) simply by \(GW^{p}(,)\). In this case, the expression (7) defines an equivalence relation \(\) among probability mm-spaces, i.e.,\(\) if and only if \(GW^{p}(,)=0\)1. A minimizer of the GW problem (7) always exists, and thus, we can replace \(\) by \(\). Moreover, similar to OT, the above GW problem defines a distance for probability mm-spaces after taking the quotient under \(\). For details, we refer to [8, Ch. 5 and 10].

## 3 The Partial Gromov-Wasserstein (PGW) Problem

The Unbalanced Gromov-Wasserstein (UGW) problem for general (compact) mm-spaces \(=(X,d_{X},),=(Y,d_{Y},)\), with \(_{+}(X),_{+}(Y)\), studied in  is defined as:

\[UGW_{}^{L}(,):=_{_{+}(X  Y)}^{ 2}(L(d_{X},d_{Y}))+(D_{}(_{1}^{  2}^{ 2})+D_{}(_{2}^{ 2}^{  2})),\] (8)

where \(>0\) is a fixed linear penalization parameter, and \(D_{}\) is a Csiszar or \(\)-divergence. The above formulation extends the classical GW problem (7) into the unbalanced setting (\(\) and \(\) are no longer necessarily probability measures but general Radon measures).

We underline two points: First, as discussed in , while the above quantity allows us to 'compare' the mm-spaces \(\) and \(\), its _metric_ property is unclear. Secondly, when \(D_{}\) is the KL divergence, a Sinkhorn solver has been proposed in . However, a solver for general \(\)-divergences has not yet been proposed.

In this paper, we will analyze the case when \(D_{}\) is the total variation norm. Specifically, for \(q 1\), we consider the following problem, which we refer to as the _Partial Gromov-Wasserstein_ (PGW) problem:

\[PGW_{,q}^{L}(,):=_{_{+}(X  Y)}^{ 2}(L(d_{X}^{q},d_{Y}^{q}))+(|^{ 2}- _{1}^{ 2}|+|^{ 2}-_{2}^{ 2}|).\] (9)

**Remark 3.1**.: _Given \((,)\), the above cost functional can be rewritten as \(^{ 2}(L(d_{X}^{q},d_{Y}^{q}))+(|^{ 2}-_{1}^{  2}|+|^{ 2}-_{2}^{ 2}|)=^{ 2} (L(d_{X}^{q},d_{Y}^{q})-2)+ +||^{2})}_{}.\)_

**Proposition 3.2**.: _Given mm-spaces \(=(X,d_{X},),=(Y,d_{Y},)\), the minimization problem (9) can be restricted to the set \(_{}(,)=\{_{+}(X Y):\,_{1} ,_{2}\}\). That is,_

\[PGW_{,q}^{L}(,)=_{_{}(, )}^{ 2}(L(d_{X}^{q},d_{Y}^{q})-2)+(||^{2} +||^{2}).\] (10)

For the proof, inspired by , we direct the reader to Appendix B.

We notice that a similar Partial Gromov-Wasserstein problem (and its solver) has been studied . Indeed, in , the \(\)-penalization in the optimization problem (10) is avoided, but the constraint set is replaced by the subset of all \(_{}(,)\) such that \(||=\) for a fixed \([0,\{||,||\}]\). We will call this formulation the _Mass-Constrained Partial Gromov-Wasserstein_ (MPGW) problem. In Appendix L, we explore the relations between PGW and MPGW, and in Section 5 and Appendices N, O, P, we analyze the performance of the different solvers through different experiments.

**Proposition 3.3**.: _If \(L(r_{1},r_{2})=|r_{1}-r_{2}|^{p}\), for \(p[1,)\), we use \(PGW_{,q}^{p}\) to denote \(PGW_{,q}^{L}\). In this case, (9) and (10) admit a minimizer._

The proof is given in Appendix C: Its idea extends results from  from probability mm-spaces to arbitrary mm-spaces.

Next, we state one of our main results: The PGW problem gives rise to a metric between mm-spaces. The rigorous statement as well as its proof is given in Appendix D.

**Proposition 3.4**.: _Let \(>0\), \(1 q,p<\) and \(L(r_{1},r_{2})=|r_{1}-r_{2}|^{p}\). Then \((PGW_{,q}^{p}(,))^{1/p}\) defines a metric between mm-spaces._

Finally, for consistency, we provide the following result when the penalization tends to infinity. Its proof is given in Appendix E.

**Proposition 3.5**.: _Consider probability mm-spaces \(=(X,d_{X},)\), \(=(Y,d_{Y},)\), that is, \(||=||=1\). Assume that \(L\) is a continuous funtion. Then \(_{}PGW_{,1}^{L}(,)=GW^{L}( ,)\)._

## 4 Computation of the Partial GW Distance

In the discrete setting, consider mm-spaces \(=(X,d_{X},_{i=1}^{n}p_{i}^{X}_{_{i}})\), \(=(Y,d_{Y},_{j=1}^{m}q_{j}^{Y}_{y_{j}})\), where \(X=\{x_{1},,x_{n}\}\), \(Y=\{y_{1},,y_{m}\}\), the weights \(p_{i}^{X}\), \(q_{j}^{Y}\) are non-negative numbers, and the distances \(d_{X}\), \(d_{Y}\) are determined by the matrices \(C^{X}^{n n}\), \(C^{Y}^{m m}\) defined by

\[C^{X}_{i,i^{}}:=d_{X}^{q}(x_{i},x_{i^{}}) i,i^{} [1:n] C^{Y}_{j,j^{}}:=d_{Y}^{q}(y_{j},y_{j^{ }}) j,j^{}[1:m].\] (11)

Let \(:=[q_{1}^{X},,q_{n}^{X}]^{}\) and \(:=[q_{1}^{Y},,q_{m}^{Y}]^{}\) denote the weight vectors corresponding to the given discrete measures. We view the sets of transportation plans \((,)\) and \(_{}(,)\) for the GW and PGW problems, respectively, as the subsets of \(n m\) matrices

\[(,):=\{^{n m} :\, 1_{m}=,^{}1_{n}=\},\,| |=_{i=1}^{n}p_{i}^{X}=1=_{j=1}^{m}q_{j}^{Y}=||;\] (12)

\[_{}(,):=\{^{n m}_{+} :\, 1_{m},^{}1_{n}\},\] (13)

for any pair of non-negative vectors \(^{n}_{+}\), \(^{m}_{+}\), where \(1_{n}\) is the vector with all ones in \(^{n}\) (resp. \(1_{m}\)), and \( 1_{m}\) means that component-wise the \(\) relation holds.

Given by a non-negative function \(L:^{n n}^{m m}_{+}\), he transportation cost \(M\) and the 'partial' transportation con \(\) are represented by the \(n m n m\) tensors:

\[M_{i,j,i^{},j^{}}=L(C^{X}_{i,i^{}},C^{Y}_{j,j^{}}) :=M-2:=M-2 1_{n,m,n,m},\] (14)

where \(1_{n,m,n,m}\) is the tensor with ones in all its entries. For each \(n m n m\) tensor \(M\) and each \(n m\) matrix \(\), we define tensor-matrix multiplication \(M^{n m}\) by

\[(M)_{ij}=_{i^{},j^{}}(M_{i,j,i^{},j^{} })_{i^{},j^{}}.\]

Then, the Partial GW problem in (10) can be written as

\[PGW^{L}_{}(,)=_{_{}( ,)}_{}()+(||^{2 }+||^{2}),\] (15)

\[_{}():=^{ 2}:=_{i,j,i^{ },j^{}}_{i,j^{},j^{}}_{i,j}_{i^ {},j^{}}=_{ij}()_{ij}_{ij}=: ,_{F},\] (16)

and \(,_{F}\) stands for the Frobenius dot product. The constant term \((||^{2}+||^{2})\) will be ignored in the rest of this paper since it does not depend on \(\).

### Frank-Wolfe for the PGW Problem - Solver 1

In this section, we discuss the Frank-Wolfe (FW) algorithm for the PGW problem (15). A second variant of the FW solver is provided in the Appendix G.

As a summary, in our proposed method, we address the discrete PGW problem (15), highlighting that the _direction-finding subproblem_ in the Frank-Wolfe (FW) algorithm is a POT problem for (15). Specifically, (15) is treated as a discrete POT problem in our Solver 1, where we apply Proposition 2.1 to solve a discrete OT problem.

For each iteration \(k\), the procedure is summarized in three steps detailed below.

The convergence analysis, detailed in Appendix K, applies the results from  to our context, showing that the FW algorithm achieves a stationary point at a rate of \((1/)\) for non-convex objectives with a Lipschitz continuous gradient in a convex and compact domain.

**Step 1. Computation of gradient and optimal direction.**

It is straightforward to verify that the gradient of the objective function (16) in (15) is given by

\[_{}()=2.\] (17)

The classical method to compute \(M\) is the following: First, convert \(M\) into an \((n m)(n m)\) matrix, denoted as \(v(M)\), and convert \(\) into an \((n m) 1\) vector \(v()\). Then, the computation of \(M\) is equivalent to the matrix multiplication \(v(M)v()\). The computational cost and therequired storage space are \((n^{2}m^{2})\). In certain conditions, the above computation can be reduced to \((n^{2}+m^{2})\). We refer to Appendices F and H for details.

Next, we aim to solve the following problem:

\[^{(k)^{}}_{_{}(, )}_{}(^{(k)}),_{F},\]

which is a discrete POT problem since it is equivalent to

\[_{_{}(,)} 2M^{(k)}, _{F}+|^{(k)}|(||+||-2||).\]

The solver can be obtained by firstly converting the POT problem into an OT problem via Proposition 2.1 and then solving the proposed OT problem.

**Step 2: Line search method.**

In this step, at the \(k\)-th iteration, we need to determine the optimal step size:

\[^{(k)}=_{}\{_{}((1-) ^{(k)}+^{(k)^{}})\}.\]

The optimal \(^{(k)}\) takes the following values (see Appendix I for details):

\[^{(k)}=0&a 0,a+b>0,\\ 1&a 0,a+b 0,\{& ^{(k)}=^{(k)^{}}-^{(k)},\\ &a=^{(k)},^{(k)}_{F}\\ &b=2^{(k)},^{(k)}_{F}. .,\] (18)

and \((,)=\{\{-,0\},1\}\).

**Step 3: Update \(^{(k+1)}(1-^{(k)})^{(k)}+^{(k)}^{(k) ^{}}\).**

### Numerical Implementation Details

**The initial guess, \(^{(1)}\).** In the GW problem, the initial guess is simply set to \(^{(1)}=^{}\) if there is no prior knowledge. In PGW, however, as \(,\) may not necessarily be probability measures (i.e., \(_{i}p_{i}^{X},_{j}q_{j}^{Y} 1\) in general), we set \(^{(1)}=^{}}{(||,||)}\). It is straightforward to verify that \(^{(1)}_{}(,)\) as

\[^{(1)}1_{m}=|}{(||,||)},\ \ ^{(1)}1_{n}=|}{(||,||)}.\]

**Column/Row-Reduction.** According to the interpretation of the penalty weight parameter in the Partial OT problem (e.g. see Lemma 3.2 in ), during the POT solving step, for each \(i[1:n]\) (or \(j[1:m]\)), if the \(i^{th}\) row (\(j^{th}\) column) of \(^{(k)}\) contains a non-negative entry, all the mass of \(p_{i}^{X}\) (\(q_{j}^{Y}\)) will be destroyed (created). Thus, we can remove the corresponding row (column) to improve the computational efficiency.

Experiments

In addition to the three experiments detailed here, we also perform a wall-clock time comparison of our proposed PGW solvers in Appendix O and a positive-unlabeled (PU) learning experiment in Appendix P.

### Toy Example: Shape Matching with Outliers

We use the moon dataset and synthetic 2D/3D spherical data in this experiment. Let \(\{x_{i}\}_{i=1}^{n},\{y_{j}\}_{j=1}^{n}\) denote the source and target point clouds. In addition, we add \( n\) (where \(=20\%\)) outliers to the target point cloud. See Figure 1 for visualization.

We visualize the transportation plans given by the GW , MPGW , UGW , and our proposed PGW problems. For MPGW, UGW, and PGW, we set the mass to be 1 for each point in the source and target point clouds. For GW, we normalize the mass of these points so that the source and target have the same total mass. From Figure 1, we observe that PGW and MPGW induce a one-by-one relation in both cases and no outlier points are matched to the source point cloud. Meanwhile, GW matches all of the outliers. For UGW, as it applies the Sinkhorn algorithm, we observe mass-splitting transportation plans in both cases. Moreover, we observe that some mass from the outliers has been matched, which is not desired.

### Shape Retrieval

**Experiment setup.** We now employ the PGW distance to distinguish between 2D shapes, as done in , and use GW, MPGW, and UGW as baselines for comparison. Given a series of 2D shapes, we represent the shapes as mm-spaces \(^{i}=(^{2},\|\|_{2},^{i})\), where \(^{i}=_{k=1}^{n^{i}}^{i}_{x_{k}^{i}}\). For the GW method, we normalize the mass for the balanced mass constraint setting (i.e. \(^{i}=}\)), and for the remaining methods we let \(^{i}=\) for all the shapes, where \(>0\) is a fixed constant. In this manner, we compute the pairwise distances between the shapes.

We then use the computed distances for nearest neighbor classification. We do this by choosing a representative at random from each class in the dataset and then classifying each shape according to its nearest representative. This is repeated over 10,000 iterations, and we generate a confusion matrix for each distance used. Finally, using the approach given by , we combine each distance with a support vector machine (SVM), applying stratified 10-fold cross validation. In each iteration of cross validation, we train an SVM using \((- D)\) as the kernel, where \(D\) is the matrix of pairwise distances (w.r.t. one of the considered distances) restricted to 9 folds, and compute the accuracy of the model on the remaining fold. We report the accuracy averaged over all 10 folds for each model.

**Dataset setup.** We test two datasets in this experiment, which we refer to as Dataset I and Dataset II. We construct Dataset I by adapting the 2D shape dataset given in , consisting of 20 shapes in

Figure 1: The set of red points comprises the source point cloud. The union of the dark blue (outliers) and light blue points comprises the target point cloud. For UGW, MPGW, and PGW, we set the mass for each point to be the same. For GW, we normalize the mass for the balanced mass constraint setting.

each of the classes bone, goblet, star, and horseshoe. For each class, we augment the dataset with an additional class by selecting either a subset of points from each shape of that class (rectangle/bone, trapezoid/goblet, disk/star) or adding additional points to each shape of that class (annulus/horseshoe). Hence, the final dataset consists of 160 shapes across 8 total classes. This dataset is visualized in Figure 5(a).

For Dataset II, we generate 20 shapes for each of the classes rectangle, house, arrow, double arrow, semicircle, and circle. These shapes were generated in pairs, such that each shape of class rectangle is a subset of the corresponding shape of class house, and similarly for arrow/double arrow and semicircle/circle. This dataset is visualized in Figure 5(b).

**Performance analysis**. We refer to Appendix N for full numerical details, parameter settings, and the visualization of the resulting confusion matrices. We visualize the two considered datasets and the resulting pairwise distance matrices in Figure 2. For the SVM experiments, GW achieves the highest accuracy on Dataset I, 98.13%, while the second best method is PGW, 96.25%. For Dataset II, PGW achieves the highest accuracy, correctly classifying 100% of the samples. The complete set of accuracies for all considered distances on each dataset is reported in Table 0(a).

In addition, we report the wall-clock time required to compute all pairwise distances for each distance in Table 0(b). We observe that GW, MPGW, and PGW have similar wall-clock times across both experiments (30-50 seconds for Dataset I, 80-140 seconds for Dataset II), with PGW admitting a slightly faster runtime in both cases. Meanwhile, UGW requires almost 1500 seconds on the experiment with Dataset I and over 500 seconds on the experiment with Dataset II.

### Partial Gromov-Wasserstein Barycenter and Shape Interpolation

By , Gromov-Wasserstein can be applied to interpolate two shapes via the concept of _Gromov-Wasserstein Barycenters_. In this paper, we introduce _Partial Gromov-Wasserstein Barycenters_ by extending the GW Barycenter to the setting of PGW as follows.

(a) Mean accuracy of SVM using each distance in kernel.

(b) Wall-clock time comparison.

Figure 2: In each row, the first figure visualizes an example shape from each class, and the second figure visualizes the resulting pairwise distance matrices. The first row corresponds to Dataset I and the second corresponds to Dataset II.

Consider the discrete mm-spaces \(^{1},,^{K}\), where \(^{k}=(X^{k},\|\|_{^{d_{k}}},_{i=1}^{n_{k}}p_{i}^{k} _{x_{i}^{k}})\), with \(X^{k}=\{x_{i}^{k}\}_{i=1}^{n_{k}}^{d_{k}}\). We denote \(C^{k}=[\|x_{i}^{k}-x_{i}^{k}\|^{2}]_{i,i^{}}\) and \(^{k}=[p_{1}^{k},,p_{n_{k}}^{k}]\). Given positive constants \(_{1},,_{K}>0\), the PGW Barycenter is defined by:

\[_{C,_{k}}_{k}_{k} M(C,C^{k})^{k},^{k }-2_{k}|^{k}|^{2}\] (19)

where each \(^{k}_{}(,^{k})\). We refer to Appendix M for the solver of (19) and details.

**Experiment setup.** We apply the PGW barycenter to the following problem: Given two shapes \(X=\{x_{i}\}_{i=1}^{n}^{d_{1}}\) and \(Y=\{y_{i}\}_{i=1}^{m}^{d_{2}}\), modeled as mm-spaces \(=(X,\|\|_{^{d_{1}}},_{i=1}^{n}_{x_{i}})\) and \(=(Y,\|\|_{^{d_{2}}},_{i=1}^{m}_{y_{i}})\), we wish to find interpolations between them. In addition, we assume \(\) is corrupted by noise, i.e., \(\) is redefined as \(=(,\|\|_{^{d_{2}}},_{i=1}^{m}_{y_ {i}}+_{i=1}^{m}_{_{i}})\) with \(=Y\{_{i}\}_{i=1}^{m}\), where \(\) is the noise level and each \(_{i}\) is randomly selected from a particular region \(^{d_{2}}\).

**Dataset setup.** We adapt the dataset given in . See Appendix M.1 for further details on the dataset. In this experiment, we test \(=5\%,10\%\). We visualize the barycenter interpolation from \(t=0/7\) to \(t=7/7\), where \((1-t),t\) are the weight of the source \(\) and the target \(\), respectively, in the barycenter (19). The visualization given in Figure 3 is obtained by applying SMACOF MDS (multidimensional scaling) of the minimizer \(C\).

**Performance analysis**. From Figure 3, we observe that in this two scenarios, the interpolation derived from GW is clearly disturbed by the noise data points. For example, in rows \(1,3\), columns \(t=1/7,2/7,3/7\), we see that the point clouds reconstructed by MDS have significantly different width-height ratios from those of the source and target point clouds. In contrast, PGW is significantly less disturbed, and the interpolation is more natural. The width-height ratio of the point clouds generated by the PGW barycenter is consistent with that of the source/target point clouds.

## 6 Summary

In this paper, we propose the Partial Gromov-Wasserstein (PGW) problem and introduce two Frank-Wolfe solvers for it. As a byproduct, we provide pertinent theoretical results, including the relation between PGW and GW, the metric property of PGW, and the PGW barycenter. Furthermore, we demonstrate the efficacy of the PGW solver in solving shape-matching, shape retrieval, and shape interpolation tasks. For the shape retrieval experiment, we observe that due to the metric property, PGW and GW have similar accuracy and outperform the other methods evaluated. In the shape matching and point cloud interpolation experiments, we demonstrate PGW admits a more robust result when the data are corrupted by outliers/noisy data.

Figure 3: In the first column, the first and second figures are the source and target point clouds in the first experiment (\(=5\%\)); the third and fourth figures are the source and target point clouds in the second experiment (\(=10\%\)).