# VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding

Houlun Chen\({}^{1}\), Xin Wang\({}^{1,2}\), Hong Chen\({}^{1}\), Zeyang Zhang\({}^{1}\)

Wei Feng\({}^{1}\), Bin Huang\({}^{1}\), Jia Jia\({}^{1,2}\)1, Wenwu Zhu\({}^{1,2}\)1

\({}^{1}\) Department of Computer Science and Technology, Tsinghua University, Beijing, China

\({}^{2}\) BNRIST, Tsinghua University, Beijing, China

{chenhl23,h-chen20,zy-zhang20,fw22,huangb23}@mails.tsinghua.edu.cn

{xin_wang,jjia,wwzhu}@tsinghua.edu.cn

https://verified-neurips.github.io

###### Abstract

Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic VidEo-text annotation pipeline to generate captions with Rellable FlnE-grained statics and Dynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in https://github.com/hlchen23/VERIFIED.

## 1 Introduction

Video Corpus Moment Retrieval (VCMR)  aims to retrieve a video moment from a large untrimmed video corpus given a text query. It requires handling two subtasks: Video Retrieval (VR)  from a corpus and Single Video Moment Retrieval (SVMR) [3; 4] within a video, which involves grasping multi-level semantic granularities across video-text and moment-text alignment. However, as shown in Figure 1(a), in the previous VCMR setting, the queries are usually coarse-grained and thus struggle to localize a video moment discriminatively, where there exists potentially relevant positive pairs [5; 6; 7] besides the ground truth, which hinders cross-modal retrieval and makes it hard for models to learn distinctive video features.

To address the problem, we propose a more challenging VCMR scenario in this paper. As shown in Fig 1(b), a fine-grained distinctive query is provided to retrieve the best-matched moment, requiring models to precisely understand the details in text descriptions and distinguish the target moments frompartially matched candidates. However, annotating such fine-grained video-text datasets  relies on intensive manual work and domain knowledge, limiting its productivity and scalability. Therefore, we resort to the power of recent large language model (LLM) and large multimodal model (LMM)  for automatic detailed video-clip annotation. Simply relying on the LLMs/LMMs for annotation faces the following two challenges: 1) how to extract as much fine-grained information from videos as possible remains unexplored, especially dynamic video details; 2) LLM or LMM are known to struggle with the hallucination problem, how to avoid the impact of the generated inaccurate content is also challenging.

To tackle these challenges, we propose **VERIFIED**, an automatic VidEo-text annotation pipeline to generate captions with Reflable FlnE-grained statics and Dynamics. To fully utilize fine-grained visual content, we design the Statics and Dynamics Enhanced Captioning modules. Specifically, for statics, we extract foreground and background attributes with image LMM and form several statics enhanced caption candidates via LLM rewriting. For dynamics, we propose a VQA-guided dynamic detail discovering method, which guides the video LMM to focus more on dynamic changes in the video, before having LLM rewrite dynamics enhanced captions. To alleviate the impact of the inaccurate annotations caused by LLM/LMM hallucinations, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model  with disturbed hard-negative data through contrastive and matching losses, so that it can better discriminate the unreasonable annotations. We apply it to evaluate each generated video-text pair, which helps to filter out inaccurate annotations.

We construct our benchmark based on the widely adopted VCMR datasets with our VERIFIED pipeline, including Charades-STA , DiDeMo , and ActivityNet Captions . As shown in

Figure 1: (a) Previous VCMR, where a query may be coarse and there are many potential positive moments (**green**) that are not annotated, making the ground truth annotations unreasonable. (b) Our Challenging Fine-Grained VCMR, where a more fine-grained query is given and the method needs to retrieve the best matched one from partially matched candidates (pink). (c) Our VERIFIED pipeline generates fine-grained annotations with reliable static (**green**) and dynamic (**blue**) details.

Fig 1(c), we obtain fine-grained Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG to better support fine-grained VCMR, which demonstrate a high level of annotation quality. Compared to previous ones, our benchmark significantly reduces the many-to-many situations, offering more precise ground truth annotations. We evaluate several state-of-the-art VCMR models on our benchmark, and the results show that models trained on previous datasets show poor performance on the fine-grained VCMR task, while our proposed training dataset significantly improves its performance. We believe this benchmark will inspire a lot of future work for fine-grained video understanding in VCMR.

Our contributions can be summarized as follows:

1. We first define a more challenging fine-grained VCMR setting, which requires models to understand video fine-grained information precisely and learn distinctive video features.
2. We propose an automatic fine-grained video clip annotation pipeline, VERIFIED, aided by LLMs/LMMs, which fully captions fine-grained statics and dynamics in visual content, demonstrating high annotation quality.
3. We evaluate several state-of-the-art VCMR models on our benchmarks to analyze their ability to localize fine-grained queries among large video corpus, indicating several important challenges and future directions.

## 2 Related Works

**Video Annotation through Multimodal Models**. Most video-text datasets heavily rely on manual work and domain knowledge, especially for fine-grained details [8; 9; 10; 11], limiting their scalability, particularly in video moment datasets [3; 18; 4]. Others construct large-scale datasets via web crawling  or ASR , but suffer from noisy cross-modal alignment. With the rapid advancement of multimodal foundation models and LLM, automatically annotating large-scale video-text datasets is becoming feasible . InternVid  integrates image captioning models to caption video clips at multiple scales, while Panda-70M  uses multimodal teacher models to caption 70M text-annotated videos. MVid  automatically captions visual, audio, and speech with LLM refinement. However, they often lack fine-grained annotations, especially for dynamic details such as motions and interactions, or rely mainly on subtitles or auxiliary text labels . To address this, we propose VERIFIED to automatically capture fine-grained static and dynamic details from the vision modality with quality management.

**Video Corpus Moment Retrieval**. Video moment retrieval (VMR) [3; 4; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37] requires localizing a matched moment within an untrimmed video for a text query and video corpus moment retrieval (VCMR)  extends VMR to search the target moment from a large untrimmed video corpus, requiring appropriate integration of video retrieval and moment localization. Among VCMR methods, XML  introduces a convolutional start-end detector with its late fusion design. CONQUER  integrates query context into video representation learning for enhanced moment retrieval by two stages. ReLoCLNet  leverages video and frame contrastive learning through separate encoder alignment. As video corpora expand, many video moments share similar semantics with subtle differences in fine-grained statics and dynamics. While some VCMR works [5; 6; 7] have explored relevance within non-ground-truth moments and texts by pseudo-positive labels or relevance-based margin, they fail to learn distinctive differences among semantically analogous clips. To address this, we propose a fine-grained VCMR scenario that requires localizing the best-matched moment among partially matched candidates for a fine-grained text query and we introduce new datasets to benchmark state-of-the-art VCMR methods.

## 3 Dataset Construction Methodology: VERIFIED

In this section, we introduce our VERIFIED pipeline to annotate fine-grained captions for video moments with reliable static and dynamic details. The annotations in the previous video moment datasets are in the form of \((V,t_{s},t_{e},q)\) for a moment \(V[t_{s}:t_{e}]\) (designated as \(v\)) from \(t_{s}\) to \(t_{e}\) seconds in the video \(V\), where \(q\) is a moment-level text caption for moment \(v\). In this paper, our VERIFIED pipeline constructs novel fine-grained video moment datasets in the form of \((V,t_{s},t_{e},\{q_{i}^{(s)}\}_{i=1}^{N_{s}},\{c_{i}^{(s)}\}_{i=1}^{N_{s}},\{ q_{i}^{(d)}\}_{i=1}^{N_{d}},\{c_{i}^{(d)}\}_{i=1}^{N_{d}})\). We annotate the same video moment in the previous dataset with multiple diverse captions containing rich static and dynamic fine-grained

[MISSING_PAGE_FAIL:4]

\(\{_{i}\}_{i=1}^{N_{qa}}\) on the video moment according to the previous coarse caption.

\[\{_{i}\}_{i=1}^{N_{qa}}=(q)\] (3)

Afterward, we feed a strong video LMM sequential video frames and such questions to have the video LMM answer these questions \(\{_{i}\}_{i=1}^{N_{qa}}\) before generating a complete fine-grained description \(}\) of the dynamics of the video moment.

\[\{_{i}\}_{i=1}^{N_{qa}},}=(\{ _{i}\}_{i=1}^{N_{qa}},v,q)\] (4)

Finally, we prompt an LLM to extract important dynamic details to rephrase it to \(N_{d}\) fine-grained caption candidates \(\{q_{i}^{(d)}\}_{i=1}^{N_{d}}\).

\[\{q_{i}^{(d)}\}_{i=1}^{N_{d}}=(\{_{i}\}_{i=1}^{N_{qa}}, \{_{i}\}_{i=1}^{N_{qa}},},q)\] (5)

### Fine-Granularity Aware Noise Evaluator

Specifically, for a piece of original sample \((V,t_{s},t_{e},q)\), we prompt LLM to generate \(N_{pos}\) positively rewritten captions \(\{q_{i}^{+}\}_{i=1}^{N_{pos}}\), \(N_{neg}\) statics disturbed negative captions \(\{q_{i}^{s-}\}_{i=1}^{N_{neg}}\), and \(N_{neg}\) dynamics disturbed negative captions \(\{q_{i}^{d-}\}_{i=1}^{N_{neg}}\) for the previous coarse caption \(q\), respectively.

\[\{q_{i}^{+}\}_{i=1}^{N_{pos}},\{q_{i}^{s-}\}_{i=1}^{N_{neg}},\{q_{i}^{d-}\}_{ i=1}^{N_{neg}}=(q)\] (6)

where we prompt LLM to generate rewritten captions \(\{q_{i}^{+}\}_{i=1}^{N_{pos}}\) that share the same meanings as the previous coarse caption \(q\), \(\{q_{i}^{s-}\}_{i=1}^{N_{neg}}\) that have an significant difference in some static attributes from \(q\), and \(\{q_{i}^{d-}\}_{i=1}^{N_{neg}}\) that have an significant difference in some dynamic content from \(q\).

Since LLM sometimes fails to generate appropriate rewritten captions, _e.g._ some \(q_{i}^{d-}\) might share the same meaning as \(q\), we select the best one from the candidates. We adopt SentenceBERT  as a semantic distance measure on captions to discover the most positive caption \(^{+}\) and most negative static or dynamic one \(^{s/d-}\), where

\[^{+} =\{(\{q_{i}^{+}\}_{i=1}^{N_{pos}},q)\}\] (7) \[^{s/d-} =\{(\{q_{i}^{s/d-}\}_{i=1}^{N_{neg} },q)\}\] (8)

Afterward, we finetune UMT  in the video-text retrieval task with the hard-negatives augmented contrastive loss \(_{c}\) and matching loss \(_{m}\). The contrastive loss \(_{c}\) is

\[_{c}=-(_{i=1}^{B},q_{i})/)}}{ _{j=1}^{B}_{\{q_{j},_{j}^{s-},_{j}^{d-}\}} ,)/)}}+_{i=1}^{B},v_{i})/ )}}{_{j=1}^{B},v_{j})/)}})\] (9)

where \(B\) is the batch size and \(s\) is a similarity measure. The \(_{i}^{s/d-}\) is a hard negative sample for \(v_{i}\) and other \(_{j}^{s/d-}(j i)\) can be seen as trivial negative samples for \(v_{i}\). The matching loss \(_{m}\) is

\[_{m}=-_{i=1}^{B}[(c(v_{i},q_{i}))+_{ {q}(v_{i})},)))}+_{ (q_{i})},q_{i})))}]\] (10)

where \(c\) is a classifier, \(\) is the Sigmoid function, and \((v_{i})\), \((q_{i})\) contains the negative samples for \(v_{i}\) and \(q_{i}\). \((v_{i})\) contains sampled trivial negative sample \(q_{j}(j i)\) and the augmented hard-negatives \(_{i}^{s-}\) and \(_{i}^{d-}\) while \((q_{i})\) contains only sampled trivial negative sample \(v_{j}(j i)\).

To alleviate potential harmful biases in LLM-generated texts and avoid degenerating into distinguishing the human-written text from other LLM-generated ones, we replace the previous coarse caption \(q\) with the selected positively rewritten caption \(^{+}\) for fine-tuning as well with loss \(_{c}^{+}\) and \(_{m}^{+}\).

Finally, we combine all these items to derive the total loss function \(\).

\[=}{2}(_{c}+_{c}^{+})+}{2}(_{ m}+_{m}^{+})\] (11)

After fine-tuning, we evaluate the matching confidence scores between the video moment \(v\) and our generated fine-grained captions \(\{q_{i}^{(s)}\}_{i=1}^{N_{s}}\) and \(\{q_{i}^{(d)}\}_{i=1}^{N_{d}}\) as \(\{c_{i}^{(s)}\}_{i=1}^{N_{s}}\) and \(\{c_{i}^{(d)}\}_{i=1}^{N_{d}}\), with our evaluator. Captions with relatively lower scores are more likely to have inaccurate content.

### Implementation Details

For the models used in our VERIFIED pipeline, we use Mistral-7B-Instruct-v0.2  as our LLM, LLMaVA-1.6-Mistral-7B  as our image LMM, Gemini-1.5-Pro  as our video LMM, all-MiniLM-L6-v2  for SentenceBERT, and ViT-L/16_25M  version for pretrained UMT. Our VERIFIED automatically annotate video moments from DiDeMo , Charades-STA , and ActivityNet Captions , named DiDeMo-FIG, Charades-FIG, and ActivityNet-FIG respectively. In the Statics Enhanced Captioning, we select up to \(L=1\), \(L=1\) and \(L=5\) key frames for DiDeMo-FIG, Charades-FIG and ActivityNet-FIG. In the Dynamics Enhanced Captioning, we extract frames at 8fps for DiDeMo-FIG and Charades-FIG and uniformly sample 64 frames for ActivityNet-FIG for the video input of the video LMM, and we set the VQA pair numbers \(N_{qa}=5\). We generate \(N_{s}=N_{d}=3\) fine-grained statics or dynamics enhanced caption candidates for both modules. In the Fine-granularity Aware Noise Evaluator, we generate \(N_{pos}=N_{neg}=3\) positive/negative disturbed captions for each previous caption \(q\). During fine-tuning, we sample 20 frames for each video moment, and the temperature \(\) is 0.07 and \(_{c}\), \(_{m}\) is 1. We collect around 125K disturbed samples from all previous datasets for fine-tuning with a learning rate of 1e-5 and \(B=16\) batch size for 10 epochs in a 4 A100-80G machine. After all, we grade each video-text pair with our evaluator and choose the fine-grained enhanced caption with the highest score for each video moment. We follow the previous dataset splits. The prompts used for LLM/LMM are attached to the supplementary materials.

### Statistical Analysis and User Study

We present the statistics of our annotated datasets compared to the previous coarse ones in Tab 1, where our annotations feature a richer vocabulary and approximately twice the number of content words with various parts of speech, particularly adjectives, indicating that our fine-grained captions provide more detailed descriptions.

To show that our datasets reduce the many-to-many pairs, as shown in Tab 2, we report the number of classes (# cls) and instances (# inst) that involve many-to-many correspondence, indicating that our fine-grained captions largely solve the problem of a lack of distinctiveness in the previous annotations with precise video-text alignment. Counting details are in the supplementary materials.

To validate the accuracy of our fine-grained annotations, we conduct user studies where we randomly sample 50 statics enhanced and 50 dynamics enhanced captions for each fine-grained dataset and ask users to i) judge if our VERIFIED generated captions capture more fine-grained static or dynamic information than previous ones, and ii) grade them in 5 levels (1\(\) 5) to measure their accuracy. We calculate the ratio of our captions \(R_{s}\) or \(R_{d}\) that users acknowledge to provide a richer array of static or dynamic details and report the average scores \(S\) that measure accuracy. Tab 3 shows statistics, indicating that our annotations effectively extract richer, fine-grained content, with over 80% being recognized by users for their static or dynamic details. The average accuracy score of 4.46 indicates

    &  &  &  &  &  \\   & COG & FIG & COG & FIG & COG & FIG & COG & FIG & COG & FIG \\  Charades-FIG & 997 & 2590 & 6.21 & 15.38 & 2.33 & 4.91 & 1.18 & 2.20 & 0.04 & 1.17 \\ DiDeMo-FIG & 5586 & 8595 & 7.50 & 16.08 & 2.54 & 5.22 & 1.11 & 2.14 & 0.57 & 1.60 \\ ActivityNet-FIG & 10203 & 14769 & 13.17 & 26.19 & 3.67 & 8.68 & 2.21 & 3.16 & 0.60 & 3.01 \\   

* COG and FIG are short of “coarse-grained” and “fine-grained”. # Vocab is the vocabulary size of all annotations and # Word, # Noun, # Verb, and # Adj are the average number of words, nouns, verbs, and adjectives for each caption.

Table 1: Statistics of our annotated fine-grained datasets (FIG) compared to the previous ones (COG).

    &  &  \\   & \# cls & \# inst & \# cls & \# inst \\  Charades-FIG & 1393 & 8805 & 194 & 422 \\ DiDeMo-FIG & 703 & 1925 & 32 & 65 \\ ActivityNet-FIG & 505 & 1691 & 3 & 6 \\   

    &  &  &  \\   & \(R_{s}\)(\%) & \(S\) & \(R_{d}\)(\%) & \(S\) & \(S\) \\  Charades-FIG & 100 & 4.76 & 80 & 4.38 & 4.57 \\ DiDeMo-FIG & 88 & 4.28 & 88 & 4.44 & 4.36 \\ ActivityNet-FIG & 100 & 4.44 & 84 & 4.44 & 4.44 \\   

Table 2: Many-to-many pair statistics.

high acceptance by human evaluators after filtering out inaccuracies. User study instructional texts are attached to supplementary materials.

### Annotation Visualization

We show representative visualized samples. As shown in Fig 3, our VERIFIED pipeline reliably captures fine-grained statics and dynamics. Furthermore, annotations with inaccurate content are allocated a lower confidence score, intuitively demonstrating the effectiveness of our fine-granularity aware noise evaluator. In Fig 4, we show several successful cases, showing advantages in recognizing interaction relationships, subtle motion details, textual information, and multiple activities.

Figure 4: Visualization of impressive cases. (1) Our annotation captures the interaction between the dog and its handler and movement trajectory. (2) Our annotation captures details of the throwing objects and conveys that the man throws them many times. (3) Our annotation reads the textual information from visual content and expresses the correct order of used ingredients.

Figure 3: Visualization of the effectiveness of our VERIFIED pipeline. (1-3) are selected from fine-grained ActivityNet-FIG, Charades-FIG, and DiDeMo-FIG, respectively. The fine-grained static and dynamic content is marked in **green** and **blue**, and inaccurate content is marked in **red**.

[MISSING_PAGE_FAIL:8]

HERO utilizes a straightforward Temporal Transformer to capture correlations in video features; however, this architecture lacks the capacity to capture fine-grained relationships within videos effectively. In contrast, XML and ReLoCLNet benefit from more complicated cross-modal fusion modules, and ReLoCLNet incorporates 4 learning tasks at different granularities, which likely contributes to their superior performance.

Among the models tested, CONQUER consistently achieves strong performance across all tasks, highlighting the effectiveness of two-stage methods for VCMR. While SQuiDNet achieves the highest accuracy in VR, likely due to continued video-level learning in its second stage, it exhibits unstable performance in VCMR and lower accuracy in SVMR. Based on these observations, we recommend avoiding the entanglement of video-level and moment-level learning during the training phase in fine-grained settings. Incorporating finer-grained information during video-level retrieval learning may interfere with precise moment localization, compromising performance.

## 5 VERIFIED Pipeline Evaluation

We first explore how important our fine-grained training data is for understanding video details, to show the significance of our whole pipeline. In Tab 6, we train XML with previous coarse-grained or our fine-grained data and evaluate its performance in the fine-grained scenario. Results indicate that the impact of training with fine-grained annotations on fine-grained SVMR is relatively minor; however, it significantly enhances the performance of fine-grained VR and VCMR. This improvement is due to the fact that while fine-grained details are often redundant within a single video, they are essential for accurately pinpointing unique moments across a vast collection of similar clips. Models trained with previous coarse annotations struggle to generalize to the fine-grained scenario, especially in a large video corpus, indicating the necessity of our insight to introduce fine-grained datasets. Besides, the VCMR and VR performances on Charades-FIG are quite suboptimal, since all videos in Charades are about in-door activities and share similar semantics, making it the most challenging benchmark to evaluate methods' capability of perceiving fine-grained video differences.

We show the visualization of XML in Charades-FIG when training on different granularities of training data in Fig 5. In Fig 5(b), when training on COG data, the ground truth video is out of the top 100 in its moment rank list. The top-ranked predictions mainly cover the laptop and omit other

    &  &  &  \\   & 0.5/r5 & 0.5/t100 & 0.7/r5 & 0.7/r100 & r1 & r5 & r10 & r100 & 0.5/r1 & 0.5/r5 & 0.7/r1 & 0.7/r5 \\   \\  COG & 0.89 & 3.87 & 0.43 & 2.37 & 0.62 & 2.85 & 5.75 & 30.11 & 24.73 & 57.82 & 11.80 & 32.04 \\ FIG & 2.63 & 9.87 & 1.29 & 5.56 & 2.80 & 8.95 & 14.11 & 51.72 & 28.20 & 61.45 & 12.90 & 34.35 \\   \\  COG & 6.08 & 27.81 & 4.73 & 22.35 & 8.70 & 26.84 & 38.50 & 80.89 & 17.92 & 57.14 & 11.94 & 46.05 \\ FIG & 9.64 & 40.29 & 7.20 & 33.04 & 14.83 & 40.39 & 53.95 & 91.53 & 20.53 & 60.25 & 15.23 & 49.61 \\   \\  COG & 4.41 & 17.41 & 2.48 & 10.23 & 6.70 & 21.73 & 33.63 & 80.37 & 24.31 & 60.11 & 13.04 & 37.15 \\ FIG & 7.86 & 26.28 & 4.58 & 15.24 & 13.46 & 36.37 & 49.99 & 89.31 & 25.23 & 63.19 & 13.60 & 37.81 \\   

Table 6: XML results with different granularities of training data.

    & 0.5/r1 & 0.5/r5 & 0.5/r10 & 0.5/r100 & 0.7/r1 & 0.7/r5 & 0.7/r10 & 0.7/r100 \\   \\  Lowest score & 0.67 & 1.75 & 2.85 & 8.47 & 0.32 & 0.97 & 1.64 & 4.49 \\ Highest score & 1.05 & 2.63 & 4.33 & 9.87 & 0.43 & 1.29 & 2.26 & 5.56 \\   \\  Lowest score & 2.04 & 6.30 & 10.86 & 32.67 & 1.27 & 4.46 & 8.15 & 26.31 \\ Highest score & 3.19 & 9.64 & 14.05 & 40.29 & 2.32 & 7.20 & 10.69 & 33.04 \\   

Table 7: Ablation on our evaluator module using XML in VCMR task.

details. In Fig 5(c), it achieves much better performance with our fine-grained data. It retrieves the target moment in rank 5, and the other candidates behind are also highly partially related to the query. It showcases the challenge of our fine-grained VCMR setting and the effectiveness of our VERIFIED generated annotations for training.

We further analyze the modules of our VERIFIED pipeline using XML  on the Charades-FIG and DiDeMo-FIG datasets in the VCMR task. To demonstrate the effectiveness of our evaluator, we select the caption with the highest or lowest score for each video moment to train the VCMR model. As shown in Tab 7, performance drops significantly when selecting captions with the lowest confidence scores, indicating that our evaluator can recognize better training data.

## 6 Conclusion and Future Work

This paper discovers that existing VCMR benchmarks' focus on coarse-grained understanding limits methods' ability to learn distinct video features and perceive fine-grained differences between video moments. Thus, we propose a more challenging fine-grained VCMR benchmark, requiring models to retrieve the best-matched moment from a video corpus given a fine-grained query, with other partially matched candidates present. To ensure efficient and high-quality video annotations, we introduce VERIFIED, an automatic video-text annotation pipeline that uses LLM and LMM to generate detailed statics and dynamics enhanced captions and filters out inaccuracies through our Fine-Granularity Aware Noise Evaluator. This evaluator is obtained via fine-tuning UMT with disturbed hard-negatives augmented contrastive and matching losses. We create the Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG datasets with high-quality annotations to support fine-grained VCMR. Benchmarking state-of-the-art VCMR methods reveals that those trained on coarse annotations struggle to generalize to fine-grained scenarios, highlighting the necessity for improved fine-grained video understanding in VCMR. In the future, the next goal might be to train a completely end-to-end captioning model to complete the fine-grained annotations with the capabilities of the complicated pipeline that combines many powerful existing models. The disturbed hard negative data enhances the ability of our evaluator to understand details. However, the gap between the captioning modules' real hallucinations and our perturbation approximation does exist and it would require more analysis to reduce this gap.

Figure 5: XML’s predictions in Charades-FIG with different granularities of training data.