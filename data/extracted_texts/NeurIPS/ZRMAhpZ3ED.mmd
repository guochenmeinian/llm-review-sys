# WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control

Claire Bizon Monroc

Inria and DI ENS, Ecole Normale Superieure, PSL Research University, Paris, France

IFP Energies nouvelles

claire.bizon-monroc@inria.fr

&Ana Busic

Inria and DI ENS, Ecole Normale Superieure, PSL Research University

Paris, France

&Donatien Dubuc

IFP Energies nouvelles

Solaize, France

&Jiamin Zhu

IFP Energies nouvelles

Rueil-Malmaison, France

###### Abstract

The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (FAST.Farm). For each simulator, \(10\) wind layouts are provided, including \(5\) real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to FAST.Farm.

## 1 Introduction

The development of wind energy plays a crucial part in the global transition away from fossil energies, and it is driven by the deployment of very large offshore wind farms . Significant gains in wind energy production can be made by increasing the amount of wind power captured by the farms . The power production of a wind farm is greatly influenced by wake effects: an operating upstream turbine causes a decrease in wind velocity and an increase in wind turbulence behind its rotor, which creates sub-optimal wind conditions for other wind turbines downstream. An illustration of this phenomenon can be seen on Figure 1. Wake effects are a major cause of power loss in wind farms, with the decrease in power output estimated to be between \(10\%\) and \(20\%\) in large offshorewind farms . Higher turbulence in wakes also increases fatigue load on the downstream turbines by \(5\%\) to \(15\%\), which can shorten their lifespans .

The wind farm control problem is challenging. Conventional model-based control strategies require tractable models of complex dynamic interactions between turbines, and suffer from the curse of dimensionality when the number of turbines increases. Moreover, optimal strategies differ significantly with modeling choices. Reinforcement Learning (RL) provides a model-free, data-based alternative, and recent work applying RL algorithms to wind farm control has yielded promising results (see e.g. ). Single agent approaches, where a single RL controller must learn a centralized policy, encounter scaling challenges , are slow to converge under dynamic conditions  and do not explore the graph structure of the problem induced by local perturbations. Several multi-agent RL approaches have been proposed to tackle this issue, relying on both centralized critics [9; 11; 31] and independent learning approaches [5; 22; 39]. Authors have published code relative to their specific applications [46; 45; 29], and  proposes a single-agent RL environment for power maximization in static simulations. There is to the best of our knowledge no open-source reinforcement learning environment for the general wind farm control problem.

In this article, we propose WFCRL, the first open suite of reinforcement learning environments for the wind farm control problem. WFCRL is highly customizable, allowing researchers to design and run their own environments for both centralized and multi-agent RL.

Wind turbines can be controlled in several ways. A turbine can adjust its _yaw_ (defined as the angle between the rotor and the wind direction) to deflect its wake, increase its _pitch_ (the angle between the turbine blades and the incoming wind) to decrease its wind energy production, or directly control the _torque_ of its rotor. WFCRL makes it possible to control yaw, pitch or torque, and a schema of these different control variables can be found in Figure 1. WFCRL offers a large set of observations including local wind statistics, power production, and fatigue loads for each turbine. This makes it possible to consider different objective, including the maximization of the total production, the minimization of loads to reduce maintenance costs over the wind turbine life-cycle , or, as wind energy becomes a larger part of the energy mix, the tracking of power or frequency targets that allows operators to offer ancillary services for grid integration .

In WFCRL, interfaces with two state-of-the-art farm simulators are implemented : a static simulator FLORIS  and a dynamic simulator FAST.Farm . Indeed, the choice of a static or dynamic model is particularly important: the overwhelming majority of proposed approaches are evaluated on static models, but it was shown in  that successful learning approaches under static conditions generally do not adapt to dynamic ones. However, online learning from scratch with dynamic simulators is often too slow, making transfer learning from static to dynamic simulators of great interest. From the broader literature on transfer learning and learning from simulators we know that it is challenging to train policies that can improve on previously learned behavior when deployed

Figure 1: Left: Wake effects in the offshore wind farm of Horns Rev 1 - Vattenfall. Right: Schema of a wind turbine . The _pitch_, _yaw_ or _torque_ can be controlled.

on new environments with unseen dynamics [48; 15]. In spite of this problem, to the best of our knowledge, most approaches so far have been trained and evaluated on the same environment, and it is therefore not clear whether the policies learned with simulators are robust enough to be useful, or even safe, when deployed on real wind farms. With two simulators of different model-fidelity (referring to how closely the model represents the real system), WFCRL offers the possibility of designing transfer learning strategies between these simulators.

Contributions of the paper

* We introduce WFCRL, the **first open reinforcement learning** suite of environments for **wind farm control**. WFCRL is highly customizable, allowing researchers to design and run their own environments for both centralized and multi-agent RL. It includes a default suite of wind farm layouts to be used in benchmark cases.
* We interface all our wind farm layouts with **two different wind farm simulators**: a static simulator FLORIS  and a dynamic simulator FAST.Farm . They can be used to **design transfer learning strategies**, with the goal to **learn robust policies that can adapt to unseen dynamics**.
* We include implementations of three state-of-the-art MARL algorithms, IPPO, MAPPO , and QMIX  adapted to our environments.
* We propose a benchmark example for **wind power maximization** with two wind condition scenarios. It takes into account the costs induced by wind turbine fatigue.

The paper is organized as follows. In Section 2, we introduce the WFCRL environment suite. First in Section 2.1 we introduce the simulators, the specifications of the simulated wind farms and turbines and the wind conditions scenarios we consider. We then lay out in Section 2.2 the cooperative MARL framework for the wind farm control problem, and finally detail the learning tasks and algorithms available with the suite in Section 2.3. In the second part Section 3, we illustrate the possibilities of the WFCRL environment suite by introducing a benchmark example: the maximization of total power production with fatigue-induced costs. In Section 3.1, we explicit the actions, observations and rewards used in this problem, then in Section 3.2, we present and discuss the results of the IPPO and MAPPO on our benchmark tasks. In Section 4, we discuss perspectives and limitations, and we conclude in Section 5

## 2 WFCRL environments suite

In this section, we present our WFCRL environments suite. We first present the simulators interfaced in WFCRL (FLORIS and FAST.Farm), several pre-defined layouts and wind condition scenarios. Note again that having two simulation environments with different model-fidelity offers the possibility of designing transfer learning strategies between simulation environments. Then, we describe briefly the MARL framework for the wind farm control problem. More precisely, we consider a wind farm with \(M\) turbines, which operate in the same wind field and create turbulence that propagates across the farm. In our multi-agent environment, each turbine is considered an agent receiving local observations, and all cooperate to maximize a common objective. Note that WFCRL also has a single-agent RL environment, which uses global observations and actions.

### The simulation environments

In WFCRL, users can choose one of the two state-of-the-art wind farm simulators (FLORIS or FAST.Farm), select a pre-defined wind farm layout or define a custom one, and choose one of the implemented wind conditions.

Various wind farm simulators can be used to evaluate wind farm control strategies . The choice of the wind farm simulators included in WFCRL relies on three criteria: the trade-off between fidelity and computation time, the popularity of the simulators in the wind farm energy community, and open-source availability. We discuss this further in Appendix A, and introduce our simulation environments in the following paragraph.

FLORIS environmentsThe wind farm simulator FLORIS implements static wind farm models, which predict the locations of wake centers and velocities at each turbine in the steady state: the dynamic propagation of wakes are neglected. The yaws of all wind turbines can be controlled, and the power production of the wind farm is then a function of all yaw angles and the so-called free-stream wind conditions: wind measurements - e.g. velocity and direction - taken at the entrance of the farm. FLORIS has been released as an open-source Python software tool1. In WFCRL environments built on FLORIS, global and local states contain time-averaged, steady-state wind and production statistics for both global and local observations.

The models used by FLORIS do not compute any estimate of fatigues on wind turbines, and we propose to use local wind statistics to compute a proxy for load estimates indeed. We detail this when introducing our benchmark example in Section 3.1.

FAST.Farm environmentsUnlike FLORIS, FAST.Farm is a dynamic simulator that produces time-dependent wind fields that take into account the dynamics of wake propagation : wakes in wind farms tend to meander, and the wakes of different turbines interact and eventually merge as they propagate in the farms. One consequence is that under dynamic conditions there is a significant delay between the time agents take an action and the time this action finally impacts the turbines downstream.

FAST.Farm is built on wind turbine simulation tool OpenFAST  which computes an estimate of the strength of the bending moment on each turbine blades. This reflects the structural loads induced on turbine blades, and thus can be used to design rewards in RL problems to reduce or avoid physical damages to turbines.

FAST.Farm is coded in Fortran. To allow for integration with the large ecosystem libraries and RL research practices developed in Python, we implement an interface between the simulator and the Python wind farm environment via MPI communication channels. The details of the interfacing infrastructure are reported in Appendix B.

Wind farm layoutsAny custom layout - the arrangement of the wind turbines in the farm - can be used in WFCRL. We also propose several pre-defined wind farm layouts for use in benchmark cases. The coordinates of the wind turbines of \(5\) real wind farms with \(7\) to \(91\) wind turbines are obtained from . A complete list of all correspondences between wind farms inspired by real environments and their locations is in Table 1, and a list of all available environments can be found in Appendix C. We also include in WFCRL several toy layouts, including a simple row of \(3\) turbines (the _Turb3Row1_ layout) for validation purpose and the \(32\) turbines layout of the _FarmConners_ benchmark . A visual representation of the layouts can be found in Appendix H.

For all cases, we simulate instances of the NREL Reference 5MW wind turbines, whose specifications have been made public by the National Renewable Energy Laboratory (NREL) . It has become standard reference for wind energy research and is used by the majority of proposed evaluations of RL methods .

Wind condition scenariosFor all environments, we distinguish three scenarios.

_Wind scenario I:_ In this scenario, all trajectories in a given environment are run under the prevailing wind velocity and direction at the location.

**WFCRL environment** & **Real wind farm** \\ Ablincourt & Ablincourt Energies onshore wind farm, Somme, France \\ Ormonde & Ormonde Offshore Wind Farm, Irish Sea, UK \\ WMR & Westernost Rough Wind Farm is an offshore wind farm, North Sea, UK \\ HornsRev1 & Horns Rev 1 Offshore Wind Farm, North Sea, Denmark \\ HornsRev2 & Horns Rev 2 Offshore Wind Farm, North Sea, Denmark \\ 

Table 1: Correspondences between WFCRL environments and real wind farms.

_Wind scenario II:_ In this scenario, we let the wind farm be subject to variations in wind change, and sample new free-stream wind conditions \(u_{},_{}\) at the beginning of each episode:

\[u_{}(,)_{}( ,_{})\] (1)

where \(\) is a Weibull distribution modeling wind speed with shape \(\) and scale \(\), and \(\) is a Normal distribution with \(\) being the dominant wind direction for a given farm.

_Wind scenario III:_ In this scenario, the wind farm is subjected to a an incoming wind that varies during a single episode. Any time series with wind speed and direction measurements can be used by WFCRL, and we provide a default time series of measurements collected on a real wind farm. A starting point in the time-series is randomly selected at the beginning of each new episode.

By default, at the beginning of each simulation, all wind turbines have the yaw angle zero. This corresponds to the so-called _greedy_ case, the strategy that would allow each of them to maximize its production in un-waked conditions.

### The MARL framework for the wind farm control problem

A Decentralized Partially Observable Markov Decision Process (Dec-POMDP) with \(M\) interacting agents is a tuple \(\{M,S,O,A,P,o^{1},,o^{M},r\}\). \(S\) is the full state space of the system, while for any \(i\{1,,M\}\), \(O_{i}\) is the observation space of the \(i\)th agent with \(O=_{i}^{M}O_{i}\). \(A_{i}\) is the local action space of the agent, and the global action space is the product of all local action spaces \(A=_{i}^{M}A_{i}\). At each iteration, all agents observe their local information, chose an action and receive a reward \(r:S A S\). The system then moves to a new state, which is sampled from the transition kernel \(P:S A S\). \(P\) gives the probability of transition from a state \(s S\) to \(s^{} S\) when agents have taken global action \(a A\). The probability for the \(i\)th agent to observe \(o_{i}\) is then defined by local observation function \(o^{i}:S A O_{i}\), and the history of all past observations is denoted \(h_{i}\). We call \(_{1},,_{M}\) the policies followed by each agent, where \(_{i}(a_{i}|h_{i})\), defines the probability for agent \(i\) to chose action \(a_{i}\) after observing \(h_{i}\). The corresponding global policy \(=(_{1},,_{M})\) simply concatenates the outputs of all local policies.

ObjectiveThe MARL problem is to find a policy \(^{*}\) that maximizes the expectation of the discounted sum of rewards collected over a finite or infinite sequence of time-steps

\[_{}_{s_{0},a_{0},s_{1},}[J], J:=_ {k=0}^{T}^{k}r_{k}\] (2)

with \(0<<1\) the discount factor and \(T\) the number of steps in the environment, or the length of an episode. For the wind farm control problem, possible rewards include the total production of the farm or a distance to a target production. As fatigue load measurements are also available, rewards can be designed to encourage actions that preserve the turbine structure. Note that knowledge of the farm layout and incoming wind direction can also be exploited to represent wake interactions between wind turbines as a time-varying graph (see Appendix G): this approach can motivate the design of local reward functions for decentralized learning RL algorithms with communication. Moreover, empirical successes applying RL to wind farm control have often relied on creative reward shaping . To support these two efforts, WFCRL implements a RewardShaper class that allows easy design of custom reward functions, and can be used to train both centralized and decentralized learning algorithms.

State and ObservationAs the production of each turbine is a function of the local wind conditions at its rotor, a Markovian description of the full state of the system should contain the whole wind velocity field of the entire farm. This is impossible to know in practice. We rather assume that local measurements of wind speed and direction are available at each wind turbine, and that an estimate of the free-stream wind speed and direction can be accessed, but might not necessarily be sent to the turbines in real time. Our environments therefore distinguish between the local observations \(o_{i}\) for each \(\{1,,M\}\) and a global observation \(o_{g}\). Each \(o_{i}=(u_{i},_{i},_{i})\) contains a local measure of the wind velocity \(u_{i}\) and direction \(_{i}\), as well as the last target value sent to each actuator \(_{i}\). On FLORIS environments, \(_{i}\) is always equal to the current value of the actuators. This is not the case on FAST.Farm, for which an inner control loop at the level of the actuators adds a response delay. The global observation \(o_{g}=(o_{i},,o_{M},u_{},_{})\) contains the concatenation of all local states, as well as the free-stream measure of the wind \(u_{},_{}\). Table 2 summarizes all observations and actions available with the two simulators.

ActionsWFCRL offers several ways to control wind turbines: the _yaw_, the _pitch_ or _torque_. Yaw control is available on the FLORIS environments, and all three can be controlled on FAST.Farm environments. The yaw is the angle between a wind turbine's rotor and the wind direction: turbines facing the wind have a yaw of \(0^{}\) which maximizes their individual power output. Increasing the yaw can deflect the wake away from downstream turbines, which may increase the total production of the wind farm. The pitch is the angle of the attack of the rotor blades with respect to the incoming wind, while the torque of the turbine's rotor directly controls the rotation speed. Increasing the blade pitch or decreasing the torque target both decrease the fraction of the power in the wind extracted by the turbine, and therefore decrease the turbulence in its wake. To reflect the fact that the actuation rate of the wind turbines is limited by physical constraints, we conceive actions as increases or decreases in the actuator target value rather than absolute values, with the limits being implemented by the upper and lower bounds of a continuous action space.

### Learning in WFCRL

All environments are implemented with standard RL and MARL Python interfaces Gymnasium  and PettingZoo . The source code of WFCRL is open-sourced under the Apache-2.0 license and publicly released at www.github.com/ifpen/wfcrl-env.

#### 2.3.1 Online Learning

Environments implemented on both FLORIS and FAST.Farm can be used in an episodic learning approach. This is the traditional setting of the RL problem, and we will refer to it as the _Online Learning_ Task. In Wind scenario I and III, we look at the evolution of the sum of rewards collected over an episode. In Wind scenario II, where a different set of wind conditions is sampled at each episode, we evaluate the policies on a predefined set of wind conditions and use a weighted average as the final score. This gives us our evaluation score:

\[(_{1},,_{M})=_{j=1}^{n_{w}}_{j}_{k=0}^{ T}r_{k}\] (3)

where \(T\) is the length of the episode, \(n_{w}\) is the number of wind conditions considered and the \(_{1},,_{n_{w}}\) are the weights on each conditions, with for all \(j\), \(0<_{j}<1\) and \(_{j}^{n_{w}}_{j}=1\). The wind conditions distributions on which policies are evaluated need not be identical to the one from which conditions were sampled during training.

#### 2.3.2 Transfer

Exploration on real wind farms is costly: as prototype models are typically not available for large wind farms, adjusting to the real dynamics of the system will require exploring in real time on an operating wind farm. Every move of explorating in a suboptimal direction is a cost for the farm operator. Learning efficient policies offline that can quickly adapt to the real system is therefore critical. Since the dynamic FAST.Farm simulator is considered a higher fidelity version of the static simulator FLORIS, we propose to use the former as a proxy of a real wind farm to evaluate the robustness of policies learned on the latter, and their ability to adjust to the real dynamics of a farm. We will refer to this as the _Transfer_ Task.

#### 2.3.3 Algorithms

We focus on two state-of-the-art algorithms IPPO (Independent PPO) and MAPPO (Multi-Agent PPO) introduced in [8; 47]. Both are based on the on-policy actor-critic PPO algorithm , and support

  & FLORIS & FAST.Farm \\  Local Observations \(o_{i}\) & \(u_{i},_{i}\) (steady-state), \(y_{i}\) & \(u_{i},_{i}\) (time-dependent), \(y_{i},p_{i},_{i}\) \\ Global Observations \(o_{g}\) & ,,o_{M},u_{},_{}\)} \\ Actions & \( y_{i}\) & \( y_{i}, p_{i},_{i}\) \\ 

Table 2: Observations (global and local) and actions available for an agent \(i\) in FLORIS and FAST.Farm environments. \(y_{i},p_{i},_{i}\) refer respectively to the yaw, pitch and torque of the turbine.

continuous action spaces. Following an approach called independent learning, IPPO builds on PPO by allowing every agent to run a PPO algorithm in parallel. MAPPO, on the other hand, maintains both \(M\) agent policies taking actions based on local information and a shared critic, which estimates the value of a global observation. The choice of the global observation fed to the critic is an important factor influencing the performance of the algorithm . We follow the recommendations of  to adapt PPO to the multi-agent case. They suggest to include both local and global observation features to the value function input. We therefore feed to our shared critic network the full global observation introduced in Section 2.2, that is both the of concatenation of all local observations and the free-stream wind velocity.

In , it was found that PPO-based methods perform very well when extended to cooperative multi-agent tasks, outperforming algorithms specifically designed for cooperative problems like QMIX . We implement both QMIX and independent Deep Q-network (DQN)  baselines, where all agents run DQN algorithms in parallel using a discrete action space. As QMIX uses a recurrent neural network, we implement independent DQN baselines with both fully connected (IDQN) and recurrent (IDRQN) neural networks. The same global observation is fed to the central MAPPO critic and the mixing network of QMIX.

For implementation, we adapt the CleanRL 2 baseline implementations of PPO and DQN to our multi-agent Petting Zoo environments. Although our analysis for the benchmark case considered in Section 3 focuses on PPO-based algorithms on continuous action spaces, additional results for baselines on discrete action spaces are reported in Appendix F.

## 3 Benchmark example: the maximization of the total power production

We consider the problem of finding the optimal yaws to maximize the total power production under a set of wind conditions, and taking into account the costs induced by turbine fatigue load. This problem is known as the _wake steering_ problem, and is an active area of research in the wind energy literature [16; 17].

### Problem formulation

Actions and observationsLocal observations include the local yaw and local wind statistics. The concatenation of all local observations along with free-stream wind statistics in the global observation is as described in Section 2.2. Recall that actions are defined as increase or decrease in the actuator target value. In this problem, all agents control their yaws, and we define the continuous action space \([-5,5]\), defining changes in yaw angle expressed in degrees. To constraint the load on the turbines caused by the control strategies and reduce its impact on the lifetime of the turbines, the time each turbine spends actuating is limited. We choose the upper bound of \(10\%\) of the time, which is the same upper bound value discussed in . At every iteration, the time needed to change the state of the actuator is computed, and any action violating this condition is not allowed.

RewardsAt each iteration \(k\), all agents receive a reward \(r_{k}^{P}\) which is the currently measured production of the wind farm in kW divided by the number of agents and normalized by the free-stream wind velocity:

\[r_{k}^{P}=_{i}^{M}_{k}^{i}}{(u_{,k})^{3}}\] (4)

where \(_{k}^{i}\) is the measured power production and \(u_{,k}\) the free-stream wind velocity at time-step \(k\). To discourage agents from taking risky policies damaging the turbines, we also return a load penalty \(r_{k}^{L}\) which increases with the sum of loads on all the turbine blades.

FLORIS does not provide estimates of the loads on structures. Instead, we evaluate the impact of actuations on loads with a proxy based on local estimates of turbulence and velocities on the surface of the rotor planes. Our proxy takes into account 2 factors increasing stress on wind turbine structures as noted in : first, the turbulence of the wind and second, the variation of velocities on the turbine rotor. We therefore define the load penalty in FLORIS environments as

\[r_{k,S}^{L}=_{i}^{M}(_{j}^{9}TI_{k}[x_{i,j},y_{i,j}]+ (u_{k})+(v_{k})+(w_{k}))\] (5)

where \(TI_{k}\) is the turbulence field at time-step \(k\), \(u_{k}\), \(v_{k}\) and \(w_{k}\) are respectively the \(x\), \(y\) and \(z\) components of the velocity field at time-step \(k\), and the \(x_{i,j}\) define the coordinates of the \(9 M\) grid points at which these values are computed for the \(M\) rotor planes. \(\) denotes the standard-deviation.

For FAST.Farm, we use the estimates of the the blades' bending moment strength as a proxy for the structural loads induced on the turbines, and define the load penalty as

\[r_{k,D}^{L}=_{i}^{M}_{j}^{3}|Mop_{k}[i,j]|+_{j} ^{3}|Mip_{k}[i,j]|\] (6)

where \(Mop_{k}\) is the \(M 3\) matrix of out-of-plane bending moments for the \(3\) blades of every turbine at time-step \(k\), and \(Mip_{k}\) is the corresponding matrix of in-plane bending moments.

Both rewards are common to all turbines, and all must therefore maximize (2) with \(r_{k}=(r_{k}^{P}- r_{k}^{L})\), where \(\) is a weighting parameter. The load penalty indicator returned by the environment is downscaled so that \(r_{k}^{P}\) and \(r_{k}^{L}\) are of similar magnitudes. By default, the value of \(\) is \(1\), but greater or less importance can be given to turbine safety by changing \(\).

Wind conditionsWe focus here on Wind Scenarios I and II. To evaluate the algorithms on Scenario II with score (3), we need to define weights \(_{j}\). We use data acquired during the SmartEole project at the location of the Ablaincourt wind farm . It consists of estimates of free-stream wind direction

Figure 2: The evolution of total reward (b), power output (c) and load penalties (d) accumulated over an episode (with \(T\)=150) on the _Ablaincourt_ environment, simulated with FLORIS. A visual representation of the layout is in (a), where the coordinates are in wind turbine diameters. During training, policies are evaluated every \(5\) training steps with deterministic policies. The curves are plotted for all \(5\) seeds.

and velocity computed from measures taken during a \(3\) months field campaign every \(10\) min. Since in real conditions wind velocity and wind direction are correlated, we compute the bi-dimensional histogram for the two variables, taking \(5\) bins for each dimension. We obtain a set of \(25\) wind condition rectangles. The wind conditions \(w_{1},,w_{j}\) are the center of each rectangle, and the corresponding weights \(_{j}\) are defined as the frequencies at which wind conditions in the time series appeared in the rectangle.

### Results

We apply algorithms IPPO and MAPPO, whose implementations are both available in WFCRL, to our benchmark example. Both are trained on the two wind scenarios I and II described in Section 2.1, on the static simulator FLORIS. For the Scenario I, the score is the reward obtained on a single policy rollout of \(150\) steps in the environment, and policies are updated after \(2048\) steps in the environment. Learned deterministic policies are evaluated every \(5\) training steps. Results for the _Ablaincourt_ layout are given in Fig. 2. For the Scenario II, the score is the one defined in (3), with \(T=2048\). The training curves for the the _Ablaincourt_ and the _Turb3Row1_ layouts are illustrated in Fig. 3.

At the end of training, we evaluate all algorithms on both scenarios, as well as on FAST.Farm environments for Scenario I. On _Turb3Row1_, IPPO has the best performance for all evaluation tasks, while on _Ablaincourt_, MAPPO performs better for \(2\) evaluation tasks out of \(3\). This confirms the good empirical results of IPPO on cooperative tasks observed in the literature , and suggests that MAPPO's shared critic becomes more beneficial as the number of agents increases. Although IPPO and MAPPO perform similarly on Scenario I, the gap in favor of IPPO increases on Scenario II, showing MAPPO to be less efficient at adapting policies to diverse wind conditions.

As expected, the best evaluation performance for each wind scenario on FLORIS is provided by the algorithms trained on this scenario. Yet on FAST.Farm, although our evaluation task is solely under Scenario I (constant wind), its noisier wind observations pose a challenge to IPPO policies trained on Scenario I. As local wind observations are now perturbed by time-dependent turbulences created by other agents, information sharing (MAPPO) or exposition to a variety of wind conditions (Scenario II) during training becomes more useful.

A table detailing all evaluation scores at convergence is available in Appendix F and hyper-parameters are given in Appendix D. The code to reproduce all experiments is available at www.github.com/ifpen/wfrcl-benchmark.

To illustrate the _Transfer_ case, we then fine-tune the learned IPPO policies on a _Turb3Row1_ on \(40k\) steps (\(1\) day in simulated time) in the corresponding FAST.Farm environment. We report in Appendix F.1 the evolution of average power output and load, and compare it to a naive deployment of strategies learned online. Our results illustrate the difficulty of adapting learned policies to unseen dynamics.

Figure 3: Evolution of the evaluation score, defined in (3), during the training of IPPO and MAPPO on the two environments _Turb3Row1_ (left) and _Ablaincourt_ (right).

## 4 Limitations

As noted in Section 2, the choice of the simulators included in WFRCL has been made on the criteria of fidelity, computation cost, popularity and availability in open source. Both FLORIS and FAST.Farm are developed and actively maintained by the US-based National Renewable Energy Laboratory 3, and have a large user base among wind energy researchers. FAST.Farm was explicitly designed to provide good fidelity at a limited computation cost . Despite this, dynamic wind farm simulators remain slow. The development and open-sourcing of faster dynamic simulators will be critical. Machine-learning accelerated simulators could be an important step in that direction. Moreover, although FAST.Farm has been extensively validated against both real wind farm data and high-fidelity simulations (see Appendix A), there has been to the best of our knowledge no explicit investigation of the transfer of yaw optimization results from FAST.Farm simulations to a real wind farm.

## 5 Conclusion

We have introduced WFCRL, the first open reinforcement learning suite of environments for wind farm control. WFCRL is highly customizable, allowing researchers to design and run their own environments for both centralized and multi-agent RL. It is interfaced with two different wind farm simulators: a static simulator FLORIS and a dynamic simulator FAST.Farm. They can be used to design transfer learning strategies with the goal to learn robust policies that can adapt to unseen dynamics. We have proposed a benchmark example for wind power maximization with two wind condition scenarios that take into account the costs induced by wind turbine fatigue. We hope that WFCRL will help building a bridge between the RL and wind energy research communities.

## 6 Acknowledgments

Part of this work is carried out in the framework of the AI-NRGY project, funded by France 2030 (Grant No: ANR-22-PETA-0004).