# DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction

DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction

Bowen Song &Jason Hu1 &Zhaoxu Luo &Jeffrey A. Fessler &Liyue Shen

Department of Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

{bowenbw, jashu, luozhx, fessler, liyues}@umich.edu

These authors contributed equally to the work

###### Abstract

Diffusion models face significant challenges when employed for large-scale medical image reconstruction in real practice such as 3D Computed Tomography (CT). Due to the demanding memory, time, and data requirements, it is difficult to train a diffusion model directly on the entire volume of high-dimensional data to obtain an efficient 3D diffusion prior. Existing works utilizing diffusion priors on single 2D image-slice with hand-crafted cross-slice regularization would sacrifice the z-axis consistency, which results in severe artifacts along the z-axis. In this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images. To the best of our knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D medical image reconstruction. Extensive experiments on sparse view and limited angle CT reconstruction show that our DiffusionBlend method significantly outperforms previous methods and achieves state-of-the-art performance on real-world CT reconstruction problems with high-dimensional 3D image (i.e., \(256 256 500\)). Our algorithm also comes with better or comparable computational efficiency than previous state-of-the-art methods. Code is available at: https://github.com/effzero/DiffusionBlend.

## 1 Introduction

Diffusion models learn the prior of an underlying data distribution, which enables sampling from the distribution to generate new images [1; 2; 3]. By starting with a clean image and gradually adding noise of different scales, diffusion sampler eventually obtains an image that is indistinguishable from pure noise. Let \(_{t}\) be the image sequence where \(t=0\) represents the clean image and \(t=T\) is pure noise. The score function of the image distribution, denoted as \((_{t})= p(_{t})\), can be learned by a neural network parametrization, which takes \(_{t}\) as input and then approximates \( p(_{t})\). The reverse process then starts with pure noise and uses the learned score function to iteratively remove noise, ending with a clean image sampled from the target distribution \(p()\).

Leveraging the learned score function as a prior, it is efficient to solve the inverse problems based on diffusion priors. Previous works have proposed to use diffusion inverse solvers for deblurring, super-resolution, and medical image reconstruction such as in magnetic resonance imaging (MRI) and computed tomography (CT), and many other applications [4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16].

Computed tomography (CT) reconstruction is an important inverse problem that aims at reconstructing the volumetric image \(\) from the measurements \(\), which is acquired from the projections at different view angles . To reduce the radiation dose delivered to the patient, sparse-view CT uses a smaller fraction of X-rays compared to the full-view CT . Additionally, limited-angle CT is useful in cases where patients may have mobility issues and cannot use full-angle CT scans . Although previous works have discussed and proposed diffusion-based methods for solving the 2D CT image reconstruction problem to demonstrate the proof-of-concept [9; 10], there is very limited work focusing on solving inverse problems for 3D images due to the practical difficulty in capturing 3D image prior. Learning efficient 2D image priors using diffusion models is already computationally expensive, which requires large-scale of training data, training time, and GPU memory. For example, previous works [2; 3] require training for several days to weeks on over a million training images in the ImageNet  and LSUN  datasets to generate high-quality 2D natural images of size \(256 256\). Hence, directly learning a 3D diffusion prior on the entire CT volume would be practically infeasible or prohibitively expensive due to the demanding requirements of training data and computational cost. In addition, real clinical CT data is usually limited and scarce and often has a resolution larger than \(256 256 400\), which makes directly training the data prior very challenging. The problem of tackling 3D image inverse problems, especially for medical imaging remains a challenging open research question.

A few recent works [13; 14; 15] have discussed and proposed to solve 3D image reconstruction problems either through employing some hand-crafted regularization to enforce consistency between 2D slices when reconstructing 3D volumetric images [13; 15], or through training several diffusion models for 2D images on each plane (axial, coronal, and sagittal), and performing reverse sampling with each model alternatively . However, all of these works only learn the distribution of a single 2D slice via the diffusion model, while having not yet explored the dependency between slices that is required to better model the real 3D image prior.

To overcome these limitations, we propose a novel method, called DiffusionBlend, that enables learning the distribution of 3D image patches (a batch of nearby 2D slices), and blends the scores of patches to model the entire 3D volume distribution for image reconstruction. Specifically, we firstly propose to train a diffusion model that models the joint distribution of 3D image patches (nearby 2D slices) in the axial plane conditioning on the slice thickness. Then, we introduce a random blending algorithm that approximates the score function of the entire 3D volume by using the trained 3D-patch score function. Moreover, we can either directly use the trained model to predict the noise of a single 2D slice by taking its corresponding 3D patch as input, or applying a random blending algorithm that firstly randomly partitions the volume into different 3D patches at each time step and then computes the score of each 3D patch during reverse sampling. Through either way, we can output the predicted noise of the entire 3D volume. In this way, our proposed method is able to enforce cross-slice consistency without any hand-crafted regularizer. Our method has the advantage of being fully data-driven and can enforce slice consistency without the TV regularizer as demonstrated in Fig. 1. Through exhaustive experiments of ultra-sparse-view and limited-angle 3D CT reconstruction on different datasets, we validate that our proposed method achieves superior reconstruction results for 3D volumetric imaging, outperforming previous state-of-the-art (SOTA) methods. Furthermore, our method achieves better or comparable inference time than SOTA methods, and requires minimum hyperparameter tuning for different tasks and settings.

Figure 1: Overview of DiffusionBlend++ compared to previous 3D image reconstruction works. Previous work used a hand-crafted TV term to “regularize” adjacent slices, whereas the proposed approach uses learned diffusion score blending between groups of slices. Here \(i\) is the slice index, and \(t\) is the reconstruction iteration.

In summary, our main contributions are as follows:

* We propose DiffusionBlend(++): a novel method for 3D medical image reconstruction through 3D diffusion priors. To the best of our knowledge, our method is the first diffusion-based method that learns the 3D-patch image prior incorporating the cross-slice dependency, so as to enforce the consistency for the entire 3D volume without any external regularization.
* Specifically, instead of independently training a diffusion model only on separated 2D slices, we propose a novel method that first trains a diffusion model on 3D image patches (a batch of nearby 2D slices) with positional encoding, and at inference time, employs a new approach of random partitioning and diffusion score blending to generate an isotropically smooth 3D volume.
* Extensive experiments validate our proposed method achieves **state-of-the-art** reconstruction results for 3D volumetric imaging for the task of ultra-sparse-view and limited-angle 3D CT reconstruction on different datasets, with improved inference time efficiency and minimal hyperparameter tuning.

## 2 Background and Related Work

Diffusion models.Diffusion models consists of a forward process that gradually adds noise to a clean image, and a reverse process that denoises the noisy images [1; 22]. The forward model is given by \(x_{t}=x_{t-1}- t}{2}x_{t-1}+} t\) where \( N(0,1)\) and \((t)\) is the noise schedule of the process. The distribution of \((0)\) is the data distribution and the distribution of \((T)\) is approximately a standard Gaussian. When we set \( t 0\), the forward model becomes \(dx_{t}=-_{t}x_{t}dt+}d_{t}\), which is a stochastic differential equation. The solution of this SDE is given by

\[dx_{t}=(--(t)_{x_{t}} p_{t}(x_{t}) )dt+d}.\] (1)

Thus, by training a neural network to learn the score function \(_{x_{t}} p_{t}(x_{t})\), one can start with noise and run the reverse SDE to obtain samples from the data distribution.

Although diffusion models have achieved impressive success for image generation, a bottleneck of large-scale computational requirements including demanding training time, data, and memory prevents training a diffusion model directly on high-dimensional high-resolution images. Many recent works have been studying how to improve the efficiency of diffusion models to extend them to large-scale data problem. For example, to reduce the computational burden, latent diffusion models  have been proposed, aiming to perform the diffusion process in a much smaller latent space, allowing for faster training and sampling. However, solving inverse problems with latent diffusion models is still a challenging task and may have sub-par computational efficiency . Very recently, various methods have been proposed to perform video generation using diffusion models, generally by leveraging attention mechanisms across the temporal dimension [25; 26; 27; 28]. However, these methods only focus on video synthesis. Utilizing these complicated priors for posterior sampling is still a challenge because if these methods were applied to physical 3D volumes, continuity would only be maintained across slices in the XY plane and not the other two planes. Finally, work has been done to perform sampling faster [29; 30; 31], which is unrelated to the training process and network architecture. However, although these methods effectively promote the efficiency of training a diffusion model, current works are not yet able to tackle the large-scale 3D image reconstruction problem in real world settings.

3D CT reconstructionComputed tomography (CT) is a medical imaging technique that allows a 3D object to be imaged by shooting X-rays through it . The measurements consist of a set of 2D projection views obtained from setting up the source and detector at different angles around the object. By definition, \(\) is the (known) set of projection views, \(\) is the (in most cases assumed to be) linear forward model of the CT measurement system, and \(\) is the unknown image. The CT reconstruction problem then consists of reconstructing \(\) given \(\). Traditional methods for solving this include regularization-based methods that enforce a previously held belief on \(\) and likelihood based methods [17; 32; 33; 34].

Data-driven methods have shown tremendous success in signal and image processing in recent years [35; 36; 37; 38; 39]. In particular, for solving inverse problems, when large amounts of training data is available, a learned prior can be much stronger than the hand-crafted priors used in traditional methods [40; 41]. For past few years, many deep learning-based method have been proposed for solving the 3D CT reconstruction problem [42; 43; 44; 45]. These methods train a convolutional neural network, such as a U-Net , that maps the partial-view filtered backprojection (FBP) reconstructed image to the ground truth image, that is, full-view CT reconstruction. However, these methods often generate blurry images and generalizes poorly for out-of-distribution data .

3D CT reconstruction with diffusion models.Diffusion models serve as a very strong prior as they can generate entire images from pure noise. Most methods that use diffusion models to solve inverse problems formulate the task as a conditional generation problem [47; 48; 49] or as a posterior sampling problem [4; 50; 6; 9; 4]. In the former case, the network requires the measurement \(\) (or an appropriate resized transformation of \(\)) during training time. Thus, at reconstruction time, that trained network can only be used for solving the specific inverse problem with poor generalizability. In contrast, for the posterior sampling framework, the network learns an unconditional image prior for \(\) that can help solve various inverse problem related to \(\) without retraining. Although these diffusion-based methods have shown great performance for solving inverse problems for 2D images in different domains, there are seldom methods that are able to tackle inverse problems for 3D images because of the infeasible computational and data requirements as aforementioned. Specifically, for 3D CT reconstruction, DiffusionMBIR  trains a diffusion model on the axial slices of volumes; at reconstruction time, it uses the total variation (TV) regularizer with a posterior sampling approach to encourage consistency between adjacent slices. Similarly, DDS  builds on this work by using accelerated methods of sampling and data consistency to greatly reduce the reconstruction time. However, although the TV regularizer has shown some success in maintaining smoothness across slices, it is not a data-driven method and does not properly learn the 3D prior. TPDM  addresses this problem by training a separate prior on the coronal slices of volumes with a conditional sampling approach, which serves as a data-driven method of maintaining slice consistency at reconstruction time, but requires that all the volumes have the same cubic shape. In exchange, this method sacrifices the speed gains made by DDS, requiring alternating updates between the two separate priors, and is also twice as computationally expensive at training time. To overcome these limitations, we aim to propose a more flexible and robust approach that can learn the 3D data prior properly for CT reconstruction, maintaining slice consistency while not sacrificing inference time.

## 3 Methods

Instead of modeling the 2D slices of the 3D volume as independent data samples during training time, and then applying regularization between slices at reconstruction time, we propose incorporating information from neighboring slices at training time to enforce consistency between slices. More precisely, our first approach models the data distribution of a 3D volume with \(H\) slices in the

Figure 2: Overview of slice blending process during reconstruction for DiffusionBlend++. At each iteration, we partition the slices of the volume in a different way; slices of the same color are inputted into the network independently. Positional encoding (PE) is also inputted to the network as information about the separation between the slices.

dimension as follows:

\[p()_{i=1}^{H}p([:,:,i][:,:,i-j:i-1], [:,:,i+1:i+j])/Z,\] (2)

where \(j\) is a positive integer indicating the number of neighboring slices above and below the target slice that are being used as conditions to predict the target slice, and \(Z\) is a normalizing constant. To deal with boundary conditions where the third index may exceed the bounds of the original volume, we apply repetition padding above and below the main volume.

For training, we simply concatenate each of the conditioned slices with the target slice along the channel dimension to serve as an input to the neural network. Then we apply denoising score matching to predict the noise of the target slice as the loss function of the neural network:

\[_{t(0,T)}_{ p()} _{(0,I)}_{i[1,H]}\|_{}(_{t}[:,:,i-j:i+j],_{t})-[:,:,i]\|_{2}^{2}.\] (3)

At reconstruction time, the score function of the entire volume decomposes as a sum of score functions of each of the slices:

\[ p()_{i=1}^{H} p([:,:,i ][:,:,i-j:i-1],[:,:,i+1:i+j]).\] (4)

In this way, we have rewritten the score of the 3D volume as sums of the scores of the 2D slices learned by the network. This means that we can now apply any algorithm that uses diffusion models to solve inverse problems to solve the 3D CT reconstruction problem. Furthermore, this method of blending together information from different slices allows us to learn a prior for the entire volume that combines information from different slices. We call this method **DiffusionBlend**.

To learn an even better 3D image prior, instead of learning the conditional distribution of individual target slices, we can learn the **joint distribution** of several neighboring slices at once, which we call a 3D patch. Letting \(k\) be the number of slices in each patch, we can partition the volume into 3D patches and approximate the distribution of the volume as

\[p()(_{i=1}^{H/k}p([:,:,(i-1)k+1:ik]))/Z,\] (5)

where \(Z\) is a normalizing constant. Comparing this with (2), the main difference is instead of conditioning on neighboring slices, we are now incorporating the neighboring slices as a joint distribution. This allows for much faster reconstruction, as \(k\) slices are updated simultaneously according to their score function. However, this method faces similar slice consistency issues as in , since certain pairs of adjacent slices (namely, pairs whose slice indices are congruent to 0 and 1 modulo \(k\)) are never updated simultaneously by the network.

To deal with this issue, we propose two additional changes. Firstly, instead of using the same partition (updating the same \(k\) slices) at once for each iteration, we can use a different partition so that the previous border slices can be included in another partition. For example, we can randomly sample the end index of the first 3D patch for adjacency slices. Let \(m\) be uniformly sampled from \(1,2,...,k\), we can use the partition

\[=\{1,2,,H\}=\{1,,m\}\{m+1,,m+k\} \{H-k+1,,H\},\] (6)

instead of \(=\{1,2,,H\}=\{1,,k\}\{k+1,,2k\} \{H-k+1,,H\}\), where \(m\) is the offset index number in the new partition. We can then compute the score on the new partition. More generally, we can choose an arbitrary partition of \(\) into \(H/k\) sets, each containing \(k\) elements for each iteration, updating each slice in the small set simultaneously for that iteration.

Secondly, to better capture information between nonadjacent slices, we apply relative positional encoding as an input to the network. More precisely, if a 3D patch has a slice thickness (the distance between two slices) of \(p\), then we let \(p\) be input of the positional encoding for that 3D patch. The positional encoding block consists of a sinusoidal encoding module and several dense connection modules, which has the same architecture as the timestamp embedding module of the same diffusion model. In this manner, the network is able to learn how to incorporate information from nonadjacent slices and captures more global information about the entire volume. Recall that for 3D patches of adjacent slices, the border between patches may have inconsistencies. To address this, we can **concatenate each border** as a new 3D patch, and then compute the score from it. If there are \(k\) slices in an adjacency-slice 3D patch, then the new 3D patch has the relative positional encoding of \(k\), and also has a size of \(k\). For instance, if the previous partition is (1,2,3),(4,5,6),(7,8,9), the new partition is (1,4,7),(2,5,8),(3,6,9). Here we are forming a new partition with jumping slices. In practice, since we need a pretrained natural image checkpoint due to scarcity of medical image data, we set \(k=3\) for facilitating fine tuning from natural image checkpoints.

We call the partitioning by 3D patch with adjacent slices as **Adjacency Partition**, and the partitioning by 3D patch with jumping slices as **Cross Partition**. Letting \(r=H/k\) be the number of 3D patches, with a random partition, this method is stochastically averaging the different estimations of the \( p()\) by different partitions. Specifically, the estimation of score by a single partition \(_{1}_{r}\) is given by \(_{i=1}^{r} p([:,:,_{i}])\). Ideally, we want to compute

\[|S|^{-1}_{=_{1}_{ r}}_{i=1}^{r} p([:,:,_{i}]).\] (7)

Similar to [4; 13; 51], we can share the summation in (7) across different diffusion steps since the difference between two adjacent iterations \(_{i}\) and \(_{i+1}\) is minimal.

In summary, we have shown how the score function of the entire volume can be written in terms of scores of the slices of the volume. Hence, similar to DiffusionBlend, this method can be coupled with any inverse problem solving algorithm. The scores of the slices can be approximated using a neural network. Training this network consists of randomly selecting \(k\) slices from a volume and concatenating them along the channel dimension to get the input to the network (along with the positional encoding of the slices), and then using denoising score matching as in (3) as the loss function; Section A.1 provides a theoretical justification for this procedure.

Sampling and reconstruction.With Eq. 7, each reconstruction step would require computing the score functions corresponding to each of the partitions of \(\), and then summing them to get the score function \(()\). We propose the variable sharing technique for this method, and only need to compute the score of one partition per time step. Hence, each iteration, we instead randomly choose one of the partitions of \(\) and update the volume of intermediate samples by the score function. Finally, we use repetition padding if \(H\) is not a multiple of \(k\). This method incorporates a similar slice blending strategy as DiffusionBlend, but allows for significant acceleration at reconstruction time as \(k\) slices are updated at once. Furthermore, it allows the network to learn joint information between slices that are farther apart without requiring the increase in computational cost associated with increasing \(k\). We call this method **DiffusionBlend++**. The pseudocode of the algorithm can be found in Alg. 1.

In practice, we choose not to select from all possible partitions, but instead select from those where the indices in each \(_{i}\) are not too far apart, as the joint information between slices that are very far apart is hard to capture. Table 12 summarizes the different 3D image prior models. The appendix provides more details about the partition selection scheme.

Krylov subspace methods.Following the work of , we apply Krylov subspace methods to enforce data consistency with the measurement. At each timestep \(t\), by using Tweedie's formula , we compute \(}_{t}=[_{0}|_{t}]\), and then apply the conjugate gradient method

\[}^{}_{t}=(^{*},^{*},}_{t},M),\] (8)

where in practice, the CG operator involves running \(M\) CG steps for the normal equation \(^{*}=^{*}\). We combine this method with the DDIM sampling algorithm  to decrease reconstruction time. To summarize, we provide the algorithm for DiffusionBlend++ below. The Appendix provides the training algorithms for our proposed method as well as the reconstruction algorithm for DiffusionBlend.

## 4 Experiments

Experimental setup.We used the public CT dataset from the AAPM 2016 CT challenge  that consists of 10 volumes. We rescaled the images in the XY-plane to have size \(256 256\) without altering the data in the Z-direction and used 9 of the volumes for training data and the tenth volume as test data. The training data consisted of approximately 5000 2D slices and the test volume had 500 slices. We also performed experiments on the LIDC-IDRI dataset . For this dataset, we first applied data preprocessing by setting the entire background of the volumes to zero. We rescaled the images in the XY-plane to have size \(256 256\), and, to compare with the TPDM method, only took the volumes with at least 256 slices in the Z-direction, truncating the Z-direction to have exactly 256 slices. This resulted in 357 volumes which we used for training and one volume used for testing.

We performed experiments for sparse view CT (SVCT) and limited angle CT (LACT). The detector size was set to 512 pixels for all cases. For SVCT, we ran experiments on 4, 6, and 8 views. We also ran additional experiments on 20, 40, 60, 80, and 100 views and report the quantitative results in the Appendix. For LACT, we used the full set of views but only spaced around a 90 degree angle. In all cases, implementations of the forward and back projectors can be found in .

For a fair comparison between DiffusionBlend and DiffusionBlend++, we selected \(j=1\) for DiffusionBlend and each \(_{i}\) to contain 3 elements for DiffusionBlend++. In this manner, both methods involve learning a prior that involves products of joint distributions on 3 slices. To train the score function for DiffusionBlend, we started from scratch using the LIDC dataset. Since this dataset consisted of over 90000 slices, the network was able to properly learn this prior. We then fine tuned this network on the much smaller AAPM dataset. For DiffusionBlend++, the input and output images both had 3 channels from stacking the slices, so we fine-tuned the existing checkpoint from . All networks were trained on PyTorch using the Adam optimizer with A40 GPUs. For reconstruction, we used 200 neural function evaluations (NFEs) for all the results. The appendix provides the full experiment hyperparameters. We observe that DiffusionBlend++ can reconstruct very high quality images that are free of artifacts as demonstrated in Fig.4 and Fig.3.

Comparison methods.We compared our proposed method with baseline methods for CT reconstruction and state of the art 3D diffusion model methods. We used the filtered back projection implementation found in . For the other baseline, we used FBP-UNet  which is a supervised method that involves training a network for each specific task mapping the F

Figure 3: Results of CT reconstruction with 4 views on AAPM dataset, axial view.

clean image. Since this is a 2D method, we learned a mapping between 2D slices and then stacked the 2D slices to get the final 3D volume. We also compared with classical CT reconstruction techniques such as SBTV, SIRT, and CGLS  to benchmark our algorithm against traditional methods. Results for these methods are reported in the Appendix. For DiffusionMBIR , we fine-tuned the score function checkpoints on our data and used the same hyperparameters as the original work. We did the same for TPDM ; however, we ran TPDM only on the LIDC dataset because TPDM requires cubic volumes. Finally, we ran two variants of DDS : one in which all the hyperparameters were left unchanged (DDS), and another in which no TV regularizer between slices was enforced (DDS 2D). Both of these methods were run with 200 NFEs. The appendix provides the experiment parameters.

Sparse-view CT.The results for different numbers of views and across different slices are shown in Tables 1, 11, and 3. DiffusionBlend++ exhibits much better performance over all the previous baseline methods (usually by a few dB) and outperforms DiffusionBlend. The second best method for each experiment is underlined and was, in most cases, DiffusionBlend. The exceptions are when the second best method is DiffusionMBIR, but this method was run with 2000 NFEs and took about 20 hours to run compared to 1-2 hours for both of our methods. The two DDS methods required similar runtime as our methods but in all cases exhibited inferior reconstruction results. Furthermore, DDS 2D generally performed worse than DDS. Thus, DDS failed to properly learn a 3D volume prior and still relied on the TV regularizer. Additionally, although TPDM should learn a 3D prior, the results were very poor compared to the other baselines. Our proposed method learned a fully 3D prior and achieved the best results in the sagittal and coronal views.

Limited-angle CT.Table 4 shows all results for limited angle CT reconstruction for both the AAPM and LIDC datasets. Our DiffusionBlend++ method obtains superior performance over all the baseline methods and DiffusionBlend obtains the second best results. Similar to the SVCT experiments, DiffusionMBIR performed the best out of the baseline methods, but took approximately 40 hours to

   &  &  \\  Method &  &  &  &  &  &  \\  & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  FBP & 14.66 & 0.359 & 13.65 & 0.293 & 11.94 & 0.222 & 14.79 & 0.217 & 14.11 & 0.191 & 13.18 & 0.169 \\ FBP-UNet & 26.00 & 0.849 & 24.15 & 0.782 & 23.37 & 0.761 & 28.58 & 0.848 & 26.48 & 0.781 & 25.19 & 0.731 \\ DiffusionMBIR & 26.30 & 0.863 & 24.99 & 0.827 & 23.66 & 0.789 & 23.67 & 0.922 & 31.18 & 0.901 & 29.02 & 0.863 \\ TPDM & - & - & - & - & - & - & 27.51 & 0.816 & 25.60 & 0.776 & 21.99 & 0.695 \\ DDS 2D & 32.89 & 0.946 & 31.04 & 0.934 & 28.77 & 0.906 & 30.82 & 0.897 & 29.38 & 0.867 & 27.54 & 0.826 \\ DDS & 33.19 & 0.945 & 31.94 & 0.942 & 29.22 & 0.916 & 31.65 & 0.915 & 30.12 & 0.888 & 27.20 & 0.808 \\ DiffusionBlend (Ours) & 34.29 & 0.955 & 33.26 & 0.949 & 31.34 & 0.944 & 33.34 & 0.933 & 0.94 & 0.905 & 27.96 & 0.849 \\ DiffusionBlend++ (Ours) & **35.69** & **0.966** & **34.68** & **0.960** & **32.93** & **0.952** & **34.46** & **0.947** & **33.03** & **0.932** & **30.98** & **0.912** \\  

Table 1: Comprehensive comparison of quantitative results on Sparse-View CT Reconstruction on Axial View for AAPM and LIDC datasets. Best results are in bold.

Figure 4: Results of DiffusionBlend++ reconstruction with multiple views on AAPM dataset, axial view.

[MISSING_PAGE_EMPTY:9]

Effectiveness of adjacency-slice blending and cross-slice blendingWe demonstrate that both the adjacency-slice blending and the cross-slice blending module are instrumental to a better reconstruction quality. Table 7 demonstrates the effectiveness of adding blending modules to the reverse sampling. Given the pretrained diffusion prior over slice patches, we observe that adding the adjacency-slice blending module improves the PSNR over a fixed partition by 1.17dB, and adding an additional cross-slice blending module further improves the PSNR by 1.63dB. Fig. 5 demonstrates that adding the cross-slice blending module removes artifacts and recovers sharper edges.

Ablation StudiesWe investigated the performance gain due to individual components. Details can be found in Appendix A.3.

## 5 Conclusion

In this work, we proposed two methods of using score-based diffusion models to learn priors of three dimensional volumes and used them to perform CT reconstruction. In both cases, we learn the distributions of multiple slices of a volume at once and blend the distributions together at inference time. Extensive experiments showed that our method substantially outperformed existing methods for 3D CT reconstruction both quantitatively and qualitatively in the sparse view and limited angle settings. In the future, more work could be done on other 3D inverse problems and acceleration through latent diffusion models. Image reconstruction methods like those proposed in this paper have the potential to benefit society by reducing X-ray dose in CT scans.