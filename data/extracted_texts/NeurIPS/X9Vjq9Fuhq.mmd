# Non-Convex Bilevel Optimization with Time-Varying Objective Functions

Sen Lin

Department of CS

University of Houston

slin50@central.uh.edu &Daouda Sow

Department of ECE

The Ohio State University

sow.53@osu.edu &Kaiyi Ji

Department of CSE

University at Buffalo

kaiyiji@buffalo.edu &Yingbin Liang

Department of ECE

The Ohio State University

liang889@osu.edu &Ness Shroff

Department of ECE & CSE

The Ohio State University

shroff.11@osu.edu

The work was done when Sen Lin was in The Ohio State University

###### Abstract

Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW.

## 1 Introduction

Bilevel optimization has attracted significant recent attention, which in general studies the following problem:

\[_{x^{d_{1}}}\ f(x,y^{*}(x))\ \ \ \ \ y^{*}(x)= _{y^{d_{2}}}g(x,y).\] (1)

Here both the outer-level function \(f\) and the inner-level function \(g\) are continuously differentiable, and the outer optimization problem is solved subject to the optimality of the inner problem. Due to its capability of capturing hierarchical structures in many machine learning problems, this nested optimization framework has been exploited in a wide variety of applications, e.g., meta-learning , hyperparameter optimization , reinforcement learning  and neural architecture search .

However, numerous machine learning problems with hierarchical structures are _online_ in nature, e.g., online meta-learning  and online hyperparameter optimization , where the current bileveloptimization framework cannot be directly applied due to the following reasons: (1) _(streaming data)_ The nature of online streaming data requires decision making on-the-fly with low regret, whereas the offline framework emphasizes more on the quality of the final solution; (2) _(time-varying functions)_ The objective functions in online applications can be time-varying because of non-stationary environments and changing tasks, in contrast to static functions considered in Equation (1); (3) _(limited information)_ The online learning is nontrivial when the decision maker only has limited information, e.g., regarding the inner-level function, which can be even more challenging with time-varying functions. For example, in wireless network control , the controller has to operate under limited knowledge about the time-varying wireless channels (see Appendix).

To reap the success of bilevel optimization in online applications, there is an urgent need to develop a new _online_ bilevel optimization (OBO) framework. Generally speaking, OBO considers the scenario where the data comes in an online manner and the agent continuously updates her outer-level decision based on the estimation of the optimal inner-level decision. Both outer-level and inner-level objective functions can be time-varying to capture the data distribution shifts in many online scenarios. Note that OBO is significantly different from single-level online optimization , due to the unavailability of the true outer-level objective function composed by \(f\) and \(y^{*}\).

The study of OBO was recently initiated in , but much of this new framework still remains under-explored and not well understood. In particular,  combines offline bilevel optimization with online optimization and proposes an online alternating time-averaged gradient method. Such an approach suffers from several limitations: 1) Multi-step update is required for inner-level decision variable \(y_{t}\) at each time \(t\), which can be problematic when only limited information of the inner-level function \(g\) is available. 2) The hypergradient estimation at each time requires the knowledge of previous objective functions in a window, and also evaluates current models on each previous function; such a design can be inefficient and infeasible in online scenarios. In this work, we seek to address these limitations and develop a new OBO algorithm that can work efficiently without the knowledge of previous objective functions.

The main contributions can be summarized as follows.

(_Efficient algorithm design_) We propose a new single-loop online bilevel optimizer with window averaging (SOBOW), which works in a fully online manner with limited information about the objective functions. In contrast to the OAGD algorithm in , SOBOW has the following major differences (as summarized in Table 1): (1) (_single-loop update_) Compared to the multi-step updates of the inner-level decision \(y_{t}\) at each round in OAGD, we only require a one-step update of \(y_{t}\), which is more practical for online applications where only limited information about the inner-level function \(g_{t}\) is available. (2) (_estimation error of Hessian inverse-vector product_) Estimating the hypergradient requires the outer-level Hessian inverse-vector product.  assumes that the exact Hessian inverse-vector product can be obtained, which can introduce high computational cost. In contrast, we consider a more practical scenario where there could be an estimation error in the Hessian-inverse vector product calculation in solving the linear system. (3) (_window averaged hypergradient descent_) While a window averaged hypergradient estimation is considered in both OAGD and SOBOW, SOBOW is more realistic and efficient than OAGD. Specifically, OAGD requires the knowledge of the most recent objective functions within a window and evaluates the current model on each previous function at every round, whereas SOBOW only stores the historical hypergradient estimation in the memory without any additional knowledge or evaluations about previous functions.

(_Novel regret analysis_) Based on the previous studies in single-level online non-convex optimization [23; 2], we introduce a new bilevel local regret as the performance measure of OBO algorithms. We show that the proposed SOBOW algorithm can achieve a sublinear bilevel local regret under mild conditions. Compared to offline bilevel optimization and OAGD, new technical challenges need to be addressed here: (1) unlike the multi-step update of inner-level variable \(y_{t}\) in OAGD, the single-step update in SOBOW will lead to an inaccurate estimation of the optimal inner-level

  Algorithm & Single-loop update & Study estimation error of HV product & Do not require previous function info \\   OAGD  & ✗ & ✗ & ✗ \\  SOBOW (this paper) & ✓ & ✓ & ✓ \\  

Table 1: Comparison of OBO algorithms. ‘HV’ product refers to the Hessian inverse-vector product in hypergradient estimation. OAGD estimates the hypergradient by \(_{i=0}^{K-1}^{i}f_{t-i}(x_{t},y_{t+1})\), which requires the evaluation of \(f_{t-i}\) on current model \((x_{t},y_{t+1})\).

decision \(y_{t}^{*}(x_{t})\) and consequently a large estimation error for the hypergradient; this problem can be addressed in offline bilevel optimization by controlling the gap \(\|y_{t}^{*}(x_{t})-y_{t+1}^{*}(x_{t+1})\|^{2}\), which depends only on \(\|x_{t}-x_{t+1}\|^{2}\) for static inner-level functions. But this technique cannot be applied here due to the time-varying \(g_{t}\); (2) the function variations in OBO can blow up the impact of the hypergradient estimation error if not handled appropriately, whereas offline bilevel optimization does not have this issue due to the static functions therein. Towards this end, we appropriately control the estimation error of the Hessian inverse-vector product at each round, and disentangle the complex couplings between the decision variables through a three-level analysis. This enables the control of the hypergradient estimation error, by using a decaying coefficient to diminish the impact of the large inner-level estimation error and leveraging the historical information to smooth the update of the outer-level decision.

(_Extensive experimental evaluations_) OBO has the potential to be used in various online applications, by capturing the hierarchical structures therein in an online manner. In this work, the experimental results clearly validate the effectiveness of SOBOW in online hyperparameter optimization and online hyper-representation learning.

## 2 Related Work

_Online optimization_ Online optimization has been extensively studied for strongly convex and convex functions in terms of both static regret, e.g., [68; 21; 47] and dynamic regret, e.g., [5; 48; 61; 64; 66]. Recently, there has been increasing interest in studying online optimization for non-convex functions, e.g., [1; 58; 34; 51; 15], where minimizing the standard definitions of regret is computationally intractable. Specifically,  introduced a notion of local regret in the spirit of the optimality measure in non-convex optimization, and developed an algorithm that averages the gradients of the most recent loss functions evaluated in the current model.  proposed a dynamic local regret to handle the distribution shift and also a computationally efficient SGD update for achieving sublinear regret.  and  studied the zeroth-order online non-convex optimization where the agent only has access to the actual loss incurred at each round.

_Offline bilevel optimization_ Bilevel optimization was first introduced in the seminal work . Following this work, a number of algorithms have been developed to solve the bilevel optimization problem. Initially, the bilevel problem was reformulated into a single-level constrained problem based on the optimality conditions of the inner-level problem [20; 55; 43; 49], which typically involves many constraints and is difficult to implement in machine learning problems. Recently, gradient-based bilevel optimization algorithms have attracted much attention due to their simplicity and efficiency. These can be roughly classified into two categories, the approximate implicit differentiation (AID) based approach [10; 52; 16; 14; 17; 42; 32] and the iterative differentiation (ITD) based approach [45; 12; 53; 44; 17]. Bilevel optimization has also been studied very recently for the cases with stochastic objective functions [14; 31; 7; 33; 18] and multiple inner minima [35; 40; 39; 56]. Notably, a novel value-function based method was first proposed in  to deal with the non-convexity of the inner-level functions. Some recent studies, e.g., [36; 9; 26], have also explored single-level algorithms for offline bilevel optimization with time-invariant objective functions, whereas the time-varying functions in online bilevel optimization make the hypergradient estimation error control more challenging for single-loop updates.

_Online bilevel optimization_ The investigation of OBO is still in the very early stage, and to the best of our knowledge,  is the only work so far that has studied OBO.

## 3 Online Bilevel Optimization

Following the same spirit as the online optimization in , the decisions are made iteratively in OBO without knowing their outcomes at the time of decision-making. Let \(T\) denote the total number of rounds in OBO. Define \(x_{t}^{d_{1}}\) and \(f_{t}:^{d_{2}}\) as the decision variable and the online function for the outer level problem, respectively. Define \(y_{t}^{d_{2}}\) and \(g_{t}^{d_{2}}\) as the decision variable and the online objective function for the inner level problem, respectively. Given the initial values of \((x_{1},y_{1})\), the general procedure of OBO is described in Algorithm 1.

Let \(y_{t}^{*}(x)=_{y^{d_{2}}}g_{t}(x,y)\) for any \(x\). The OBO framework in Algorithm 1 can be interpreted from two different perspectives: (1) (_single-player_) The player makes the decision on \(x_{t}\) without knowing the optimal inner-level decision \(y_{t}^{*}(x)\). Note, \(y_{t}\) serves as an estimation of \(y_{t}^{*}(x)\) from the player's perspective based on her knowledge of function \(g_{t}\); (2) (_two-player_) OBO can also be viewed as a leader (\(x_{t}\)) and follower (\(y_{t}\)) game, where each player considers an online optimization problem and the leader seeks to play against the optimal decision \(y_{t}^{*}(x)\) of the follower at each round under limited knowledge of \(g_{t}\).

It is worthwhile noting that OBO is quite different from the single-level online optimization. First, the outer-level objective function with respect to (w.r.t) \(x_{t}\), i.e., \(f_{t}(x_{t},y_{t}^{*}(x_{t}))\), is not available to update \(x_{t}\), whereas, in standard single-level online optimization, the true loss is revealed immediately after making decisions. Besides, as a composite function of \(f_{t}(x,y)\) and \(y_{t}^{*}(x)\), \(f_{t}(x,y_{t}^{*}(x))\) is _non-convex_ in general w.r.t. the outer-level decision variable \(x\). Hence, standard regret definitions in online convex optimization  are not directly applicable here.

Motivated by the dynamic local regret defined in online non-convex optimization , we consider the following bilevel local regret:

\[BLR_{w}(T)=_{t=1}^{T}\| F_{t,}(x_{t},y_{t}^{*}(x_{t})) \|^{2}\] (2)

where

\[F_{t,}(x_{t},y_{t}^{*}(x_{t}))=_{i=1}^{K-1}^{ i}f_{t-i}(x_{t-i},y_{t-i}^{*}(x_{t-i})),\]

and \(W=_{i=0}^{K-1}^{i}\), \((0,1)\), and \(f_{t}(,)=0\) for \(t 0\). In contrast, the static regret in  evaluates the objective at time slot \(i\) using variable updates at different time slot \(j\), which does not properly characterize the online learning performance of the model update for time-varying functions (see Appendix for more discussion). Intuitively, the regret in Equation (2) is defined as a sliding average of the hypergradients w.r.t. the decision variables at the corresponding instant for all rounds in OBO. This indeed approximately computes the exponential average of the outer-level function values \(f_{t}(x_{t},y_{t}^{*}(x_{t}))\) at the corresponding decision variables over a sliding window . Larger weights will be assigned to more recent updates. The objective here is to design efficient OBO algorithms with sublinear bilevel regret \(BLR_{w}(T)\) in \(T\), which implies that the outer-level decision is becoming better and closer to the local optima for the outer-level optimization problem at each round. This gradient-norm based regret shares the same spirit as the first-order optimality criterion [14; 32], which is widely used in offline bilevel optimization to characterize the convergence to the local optima.

## 4 Algorithm Design

It is well known that online gradient descent (OGD)  has achieved great successes in single-level online optimization. On the other hand, gradient-based methods (e.g., [16; 14; 42; 40; 29]) have become extremely popular for solving offline bilevel optimization due to their high efficiency. Thus, we also study the online gradient descent based algorithm to solve the OBO problem. As mentioned earlier, the unique challenges of OBO should be carefully handled in the algorithm design, including 1) the inaccessibility of the objective function and accurate hypergradients compared to single-level online optimization and 2) the time-varying functions and limited information compared to offline bilevel optimization. To this end, our algorithm includes two major designs, i.e., efficient hypergradient estimation with limited information and window averaged outer-level decision update.

**Hypergradient estimation** In OBO, the exact hypergradient \( f_{t}(x_{t},y_{t}^{*}(x_{t}))\) w.r.t. \(x_{t}\) can be represented as

\[ f_{t}(x_{t},y_{t}^{*}(x_{t}))=_{x}f_{t}(x_{t},y_{t}^{*}(x_{t}))- _{x}_{y}g_{t}(x_{t},y_{t}^{*}(x_{t}))v_{t}^{*}\] (3)

where \(v_{t}^{*}\) solves the linear system \(_{y}^{2}g_{t}(x_{t},y_{t}^{*}(x_{t}))v=_{y}f_{t}(x_{t},y_{t}^{*}(x _{t}))\). The optimal inner-level decision \(y_{t}^{*}(x_{t})\) is generally unavailable in OBO. To estimate the hypergradient \( f_{t}(x_{t},y_{t}^{*}(x_{t}))\)the AID-based approach [10; 14; 32] for offline bilevel optimization can be leveraged here, which will involve the following steps: (1) given \(x_{t}\), run \(N\) steps of gradient descent w.r.t. the inner-level objective function \(g_{t}\) to find a good approximation \(y_{t}^{N}\) close to \(y_{t}^{*}(x_{t})\); (2) given \(y_{t}^{N}\), obtain \(v_{t}^{Q}\) by solving \(_{y}^{2}g_{t}(x_{t},y_{t}^{N})v=_{y}f_{t}(x_{t},y_{t}^{N})\) with \(Q_{t}\) steps of conjugate gradient. The estimated hypergradient is constructed as

\[f_{t}(x_{t},y_{t}^{N})= _{x}f_{t}(x_{t},y_{t}^{N})-_{x}_{y}g_{t}(x_{t}, y_{t}^{N})v_{t}^{Q}.\] (4)

Nevertheless, the \(N\) steps gradient descent for estimating \(y_{t}^{*}(x_{t})\) require multiple inquiries about the inner-level function \(g_{t}\), which can be inefficient and infeasible for online applications. For the algorithm being used in more practical scenarios with limited information about \(g_{t}\), we consider the extreme case where \(N=1\), i.e.,

\[y_{t+1}=y_{t}^{1}=y_{t}-_{y}g_{t}(x_{t},y_{t}),\] (5)

where \(\) is the inner-level step size. This would lead to an inaccurate estimation of \(y_{t}^{*}(x_{t})\), which can pose critical challenges for making satisfying outer-level decisions in OBO due to the unreliable hypergradient estimation, especially when the objective functions are time-varying.

**Window averaged decision update** To deal with the non-convex and time-varying functions, inspired by time-smoothed gradient descent in online non-convex optimization [23; 2; 67; 19], we consider a time-smoothed hypergradient descent for updating the outer-level decision variable \(x_{t}\):

\[x_{t+1}=_{}(x_{t}-F_{t,}(x_{t},y_{t+1}))\] (6)

where \(_{}\) is the projection onto the set \(\), \(\) is the outer-level step size and

\[F_{t,}(x_{t},y_{t+1})=_{i=1}^{K-1}^{i} f_{t-i}(x_{t-i},y_{t+1-i}).\] (7)

Here \(f_{t-i}(x_{t-i},y_{t+1-i})\) is the hypergradient estimation at the round \(t-i\), as in Equation (4). Intuitively, the update of the current \(x_{t}\) is smoothed by the historical hypergradient estimations w.r.t. the decision variables at that time, which is particularly important here for OBO due to the following reasons: (1) To compute the averaged \(F_{t,}(x_{t},y_{t+1})\) at each round \(t\), we only store the hypergradient estimation for previous rounds in the memory, i.e., store \(f_{i}(x_{i},y_{i+1})\) at each round \(i\), and estimate the current hypergradient \(f_{t}(x_{t},y_{t+1})\) using the available information at current round \(t\). Compared to OAGD in , there is no need to access to the previous outer-level and inner-level objective functions and evaluate the current decisions on those functions, which is clearly more efficient and practical for online applications. (2) Leveraging the historical information in the window to update the current decision is helpful to deal with the inaccurate hypergradient estimation for the current round, especially under mild function variations. This indeed shares the same rationale with using past tasks to facilitate forward knowledge transfer in online meta-learning  and continual learning , for better decision making in the current task and improving the overall performance in non-stationary environments.

Building upon these two major components, we can have our main OBO algorithm, Single-loop Online Bilevel Optimizer with Window averaging (SOBOW), as summarized in Algorithm 2. At the round \(t\), we first estimate \(y_{t+1}\) as in Equation (5) given \(x_{t}\) and \(y_{t}\), which will be next leveraged to solve the linear system and construct the hypergradient estimation as in Equation (4). Based on the historical hypergradient estimations stored in the memory for previous rounds, we next update \(x_{t}\) based on Equation (6).

## 5 Theoretical Analysis

In this section, we provide the theoretical analysis of the regret bound for SOBOW.

### Technical Assumptions

Let \(z=(x,y)\). Before the regret analysis, we first make the following assumptions.

**Assumption 5.1**.: The inner-level function \(g_{t}(x,y)\) is \(_{g}\)-strongly convex w.r.t. \(y\), and the composite objective function \(f_{t}(x,y_{t}^{*}(x))\) is non-convex w.r.t \(x\).

**Assumption 5.2**.: The following conditions hold for objective functions \(f_{t}(z)\) and \(g_{t}(z)\), \( t[1,T]\): (1) The function \(f_{t}(z)\) is \(L_{0}\)-Lipschitz continuous; (2) \( f_{t}(z)\) and \( g_{t}(z)\) are \(L_{1}\)-Lipschitz continuous; (3) The high-order derivatives \(_{x}_{y}g_{t}(z)\) and \(_{y}^{2}g_{t}(z)\) are \(L_{2}\)-Lipschitz continuous.

Note that both Assumption 5.1 and Assumption 5.2 are standard and widely used in the literature of bilevel optimization, e.g., [52; 14; 29; 59; 27].

**Assumption 5.3**.: For any \(t[1,T]\), the function \(f_{t}(x,y_{t}^{*}(x))\) is bounded, i.e., \(|f_{t}(x,y_{t}^{*}(x))| M\) with \(M>0\). Besides, the closed convex set \(\) is bounded, i.e., \(\|x-x^{}\| D\) with \(D>0\), for any \(x\) and \(x^{}\) in \(\).

Assumption 5.3 on the boundedness of the objection functions is also standard in the literature of non-convex optimization, e.g., [23; 2; 50; 59]. Moreover, to guarantee the boundedness of the hypergradient estimation error, previous studies (e.g., [14; 28; 17]) in offline bilevel optimization usually assume that the gradient norm \(\| f(z)\|\) is bounded from above for all \(z\). In this work, we make a weaker assumption on the feasibility of \(_{y}f_{t}(x,y_{t}^{*}(x))\), which generally holds since \(y_{t}^{*}(x)\) is usually assumed to be bounded in bilevel optimization:

**Assumption 5.4**.: There exists at least one \(\) such that \(\|_{y}f_{t}(,y_{t}^{*}())\|\) where \(>0\) is some constant.

### Theoretical Results

**Technical challenges in analysis:** To analyze the regret performance of SOBOW, several key and unique technical challenges need to be addressed, compared to offline bilevel optimization and OAGD in : (1) unlike the multi-step update of inner-level variable \(y_{t}\) in OAGD, the single-step update in SOBOW will lead to an inaccurate estimation of \(y_{t}^{*}(x_{t})\) and consequently a large estimation error for the hypergradient \( f_{t}(x_{t},y_{t}^{*}(x_{t}))\); this problem can be addressed in offline bilevel optimization by controlling the gap \(\|y_{t}^{*}(x_{t})-y_{t+1}^{*}(x_{t+1})\|^{2}\), which depends only on \(\|x_{t}-x_{t+1}\|^{2}\) for static inner-level functions, but this technique cannot be applied here due to the time-varying \(g_{t}\); (2) the function variations in OBO can blow up the impact of the hypergradient estimation error if not handled appropriately, whereas offline bilevel optimization does not have this issue due to the static functions therein; (3) a new three-level analysis is required to understand the involved couplings among the estimation errors about \(v_{t}\), \(y_{t}\) and \(x_{t}\) in online learning.

Towards this end, the very first step is to understand the estimation error of the optimal solution \(v_{t}^{*}\) to the linear system \(_{y}^{2}g_{t}(x_{t},y_{t}^{*}(x_{t}))v=_{y}f_{t}(x_{t},y_{t}^{*}( x_{t}))\). Here \(v_{t}^{*}=(_{y}^{2}g_{t}(x_{t},y_{t}^{*}(x_{t})))^{-1}_{y}f_{t}(x _{t},y_{t}^{*}(x_{t}))\). We have the following lemma about \(\|v_{t}^{Q}-v_{t}^{*}\|^{2}\), where \(v_{t}^{Q}\) is obtained by solving \(_{y}^{2}g_{t}(x_{t},y_{t+1})v=_{y}f_{t}(x_{t},y_{t+1})\) using \(Q_{t}\) steps of conjugate gradient.

**Lemma 5.5**.: _Suppose Assumptions 5.1-5.4 hold, \(}\), \(}\) and \(Q_{t+1}-Q_{t}}{2})}{2(1-_{ })}\). We can have that_

\[\|v_{t}^{Q}-v_{t}^{*}\|^{2} c_{2}\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}+_ {t}^{2}\]

_where \(c_{2}>0\) is some constant and the error \(_{t}^{2}\) decays with \(t\), i.e., \(_{t+1}^{2}(1-_{}/2)_{t}^{2}\)._

Lemma 5.5 characterizes the estimation error \(\|v_{t}^{Q}-v_{t}^{*}\|^{2}\) in a neat way, by constructing an upper bound with the estimation error of the inner-level optimal decision \(y_{t}^{*}(x_{t})\), i.e., \(\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}\), and a decaying error term \(_{t}^{2}\). The way of controlling \(\|v_{t}^{Q}-v_{t}^{*}\|^{2}\) here is particularly important, which not only clarifies the coupling between \(v_{t}\) and \(y_{t}\) but also helps to control the hypergradient estimation error. Note that solving the linear system with a larger \(Q_{t}\) does not require more information about the inner-level function, and the introduced computation cost can be negligible, because the conjugate gradient only involves Hessian-vector product which can be efficiently computed.

Next we seek to bound the hypergradient estimation error \(\| f_{t}(x_{t},y_{t}^{*}(x_{t}))-f_{t}(x_{t},y_{t+1})\|^ {2}\) at the round \(t\). Intuitively, the hypergradient estimation error depends on both \(\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}\) and \(\|v_{t}^{Q}-v_{t}^{*}\|^{2}\). Building upon Lemma 5.5, this dependence can be shifted to the joint error of \(\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}\) and \(_{t}^{2}\), which contains iteratively decreasing components after careful manipulations. Specifically, let \(G_{1}=1+c_{2}+^{2}(_{y}+DL_{1}^{2}+DL_{1}_{})^{2}}{ L_{1}^{2}_{}^{2}}\) and \(G_{2}=2G_{1}(1+})(1-_{g})\). We have the following theorem to characterize the hypergradient estimation error.

**Theorem 5.6**.: _Suppose that Assumptions 5.1-5.4 hold, \(}\), \(}\) and \(Q_{t}-Q_{t-1}}{2})}{2(1-_{g})}\). For \(t[2,T]\), we can bound the hypergradient estimation error as follows:_

\[\| f_{t}(x_{t},y_{t}^{*}(x_{t})) -f_{t}(x_{t},y_{t+1})\|^{2} 3L_{1}^{2} ^{2}G_{2}}{_{g}^{2}}_{j=0}^{t-2}(1-}{2})^{j}\|x_{t-1-j}-x_{t-j}\|^{2}\] \[+G_{2}_{j=0}^{t-2}(1-}{2})^{j} \|y_{t-1-j}^{*}(x_{t-1-j})-y_{t-j}^{*}(x_{t-1-j})\|^{2}+(1-}{2})^{t-1}_{1}}\]

_where \(_{1}=G_{1}\|y_{2}-y_{1}^{*}(x_{1})\|^{2}+_{1}^{2}\)._

As shown in Theorem 5.6, the upper bound of the hypergradient estimation error includes three terms: (1) The first term decays with \(t\), which captures the iteratively decreasing component in the joint error of \(\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}\) and \(_{t}^{2}\); (2) The second term characterizes the dependence on the variation of the outer-level decision between adjacent rounds in the history; (3) The third term characterizes the dependence on the variation of the optimal inner-level decision between adjacent rounds.

To control the hypergradient estimation error as in Theorem 5.6, the key idea is to decouple the source of the estimation error \(\|y_{t+1}-y_{t}^{*}(x_{t})\|^{2}\) at the current round into three different components, i.e., \(\|y_{t}-y_{t-1}^{*}(x_{t-1})\|^{2}\) for the previous round, the variation of the out-level decision, and the variation of the optimal inner-level decision. Since the inner-level estimation error is large due to the single-step update, we diminish its impact through a decaying coefficient, which inevitably enlarges the impact of the other two components. The variation of the optimal inner-level decision is due to nature of the OBO problem, which cannot be controlled. One has to impose some regularity constraints on this variation to achieve a sublinear regret, in the same spirit to the regularities on functional variations widely used in the dynamic regret literature (e.g., ). Therefore, the key point now becomes the control of the variation of the out-level decision \(\|x_{t-1-j}-x_{t-j}\|^{2}\), which can be achieved through the window averaged update of \(x_{t}\) in Equation (6). Intuitively, by leveraging the historical information, the window averaged hypergradient in Equation (7) smooths the outer-level decision update, which serves as a better update direction compared to the deviated single-round estimation \(f_{t}(x_{t},y_{t+1})\). Before presenting the main result, we first introduce the following definitions to characterize the variations of the objective function \(f_{t}(,y_{t}^{*}())\) and the optimal inner-level decision \(y_{t}^{*}()\) in OBO, respectively:

\[V_{1,T}=_{t=1}^{T}_{x}[f_{t+1}(x,y_{t+1}^{*}(x))-f_{t}(x,y_{t}^{*}(x))],\;\;H_{2,T}=_{t=2}^{T}_{x}\|y_{t-1}^{*}(x)-y_{t}^{*}(x)\|^{2}.\]

Intuitively, \(V_{1,T}\) measures the overall fluctuations between the adjacent objective functions in all rounds under the same outer-level decision variable, and \(H_{2,T}\) can be regarded as the inner-level path length to capture the variation of the optimal inner-level decisions as in . Note that \(V_{1,T}\) is a weaker regularity for the functional variation compared to absolute values used in single-level online optimization for dynamic regret . When the functions are static, these variations terms are simply 0. We are interested in the case where both \(V_{1,T}\) and \(H_{2,T}\) are \(o(T)\) as in the literature of dynamic regret.

Based on Theorem 5.6, we can have the following theorem to characterize the regret of SOBOW.

**Theorem 5.7**.: _Suppose that Assumptions 5.1-5.4 hold. Let \(}\), \(}\), \(Q_{t+1}-Q_{t}}{2})}{2(1-_{g})}\), \((1-}{2},1)\), and \(\{},^{2}L_{f}W(1-)(-1+ _{g}/2)}{24L_{1}^{2}G_{2}}\}\) where \(L_{f}\) is the smoothness parameter of the function \(f_{t}(,y_{t}^{*}())\). Then we can have_

\[BLR_{w}(T) O(+}{}+H_{2,T}).\]

The value of \(L_{f}\) can be found in the proof in Appendix. Note that the hypergradient estimation at the current round depends on all previous outer-level decisions as shown in Theorem 5.6. While these decisions may not be good at the early stage in OBO, choosing \((1-}{2},1)\) would diminish their impact on the local regret. When the variations \(V_{1,T}\) and \(H_{2,T}\) are both \(o(T)\), a sublinear bilevel local regret can be achieved for an appropriately selected window, e.g., \(W=o(T)\) when \(=1-h(T)\), where \(h(T) 0\) as \(T\). Note that the value \(\) does not change substantially since \(W(1-)\) converges to \(1\). Particularly, when \(=1-o()\), \(W=(T)\). In this case, we can have the smallest regret \(O(}{}+H_{2,T})\) that only depends on the function variations in OBO.

## 6 Experiments

In this section, we conduct experiments in multiple domains to corroborate the utility of the OBO framework and the effectiveness of SOBOW.

Specifically, we compare our algorithm SOBOW with the following baseline methods: (1) **OAGD**, which is the only method for OBO in the literature; (2) **OGD**, a natural method which updates the outer-level decision by using the current hypergradient estimation only without any window averaging. Intuitively, _OGD is not only a special case of OAGD when the information of previous functions is not available_, but also a direct application of offline bilevel optimization, e.g., AID-based method . We also denote SOBOW-\(K\)/OAGD-\(K\) as SOBOW and OAGD with window size \(K\), respectively. And we evaluate the regret using the definition in Equation (2). We also compare the performance using the regret in  in Appendix where similar results can be observed.

**Online Hyper-representation Learning** Representation learning [13; 17] seeks to extract good representations of the data. The learnt representation mapping can be used in downstream tasks to facilitate the learning of task specific model parameters. This formulation is typically encountered in a multi-task setup, where \(\) captures the common representation extracted for multiple tasks and \(w\) defines the task-specific model parameters. When the data/task arrives in an online manner, the hyper-representation needs to be continuously adapted to incorporate the new knowledge.

Following , we study online hyper-representation learning (Online HR) with linear models. Specifically, at each round \(t\), the agent applies the hyper-representation \(_{t}^{p d}\) and the linear model prediction \(w_{t}^{d}\), and then receives small minibatches \((X_{t}^{f},Y_{t}^{f})\) and \((X_{t}^{g},Y_{t}^{g})\). Based on \(_{t}\) and \((X_{t}^{g},Y_{t}^{g})\), the agent updates her linear model prediction \(w_{t+1}\) as an estimation of \(w^{*}(_{t})=_{w^{d}}\ g_{t}(_{t},w) \|X_{t}^{g}_{t}w-Y_{t}^{g}\|^{2}+\|w\|^{2}\). Based on the estimation \(w_{t+1}\) and \((X_{t}^{f},Y_{t}^{f})\), the agent further updates her decision \(_{t+1}\) about the hyper-representation to minimize the loss \(f_{t}(,w_{t}^{*}())\|X_{t}^{f} w_{t}^{*}( )-Y_{t}^{f}\|^{2}\). In our experiments, we consider synthetic data generated as in  and explore two distinct settings: (i) a static setup where the underlying model generating the minibatches is fixed; and (ii) a staged dynamic setup where the model changes after some steps.

As shown in Figure 1(a) and Figure 1(b), SOBOW achieves comparable regret with OAGD in both static and dynamic setups, without the need of knowing previous functions. In terms of the running

Figure 1: Evaluation for online HR. As shown in subfigures (a) and (b), SOBOW performs similarly to OAGD and significantly outperforms OGD in both static and dynamic setups. Subfigure (c) shows the performance of SOBOW under different values of the averaging parameter \(\) for online HR. Better performance is achieved as \( 1\). Subfigure (d) shows the performance of SOBOW under different values of inner steps \(N\) when the data stream contains two data points (one in \(X_{t}^{g}\) and one in \(X_{t}^{f}\)). The performance saturates at \(N=2\).

time for 5000 steps with \(K=50\), SOBOW takes 11 seconds, OAGD takes 228 seconds and OGD takes 7 seconds. Therefore, SOBOW is much more computationally efficient compared to OAGD, because SOBOW does not need to re-evaluate the previous functions on the current model at each round. On the other hand, _SOBOW performs substantially better than OGD (i.e., OAGD when previous functions are not available) with similar running time_. These results not only demonstrate the usefulness of SOBOW when the previous functions are not available, but also corroborate the benefit of window-averaged outer-level decision update by leveraging the historical hypergradient estimations in OBO. Figure 1(c) shows the performance of SOBOW under different values of the averaging parameter \(\). The performance is better as \( 1\), which is also consistent with our theoretical results. Figure 1(d) indicates that a small number of updates for the inner-level variable is indeed enough for online HR.

**Online Hyperparameter Optimization** The goal of hyperparameter optimization (HO) [13; 17] is to search for the best values of hyperparameters \(\), which seeks to minimize the validation loss of the learnt model parameters \(w\) and is usually done offline. However, in online applications where the data distribution can dynamically change, e.g., the unusual traffic patterns in online traffic time series prediction problem , keeping the hyperparameters static could lead to sub-optimal performance. Therefore, the hyperparameters should be continuously updated together with the model parameters in an online manner.

Specifically, at each online round \(t\), the agent applies the hyperparameters \(_{t}\) and the model \(w_{t}\), and then receives a small dataset \(_{t}=\{_{t}^{},_{t}^{}\}\) composed of a training subset \(_{t}^{}\) and a validation subset \(_{t}^{}\). Based on \(_{t}\) and \(_{t}^{}\), the agent first updates her model prediction \(w_{t+1}\) as an estimation of \(w_{t}^{*}(_{t})_{w}\,_{t}^{}( _{t},w)\), where \(_{t}^{}(,w):=_{t}^{}|}_{_{t}^{}}(w;)+ (,w)\), \((w,)\) is a cost function computed on data point \(\) with prediction model \(w\), and \((w,)\) is a regularizer. Based on the model prediction \(w_{t+1}\) and \(_{t}^{}\), the agent updates the hyperparameters \(_{t+1}\) to minimize the validation loss \(_{t}^{}(,w_{t}^{*}()) _{t}^{}|}_{_{t}^{}}(w_{t}^{*}();)\).

We consider an online classification setting on the 20 Newsgroup dataset, where the classifier is modeled by an affine transformation and we use the cross-entropy loss as the losscost function. For \((,w)\), we use one \(_{2}\)-regularization parameter for each row of the transformation matrix in \(w\), so that we have one regularization parameter for each data feature (i.e., \(||\) is given by the dimension of the data). We remove all news headers in the 20 Newsgroup dataset and pre-process the dataset so as to have data feature vectors of dimension \(d=99238\). In our implementations, we approximate the hypergradient using implicit differentiation with the fixed point method . We consider two different setups: (i) a static setup where the agent receives a stream of clean data batches \(\{_{t}\}_{t}\); (ii) a dynamic setting in which the agent receives a stream of corrupted batches \(\{_{t}\}_{t}\), where the corruption level changes after some time steps. For both setups the batchsize is fixed to \(16\). For the dynamic setting we consider four different corruption levels \(\{5\%,10\%,20\%,30\%\}\) and also optimize the learning rate as an additional hyperparameter.

We evaluate the testing accuracy for SOBOW and OAGD in Table 2 for both static (Left) and dynamic (Right) setups. It can be seen that compared to OAGD, SOBOW achieves similar accuracy but with a much shorter running time. When the window size increases in Table 2, performance of both SOBOW and OAGD increases and the computational advantage of SOBOW becomes more significant. In particular, SOBOW runs around \(20\) times faster than OAGD when the window size is 50.

## 7 Conclusions and Discussion

In this work, we study non-convex bilevel optimization where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. We proposed a single-loop

  Method & Accuracy (\%) & Test Loss & Time (s) \\   SOBOW-4 & \(65.87\) & \(1.287\) & 899 \\  OAGD-4 & \(65.96\) & \(1.285\) & 2304 \\   SOBOW-50 & \(66.32\) & \(1.28\) & 1188 \\  OAGD-50 & \(66.44\) & \(1.273\) & 20161 \\   
  Method & End 20\% stream & End 30\% stream & Time (s) \\   SOBOW-4 & \(58.39\) & \(59.70\) & 1198 \\  OAGD-4 & \(62.61\) & \(59.26\) & 3072 \\  

Table 2: **Left:** Comparison for static online HO. We report accuracy and loss on a separate test split after 12000 steps. **Right:** Comparison for dynamic online HO. We report accuracy on a separate test split at the end of stream with corruption level 20% and 30%. Each level lasts for 4000 steps.

online bilevel optimizer with window averaging (SOBOW) to handle the function variations and the unavailability of the true hypergradients in OBO. Compared to existing algorithms, SOBOW is computationally efficient and does not require previous function information. We next developed a novel analytical technique to tackle the unique challenges in OBO and showed that SOBOW can achieve a sublinear bilevel local regret. Extensive experiments justified the effectiveness of SOBOW. We also discuss the potential applications of the OBO framework in online meta-learning and online adversarial training (see Appendix).

Limitation and future directionsThe study of online bilevel optimization is still in a very early stage, and much of this new framework still remains under-explored and not well understood. We started with the second-order approach for hypergradient estimation, which is less scalable. One future direction is to leverage the recently developed first order approaches for hypergradient estimation. Another limitation is that we assume that the inner-level objective function is strongly convex. In the future, we will investigate the convex and even non-convex case.