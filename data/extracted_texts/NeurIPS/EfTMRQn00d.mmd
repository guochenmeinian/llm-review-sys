# Streamer: Streaming Representation Learning and Event Segmentation in a Hierarchical Manner

Ramy Mounir  Sujal Vijayaraghavan  Sudeep Sarkar

Department of Computer Science and Engineering, University of South Florida, Tampa

{ramy, sujal, sarkar}@usf.edu

###### Abstract

We present a novel self-supervised approach for hierarchical representation learning and segmentation of perceptual inputs in a streaming fashion. Our research addresses how to semantically group streaming inputs into chunks at various levels of a hierarchy while simultaneously learning, for each chunk, robust global representations throughout the domain. To achieve this, we propose STREAMER, an architecture that is trained layer-by-layer, adapting to the complexity of the input domain. In our approach, each layer is trained with two primary objectives: making accurate predictions into the future and providing necessary information to other levels for achieving the same objective. The event hierarchy is constructed by detecting prediction error peaks at different levels, where a detected boundary triggers a bottom-up information flow. At an event boundary, the encoded representation of inputs at one layer becomes the input to a higher-level layer. Additionally, we design a communication module that facilitates top-down and bottom-up exchange of information during the prediction process. Notably, our model is fully self-supervised and trained in a streaming manner, enabling a single pass on the training data. This means that the model encounters each input only once and does not store the data. We evaluate the performance of our model on the egocentric EPIC-KITCHENS dataset, specifically focusing on temporal event segmentation. Furthermore, we conduct event retrieval experiments using the learned representations to demonstrate the high quality of our video event representations. Illustration videos and code are available on our project page: https://ramymounir.com/publications/streamer.

## 1 Computational theory

In temporal event analysis, an event is defined as "a segment in time that is perceived by an observer to have a _beginning_ and an _end_" . Events could be described by a sequence of constituent events of relatively finer detail, thus forming a hierarchical structure. The end of an event and the beginning of the next is a _segmentation boundary_, marking an event transition. Segmentation boundaries in the lower levels of the hierarchy represent event transitions at relatively granular scales, whereas boundaries in higher levels denote higher-level event transitions.

We propose a structurally self-evolving model to learn the hierarchical representation of such events in a self-supervised streaming fashion through predictive learning. Structural evolution refers to the model's capability to create learnable layers _ad hoc_ during training. One may argue that existing deep learning architectures are compositional in nature, where high-level features are composed of lower-level features, forming a hierarchy of features. However, it is important to distinguish between a feature hierarchy and an event hierarchy: an event hierarchy is similar to a part/whole hierarchy in the sense that each event has clear boundaries that reflect the beginning and the end of a coherent chunk of information. One may also view the hierarchy as a redundancy pooling mechanism, whereinformation grouped as one event is considered redundant for a higher level and can be summarized into a single representation for higher-level processing.

Our model is capable of generating a hierarchy of event segments (Figure 1) by learning unique semantic representations for each event type directly from video frames. This is achieved through predictive learning, which models the causal structure of events. These learned representations are expressive enough to enable video snippet retrieval across videos. Each level in the hierarchy selectively groups inputs from the level below to form coherent event representations, which are then sent to the level above. As a result, the hierarchy exhibits temporally aligned boundaries, with each level containing a subset of the boundaries detected in the lower level.

As often prescribed [28; 24], we impose the following biologically-plausible constraints on our learning algorithm:

1. The learning algorithm should be **continuous and online**. Most existing learning algorithms offer batch-based offline learning. However, the learning in the neocortex occurs continuously in a streaming fashion while seeing each datapoint only once
2. The learning should involve the ability to make **high-order predictions** by "incorporating contextual information from the past. The network needs to dynamically determine how much temporal context is needed to make the best predictions"  (Section 1.1)
3. Learning algorithms should be **self-supervised** and should not assume labels for training ; instead, they should be able to figure out the learning objective from patterns and causal structures within the data
4. The learning should stem from a **universal general-purpose algorithm**. This is supported by observations of the brain circuitry showing that all neocortical regions are doing the same task in a repeated structure of cells . Therefore, there should be no need for a global loss function (_i.e._, end-to-end training with high-level labels); local learning rules should suffice (Section 2)

### Predictive learning

Predictive learning refers to the brain's ability to generate predictions about future events based on past experiences. It is a fundamental process in human cognition that guides perception, action, and thought [58; 31]. The discrepancy between the brain's predictions and the observed perceptual

Figure 1: Comparison of STREAMR’s hierarchical output to single-level ground truth annotations from EPIC-KITCHENS. The ground truth contains redundant narrations for successive annotations (_e.g._, _add chicken_\(\), \(\)); STREAMER identifies such instances as a single high level event (\(\)). (Narrations from ground truth)

inputs forms a useful training signal for optimizing cortical functions: if a model can predict into the future, it implies that it has learned the underlying causal structure of the surrounding environment. Theories of cognition hypothesize that the brain only extracts and selects features from the previous context that help in minimizing future prediction errors, thus making the sensory cortex optimized for prediction of future input . A measure of intelligence can be formulated as the ability of a model to generate accurate, long-range future prediction .To this end, we design an architecture with the main goal of minimizing the prediction error, also referred to as maximizing the _model evidence_ in Bayesian inference according to the free energy principle [19; 18].

Event segmentation theory (EST) suggests that desirable properties such as event segmentation emerge as a byproduct of minimizing the prediction loss . Humans are capable of _chunking_ streaming perceptual inputs into events (and _chunking_ spatial regions into objects ) to allow for memory consolidation and event retrieval for better future predictions. EST breaks down streaming sensory input into chunks by detecting event boundaries as transient peaks in the prediction error. The detected boundaries trigger a process of transitioning (_i.e._, shifting) to a new event model whereby the current event model is saved in the event schemata, and a different event model is retrieved, or a new one initialized to better explain the new observations. One challenge in implementing a computational model of EST is encoding long-range dependencies from the previous context to allow for contextualized representations and accurate predictions. To address this challenge, we construct a hierarchy of event models operating at different time-scales, predicting future events with varying granularity. This hierarchical structure enables the prediction function at any layer to extract context information dynamically from any other layer, enhancing prediction during inference (learning constraint 2). Recent approaches [42; 40; 41; 55] inspired by EST have focused on event boundary detection using predictive learning. However, these methods typically train a single level and do not support higher-order predictions.

### Hierarchical event models

A single-level predictive model considers events that occur only at a single level of granularity rendering them unable to encode long-range, higher-order causal relationships in complex events. Conversely, a high-level representation does not contain the level of detail needed for accurately predicting low-level actions; it only encodes a high-level conceptual understanding of the action. Therefore, a hierarchy of event models is necessary to make predictions accurately at different levels of granularity [37; 26]. It is necessary to continuously predict future events at different levels of granularity, where low-level event models encode highly detailed information to perform short-term prediction and high-level event models encode conceptual low-detail features to perform long-term prediction.

EST identifies event boundaries based on transient peaks in the prediction error. To learn a hierarchical structure, we extend EST: we use event models at the boundaries in a layer as inputs to the layer above. The prediction error of each layer determines event demarcation, regulating the number of inputs pooled and sent to the layer above. This enables dynamic access to long-range context for short-term prediction, as required. This setup results in stacked predictive layers that perform the same prediction process with varying timescales subjective to their internal representations.

### Cross-layer communication

As noted in Section 1.2, coarsely detailed long-range contexts come from higher layers (the top-left block of Figure 2), and highly detailed short-range contexts come from lower layers (the bottom-left block of Figure 2), both of which are crucial to predict future events accurately. Therefore, the prediction at each layer should be conditioned upon its own representation and those of the other layers (Equation (2)). These two types of contexts can be derived by minimizing the prediction error at different layers. Hence, making perfect predictions is not the primary goal of this model but rather continuously improving its overall predictive capability.

#### 1.3.1 Contextualized inference

A major challenge in current architectures is modeling long-range temporal dependencies between inputs. Most research has focused on modifying recurrent networks [27; 10] or extending the sequence length of transformers [11; 57] to mitigate the problem of vanishing and exploding gradientsin long-range backpropagation through time . Instead, we solve this problem by allowing the multi-level context representations to be shared across layers during inference. It is worth noting that this type of inference is rarely used in typical deep learning paradigms, where the top-down influence only comes from backpropagating a supervised loss signal (_i.e._, top-down optimization). Biologically-inspired architectures such as PredNet  utilize top-down inference connections to improve low-level predictions (_i.e._, frames); however, these predictive coding architectures send the prediction error signal from each low-level observation (_i.e._, each frame) to higher levels which prevents the network from explicitly building hierarchical levels with varying degrees of context granularity.

#### 1.3.2 Contextualized optimization

Contextualized inference improves prediction, which is crucial for event boundary detection. However, we also aim to learn rich, meaningful representations. In Section 1, we noted that a 'parent' event could consist of multiple interchangeable low-level events. For instance, making a sandwich can involve spreading butter or adding cheese. From a high-level, using either ingredient amounts to the same parent event: "making a sandwich". Despite their visual differences, the prediction network must embed meaning and learn semantic similarities between these low-level events (_i.e._, spreading butter and adding cheese).

We implement this through "contextualized optimization" of events (Section 2.2), where each layer aligns the input representations from the lower level to minimize its own prediction loss using its context. It must be noted that the contextualization from higher layers (Figure 2, bottom-right) is balanced by the predictive inference at the lower levels (Figure 2, top-right), which visually distinguishes the interchangeable events. This balance of optimization embeds meaningful representations into the distinct low-level representations without collapsing the model. These representations can also be utilized for event retrieval at different hierarchical levels (Figure 5). Unlike other representation learning frameworks that employ techniques like exponential moving average (EMA) or asymmetric branches to prevent model collapse [7; 21; 9], we ensure that higher layers remain grounded in predicting lower-level inputs through bottom-up optimization. 1

## 2 Algorithm

Our goal is to incrementally build a stack of identical layers over the course of the learning, where each layer communicates with the layers above and below it. The layers are created as needed and are trained to function at different timescales; the output events from layer \(l\) become the inputs to the layer \((l+1)\), as illustrated in Figure 3. We describe the model and its connections for a single layer \(l\)

Figure 2: Given a stream of inputs at any layer, our model combines them and generates a bottleneck representation, which becomes the input to the level above it. The cross-layer communication could be broken down into top-down and bottom-up contextualized inference (left) and optimization (right).

but the same structure applies to all the layers in the model (learning constraint 4). In what follows, we describe the design of a mathematical model for a single predictive layer that is capable of (1) encoding temporal input into unique semantic representations (the _event model_) contextualized by previous events, (2) predicting the location of event boundaries (event demarcation), and (3) allowing for communication with other existing layers in the prediction stack to minimize its own prediction loss.

### Temporal encoding

Let \(^{(l)}=\{_{(t-m)}^{(l)},,_{t}^{(l)}\}\) be a set of \(m\) inputs to a layer \(l\) at discrete time steps in the range \((t-m,t]\) where each input \(_{i}^{d}\). First, we aim to generate an "_event model_"2\(^{(l)}\) which is a single bottleneck representation of the given inputs \(^{(l)}\). To accomplish this, we define a function \(f^{(l)}:^{m d}^{d}\) with temporally shared learnable weights \(^{(l)}\) to evaluate the importance of each input in \(^{(l)}\) for solving the prediction task at hand, as expressed in Equation (1).

\[^{(l)}=f^{(l)}(^{(l)};^{(l)})\] (1)

This event model will be trained to extract information from \(^{(l)}\) that is helpful for hierarchical prediction. Ideally, the bottleneck representation should encode top-down semantics, which allow for event retrieval and a bottom-up interpretation of the input to minimize the prediction loss of the following input. The following subsection describes the learning objective to accomplish this encoding task.

### Temporal prediction

At the core of our architecture is the prediction block, which serves two purposes: event demarcation and cross-layer communication. As previously mentioned, our architecture is built on the premise that minimizing the prediction loss is the only needed objective function for hierarchical event segmentation and representation learning.

Cross-layer communicationallows the representation \(^{(l)}\) to utilize information from higher \(\{^{(l+1)},,^{(L)}\}\) and lower layers \(\{^{(1)},,^{(l-1)}\}\) when predicting the next input at layer \(l\)

Figure 3: A diagram illustrating information flow across stacked identical layers. Each layer compares its prediction \(}_{t}\) with the input \(_{t}\) received from the layer below. If the prediction error \(_{}\) is over a threshold \(_{t-1}\), the current representation \(_{t-1}\) becomes the input to the layer above, and the working set is reset with \(_{t}\); otherwise, \(_{t}\) is appended to the working set \(_{t}\)

where \(L\) is the total number of layers. Let \(_{t}=\{_{t}^{(1)},,_{t}^{(L)}\}\) be a set of event models where each element is the output of the temporal encoding function \(f\) at its corresponding layer as expressed in Equation (1). Note that the same time variable \(t\) is used for representation across layers for simplicity; however, each layer operates in its own subjective timescale. Let \(p^{(l)}:^{L d}^{d}\) be a function of \(\) to predict the next input at layer \(l\) as expressed in Equation (2)

\[}_{t+1}^{(l)}=p^{(l)}(_{t};^{(l)})\] (2)

where \(^{(l)}\) denotes the learnable parameters of the predictor at layer \(l\). The difference between the layer's prediction \(}_{t+1}^{(l)}\) and the actual input \(_{t+1}^{(l)}\) is minimized, allowing the gradients to flow back into the \(f^{()}\) functions to modify each layer's representation as expressed in Equation (3).

\[_{}(}_{t+1}^{(l)}, _{t+1}^{(l)})&=}_{t+1}^{(l)}_{t+1}^{(l )}\\ ^{*(i)},^{*(l)}&* {arg\,min}_{^{(i)},^{(l)}}_{}(}_{t+1}^{(l)},_{t+1}^{(l)})\;i\{1 L\}\] (3)

The symbol \(\) represents an appropriate distance measure between two vectors.

Event demarcationis the process of detecting event boundaries by using the prediction loss, \(_{}\), from Equation (3). As noted earlier, according to EST, when a boundary is detected, an event model transition occurs, and a new event model is used to explain the previously unpredictable observations. Instead of saving the event model to the event schemata at boundary locations as described in EST, we use it as a detached input (denoted by \([]\)) to train the predictive model of the layer above it (_i.e.,_\(^{(l+1)}[^{(l)}]\)). We compute the running average of the prediction loss with a window of size \(w\), expressed by Equation (5), and assume that a boundary is detected when the new prediction loss is higher than the smoothed prediction loss, as expressed by the decision function in Equation (4).

\[(_{t}^{(l)};_{t-1}^{(l)})=1&_{ }(}_{t}^{(l)},_{t}^{(l)})>_{t-1}^{(l)}\\ 0&\] (4)

where the running average is given by

\[_{t}^{(l)}=_{i=t-w}^{t}_{}(}_{i}^{(l)},_{i}^{(l)})\] (5)

### Hierarchical gradient normalization

It is necessary to scale the gradients differently from conventional gradient updates because of the hierarchical nature of the model and its learning based on dynamic temporal contexts. There are three variables influencing the amount of accumulation of gradients:

1. The **relative timescale** between each layer is determined by the number of inputs. For instance, let the event encoder in layer \((l-2)\) have seen \(a=|^{(l-2)}|\) inputs, that at layer \((l-1)\) have seen \(b=|^{(l-1)}|\) inputs, and that at \(l\) have seen \(c=|^{(l)}|\) inputs. Then the input to layer \(l\) is a result of seeing a total of \((a b c)\). This term can then be used to scale up the learning at any level \(l\), expressed as \(_{i=1}^{l}|^{(i)}|\).
2. The **reach of influence** of each level's representation on a given level's encoder is influenced by its distance from another. For instance, if the input to \(f^{(l)}\) comes from the levels \(\{l+2,l+1,l,l-1,l-2\}\), then the weight of learning should be centered at \(l\) and diminish as the distance increases. Such a weight at any level \(l\) is given by \(^{-|l-r|}\,\,r\{-2,-1,0,1,2\}\). To ensure that the learning values sum up to 1 when this scaling is applied, the weights are normalized to add up to 1 as \(}{_{i=1}^{l}^{-|l-i|}}\).
3. The encoder receives **accumulated feedback** from predictors of all the layers; therefore the change in prediction loss with respect to encoder parameters in any given layer should be normalized by the total number of layers, given by \(\).

The temporal encoding model can be learned by scaling its gradients as expressed by the scaled Jacobian \(^{}_{}()\) in Equation (6).

\[^{}_{}()=_{}()=c_{1,1}^{(1)}}{ ^{(1)}}&&c_{1,L}^{(1)}}{^{(L)} }\\ &&\\ c_{L,1}^{(L)}}{^{(1)}}&&c_{L,L} ^{(L)}}{^{(L)}}\] (6)

where

\[c_{l,r}=}_{}}{_{i=1}^{L}^{-|l-i|}}}_{}^{l}|^{(i)}|}_{}\] (7)

Similarly, the temporal prediction model's gradients are controlled with scaling factors as expressed in Equation (8).

\[^{}_{}()=_{}( {})=s_{1}^{(1)}}{ ^{(1)}}&&s_{L}^{(L)}}{^{(L)}} \] (8)

where

\[s_{l}=^{L}^{-|l-i|}}}_{}^{l}|^{(i)}|}_{}\] (9)

## 3 Implementation

### Training details

We resize video frames to \(128 128 3\) and use a 4-layer CNN autoencoder (only for the first level) to project every frame to a single feature vector of dimension \(1024\) for temporal processing. For predictive-based models (STREAMER and LSTM+AL), we sample frames at 2 fps, whereas for clustering-based models, we use a higher sampling rate (5 fps) to reduce noise during clustering. We use cosine similarity as the distance measure (\(\)) and use the Adam optimizer with a constant learning rate of \(1e-4\) for training. We do not use batch normalization, regularization (_i.e._, dropout, weight decay), learning rate schedule, or data augmentation during training. We use transformer encoder architecture for functions \(f\) and \(p\); however, ablations show different architectural choices. A window size \(w\) of 50 inputs (timescale respective) is used to compute the running average in Equation 5, and a new layer \((l+1)\) is added to the stack after layer \((l)\) has processed 50K inputs.

### Delayed gradient stepping and distributed learning

Unlike our proposed approach, conventional deep learning networks do not utilize high-level outputs in the intermediate-level predictions. Since our model includes a top-down inference component, such that a lower level (_e.g._, \((l)\)) backpropagates its loss gradients into the temporal encoding functions of a higher level (_e.g._, \(f^{(>l)}\)), we cannot apply the gradients immediately after loss calculation at layer \((l)\). Therefore, we allow for scaled (_i.e._, Section 2.3 and Equation (8)) gradients to accumulate at all layers, then perform a single gradient step when the highest layer \(L\) backpropagates its loss.

In our streaming hierarchical learning approach, event demarcation is based on the data (_i.e._, some events are longer than others), posing a challenge for traditional parallelization schemes. We cannot directly batch events as inputs because each layer operates on a different subjective timeline. Therefore, each model is trained separately on a single stream of video data, and the models' parameters are averaged periodically during training. We train eight parallel streams on different sets of videos and average the models' parameters every 1K frames.

### Datasets and comparisons

In our training and evaluation, we use two large-scale egocentric datasets: Ego4D  and EPIC-KITCHENS 100 . Ego4D is a collection of videos with a total of 3670 hours of daily-life activity collected from 74 worldwide locations. EPIC-KITCHENS 100 contains 100 hours of egocentric video, 90K action segments, and 20K unique narrations. We train our model in a self-supervised layer-by-layer manner (using only RGB frames and inputting them exactly once) on a random 20% subset of Ego4D and 80% of EPIC-KITCHENS, then evaluate on the rest 20% of EPIC-KITCHENS. We define two evaluation protocols: Protocol 1 divides EPIC-KITCHENS such that the 20% test split comes from kitchens that have not been seen in the training set, whereas Protocol 2 ensures that the kitchens in the test set are also in the training set.

We compare our method with TW-FINCH , Offline ABD , Online ABD , and LSTM+AL . ABD, to the best of our knowledge, is the state of the art in unsupervised event segmentation. Clustering-based event segmentation models do not evaluate on egocentric datasets due to the challenges of camera motion and noise. Most clustering based-approaches use pre-trained or optical-flow-based features, which are not effective when clustered in an egocentric setting. We re-implement ABD due to the unavailability of official code and use available official implementations for the other methods.

### Evaluation metrics and protocols

Event segmentationThe 20K unique _narration_s in EPIC-KITCHENS include different labels referring to the same actions (_e.g._, turn tap on, turn on tap); therefore we cannot evaluate the labeling performance of the model. We follow the protocol of LSTM+AL  to calculate the Jaccard index (IoU) and mean over frames (MoF) of the one-to-one mapping between the ground truth and predicted segments. Unlike LSTM+AL , which uses Hungarian matching to find the one-to-one mapping, we design a generalized recursive algorithm called Hierarchical Level Reduction (HLR) which finds a one-to-one mapping between the ground truth events and (a single-layer or multi-layer hierarchical) predicted events. A detailed explanation of the algorithm can be found in Supplementary Material.

Representation qualityTo assess the quality of the learned representations, we use the large language model (LLM) GPT 3.5 to first create a dataset of events labels ranked by semantic similarity according to the LLM. In particular, we generate 1K data points sampled from EPIC-KITCHENS, where each data point comprises a 'query' narration and a set of 10 'key' narrations, and each key is ranked by its similarity to the query. We then retrieve the features for each event in the comparison and compute the appropriate vector similarity measure, and accordingly rank each key event. This rank list is then compared with the LLM ranking to report the MSE and the Levenshtein edit distance between them. Examples of LLM similarity rankings are available in Supplementary Material.

### Experiments

We evaluate STREAMER's performance of event segmentation and compare it with streaming and clustering-based SOTA methods as shown in Table 1. Our findings show that the performance of a single layer in STREAMER's hierarchy (the best-performing layer out of three per video) and the

    &  &  &  &  &  \\   & & & & **MoF \(\)** & **IoU**\(\) & **MoF**\(\) & **IoU**\(\) \\  LSTM+AL  & VGG16  & ImageNet  & 1 & 0.694 & 0.417 & 0.659 & 0.442 \\  TW-FINCH  &  &  & 1 & 0.707 & 0.443 & 0.692 & 0.442 \\ Offline ABD  & & & 1 & 0.704 & 0.438 & 0.699 & 0.432 \\ Online ABD  & & & 1 & 0.610 & 0.496 & 0.605 & 0.487 \\  STREAMER & 4-layer CNN & - & 1 & **0.759** & **0.508** & **0.754** & **0.489** \\  & & & 3 & **0.736** & **0.511** & **0.729** & **0.494** \\   

Table 1: Event segmentation comparison of MoF and average IoU, evaluated on EPIC-KITCHENS. None of the methods listed below requires labels.3 The column ‘Layers’ refers to the number of layers evaluated against the ground truth: 1 reports the performance of the best layer in the prediction hierarchy, whereas 3 uses the proposed Hierarchical Level Reduction algorithm for evaluation.

full 3-layer hierarchy outperform all other state of the art using IoU and MoF metrics on both testing protocols. It is worth noting that all the other methods use a large CNN backbone with supervised pre-trained weights (some on the same test dataset: EPIC-KITCHENS), whereas our model is trained from scratch using random initialization with a simple 4-layer CNN. We show comparative qualitative results in Figure 4. More qualitative results are provided in Supplementary Material.

Additionally, we evaluate the quality of event representation in Table 2. We show that self-supervising STREAMER from randomly initialized weights outperforms most clustering-based approaches with pre-trained weights; we perform on par with TW-FINCH when using supervised EPIC-KITCHENS pre-trained weights. Qualitative results of retrieval for the first three nearest neighbors on all the events in the test split are shown in Figure 5. More qualitative results are reported in Supplementary Material.

AblationsWe investigate three main aspects of STREAMER (Table 3): (1) varying the architecture of the temporal encoding model \(f\), (2) varying the predictor function \(p\), and (3) experimenting with the'reach of influence' parameter \(\) in Equation 7. Our findings suggest that STREAMER is robust to different architectural choices of \(f\). Our experiments also illustrate the importance of the cross-layer communication of \(p\): simply taking the average of \(\) as the prediction performs worse than applying a layer-specific MLP to the average; using a transformer to retrieve context from other layers dynamically performs the best. Finally, adjusting the reach of influence by gradient scaling improves the segmentation performance.

To determine the quality of the backbone features learned by STREAMER, we run ablations of using our 4-layer pretrained CNN features on SoTA clustering methods. The results, plotted in Figure 6, show significant improvement in the average mean over frames (MoF) performance of

  
**Method** & **Weights** & **MSE \(\)** & **LD \(\)** \\   \\   & EPIC & 1.00 & **0.67** \\   & IN & 1.018 & 0.710 \\   & EPIC & 1.02 & 0.71 \\   & IN & 1.005 & 0.708 \\   & EPIC & 1.00 & 0.70 \\   & IN & 1.039 & 0.704 \\   \\  STREAMER & - & **0.967** & 0.695 \\   

Table 2: Retrieval evaluation based on MSE and the Levenshtein edit distance (LD) of the features. All experiments are on EPIC-KITCHENS.3

Figure 4: Qualitative comparisons of event segmentation. The Gantt chart shows a more accurate alignment of STREAMER’s predictions with the ground truth compared to other methods.

event segmentation on the EPIC-KITCHENS dataset. This improvement could be attributed to the robust representations learned by the encoder through hierarchical predictive learning. In particular, since these features are learned through top-down optimization, the CNN backbone is able to predict longer events at higher levels, thus improving the features and contextualization quality.

## 4 Conclusion

In conclusion, we present STREAMER, a self-supervised and structurally evolving hierarchical temporal segmentation model that is shown to perform well on egocentric videos and is robust to hyperparameter variations. The learned representations are hierarchical in nature, representing events at different levels of granularity and semantics. As part of this, we design a gradient scaling mechanism necessary for such hierarchical frameworks with varying time-scales.

STREAMER adheres to several biologically-inspired constraints and exhibits the ability to process long previous contexts in a streaming manner, seeing each input exactly once. Our method is designed to be trained in a streaming manner which allows models to perform inference simultaneously during training, appealing to applications that require real-time adaptability . We demonstrate its ability to perform event segmentation on large egocentric videos of varying perceptual conditions and demonstrate the quality of the representations through event retrieval and similarity ranking experiments.

**Broader impact and limitations** STREAMER requires large amounts of data to model complex high-level causal structures, and the training time increases as a layer is added. However, as self-supervised learning is becoming of increasing essence, new models must be able to continually learn from large, unlabeled data from constantly evolving domains. STREAMER caters to such online learning paradigms by fully exploiting large unlabelled video data. A much broader impact of this method extends to multi-modal data and domains beyond egocentric videos.