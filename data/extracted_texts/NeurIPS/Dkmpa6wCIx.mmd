# Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization

Kaiyue Wen

Tsinghua University

wenky20@mails.tsinghua.edu.cn

Zhiyuan Li

Stanford University

zhiyuanli@stanford.edu

Tengyu Ma

Stanford University

tengyuma@stanford.edu

###### Abstract

Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks.

## 1 Introduction

It remains mysterious why stochastic optimization methods such as stochastic gradient descent (SGD) can find generalizable models even when the architectures are overparameterized (Zhang et al., 2016; Gunasekar et al., 2017; Li et al., 2017; Soudry et al., 2018; Woodworth et al., 2020). Many empirical and theoretical studies suggest that generalization is correlated with or guaranteed by the flatness of the loss landscape at the learned model (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Dziugaite and Roy, 2017; Jastrzebski et al., 2017; Neyshabur et al., 2017; Wu et al., 2018; Jiang et al., 2019; Blanc et al., 2019; Wei and Ma, 2019, 2019; HaoChen et al., 2020; Foret et al., 2021; Damian et al., 2021; Li et al., 2021; Ma and Ying, 2021; Ding et al., 2022; Nacson et al., 2022; Wei et al., 2022; Lyu et al., 2022; Norton and Royset, 2021; Wu and Su, 2023). Thus, a natural theoretical question is

**Question 0**.: _Does the flatness of the minimizers always correlate with the generalization capability?_

The answer to the question turns out to be false. First, Dinh et al. (2017) theoretically construct _very sharp_ networks with good generalization. Second, recent empirical results (Andriushchenko et al., 2023) find that sharpness may not have a strong correlation with test accuracy for a collection of modern architectures and settings, partly due to the same reason--there exist sharp models with good generalization. We note that, technically speaking, Question 0 is ill-defined without specifying the collection of models on which the correlation is evaluated. However, those sharp but generalizable models appear to be the main cause for the non-correlation.

Observing the existing theoretical and empirical evidence, it is natural to ask the one-side version of Question 0, where we are only interested in whether sharpness implies generalization but not vice versa.

**Question 1**.: _Do all the flattest neural network minimizers generalize well?_

Though there are some theoretical works that answer Question 1 affirmatively for simplified linear models (Li et al., 2021; Ding et al., 2022; Nacson et al., 2022; Gatmitry et al., 2023), the answer to Question 1 for standard neural networks remains unclear. Those theoretical results linking generalization to sharpness for more general architectures typically also involve other terms in generalization bounds, such as parameter dimension or norm (Neyshabur et al., 2017; Foret et al., 2021; Wei and Ma, 2019, 2019; Norton and Royset, 2021), thus do not answer Question 1 directly.

Our first contribution is a theoretical analysis showing that the answer to Question 1 can be **false**, even for simple architectures like 2-layer ReLU networks. Intriguingly, we also find that the answer to Question 1 subtly depends on the architectures of neural networks. For example, simply removing the bias in the first layer turns the aforementioned negative result into a positive result, as also shown in the Theorem 4.3 of Wu and Su (2023) (that the authors only came to be aware of after putting this work online).

More concretely, we show that for the 2 parity xor problem with mean square loss and with data sampled from hypercube \(\{-1,1\}^{d}\), all flattest 2-layer ReLU neural networks without bias provably generalize. However, when bias is added, for the same data distribution and loss function, there exists a flattest minimizer that fails to generalize for every unseen data. Since adding bias in the first layer can be interpreted as appending a constant input feature, this result suggests that the generalization of the flattest minimizer is sensitive to both network architectures and data distributions.

Recent theoretical studies (Wu et al., 2018; Blanc et al., 2019; Damian et al., 2021; Li et al., 2021; Arora et al., 2022; Wen et al., 2022; Nacson et al., 2022; Lyu et al., 2022; Bartlett et al., 2022; Li et al., 2022) also show that optimizers including SGD with large learning rates or label noise and Sharpness-Aware Minimization (SAM, Foret et al. (2021)) may implicitly regularize the sharpness of the training loss landscape. These optimizers are referred to as _sharpness minimization algorithms_ in this paper. Because Question 1 is not always true, it is then natural to hypothesize that sharpness-minimization algorithms will fail for architectures and data distributions where Question 1 is not true.

**Question 2**.: _Will sharpness minimization algorithm fail to generalize when there exist non-generalizing flattest minimizers?_

A priori, the authors were expecting that the answer to Question 2 is affirmative, which means that a possible explanation is that the sharpness minimization algorithm works if and only if for certain architecture and data distribution, Question 1 is true. However, surprisingly, we also answer this question negatively for some architectures and data distributions. In other words, we found that sharpness-minimization algorithms can still generalize well even when the answer to Question 1 is false. The result is consistent with our theoretical discovery that for many architectures, there exist both non-generalizing and generalizing flattest minimizers of the training loss. We show empirically that sharpness-minimization algorithms can find different types of minimizers for different architectures.

Our results are summarized in Table 1. We show through theoretical and empirical analysis that the relationship between sharpness and generalization can fall into three different regimes depending on the architectures and distributions. The three regimes include:

   & All Flattest Minimizers & Sharpness Minimization \\  & Generalize Well. & Algorithms Generalize. \\ 
2-layer w/o Bias & \(\) (Theorem 3.1) & \(\) \\
2-layer w/ Bias & \(\) (Theorem 4.1) & \(\) \\
2-layer w/ simplified BatchNorm & \(\) (Theorem 3.2) & \(\) \\
2-layer w/ simplified LayerNorm & \(\) (Theorem 5.1) & \(\) \\  

Table 1: **Overview of Our Results.** Each row in the table corresponds to one architecture. The second column indicates whether all flattest minimizers of training loss generalize well. \(\) indicates that all (near) flattest minimizers of training loss provably generalize well and \(\) indicates that there provably exists flattest minimizers that generalize poorly. The third column indicates whether the sharpness minimization algorithms generalize well in our experiments. Results in row \(2\) and \(4\) deny Question 1 and Question 2 respectively.

* **Scenario 1.** Flattest minimizers of training loss provably generalize and sharpness minimization algorithms find generalizable models. This regime (Theorems 3.1 and 3.2) includes 2-layer ReLU MLP without bias and 2-layer ReLU MLP with a simplified BatchNorm (without mean subtraction and bias). We answer both the Question 1 and Question 2 affirmatively in this scenario.1 * **Scenario 2.** There exists a flattest minimizer that has the worst generalization over all minimizers. Also, sharpness minimization algorithms fail to find generalizable models. This regime includes \(2\) layer ReLU MLP with bias. We deny Question 1 while affirm Question 2 in this scenario.
* **Scenario 3.** There exist flattest minimizers that do not generalize but the sharpness minimization algorithm still finds the generalizable flattest model empirically. This regime includes 2-layer ReLU MLP with a simplified LayerNorm (without mean subtraction and bias). In this scenario, the sharpness minimization algorithm relies other unknown mechanisms beyond minimizing sharpness to find a generalizable model. We deny both Question 1 and Question 2 in this scenario.

## 2 Setup

**Rademacher Complexity.** Given \(n\) data \(S=\{x_{i}\}_{i=1}^{n}\), the _empirical Rademacher complexity_ of function class \(\) is defined as \(_{S}()=}_{ \{ 1\}^{n}}_{f}_{i=1}^{n}_{i}f(x_{i})\). **Architectures.** As summarized in Table 1, we will consider multiple network architectures and discuss how architecture influences the relationship between sharpness and generalization. For each model \(f_{}\) parameterized by \(\), we will use \(d\) to denote the input dimension and \(m\) to denote the network width. We will now describe the architectures in detail.

2-MLP-No-Bias.\(f_{}^{}(x)=W_{2}(W_{1}x)\) with \(=(W_{1},W_{2})\).

2-MLP-Bias.\(f_{}^{}(x)=W_{2}(W_{1}x+b_{1})\) with \(=(W_{1},b_{1},W_{2})\). We additionally define MLP-Bias as \(f_{}^{}(x)=W_{D} W_{2}( W_{1}x+b_{1})\),

2-MLP-Sim-BN.\(f_{}^{}(x,\{x_{i}\}_{i[n]})=W_{2}_{}( (W_{1}x+b_{1}),\{(W_{1}x_{i}+b_{1} )\})\), where the simplified BatchNorm \(\) is defined as \( m,n N, i[n],x,x_{i}^{m},j[m],_{}(x,\{x_{i}\}_{i[n]})[j]= x[j]/(_{i=1}^{n}(x_{i}[j ])^{2}/n)^{1/2}\) and \(=(W_{1},b_{1},,W_{2})\).

2-MLP-Sim-LN.\(f_{}^{}(x)=W_{2}(W_{1}x+b_{1})}{\{ \|(W_{1}x+b_{1})\|_{2},\}}\) where \(\) is a sufficiently small positive constant.

Surprisingly, our results show that the relationships between sharpness and generalization are strikingly different among these simple yet similar architectures.

**Data Distribution.** We will consider a simple data distribution as our testbed. Data distribution \(_{}\) is a joint distribution over data point \(x\) and label \(y\). The data point is sampled uniformly from the hypercube \(\{-1,1\}^{d}\) and the label satisfies \(y=xx\). Many of our results, including our generalization bound in Section 3 and experimental observations can be generalized to broader family of distributions (Appendix B).

**Loss.** We will use mean squared error \(_{}\) for training and denote the training loss as \(L\). In Appendix B, we will show that all our theoretical results and empirical observations hold for logistic loss with label smoothing probability \(p>0\). We will also consider zero one loss \((yf_{}(x)>0)\) for evaluating the model. We will use interpolating model to denote the model with parameter \(\) that minimizes \(L\).

**Definition 2.1** (Interpolating Model).: _A model \(f_{}\) interpolates the dataset \(\{(x_{i},y_{i})\}_{i=1}^{n}\) if and only if \( i,f_{}(x_{i})=y_{i}\)._

**Sharpness.** Our theoretical analysis focuses on understanding the _sharpness_ of the trained models. Precisely, for a model \(f_{}\) parameterized by \(\), a dataset \(\{(x_{i},y_{i})\}_{i=1}^{n}\) and loss function \(\), we will use the trace of Hessian of loss function, \((^{2}L())\) to measure how sharp the loss is at \(\), which is a proxy for the sharpness along a random direction (Wen et al., 2022), or equivalently, the expected increment of loss under a random gaussian perturbation (Foret et al., 2021; Orvieto et al., 2022).

\((^{2}L())\) is not the only choice for defining sharpness, but theoretically many sharpness minimization algorithms have been shown to minimize this term over interpolating models. In particular,under the assumptions that the minimizer of the training loss form a smooth manifold Cooper (2018); Fehrman et al. (2020), Sharpness-Aware Minimization (SAM) (Foret et al., 2021) with batch size \(1\) and sufficiently small learning rate \(\) and perturbation radius \(\)(Wen et al., 2022; Bartlett et al., 2022), or Label Noise SGD with sufficiently small learning rate \(\)(Blanc et al., 2019; Damian et al., 2021; Li et al., 2021), prefers interpolating models with small trace of Hessian of the loss. Hence, we choose to analyze trace of Hessian of the loss and will use SAM with batch size \(1\) (we denote it by 1-SAM) as our sharpness minimization algorithm in our experiments.

**Notations.** We use \(\) to denote the trace of a matrix and \(x[i]\) to denote the value of the \(i\)-th coordinate of vector \(x\). We will use \(\) to represent element-wise product. We use \(\) as the (coordinate-wise) indicator function, for example, \([x>0]\) is a vector of the same length as \(x\) whose \(j\)-th entry is \(1\) if \(x[j]>0\) and \(0\) otherwise. We will use \((x)\) to hide logarithmic multiplicative factors.

## 3 Scenario I: All Flattest Models Generalize

### Flattest models provably generalize

When the architecture is 2-MLP-No-Bias, we will show that the flattest models can provably generalize, hence answering Question 1 affirmatively for this architecture and data distribution \(_{}\).

**Theorem 3.1**.: _For any \((0,1)\) and input dimension \(d\), for \(n=(d())\), with probability at least \(1-\) over the random draw of training set \(\{(x_{i},y_{i})\}_{i=1}^{n}\) from \(_{}^{n}\), let \(L()_{i=1}^{n}_{}(f_{}^{ }(x_{i}),y_{i})\) be the training loss for 2-MLP-No-Bias, it holds that for all \(^{*}_{L()=0}(^{2}L( ))\), we have that_

\[_{x,y_{}}[_{}( f_{^{*}}^{}(x),y)] (d/n).\]

Theorem 3.1 shows that for \(_{}\), flat models can generalize under almost linear sample complexity with respect to the input dimension. We note that Theorem 3.1 implies that \(_{x,y_{}}[f_{^{*}}^{ }(x)y>0](d/n).\) because if \(f_{^{*}}^{}(x)y 0\), it holds that \(_{}(f_{^{*}}^{}(x),y ) 1\). This shows that the model can classify the input with high accuracy. The major proof step is relating sharpness to the norm of the weight itself.

**Lemma 3.1**.: _Define \(_{C}\{=(W_{1},W_{2})_{j=1}^{m}\|W_{1,j}\|_{2}| W_{2,j}| C\}\). Under the setting of Theorem 3.1, there exists a absolute constant \(C\) independent of \(d\) and \(\), such that with probability at least \(1-\), \(_{L()=0}(^{2}L()) _{C}\) and \(_{S}(\{f_{}^{}_{C}\}) ()\)._

We would like to note that similar results of Theorem 3.1 and lemma 3.1 have also been shown in a prior work Wu and Su (2023) (that the authors were not aware of before the first version of this work was online).

The almost linear complexity in Theorem 3.1 is not trivial. For example, Wei et al. (2019) shows that learning the distribution will require \((d^{2})\) samples for Neural Tangent Kernel (NTK) (Jacot et al.,

Figure 1: **Scenario I. We train a 2-layer MLP with ReLU activation without bias using gradient descent with weight decay and 1-SAM on \(_{}\) with dimension \(d=30\) and training set size \(n=100\). In both cases, the model reaches perfect generalization. Notice that although weight decay doesn’t explicitly regularize model sharpness, the flatness of the model decreases through training, which is consistent with our Lemma 3.1 relating sharpness to the norm of the weight.**

2018). In contrast, our result shows that learning the distribution only requires \((d)\) samples as long as the flatness of the model is controlled.

Beyond reducing model complexity, flatness may also encourage the model to find a more interpretable solution. We prove that under a stronger than i.i.d condition over the training set, the near flattest interpolating model with architecture 2-MLP-Sim-BN will provably generalize and the weight of the first layer will be centered on the first two coordinates of the input, i.e., \(\|W_{1,i}[3:d]\|_{2}\|W_{1,i}\|_{2}\).

**Condition 1** (Complete Training Set Condition).: _There exists set \(S\{-1,1\}^{d-2}\), such that the linear space spanned by \(S-S=\{s_{1}-s_{2} s_{1},s_{2} S\}\) has rank \(d-2\) and the training set is \(\{(x,y) x^{d},x[3:d] S,x,x\{-1,1\},y=x x\}\)._

**Theorem 3.2**.: _Given any training set \(\{(x_{i},y_{i})\}_{i=1}^{n}\)satisfying Condition 1, for any width \(m\) and any \(>0\), there exists constant \(>0\), such that for any width-\(m\) 2-MLP-Sim-BN, \(f^{}_{}\), satisfying \(f^{}_{}\) interpolates the training set and \((^{2}L())+_{L(^{ })=0}(^{2}L(^{}))\), it holds that \( x\{-1,1\}^{d},|xx-f_{}(x)|\) and that \( i[m],\|W_{1,i}[3:d]\|_{2}\|W_{1,i}\|_{2}\)._

One may notice that in Theorem 3.2 we only consider the approximate minimizer of sharpness. This is because the gradient of output with respect to \(W_{1},b_{1}\), despite never being zero, will converge to zero as the norm of \(W_{1},b_{1}\) converges to \(\).

Condition 1 may seem stringent. In practice (Figure 1(b)), we find it not necessary for 1-SAM to find a generalizable solution. We hypothesize that this condition is mainly technical. Theorem 3.2 shows that sharpness minimization may guide the model to find an interpretable and low-rank representation. Similar implicit bias of SAM has also been discussed in Andriushchenko et al. (2023) The proof is deferred to Appendix B.1

### SAM empirically finds the flattest model that generalizes

We use 1-SAM to train 2-MLP-No-Bias on data distribution \(_{}\) to verify our Theorem 3.1 (Figure 1). As expected, the model interpolates the training set and reaches a flat minimum that generalizes perfectly to the test set.

We then verify our Theorem 3.2 by training a 2-layer MLP with simplified BN on data distribution \(_{}\) (Figure 1(a)). Here we do not enforce the strong theoretical Condition 1. However, we still observe that SAM finds a flat minimum that generalizes well. We then perform a detailed analysis of the model and find that the model is indeed interpretable. For example, the four largest neurons in the first layer approximately extract features \(\{(c_{1}x+c_{2}x) c_{1},c_{2}\{-1,1\}\}\) (Figure 1(b)). Also, the first 2 columns of the weight matrix of the first layer, corresponding to the useful features \(\{(c_{1}x+c_{2}x) c_{1},c_{2}\{-1,1\}\}\), have norms \(42.47\) and \(42.48\), while the largest column norm of the rest of the weight matrix is only \(5.65\).

Figure 2: **Interpretable Flattest Solution** We train a 2-layer MLP with simplified BN using 1-SAM on \(_{}\) with dimension \(d=30\) and training set size \(n=100\). After training, we find that the model is indeed interpretable. In Figure 1(b), we inspect the weight of the four neurons of the four largest neurons in the first layer and we observe that the four neurons approximately extract features \( x x\).

Scenario II: Both Flattest Generalizing and Non-generalizing Models Exist, and SAM Finds the Former

### Both generalizing and non-generalizing solutions can be flattest

In previous section, we show through Theorems 3.1 and 3.2 that sharpness benefits generalization under some assumptions. It is natural to ask whether it is possible to extend this bound to general architectures. However, in this section, we will show that the generalization benefit depends on model architectures. In fact, simply adding bias to the first layer of 2-MLP-No-Bias makes non-vacuous generalization bound impossible for distribution \(_{}\). This then leads to a negative answer to Question 1.

**Definition 4.1** (Set of extreme points).: _A finite set \(S^{d}\) is a set of extreme points if and only if for any \(x S\), \(x\) is a vertex of the convex hull of \(S\)._

**Definition 4.2** (Memorizing Solutions).: _A \(D\)-layer network is a memorizing solution for a training dataset if (1) the network interpolates the training dataset, and (2) for any depth \(k[D-1]\), there is an injection from the input data to the neurons on depth \(k\), such that the activations in layer \(k\) for each input data is a one-hot vector with the non-zero entry being the corresponding neuron._

**Theorem 4.1**.: _For any \(D 2\), if the input data points \(\{x_{i}\}\) of the training set form a set of extreme points (Definition 4.1), then there exists a width \(n\) layer \(D\) MLP-Bias that is a memorizing solution (Definition 4.2) for the training dataset and has minimal sharpness over all the interpolating solutions._

As one may suspect, these memorizing solutions can have poor generalization performance.

**Proposition 4.1**.: _For data distribution \(_{}\), for any number of samples \(n\), there exists a width-\(n\) 2-MLP-Bias that memorizes the training set as in Theorem 4.1, reaches minimal sharpness over all the interpolating models and has generalization error \(\{1-n/2^{d},0\}\) measured by zero one error._

This corollary shows that a flat model can generalize poorly. Comparing Theorems 3.1 and 4.1, one may observe the perhaps surprising difference caused by slightly modifying the architectures (adding bias or removing the BatchNorm). To further show the complex relationship between sharpness and generalization, the following theorem suggests, despite the existence of memorizing solutions, there also exists a flattest model that _can_ generalize well.

**Proposition 4.2**.: _For data distribution \(_{}\), for any number of samples \(n\), there exists a width-\(n\) 2-MLP-Bias that interpolates the training dataset, reaches minimal sharpness over all the interpolating models, and has zero generalization error measured by zero one error._

The flat solution constructed is highly simple. It contains four activated neurons, each corresponding to one feature in \( x x\) (Equation (5)).

**Proof sketch.** For simplicity, we will consider 2-MLP-Bias here. The construction of the memorizing solution in Theorem 4.1 is as follows (visualized in Figure 3). As the input data points form a set of extreme points (Definition 4.1), for each input data point \(x_{i}\), there exists a vector \(\|w_{i}\|=1,w_{i}^{d}\), such that \( j i,w_{i}^{}x_{i}>w_{i}^{}x_{j}\). We can then choose

\[W_{1}=[|y_{i}|}w_{i}/]^{}_{i},b_{1}=[|y_{i}| }(-w_{i}^{}x_{i}+)/]^{},W_{2}=[(y_{i})|/r_{i}}]_{i}.\]

Here \(r_{i}=(\|x_{i}\|^{2}+1)^{1/2}\) and \(\) is a sufficiently small positive number. Then it holds that \((W_{1}x_{i}+b_{1})=|y_{i}|}e_{i}\), where \(e_{i}\) is the \(i-\)th coordinate vector. This shows there is a one-to-one correspondence between the input data and the neurons. It is easy to verify that the model interpolates the training dataset. Furthermore, for \(_{}\) and sufficiently small \(\), for any input \(x\{x_{i}\}_{i[n]}\), it holds that \((W_{1}x+b_{1})=0\). Hence the model will output the same label \(0\) for all the data points outside the training set. This indicates Proposition 4.1.

To show the memorization solution has minimal sharpness, we need the following lemma that relates the sharpness and the Jacobian of the model.

**Lemma 4.1**.: _For mean squared error loss \(l_{mse}\), if model \(f_{}\) is differentiable and interpolates dataset \(\{(x_{i},y_{i})\}_{i[n]}\), then \((^{2}L())=_{i=1}^{n}\|_ {}f_{}(x_{i})\|^{2}\)._Proof of Lemma 4.1.: By standard calculus, it holds that,

\[(^{2}L()) =_{i=1}^{n}(_{}^ {2}[(f_{}(x_{i})-y_{i})^{2}])\] \[=_{i=1}^{n}(_{}^ {2}f_{}(x_{i})(f_{}(x_{i})-y_{i})+(_{}f_{}( x_{i}))(_{}f_{}(x_{i}))^{})\] \[=_{i=1}^{n}((_{ }f_{}(x_{i}))(_{}f_{}(x_{i}))^ {})=_{i=1}^{n}\|_{}f_{}(x_{i})\|_{2 }^{2}.\] (1)

The first equation in Equation (1) use \( i,f_{}(x_{i})=y_{i}\). The proof is then complete. 

After establishing Lemma 4.1, one can then explicitly calculate the lower bound of \(\|_{}f_{}(x_{i})\|^{2}\) condition on \(f_{}(x_{i})=y_{i}\). For simplicity of writing, we will view the bias term as a part of the weight matrix by appending a \(1\) to the input data point. Precisely, we will use notation \(x_{i}^{}^{d+1}\) to denote transformed input satisfying \( j[d],x_{i}^{}[j]=x_{i}[j],x_{i}^{}[d+1]=1\) and \(W_{1}^{}=[W_{1},b_{1}]^{m(d+1)}\) to denote the transformed weight matrix.

By the chain rule, we have,

\[\|_{}f_{}(x_{i})\|^{2} =\|_{W_{1}^{}}f_{}(x_{i})\|_{F}^{2}+\|_{W _{2}}f_{}(x_{i})\|_{F}^{2}\] \[=\|(W_{2}\,[W_{1}^{}x_{i}^{}>0])x_{i} ^{}\|_{F}^{2}+\|(W_{1}^{}x_{i}^{} )\|_{2}^{2}.\] \[=\|W_{2}\,[W_{1}^{}x_{i}^{}>0]\,\|_{2 }^{2}\|x_{i}^{}\|^{2}+\|(W_{1}^{}x_{i}^{} )\|_{2}^{2}.\] (2)

Then by Cauchy-Schwarz inequality, we have

\[\|_{}f_{}(x_{i})\|^{2} =\|W_{2}\,[W_{1}^{}x_{i}^{}>0]\,\|_{2 }^{2}\|x_{i}^{}\|^{2}+\|(W_{1}^{}x_{i}^{} )\|_{2}^{2}\] \[ 2\|x_{i}^{}\|(W_{2}\,[W_{1}x_{i}>0] )^{}(W_{1}^{}x_{i}^{})=2 \|x_{i}^{}\|\|y_{i}|.\] (3)

In Equation (3), we use condition \(f_{}(x_{i})=y_{i}\). Finally, notice that the lower bound is reached when

\[W_{2}\,[W_{1}^{}x_{i}^{}>0]=(W_ {1}^{}x_{i}^{})/\|x_{i}^{}\|.\] (4)

Condition Equation (4) is clearly reached for the memorization construction we constructed, where both sides of the equation are equal to \(|/\|x_{i}^{}\|}\|e_{i}\). This completes the proof of Theorem 4.1.

However, the memorization network is not the only parameter that can reach the lower bound. For example, for distribution \(_{}\), if parameter \(\) satisfies,

\[ i,j\{0,1\},W_{1,2i+j+1}=r[(-1)^{i},(-1)^{j},...,0],b_{1}[2i+j+1]=-r, W_{2}[2i+j]=(-1)^{i+j}/r.\] (5)

\[ k>4,W_{1,k}=[0,...,0],b_{1}[k]=0,W_{2}[k]=0,\]

Figure 3: **Visualization of Memorization Solutions.** This is an illustration of the memorizing solutions constructed in Theorem 4.1. Here the input data points come from a unit circle and are marked as dots. The shade area with the corresponding color represents the region where the corresponding neuron is activated. One can see that the network can output the correct label for each input data point in the training set as long as the weight vector on the corresponding neuron is properly chosen. Further, the network will make the same prediction \(0\) for all the input data points outside the shade area and this volume can be made almost as large as the support of the training set by choosing \(\) sufficiently small. Hence the model can interpolate the training set while generalizing poorly.

with \(r=(d^{2}+1)^{1/4}\). then for any \(x\{-1,1\}^{d}\), it holds that \((W_{1}x+b_{1})=re_{5/2-x-x/2}\) and \(f_{}(x)=x x\). Hence it is possible for Equation (5) to hold while the model has perfect generalization performance.

### SAM empirically finds the non-generalizing solutions

In this section, we will show that in multiple settings, SAM can find solutions that have low sharpness but fail to generalize compared to the baseline full batch gradient descent method with weight decay. This proves that flat minimization can hurt generalization performance. However, one should note that Question 2 is not denied for the current architectures.

**Converged models found by SAM fail to generalize.** We perform experiments on data distribution \(_{}\) in Figure 4. We apply small learning rate gradient descent with weight decay as our baseline and observe that the converged model found by SAM has a much lower sharpness than the baseline. However, the generalization performance of SAM is much worse than the baseline. Moreover, the generalization performance even starts to degenerate after 4000 epochs. We conclude that in this scenario, sharpness minimization can empirically hurt generalization performance.

**1-SAM may fail to generalize with other activation functions.** A natural question is whether the phenomenon that 1-SAM fails to generalize is limited to ReLU activation. In Figure 5, we show empirically that 1-SAM fails to generalize for 2-layer networks with softplus activation trained on the same dataset, although there is no known guarantee for the existence of memorizing solutions.

Figure 4: **Scenario II. We train a 2-layer MLP with ReLU activation with Bias using gradient descent with weight decay and 1-SAM on \(_{}\) with dimension \(d=30\) and training set size \(n=100\). One can clearly observe a distinction between the two settings. The minimum reached by 1-SAM is flatter but the model fails to generalize and the generalization performance even starts to degenerate after 4000 epochs. The difference between Figures 0(b) and 3(b) indicates a small change in the architecture can lead to a large change in the generalization performance.**

Figure 5: **Scenario II with Softplus Activation. We train a 2-layer MLP with Softplus activation (\((x)=(1+e^{x})\)) with bias using gradient descent with weight decay and 1-SAM on \(_{}\) with dimension \(d=30\) and training set size \(n=100\). We observe a similar phenomenon as Figure 4.**

## 5 Scenario III: Both Flattest Generalizing and Non-generalizing Models

Exist, and SAM Finds the Latter

### Both generalizing and non-generalizing solutions can be flattest

Despite the surprising contrary between Theorems 3.1 and 4.1, experiments show that Question 2 consistently hold. However, we will provide a counterexample in this section. Specifically, we will consider data distribution \(_{}\) and 2-layer ReLU MLP with simplified LayerNorm. One can first show both generalizing and non-generalizing solutions exist similar to Theorem 4.1 and propositions 4.1 and 4.2.

**Theorem 5.1**.: _If the input data points \(\{x_{i}\}\) of the training set form a set of extreme points (Definition 4.1), for sufficiently small \(\), then there exists a width-\(n\) 2-MLP-Sim-LN with hyperparameter \(\) that is a memorizing solution (Definition 4.2) for the training dataset and has minimal sharpness over all the interpolating solutions._

**Proposition 5.1**.: _For data distribution \(_{}\), for sufficiently small \(\), for any number of samples \(n\), there exists a width-\(n\) 2-MLP-Sim-LN with hyperparameter \(\) that memorizes the training set as in Theorem 4.1, reaches minimal sharpness over all the interpolating models and has generalization error \(\{1-n/2^{d},0\}\) measured by zero one error._

**Proposition 5.2**.: _For data distribution \(_{}\), for sufficiently small \(\), for any number of samples \(n\), there exists a width-\(n\) 2-MLP-Sim-LN with hyperparameter \(\) that interpolates the training dataset, reaches minimal sharpness over all the interpolating models, and has zero generalization error measured by zero one error._

The construction and intuition behind Theorem 5.1 and propositions 5.1 and 5.2 are similar to that of Theorem 4.1 and propositions 4.1 and 4.2. The proof is deferred to Appendix B.

### SAM empirically finds generalizing models

Notice in Section 5.1 our theory makes the same prediction as in Section 4. However, strikingly, the experimental observation is reversed (Figure 6). Now running SAM can greatly improve the generalization performance till the model perfectly generalizes. This directly denies Question 2 as now we have a scenario in which sharpness minimization algorithms can improve generalization till perfect generalization while there exists a flattest minimizer that will generalize poorly.

## 6 Discussion and Conclusion

We present theoretical and empirical evidence for (1) whether sharpness minimization implies generalization subtly depends on the choice of architectures and data distributions, and (2) sharpness minimization algorithms including SAM may still improve generalization even when there exist flattest models that generalize poorly. Our results suggest that low sharpness may not be the only cause of the generalization benefit of sharpness minimization algorithms.

Figure 6: **Scenario III. We train two-layer ReLU networks with simplified LayerNorm on data distribution \(_{}\) with dimension \(d=30\) and sample complexity \(n=100\) using 1-SAM. In Figure 5(a), we use standard training. In Figure 5(b), we restricted the norm of the weight and the bias of the first layer as \(10\), to avoid minimizing the sharpness by simply increasing the norm. We can see that in both cases, the models almost perfectly generalize.**