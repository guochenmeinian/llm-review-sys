# Diffusion Self-Guidance for

Controllable Image Generation

 Dave Epstein\({}^{1,2}\) Allan Jabri\({}^{1}\) Ben Poole\({}^{2}\) Alexei A. Efros\({}^{1}\) Aleksander Holynski\({}^{1,2}\)

\({}^{1}\)UC Berkeley \({}^{2}\)Google Research

###### Abstract

Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer the sampling process. Self-guidance operates similarly to standard classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of specific objects, merging the appearance of objects in one image with the layout of another, composing objects from multiple images into one, and more. We also show that self-guidance can be used for editing real images. See our project page for results and an interactive demo: https://dave.ml/selfguidance

## 1 Introduction

Generative image models have improved rapidly in recent years with the adoption of large text-image datasets and scalable architectures . These models are able to create realistic images given a text prompt describing just about anything. However, despite the incredible abilities of these systems, discovering the right prompt to generate the _exact_ image a user has in mind can be surprisingly challenging. A key issue is that all desired aspects of an image must be communicated through text, even those that are difficult or even impossible to convey precisely.

To address this limitation, previous work has introduced methods  that tune pretrained models to better control details that a user has in mind. These details are often supplied in the form of reference images along with a new textual prompt  or other forms of conditioning . However, these approaches all either rely on fine-tuning with expensive paired data (thus limiting the scope of possible edits) or must undergo a costly optimization process to perform the few manipulations they are designed for. While some methods  can perform zero-shot editing of an input image using a target caption describing the output, these methods only allow for limited control, often restricted to structure-preserving appearance manipulation or uncontrolled image-to-image translation.

By consequence, many simple edits still remain out of reach. For example, how can we move or resize one object in a scene without changing anything else? How can we take the appearance of an object in one image and copy it over to another, or combine the layout of one scene with the appearance of a second one? How can we generate images with certain items having precise shapes at specific positions on the canvas? This degree of control has been explored in the past in smaller scale settings , but has not been convincingly demonstrated with modern large-scale diffusion models .

We propose _self-guidance_, a zero-shot approach which allows for direct control of the shape, position, and appearance of objects in generated images. Self-guidance leverages the rich representations learned by pretrained text-to-image diffusion models - namely, intermediate activations and attention - to steer attributes of entities and interactions between them. These constraints can be user-specified or transferred from other images, and rely only on knowledge internal to the diffusion model. Through a variety of challenging image manipulations, we demonstrate that self-guidance using only a few simple properties allows for granular, disentangled manipulation of the contents of generated images (Figure 1). Further, we show that self-guidance can also be used to reconstruct and edit real images.

Our key contributions are as follows:

* We introduce _self-guidance_, which takes advantage of the internal representations of pretrained text-to-image diffusion models to provide disentangled, zero-shot control over the generative process without requiring auxiliary models or supervision.
* We find that properties such as the size, location, shape, and appearance of objects can be extracted from these representations and used to meaningfully guide sampling in a zero-shot manner.
* We demonstrate that this small set of properties, when composed, allows for a wide variety of surprisingly complex image manipulations, including control of relationships between objects and the way modifiers bind to them.
* Finally, by reconstructing captioned images using their layout and appearance as computed by self-guidance, we show that we can extend our method to editing real images.

## 2 Background

### Diffusion generative models

Diffusion models learn to transform random noise into high-resolution images through a sequential sampling process [12; 33; 35]. This sampling process aims to reverse a fixed time-dependent destructive process that corrupts data by adding noise. The learned component of a diffusion model is a neural network \(_{}\) that tries to estimate the denoised image, or equivalently the noise \(_{t}\) that was added to create the noisy image \(z_{t}=_{t}x+_{t}_{t}\). This network is trained with loss:

\[L()=_{t(1,T),_{t}(0, )}w(t)||_{t}-_{}(z_{t};t,y)||^{2},\] (1)

Figure 1: **Self-guidance** is a method for controllable image generation that guides sampling using the attention and activations of a pretrained diffusion model. With self-guidance, we can move or resize objects, or even replace them with items from real images, without changing the rest of the scene (b-d). We can also borrow the appearance of other images or rearrange scenes into new layouts (e-f).

where \(y\) is an additional conditioning signal like text, and \(w(t)\) is a function weighing the contributions of denoising tasks to the training objective, commonly set to 1 [12; 15]. A common choice for \(_{}\) is a U-Net architecture with self- and cross-attention at multiple resolutions to attend to conditioning text in \(y\)[28; 29; 32]. Diffusion models are score-based models, where \(_{}\) can be seen as an estimate of the score function for the noisy marginal distributions: \(_{}(z_{t})-_{t}_{z_{t}} p(z_{t})\).

Given a trained model, we can generate samples given conditioning \(y\) by starting from noise \(z_{T}(0,I)\), and then alternating between estimating the noise component and updating the noisy image:

\[_{t}=_{}(z_{t};t,y), z_{t-1}=(z_ {t},_{t},t,t-1,_{t-1}),\] (2)

where the update could be based on DDPM , DDIM , or another sampling method (see Appendix for details). Unfortunately, naively sampling from conditional diffusion models does not produce high-quality images that correspond well to the conditioning \(y\). Instead, additional techniques are utilized to modify the sampling process by altering the update direction \(_{t}\).

### Guidance

A key capability of diffusion models is the ability to adapt outputs after training by _guiding_ the sampling process. From the score-based perspective, we can think of guidance as composing score functions to sample from richer distributions or to introduce conditioning on auxiliary information [7; 17; 35]. In practice, using guidance involves altering the update direction \(_{t}\) at each iteration.

_Classifier guidance_ can generate conditional samples from an unconditional model by combining the unconditional score function for \(p(z_{t})\) with a classifier \(p(y|z_{t})\) to generate samples from \(p(z_{t}|y) p(y|z_{t})p(z_{t})\)[7; 35]. To use classifier guidance, one needs access to a labeled dataset and has to learn a noise-dependent classifier \(p(y|z_{t})\) that can be differentiated with respect to the noisy image \(z_{t}\). While sampling, we can incorporate classifier guidance by modifying \(_{t}\):

\[_{t}=_{}(z_{t};t,y)-s_{t}_{z_{t}} p (y|z_{t}),\] (3)

where \(s\) is an additional parameter controlling the guidance strength. Classifier guidance moves the sampling process towards images that are more likely according to the classifier , achieving a similar effect to truncation in GANs , and can also be applied with pretrained classifiers by first denoising the intermediate noisy image (though this requires additional approximations ).

In general, we can use any energy function \(g(z_{t};t,y)\) to guide the diffusion sampling process, not just the probabilities from a classifier. \(g\) could be the approximate energy from another model , a similarity score from a CLIP model , an arbitrary time-independent energy as in universal guidance , bounding box penalties on attention , or any attributes of the noisy images. We can incorporate this additional guidance alongside _classifier-free guidance_ to obtain high-quality text-to-image samples that also have low energy according to \(g\):

\[_{t}=(1+s)_{}(z_{t};t,y)-s_{}(z_{t} ;t,)+v_{t}_{z_{t}}g(z_{t};t,y),\] (4)

where \(s\) is the classifier-free guidance strength and \(v\) is an additional guidance weight for \(g\). As with classifier guidance, we scale by \(_{t}\) to convert the score function to a prediction of \(_{t}\). The main contribution of our work is to identify energy functions \(g\) useful for controlling properties of objects and interactions between them.

### Where can we find signal for controlling diffusion?

While guidance is a flexible way of controlling the sampling process, energy functions typically used [1; 39] require auxiliary models (adapted to be noise-dependent) as well as data annotated with properties we would like to control. Can we circumvent these costs? Recent work [11; 36] has shown that the intermediate outputs of the diffusion U-Net encode valuable information [16; 25] about the structure and content of the generated images. In particular, the self and cross-attention maps \(\{_{i,t}^{H_{i} W_{i} K}\}\) often encode structural information  about object position and shape, while the network activations \(\{_{i,t}^{H_{i} W_{i} D_{i}}\}\) allow for maintaining coarse appearance  when extracted from appropriate layers. While these editing approaches typically share attention and activations naively between subsequent sampling passes, drastically limiting the scope of possible manipulations, we ask: what if we tried to harness model internals in a more nuanced way?

## 3 Self-guidance

Inspired by the rich representations learned by diffusion models, we propose self-guidance, which places constraints on intermediate activations and attention maps to steer the sampling process and control entities named in text prompts (see Fig. 2). These constraints can be user-specified or copied from existing images, and rely only on knowledge internal to the diffusion model.

We identify a number of properties useful for meaningfully controlling generated images, derived from the set of softmax-normalized attention matrices \(\{_{i,t}^{H_{i} W_{i} K}\}\) and activations \(\{_{i,t}^{H_{i} W_{i} D_{i}}\}\) extracted from the standard denoising forward pass \(_{}(z_{t};t,y)\). To control an object mentioned in the text conditioning \(y\) at token indices \(k\), we can manipulate the corresponding attention channel(s) \(_{i,t,,k}^{H_{i} W_{i}|k|}\) and activations \(_{i,t}\) (extracted at timestep \(t\) from a noisy image \(z_{t}\) given text conditioning \(y\)) by adding guidance terms to Eqn. 4.

Object position.To represent the position of an object (omitting attention layer index and timestep for conciseness), we find the center of mass of each relevant attention channel:

\[(k)=_{h,w,k}} _{h,w}w_{h,w,k}\\ _{h,w}h_{h,w,k}\] (5)

We can use this property to guide an object to an absolute target position on the image. For example, to move "burger" to position \((0.3,0.5)\), we can minimize \(\|(0.3,0.5)-(k)\|_{1}\). We can also perform a relative transformation, e.g., move "burger" to the right by \((0.1,0.0)\) by minimizing \(\|_{}(k)+(0.1,0.0)- (k)\|_{1}\).

Object size.To compute an object's size, we spatially sum its corresponding attention channel:

\[(k)=_{h,w}_{h,w,k}\] (6)

In practice, we find it beneficial to differentiably threshold the attention map \(_{}\) before computing its size, to eliminate the effect of background noise. We do this by taking a soft threshold at the midpoint of the per-channel minimum and maximum values (see Appendix for details). As with position, one can guide to an absolute size (_e.g._ half the canvas) or a relative one (_e.g._\(10\%\) larger).

Object shape.For even more granular control than position and size, we can represent the object's exact shape directly through the thresholded attention map itself:

\[(k)=_{k}^{}\] (7)

This shape can then be guided to match a specified binary mask (either provided by a user or extracted from the attention from another image) with \(\|\_-(k)\|_{1}\). Note that we can

Figure 2: **Overview:** We leverage representations learned by text-image diffusion models to steer generation with _self-guidance_. By constraining intermediate activations \(_{t}\) and attention interactions \(_{t}\), self-guidance can control properties of entities named in the prompt. For example, we can change the position and shape of the burger, or copy the appearance of ice cream from a source image.

apply any arbitrary transformation (scale, rotation, translation) to this shape before using it as a guidance target, which allows us to manipulate objects while maintaining their silhouette.

Object appearance.Considering thresholded attention a rough proxy for object extent, and spatial activation maps as representing local appearance (since they ultimately must be decoded into an unnoised RGB image), we can reach a notion of object-level appearance by combining the two:

\[(k)=(k)}{_{h,w} (k)}\] (8)

## 4 Composing self-guidance properties

The small set of properties introduced in Section 3 can be composed to perform a wide range of image manipulations, including many that are intractable through text. We showcase this collection of manipulations and, when possible, compare to prior work that accomplishes similar effects. All experiments were performed on Imagen , producing \(1024 1024\) samples. For more samples and details on the implementation of self-guidance, please see the Appendix.

Adjusting specific properties.By guiding one property to change and all others to keep their original values, we can modify single objects in isolation (Fig. 2(b)-2(e)). For a caption \(C=y\) with words at indices \(\{c_{i}\}\), in which \(O=\{o_{j}\} C\) are objects, we can move an object \(o_{k}\) at time \(t\) with:

\[g =w_{0}_{o o_{k} O} {||}_{i=0}^{||}_{i,t,}(o)-_{i,t}(o)_{1}}^{}\] (9) \[+w_{1}_{o O}_{t,}(o)-_{t}(o)_{1}}^{\,o_{k}}\] \[+w_{2}|}_{i=0}^{| |}(_{i,t,}(o_{k}))- _{i,t}(o_{k})_{1}}^{\,o_{k}}\]

Where \(_{}\) and \(\) are extracted from the generation of the initial and edited image, respectively. Critically, \(\) lets us define whatever transformation of the \(H_{i} W_{i}\) spatial attention map we

Figure 3: **Moving and resizing objects. By only changing the properties of one object (as in Eqn. 9), we can move or resize that object without modifying the rest of the image. In these examples, we modify “massive sun”, “oil painting of cheese”, and “raccoon in a barrel”, respectively.**

want. To move an object, \(\) translates the attention mask the desired amount. We can also resize objects (Fig. 2(f)-2(g)) with Eqn. 9 by changing \(\) to up- or down-sample shape matrices.

Constraining per-object layout but not appearance finds new "styles" for the same scene (Fig. 4):

\[g=w_{0}_{o O}|}_{i=0}^ {||}\|_{i,t,}(o)-_{i,t}(o) \|_{1}}^{}\] (10)

We can alternatively choose to guide all words, not just nouns or objects, changing summands to \(_{c o_{k} C}\) instead of \(_{c o_{k} O}\). See Appendix for further discussion.

Composition between images.We can compose properties across multiple images into a cohesive sample, _e.g._ the layout of an image \(A\) with the appearance of objects in another image \(B\) (Fig. 5):

\[g =w_{0}_{o O}| }_{i=0}^{||}\|_{i,t,A}(o)-_{i,t}(o) \|_{1}}^{}\] (11) \[+w_{1}_{o O}\|_{t,B}(o)-_{t}(o)\|_{1}}^{}\]

We can also borrow only appearances, dropping the first term to sample new arrangements for the same objects, as in the last two columns of Figure 5.

Highlighting the compositionality of self-guidance terms, we can further inherit the appearance and/or shape of objects from several images and combine them into one (Fig. 6). Say we have \(J\) images, where we are interested in keeping a single object \(o_{k_{j}}\) from each one. We can collage these objects "in-place" - _i.e._ maintaining their shape, size, position, and appearance - straightforwardly:

\[g =w_{0}_{j}|}_ {i=0}^{||}\|_{i,t,j}(o_{k_{j}})-_{i,t }(o_{k})\|_{1}}^{}\] (12) \[+w_{1}_{j}\|_{t,j} (o_{k_{j}})-_{t}(o_{k})\|_{1}}^{}\]

We can also take only the appearances of the objects from these images and copy the layout from another image, useful if object positions in the \(J\) images are not mutually compatible (Fig. 6f).

Figure 4: **Sampling new appearances.** By guiding object shapes (Eqn. 7) towards reconstruction of a given image’s layout (a), we can sample new appearances for a given scene (b-d).

Figure 5: **Mix-and-match.** By guiding samples to take object shapes from one image and appearance from another (Eqn. 11), we can rearrange images into layouts from other scenes. Input images are along the diagonal. We can also sample new layouts of a scene by only guiding appearance (right).

Figure 6: **Compositional generation.** A new scene (d) can be created by collapsing objects from multiple images (Eqn. 12). Alternatively – _e.g._ if objects cannot be combined at their original locations due to incompatibilities in these images’ layouts (*as in the bottom row) – we can borrow only their appearance, and specify layout with a new image (e) to produce a composition (f) (Eqn. 19).

Editing with real images.Our approach is not limited to only images generated by a model, whose internals we have access to by definition. By running \(T\) noised versions of a (captioned) existing image through a denoiser - one for each forward process timestep - we extract a set of intermediates that can be treated as if it came from a reverse sampling process (see Appendix for more details). In Fig. 8, we show that, by guiding shape and appearance for all tokens, we generate faithful reconstructions of real images. More importantly, we can manipulate these real images just as we can generated ones, successfully controlling properties such as appearance, position, or size. We can also transfer the appearance of an object of interest into new contexts (Fig. 7), from only one source image, and without any fine-tuning:

\[g=w_{0}_{t,}(o_{k_{ }})-_{t}(o_{k})\|_{1}}^{}\] (13)

Attributes and interactions.So far we have focused only on the manipulation of objects, but we can apply our method to any concept in the image, as long as it appears in the caption. We demonstrate manipulation of verbs and adjectives in Fig. 9, and show an example where certain self-guidance constraints can help in enforcing attribute binding in the generation process.

## 5 Discussion

We introduce a method for guiding the diffusion sampling process to satisfy properties derived from the attention maps and activations within the denoising model itself. While we propose a number of

Figure 8: **Real image editing. Our method enables the spatial manipulation of objects (shown in Figure 3 for generated images) for _real_ images as well.**

Figure 7: **Appearance transfer from real images. By guiding the appearance of a generated object to match that of one in a real image (outlined) as in Eqn. 13, we can create scenes depicting an object from real life, similar to DreamBooth , but _without any fine-tuning and only using one image_.**

such properties, many more certainly exist, as do alternative formulations of those presented in this paper. Among the proposed collection of properties, a few limitations stand out.

The reliance on cross-attention maps imposes restrictions by construction, precluding control over any object that is not described in the conditioning text prompt and hindering fully disentangled control between interacting objects due to correlations in attention maps (Fig. 9(e)-9(f)). Selectively applying attention guidance at certain layers or timesteps may result in more effective disentanglement.

Our experiments also show that appearance features often contain undesirable information about spatial layout (Fig. 9(a)-9(b)), perhaps since the model has access to positional information in its architecture. The reverse is also sometimes true: guiding the shape of multiple tokens occasionally betrays the appearance of an object (Fig. 9(c)-9(d)), implying that hidden high-frequency patterns arising from interaction between attention channels may be used to encode appearance. These findings suggest that our method could serve as a window into the inner workings of diffusion models and provide valuable experimental evidence to inform future research.

## Broader impact

The use-cases showcased in this paper, while transformative for creative uses, carry the risk of producing harmful content that can negatively impact society. In particular, self-guidance allows for a level of control over the generation process that might enable potentially harmful image manipulations, such as pulling in the appearance or layout from real images into arbitrary generated content (e.g., as in Fig. 7). One such dangerous manipulation might be the injection of a public figure into an image containing illicit activity. In our experiments, we mitigate this risk by deliberately refraining from generating images containing humans. Additional safeguards against these risks include methods for embedded watermarking  and automated systems for safe filtering of generated imagery.