# Graph Bernoulli Pooling

Paper Id: 3310

###### Abstract

Graph pooling is crucial for enlarging receptive field and reducing computational cost in deep graph representation learning. In this work, we propose a simple but effective non-deterministic graph pooling method, called graph Bernoulli pooling (BernPool), to facilitate graph feature learning. In contrast to most graph pooling methods with deterministic modes, we design a probabilistic Bernoulli sampling to reach an expected sampling rate through deducing a variational bound as the constraint. To further mine more useful info, a learnable reference set is introduced to encode nodes into a latent expressive probability space. Hereby the resultant Bernoulli sampling would endeavor to capture salient substructures of the graph while possessing much diversity on sampled nodes due to its non-deterministic manner. Considering the complementarity of node dropping and node clustering, further, we propose a hybrid graph pooling paradigm to combine a compact subgraph (via dropping) and a coarsening graph (via clustering), in order to retain both representative substructures and input graph info. Extensive experiments on multiple public graph classification datasets demonstrate that our BernPool is superior to various graph pooling methods, and achieves state-of-the-art performance. The code is publicly available in an anonymous format at https://github/BernPool.

## 1 Introduction

Graph Neural Networks (GNNs) [13; 36] have been widely used to learn expressive representation from ubiquitous graph-structured data such as social networks , chemical molecules  and biological networks . To improve representation ability, multiple GNN variants, e.g., graph convolutional networks (GCNs)  and graph attention networks (GATs) , have been developed to facilitate various graph-related tasks including node classification, link prediction[19; 35], and graph classification. Specifically, for graph-related learning tasks, graph pooling has become an essential component in various GNN architectures. Aiming to learn compact representation for graphs, graph pooling facilitates graph topology modeling by enlarging receptive fields as well as scaling down the graph size which effectively reduces computational costs.

The existing graph pooling techniques generally fall into two main categories, i.e., the global graph pooling [5; 27; 14; 33; 26; 36] and hierarchical graph pooling. The former directly compresses a set of nodes into a compact graph-level representation. This operation results in a flat feature as a whole graph embedding. In contrast, the hierarchical pooling coarsens graphs gradually and outputs the corresponding pooled graphs of smaller sizes. For this purpose, two different types of coarsening, named node dropping [17; 10; 20; 37; 18; 11] and node clustering [33; 1; 34], are often employed. The node dropping picks up a subset of nodes to construct the coarsened graph, while the node clustering learns an assignment matrix to aggregate those nodes in the original graph into new clusters. In this work, our proposed BernPool falls in the category of the latter.

Even though considerable progress has been made, most pooling methods select a part of nodes or cluster nodes in a deterministic manner according to the importance scores of nodes, which degrades the sampling diversity. For this issue, some previous works [21; 7] propose stochastic node dropping independent from the data, i.e., randomly dropping. However, they do not consider the intrinsicstructural characteristics of data while enriching the sampling diversity. Thus the current bottleneck is how to adaptively extract expressive substructures and meanwhile keeping rich sampling diversity in graph pooling, with the precondition of high-efficient and effective graph representation learning.

To address the above problem, in this work, we propose a graph Bernoulli pooling method called BernPool to facilitate graph representation learning. Different from the existing graph pooling methods, we design a probabilistic Bernoulli sampling by estimating the sampling probabilities of graph nodes. To restrict the sampling process, we formulate a variational Bernoulli learning constraint by deriving an upper bound between an expected distribution and a learned distribution. To better capture expressive info, a learnable reference set is further introduced to encode nodes into a latent expressive probability space. Thus the advantage of the resultant Bernoulli sampling is two-fold: i) capture representative substructures of graph; and ii) preserve certain diversity (like random dropping) due to its non-deterministic manner. Considering the complementary characteristics between node dropping and node clustering, we propose a hybrid graph pooling paradigm to fuse a compact subgraph after node dropping and a coarsening graph after node clustering. The node clustering in our framework is also Bernoulli-induced without high-computation cost because it adopts the sampled nodes as clustering centers. The hybrid graph pooling can jointly learn representative substructures and preserve the input graph topology. We conduct extensive experiments on 8 public graph classification datasets to test our BernPool, and the experimental results validate that our BernPool achieves better performance than those existing pooling methods and keep high efficiency on par with those node-dropping methods.

The contributions of this work are summarized as: i) propose a probabilistic Bernoulli sampling method to not only learn effective sampling but also preserve high efficiency; ii) propose a hybrid graph pooling way to retain both those sampled substructures and the remaining info; iii) verify the effectiveness and high-efficiency of our BernPool, and report the state-of-the-art performance.

## 2 Related Work

In this section, we first review the previous methods of Graph Neural Networks (GNNs), then introduce the related Hierarchical pooling methods.

**Graph Neural Network.** GNNs were introduced as a form of recurrent neural network by Gori et.al.  and Scarselli et al. . Subsequently, Duvenaud et al. introduced a convolution-like propagation rule on graphs to extract node representations for graph-level classification. To enhance the graph representation ability, several convolution operations were proposed (e.g. Graph Convolution Network(GCN), Graph Attention Network(GAT), GraphSAGE, GIN) to extract expressive node representations by aggregating neighbor node features and have achieved promising performance in various graph-related tasks in recent years. In particular, GCN utilizes a first-order approximation of spectral convolution via Chebyshev polynomial iteration to improve efficiency. However, it suffers from the issue of fixed and equal weighting for neighbor nodes during aggregation, which may not be optimal for all nodes and can lead to information loss. To address this problem, GAT introduced an attention mechanism to assign different weights to neighbor nodes during message passing. Furthermore, GrapSAGE learned node embeddings by aggregating feature information from the neighborhood in an inductive manner. Despite the considerable progress made by GNNs, they are limited in their ability to generate hierarchical graph representations due to the lack of pooling operations.

**Hierarchical Graph Pooling.** Graph pooling is a critical operation to obtain robust representations and scale down the size of graphs, which can be classified into two categories: global pooling and hierarchical pooling. The former [5; 27; 14; 33; 26; 36] aggregates node-level features to generate a graph-level representation. For instance, SortPool ranks and groups the nodes into clusters according to their features, then aggregates the resulting clusters to generate the graph-level representation. However, the global pooling suffers from the issue of discarding structure information in generating graph-level representation. On the other hand, the hierarchical pooling methods could progressively compress the graph into a smaller one and capture the hierarchical structure. They can be further divided into node clustering pooling methods [33; 1; 34], node drop pooling methods [17; 10; 20; 37; 18; 11] and other pooling methods[28; 3]. Among them, the node drop pooling methods deleted the unimportant nodes based on certain criteria. For instance, SAGPool  computed the node attention scores using graph convolution to preserve the most important nodes. But the node drop pooling methods may not preserve the original structure well during the graph compression process. To alleviate this problem, Pang et.al  applied contrastive learning to maximize the mutual information between the input graph and the pooled graphs to preserve the graph-level dependencies in the pooling layers. Gao et.al  proposed a criterion to assess each node's information among its neighbors to retain informative features. Our BernPool introduces probabilistic deduced Bernoulli sampling based on reference set to progressively compress the original graph by preserving important nodes, rather than selecting nodes in a deterministic way. This probabilistic manner can lead to more diverse sampling situations and capture the data of intrinsic characteristics, promoting graph discriminative representation learning. Furthermore, we propose a hybrid graph pooling module to alleviate the node drop pooling method's issue of cannot preserve structure well.

## 3 Preliminaries

**Notations** For an arbitrary graph \(=(,,)\) with \(n=||\) nodes and \(||\) edges. \(^{n d^{}}\) represents the node feature matrix, where \(d^{}\) is the dimension of node attributes, and \(^{n n}\) denotes the adjacency matrix describing its edge connection information. The graph \(\) has a one-hot vector \(_{i}\) w.r.t its label. A pooled graph of the original graph \(\) is denoted by \(}=(},}, })\) with adjacency matrix as \(}^{}\), where \(\) denotes the number of pooled nodes.

**Graph Convolution** In this work, we employ the classic graph convolution network (GCN) as the backbone to extract features, where the \(l\)-th convolutional layer is formulated as:

\[^{(l+1)}=(}^{-}} }^{}^{(l)}^{(l)}),\] (1)

where \(()\) is a non-linear activation function, \(^{(l)}\) is the hidden-layer feature, \(}\) is the added self-loop adjacent matrix, \(}\) denotes the degree matrix of \(}\), and \(^{(l)}\) represents a learnable weight matrix at the \(l\)-th layer. The initial node features are used at the first convolution, i.e., \(^{(0)}=\).

## 4 The Proposed BernPool

### Overview

The whole framework is illustrated in Fig. 1, where the convolution and our proposed BernPool are stacked alternately. The proposed BernPool could be seamlessly engaged with any type of graph convolution to facilitate graph representation. In BernPool, there contains two main modules: graph Bernoulli sampling (GBS) and hybrid graph pooling (HGP). To boost graph Bernoulli sampling, we specifically design a reference set \(\) to encode the importance of each graph node, and the reference set is configured as the optimizable parameters. The reference set is further conditioned on the orthogonal space so as to reduce redundancy. After transforming via the reference set, we can

Figure 1: The architecture of the proposed BernPool framework. Please see Overview in Section 4.1.

estimate the sampling probabilities \(\) of graph nodes. In particular, the probabilities of nodes are totally restricted with an expected/predefined distribution, which is formulated to maximize the upper bound of KL-divergence (please see Section 4.2). Just due to the restriction, we can probabilistically sample a specified proportion of graph nodes. The detail of GBS can be found in Section 4.3. In the stage of hybrid graph pooling, on the one hand, we prune those unsampled nodes and the associated edges to generate a compact subgraph; on the other hand, to preserve graph topological structure, we perform neighbor nodes clustering to form a coarsening graph. Both the compact subgraph and coarsening graph are fused to form the final pooled graph. The detail of HGP can be found in Section 4.4. The BernPool attempts to learn the reference set and a few linear transformations. The whole framework can be optimized in an end-to-end mode through back-propagation.

### Bernoulli Sampling Optimization Objective

To sample a certain proportion of graph nodes in a probabilistic manner, we derive a KL-divergence constraint, which makes node probabilities tend to be a predefined distribution. To this end, we again dissect mutual information between the learned subgraph embeddings and their corresponding labels. Formally, we aim to maximize the mutual information function:

\[_{MI}=MI(,f_{,}(,)),\] (2)

where \(\) denotes the parameters of the BernPool, \(\) is the parameters of other modules (e.g., convolution, classifier), and \(f_{,}()\) represents the graph embedding process.

Suppose the sampling factor \(\) in graph pooling, we resort to the relationship between mutual information and expectation, and rewrite Eqn. (2) as:

\[_{|,}[ p_{} (|,,)p_{}(|,)d]\] \[=_{|,}[ q _{}(|,)p_{}(|, ,)(|,)}{q_ {}(|,)}d],\] (3)

where \(p_{}(|,,)\) is the conditional probability of label \(\), and \(p_{}(|,)\) denotes the conditional probability of the factor \(\) that is usually intractable. After a series of derivation from Eqn. (3), we can deduce a bound with KL-divergence between expected Bernoulli distribution \(q_{}\) and learned distribution \(p_{}\):

\[_{|,}[ p_{ }(|,,)p_{}(|,)d]\] \[_{|,}[ p_{ }(|,,)]-D_{KL}(q_{}(|,)||p_{}(|,))\] (4) \[=-_{CE}-D_{KL}(q_{}(|,)||p_{}(|,)),\] (5)

where \(p_{}(|,)\) represents the predicted probability of label based on the input graph and reference set, \(_{CE}\) is the cross entropy loss function. **Please see the detailed derivation in the supplementary file**. As the expected Bernoulli distribution \(q_{}(|,)\) is independent from the input graph and reference set, \(q_{}(|,)\) can be denoted as \(q()\). After adding the soft-orthogonal constraint on the reference set, therefore, the final optimization objective can be converted to minimize:

\[=_{CE}+D_{KL}(q()||p_{}(|, ))+||^{}-c||_{F},\] (6)

where the second term forces the sampling factor to follow an expected distribution \(q\), the matrix \(\) stacks the vectors of reference set \(\) in the third term, \(\) denotes the identity matrix, \(\) is a trade-off hyper-parameter, and \(c\) is a learnable scalar. Thus, the learning of the sampling factor could be integrated into the objective function as a joint training process. In addition, we can easily extend the above single-layer BernPool into multi-layer networks by deploying independent sampling factors in sequential graph pooling.

### Bernoulli Sampling Factor Learning

To extract an expressive sub-graph \(}\) from the original or former-layer graph \(\), we estimate a probabilistic factor \(=(z_{1},,z_{n})^{}\{0,1\}^{n}\) that conforms to Bernoulli distribution, instead of a deterministic way such as top-k. In contrast to the deterministic way, our BernPool possessesa more diverse sampling in mining substructures and graph topological variation of input data. However, as mentioned above in Eqn. (3), it's rather non-trivial to infer \(\) through Bayes rule: \(p(|,)=p()p(,| )/p(,)\). A reason is that the prior \(p(,|)\) is intractable. We resort to the variational inference to approximate the intractable true posterior \(p(|,)\) with \(q()\) by constraining the KL-divergence \(D_{KL}(q_{}()||p_{}(|,))\).

To make the gradient computable, we employ the reparameterization trick to derive \(p(|,)=()\), where \(()\) denotes the Bernoulli distribution based on the probability vector. Concretely, we take the graph convolution feature \(\) and the matrix \(\) of reference set as input, to learn the sampling probability \(\). A simple formulation with a fully-connected operation is given as follows:

\[=(((,))),\] (7)

where \((,)\) computes the pairwise similarities (across all graph nodes and all reference points) through the cosine measurement, and \(()\) is the sigmoid function.

### Bernoulli Hybrid Graph Pooling

Based on the above inferred sampling factor \(\), we propose a hybrid graph pooling to learn expressive substructures while endeavoring to reserve graph information. The hybrid graph pooling contains two components: Bernoulli node dropping and Bernoulli node clustering. The former prunes those unsampled nodes and associated edges to generate a compact graph; the latter clusters neighbor nodes to form a coarsening graph.

**Bernoulli Node Dropping.** Let \(^{}\) denotes the indices of the preserved nodes according to the sampling factor \(\), thus a projection matrix \(\{0,1\}^{ n}\) can be defined formally:

\[=()[,:],\] (8)

where \(()\) is the vector diagonalization operation, and \([,:]\) extracts those rows of \(\) w.r.t the indices \(\). Accordingly, the compact subgraph \(}=(},})\) can then be computed by:

\[}=,}= ^{},\] (9)

where \(}^{ d}\) is the feature matrix of the compact graph, and \(}^{}\) is the subgraph adjacency matrix. Intuitively, the dropping directly removes those unselected nodes and connective edges from the input graph.

**Bernoulli Node Clustering.** After Bernoulli sampling, we can obtain those reserved nodes, which could be used as the clustering centers. Hereby, we only need to transmit those unselected nodes' messages to cluster centers, which would preserve more information of the whole input graph. Compared with previous graph clustering methods, Bernoulli clustering is more efficient because the learning of the assignment matrix is bypassed, while increasing the diversity of graph perception. To be specific, our assignment matrix \(^{}^{ n}\) is from the original adjacency matrix \(\), and we can get the coarsening graph as:

\[^{}=(()[,:]),}^{}=^{},\] (10)

where \(}^{}^{ d}\) denotes the diffusion features based on input graph, the coarsening graph has the same adjacency matrix \(}_{i}\) as in Eqn. (9).

**Hybrid Graph.** Based on the aforementioned operations, we can obtain the compact subgraph \(}=(},})\) and the coarsening graph \(}^{}=(}^{},})\), which reflect different aspects of information in the original graph. To fully exploit the extracted informative node representations and expressive substructures, the two subgraphs are fused to form the final pooled graph by the following formulation:

\[}=((}+}^{ })_{h}),\] (11)

where \(\) denotes a non-linear activation function and \(_{h}^{d d}\) is a learnable weight, and \(}^{ d}\) is the aggregated node embeddings of the pooled graph.

### Readout Function

The proposed framework repeats the graph convolution and BernPool operations three times. To obtain a fixed-size graph-level representation, we apply the concatenation of max-pooling and mean-pooling in each subgraph following the previous works [37; 29; 22]. Finally, those graph-level representations can be summarized to form the final embeddings:

\[=_{l=1,}^{(l)},^{(l )}=(^{(l)}}_{i=1}^{n_{i}^{(l)}}}_{i} ^{(l)}||_{i=1}^{n_{i}^{(l)}}}_{i}^{(l)}),\] (12)

where \(}_{i}^{(l)}\) denotes the \(i\)-th node feature at the \(l\)-th pooling, \(\) is the same non-linear activation function, and \(||\) denotes the feature concatenation operation. The resulting embeddings would finally be fed into a multi-layer perceptron to predict graph labels.

### Computational Complexity

The computational complexity of one-layer BernPool can be expressed as \(O(N K d+N^{} N d+N^{} d d)\), where \(N\) denotes the number of nodes, \(K\) is the number of reference points, d is the dimensionality of nodes features, \(N^{}\) represents the number of preserved nodes. Specifically, the complexity of probability score computation is \(O(N K d)\), and the complexity of the Bernoulli hybrid graph pooling module is \(O(N^{} N d+N^{} d d)\).

## 5 Experiments

### Experimental Setup

**Datasets**. To comprehensively evaluate our proposed model, we conduct extensive experiments on eight widely used datasets in the graph classification task, including three social network datasets (IMDB-BINARY, IMDB-MULTI  and COLLAB) and five Bioinformatics datasets (PROTEINS, DD, NCI1, Mutagenicity and ENZYMES). The detailed information and statistics of these datasets are summarized in Table. 1.

**Baselines**. We compare our proposed method with several state-of-the-art graph pooling methods, including three backbones (GCN, GAT, GraphSAGE), eight node drop graph pooling methods (TopkPool, SAGPool, ASAP, VIPool, iPool, CGIPool, SEP-G  and MVPool ), three clustering pooling methods (Diffpool, MincutPool, and StructPool), three global pooling methods (Set2Set, SortPool, DropGIN) and one other pooling method (EdgePool).

**Implementation Details**. We employ the 10-fold cross-validation protocol following the settings of [29; 22] and report the average classification accuracies and standard deviation. For all used datasets, we set the expected pooling ratio as 0.8, the node embedding dimension \(d\) as 128, the number of reference points as 32, and the hyper-parameter \(\) in Eqn. 6 as 5. We adopt the Adam optimizer to train our model with 1000 epochs, where the learning rate and weight decay are set as 1e-3 and 1e-4, respectively. Our proposed BernPool is implemented with PyTorch and Pytorch Geometric .

### Comparison with the state-of-the-art Methods

The graph classification results of BernPool and other state-of-the-art methods are presented in Table 1. In general, most hierarchical pooling approaches including our proposed BernPool can perform better than those global pooling ones in the graph classification task. This may be because global pooling methods ignore the hierarchical graph structures in generating graph-level representation. In particular, our BernPool achieves state-of-the-art performance on all datasets, which demonstrates the robustness of our framework against graph structure data variation. In contrast, previous methods cannot perform well on all eight datasets, while the second highest performances on different datasets are obtained by five different methods. Compared with those methods, Our BernPool outperforms respectively by 1.09%, 3.16%, 13.6%, 2.7%, and 1.86% on the PROTEINS, Mutagenicity, ENZYMES, IMDB-BINARY and COLLAB datasets.

[MISSING_PAGE_FAIL:7]

Notably, our BernPool employs just one channel can achieve good results, which demonstrates the effectiveness of BernPool leveraging a probabilistic manner to infer sampling factors. We can observe that the "BernPool w/o Dropping" outperforms "BernPool w/o Clustering" by 0.46%, 0.42%, 0.70%, and 0.54% on PROTEINS, DD, IMDB-BINARY, and COLLAB datasets, respectively. Furthermore, jointly using both channels can outperform either "BernPool w/o Dropping" or "BernPool w/o Clustering", which verifies the effectiveness of our proposed hybrid graph pooling.

**Comparison between the probabilistic and deterministic manner.** To make clear the benefit of our proposed probabilistic sampling method, we conduct experiments by replacing Bernoulli sampling with Topk pooling which is in a deterministic manner. In the TopK experiments, we still employ reference set to assess the importance of nodes and the hybrid graph pooling module to jointly learn representative sub-structures and preserve graph topological information. The comparison between "BernPool-TopK" and "BernPool" is presented in Table 4. It can be observed that BernPool outperforms BernPool-TopK on all used eight datasets, especially 7.83% accuracy gains in the ENZYMES dataset, verifying the effectiveness of our designed Bernoulli-deduced sampling strategy.

**Benefit of the orthogonality for reference set.** To evaluate the benefit of orthogonality for the reference set, we conduct experiments that remove the orthogonal constraint from BernPool (referred to as "BernPool w/o orthogonal" in Fig. 2(a)). The results demonstrate that employing the orthogonal reference set can improve average accuracy by more than 0.6% on three datasets. This verifies the effectiveness of the orthogonal constraint for the reference set.

**Model parameter quantity comparison.** We compare the test accuracy and parameter quantity (only the pooling layer) of BernPool with other pooling methods in the PROTEINS dataset, where the hidden layer dimension is set to 128. BernPool achieves superior performance while using fewer parameters. Specifically, BernPool owns 97% fewer parameters than CGIPool and 76% fewer parameters than ASAP. In terms of accuracy, BernPool performs the best, achieving 9.19% higher than CGIPool and 9.10% higher accuracy than ASAP.

    &  &  \\   & PROTEINS & DD & NCI1 & Mutagenicity & ENZYMES & IMDB-B & IMDB-M & COLLAB \\  \#Graphs(Classes) & 1113 (2) & 1178 (2) & 4110 (2) & 4337 (2) & 600 (6) & 1000 (2) & 1500 (3) & 5000 (3) \\  BenPool-TopK & 81.77\(\)3.53 & 82.09\(\)3.25 & 81.09\(\)1.77 & 83.26\(\)1.08 & 68.17\(\)3.72 & 80.20\(\)3.74 & 55.47\(\)3.51 & 84.50\(\)1.40 \\ BernPool & 83.29\(\)3.69 & 83.27 \(\) 2.95 & 81.44 \(\) 1.09 & 83.81 \(\) 1.43 & 76.00 \(\) 3.78 & 81.30 \(\) 3.5 & 55.93 \(\) 3.8 & 85.26 \(\) 1.35 \\   

Table 4: **Performance comparison between deterministic and probabilistic manner.**

    &  &  \\   & PROTEINS & DD & NCI1 & Mutagenicity & ENZYMES & IMDB-B & IMDB-M & COLLAB \\  \#Graphs(Classes) & 1113 (2) & 1178 (2) & 4110 (2) & 4337 (2) & 600 (6) & 1000 (2) & 1500 (3) & 5000 (3) \\  GCN & 74.84\(\)2.82 & 78.12\(\)4.33 & 76.3\(\)1.8 & 79.8\(\)1.6 & 50.00\(\)5.87 & 72.67\(\)4.62 & 54.04\(\)3.02 & 71.92\(\)3.24 \\ BernPool-GCN & 83.29\(\)3.69 & 83.27 \(\)2.95 \(\) & 81.44\(\)1.09 \(\) & 83.81\(\)1.43 \(\) & 76.00\(\)3.78 \(\) & 81.30\(\)3.5 \(\) & 55.93\(\)3.8 \(\) & 85.26\(\)1.35 \(\) \\  GAT & 74.07\(\)4.53 & 75.56\(\)3.72 & 74.9\(\)1.7 & 78.8\(\)1.2 & 51.00\(\)5.23 & 74.07\(\)5.43 & 49.67\(\)4.30 & 75.80\(\)1.60 \\ BernPool-GAT & 81.49\(\)3.81 \(\) & 83.44\(\)3.57 \(\) & 81.29\(\)1.77 \(\) & 84.34\(\)1.58 \(\) & 69.50\(\)5.45 \(\) & 81.20\(\)3.39 \(\) & 55.09\(\)4.47 \(\) & 83.86\(\)1.57 \(\) \\  GraphSAGE & 73.75\(\)2.97 & 77.27\(\)4.06 & 74.7\(\)1.3 & 78.9\(\)2.1 & 53.33\(\)3.42 & 72.17\(\)5.29 & 48.53\(\)5.43 & 79.70\(\)1.70 \\ BernPool-GraphSAGE & 83.20\(\)4.21 \(\) & 82.25\(\)3.49 \(\) & 82.34\(\)1.61 \(\) & 84.67\(\)1.26 \(\) & 75.67\(\)3.78 \(\) & 81.60\(\)3.60 \(\) & 55.47\(\)3.87 \(\) & 84.56\(\)1.02 \(\) \\   

Table 2: **Graph classification accuracies of BernPool using different backbones. The default backbone is GCN.**

    &  &  \\   & PROTEINS & DD & NCI1 & Mutagenicity & ENZYMES & IMDB-B & IMDB-M & COLLAB \\  \#Graphs(Classes) & 1113 (2) & 1178 (2) & 4110 (2) & 4337 (2) & 600 (6) & 1000 (2) & 1500 (3) & 5000 (3) \\  BenPool w/o Clustering & 81.94\(\)4.27 & 81.66\(\)2.82 & 78.22\(\)7.69 & 82.57\(\)1.44 & 73.00\(\)3.67 & 80.60\(\)3.31 & 55.80\(\)4.11 & 84.52\(\)1.44 \\ BernPool w/o Dropping & 82.40\(\)4.02 & 82.08\(\)3.98 & 76.20\(\)10.24 & 82.75\(\)1.44 & 72.50\(\)4.25 & 81.30\(\)3.23 & 55.60\(\)4.16 & 85.06\(\)1.18 \\ BernPool & 83.29\(\)3.69 & 83.27 \(\) 2.95 & 81.44 \(\) 1.09 & 83.81 \(\) 1.43 & 76.00 \(\) 3.78 & 81.30 \(\) 3.5 & 55.93 \(\) 3.8 & 85.26 \(\) 1.35 \\   

Table 3: **Performance Comparison between BernPool and its variants.**

**Inference time comparison.** The computation complexity of our BernPool is described in Section 4.6. Moreover, the inference time also plays a crucial role in evaluating the efficiency of pooling methods. Thus we conduct experiments on the PROTEINS dataset containing about 20,000 nodes and 70,000 edges to compare the inference time (single-layer). As shown in Fig. 2(c), BernPool costs less inference time than EdgePool and ASAP while maintaining superior performance. Notably, SAGPool has a similar inference time as our method, but our method outperforms it in terms of classification accuracy. The comparison results verify the high efficiency of our BernPool.

**Sensitivity of Hyper-parameters.** To evaluate the sensitivity of hyper-parameters, including the pooling ratio, layer number, and hidden layer dimension, we additionally conduct experiments on PROTEINS and NCI1 datasets. Specifically, we vary the pooling ratio from 0.2 to 0.8 with a step length of 0.2, the number of layers from one to four, and the hidden layer dimensions range from 16 to 128. The results are presented in Fig. 3. We can observe that BernPool overall exhibits robustness to variations of parameters. However, performance fluctuations can be observed on the NCI1 dataset when we evaluate the effect of different pooling ratios for BernPool. This may be attributed to the relatively less average number of nodes resulting in less information being retained after performing three pooling layers consecutively. The results shown in Fig. 3(b) indicate that setting the layer number to three achieves the best performance on the PROTEINS and NCI1 datasets. However, increasing the number of layers will require more computation resources and longer training time. As shown in Fig. 3(c), the highest accuracy is achieved when the dimension size is set as 128. With the dimension increasing, the accuracy presents a slight increase trend, which suggests that increasing the dimension size can enhance the model's capacity to capture more complex representations of the input graph. However, larger dimension sizes increase computation burdens. Thus we set the pooling ratio as 0.8, the layer number as 3, and the dimension as 128 in our framework.

## 7 Conclusion

In this paper, we proposed a simple and effective graph pooling method, called Graph Bernoulli Pooling (BernPool) to promote the graph classification task. Specifically, a probabilistic Bernoulli sampling was designed to estimate the sampling probabilities of graph nodes, and to further extract more useful information, we introduced a learnable reference set to encode nodes into a latent expressive probability space. Compared with the deterministic way, BernPool possessed more diversity to capture salient substructures. Then, to jointly learn representative substructures and preserve graph topology information, we proposed a hybrid graph pooling paradigm that fuses two pooling manners. We evaluate BernPool on multiple widely used datasets and dissected the framework with ablation analysis. The experimental results show that BernPool outperforms state-of-the-art methods and demonstrates the effectiveness of our proposed modules.

Figure 3: The performance of different hyper-parameters on PROTEINS and NCI1 datasets.

Figure 2: The benefit of orthogonality reference set and comparison of parameter quantity and inference time.