# Identifying and Addressing Delusions for Target-Directed Decision-Making

Mingde Zhao\({}^{1,2,3}\), Tristan Sylvain\({}^{2}\), Doina Precup\({}^{1,3,5}\), Yoshua Bengio\({}^{3,4}\)

\({}^{4}\)McGill University, \({}^{2}\)Borealis AI, \({}^{3}\)Mila, \({}^{4}\)Universite de Montreal, \({}^{5}\)Google DeepMind

mingde.zhao@mail.mcgill.ca, tristan.sylvain@borealisai.com

dprecup@cs.mcgill.ca, yoshua.bengio@mila.quebec

###### Abstract

Target-directed agents utilize self-generated targets, to guide their behaviors for better generalization. These agents are prone to blindly chasing problematic targets, resulting in worse generalization and safety catastrophes. We show that these behaviors can be results of delusions, stemming from improper designs around training: the agent may naturally come to hold false beliefs about certain targets. We identify delusions via intuitive examples in controlled environments, and investigate their causes and mitigations. With the insights, we demonstrate how we can make agents address delusions preemptively and autonomously. We validate empirically the effectiveness of the proposed strategies in correcting delusional behaviors and improving out-of-distribution generalization.

## 1 Introduction

An important use of Reinforcement Learning (RL) is to learn skills generalizable for application, after training from limited training tasks. Despite numerous efforts to bridge the "generalization gap" between performance during training and evaluation, generalization abilities of RL agents remain largely unsatisfactory. Recent works attributed this to existing agents' lack of _reasoning_ abilities to face Out-Of-Distribution (OOD) changes (Di Langosco et al., 2022).

Thus, embracing the concept of intentions, a certain type of decision-time planning agents are developed, which make use of their more adaptive planning outcomes, which we call _targets_, to direct its behaviors to generalize better in novel situations (Alver and Precup, 2022). For instance, these targets can help decompose a complicated task into small and familiar steps. In this paper, we use _targets_ to denote (sub-)goals produced by the agents themselves during decision-time planning. These "target-directed" agents make use of _generators_ to propose candidate targets, as well as optional _estimators_ to evaluate the favorability of the candidates, to select a target to commit to.

While target-directed agents were supposed to be able to generalize well OOD, they are often observed to be blindly chasing unreachable or unsafe targets (Bengio et al., 2024). This paper focuses on the investigations of these behaviors, which existing literature has largely neglected. We introduce the readers to a new perspective to show that these agents are likely designed to be delusional. Unlike hallucinations, _delusions_ are obviously wrong beliefs that an agent holds, and reflect an _inability to reject false beliefs_(Kiran and Chaudhury, 2009). These in the RL context correspond to false beliefs that are _natural_ results of an agent's improper learning process, which limit the agents' situational understanding and lead to worse generalization or even safety catastrophes. Delusional behaviors stem from the incoordination of the _generator_ and the _estimator_: problematic targets should not be generated, and even if they were, they should not be favored (Corlett, 2019).

This paper discusses some necessary conditions for agents to address delusions autonomously and preemptively: 1) incorporation of an estimator, enabling the rejection of problematic targets; 2) adequate learning rules with which false beliefs about targets can be counteracted and 3) proper training data that can counteract delusions. With clear diagnoses in the controlled environments, we identify types of delusions, and investigate proper designs of the training processes. We then applied our ideas to hindsight relabeling, a fundamental form of target-directed RL training, and validated experimentally that our strategies led to lower delusion-related errors and reduction in delusional behaviors, resulting in significant improvements in OOD generalization performance.

## 2 Preliminaries

**RL & Problem Setting.**: An RL agent takes actions to interact with an environment to gain rewards. The interaction may be modeled as a Markov Decision Process (MDP) \(,,P,R,d,\), where \(\) and \(\) are the sets of states and actions, \(P:()\) is the state transition function, \(R:\) is the reward function, \(d:()\) is the initial state distribution, and \((0,1]\) is the discount factor. An agent needs to learn a policy \(:()\) that maximizes the value, _i.e._ the expected discounted cumulative reward \(_{,P}[_{t=0}^{T_{}}^{t}R(S_{t},A_{t},S_{t+1})|S_{0}  d]\), where \(T_{}\) denotes the timestep when the episode terminates. Often, environments are partially observable, which means, instead of a state, the agent receives an observation \(x_{t+1}\), with which the agent needs to infer the state from the history.

**Target-Directed RL** offers a perspective to better identify delusions, inspired by delusion-related research in psychiatry (Kiran and Chaudhury, 2009). It abstracts existing methods that belong to the intersection of decision-time planning and generalized goal-conditioned RL (Zadem et al., 2024). The framework emphasizes an algorithmic design, where a _generator_ proposes candidate targets, and an (optional) _estimator_ evaluates them and select one target for policies to execute (Dayan and Hinton, 1992). The two components correspond to the belief formation and belief evaluation systems, respectively, whose incoordination causes delusions in the human brain, as shown in Fig. 1.

**Source-Target Pairs & Hindsight Relabeling Essentially, target-directed agents are trained on "source-target pairs", which are organized training data that could let the agents learn about the relationship between the source state and the target state. Training target-directed agents with only contemporary targets (the ones being followed) can lead to poor performance (Dai et al., 2021), since contemporary targets may be low-quality, hard to achieve, or lack in diversity (Moro et al., 2022; Davchev et al., 2021). Hindsight Experience Replay (HER) is proposed to be an experience organization mechanism, transforming collected experiences into relabeled transitions augmented with alternative targets (Andrychowicz et al., 2017). HER augments a transition \( x_{t},a_{t},r_{t+1},x_{t+1}\) with an additional observation \(x^{}\) (or its encoding), the relabeled target, creating a _source-target pair_, with one decision point for the current step and another for the relabeled target. **Relabeling strategies**, which correspond to how \(x^{}\) is selected, are critical for the performance of HER-trained agents (Shams and Fevens, 2022). Most popular choices are _trajectory-level_, meaning \(x^{}\) comes from the same trajectory as \(x_{t}\). These include "future", where \(x^{}=x_{t^{}}\) with \(t^{}>t\), and "episode", with \(0 t^{} T_{}\).

**SSM** In existing works, failure modes of target-directed agents are often overlooked, possibly due to a lack of access to ground truths in benchmark environments. To identify the causes, and provide intuitive examples, we craft a set of fully-observable environments based on the MiniGrid-BabyAI framework (Chevalier-Boisvert et al., 2018; Hui et al., 2020), named SwordShieldMonster (SSM for short). In SSM, the agent moves one step at a time in \(4\) absolute directions to navigate fields with episode-terminating, randomly placed lava traps. The density of traps is specified by a _difficulty_ parameter \(\), while guaranteeing a viable path to success, _i.e._ acquiring a sparse terminal reward. Agents must collect a sword and a shield, both also randomly placed, before reaching the "monster" guarding the reward. Reaching the monster prematurely ends the episode. Visualizations of SSM are in Fig. 2, with examples of delusional behaviors discussed later. The sword and shield are picked up by moving to their respective grid cells, and agents cannot drop them. Such design introduces temporary unreachability in the state structure, as non-terminal states are not fully traversable from one to another. Semantically, this segments SSM states into \(4\) equivalence classes, formed by \(2\) binary indicators: the agent's possession of the sword and shield. For example, \( 0,1\) denotes sword not acquired, shield acquired". Despite SSM's simplicity in terms of observation space, a similar set of environments much simpler than SSM was already shown to be a significant challenge for state-of-the-art methods due to the OOD challenges (Zhao et al., 2024).

Figure 1: **Target-Directed Framework:**_A generator proposes candidate target(s). An estimator can be used to evaluate and select the favorable among candidates._

## 3 Delusions in Target-Directed RL

To ultimately enable the agents to address delusions autonomously, we first need to identify where the problems reside. We first look into the generator to find its contributions to the emergence of delusional behaviors, and then locate delusions and their causes in the estimator.

### Generator's Role in Delusional Behaviors: Proposing Bad Targets

Most learned generators are subjected to unwanted generalization, thus inevitably generate problematic targets (unreachable or unsafe), a behavior characterized as _hallucination_(Jafferjee et al., 2020). While hallucination can be reduced to a degree, they are generally unavoidable, especially in OOD scenarios. Despite that not all delusions are caused by problematic targets, generator hallucinations pose a major safety risk (Liu et al., 2022; Bengio et al., 2024).

Generators are implemented in various ways, thus it is ineffective to enumerate the detailed causes of problematic target generation. We skip these discussions and directly focus on a categorization of problematic targets, based on an often neglected perspective - the temporal relationship inside the source-target pairs. This leads to the following two _disjoint_ types of problematic targets:

#### 3.1.1 Type G.1 - Nonexistent

The first type of problematic targets includes those that do not _correspond_ to valid states in the task MDP. They can be classified into: 1) **Invalid**: semantically invalid targets for the task, such as a target SSM observation generated without the agent's location; 2) **Impossible**: semantically valid but unreachable targets, _e.g._ an SSM target cell surrounded by lava traps, (such as in Fig. 2 a). G.1 targets only cause delusional behavior if the estimator produces matching E.1 delusions, to be discussed later. Imperfectly generated targets _corresponding_ to valid states may not be problematic, as estimators may filter out imperfections, possibly using a state encoder.

#### 3.1.2 Type G.2 - Temporarily Unreachable

The second type includes targets that correspond to valid states in the task MDP, but _cannot be fulfilled from the current state_. Unlike G.1, these G.2 targets are only temporarily inappropriate and could be perfectly good targets if the agent were in a different state. For the time-dependency, G.2 can be easily overlooked. There are some notable subtypes, such as: 1) **Irreversible**: targets once reachable but are now blocked due to a past transition. For instance, after acquiring the sword in SSM, the agent transitions from class \( 0,0\) to \( 1,0\), sealing off access to \( 0,0\) or \( 0,1\); 2) **Segregated**: targets that can only be reached from specific initial states that are not available to the agent. For example, an agent spawned in class \( 1,0\) cannot reach states in \( 0,0\) or \( 0,1\).

G.2 targets are often overlooked in literature, since hallucinations are mostly discussed in contexts without temporal progression, and also they do not exist in all MDPs. G.2 can appear more frequently if we do not avoid training the generators with them. For example, a conditional target

Figure 2: **Delusional Behaviors in SSM**: _The agent location is marked as a red triangle, lava traps’ as orange squares, and monsters’ as green loops. In both cases, the estimators, lacking understanding of the problematic targets, fail to estimate the reachability of the problematic targets proposed at decision-time (yellow dots), leading to the delusions that shorter paths to the task goal exist through the problematic targets.

generator which learns from "episode" will be more likely to produce G.2 targets (compared to "future"), such as the one in Fig. 2 b). This applies to training beyond HER as well.

Problematic targets such as G.1 and G.2 can cause delusional behaviors of chasing unreachable targets, if they are favored during decision-time.

### Estimator's Role in Delusional Behaviors: Bad Evaluation

An estimator evaluates the favorability of proposed targets, based on some relationship between the current states (sources) and the candidates (targets), which often involve assessing the implementability of targets, represented as, _e.g._, distances, _etc._ Acting as a firewall, the estimator of a target-directed agent can enable the selection among the candidates, enabling the potential to filter out hallucinated targets. Agents without estimators, _e.g._ Director(Hafner et al., 2022), must accept proposed targets unconditionally, and thus are at significant risk of chasing problematic targets.

Incorporating an estimator only delegates the responsibility of avoiding delusional behavior from the generator to the estimator, as the estimators' own delusional beliefs about targets can directly cause delusional behaviors. A lack of either of the following two necessary aspects results in blind spots of knowledge about targets, _i.e._ delusions regarding both delusional or _non-delusional_ targets.

**Update Rules**: we expect estimators to learn to reject problematic, undesirable or unsafe targets, by evaluating them with low favorability. However, such expectation may not be achieved, if the estimators' update rules do not have the characteristic that _(continuously) punishes unachieved / unsafe targets_, when these targets are sampled during training for the estimator to learn from.

**Training Data** Even with proper update rules, delusions can still arise from the lack of exposure to data counteracting the estimators' false beliefs. This is often caused by improper experience organization mechanisms, responsible for transforming the experienced interactions with environments into training data, as shown in Fig. 1. Despite these mechanisms like HER can lead to more diversity in training data, they are often accompanied by delusions: **1)** Certain relabeling strategies naturally cause delusions. For instance, "future" only relabels with future observations, thus only exposes a learner to future reachable targets, leaving the estimator to guess when a "past" target is proposed at decision-time; **2)** Trajectory-level relabeling can also be problematic. Short trajectories, common in many training procedures, only cover limited portions of the state space and prevent estimators from learning about distant targets, leaving estimators to guess when a distant target is generated at decision-time. Short trajectories can be a product of experimental design (initial state distributions, Maximum Episode Lengths, MELs (Erraqabi et al., 2021)) or innate environmental characteristics (density of terminal states). It is worth noting that delusions caused by improper training data (organizing mechanisms) also extend to cases like few-shot generalization.

With these necessary conditions, we identify types of estimator delusions based on the targets they evaluate. We use the identifiers E.0, E.1, and E.2 for their resulting delusional behaviors.

#### 3.2.1 Type E.0 - Misevaluating Non-Delusional Targets

The standalone type of delusions describes false estimations about _non-delusional_ targets. E.0 delusions can lead to undesirable or delusional targets being favored and thus can hurt generalization.

#### 3.2.2 Type E.1 - Misevaluating G.1 Targets

This type of delusion appears when an estimator misevaluates the favorability of a G.1 target. If effective learning rules are present, this can still be caused by the lack of necessary training data (_e.g._, relabeled transitions with G.1 targets in buffer). In Fig. 2 a), an example is visualized. Note that the resulting E.1 behaviors, _i.e._ those chasing selected G.1 targets, can be potentially catastrophic if the G.1 targets are beyond safety constraints.

#### 3.2.3 Type E.2 - Misevaluating G.2 Targets

Similar to E.1, in Fig. 2 b), an example of delusional behavior is visualized, a result of a combination of G.2 (temporarily unreachable) and E.2. E.2 behaviors can hurt the agents' generalization abilities.

Addressing Delusions in Target-Directed Agents

Having identified delusions, taking advantage of the shared reliance on source-target pairs, we can develop strategies applicable to general target-directed agents coming from various training procedures. Then, we provide examples on how to materialize such strategies in agents trained with HER, because of its popularity, its direct association with source-target pairs and the fact that one can trivially transform HER-based training into losses established over two sampled transitions from ordinary experience replays.

Without the loss of generality, assume we are dealing with a hypothetical target-directed agent with a dual-component architecture as in Fig. 1, learning both components _exclusively_ from hindsight-relabeled transitions. The generator proposes targets corresponding to potentially distant states, and the estimator learns the favorability of targets by (continuously) punishing the unachieved or unsafe targets sampled during training. Note that even for methods whose generators are not trained with HER, our estimator-focused strategies still apply.

As discussed earlier, to address delusions, we need effective _update rules_ and _training data_. By assuming this framework, we skip discussing update rules, as they depend on the specific designs of the chosen target-directed agents. Our emphasis on the training data shines light on the fact that most existing RL agents only learn from experienced data, while addressing delusions requires learning from targets that can never be experienced. This corresponds to a discrepancy between (most existing) target-directed agents' behaviors and training: at decision-time, targets outside experience can be proposed; while during training, only experienced targets are learned from.

### Assistive Strategies for Addressing Delusions

We first introduce two ideas to improve training data distributions, which can then be materialized as two hindsight relabeling strategies for HER. These ideas seek to expand the support of the training data distribution, to include those source-target combinations that the agent could never experience, _i.e._ those involving G.1 and G.2 targets. Since our proposed ideas do not rely on additional assumptions, they should be expected to be applicable generally. Additionally, with convergent learning rules (provided by the target-directed framework that these strategies are applied to), the strategies should also lead to the correct estimation of G.1 and G.2 targets.

Later, we compare the ideas behind all HER strategies, including the existing and the newly proposed, and study how the two groups can be combined to accommodate the learning needs.

#### 4.1.1 "Generate": Let Estimators Learn About Candidates (to be generated)

The first strategy, named "generate", is to _let the estimator learn about targets that could be proposed at decision time, s.t._ it could figure out preemptively that problematic targets are not favorable.

Zhao et al. (2024) identified delusional behaviors resulted from E.1 delusions, in the language of this paper, and proposed to train the estimator additionally with candidate targets proposed by the generator. With HER, we can transform this auxiliary loss into a Just-In-Time (JIT) HER strategy that, as a transition is sampled for training, relabels it with a proposed target by the generator. We can expect "generate" to be effective for training estimators, as the estimators will get exposed to all kinds of problematic targets that the generator could offer: When a generator learns to generate mostly viable targets, "generate" will be primarily effective at dealing with the E.0 and E.1 (G.1s are unlikely to be eradicated). "generate" requires the use of the generator, thus it incurs additional computational burden, depending on the complexity of target generation processes.

Adapting "generate" beyond HER should be straightforward: whenever a target is involved during estimator learning, we may replace it with generated targets. "generate" comes at a cost of extra computation, to which attention may be required in real-world applications with speed demands.

#### 4.1.2 "petask": Let Estimators Learn About Experienced Targets Outside the Episode

The second strategy, namely "petask", is to _expose the estimator to all targets experienced before_, s.t. it could figure out that some previously achieved targets are unreachable from the current state.

We materialize "pertask" into an assistive atomic strategy that relabels transitions with observations from the same training task, sampled across the entire memory. Importantly, "pertask" brings exposure to the estimator to learn against E.2 delusions and to the long-distance source-target pairs against E.0 caused by short trajectories. Take SSM as an intuitive example, a current state in situation \( 1,0\) in the current episode can now be paired with a target in \( 0,1\) in another episode, _s.t._ the estimator learns such G.2 target is unreachable, as shown in Fig. 4 (in Appendix).

It is worth noting that "pertask" also biases the training data distribution, making the agent spread out its efforts into learning the source-target pairs potentially far away from each other. Despite increasing training data diversity, long-distance pairs are less likely to contribute to better decision-making compared to the shorter-distance in-episode ones offered by "episode", as shown later.

Adapting "pertask" beyond HER requires algorithmic designs recording all past observations.

Both strategies help the estimator figure out the features shared by problematic targets, _s.t._ OOD delusions can also be identified. Note that "pertask" cannot be used to address E.1 delusions. However, "generate" can be used for E.2 if the generator generates the respective problematic targets.

### Mixtures

Creating a mixture of sources of training data increases the diversity of source-target combinations. For HER specifically, each atomic strategy, enumerated in Tab. 1, exhibits a tradeoff in estimation accuracy among all sorts of source-target pairs, including short-distance and long-distance ones involving only valid targets, and those involving problematic targets.

A mixture of more-than-one atomic strategies in certain proportions while relabeling (Nasiriany et al., 2019; Yang et al., 2021), can be used to achieve a tradeoff in HER-based training, _s.t._ the shortcomings of each atomic strategy are mitigated by the introduction of others.

When the training budget is fixed, _i.e._, training frequency, batch sizes, _etc._, stay unchanged, the mixing proportions of strategies pose a tradeoff to the investment towards different kinds of source-target pairs, and the resulting accuracies. In experiments, we show that, assisting "episode" with "generate" and "pertask" often results in better performance in estimator training, striking a balance between the investment of training budgets in non-delusional and delusional estimations.

Mixtures of multiple relabeling strategies can also be extended to other training procedures, and can also be implemented by independent training losses based on different sources.

### Hybrid Strategies - A \(2\)-Slotted Approach

While an estimator needs the exposure to problematic targets to counteract delusions, a generator benefits from learning to generate only useful targets, and should avoid exposure to problematic targets. Thus, generators and estimators often have conflicting needs during training.

  
**Strategies** & **Advantages** & **Disadvantages** & **Gist** \\   “episode” \\  } & Efficient for estimator & Can cause G.2 targets when used to train & Creates training data \\  & to learn close-proximity & generators: When used exclusively to train & with source-target pairs \\  & relationships & estimator, 1) cannot handle E.2; 2) prone to E.0 - & sampled from the same \\  & cannot learn well from short trajectories & episodes & \\   “future” \\  } & Can be used to learn a & In addition to the shortcomings of “episode” & Creates training data \\  & conditional generator & (those for estimators only, this additionally & with temporally ordered \\  & with temporal & causes E.0 when used as the exclusive strategy & source-target pairs from \\  & abstractions & for estimator training & the same episodes \\   “generate” \\  } & Addresses E.1 with data & Relies on the generator with additional & Augments training data \\  & diversity (also E.2 when & computational costs; Potentially low efficiency & to include candidate \\  & generator produces G.2) & in learning non-delusional relationships. & targets proposed at \\   “pertask” \\  } & Addresses estimator & Can cause extensive G.2 targets if used to train & Augments training data \\  & delusions (E.2 \& E.0 for & generators; low efficiency in learning & to include targets that \\  & long-distance pairs) & close-proximity source-target relationships & were experienced \\   

Table 1: **Hindsight Relabeling Strategies & The Ideas Behind**: “_episode” and “future” are widely used as they increase sample efficiency in non-delusional cases significantly; “_generate” and “pertask”, proposed in this paper, are effective against delusions, useful in specific scenarios._Each source of training data has its own biases. Atomic hindsight relabeling strategies in HER, as shown in Tab. 1, can simultaneously help one component while hindering another. Instead of trying to tradeoff the needs of both components with a single source of training data, we propose a hybrid (2-slotted) approach, allowing the generator and estimator to receive data tailored to their needs, through two independent relabeling processes. This combination of 2 slots and mixtures can produce various hybrid strategies, as demonstrated in Sec. 5.4.

Separating training data for the generator and estimator based on their needs is straightforward in training procedures beyond HER.

_In the Appendix, we provide summaries for the design of mitigation strategies on agents beyond HER._

## 5 Experiments

To investigate the effectiveness of our strategies against delusions and how they help target-directed agents achieve better generalization, we implement a combination of _environments_ with clearly defined delusional cases, as well as _methods_ whose estimators can be easily analyzed. This leads to our \(4\) sets of experiments, coming from a combination of \(2\) environments (one dominantly haunted by G.1, and another by G.2) and \(2\) target-directed frameworks. _Due to page limit, \(3\) out of \(4\) sets of experiments are presented in the Appendix. We summarize those at the end of this section._

### Environments \(}\) Settings

For environments, we favor tasks where the delusions are present, dangerous and intuitive to inspect, for which the introduced SSM offers us clear advantage. _Introduction and the results on another environment are presented in the Appendix._

For each training seed, we sample and preserve \(50\) training tasks of size \(12 12\) and difficulty \(=0.4\). All agents are trained for \(1.5 10^{6}\) interactions by randomly selecting one of the \(50\) frozen tasks for each training episode. To speed up training, we make the initial state distributions span all the non-terminal states in each training task. This change increases risks of E.2, due to the presence of dense episode-terminating lava traps and relatively short MELs (\(128\) for SSM).

### Evaluative Criteria

We use the following criteria to investigate the changes in agent's estimations and behaviors:

**Estimation Error - E.0, E.1, E.2 & Non-Delusional**: At each evaluation point, we solve the ground truth distances between states and inspect the agents' estimation errors. The errors are split based on the source-target pairs, into those involving problematic targets, _i.e._, E.1 and E.2, and those involving only valid targets, which includes the case of E.0. The estimation errors show the internal degree of delusions about targets within the estimators.

**Delusional Behavior Frequencies**: We also monitor the frequency of a problematic target (G.1 or G.2) being chosen by the agents, as the result of their delusions in decision-time planning. The frequencies demonstrate how often delusional targets are being favored because of delusions.

**Improvement on OOD Generalization Performance**: We analyze the changes in agents' OOD generalization performance, due to the strategies introduced to handle delusions. The evaluation tasks are sampled from a gradient of OOD difficulties - \(0.25\), \(0.35\), \(0.45\) and \(0.55\). For aggregated OOD performance, such as in Fig. 3 g), we sample \(20\) tasks from each of the \(4\) OOD difficulties, and combine the performance across all \(80\) episodes, which have a mean difficulty matching the training tasks. By comparing the resulted performances of atomic strategies with those of the newly proposed hybrids, we can also have an intuitive grasp of the "delusion gap", which is the amount of performance degradation caused by delusions. To maximize difficulty, the initial state is fixed in each novel evaluation task instance: the agents are not only spawned to be at the furthest side of the monster, but also in semantic class \( 0,0\), _i.e._, without the sword nor the shield in hand.

### Methods

We seek to demonstrate the generality of our strategies by applying them onto different methods with very different behaviors. Thus, we adapted the following target-directed methods:

Skipper: generates candidate target states that, together with the current state, form a directed graph at decision-time. The states act as vertices and the edges are pairwise estimations of cumulative rewards and discounts (interchangeable with distances), under its evolving policy. A target state is chosen after applying _value iteration (VI)_, _i.e._ the favorability of targets are defined by the \(Q\) values of the planned paths (Zhao et al., 2024).

LEAP (in Appendix): Instead of VI, LEAP uses an evolutionary algorithm to evolve sequences of subgoals leading to the task goal, with distances as fitness. The immediate subgoal of the elitest sequence is chosen to be the target, and used to condition a lower-level policy. LEAP is prone to delusions, since one problematic subgoal can destroy a whole sequence (Nasiriany et al., 2019).

For both agents, the generators and estimators are trained with HER, allowing easy switching of re-labeling strategies for comparison. Roughly, both agents employ update rules that can reject wrong estimations with data exposure (more discussions in Appendix). Targets are acquired by encoding observations imagined by the generators, where G.1 & G.2 propositions are clearly identified.

### Hindsight Relabeling Strategies

We focus on the following _baselines_ for fairer comparison (excluding bad generator performance):

* **"F-E"**: "future" for generator, "episode" for estimator;
* **"F-P"**: "future" for generator, "pertask" for estimator;
* **"F-G"**: "future" for generator, with \(100\%\) chance using "generate" JIT on estimator;

Additionally, the following hybrid strategies are proposed and compared:

* **"F-(E+G)"**: A hybrid strategy mostly for E.1. "future" for generator, "episode" for estimator with \(50\%\) chance using "generate" JIT, resulting in a half-half mixture of "episode" & "generate";
* **"F-(E+P)"**: mostly for E.2. "future" for generator, half "episode" & half "pertask" for estimator;
* **"F-(E+P+G)"**: for both E.1 & E.2 behaviors. "future" for generator, mixture of \(2/3\) "episode" and \(1/3\) "pertask" for estimator, with \(1/4\) chance using "generate" JIT, resulting in a mixture of \(50\%\) "episode", \(25\%\) "pertask" and \(25\%\) "generate" for estimator;

### Skipper on SSM(Set 1/4)

**Generators**: We first verify the generator's contributions to delusional behaviors. For the HER-trained generators, Fig. 3**a)**, shows that none among "future", "episode" and "pertask" could significantly address G.1 target generation, validating our claims about the challenges of hallucination. While, Fig. 3**e)** indicates that, "future" generates G.2 delusional targets significantly less frequently than "episode" and "pertask", as the other two wasted training budget on G.2 targets, especially "pertask" that brings in more problematic training samples from long distances. _For fairer comparison in the estimators_, we _only compare variants with "future" for the generator training, _i.e._, "F-*".

**Estimators**: Now, we look into the degrees of delusions inside the learned estimators. Exclusive use of "episode" for estimators resulted in high accuracy in non-delusional cases (cases involving pairs of non-delusional states, "F-E" in Fig. 3**d)**) for source-target pairs of both short and long distances, but low accuracies in E.1 and E.2 cases (Fig. 3**b) & f)**). This shows the source of popularity of these baseline strategies in existing literature; Unsurprisingly, exclusive use of "pertask" results in significantly worse non-delusional short-distance estimates ("F-P" in **d)**), yet much better than "episode" ("F-E") and "future" ("F-F") in long-distance cases (indicating that trajectory-level "future" and "episode" lead to E.0 cases for longer-distance source-target pairs). This is likely because, without the presence of a backbone strategy such as "episode", the estimator wastes its budget learning long-distance non-delusional or E.2 cases; Demonstrating a similar tradeoff, for "generate" ("F-G"), high accuracy is achieved for E.1 related cases, at the sacrifice of E.2 and non-delusional cases; All \(3\) hybrid non-baseline strategies achieve good estimation accuracies in bothshort- and long-distance non-dolusional estimation, as shown in **d)**. At the same time, we can observe particularly significant improvement in accuracy in E.2 delusional estimates in **f)**, by "F-(E+P)" and "F-(E+P+G)", the hybrids assisted by "pertask". These indicate their effectiveness.

**Behaviors**: Finally, we examine how reducing delusions affect the agents' behaviors. For all compared variants, we can deduce from 3 that generally, less frequent G.2 generation **(e)** and lower E.2 errors **(f)** lead to less frequent delusional behaviors in **g)**, which in turn improves the OOD performance in **h)**; "episode" ("F-E") showed mediocre OOD performance **(h)**) for its good estimation accuracy in short-distance non-dolusional cases, but was prone to delusional behaviors; While, despite that "pertask" ("F-P") showed decent accuracies in longer-distance non-dolusional and E.2 cases **(d) & f)**), its low accuracy in short-distance non-dolusional estimation **(d)**) devastated the baseline to the lowest performance in **h)**; Similarly, despite "generate"'s ("F-G"'s) effectiveness in addressing E.1-behaviors **(c)**), its resulting bad estimation accuracies in the non-dolusional cases destroyed its OOD performance. In contrast, all 3 hybrids achieve significantly better OOD performance in **h)**. Assisted by "pertask", "F-(E+P)" and "F-(E+P+G)" performed the best in reducing E.2 delusions (in **f)**) and consequently addressed the most delusional behaviors (in **g)**). This indicates a major presence of E.2 behaviors of Skipper's failure modes on SSM.

### Summary of Experiments

With the proposed strategies, we saw a reduction in delusions in terms of estimation errors and in delusional behaviors in both Skipper and LEAP (in Appendix, more prone to delusions), which led to better OOD generalization performance in \(2\) sets of environments, posing challenges of G.1 & G.2, respectively (more results in Appendix). All 4 sets of experiments align in terms of conclusions.

Figure 3: Skipper **on SSM**: _each mean curve and CI (95% for all subfigures except **c) & g)**, which used 50% due to the chaotic overlap) bar are established over \(20\) seed runs. Legends are shared among **b,c,d,f,g,h). **a-c)** focus on E.1 delusions: **a)** Evolving ratio of G.1 targets among all candidates at each target selection, throughout training. Variants are differentiated only with the generator HER strategies; **b)** Evolving E.1 delusions measured by \(L_{1}\) error in estimated distance throughout training. The distances to unreachable targets are clipped to be the maximum values that the estimator could output (\(16\) in experiments). A more detailed breakdown of this error is in the Appendix; **c)** The curves represent the frequencies of choosing G.1 targets whenever a selection of targets is initiated; **d)** Final estimation accuracies of non-dolusional source-target pairs after training completed, across a spectrum of ground truth distances. Both estimated and true distances are conditioned on the policies of the last timestep. Note that a part of the errors comes from E.0; Subfigures **e-g)** are the E.2-counterparts of **a-c)**: **h)** Each data point represents OOD evaluation performance aggregated over \(4 20\) newly generated tasks, with mean difficulty matching training. The decomposed curves are presented in the Appendix._

Related Works

**Target-Directed Agents.** Dual-component frameworks are explicitly investigated with temporal abstraction for purposes such as path planning (Nasiriany et al., 2019), OOD generalization (Zhao et al., 2024) and task decomposition (Nair and Finn, 2020; Zadem et al., 2024). Davchev et al. (2021) employs the similar framework to assist exploration, without using HER for training.

**Hindsight Relabeling** is crucial for enhancing _sample efficiency_ in goal-directed RL (Andrychowicz et al., 2017), as it enables learning from failed experiences (Dai et al., 2021). From the perspective of source-target pairs, improvements were proposed, including divergence maximization (Zhang and Stadie, 2022), addressing distributional change (Bai et al., 2023), multistep relabeling strategies (Yang et al., 2021), prioritizing rarely-seen achieved transitions (Kuang et al., 2020), _etc_.

HER's success was centered on its performance gains via sample efficiency (in non-delusional cases), around which most follow-up works revolved as well. In reality, performance deficiencies of target-directed frameworks can be resulted from multiple factors, one of which is delusions. Shams and Feyens (2022) studied the performance of atomic strategies from the view of sample efficiency, without looking into the failure modes. Deshpande et al. (2018) detailed experimental techniques in sparse reward settings using "future". In (Yang et al., 2021), a mixture strategy similar to "generate" improved non-delusional estimations, though the impact on delusions was not explored, possibly because of the lack of an appropriate environment. Nasiriany et al. (2019) used a mixture of up to \(3\) atomic strategies for producing effective training data in a single-task setting, while the contributions of the mixture to addressing delusions were not investigated. It is widely known that performance of existing HER-trained agents are often limited by their exclusive reliance on "future" or "episode" (He et al., 2020), whose delusions this paper intends to address.

**Delusions** in RL can stem from other improper designs. Lu et al. (2018) identified delusions caused by _the limitations of function approximators_ for greedy policies in model-free RL; Often accompanying hallucinations, delusions can arise in non-target-directed frameworks as well. In an offline planning framework such as Dyna, hallucinations can irreversibly destabilize value estimations (Jafferjee et al., 2020). Di Langosco et al. (2022) classified goal misgeneralization, a class of delusional behaviors with which an agent competently pursues an undesired target leading in novel test situations. Zhao et al. (2024) gave first examples of delusional behaviors caused by improper hindsight relabeling. The perspective of delusions can often be overlooked because of the lack of estimators in target-directed methods, as discussions regarding hallucinations get magnified.

## 7 Empirical Guidelines

The following steps can be applied to target-directed methods (applicable also beyond HER):

1. Incorporate an estimator to enable rejection of unwanted targets;
2. Use proper update rules and try to maximize the diversity of the training data (against E.0);
3. Inspect the candidate targets if possible, as their low quality may indicate higher E.1 risks, which can be mitigated with "generate";
4. Analyze the state structure of the target tasks and identify temporary unreachabilities. These E.2 risks may indicate the effectiveness of "pertask".

## 8 Conclusion \(}\) Future Work

We investigated the causes of delusional behaviors, a shared failure mode of target-directed agents. When applying our mitigation strategies on HER, the resulting hybrid hindsight relabeling strategies flexibly satisfy the needs of the generator and the estimator, granting the agents the ability to address delusions autonomously and preemptively avoid delusional behaviors.

It is likely that we did not exhaustively identify all potential types of delusions in target-directed agents. We wish to continue the investigation into unidentified causes of failure modes and design more useful skill-learning agents.

## 9 Reproducibility Statement

The results presented in the experiments are **fully**-reproducible with the submitted source code.