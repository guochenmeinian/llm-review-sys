# Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals

Tam Nguyen

Department of Electrical & Computer Engineering

Rice University

Houston, USA

mn72@rice.edu

&Tan M. Nguyen

Department of Mathematics

National University of Singapore

Singapore

tamm@nus.edu.sg

&Richard G. Baraniuk

Department of Electrical & Computer Engineering

Rice University

Houston, USA

richb@rice.edu

###### Abstract

Transformers have achieved remarkable success in a wide range of natural language processing and computer vision applications. However, the representation capacity of a deep transformer model is degraded due to the over-smoothing issue in which the token representations become identical when the model's depth grows. In this work, we show that self-attention layers in transformers minimize a functional which promotes smoothness, thereby causing token uniformity. We then propose a novel regularizer that penalizes the norm of the difference between the smooth output tokens from self-attention and the input tokens to preserve the fidelity of the tokens. Minimizing the resulting regularized energy functional, we derive the Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models that can mitigate the over-smoothing issue. We empirically demonstrate the advantages of NeuTRENO over the baseline transformers and state-of-the-art methods in reducing the over-smoothing of token representations on various practical tasks, including object classification, image segmentation, and language modeling.

## 1 Introduction

Transformer models  have achieved substantial success in natural language processing , reinforcement learning , computer vision , and other practical applications . Transformers also excel at transferring knowledge from pre-trained models to new tasks, even when limited supervision is available . At the heart of transformers lies the self-attention mechanism, which computes a weighted average of token representations within a sequence. These weights are determined based on the similarity scores between pairs of tokens, determining their relative importance in the sequence . This flexibility in capturing diverse syntactic and semantic relationships has been identified as a crucial factor contributing to the success of transformers .

### Background: Self-Attention

For a given input sequence \(:=[(1),,(N)]^{}^{N D_{x}}\) of \(N\) feature vectors, self-attention transforms \(\) into the output sequence \(\) in the following two steps:

**Step 1.** The input sequence \(\) is projected into the query matrix \(\), the key matrix \(\), and the value matrix \(\) via three linear transformations

\[=_{Q}^{};=_{K}^ {};=_{V}^{},\] (1)

where \(_{Q},_{K}^{D_{qk} D_{x}}\), and \(_{V}^{D D_{x}}\) are the weight matrices. We denote \(:=[(1),,(N)]^{},:=[(1),,(N)]^{}\), and \(:=[(1),,(N)]^{}\), where the vectors \((i),(i)\), and \((i)\), for \(i=1,,N\) are the query, key, and value vectors, respectively.

**Step 2.** The output sequence \(:=[(1),,(N)]^{}^{N D_{qk}}\) is then computed as follows

\[=^{}/} :=,\] (2)

where the softmax function is applied to each row of the matrix \(^{}/}\). The matrix \(:=^{}}{}}^{N N}\) and its component \(a_{ij}\) for \(i\), \(j=1,,N\) are called the attention matrix and attention scores, respectively. For each query vector \((i)\) for \(i=1,,N\), an equivalent form of Eqn. (2) to compute the output vector \((i)\) is given by

\[(i)=_{j=1}^{N}(i)^{}(j)/ }(j).\] (3)

The self-attention computed by Eqn. (2) and (3) is refered as softmax attention. In our work, we refer to a transformer that uses softmax attention as a softmax transformer.

### Over-smoothing in Transformers

Despite their remarkable success, deep transformer-based models have been observed to suffer from the over-smoothing issue, in which all token representations become identical when more layers are added to the models [55; 65; 18]. This over-smoothing phenomenon, also known as the "token uniformity" problem, significantly limits the representation capacity of transformers. To illustrate this phenomenon, we examine the average cosine similarity between pairs of token representations across different layers in a softmax transformer trained for the Imagenet object classification and ADK20 image segmentation tasks . As depicted in Fig. 1, in both tasks, this cosine similarity between tokens increases as the models become deeper. Particularly, in the last two layers, the cosine similarity scores are approximately 0.9, indicating a high degree of similarity among tokens.

### Contribution

We develop a nonlocal variational denoising framework for self-attention, providing insights into the over-smoothing phenomenon in transformers. In particular, by viewing self-attention as a gradient descent step toward minimizing a nonlocal functional that penalizes high-frequency noise in the signal, we uncover the diffusive nature of self-attention, which explains the over-smoothing issue of transformers. Motivated by this understanding, we propose the Neural Transformer with a Regularized

Figure 1: The cosine similarity between tokens representations across layers of NeuTRENO DeiT vs. the baseline DeiT models on the Imagenet classification and ADE20K image segmentation tasks. In both tasks, the DeiT baseline suffers from over-smoothing as tokens become similar to identical when the model gets deeper. In contrast, tokens in NeuTRENO models are significantly more diverse, suggesting a reduction in over-smoothing. Further details regarding this analysis can be found in Appendix E.

Nonlocal Functional (NeuTRENO), a novel class of transformers designed to mitigate over-smoothing. NeuTRENO is derived by optimizing a regularized nonlocal functional, which includes an additional convex fidelity term. This fidelity term penalizes the norm of the difference between the smooth output tokens from self-attention and the input tokens, thereby reducing the over-smoothing effect. Our contribution is three-fold.

1. We develop a nonlocal variational denoising framework for self-attention and shed light on the over-smoothing issue that hampers the representation capacity of transformers.
2. We develop NeuTRENO, a novel class of transformers that are capable of alleviating the over-smoothing issue.
3. We theoretically prove that transformers with softmax self-attention are prone to over-smoothing while NeuTRENO can avoid this issue.

We empirically demonstrate the benefits of NeuTRENO on various large-scale applications, including the ImageNet object classification, ADE20K image segmentation, and WikiText-103 language modeling tasks.

**Organization**: We organize our paper as follows: in Section 2, we develop a nonlocal variational denoising framework for self-attention and provide an explanation for the over-smoothing issue in transformer-based models. In section 3, we propose NeuTRENO, and present a theoretical result that guarantees NeuTRENO's capability of mitigating over-smoothing. In Section 4, we empirically validate the benefits of NeuTRENO. We discuss the related work in Section 6. Finally, we conclude our main contributions and remarks. Further results, details, and proofs are provided in the Appendix.

## 2 A Nonlocal Variational Denoising Framework for Self-attention

We first consider the output matrix \(:=[(1),,(N)]^{}^{N D}\) in self-attention as given by Eqn. 2 in Section 1.1. Let \(\), \(x\), and \((x):=[u_{1}(x),,u_{D}(x)]^{T}\) be a real vector-valued function, \(:^{D}\), \( L^{2}()\). The output matrix \(\) in self-attention discretizes the function \((x)\) on a 1-D grid. In the context of signal/image denoising, \(\) can be considered as the _desired clean signal_, and \((x)\) is its corresponding intensity function denoting the signal values at the position \(x\). We further let the observed intensity function \((x)\) denote the values of the _observed noisy signal_ at \(x\), \(:^{D}\), \( L^{2}()\). For example, \((x)\) can be given as

\[(x)=(x)+(x),\] (4)

where \(\) is the additive noise. We wish to reconstruct \((x)\) from \((x)\). Following the variational denoising method proposed in  and , the denoised image \((x)\) can be obtained by minimizing the following regularized functional with respect to \(\):

\[E(,) =J()+G(,)\] (5) \[=_{}\|(x)-(y)\|_{2}^ {2}k(x,y)dxdy+_{}\|(x)-(x)\|_{2}^{2}dx.\]

Here, \(J()=_{}\|(x)-(y)\|_{2}^{2}k (x,y)dxdy\) is a nonlocal functional of weighted differences. The weights \(k(x,y)\) represent the affinity between signal values at positions \(x\) and \(y\). For example, for images, \(k(x,y)\) captures the proximity between pixels \(x\) and \(y\) in the image. \(J()\) works as a regularizer. Minimizing \(J()\) promotes the smoothness of at and penalizes high-frequency noise in the signal. Adding the convex fidelity term \(G(,)=_{}\|(x)-(x)\|_{2}^{2}dx\) to the functional \(J()\) allows the denoised signal \((x)\) to preserve relevant information in the observed noisy signal \((x)\). The regularized functional \(E(,)\) can be considered as an energy functional.

### Self-attention as a Gradient Descent Step to Minimize the Nonlocal Functional \(J\)

We show that self-attention is equivalent to taking a gradient descent step toward minimizing the functional \(J()\) in the energy functional \(E(,)\). We expand \(J()\) as follows

\[J()=_{}_{j=1}^{D}(u_{j}(x)-u_{j}(y) )^{2}k(x,y)dxdy\] (6)

The gradient of \(J\) with respect to \(\) is then given by

\[_{}J()=[},},,}]^{T}.\] (7)The partial derivative \( J/ u_{j}\), \(j=1,2,,D\), is defined through its dot product with an arbitrary function \(h_{j} L^{2}()\) as follows

\[} h_{j}(x) =J(u_{j}+ h_{j})_{=0}\] \[=(_{}(u_{j}( x)-u_{j}(y)+ h_{j}(x)- h_{j}(y))^{2}k(x,y)dxdy)_{=0}\] \[=(_{}(u_{j}(x)-u_{j}(y)+ h_{j}(x)-  h_{j}(y))(h_{j}(x)-h_{j}(y))k(x,y)dxdy)_{=0}\] \[=_{}(u_{j}(x)-u_{j}(y))(h_{j}(x)-h_{j}(y))k (x,y)dxdy\] \[=_{}(u_{j}(x)-u_{j}(y))h_{j}(x)k(x,y)dxdy- _{}(u_{j}(x)-u_{j}(y))h_{j}(y)k(x,y)dxdy\]

Applying a change of variables \((x,y)(y,x)\) to the second term of the above integral, we have

\[} h_{j}(x) =_{}(u_{j}(x)-u_{j}(y))h_{j}(x)k(x,y)dxdy- _{}(u_{j}(y)-u_{j}(x))h_{j}(x)k(y,x)dxdy\] \[=_{}(u_{j}(x)-u_{j}(y)(k(x,y)+k(y,x))dyh_{j }(x)dx\]

Thus, the Frechet derivative of J with respect to \(u_{j}\) is given by

\[}=_{}(u_{j}(x)-u_{j}(y)(k(x,y)+k(y, x))dy.\] (8)

Substituting the formula for \( J/ u_{j}\) in Eqn. 8 into Eqn. 7 for \(_{}J()(x)\), we obtain the following gradient flow

\[(x,t)}{dt}=-_{}J()=_{} (y,t)-(x,t)k(x,y)+k(y,x)dy,\] (9)

where \(t\) is the time variable we introduce to capture the dynamics of \(\) when gradient descent is applied to minimize \(J()\). Let \((x):=[v_{1}(x),,v_{D}(x)]^{T}\) be a real vector-valued function, \(:^{D}\), \( L^{2}()\). We discretize \((x)\) on a 1-D grid to attain the value vectors \((1),,(N)^{D}\), which form the value matrix \(:=[(1),,(N)]^{}^{N D}\) in self-attention as defined in Eqn. 2. We initialize \(\) at \(t=0\) with \((x)\), i.e., \((x,0)=(x)\).

**Self-attention is an Euler Discretization of the Gradient Flow Given in 9.** We discretize the gradient flow in Eqn. 9 using the Euler method  with step size \( t(x)=1/_{}k(x,y)+k(y,x)dy\) and obtain the following update

\[(x, t(x)) =(x,0)+ t(x)_{}(y,0)-( x,0)k(x,y)+k(y,x)dy\] \[=_{}k(x,y)+k(y,x)(y,0)}{ _{}k(x,y^{})+k(y^{},x)dy^{}}dy= _{}(y)}{_{}K(x,y^{})dy^{}}dy.\] (10)

Here, \(K(x,y):=k(x,y)+k(y,x)\) is a symmetric kernel and \((y,0)=(y)\) since \(\) is initialized at \(t=0\) with \(\) as aforementioned. Let \((x):=[k_{1}(x),,k_{D_{$}}}k}(x)]^{T}\) be a real vector-valued function, \(:^{D_{$}}}}\), \( L^{2}()\). Similar to \((x)\) and \((x)\), we can discretize \((x)\) on a 1-D grid to attain the key vectors \((1),,(N)^{D_{$}}}k}\), which form the key matrix \(:=[(1),,(N)]^{}^{N D_{$}}}k}\) in self-attention as defined in Eqn. 2. We choose \(K(x,y)=(x)^{T}(y)/}\) and rewrite Eqn. 10 as follows

\[(x, t(x))=_{}(x)^{T}(y)/ }}{_{}(x)^{T}(y^{ })/}dy^{}}(y)dy.\] (11)Estimating the integrals in Eqn. 11 via Monte-Carlo approximation using the key vectors \((1),,(N)^{D_{}}\) and and value vectors \((1),,(N)^{D}\), we obtain

\[(x, t(x))_{j=1}^{N}(x)^{T} (j)/}}}{_{j^{}=1}^{N}(x) ^{T}(j^{})/}}}(j).\] (12)

Discretizing \((x, t(x))\) on another 1-D grid, we attain

\[(i) _{j=1}^{N}(i)^{T}(j)/ }}}{_{j^{}=1}^{N}(i)^{T} (j^{})/}}}(j)\] \[=_{j=1}^{N}(i)^{}(j)/ }}(j),\ \ i=1,,N.\] (13)

Comparing Eqn. 13 and Eqn. 3, we observe that Eqn. 13 implement a symmetric self-attention, in which the query matrix \(\) and the key matrix \(\) are the same, i.e. \(_{Q}=_{K}\) where \(_{Q}\) and \(_{K}\) are the linear projections that map the input sequence \(\) into \(\) and \(\) as given in Eqn. 1. This symmetry of the attention scores is desirable in some image processing tasks due to the symmetric similarities between pixels, but can be relaxed for other tasks. To break the symmetry of attention scores in Eqn. 13, we replace the key vectors \((i)\) by the query vectors \((i)\), \(i=1,,N\), to obtain the exact formula of self-attention given by Eqn. 3. The following theorem summarizes our results:

**Theorem 1** (Self-attention as a Gradient Descent Step to Minimize a Nonlocal Functional).: _Given the nonlocal functional \(J()=_{}\|(x)-(y)\|_{2}^{2}k (x,y)dxdy\) of a vector-valued function \(:^{D}\), \( L^{2}()\), and let \(K(x,y):=k(x,y)+k(y,x)=(x)^{T}(y)/}} \), where \(:^{D_{}}\), \( L^{2}()\). Then, taking a gradient descent step on \(\) at time \(t=0\), where \((x,0)=(x)\), with an adaptive step size \( t(x):=k(x,y)+k(y,x)dy}\) to minimize \(J\) is equivalent to updating \(\) via a symmetric self-attention_

\[(x, t(x))=_{j=1}^{N}(x)^{} (j)/}}(j),\]

_which results in_

\[(i)=_{j=1}^{N}(i)^{}(j)/ }}(j),\ \ i=1,,N.\] (14)

_Here, \((n)\), \((n)\), and \((n)\), \(n=1,,N\), are the key, value, and output vectors in self-attention, respectively. Breaking the symmetry of the attention scores by replacing \((i)\) with \((i)\), \(i=1,,N\), in Eqn. 14, we obtain the exact formula of self-attention_

\[(i)=_{j=1}^{N}(i)^{}(j)/ }}(j),\ \ i=1,,N.\]

**Remark 1**.: _In Eqn. 9, the change in \(\) at position \(x\) is proportional to the sum of differences between \((x)\) and \(\) at other position in the domain \(\). In particular, when \((x)\) is smaller or larger than the values at other positions, it will increase or decrease, respectively. This is analogous to a diffusion process in which particles or substances move from high-concentration to low-concentration regions. It has been proved that a diffusion process converges to a saturating state in which the concentrations at all positions are the same. This suggests that \((x)\) tends to suffer from the over-smoothing issue._

### Random Walk Analysis of Over-smoothing

The diffusion process and random walk are closely related concepts, as diffusion can be seen as a collective behavior of numerous random walks performed by individual particles or molecules. Inspired by the analogy between the dynamics of \(\) in Eqn 9 and a diffusion process, as well as the relationship between diffusion process and random walk, in this section, we show the connection between the evolution of \(\) and a random walk. By adopting a random walk perspective on graph neural network , we demonstrate that \((x)\) under the dynamics given in Eqn 9 suffers from over-smoothing.

Recall from the gradient flow in Eqn 9, by using Euler method discretization, after \(k\) update steps starting from the initial \((x,0)=(x)\), with adaptive stepsize \( t=1/\!_{}k(x,y)+k(y,x)dy\), we obtain the following

\[(x,k t(x))=_{}(y,(k-1) t(x))}{ _{}K(x,y^{})dy^{}}dy.\] (15)

Discretizing \((x,k t(x))\) and using Monte-Carlo approximation for the integrals in 15, we attain

\[^{(k)}(i)=_{j=1}^{N}_{ij}^{(k-1)}(j)\] (16)

where \(_{ij}\) is computed using the keys and queries as either \((i)^{}(j)/}\) or \((i)^{}(j)/}\). Let \(\{^{(k)}(i)\}_{k K}\) be a random walk on \(\{(i)\}_{i=1}^{N}\) as defined:

\[^{(0)}(i)=(i)\] (17) \[(^{(k+1)}(l)=(j)|^{(k)}(l)= (i))=_{ij}\]

where \(^{(k)}(n)\) is the random value of a \(k\)-step walk, starts at node \(n\), and \((n)\) is the initial value at node \(n\), respectively, for \(n=1,2,,N\). The transition probability \(\) is defined as above. To investigate the connection between the update process of \(\) and the random walk defined in 17, we show that, for \(i=1,2,,N\), after \(k\) update steps as in 16, with initial value \(^{(0)}(i)=(i)\), \((i)^{(k)}\) equals to the expected value of the \(k\)-step walk, starting at node \(i\):

**Lemma 1**.: _Let \(^{(k)}(i)\) defined in 16 and \(\{^{(k)}(i)\}_{k K}\) is the random walk defined by 17. Then_

\[^{(k)}(i)=[^{(k)}(i)].\] (18)

We next present the Lemma 2 which is necessary to show the convergence of \(^{(k)}(i)\).

**Lemma 2**.: _The random walk \(^{(k)}(i)\) in 17 with the transition matrix \(\) either be \(_{ij}=(i)^{}(j)/ }\) or \(_{ij}=(i)^{}(j)/ {D_{qk}}\), has a unique stationary distribution \(=[_{1},_{2},,_{N}]\) such that \(_{i}:=P(^{(k)}(j)=(i))\), for \(i,j=1,2,,N\), \(_{i=1}^{N}_{i}=1\), and \(^{T}=^{T}\)._

_If \(_{ij}=(i)^{}(j)/ }\), the stationary distribution is:_

\[=}{_{j=1}^{N}d_{j}},}{_{j=1}^{ N}d_{j}},,}{_{j=1}^{N}d_{j}},\] (19)

_where \(d_{i}=_{j=1}^{N}(i)^{}(j)/}\), \((1),(2),,(N)\) are the key vectos._

_In general, \(_{i}\) can be found by finding the left eigenvector of \(\) corresponding to the dominant eigenvalue 1._

From the Lemma 1 and Lemma 2, we see that, for all \(i=1,2,,N\),

\[^{(k)}(i)=[^{(k)}(i)]=_{j=1}^{N}(j)(^{(k-1)}(i)=(j))_{j=1}^{N}_{j}(j)=: }.\] (20)

as \(k\). This shows that when \(k\) increases, \((i)^{(k)}\) converges to a constant vector, indicating that \((x)\), under the dynamic in 9, suffers from over-smoothing.

## 3 NeuTRENO: Mitigating the Over-smoothing in Transformers via Minimizing a Regularized Functional

In Section 2.1, we have shown that self-attention implicitly performs a gradient descent step to minimize the nonlocal functional \(J()\) in Eqn. 5, which results in the diffusive characteristics of \(\) and causes the over-smoothing phenomenon in transformers, as proved in Section 2.2. Fortunately, our objective is not to minimize \(J()\) but the energy/regularized functional \(E(,)\) defined by Eqn. 5. This regularized functional consists of not only \(J()\) but also the convex fidelity term \(G(,)=_{}\|(x)-(x)\|_{2}^{2}dx\). This fidelity term aims to preserve the relevant information in the observed noisy signal \((x)\) by penalizing solution \((x)\) that deviates significantly from \((x)\), thereby mitigating the effects of over-smoothing caused by minimizing \(J()\).

In this section, we will derive our Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO) by minimizing the regularized functional \(E(,)\). We then provide a theoretical result to prove that NeuTRENO does not suffer from over-smoothing. Recall from Eqn. 5 that \(E(,)\) is given by

\[E(,)=J()+G(,)=J()+_{}_{j=1}^{D}(u_{j}(x)-f_{j}(x))^{2}dx\]

Following a similar derivation as in Section 2.1 (see Appendix C for the detailed derivation), we obtain the following gradient flow when minimizing \(E(,)\) using gradient descent

\[(x,t)}{dt}=-_{}E(,)=- _{}J()-(x)-(x),\] (21)

**NeuTRENO-attention is an Euler Discretization of the Gradient Flow Given in 21.** Following the similar derivation in Section 2.1, we discretize the gradient flow in Eqn. 21 using the Euler method  with step size \( t(x)=1/\!_{}\!k(x,y)+k(y,x)dy\) and initializing \(\) at \(t=0\) with \((x)\), i.e., \((x,0)=(x)\). Choosing \(=/ t(x)\), we obtain the following update

\[(x, t(x)) =(x,0)- t(x)_{}J- t(x) (x,0)-(x)\] \[=_{}(y)}{_{}K(x,y^{} )dy^{}}dy+(x)-(x).\] (22)

We choose the observed noisy signal \((x)=^{0}(x)\) where \(^{0}(x)\) is \((x)\) at the first layer in the transformer model. The update in Eqn. 22 becomes

\[(x, t(x))=_{}(y)}{_{ }K(x,y^{})dy^{}}dy+^{0}(x)-(x).\] (23)

Applying the Monte-Carlo method to approximate the integrals in Eqn. 23 and discretizing \((x, t(x))\), \((x)\), and \(^{0}(x)\) on a 1-D grid, we attain the following new formula for calculating symmetric self-attention:

\[(i)=_{j=1}^{N}(i)^{} (j)/}(j)+(^{0}(i)-(i )),\ \ i=1,,N.\] (24)

Figure 2: Our proposed NeuTRENO model adds a proportion of the difference between the values of the first and that of the current layer to the self-attention’s output at each layer.

Its corresponding asymmetric self-attention is obtained by replacing the key vectors \((i)\) with the query vectors \((i)\), \(i=1,,N\), and given by

\[(i)=_{j=1}^{N}(i)^{}(j)/}(j)+(^{0}(i)-(i)),\ \ i=1,,N.\] (25)

Leveraging Eqn. 25, we define the Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO) as follows.

**Definition 1** (Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO)).: _Given a set of key and value vectors \(\{^{}(j),^{}(j)\}_{j=1}^{}\) in each layer \(\), \(=1,,L\), for each query vector \(^{}(i)\), \(i=1,,N\), in the same layer, the self-attention unit at layer \(\) in a Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO) computes the corresponding output vector \(^{}(i)\) of the query \(^{}(i)\) by the following attention formula:_

\[^{}(i)=_{j=1}^{N}^{}(i)^{} ^{}(j)/}^{}(j)+(^ {0}(i)-^{}(i)),\ \ i=1,,N.\] (26)

_where \(^{0}(1),^{0}(N)^{D}\) are the value vectors in the first layer of NeuTRENO._

Fig. 2 illustrates the architecture of NeuTRENO.

**Proposition 1**.: _The evolution of \((x)\) under the dynamic in 21 does not converge to a constant vector._

Proposition 1 indicates that our NeuTRENO mitigates the over-smoothing issue, suggesting the benefit of our method. The proof for Proposition 1 is given in Appendix B.3.

## 4 Experimental Results

In this section, we empirically demonstrate the advantages of our proposed NeuTRENO approach across various tasks, including ImageNet classification , ADE20K image segmentation , and language modeling on the WikiText-103 . Our aim to show: (i) NeuTRENO significantly outperforms the transformer baseline with softmax-attention defined in 2 across various tasks; moreover, NeuTRENO surpass FeatScale, a vision transformer that addresses over-smoothing, combining NeuTRENO with FeatScale is beneficial; (ii) the advantages of incorporating our proposed method with pre-trained models. We also demonstrate the benefits of our NeuTRENO in the symmetry setting and we point to Appendix D for the results. Throughout our experiments, we compare the performance of our proposed models with baselines of the same configuration. For additional details regarding datasets, models, and training procedures, please refer to Appendix A.

**Object classification on ImageNet.** To demonstrate the advantage of our NeuTRENO method, we compare it with the DeiT baseline  on the ImageNet image classification task. Our NeuTRENO DeiT surpasses the DeiT baseline, as shown in Table 1. Notably, our NeuTRENO DeiT achieves significantly higher performance in terms of both Top-1 Accuracy and Top-5 Accuracy. We also compare our method with FeatScale , a vision transformer model addressing over-smoothing (see Table 1). Our NeuTRENO significantly outperforms FeatScale, and combining NeuTRENO with FeatScale leads to substantial improvements. These results confirm the benefits of our model.

**Image Segmentation on ADE20K dataset.** To further validate the advantages of our proposed methods, we compare the performance of the Segmenter models  using the NeuTRENO DeiT

   Model/Metric & Top-1 Acc (\%) & Top-5 Acc (\%) \\  _Softmax DeiT_ & 72.17 & 91.02 \\ NeuTRENO-DeiT & **73.01** & **91.56** \\ NeuTRENO Adaptation & 72.63 & 91.38 \\  _DeiT + FeatScale_ & 72.346 & 91.22 \\ NeuTRENO DeiT + FeatScale & **73.23** & **91.73** \\   

Table 1: Top-1 and Top-5 accuracy (%) of NeuTRENO DeiT vs. DeiT on the ImageNet benchmark. We also present the performance of adapting NeuTRENO to the pre-trained DeiT baseline, NeuTRENO Adaptation. In addition, we compare NeuTRENO with FeatScale  and incorporate our method with FeatScale model.

**Language Model on WikiText-103.** In addition to computer vision tasks, we also evaluate the effectiveness of our model on a large-scale natural language processing application, specifically language modeling on WikiText-103. Our NeuTRENO language model demonstrates better performance in terms of both test perplexity and valid perplexity when compared to the softmax transformer language model . These findings, combined with the results obtained across various tasks, empirically confirm the significant benefits of our NeuTRENO models.

**Combine with pre-trained models.** Furthermore, our proposed method is also beneficial to combine with pre-trained models. To empirically demonstrate that we incorporate NeuTRENO with pre-trained DeiT and fine-tune on the ImageNet dataset with one-third number of epochs that are used in training. The result is presented in Table 1, showing that combined with our method improves both the Top-1 and Top-5 accuracies of the pre-trained models.

## 5 Empirical Analysis

**Applying Softmax-Attention Reduces the functional \(J()\).** We present evidence supporting that the employment of softmax attention minimizes the functional \(J()\). Initially, we observe that the average cosine similarity between the numerical approximation of \(_{}J()\) using symmetric or asymmetric kernel \(K(x,y)\) for both the trained Sym-DeiT (using symmetric self-attention 14) and DeiT models, closed 1, as shown in Table 4. This suggests that reversing the direction of the asymmetric approximation effectively decreases \(J()\). Considering that softmax attention takes steps in this reversed direction numerically, its application leads to a reduction in \(J()\). This is further substantiated by Fig. 3, which demonstrates a decrease in \(J()\) as the depth of the trained DeiT increases when softmax attention is employed. More details of this analysis are in Appendix E

**Over-smoothing Analysis.** We empirically illustrate the effectiveness of NeuTRENOs in mitigating the over-smoothing problem in transformers. Fig. 1 compares the cosine similarity between token representations across layers for both NeuTRENO and softmax baseline models, specifically focusing on the Imagenet classification task (Left) and ADE20K image segmentation (Right). The token

   Model/Metric & SS MIoU & MS MIoU (\%) \\  _Softmax DeiT_ & 35.72 & 36.68 \\ NeuTRENO DeiT & **37.24** & **38.06** \\   

Table 2: Single-scale (SS) MIoU and multi-scale MIoU (MS) of the NeuTRENO DeiT vs. the DeiT on the ADE20K image segmentation.

Figure 3: The average value of functional \(J()\) over 1000 training (Left) samples and test (Right) samples. When softmax attention is applied, the functional decreases as the depth of the trained DeiT increases.

and DeiT backbones the on ADE20K image segmentation task , as shown in Table 2. The results demonstrate the substantial performance improvements achieved by utilizing the NeuTRENO DeiT backbone over the DeiT backbone, in terms of both single-scale (SS) MIoU and multi-scale (MS) MIoU metrics. These results strongly emphasize the effectiveness of our NeuTRENO approach in enhancing image segmentation performance.

features extracted by NeuTRENOs exhibit significantly lower similarity, particularly in the final layers. This finding highlights the ability of NeuTRENOs to address the over-smoothing issue and improve the diversity of token representations. We provide more details of this analysis in Appendix E.

## 6 Related Work

**Over-smoothing in Transformers.** Over-smoothing in deep transformers has been observed in various domains and applications from natural language processing  to computer vision [65; 18]. In vision tasks,  observes that the performance of the vision transformer (ViT ) quickly saturates as more layers are added to the model. Moreover, experiments in  show that the 32-layer ViT underperforms the 24-layer ViT, indicating the difficulty of ViTs in gaining benefits from deeper architectures. The authors point out that over-smoothing results in this phenomenon by causing the token representations to become identical when the model grows deeper. Based on this observation, the authors propose a cross-head communication method that helps enhance the diversity of both token representations and attention matrices. Furthermore, it has been shown in  that the training of ViT models encounters instability with greater depths.  proposes that this instability arises from the over-smoothing, where token representation for patches within an image becomes progressively alike as the model's depth increases. To explain this issue,  finds out that self-attention acts as a low-pass filter and smoothens the token representations in ViTs. This leads to the proposal of the FeatScale method , which regulates feature frequencies, whether low or high, to counteract the consequences of over-smoothing.

In addition,  observes the phenomenon in BERT , a deep language model, and explores over-smoothing through the graph perspective. The work utilizes hierarchical fusion strategies by preserving the output of self-attention through all layers, which is memory-costly. On the other hand, [65; 18] investigate over-smoothing in the image domain through the lens of Fourier spectrum, showing that self-attentions are low-pass filters, retaining only low-frequency, causing over-smoothed outputs. Our work is an orthogonal explanation of the previous work. We focus on developing a variational denoising framework to understand the self-attention of transformers as a gradient descent approximation of a functional. Our new finding explains the over-smoothing issue of transformers due to self-attention minimizing a functional and inspires us to derive the novel NeuTRENO method to overcome over-smoothing.

**Nonlocal Functionals for Image Processing.** Total variation  is well-known as an image-denoising technique. It denoises a noisy image by solving a constraint optimization problem. The method is also related to PDE-flow-based image-denoising techniques , namely isotropic and anisotropic diffusion  models. The method is edge preserving, meaning to avoid over-blurring edges' information . Nonlocal functionals [35; 24] is considered as an extension of total variation to a nonlocal scale. Nonlocal functional and edge preservation properties are the motivation of our work to explain and overcome over-smoothing in transformers.

## 7 Concluding Remarks

In this paper, we establish a nonlocal variational denoising framework for self-attention. From this variational perspective, we explain over-smoothing in self-attention, which hinders the representation capacity of transformer models. We also derive the novel Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO) to alleviate the over-smoothing. We empirically verify the benefits of NeuTRENO with a wide range of large-scale applications including ImageNet object classification, ADE20K object segmentation, and WikiText-103 language modeling. A limitation of our paper is that the privacy-preserving of NeuTRENO has not been addressed. It is interesting to explore if regularized nonlocal functional can also help improve the privacy-preserving of transformer models. We leave this exciting research idea as future work.

   Model & Training data & Test data \\   Sym-DeiT & 0.982 & 0.976 \\ Softmax DeiT & 0.973 & 0.964 \\   

Table 4: The average cosine similarity between the numerical approximation of \( J()(x)\) using symmetric or asymmetric kernel \(K(x,y)\), for the trained Sym-DeiT and softmax DeiT models. The metric is evaluated on 1000 training and 1000 test data samples. The average score close to 1 shows a strong alignment between symmetric and asymmetric gradient approximations, suggesting that reversing the direction of the asymmetric approximation effectively reduces the functional \(J()\).