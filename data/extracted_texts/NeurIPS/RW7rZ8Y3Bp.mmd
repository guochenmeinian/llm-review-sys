# Federated Spectral Clustering via Secure Similarity Reconstruction

Dong Qiao\({}^{1,2}\)  Chris Ding\({}^{1}\)  Jicong Fan \({}^{1,2}\)

\({}^{1}\)The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\)Shenzhen Research Institute of Big Data, Shenzhen, China

dongqiao@link.cuhk.edu.cn {chrisding,fanjicong}@cuhk.edu.cn

Corresponding author

###### Abstract

Federated learning has a significant advantage in protecting data and information privacy. Many scholars proposed various secure learning methods within the framework of federated learning but the study on secure federated unsupervised learning especially clustering is limited. We in this work propose a secure kernelized factorization method for federated spectral clustering on distributed data. The method is non-trivial because the kernel or similarity matrix for spectral clustering is computed by data pairs, which violates the principle of privacy protection. Our method implicitly constructs an approximation for the kernel matrix on distributed data such that we can perform spectral clustering under the constraint of privacy protection. We provide a convergence guarantee of the optimization algorithm, reconstruction error bounds of the Gaussian kernel matrix, and the sufficient condition of correct clustering of our method. We also present guarantees of differential privacy. Numerical results on synthetic and real datasets demonstrate that the proposed method is efficient and accurate in comparison to baselines.

## 1 Introduction

In the era of big data, human beings can analyze massive data in various fields due to the improvement of storage and computational capabilities of computing devices . Some popular fields such as artificial intelligence, machine learning, internet of things (IoT), and cloud computing have seen explosive development over the past few years. Nevertheless, a side effect of this trend is that individuals and organizations have more and more concerns about potential violation of privacy . As a result, it has become a challenge to mine valuable information from user data but not to directly access it.

Federated learning  can train a global model without retrieving dispersed data . This advantage has made it so popular that many scholars have put much effort into the study of federated learning. For example, Yang _et al._ presented the definitions of horizontal federated learning, vertical federated learning, and federated transfer learning. Some privacy-preserving machine learning models were also presented. For instance, He _et al._ developed a federated group knowledge transfer algorithm to train small CNNs on edge devices. Chen _et al._ proposed a protocol to conduct privacy-preserving ridge regression over high-dimensional data. Besides, Kim _et al._ proposed a block-chained federated learning architecture that enables on-device learning without any central coordination.

Regardless of the great progress of federated learning, it can be found that most of the existing studies are for supervised learning . Note that collecting labeled data may deserve very high cost in real situations  while unlabeled data are abundant. Thus, itis necessary and important to study federated learning for unsupervised learning (Zhang et al., 2020; Tzinis et al., 2021; Zhuang et al., 2021; Dennis et al., 2021) such as clustering (Ng et al., 2001; Fan and Chow, 2017; Fan et al., 2018, 2021; Fan, 2021; Cai et al., 2022; Fan et al., 2022). For example, Li et al. (2021) proposed a federated matrix factorization with a privacy guarantee for recommendation systems. Wang and Chang (2022) proposed two federated matrix factorization algorithms that can be used for federated clustering. Besides, there are some studies on federated spectral clustering. For instance, Wang et al. (2020) presented a federated multi-view spectral clustering method under the assumption that the data of each view are in one client. Hernandez-Pereira et al. (2021) developed a cooperative spectral clustering model to deal with distributed data but the model is linear. However, the study on federated spectral clustering is still very limited and deserves more attention and effort.

In this paper, we propose a federated kernelized factorization method to reconstruct a similarity matrix for secure spectral clustering on distributed data. Our contributions are as follows.

* We propose a federated spectral clustering algorithm and provide convergence guarantee for the optimization.
* We further propose to add noise to the data or the learned factors to enhance the security of clustering and provide guarantees of differential privacy.
* We provide upper bounds for the reconstruction error of the true similarity matrix and theoretical guarantees for correct clustering.

We test our method on both synthetic data and real datasets in comparison to baselines, which verify the effectiveness of our method.

NotationsWe use \(y\), \(\), and \(\) to denote scalar, vector, and matrix, respectively. The element of \(\) at row \(i\) and column \(j\) is denoted by \(y_{ij}\). We use \(\|\|_{2}\) to denote the \(_{2}\) norm of a vector and use \((),\|\|_{F}\), and \(\|\|_{sp}\) to denote the trace, Frobenius norm, and spectral norm of a matrix respectively. The \(_{}\) norm and \(_{2,}\) norm of a matrix \(\) are defined as \(\|\|_{}=_{ij}|y_{ij}|\) and \(\|\|_{2,}=_{j}y_{ij}^{2}}\) respectively. \(\), \(\), \(\), and \(k\) denote the kernel matrix, kernel function, number of clusters, and the number \(k\) in KNN, respectively. \(\) denotes the feature map induced by \(\).

## 2 Federated Spectral Clustering (FedSC)

Suppose we have \(n\) data points of dimension \(m\) distributing in \(P\) clients. For convenience, we denote by \(^{m n}\) the matrix composed of all the \(n\) data points and denote by \(_{p}^{m N_{p}}\) the matrix composed of the \(N_{p}\) data points in client \(c_{p}\), where \(N_{p} 1\), \(p=1,,P\), and \(_{p=1}^{P}N_{p}=n\). Without loss of generality, we let \(=[_{1},_{2},,_{P}]\), which means \(\{_{p}\}_{p=1}^{P}\) are submatrices of \(\). Our goal is to perform spectral clustering on these data to partition them into \(\) groups, under the constraint that the data in each client cannot leave the client itself and the privacy of the data should be protected as much as possible, though there could be a central server conducting clustering.

The aforementioned task is non-trivial because in spectral clustering, the first step is constructing an adjacency matrix \(^{n n}\), which has to evaluate the similarity between every data pair \((_{i},_{j})\) using a metric function \((,)\) and hence violates the privacy constraint in the task. To solve the problem, we present a federated spectral clustering model in this section.

### Similarity Reconstruction via Feature Space Factorization

In spectral clustering, for \((,)\), there are many choices such as \(k\) nearest neighbor similarity and various kernel functions. Let \((,)\) be a kernel function and we have

\[(_{i},_{j})=(_{i})^{T}(_{j}),\] (1)

where \(:^{m}^{m^{}}\) is a feature map induced by the kernel function2 and does not need to be carried out explicitly. When it comes to federated spectral clustering, the central server has no access to the raw data distributed in clients and hence cannot compute \((_{i},_{j})\) using (1). However, if the central server can learn an effective approximation (denoted by \(_{i})}\)) for each \((_{i})\) without accessing \(_{i}\), \((_{i},_{j})\) can be estimated, i.e.,

\[(_{i},_{j})_{i})}^{T} _{j})}.\] (2)

Thus, inspired by , we propose to approximate each \((_{i})\) by

\[_{i})}=()_{i},\] (3)

where \(=[_{1},_{2},,_{d}]^{m d}\), \(()=[(_{1}),(_{2}),(_{d})]\), and \(_{i}^{d}\). Both \(\) and \(_{i}\) are learned from individual columns of \(\) and they can be regarded as intermediate variables avoiding the direct access of central server to \(_{i}\) (details of the learning will be introduced later). It follows from (2) and (3) that

\[(_{i},_{j})_{i}^{T}()^{} ()_{j}.\] (4)

Thus we can reconstruct the similarity between \(_{i}\) and \(_{j}\) via (4). For convenience, let \(_{xx}=(,)=()^{}()\), \(_{zz}=(,)=()^{}() ^{d d}\), and \(=[_{1},_{2},,_{n}]^{d n}\). Then we have

\[_{xx}(())^{T}(())=^{T} _{zz}}_{xx}.\] (5)

Now we use \(}_{xx}\) as a reconstructed similarity matrix for spectral clustering.

In the form of federated learning, we expand (3) to

\[(_{p})()_{p}, p=1,,P.\] (6)

It indicates that \(\) is shared for all \(P\) clients and \(_{p}\) is private for client \(c_{p}\). Note that (6) is a matrix factorization problem in the feature space induced by a kernel on the data in client \(c_{p}\), \(p=1,,P\). Letting \(=[_{1},,_{P}]\), we solve the following distributed optimization problem3

\[,\,\,}{}\,\,\,F(,) _{p=1}^{P}_{p}f_{p}(,_{p}).\] (7)

In (7), \(f_{p}\) is a local objective function for client \(c_{p}\) and \(_{1},,_{P}\) are nonnegative weights for the clients. In this work, we let

\[ f_{p}(,_{p})=& \|(_{p})-()_{p}\|_{F}^{2}+\|_{p}\|_{F}^{2}\\ =&((_{p}, _{p}))-(_{p}^{T}(,_{p}))+ (_{p}^{T}(,)_{p})+\|_{p}\|_{F}^{2},\] (8)

where \( 0\) is a penalty parameter. To guarantee the privacy of information, problem (7) shall be solved in the framework of federated learning.

### FedSC by Similarity Reconstruction and Model Averaging

In this section, we develop a FedSC algorithm by similarity reconstruction and model averaging. As a classic and popular framework, FederatedAveraging (or FedAvg) is first introduced in  for federated learning. In our work, the proposed FedSC is, therefore, built up based on the backbone of FedAvg as in Figure 1. FedSC consists of two stages. The first stage, shown by the left plot of Figure 1, is federated similarity reconstruction, which constructs a similarity matrix in the manner of federated learning. The second stage, shown by the left plot of Figure 1, is using the reconstructed similarity matrix to implement spectral clustering.

**Stage I Federated Similarity Reconstruction**

Step I: As the startup settings for our algorithm, the shared variable \(\) (_i.e._, the dictionary matrix \(\)) and each local coefficient matrix \(_{p}\) for \(p=1,2,,P\) are initialized randomly in the central server and each client, respectively.

Step 2: For each round \(s\), where \(1 s S\), the previous shared variable \(\) will firstly be broadcast to each participated client. After that, every client uses this received dictionary matrix \(\) to run its own iterative updates of local variables in the Local Update Module (LUM) as:

\[_{p}^{s}= *{arg\,min}_{_{p}}f_{p}(^{s-1},_{p })=*{arg\,min}_{_{p}}\|(_{p})- (^{s-1})_{p}\|_{F}^{2}+\|_{p }\|_{F}^{2}\] (9) \[_{p}^{s}= *{arg\,min}_{_{p}}f_{p}(_{p},_{p }^{s})=*{arg\,min}_{_{p}}\|(_{p})- (_{p})_{p}^{s}\|_{F}^{2}+\|_ {p}^{s}\|_{F}^{2}\] (10)

Step 3: Each client sends back its own dictionary matrix \(_{p}^{s}\), \(p=1,2,,P\), to the central server.

Step 4: The central server collects all (or a subset \(^{s-1}\) of) these uploaded matrices \(\{_{p}^{s}\}_{p=1}^{P}\) and averages them into one new matrix \(^{s}\) in Aggregation Module (AM), i.e.,

\[^{s}=^{s-1}|}_{p^{s-1} }_{p}^{s}\] (11)

where \(|^{s-1}|\) is the number of participated clients. In our study, we fix the number of participating clients for each round \(s\). Therefore, we use the notation \(\) instead of \(|^{s-1}|\) in the sequel. This aggregated dictionary matrix \(^{s}\) will then be used to push the next round of federated iteration until the tolerance condition is broken.

Step 5: When Stage I comes to an end, the spectral clustering will start.

**Stage II Spectral Clustering**

Step 6: Each client sends \((_{p}^{S},_{p}^{S})\) back to the central server for the final aggregation of information.

Step 7: The required similarity matrix is then constructed based on the obtained dictionary matrix \(^{S}\) and coefficient matrix \(^{S}\) in Spectral Clustering Module (SCM). Based on this approximated similarity matrix, the standard spectral clustering is implemented as usual:

\[=(^{T}(,),).\] (12)

Step 8: The central server broadcasts its clustering results to every corresponding client.

Figure 1: Diagram of the proposed FedSC. Stage I (left plot): Federated Similarity Reconstruction (Steps 1-5). Stage II (right plot): Spectral Clustering (Steps 6-8).

### Optimization Algorithm for Federated Similarity Reconstruction

As described in the above section, alternate updating of local variables is a key to solving the proposed FedSC problem. In the following two parts, we discuss the optimization for \(\) and \(\), respectively.

For a client \(c_{p}\), consider the corresponding local optimization problem

\[,}{}\ f_{p}(,)\] (13)

where \(f_{p}(,)=\|(_{p})-() \|_{F}^{2}+\|\|_{F}^{2}= ((_{p},_{p}))-(^{T}( ,_{p}))+(^{T}(,) )+\|\|_{F}^{2}\). Let the derivative of \(f_{p}(,)\) w.r.t. \(\) be zero, we get the following one-step update for \(\):

\[_{p}^{s}=((^{s-1},^{s-1})+_{d})^{-1 }(^{s-1},_{p}), p=1,2,,P.\] (14)

The derivative of \(f_{p}(,)\) w.r.t. \(\) is

\[}{}=}(_{p} {W}_{Z}-}_{Z})+}(_{Z}- }_{Z}),\] (15)

where the intermediate variables are detailed as

\[_{Z}=-^{T}(_{p},)& }_{Z}=(_{1}^{T}_{Z})\\ _{Z}=(0.5^{T})(,)&}_ {Z}=(_{d}^{T}_{Z}).\]

Here \(_{n}\) and \(_{d}\) are the column vectors with all elements of \(1\). Because of the kernel function, \(\) cannot be updated like \(_{p}\). Here, we use the gradient method to update it. At local iteration \(t\), by setting \(_{p}^{s,0}=^{s-1}\), the update scheme of \(\) is

\[_{p}^{s,t}=_{p}^{s,t-1}-_{t}}{ {Z}}(_{p}^{s,t-1}).\] (16)

where \(_{t}\) is the step size and can be set as the reverse of the Lipschitz constant of gradient if possible. We summarize the optimization details in Algorithm 1 (shown in Appendix A).

### Convergence Analysis of The Proposed Algorithm

First of all, it is obvious that all local objective functions \(f_{p}(,)\) for \(p=1,,P\) are lower bounded. To analyze the convergence of Algorithm 1, we make two assumptions. The first one is the Lipschitz continuity of the gradient of the local objective functions.

**Assumption 2.1**.: The gradients of all local objective functions \(f_{p}(,)\) for \(p=1,,P\) are \(L_{Z_{p}}^{s}\)-Lipschitz continuous in \(\), that is

\[\|_{Z}f_{p}(^{s,t},_{p}^{s})-_{Z}f_{p}(^{s,t-1},_{p}^{s})\|_{F} L_{Z_{p}}^{s}\|^{s,t}-^ {s,t-1}\|_{F}.\] (17)

In addition, there exist some lower and upper bounds for \(L_{Z_{p}}^{s}\), _i.e._, \(0<_{Z} L_{Z_{p}}^{s}_{Z}\) hold for all \(p=1,,P\) and \(s=1,,S\).

The second assumption, similar to (Li et al., 2019; Lian et al., 2017), is as follows.

**Assumption 2.2**.: The difference between the local gradient and the global gradient is bounded as

\[\|_{Z}f_{p}(,_{p})-_{Z}F(,)\|_{F },\;p=1,,P.\] (18)

To build the convergence condition, we define the following iterative terms of \(^{s,t}\) and \(^{s}\) for all \(t=1,,Q\) and \(s=1,,S\):

\[T_{C}(^{s,0},^{s})=_{p=1}^{P}_{p} \|_{p}^{s}-_{p}^{s-1}\|_{F}^{2}\\ T_{Z}(^{s,t},^{s})=\|^{s,t}-^{s,t-1}\|_{F}^{2} \] (19)

where the instantaneous average \(^{s,t}\) is defined as

\[^{s,t}=}_{p^{s}}_{p}^{s,t}.\] (20)

Based on the above assumptions, we provide the following convergence guarantee for Algorithm 1.

**Theorem 2.3** (Convergence of Algorithm 1).: _Suppose Assumption 2.1 and Assumption 2.2 hold. Let \(T=S(1+Q)\) be the total number of global and local rounds. Then the sequence \(\{(^{s,t},^{s})\}\) generated by Algorithm 1 with stepsize \(1/L_{Z_{p}}^{s}\) and \(_{p}=}{n}\) satisfies_

\[[_{s=1}^{S}T_{C}(^{s,0},^{s})+_{s=1}^{S} _{t=1}^{Q}T_{Z}(^{s,t},^{s})][F(^{1,0},^{0})-]+ D}{_{Z}}\] (21)

_where \(=1++8)(Q-1)(2Q-1)}{-4(Q-1)^{2}(1+_{Z}^{2} /L_{Z}^{2})}\) and \(D=+}+_{Z}}\)._

The proof can be found in Appendix E. We see that when \(T\), the algorithm converges to a finite value, which is small if \(\) is small and \(_{Z}\) is close to \(_{Z}\).

## 3 Security-Enhanced FedSC

In order to enhance the security of FedSC, we present two noise-augmented variants of the proposed algorithm in this section.

### FedSC with Perturbed Data

We add random noise to the data in each client and then perform Algorithm 1 to reconstruct a similarity matrix, which further improves the privacy of data. Specifically, the data \(^{m n}\) is perturbed by a noise matrix \(^{m n}\) to form the noisy data matrix

\[}=+,\] (22)

where \(E_{ij}(0,^{2})\). We then perform Algorithm 1 with a Gaussian kernel of parameter \(r\) on \(}\) and obtain \(\), \(=[_{1},_{2},,_{P}]\), and

\[}_{}=^{T}(,).\] (23)

We have the following reconstruction (for the true similarity matrix \(_{xx}=(,)\)) error bound4.

**Theorem 3.1** (Error bound of similarity matrix reconstruction).: _Suppose \(\|\|_{2,}=\), \(\|\|_{2,}=_{C}\), and \(\|()-(})\|_{2,}\), where \(\), \(_{C}\), and \(\) are some nonnegative constants. Then with the probability at least \(1-n(n-1)e^{-t}\), the reconstructed similarity matrix \(}_{}\) satisfies_

\[\|}_{}-_{xx}\|_{} {1}{r^{2}}[(+)^{2}-2^{2}]+( _{C}+1)\] (24)

_where \(=+2t)}\)._

Note that \(r\) is the hyperparameter of the Gaussian kernel. In our experiment, \(r\) was automatically estimated as the mean of all pairwise distances between data points, _i.e._, \(r=}_{i,j}\|_{i}-_{j}\|_{2}\). Assume \(|x_{ik}-x_{jk}|=()\) for all \(i,j[n],k[m]\), then \(\|_{i}-_{j}\|=()\), which means \(r^{2}\) is linear with \(m^{2}\). Thus, the reconstruction error \(\|}_{}-_{xx}\|_{}\) is upper bounded by \((^{2}/^{2}+_{C})\), where \(^{2}/^{2}\) can be regarded as a signal-noise ratio. Therefore, the bound is useful. In general, Theorem 3.1 indicates that when the added noise is small and the optimization makes \(\) small, the reconstruction error for the true similarity matrix is less than a small constant with high probability. This verified the effectiveness of our similarity reconstruction method.

It should be pointed out that \(}_{}\) is not guaranteed to be a sparse matrix and hence the corresponding graph may not contain multiple connected components. We therefore use an extra KNN-based operation to get a sparse similarity matrix, which may also reduce the computational cost of eigenvalue decomposition when \(n\) is very large. Specifically, we let

\[}_{}=(}_{ },k)\] (25)which only keeps the largest \(k\) connections from each point to other points. Finally, we perform spectral clustering using \(}_{}\). The central server broadcasts the clustering results to each participating client.

As mentioned before, one can choose to inject some noise into its raw data to avoid privacy leakage. However, a question is how much noise we can add to the data to the largest extent for the guarantee of correct clustering. We first present the following definitions.

**Definition 3.2** (Local neighbor set).: Suppose \(_{i}\) and \(_{j}\) are data points of \(^{m n}\) with class labels \(L_{i}\) and \(L_{j}\) respectively, and let \((_{i})\) be the set of the k-nearest neighbors of \(_{i}\). We define

\[_{i}^{k,intra}:=\{_{j}|L_{i}=L_{j}_{j} (_{i})\}.\] (26)

**Definition 3.3** (Global neighbor set).: Suppose \(_{i}\) and \(_{j}\) are data points of \(^{m n}\) with class labels \(L_{i}\) and \(L_{j}\) respectively, and let \((_{i})\) be the point set of k-nearest neighbors of \(_{i}\). We define

\[_{i}^{k,global}:=\{_{j}|_{j}( {x}_{i})\}.\] (27)

If we call the local neighbor of data point \(_{i}\) the _intra-class neighbor_ of \(_{i}\), another definition called _inter-class neighbor_ of \(_{i}\) can be further introduced as follows.

**Definition 3.4** (Inter-class neighbor set).: Suppose \(_{i}\) and \(_{j}\) are data points of data matrix \(^{m n}\) with class labels, \(L_{i}\) and \(L_{j}\), respectively, and let \((_{i})\) be the point set of k-nearest neighbors of \(_{i}\). We define

\[_{i}^{k,inter}:=\{x_{j}|L_{i} L_{j}_{j}(_{i})\}.\] (28)

Based on the above definitions, the following definition is presented to determine whether a data point can be correctly clustered or not.

**Definition 3.5** (Correct clustering).: Suppose \(_{i}^{m}\) and \(_{j}^{m}\) are data points of data matrix \(^{m n}\), \(_{i}\) is said to be correctly clustered with a tolerance of \(\) if

1. \(}_{ij}_{k}(}_{ik}^{inter})-\) for any of \(_{j}_{i}^{k,intra}\);
2. \(}_{ij}_{k}(}_{ik}^{intra})+\) for any of \(_{j}_{i}^{k,inter}\).

Based on Definition 3.5, the following theorem gives the guarantee of our security-enhanced FedSC.

**Theorem 3.6** (Guarantee of noisy spectral clustering).: _Let \(B()=}[(+)^{2}-2^{2} ]+(_{C}+1)\). Then with the probability of at least \(1-n(n-1)e^{-t}\), performing spectral clustering using \(}_{}\) yields correct clustering results if_

\[B()-_{i}[_{k}(_{ik}^ {inter})-_{k}(_{ik}^{intra})]\] (29)

where \(_{ik}^{inter}=(_{xx})_{ik}^{inter}\) and \(_{ik}^{intra}=(_{xx})_{ik}^{intra}\).

Based on Theorem 3.1 and Theorem 3.6, we can get a bound on the variance of noise for FedSC with perturbed data:

\[[(B_{1}-B_{2})+2^{2}}- ]\] (30)

where \(B_{1}=-_{i}[_{k}(_{ik}^{inter}) -_{k}(_{ik}^{intra})]\) and \(B_{2}=(_{C}+1)\). This bound indicates that the intensity of noise should not be too strong otherwise it may seriously affect the performance of federated spectral clustering. But, at least under this bound, one can choose to inject as much noise as possible into the raw data to ensure data security and privacy.

Using Theorem 3.22 in [Dwork _et al._, 2014] and the post-processing property of differential privacy, we have the following privacy guarantee for this enhanced FedSC algorithm.

**Proposition 3.7**.: _FedSC with perturbed data given by (22) is \((,)\)-differentially private if \( 2cr_{}/\), where \(c^{2}>2(1.25/)\)._Based on this proposition and (30), we obtain the following privacy-utility trade-off:

\[2_{X}/<[ (B_{1}-B_{2})+2^{2}}-].\] (31)

This ensures both clustering performance and \((,)\)-differential privacy. In particular, if we substitute \(\) with the upper bound, we can get a strong level of privacy but the worst utility. By the way, \(B_{1}-B_{2}\) is related to the property of the data. A larger \(B_{1}-B_{2}\) means a better property for clustering, which further provides a larger upper bound for the noise level \(\), yielding a stronger privacy guarantee.

### FedSC with Perturbed Factors

In FedSC with perturbed factor, we added Gaussian noise to \(\) in every round of the optimization but added Gaussian noise to \(\) in the last round of the optimization. To be more specific, \(}=+_{C}\), and \(}=+_{Z}\), where the entries of \(_{C}\) and \(_{Z}\) are drawn from \((0,_{C}^{2})\) and \((0,_{Z}^{2})\) respectively. Then we perform spectral clustering using the following reconstructed kernel matrix:

\[}_{xx}=}^{T}(},})}.\] (32)

The following theorem shows the reconstruction error bound for the ground truth kernel matrix \(_{xx}\).

**Theorem 3.8**.: _Assume \(\|()-()\|_{2,}\), \(\|\|_{2,}_{C}\). Then with probability at least \(1-(n+d)e^{-t}\), it holds that_

\[\|}_{xx}-_{xx}\|_{}_{zc}( _{zc}+2)\] (33)

_where \(_{zc}=+(_{C}_{d}+_{C}^{2}_{d}^{2}}{2r^{2}}))})\) and \(_{d}^{2}=d+2+2t\)._

We see that, given a fixed \(\), the reconstruction error becomes smaller if \(_{Z}\) and \(_{C}\) are smaller. Based on Theorem 3.8 and Definitions 3.2-3.5, we can obtain a bound similar to that in Theorem 3.6 to guarantee correct clustering, which will not be detailed here.

**Theorem 3.9**.: _In FedSC, assume \(_{(p,j)}\{\|_{p}\|,\|^{}_{p}\|\} _{X}\), \(_{(i,j)}\|_{i}-_{j}\|_{}=\), \(\|^{s}_{p}\|_{sp}_{Z}\)\( s\), and \(\|^{S}\|_{2,}_{C}\), we perturb \(\{^{s}_{p}\}_{p=1}^{P}\), \( s=1,2,,S\) with noise drawn from \((0,_{Z}^{2})\) with the parameter \(_{Z}(g_{Z})(e+(_{Z}/_{Z}))/ _{Z}^{2})}\) where \((g_{Z})=_{C}_{X}_{X}_{k}}{r^{2}}\{1+ (_{X}+_{Z})+)}{r^{2}}\}\) and perturb \(\{^{S}_{p}\}_{p=1}^{P}\) with noise drawn from \((0,_{C}^{2})\) with the parameter \(_{C} 2c^{-1}_{X}(_{X}+)/(r^{2} _{C})\) for \(c^{2}>2(1.25/_{C})\). Then, FedSC is \((_{C}+_{Z},_{C}+_{Z})\)-differentially private._

Theorem 3.9 shows that our FedSC with perturbed factors can protect the data privacy provided that the noises added to \(\) and \(\) are sufficiently large. Similarly to (31), we can also get a privacy-utility trade-off using Theorem 3.8 and Theorem 3.9, which is detailed in Appendix B.

## 4 Related Work

It should be pointed out that the study on federated spectral clustering in literature is very limited. Besides our work, the only work that aims to address the problem is [Hernandez-Pereira _et al._, 2021]. More introduction and discussion about the related work (federated matrix factorization/clustering [Yang _et al._, 2021; Ghosh _et al._, 2020; Dennis _et al._, 2021; Wang and Chang, 2022] and spectral clustering [Von Luxburg, 2007; Hernandez-Pereira _et al._, 2021]) are in the supplementary material.

## 5 Experiments

### Performance on similarity reconstruction

Taking the COIL20 dataset [Nene _et al._, 1996] as an example, we first obtain the similarity matrix from vanilla spectral clustering based on the same kernel function. Then, we use the proposed method to derive the estimated similarity matrix \(}_{}\) which is actually an approximation of ground truth. Tomake it clearer, we also give the corresponding sparse similarity matrices by KNN sparsification (25). Figure 2 shows the similarity matrices constructed by different methods. We see that the proposed method can be able to successfully reconstruct the similarity matrix in the federated scenarios. The reconstruction errors on synthetic data, iris, banknote authentication, and COIL20 are in the supplementary material.

### Clustering performance of FedSC

In this subsection, we check the clustering performance of the proposed security-enhanced FedSC method on both synthetic and real-world datasets. The synthetic dataset is generated from concentric circles. The details are in the supplementary. This synthetic dataset is visualized in Figure 3(a). Here, we continue to adopt the aforementioned COIL20 as an example of real-world datasets.

Taking the synthetic dataset as an example, the first group of cases helps illustrate the effectiveness of the proposed FedSC method. we first apply the vanilla spectral clustering method to the clean data. The predictive result is shown in Figure 3(b). It is clear that the vanilla spectral clustering method correctly clusters the data points lying in concentric circles. We then use the proposed FedSC method to cluster the data. One can find in Figure 3(c) that almost all of the data points also have been grouped correctly. However, when we inject some volume of noise into the raw data, things may change a lot. Figure 3(d) is actually the ground truth Figure 3(a) adding some Gaussian noise. When focusing on this noisy data of concentric circles, we see from Figure 3(e) that the vanilla SC failed to cluster the data points while the proposed FedSC method is still able to correctly cluster the data to some extent as in Figure 3(f). As we know, the similarity graph directly constructed from raw data could be very sensitive to each data point. When we add too much noise, the similarity graph may fail to model the local neighborhood relationships which may be the reason why data points in Figure 3(e) are not separable for vanilla SC. Instead, FedSC is based on matrix factorization in the high-dimensional feature space and has a potential denoising effect. Therefore, it is possible for our method to achieve a better performance.The visualization of COIL20 can be found in the supplementary material.

Figure 3: FedSC on concentric circles: (a) ground truth; (b) cluster assignment generated by vanilla SC; (c) cluster assignment generated by FedSC; (d) Noisy ground truth; (e) cluster assignment generated by vanilla SC on noisy data; (f) cluster assignment generated by FedSC on noisy data.

Figure 2: Visualization of similarity matrices: (a) similarity matrix of vanilla spectral clustering; (b) approximated similarity matrix of the proposed method; (c)(d) the corresponding sparse similarity matrices generated by KNN sparsification.

### Comparison with baselines

We compare our method with the clustering method DSC proposed by . Because the existing literature on federated spectral clustering is rare, we here select both classic K-means and spectral clustering as the baselines. Two metrics including accuracy and NMI are adopted to evaluate the clustering results on four datasets including iris , COIL20 , banknote authentication , and USPS . The details are in the supplementary material. Besides the clean data, we also consider adding noise to them to test the performance of methods under the condition of privacy protection. We directly inject Gaussian noise with zero mean and variance \(^{2}\) to the raw matrix \(^{m n}\) as \(}=+\) where \(\) is a Gaussian noise matrix, each element \(_{i,j}\) of which is i.i.d. with \((0,^{2})\).

Table 1 shows the clustering accuracy. Our FedSC almost always achieves comparable clustering results to vanilla SC. It even outperformed vanilla SC in some cases and K-means in most cases since FedSC has a potential denoising effect by approximating a similarity matrix. More importantly, FedSC significantly outperformed DSC in almost all cases. The reason is that DSC performs spectral clustering on each local dataset, which may lead to very unstable and inaccurate results.

### More numerical result

The tSNE visualization, clustering results in terms of NMI, the performance of FedSC with perturbed factors, etc, are in the supplementary material.

## 6 Conclusion

This paper has proposed a secure kernelized factorization method for federated spectral clustering on distributed data. We provide theoretical guarantees for optimization convergence, correct clustering, and differential privacy. The numerical experiments on synthetic and real image datasets verified the effectiveness of our method. To the best knowledge of the authors, this is the work that successfully addresses the problem of federated spectral clustering. One limitation of this work is that we haven't tested our FedSC on very large datasets, though the moderate-size datasets are sufficient to justify the effectiveness of our FedSC. Note that for large-scale datasets, the bottleneck of clustering is the eigenvalue decomposition of the Laplacian matrix, not our FedSC algorithm.

   & & Kmeans & SC & DSC & FedSC \\  \)} & Iris & \(0.8933 0.0000\) & \(0.9000 0.0000\) & \(0.5480 0.0679\) & \(0.9000 0.0031\) \\  & COIL20 & \(0.6113 0.0534\) & \(0.8025 0.0009\) & \(0.1009 0.0100\) & \(0.7828 0.0231\) \\  & Bank & \(0.6122 0.0000\) & \(0.5918 0.0000\) & \(0.5582 0.0045\) & \(0.7672 0.1457\) \\  & USPS & \(0.6704 0.0047\) & \(0.6635 0.0000\) & \(0.1686 0.0014\) & \(0.6596 0.0021\) \\  & ORL & \(0.6325 0.0270\) & \(0.7865 0.0106\) & \(0.1653 0.0073\) & \(0.7235 0.0170\) \\  }\) with \(0.1\)} & Iris & \(0.8940 0.0152\) & \(0.9120 0.0332\) & \(0.4533 0.0658\) & \(0.8993 0.0299\) \\  & COIL20 & \(0.6283 0.0484\) & \(0.8024 0.0020\) & \(0.0995 0.0122\) & \(0.7790 0.0213\) \\  & Bank & \(0.6067 0.0022\) & \(0.6067 0.0944\) & \(0.5558 0.0023\) & \(0.7168 0.1068\) \\  & USPS & \(0.6732 0.0035\) & \(0.6647 0.0016\) & \(0.1690 0.0007\) & \(0.6643 0.0022\) \\  & ORL & \(0.6195 0.0329\) & \(0.7810 0.0065\) & \(0.1600 0.0089\) & \(0.7323 0.0250\) \\  }\) with \(0.3\)} & Iris & \(0.8420 0.0274\) & \(0.8327 0.0267\) & \(0.4533 0.0674\) & \(0.8427 0.0404\) \\  & COIL20 & \(0.6422 0.0366\) & \(0.7997 0.0029\) & \(0.0981 0.0084\) & \(0.7793 0.0240\) \\  & Bank & \(0.6020 0.0038\) & \(0.5859 0.0105\) & \(0.5588 0.0074\) & \(0.6046 0.0064\) \\  & USPS & \(0.6704 0.0063\) & \(0.6720 0.0044\) & \(0.1673 0.0019\) & \(0.6884 0.0509\) \\  & ORL & \(0.6098 0.0167\) & \(0.7885 0.0047\) & \(0.1665 0.0057\) & \(0.7417 0.0280\) \\  }\) with \(0.5\)} & Iris & \(0.7410 0.0252\) & \(0.7313 0.0494\) & \(0.3833 0.0204\) & \(0.7540 0.0336\) \\  & COIL20 & \(0.6389 0.0296\) & \(0.7950 0.0080\) & \(0.1033 0.0167\) & \(0.7403 0.0294\) \\   & Bank & \(0.6051 0.0076\) & \(0.5923 0.0094\) & \(0.5566 0.0030\) & \(0.6086 0.0073\) \\   & USPS & \(0.6699 0.0031\) & \(0.7843 0.0030\) & \(0.1683 0.0017\) & \(0.7778 0.0062\) \\   & ORL & \(0.5983 0.0295\) & \(0.7930 0.0172\) & \(0.1615 0.0096\) & \(0.7107 0.0345\) \\  }\) with \(0.7\)} & Iris & \(0.6500 0.0420\) & \(0.6087 0.0468\) & \(0.3927 0.0349\) & \(0.6120 0.0455\) \\   & COIL20 & \(0.6220 0.0627\) & \(0.7662 0.0172\) & \(0.0893 0.0055\) & \(0.6803 0.0198\) \\   & Bank & \(0.6100 0.0112\) & \(0.6046 0.0144\) & \(0.5566 0.0035\) & \(0.6106 0.0107\) \\   & USPS & \(0.6638 0.0044\) & \(0.7747 0.0040\) & \(0.1675 0.0004\) & \(0.7587 0.0117\) \\   & ORL & \(0.5723 0.0360\) & \(0.7860 0.0093\) & \(0.1613 0.0066\) & \(0.6910 0.0232\) \\  

Table 1: Comparison of clustering accuracy (\(\) and \(}\) denote the raw data and corrupted data respectively). The results of NMI are in Section D.4.