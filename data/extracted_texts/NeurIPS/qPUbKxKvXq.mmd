# Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context

Lakshya A Agrawal

Microsoft Research

Bangalore, India

t-lakagrawal@microsoft.com

&Aditya Kanade

Microsoft Research

Bangalore, India

kanadeaditya@microsoft.com

&Navin Goyal

Microsoft Research

Bangalore, India

navingo@microsoft.com

&Shuvendu K. Lahiri

Microsoft Research

Redmond, United States

shuvendu.lahiri@microsoft.com

&Sriram K. Rajamani

Microsoft Research

Bangalore, India

sriram@microsoft.com

###### Abstract

Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.

Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose _monitor-guided decoding_ (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.

We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen.

## 1 Introduction

Language models of code (LMs), such as in Chen et al. (2021); Nijkamp et al. (2023); Allal et al. (2023) and many others, are revolutionizing code generation. Many commercial offerings based on LMs, like GitHub Copilot, Amazon Code Whisperer and Replit, are now available. The LMs work well when the surrounding code in the vicinity of generation provides sufficient context. Thiscondition does not hold when it becomes necessary to use types, functionality or APIs defined in another module in the repository or an external library, especially those not seen during training. In the absence of awareness of such global context, the LMs end up hallucinating, e.g., using types defined in other files incorrectly. Further, devoid of the repository context, the LMs may lack awareness of other semantic information like the number of arguments required by a called method, or the ordering constraints on method calls (API protocols) to be followed. Often, such type and semantic information can come from artifacts generated at build-time, like Project Lombok (lom, 2009) and ProtoBuf (pro, 2008), and therefore may not even be present as code context in the repository.

As an example, consider the partial code (method to be completed) in Figure 1(a). To complete this code, an LM has to generate identifiers consistent with the type of the object returned by ServerNode.Builder.newServerNode(). The method newServerNode and its return type, class ServerNode.Builder, are defined in another file. If an LM does not have information about the ServerNode.Builder type, it ends up hallucinating.

We show a completion generated by the OpenAI text-davinci-003 (Ouyang et al., 2022) and SantacCoder (Allal et al., 2023) models in the box decorated with \(\) in Figure 1(a). The completion uses identifiers host and port, which do not exist in the type ServerNode.Builder. The generated code therefore results in "symbol not found" compilation errors. The lack of awareness of other semantic information from the global context may result in other kinds of errors like compile-time errors (e.g.,"actual and formal argument lists differ in length" on using wrong number of arguments) or runtime errors (e.g., IllegalStateException on violation of API protocols).

Integrated development environments (IDEs) have been at the forefront of assisting developers. Our inspiration is the use of static analysis by IDEs to bring the global context at the fingertips of developers. Many analyses are integrated in IDEs (Fuhrer, 2013) to infer and enforce semantic constraints on the code under development, e.g., resolving def-use, symbol references, and type hierarchies. Recently, there has been a rise in the use of Language Server Protocol (LSP) (lsp), which is an open industry standard of communication between IDEs and programming language specific tools like static analyzers and compilers, called Language Servers. There are a large number of Language Servers available, targetting most programming languages (lan, 2023), and providing a variety of syntactic and semantic information. In this work, we focus on the type-directed code completion analysis available through LSP in a language-agnostic manner, to provide guidance to an LM.

We propose a notion of _monitors_ as a stateful interface between LMs and static analysis. A monitor observes the code generated by an LM and queries static analysis at pre-defined trigger points. The suggestions returned by the static analysis are converted to masks which are used for reshaping the logits (or equivalently, token-generation probabilities) produced by the LM in the subsequent decoding steps. We call our method _monitor-guided decoding_ (MGD). Unlike an LM, a static analysis operates on the entire repository and its dependencies. While the LM generates completions by conditioning on the local context, the static analysis ensures consistency with the rest of the code in the repository. Through MGD, we bring the two together without the need to retrain the LM, and making a minor and modular addition to the decoding stage of the LM.

Figure 1(a) also shows the code generated by the SantaCoder model with MGD in the box decorated with \(\). This code makes use of identifiers that are actually defined in the class

Figure 1: Motivating example to illustrate monitor-guided decoding (MGD).

ServerNode.Builder. It compiles and matches the ground truth. In comparison, the _same_ SantaCoder model without MGD generates the erroneous code shown in the box decorated with.

Some recent approaches use static analysis (Shrivastava et al., 2022; Ding et al., 2022; Pei et al., 2023) or retrieval (Zhang et al., 2023) to extract relevant code fragments from the global context. These approaches expand the prompt (Shrivastava et al., 2022; Pei et al., 2023; Zhang et al., 2023) or require architecture modifications (Ding et al., 2022) and additional training (Ding et al., 2022; Pei et al., 2023). In comparison, we provide token-level guidance to a frozen LM by invoking static analysis on demand. Our method is complementary to these approaches as they condition the generation by modifying the _input_ to the LM, whereas we apply _output_-side constraints by reshaping the logits.

We make the following contributions in this paper:

* Monitor-Guided Decoding (MGD) as a stateful interface between LMs and static analysis. A monitor watches the LM generating code, queries static analysis in the background, and uses the information from the static analysis to effectively guide the decoding stage of LMs.
* PragmaticCode, a publicly-released dataset of Java code repositories complete with their development environments and dependencies.
* Instantiation of MGD for generating code having type-consistent identifier dereferences.
* Large scale evaluation on PragmaticCode showing: (1) MGD consistently improves the ability of an LM (across varying parameter scales) to generate type-consistent identifiers, compilation rates and agreement with ground truth. (2) Further, LMs with fewer parameters, in conjunction with MGD, can outperform larger LMs. For instance, SantaCoder-1.1B with MGD achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model, when both have a budget of 1 generation each. With a budget of 4 generations, it also surpasses agreement with ground truth of text-davinci-003. (3) We also evaluate how MGD complements different prompt augmentation and decoding strategies.
* Microbenchmark to demonstrate generalizability of MGD to different (1) programming languages, (2) coding scenarios, and (3) use of other static analysis techniques for guiding with rich semantic properties. Notably, we demonstrate that small-LMs can be guided to adhere to rich static properties like satisfaction of stateful API protocols.
* We open source our implementation and provide an extensible Python library called multispy with static analysis bindings for multiple languages over LSP, suitable for monitoring of LMs. It can be used for other AI4Code scenarios as well.

## 2 Monitor-Guided Decoding

**Background.** Static analysis of code (Nielson et al., 2015) is used widely in industry in various applications such as in detecting bugs and optimizing code. While analysis is usually performed on complete code, IDEs have long applied static analysis on incomplete code under development (Reps et al., 1983; Dagenais and Hendren, 2008), using algorithms for incremental parsing and semantic analysis (e.g., type inference) of partial code (Hedin, 1992; Wagner, 1997; Maddox III, 1997). These analyses have now become a standard part of language servers.

A key abstraction used in analysis of partial code is partial abstract syntax trees (ASTs) with special nodes to indicate incomplete parts of code. These ASTs are further decorated by semantic information through attribute grammars (Reps et al., 1983) that decorate each AST node with attributes that capture the static semantics not captured in the context-free grammar (such as consistency of types among expressions in an assignment). This can range from computing the type-hierarchy for object oriented languages, binding the variables in AST nodes to their declarations, resolving the types of expressions as well as computing the def-use relationships for resolved AST nodes (Fuhrer, 2013).

Figure 1(b) shows the partial AST for the incomplete code in Figure 1(a). All the statements upto the incomplete return statement are completely parsed and subtrees corresponding to them are constructed. The subtree for the return statement includes a node [UNKNOWN] indicating the incomplete part. As shown in Figure 1(b), an incremental semantic analysis resolves the type of the partial expression ServerNode.Builder.newServerNode() to ServerNode.Builder. Later, we show how to use this type information to construct a monitor, which can then be used to guide an LM to generate type-consistent identifier completio

**Basic Concepts and Notation.** Consider an LM \(L_{}\) operating on a vocabulary \(V\). Let \(x_{1},,x_{n}\) be the _partial code_ that has been generated by the LM and \(x_{n+1}\) be a candidate next token. Though a vanilla (auto-regressive) prompt would consist only of \(x_{1},,x_{n}\), today many approaches augment it with additional information. We use \(p\) to indicate this _additional prompt_, e.g., the suffix information used in fill-in-the-middle prompting (Donahue et al., 2020; Fried et al., 2022; Bavarian et al., 2022).

A _property_\(\) specifies the constraints that a piece of code needs to satisfy. A _static analysis_\(A_{}\) checks whether the partial code satisfies \(\). If yes, it returns suggestions to extend it so that the extended code continues to satisfy \(\). The static analysis works on the _repository context_\(C\) which not only includes code spread across multiple files in the repository, but also external dependencies and intermediate artifacts (e.g., code bindings) generated during the build process. Such repository context is often very large, diverse and complex. Directly including it as an input to the LM will result in bloating and pass the burden of distilling useful information from it to the LM.

A _monitor_\(M_{}\) for a property \(\) is a tuple \((A_{},s_{0},S,,,)\). The monitor starts in the _wait state_\(s_{0}\). If the partial code satisfies the _pre-condition_pre then the monitor is triggered and it invokes \(A_{}\) on the partial code. The monitor maintains the suggestions returned by \(A_{}\) in its state and uses them to guide sampling of the next token \(x_{n+1}\). Let \(S\) be the set of states and \(s,s^{} S\) respectively be the current and next states of the monitor. With each sampled token \(x_{n+1}\), the monitor _updates_ its state using a function \((s,x_{n+1})\) to a new state \(s^{}\), which tracks the residual suggestions after the token \(x_{n+1}\) is output. When the suggestions are exhausted, it reverts to the wait state \(s_{0}\). We explain the function \(\) below.

**Decoding Process.** Usually, the next token \(x_{n+1}\) can be any token from the vocabulary \(V\), sampled based on the logits \(\) determined by the LM. Unlike the usual decoding, in _monitor-guided decoding_, we supervise the code generation using a monitor \(M_{}\) for a property \(\). We denote the composition of \(L_{}\) and \(M_{}\) by \(L_{}\|M_{}\), meaning that both the LM and the monitor are running concurrently and sampling the tokens jointly. The decoding is conditioned on the partial code \(x_{1},,x_{n}\), the repository context \(C\), the prompt \(p\) and the current state \(s\) of the monitor.

Eq. (1) states that whenever the monitor is in the wait state \(s_{0}\), we sample \(x_{n+1}\) as per the logits \(\) determined by the LM (Eq. (2)). Otherwise, the logits are combined with a mask \(m\) using a function \(\) such that if \(m[x]=0\) then \([x]\) is reset to a large negative value \(-K\) and is left unchanged otherwise. This mask is computed by the function \(\) in Eq. (3) guided by the current state \(s\) of the monitor. Eq. (4) defines how the state of the monitor evolves. When the pre-condition \((s;x_{1},,x_{n})\) evaluates to true, the next state \(s^{}\) of the monitor is determined by the suggestions returned by the static analysis \(A_{}\). Otherwise, it is determined by the update function.

\[(L_{}||M_{})(x_{n+1}|x_{1},,x_{n};C,p,s) =()[x_{n+1}]&$ is the wait state}\\ ( m)[x_{n+1}]&\] (1) \[ =L_{}(\ \ |x_{1},,x_{n};p)\] (2) \[m =(s,V)\] (3) \[s^{} =A_{}(x_{1},,x_{n};C)& (s;x_{1},,x_{n})$}\\ (s,x_{n+1})&\] (4)

The specifics of the monitor state, and the \(\), \(\) and \(\) functions depend on the static analysis \(A_{}\) used by the monitor. Our formulation is general and even allows combining multiple static analyses by taking a product of the state-spaces of their respective monitors. In the following, we discuss a specific instantiation of this framework of monitor-guided decoding.

**Monitoring for Use of Type-Consistent Identifiers.** When an object obj of a type \(T\) is dereferenced, the next token (or more generally, the sequence of subtokens) should refer to an identifier of a field or method defined by the type \(T\). It can otherwise result in a "symbol not found" error. The type \(T\) could be defined in another package, imported file or in a library. Unless \(T\) comes from a popular library seen during training, the LM may not have knowledge about \(T\).

Our monitor \(M_{}\) is triggered when the partial code \(x_{1},,x_{n}\) ends with a partial object dereference expression "obj." where " " is the dereference operation. This is the pre-condition \(\) we use. We employ a static analysis \(A_{}\) which returns all the type-consistent identifiers that can be referenced through \(\). For this, \(A_{}\) performs a global analysis over the partial code, imported files, libraries used, and class hierarchies to infer the type \(T\) of \(\) and the identifiers accessible through \(T\). The set of type-consistent identifiers returned by \(A_{}\) forms the state of the monitor (see Eq. (4)).

Given a state \(s\) and the vocabulary \(V\) of the LM, maskgen identifies all the tokens in \(V\) that are consistent with the suggestions in \(s\). The identifiers returned by the static analysis are tokens as per the programming language, whereas the vocabulary \(V\) may use its own space of subtokens (Schuster & Nakajima, 2012; Kudo & Richardson, 2018).The mask \(m\) (Eq. (3)) is generated by string matching. For all tokens \(t V\) that form prefixes of the identifiers in \(s\), the mask value \(m[t]\) is set to \(1\), indicating that they can be sampled. Let \(E\) be the set of special symbols that indicate end of an identifier name, e.g., the symbol '(' in 'getName(' or ',' in 'x,'. Let \(w\) be a string in \(s\) and \(\) be the set of all possible characters. If a token \(t V\) matches the regular expression \(w E^{*}\) then its mask value \(m[t]\) is also set to \(1\). For all other tokens \(t\) in \(V\), the mask value \(m[t]\) is set to \(0\).

Let \(x_{n+1}\) be the next token sampled according to the second equation in Eq. (1). If \(x_{n+1}\) contains a symbol from \(E\), indicating that a complete identifier name has been generated in accordance with the set returned by \(A_{}\), the monitor reverts to the wait state to wait for the next trigger. Otherwise, the token \(x_{n+1}\) must be a prefix of a member of \(s\). The update function removes the members in \(s\) that are not prefixed by \(x_{n+1}\), and those prefixed by \(x_{n+1}\) are updated by pruning the prefix string \(x_{n+1}\). The resulting set of character strings forms the next state \(s^{}\) (see the second equation in Eq. (4)). If \(A_{}\) returns an empty set to start with, we abandon the current run. Note that a single identifier may need to be generated using multiple tokens. Figure 4 (see Appendix A) shows the interaction between the LM and the monitor for the example in Figure 1, and specifically illustrates how the complete identifier names suggested by the static analysis are gradually pruned by the monitor to corresponding suffixes in each successive state as the prefixes get generated as tokens.

## 3 Experimental Setup

**Dataset Creation.** In order to evaluate MGD, we need real-world repositories with their build environments and dependencies. Most published datasets are standalone, with the exception of CoderEval (Yu et al., 2023) and PyEnvs (Pei et al., 2023), both of which are not publicly available at the time of this writing. Hence we curated PragmaticCode, a dataset of real-world open-source Java projects complete with their development environments and dependencies. We ensure that these repositories were released publicly only after the determined training dataset cutoff date (31 March 2022) of the models which we use to evaluate MGD.

From PragmaticCode, we identify a set of method-level completion task instances, creating DotPrompts as a method-level code completion benchmark. Each testcase in DotPrompts consists of a prompt upto a dereference location (using the "." operator in Java) within a target method, and the task is to complete the remainder of the method. We ensure sufficient complexity in the identified target methods in DotPrompts by including methods that satisfy a set of complexity filters (e.g., the method should have at least 7 lines of code) described in detail in appendix B. Overall, PragmaticCode consists of **100** repositories, and DotPrompts consists of **1420** methods and **10538** dereference prompts. Appendix B gives further details.

**Models.** We study the effect of performing MGD on code generation with the HuggingFace Transformers (Wolf et al., 2020) implementation of Salesforce CodeGen family of models (CodeGen- {350M, 2B, 6B}-Multi, abbreviated as **CG-{350M, 2B, 6B}** hereafter) (Nijkamp et al., 2023) and BigCode SantaCoder-1.1B (**SC** or SantaCoder hereafter) (Allal et al., 2023). We also evaluate OpenAI text-davinci-003 (**TD-3** hereafter) with and without MGD, available on Azure.

**Prompting Strategies.** We study the effect of different prompt augmentation techniques when combined with MGD: (1) **Standard**: Include the local file content up to the dereference point and truncate from left to fit the prompt budget. (2) **classExprTypes**: For a given target method belonging to a class C, identify the type of all expressions occurring in C (after masking out the target method to prevent leakage) and include the concatenated file contents for the type definitions of all the identified files, truncating from the left as necessary. We assign a budget of 20% tokens of total prompt budget to classExprTypes. (3) **RLPG**: Use the prompt augmentation technique proposed in Shrivastava et al. (2022). We use their released source code and model checkpoints to adapt RLPG to DotPrompts.

**Decoding Strategies.** We experiment with two decoding strategies: (1) **Autoregressive:** for left-to-right decoding. (2) **Fill-in-the-middle:** Use fill-in-the middle (FIM) setting implemented in SantaCoder (Allal et al., 2023).

**Metrics.** We use the following metrics to measure the quality of generated code: (1) **Compilation Rate (CR):** We replace the ground truth method with the generated method in the context of the complete repository and invoke a clean build. We assign a score of 1 if the compilation succeeds, and 0 otherwise. (2) **Match with the ground truth:** We use three specific metrics to measure how closely the generation matches ground truth, namely (a): **Next Identifier Match (NIM):** If the first Java token generated by the LM matches with the ground truth, we assign a score of 1, 0 otherwise; (b) **Identifier Sequence Match (ISM):** Longest prefix match between the ordered set of identifier names in the ground truth and generated completion, normalized by the number of identifiers in the ground truth; and (c) **Prefix Match (PM):** Longest prefix match between the ordered set of Java tokens (as obtained from a Java Lexer) between the ground truth and generated completion, normalized by the number of tokens in the ground truth. Except NIM, all other metrics - namely CR, ISM and PM - evaluate the complete method-level generation by the model.

In our experiments, for all the evaluated model configurations, we use nucleus sampling (Holtzman et al., 2020) with a top-p value of 0.95 to generate \(n=6\) independent samples. For a budget of \(k[1,n]\) samples, we compute the aggregate score \(score@k\) (see Appendix D). On the discrete-valued metrics (CR and NIM), it is identical to \(pass@k,n\) (Chen et al., 2021), estimating the expected number of times the list of \(k\) candidates contains at least one successful compilation or match with ground-truth identifier. On the real-valued metrics (ISM and PM), it estimates the expectation of the maximum value of the corresponding metric given \(k\) chances.

**Python Library for MGD.** We are releasing an extensible Python library, multispy, for interfacing between LMs and language servers using LSP. It can be used with multiple programming languages, static analyses and LMs. It also supports an approximate mechanism for MGD of black-box LMs with limited logit-masking support. Please refer to Appendix C for more details.

## 4 Evaluation

Table 1 shows the summary of the results for all of our experiments and metrics. For the "-MGD" configurations, we also report the relative improvement over the _base model_ in parentheses, where the base model is the same model configuration without MGD. Below, we present a detailed evaluation.

### Effect of MGD on Models across Parameter Scale and Architectures

We present results for all the models on Standard prompts described in Section 3.

**Compilation Rate.** As shown in Figure 1(a), all the base models with MGD, including the smallest model CodeGen-350M for \(k 4\), outperform the largest model text-davinci-003, by maximum relative margin of 16.55% achieved by SantaCoder. All the models with MGD outperform their respective base models on Compilation Rate, by a relative improvement ranging between 21.77%-24.69%. TD-3-MGD outperforms TD-3 by a relative margin of 18.52%.

 Config\{ (Metric, score@6) & CR & NIM & ISM & PM \\ CG-350M & 52.43 & 76.94 & 31.86 & \(26.86\) \\
**CG-350M-MGD** & **65.37 (24.69\%)** & **83.80 (8.92\%)** & **34.31 (7.70\%)** & **28.69 (6.82\%)** \\ CG-2B & 57.01 & 81.11 & 36.38 & 30.95 \\
**CG-2B-MGD** & **70.91 (24.38\%)** & **87.32 (7.66\%)** & **39.03 (7.29\%)** & **33.06 (6.82\%)** \\ CG-6B & 58.64 & 81.55 & 37.17 & 31.69 \\
**CG-6B-MGD** & **72.28 (23.25\%)** & **87.35 (7.11\%)** & **39.55 (6.41\%)** & **33.56 (5.89\%)** \\ SC & 59.97 & 82.40 & 38.14 & 32.10 \\
**SC-MGD** & **73.03 (21.77\%)** & **88.42 (31.31\%)** & **40.69 (6.68\%)** & **34.25 (6.72\%)** \\ 
**SC-classExprTypes** & 64.57 & 84.91 & 39.67 & 33.55 \\
**SC-classExprTypes**-MGD** & **75.01 (16.18\%)** & **89.37 (5.25\%)** & **41.56 (4.78\%)** & **34.98 (4.26\%)** \\
**SC-RLPG** & 66.39 & 85.42 & 42.35 & 36.21 \\
**SC-RLPG-MGD** & **78.14 (17.70\%)** & **89.89 (5.23\%)** & **44.47 (5.00\%)** & **37.97 (4.87\%)** \\ SC-FIM & 68.23 & 85.56 & 42.22 & 36.12 \\
**SC-FIM-MGD** & **80.19 (17.52\%)** & **89.89 (5.07\%)** & **44.40 (5.39\%)** & **37.91 (4.95\%)** \\ SC-FIM-classExprTypes** & 70.97 & 86.99 & 42.67 & 36.36 \\
**SC-FIM-classExprTypes**-MGD** & **80.33 (13.18\%)** & **90.42 (39.94\%)** & **44.18 (3.54\%)** & **37.75 (3.82\%)** \\
**TD-3** & 62.66 & 86.18 & 44.97 & 38.77 \\
**TD-3-MGD** & **74.26 (18.52\%)** & **91.19 (5.81\%)** & **47.33 (5.24\%)** & **39.94 (30.3\%)** \\ 

Table 1: Summary of results with a budget of 6 generations per model: The numbers in parentheses are relative improvements of the “-MGD” configuration over the respective base model.

**Next Identifier Match.** As seen in Figure 1(b), all the models with MGD outperform the respective base models, with a relative improvement of 7.11%-8.92%. The smallest model CodeGen-350M with MGD outperforms the much larger CodeGen-6B with a relative improvement of 2.76%. SantaCoder with MGD outperforms the larger CodeGen-6B by a relative margin of 8.42%, and even the largest model text-davinci-003 by 2.60%. TD-3-MGD outperforms TD-3 with a relative margin of 5.81%.

**Identifier Sequence Match.** Figure 1(c) shows that all the models with MGD outperform their respective base models on ISM, showing a relative improvement ranging between 6.41%-7.70%. SantaCoder and CodeGen-2B with MGD outperform the larger CodeGen-6B with a relative margin of 9.47% and 5.00% respectively. TD-3-MGD outperforms TD-3 with a relative margin of 5.24%

**Prefix Match.** Figure 1(d) shows percentage prefix match with ground truth. All the models with MGD outperform their respective base models with a relative improvement of 5.89%-6.82%. Both SantaCoder and CodeGen-2B with MGD outperform the larger CodeGen-6B with a relative margin of 8.08% and 4.30%. TD-3-MGD outperforms TD-3 with a relative margin of 3.03%.

**SC-MGD vs. TD-3.** SC is a 1.1B parameter model whereas TD-3 has 175B parameters. Being a much larger model and possibly due to other differences in training, TD-3 does better than SC across all metrics. Interestingly, with MGD, SC-MGD outperforms TD-3 on CR (Figure 1(a)) and NIM (Figure 1(b)). ISM and PM are method-level metrics and the relative advantage of the larger model prevails. Even then, with a small increase in the sampling budget, from \(k=1\) to \(k=4\), SC-MGD manages to surpass TD-3's performance with \(k=1\) on ISM (Figure 1(c)) and PM (Figure 1(d)).

**Summary.** MGD improves the compilability of code significantly, across architectures and parameter scale, leading even the smallest CodeGen-350M with MGD to outperform the largest LM, text-davinci-003. We also see improvements in all ground truth agreement metrics. Notably, smaller LMs with MGD outperform larger LMs (CodeGen-350M with MGD outperforms text-davinci-003 in CR and NIM, CodeGen-2B with MGD outperforms CodeGen-6B on ISM and PM) across all metrics.

### Effect of MGD and Prompt Augmentation Strategies

We choose the best-performing base model, SantaCoder, from Section 4.1 to study the effect of prompting when combined with MGD. Figure 3 shows results for SantaCoder-{Standard, classExprTypes, RLPG} and TD-3, compared against SantaCoder with respective prompts and MGD.

**Compilation Rate.** Figure 2(a) shows the results for Compilation Rate. We observe improvements in compilation rate with both the prompting techniques, classExprTypes and RLPG, with RLPG marginally outperforming classExprTypes. We note that SantaCoder with Standard prompt and MGD is able to relatively improve over both RLPG and classExprTypes augmentation by 10.01% and 13.11% respectively. Further, SantaCoder with RLPG and MGD is able to outperform SantaCoder-RLPG and SantaCoder-classExprTypes with a relative margin of 17.70% and 21.02% respectively, while increasing the margin of relative improvement over text-davinci-003 to 24.70%.

**Next Identifier Match.** As seen in Figure 2(b), similar to compilation, both RLPG and classExprTypes prompt augmentation leads to improvement over the base model. However, SantaCoder with either prompt augmentations underperforms text-davinci-003. SantaCoder with MGD outperforms

Figure 2: score@k for models with MGD and Standard prompt compared against base models. The values of \(k\) are marked on the X-axis.

TD-3, and consequently, both SantaCoder-RLPG and SantaCoder-classExprTypes with a relative improvement of 2.60%, 3.51% and 4.14% respectively. SantaCoder with prompting and MGD outperform their respective baselines (SantaCoder with prompting) by a relative margin in the range of 5.23%-5.25%. We note that SantaCoder with RLPG and MGD increases the relative improvement with respect to the largest model, text-davinci-003 to 4.31%.

**Identifier Sequence Match.** On ISM, SantaCoder with prompt augmentation and MGD is able to outperform its respective baseline by a relative improvement of 4.78%-5.00%, while both the prompt augmentations result in an improvement over the base model. SantaCoder with RLPG and MGD is able to significantly reduce the gap with text-davinci-003, underperforming it by just 1.11%.

**Prefix Match.** As seen in Figure (d)d, SantaCoder with prompt augmentation and MGD is able to outperform its respective baseline by 4.26%-4.87%.

**Summary.** While prompt augmentation techniques help in improving performance on all metrics, we see further improvement with MGD augmentation, and conclude that the contributions by prompt augmentation and MGD are complementary. Notably, SantaCoder-RLPG with MGD improves the relative margin for compilation rate with respect to text-davinci-003 to 24.70% compared to the 16.55% improvement achieved by SC-MGD, or 5.95% improvement achieved by SC-RLPG.

### Effect of MGD on Fill-in-the-middle (FIM) Decoding

Among the base models, SantaCoder supports the FIM modality. We evaluated SantaCoder with autoregressive and FIM decoding strategies and text-davinci-003, and compared them with respective configurations of SantaCoder with MGD. Similar to our observations with prompt augmentation, while FIM modality leads to improvements across all metrics, we see continued improvement when using both FIM and MGD. Due to space limitations, detailed results are in Appendix E. Motivated by the complementary nature of FIM and MGD, we further evaluated SC-FIM-classExprTypes-MGD, combining both prompt augmentation and FIM modality. Consistent with our findings, it leads to a further improvement over SC-FIM-classExprTypes, as seen in Figure 5 (Appendix E).

### Effect of Identifier Complexity on Next Identifier Match

Identifier names in repositories can get specific and long (Karampatsis et al., 2020). Due to this, while commonly used APIs may get tokenized into single tokens, identifiers specific to the context of individual repositories, especially in private settings, can span multiple subtokens in the LM vocabulary. We define the complexity of an identifier as the mean number of subtokens required to decode it. We show that the ability of LMs to accurately predict the identifier name decreases sharply with an increase in identifier complexity, while augmenting them with MGD improves their performance. MGD provides an improvement in the range of 21.79%-27.91% compared to the base model without MGD, across parameter scales, for the highest identifier complexity, which is prevalant in more than 36% of the methods in DotPrompts. The detailed results are available in Appendix F.

Figure 3: score@k for models with MGD and prompt augmentation compared against base models.

Generalizability Study

We curate MGDMicroBench as a micro benchmark (10 examples), spanning 3 programming languages (Java, C#, Rust), 4 coding scenarios, and requiring use of 2 different static analyses, to evaluate generalizability of MGD. The detailed results are discussed in Appendix H.

**Different Programming Languages.** MGD utilizes static analyses that help infer and enforce semantic constraints on the code under development. Such analyses are available through the Language Server Protocol (LSP) (lsp) for most programming languages, e.g., clangd for C/C++ (cla, 2020), Jedi for Python (Roeca, 2019) and Rust Analyzer for Rust (rus, 2018). The monitor used in MGD can be instantiated as a thin client around LSP. Supporting new languages is easy and doesn't necessitate changes to the monitor's static analysis interface. Hence, MGD is applicable to most programming languages. Results on MGDMicroBench demonstrates MGD for Java, C# and Rust.

**Different Coding Scenarios.** A monitor under MGD triggers when a specified precondition is met during code generation. Flexibility in defining preconditions allows MGD to be applied to a variety of code scenarios, as follows: **1) Instantiation of valid classes:** A monitor triggers on 'new ', invokes a static analysis that identifies instantiable classes from the local & global context to ensure only valid classes are instantiated. **2) switch over enum:** A switch statement over enum uses named enums in case <val> to select branch in C# and Java. A monitor triggering on 'case'is used to generate valid enums. **3) Correct number of arguments:** A stack-based monitor is implemented to guide for generation of right number of arguments to methods, that also handles nested function calls. **4) Joint monitoring for multiple properties:** Unlike previous examples that monitored for a single static property, we instantiate 2 joint-monitors: **a)** jointly guiding for type-correct dereferences & along with right number of arguments, **b)** jointly guiding for valid switch enum branches & type-correct dereferences. MGDMicroBench (appendix H.2) provides results over all of these scenarios.

**Different Static Analyses.** MGD is able to utilize results from various static analyses to guide the generation of code with LMs. We demonstrate MGD over the following properties in MGDMicroBench: **1) Typestates (Strom & Yemini, 1986):** Many APIs are stateful, and require callers to follow specific ordering of calls as part of the API contract. For example, a type representing a file handle would have a contract disallowing calls to read after close has been called. Such contracts can be expressed as finite state machines (FSMs), called typestates. **2) Session Types (Jespersen et al., 2015)** ensure that messages between concurrent programs are sent and received in the expected order, following a protocol, and are specified as communicating FSMs. In MGDMicroBench, we demonstrate a monitor for Rust that utilizes typestate and session-type design analyses.

## 6 Discussion

**Limitations.** Though static analysis is a mature field with strong theoretical foundations and several robust implementations of tools, analyzing partial and incomplete programs is still a difficult problem. In practice, editors such as Eclipse and Visual Studio support static analysis with heuristics. Though these heuristics are well-engineered and are widely used, they can be both imprecise (they can give incorrect suggestions) and incomplete (they can leave out correct suggestions). Satisfying functional-correctness specifications like pre/post-conditions and invariants is beyond the scope of this work. Consequently, though our results from guiding LMs using these analyses (through MGD) show improvements in quality metrics, additional steps such as testing and human inspection are needed to guarantee correctness of generated code.

**Societal Impact.** Software pervasively affects all aspects of our lives. With LMs being widely deployed as copilots and intelligent assistants to help developers write code, it is crucially important to develop tools like MGD to improve the quality of code generated by LMs (even if humans review and accept the suggestions given by LMs). Without such tools, we risk introducing bugs in code due to incorrect suggestions made by LMs, which has the potential to impact all of our lives negatively.

**Amount of Compute.** Our experiments do not involve any training, and we only perform inferences. We used machines of the following configurations: (1) CPU: 24-core AMD Epvc with 220GB RAM, GPU: Nvidia A100 80GB. (2) CPU: Intel Xeon(R) Platinum 8168 with 290GB RAM, GPU: Nvidia Tesla V100 16GB. For the experiments to evaluate text-davinci-003, we used the Azure API.

## 7 Related Work

**Pre-trained Models of Code.** Many powerful pre-trained models have been designed for code. These include encoder-only models like CodeBERT (Feng et al., 2020), GraphCodeBERT Guo et al. (2020) and CuBERT (Kanade et al., 2020); encoder-decoder models like PLBART (Ahmad et al., 2021), CodeT5 (Wang et al., 2021) and AlphaCode (Li et al., 2022); or decoder-only models like Codex (Chen et al., 2021), GPT-J (Wang and Komatawaki, 2021), Austin et al. (2021), GPT-Neo (Black et al., 2021), GPT-NeoX (Black et al., 2022), CodeParrot (Tunstall et al., 2022), PolyCoder (Xu et al., 2022), Inocder (Fried et al., 2022), CodeGen (Nijkamp et al., 2023), SantaCoder (Allal et al., 2023) and StarCoder (Li et al., 2023); or unified models like UniXCoder (Guo et al., 2022). Our monitor-guided decoding works only with logits and hence can be used with any model.

**Global Context of Code.** Hellendoorn and Devanbu (2017) build an n-gram model with a cache to track directory-level context. Xu et al. (2021) use locality based on directory-structure in retrieval-augmented modeling (Khandelwal et al., 2019). Many approaches use static analysis or previous generations (Zhang et al., 2023) to extract relevant code. They use the relevant context to either augment the prompt (Shrivastava et al., 2022; Pei et al., 2023; Zhang et al., 2023) or embeddings (Pashakhanloo et al., 2022; Ding et al., 2022) presented as input to the LM. Due to the limit on the prompt or embedding size, these approaches filter information through random walks (Pashakhanloo et al., 2022), classification (Shrivastava et al., 2022) or using fixed pruning strategies (Ding et al., 2022).

As we neither augment the prompt nor use extra embeddings, we do not need to prune the global context. We let the static analysis generate completion suggestions using the entire repository-level context. Zan et al. (2022) and Zhou et al. (2022) retrieve information from library documentation for prompt augmentation. Our static analysis analyzes libraries along with the repository-level source code. Several of these techniques require architecture modifications (Pashakhanloo et al., 2022; Ding et al., 2022) or finetuning (Zan et al., 2022; Ding et al., 2022; Pei et al., 2023). We use a simple interface between logits and static analysis with a frozen LM. Most of these approaches, excluding (Xu et al., 2021; Zhang et al., 2023), use one-time _a priori_ retrieval. In contrast, we provide token-level guidance by invoking static analysis on demand. Our method is _complementary_ to all the above approaches as they all try to condition the generation by modifying the _input_ to the LM whereas we apply _output_-side constraints by reshaping the logits.

**Syntactic and Semantic Constraints.** There are two primary lines of work to enforce syntactic and semantic constraints on code generation, based on specialized modeling and through constrained decoding. GNN2NAG (Brockschmidt et al., 2019) and NSG (Mukherjee et al., 2021) are examples of the first and use attribute grammars. They are respectively evaluated on expressions that do not use user-defined methods or on methods with class-level context. We consider repository context for method-level completion. Unlike these approaches, our work is applicable to off-the-shelf LMs. PICARD (Scholak et al., 2021) and Synchromesh (Poesia et al., 2022) are constrained decoding approaches similar to ours. They use incremental parsing for syntactic validity and design domain-specific semantic checks to ensure semantic validity. Both are evaluated on SQL, and Synchromesh additionally considers domain-specific languages for visualization and calendar applications. In comparison, we target generation of general-purpose programming languages with focus on semantic constraints like type-consistency, API protocols, etc. using static analysis over repository context.

## 8 Conclusions and Future Work

In this work, we show how to use repository-wide information computed by static analysis (specifically, type-based analysis) using a stateful monitor as an interface, to improve quality of code generated by LMs. Our experimental results show the potential for significant quality improvements for code generation using this approach. Our approach is complementary to prompt augmentation techniques. It allows smaller models to achieve better or competitive performance compared to much larger models. This could open up the possibility of using smaller models directly within IDEs, alongside our monitor, as an alternative to the use of remotely-hosted Large LMs (LLMs), reducing inference costs and improving privacy. Our method is general and applicable to various coding scenarios where LMs are used generatively, such as code refactoring, code repair, or code completion, even if the repositories are in a transient state. We plan to expand the scope of MGD to more languages and deeper semantic analyses such as pre/post-conditions for which advanced constrained decoding methods (including backtracking and beam-search) might be needed.