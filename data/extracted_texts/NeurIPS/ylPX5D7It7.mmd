# Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization

Yan Sun

The University of Sydney

ysun9899@uni.sydney.edu.au

Li Shen

JD Explore Academy

mathshenli@gmail.com

&Dacheng Tao

The University of Sydney

dacheng.tao@gmail.com

Li Shen is the corresponding author.

###### Abstract

Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the "client-drift" problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the "client drift" and explore its substance in FL, in this paper, we first design an efficient FL algorithm _FedInit_, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, _FedInit_ initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consistency level. Moreover, to further understand how inconsistency disrupts performance in FL, we introduce the excess risk analysis and study the divergence term to investigate the test error of the proposed _FedInit_ method. Our studies show that optimization error is not sensitive to this local inconsistency, while it mainly affects the generalization error bound in _FedInit_. Extensive experiments are conducted to validate this conclusion. Our proposed _FedInit_ could achieve state-of-the-art (SOTA) results compared to several advanced benchmarks without any additional costs. Meanwhile, stage-wise relaxed initialization could also be incorporated into the current advanced algorithms to achieve higher performance in the FL paradigm.

## 1 Introduction

Since McMahan et al.  developed federated learning, it becomes a promising paradigm to effectively make full use of the computational ability of massive edge devices. Kairouz et al.  further classify the modes based on the specific tasks and different environmental setups. Different from centralized training, FL utilizes a central server to coordinate the clients to perform several local training stages and aggregate local models as one global model. However, due to the heterogeneous dataset, it still suffers from significant performance degradation in practical scenarios.

Several previous studies explore the essence of performance limitations in FL and summarize it as the "client-drift" problem . From the perspective of the global target, Karimireddy et al.  claim that the aggregated local optimum is far away from the global optimum due to the heterogeneity of the local dataset, which introduces the "client-drift" in FL. However, under limited local training steps, local clients can not genuinely approach the local optimum. To describe this negative impact more accurately, Acar et al.  and Wang et al.  point out that each locally optimized objective should be regularized to be aligned with the global objective. Moreover, beyond the guarantees of local consistent objective, Xu et al.  indicate that the performancedegradation could be further eliminated in FL if it guarantees the local consistent updates at each communication round, which is more similar to the centralized scenarios. These arguments intuitively provide forward-looking guidance for improving the performance in FL. However, in the existing analysis, there is still no solid theoretical support to understand the impact of the consistency term, which also severely hinders the further development of the FL paradigm.

To alleviate the negative impact of the "client-drift" problem and strengthen consistency in the FL paradigm, in this paper, we take into account adopting the personalized relaxed initialization at the beginning of each communication round, dubbed _FedInit_ method. Specifically, _FedInit_ initializes the selected local state by moving away from the current global state towards the reverse direction of the current latest local state. Personalized relaxed initialization helps each local model to revise its divergence and gather together with each other during the local training process. This flexible approach is surprisingly effective in FL and only adopts a constant coefficient to control the divergence level of the initialization. It could also be easily incorporated as a plug-in into other advanced benchmarks to further improve their performance.

Moreover, to explicitly understand how local inconsistency disrupts performance, we introduce the excess risk analysis to investigate the test error of _FedInit_ under the smooth non-convex objective, which includes an optimization error bound and a generalization error bound. Our theoretical studies indicate that the optimization error is insensitive to local inconsistency, while it mainly affects the generalization performance. Under _PL_-condition, consistency performs as the dominant term in the excess risk. Extensive empirical studies are conducted to validate the efficiency of the _FedInit_ method. On the CIFAR-\(10/100\) dataset, it could achieve SOTA results compared to several advanced benchmarks without additional costs. It also helps to enhance the consistency level in FL.

In summary, the main contributions of this work are stated as follows:

* We propose an efficient and novel FL method, dubbed _FedInit_, which adopts the personalized relaxed initialization state on the selected local clients at each communication round. Relaxed initialization is dedicated to enhancing local consistency during training, and it is also a practical plug-in that could easily to incorporated into other methods.
* One important contribution is that we introduce the excess risk analysis in the proposed _FedInit_ method to understand the intrinsic impact of local consistency. Our theoretical studies prove that the optimization error is insensitive to consistency, while it mainly affects the test error and generalization error bound.
* Extensive numerical studies are conducted on the real-world dataset to validate the efficiency of the _FedInit_ method, which outperforms several SOTA benchmarks without additional training costs. Meanwhile, as an efficient plug-in, relaxed initialization (_FedInit_) could also help the other benchmarks in our paper to achieve higher performance with effortlessness.

## 2 Related Work

**Consistency in FL**. FL employs an enormous number of edge devices to jointly train a single model among the isolated heterogeneous dataset [17; 26]. As a standard benchmark, _FedAvg_[2; 26; 48] allows the local stochastic gradient descent (local SGD) [10; 23; 45] based updates and uniformly selected partial clients' participation to alleviate the communication bottleneck. The stage-wise local training processes lead to significant divergence for each client [5; 25; 43; 44]. To improve the efficiency of the FL paradigm, a series of methods are proposed. Karimireddy et al.  indicate that inconsistent local optimums cause the severe "client drift" problem and propose the _SCAFFOLD_ method which adopts the variance reduction [6; 16] technique to mitigate it. Li et al.  penalize the prox-term on the local objective to force the local update towards both the local optimum and the last global state. Zhang et al.  utilize the primal-dual method to improve consistency via solving local objectives under the equality constraint. Specifically, a series of works further adopt the alternating direction method of multipliers (ADMM) to optimize the global objective [1; 9; 41; 52], which could also enhance the consistency term. Beyond these, a series of momentum-based methods are proposed to strengthen local consistency. Wang et al.  study a global momentum update method to stabilize the global model. Further, Gao et al.  use a local drift correction via a momentum-based term to revise the local gradient, efficiently reducing inconsistency. Ozfatura et al. , Xu et al. , Sun et al.  propose a similar client-level momentum to force the local update towards the last global direction. A variant of client-level momentum that adopts the inertial momentum to further improve the local consistency level [24; 40]. At present, improving the consistency in FL remains a very important and promising research direction. Though these studies involve the heuristic discussion on consistency, in this paper we focus on the personalized relaxed initialization.

**Generalization in FL.** A lot of works have studied the properties of generalization in FL. Based on the margin loss [3; 7; 27], Reisizadeh et al.  develop a robust FL paradigm to alleviate the distribution shifts across the heterogeneous clients. Shi et al.  study the efficient and stable model technique of model ensembling. Yagli et al.  prove the information-theoretic bounds on the generalization error and privacy leakage in the general FL paradigm. Qu et al.  propose to adopt the sharpness aware minimization (SAM) optimizer on the local client to improve the flatness of the loss landscape. Caldarola et al. , Sun et al. [37; 38], Shi et al. [33; 34] propose several variants based on SAM that could achieve higher performance. However, these works only focus on the generalization efficiency in FL, while in this paper we prove that its generalization error bound is dominated by consistency.

## 3 Methodology

### Preliminaries

Under the cross-device FL setups, there are a very large number of local clients to collaboratively train a global model. Due to privacy protection and unreliable network bandwidth, only a fraction of devices are open-accessed at any one time [17; 29]. Therefore, we define each client stores a private dataset \(_{i}=\{z_{j}\}\) where \(z_{j}\) is drawn from an unknown unique distribution \(_{i}\). The whole local clients constitute a set \(=\{i\}\) where \(i\) is the index of each local client and \(||=C\). Actually, in the training process, we expect to approach the optimum of the population risk \(F\):

\[w^{}_{}*{arg\,min}_{w}\{F(w) _{i}F_{i}(w)\},\] (1)

where \(F_{i}(w)=_{z_{j}_{i}}F_{i}(w,z_{j})\) is the local population risk. While in practice, we usually consider the empirical risk minimization of the non-convex finite-sum problem in FL as:

\[w^{}*{arg\,min}_{w}\{f(w)_ {i}f_{i}(w)\},\] (2)

where \(f_{i}(w)=}_{z_{j}_{i}}f_{i}(w;z_{j})\) is the local empirical risk. In Section 4.1, we will analyze the difference between these two results. Furthermore, we introduce the excess risk analysis to upper bound the test error and further understand how consistency works in the FL paradigm.

### Personalized Relaxed Initialization

In this part, we introduce the relaxed initialization in _FedInit_ method. _FedAvg_ proposes the local-SGD-based implementation in the FL paradigm with a partial participation selection. It allows uniformly selecting a subset of clients \(\) to participate in the current training. In each round, it initializes the local model as the last global model. Therefore, after each round, the local models are always far away from each other. The local offset \(w^{t-1}_{i,K}-w^{t}\) is the main culprit leading to inconsistency. Moreover, for different clients, their impacts vary with local heterogeneity. To alleviate this divergence, we propose the _FedInit_ method which adopts the personalized relaxed initialization at the beginning of each round. Concretely, on the selected active clients, it begins the local training from a new personalized state, which moves away from the last global model towards the reverse direction from the latest local state (Line.6 in Algorithm 1). A coefficient \(\) is adopted to control the level of personality. This offset \((w^{t}-w_{i,K}^{t-1})\) in the relaxed initialization (RI) provides a correction that could help local models gather together after the local training process. Furthermore, this relaxed initialization is irrelevant to the local optimizer, which means, it could be easily incorporated into other methods. Additionally, _FedInit_ does not require extra auxiliary information to communicate. It is a practical technique in FL.

## 4 Theoretical Analysis

In this section, we first introduce the excess risk in FL which could provide a comprehensive analysis on the joint performance of both optimization and generalization. In the second part, we introduce the main assumptions adopted in our proofs and discuss them in different situations. Then we state the main theorems on the analysis of the excess risk of our proposed _FedInit_ method.

### Excess Risk Error in FL

Since Karimireddy et al.  pointed out that client-drift problem may seriously damage the performance in the FL paradigm, many previous works [15; 18; 19; 30; 36; 44; 46; 48] have learned its inefficiency in the FL paradigm. However, most of the analyses focus on the studies from the onefold perspective of optimization convergence but ignore investigating its impact on generality. To further provide a comprehensive understanding of how client-drift affects the performance in FL, we adopt the well-known excess risk in the analysis of our proposed _FedInit_ method.

We denote \(w^{T}\) as the model generated by _FedInit_ method after \(T\) communication rounds. Compared with \(f(w^{T})\), we mainly focus on the efficiency of \(F(w^{T})\) which corresponds to its generalization performance. Therefore, we analyze the \([F(w^{T})]\) from the excess risk \(_{E}\) as:

\[_{E}=[F(w^{T})]-[f(w^{*})]=[F(w^{T})-f(w^{T})]}_{_{G:}}+[f(w^{T})-f(w^{*})]}_{_ {O:}}.\] (3)

Generally, the \([f(w^{*})]\) is expected to be very small and even to zero if the model could well-fit the dataset. Thus \(_{E}\) could be considered as the joint efficiency of the generated model \(w^{T}\). Thereinto, \(_{G}\) means the different performance of \(w^{T}\) between the training dataset and the test dataset, and \(_{O}\) means the similarity between \(w^{T}\) and optimization optimum \(w^{}\) on the training dataset.

### Assumptions

In this part, we introduce some assumptions adopted in our analysis. We will discuss their properties and distinguish the proofs they are used in.

**Assumption 1**: _For \( w_{1},w_{2}^{d}\), the non-convex local function \(f_{i}\) satisfies \(L\)-smooth if:_

\[\| f_{i}(w_{1})- f_{i}(w_{2})\| L\|w_{1}-w_{2}\|.\] (4)

**Assumption 2**: _For \( w^{d}\), the stochastic gradient is bounded by its expectation and variance as:_

\[[g_{i,k}^{t}]= f_{i}(w_{i,k}^{t}), |g_{i,k}^{t}- f_{i}(w_{i,k}^{t})\|^{2}_{i}^{2}.\] (5)

**Assumption 3**: _For \( w^{d}\), the heterogeneous similarity is bounded on the gradient norm as:_

\[\| f_{i}(w)\|^{2} G^{2}+B^{2}\| f(w)\|^{2}.\] (6)

**Assumption 4**: _For \( w_{1},w_{2}^{d}\), the global function \(f\) satisfies \(L_{G}\)-Lipschitz if:_

\[\|f(w_{1})-f(w_{2})\| L_{G}\|w_{1}-w_{2}\|.\] (7)

**Assumption 5**: _For \( w^{d}\), let \(w^{}_{w}f(w)\), the function \(f\) satisfies PL-condition if:_

\[2(f(w)-f(w^{}))\| f(w)\|^{2}.\] (8)

Discussions.Assumptions 1\(\)3 are three general assumptions to analyze the non-convex objective in FL, which is widely used in the previous works [15; 18; 19; 30; 36; 44; 46; 48]. Assumption 4 is used to bound the uniform stability for the non-convex objective, which is used in [11; 51]. Different from the analysis in the margin-based generalization bound [27; 29; 31; 38] that focus on understanding how the designed objective affects the final generalization performance, our work focuses on understanding how the generalization performance changes in the training process. We consider the entire training process and adopt uniform stability to measure the global generality in FL. For the general non-convex objective, one often uses the gradient norm \(\| f(w)\|^{2}\) instead of bounding the loss difference \([f(w^{T})-f(w^{})]\) to measure the optimization convergence. To construct and analyze the excess risk, and further understand how the consistency affects the FL paradigm, we follow  to use Assumption 5 to bound the loss distance. Through this, we can establish a theoretical framework to jointly analyze the trade-off on the optimization and generalization in the FL paradigm.

### Main Theorems

#### 4.3.1 Optimization Error \(_{o}\)

**Theorem 1**: _Under Assumptions 1\(\)3, let participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfy \(\{,\}\) where \(K 2\), let the relaxation coefficient \(}{12}\), and after training \(T\) rounds, the global model \(w^{t}\) generated by FedInit satisfies:_

\[_{t=0}^{T-1}\|f(w^{t})\|^{2})-f(w^{}))}{ KT}+ L}{  N}_{l}^{2}+ KL}{ N}G^{2},\] (9)

_where \((0,1)\), \(_{1}=}{1-72^{2}}+17\), and \(_{2}=}{1-72^{2}}+13\) are three constants. Further, by selecting the proper learning rate \(=(})\) and let \(D=f(w^{0})-f(w^{})\) as the initialization bias, the global model \(w^{t}\) satisfies:_

\[_{t=0}^{T-1}\|f(w^{t})\|^{2}( ^{2}+KG^{2})}{}).\] (10)

Theorem 1 provides the convergence rate of the _FedInit_ method without the _PL-condition_, which could achieve the \((1/)\) with the linear speedup of \(N\). The dominant term of the training convergence rate is the heterogeneous bias \(G\), which is \(K\) larger than the initialization bias \(D\) and stochastic bias \(_{l}\). According to the formulation (10), by ignoring the initialization bias, the best local interval \(K=(_{l}^{2}/G^{2})\). This selection also implies that when \(G\) increases, which means the local heterogeneity increases, the local interval \(K\) is required to decrease appropriately to maintain the same efficiency. More importantly, though _FedInit_ adopts a weighted bias on the initialization state at the beginning of each communication round, the divergence term \(\|w_{i,K}^{t-1}-w^{t}\|^{2}\) does not affect the convergence bound whether \(\) is \(0\) or not. This indicates that the FL paradigm allows a divergence of local clients from the optimization perspective. Proof details are stated in Appendix A.2.3.

**Theorem 2**: _Under Assumptions 1\(\)3 and 5, let participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfy \(\{,,\}\) where \(K 2\), let the relaxation coefficient \(}{12}\), and after training \(T\) rounds, the global model \(w^{t}\) generated by FedInit satisfies:_

\[[f(w^{T})-f(w^{})] e^{- KT}[f(w^{0 })-f(w^{})]+ KL}{2N}G^{2}+ L}{2N}_{l}^{2},\] (11)

_where \(,_{1},_{2}\) is defined in Theorem 1. Further, by selecting the proper learning rate \(=()\) and let \(D=f(w^{0})-f(w^{})\) as the initialization bias, the global model \(w^{t}\) satisfies:_

\[[f(w^{T})-f(w^{})]=}( ^{2}+KG^{2})}{NKT}).\] (12)

To bound the \(_{O}\) term, we adopt Assumption 5 and prove that _FedInit_ method could achieve the \((1/NKT)\) rate where we omit the \(((NKT))\) term. It maintains the properties stated in the Theorem 1. Detailed proofs of the convergence bound are stated in Appendix A.2.4.

#### 4.3.2 Generalization Error \(_{g}\)

**Uniform Stability.** One powerful analysis of the generalization error is the uniform stability . It says, for a general proposed method, its generalization error is always lower than the bound of uniform stability. We assume that there is a new set \(}\) where \(\) and \(}\) differ in at most one data sample on the \(i^{*}\)-th client. Then we denote the \(w^{T}\) and \(^{T}\) as the generated model after training \(T\) rounds on these two sets, respectively. Thus, we have the following lemma:

**Lemma 1**: _(Uniform Stability. ) For the two models \(w^{T}\) and \(^{T}\) generated as introduced above, a general method satisfies \(\)-uniformly stability if:_

\[_{z_{j}(_{i})}[f(w^{T};z_{j})-f(^{T };z_{j})].\] (13)

_Moreover, if a general method satisfies \(\)-uniformly stability, then its generalization error satisfies \(_{G}_{z_{j}\{_{i}\}}[f(w^{T};z_{j} )-f(^{T};z_{j})]\)._

**Theorem 3**: _Under Assumptions 1, 2, 4, and 5, let all conditions above be satisfied, let learning rate \(=()=\) where \(c=}{R}\) is a constant, and let \(|_{i}|=S\) as the number of the data samples, by randomly selecting the sample \(z\), we can bound the uniform stability of our proposed FedInit as:_

\[\|f(w^{T+1};z)-f(^{T+1};z)\|\] (14) \[^{2}+SL_{G}_{l})(UTK) ^{cL}}{L}^{}+(1+)^{}^{2}+SL_{G}_{l})}^{}_{t=1}^{T} {}}{T},\]

_where \(U\) is a constant and \(^{t}=_{i}\|w_{i,K}^{t-1}-w^{t}\|^ {2}\) is the divergence term at round \(t\)._

For the generalization error, Theorem 3 indicates that \(_{G}\) term contains two main parts. The first part comes from the stochastic gradients as the vanilla centralized training process , which is of the order \(((TK)^{}/S)\). The constant \(c\) is of the order \((1/K)\) as \(c=}{K}\), thus we have \(=}{K+_{0}L}\). If we assume the \(_{0}L\) is generally small, we always expect to adopt a larger \(K\) in the FL paradigm to reduce generalization error. For instance, if we select \(K\), then \(((TK)^{}/S)(T^{}/S)\) which is a very strong upper bound of the generalization error. However, the selection of local interval \(K\) must be restricted from the optimization conditions and we will discuss the details in Section 4.3.4. In addition, the second part in Theorem 3 comes from the divergence term, which is a unique factor in the FL paradigm. As we mentioned above, the divergence term measures the authentic client-drift in the training process. The divergence term is not affected by the number of samples \(S\) and it is only related to the proposed method and the local heterogeneity of the dataset. Proof details are stated in Appendix A.3.

#### 4.3.3 Divergence Term

In the former two parts, we provide the complete theorem to measure optimization error \(_{O}\) and generalization error \(_{G}\). And we notice that, in the FL paradigm, the divergence term mainly affects the generalization ability of the model instead of the optimization convergence. In this part, we focus on the analysis of the divergence term of our proposed _FedInit_ method. Due to the relaxed initialization at the beginning of each communication round, according to the Algorithm 1, we have \(w_{i,K}^{t}=w^{t}+(w^{t}-w_{i,K}^{t-1})-_{k=0}^{K-1}g_{i,k}^{t}\). Thus, we have the following recursive relationship:

\[-w_{i,K}^{t}}_{}=\ ^{t-1}-w^{t})}_{}+-w^{t})}_{}+_{k=0}^{K-1}_{i,k}^{t}.\] (15)

According to the formulation (15), we can bound the divergence \(^{t}\) via the following two theorems.

**Theorem 4**: _Under Assumptions 1\(\)3, we can bound the divergence term as follows. Let the learning rate satisfy \(\{,,}{}\}\) where \(K 2\), and after training \(T\) rounds, let \(0<<}{24}\), the divergence term \(\{^{t}\}\) generated by FedInit satisfies:_

\[_{t=0}^{T-1}^{t}=(^{2}+ KG^{2})}{T}+B^{2}[D+L(_{l}^{2}+KG^{2})]}{T^{ }}).\] (16)

Theorem 4 points out the convergence order of the divergence \(^{t}\) generated by _FedInit_ during the training process. This bound matches the conclusion in Theorem 1 with the same learning rate. The dominant term achieves the \((NK/T)\) rate on the heterogeneity bias \(G\). It could be seen that the number of selected clients \(N\) will inhibit its convergence and the local consistency linearly increases with \(N\). Different from the selection in Theorem 1, local interval \(K\) is expected as small enough to maintain the high consistency. Also, the initialization bias \(D\) is no longer dominant in consistency. We omit the constant weight \(}\) in this upper bound. Proof details are stated in Appendix A.2.5.

**Theorem 5**: _Under Assumptions 1-3 and 5, we can bound the divergence term as follows. Let the learning rate satisfy \(\{,,\}\) where \(K 2\), and after training \(T\) rounds, let \(0<<}{24}\), the divergence term \(^{T}\) generated by FedInit satisfies:_

\[^{T}=}(}{T^{2}}+^{2}+KG^{2}}{NKT^{2}}+}).\] (17)

Theorem 5 indicates the convergence of the divergence \(\) under the _PL-condition_ which matches the conclusion in Theorem 2 with the same learning rate selection. Assumption 5 establishes a relationship between the gradient norm and the loss difference on the non-convex function \(f\). Different from the Theorem 4, the initialization bias \(D\) and the heterogeneous bias \(G\) are the dominant terms. Under Assumption 5, the _FedInit_ supports a larger local interval \(K\) in the training process. This conclusion also matches the selection of \(K\) in Theorem 2. When the model converges, _FedInit_ guarantees the local models towards the global optimum under at least \((1/T^{2})\) rate. Similarly, we omit the constant weight \(}\) and we will discuss the \(\) in Section 4.3.4. Proof details are stated in Appendix A.2.6.

#### 4.3.4 Excess Risk

In this part, we analyze the excess risk \(_{E}\) of _FedInit_ method. According to the theorems above,

**Theorem 6**: _Under Assumption 1-5, let the participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfies \(\{,,,\}\) where \(K 2\), let the relaxed coefficient \(0<}{24}\), and let \(|_{i}|=S\). By selecting the learning rate \(=()\), after training \(T\) communication rounds, the excess risk of the FedInit method achieves:_

\[_{E}}(^{2}+KG^{2})}{NKT})}_{}+ ([_{l}(TK)^{eL}]^{})}_{ }+}(}K^{}}{T^{}})}_{}.\] (18)

According to the Theorems 2, 3, and 5, we combine their dominant terms to upper bound the excess risk of _FedInit_ method. The first term comes from the optimization error, the second term comes from the stability bias, and the third term comes from the divergence bias. From the perspective of excess risk, the main restriction in the FL paradigm is the divergence term with the bound of \(}(}})\). The second term of excess risk matches the conclusion in SGD  which relies on the number \(S\). Our analysis of the excess risk reveals two important corollaries in FL:

* From the perspective of optimization, the FL paradigm is insensitive to local consistency in the training process (Theorems 1&2).
* From the perspective of generalization, the local consistency level significantly affects the performance in the FL paradigm (Theorem 6).

Then we discuss the best selection of the local interval \(K\) and relaxed coefficient \(\).

**Selection of K.** In the first term, to minimize the optimization error, the local interval \(K\) is required to be large enough. In the second term, since \( 1\), the upper bound expects a small local interval \(K\). In the third term, since \(=L}<1\), it expects a large \(K\) to guarantee the order of \(T\) to approach \((1/T)\), where the divergence bias could maintain a high-level consistency. Therefore, there is a specific optimal constant selection for \(K>1\) to minimize the excess risk.

**Selection of \(\).** As the dominant term, the coefficient of the divergence bias also plays a key role in the error bound. In Theorem 5, the constant weight we omit for the divergence term \(^{T}\) is \(}\). Thus the coefficient of \(}\) is \(}}\). Combined with Theorem 3, we have the coefficient for the 

[MISSING_PAGE_FAIL:8]

### Experiment results

In this part, we mainly introduce the experiment results compared with the other benchmarks.

In Table 1, our proposed _FedInit_ method performs well than the other benchmarks with good stability across different experimental setups. On the results of ResNet-18-GN on CIFAR-10, it achieves about 3.42\(\%\) improvement than the vanilla _FedAvg_ on the high heterogeneous splitting with \(D_{r}=0.1\). When the participation ratio decreases to \(5\%\), the accuracy drops only about \(0.1\%\) while _FedAvg_ drops almost \(1.88\%\). Similar results on CIFAR-100, when the ratio decreases, _FedInit_ still achieves \(43.77\%\) while the second best method _SCAFFOLD_ drops about \(3.21\%\). This indicates the proposed _FedInit_ holds good stability on the varies of the participation. In addition, in Table 2, we incorporate the relaxed initialization (RI) into the other benchmarks to test its benefit. "-" means the vanilla benchmarks, and "+RI" means adopting the relaxed initialization. It shows that the relaxed initialization holds the promising potential to further enhance the performance. Actually, _FedInit_ could be considered as (RI + _FedAvg_), whose improvement achieves about over \(3\%\) on each setup. Table 1 shows the poor performance of the vanilla _FedAvg_. Nevertheless, when adopting the RI, _FedInit_ remains above most benchmarks on several setups. When the RI is incorporated into other benchmarks, it helps them to achieve higher performance without additional communication costs.

### Ablation

In this part, we mainly introduce the ablation results of different hyperparameters.

**Hyperparameters Sensitivity.** The excess risk and test error of _FedInit_ indicate there exists best selections for local interval \(K\) and relaxed coefficient \(\), respectively. In this part, we test a series of selections to validate our conclusions. To be aligned with previous studies, we denote \(K\) as training epochs. In Figure 1 (a), we can see that the selection range of the beta is very small while it has great potential to improve performance. When it is larger than the threshold, the training process will diverge quickly. As local interval \(K\) increases, test accuracy rises first and then decreases. Our analysis provides a clear explanation of the phenomenon. The optimization error decreases as \(K\) increases when it is small. When \(K\) exceeds the threshold, the divergence term in generalization cannot be ignored. Therefore, the test accuracy will be significantly affected.

**Consistency.** In this part, we test the relationship between the test accuracy and divergence term \(^{T}\) under different \(\) selections. As introduced in Algorithm 1 Line.6, negative \(\) means to adopt the relaxed initialization which is close to the latest local model. _FedInit_ degrades to _FedAvg_ when \(=0\). Table 3 validates that RI is required to be far away from the local model (a positive \(\)). When \(\) is small, the correction is limited. The local divergence term is difficult to be diminished efficiently. While it becomes too large, the local training begins from a bad initialization, which can not receive enough guidance of global information from the global models. Furthermore, as shown in Table 3, if the initialization is too far from the local model, the quality of the initialization state will not be effectively guaranteed.

   &  &  \\   &  &  &  &  \\   & - & +RI & - & +RI & - & +RI & - & +RI \\  FedAvg & 78.77 & 83.11 & 72.53 & 75.95 & 74.81 & 80.58 & 70.65 & 74.92 \\ FedAdam & 76.52 & 78.33 & 70.44 & 72.55 & 73.28 & 78.33 & 68.87 & 71.34 \\ FedSAM & 79.23 & **83.36** & **72.89** & 76.34 & 75.45 & 80.66 & 71.23 & 75.08 \\ SCAFFOLD & 81.37 & 83.27 & 75.06 & **77.30** & 78.17 & **81.02** & 74.24 & **76.22** \\ FedDyn & 82.43 & 81.91 & 75.08 & 75.11 & 79.96 & 79.8 & 74.15 & 74.34 \\ FedCM & 81.67 & 81.77 & 73.93 & 73.71 & 79.49 & 79.72 & 73.12 & 72.98 \\  

Table 2: We incorporate the relaxed initialization (RI) into the benchmarks to test improvements on ResNet-18-GN on CIFAR-10 with the same hyperparameters and specific relaxed coefficient \(\).

Figure 1: THyperparameters sensitivity studies of local intervals \(K\) and relaxed coefficient \(\) of the _FedInit_ method on CIFAR-10. To fairly compare their efficiency, we fix the total communication rounds \(T=500\).

### Discussions of Relaxed Initialization

In this part, we mainly discuss the improvements of the proposed relaxed initialization.

In vanilla classical _FedAvg_ and the most advanced methods, at the beginning of each communication round, we are always caught in a misunderstanding of the high consistency. Because the target of FL is a globally consistent solution, it is always an involuntary aggregation in the algorithm to ensure consistency. We prove that this does contribute to the efficiency of the optimization process, but it is not the best selection for generalization. To better improve the generalization, we prove that a relaxed initialization state will contribute more. We compare their difference in Figure 2.

As shown in the above figure, we can clearly see why _FedInit_ contributes more to the consistency. When we move a little in the opposite direction of the last local optimization state, we will move further away from local optimal solutions in the current communication round. The working mode of RI is similar to the idea of "lookahead". Differently, (1) "lookahead" only works at the end of each stage; (2) "lookahead" only works for the global models on the global server. However, RI helps each local client to backtrack a small distance at the beginning of each stage. Therefore, after the local training in the next stage, the trained local models will get closer to each other than before.

## 6 Conclusion

In this work, we propose an efficient and novel FL method, dubbed _FedInit_, which adopts the stage-wise personalized relaxed initialization to enhance the local consistency level. Furthermore, to clearly understand the essential impact of consistency in FL, we introduce the excess risk analysis in FL and study the divergence term. Our proofs indicate that consistency dominates the test error and generalization error bound while optimization error is insensitive to it. Extensive experiments are conducted to validate the efficiency of relaxed initialization. As a practical and light plug-in, it could also be easily incorporated into other FL paradigms to improve their performance.

**Limitations & Broader Impact.** In this work, we analyze the excess risk for the _FedInit_ method to understand how consistency works in FL. Actually, the relaxed initialization may also work for the personalized FL (pFL) paradigm. It is a future study to explore its properties in the pFL and decentralized FL, which may inspire us to design novel efficient algorithms in the FL community.