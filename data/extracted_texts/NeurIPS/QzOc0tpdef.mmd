# NLIR: Natural Language Intermediate Representation

for Mechanized Theorem Proving

Laetitia Teodorescu

Adaptive ML

Guillaume Baudart

Universite Paris Cite

CNRS, Inria, IRIF

Emilio Jesus Gallego Arias

Universite Paris Cite

CNRS, Inria, IRIF

&Marc Lelarge

DI ENS, PSL University

Inria

###### Abstract

Formal theorem proving is challenging for humans as well as for machines. Thanks to recent advances in LLM capabilities, we believe natural language can serve as a universal interface for reasoning about formal proofs. In this paper, 1) we introduce _Petanque_, a new lightweight environment to interact with the Coq theorem prover; 2) we present two interactive proof protocols leveraging natural language as an intermediate representation for designing proof steps; 3) we implement beam search over these interaction protocols, using natural language to rerank proof candidates; and 4) we use Petanque to benchmark our search algorithms. Using our method with GPT-4o we can successfully synthesize proofs for 58% of the first 100/260 lemmas from the newly published Busy Beaver proofs.

## 1 Introduction

The general knowledge and reasoning abilities of frontier large language models (LLMs) makes them practical as a backbone for building agents able to interact with interactive theorem provers (ITP). These agents should iteratively build proofs with help from proof engine feedback. While previous work (e.g. Yang et al. ) used a costly data collection procedure to finetune modestly sized language models, we believe that reasoning in natural language before outputting tactics will lead to better and more interpretable results. Recently, Thakur et al.  showed promising preliminary results by using GPT-4 as an agent proposing tactics inside a backtracking search and using rich feedback from the proof environment.

In this work, we develop infrastructure to allow communication between a GPT-4o-based agent and the Coq proof environment [The Coq Development Team, 2024]. Our key idea is to rely on natural language as much as possible when generating proofs. Using natural language leverages the strength of LLMs, and allows us to use chain-of-thought [Wei et al., 2022] by asking for an informal mathematical proof before generating the formal proof, making it more intuitive and comprehensible compared to purely automatic formal techniques. Additionally, partial proofs expressed in natural language are easier for humans to understand, adapt, or reuse, allowing for greater flexibility and collaboration between machine-generated suggestions and human mathematicians.

We present the following contributions: 1) _Petanque_: A new fast and lightweight environment to interact with the Coq theorem prover. 2) Two interactive proof protocols both leveraging natural language reasoning: tactic-by-tactic proof construction, and hierarchical proof templating. 3) We couple both protocols with standard search algorithms leveraging feedback from the ITP and using natural language to rerank proof candidates. 4) We evaluate this agent on a new dataset of textbookexercises and intermediate theorems from the recent Busy Beaver proof formalized in Coq of BB\((4)=107\), . NLIR is open source (https://github.com/llm4coq/nlir).

## 2 Petanque: a lightweight interactive environment for Coq

A common difficulty when interacting with interactive proof assistants in the context of machine learning is inadequate tooling (see for example ). Following existing work , we have built a new environment for machine to machine interaction for the Coq proof assistant, particularly tailored for interactive, high-throughput, low-latency learning applications. Petanque is based on Fleche , a new document manager for Coq. We extend Fleche by enabling Petanque to access the Coq proof engine directly without requiring edits in the associated document. This makes our environment fast and lightweight. A Python interface provides easy access to the API. See Appendix B for more information on Fleche and Petanque.

## 3 Proof interaction protocols

In this section, we present two approaches leveraging LLMs' ability to reason in natural language in order to find a formal proof with the help of a proof assistant. _Tactic-by-tactic proof construction_ mimics the typical behavior of a standard Coq user: given the current goals, the agent generates one or several tactics that updates the goals and repeats this process until the proof is complete. By contrast, _hierarchical proof templating_ tries to generate full proofs directly. Failed tactics are then replaced with _holes_ to obtain a proof _template_. The agent repeats the process of filling each hole until the proof is complete. Our approach's originality is that although both protocols' inputs (goals) and outputs (tactics) are Coq code, the agent internally uses natural language as an intermediate representation to analyze the input and guide the code generation.

### Tactic-by-tactic proof construction

An overview of the tactic-by-tactic proof construction agent is presented in Figure 1. Given a Coq theorem, the agent first uses natural language to describe the goal and explain how to continue the proof (chain-of-thought). The last step synthesizes the corresponding Coq tactics. For instance, in Figure 1, the goal is to prove that addition over natural numbers is commutative. The agent decides to try a proof by induction and correctly synthesizes a sequence of two tactics: intros n m. introduces two variables n and m of type nat (natural numbers), and induction n. starts an induction over n.

The tactics are sent to the Petanque environment, which parses and executes each tactic to update the current goal. A textual representation of the new goal is then fed back to the agent to make further progress in the proof. If the execution returns an error, the current goal does not change, but we augment the prompt with the failed tactics and ask the LLM to try something else for the next attempt. For instance, in Figure 1, both tactics succeed and generate two new subgoals: the base case (for n=0, prove m + 0 = 0 + m) and the induction case (given

Figure 1: Tactic-by-tactic proof construction.

Ith: n + m = m + n, prove (n + 1) + m = m + (n + 1) ). The textual representation of a goal uses the the symbol \(\) to separate hypotheses from the conclusion, and S \(n\) denotes n + 1.

**Model Interface.** In early experiments, we observed that conversation-style reasoning often diverges: after a few rounds, the output makes very little sense, and the agent never recovers. Following (Yang et al., 2024) - and similarly to (Thakur et al., 2024) - we use a synthetic interface to summarize at each goal the global objective (initial theorem), the current goal (in the middle of a proof), and failed attempts to solve the same goal.

### Hierarchical proof templating

An example execution of the hierarchical proof templating agent is presented in Figure 2. The agent pipeline is similar to the tactic-by-tactic method, but instead of focusing only on the next step, the agent generates a complete proof in natural language, before translating the proof in Coq syntax. For instance, in Figure 2, the agent uses the inversion tactics on the hypothesis H which generate two subgoals with a simpler hypothesis H0, and then tries to solve each subgoals using this H0 hypothesis.

The Petanque environment then repairs the proof, replacing failed tactics by _holes_ which admit and close the current subgoal, removing subsequent tactics until the focus moves to the next subgoal. Petanque then checks that the resulting _template_ is correct, i.e., assuming a valid proof for each holes, the proof is complete. A textual representation of each holes is then fed back to the agent which repeat the process to fill the holes one by one. For instance, in Figure 2, apply H0 fails on both subgoals. The agent then repeats the process for each holes, using focused fine-grain reasoning to prove the corresponding subgoal. The proof is complete when there are no more holes.

## 4 Proof search

We combine our interactive protocol with the classic beam search algorithm. Inspired by (Yao et al., 2023), we use the LLM to rank and sort the proposals at each step of the search. A simplified version of the code is presented on the right. At each step, agent.generate generates n_actions possible steps (tactics or proofs). Each step is then validated with petanque.step and the state of all the resulting candidates is stored. Then agent.sort calls the LLM to discuss, compare and finally rank and sort the candidates for the next step.

``` defbeam_search(n_steps,n_actions,beam_size): s = petanque.start(Ith) = [s] # Initial state for step in range(n_steps): candidates = [] for s in beam: # Try multiple actions for each state for a in agent.generate(s,n_actions): sa = petanque.steps,a) if petanque.proof_finished(sa): returns ss.proof # Proof found else: candidates = candidates + [sa] # Rank and sort candidates  beam agent.sort(Candidates)[:beam_size] returnNone# No proof found

Figure 2: Hierarchical proof templating.

[MISSING_PAGE_FAIL:4]

training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities.

Reasoning in LLMsThis work is also related to recent investigations on the reasoning abilities of LLMs (Plaat et al., 2024). Chain-of-Thought (CoT) prompting (Wei et al., 2022) was shown to improve LLM's answers; subsequent work found that these reasoning abilities could be elicited zero-shot (Kojima et al., 2022). Further work interleaved CoT with decision-making (Yao et al., 2022), added search and complex control flow to reasoning (Chen et al., 2022; Yao et al., 2023; Besta et al., 2024), incorporated refinement and feedback (Madaan et al., 2024; Shinn et al., 2024), and learned to generate novel reasoning traces that proved beneficial for further training (Zelikman et al., 2022, 2024). Like our work, many of these methods - especially the ones using search and refinement - make use of LLM-based scoring or ranking functions (Zheng et al., 2023).

ConclusionIn this work, we have presented a new agent for building proofs leveraging chain of thought as an intermediate representation, and generating proofs by outputting step-by-step tactics or hierarchical proof templates. We couple this with beam search and natural language reranking and obtain good performance on a new evaluation set built with the help of our novel proof environment, _Petanque_. Future work could investigate how one could use reinforcement learning to obtain better reasoning and performance with smaller models (OpenAI, 2024).

AcknowledgementsWe thank Cyril Cohen and Pierre Boutillier for many insightful discussions. We also thank Alex Sanchez-Stern for his feedback on early versions of Petanque. This work is supported by the Inria Defi LLM4Code and the project ReaLiSe, Emergence Ville de Paris 2021-2025.