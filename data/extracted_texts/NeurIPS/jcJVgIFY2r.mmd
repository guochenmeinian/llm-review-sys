# Generator Born from Classifier

Runpeng Yu Xinchao Wang

National University of Singapore

r.yu@u.nus.edu xinchao@nus.edu.sg

 Corresponding author.

###### Abstract

In this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. From a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. As such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. Grounded on the theory of Maximum-Margin Bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. Empirical validation from various image generation tasks substantiates the efficacy of our strategy.

## 1 Introduction

The majority of machine learning research bifurcates into two distinct branches of study: the predictive task and the generative task. Given the input \(\) and the label \(y\), the former one focuses on the training of a high-performing classifier or regressor, which approximates \(p(y|)\), whereas the latter one aims to train a generative model capable of sampling from \(p(|y)\) or \(p(,y)\). The gap between the predictive and the generative models, as a result, predominantly arises from the lack of information in the predictive models about the marginal distribution \(p()\). In the realm of deep neural networks, however, the over-parameterization leads to the overfitting on the training distribution and the memorization of the training samples , which, in turn, make the network implicitly retain information about \(p()\). With this component in hand, it prompts the question of whether it is feasible to derive a generative model from a predictive one.

In this paper, we explore this novel task, which attempts to learn a generator directly from a pre-trained classifier, _without_ the assistance of any training data. Unarguably, this is a highly ambitious task with substantial difficulty, as either explicitly extracting information about \(p()\) from a pre-trained classifier or directly solving this inverse problem from classifier to generator poses significant challenges. Despite these challenges, the value of this task lies in its potential to offer a new approach to training generators that mitigates the direct dependence on large volumes of training data. This provides a possible solution for learning tasks in scenarios where data is scarce or unavailable. Moreover, this task presents a novel way to utilize and analyze the pre-trained predictive models, facilitating our understanding of the encoded information within the parameters.

To this end, we propose a novel learning scheme. Our approach is grounded in the theory of Maximum-Margin Bias of gradient descent, which demonstrates that the parameters of a neural network trained via gradient descent will converge to the solution of a specific optimization problem. This optimization problem minimizes the norm of the neural network parameters while maximizing the classification margin on the training dataset. The necessary condition for the solution of thisoptimization problem constructs a system of equations, describing the relationship between the pre-trained neural network parameters and the training data distribution.

Since our aim is to learn a generator from the pre-trained classifier parameters, the generator is therefore expected to approximate a distribution that satisfies the necessary condition of this optimization problem. To accomplish this, we design the loss function for training the generator and the corresponding training algorithm based on the necessary condition. The entire training process does not rely on any training data; all available information related to the pre-trained data is encapsulated within the parameters of the pre-trained classifier.

The intuition behind our design is twofold: on one hand, the generator should guarantee that the pre-trained classifier performs well under the data distribution it approximated, and on the other hand, the generator should ensure that the current classifier parameters are the convergence point of the gradient descent algorithm under the data distribution it approximated. It's noteworthy that the original data distribution naturally satisfies these conditions. Therefore, we anticipate that employing the proposed method will guide the generator to discover the original data distribution.

We conduct experiments on commonly used image datasets. Fig. 1 shows some generated images of the MNIST and CelebA datasets. Remarkably, even trained without access to the original data, the generator is able to perform conditional sampling and generate the digits and faces.

Our contribution is therefore a novel approach that, for the first time, attempts to train a generator from a pre-trained classifier without utilizing training data. The proposed approach produces encouraging results on synthetic and real-world images.

A list of symbols utilized in this paper and corresponding descriptions can be found in the Appendix.

## 2 Related Work

**Generative Adversarial Networks.** Generative Adversarial Network (GAN) consists of a generator and a discriminator collectively optimizing a minimax problem to learn and replicate the original data distribution (Goodfellow et al., 2014). Numerous extensions of the original GAN have been investigated, including functionality enhancement (Chen et al., 2016; Mirza and Osindero, 2014; Odena et al., 2017; Donahue et al., 2017); architecture optimization and scaling (Karras et al., 2018; Brock et al., 2019; Denton et al., 2015); and training loss design (Nowozin et al., 2016; Arjovsky et al., 2017; Wei et al., 2018). Owing to their superior generation quality, GANs have found wide-ranging applications in image synthesis (Yang et al., 2017), blending (Wu et al., 2017), inpainting (Yeh et al., 2017; Yang et al., 2017; Yu et al., 2018), super-resolution (Ledig et al., 2017), denoising (Linh et al., 2020); image-to-image translation (Zhu et al., 2017; Isola et al., 2017); 3D object generation (Wu et al., 2016); video generation (Vondrick et al., 2016; Tulyakov et al., 2018); etc.

Both our work and GAN require an additional classifier to guide the training of the generation and provide a measure of authenticity for generated data. However, in the GAN framework, the classifier is trained concurrently with the generator, with the explicit goal of discerning the quality of generated results. In contrast, our method utilizes a pre-trained classifier, which can be arbitrary, and its training objective is to maximize classification accuracy, not to judge the quality of generated results. This imbues our approach with considerable flexibility. Furthermore, during the training of GAN, the generator has access to training data. However, in our task, the training data is not available to the generator, which makes our task harder.

**Feature Visualization and Model Inversion.** Besides our task, neural network feature visualization and model inversion share the objective of extracting information relevant to the training data from

Figure 1: Images produced by the generator trained only using pre-trained classifier. The generated images, positioned in the first row, are accompanied by their nearest neighbors from the original dataset, displayed in the second row.

pre-trained classifiers. Neural network feature visualization is a technique aimed at identifying input data that maximally activates specific neurons or layers within the network, thereby providing insights into the patterns or features that the network is primed to recognize. (Engstrom et al., 2019; Olah et al., 2017; Nguyen et al., 2016) Model inversion in neural networks refers to the process of inferring or reconstructing input data given the trained model. (Yin et al., 2020) In the realm of adversarial attacks, model inversion is deployed to discover sensitive information about the training data from the model's outputs. (He et al., 2019; Zhao et al., 2021; Fredrikson et al., 2015)

Unlike these tasks, where an independent gradient optimization is required for each generation, our goal is to develop a generator capable of sampling from the training data distribution. While there is research in neural network feature visualization and model inversion that utilizes a generative model to assist in the restoration of specific training data, these works typically employ the generator more as a prior for the recovery process. (Nguyen et al., 2017; Jeon et al., 2021; Yang et al., 2019; Zhang et al., 2020) The training of such a generator necessitates additional training data, which should be similar to or encompass the original training data used for the classifier. In contrast, our approach directly derives a generator from the classifier without the utilization of extraneous training data.

**Energy-Based Model.** Within the framework of energy-based models, pre-trained classifiers have also been demonstrated to be capable of acting as a conditional probability distribution, generating data samples. (LeCun et al., 2006; Grathwohl et al., 2020; Guo et al., 2023) Although the energy-based models theoretically bridge predictive and generative models, practical implementation for sample generation based on it still relies on an optimization target and executes a multi-step gradient optimization process, akin to model inversion. In contrast, our objective is to develop a generator with the ability for random or condition-based sampling, procuring data samples directly through the forward pass of the neural network.

**Maximum-Margin Bias of Gradient Descent.** Our work is based on the study of the Maximum-Margin Bias of gradient descent. These investigations primarily seek to elucidate why gradient descent algorithms are capable of learning models with robust generalization capabilities, even in the absence of additional regularization constraints. It has been discovered that under the guidance of gradient descent, the parameters of a neural network converge to a solution to an optimization problem aiming to maximize the classification boundary while concurrently minimizing the norm of the network parameters. Initial studies focused on linear logistic regression models (Rosset et al., 2003; Soudry et al., 2018), which were subsequently extended to homogeneous neural networks (Wei et al., 2019; Xu et al., 2018; Nacson et al., 2019; Lyu and Li, 2020; Ji and Telgarsky, 2020; Le and Jegelka, 2022) and, more recently, to a broader class of quasi-homogeneous neural networks (Kunin et al., 2023). The theory of Maximum-Margin Bias has also been leveraged by Haim et al. (2022) to recover training data. However, their objective was the restoration of data rather than training a generator, and their work was exclusively confined to binary classification datasets and fully connected networks without bias terms.

## 3 Preliminary

To extract the information about the training dataset from the parameters of the pre-trained classification model, we leverage the theory of Maximum-Margin Bias.

Let \((;):^{d}\) denote the classifier parameterized by \(\) and trained on multi-class classification dataset \(D=\{(_{i},y_{i})\}_{i=1}^{N}\) with each \((_{i},y_{i})^{d}\). let \(L():=_{i=1}^{N}l((_{i};),y_{i})\) denote the standard cross-entropy loss of \(\) on \(D\). To extend the applicability of Maximum-Margin Bias analysis to various neural network structures, the definition of the \(\)_-quasi-homogeneous model_ is introduced. For a (non-zero) positive semi-definite diagonal matrix \(\), a model \((;)\) is \(\)-quasi-homogeneous if the model output scales as \((;_{}())=e^{}(;)\) for all \(\) and input \(\), when the parameter scales as \(_{}():=e^{}\). Many commonly used neural network architectures satisfy the definition of \(\)-quasi-homogeneous model, such as convolutional neural networks and fully connected networks with biases, residual connections, and normalization layers. The seminorm of \(\) corresponding to \(\) is defined as \(||||_{}^{2}:=^{T}\). Let \(_{max}:=_{j}_{jj}\) denote the maximal element in \(\), \(\) is the matrix setting all elements less than \(_{max}\) to \(0\), _i.e._, \(_{jj}=_{max}\) if \(_{jj}=_{max}\), otherwise, \(_{jj}=0\). Accordingly, the seminorm of \(\) corresponding to \(\) is defined as \(||||_{}^{2}:=^{T}\). The normalized parameters is defined as \(:=_{}()\), such that \(\|\|_{}^{2}=1\). The Quasi-Homogeneous Maximum-Margin Theorem states as follows.

**Theorem 1** (Paraphrased from ).: _Let \((;)\) denote a \(\)-quasi-homogeneous classifier trained on \(D\) with cross-entropy loss \(L\). Assume that: (1) for any fixed \(\), \((;)\) is locally Lipschitz and admits a chain rule ; (2) the learning dynamic is described by a gradient flow ; (3) \(_{t}(t)\) exists; (4) \(>0\) such that only \(\) with \(||||_{}\) separates the training data; and (5) \( t_{0}\) such that \(L((t_{0}))<N^{-1} 2\). \(3\) such that \(:=_{}(_{t}(t))\) is a first-order stationary point of the following maximum-margin problem_

\[}{} \|^{}\|_{}^{2}\] (1a) _s. t._ \[_{c/\{y_{i}\}}_{y_{i}}(x_{i};^{} )-_{c}(x_{i};^{}) 1 i[N],\] (1b)

_where \(_{c}(;^{})\) is the prediction of \(\) for the class \(c\)._

## 4 Method

Theorem 1 implies that the neural network parameters converge to the first-order stationary point (or the Karush-Kuhn-Tucker point (KKT) point) of the optimization problem in Eq. (1). Let \(\{_{ic}\}_{i[N],c/\{y_{i}\}}\) denote the set of KKT multipliers, the KKT condition can be written as follows.

\[=_{i[N]}_{c/\{y_{i}\}} _{ic}[_{}_{c}(x_{i};)-_{}_{y_{i}}(x_{i};)];\] (2a) \[i[N],c/\{y_{i}\}:\] \[_{y_{i}}(x_{i};)-_{c}(x_{i};)  1,\] (2b) \[_{ic} 0,\] (2c) \[_{ic}[1+_{c}(x_{i};)-_{y_{i}}(x_{i}; )]=0,\] (2d)

where the Eqs. (2a) to (2d) are known as the stationarity condition, primal feasibility condition, dual feasibility condition, and the complementary slackness condition, respectively.

The intuition behind the stationarity condition can be summarized as follows. According to the value of the corresponding element in \(\), the neural network parameters can be divided into two groups: the first group \(_{1}:=\{_{j}|_{jj}=_{max}\}\) includes all the parameters whose corresponding elements in \(\) are equal to \(_{max}\), and the second set \(_{2}:=\{_{j}|_{jj}_{max}\}\) contains all the parameters whose corresponding elements in \(\) are not equal to \(_{max}\). The stationarity condition in Eq. (2a) evaluates the linear combination of the derivatives of the neural network output corresponding to the parameters, with KKT multipliers \(\{_{ic}\}_{i[N],c/\{y_{i}\}}\) as coefficients of the combination. Such a linear combination of the derivatives corresponding to the parameters in \(_{1}\) is equal to the parameters in \(_{1}\). In contrast, such a linear combination of the derivatives corresponding to the parameters in \(_{2}\) is equal to a zero vector.

The intuition behind other conditions can be summarized as follows. According to the complementary slackness condition in Eq. (2d), \(_{ic}\) is nonzero only when \(_{y_{i}}(x_{i};)-_{c}(x_{i};)=1\). Two conditions are required for \(_{ic}\) to be nonzero. First, for a pair of \((i,c)\), \(_{ic}\) can only be nonzero when \(c\) is the class with the second largest predicted probability for the sample \(x_{i}\), _i.e._, \(c_{i}\), where \(_{i}:=\{y|_{y}(x_{i};)=_{y^{}/\{y_{i}\}}_{y^{}}(x_{i};)\}\). Second, according to primal feasibility in Eq. (2b), the minimum possible value of \(_{y_{i}}(x_{i};)-_{c}(x_{i};)\) is \(1\). Therefore, for a pair of \((i,c)\), \(_{ic}\) can only be nonzero when the margin between the true class \(y_{i}\) and the class with the second largest predicted probability for the sample \(x_{i}\) is minimum.

The evaluation of \(_{}(;)\) and \((;)\) requires to first scale all parameters of a pre-trained network. For the convenience of practical implementation and following derivation, we transform \(\) back to \(\) using the definition of the quasi-homogeneous function. The KKT conditions in Eq. (2) are rewrittenas:

\[=_{i[N]}_{c/\{y_{i}\}}_{ ic}[_{}_{c}(x_{i};)-_{}_{y_{i}}(x_{i};)];\] (3a) for all \[i[N]\], and \[c/\{y_{i}\}:\] \[_{y_{i}}(x_{i};)-_{c}(x_{i};) e^{-},\] (3b) \[_{ic} 0,\] (3c) \[_{ic}[e^{-}+_{c}(x_{i};)-_{y_{i}}(x_{i}; )]=0,\] (3d)

where the new scaling parameter \(:=e^{(2-)}\).

**From KKT condition to loss function.** Given only the pre-trained neural network \((;)\), the undetermined parts in the KKT conditions in Eq. (3) include the KKT multipliers, a set of \((x,y)\) pairs, and constant \(\). Regarding the KKT multiplier a predictable objective, We use a neural network to learn it. Given an approximate distribution of the discrete random variable \(y\), we sample \(y\) directly. Our goal is to train a conditional generator \(g\) parameterized by \(\) to generate input \(x=g(,y;)\) given the corresponding label \(y\) and random noise \(\). We also treat \(\) as a learnable parameter, which will be discussed in detail later. In the following paragraphs, we first discuss how to design the loss function for learning these parameters and training the generator.

First, we discuss how to ensure that the generated samples satisfy the stationary condition in Eq. (3a). The evaluation of the right-hand side of Eq. (3a) requires generating a dataset with a fixed number of samples every time to calculate the summation. Alternatively, we divide both sides of Eq. (3a) by the number of training samples \(N\), which converts the sum on the right-hand side to an expectation:

\[=_{x,y}_{c /\{y\}}_{c}[_{}_{c}(x;)-_{}_{ y}(x;)].\] (4)

Thus, according to the law of large numbers, the expectation can be estimated by the empirical average over a batch of \(M\) random samples, where \(M\) is a hyper-parameter. To ensure that the rescaled stationary condition in Eq. (4) can be satisfied, we use the following \(L_{stationarity}\) to minimize the norm of the difference between both sides of Eq. (4).

\[L_{stationarity}(,):=||- _{i[M]}_{c/\{y_{i}\}}_{ic}[_{ }_{y_{i}}(x_{i};)-_{}_{c}(x_{i};)]||.\] (5)

Conditions in Eqs. (3b) to (3d) constrain the valid values of the KKT multipliers. Accordingly, in order to satisfy the positivity condition of the KKT multipliers in dual feasibility, instead of directly optimizing the KKT multipliers, we use the proxy variables \(^{}_{ic}\), define \(_{ic}:=ReLU(^{}_{ic})\). For each generated sample \(x_{i}\), we set up a \(^{}_{i}^{||}\) with \(^{}_{ic}\) is its \(c\)-th element. We learn \(^{}_{i}=h(x_{i},y_{i};)\) by network \(h\) parameterized by \(\). To approximate primal feasibility and complementary slackness, we require each generated sample \(x_{i}\) to satisfy \(0_{y_{i}}(x_{i};)-_{c}(x_{i};)-e^{-}\), where \(0< 1\) is a hyper-parameter to ensure numerical stability. We minimize \(L_{duality}\) to approximate this constraint. Fig. 2 illustrates the shape of \(L_{duality}\).

\[L_{duality}(,):=_{i[M]}_{c _{i}} _{y_{i}}(x_{i};)-_{c}(x_{i};)-e^{- }-,0\] \[-_{y_{i}}(x_{i};)-_{c}(x_{i};)-e^{- },0\] (6)

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

recovering the original distribution of the training data. Subsequently, by employing the extension of the proposed method, we train a generator \(g_{1+2}\) using \(_{1}\) and \(_{2}\) together. As shown in Fig. 4(d), \(g_{1+2}\) possesses the generative capabilities of both \(g_{1}\) and \(g_{2}\), capable of generating samples on the entire training data \(D\). We further fix the classifier index of \(g_{1+2}\) to either \(1\) or \(2\), randomly sample noise and category labels, and observe the data generated by \(g_{1+2}\). As depicted in Figs. 4(e) and 4(h), \(g_{1+2}\) is also capable of independently generating data belonging to either \(D_{1}\) or \(D_{2}\).

### Image Generation

In this subsection, we showcase the experimental results on the MNIST (Lecun et al., 1998) and CelebA (Liu et al., 2015) datasets. More results and implementation details are left in the appendix. For the MNIST dataset, we set up a classification task corresponding to the digits 0-9 with 500 training data (50 images per class) randomly sampled from the original training set. For the CelebA dataset, we utilized various binary attributes to construct binary classification tasks on facial images, for example, distinguishing between males and females. For each task, we randomly sampled 100 images (50 images per class) from the original training dataset and resize them to 32x32 to be our training dataset.

We employed a three-layer fully-connected network with a ReLU activation function and batch normalization as the classifier for the aforementioned classification tasks. The networks were trained until the classification loss converges using full batch gradient descent, which ensures the parameters are close to the convergence point required in the theory of Maximum-Margin Bias.

Figure 5: An illustrative 2D example showcases the process of using two pre-trained classifiers to train a generator. Figs. 4(a) to 4(c) and Figs. 4(f) to 4(h) are two groups of training data, classifierâ€™s learned prediction landscape, and generatorâ€™s generated samples. Fig. 4(d) shows the generated samples of the generator trained using two classifiers. Figs. 4(e) and 4(i) are the generated samples of the generator in Fig. 4(d) with fixed classifier index.

Figure 6: Generator-produced samples. The generator is trained using a single classifier trained on the MNIST dataset.

We use generators composed of three fully-connected layers followed by three transposed convolution layers, with ReLU activation function and batch normalization. Network parameters were initialized using Kaiming initialization (He et al., 2015) and trained for \(50,000\) epochs. The batch size and learning rate were set as hyperparameters and optimized via random search. In order to control the noise in generated images, we also utilized total variation loss in pixel space as a regularization term.

The generated results on MNIST and CelebA are shown in Figs. 6 and 7. Odd rows present the generated images, and even rows present the images from the training dataset that are closest to the generated images. The distance between images is measured by the SSIM metric. As shown by the results, the generator trained using our method is capable of generating digits and facial images, even though it has never been exposed to images of digits or faces.

To validate the extended method we proposed for multiple classifiers, we partitioned the aforementioned digit classification dataset into two subsets including digits \(0-4\) and digits \(5-9\), respectively, and trained two classifiers separately. We then employed our method to train a single generator using both classifiers. Fig. 8 showcases the final images generated. The trained generator successfully integrates information from both classifiers, being capable of generating all digits from 0-9.

## 6 Conclusion

In this research, we investigate a pioneering task: training a generator directly utilizing a pre-trained classifier, devoid of training data. Based on the maximum margin bias theory, we present the relationship between pre-trained neural network parameters and the training data distribution. Consequently, we devise an innovative loss function to enable the generator's training. Essentially, our loss function requires the generator to guarantee the optimality of the parameters of the pre-trained classifier under its generated data distribution. From a broader perspective, the reuse and revision of pre-trained neural networks have been a widely studied direction. (Ma et al., 2023; Fang et al., 2023; Yu et al., 2023; Yang et al., 2022; Yang et al., 2022; Yang et al., 2022; Yang et al., 2022) Our method offers a novel direction for leveraging pre-trained models.