# Dynamic Negative Guidance of Diffusion Models:

Towards Immediate Content Removal

 Felix Koulischer

IDLab - Ghent University - imec,

Ghent, Belgium

Johannes Deleu

IDLab - Ghent University - imec,

Ghent, Belgium

Gabriel Raya

Jheronimus Academy of Data Science

Tilburg University

Thomas Demeester

IDLab - Ghent University - imec,

Ghent, Belgium

Luca Ambrogioni

Radboud University

Donders Institute for Brain,

Cognition and Behaviour

Joint Senior Authors

###### Abstract

The rise of highly realistic large scale generative diffusion models comes hand in hand with public safety concerns. In addition to the risk of generating _Not-Safe-For-Work_ content from models trained on large internet-scraped datasets, there is a serious concern about reproducing copyrighted material, including celebrity images and artistic styles. We introduce _Dynamic Negative Guidance_ a theoretically grounded negative guidance scheme that can avoid the generation of unwanted content without drastically harming the diversity of the model. Our approach avoids some of the disadvantages of the widespread, yet theoretically unfounded, Negative Prompting algorithm. Our guidance scheme does not require retraining the conditional model and can therefore be applied as a temporary solution to meet customer requests until model fine-tuning is possible.

## 1 Introduction

Since first proposed as generative models , Diffusion models (DMs) have become state-of-the-art models in Text-To-Image (T2I) generation . Recently, models like Midjourney2 and DALL-E  have captured considerable public attention. These models are capable of generating highly realistic images of very diverse sort . These astonishing qualities have also come with their safety concerns. Not only can such models potentially generate _Not Safe For Work_ content containing nudity, violence or hateful depictions , these models can also memorize copyrighted images . As these large models are trained by scraping data from the internet , they can be exposed to copyrighted content, this is highlighted by the fact that models such as Stable Diffusion generate watermarks. Even if the data is free of copyrighted content, diffusion models have further been publicly criticized for replicating artistic styles, or celebrities faces, without the artists or celebrities consent. In practice, filtering datasets of the scale required for the training of T2I models is not feasible in practice, such that removing specific concepts from the outputs of large, mostly black box, model is all but trivial. The most practical solution proposed in the literature consists in fine-tuning specific layers of existing models such as to remove these undesired concepts. Fine-tuning these models however requires time, especially as it is of paramount importance to verify that the fine-tuning did not harm the general capacities of the model.

To resolve this, we propose a temporary solution to meet user demand in real-time using a novel theoretically grounded negative guidance scheme, coined _Dynamic Negative Guidance_3. Our method can be seen as a replacement for the widespread, yet severely understudied, Negative Prompting (NP) algorithm .

Similarly to NP, our method uses the model's own understanding of a concept for the removal . The fundamental flaw of NP is that, regardless of whether a specific feature \(\) is being generated, the guidance impacts the output of the model. Furthermore, as in NP the guidance is present throughout the entire denoising trajectory, it can cause large deviations from the unguided setting. On the contrary, we find that the theoretically optimal negative guidance scale is of dynamic nature. It depends on the probability of a feature being present, i.e. on the posterior \(p(|)\). Should there be a high likelihood that an undesired feature be absent from the output, the guidance scale would deactivate itself. Mainstream diffusion models lack access to the posterior \(p(|)\) during inference. To overcome this limitation, we propose a new method that approximates the posterior by tracking the relevant diffusion Markov Chains during the denoising phase.

To analyze the effect of negative guidance, the task of single class removal from an unconditional diffusion model trained on MNIST and CIFAR10 is studied. In this setting, Dynamic Negative Guidance demonstrates superior performance in preserving model diversity when compared to Negative Prompting and Safe Latent Diffusion , especially in the high safety regime. To illustrate the flexibility of the approach, DNG is tested in the context of T2I using Stable Diffusion . These proof-of-concept experiments show that DNG is not only capable of editing images on par with NP, but crucially is also capable of deactivating itself when the model is negatively prompted with semantically unrelated text. Such a dynamic property is crucial when using generic negative prompts, such as a celebrities name, as all generated images are potentially affected by its presence. While methods like fine-tuning are essential to guarantee public safety by entirely removing a concept from the network weights, these approaches can take non-negligible amounts of time. In this regard, we believe that the dynamic nature of our approach makes DNG an attractive temporary solution for immediate concept removal in large pretrained models.

Furthermore, as diffusion models, and AI in general, become increasingly central to technological advancements, understanding their inner functioning is of fundamental importance. By building on solid mathematical foundations, our research provides deeper insights into the concept of negative guidance. This fundamental understanding is essential to mitigate potential risks, allowing us to develop strategies that prevent future harm from misuse or unintended model behavior.

## 2 Approaches to concept erasure

In response to rising public concerns, the research community has been actively pursuing techniques that can effectively remove entire concepts from the outputs of diffusion models. Most methods rely on the finetuning of specific layers inside the U-Net, whereby typically the attention layers are

Figure 1: Comparison between CFG, NP, and DNG using the exact posterior (all used with \(=1\)). DNG with the exact posterior is equivalent to CFG, while NP fails to sample the target distribution.

[MISSING_PAGE_FAIL:3]

### Dynamic Negative Guidance

A well-defined negative guidance scheme can be derived by realizing that the desired posterior can be expressed as the opposite of the undesired posterior, i.e. \(p_{t}(|_{})=1-p_{t}(|_{})\). Sampling from this conditional distribution, with the posterior emphasized by a positive exponent \(_{0}\) similar to the guidance scale, happens through the following score:

\[_{} p_{t}(|_{})& =_{} p_{t}()+_{0}_{} 1-p_{t}(_{}|)\\ &=_{} p_{t}()-_{0}( _{}|)}{1-p_{t}(_{}|)}_{} p _{t}(|_{})-_{} p_{t}()\\ &=_{} p_{t}()-(,t) _{} p_{t}(|_{})-_{} p_{t}( )\] (2)

This equation reveals that the guidance scale should be _dynamically_ rescaled throughout the denoising. The guidance scale becomes asymptotically large as \(p_{t}(_{}|) 1\), while it remains small when \(p_{t}(_{}|) 0\). These equations are validated in the one dimensional setting in which the posterior is explicitly available. This can be seen in Figure 1 on which it is visible that applying DNG with a repulsive force directed away from the undesired mode is equivalent to applying CFG with an attractive force towards the desired modes.

Diffusion models being score-based models do not give the possibility of easily computing the likelihoods. Inspired by recent work of Li et al.  that recognizes DMs as zero-shot classifiers, we propose to track the likelihoods of the conditional and unconditional Markov Chains throughout the diffusion process to recompose the posterior. Further, realizing that both \(p_{t}(_{t}|_{t+1},_{};)\) and \(p_{t}(_{t}|_{t+1};)\) are Gaussian of same variance with modelled means \(_{t,}(_{t+1}|_{})\) and \(_{t,}(_{t+1})\) allows us to summarize the approach as:

\[ p_{t}(_{}|_{t-1})&  p_{t}(_{}|_{t})\\ &= p_{t}(_{})+_{i=T}^{t} p_{i-1}( _{i-1}|_{i},_{};)- p_{i-1}(_{i-1}|_{i};)\\ &= p_{t+1}(_{}|_{t+1})+ p_{t}( _{t}|_{t+1},_{};)- p_{t}(_{t}|_{ t+1};)\\ &= p_{t+1}(_{}|_{t+1})-^{2}}\|_{t}-_{t,}(_{t+1}|_{})\|^ {2}-\|_{t}-_{t,}(_{t+1})\|^{2}\] (3)

The often used assumption of an infinitesimal diffusion process for which \( p_{t-1} p_{t}\) is required to avoid an implicitly defined scheme. The term added to the posterior in Eq. (10) can be positive or negative, respectively corresponding to an increase or a decrease of the posterior likelihood, and by extension thereof, of the guidance scale. To regularize this dynamic posterior estimation, we propose adding a linear transformation before the difference of Euclidean distances. Rescaling the difference by a factor \(]0,1[\) can diminish stochastic fluctuations present during denoising, while a small offset \(\) creates a slight bias towards increasing the posterior. Should an allowed image being generated, this offset is completely dominated by the very large difference.

## 4 Experiments

### Class removal

To analyze the invasiveness of our proposed method, the different guidance schemes are compared in the context of image generation on labelled datasets, in the present case MNIST and CIFAR10 are considered. The objective is to avoid generating one of the classes by guiding an unconditional model with a model trained solely on that specific class. The _safety_ of the approach is quantified by a classifier that assesses the percentage of generated images that belong to the forbidden class, while the _diversity_ is measured by examining the overall distribution of generated classes across all images. This distribution ideally contains a single zero and equal weight on all other classes (see Figure 2c). To measure how this ideal case is approximated, the KL-divergence between ideal and generated distributions is computed. The _quality_ of the model is measured through the standard FID metric computed between 10420 generated images and the training data excluding the undesired class 5.

Both the FID and the KL-divergence can be compared for different approaches at equal safety. To vary the safety of different approaches a sweep over the initial guidance scale \(_{0}\) is performed. These graphs are shown for MNIST in Fig. 1(a) and for CIFAR10 in Fig. 1(b). In the regime of high safety, where as few as possible forbidden images are generated, DNG significantly outperforms concurrent approaches, showcasing that the image removal performed by DNG is less invasive.

### Illustrative Results in T2I

While previous experiments showcase the capacities of DNG, it is impractical to train a network on solely forbidden content. In the case of celebrity removal, this would imply requiring a model only capable of generating a single celebrity, for instance "Taylor Swift" before it can be removed. Instead, our framework solely requires a conditional model, a prerequisite met by all T2I models. In this setting, the model's knowledge of certain concept such as "Taylor Swift" can be used to our own advantage, by prompting the model with the name of the celebrity such a specialized negative model is obtained. As proof of concept that the proposed scheme remains sensible in this setting, we prompt Stable Diffusion 2  to generate an image of "Taylor Swift riding a horse" and then add a negative prompt containing "Taylor Swift" (visible in Figure 3 (e)-(f)). These preliminary experiments demonstrate that when the hyperparameters of DNG are correctly tuned, our approach is just as efficient as NP at removing visible features. On the other hand, we also show that when generating an image completely unrelated to "Taylor Swift", such as for instance "An English breakfast", the dynamic guidance scale defined by DNG falls to zero, leaving the images close to unaltered (visible in Figure 3 (a)-(c)). This is not at all the case when using NP, which is just as strong regardless whether the undesired feature is present or not in the image.

As already highlighted in the literature [5; 25; 30; 31], the generation of images happens in phases. The main advantage of our self-regulated guidance scale is that such events can be observed by tracking the posterior, or equivalently, the guidance scale. This is visualized in Figure 4. The red line corresponds to semantically unrelated negative guidance (being "A truck" when generating "Taylor Swift riding a horse", and "Taylor Swift" when generating "An English breakfast") for which it is visible that the guidance is efficiently deactivated. The green line corresponds to semantically related negative guidance (being "Taylor Swift" when generating "Taylor Swift riding a horse", and "An egg" when generating "An English breakfast") for which it is visible that the guidance is active.

Figure 2: Comparison of diversity (top) and quality (bottom) as a function of safety for SLD, NP and DNG (ours) when removing a specific class measured respectively using the KL-divergence and the FID. To reduce the percentage of undesired images generated, the initial guidance scale is increased. In Fig. 1(a) the number zero is removed. In Fig. 1(b) the class _airplane_ is removed. An example showing how the KL-divergence is computed is shown in Fig. 1(c), the example is taken using our approach on MNIST. The class corresponding to the number one is oversampled, as it lies furthest from the undesired class zero.

Figure 4: Illustration of our _dynamic_ guidance scale. For semantically unrelated guidance, such as “Taylor Swift” for “An English breakfast”, the guidance scale drops to zero. While for of semantically related negative guidance, such as “Taylor Swift” for “Taylor Swift riding a horse” the guidance is activated.

Figure 3: Examples illustrating that our DNG scheme keeps the model diversity. The guidance is deactivated in the case of an unrelated positive prompt (illustrated in (a)-(c)), while still capable of removing the celebrity’s identity should it be present in the generated images (illustrated in (e)-(f))

Conclusion

We presented Dynamic Negative Guidance, a novel scheme for negative guidance of diffusion models, which overcomes fundamental limitations of the popular Negative Prompting approach. Our method better preserves the diversity of underlying models in the context of single class removal. Crucially, the dynamic nature of the guidance scale allows our method to switch off automatically when the undesired feature defined by the negative prompt is not present. Our method could be used as a temporary but immediate solution (i.e., without requiring any diffusion model retraining), for example to comply with demands from public figures or artists requiring the removal of a model's ability to reproduce their appearance or style, even when prompted to do so by users.

#### Acknowledgments

This research was partly funded by the Research Foundation - Flanders (FWO-Vlaanderen) under grant GOC2723N and by the Flemish Government (AI Research Program). Gabriel Raya was funded by the Dutch Research Council (NWO) as part of the CERTIF-AI project (file number 17998).