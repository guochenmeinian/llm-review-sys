# Schur Nets: effectively exploiting local structure for equivariance in higher order graph neural networks

Schur Nets: effectively exploiting local structure for equivariance in higher order graph neural networks

Qingqi Zhang\({}^{1*}\), Ruize Xu\({}^{2*}\) and Risi Kondor\({}^{1,2,3}\)

\({}^{1}\)Computational and Applied Mathematics

Departments of \({}^{2}\)Computer Science and \({}^{3}\)Statistics

University of Chicago

{qingqi,richardixur,risi}@uchicago.edu

###### Abstract

Recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses having to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. We show that this approach can boost the performance of GNNs on some standard benchmarks.

+
Footnote †: * denotes equal contribution.

## 1 Introduction

Message pasing neural networks (MPNNs) are the most popular paradigm for building neural networks on graphs . While MPNNs have proved to be remarkably effective in a range of domains from program analysis  to drug discovery , a series of both empirical  and theoretical  results have shown that the fact that MPNNs are based on just vertices sending messages to their neighbors limits their expressive power. This problem is especially acute in domains such as chemistry, where the presence or absence of specific small structural units (functional groups) directly influences the behavior and properties of molecules.

Recent papers proposed to address this issue by extending the message paradigm to also allow for message passing between vertices and edges  or between subgraphs . However, if the actual messages remain scalars, the added expressive power of these networks is relatively limited.

A newer development is the appearance of _higher order MPNN_s, where the messages are not just scalars, but more complex objects indexed by the vertices of the sending and receiving subgraphs . For example, in an organic molecule, the internal state of a benzene ring (six carbon atoms arranged in a cycle) can be represented as a matrix \(T^{6 c}\), where the rows correspond to the individual carbons. When this benzene ring passes a message to a neighboring benzene ring that it shares an edge with, information relating to the two shared carbons can be sent directly to the corresponding atoms in the second ring. Information relating to the other vertices has to be treated differently. Hypergraph neural networks  and neural networks on more abstract mathematical structures such as simplicial complexes  are closely related, since these also involve higher order generalizations of message passing between combinatorial objects interlocked in potentially complicated ways.

In prior work we introduced a formalism called \(P\)-tensors that provides a flexible framework for implementing and reasoning about higher order MPNNs . \(P\)-tensors build closely on the by now substantial literature on permutation equivariance in neural networks from Deep Sets , to various works studying higher order permutation equivariant maps . It is also related to the category theoretic approach employed in Natural Graph Neural Networks .

One aspect of higher order message passing that has hithero not been studied in detail is the interaction between the local graph structure and the equivariance constraint. Intuitively, the reason that GNNs must be equivariant to permutations is that when the same, or analogous, graphs are presented to the network with the only difference being that the vertices have been renumbered, the final output must remain the same. However, Thiede et al.  observed that enforcing full permutation equivariance, especially at the local level, can be too restrictive. When considering specific, structurally important subgraphs such as paths or cycles, the operations local to such a subgraph \(S\) should really only be equivariant to the automorphism group of \(S\), rather than all \(|S|!\) permutations. The Autobahn architecture described in  explicitly accounts for the automorphism group of two specific types of subgraphs, cycles and path. Defining bespoke convolution operations on these two types of subgraphs was shown to improve performance on molecular datasets like ZINC .

The drawback of the Autobahn approach is that it requires explicitly identifying the automorphism group of each type of subgraph, and crafting specialized equivariant operations based on its representation theory. For more flexible architectures leveraging not just cycles and paths but many other types of subgraphs this quickly becomes infeasible.

**Main contributions.** The main contribution of this paper is a simple algorithm based on spectral graph theory for constructing a basis of automorphism equivariant operations on any possible subgraph _without_ explicitly having to determine its automorphism group, let alone derive its irreducible representations. The algorithm is based on imitating the structure of the group theoretical approach to equivariance, which we summarize in Section 3, but bypassing having to use actual group theory. Section 4 describes the algorithm itself and the resulting neural network operations, which we call _Schur layers_. In Section 5 we show that the introduction of Schur layers does indeed improve the performance of higher order graph neural networks on standard molecular benchmarks.

## 2 Background: equivariance with side information

Let \(\) be an undirected graph with vertex set \(V=\{1,2,,n\}\) and edge set \(E V V\) represented by the adjacency matrix \(A^{n n}\). Graph neural networks (GNNs) address one of two, closely related tasks: (a) given a function \(f^{} V\) on the vertices of \(\), learn to transform it to another function \(f^{} V\); (b) given not just one graph, but an entire collection of graphs, learn to map each graph \(\), or rather its adjacency matrix, to a scalar or vector \((A)\) that characterizes \(\)'s structure.

In both cases, the critical constraint is that the network must behave appropriately under permuting the order in which the vertices are listed. The group of all permutations of \(\{1,2,,n\}\) is called the _symmetric group_ of degree \(n\) and denoted \(_{n}\). Applying a permutation \(_{n}\) to the vertices changes the adjacency matrix to \(A^{}=(A)\) with

\[A^{}_{i,j}=A_{^{-1}(i),^{-1}(j)}.\] (1)

The basic constraint on algorithms that operate on functions on graphs is that if the input is transformed along with the numbering of the vertices, \(f^{}_{i}\!\!=\!f^{}_{^{-1}(i)}\), then the output must transform the same way, \(f^{}_{i}\!\!=\!f^{}_{^{-1}(i)}\). This property is called _equivalence equivariance_. Formally, denoting the mapping from inputs to outputs \( f^{} f^{}\), equivariance states that the action \( f f^{}\) of permutations on functions given by \(f^{}_{i}=f_{^{-1}(i)}\) must commute with \(\), that is,

\[((f^{}))=((f^{}))\] (2)

for any \(_{n}\) and any input function \(f^{}^{V}\). In contrast, for networks that just learn embeddings \((A)\), the constraint is _invariance_ to permutations, i.e., \((A)=((A))\). In practice, the two cases are closely related because most graph embedding networks work with functions defined on the vertices, and then symmetrize over permutations in their final layer using a readout function such as \(()=_{i}f_{i}^{}\).

Enforcing (2) on the hypothesis space directly would be very limiting, effectively constraining GNNs to be composed of symmetric polynomials of \(f^{}\) combined with pointwise nonlinearities. Message passing graph neural networks cleverly get around this problem by using the adjacency matrix itself as _side information_ to guide equivariance. For example, in classical (zeroth order) MPNNs, the output of each layer is a (vector valued) function on the graph, and the update rule from layer \(\) to layer \(+1\) in the simplest case is

\[f_{i}^{+1}=W_{j(i)}f_{j}^{}+,\] (3)

where \((i)\) denotes the neighbors of node \(i\) in \(\), \(W\) and \(\) are learnable weight matrices/vectors and \(\) is a suitable nonlinearity. The fact that the summation only extends over the neighbors of vertex \(i\) induces a dependence of \(\) on the adjacency matrix \(A\). Hence it would be more accurate to use the notation \(_{A}\) for the mapping, and write the equivariance condition as

\[_{(A)}((f^{}))=(_{A}(f^{})).\] (4)

In this interpretation, when the vertices are permuted, \(A\) can implicitly provide information about this to the algorithm, allowing it to compensate. However, \(A\) cannot convey any information about permutations that leave it invariant, so \(_{A}\) must still be equivariant to the group of all such permutations, called the _automorphism group_ of \(\), denoted \(()\) or just \((A)\). This observation about \(A\) acting as a source of side information that can reduce the size of the group that individual GNN operations need to be equivariant to is the starting point for the rest of this paper.

### Higher order GNNs

Despite the great success of message passing networks, a long sequence of empirical as well as theoretical results [40; 34; 30; 13; 35] over the last several years have made it clear that the expressive power of algorithms based on simple update rules like (3) is severely limited. In response, the community has extended the message passing paradigm to message passing between vertices and edges or between carefully selected subgraphs [18; 34; 6; 8]. These networks maintain the local and equivariant character of earlier MPNNs, but they can more faithfully reflect local topological information and are particularly well suited to domains such as chemistry, where capturing local structures such as functional groups is critical.

In tandem, researchers have developed _higher order MPNN_ architectures, where the outputs of individual edge or subgraph "neurons" are not just scalars, but vector or tensor quantities indexed by the vertices of the graph involved in the given substructure. For example, in the chemistry setting, if n is a titular neuron corresponding to a benzene ring in a molecule, the output of n, assuming we have \(C\) channels, might be a matrix \(T^{6 C}\) or a tensor \(T^{6 6 C}\), or \(T^{6 6 6 C}\), etc.. In each of these cases the non-channel dimensions correspond to the six atoms making up the ring, and when n communicates with other subgraph-neurons, must be treated accordingly. Such higher order representations offer greater expressive power because they allow \(T\) to capture information about _relations_ between pairs of vertices, triples, and so on. The general trend towards studying higher order message passing is also closely tied to the emergence of hypergraph neural networks , "topological" neural networks and simplicial complex networks [7; 6].

Recently,  proposed a general formalism for describing such higher order architectures using so-called \(P\)-tensors. In this formalism, given a neuron n attached to a subgraph \(S\) with \(m\) vertices, we say that the output of n is a _zeroth order \(P\)-tensor_\(T\) with \(C\) channels if it is simply a vector \(T^{C}\). The elements of this vector are scalars in the sense that they are invariant to permutations of the vertices of \(S\). We say that \(T\) is a _first order \(P\)-tensor_ if it is a matrix \(T^{m C}\) whose columns transform under permutations of \(S\) similarly to how \(f^{}\) and \(f^{}\) transform under global permutations of the full graph:

\[ T T^{} T^{} T^{}_{i,c}=T_{^{-(i)},c} _{m}.\] (5)

We say that \(T\) is a _second order \(P\)-tensor_\(T^{m m C}\), if each slice corresponding to a given channel transforms according to the second order action of the symmetric group, similarly (1):

\[ T T^{} T^{}_{i,j,c}=T_{^{-(i)},\,^{-(j)},c} _{m}.\] (6)Continuing this pattern, a \(k\)_'th order \(P\)-tensor_\(T\!\!^{m m m C}\) transforms under local permutations as

\[\!:T T^{} T^{} T^{}_{i_{1},,i_{k},c}=T_{^{-1}(i_{1}),,\, ^{-1}(i_{k}),c} _{m}.\] (7)

 derives the general rules for equivariant message passing between such \(P\)-tensors in the cases that the sending and receiving subgraphs \(S\) resp. \(S^{}\) are (a) the same (b) partially overlap (c) are disjoint. However, in each of these cases however it was assumed that \(T\) needs to be equivariant to _all possible_ permutations of the vertices of the underlying subgraphs \(S\) and \(S^{}\).

As we discussed above, this is an overly restrictive condition that limits the extent to which a higher order subgraph neural network can exploit the underlying topology. In the following sections we focus on just the type of messages that are sent from a given subgraph \(S\) to _itself_ (called _linmaps_ in the \(P\)-tensors nomenclature), and derive a way of making these messages equivariant to just \((S)\) rather than the full symmetric group \(_{m}\).

## 3 Equivariance to local permutations: the group theoretic approach

Recall that a _representation_ of a finite group \(G\) such as \(_{m}\) or \((S)\) is a (complex) matrix valued function \(\!:G^{d_{} d_{}}\) where the \(()\) matrices multiply the same way as the corresponding group elements do

\[(_{2}_{1})=(_{2})(_{1}) _{1},_{2} G.\]

Two representations \(\) and \(^{}\) are said to be _equivalent_ if there is an invertible matrix \(Q\) such that \(^{}()=Q^{-1}()\,Q\) for all group elements. A representation of a finite group is said to be _reducible_ if there is some invertible matrix \(Q\) that reduces it to a block diagonal form

\[()=Q^{-1}(_{1}()&_{2}( ))\;Q.\]

Some fundamental results in representation theory tell us that G only has a finite number of inequivalent irreducible representations (_irreps_, for short), these irreps can be chosen to all be unitary, and that any representation of \(G\) is reducible to some combination of them . These facts give rise to a type of generalized Fourier analysis on finite groups that can decompose vectors that \(G\) acts on into parts transforming according to the unitary irreps of the group.

The general approach to defining \((S)\)-equivariant maps for first, second, and higher order subgraph neurons would use this machinery. In particular, defining permutation matrices as usual as

\[[P_{}]_{i,j}=\{1&\;(j)=i\\ 0&,.\]

dropping the channel indices without loss of generality, and writing our \(P\)-tensors in vectorized form \(=(T)^{m^{k}}\), (5)-(7) can be written in a unified form

\[}=P^{k}()\,\]

where \(P^{k}()\) is the \(k\)-fold Kronecker product matrix \(P^{k}()=P_{} P_{} P_{}\). Crucially, as \(\) ranges over the automorphisms of \(S\), these product matrices \(P^{k}()\), form a unitary representation of the automorphism group.

According to representation theory, \(P^{k}\) must then be decomposable into a direct sum of irreps \(_{1},,_{p}\) of \((S)\) with corresponding multiplicities \(_{1},,_{p}\). The same unitary matrix \(Q\) that accomplishes this can also be used to decompose \(\) into a combinations of smaller vectors \((_{j}^{i}\!\!^{d_{_{i}}})_{i,j}\):

\[Q=_{i}_{j=1}^{_{i}}_{j}^{i},\]

where each \(_{j}^{i}\) now transforms independently under the action of the group as \(_{j}^{i}_{i}()\,_{j}^{i}\). Alternatively, stacking all \(_{j}^{i}\) vectors transforming according to the same irrep \(_{i}\) together in a matrix \(^{i}\!\!^{d_{_{i}}_{i}}\), we arrive at a sequence of matrices \(^{1},^{2},,^{p}\) transforming as

\[^{1}_{1}()\,^{1} ^{2}_{2}()\,^{2}  ^{p}_{p}()\,^{p}.\] (8)It is very easy to see how one might construct learnable linear operations that are equivariant to these actions: simply multiply each \(^{i}\)_from the right_ by a learnable weight matrix \(W^{i}\).

This general, group theoretic approach to constructing automorphism group equivariant linear maps between \(k\)'th order \(P\)-tensors can be seen as a special case of . Operationally, it just reduces to the following sequence of steps:

1. Find the unitary matrix \(Q\) that decomposes \(P^{k}()=P_{} P_{}\) into a direct sum of \((S)\) irreps.
2. Use \(Q\) to decompose the input \(P\)-tensor into a sequence of matrices \(^{1}_{}^{p}\) transforming as (8).
3. Multiply each \(^{i}\) by an appoppriate learnable weight matrix \(W^{i}^{_{i}_{i}}\).
4. Use the inverse map \(Q^{-1}=Q^{}\) to reassemble \(^{1},^{2},,^{p}\) into the output \(P\)-tensor \(T^{}\).

Notwithstanding its elegance, this representation approach to implementing automorphism group equivariance also has some disadvantages. Specifically, it requires to (a) determine the automorphism group of each subgraph, and (b) explicitly find its irreducible representations, which is also not trivial. The underlying mathematical structure however is important because it forms the basis to generalizing the approach to a much simpler framework in the next section:

1. We have a collection of (orthogonal) linear maps \(\{P^{k}() U U\}_{(S)}\) (with \(U=^{m^{k}}\)) that the neuron's operation needs to be equivariant to.
2. \(U\) is decomposed into an orthogonal sum of subspaces \(U=U_{1} U_{p}\) corresponding to the different irreps featured in the decomposition of \(P^{k}\).
3. Each \(U_{i}\) is further decomposed into an orthogonal sum of subspaces \(U_{i}=V_{1}^{i} V_{_{i}}^{i}\) corresponding to the different columns of the \(^{i}\) matrices.
4. The decomposition is such that the \(\{P^{k}()\}\) maps fix each \(V_{j}^{i}\) subspace. Moreover, for a fixed \(i\), \(\{P^{k}()\}\) acts the _same_ way on each \(V_{j}^{i}\) subspace by \(_{i}()\).
5. This structure implies that any linear map that linearly mixes the \(V_{1}^{i}, V_{_{i}}^{i}\) subspaces but does _not_ mix information across subspaces with different values of \(i\) is equivariant.

## 4 Equivariance via spectral graph theory: Schur layers

In place of the representation theoretical approach described in the previous section, in this paper we advocate a simpler way of implementing automorphism group equivariance based on just spectral graph theory. The cornerstones of this approach are the following two theorems. The proofs can be found in the Appendix.

**Theorem 1**.: _Let \(G\) be a finite group acting on a space \(U\) by the linear action \(\{g U U\}_{g G}\). Assume that we have a decomposition of \(U\) into a sequence of spaces of the form_

\[U=U_{1} U_{p}\]

_where each \(U_{i}\) is invariant under the action of the group (this means that for any \(g G\) and \(v U_{i}\), we have \(g(v) U_{i}\)). Let \( U U\) be a linear map that is a homothety on each \(U_{i}\), i.e., \((w)=_{i}w\) for some fixed scalar \(_{i}\) for any \(w U_{i}\). Then \(\) is equivariant to the action of \(G\), i.e., \((g(u))=g((u))\) for any \(u U\) and any \(g G\)._

The representation theoretic result of the previous section corresponds to a refinement of this result involving a further decomposition of each \(U_{i}\) space into a sequence of smaller subspaces.

**Theorem 2**.: _Let \(G\) and \(U\) be as in Theorem 1, but now assume that each \(U_{i}\) further decomposes into an orthhogonal sum of subspaces in the form \(U_{i}=V_{1}^{i} V_{_{i}}^{i}\) such that_

1. _Each_ \(V_{j}^{i}\) _subspace is individually invariant by_ \(G\)_;_
2. _For any fixed value of_ \(i\)_, the spaces_ \(V_{1}^{i},,V_{_{i}}^{i}\) _are isomorphic and there is a set of canonical isomorphisms_ \(^{i}_{j j^{}} V_{j}^{i} V_{j^{}}^{i}\) _between them such that_ \[g(^{i}_{j j^{}}(v))=^{i}_{j j^{}}(g(v)) \,v V_{j}^{i}.\]

_Let \( U U\) be a map of the form_

\[(v) =_{j^{}}^{i}_{j,j^{}}^{i}_{j j^{ }}(v) v  V_{j}^{i}\]

_for some fixed set of coefficients \(\{^{i}_{j,j^{}}\}\). Then \(\) is equivariant to the action of \(G\) on \(U\)._In the matrix language of the previous section, \(U_{1},,U_{p}\) correspond to the \(^{1},^{2},,^{p}\) matrices, whereas the \(V_{1}^{i},,V_{_{i}}^{i}\) subspaces of \(U_{i}\) correspond to individual columns of \(^{i}\). For any fixed \(i\), the \((_{j,j^{}}^{i})_{j,j^{}_{i}}\) scalars correspond to the individual matrix entries of the learnable weight matrix \(W^{i}\). For our simplified spectral approach to automorphism group equivariance we will content ourselves with using Theorem 1 rather than Theorem 2.

### Automorphism invariance the simple way via spectral graph theory

The key insight of this paper is that we do not necessarily need to use heavy representation theoretic machinery to find a system of subspaces to plug into Theorem 1. In particular, we have the following simple lemma.

**Lemma 1**.: _Let \(S\) be an undirected graph with \(m\) vertices, \(_{S}\) its automorphism group, and \(L\) its combinatorial graph Laplacian. Assume that \(L\) has \(p\) distinct eigenvalues \(_{1},,_{p}\) and corresponding subspaces \(U_{1},,U_{p}\). Then each \(U_{i}\) is invariant under the first order action (5) of \(_{S}\) on \(^{m}\)._

Proof.: Since \(_{S}\) is a subgroup of the full group of vertex permutations, its action on \(^{m}\) is just \(()=P_{}\) with \(_{S}\). \(L\) is a real symmetric matrix, so its eigenspaces \(U_{1},,U_{p}\) are mutually orthogonal and \(U_{1} U_{p}=^{n}\). Furthermore, \( U_{i}\) if and only if \(L=_{i}\). By definition, \(_{S}\) is the set of permutations that leave the adjacency matrix, and consequently the Laplacian invariant, so. In particular, \(P_{}LP_{^{-1}}=L\) for any \(_{S}\). Therefore, for any \( U_{i}\)

\[L(())=L\,P_{}=P_{}\,LP_{ ^{-1}}P_{}=P_{}L=_{i}P_{} =_{i}() 28.452756pt\; _{S}\]

showing that \(() U_{i}\). Hence \(U_{i}\) is an invariant subspace. 

The following Corollary puts this lemma to use, providing a surprisingly easy way of creating locally automorphism equivariant neurons. We define a _Schur layer_ as a neural network module that applies this operation to every instance of a given subgraph in the graph, for example, every benzene ring in a molecule. For the sake of global permutation equivariance, the weight matrices for any given subgraph \(S\) must be shared across all instances of \(S\) across in the graph.

**Corollary 1**.: _Consider a GNN on a graph that involves a neuron \(_{S}\) corresponding to a subgraph \(S\) with \(m\) vertices. Assume that the input of \(_{S}\) is a matrix \(T^{m c_{}}\), the rows of which transform covariantly with permutations of \(S\) and \(c_{}\) is the number of channels. Let \(L\) be the combinatorial Laplacian of \(S\), \(U_{1},,U_{p}\) be the eigenspaces of \(L\), and \(M_{i}\) an orthognal basis for the \(i\)'th eigenspace stacked into an \(^{n dim(U_{i})}\) dimensional matrix. Then for any collection of learnable weight matrices \(W_{1},,W_{p}^{c_{} c_{_{S}}}\),_

\[ T_{i=1}^{p}M_{i}M_{i}^{}T\,W_{i}\] (9)

is a permutation equivariant linear operation.

The spectral approach also generalizes to higher order permutation equivariance, in which case we can take advantage of the more refined two-level subspace structure implied by Theorem 2.

**Theorem 3**.: _Let \(S\), \(L\) and the \(M_{i}\)'s be as in Corollary 1. Given a multi-index \(=(i_{1},,i_{k})\{1,,p\}^{k}\), we define its type as the tuple \(=(n_{1},,n_{p})\), where \(n_{j}\) is the number of occurrences of \(j\) in \(\) and we define \(_{}\) as the set of all multi-indices of type \(\). Assume that the input to neuron \(_{S}\) is a \(k\)'th order permutation equivariant tensor \(T^{m m c_{}}\), as defined in Section 3. For any given \(\), define the \(k\)'th order eigen-projector_

\[_{}=_{}(M_{i_{1}}^{} M_{i_{2}}^{ } M_{i_{k}}^{} I)^{m  m c}^{m m c}\]

_where \(_{}\) is a permutation map that canonicalizes the form of the projection, as defined in the Appendix. Let \(T W\) denote multiplying \(T\) by the matrix \(W\) only along its last dimension. Then for any collection of weight matrices \(\{W_{i^{},}}^{c_{} c_{ }}\}\) the map_

\[ T_{}_{}_{}}_{_{}}_{}}^{}(_{}(T W_{},}}))\]

_isExperiments

To empirically evaluate our Schur layers, we implement the first order case described in Corollary 1 and compare it with other higher order MPNNs. Note that the theorem only gives the equivariant maps to transform local representation on a subgraph, i.e., \(:T^{} T^{}\) where \(T^{}^{m c_{}}\) and \(T^{}^{m c_{}}\) are the inputs and output of a neuron corresponding to a specific subgraph. The rest of the message passing is conducted in the usual way, in particular, everything is implemented in the \(P\)-tensors framework .

There are several design details about the architecture that are worth mentioning: (1) We chose cycles of lengths three to eight in most of the experiments and also added branched cycles to show our algorithm's scalability. The reason is that in the chemical dataset we used, cycles are the most important functional group to the property to be predicted, other subgraphs such as \(m\)-stars, and \(m\)-paths did not help the performance. (2) While having first-order representation on the cycles, we also maintain 0'th-order representation (i.e., scalars) on node and edges, as in [6; 21]. These node and edge representations capture more elementary information about the graph, such as \(k\)-hop neighborhoods, and are still important in higher order MPNNs. The representations pass messages with each other by intersection rule as defined in \(P\)-tensor framework. (3) As discussed in Appendix G, we view _Schur_ layer's operation as a spectral convolution filter  applied to the subgraph and the number of channels to indicate how many times it expands the input feature to the output feature. The codes used to run our experiments can be found at https://github.com/risilab/SchurNet.

### _Schur_ layer improves over _Linmaps_

First of all, we want to show that considering more possible equivariant maps on the subgraph can indeed boost the performance. To this end, we performed controlled experiments to compare _Schur_ layer with the equivariant maps w.r.t. \(_{m}\) (following  we called these _linmaps_). To make a fair comparison, we use the same architecture including MLPs and message passing defined by \(P\)-tensor and only replace the equivariant maps used in _Linmaps_ by what is defined in Corollary 1. We didn't compare with Autobahn  because it didn't use the \(P\)-tensor framework in the original paper and it's hard for us to implement the convolution w.r.t. automorphism group for all the subgraphs we chose. However we note that for the case of the cycles (see Table 4), the equivariant maps given by our approach are equal to that given by the group theoretical approach.

We'll present the results on the commonly used molecular benchmark ZINC-12K  dataset in the main text. Results on TUdatasets as well as runtime comparison can be found in Appendix H. The task on ZINC-12K is to regress the \( P\) coefficient of each molecule and the Mean absolute error (MAE) between the prediction and the ground truth is used for evaluation.

We design experiments to compare _Linmaps_ and _Schur_ layers in various scenarios, to showcase the robustness of the improvement. Table 1 shows under various message passing schemes between edges and cycles, _Schur_ Layer consistently outperforms _Linmaps_. Those are indications of the added expressive power of extra equivariant maps in _Schur_ layer, and they're effective in various architectural design settings. Another ablation study regarding different cycle sizes can be found in Appendix H.

Then we studied the possibilities of adding _Schur_ layer in different places of higher-order message passing scheme. In table 2, we observed that the more condensed the higher-order feature is, the more improvement that the _Schur_ Layer brings to us over _Linmaps_. We attribute the improvements of adding/replacing _Schur_ layer in various scenarios over _Linmaps_ the benefit gained from utilizing the

  Layer & Pass message when overlap \( 1\) & Pass message when overlap \( 2\) \\  _Linmaps_ (baseline) & \(0.074 0.008\) & \(0.074 0.005\) \\  _Schur_ layer & \(\) & \(\) \\  

Table 1: Comparison between _Schur_ Layer and _Linmaps_ with different message passing schemes. The message passing scheme is a design choice in \(P\)-tensor framework, where the user can set when two subgraph’s representations communicate. The mostly common use case is to require at least \(k\) vertices in the intersection of two subgraphs for them to communicate. Experiments on ZINC-12k dataset and all scores are test MAE. Cycle sizes of {3,4,5,6,7,8,11} are used.

subgraph structure and increased number of equivariant maps. We also tried other ways to use _Schur_ layer, the result is summarized in Appendix G.

### Flexibility

The other advantage of _Schur_ layer is that it computes the feature transform only based on the subgraph's Laplacian, bypassing a difficult step of finding the automorphisms group and _irreps_ of the subgraph it acts on. As discussed in the theory, _Schur_ layer constructs equivariant maps only based on the subgraph's Laplacian and is applicable directly to any subgraphs, making the implementation much easier when different subgraphs are chosen than the group theoretical approach. This allows it to easily extend to any subgraph templates that're favorable by the user. To demonstrate this, we augment the subgraphs in the model by all the five and six cycles with one to three branches (including in total 16 non-isomorphic subgraph templates), comparing with baseline model where only the cycle itself is considered. Results can be found in Appendix G.

### Benchmark results

Finally, and most importantly, we compare the _Schur_-Net to several other higher-order MPNNs 1 on ZINC-12k dataset and OGB-HIV dataset  in table 3. We included baselines of (1) classical MPNNs: GCN, GIN , GINE , PNA, HIMP  (2) higher order MPNNs: \(N^{2}\)-GNN 2, CIN , \(P\)-tensors  (3) Subgraph-GNNs: DS-GNN(EGO+) and DSS-GNN(EGO+) , GNN-AK+ , SUN(EGO+) (4) Autobahn .

We find that _Schur_ Net ranked second on ZINC-12K and outperformed all other baselines on OGB-HIV dataset. This shows the expressivity of adding more equivariant maps by leveraging the subgraph topology. Furthermore, note that while \(N^{2}\)-GNN outperforms _Schur_ Net on ZINC-12K, it's a second-order model whereas in our experiment, we only used first-order activation. Also, the partial reason _Autobahn_ didn't perform well is in the original implementation, the authors didn't use \(P\)-tensor framework and used only a part of all possible linear message passing schemes. This shows to get the full power of equivariant maps w.r.t. subgraph automorphism group, we need to combine it with a general message passing framework between subgraphs as well.

## 6 Limitations

Unlike the representation theoretic approach, the spectral approach is not guaranteed the give the finest possible decomposition into invariant subspaces. Hence, equations like (9) do not necessarily define the most general possible automorphism-equivariant linear maps. In this paper we did not investigate from a theoretical point of view the extent of this gap. In general, being able to craft automorphism-equivariant layers of any order for any types of subgraphs opens up a host of possibilities for making GNNs more powerful. Our experiments are limited to some of the simplest cases, such as exploiting cycles and edges. We also only used first order activations.

  Model & Test MAE \\  _Limmaps_ & \(0.071 0.004\) \\ Simple _Schur_-Net & \(0.070 0.005\) \\ Linmap _Schur_-Net & \(\) \\ Complete _Schur_-Net & \(\) \\  

Table 2: An experiment demonstrating different ways of using _Schur_ layer. ”Complete _Schur_ Layer” means that we apply _Schur_ Layer on the incoming messages together with the original cycle representation. ”Linmap SchulLayer” means that we just apply the _Schur_ Layer on the aggregated subgraph representation feature. ”Simple _Schur_ Layer” means we directly apply _Schur_ Layer on the subgraph features without any preprocessing. We can observe that as the subgraph information diversifies, _Schur_ layer tends to decouple the dense information better and results in better performance. The test MAE of _Limmaps_ in this table is taken from .

## 7 Conclusions

Enforcing equivariance to the automorphism group of subgraphs in higher order neural networks seemingly requires the use of advanced tools from group representation theory. This is likely a large part of the reason why automorphism-based architectures such as  are not used more commonly in practical applications. In this paper we have shown that a simpler approach based on spectral graph theory, following the same underlying logic as the group theoretic approach but bypassing having to enumerate all irreducible representations of the automorphism group, can lead to an architecture that is almost as expressive. Our algorithm, called Schur Nets, easily generalizes to higher order activations, as well as incorporating other types of side information such as vertex labels. In a practical setting, Schur Nets is easiest to deploy in conjunction with a message passing framework like \(P\)-tensors that hides the complexities of the higher order message passing component. The empirical performance of Schur Nets on the ZINC 12K dataset is superirror to all other comparable (non-transformer based) architectures that we are aware of.

Given the similarity between the way we utilize the eigenspaces of the graph Laplacian and the so-called graph Fourier transform, our approach exposes heretofore unexplored connections between permutation equivariance and spectral GNNs such as [9; 22]. It also highlights the fact that while permutation equivariance is a fundamental constraint on graph neural networks, the key to building high performing, expressive GNNs is to reduce the size of the group that the network needs to be equivariant to as much as possible, using whatever side-information we can employ, whether that be the adjacency matrix, vertex degrees or something else.