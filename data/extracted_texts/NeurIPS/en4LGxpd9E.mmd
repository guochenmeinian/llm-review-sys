# Getting ViT in Shape:

Scaling Laws for Compute-Optimal Model Design

 Ibrahim Alabdulmohsin\({}^{*}\), Xiaohua Zhai\({}^{*}\), Alexander Kolesnikov, Lucas Beyer\({}^{*}\)

Google DeepMind

Zurich, Switzerland

{ibomohsin,xzhai,akolesnikov,lbeyer}@google.com

Significant technical contributions.

###### Abstract

Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal _model shapes_, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models and pave a path for a more informed scaling.

## 1 Introduction

The de-facto approach for improving performance of vision and language models today is scale: large models are trained on more data for longer . Empirically, it has been observed that the benefit of scale often follows a predictable power law in which the performance \(f(x)\) (e.g. error rate or log-perplexity) satisfies \(f(x) x^{-c}+_{}\) for some \(,c>0\) as one varies the scaling dimension \(x\) (e.g. data or model size), if the remaining dimensions are not bottlenecks . Here, \(_{}\) is the irreducible loss.

However, the simple power-law relation becomes more complicated when compute is considered. In this case, power laws are observed _only_ along the compute-optimal frontier. Otherwise, scaling up the model size for a fixed compute budget can deteriorate performance (see  and Figure 4). Since one often has a fixed compute budget in mind (e.g. available hardware and time), one should pick the model size that maximizes performance subject to the compute budget constraint, which may imply not training until convergence. Indeed, this approach was used successfully in the recent Chinchilla  that outperformed its predecessor Gopher  despite being \(4\) smaller in size.

Unfortunately, in both  and  among others, the "size" of a model is equated with its parameter count, with no special consideration for model "shape dimensions", such as "depth" or "width". The rationale behind this choice follows from the surprising observation that the transformer shape had little impact on its scaling behavior in language modeling (LM) when performance is measured upstream (e.g. using log-perplexity) . Nevertheless, follow-up analysis suggests that shape plays a pivotal role in other domains, such as in machine translation  and also in language modeling for _downstream_ performance , with recent works even advocating for extreme aspect ratios, such as a single wide attention layer .

In vision, in particular, much earlier works using convolutional neural networks (CNNs) pointed out that the parameter count is indeed a poor predictor of performance. For example, scaling all dimensions  in ResNets  is more effective than scaling a single dimension such as depth alone. In addition, scaling width  is often more effective than depth, especially for small models . Hence, optimizing the "shape" of transformers seems worthwhile.

In this work, we present **SoViT**: a **shape-optimized vision transformer** that matches the performance of much larger models despite being pre-trained with equal compute. It is derived from a recipe we introduce for optimizing the shape of neural architectures, such as their depth and width. A principled approach for scaling multiple dimensions is advantageous because although one can scale dimensions via brute-force search, this requires extensive computation and often remains sub-optimal . Our recipe allows us to extrapolate without having to conduct an extensive set of experiments. For example, after only 115 experiments, we identify a scaling strategy in ViT for _all_ three dimensions: width (internal representation), depth, and MLP size. For comparison,  requires over 400 experiments to optimize a single dimension (the parameter count) alone.

One major finding is that small vision models can perform on par with larger ones with the _same compute_ if we optimize their shape. In language, recent works have demonstrated the value of scaled-down architectures, such as the Chinchilla model  discussed earlier -- a 70B parameter model that outperforms the 280B-parameter Gopher  and 175B-parameter GPT3  -- as well as LLaMA with its 13B parameter variant outperforming GPT3 on most benchmarks . By introducing SoViT, we establish this phenomenon in vision as well.

Figure 1 summarizes how the various shape dimensions are scaled in SoViT (see Section 3 for derivation). The MLP dimension is scaled faster than depth, which in turn is scaled faster than width. When summarized by their parameter count (rightmost plot), compute-optimal ViTs are smaller than was previously used. With this scaling strategy, we find the shape of a ViT for the compute-equivalent of ViT-g/14  pretrained on 16B JFT images . We call this \(2.5\) smaller model SoViT-400m/14. It achieves 90.3% fine-tuning accuracy on ILSRCV2012  and 82.2% zero-shot accuracy in the locked-image text tuning (LiT) setup . We further evaluate SoViT-400m/14 on captioning, VQA and panoptic segmentation and highlight some results in Figure 2.

**Statement of Contribution.** In summary, our contribution is to:

* Introduce a new method for optimizing _the shape_ of neural networks, such as their depth and width. Our technique expands and improves previous methods by optimizing _multiple_ shape dimensions _jointly_ while requiring significantly fewer experiments.
* Demonstrate the effectiveness of scaled-down architectures in vision. We optimize ViT for the compute-equivalent of ViT-g/14, leading to a smaller, faster model of equal quality.
* Present new qualitative insights for scaling vision transformers, such as on how to scale individual shape dimensions and how optimal ViT shapes vary across domains.
* Conduct extensive evaluation across tasks like image classification, image captioning, VQA, zero-shot classification and panoptic segmentation, identifying both gains and limitations.

Figure 1: Predicted efficiency frontier (depth, width, MLP dimension, and parameter count) in SoViT. In large models, optimal shapes follow a similar trajectory in both image classification and multimodal tasks (see Section 4) although they can be different in small models (see Figure 3). We provide on the right (in blue) the amount of increase when compute goes from 1T to 100T GFLOPS.

## 2 Related Work

Optimizing training for compute has received a significant amount of attention in recent years, partly due to the financial and environmental costs of training large models [52; 55]. However, conflicting results are sometimes reported. For example, in language modeling,  argues that the model size should be scaled faster than the data size, implying it is compute optimal to "undertrain" large models. Similar conclusions are found in . On the other hand,  argues that the model size should be scaled uniformly with the data size, and highlights that transformers were not trained long enough, leading to some recent efforts  "overtraining" their models instead. Our analysis for ViT in Section 4 agrees partially with the latter result.

Scaling the size of vision transformers has led to remarkable results achieving, for instance, 90.4% top-1 accuracy on ImageNet (ILSRCV2012) with 2 billion parameters  and 90.9% top-1 accuracy with 4 billion parameters . When scaled to 22 billion parameters, ViT exhibits state-of-the-art alignment to human visual perception in terms of shape/texture bias, among other findings .

Despite the clear benefit of scale, there has been little investigation into optimally scaling the shape of ViTs.  suggest preferentially increasing depth before scaling other dimensions uniformly. For ViT, however, they only consider small ViT-S and ViT-B models and the reported accuracy improvement comes with an _increase_ in FLOPs of up to \( 4\), making it difficult to draw conclusions about the suggested shape's quality. In contrast  recommend scaling width over depth, but the authors do not observe any improvement when applying their strategy to ViT.

Our analysis draws inspiration from "compound scaling" in MobileNet  and EfficientNet , while differing in significant ways. EfficientNet uses an exhaustive grid search to determine the optimal architecture for a fixed increase in compute (e.g. \( 2\)). Afterwards, each dimension is scaled up by the same ratio with every subsequent increase in compute. In contrast, we expand scaling laws to simultaneously account for model size and compute beyond the efficient frontier and leverage them to derive the optimal scaling exponents for each dimension separately, as outlined in Section 3.

Throughout our analysis, we use _downstream_ metrics, e.g. ImageNet 10-shot error, when measuring performance instead of upstream metrics. This follows recent reports arguing that upstream performance may not reflect downstream performance in language and vision [65; 80].

We use GFLOPs as a proxy for compute since it is hardware-agnostic and correlates well with actual wall-clock core-hours (see Figure 4). However, GFLOPs can have limitations [5; 20] and may not be a perfect predictor for the metric of interest (e.g. core hours) in all model and hardware types. Note that we focus on scaling the shape of the architecture, not on improving its training protocol, which can be similarly beneficial [5; 67; 62; 68].

## 3 Scaling Strategy

**Notation.** We begin with a formal description of the problem. We represent a neural architecture as a tuple \(=(_{1},_{2},,_{D})^ {D}\) containing \(D\) shape dimensions, such as width, depth and MLP size. We denote compute such as GFLOPs by \(\). We designate \(f:^{D}^{+}\) a performance metric of interest, such as downstream ImageNet 10-shot error rate. Specifically, \(f(,)\) results from (pre)-training an architecture \(\) for a fixed compute budget \(\). We always assume that \(f\) corresponds to a loss, meaning lower values are better.

Figure 2: Optimizing for the compute-equivalent of ViT-g/14 results in the \(2.5\) smaller SoViT-400m/14 model achieves equivalent results across a wide range of benchmarks. Our model performs exceptionally well on the competitive ImageNet (ILSRCV2012) benchmark in comparison with significantly larger models from the recent literature [61; 78; 49; 80].

The goal of optimizing shape for fixed compute \(\) is to identify \(^{}\) (depending on \(\)) such that:

\[f(^{},)-_{x^{D}}f(x,)\;\; , \]

for some small tolerance \(>0\). Due to modeling assumptions, approximations, and the finite possible number of experiments conducted, we cannot hope for \(=0\) and have to tolerate a small excess loss.

Single Dimension.As demonstrated in Figure 3, the shape of a pretrained vision transformer has an impact on its downstream performance. To determine an optimal shape scaling strategy, we begin by considering both compute \(\) and a _single_ shape dimension \(_{k}\) for \(k[D]\), such as depth. In prior works, optimizing a single dimension \(_{k}\) for compute involves running a large number of experiments in order to identify the Pareto optimal frontier, from which power laws on \(_{k}\) or \(\) are derived . Since this is expensive, we propose the following joint functional form instead:

\[f_{k}(_{k},\,)_{k}_{k}^{-a_{k}}+( _{k}_{k}^{b_{k}}+_{k})\,^{-c}+_{k}, \]

where \(_{k},a_{k},_{k},b_{k},c,_{k},_{k}>0\). Here, \(f_{k}\) focuses on the dimension \(k\) alone and assumes that all other shape dimensions \(j k\) are sufficiently large such that they do not constitute a bottleneck. We also assume that data is unlimited so that there is no risk of overfitting. We estimate the parameters in (2) by minimizing the _relative_ error. In (2), \(a_{k}\) are scaling exponents when varying the corresponding shape dimension in the compute-unbounded regime, \(c\) is the data scaling exponent, while \(b_{k}\) relates to the impact of the model shape on compute.

Our argument for this particular functional form is six-fold:

1. If compute is unbounded, we recover the familiar power law relation on model size \(f_{k}(_{k})_{k}_{k}^{-a_{k}}+_{k}\). In addition, increasing the model size \(x_{k}\) while keep the data size fixed does not imply that \(f_{k}(_{k},\,)_{k}\) because \(_{k}^{b}\) can increase faster than \(^{c}\) in (2).
2. For any _fixed_ model size, the relation above reduces to the power law \(f_{k}() A^{-c}+B\), where \(A=_{k}_{k}^{b_{k}}+_{k}\) and \(B=_{k}_{k}^{-a_{k}}+_{k}\). Since the model size is fixed, \(\) is proportional to the size of the data. Such data scaling laws have been demonstrated extensively in various domains .
3. For fixed compute, the relation w.r.t. \(_{k}\) is non-monotone, quasiconvex (see Appendix A), in agreement with empirical measurements . See IsoFlop curves in Figure 4.
4. Arguments for power law behavior using space partitioning suggest that the exponent \(c\) is independent of the shape dimension. In particular, \(c=(1/d)\), where \(d\) is the intrinsic dimension of the data manifold . From this, we conclude that assuming the functional form in (2) for every shape dimension _separately_ cannot lead to any contradictions since this assumption is satisfied by the decomposable loss: \[f(,)=_{k}_{k}_{k}^{-a_{k}}+_{k} _{k}_{k}^{b_{k}}^{-c}+^{-c}+ _{},\] (3) for some constants \(,_{}>0\).

Figure 3: A grid sweep over multiple ViT shapes pretrained on 600M JFT examples highlights the important role of shape. Each dot corresponds to a model architecture pretrained on 600M examples and evaluated on a downstream metric, e.g. Imagenet-1k 5-shot in the leftmost plot. The two architectures marked in blue and red – identical in all four figures – are compute-optimal for classification and image-to-text tasks (captioning/VQA), respectively. For captioning/VQA, we average log-perplexity scores (see Section 4.2). In the leftmost three figures, using Imagenet-1k few-shot evaluation, the compute-optimal model highlighted in blue is compute-optimal in all three cases, but it is not compute-optimal for image-to-text tasks as shown in the rightmost figure. So, in _small_ models, an optimal shape in one domain is not necessarily optimal in others.

5. When optimizing the shape dimension \(_{k}\) for fixed compute \(\), the optimal value \(_{k}^{}\) is: \[_{k}^{}=(\,a_{k}\,^{}}{ _{k}b_{k}})^{+a_{k}}}=O(^{_{k}} ),s_{k}=+a_{k}}.\] (4) Recall that the scaling exponent \(s_{k}\) in (4) is positive because \(a_{k},b_{k},c>0\). Using the relation (4), we rearrange the terms in Eq. (2), and obtain the scaling law for model performance along the compute-optimal frontier (Appendix A): \[f_{k}(_{k},t)=F_{k}^{-a_{k}}+G^{-c}+_ {k},\] (5) for some constants \(F\) and \(G\), which is a sum of power law terms involving the model size and compute. Indeed, this decomposition has been demonstrated to hold within the compute-optimal frontier by  and .
6. Eq. (2) fits empirical measurements and extrapolates accurately as well, see Figure 4.

Multiple Dimensions.Next, we expand upon the previous approach by incorporating multiple dimensions. To reiterate, our method involves both a functional form (2) and a novel procedure. Our procedure significantly decreases the number of large-scale experiments required to identify compute-optimal architectures, by an order of magnitude compared to prior work .

_Star Sweep_ - Conducting a brute-force grid search to estimate scaling parameters across all dimensions is expensive, since it requires \(O(2^{D})\) experiments to cover the search space. Instead, we demonstrate that a "star sweep" is sufficient: (1) starting from a _large_ model \(^{(c)}\) (the star center), we vary a single dimension \(k[D]\) at a time in an exponentially-spaced grid, such that all values are much smaller than \(_{k}^{(c)}\). In our experiments, for instance, we optimize three shape parameters: width, depth, and MLP dim (see Section 4 for a brief definition of each dimension). Our star center is \(^{(c)}=(1968,\,40,\,6144)\); i.e. has width 1968, depth 40, and MLP dim 6144. When varying MLP dim in the star sweep, we use the grid \((1088,\,1360,\,1728,\,2160,\,2592,\,3072)\), corresponding to about 20% increase in each step, while fixing width to 1968 and depth to 40. We do this to ensure that other dimensions do not form a bottleneck when estimating the parameters in (2). This gives us the scaling exponents \(s_{k}\) in (4).

_Grid Sweep_ - The second stage is a grid sweep for _small_ models trained for _short_ compute. Depending on the number of shape dimensions involved, the cost of running this grid sweep can be negligible. Its goal is to identify a single architecture \(^{(0)}\) that lies in the Pareto optimal frontier for small compute as illustrated in Figure 3. This is important since a suboptimal \(^{(0)}\) can significantly skew results . Our grid sweep identifies \(^{(0)}\) to be \((608,\,10,\,928)\), the blue star in Figure 3. The advantage of this step is to absorb the leading coefficients in \(_{k}^{}=O(^{s_{k}})\) in (4) so that the star sweep focuses on estimating the _exponents_\(s_{k}\) alone. We demonstrate in Figure 5 that the scaling exponents \(s_{k}\) are robust to the choice of the evaluation metric \(f\). In Appendix B.3, we discuss important considerations that were taken into account during this analysis.

Scaling.Finally, we scale all dimensions jointly. Starting from the small compute-optimal architecture \(^{(0)}\) and the amount of compute \(^{(0)}\) it is optimal for, suppose we increase compute by a factor \(>1\) (i.e. the new compute is \(\,^{(0)}\)). By treating this increment \(\) as a _sequence_ of \(D\) smaller increments of size \(^{w_{k}}\) each with \(_{k}w_{k}=1\), an increase in compute by a factor of \(\) is accompanied by an increase in every shape dimension \(k\) by a factor of \(^{w_{k}}\), respectively. In this work, the adopt the simplest strategy of setting \(w_{k}=1/D\), but acknowledge that more sophisticated approaches might lead to better results.

## 4 Shape-optimized ViT

We implement the scaling strategy in Section 3 in vision transformers  pretrained on JFT-3B, a proprietary dataset with about 30k classes and around 3 billion examples , using the Adam optimizer . As mentioned in Section 3, we focus on optimizing three shape dimensions: width (size of internal representation), depth (number of encoder blocks) and MLP dim (hidden dimension). Following , we remove near-duplicate examples between upstream JFT-3B data and all the downstream train and test sets. Appendix B contains the full set of hyper-parameters used in the experiments, including full details about the star and grid sweeps described in Section 3. We fix the patch size in our analysis to \(14 14\), but study "flexifying" to arbitrary sequence lengths following  in Section 5.5.

As an evaluation metric \(f\), we consider two domains: (1) image classification, with ImageNet linear 10-shot error rate as the metric, and (2) image-to-text LiT-decoding following . In the latter case, the evaluation metric \(f\) is an average of four perplexity scores: COCO captioning, optical character recognition (OCR), and question answering (VQAv2 and GQA). Refer to  for details about the LiT-decoder setup. By considering such distinct domains, our goal is to identify similarities and differences (if any) in how to optimally scale the shape of vision transformers (ViT).

### Image Classification

We use the aforementioned star center \(^{(c)}=(1968,\,40,\,6144)\) as our starting point. To estimate the scaling exponents \(s_{k}\) in (4) for each dimension separately, we vary width in the grid \((608,\,768,\,928,\,1088,\,1328,\,1648)\), depth in the grid \((8,\,10,\,12,\,16,\,20,\,24)\), and MLP dim in the grid \((1088,\,1360,\,1728,\,2160,\,2592,\,3072)\). As discussed in Section 3, we use an exponential spacing with all values being much smaller than in the star center \(^{(c)}\). Following , we evaluate quality using few-shot linear transfer by using pre-trained models to extract features and fitting a linear regression head mapping them to the one-hot encoding of the target labels.

The individual scaling exponents we find are \(s_{} 0.45\), \(s_{} 0.22\), and \(s_{} 0.6\). Importantly, these exponents are quite robust to the choice of the metric. As shown in Figure 5, changing the metric from ImageNet 10-shot to either 5-shot or 25-shot can change the best-fit estimate of the other exponents \(a_{k},b_{k},c_{k}\) in (2) but the scaling exponent \(s_{k}\) is relatively unchanged, since it is formed as a _ratio_ over other exponents. In addition, the data scaling exponent \(c\) appears to be independent of the choice of the shape dimension. As mentioned earlier, this is consistent with space partitioning arguments for power law scaling .

The estimated scaling exponents \(s_{k}\) point to the following picture:

1. MLP dimension should be scaled faster than depth, and depth faster than width.

Figure 4: left: Comparison between ILSRCV2012 (denoted INet-1k) 10-shot error rate predicted by Eq. (2) and actual. The value marked in violet corresponds to the star center \(^{(c)}\) that is never used when estimating scaling parameters. Eq. (2) is consistent with empirical measurements and extrapolates accurately. middle: IsoFlop curves in ViT as one varies the width dimension. right: GFLOPs is well-correlated with actual TPU core hours across models (correlation coefficient \( 0.99\)).

Figure 5: A plot of the estimated values of the exponents in (2) for different evaluation metrics \(f\). The scaling exponent \(s_{k}\) tends to be less sensitive to the choice of metric than other exponents. Moreover, the data scaling exponent \(c\) is approximately \(c 0.65.06\), independently of the choice of the shape dimension, in agreement with what would be expected using space partitioning arguments .

2. The size of ViT, as quantified by its parameter count, is scaled more slowly than the allocated compute. More precisely, for every increment in compute by a factor of \(10\), the parameter count of the optimized model shape increases by a factor of \( 2.5\).
3. As demonstrated in Figure 1, small ViT models can match the performance of much larger ones when their shape and training duration are jointly optimized for the available compute.

We validate these predictions by optimizing the shape of ViT for the compute-equivalent of ViT-g/14 when the latter is pretrained on 16 billion JFT-3B examples as done in . The resulting model, SoViT-400m/14, is significantly smaller and faster, yet equally competitive. It has a width of 1152, depth 27, and MLP dim 4304. Fine-tuning it on ImageNet results in a 90.3% top-1 accuracy, see Figure 2. Section 5 presents various other evaluations.

In Figure 6, we also optimize the shape of ViT for the compute-equivalent of ViT-B/14 pretrained on 4 billion examples of JFT-3B using Imagenet 10-shot error rate as an evaluation metric, resulting in SoViT-150m/14. It has a width of 880, depth 18, and MLP dim 2320. As shown in Figure 6, optimizing the shape of ViT leads to a significant improvement in performance, from 76.6% in ViT-B/14 to 78.5% in SoViT-150m/14 when both are trained for the same amount of compute. We also vary the optimized shape by decreasing/increasing one dimension at a time and retraining the corresponding model while keeping the total compute fixed. As shown in Figure 6, small deviations from the predicted optimal shape can lead to a notable drop in performance, especially for width since it has the smallest scaling exponent (see Figure 5). We also include in Figure 6 (left) a comparison with a model, denoted B-150m, which has the same _shape_ as ViT-B/14 but the same _size_ as SoViT-150m/14. This confirms that while optimizing the model size improves performance, optimizing the shape improves it even further.

Importantly, the model shapes in Figure 6 bear no resemblance to those observed during the star or grid sweeps. To recall, the star sweep is centered around an architecture \(^{(c)}\) whose shape dimensions are significantly larger than in ViT-B/14, whereas the grid sweep pretrains models that are substantially smaller and for only 600M examples. The ability of our strategy to accurately identify a near-optimal model shape within this context underscores its robust extrapolation capability.

### Multitask Decoder

Besides image classification, there has been a significant interest in multimodal applications, mostly fueled by the convergence across language and vision on the transformer architecture . In particular, an encoder-decoder transformer with an autoregressive decoder is a popular choice because it allows reusing pretrained image encoders. We repeat the analysis conducted in Section 4.1 to optimize the shape of the image encoder, while fixing the decoder architecture to two layers as was used in . Further details are provided in Appendix C. As an evaluation metric \(f\), we use the average of four perplexity scores: COCO captioning , OCR , VQAv2  and GQA , without normalization since they share a similar scale. For the learning rate and weight decay hyper-parameters, we conduct a sweep where we vary the learning rate in \(\{10^{-3},\,3 10^{-4},\,10^{-4}\}\) and the weight decay in \(\{3 10^{-4},\,10^{-4},\,3 10^{-5}\}\). We pick the largest learning rate and the corresponding weight decay that result in a stable training run (i.e. smooth training loss curve and gradient norms) for both the largest and smallest image encoder architectures. From this, a learning rate of \(3 10^{-4}\) and a weight decay of \(10^{-4}\) are selected.

Figure 6: left: Optimizing ViT shape for the compute-equivalent of ViT-B/14 results in SoViT-150m/14, which improves performance significantly. See Section 4.1. center & right: Impact of deviating from the optimal shape in SoViT-150m/14 (in green) while keeping compute fixed by changing the training duration such that the total FLOPs is the same in all models.

Using this analysis, the derived scaling exponents are approximately \(0.25,0.49\) and \(0.62\) for width, depth and MLP size, respectively. Hence, whereas the optimal shape dimensions in small architectures can be quite different between image classification and multitask decoding, as shown in Figure 3, the scaling exponents are nearly identical, so the same scaling recipe is used in both domains.

## 5 Evaluations

**Overview.** We now evaluate SoViT-400M in various contexts to verify whether it broadly matches ViT-g/14's performance, or only in the ILSRCV2012 10-shot metric it was optimized for. The settings we cover are few-shot, frozen linear probes on ImageNet, zero-shot transfer, image-language multitasking including captioning, OCR, and question answering, as well as panoptic segmentation. In each of these settings, we compare SoViT-400m/14 to ViT-L/16 and a ViT-g/14, all trained on the

**Compute.** Experiments are executed on Tensor Processing Units (TPU). SoViT-400m/14 is pre-trained on 40 billion examples, which amounts to 9T GFLOPs and 230K TPUv3 core-hours. ViT-g/14 was pretrained on 16 billion examples, corresponding to 9T GFLOPs and 210K TPUv3 core-hours.

### Image Classification

We verify classification performance in three common and widely useful setups: full fine-tuning, linear probes on the frozen model, and few-shot linear classification.

**Fine-tuning on ImageNet.** Pre-trained image encoders are most commonly  evaluated by fine-tuning them on the ILSVRC2012 classification task. The detailed fine-tuning settings are provided in Appendix E. One important aspect is to increase image resolution  as a way of further increasing the capacity of the pre-trained model during fine-tuning . Table 1 shows the performance of SoViT-400m/14 in comparison with ViT-L/16, ViT-g/14 fine-tuned at various resolutions, along with a few more representative models from the literature. The results confirm that SoViT-400m/14 achieves the goal of matching ViT-g/14 while being significantly smaller.

**Linear probing on ImageNet.** The quality of the pre-trained representation learned by the model is often more directly assessed by performing _linear probes_, meaning learning a linear classifier on top of unmodified, frozen output features from the model. We present results of this evaluation on the full ImageNet-1k  dataset in Table 2, including robustness evaluations of the learned

    &  &  &  &  &  &  \\    & & & & & & & & \\  L/16 & 86.7 & 90.0 & 78.5 & 88.9 & 67.8 & 63.5 \\ SoViT & 88.2 & **90.3** & 80.6 & 89.0 & 76.4 & **68.7** \\ g/14 & **88.4** & 90.2 & **80.8** & **90.3** & **76.6** & 67.7 \\   

Table 2: Linear ILSVRC2012 probes.

    &  &  &  \\   & & Input & Params & FLOPs & Val  & Real.  & v2  \\  SoViT-400m/14 & JFT-3B & 224\({}^{2}\) & 428 M & 221 G & 88.9 & 90.3 & 80.7 \\ ViT-L/16  & JFT-3B & 384\({}^{2}\) & 303 M & 383 G & 88.5 & 90.4 & 80.4 \\ SoViT-400m/14 & JFT-3B & 384\({}^{2}\) & 428 M & 672 G & 90.0 & 90.9 & 83.2 \\ ViT-g/14  & JFT-3B & 518\({}^{2}\) & 1011 M & 3208 G & 90.2 & 90.9 & - \\ SoViT-400m/14 & JFT-3B & 518\({}^{2}\) & 428 M & 1374 G & 90.3 & 91.0 & 83.4 \\ ViT-G/14  & JFT-3B & 518\({}^{2}\) & 1882 M & 5668 G & 90.4 & 90.8 & 83.3 \\  SwinV2-G  & IN-21k + 70M & 640\({}^{2}\) & 3000 M & - & 90.2 & - & 84.0 \\ CoAtNet-6  & JFT-3B & 512\({}^{2}\) & 1470 M & 1521 G & 90.4 & - & - \\ MAE\(\)WSP  & IG-3B & 518\({}^{2}\) & 1890 M & 5679 G & 89.7 & 90.9 & 83.0 \\ CoCa  & JFT-3B + ALIGN-1.8B & 576\({}^{2}\) & 2100 M & - & 91.0 & - & - \\   

Table 1: ImageNet fine-tuning. The top shows models trained in the same controlled setting, and the bottom a representative set of large well-performing models. SoViT compares favorably. Contrary to common practice, we use a held-out 2% of Train to select hyper-parameters. Selecting them on Val would increase all scores. FLOPs according to XLA; PyTorch reports MACs.

probe according to Real , ImageNet-v2 , ImageNet-Renditions , ImageNet-Adversarial , and ObjectNet  testsets. SoViT-400m/14 is generally on par with ViT-g/14 despite its smaller output width.

**Broad few-shot linear transfer.** We follow [24; 80] and evaluate a closed-form linear regression probe for 10-shot classification across a wide range of classification tasks in Table 3. Again, SoViT-400m/14 performs on-par with ViT-g/14 across the board.

### Contrastive image-text tuning

Next, we follow the locked-image text tuning (LiT) recipe  on the WebLI dataset  to add zero-shot classification abilities to the pre-trained ViT-L/16, SoViT-400m/14 and ViT-g/14 image encoders. In this setup, a new text encoder is trained using the contrastive image-text matching objective . See Appendix D for details. Table 4 (second column) shows that SoViT-400m/14 is competitive with ViT-g/14, and substantially better than ViT-L/16.

### Multitask Decoding

We also evaluate the three pretrained ViT models in multitask decoding as described in Section 4.2, where we follow the setup studied in . We fix the decoder architecture to two layers since it was found to perform well . For evaluation, we report COCO CIDEr [48; 14; 73], OCR , VQAv2  and GQA  accuracy and log-perplexity. In brief, the CIDEr score measures the similarity between a generated caption and reference captions, considering \(n\)-gram statistics, OCR evaluates optical character recognition, whereas both VQAv2 and GQA are question-answering evaluations. Results are summarized in Table 4. SoViT-400M performs on par with ViT-g/14.

### Panoptic Segmentation

Additionally, we evaluate SoViT-400m/14 on panoptic segmentation , which is a challenging dense scene understating task by closely following the setup in UViM . At a high level, UViM panoptic segmentation model consists of a visual image encoder and a decoder which maps the image representation to an intermediate code. The code is later decoded to the panoptic segmentation mask using a fixed VQVAE  model, which was pretrained on panoptic masks . In our experiments we initialize UViM's image encoder with ViT-L/16, SoViT-400m/14 and ViT-g/14.

Following , we train the UViM model using the COCO panoptic dataset (with \(512 512\) input resolution) and report the PQ metric. We achieve 43.5, 43.7 and 44.8 PQ points for ViT-L/16, SoViT-400m/14 and ViT-g/14 respectively. Our results indicate that dense segmentation tasks can be a limitation of the proposed optimal model shape, and a different model shape might be derived in this domain. We leave this investigation for future work.

### Flexifying SoViT-400m

Finally, since we do not include the patch size (sequence length) as part of the shape optimization, we verify that this is not a limitation by _flexifying_ SoViT-400m/14 on ILSVRC2012 for 300 epochs. The performance of the resulting FlexiSoViT-400m is shown in Fig 7 as green

Figure 7: Flexification of SoViT-400m/14 (abbr. So/14). See Section 5.5.

    & INet & CIFAR100 Pets & Birds & Caltech & Cars & Colorectal DTD & UC \\  &  &  &  &  &  &  &  &  &  \\  ViT-L/16 & 81.5 & 82.2 & 97.0 & 97.1 & 89.9 & 93.8 & 79.4 & 72.0 & 96.3 \\ SoViT-400m/14 & **84.1** & 86.7 & **97.6** & **88.8** & **91.3** & 93.6 & **81.5** & 72.5 & 97.7 \\ ViT-g/14 & 84.0 & **87.2** & 97.4 & 88.5 & 89.3 & **93.9** & 78.9 & **74.1** & **98.2** \\   

Table 3: SoViT-400m/14 performs competitively with ViT-g/14 in 10-shot classification.

curve when varying the patch-size at inference time. A few reference ViT models from Table 1 and  are added, confirming that SoViT-400m maintains a clear advantage. It is worth noting that flexifying does not rule out that other patch sizes could be compute-optimal. It merely demonstrates that SoViT-400M continues to perform quite well for other patch sizes when it is flexified.

## 6 Conclusion

In conclusion, we introduce an efficient method for optimizing the shape of neural architectures and successfully apply it to vision transformers. Our analysis demonstrates that smaller models, trained at their optimal architecture shape for the right amount of compute, can match much larger models.