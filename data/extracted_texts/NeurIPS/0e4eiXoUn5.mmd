# SAMoSSA: Multivariate Singular Spectrum Analysis

with Stochastic Autoregressive Noise

 Abdullah Alomar

MIT

aalomar@mit.edu

&Munther Dahleh

MIT

dahlen@mit.edu

&Sean Mann

MIT

seanmann@mit.edu

&Devavrat Shah

MIT

devavrat@mit.edu

###### Abstract

The well-established practice of time series analysis involves estimating deterministic, non-stationary _trend_ and _seasonality_ components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correlated stationary AR component, which is subsequently learned from the residual time series. We provide a finite-sample forecasting consistency bound for the proposed algorithm, SAMoSSA, which is data-driven and thus requires minimal parameter tuning. To establish theoretical guarantees, we overcome three hurdles: (i) we characterize the spectra of Page matrices of stable AR processes, thus extending the analysis of mSSA; (ii) we extend the analysis of AR process identification in the presence of arbitrary bounded perturbations; (iii) we characterize the out-of-sample or forecasting error, as opposed to solely considering model identification. Through representative empirical studies, we validate the superior performance of SAMoSSA compared to existing baselines. Notably, SAMoSSA's ability to account for AR noise structure yields improvements ranging from 5% to 37% across various benchmark datasets.

## 1 Introduction

**Background.** Multivariate time series have often been modeled as mixtures of stationary stochastic processes (e.g. AR process) and deterministic non-stationary components (e.g. polynomial and harmonics). To handle such mixtures, classical time series forecasting algorithms first attempt to estimate and then remove the non-stationary components. For example, before fitting an Autoregressive Moving-average (ARMA) model, polynomial trend and seasonal components must be estimated and then removed from the time series. Once the non-stationary components have been eliminated1, the ARMA model is learned. This estimation procedure often relies on domain knowledge and/or fine-tuning, and theoretical analysis for such multiple-stage algorithms is limited in the literature.

Prior work presented a solution for estimating non-stationary deterministic components without domain knowledge or fine-tuning using mSSA . This framework systematically models a wideclass of (deterministic) non-stationary multivariate time series as linear recurrent formulae (LRF), encompassing a wide class of spatio-temporal factor models that includes harmonics, exponentials, and polynomials. However, mSSA, both algorithmically and theoretically, does not handle additive correlated stationary noise, an important noise structure in time series analysis. Indeed, all theoretical results of mSSA are established in the noiseless setting  or under the assumption of independent and identically distributed (i.i.d.) noise .

On the other hand, every stable stationary process can be approximated as a finite order AR process . The classical OLS procedure has been shown to accurately learn finite order AR processes . However, the feasibility of identifying AR processes in the presence of a non-stationary deterministic component has not been addressed.

In summary, despite the pervasive practice of first estimating non-stationary deterministic components and then learning the stationary residual component, neither an elegant unified algorithm nor associated theoretical analyses have been put forward in the literature.

A step towards resolving these challenges is to answer the following questions: (i) Can mSSA consistently estimate non-stationary deterministic components in the presence of correlated stationary noise? (ii) Can the AR model be accurately identified using the residual time series, after removing the non-stationary deterministic components estimated by mSSA, potentially with errors? (iii) Can the out-of-sample or forecasting error of such a multi-stage algorithm be analyzed?

In this paper, we resolve all three questions in the affirmative: we present SAMoSSA, a two-stage procedure, where in the first stage, we apply mSSA on the observations to extract the non-stationary components; and in the second stage, the stationary AR process is learned using the residuals.

**Setup.** We consider the discrete time setting where we observe a multivariate time series \(Y(t)[y_{1}(t),,y_{N}(t)]^{N}\) at each time index \(t[T]\{1,,T\}\) where \(T N\)2. For each \(n[N]\), and each timestep \(t[T]\), the observations take the form

\[y_{n}(t)=f_{n}(t)+x_{n}(t),\] (1)

where \(f_{n}:^{+}\) denotes the non-stationary deterministic component, and \(x_{n}(t)\) is a stationary AR noise process of order \(p_{n}\) (\((p_{n})\)). Specifically, each \(x_{n}(t)\) is governed by

\[x_{n}(t)=_{i=1}^{p_{n}}_{ni}x_{n}(t-i)+_{n}(t),\] (2)

where \(_{n}(t)\) refers to the per-step noise, modeled as mean-zero i.i.d. random variables, and \(_{ni}\ \ i[p_{n}]\) are the parameters of the \(n\)-th \((p_{n})\) process.

**Goal.** For each \(n[N]\), our objective is threefold. The first is estimating \(f_{n}(t)\) from the noisy observations \(y_{n}(t)\) for all \(t[T]\). The second is identifying the AR process' parameters \(_{ni}\ \ i[p_{n}]\). The third is out-of-sample forecasting of \(y_{n}(t)\) for \(t>T\).

**Contributions.** The main contributions of this work is SAMoSSA, an elegant two-stage algorithm, which manages to learn both non-stationary deterministic and stationary stochastic components of the underlying time series. A detailed summary of the contributions is as follows.

_(a) Estimating non-stationary component with mSSA under AR noise._ The prior theoretical results for mSSA are established in the deterministic setting  or under the assumption of i.i.d. noise . In Theorem 4.1, we establish that the mean squared estimation error scales as \( 1/\) under AR noise - the same rate was achieved by  under i.i.d. noise. Key to this result is establishing spectral properties of the "Page" matrix of AR processes, which may be of interest in its own right (see Lemma A.2).

_(b) Estimating AR model parameters under bounded perturbation._ We bound the estimation error for the OLS estimator of AR model parameters under _arbitrary and bounded_ observation noise, which could be of independent interest. Our results build upon the recent work of  which derives similar results but _without_ any observation noise. In that sense, ours can be viewed as a robust generalization of .

_(c) Out-of-sample finite-sample consistency of SAMoSSA._ We provide a finite-sample analysis for the forecasting error of SAMoSSA- such analysis of two-stage procedures in this setup is nascent despite its ubiquitous use in practice. Particularly, we establish in Theorem 4.4 that the out-of-sample forecasting error for SAMoSSA for the \(T\) time-steps ahead scales as \(+}\) with high probability.

_(d) Empirical results._ We demonstrate superior performance of SAMoSSA using both real-world and synthetic datasets. We show that accounting for the AR noise structure, as implemented in SAMoSSA, consistently enhances the forecasting performance compared to the baseline presented in . Specifically, these enhancements range from 5% to 37% across benchmark datasets.

**Related Work.** Time series analysis is a well-developed field. We focus on two pertinent topics.

_mSSA._ Singular Spectrum Analysis (SSA), and its multivariate extension mSSA, are well-studied methods for time series analysis which have been used heavily in a wide array of problems including imputation [4; 3], forecasting [18; 2], and change point detection [22; 6]. Refer to [16; 15] for a good overview of the SSA literature. The classical SSA method consists of the following steps: (1) construct a Hankel matrix of the time series of interest; (2) apply Singular Value Decomposition (SVD) on the constructed Hankel matrix, (3) group the singular triplets to separate the different components of the time series; (4) learn a linear model for each component to forecast. mSSA is an extension of SSA which handles multiple time series simultaneously and attempts to exploit the shared structure between them . The only difference between mSSA and SSA is in the first step, where Hankel matrices of individual series are "stacked" together to create a single stacked Hankel matrix. Despite its empirical success, mSSA's classical analysis has mostly focused on identifying which time series have a low-rank Hankel representation, and defining sufficient _asymptotic_ conditions for signal extraction, i.e., when the various time series components are separable. All of the classical analysis mostly focus on the deterministic case, where no observation noise is present.

Recently, a variant of mSSA was introduced for the tasks of forecasting and imputation . This variant, which we extend in this paper, uses the Page matrix representation instead of the Hankel matrix, which enables the authors to establish finite-sample bounds on the imputation and forecasting error. However, their work assumes the observation noise to be i.i.d., and does not accommodate correlated noise structure, which is often assumed in the time series literature . Our work extends the analysis in Agarwal et al.  to observations under AR noise structure. We also extend the analysis of the forecasting error by studying how well we can learn (and forecast) the AR noise process with perturbed observations.

_Estimating AR parameters._ AR processes are ubiquitous and of interest in many fields, including time series analysis, control theory, and machine learning. In these fields, it is often the goal to estimate the parameters of an AR process from a sample trajectory. Estimation is often carried out through OLS, which is asymptotically optimal . The asymptotic analysis of the OLS estimator is well established, and recently, given the newly developed statistical tools of high dimensional probability, cf. [31; 29], various recent works have tackled its finite-time analysis as well. For example, several works established results for the finite-time identification for general first order vector AR systems [20; 19; 27; 24; 21]. Further, Gonzalez et al.  provides a finite-time bound on the deviation of the OLS estimate for general \((p)\) processes. To accommodate our setting, we extend the results of  in two ways. First, we extend the analysis to accommodate any sub-gaussian noise instead of assuming gaussianity; Second, and more importantly, we extend it to handle arbitrary bounded observation errors in the sampled trajectory, which can be of independent interest.

## 2 Model

**Deterministic Non-Stationary Component.** We adopt the spatio-temporal factor model of [3; 6] described next. The spatio-temporal factor model holds when two assumptions are satisfied: the first assumption concerns the spatial structure, i.e., the structure across the \(N\) time series \(f_{1},,f_{N}\); the second assumption pertains to the "temporal" structure. Before we describe the assumptions, we first define a key time series representation: the Page matrix.

**Definition 2.1** (Page Matrix).: _Given a time series \(f:^{+}\), and an initial time index \(t_{0}>0\), the Page matrix representation over the \(T\) entries \(f(t_{0}),,f(t_{0}+T-1)\) with parameter \(1 L T\) is given by the matrix \((f,L,T,t_{0})^{L T/L}\) with \((f,L,T,t_{0})_{ij}=f(t_{0}+i-1+(j-1) L)\) for \(i[L],\ j[ T/L]\)._In words, to construct the \(L T/L\) Page matrix of the entries \(f(t_{0}),,f(t_{0}+T-1)\), partition them into \( T/L\) segments of \(L\) contiguous entries and concatenate these segments column-wise (see Figure 1). Now we introduce the main assumptions for \(f_{1},,f_{N}\).

**Assumption 2.1** (Spatial structure).: _For each \(n[N]\), \(f_{n}(t)=_{r=1}^{R}u_{nr}w_{r}(t)\) for some \(u_{nr}\) and \(w_{r}:^{+}\)._

Assumption 2.1 posits that each time series \(f_{n}()\)\( n[N]\) can be described as a linear combination of \(R\) "fundamental" time series. The second assumption relates to the temporal structure of the fundamental time series \(w_{r}()\)\( r[R]\), which we describe next.

**Assumption 2.2** (Temporal structure).: _For each \(r[R]\) and for any \(T>1,\ 1 L T,t_{0}>0\), \(((w_{r},L,T,t_{0})) G\)._

Assumption 2.2 posits that the Page matrix of each "fundamental" time series \(w_{r}\) has finite rank. While this imposed temporal structure may seem restrictive at first, it has been shown that many standard functions that model time series dynamics satisfy this property . These include any finite sum of products of harmonics, low-degree polynomials, and exponential functions (refer to Proposition 2.1 in ).

**Stochastic Stationary Component.** We adopt the following two assumptions about the AR processes \(x_{n}\)\( n[N]\). We first assume that these AR processes are stationary (see Definition H.4 in Appendix H).

**Assumption 2.3** (Stationarity and distinct roots).: \(x_{n}(t)\) _is a stationary \((p_{n})\) process \( n[N]\). That is, let \(_{ni}\), \(i[p_{n}]\) denote the roots of \(g_{n}(z) z^{p_{n}}-_{i=1}^{p_{n}}_{ni}z^{p_{n}-i}\). Then, \(|_{ni}|<1\)\( i[p_{n}]\). Further, \( n[N]\), the roots of \(g_{n}(z)\) are distinct._

Further, we assume the per-step noise \(_{n}(t)\) are i.i.d. sub-gaussian random variables (see Definition H.1 in Appendix H).

**Assumption 2.4** (Sub-gaussian noise).: _For \(n[N],t[T]\), \(_{n}(t)\) are zero-mean i.i.d. sub-gaussian random variables with variance \(^{2}\)._

**Model Implications.** In this section, we state two important implications of the model stated above. The first is that the stacked Page matrix defined as

\[_{f}(L,T,t_{0})=[(f_{1},L,T,t_{0})(f_ {2},L,T,t_{0})(f_{n},L,T,t_{0})],\]

is low-rank. Precisely, we recall the following Proposition stated in .

**Proposition 2.1** (Proposition 2 in  ).: _Let Assumptions 2.1 and 2.2 hold. Then for any \(L\) with any \(T 1\), \(t_{0}>0\), the rank of the Page matrix \((f_{n},L,T,t_{0})\) for \(n[N]\) is at most \(R G\). Further, the rank of the stacked Page matrix \(_{f}(L,T,t_{0})\) is \(k R G\)._

Throughout, we will use the shorthand \(_{f}:=_{f}(L,T,1)\) and \(_{f}(t_{0})_{f}(L,T,t_{0})\)3. The second implication is that there exists a linear relationship between the last row of \(_{f}\) and its top \(L-1\) rows. The following proposition establishes this relationship. First, Let \([_{f}]_{L}\) denote the \(L\)-th row of \(_{f}\) and let \(^{}_{f}^{(L-1)(N T/L)}\) denote the sub-matrix that consist of the top \(L-1\) rows of \(_{f}\).

**Proposition 2.2** (Proposition 3 in  ).: _Let Assumptions 2.1 and 2.2 hold. Then, for \(L>RG\), there exists \(^{*}^{L-1}\) such that \([_{f}]_{L}^{}.=_{f}}^{}^{*}\). Further, \(\|^{*}\|_{0} RG\)._

## 3 Algorithm

The proposed algorithm provides two main functionalities. The first one is _decomposing_ the observations \(y_{n}(t)\) into an estimate of the non-stationary and stationary components for \(t T\). The second is forecasting \(y_{n}(t)\) for \(t>T\), which involves learning a forecasting model for both \(f_{n}(t)\) and \(x_{n}(t)\).

**Univariate Case**. For ease of exposition, we will first describe the algorithm for the univariate case (\(N=1\)). The algorithm has the following parameters: \(1<L\), and \(1 L\) (referto Appendix B.2 for how to choose these parameters). For clarity, we will assume, without loss of generality, that \(L\) is chosen such that \(T/L\) is an integer4. In the first step of the algorithm, we transform the observations \(y_{1}(t),t[T]\) into the \(L T/L\) Page matrix \((y_{1},L,T,1)\). We will use the shorthand \(_{y_{1}}:=(y_{1},L,T,1)\) henceforth.

_Decomposition_. We compute the SVD \(_{y_{1}}=_{=1}^{L}s_{}u_{}v_{}^{}\), where \(s_{1} s_{2} s_{L} 0\) denote its ordered singular values, and \(u_{}^{L},v_{}^{T/L}\) denote its left and right singular vectors, respectively, for \([L]\). Then, we obtain \(}_{f_{1}}\) by retaining the top \(\) singular components of \(_{y_{1}}\) (i.e., by applying Hard Singular Value Thresholding (HSVT) with threshold \(\)). That is, \(}_{f_{1}}=_{=1}^{}s_{}u_{}v_{}^ {}\).

We denote by \(_{1}(t)\) and \(_{1}(t)\) the _estimates_ of \(f_{1}(t)\) and \(x_{1}(t)\) respectively. We read off the estimates \(_{1}(t)\) directly from the matrix \(}_{f_{1}}\) using the entry that corresponds to \(t[T]\). More precisely, for \(t[T]\), \(_{1}(t)\) equals the entry of \(}_{f_{1}}\) in row \((t-1 L)+1\) and column \( t/L\), while \(_{1}(t)=y_{1}(t)-_{1}(t)\).

_Forecasting_. To forecast \(y_{1}(t)\) for \(t>T\), we produce a forecast for both \(_{1}(t)\) and \(_{1}(t)\). Both forecasts are performed through linear models (\(\) for \(_{1}(t)\) and \(_{1}\) for \(_{1}(t)\).) For \(_{1}(t)\), we first learn a linear model \(\) defined as

\[=*{argmin}_{^{L-1}}_{ m=1}^{T/L}(y_{1}(Lm)-^{}_{m})^{2},\] (3)

where \(_{m}=[_{1}(L(m-1)+1),,_{1}(L m-1)]\) for \(m[T/L]\)5 We then use \(\) and the \(L-1\) lagged observations to produce \(_{1}(t)\). That is \(_{1}(t)=^{}Y_{1}(t-1),\) where \(Y_{1}(t-1)=[y_{1}(t-1),,y_{1}(t-L)]\).

For \(_{1}(t)\), we first estimate the parameters for the \((p_{1})\) process. Specifically, define the \(p_{1}\)-dimensional vectors \(_{1}(t)[_{1}(t),,_{1}(t-p_ {1}+1)]^{6}\). Then, define the OLS estimate as,

\[_{1}=*{argmin}_{^{p_{1}}} _{t=p_{1}}^{T-1}(_{1}(t+1)-^{}_{1}(t ))\] (4)

Then, let \(_{1}(t^{})=y_{1}(t^{})-_{1}(t^{})\) for any \(t^{}<t\), 7. We then use \(_{1}\) and the \(p_{1}\) lagged entries of \(_{1}()\) to produce \(_{1}(t)\). That is \(_{1}(t)=_{1}^{}_{1}(t-1)\), where \(_{1}(t-1)=[_{1}(t-1),,_{1}(t-p_ {1})]\). Finally, produce the forecast \(_{1}(t)=_{1}(t)+_{1}(t)\).

**Multivariate Case.** The key change for the case \(N>1\) is the use of the stacked Page matrix \(_{y}\), which is the column-wise concatenation of the Page matrices induced by individual time series. Specifically, consider the stacked Page matrix \(_{y}^{L NT/L}\) defined as

\[_{y}=[(y_{1},L,T,1)(y_{2},L,T,1) (y_{n},L,T,1)]\,.\] (5)

_Decomposition_. The procedure for learning the non-stationary component \(f_{1},,f_{N}\) for \(t T\) is similar to that of the univariate case. Specifically, we perform HSVT with threshold \(\) on \(_{y}\) to produce \(}_{y}\). Then, we read off the _estimate_ of \(f_{n}(t)\) from \(}_{y}\). Specifically, for \(t[T]\), and \(n[N]\), let \(}_{f_{n}}\) refer to sub-matrix of \(}_{f}\) induced by selecting only its \([(n-1)(T/L)+1,,n T/L]\)columns. Then for \(t[T]\), \(_{n}(t)\) equals the entry of \(}_{f_{n}}\) in row \((t-1 L)+1\) and column \( t/L\). We then produce an _estimate_ for \(x_{n}(t)\) as \(_{n}(t)=y_{n}(t)-_{n}(t)\).

_Forecasting._ To forecast \(y_{n}(t)\) for \(t>T\), as in the univariate case, we learn a linear model \(\) defined as

\[=*{argmin}_{^{L-1}}_{m=1}^ {N T/L}(y_{m}-^{}_{m})^{2},\] (6)

where \(y_{m}\) is the \(m\)-th component of \([y_{1}(L),\;y_{1}(2 L),,y_{1}(T),\;y_{2}(L),,y_{2}(T),, y_{N}(T)]^{N T/L}\), and \(_{m}^{L-1}\) corresponds to the vector formed by the entries of the first \(L-1\) rows in the \(m\)th column of \(}_{f}\)8 for \(m[N T/L]\). We then use \(\) to produce \(_{n}(t)=^{}Y_{n}(t-1)\), where again \(Y_{n}(t-1)\) is the vector of the \(L-1\) lags of \(y_{n}(t-1)\). That is \(Y_{n}(t-1)=[y_{n}(t-1)) y_{n}(t-L)]\). Then, for each \(n[N]\), we estimate \(_{ni}\; i[p_{n}]\), the parameters for the \(n\)-th AR\((p_{n})\) process. Let \(_{n}\) denote the OLS estimate defined as

\[_{n}=*{argmin}_{^{p_{n}}} _{t=p_{n}}^{T-1}(_{n}(t+1)-^{}_{n}(t))^{2}.\] (7)

Then, produce a forecast for \(x_{n}\) as \(_{n}(t)=_{n}^{}_{n}(t-1),\) where again \(_{n}(t-1)=[_{n}(t-1),,_{n}(t-p_ {n})]\). Finally, produce the forecast for \(y_{n}\) as \(_{n}(t)=_{n}(t)+_{n}(t)\). For a visual depiction of the algorithm, refer to Figure 1.

## 4 Results

In this section, we provide finite-sample high probability bounds on the following quantities:

**1. Estimation error of non-stationary component.** First, we give an upper bound for the estimation error of each one of \(f_{1}(t),,f_{N}(t)\) for \(t[T]\). Specifically, we upper bound the following metric

\[(N,T,n)=_{t=1}^{T}(_{n}(t)-f_{n}(t) )^{2},\] (8)

Figure 1: A visual depiction of SAMoSSA. The algorithm five steps are (i) transform the time series into its stacked Page matrix representation; (ii) decompose time series into non-stationary and stationary components (iii) estimate \(\); (iv) estimates \(_{n}\; n[N]\); (v) produce the forecast \(_{n}(t)\) for \(t>T\).

where \(_{n}(t)\)\(n[N],t[T]\) are the estimates produced by the algorithm we proposed in Section 3.

**2. Identification of AR parameters.** Second, we analyze the accuracy of the estimates \(_{n}\)\( n[N]\) produced by the proposed algorithm. Specifically, we upper bound the following metrics

\[\|_{n}-_{n}\|_{2} n[N],\] (9)

where \(_{n}=[_{n1},,_{np_{n}}]\).

**3. Out-of-sample forecasting error.** Finally, we provide an upper bound for the forecasting error of \(y_{1}(t),,y_{N}(t)\) for \(t\{T+1,,2T\}\). Specifically, we upper bound the following metric

\[(N,T)=_{n=1}^{N}_{t=T+1}^{2T}( _{n}(t)-[y_{n}(t)\ |\ y_{n}(t-1),,y_{n}(1)])^{2},\] (10)

where \(_{n}()\)\(\)\(n[N],t\{T+1,,2T\}\) are the forecasts produced by the algorithm we propose in Section 3. Before stating the main results, we state key additional assumptions.

**Assumption 4.1** (Balanced spectra).: _Let \(_{f}(t_{0})\) denote the \(L NT/L\) stacked Page matrix associated with all \(N\) time series \(f_{1}(),,f_{N}()\) for their \(T\) consecutive entries starting from \(t_{0}\). Let \(k=(_{f}(t_{0}))\), and let \(_{k}(_{f}(t_{0}))\) denote the \(k\)-th singular value for \(_{f}(t_{0})\). Then, for any \(t_{0}>0\), \(_{f}(t_{0})\) is such that \(_{k}(_{f}(t_{0}))/\) for some absolute constant \(>0\)._

This assumption holds whenever the the non-zero singular values are "well-balanced", a standard assumption in the matrix/tensor estimation literature . Note that this assumption, as stated, ensures balanced spectra for the stacked Page matrix of _any_ set of \(T\) consecutive entries of \(f_{1}(),,f_{N}()\).

Finally, we will impose an additional necessary restriction on the complexity of the \(N\) time series \(f_{1}(t),,f_{N}(t)\) for \(t>T\) (as is done in ). Let \(^{}_{f}\) denote the \((L-1)(NT/L)\) matrix formed using the top \(L-1\) rows of \(_{f}\). Further, for any \(t_{0}[T+1]\), let \(^{}_{f}(t_{0})\) denote the \((L-1)(NT/L)\) matrix formed using the top \(L-1\) rows of \(_{f}(t_{0})\). For any matrix \(\), let \(()\) denote the subspace spanned by the its columns. We assume the following property.

**Assumption 4.2** (Subspace inclusion).: _For any \(t_{0}[T+1]\), \((^{}_{f}(t_{0}))(^{}_{f})\)._

This assumption is necessary as it requires the stacked Page matrix of the out-of-sample time series \(^{}_{f}(t_{0})\) to be only as "rich" as that of the stacked Page matrix of the "in-sample" time series \(^{}_{f}\).

### Main Results

First, recall that \(R\) is defined in Assumption 2.1, \(G\) in Assumption 2.2, \(k\) is in Proposition 2.1, while \(\) is defined in Assumption 4.1. Further, recall that \(_{ni}\) for \(i[p_{n}]\) are the roots of the characteristic polynomial of the \(n\)-th AR process, as defined in Assumption 2.3. Throughout, let \(c\) and \(C\) be absolute constants, \(f_{}_{n,t 2T}|f_{n}(t)|\), \(p_{n}p_{n}\), \(_{}=_{n}\|_{n}\|_{2}\), and \(C(f_{},)\) denote a constant that depends only (polynomially) on model parameters \(f_{}\) and \(\). Last but not least, define the key quantity

\[_{x}}{(1-_{})}\]

where \(_{}=_{i,n}|_{ni}|\) and \(c_{}\) is a constant that depends only on \(_{ni}\)9. Note that \(_{x}^{2}\) is a key quantity that we use to bound important spectral properties of AR processes, as Lemma A.2 establishes.

#### 4.1.1 Estimation Error of Non-Stationary Component

**Theorem 4.1** (Estimation error of \(f\)).: _Assume access to the observations \(y_{1}(t),,y_{N}(t)\) for \(t[T]\) as defined in (1). Let assumptions 2.1, 2.2, 2.3, 2.4 and 4.1 hold. Let \(L=\) and \(=k\), then, for any \(n[N]\), with probability of at least \(1-}\)_

\[(N,T,n) C(f_{},)^{4}GR (NT)}{}.\] (11)This theorem implies that the mean squared error of estimating \(f\) scales as \((})\)10 with high probability. The proof of Theorem 4.1 is in Appendix C.

#### 4.1.2 Identification of AR Processes

**Theorem 4.2** (AR Identification).: _Let the conditions of Theorem 4.1 hold. Let \(_{n}\) be as defined in (7), and \(_{n}=[_{n1},,_{np_{n}}]\) as defined in (2). Then, for a sufficiently large T such that \(}<}{p_{x}^{2}_{}^{2} (()}{_{}()})}\), and for any \(n[N]\) where \((N,T,n)_{}()}{6p}\), we have with probability of at least \(1-}\),_

\[\|_{n}-_{n}\|_{2}^{2}() }((()}{_{}() })+^{2}(N,T,n)(T)}{^{2} \{1,^{2}_{}()\}}).\] (12)

Recall that \(_{n}\) is estimated using \(_{n}()\), a perturbed version of the true AR process \(x_{n}()\). This perturbation comes from the estimation error of \(x_{n}()\), which is a consequence of \((N,T,n)\). Hence, it is not surprising that \((N,T,n)\) shows up in the upper bound. Indeed, the upper bound we provide here consists of two terms: the first, which scales as \(()\) is the bound one would get with access to the true AR process \(x_{n}()\), as shown in . The second term characterizes the contribution of the perturbation caused by the estimation error, and it scales as \(O((N,T,n))\), which we show is \((})\) with high probability in Theorem 4.1. Also, note that the theorem holds when the estimation error is sufficiently small (\((N,T,n)_{}()}{6p}\)). This condition ensures that the perturbation arising from the estimation error remains below the lower bound for the minimum eigenvalue of the sample covariance matrix associated with the autoregressive processes. Note that \(_{}(),_{}()\) and \(_{}()\) are quantities that relate to the parameters of the AR(\(p\)) processes and their "controllability Gramian" as we detail in Appendix A.1. The proof of Theorem 4.2 is in Appendix D.

#### 4.1.3 Out-of-sample Forecasting Error

We first establish an upper bound on the error of our estimate \(\).

**Theorem 4.3** (Model Identification (\(^{*}\))).: _Let the conditions of Theorem 4.1 hold. Let \(\) be defined as in (6), and \(^{*}\) as defined in Proposition 2.2. Then, with probability of at least \(1-}\)_

\[\|-^{*}\|_{2}^{2} C(f_{},)^{4}G^{2}R^{2}(NT)}{}\{\|^{*}\|_{1}^{2 },1\}.\] (13)

This shows that the error of estimating \(^{*}\) (in squared euclidean norm) scales as \((})\).

**Theorem 4.4**.: _Let the conditions of Theorem 4.2 and Assumption 4.2 hold. Then, with probability of at least \(1-}\)_

\[(N,T)G^{3}R^{3}p^{2}_{x}^{6}(T)}{T}+^{6}}{\{ ^{2},^{4}\}}}{},\]

_where \(c\) is an absolute constant, and \(\) denotes a constant that depends only (polynomially) on \(f_{},,_{}(),_{}(),^{*}\) and \(_{}\)._

This theorem establishes that the forecasting error for the next \(T\) time steps scales as \((+})\) with high probability. Note that the \(()\) term is a function of \(T\) only, as it is a consequence of the error incurred when we estimate each \(_{n}\). Recall that we estimate \(_{n}\) separately for \(n[N]\) using the available (perturbed) \(T\) observation of each process. The \((})\) term on the other hand is a consequence of the error incurred when learning and forecasting \(f_{1},,f_{n}\), which is done collectively across the \(N\) time series. Finally, Theorem 4.4 implies that when \(N=(T)\), the forecasting error scales as \(()\). The proof of Theorems 4.3 and 4.4 are in Appendix F and G, respectively.

## 5 Experiments

In this section, we support our theoretical results through several experiments using synthetic and real-world data. In particular, we draw the following conclusions:

1. Our results align with numerical simulations concerning the estimation of non-stationary components under AR stationary noise and the accuracy of AR parameter estimation (Section 5.1).
2. Modeling and learning the autoregressive process in SAMoSSA led to a consistent improvement over mSSA. The improvements range from 5% to 37% across standard datasets (Section 5.2).

### Model Estimation

**Setup.** We generate a synthetic multivariate time series (\(N=10\)) that is a mixture of both harmonics and an AR noise process. The AR process is stationary, and its parameter is chosen such that \(^{*}=_{n,i}|_{ni}|\) is one of three values \(\{0.3,0.6,0.95\}\). Refer to Appendix B.1 for more details about the generating process. We then evaluate the estimation error \((N,T,1)\) and the AR parameter estimation error \(\|_{1}-_{1}\|_{2}\) as we increase \(T\) from \(200\) to \(500000\).

**Results.** Figure 1(a) visualizes the mean squared estimation error of \(f_{1}\), while Figure 1(b) shows the AR parameter estimation error. The solid lines in both figures indicate the mean across ten trials, whereas the shaded areas cover the minimum and maximum error across the ten trials. We find that, as the theory suggests, the estimation error for both the non-stationary component and the AR parameter decay to zero as \(NT\) increases. We also see that the estimation error is inversely proportional to \((1-^{*})\), which is being reflected in the AR parameter estimation error as well.

### Forecasting

We showcase SAMoSSA's forecasting performance relative to standard algorithms. Notably, we compare SAMoSSA to the mSSA variant in  to highlight the value of learning the autoregressive process.

**Setup.** The forecasting ability of SAMoSSA was compared against (i) mSSA , (ii) ARIMA, a very popular classical time series prediction algorithm, (iii) Prophet , (iv) DeepAR  and (v) LSTM on three real-life datasets and one synthetic dataset containing both deterministic trend/seasonality and a stationary noise process (see details in Appendix B). Each dataset was split into train, validation, and test sets (see Appendix B.1). Each dataset has multiple time series, so we

    & Traffic & Electricity & Exchange & Synthetic \\  SAMoSSA & 0.776 & **0.829** & 0.731 & **0.476** \\ mSSA & 0.747 & 0.605 & 0.674 & 0.366 \\ ARIMA & 0.723 & -10 & **0.756** & 0.305 \\ Prophet & 0.462 & 0.197 & -10 & -0.445 \\ DeepAR & **0.824** & 0.764 & 0.579 & 0.323 \\ LSTM & 0.821 & -1.261 & -1.825 & 0.381 \\   

Table 1: Performance of algorithms on various datasets, measured by mean \(R^{2}\).

Figure 2: The error in SAMoSSA’s estimation of the non-stationary components and the AR parameters decays to zero as \(NT\) increases as the theorem suggests.

aggregate the performance of each algorithm using the mean \(R^{2}\) score. We use the \(R^{2}\) score since it is invariant to scaling and gives a reasonable baseline: a negative value indicates performance inferior to simply predicting the mean.

**Results.** In Table 1 we report the mean \(R^{2}\) for each method on each dataset. We highlight that, across all datasets, SAMoSSA consistently performs the best or is otherwise very competitive with the best. The results also underscore the significance of modeling and learning the autoregressive process in real-world datasets. Notably, learning the autoregressive model in SAMoSSA consistently led to an improvement over mSSA, with the increase in \(R^{2}\) values ranging from 5% to 37%. We note that this improvement is due to the fact that mSSA, as described in , overlooks any potential structure in the stochastic processes \(x_{1}(),,x_{N}()\) and assumes i.i.d.mean-zero noise process. While in SAMoSSA, we attempt to capture the structure of \(x_{1}(),,x_{N}()\) through the learned AR process.

## 6 Discussion and Limitations

We presented SAMoSSA, a two-stage procedure that effectively handles mixtures of deterministic non-stationary and stationary AR processes with minimal model assumptions. We analyze SAMoSSA's ability to estimate non-stationary components under stationary AR noise, the error rate of AR system identification via OLS under observation errors, and a finite-sample forecast error analysis.

We note that our results can be readily adapted to accommodate (i) _approximate low-rank_ settings (as in the model by Agarwal et al ); and (ii) scenarios with incomplete data. We do not discuss these settings to focus on our core contributions, but they represent valuable directions for future studies.

Our analysis reveals some limitations, providing avenues for future research. One limitation of our model is that it only considers stationary stochastic processes. Consequently, processes with non-stationary stochastic trends, such as a random walk, are not incorporated. Investigating the inclusion of such models and their interplay with the SSA literature is a worthy direction for future work. Second, our model assume non-interaction between the \(N\) stationary processes \(x_{1},,x_{N}\). Yet, it might be plausible to posit the existence of interactions among them, possibly through a vector AR model (VAR). Examining this setting represents another compelling direction for future work.