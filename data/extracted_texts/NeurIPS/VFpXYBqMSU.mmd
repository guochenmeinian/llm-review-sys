# Slight Corruption in Pre-training Data

Makes Better Diffusion Models

 Hao Chen\({}^{1}\)

Jujin Han\({}^{2}\)

Diganta Misra\({}^{1,3}\)

Xiang Li\({}^{1}\)

Kai Hu\({}^{1}\)

Difan Zou\({}^{2}\)

Masashi Sugiyama\({}^{4,5}\)

Jindong Wang\({}^{6}\)

Bhiksha Raj\({}^{1,7}\)

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)The University of Hong Kong,\({}^{3}\)Mila - Quebec AI Institute,

\({}^{4}\) RIKEN AIP, \({}^{5}\)The University of Tokyo, \({}^{6}\)William & Mary, \({}^{7}\)MBZUAI

haoc3@andrew.cmu.eduCorrespondence to: jwang80@wm.edu

###### Abstract

Diffusion models (DMs) have shown remarkable capabilities in generating realistic high-quality images, audios, and videos. They benefit significantly from extensive pre-training on large-scale datasets, including web-crawled data with paired data and conditions, such as image-text and image-class pairs. Despite rigorous filtering, these pre-training datasets often inevitably contain _corrupted_ pairs where _conditions_ do not accurately describe the data. This paper presents the first comprehensive study on the impact of such condition corruption in pre-training data of DMs. We synthetically corrupt ImageNet-1K and CC3M to pre-train and evaluate over \(50\) conditional DMs. Our empirical findings reveal that various types of slight corruption in pre-training can significantly enhance the quality, diversity, and fidelity of the generated images across different DMs, both during pre-training and downstream adaptation stages. Theoretically, we consider a Gaussian mixture model and prove that slight corruption in the condition leads to higher entropy and a reduced 2-Wasserstein distance to the ground truth of the data distribution generated by the corruptly trained DMs. Inspired by our analysis, we propose a simple method to improve the training of DMs on practical datasets by adding condition embedding perturbations (CEP). CEP significantly improves the performance of various DMs in both pre-training and downstream tasks. We hope that our study provides new insights into understanding the data and pre-training processes of DMs and all models are released at https://huggingface.co/DiffusionNoise.

## 1 Introduction

Recently, diffusion models (DMs) have been demonstrating unprecedented capabilities in generating high-quality, realistic, and faithful images [1; 2; 3; 4; 5], audios [6; 7], and videos . In addition, they exhibit impressive conditional generation results [9; 10; 11] when trained with classifier-free guidance . The successes of DMs are often attributed to the massive pre-training on large-scale datasets consisting of paired data and conditions [13; 14; 15; 16; 17], which also empowered and facilitated numerous downstream applications and personalization of pre-trained models, such as subject-driven generation [18; 19], controllable conditional generation [20; 21; 22], and synthetic data training [23; 24; 25].

The large-scale pre-training datasets of paired data and conditions are usually web-crawled. For example, Stable Diffusion  was pre-trained on LAION-2B , which contains billion-scale image-text pairs collected from Common Crawl . Despite the heavy filtering mechanisms used in collecting pre-training datasets [17; 28], they still inevitably contain corrupted pairs where conditions do not correctly describe or match the data, such as corrupted labels and texts [29; 30; 31; 32]. While large-scale datasets are necessary for DMs to perform well, the corruption may lead to unexpectedbehavior or generalization performance of models  during both pre-training and adaptation stages, especially for safety-critical domains such as healthcare  and autonomous driving .

Conventional wisdom may suggest that training under corrupted conditions could lead to deterioration in performance. For example, Noisy Label Learning  aims to improve the generalization of models when training with corrupted labels. Label-noise robust conditional generative adversarial nets  and DMs  have also been studied. However, these works are primarily concerned with supervised learning in downstream scenarios with assumptions of high noise ratios and the same training and testing data distributions. Due to the misalignment with large-scale self-supervised pre-training in practice on filtered datasets with relatively smaller noise ratios, the effects of corruption in pre-training can also differ from those in downstream .Understanding the effects of pre-training with such corruption is challenging and still remains largely unexplored.

In this paper, we provide the first comprehensive and practical study on condition corruption in the pre-training of DMs. Through inception analysis, we empirically, theoretically, and methodologically verify that **slight condition corruption in pre-training makes better DMs**. We pre-train over \(50\) class-conditional and text-conditional DMs using classifier-free guidance (CFG)  on ImageNet-1K (IN-1K)  and CC3M  with synthetically corrupted conditions, i.e., classes and texts, of various levels. Our study covers a wide range of DM families, including Latent Diffusion Model (LDM) , Diffusion Transformer (DiT) , and Latent Consistency Model (LCM) . Due to the known obstacles of evaluating generative models , we conduct both pre-training and downstream evaluation from the perspectives of image quality, fidelity, diversity, complexity, and memorization, to comprehensively understand the effects of pre-training corruption of DMs. More specifically, for pre-training, we directly evaluate the images generated from the pre-trained models, and for downstream adaptation, we evaluate on the images generated using personalized models with ControlNet  and T2I-Adapter  from the pre-trained ones. In addition, we theoretically investigate how slight corruption in conditional embeddings benefits the training and generative processes of DMs. Our key findings include:

* Empirically, slight corruption in pre-training facilitates the DMs to generate images with higher quality and more diversity, both qualitatively (in Fig. 1) and quantitatively (in Fig. 2).
* Theoretically, we employ a Gaussian mixture model to show slight condition corruption improves the diversity and quality of generation by increasing entropy over clean condition generation and reducing the quadratic 2-Wasserstein distance to the true data distribution (in Section 4).

Figure 1: Visualization from class and text-conditional DMs pre-trained with clean, slight, and severe condition corruption. Slight corruption in pre-training improves the quality and diversity of images.

Figure 2: (a) FID and (b) IS of DMs pre-trained on IN-1K and CC3M with various corruption. Slight corruption of various types helps DMs achieve better performance, compared to the clean ones.

* Methodologically, based on our analysis, we propose a simple method to improve the pre-training of DMs by adding conditional embedding perturbations (CEP). We show that CEP can significantly boost the performance of various DMs in both pre-training and downstream tasks (in Section 5).

Going beyond images, we do see the potential of this study in other modalities. Our efforts may also inspire future investigation on other types of corruption and bias inside pre-training datasets. We hope that our work can shed light on the future research of diffusion models and responsible AI.

## 2 Preliminary

**Denoising Diffusion Models.** DMs are probabilistic models that learn the data distribution \(()\), with \(\) denoting the observed data3, over a set of latent variables \(_{1},,_{T}\) with length \(T\)[1; 57]. It assumes a forward diffusion process, gradually adding Gaussian noise to the data with a fixed Markov chain: \(q(_{t}|)=(_{t}},(1 -_{t}))\), which can be re-parameterized as \(_{t}=_{t}}+_{t}} \) with \((,)\) and \(_{t}\) as constants produced by a noise scheduler. DMs are trained via the reverse process, inverting the forward process as: \(p_{}(_{t-1}|_{t})=(_{}(_{t}),_{}( _{t}))\), with a network that predicts the statistics of \(p_{}\). Setting \(_{}(_{t})=(1-_{t}) \) to untrained constants, the reverse process is simplified as training equally weighted denoising autoencoders \((_{t},t)\) with uniformly sampled \(t\):

\[_{}=_{, (,),t(1,T)}[\| -_{}(_{t},t )\|_{2}^{2}].\] (1)

After training, new images can be generated by sampling \(_{t-1}_{}(_{t-1}|_{t})\) starting with \((,)\).

**Classifier-free Guidance (CFG).** Extra condition information \(y\), such as class labels and text prompts, can be injected into DMs with conditional embeddings \(_{}(y)\) from modality-specific encoders  for conditional generation: \(_{}(_{t-1}|_{t},_{}(y))\). CFG  jointly learns a unconditional model \(_{}(_{t},t,_{}())\) with an empty condition \(y=\) and a conditional model \(_{}(_{t},t,_{}(y))\), and combines them linearly to control the trade-off of sample quality and diversity in generation:

\[}_{}(_{t},t,_{}(y)) =_{}(_{t},t,_{}( ))+s(_{}(_{t},t, _{}(y))-_{}(_{t},t,_{ }())),\] (2)

where \(s>1\) denotes the guidance scale. We adopt CFG by default with the training objective:

\[_{}=_{,y, (,),t(1,T)}[\| -_{}(_{t},t, _{}(y))\|_{2}^{2}].\] (3)

Condition Corruption.Ideally, each \(y\) should accurately describe and match \(\). However, in practice, due to errors from the collection of web-crawled datasets, conditions \(y^{c}\) may un-match \(\). We define \((,y^{c})\) as pairs with condition corruption, and assume that \(_{}(y^{c})=_{}(y;,)\), where \(\) denotes certain noise and \(\) denotes corruption ratio that implicitly controls the noise magnitude.

## 3 Understanding the Pre-training Corruption in Diffusion Models

In this section, we conduct the first comprehensive and practical study on pre-training DMs with condition corruption. Through holistic exploration with synthetically corrupted datasets, we reveal a surprising observation that slight pre-training corruption can be beneficial for DMs.

### Pre-training Evaluation

**Pre-training Setup**. Here, we adopt Latent Diffusion Models (LDMs)  with the pre-trained VQ-VAE [58; 56] and a down-sampling factor of 4 for the latent space of observed data \(\), denoted as LDM-4. More specifically, we train class-conditional and text-conditional LDM-4 from scratch on synthetically corrupted IN-1K  and CC3M , respectively, with a resolution of \(256 256\). We use a class embedding layer and a learnable pre-trained BERT  to compute the conditional embeddings of the IN-1K class labels and the CC3M text prompts. To introduce synthetic corruption into the conditions, we randomly flip the class label into a random class for IN-1K, and randomly swap the text of two sampled image-text pairs for CC3M, following [48; 49] (other corruption types studied in Section 3.3). We train models with different corruption ratios \(\{0,2.5,5,7.5,10,15,20\}\%\) More details on synthetic corruption and pre-training recipes are shown in Appendix B.1 and B.3.

**Evaluation of Pre-trained Models**. We directly use the pre-trained LDMs to generate images to study the effects of condition corruption in the pre-training stage. We use IN-1K class labels for class-conditional LDMs and MS-COCO text prompts  for text-conditional LDMs to generate 50K images and compare with the real validation images. The images are generated using a set of guidance scales \(s\{1.5,2.0,,10.0\}\) and DPM  scheduler with 50 steps for faster inference speed4. We adopt Frechet Inception Distance (FID) , Inception Score (IS) , Precision, and Recall  to evaluate the quality, fidelity, and coverage of the generated images. For CC3M models, we use the CLIP score (CS)  to measure the similarity of the generated images and conditional text prompts. From the perspectives of sample complexity and diversity, we compute the top-\(1\%\) Relative Mahalanobis Distance (RMD) [67; 68], calculated from the estimated class-specific and class-agnostic distributions of generated data, and the sample entropy [69; 70], calculated from the VQ-VAE codebook. We also adopt other metrics, including sFID , TopPR F1 , average \(L_{2}\) distance, and memorization ratio . More details of the metrics used are shown in Appendix B.7.

**Results**. We present the main quantitative results of pre-training in Fig. 3 and 4, and the qualitative results in Fig. 5. More results are shown in Appendix C. In summary, we found that **slight pre-training corruption5 can facilitate the quality, fidelity, and diversity of generated images**:

* Class and text-conditional models pre-trained with slight corruption achieve significantly lower FID and higher IS and CLIP score (Fig. 3(a) and 3(c)). They also present comparable and better Precision-Recall curves (Fig. 3(b) and 3(d)), compared to clean pre-trained models.
* Models pre-trained with slight corruption generate images with higher complexity and diversity, with a right-shifted density of RMD (Fig. 4(a) and 4(c)), and larger entropy (Fig. 4(b) and 4(d)).
* Qualitatively, models with slight corruption learn a more diverse distribution. Generated images present better variability in the circular walk around the latent space (Fig. 5(a) and 5(b)).

Figure 4: Quantitative evaluation of complexity and diversity of class and text-conditional LDMs. We plot the top-\(1\%\) RMD score ((a) and (c)) which measures the complexity and diversity of samples (with \(s=2.0\) and \(s=3.0\) for IN-1K and CC3M LDMs), and the sample entropy ((b) and (d)) as a proxy measure of diversity, where each point indicates the result of a guidance scale. Models pre-trained with slight condition corruption generate samples of higher complexity and diversity.

Figure 3: Quantitative evaluation of generated images from class and text-conditional LDMs pre-trained with condition corruption. All metrics are computed over \(50K\) generated images and validation images of IN-1K and MS-COCO. We plot FID vs. IS or CS ((a) and (c)), and Precision vs. Recall ((b) and (d)), where each point indicates the results computed from using a guidance scale. Models pre-trained with slight condition corruption achieve better FID, IS or CS, and PR trade-off.

[MISSING_PAGE_FAIL:5]

overlapped classes with CIFAR-100 , while maintaining others as clean. For CC3M, we prompt GPT-4  to corrupt the texts. More details of the corruption are shown in Appendix B.1.

**Diffusion Models.** LDMs utilize U-Net  as backbone and Cross-Attention for adding conditional information . We pre-train class-conditional diffusion transformers on IN-1K for extra assessment, termed DiT-XL/2 , with Transformer  as backbone and adaptive LayerNorm [83; 84; 85; 86] for conditional information. We also pre-train the recent text-conditional Latent Consistency Models (LCMs) [52; 51] on CC3M, which distill Stable Diffusion v1.5  models to enable swift inference with minimal steps, noted as LCM-v1.5. Detailed setup is shown in Appendices B.4 and B.5.

**Results**. We present the main results in Fig. 2 due to the space limit. Full results are shown in Appendix C. We find that slight condition corruption of various types universally facilitates the performance of different DMs and consistently makes them outperform the clean pre-trained ones.

## 4 Theoretical Analysis

In this section, we theoretically analyze condition corruption and find that slight corruption prevents the generated distribution from collapsing to the empirical distribution of the training data and encourages coverage of the entire data space, thereby enhancing diversity and alignment with the ground truth. We present a concise overview here and provide a comprehensive analysis in Appendix A.

**Data Distribution.** We concentrate on the prototypical problem of sampling from Gaussian mixture models (GMMs). Specifically, we consider the distribution of data \(^{d}\) that satisfies:

\[():=_{y}w_{y}(_{y}, ),\] (4)

where \(y\) denote class labels of a finite set \(y\{1,,||\}\). Given any class, \(|y\) follows a Gaussian \((_{y},)\), and \(w_{y}\) represents the weight of the Gaussian components which satisfies \(_{y}w_{y}=1\)

Figure 6: Quantitative evaluation of ControlNet and T2I-Adapter personalized class and text-conditional LDMs. FID ((a) and (c)) and IS ((b) and (d)) are computed using the 5K generated images. Slightly corrupted pre-trained models also present better performance in downstream personalization.

Figure 7: Qualitative evaluation of ControlNet and T2I-Adapter (a) IN-1K and (b) CC3M LDMs.

**Denoising Networks and Condition Corruption.** Inspired by recent works that also target on GMMs [87; 88] of DMs, we parameterize the denoising network as a piece-wise linear function:

\[_{}(_{t},y^{c})=_{k=1}^{||}_{y^{c}=k}_{t}^{k}_{t}+_{t}^{k}(y^{c}),\] (5)

where \((y^{c})\) is the one-hot encoding of corrupted label \(y^{c}\) and \(\{_{t}^{k},_{t}^{k}|_{k=1}^{||}\}\) are trainable parameters. Specifically, following a line of existing work [89; 90; 91], we adopt a simpler label-noise model by adding Gaussian perturbation to the label embedding, perturbing the clean condition \((y)\) with standard Gaussian noise \(\) to obtain \((y^{c})=(y)+\). Here, the corruption control parameter \(\) corresponds to the corruption ratio \(\) for a more direct noise magnitude control. While our theoretical framework focuses on Gaussian noise, it can also be extended to distributions such as uniform.

### Generation Diversity: Clean vs. Corrupted Conditions

We employ entropy to evaluate the diversity of generated images, following Wu et al. . Higher entropy suggests a wider spread of data, yielding greater diversity in generated images, while lower entropy implies a more concentrated distribution with less diversity. We present Theorem 1, showing the difference in entropy between generations with corrupted and clean conditions:

**Theorem 1**.: _For any class \(k\) and sufficiently large length \(T\), assuming the norm of corresponding expectation \(\|_{k}\|_{2}^{2}\) is a constant and the empirical covariance of training data is full rank, let \(_{T}\) and \(_{T}^{c}\) be the generation with clean and corrupted conditions respectively, then it holds that_

\[H(_{T}^{c}|y=k)-H(_{T}|y=k)=(^{2}d),\] (6)

_where \(\) is the corruption control parameter and \(d\) is the data dimension._

The proof is provided in Appendix A.4.1. Theorem 1 indicates that for any class \(k\), corrupted conditions enhance image diversity by increasing generation entropy. Moreover, with suitable \(\) values, image diversity can grow with noise, aligning with observations in Fig. 4.

### Generation Quality: Clean vs. Corrupted Conditions

We then analyze why corrupted conditions benefit the quality of generated images, as also observed in Section 3.1. We employ the \(2\)-Wasserstein distance as a metric to evaluate the sampling error between the true and the generated distributions, with clean and corrupted conditions. A distributed generated closer to the real data distribution indicates better image quality . In Theorem 2, we analyze the difference in the quality of data generated by corrupted DMs and clean ones:

**Theorem 2**.: _For any \(k\) and sufficiently large length \(T\), assuming the norm of corresponding expectation \(\|_{k}\|_{2}^{2}\) is constant, let \(\), \(_{}\) and \(_{}^{c}\) be the ground truth, clean, and corrupted condition distributions over training data \(\). Then if \(=O(1/n_{k}})\), it holds that_

\[_{}_{2}^{2}(,_{ })-_{2}^{2}(,_{}^{c})|y=k =d}{n_{k}},\] (7)

_where \(_{2}(,)\) denotes the \(2\)-Wasserstein distance between two distributions, \(n_{k}\) is the sample size of \(k\)-labeled dataset, and \(d\) is the data dimension._

Here the expectation is taken over the random sample of the training dataset from the data distribution. Detailed proof is shown in Appendix A.4.2. Theorem 2 reveals that for any class \(k\), small corruption yields generation distributions closer to the true distribution than clean ones. This partially verifies that the generation quality of the uncorruptly trained diffusion model can be improved by adding slight corruption to the training data. This is also well consistent with our empirical observation in Section 3.1, where the noise we used is approximately \(0.04\), close to the theoretical noise level of \(0.03\), showing that the FID of the generated images can be improved with a small corruption.

## 5 Improving Diffusion Models with Conditional Embedding Perturbation

### Method

Our previous analysis demonstrates that slight condition corruption in the pre-training could potentially benefit both the image quality and diversity of DMs, which inspires us to improve thepre-training of DMs using this conclusion. In practice, it is usually infeasible to directly corrupt the conditions in the pre-training datasets either due to their large-scale nature or difficulties to select which conditions to corrupt. Instead, we propose to add the perturbation directly to the _conditional embeddings_ of DMs, which is termed _conditional embedding perturbation (CEP)_. Compared to the fixed proportion of condition corruption in datasets we studied before, CEP adds perturbation to every data instance during training on the fly. Specifically, CEP slightly modifies the DM objective:

\[_{}=_{,y, (0,),t(1,T)}[\|-_{}(_{t},t,_{ }(y)+)\|_{2}^{2}],\] (8)

where \(\) denotes the perturbation added to conditional embeddings \(_{}(y)\). We simply set the perturbation to Uniform, i.e., \((-}, })\), or to Gaussian, i.e., \((0,})\), where the design of the factor \(}\) mainly follows previous works , \(d\) denotes the dimension of \(_{}(y)\), and \(\) controls the perturbation magnitude, mimicing the corruption ratio \(\). The main purpose of CEP is to learn better DMs with perturbation on relatively clean and heavily filtered datasets, such as CC3M and IN-1K studied in this paper, but it is also applicable to slightly corrupted datasets. Recently, Ning et al.  found that adding input perturbations (IP) to latent variables \(_{t}\) during the forward process also helps diffusion training by mitigating exposure bias . Compared to IP, CEP does not alter the marginal data distribution, but encourages the learned joint distribution to be more diverse.

### Experiments

**Setup**. We pre-trained previous class-conditional LDM-4, text-conditional LDM-4, class-conditional DiT-XL/2, and text-conditional LCM-v1.5 with CEP, and compare with IP and clean pre-trained ones. We use both Uniform and Gaussian perturbation, denoted as CEP-U and CEP-G, respectively. We set \(=1\) for all models, with an ablation study with class-conditional LDM-4 with different \(\)s. We evaluated the pre-trained class-conditional models on IN-1K and and text-conditional models on MS-COCO with FID, IS, Precision, and Recall. Additionally, we personalize the pre-trained LDMs with ControlNet on IN-100 to validate the effectiveness of CEP pre-training at downstream.

**Results**. We present the pre-training results of CEP in Table 1. CEP significantly and universally improves the performance for different class and text-conditional DMs, e.g., **2.53** and **1.25** FID improvement, and **42.31** and **10.27** IS improvement of LDM-4 and DiT-XL/2. CEP also improves precision and recall of DMs. In contrast, IP only achieves marginal improvement and yields slightly worse precision. Adopting CEP in pre-training also benefits the personalization tasks, especially for text-conditional LDMs, with FID improvement

   Model & Perturb. & FID (\(\)) & IS (\(\)) & Precision (\(\)) & Recall (\(\)) \\   LDM-4  \\ IN-1K \\ (\(s=2.0\)) \\  } & - & 9.44 & 138.46 & 0.71 & 0.43 \\  & IP & 9.18 & 141.77 & 0.67 & 0.43 \\  & CEP-U & 7.00 & 170.73 & 0.73 & **0.45** \\  & CEP-G & **6.91** & **180.77** & **0.76** & 0.44 \\   DFT-XL/2  \\ IN-1K \\ (\(s=1.75\)) \\  } & - & 6.76 & 179.67 & 0.74 & 0.46 \\  & IP & 6.75 & 182.78 & 0.75 & 0.45 \\  & CEP-U & **5.51** & **189.94** & **0.77** & 0.46 \\  & CEP-G & 5.92 & 185.21 & 0.75 & 0.45 \\   LDM-4  \\ CCM \\ (\(s=3.0\)) \\  } & - & 19.85 & 30.09 & 0.61 & 0.42 \\  & IP & 19.48 & 30.17 & 0.59 & 0.42 \\  & CEP-U & **17.93** & **30.77** & **0.55** & **0.41** \\  & CEP-G & 185.9 & 30.50 & **0.67** & 0.39 \\   LCM-v1.5  \\ CCM \\ (\(s=4.5\)) \\  } & - & 23.59 & 39.15 & 0.67 & 0.35 \\  & IP & 23.63 & 40.07 & 0.65 & 0.35 \\  & CEP-U & **22.91** & **40.31** & 0.67 & 0.35 \\  & CEP-G & **23.40** & **40.12** & **0.68** & **0.36** \\   LCM-v1.5  \\ CCM \\ (\(s=4.5\)) \\  } & - & 23.59 & 39.15 & 0.67 & 0.35 \\  & IP & 23.63 & 40.07 & 0.65 & 0.35 \\  & CEP-U & **22.91** & **40.31** & 0.67 & 0.35 \\  & CEP-G & **23.40** & **40.12** & **0.68** & **0.36** \\   

Table 1: Pre-training results of IN-1K and MS-COCO using diffusion models pre-trained with perturbation. CEP achieves the best results (in bold).

   Control & Perturb. & FID (\(\)) & IS (\(\)) & Precision (\(\)) & Recall (\(\)) \\   IN-1K \\ Canny \\ (\(s=2.25\)) \\  } & - & 11.59 & 57.01 & 0.82 & 0.61 \\  & IP & 12.31 & 57.39 & 0.77 & 0.59 \\  & CEP-U & **11.46** & **59.29** & **0.84** & 0.58 \\  & CEP-G & 11.53 & 57.59 & 0.83 & **0.61** \\   IN-1K \\ SMA \\ (\(s=2.25\)) \\  } & - & 13.74 & 54.52 & 0.79 & 0.49 \\  & IP & 13.61 & 55.13 & 0.75 & 0.48 \\  & CEP-U & **12.95** & **56.68** & 0.79 & **0.50** \\  & CEP-G & **13.34** & **56.81** & **0.80** & 0.49 \\   CCM \\ CCMV \\ (\(s=5.0\)) \\  } & - & 40.65 & 32.56 & 0.63 & 0.51 \\  & CEP-G & 10.42 & 32.43 & 0.62 & 0.52 \\  & CEP-G & **35.91** & **33.86** & **0.71** & 0.51 \\  & CEP-G & **34.57** & **36.59** & **0.68** & **0.53** \\   CCM \\ SAM \\ (\(s=4.0\)) \\  } & - & 42.64 & 32.00 & 0.63 & 0.51 \\  & CEP-G & 14.79 & 32.17 & 0.64 & 0.49 \\  & CEP-U & 80.00 & **32.98** & 0.67 & **0.51** \\   CCM \\ SAM \\ (\(s=4.0\)) \\  } & - & 23.64 & 32.00 & 0.63 & 0.51 \\  & CEP-G & 14.37 & 32.17 & 0.64 & 0.49 \\   CCM \\ SAM \\ (\(s=4.5\)) \\  } & - & 38.00 & 32.98 & 0.67 & 0.51 \\  & CEP-G & **35.02** & **35.77** & **0.67** & **0.53** \\   

Table 2: ControlNet personalization results of IN-100 using LDMs pre-trained with perturbation. CEP achieves the best results (in bold).

Figure 8: Ablation with LDM-4 IN-1K. (a) FID and average \(L_{2}\) distance of conditional embeddings against clean ones with \(=\{0.1,0.5,1.0,5.0,10.0\}\), indicated by square points (left to right). We compare with fixed synthetic corruption \(=\{2.5,5,10,15\}\%\), shown by circle points. (b) CEP on corrupted IN-1K.

of **6.08** and **7.02** for Canny and SAM spatial control, as shown in Table 2. Qualitatively, as shown in Fig. 9, images generated from DMs with CEP also look more visually appealing and realistic.

The ablation results of \(\) are shown in Fig. 8(a). We also compare the average \(L_{2}\) distance of CEP and fixed corruption against the clean condition embeddings. Interestingly, one can observe that CEP achieves a lower FID with more corruption in the embedding space (larger \(L_{2}\) from the clean ones), demonstrating its effectiveness. CEP is applicable to slightly corrupted datasets that we may often encounter in practice, as shown in Fig. 8(b), where it also facilitates the performance significantly.

In addition, we also compare the proposed CEP with traditional regularization methods, such as Dropout  and Label Smoothing , and study the effects of fixed and random perturbation during training in Appendix E.3 and Appendix E.4. The results show that CEP is more effective.

## 6 Related Work

**Diffusion Models**. Inspired by thermodynamics, DMs were first proposed by Sohl-Dickstein et al. . DMs have soon been developed into image generation with a fixed Gaussian noise diffusion process . Various techniques have then been proposed for more effective and efficient DMs . One of the most well-known is modeling the diffusion process at the latent space of pre-trained image encoders as a strong prior , instead of raw pixels spaces , which allows for high-quality image generation with affordable inference speed. Numerous foundational DMs that generate photorealistic images have thus been built . These powerful models are generally pre-trained on web-crawled billion-scale data with conditions (usually text), which may inevitably contain corruption . Recently, consistency models  were also developed from DMs, allowing generation with much fewer inference steps. These foundational DMs also enabled many downstream applications . However, the effects of the pre-training corruption on downstream applications remain unknown.

**Learning with Noise**. Learning with noise is a long-standing challenge . Noisy label learning has been widely studied in classification, from noise correction  and noise-robust loss functions . Learning with noise has also been studied in the context of generative models . Robust GANs and DMs  alleviated the quality degradation and condition misalignment of training generative models with label noise. In contrast, we study a more practical scenario, where the models are trained on corrupted pre-training data with a low noise ratio, and then adapted to downstream tasks.

In fact, more aligned with our work, there are several recent studies on exploring and exploiting the pre-training noise. Chen et al.  found that slight label noise in supervised pre-training can be beneficial for in-domain downstream tasks, whereas detrimental for out-of-domain tasks. NoisyTune , NEFTune , and SymNoise  found that introducing noise to the weights and embedding of pre-trained language models can facilitate downstream performance. Ning et al.  also found that adding perturbation in the forward diffusion process helps reduce the exposure bias of DMs . Similarly, Naderi et al.  introduced noise into the input of image translation

Figure 9: Comparison of DMs pre-trained with CEP against IP and without perturbation.

networks for better learning with limited data. Synthetic data (potentially with corruption) have also been found to be useful in pre-training . We demonstrate that slight corruption in conditions of the pre-training DMs can also be beneficial at both the pre-training and downstream.

## 7 Conclusion and Limitation

We presented the first comprehensive study on condition corruption in pre-training of DMs. Our empirical and theoretical analysis surprisingly demonstrate that slight condition corruption benefits DMs in both the pre-training and downstream adaptation, based on which we proposed CEP as a simple yet general technique that significantly improves the performance of DMs. We hope our findings could inspire more future work on understanding the pre-training data of foundation models.

This work has the following limitations. First, due to a lack of computing resources, we cannot study all types of DMs on larger datasets. Second, the theoretical analysis is based on several assumptions that might be further explored in the future. Third, the evaluation of image generation remains an open question, and we used most of the existing criteria for fair comparison.

## Disclaimer

While we study DMs for image generation in this paper, it is important to note that all generations have been selected and verified by human experts to ensure that they are responsible. Although we release all the pre-trained models under different corruption settings, it is possible that these models will generate inappropriate content due to the scale of pre-training and without alignment with human preferences. The main purpose of this research is to raise the awareness of the community on data cleaning and corruption in the research of diffusion models.

## Acknowledge

MS was supported by the Institute for AI and Beyond, UTokyo. DZ was supported by NSFC 62306252, Guangdong NSF 2024A151501244, and Hong Kong ECS awards 27309624.