# Robust Learning for Smoothed Online Convex Optimization with Feedback Delay

Pengfei Li

University of California Riverside

Riverside, CA, USA

pli081@ucr.edu &Jianyi Yang

University of California Riverside

Riverside, CA, USA

jyang239@ucr.edu &Adam Wierman

California Institute of Technology

Pasadena, CA, USA

adamw@caltech.edu &Shaolei Ren

University of California Riverside

Riverside, CA, USA

shaolei@ucr.edu

###### Abstract

We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically, we prove that RCL is able to guarantee \((1+)\)-competitiveness against any given expert for any \(>0\), while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly, RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay. We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportation as a case study.

## 1 Introduction

This paper studies _Smoothed Online Convex Optimization (SOCO)_, a model that has seen application in a wide variety of settings. The goal of SOCO is to minimize the sum of a per-round hitting cost and a switching cost that penalizes temporal changes in actions. The added (even single-step) switching cost creates substantial algorithmic challenges, and has received more than a decade of attention (see  and the references therein). While there have been various competitive online algorithms, e.g., ROBD, to guarantee the worst-case performance robustness for SOCO , their average performance is typically far from optimal due to the conservativeness needed to address potentially adversarial instances. In contrast, machine learning (ML) based optimizers can improve the average performance by exploiting rich historical data and statistical information , but they sacrifice the strong robustness in terms of provable competitive bounds needed by safety-critical applications, especially when there is a distributional shift , the ML model capacity is limited, and/or inputs are adversarial .

More recently, _ML-augmented online algorithms_ have emerged as potential game changers in classic online problems such as ski rental and caching systems . The goal is to obtain the best of both worlds by utilizing good ML predictions to improve the average performance while ensuring bounded competitive ratios even when ML predictions are arbitrarily bad. In the context of SOCO, there has been initial progress on ML-augmented algorithms in the past year . However, these studies target the simplest case of SOCO where there is no feedback delay and theswitching costs are linear. Crucially, their specific designs make it difficult, if not impossible, to apply to more general and practical settings where there is hitting cost feedback delay and multi-step nonlinear memory in the switching cost. In addition, with a few exceptions [12; 25], a common assumption in the existing SOCO studies is that the ML model is pre-trained as a black box without awareness of the downstream operation, which creates a mismatch between training and testing and degrades the average performance.

Even without ML predictions, addressing the hitting cost feedback delay and multi-step nonlinear memory is already challenging, as the agent must make decisions semi-_blindly_ without receiving the immediate hitting cost feedback and the decision at each step can affect multiple future decisions in a complex manner due to multi-step nonlinear switching costs [6; 26]. Incorporating ML predictions into the decision process adds substantial challenges, requiring novel algorithmic techniques beyond those used in the simple SOCO setting with single-step memory and no feedback delay [1; 14; 7].

**Contributions.** We propose a novel ML-augmented algorithm, called Robustness-Constrained Learning (RCL) that, for the first time, provides both robustness guarantees and good average performance for SOCO in general settings with hitting cost feedback delay and multi-step nonlinear memory in the switching cost. The foundation of RCL is to utilize an existing online algorithm (referred to as expert) as well as a novel reservation cost to hedge against future risks while closely following the ML predictions. Without having the immediate hitting cost feedback, RCL robustifies untrusted ML predictions at each step by judiciously accounting for the hitting cost uncertainties and the non-linear impact of the current decision on future switching costs. Importantly, by design, the resulting cost of RCL is no greater than \((1+)\) times the expert's cost for any \(>0\) and any problem instance, while a larger \(\) allows RCL to better explore the potential of good ML predictions.

Our main technical results provide bounds on both the worst-case and average-case performance of RCL. In particular, we prove a novel worst-case cost bound on RCL in Theorem 4.1 and a bound on the average-case performance in Theorem 4.2. Our cost bound is proven by utilizing a new reservation cost as the core of RCL. The form of the reservation cost allows us to develop a new proof approach (potentially of independent interest) that decouples the dependency of the online action on the history and constructs a new sufficient robustness constraint to bound the distance between the actions and ML predictions. Importantly, this approach enables us to account for multi-step non-linear memory in the switching cost and arbitrarily delayed hitting cost feedback in SOCO, which cannot be addressed by the existing algorithms [1; 14; 7; 12; 25]. We also provide a first-of-its-kind condition for simultaneously achieving both finite robustness and 1-consistency, which has been shown to be impossible in general . Finally, we evaluate the performance of RCL using a case study focused on battery management in electric vehicle (EV) charging stations. Our results highlight the advantage of RCL in terms of robustness guarantees compared to pure ML-based methods, as well as the benefit of training a robustification-aware ML model.

In summary, our work makes significant contributions to the growing SOCO literature. First, we propose a novel ML-augmented algorithm that provides the first worst-case cost bounds in a general SOCO setting with hitting cost feedback delay and multi-step non-linear switching costs. Our algorithm design and analysis (Theorem 4.1) are new and significantly differ from those used in simple SOCO settings [1; 14; 7; 12; 25]. Second, we provide a first sufficient condition under which finite robustness and 1-consistency are simultaneously achieved (Corollary 4.1.1). Finally, we introduce and analyze the first algorithm that allows robustification-aware training, highlighting its advantage over the commonly-assumed robustification-oblivious training in terms of the average cost.

## 2 Related Work

SOCO has been actively studied for over a decade under a wide variety of settings [2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12]. For example, designed based on classic algorithmic frameworks, expert online algorithms include online gradient descent (OGD) , online balanced descent (OBD) , regularized OBD (ROBD) [13; 5], among many others. These algorithms are judiciously designed to have bounded competitive ratios and/or regrets, but they may not perform well on typical instances due to the conservative choices necessary to optimize the worst-case performance. Assuming the knowledge of (possibly imperfect) future inputs, algorithms include standard receding horizon control (RHC)  committed horizon control (CHC) , and receding horizon gradient descent (RHGD) [29; 4]. Nonetheless, the worst-case performance is still unbounded when the inputs have large errors.

By tapping into historical data, pure ML-based online optimizers, e.g., recurrent neural networks, have been studied for online problems [30; 31; 32]. Nonetheless, even with (heuristic) techniques such as distributionally robust training and/or addition of hard training instances (e.g., adversarial samples) [17; 16], they cannot provide formal worst-case guarantees as their expert counterparts.

By combining potentially untrusted ML predictions with robust experts, ML-augmented algorithms have emerged as a promising approach [22; 24; 33; 20]. The existing ML-augmented algorithms for SOCO [25; 1; 14; 7; 15; 12] only focus on simple SOCO settings where the hitting cost is known without delays and the switching cost is linear. Extending these algorithms [25; 1; 14; 7; 15; 12] to the general SOCO setting requires substantially new designs and analysis. For example,  utilizes the simple triangle inequality for linear switching costs in the metric space to achieve robustness, whereas this inequality does not hold given (multi-step) non-linear memory in terms of squared switching costs  even when there is no feedback delay. In fact, even without considering ML predictions, the general SOCO setting with feedback delays and multi-step non-linear switching costs presents significant challenges that need new techniques [26; 6] beyond those for the simple SOCO setting. Thus, RCL makes novel contributions to the growing ML-augmented SOCO literature. Customizing ML to better suit the downstream operation to achieve a lower cost has been considered for a few online problems [34; 35; 25]. In RCL, however, we need implicit differentiation through time to customize ML by considering our novel algorithm designs in our general SOCO setting.

In online learning with expert predictions [36; 37; 38], experts are dynamically chosen with time-varying probabilities to achieve a low regret compared to the best expert in hindsight. By contrast, RCL considers a different problem setting with feedback delay and multi-step non-linear memory, and focuses on constrained learning by bounding the total cost below \((1+)\) times of the expert's cost for any instance and any \(>0\). Finally, RCL is also broadly relevant to conservative bandits and reinforcement learning . Specifically, conservative exploration focuses on unknown cost functions (and, when applicable, transition models) and uses a baseline policy to guide the exploration process. But, its design is fundamentally different in that it does not hedge against future uncertainties when choosing an action for each step. Additionally, constrained policy optimization [40; 41] focuses on constraining the _average_ cost, whereas RCL focuses on the worst-case cost constraint.

## 3 Model and Preliminaries

In a SOCO problem, an agent, a.k.a., decision maker, must select an irrevocable action \(x_{t}\) from an action space \(^{n}\) with size \(||\) at each of time \(t=1,,T\). Given the selected action, the agent incurs the sum of (i) a non-negative hitting cost \(f(x_{t},y_{t}) 0\) parameterized by the context \(y_{t}^{m}\), where \(f():^{n}_{ 0}\), and (ii) a non-negative switching cost \(d(x_{t},x_{t-p:t-1})=\|x_{t}-(x_{t-p:t-1})\|^{2}\), where the constant \(\) is added for the convenience of derivation, \(\|\|\) is the \(l_{2}\) norm by default, and \(():^{p n}^{n}\) is a (possibly non-linear) function of \(x_{t-p:t-1}=(x_{t-p},,x_{t-1})\). We make the following standard assumptions.

**Assumption 1**.: _At each \(t\), the hitting cost \(f(x_{t},y_{t})\) is non-negative, \(_{h}\)-strongly convex, and \(_{h}\)-smooth in \(x_{t}\). It is also Lipschitz continuous with respect to \(y_{t}\)._

**Assumption 2**.: _In the switching cost \(d(x_{t},x_{t-p:t-1})=\|x_{t}-(x_{t-p:t-1})\|^{2}\), the function \((x_{t-p:t-1})\) is \(L_{i}\)-Lipschitz continuous in \(x_{t-i}\) for \(i=1 p\), i.e., for any \(x_{t-i},x^{}_{t-i}\), we have \(\|(x_{t-p},,x_{t-i}, x_{t-1})-(x_{t-p},,x^{ }_{t-i}, x_{t-1})\| L_{i}\|x_{t-i}-x^{}_{t-i}\|\)._

The convexity of the hitting cost is standard in the literature and needed for competitive analysis, while smoothness (i.e., Lipschitz-continuous gradients) guarantees that bounded action differences also result in bounded cost differences . A common example of the hitting cost is \(f(x_{t},y_{t})=\|x_{t}-y_{t}\|^{2}\) as motivated by object tracking applications, where \(y_{t}\) is the online moving target [6; 26]. In the switching cost term, the previous \(p\)-step actions \(x_{t-p:t-1}\) are essentially encoded by \((x_{t-p:t-1})\). Let us take drone tracking as an example. The switching cost can be written as \(d(x_{t},x_{t-1})=\|x_{t}-x_{t-1}+C_{1}+C_{2}|x_{t-1}| x_{ t-1}\|^{2}\) and hence \((x_{t-1})=x_{t-1}-C_{1}-C_{2}|x_{t-1}| x_{t-1}\) is nonlinear, where \(x_{t}\) is the drone's speed at time \(t\) and the constants of \(C_{1}\) and \(C_{2}\) account for gravity and the aerodynamic drag . For additional examples of switching costs in other applications, readers are further referred to .

For the convenience of presentation, we use \(y=(y_{1},,y_{T})^{T}\) to denote a problem instance, while noting that the initial actions \(x_{-p+1:0}=(x_{-p+1},,x_{0})\) are also provided as an additional input. Further, for \(1 t_{1} t_{2} T\), we also rewrite \(_{=t_{1}}^{t_{2}}f(x_{},y_{})+d(x_{},x_{-p:-1})\) as \((x_{t_{1}:t_{2}})\), where we suppress the context \(y_{t}\) without ambiguity.

For online optimization, the key challenge is that the switching cost couples online actions and the hitting costs are revealed online. As in the recent literature on SOCO , we assume that the agent knows the switching cost, because it is determined endogenously by the problem itself and the agent's previous actions. The agent also knows the smoothness constant \(_{h}\), although the hitting cost function itself is revealed online subject to a delay as defined below.

### Feedback Delay

There may be feedback delay that prevents immediate observation of the context \(y_{t}\) (which is equivalent to delayed hitting cost function) [43; 6]. For example, in assisted drone landing, the context parameter \(y_{t}\) can represent the target velocity at time \(t\) sent to the drone by a control center, but the communications between the drone and control center can experience delays due to wireless channels and/or even packet losses due to adversarial jamming [26; 42].

To model the delay, we refer to \(q 0\) as the maximum feedback delay (i.e., context \(y_{t}\) can be delayed for up to \(q 0\) steps), and define \(q\)-delayed time set of arrival contexts.

**Definition 1** (\(q\)-delayed time set of arrival contexts).: _Given the maximum feedback delay of \(q 0\), for each time \(t=1,,T\), the \(q\)-delayed time set of arrival contexts contains the time indexes whose contexts are newly revealed to the agent at time \(t\) and is defined as \(_{t}^{q}\{\ |t-q t\}\) such that \(\{\ | t-q\}(_{1}^{q} _{t}^{q})\)._

Naturally, given the maximum delay of \(q 0\), we must have \(\{\ | t-q\}(_{1}^{q} _{t}^{q})\), i.e., at time \(t\), the agent must have already known the contexts \(y_{}\) for any \(=1,,t-q\).

It is worth highlighting that our definition of \(_{t}^{q}\) is flexible and applies to various delay models. Specifically, the no-delay setting corresponds to \(q=0\) and \(D_{t}^{q=0}=\{t\}\), while \(q=T\) captures the case in which the agent may potentially have to choose actions without knowing any of the contexts \(y_{1},,y_{T}\) throughout an entire problem instance. Given the maximum delay \(q 0\), the delayed contexts can be revealed to the agent in various orders different from their actual time steps, i.e., the agent may receive \(y_{t_{1}}\) earlier than \(y_{t_{2}}\) for \(t_{1}>t_{2}\). Also, the agent can receive a batch of contexts \(y_{t-q},,y_{t}\) simultaneously at time \(t\), and receive no new contexts some other time steps.

In online optimization, handling delayed cost functions, even for a single step, is challenging [6; 44; 26]. Adding ML predictions into online optimization creates further algorithmic difficulties.

### Performance Metrics

Our goal is to minimize the sum of the total hitting costs and switching costs over \(T\) time steps: \(_{x_{1}, x_{T}}_{t=1}^{T}f(x_{t},y_{t})+d(x_{t},x_{t-p:t-1})\). We formally define our two measures of interest.

**Definition 2** (Competitiveness).: _An algorithm \(ALG_{1}\) is said to be \(CR\)-competitive against another baseline algorithm \(ALG_{2}\) if \(cost(ALG_{1},y) CR cost(ALG_{2},y)\) is satisfied for any problem instance \(y^{T}\), where \(cost(ALG_{1},y)\) and \((ALG_{2},y)\) denote the total costs of \(ALG_{1}\) and \(ALG_{2}\), respectively._

**Definition 3** (Average cost).: _The average cost of an algorithm \(ALG\) is \((ALG)=_{y_{y}}[cost(ALG,y)]\), where \(cost(ALG,y)\) denotes the cost of \(ALG\) for a problem instance \(y\), and \(_{y}\) is the exogenous probability distribution of \(y=(y_{1},,y_{T})^{T}\)._

Our definition of competitiveness against a general baseline algorithm is commonly considered in the literature, e.g., . The two metrics measure an online algorithm's robustness in the worst case and expected performance in the average case, which are both important in practice.

We consider an expert (online) algorithm \(\) which chooses \(x_{}^{}\) at time \(t\) and an ML model \(h_{W}\) which, parameterized by \(W\), produces \(_{t}=h_{W}(_{t-p:t-1},\{y_{}|_{t} ^{q}\})\) at time \(t\). As in the existing ML-augmented online algorithms [7; 14], RCL chooses an actual online action \(x_{t}\) by using the two actions \(x_{t}^{}\) and \(_{t}\) as advice. In general, it is extremely challenging, if not impossible, to simultaneously optimize for both the average cost and the competitiveness. Here, given a robustness requirement \(>0\), we focus on minimizing the average cost while ensuring \((1+)\)-competitiveness against the expert \(\). Crucially, the optimal expert for our setting is iROBD, which has the best-known competitiveness against the offline optimal \(OPT\) with complete information . Thus, by using iROBD as the expert \(\), \((1+)\)-competitiveness of RCL against \(\) immediately translates into a scaled competitive ratio of \((1+) CR_{iROBD}\) against \(OPT\), where \(CR_{iROBD}\) is iROBD's competitive ratio against \(OPT\).

## 4 Rcl: The Design and Analysis

In this section, we present RCL, a novel ML-augmented that combines ML predictions (i.e., online actions produced by an ML model [1; 45; 12; 15]) with a robust expert online algorithm to the worst-case cost while leveraging the benefit of ML predictions for average performance.

### Robustness-Constrained Online Algorithm

Our goal is to "robustify" ML predictions, by which we mean that we want to ensure a robustness bound on the cost of no greater than \((1+)\) times of the expert's cost, i.e., for any problem instance \(y\), we have \((x_{1:T})(1+)(x_{1:T}^{})\), where \(>0\) is a hyperparameter indicating the level of robustness we would like to achieve. Meanwhile, we would like to utilize the benefits of ML predictions to improve the average performance.

Because of the potential feedback delays, RCL needs to choose an online action \(x_{t}\) without necessarily knowing the hitting costs of the expert's action \(x_{t}^{}\) and ML prediction \(_{t}\). Additionally, the action \(x_{t}\) can affect multiple future switching costs due to multi-step non-linear memory. Thus, it is very challenging to robustify ML predictions for the SOCO settings we consider. A simple approach one might consider is to constrain \(x_{t}\) such that the cumulative cost up to each time \(t\) is always no greater than \((1+)\) times of the expert's cumulative cost, i.e., \((x_{1:t})(1+)(x_{1:t}^{})\). However, even without feedback delays, such an approach may not even produce feasible actions for some \(t=1,,T\). We explain this by considering a single-step switching cost case. Suppose that \((x_{1:t})(1+)(x_{1:t}^{})\) is satisfied at a time \(t<T\), and we choose an action \(x_{t} x_{t}^{}\) different from the expert. Then, at time \(t+1\), let us consider a case in which the expert algorithm has such a low cost that even choosing \(x_{t+1}=x_{t+1}^{}\) will result in \((x_{1:t})+f(x_{t+1}^{},y_{t+1})+d(x_{t+1}^{},x_{t})>(1+ )[(x_{1:t}^{})+f(x_{t+1}^{},y_{t+1})+d(x_{t+1}^ {},x_{t}^{})]\). This is because the actual switching cost \(d(x_{t+1}^{},x_{t})\) can be significantly higher than the expert's switching cost \(d(x_{t+1}^{},x_{t}^{})\) due to \(x_{t} x_{t}^{}\). As a result, at time \(t+1\), it is possible that there exist no feasible actions that satisfy \((x_{1:t+1})(1+)(x_{1:t+1}^{})\). Moreover, when choosing an online action close to the ML prediction, extra caution must be exercised as the hitting costs can be revealed with delays of up to \(q\) steps.

To address these challenges, RCL introduces novel reservation costs to hedge against any possible uncertainties due to hitting cost feedback delays and multi-step non-linear memory in the switching costs. Concretely, given both the expert action \(x_{t}^{}\) and ML prediction \(_{t}\) at time \(t\), we choose \(x_{t}=_{x_{t}}\|x-_{t}\|^{2}\) by solving a constrained convex problem to project the ML prediction \(_{t}\) into a robustified action set \(x_{t}\) where \(x_{t}\) satisfies:

\[_{_{t}}f(x_{},y_{})+_{=1}^ {t}d(x_{},x_{-p:-1})+_{_{t}}H(x_{},x_{ }^{})+G(x_{t},x_{t-p:t-1},x_{t-p:t}^{})\] (1) \[ (1+)(_{_{t}}f(x_{}^{},y _{})+_{=1}^{t}d(x_{}^{},x_{-p:-1}^{})),\]

in which \(>0\) is the robustness hyperparameter, \(_{t}^{q}=(_{1}^{q}_{t}^{ q})\) and \(_{t}^{q}=\{1,,t\}_{t}\) are the sets of time indexes for which the agent knows and does not know the context parameters up to time \(t\), respectively. Most importantly, the two novel reservation costs \(H(x_{},x_{}^{})\) and \(G(x_{t},x_{t-q:t-1},x_{t-q:t}^{})\) are defined as

\[H(x_{},x_{}^{})=}{2}(1+})\|x_{}-x_{}^{}\|^{2},\] (2) \[G(x_{t},x_{t-p:t-1},x_{t-p:t}^{})=})(1+_{k=1}^{p}L_{k})}{2}_{k=1}^{p}L_{k}\|x_{t }-x_{t}^{}\|^{2}+_{i=1}^{p-k}L_{k+i}\|x_{t-i}-x_{t-i}^{}\|^{2} ,\] (3)where \(_{h}\) is the smoothness constant of the hitting cost in Assumption 1, \(L_{k}\) is the Lipschitz constant of \((x_{t-p:t-1})\) in Assumption (2), and \(_{0}(0,)\) with the optimum being \(_{0}=-1\) (Theorem 4.1). The computational complexity for projection into (1) is tolerable due to convexity.

The interpretation of \(H(x_{},x_{}^{})\) and \(G(x,x_{t-q:t-1},x_{t-q:t}^{})\) is as follows. If RCL's action \(x_{}\) deviates from the expert's action \(x_{}^{}\) at time \(\) and the hitting cost is not known yet due to delayed \(y_{}\), then it is possible that RCL actually experiences a high but unknown hitting cost. In this case, to guarantee the worst-case robustness, we include an upper bound of the cost difference as the reservation cost such that \(f(x_{},y_{})-(1+)f(x_{}^{},y_{}) H(x_{},x_{ }^{})\) regardless of the delayed \(y_{}\). If \(y_{}\) has been revealed at time \(t\) (i.e., \(_{t}\)), then we use the actual costs instead of the reservation cost. Likewise, by considering the expert's future actions as a feasible backup plan in the worst case, the reservation cost \(G(x,x_{t-p:t-1},x_{t-p:t}^{})\) upper bounds the maximum possible difference in the future switching costs (up to future \(p\) steps) due to deviating from the expert's action at time \(t\).

With \(H(x_{},x_{}^{})\) and \(G(x,x_{t-q:t-1},x_{t-q:t}^{})\) as reservation costs in (1), RCL achieves robustness by ensuring that following the expert's actions in the future is always feasible, regardless of the delayed \(y_{t}\). The online algorithm is described in Algorithm 1, where both the expert and ML model have the same online information \(\{y_{}|_{t}^{q}\}\) at time \(t\) and produce their own actions as advice to RCL.

```
0:\(>0\), \(_{0}(0,)\), initial actions \(x_{1-p:0}\), expert algorithm \(\), and ML model \(h_{W}\)
1:for\(t=1,,T\)
2: Receive a set of contexts \(\{y_{}|_{t}^{q}\}\)
3: Get the expert's action \(x_{}^{}\) given its own history
4: Get \(_{t}=h_{W}(_{t-p:t-1},\{y_{}|_{t} ^{q}\})\)
5: Choose \(x_{t}=_{x_{t}}\|x-_{t}\|^{2}\) subject to the constraint (1) //Robustification ```

**Algorithm 1** Online Optimization with RCL

### Analysis

We now present our main results on the cost bound of RCL, showing that RCL can indeed maintain the desired \((1+)\)-competitiveness against the expert \(\) while exploiting the potential of ML predictions.

**Theorem 4.1** (Cost bound).: _Consider a memory length \(p 1\) and the maximum feedback delay of \(q 0\). Given a context sequence \(y=(y_{1},,y_{T})\), let \((_{1:T})\) and \((x_{1:T}^{})\) be the costs of pure ML predictions \(_{1:T}\) and expert actions \(x_{1:T}^{}\), respectively. For any \(>0\), by optimally setting \(_{0}=-1\), the cost of RCL is upper bounded by_

\[(x_{1:T})(1+)(x_{1:T}^{}), (_{1:T})}++^{2} }{2}()}^{2},\] (4)

_where \(()=_{i=1}^{T}\|_{t}-x_{t}^{}\|^{2}--1)^{2}}{(_{h}+^{2})}_{t}^{} ^{+}\) in which \(_{t}^{}=_{_{t}^{q}}f(x_{}^{},y_{})+d(x_{t}^{},x_{t-p:t-1}^{})\) is the total of revealed hitting costs and switching cost for the expert at time \(t\), \(_{h}\) is the smoothness constant of the hitting cost (Assumption 1), and \(=1+_{i=1}^{p}L_{i}\) with \(L_{1} L_{p}\) being the Lipschitz constants in the switching cost (Assumption 2). _

Theorem 4.1 is the _first_ worst-case cost analysis for ML-augmented algorithms in a general SOCO setting with delayed hitting costs and multi-step switching costs. Its proof is available in the appendix and outlined here. We prove the competitiveness against the expert \(\) based on our novel reservation cost \(H(x_{},x_{}^{})\) and \(G(x_{t},x_{t-q:t-1},x_{t-q:t}^{})\) by induction. It is more challenging to prove the competitiveness against the ML prediction, because \(x_{t}\) implicitly depends on all the previous actions of ML predictions and expert actions up to time \(t\). To address this challenge, we utilize a novel technique by first removing the dependency of \(x_{t}\) on the history. Then, we construct a new sufficient robustness constraint that allows an explicit expression of another robustified action, whose distance to the ML prediction \(_{t}\) is an upper bound by the distance between \(x_{t}\) and \(_{t}\) due to the projection of \(_{t}\) into (1). Finally, due to the smoothness of the hitting cost function and the switching cost, the distance bound translates into the competitiveness of RCL against ML predictions.

The two terms inside the \(\) operator in Theorem 4.1 show the tradeoff between achieving better competitiveness and more closely following ML predictions. To interpret this, we note that the first term inside \(\) operator shows \((1+)\)-competitiveness of \(\) against the expert \(\), while the second term inside \(\) operator shows \(\) can also exploit the potential of good ML predictions. A smaller \(>0\) means that we want to be closer to the expert for better competitiveness, while a larger \(>0\) decreases the term \(()=_{i=1}^{T}\|_{t}-x_{t}^{}\|^{2}--1)^{2}}{(_{h}+^{2})}_{t}^{}^ {+}\) and hence makes \(\) follow the ML predictions more closely.

The term \(()\) in (4) essentially bounds the total squared distance between the actual online action and ML predictions. Intuitively, \(\) should follow the ML predictions more aggressively when the expert does not perform well. This insight is also reflected in \(()\) in Theorem 4.1. Concretely, when the expert \(\)'s total revealed \(cost_{t}^{}\) is higher, \(()\) also becomes smaller, pushing \(\) closer to ML. On the other hand, when the expert's cost is lower, \(\) stays closer to the better-performing expert for guaranteed competitiveness.

When \(\|_{t}-x_{t}^{}\|^{2}\) is larger (i.e., greater discrepancy between the expert's action and ML prediction), it is naturally more difficult to follow both the expert and ML prediction simultaneously. Thus, given a robustness requirement of \(>0\), we see from \(()\) that a larger \(\|_{t}-x_{t}^{}\|^{2}\) also increases the second term in the \(\) operator in Theorem 4.1, making it more difficult for \(\) to exploit the potential of ML predictions. Moreover, deviating from the expert's action at one step can have impacts on the switching costs in future \(p\) steps. Thus, the memory length \(p\) creates some additional friction for \(\) to achieve a low cost bound with respect to ML predictions: the greater \(p\), the greater \(=1+_{i=1}^{p}L_{i}\), and hence the greater the second term in (4).

**Robustness and consistency.** It remains to show the worst-case competitiveness against the offline optimal algorithm \(OPT\), which is typically performed for two extreme cases -- when ML predictions are extremely bad and perfect -- which are respectively referred to as _robustness_ and _consistency_ in the literature  and formally defined below.

**Definition 4** (Robustness and consistency).: _The robustness of \(\) is \(CR()\) if \(\) is \(CR()\)-competitive against \(OPT\) when the ML's competitiveness against \(OPT\) is arbitrarily large (denoted as \(\)) ; and the consistency of \(\) is \(CR(1)\) if \(\) is \(CR(1)\)-competitive against \(OPT\) when the ML's competitiveness against \(OPT\) is \(=1\)._

Robustness indicates the worst-case performance of \(\) for any possible ML predictions, whereas consistency measures the capability of \(\) to retain the performance of perfect ML predictions. In general, the tradeoff between robustness and consistency is unavoidable for online algorithms . The state-of-the-art expert algorithm iROBD recently proposed in  has the best-known competitive ratio under the assumption of identical delays for each context (i.e., \(_{t}^{q}=\{t-q\}\) -- each context is delayed by \(q\) steps). The identical-delay model essentially ignores any other contexts \(y_{}\) for \(\{\ |t-q+1 t\}\). Thus, it is the worst case of a general \(q\)-step delay setting, whose competitive ratio is upper bounded by that of iROBD. Consequently, due to \((1+)\)-competitiveness against any expert \(\) in Theorem 4.1, we immediately obtain a finite robustness bound for \(\) by considering iROBD as the expert.

Nonetheless, even for the simplest SOCO setting with no feedback delay and a switching cost of \(d(x_{t},x_{t-1})=\|x_{t}-x_{t-1}\|^{2}\), a recent study  has shown that it is _impossible_ to simultaneously achieve 1-consistency and finite robustness. Consequently, in general SOCO settings, the finite robustness of \(\) given any \(>0\) means the impossibility of achieving 1-consistency by following perfect ML predictions without further assumptions.

Despite this pessimistic result due to the fundamental challenge of SOCO, we find a sufficient condition that can overcome the impossibility, which is formalized as follows.

**Corollary 4.1.1** (1-consistency and finite robustness).: _Consider iROBD as the expert \(\), whose competitive ratio against \(OPT\) is denoted as \(CR_{iROBD}\). If the expert's switching cost always satisfies \(d(x_{t}^{},x_{t-p:t-1}^{})>0\) for any time \(t=1,,T\), then by setting \(|^{2}(^{2}+_{h})}{2}+|^{2}(^{2}+_{h})}{}}( )\) and optimally using \(_{0}=-1\), \(\) achieves \((1+) CR_{iROBD}\)-robustness and 1-consistency simultaneously. _

Corollary 4.1.1 complements the impossibility result for SOCO  by providing the first condition under which finite robustness and 1-consistency are simultaneously achievable. The intuition is that if the expert has a strictly positive switching cost no less than \(>0\) at each time, then its per-stepcost is also no less than \(\), which provides RCL with the flexibility to choose a different action than the expert's action \(x_{t}^{}\) due to the \((1+)\) cost slackness in the competitiveness requirement. Therefore, by choosing a sufficiently large but still finite \(()\), we can show that \(()=0\) in (4) in Theorem 4.1, which means RCL can completely follow the ML predictions. Without this condition, it is possible that the expert's cost is zero in the first few steps, and hence RCL must follow the expert's actions at the beginning to guarantee \((1+)\)-competitiveness in case the expert continues to have a zero cost in the future -- even though ML predictions are perfect and offline optimal, RCL cannot follow them at the beginning because of the online process and \((1+)\)-competitiveness requirement.

Importantly, our sufficient condition is not unrealistic in practice. For example, in moving object-tracking applications, the condition \(d(x_{}^{},x_{-p:-1}^{})>0\) is satisfied if the expert's action \(_{t}\) keeps changing to follow the moving object over time or alternatively, we ignore the dummy time steps with no movement.

### ML Model Training in RCL

We present the training details and highlight the advantage of training the ML model in a robustification-aware manner to reduce the average cost.

#### 4.3.1 Architecture, loss, and dataset

Because of the recursive nature of SOCO and the strong representation power of neural networks, we use a recurrent neural network with each base model parameterized by \(W\) (illustrated in Fig. 1 in the appendix). Such architectures are also common in ML-based optimizers for other online problems [17; 31]. With historical data available, we can construct a training dataset \(\) that contains a finite number of problem instances. The dataset can also be enlarged using data augmentation techniques (e.g., adding adversarial samples) [16; 17].

**Robustification-oblivious.** The existing literature on ML-augmented algorithms [22; 1; 45; 7] has commonly assumed that the ML model \(h_{W}\) is separately trained in a robustification-oblivious manner without being aware of the downstream algorithm used online. Concretely, the parameter \(W\) of a robustification-oblivious ML model is optimized for the following loss

\[W^{*}=_{W}|}_{}cost (_{1:T}),\] (5)

where \(_{t}=h_{W}(_{t-p:t-1},\{y_{}|_{t }^{q}\})\) is the ML prediction at time \(t\).

**Robustification-aware.** There is a mismatch between the actual objective \((x_{1:T})\) and the training objective \((_{1:T})\) of a robustification-oblivious ML model. To reconcile this, we propose to train the ML model in a robustification-aware manner by explicitly considering the robustification step in Algorithm 1. For notational convenience, we denote the actual action as \(x_{t}=_{}(h_{W})=_{}(_{t})\), which emphasizes the projection of \(_{t}\) into the robust action set (2). Thus, the parameter \(W\) of a robustification-aware ML model is optimized to minimize the following loss

\[^{*}=_{W}|}_{}cost(_{}(_{1:T})),\] (6)

which is different from (5) that only minimizes the cost of pre-robustification ML predictions.

#### 4.3.2 Average cost

We bound the average cost of RCL given an ML model \(h_{W}\).

**Theorem 4.2** (Average cost).: _Assume that the ML model is trained over a dataset \(\) drawn from the training distribution \(_{y}^{}\). With probability at least \(1-,(0,1)\), the average cost \(_{y}[(x_{1:T})]\) of RCL over the testing distribution \(y_{y}\) is upper bounded by_

\[()\{(1+)(), _{}(_{}(h_{W}))+ (_{}(_{}())+)}{||}})\},\]

_where \(_{}(_{}(h_{W}))=|}_{}cost(x_{1:T})\) is the empirical average cost of robustified ML predictions in \(\), \(_{}(_{}())\) defined in Definition 5 in the appendix is the Rademachercomplexity with respect to the ML model space parameterized by \(\) with robustification on the training dataset \(\), the scaling coefficient inside \(\) for \(_{}(_{}())\) is \(_{x}=}||[_{h}+(1+_{i=1} ^{p}L_{i})(1+_{i=1}^{p}L_{i})]\) with \(||\) being the size of the action space \(\) and \(_{h}\), \(L_{i}\), and \(p\) as the smoothness constant, Lipschitz constant of the nonlinear term in the switching cost, and the memory length as defined in Assumptions 1 and 2, and the coefficient for \(}}}}}}}}}}\) is \(3\) with \(\) being the upper bound of the total cost for an episode._

Theorem 4.2 bounds the average cost of \(\) by the minimum of two bounds. The first bound \((1+)()\) further highlights the guaranteed \((1+)\)-competitiveness of \(\) with respect to the expert's average cost \(AVG()\). The second bound includes a term \(_{}(_{}(h_{W}))=|}_{}cost(x_{1:T})\), which is the empirical average cost of \(\) given an ML model \(h_{W}\) and decreases when \(>0\) increases. The reason is that with a larger \(>0\), the robust action set (1) is enlarged and \(\) has more freedom to follow ML predictions due to the less stringent competitiveness constraint. Given an ML model \(h_{W}\), Theorem 4.1 shows how the upper bound on \(_{}(_{}(h_{W}))\) varies with \(\). Note that \(h_{W}\)* in (5) and \(h_{^{*}}\) in (6) minimize \(_{}(h_{W^{*}})\) and \(_{}(_{}(h_{^{*}}))\), respectively, while the post-robustication cost (i.e., \(_{}(_{}(h_{^{*}}))\)) is the actual cost of \(\). Thus, we can further reduce the average cost by using the optimal robustification-aware ML model \(h_{^{*}}\), compared to a robustification-oblivious model \(h_{W^{*}}\).

The other terms inside the second bound in (7) are related to the training dataset: the larger dataset, the smaller Rademacher complexity \(_{}(())\) and \(})}{||}}\). Note that the Rademacher complexity \(_{}(())\) of \(\) is no greater than that of using the ML model alone (i.e. \(_{}(())_{}()\), shown in the appendix). The intuition is that \(\) limits the action space for robustness. Thus, Theorem 4.1 provides the insight that, given \(>0\), robustification in \(\) is more valuable in terms of bounding the average cost when the ML model \(h_{W}\) is not well trained (e.g., due to inadequate training data). In practice, the hyperparameter \(>0\) can be set to improve the empirical average performance subject to the robustness constraint based on a held-out validation dataset along with the tuning of other hyperparameters (e.g., learning rates).

#### 4.3.3 Robustification-aware training and experimental verification

Despite the advantage in terms of the average cost, it is non-trivial to train a robustification-aware ML model using standard back-propagation. This is because the operator of projecting ML predictions into a robust set (1) is a recursive _implicit_ layer that cannot be easily differentiated as typical neural network layers. Due to space limitations, we defer to the appendix the the differentiation of the loss function with respect to ML model weights \(W\).

We validate the theoretical analysis by exploring the empirical performance of \(\) using a case study of battery management in electric vehicle (EV) charging stations . Our results are presented in the appendix. The results highlight the advantage of \(\) in terms of robustness compared to pure ML models, as well as the benefit of using a robustification-aware ML model in terms of the average cost.

## 5 Experiments

We now explore the performance of \(\) using a case study focused on battery management in electric vehicle (EV) charging stations . We first formulate the problem as an instance of COCO. More details can be found at Appendix B.1. Then, we test \(\) on a public dataset provided by ElaadNL, compared with several baseline algorithms including ROBD , EC-L2O , and HitMin (details in Appendix B.2). Our results highlight the advantage of \(\) in terms of robustness guarantees compared to pure ML models, as well as the benefit of training a robustification-aware ML model in terms of the average cost.

### Results

We now present some results for the case in which the hitting cost function (parameterized by \(y_{t}\)) is immediately known without feedback delay. The results for the case with feedback delay are presented in Appendix B.4. Throughout the discussion, the reported values are normalized with respect to those of the respective OPT. The average cost (**AVG**) and competitive ratio (**CR**) are all empirical results reported on the testing dataset.

By Theorem 4.1, there is a trade-off (governed by \(>0\)) between exploiting ML predictions for good average performance and following the expert for robustness. Here, we focus on the default setting of \(=1\) and discuss the impact of different choices of \(\) on both RCL and RCL-0 in Appendix B.3.2. As shown in Table 1, with \(=1\), both RCL and RCL-0 have a good average cost, but RCL has a lower average cost than RCL-0 and is outperformed only by ML in terms of the average cost. RCL and RCL-0 have the same competitive ratio (i.e., \((1+)\) times the competitive ratio of ROBD). Empirically, RCL has the lowest competitive ratio among all the other algorithms, demonstrating the practical power of RCL for robustifying, potentially untrusted, ML predictions. In this experiment, RCL outperforms ROBD in terms of the empirical competitive ratio because it exploits the good ML predictions for those problem instances that are adversarial to ROBD. This result complements Theorem 4.1, where we show theoretically that RCL can outperform ROBD in terms of the average cost by properly setting \(\). By comparison, ML performs well on average by exploiting the historical data, but has the highest competitive ratio due to its expected lack of robustness. The two alternative baselines, EC-L2O and HitMin, are empirically good on average and also in the worst case, but they do not have guaranteed robustness. On the other hand, ROBD is very robust, but its average cost is also the worst among all the algorithms under consideration.

More results, including using HitMin as the expert and large distributional shifts, are available in Appendix B.3.

## 6 Conclusion

We have considered a general SOCO setting (including multi-step switching costs and delayed hitting costs) and proposed RCL, which ensures a worst-case performance bound by utilizing an expert algorithm to robustify untrusted ML predictions. We prove that RCL is able to guarantee \((1+)\)-competitive against any given expert for any \(>0\). Additionally, we provide a sufficient condition that achieves finite robustness and 1-consistency simultaneously. To improve the average performance, we explicitly train the ML model in a robustification-aware manner by differentiating the robustification step, and provide an explicit average cost bound. Finally, we evaluate RCL using a case study of battery management for EV stations, which highlights the improvement in both robustness and average performance compared to existing algorithms.

**Limitations and future work.** We discuss two limitations of our work. First, we make a few assumptions (e.g., convexity and smoothness) on the hitting costs and switching costs. While they are common in the SOCO literature, these assumptions may not always hold in practice, and relaxing them will be interesting and challenging. Second, for online optimization, we only consider the ML prediction for the current step, whereas predictions for future steps can also be available in practice and may be explored for performance improvement. There are also a number of interesting open questions that follow from this work. For example, it is interesting to study alternative reservation costs and the optimality in terms of the tradeoff between bi-competitiveness. Additionally, it would also be interesting to extend RCL to other related problems such as convex body chasing and metrical task systems.

**Acknowledgement.** We would like to thank the anonymous reviewers for their helpful comments. Pengfei Li, Jianyi Yang and Shaolei Ren were supported in part by the U.S. NSF under the grant CNS-1910208. Adam Wierman was supported in part by the U.S. NSF under grants CNS-2146814, CPS-2136197, CNS-2106403, NGSDI-2105648.

    &  &  &  &  &  &  \\   & x\(\)0.6 & x\(\)1 & x\(\)5 & x\(\)5 & x\(\)6 & x\(\)1 & x\(\)2 & x\(\)5 & x\(\)5 & **RCL-0** & **RCD** & **HitMin** \\ 
**AVG** & 1.4704 & 1.1144 & 1.0531 & **1.0441** & 1.4780 & 1.2432 & 1.0855 & 1.0738 & 1.0668 & 1.1727 & 1.6048 & 1.2003 \\ 
**CR** & 1.7672 & **1.2905** & 1.4400 & 1.1004 & 2.22103 & 2.4289 & 2.4290 & 3.0322 & 3.2566 & 2.0614 & 1.7291 & 2.0865 \\   

Table 1: Competitive ratio and average cost comparison of different algorithms.