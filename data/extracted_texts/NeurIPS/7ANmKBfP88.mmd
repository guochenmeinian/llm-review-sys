# _Right_ this way: Can VLMs Guide Us to See More to Answer Questions?

Li Liu   Diji Yang   Sijia Zhong   Kalyana Suma Sree Tholeti

**Lei Ding   Yi Zhang   Leilani H. Gilpin**

University of California, Santa Cruz

{lliu112,dyang39,szhong16,ktholeti,lding25,yiz,lgilpin}@ucsc.edu

Equal contribution.Corresponding author: lgilpin@ucsc.edu.

###### Abstract

In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating "where to know" scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans. Our dataset and code are available at: https://github.com/LeoLee7/Directional_guidance.

## 1 Introduction

In recent years, Vision-Language Models (VLMs) have made significant strides in general multimodal tasks such as visual recognition and Visual Question Answering (VQA) [1; 49]. This progress has opened up a vast potential for various applications, including enhancing visual accessibility for visually impaired individuals [4; 20], supporting decision-making in autonomous systems [50; 32], enabling interactive technologies , etc. Despite these advances, VLMs still fall short of human capabilities. Humans can intuitively assess whether the available information is sufficient to answer a question and seek additional details when necessary [36; 11]. In contrast, VLMs typically tend to provide direct, single-response outputs even when information is insufficient to answer the question accurately. This limitation reduces their effectiveness in real-world applications . To address this issue, recent studies have explored ways to teach VLMs to assess information sufficiency . These studies aim to have VLMs either provide concrete answers or label questions as _unanswerable_, using benchmark datasets from real user questions like VizWiz .

However, a significant gap remains in handling _unanswerable_ cases: deciding what actions to take when VLMs identify a question to be _unanswerable_. Humans naturally possesses the ability to seek additional details when faced with unanswerable questions -- a challenge often encountered in real-world VQA tasks due to poor image quality, ambiguous questions, or loss of context [3; 9]. Tothe best of our knowledge, no existing benchmarks focused on "what to do" after the model identifies information insufficiency. This active process of information acquisition, fundamental to human cognition, has not been replicated in VLMs and remains largely unexplored.

To narrow the gap between VLMs and human intelligence, we suggest going beyond improving accuracy on answer generation or merely deciding on information sufficiency. Instead, we focus on enhancing the model's capability to provide constructive feedback when encountering _unanswerable_ questions. In response to this challenge, we introduce a novel VQA task aimed at providing Directional Guidance, which aligns with real-world needs, particularly for visually impaired individuals. As indicated in previous studies , a common issue is that many images taken by visually impaired users are ill-framed. Our task aims to guide users on how to reframe their images during the interactive VQA process. This task evaluates the model's ability to understand visual direction and determine a potential direction to obtain more relevant information.

Moreover, to empower VLM with such guiding capability, we propose an automatic VQA data augmentation framework. This framework begins by prompting a pretrained VLM to filter a set of _answerable_ questions from the given VQA dataset. The corresponding images are then perturbed using predefined rules that crop relevant visual information, making it more challenging for the model to answer the questions correctly. Finally, the VLM is fine-tuned using this augmented dataset, with the task of providing Directional Guidance on resolving the predefined perturbations. This approach simulates information inadequacy scenarios and holds promising potential for enhancing the model's ability to guide users in acquiring relevant information.

To validate the effectiveness of the approach, we contribute a manually labeled test set containing the Directional Guidance for real-world _unanswerable_ datasets with images taken by visually impaired individuals. Our experiments on three popular open-source VLMs show significant improvements in the models' performance on the Directional Guidance task after fine-tuning with our synthetic training data. Notably, the best-performing model outperforms GPT-4o (CoT)  by 3% accuracy score.

The contributions of this study are:

* **Directional Guidance task:** We define a novel VQA task. As shown in Figure 1, the proposed task assesses the model's ability to identify the information sufficiency and provide Directional Guidance when needed.
* **Directional Guidance dataset:** We create a human-labeled test set to benchmark the guidance-providing capability of VLMs.
* **Directional Guidance framework:** We propose a data-efficient framework for training models on the Directional Guidance task. This framework includes synthetic training data generation and model fine-tuning, which can be generalized with any VQA dataset with grounding information.

## 2 Related Work

**Directional Visual Understanding.** Many studies have identified that current VLMs struggle to interpret and understand spatial relationships within an input image, especially on fundamental visual concepts like relative directions [15; 40]. This ability is important for interactive VQA applications

Figure 1: The examples of the Directional Guidance task. The model utilizes self-knowledge to distinguish between known and unknown information and provides guidance on where to find more information.

like autonomous agents [18; 17], visual navigation, and assistive technologies designed for visually impaired individuals [24; 13]. To enhance VLMs' capability to understand directional relationships, researchers construct extensive training data , add assistive visual prompt [47; 30], or include collaborative VLMs to communicate and ensemble their decisions . However, these methods often require heavy data collection or introduce additional models. Previous studies have investigated visual learning through simulation , but they rely on virtual interactive environments that may not accurately reflect real-world scenarios. Another trend involves generating training data by asking questions about directional relationships in existing images [29; 27], but this also requires additional involvement of advanced models. In our study, we aim to improve the model's directional understanding with simple data augmentation methods, using images collected from real users.

**Assistive technology for visually impaired individuals.** Over the past decade, applications like VizWiz  and Be My Eyes  have used real-time video connections to enhance visual accessibility. VLMs present a more accessible and responsive solution to satisfy the user's needs, as they can provide immediate responses when given a photo-query pair. However, as noted in , visually impaired users often face challenges in capturing clear images. In real-world conditions, many images suffer from quality issues such as blurriness, obstructions, and improper exposure, making them difficult to recognize. These issues often result in divergence in human annotations [9; 3]. Moreover, even when the images are clear, the questions may still be difficult to answer due to the off-framing of the target objects . Addressing these challenges typically requires multiple rounds of queries and adjustments to properly frame the key object. For VLMs, these difficulties may be amplified because their training data typically lack examples of unrecognizable images. Additionally, to align with the multiple adjustment interaction offered by human operators in Be My Eyes applications, VLMs need to offer honest and effective guidance to navigate to target objects.

**Self-knowledge.** Self-knowledge refers to the model's ability to recognize what is known and unknown . When confronted with unanswerable questions due to ambiguity or insufficient information, VLMs/LLMs often generate hallucinated responses [26; 46]. Previous research has introduced methods to help LLMs understand limitations regarding unknowns [48; 2; 34; 41]. Subsequent studies, such as , have explored explaining unanswerability by constructing known and unknown datasets through data augmentation and refining base models with a self-curation method. For VLMs,  presents a robust visual instruction tuning dataset that includes negative instructions at different semantic levels, i.e. nonexistent object manipulation, existent object manipulation, and knowledge manipulation, all implemented by GPT-4. Although these studies validate the benefits of data augmentation, they have focused on generating negative or unknown data primarily within the language modality. Instead, our study extends this exploration into multimodal data by incorporating the vision modality.

## 3 The Cognitive Question: From What's Unknown and Where to Know

To understand how a statistical model conceptualizes the world, one effective approach is to draw an analogy to human cognition. In the meta task in NLP (i.e., Question-Answering as other core NLP tasks can be transformed into QA), human cognitive processes in problem-solving and learning are multifaceted, involving not just the retrieval of stored information but also the recognition of one's knowledge boundaries and the strategic acquisition of new knowledge. To simulate these processes, we propose a hierarchical cognitive process pattern comprising three levels:

1. **Response Generation (knowing what's known):** At the foundational level, the model utilizes its existing knowledge base and basic analysis capabilities to generate responses to queries. This process mirrors the human cognitive function of retrieving known information from memory, akin to recall or recognition tasks in cognitive psychology[28; 35]. It reflects the model's ability to combine available information into coherent answers.
2. **Awareness of Knowledge Limits (knowing what's unknown):** The second level reflects the model's metacognitive ability to evaluate its own knowledge state, recognizing when it lacks sufficient information to answer a question accurately [14; 36; 37]. This awareness is crucial for intellectual honesty and mirrors the human cognitive process of monitoring and evaluating one's understanding and capabilities, a key aspect of metacognition [51; 38; 11].
3. **Knowledge Acquisition Direction (knowing where to know the unknown):** At the most advanced level, the model identifies pathways for acquiring new knowledge when existinginformation is insufficient. This ability to seek out and engage in learning opportunities mirrors the human cognitive strategies for addressing knowledge gaps, such as identifying resources, formulating questions, or modifying learning strategies. It signifies the model's capacity for self-guided learning and adaptation, similar to strategic learning and problem-solving in human cognition.

As mentioned in Section 1, most existing works on VLMs cognitive questions are focused on the first two levels [42; 15; 5], and the third level is mostly under-explored. We argue that the challenges lie in the difficulty of collecting suitable data for benchmark and training data: there are few VQA samples that exhibit both awareness of knowledge limits and knowledge acquisition direction. Therefore, in our study, we focus on benchmark dataset curation and training data generation.

## 4 Method

### Directional Guidance Task

We define our Directional Guidance task as follows: in the context of VQA, given an image-question pair \(<I,Q>\), the model \(M\) should determine whether the image needs to be reframed. To be specific, if the target object is only partially visible and not sufficient to answer the question, the model should give clear guidance for the reframing direction (left, right, up, or down). Otherwise, the model should inform whether the question is already answerable (no need to change) or remains unanswerable even with potential reframing (none of the other options). This task mirrors real-world scenarios where visually impaired individuals need guidance to position their cameras correctly through many attempts. Although the target object might be only partially visible on each attempt, with continuous adjustments under guidance, the user can always capture a better view and finally have a better chance to get the question answered. This task goes beyond simply detecting the ill-framing issue of the image: it assesses whether the framing issue impacts the model's ability to answer the specific question posed. For example, reframing may not be necessary if the question can be directly answered with the available visual information even if the image is ill-framed. We regard these guide responses as an additional output that complements the original VQA answering process.

This task exemplifies three levels of the hierarchical cognitive pattern discussed in Section 3. Instead of a binary classification of answerable/unanswerable as proposed in , this task emphasizes the model's ability to effectively utilize available visual information. It requires the model to assess what is known and determine where to acquire extra information, standing in the transition from unanswerable to answerable.

### Directional Guidance Dataset

**Benchmark dataset.** To evaluate model performance in our task setting, we created a benchmark dataset derived from VizWiz dataset families [20; 7]. The VizWiz dataset consists of real VQA queries collected from visually impaired individuals . From this dataset, we used all the _unanswerable_ samples (1.4k) from the validation set as the training set may have potential leakage issues during the pre-training process of VLMs. We invited 26 human annotators to identify ill-framed photos and label the most promising direction to move the camera, by which the reframing action could potentially help to answer the question (more details are available in Appendix A.1). After cleaning and re-evaluation, we collected 291 samples where reframing could potentially lead to an answer, and 230 samples unlikely to be answered even with reframing. The rest samples are where the human annotators have disagreements. The details of the data collection are presented in the Supplementary Materials. To ensure a balanced distribution in the test set, we randomly selected 300 samples from the VizWiz-grounding test set and simulated the case where the current image already has sufficient information to answer the question, under the assumption that the visual evidence could be theoretically grounded in the image. Combining those three groups, we get a high-quality Directional Guidance benchmark dataset including 821 samples. Despite the size of the dataset being relatively small, this reflects the inherent challenge of the task, where ill-framed images are rare in standard VQA datasets but commonly seen in real-world scenarios. Furthermore, our dataset's diversity and comprehensiveness make it suitable for evaluating model performance on the target task, providing a valuable foundation for future studies.

**Training dataset.** We propose a data augmentation process to simulate the ill-framed samples, instead of collecting ad-hoc images that suit the task. Initially, we take all the training samples from a dataset pool - the validation set of VizWiz-grounding dataset  as it includes the visual groundings for each answerable VQA query. With that visual grounding information, manual perturbations have been applied to simulate ill-framing. Specifically, we identify the bounding box surrounding the target object and divide it into 10 zones, horizontally and vertically. We then choose a specific zone for cropping, resulting in an image that has some missing information while retaining a part of the target object. With a series of perturbations, we observe the consistency of the model's response to the initial VQA query and capture the cases where an ill-framing issue impacts the question-answering. As the VizWiz is an open-ended task, we use precision as the evaluation:

\[=|P(w) T(w)|}{_{w}|P(w)|}\] (1)

\(P(w)\) and \(T(w)\) denote a word from the model prediction and from the ground-true answer. Precision calculates how many words in the predictions also appear in the ground-truth answer, and we set a threshold \(e\) to identify the correctness. Following , only non-stop words have been taken into consideration. Figure 2 and algorithm 1 outline the process of generating training data with guidance labels.

Another crucial case in the benchmark test set involves samples that remain unanswered even after adjusting the camera. One more data argumentation technique has been placed: we mismatch the questions and images from the same dataset pool to create new pairs with different semantic information. Most questions in the original dataset pool are generic, as a highly frequent question is "What is this?" without semantic information. Correspondingly, our GPT-4 enabled argumentation helped rephrase the paired question and answer. For example, given an image \(I_{i}\) with the question \(Q_{i}\) "What is this?" and an answer \(A_{i}\) "laptop," the new question \(Q^{}_{i}\) will be rephrased to "What's the color of this laptop?." Then, we mismatch the \(Q^{}_{i}\) with another irrelevant image \(I_{j}\) to form a new pair. This augmentation generates complex, real-world queries where straightforward answers are infeasible, compelling models to learn deeper semantic information.

``` Input: A set of image-question pairs \((I,Q)\) Output: A dataset \(D\) with perturbed image-question pairs and their corresponding guidance  Initialize dataset \(D\) as empty  Define a range of perturbation magnitudes \(\) with types {left, right, up, down} foreach pair \((I,Q)\)do ifmodel\(M\) predicts \((I,Q)\) correctlythen foreach perturbation \(P\) in \(\)do \(I^{}\) Apply \(P\) to \(I\) if\(M\) still correctly predicts \((I^{},Q)\)then \(G\) 'leave it unchanged' else \(P_{}\) Reverse operation of \(P\) \(G P_{}\)  Add \((I^{},Q,G)\) to \(D\) ```

**Algorithm 1**Generate synthetic training set with data augmentation

Figure 2: The training set generation framework.

### Experiment settings

Model Selection.To verify the feasibility and effectiveness of our approach for different model architectures and sizes, we analyze the experiments of four mainstream open-source large models with different sizes, including: LLaVA-1.5 , InstructBlip , GPT-4o , and CLIP . First, we benchmark the test set on the LLaVA-1.5, InstructBlip, and GPT-4o on the zero-shot setting. A series of prompts has been designed to test their zero-shot performances, serving as our baselines. Next, we generate a training dataset using algorithm 1 and apply LoRA  fine-tuning on the open-sourced models. We anticipate the effectiveness of our proposed training framework will be reflected by the improvement of model performance compared with the zero-shot baseline.

Task format.To quantitatively analyze the model's ability to provide guidance, we format the task with a basic VQA multiple choice template: "<image>{Original_Question} To improve the image and answer the question, how should the camera be moved? A.Leave it unchanged. B.Left. C.Right. D.Up. E.Down. F.None of the other options." Each option reflects the model's decision of Directional Guidance: The leave it unchanged option indicates that the current image contains all the necessary information to answer the question. The four directional options suggest that the relevant object is only partially visible, and further image adjustment is needed. The None of the other options implies that moving the camera will not help because the question is inherently unanswerable, i.e. due to the ambiguity, or the relevant object is absent from the current image. We use the F1 score and accuracy as the evaluation metrics and also analyze the confusion matrix of the different options.

Zero-shot prompt setting.For the zero-shot baseline, we enhance the basic template with additional instructions and explanations tailored for each model. We designed two prompt settings to accommodate their varying capabilities. The first setting is a single-round query where the model makes predictions from six options directly. The second setting is a two-round prompt, following the Chain-of-Thought  process. This two-round prompt decomposes the tasks and works as follows: Initially, we prompt the model to determine if the target object is fully present in the image, partially visible, or if the question is unanswerable. The corresponding options are: leave it unchanged, reframe, and none of the other options. If the model indicates that the target object is only partially visible, we then ask it to decide a specific direction for movement: left, right, up, or down. To ensure reproducibility, we include all prompts we used in Supplementary Materials A.5.

Fine-tune setting.In our training framework, we utilize data augmentation to generate potential samples with guidance labels. We assess the consistency of the model's predictions before and after perturbations and categorize the samples into two groups. Samples where the model fails to predict post-perturbation are considered positive, and their Directional Guidance labels are assigned one of four directions: left, right, up, or down. Conversely, samples where the model maintains correct predictions are labeled as negative, with the Directional Guidance label set to leave it unchanged. Upon analyzing these groups, we observed that negative samples predominated the generated training set. To ensure a balanced distribution within the training dataset, we under-sampled the negative samples to align with the average count of the four directional categories. We also adjusted the number of None of the other options samples to achieve an even distribution across the entire training set.

After generating the training set, we format the new pairs into a standardized instruction fine-tuning layout: each sample, comprising \(<I^{},question>\), is supplemented with option choices and instructions. Since the task requires models to respond with a single letter, the prediction process is equivalent to a classification task. Following the settings in , the loss is only computed on the token for the chosen letter and the \(<eos>\) (end of the sentence) token. Also, to prevent the model from memorizing the letter distribution, we randomly shuffle the association between letters and options, ensuring each letter (from A to F) is paired with an option randomly in each training sample. When fine-tuning each model, most training configurations follow the officially suggested settings, and more training details are presented in Supplementary Materials A.2.

None-generative Models.Since the task has been simplified to a classification problem, we also investigate whether a non-generative model with a simpler architecture could suffice. Accordingly, we add a linear probe layer onto CLIP and perform a classification head, using a vision encoder (CLIP-ViT-L-336px) aligned with LLaVA-1.5 and a text encoder in the default setting. We concatenate the image feature and the text feature as the input for the classification head. Since the CLIP model can not generate open-ended answers, we use the synthetic training dataset generated by LLaVA-1.5 13b.

## 5 Results

### Directional Guidance benchmark dataset and baseline performance

Fig. 3 (a) shows the distribution of four directions in our benchmark dataset. The horizontal directions are the most common, with left at 38.5% and right at 29.6%. The figure displays four typical samples from each direction. We also identified a frequent scenario where users need to take another photo and attempt a different question, as shown in Fig. 3 (e). This pattern reveals a common challenge for visually impaired individuals: without continuous guidance, the user and assistant can easily lose the context of the original VQA. These findings emphasize the importance of providing clear, sequential dialogue-based guidance for effectively adjusting the camera position.

As mentioned in Section 4.3, we use different prompt settings for each group of models that suit their capabilities. For the 7b models, we use the two-round prompt because these models benefit from a more structured, step-by-step approach, which helps them handle the task more effectively. In contrast, we tested the LLAVA-1.5 13b and GPT-4o model with a single-round of prompting to see if they were capable of this task. The prediction results are presented in Fig. 4, from (a1) - (a4). We observe that three open-sourced models (LLaVA-1.5 7b/13b, and Instructblijn 7b) show similar behaviors: these models tend to avoid predicting the reframing cases and mistakenly categorize them

Figure 4: The heatmaps of the model’s prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. ‘O’ denotes the class leave it unchanged, and ‘X’ denotes the class none of the other options.

Figure 3: The distribution of four directions in our benchmark dataset (a) and examples (b-e). The upper caption is the Directional Guidance label and the lower caption is the original question.

as either leave it unchanged or none of the other options. With the two-round prompt, LLaVA-1.5 Tb and InstructBlip 7b make some correct predictions in the left and right categories. However, the correct and incorrect predictions are nearly balanced, with frequent misclassifications in the opposite direction. For example, the number of true left predictions is equivalent to the number of erroneous left predictions that were intended to be right. For GPT-4o, there are fewer errors in categorizing reframing cases into the wrong categories, and more cases within the reframing category are correctly predicted. However, contradictory predictions also occur frequently within the reframing cases. The result demonstrates that all the models are generally incapable of accurately predicting the reframing cases under zero-shot prompt settings. However, every baseline model performs well in the none of the other options category. We present the examples from each model in the Supplementary material A.4 to visualize the model's pretrained capability on the Directional Guidance Task.

### Model's performance after fine-tuning

We present the heat maps of the fine-tuned model predictions in Fig. 4. By comparing the prediction results between the zero-shot baselines and the fine-tuned models, we observe significant and consistent improvements in prediction performance, demonstrating the effectiveness and generalizability of our proposed method. Although there is considerable potential to improve the overall accuracy, the fine-tuned models reduce confusion between reframing, leaving it unchanged(O), and none of the other options(X). The fine-tuned models are more likely to provide directional guidance on the reframing cases. Moreover, the predictions within reframing cases show noticeable improvement, as indicated by the clear diagonal line in the heat map. Another interesting finding is the substantial reduction in wrong predictions with opposite directions (e.g., predicting an up case as down). This clarity is meaningful, as it lowers the chance of users receiving conflicting guidance, thereby enhancing safety and efficiency in real-world applications. Overall, the fine-tuned models reduce errors across all options, showing significant improvement in both cross-category and within reframing predictions.

To evaluate our training framework's sensitivity to different settings, we conducted groups of comprehensive experiments. We used three metrics to quantitatively assess the model's performance: overall F1 score, overall Accuracy, and Accuracy on the reframing cases denoted as ACC(F). The metrics for the baseline models are presented in Table 1. In some baseline experiments, we found that the zero-shot setting did not always ensure a standard output format. In such cases, we performed post-processing and excluded samples with predictions that did not fall within our options. The total number of excluded samples was fewer than 10, and this only occurred in the zero-shot baseline models. The results, as shown in Table 1, include a combination of different settings from two aspects: varying perturbation ranges and the impact of shuffling letters and options in the training data. Regarding the choice to shuffle, we observed that randomly mixing letters and options does not consistently enhance performance. For instance, with a perturbation range of 0.1-0.9, the unshuffled approach often outperformed the shuffled version, while with a perturbation range of 0.3-0.7, shuffling generally resulted in inferior performance.

The CLIP with linear probing method achieves comparable performance with the zero-shot performance of InstructBlip, but it's still not able to provide informative guidance (the accuracy for random choice for a six classification task is 16.6%). This suggests that simple CLIP-based encoders, lacking integration with a language model, may not be sufficient for this task. While the task largely relies on the model's perception of salient features, the contextual information within questions is also essential. For instance, the VizWiz dataset includes many generic questions such as 'What is the color of this?' Many of these questions are answerable even though the photos are heavily ill-framed. Since these questions do not concern spatial details, the appropriate guidance is to leave it unchanged. This underlines a key distinction between our task and other image quality detection tasks, especially those focusing on ill-framing solely on image modality.

## 6 Discussion

In this section, we analyze the effect of different settings, including perturbation range and shuffling operations, on the generation of training data. A detailed heatmap of the model's predictions is presented in Supplementary Materials with Figure 6. The perturbation range determines the crop ratios used to generate the training samples, ranging from 0.1 (minimal crop) to 0.9 (maximum crop). We observed that positive Guidance samples tend to cluster at high crop ratios, while lower ratios often correspond to negative samples (where the Guidance is leave it unchanged). Therefore, a range of 0.3-0.7 leads to a more balanced selection of training data, while a range of 0.1-0.9 provides a more comprehensive and varied dataset. This setting can affect the model's performance due to the balance between the diversity and complexity of the generated training samples, and the trade-off works as follows: when the perturbation becomes more severe (e.g., at a ratio of 0.9), images are aggressively corrupted. This increases the chance of obtaining positive Guidance samples, as the model is more likely to fail in predicting these heavily perturbed samples, which it could have predicted accurately without perturbation. However, it also results in significant information loss and greater challenges in detecting objects, making it a harder sample to learn. Conversely, a moderate perturbation ratio results in less aggressive cropping, allowing the model to access more information and better respond to the original question. However, this can lead to fewer positive Guidance samples, as the perturbation does not sufficiently challenge the predictions. The differences with the shuffling settings could be attributed to the regularization effect, which prevents the model from memorizing fixed patterns and increases training difficulty, especially when training data is scarce. In scenarios with less training data, shuffling acts as a form of data augmentation, increasing the diversity of training examples and making the model more robust. However, shuffling might add unnecessary complexity to a larger training dataset derived from a wider perturbation range, making it harder for the model to learn effectively. In such cases, the unshuffled approach allows the model to quickly identify and leverage consistent patterns, facilitating faster and more efficient learning processes.

Ablation StudyTo gain a more comprehensive understanding of how different perturbation ranges affect model performance, we conducted a more fine-grained ablation study with LLaVA1.5-7b. Specifically, we evaluated perturbation ranges of 0.1-0.3, 0.3-0.5, 0.5-0.7, and 0.7-0.9, alongside our main experiments of 0.1-0.9 and 0.3-0.7. The results, presented in Table 2, reveal that as the perturbation range increases from 0.1-0.3 to 0.5-0.7, both overall F1 scores and overall accuracy show substantial improvements, stabilizing around 0.49. However, at the highest perturbation range of 0.7-0.9, we observe a slight decrease in the ACC(F) metric, suggesting that overly aggressive perturbations may introduce excessive complexity, hindering the model's ability to accurately identify relevant objects. Notably, the broader range of 0.1-0.9 achieves the highest overall F1 and accuracy scores, suggesting that a wide perturbation range strikes an effective balance between data diversity and sample complexity. Additionally, all perturbation ranges demonstrate improvements in ACC(F), with enhanced reframing direction performance as perturbation increases, except at the highest range. These findings support our initial discussion by emphasizing the trade-off between data diversity and the complexity of perturbed samples. Future work could explore optimized strategies for selecting perturbation ranges, potentially employing dynamic or adaptive methods to further improve model performance based on specific dataset characteristics.

   &  &  &  \\  Perturbation range &  &  &  &  &  \\  Metrics & F1 & ACC & ACC(F) & F1 & ACC & ACC(F) & F1 & ACC & ACC(F) & F1 & ACC & ACC(F) & F1 & ACC & ACC(F) \\   & CLIP-linear probe & 0.30 & 0.31 & 0.19 & 0.32 & 0.32 & 0.18 & 
  
**Perturbation Range** & **Overall F1** & **Overall Accuracy** & **ACC(F)** \\ 
0.1-0.3 & 0.19 & 0.24 & 0.31 \\
0.3-0.5 & 0.49 & 0.49 & 0.38 \\
0.5-0.7 & 0.49 & 0.49 & 0.43 \\
0.7-0.9 & 0.50 & 0.49 & 0.40 \\   

Table 2: Model performance under different perturbation ranges with LLaVA-1.5 7b.

Acknowledgment

We sincerely appreciate all collaborators who contributed to this project for their invaluable insights and support. Special thanks to our human labelers for their diligent efforts in creating the benchmark dataset. We are also grateful to Prof. David Lee and Prof. James Davis for their expert guidance, constructive feedback, and encouragement throughout this research.

We thank the VizWiz community for providing an inspiring platform that significantly influenced our work. We acknowledge the use of the VizWiz VQA and VizWiz Grounding datasets(both under CC-BY 4.0)). Our model development was based on LLAVA (Apache-2.0 license) and InstructBLIP (in compliance with the license terms of LLAMA and Vicuna).

This material is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-24-1-0149.