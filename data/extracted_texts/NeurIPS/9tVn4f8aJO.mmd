# HEMM: Holistic Evaluation of

Multimodal Foundation Models

 Paul Pu Liang1, Akshay Goindani1, Talha Chafekar, Leena Mathur, Haofei Yu,

**Ruslan Salakhutdinov, Louis-Philippe Morency**

Machine Learning Department and Language Technologies Institute

Carnegie Mellon University

[https://github.com/pliang279/HEMM](https://github.com/pliang279/HEMM)

These authors contributed equally to this work

###### Abstract

Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. _Basic multimodal skills_ are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. _Information flow_ studies how multimodal content changes during a task through querying, translation, editing, and fusion. _Use cases_ span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify _dataset dimensions_ (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different _modeling dimensions_ (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.

## 1 Introduction

Building upon rapid progress in large-scale language and vision pretraining , the new generation of multimodal foundation models is increasing adept at learning interactions between modalities , enables both static prediction and dynamic interaction , and even shows emergent properties never seen before in pretraining corpora . Previous standards for benchmarking multimodal models based on collections of modality and task-specific datasets  are increasingly insufficient in light of these general capabilities. In order to study fundamental questions regarding _why_ multimodal foundation models exhibit certain behaviors, _when_ they perform well in the real world, and _which_ modeling paradigms are most effective, there is a need for a holistic evaluation scheme beyond individual datasets or contexts.

To address this need, we contribute **Holistic Evaluation of Multimodal Models (HEMM)**, visualized in Figure 1. HEMM, as an evaluation framework, goes beyond conventional lists of datasets to emphasize holistic benchmarking at three levels. The first level benchmarks _basic multimodal skills_: fundamental internal abilities required to address multimodal problems, such as interactions betweenredundant, unique, and synergistic features [26; 68], alignment of fine-grained and coarse-grained information , reasoning across compositional features , and integration of external knowledge . The second level benchmarks _information flow_: how multimodal information transforms during tasks such as querying , translation , editing , and fusion . The third level benchmarks _multimodal use cases_: how models perform in real-world challenges across domains, including multimedia, affective computing, natural sciences, healthcare, and human-computer interaction (HCI). Together, these three levels taxonomize a wide spectrum of \(30\) image-text datasets, enabling HEMM to serve as a holistic framework to evaluate multimodal models.

To aid in HEMM evaluation, we also present a new categorization of models spanning key _modeling decisions_, such as model size and modality processing (e.g., interleaved inputs), and _training decisions_, such as pretraining and fine-tuning objectives. We (1) identify key _dataset dimensions_ (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different _modeling and training decisions_ (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence downstream task performance. Our analysis yields tangible directions for future work, including challenging multimodal skills, tasks, and use cases, impacts of diversity and scale, and guidelines on modeling architectures and training objectives. HEMM is publicly available at anon, and encourages community involvement in its expansion of datasets, annotations, models, and evaluation metrics.

## 2 Key Benchmarking Principles and Datasets in HEMM

HEMM includes 30 datasets summarized in Table 1. These datasets require different multimodal skills to solve, display different types of multimodal information flow, and belong to different real-world use cases with domain-specific challenges.

### Basic multimodal skills

Multimodal skills are internal abilities required to solve multimodal tasks, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and using external knowledge.

**Multimodal interactions** study how modality information is integrated for a multimodal task [69; 77; 52; 9], which can be _redundant_: shared between modalities, such as smiling while telling a humorous joke [43; 89], _unique_: present in only one of the modalities [35; 54], and _synergistic_: emergence of new information from both modalities, such as conveying sarcasm through conflicting verbal and nonverbal cues [15; 68]. Datasets with high referential information between modalities test for redundancy, such as in VQA, and translation on NoCaps. Tasks with uniqueness or synergy include understanding movie posters (MM-IMDb), memes (MemCap), figurative language (IRFL), facial expressions (FER-2013), and cartoons (New Yorker Cartoon).

Figure 1: HEMM is an evaluation framework that characterizes multimodal models along several dimensions (size, architecture, pretraining objective, fine-tuning objective, training data) and emphasizes holistic benchmarking of these models at three disentangled levels: basic skills, information flow, and use cases.

**Granularity of multimodal alignment** involves identifying alignment across elements in different modalities. For example, answering a question might require a model to perform fine-grained alignment to reference one specific object out of many possible objects in an image. Tasks that explicitly test for fine-grained alignment include localized reasoning on Visual Genome, Winoground, while tasks that emphasize coarse-grained alignment (e.g., making a prediction relevant to a whole image) include interpreting cartoon images , movie posters , and memes .

**Reasoning and external knowledge** involve the combination of local pieces of information to form increasingly rich and complex multimodal representations. For example, being able to perform multi-hop inference from Wikipedia text and images  or solving science questions given visual diagrams and executing multiple logical steps . Tasks like Winoground explicitly test for reasoning and tasks like OK-VQA are designed to assess external knowledge.

### Multimodal information flow

Multimodal information flow studies how information transforms across tasks, including cross-modal translation, editing, querying, and fusion.

**Cross-modal translation** exploits shared information by mapping data in one modality to another. Examples include translating from text to image for image generation (e.g., LNOCO) and translating from image to text for image captioning (e.g., NoCaps, Screen2Words).

**Cross-modal editing** involves semantically editing data in one modality according to another modality (e.g., given an image, following a natural language instruction to "change the background from day to night"). The model takes in the original image (with potentially more reference images), along with a task description specifying the edit, and outputs the edited image. We use the Magic Brush dataset to test cross-modal editing.

**Cross-modal querying** involves a model's ability to answer natural language questions that query specific information about an input. The model takes in the original image, a text description, the

   Dataset & \# Samples & Interactions & Fine-grained & Reasoning & Knowledge & Info. Flow & Use case \\   VQA  & 614K & Redundancy & Yes & Less & No & Querying & Multimedia \\ Visual Genome  & 1.7M & Redundancy & Yes & Less & No & Querying & Multimedia \\ VCR  & 290K & Redundancy & Yes & Less & No & Fusion & Multimedia \\ OK-VQA  & 14K & Redundancy & Yes & Less & Yes & Querying & Multimedia \\ GQA  & 22M & Redundancy & Yes & Less & No & Querying & Multimedia \\ NoCaps  & 15K & Redundancy & No & Less & No & Translation & Multimedia \\ Flickr30K  & 30K & Redundancy & No & Less & No & Translation & Multimedia \\ WINoground  & 1.6K & Redundancy & Yes & Less & No & Querying & Multimedia \\ Nlvr  & 92K & Redundancy & Yes & Less & No & Querying & Multimedia \\ Nlvr2  & 107K & Redundancy & No & Less & No & Querying & Multimedia \\ IREL  & 3.9K & Synergy & No & More & No & Fusion & Multimedia \\ MM-IMDB  & 25K & Synergy & No & Less & No & Fusion & Multimedia \\ Magic Brush  & 10K & Synergy & Yes & Less & No & Editing & Multimedia \\ LNCOCO  & 8.5K & Uniqueness & Yes & Less & Yes & Translation & Multimedia \\ NY Cartoon  & 364 & Synergy & No & More & Yes & Fusion & Affect \\ Hateful Memes  & 10K & Synergy & No & More & Yes & Fusion & Affect \\ MemeCap  & 560 & Synergy & No & More & Yes & Fusion & Affect \\ Mention  & 10K & Synergy & No & More & Yes & Fusion & Affect \\ FER-2013  & 30K & Uniqueness & No & Less & No & Querying & Affect \\ Sciencepa  & 21K & Synergy & No & Less & Yes & Fusion & Science \\ Resisc45  & 31K & Uniqueness & No & Less & No & Querying & Science \\ UCMerced Land use  & 2K & Uniqueness & No & Less & No & Querying & Science \\ inaturalist  & 675K & Uniqueness & Yes & Less & Yes & Querying & Science \\ Decime  & 5K & Uniqueness & No & More & Yes & Translation & Science \\ PathVQA  & 33K & Redundancy & Yes & Less & Yes & Querying & Healthcare \\ VQARAD  & 3.5K & Redundancy & Yes & More & Yes & Querying & Healthcare \\ OpenPath  & 218K & Redundancy & Yes & More & Yes & Querying & Healthcare \\ Slake  & 13K & Redundancy & Yes & More & Yes & Querying & Healthcare \\ Enrico  & 1.4K & Uniqueness & No & Less & No & Querying & HCI \\ Screen2Words  & 112K & Uniqueness & No & Less & No & Translation & HCI \\   

Table 1: HEMM includes a comprehensive suite of \(30\) datasets to benchmark multimodal foundation models. We categorize each dataset based on the _basic multimodal skills_ needed to solve them – the type of multimodal interaction, granularity of multimodal alignment, level of reasoning, and need for external knowledge, how _information flows_ between modalities, and the real-world _use cases_ they impact.

query, and must output the desired answer (typically in natural language). Querying can be done for visual scenes (GQA), environmental indicators (Resisc45), and medical data (VQARAD).

**Multimodal fusion** aims to learn interactions to combine information from different modalities, such as classifying diseases given x-ray images and medical tests, or detecting humor from cartoon images and captions. Multimodal fusion takes in the image, text, and a description of the task, and then outputs a prediction, which can include affective states like humor in New Yorker Cartoon, hate speech detection in Hateful Memes, or in science problems (ScienceQA).

### Real-world Use Cases

Each use case is drawn from a real-world application with their own specific challenges.

**Multimedia** includes efficient search, retrieval, indexing, and generation of digital content. Multimedia tasks in HEMM include question answering about images and videos (VQA, VCR), multimedia captioning (Flickr30K, NoCaps), compositional visual reasoning (Winoground, Nlvr), understanding cartoons, movie posters (MM-IMdb), memes (MemeCap and Memotion), and figurative language (IRFL), and editing images (Magic Brush).

**Affective computing** aims to perceive human affective states (emotions, sentiment, personalities, humor, sarcasm, social interactions) , and is important for building emotionally and socially-intelligent AI  and human-AI interaction . HEMM includes New Yorker Cartoon (cartoon images and captions), Hateful Memes (hateful content in memes), FER-2013 for facial expressions, MemeCap for meme captioning, and Memotion for emotions in memes.

**Natural sciences** aims to deepen our knowledge of physical, chemical, biological, and environmental sciences. These can involve satellite images, chemical bonds, land and agriculture use, wildlife, and specific scientific terminologye . Tasks in HEMM include ScienceQA testing different science topics and Resisc45 for land scene classification.

**Healthcare** involves integrating multimodal signals such as lab tests, imaging reports, and doctor-patient interactions to help doctors interpret high-dimensional data and assist them in diagnosis . We include processing text reports and medical images in the form of PathVQA for pathology, VQARAD for radiology images, and Slake for medical visual question answering.

**HCI** involves user design, usability, user experience, and other challenges related to humans interacting with computers . HCI tasks can involve visual information such as screen layouts, user actions, and feedback mechanisms. HCI tasks in HEMM include Enrico for classifying mobile UI designs and Screen2Words for UI screen content summarization.

## 3 Key Modeling Principles and Models in HEMM

Table 2 summarizes the 11 models we evaluate in HEMM, which span different numbers of parameters, model architectures, training datasets, pretraining objectives, and fine-tuning objectives.

### Modeling decisions

Model parametersParameters can vary greatly across different multimodal models, from 100M params to approximately 1000B params. We consider models with total number of parameters less

   Model & \#Param & Data Size & Data Diversity & Training Type & INST & Modality Proc \\  Kosmos-2  & 1.6B & 90M & Yes & End-to-end & Yes & interleaved \\ OpenFlamingo & 3.2B & 180M & No & Modular Fine-tune & No & interleaved \\ Instvuc-BLIP  & 4.0B & 244M & Yes & Modular Fine-tune & Yes & separate \\ LLAMA-Daptev & 7.0B & 567K & No & Modular Fine-tune & Yes & separate \\ MPLUG-Owl  & 7.2B & - & Yes & Modular Fine-tune & Yes & separate \\ Fuyu-8B  & 9.3B & - & Yes & End-to-end & No & interleaved \\ BLIP-2  & 12.1B & 244M & No & Modular Fine-tune & No & separate \\ Mini-GP-4  & 13.0B & 5M & No & Modular Fine-tune & Yes & separate \\ Envu & 14.0B & 82M & Yes & End-to-end & No & interleaved \\ Gemini & - & - & Yes & - & Yes & interleaved \\ GPT-4V & - & - & Yes & - & Yes & - \\   

Table 2: Models used in HEMM, ranked from small to large, and categorized by #Param (model size), Data Size (pretraining data size), Data Diversity (pretraining data diversity), Training Type (end-to-end training or frozen alignment), INST (instruction tuning), Modality Proc (interleaved or separate modality inputs).

than or equal to 4B (e.g., Instruct-BLIP) as _small_, whereas those having more than 4B parameters (e.g., Fuyu-8B) are considered _medium_. GPT-4V and Gemini are considered _large_.

Modality processingSome multimodal models (e.g., Fuyu-8B) support interleaved inputs like "<dog_img> This is a very cute dog.<cat_img> This is a very cute cat.", unlike models that only support separate image and text queries (e.g., BLIP-2, Mini-GPT-4).

### Training Characteristics

Training typeEnd-to-end training involves fine-tuning unimodal encoders, pretrained LLMs, and a multimodal model jointly, as seen in Emu, Fuyu-8B, etc. Another category operates by freezing unimodal encoders and LLM, and then training only a mapping that aligns frozen image features with frozen LLM features. These trainable mappings include Q-former  (used in Instruct-BLIP), linear layers [128; 92] (used in Mini-GPT-4), and attention blocks used in OpenFlamingo.

Size of pre-training dataWe consider the total size of pre-training data used for training, including instruction and supervised data. Emu has _small_ data scale, with less than 100M training data points. Fuyu-8B has _medium_ data-scale, with more than 100M training data points. While GPT-4V and Gemini do not release data sizes, we estimate their size to be much larger than other models and therefore are considered to have _large_ data scale.

Diversity of pre-training dataWe consider the diversity of multimodal tasks used for training, including visual QA, visual conversations, and interleaved images and text. Instruct-BLIP and Emu are pre-trained on diverse data, in contrast to LLaMA-Adapter, OpenFlamingo, etc., which only use image captioning data for training.

Instruction tuningBy transforming supervised tasks into an 'instruction' format, instruction tuning has been shown to benefit performance and improve the controllability of LLMs. Mini-GPT-4 and Instruct-BLIP include an instruction tuning stage, while models like BLIP-2 do not.

## 4 Experiments

In this section, we discuss extensive experiments conducted to holistically evaluate the performance of multimodal foundation models based on HEMM.

  
**Dimension** & **Category** & **Perf** (\(\)) \\   \\  Modality & Interleaved & 22.94 \\ processing & Separate & **28.58** \\   & Small & 23.34 \\  & Medium & 23.87 \\  & Large & **42.33** \\   \\   & Modular & **24.92** \\  & End-to-end & 21.26 \\   & Small & 16.80 \\  & Medium & 30.10 \\  & Large & **31.77** \\  Diversity of training data & Non-diverse & 21.71 \\  & Diverse & **30.15** \\   & No & 22.49 \\  & Yes & **29.71** \\   

Table 4: Performance on different modeling decisions, as measured via the mean BARTscore for each model across all 30 tested multimodal datasets.

  
**Dimension** & **Category** & **Perf** (\(\)) \\   & Multimedia & **31.30** \\  & Affect & 30.35 \\  & Health & 20.24 \\  & Science & 19.83 \\  & HCI & 15.70 \\   & Redundancy & 29.04 \\  & Uniqueness & 19.60 \\  & Synergy & **33.73** \\   & More Reasoning & 27.50 \\  & Less Reasoning & 26.84 \\   & Fine-grained & 26.52 \\  & Coarse-grained & 27.52 \\   & External & 23.51 \\  & None & **29.62** \\   & Querying & 25.88 \\  & Translation & 18.97 \\  & Fusion & **33.77** \\   

Table 3: Performance on different dataset dimensions, as measured via the mean BARTscore on each dataset across all 11 tested multimodal models.

### Experimental setup

Individual metricsFor all text generation tasks, we use the established natural language generation evaluation metric BARTScore , which was found to have the highest correlation with human judgement . We compute BARTScore(r, c), where r is the reference and c is the candidate. It can be interpreted as the probability of generating the candidate sentence from the reference. For example, a model might caption an image with the following generated candidate: _A row of violins hanging on a wall._. The reference (ground truth) of _A painting of 5 cello's with a green background_ would be used to compute the BARTScore with respect to c.

Aggregating metricsTo aggregate scores across multiple tasks or models, we normalize scores using min-max scaling. Following Chang et al. , _min_ represents the score of the worst multi-modal model and _max_ represents the identity score BARTScore(r, r), where r is the ground truth. Subsequently, these normalized scores in the 0 to 1 range can be interpreted as a percentage of model performance relative to the ground truth.

ComputationSince GPT-4V and Gemini have query limits, we evaluate their performance on 100 random samples for each dataset (2800 total data points). For a fair comparison with other models, we present the results and findings below based on the performance of those 100 samples per dataset. In Appendix C we present the results of the other models on the full evaluation sets. We evaluate all the models on a single NVIDIA A100 80GB GPU with the inference time for a single image-text pair ranging from 0.1 seconds to 63.7 seconds. We report the average inference times for the models across all datasets and include additional details on the evaluation protocol in Appendix B.

### Main results

We summarize our main results here and include full details in Appendix C. We first explain performance trends across the datasets in HEMM, before explaining performance differences across different multimodal foundation models and their design decisions.

#### 4.2.1 Performance across dataset dimensions

Overall comparisonsWe summarize overall trends in Figure 3 and Table 3. On average, models perform better on multimedia datasets, with IRFL (0.58), Nlvr (0.50), and Winoground (0.49) showing the highest scores. The lowest scores are for Healthcare, HCI, and Science use cases, such as on Decimer (0.07), iNaturalist (0.08), Enrico (0.12), PathVQA (0.15), and MemeCap (0.32). For predicting molecular structures on Decimer, models are not able to generate correct chemical notations (in Simplified Molecular Input Line Entry System notation) and instead only generate names of individual atoms or compounds (see Figure 2). Other challenging datasets include inNaturalist due to fine-grained visual differences between 5000 species of plants and animals, and healthcare datasets that require intricate analysis of pathology images to identify organs, tissues, and anomalies (see Figure 8). Datasets related to menies were also challenging (0.32 and 0.38 on MemeCap and Memotion), requiring knowledge about current events, pop culture, and metaphors beyond literal meanings.

Figure 2: Responses of GPT-4V and Gemini on samples from the science category. These failure cases show that the models lack domain knowledge and are unable to correctly translate the images of molecules to the SMILES notations (a). Example (b) shows that the models struggle on tasks requiring complex reasoning, failing to comprehend the relation between the force and the size of the magnets. In (c), all models except GPT-4V are unable to capture the fine-grained details and misclassify the image as an airport instead of a runway.

Multimodal skills 1: InteractionsThe average scores for redundant, unique, and synergistic interactions are 0.29, 0.20, and 0.33. One reason for lower uniqueness scores is the presence of highly challenging visual datasets like Decimer and Enrico. On average, the easiest tasks in redundancy are Nlvr (0.50) and Winoground (0.49). The hardest datasets in uniqueness are iNaturalist (0.08) and Decimer (0.07), and in synergy are MemeCap (0.14) and Memotion (0.21).

Multimodal skills 2: GranularityWe do not find that fine-grained datasets are significantly harder than those with coarse-grained alignment. Tasks requiring fine-grained alignment between image and text like GQA and Winoground achieve a score of 0.26, while those only needing coarse-grained alignment (e.g., Enrico, ScienceQA) are still quite challenging (score: 0.27).

Multimodal skills 3: ReasoningWe do not find a significant difference between the performance of the models on tasks requiring more (average score = 0.275) or less reasoning (average score = 0.268). The most challenging datasets requiring less reasoning include iNaturalist (0.08) and Enrico (0.12) due to challenges in fine-grained visual perception and external knowledge, while there are also several challenging datasets requiring more complex reasoning like VCR (0.34) and MemeCap (0.14), where the models encounter difficulties with samples requiring commonsense and compositional reasoning (See Figure 4 for examples).

Multimodal skills 4: External knowledgeThe average performance on tasks requiring external knowledge is 0.23, compared to 0.30 for those not requiring external knowledge. For example, Instruct-BLIP performs well on Winoground and VCR that do not require external knowledge but struggles more on knowledge-intensive tasks e.g., iNaturalist, which requires knowledge

Figure 4: Tasks requiring commonsense and compositional reasoning are challenging. In (a), GPT-4V and Gemini are unable to employ social commonsense to analyze the relationships between the two people. Example (b) demonstrates the models’ difficulty in composing information from both modalities, leading to their failure to comprehend the scenario where _a tree smashed into the car_ (not a car smashed into the tree). In (c), all models except GPT-4V fail to grasp the visual metaphors and the juxtaposition of the two scenarios.

Figure 3: Average scores are higher for multimedia datasets as compared to other use cases, and lowest for healthcare, HCI, and science. The models struggle on iNaturalist, Decimer, Enrico, PathVQA, and MemeCap which require external knowledge, fine-grained alignment, and complex reasoning.

about characteristics of a vast number of species, and Slake, where medical knowledge is needed to identify the abnormalities in organs.

Multimodal Skills 5: Information flowTranslation has the lowest average score amongst all types of information flow (0.19), whereas the average scores on querying and fusion are 0.26 and 0.33 respectively. The low performance on translation is due to the presence of challenging datasets like Decimer and Screen2Words requiring mapping images of chemicals and screenshots into text. Although the average score for fusion is high, the performance on some datasets is still quite low, such as Instruct-BLIP achieving a score of only 0.04 on MemeCap and 0.15 on MM-IMDb.

#### 4.2.2 Performance across modeling dimensions

We now compare different modeling decisions and training objectives in Table 4.

Overall comparisons across modelsGemini  (0.44), Instruct-BLIP  (0.41), BLIP-2  (0.41), and GPT-4V  (0.40) achieve the best average performance across all tasks. The low scores of GPT-4V as compared to Gemini and Instruct-BLIP are due to its generation of keywords like "Indeterminate", "Uncertain", and "Unknown" on datasets like VQA and GQA, perhaps due to its alignment process. Further, on some datasets related to Memes (e.g., Hateful Memes) and Health (e.g., Slake), GPT-4V refrains from answering the questions and instead generates a response saying _Cannot assist with the request_. OpenFlamingo (0.06), Emu (0.11) have the lowest average scores. From their generations, we find that these models struggle to follow the instructions for challenging datasets like Decimer and Enrico, and generate hallucinated responses. Moreover, with relatively easier datasets such as Flickr30K, the captions produced by Emu and OpenFlamingo tend to fixate on specific objects rather than providing a comprehensive description of the scene, often leading to instances of hallucination related to these objects. As a result, these models rank lowest on many datasets, receiving a normalized score of 0.

Model scaleWe find that the performance of larger models (both total and trainable parameters) is significantly better than the models with a medium or small number of parameters (Figure 5). When grouped based on the total number of parameters, the average scores achieved by large, medium, and small models are 0.42, 0.24, and 0.23 respectively. The difference between the performance of large and medium models is significant (p-value for paired t-Test < 0.001). In particular, large models showed the most improvement on MM-IMDb, MemeCap, and Hateful Memes datasets, which fall into the category of tasks requiring synergistic interactions. On average, the large models perform the best on synergistic tasks with a score of 0.53 compared to 0.30 for medium and 0.23 for small models. For instance, on the MM-IMDb dataset, we observed significant gains in performance when increasing model size: from 0.15 for Instruct-BLIP (small) to 0.36 for BLIP-2 (medium) and 0.48 for Gemini (large).

Pretraining data scaleAverage scores of the models in _large_ and _medium_ data size categories are 0.31 and 0.30 respectively, whereas models with _small_ pretraining data achieve a significantly lower score of 0.17. We also find that for all datasets, the average score of models with _medium_ pretraining data is higher than the models with _small_ pretraining data. For instance, on the Winoground dataset which requires fine-grained alignment between the modalities, the maximum scores achieved by the models with _medium_ and _small_ pretraining data are 0.45 and 0.80. We also find a significant gap between the maximum scores achieved by the models in the _medium_ (maximum score - 0.18) and _small_ categories (maximum score - 0.70), on the Nlvr2 dataset for visual reasoning.

Diversity of pre-training dataOn average, models trained on diverse datasets perform better (score: 0.30) than models trained only on image captioning datasets (score: 0.21). Diverse training

Figure 5: On average, large models are better than small and medium models (p-values < 0.001). Instruct-BLIP and BLIP-2 are outliers - despite having fewer params, they achieve relatively high performance, even close to GPT-4V and Gemini.

data allows the models to share learned knowledge and generalize across different tasks. For example, models pretrained with diverse datasets perform significantly better on the knowledge-intensive iNaturalist task, such as BLIP-2 (non-diverse) scoring 0.08 and Gemini scoring 0.24. For the MemeCap dataset which requires external knowledge and complex reasoning, we observe that BLIP-2 (non-diverse) scores 0.06 and mPLUG-Owl (diverse) scores 0.21.

Instruction tuning vs supervised fine-tuningOn average, instruction-tuned models (average score of 0.30) performed better than the models trained using only supervised fine-tuning (average score of 0.22). The top 3 tasks with the largest performance gap between instruction-tuned and non-instruction-tuned models are Decimer, MemeCap, and Screen2Words, with improvements of 0.15, 0.09, and 0.09 respectively. We also observe that translation tasks (image-to-text) (e.g., Flickr30K, NoCaps) benefit from instruction tuning, where the models generate more accurate and detailed captions after human instruction.

### Human evaluation

To assess how well HEMM aligns with human preferences, we performed human preference-based evaluation, following Chiang et al. , where annotators are shown the outputs of two different models for the same inputs and choose the better output or a tie option. Across 1000 pairwise comparisons by 5 annotators, the pairwise rankings are used to calculate each model's average win rate and Elo rating (see Appendix B.5 for calculation details).

The models ranked by Elo ratings are Gemini (1074), GPT-4V (1057), BLIP-2 (1033), and Instruct-BLIP (1032) (see Table 5). The top 4 models based on the Elo Rating are the same as the top 4 models ranked by BARTScore. Elo Rating of GPT-4V is better than BLIP-2 and Instruct-BLIP. However, the average BARTScore for GPT-4V (0.40) is lower than Instruct-BLIP (0.42) and BLIP-2 (0.41). We also find Elo Rating of bottom two models to be consistent with BARTScore rankings - Emu (0.11) and OpenFlamingo (0.06).

## 5 Related Work

**Multimodal machine learning** brings unique challenges for ML research due to the heterogeneity between modalities and the interconnections found between them . It has inspired many theoretical studies in data heterogeneity and interactions , as well as diverse applications in multimedia , affective computing , robotics , finance , HCI , education  and healthcare .

**Evaluation frameworks for multimodal models** have significantly shaped the multimodal research landscape, through holistic  and domain-specific benchmarks . Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME , MMBench , LVLM-chub , SEED-Bench , Touchstone , Mm-vet , ReFormEval , VisIT-Bench , FLAVA . Other benchmarks focus on evaluating hallucination  and applications in medicine  and autonomous driving . These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.

**Multimodal foundation models** are promising foundations for the future of AI, with impressive reasoning , interactive dialogue , and few-shot generalization abilities . These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks , or based on adapting language models with vision to enable text generation conditioned on images . Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data . Additionally, composable

    & Avg. & Elo & Avg. \\  & Win Rate & Rating & BARTScore \\  Gemini & **0.73** & **1074** & **0.44** \\ GPT-4V & 0.68 & 1057 & 0.40 \\ BLIP-2 & 0.52 & 1033 & 0.41 \\ Instruct-BLIP & 0.60 & 1032 & 0.42 \\ MPLUG-Owl & 0.45 & 1010 & 0.21 \\ LLaMA-Adapter & 0.45 & 1008 & 0.19 \\ Fuyu-8B & 0.42 & 992 & 0.31 \\ Mini-GPT-4 & 0.38 & 990 & 0.20 \\ Kosmos-2 & 0.39 & 968 & 0.22 \\ Emu & 0.20 & 924 & 0.11 \\ OpenFlamingo & 0.17 & 911 & 0.06 \\   

Table 5: Average win rate and Elo Rating of 11 models calculated based on the human evaluation of 1000 pair-wise comparisons of model responses. Elo rating is reported as the median over 1000 runs with shuffled batches sequences and an initial rating of 1000 for each model. Top 4 and bottom 2 models identified by Elo Rating are consistent with those found by Average BARTScore.

models  and mixtures of experts  can be used to further generate combinations of output modalities.

**Adapting language models for multimodality** is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs . These approaches typically use parameter-efficient modules like LLaMA-Adapter V2  and MAGMA  for efficient finetuning. Vision-language instruction tuning has also emerged as a useful technique, as it allows the models to better follow human instructions . Our goal is to make HEMM the most comprehensive benchmark to study the current and future generation of multimodal foundation models, and for the community to continuously contribute to its expansion.

## 6 Conclusion

Holistic Evaluation of Multimodal Models (HEMM) is a framework for benchmarking multimodal foundation models. Through a new taxonomy of multimodal skills, information flow, and real-world use cases, HEMM enables comprehensive analysis of multimodal models. HEMM is publicly available, will be regularly updated, and encourages community involvement in its expansion.

**Limitations and social impact** The evaluation of multimodal models is done only on a subset of all possible skills, information, and use cases in the world. Future work can improve the categorization of datasets into skills, information, and use cases, and discover new dimensions that pose challenges to multimodal models. Such evaluation is critical to ensure that models are sufficiently robust when deployed in real-world scenarios, to prevent unexpected and unintended consequences. Future work should also add new metrics to HEMM measuring real-world societal concerns such as fairness, robustness, social biases, privacy, and efficiency of multimodal models.