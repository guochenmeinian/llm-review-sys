# Transformers learn to implement preconditioned gradient descent for in-context learning

Kwangjun Ahn

MIT EECS/LIDS

kjahn@mit.edu

&Xiang Cheng

MIT LIDS

chengx@mit.edu

&Hadi Daneshmand1

MIT LIDS/FODSI

hdanesh@mit.edu

&Suvrit Sra

TU Munich / MIT

suvrit@mit.edu

Equal contribution, alphabetical order.

###### Abstract

Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: _Can transformers learn to implement such algorithms by training over random problem instances?_ To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with \(L\) attention layers, we prove certain critical points of the training objective implement \(L\) iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.

## 1 Introduction

In-context learning (ICL) is the striking capability of large language models: Given a prompt containing examples and a query, the transformer produces the correct output based on the context provided by the examples, _without adapting its parameters_(Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Black et al., 2022). This property has become the focus of body of recent research that aims to shed light on the underlying mechanism of large language models (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023; Li and Malik, 2017; Min et al., 2021; Xie et al., 2021; Elhage et al., 2021; Olsson et al., 2022).

A line of research studies ICL via the expressive power of transformers. Transformer architectures are powerful Turing machines, capable of implementing various algorithms (Perez et al., 2021; Wei et al., 2022). Given an in-context prompt, Edelman et al. (2022); Olsson et al. (2022) argue that transformers are able to implement algorithms through the recurrence of multi-head attentions to extract coarse information from raw input prompts. Akyurek et al. (2022); von Oswald et al. (2023) assert that transformers can implement gradient descent on linear regression encoded in a given input prompt. It is thought provoking that transformers can implement such algorithms.

Although transformers are universal machines to implement algorithms, they need specific parameter configurations for achieving these implementations. In practice, their parameters are adjusted via training using non-convex optimization over random problem instances. Hence, it remains unclear whether this non-convex optimization can be used to learn algorithms. The present paper investigates _the possibility of learning algorithms via training over random problem instances._More specifically, we investigate the learning of gradient-based methods. It is hard to mathematically formulate what it means to learn gradient descent for general functions with transformers. Yet, Garg et al. (2022) elegantly examine it in the specific setting of ICL for learning functions. Empirical evidence suggests that transformers indeed learn to implement gradient descent, after training on random instances of linear regression (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). Motivated by these observations, we theoretically investigate the loss landscape of a simple transformer architecture based on _attention without softmax_(Schlag et al., 2021; von Oswald et al., 2023) (see Section 2 for details).

_Summary of our main results._ Our main contributions are the following:

* We provide a complete characterization of the global optimum of a single-layer linear transformer. In particular, we observe that, with the optimal parameters, the transformer implements a single step of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the distribution of input data but also to the variance caused by data inadequacy. We present this result in Theorem 1 in Section 3.
* Next, we focus on a subset of the transformer parameter space, defined by a special sparsity condition (8). Such a parameter configuration allows us to formulate training transformers as a search over \(k\)_-step adaptive gradient-based algorithms_. Theorem 2 characterizes the global minimizers of the training objective of a two-layer linear transformer over isotropic regression instances, and shows that the optima correspond to gradient descent with adaptive stepsizes. For multilayer transformers, Theorem 3 demonstrates that gradient descent, with a data-dependent preconditioning, can be derived from a critical point of the training objective.
* Finally, we study the loss landscape in the absence of the sparsity condition (8), which goes beyond searching over conventional gradient-based optimization methods. In this case, we prove and interpret the structure of a critical point of the training objective. We show that a certain critical point in parameter space leads to an intriguing gradient-based algorithm that simultaneously takes gradient steps preconditioned by data covariance, and applies a linear transformation to further improve the conditioning. In the specific case when data covariance is isotropic, this algorithm corresponds to the GD++ algorithm of von Oswald et al. (2023) which is experimentally observed to be the outcome of training.

We empirically validate the critical points analyzed in Theorem 3 and Theorem 4. For a transformer with three layers, our experimental results confirm the structural of critical points. Furthermore, we observed the objective value associated with these critical points is close to \(0\), suggesting that the critical points might be global optima. These experiments substantiate our theoretical analysis and suggests that our theory indeed _aligns with practice_. Code for our experiments is available at https://github.com/chengxiang/LinearTransformer.

### Related works

The ability of neural network architectures to implement algorithms has been investigated in various context. The seminal work by Siegelmann and Sontag (1992) investigate the Turing completeness of recurrent neural networks. Despite this computational power, training recurrent networks remains a challenge. Graves et al. (2014) design an alternative neural architecture known as the _neural Turing machine_, building on _attention layers_ introduced by Hochreiter and Schmidhuber (1997). Leveraging attention, Vaswani et al. (2017) propose transformers as powerful neural architectures, capable of solving various tasks in natural language processing (Devlin et al., 2019). This capability inspired a line of research that examines the algorithmic power of transformers (Perez et al., 2021; Wei et al., 2022; Giannou et al., 2023; Akyurek et al., 2022; Olsson et al., 2022). What sets transformers apart from conventional neural networks is their impressive performance after training. In this work, we focus on understanding _how transformers learn to implement algorithms_ by training over problem instances.

A line of research investigates how deep neural networks process data across their layers. The seminal work by Jastrzebski et al. (2018) observes that hidden representations across the layers of deep neural networks approximately implement gradient descent. Recent observations provide novel insights into the working mechanism of ICL for large language models, showing they can implement optimization algorithms across their layers (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). Moreover, Zhao et al. (2023); Allen-Zhu and Li (2023) observe transformer perform dynamic programming to generate text. In this work, we theoretically study how transformer learns gradient-based algorithms for ICL.

We discuss here two related works (Zhang et al., 2023; Mahankali et al., 2023) that appeared shortly after publication of our original draft. Both of these studies focus on a single layer attention network (see Section 3). Zhang et al. (2023) prove the global convergence of gradient descent to the global optimum whose structure is analyzed independently from this study and it the same as that in Theorem 1. Mahankali et al. (2023) also characterize the global minimizer of a single layer attention without softmax for a different data distribution. In addition to results for a single-layer attention, we analyze the landscape of two and multi-layer transformers.

## 2 Setting: training linear transformers over random linear regression

In order to understand the mechanism of ICL, we consider the setting of training transformers over the random instances of linear regression, following (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). In particular, the random instances of linear regression are formalized as follows.

_Data distribution: random linear regression instances._ Let \(x^{(i)}^{d}\) be the covariates drawn i.i.d. from a distribution \(D_{}\), and \(w_{}^{d}\) be drawn from \(D_{}\). Let \(X^{(n+1) d}\) be the matrix of covariates whose row \(i\) contains tokens \(x^{(i)}\). Given \(x^{(i)}\)'s and \(w_{}\), the responses are defined as \(y=[ x^{(1)},w_{},, x^{(n)},w_{}] ^{n}\). Define the _input matrix_\(Z_{0}\) as

\[Z_{0}=z^{(1)}\:z^{(2)}\;\;z^{(n)}\:z^{(n+1)} =x^{(1)}&x^{(2)}&&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&&y^{(n)}&0^{(d+1)(n+1)},\] (1)

where zero in the above matrix is used to replace the unknown response variable corresponding to \(x^{(n+1)}\). Then, our goal is to predict \(w_{}^{}x^{(n+1)}\) given \(Z_{0}\). In other words, the training data consists of pairs \((Z_{0},w_{}^{}x^{(n+1)})\) for \(x^{(i)} D_{}\) and \(w_{} D_{}\). We then consider training transformers over this data distribution.

_Self-attention layer without softmax._ Following (Schlag et al., 2021; von Oswald et al., 2023), we consider the linear self-attention layer. To motivate, we first briefly review the standard self-attention layer (Vaswani et al., 2017). Letting \(Z^{(d+1)(n+1)}\) be the input matrix with \(n+1\) tokens in \(^{d+1}\), a single-head self-attention layer denoted by \(^{}\) is a parametric map defined as

\[^{}_{W_{v},W_{q,v}}(Z)=W_{v}ZM (Z^{}W_{k}^{}W_{q}Z)\,, MI_ {n}&0\\ 0&0^{(n+1)(n+1)},\] (2)

where \(W_{v},W_{k},W_{q}^{(d+1)(d+1)}\) are the (value, key and query) weight matrices, and \(()\) is the softmax operator which applies softmax operation to each column of the input matrix. Note that the prompt is asymmetric since the label for \(x^{(n+1)}\) is excluded from the input. To reflect this asymmetric structure, the mask matrix \(M\) is included in the attention. In our setting, we consider the self-attention layer that omits the softmax operation in (2). In particular, we reparameterize weights as \(P W_{v}^{(d+1)(d+1)}\) and \(Q W_{k}\,^{}W_{q}^{(d+1)(d+1)}\) and consider

\[_{P,Q}(Z)=PZM(Z^{}QZ)\,.\] (3)

At first glance, the omission of the softmax operation (3) might seem over-simplified. But, (von Oswald et al., 2023) proves such attention can implement gradient descent, and we will prove in Lemma 1 that it can also implement various algorithms to solve linear regression in-context.

_Architecture for prediction._ We now present the neural network architecture that will be used throughout this paper. For the number of layers \(L\), we define an _\(L\)-layer transformer_ as a stack of \(L\) linear self-attention blocks. Formally, denoting by \(Z_{}\) the output of the \(^{}\) layer attention, we define

\[Z_{+1}=Z_{}+_{P_{},Q_{}}(Z_{}) =0,1,,L-1,\] (4)

The scaling factor \(}{{n}}\) is used only for ease of notation and does not influence the expressive power of the transformer. Given \(Z_{L}\), we define \(_{L}(Z_{0};\{P_{},Q_{}\}_{=0,1, L-1})=-[Z_{L}]_{(d+ 1),(n+1)}\), i.e., the \((d+1,n+1)\)-th entry of \(Z_{L}\). The reason for the minus sign is to be consistent with (von Oswald et al., 2023), and we will clarify such a choice in Lemma 1. For training, the parameters are optimized to minimize in-context loss as

\[f(\{P_{},Q_{}\}_{=0}^{L})=_{(Z_{0},w_{})} _{L}(Z_{0},\{P_{},Q_{}\}_{=0}^{L})+w_{ }^{}x^{(n+1)}^{2}.\] (5)

_Goal: the landscape analysis of the training objective functions._ We are interested in understanding how the optimization of \(f\) leads to in-context learning. We investigate this question by analyzing its loss landscape. Such analysis is challenging due to two major reasons: _(i) \(f\) is non-convex in parameters \(\{P_{i},Q_{i}\}\) even for a single layer transformer. (ii) The cross-product structures in attention makes \(f\) a highly nonlinear function in its parameters._ Hence, we analyze a spectrum of settings from single-layer transformers to multi-layer transformers. For simpler settings such as single-layer transformers, we prove stronger results such as the full characterization of the global minimizers. For networks with more layers, we characterize the structure of critical points. Furthermore, we provide algorithmic interpretations of the critical points. Table 1 summarizes our results for various parameteric models.

**Remark 1** (_Optimizing_ (5) _vs. practical transformer optimization_).: _Interestingly, a recent work by Ahn et al. (2023) reports that common optimization algorithms such as SGD/ADAM behave remarkably similarly on the (linear Transformers + linear regression) problem as they do on (practical transformers + real language modeling tasks). In particular, they reproduce several distinctive features of transformer optimization under a simple shallow linear transformer. This work suggests that (linear transformer + linear regression) may serve as a good proxy for understanding practical transformer optimization._

## 3 The global optimum for a single-layer transformer

For the single layer case of \(L=1\), the following result characterizes the optimal parameters \(P_{0}\) and \(Q_{0}\) for the in-context loss (5).

**Theorem 1** (**Single-layer; non-isotropic data**).: _Assume that vector \(x^{(i)}\) is sampled from \((0,)\), i.e., a Gaussian with covariance \(=U U^{}\) where \(=(_{1},,_{d})\). Moreover, assume that \(w_{}\) is sampled from \((0,I_{d})\). Then, the following choice of parameters_

\[P_{0}=0_{d d}&0\\ 0&1, Q_{0}=-U(\{ _{i}+(_{k}_{k} )}\}_{i=1,,d})U^{}&0\\ 0&0.\] (6)

_is a global minimizer of \(f(P,Q)\) up to re-scaling, i.e., \(P_{0} P_{0}\) and \(Q_{0}^{-1}Q_{0}\) for a scalar \(\)._

See Appendix A for the proof of Theorem 1. In the specific case when the Gaussian is isotropic, i.e., \(=I_{d}\), the optimal \(Q_{0}\) has the following simple form

\[Q_{0}=-+(d+2))}I _{d}&0\\ 0&0.\] (7)

Up to scaling, the above parameter configuration is equivalent to the parameters used by von Oswald et al. (2023) to perform one step of gradient descent. Thus, in the single-layer setting, the in-context loss is indeed minimized by a transformer that implements the gradient descent algorithm.

 Results & \(x^{(i)}\) & \(w_{}\) & Setting & Guarantees \\   Theorem 1 & \((0,)\) & \((0,I)\) & single-layer & global minimizers \\ Theorem 2 & \((0,I)\) & \((0,I)\) & two-layer + symmetric (8) & global minimizers \\ Theorem 3 & \((0,)\) & \((0,^{-1})\) & multi-layer + (8) & critical points \\ Theorem 4 & \((0,)\) & \((0,^{-1})\) & multi-layer + (11) & critical points \\ Theorem 5 & \((0,I)\) & \((0,I)\) & single-layer + ReLU activation & global minimizers \\   

Table 1: Summary of our analyses for various models and input distributions. The additional conditions (8) and (11) are about the sparsity structure of parameters. In addition, “symmetric (8)” means we additionally impose the weights to be symmetric.

More generally, when the in-context samples are non-isotropic, the transformer learns to implement one step of a _preconditioned_ gradient descent as we shall detail in Lemma 1. Here the "preconditioning matrix" given in (6) has interesting properties:

* When the number of samples \(n\) is large, the first \(d d\) submatrix of \(Q_{0}\) approximates \(^{-1}\), the inverse of the data covariance matrix, which is also close to the Gram matrix formed from \(x^{(1)},,x^{(n)}\). Hence the preconditioning can lead to considerably faster convergence rate when \(\) is ill-conditioned.
* Moreover, \(_{k}_{k}\) in (6) acts as a regularizer. It becomes more significant when \(n\) is small and variance of the \(x^{(i)}\)'s is high. Such an adjustment resembles structural risk minimization (Vapnik, 1999) where the regularization strength is adapted to the sample size.

## 4 Multi-layer transformers with sparse parameters

Theorem 1 proves a single layer of linear attention can implement a single step of preconditioned gradient descent. Inspired by this result, we investigate the algorithmic power of the linear transformer architecture. We show that the model can implement various optimization methods even under sparsity constraints. In particular, we impose the following restrictions on the parameters:

\[P_{i}=0_{d d}&0\\ 0&1, Q_{i}=-A_{i}&0\\ 0&0A_{i}^{d d}.\] (8)

The next lemma proves that a forward-pass of a \(L\)-layer transformer, with the parameter configuration (8) is the same as taking \(L\) steps of gradient descent, preconditioned by \(A_{}\).

**Lemma 1** (_Forward pass as a preconditioned gradient descent_).: _Consider the \(L\)-layer linear transformer parameterized by \(A_{0},,A_{L-1}\) as in (8). Let \(y_{}^{(n+1)}\) be the \((d+1,n+1)\)-th entry of the \(\)-th layer output, i.e., \(y_{}^{(n+1)}=[Z_{}]_{(d+1),(n+1)}\) for \(=1,,L\). Then, it holds that \(y_{}^{(n+1)}=- x^{(n+1)},w_{}^{}\) where \(\{w_{}^{}\}\) is defined as \(w_{0}^{}=0\) and as follows for \(=1,,L-1\):_

\[w_{+1}^{}=w_{}^{}-A_{} R_{w_{}} (w_{}^{}) R_{w_{}}(w):= _{i=1}^{n}(w^{}x_{i}-w_{}{}^{}x_{i})^{2}.\] (9)

See Subsection C.1 for a proof. The iterative scheme (9) includes various optimization methods including gradient descent with \(A_{}=_{}I_{d}\), and (adaptive) preconditioned gradient descent, where the preconditioner \(A_{}\) depends on the time step. In the upcoming sections, we characterize how the optimal \(\{A_{}\}\) are linked to the input distribution.

### Warm-up: optimal two-layer transformer with symmetric weights

For the rest of this section, we will study the optimal parameters for the in-context loss under the constraint of Eq. (8). Later in Section 5, we analyze the optimal model for a more general parameters. For a two-layer transformer, the next Theorem proves the optimal in-context loss obtains the simple gradient descent with adaptive coordinate-wise stepsizes.

**Theorem 2** (Global optimality for the two-layer (symmetric) transformer).: _Consider the optimization of in-context loss for a two-layer transformer with the parameter configuration in Eq. (8), and additionally assume that \(A_{1},A_{2}\) are symmetric matrices. More formally, consider_

\[_{A_{1},A_{2}}f\{P_{}=0_{d  d}&0\\ 0&1,\;Q_{}=-A_{}&0\\ 0&0\}_{=1,2}.\]

_Assume \(x^{(i)}}{{}}N(0,I_{d})\) and \(w_{} N(0,I_{d})\); then, there are diagonal matrices \(A_{1}\) and \(A_{2}\) that are a global minimizer of \(f\)._

Combining the above result with Lemma 1 concludes that the two iterations of gradient descent with _coordinate-wise adaptive stepsizes_ achieve the minimal in-context loss for isotropic Gaussian inputs. Gradient descent with adaptive stepsizes such as Adagrad (Duchi et al., 2011) are widely used in machine learning. While Adagrad adjusts its stepsize based on the individual problem instance, the algorithm learned adjusts its stepsize to the underlying data distribution.

### Multi-layer transformers

We now turn to the setting of general \(L\)-layer transformers, for any positive integer \(L\). The next theorem proves that certain critical points of the in-context loss effectively implement a specific preconditioned gradient algorithm, where the preconditioning matrix is the inverse covariance of the input distribution. Before stating this result, let us first consider a motivating scenario in which the data-covariance matrix is non-identity:

_Linear regression with distorted view of the data:_ Suppose that \(_{}(0,I)\) and the _latent_ covariates are \(^{(1)},,^{(n+1)}\), drawn i.i.d from \((0,I)\). We are given \(y^{(1)},,y^{(n)}\), with \(y^{(i)}=^{(i)},_{}\). However, we _do not observe_ the latent covariates \(^{(i)}\). Instead, we observe the _distorted_ covariates \(x^{(i)}=W^{(i)}\), where \(W^{d d}\) is a distortion matrix. Thus the prompt consists of \((x^{(1)},y^{(1)}),,(x^{(n)},y^{(n)})\), as well as \(x^{(n+1)}\). The goal is still to predict \(y^{(n+1)}\). Note that this setting is quite common in practice, when covariates are often represented in an arbitrary basis.

Assume that \(:=WW^{} 0\). We verify from our definitions that for \(w_{}:=^{-1/2}_{}\), \(y^{(i)}= x^{(i)},w_{}\). Furthermore, \(x^{(i)}(0,)\) and \(w_{}(0,^{-1})\). From Lemma 1, the transformer with weight matrices \(\{A_{0},,A_{L-1}\}\) implements preconditioned gradient descent with respect to \(R_{w_{}}(w)=(w-w_{})^{T}XX^{}(w-w_{})\), with \(X=[x^{(1)},,x^{(n)}]\). Under this loss, the Hessian matrix \(^{2}R_{w_{}}(w)=XX^{}\) (at least in the case of large \(n\)). For any fixed prompt, Newton's method corresponds to \(A_{i}(XX^{})^{-1}\), which makes the problem well-conditioned even if \(\) is very degenerate. As we will see in Theorem 3 below, the choice of \(A_{i}^{-1}=[XX^{}]^{-1}\) appears to be a _stationary point_ of the loss landscape, in expectation over prompts.

Before stating the theorem, we introduce the following simplified notation: let \(A:=\{A_{i}\}_{i=0}^{L-1}^{L d d}\). We use \(f(A)\) to denote the in-context loss of \(f(\{P_{i},Q_{i}\}_{i=0}^{L-1})\) as defined in (5), when \(Q_{i}\) depends on \(A_{i}\), and \(P_{i}\) is a constant matrix, as described in (8).

**Theorem 3**.: _Assume that \(x^{(i)}(0,)\) and \(w_{}(0,^{-1})\), for \(i=1,,n\), and for some \( 0\). Consider the optimization of in-context loss for a \(k\)-layer transformer with the the parameter configuration in Eq. (8) given by:_

\[_{\{A_{i}\}_{i=0}^{L-1}}f(A).\]

_Let \(^{L d d}\) be defined as follows: \(A\) if and only if for all \(i=0,,L-1\), there exists scalars \(a_{i}\) such that \(A_{i}=a_{i}^{-1}\). Then_

\[_{(A,B)}_{i=0}^{L-1}\|_{A_{i}}f(A,B)\| _{F}^{2}=0,\]

_where \(_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\|A_{i}\|_{F}\)._

As discussed in the motivation above, under the setting of \(A_{i}=a_{i}^{-1}\), the linear transformer implements an algorithm that is reminiscent of Newton's method (as well as a number of other adaptive algorithms such as the full-matrix variant of Adagrad); these can converge significantly faster than vanilla gradient descent when the problem is ill-conditioned. The proposed parameters \(A_{i}\) in Theorem 3 are also similar to \(A_{i}\)'s in Theorem 1 when \(n\) is large. However, in contrast to Theorem 1, there is no trade-off with statistical robustness; this is because \(w_{}\) has covariance matrix \(^{-1}\) in the Theorem 3, while Theorem 1 has isotropic \(w_{}\).

Unlike our prior results, Theorem 3 only guarantees that the set \(\) of transformer prameters satisfying \(\{A_{i}^{-1}\}_{i=0}^{L-1}\)_essentially2_ contains critical points of the in-context loss. However, in the next section, we show experimentally that this choice of \(A_{i}\)'s does indeed seem to be recovered by training.

We defer the proof of Theorem 3 to Subsection B.2. Due to the complexity of the transformer function, even verifying critical points can be challenging. We show that the in-context loss can be equivalently written as (roughly) a matrix polynomial involving the weights at each layer. Byexploiting invariances in the underlying distribution of prompts, we construct a flow, contained entirely in \(\), whose objective value decreases as fast as gradient flow. Since \(f\) is lower bounded, we conclude that there must be points in \(\) whose gradient is arbitrarily small.

### Experimental validations for Theorem 3

We present here an empirical verification of our results in Theorem 3. We consider the ICL loss for linear regression. The dimension is \(d=5\), and the number of training samples in the prompt is \(n=20\). Both \(x^{(i)}(0,)\) and \(w_{}(0,^{-1})\), where \(=U^{T}DU\), where \(U\) is a uniformly random orthogonal matrix, and \(D\) is a fixed diagonal matrix with entries \((1,1,0.25,0.0625,1)\).

We optimizes \(f\) for a three-layer linear transformer using ADAM, where the matrices \(A_{0},A_{1},\) and \(A_{2}\) are initialized by i.i.d. Gaussian matrices. Each gradient step is computed from a minibatch of size 20000, and we resample the minibatch every 100 steps. We clip the gradient of each matrix to 0.01. All plots are averaged over \(5\) runs with different \(U\) (i.e. \(\)) sampled each time.

Figure 0(d) plots the average loss. We observe that the training converges to an almost \(0\) value, suggesting the convergence to global minimum. The parameters at convergence match the stationary point introduced in Theorem 3, and indeed appear to be globally optimal.

To quantify the similarity between \(A_{0},A_{1},A_{2}\) and \(^{-1}\) (up to scaling), we use the _normalized Frobenius norm distance_: \((M,I):=_{}}\), (equivalent to choosing \(:=_{i=1}^{d}M[i,i]\)). This is essentially the projection distance of \(M/\|M\|_{F}\) onto the space of scaled identity matrices.

We plot \((A_{i},I)\), averaged over \(5\) runs, against iteration in Figures 0(a),0(b),0(c). In each plot, the blue line represents \((^{1/2}A_{i}^{1/2},I)\), and we verify that the optimal parameters are converging to the critical point introduced in Theorem 3, which implements preconditioned gradient descent. The red line represents \((A_{i},I)\); it remains constant indicating that the trained transformer is not implementing plain gradient descent. Figures 1(a)-1(c) visualize each \(^{1/2}A_{i}^{1/2}\) matrix at the end of training to further validate that the learned parameter is as described in Theorem 3.

## 5 Multi-layer transformers beyond standard optimization methods

In this section, we study the more general setting of

\[P_{i}=B_{i}&0\\ 0&1, Q_{i}=A_{i}&0\\ 0&0A_{i},B_{i}^{d d}.\] (11)

Note that \(A_{i},B_{i}\) are not constrained to be symmetric. Similar to Section 4, we introduce the following simplified notation: let \(A:=\{A_{i}\}_{i=0}^{L-1}^{L d d}\) and \(B:=\{B_{i}\}_{i=0}^{L-1}^{L d d}\). We use \(f(A,B)\) to denote the in-context loss of \(f(\{P_{i},Q_{i}\}_{i=0}^{L-1})\) as defined in (5), when \(P_{i}\) and \(Q_{i}\) depend on \(B_{i}\) and \(A_{i}\) as described in (11).

Figure 1: Plots for verifying convergence of general linear transformer, defined in Theorem 3. Figure (d) shows convergence of loss to \(0\). Figures (a),(b),(c) illustrate convergence of \(A_{i}\)’s to identity. More specifically, the blue line represents \((^{1/2}A_{i}^{1/2},I)\), which measures the convergence to the critical point introduced in Theorem 3 (corresponding to \(^{-1}\)-preconditioned gradient descent). The red line represents \((A_{i},I)\); it remains constant indicating that the trained transformer is not implementing plain gradient descent.

With this relaxed parameter configuration, it turns out transformers can learn algorithms beyond the conventional preconditioned gradient descent. The next theorem asserts the possibility of learning a novel preconditioned gradient method. Let \(L\) be a fixed but arbitrary number of layers.

**Theorem 4**.: _Let \(\) denote any PSD matrix. Assume that \(x^{(i)}(0,)\) and \(w_{}(0,^{-1})\), for \(i=1,,n\), and for some \( 0\). Consider the optimization of in-context loss for a \(L\)-layer linear transformer with the the parameter configuration in Eq. (11) given by:_

\[_{\{A_{i},B_{i}\}_{i=0}^{L-1}}f(A,B).\]

_Let \(^{2 L d d}\) be defined as follows: \((A,B)\) if and only if for all \(i\{0,,k\}\), there exists scalars \(a_{i},b_{i}\) such that \(A_{i}=a_{i}^{-1}\) and \(B_{i}=b_{i}I\). Then_

\[_{(A,B)}_{i=0}^{L-1}\|_{A_{i}}f(A,B)\| _{F}^{2}+\|_{B_{i}}f(A,B)\|_{F}^{2}=0,\] (12)

_where \(_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\|A_{i}\|_{F}\)._

In words, parameter matrices in \(\) implement the following algorithm: \(\{A_{i}=a_{i}^{-1}\}_{i=0}^{L-1}\) plays the role of a distribution-dependent preconditioner for the gradient steps. At the same time, \(B_{i}=b_{i}I\) transforms the covariates themselves to make the Gram matrix have better condition number with each iteration. When the \(=I\), the algorithm implemented by \(A_{i} I,b_{i} I\) is exactly the GD++ algorithm proposed in (von Oswald et al., 2023) (up to stepsize).

The result in (12) says that the set \(\)_essentially3_ contains critical points of the in-context loss \(f(A,B)\). In the next section, we provide empirical evidence that the trained transformer parameters do in fact converge to a point in \(\).

### Experimental validations for Theorem 4

The experimental setup is similar to Subsection 4.3: we consider ICL for linear regression with \(n=10,d=5\), with \(x^{(i)}(0,)\) and \(w_{}(0,^{-1})\), where \(=U^{T}DU\), where \(U\) is a uniformly random orthogonal matrix, and \(D\) is a fixed diagonal matrix with entries \((1,1,0.25,0.0625,1)\). We train a three-layer linear transformer, under the constraints in (11) which is less restrictive than (8) in Subsection 4.3. We train the matrices \(A_{0},A_{1},A_{2},B_{0},B_{1}\)4 using ADAM with the same setup as in Section Subsection 4.3. We repeat this experiment 5 times with different random seeds, each time we sample a different \(U\) (i.e. \(\)).

Figure 2: Visualization of learned weights for the setting of Theorem 3. We visualize each \(^{1/2}A_{i}^{1/2}\) matrix at the end of training. Note that the optimized weights match the stationary point discussed in Theorem 3.

In Figure 2(c), we plot the in-context loss through the iterations of ADAM; the loss appears to be converging to 0, suggesting that parameters are converging to the global minimum.

We next verify that the parameters at convergence are consistent with Theorem 4. We will once again use \((M,I)\) to measure the distance from \(M\) to the identity matrix, up to scaling (see Subsection 4.3 for definition of \(Dist\)). Figures 2(a) and 2(b) show that \(B_{0}\) and \(B_{1}\) are close to identity, as \((B_{i},I)\) appears to be decreasing to 0. Figures 2(d), 2(e) and 2(f) plot \((A_{i},I)\) (red line) and \((^{1/2}A_{i}^{1/2},I)\) (blue line); the results here suggest that \(A_{i}\) is converging to \(^{-1}\), up to scaling. In Figures 2(a) and 2(b), we observe that \(B_{0}\) and \(B_{1}\) also converge to the identity matrix (_without_ left and right multiplication by \(^{1/2}\)), consistent with Theorem 4.

We visualize each of \(B_{0},B_{1}\) in Figure 3(d) and \(A_{0},A_{1},A_{2}\) in Figure 4(a)-4(c) at the end of training. We highlight two noteworthy observations:

1. Let \(X_{k}^{d n}\) denote the first \(d\) rows of \(Z_{k}\), which are the output at layer \(k-1\) defined in (4). Then the update to \(X_{k}\) is \(X_{k+1}=X_{k}+B_{k}X_{k}MX_{k}^{T}A_{k}X_{k} X_{k+1}=X_{k}(I-|a_{ k}b_{k}|MX_{k}^{T}X_{k})\), where \(M\) is a mask defined in (2). As noted by von Oswald et al. (2023), this may be motivated by curvature correction.
2. As seen in Figures 4(a)-4(c) in the Appendix, \(\|A_{0}\|\|A_{1}\|\|A_{2}\|\) that implies the transformer implements gradient descent with a small stepsize at the beginning and a large stepsize at the end. This makes intuitive sense as \(X_{2}\) is better-conditioned compared to \(X_{1}\), due to the choice of \(B_{0},B_{1}\). This can be contrasted with the plots in Figures (2a)-(2c), where similar trends are not as pronounced because \(B_{i}\)'s are constrained to be 0.

## 6 Discussion

We take a first step toward proving that transformers can learn algorithms when trained over a set of random problem instances. Specifically, we investigate the possibility of learning gradient based methods when training on the in-context loss for linear regression. For a single layer transformer, we prove that the global minimum corresponds to a single iteration of preconditioned gradient descent. For multiple layers, we show that certain parameters that correspond to the critical points of the in-context loss can be interpreted as a broad family of adaptive gradient-based algorithms.

Figure 3: Plots for verifying convergence of general linear transformer, defined in Theorem 4. Figure (c) shows convergence of loss to \(0\). Figures (a),(b) illustrate convergence of \(B_{0},B_{1}\) to identity. Figures (d),(e),(f) illustrate convergence of \(A_{i}\)’s to \(^{-1}\).

We discuss below two interesting future directions.

**Beyond linear attention.** The standard transformer architecture comes with nonlinear activations in attention. Hence, the natural question here is to ask the effect of nonlinear activations for our main results. Empirically, von Oswald et al. (2023) have observed that for linear regression task, softmax activations generally degrade the prediction performance, and in particular, softmax transformers typically need more attention heads to match their performance with that of linear transformers.

As a first step analysis, we consider the nonlinear attention defined as

\[_{P,Q}^{}(Z) PZM\;(Z^{}QZ):.\]

The following result is an analog of Theorem1 for single-layer nonlinear attention. It characterizes a global minimizer for this setting with ReLU activation. Here, our choice of ReLU activation was motivated by Wortsman et al. (2023) who observed that ReLU attention matches the performance of softmax attention for vision transformers.

**Theorem 5**.: _Consider the single layer nonlinear attention setting with \(=\). Assume that vector \(x^{(i)}\) is sampled from \((0,I_{d})\). Moreover, assume that \(w_{}\) is sampled from \((0,I_{d})\). Consider the parameter configuration \(P_{0},Q_{0}\) where we additionally assume that the last row of \(Q_{0}\) is zero. Then, the following parameters form a global minimizer of the corresponding in-context loss:_

\[P_{0}=0_{d d}&0\\ 0&1, Q_{0}=-+(d+2)}I_{d}&0\\ 0&0.\]

The proof of Theorem5 involves an instructive argument and leverages tools from Erdogdu et al. (2016); we defer it to SubsectionA.4. Thus, for isotropic Gaussian data, the structure of global minimum under ReLU attention is similar to the global minimum with linear attention, established in Theorem1 (specifically the minimizer for the isotropic date given in (7)).

**Refined landscape analysis for multilayer transformer.** Theorem4 proves that a stationary point of the in-context loss corresponds to implementing a preconditioned gradient method. However, we do not prove that all critical points of the non-convex objective lead to similar optimization methods. In fact, in Lemma4 in AppendixB, we prove that the in-context loss can have multiple critical points. It will be interesting to analyze the set of all critical points and try to understand their algorithmic interpretations, as well as quantify their (sub)optimality.