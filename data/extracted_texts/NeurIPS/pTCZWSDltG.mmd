# CorresNeRF: Image Correspondence Priors for Neural Radiance Fields

Yixing Lao

The University of Hong Kong

yxlao@cs.hku.hk

&Xiaogang Xu

Zhejiang Lab, Zhejiang University

xgxu@zhejianglab.com

&Zhipeng Cai

Intel Labs

zhipeng.cai@intel.com &Xihui Liu

The University of Hong Kong

xihuiliu@eee.hku.hk &Hengshuang Zhao

The University of Hong Kong

hszhao@cs.hku.hk

Corresponding author

###### Abstract

Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf/.

## 1 Introduction

Building on coordinate-based implicit representations [1; 2; 3], Neural Radiance Field (NeRF)  has achieved great success in solving the fundamental computer vision problem of reconstructing 3D geometries from RGB images, benefiting various downstream applications. However, training such implicit representations typically requires a large number of input views, especially for objects with complex shapes, which can be costly to collect. Therefore, training NeRFs with sparse input RGB views remains a challenging yet important problem, where solving it can benefit various real-world applications, e.g., 3D portrait reconstruction in the monitoring system [5; 6; 7], the city digital reconstruction [8; 9; 10], etc.

Several works [11; 12; 13; 14; 15; 16] have been proposed to address this problem by optimizing the rendering process or adding training constraints. However, these methods may suffer from poor real-world performance , since only sparse 2D input views are an under-constrained problem [17; 13], and the training process is prone to overfitting on the limited input views. Recent works have proposed to utilize extra priors to supervise NeRF training [14; 16; 18]. For example, some work proposed to train a separate network to compute depth priors . However, current priors are not robust enough against the sparse property of the target scene, e.g., DS-NeRF  relies on running external SfM module  which may not yield sufficient point clouds for supervision, or not have an absolute scale and shift from the monocular depth estimation .

Our key observation is that image correspondences can provide strong supervision signals for the training of NeRF. In Figure 2, we visualize the point cloud created from triangulating the image correspondences pre-processed by our automatic augmentation and filtering (Section 3.2). Without training any NeRF networks, this triangulated point cloud clearly captures rich geometrical information about the target scene, which suggests that the image correspondences, along with the given camera parameters, can provide strong supervision signals for NeRF Training. The idea of using image correspondences as priors is widely applicable, since the correspondences can be estimated as long as there are sufficient textured overlapping regions among the input views, regardless of the NeRF variants. Additionally, the acquisition of image correspondence is inexpensive, as they can be directly computed using pre-trained off-the-shelf methods [22; 23; 24].

To improve the robustness of our method, we propose an automatic augmentation and outlier filtering process for the correspondences, ensuring their quantity and quality. Ablation studies demonstrate the effectiveness of our proposed augmentation and filtering strategy. To incorporate the correspondences into the NeRF training, we design novel correspondence loss terms, including pixel reprojection loss and depth loss. The reprojection loss is designed to constrain the distances between the reprojected corresponding points in the 2D pixel coordinates and the pixel-level distances. The depth loss is designed to constrain the relative depth differences between the corresponding points. Moreover, the confidence values from correspondence estimation are adopted as loss weights to avoid the negative impact of mismatching correspondences.

We evaluate our method on novel view synthesis and surface reconstruction tasks on LLFF  and DTU  datasets. We observe that the combination of correspondence priors via our method leads to significantly improved performance in novel-view synthesis (e.g., the PSNR of NeRF baseline  on LLFF is improved more than 3dB, SSIM is enhanced by 14%) and surface reconstruction (the DepthMAE of NeRF baseline on LLFF is decreased from 1.66 to 0.91, and Chamfer-\(L_{1}\) distance is reduced from 6.16 to 2.63 for NeuS  on DTU). Furthermore, we benchmark our method against other state-of-the-art sparse-view reconstruction methods on different datasets, and show that our approach (built based on the simple typical NeRF, i.e., vanilla NeRF  for view synthesis and NeuS  for surface reconstruction) outperforms the comparative methods in terms of photometric and geometrical metrics.

Figure 1: **Novel view synthesis and surface reconstruction from sparse inputs with image correspondence priors**. Given a sparse set of input images (column 1), our method leverages the image correspondence priors computed from pre-trained models (column 2) to supervise NeRF training. The color of the highlighted pixels represents the confidence of the correspondence, where higher confidence values are greener. With image correspondence supervision, we achieve much higher quality novel view synthesis (column 3) and surface reconstruction (column 4) compared to the baseline UNISURF  and NeuS  models. The correspondence priors can be used in both density-based and SDF-based neural implicit representations on both novel-view synthesis and surface reconstruction tasks. Best viewed in color.

We summarize our contributions below:

* We propose to use image correspondence as priors to supervise NeRF training, leading to better performance under challenging sparse-view scenarios.
* We design an adaptive pipeline to automatically augment and filter image correspondences, ensuring their quantity and quality.
* We propose robust correspondence loss, including pixel reprojection loss and depth loss based on correspondence priors.
* Extensive experiments are conducted on different baselines and datasets, showing the effectiveness of our method under different choices of neural implicit representations.

## 2 Related Work

Nerual implicit representations.Unlike traditional explicit 3D representations such as point cloud, mesh, or voxels, neural implicit representations [2; 1; 3] define a 3D scene with an implicit function that maps 3D coordinates and view directions to the corresponding properties (e.g., color and density) or features. The implicit function is typically modeled with neural networks. These types of representations are more compact and flexible towards different scenes, and they have been successfully applied to various tasks such as 3D reconstruction  and novel view synthesis .

Sparse-view NeRFs.Due to the practical limitation and the high cost of data collection, the data used for neural implicit representation learning is often sparse. To address this problem, researchers have recently proposed to incorporate geometric [27; 14; 13; 15; 28; 29; 16; 30] or pre-trained priors [12; 11; 31; 32; 33] to supervise the training of neural implicit representations. As a fundamental property of a scene, depth priors are commonly used, including depth generated from an SfM system , depth computed via monocular depth estimation , and depth predicted from a depth completion network [27; 14]. In contrast to these works, we rely on a more robust and flexible low-level feature, i.e., image correspondences, as supervision. The image correspondences obtained from modern image matchers [24; 23] are typically denser than the depth maps generated from traditional SfM system , when given sparse input views, since the matching is completed according to various characteristics in addition to geometrical information. With the known camera parameters, one can also compute absolute depth from the image correspondences, avoiding the ambiguity of shift and scale of depth maps from monocular depth estimation methods . ConsistentNeRF  uses the depths from pre-trained MVSNeRF  to derive the correspondence mask to emphasize the multi-view appearance consistency, while our method uses the correspondences computed from off-the-shelf image matchers to regularize geometric properties of the scene. Concurrent work

Figure 2: **Reconstructing point cloud by triangulating dense image correspondences. Column 1 shows the input images. Column 2 shows the correspondences between image pairs. With these correspondences and camera parameters, one can directly reconstruct 3D points without any training. Column 3 shows the reconstructed point cloud visualized using the same input cameras. The rightmost column shows the points rendered in novel view, as well as the camera poses. This example showcases the strong supervision signals that correspondences can bring to NeRF training. Best viewed in color.**

SPARF  uses image correspondences to refine noisy poses from sparse input views with a 2D pixel-space correspondence loss, while our method uses both a 2D pixel-space loss and a 3D depth-space loss. Our depth-space loss is normalized to compute the relative depth differences, adapting to different scene scalings and geometry properties.

Geometric matching.The task of finding pixel-level correspondences between two views of a 3D scene, known as geometric matching, is a fundamental computer vision problem. Early works of geometric matching are based on matching measurement via hand-crafted local features [36; 37; 38]. Later, detectors and feature descriptors learned through data-driven processes [39; 40; 41; 22] were proposed to substitute the hand-crafted features, surpassing their performance. Recently, detector free methods , transformer-based [42; 43], and dense geometric matching [44; 45; 24] have been proposed to further improve the performance. In this work, we use image correspondences generated by the state-of-the-art dense image matcher  as supervision for neural implicit representations.

## 3 Method

### Background of Neural Radiance Fields

For a given 3D point \(^{3}\) and a viewing direction \(^{3}\), a neural radiance field  predicts the corresponding density \([0,)\) and RGB color \(^{3}\), modeled by an MLP network, as

\[f_{}:((),())(,),\] (1)

where \(\) is the positional encoding function.

A ray is defined as \((t)=+t,t[t_{n},t_{f}]\), where \(\) is the camera center and \(\) is the ray direction, \(t_{n}\) is the near bound and \(t_{f}\) is the far bound. To render the ray \(\) with a pre-defined \(t_{n}\) and \(t_{f}\), we integrate the density \(\) and color \(\) along the ray, as

\[}_{}()=_{t_{n}}^{t_{f}}T(t)_{}( (t))_{}((t),)dt, T(t)= (-_{t_{n}}^{t}_{}((t))dt),\] (2)

where \(T(t)\) is the accumulated transmittance, \(_{}((t),)\) and \(_{}((t))\) are the predicted color and density output from \(f_{}\), respectively. The rendering is implemented via stratified sampling approach, where \(M\) points are sampled in \([t_{n},t_{f}]\), as \(\{x_{1},...,x_{M}\}\). The density and color can be obtained as

\[}_{}()=_{i=1}^{M}T_{i}(1-(-_{ }(x_{i})_{i}))_{}(x_{i},), T_{i}= (-_{j=1}^{i-1}_{}(x_{j})_{j}),\] (3)

where \(_{j}=t_{j+1}-t_{j}\) is the distance between adjacent samples. Specifically, for a ray \(r\), its predicted 3D point can be obtained by summing up the weighted depth values along the ray, as

\[=+(_{i=1}^{M}T_{i}(1-(-_{}(x_{i}) _{i}))t_{i}).\] (4)

To optimize parameter \(\) in the NeRF model, a set of input images and camera parameters are provided, and the mean squared error color loss is minimized for optimization, as

\[_{}(,)=_{}\|}_{}()-( )\|_{2}^{2},\] (5)

where \(\) is the set of rays in the training views, and \(()\) is the ground-truth color of the ray \(\).

### Generating Correspondences

In this paper, we focus on how to utilize the computed image correspondences to enhance the performance of neural implicit representations in NeRF. Thus, the quality of correspondence is crucial. For each pair of images in the training views, we compute the correspondences using an off-the-shelf SOTA pre-trained image-matching model. In particular, DKMv3  is used because it provides dense matching results, which is suitable for our use case. To improve generalization ability, we fuse the predictions of the indoor and outdoor models, which are pre-trained on ScanNet  and MegaDepth  respectively. To further enhance the reliability of the correspondences, we propose to utilize the correspondence confidence, and also design the automatic and adaptive correspondence process algorithm, increasing the convincing correspondences and removing the outliers.

Confidence.For ray \(_{q}\) in training rays \(\), the image matcher computes a set of corresponding rays \((_{q})\). For each pair of correspondences \(_{q}\) (query) and \(_{s}\) (support) in \(\{(_{q},_{s})_{q},_ {s}(_{q})\}\), a confidence value \(_{q,s}[0.5,1]\) is also predicted by DKMv3. Note that a ray \(_{q}\) can have 0, 1, or more corresponding rays \(_{s}\) in different images. Confidence scores are used as the scaling factors for the correspondence losses, which will be described in Sec. 3.3.

Augmentation.To increase the number of correspondences, we perform augmentations for correspondences. The first type of augmentation is image transformations, including flipping, swapping query and support images, and scaling. These image transformations can effectively increase the density of the predicted correspondences since the image transformations can provide various context conditions to generate correspondences. The second type of augmentation propagates correspondences across image pairs, effectively increasing the area coverage of the correspondences. We build an undirected graph \(=(,)\), with vertices \(=\{\}\), and edges \(=\{(_{q},_{s})_{s} (_{q})\}\). For each edge \((_{q},_{s})\), a confidence value \(_{q,s}\) is assigned. We then propagate the correspondence relationship to vertex pairs within each connected component in \(\). In particular, let \(_{q}\) and \(_{s}\) be two vertices with distance \(d\), where is a path \((_{q},_{1},_{2},,_{d-1},_{s})\) connecting them. We assign a correspondence relationship between \(_{q}\) and \(_{s}\) with confidence \(_{q,s}=_{q,1}_{1,2}_{d-1,s}\). In practice, we cap the propagation distance \(d d_{}\), where we use \(d_{}=2\) in our experiments. Figure 3 (B) and (C) show the original and augmented correspondences, respectively.

Outlier filtering.To increase the quality of the correspondences to guide the supervision, we remove outliers after calculating and enhancing the correspondences. First, we remove outliers according to the projected ray distance between corresponding points. Suppose \(_{q}\) and \(_{s}\) be a pair of 2D correspondence in \(I_{q}\) to \(I_{s}\), \(_{q}\) and \(_{s}\) be the world-to-pixel projection for \(I_{q}\) and \(I_{s}\), respectively. Given a pair of correspondences and camera parameters, we compute the closest 3D points \(_{q}\) and \(_{s}\) along the two rays shot from the camera centers through the two correspondences. Then, we project these two 3D points to the correspondence's image plane. The projected ray distance  is defined as the averaged Euclidean distance between the projected points and the correspondences:

\[d_{}=(_{s})-_{q}\|_{2}+\|_{s }(_{q})-_{s}\|_{2}}{2}.\] (6)

We remove the correspondences with projected ray distance \(d_{}\) larger than a threshold. Second, we remove outliers by checking if a point is statistically far from its neighbors. For each pair of correspondences, two 3D points \(_{q}\) and \(_{s}\) can be obtained, which has been indicated in the last paragraph. We then consider \((_{q}+_{s})\) to be the 3D point of the correspondence. We do this for all correspondence pairs to obtain a set of 3D points \(\). For each 3D point in \(\), we compute the average distance to its \(k\) nearest neighbors and remove the point (as well as its matched correspondence pair) if the distance is larger than a threshold. This threshold is determined by the standard deviation of all points' average distances in \(\).

Figure 3: **Correspondence generation process. The left column illustrates correspondence propagation in a connected component in the correspondence graph \(\). Given correspondence pairs \((_{q},_{m})\) and \((_{m},_{s})\) with confidence \(_{q,m}\) and \(_{m,r}\), we assign correspondence relationship to pair \((_{q},_{s})\) with \(_{q,s}=_{q,m}_{m,s}\). The right columns show the overall correspondence generation process: (A) input images; (B) correspondence pairs generated by the vanilla matcher; (C) correspondence pairs augmented by image transformations and correspondence propagation; and (D) correspondence pairs after outlier removal. Best viewed in color.**

Experiments in Sec. 4.5 demonstrate the effects of our proposed augmentation and removal strategy since they can increase the reliability of the correspondences utilized in NeRF's training.

### Correspondence Loss

We incorporate the obtained correspondences into NeRF training from two aspects: correspondence pixel reprojection loss and depth loss, as illustrated in Fig. 4.

Correspondence pixel reprojection loss.Let \(_{q}\) and \(_{s}\) be the corresponding 3D points predicted via NeRF, by accumulating the weighted sum of z-depth values to the ray origin according to Eq. 4. We compute the pixel-space reprojection loss as follows:

\[_{}(,)=_{_{q} ,_{s}(_{q})}\;_{q,s} (\|_{q}(_{s})-_{q}\|_{2}+\|_{s}(_{q} )-_{s}\|_{2}).\] (7)

The pixel-space reprojection loss is similar to the projected ray distance . The difference is that we reproject the predicted 3D points \(_{q}\) and \(_{s}\), instead of reprojecting the closest points along the two rays. Moreover, we utilize the correspondence score \(_{q,s}\) as weights to provide a more reliable metric. Since the camera parameters are known, the projected ray distance is constant. However, the reprojection of the predicted 3D points can provide a supervision signal to guide the NeRF model.

Correspondence depth loss.The correspondence depth loss term penalizes the difference between the predicted depth and the ground-truth depth. The ground-truth depth is estimated by computing the closest 3D points \(_{q}\) and \(_{s}\) along the two rays shot from the camera centers \(_{q}\) and \(_{s}\) through the two correspondences. We use a relative depth loss, which is adaptive to different scalings and can be described with the following equation:

\[_{}(,)=_{_{q} ,_{s}(_{q})}\;_{q,s} (\|_{q}-_{q}\|_{2}}{\|_{q}- _{q}\|_{2}}-1\|+|_{s}-_{s}\|_ {2}}{\|_{s}-_{s}\|_{2}}-1|),\] (8)

where \(_{q,s}\) is the confidence of correspondence to regular the effects of the relative depth loss. In the ideal scenario, \(_{q}\) should overlap with \(_{q}\) and similarly \(_{s}\) should overlap with \(_{s}\), so the loss will be zero. The relative depth loss is a smooth function and is minimized when the predicted depth is close to the estimated depth of the ground truth.

Training procedure.For each ray in a batch of rays \(_{}\), we query the correspondence map to find the corresponding rays in all training rays \(\). We assume that each ray may have \(B\) corresponding rays, \(B[0,+)\). For each pair of corresponding rays, we run the forward pass of the NeRF model to obtain the predicted 3D points. We then compute the pixel-space reprojection loss and the correspondence depth loss for each pair of corresponding rays. The final loss is the sum of pixel-space correspondence reprojection loss, correspondence depth loss, and the regular NeRF color loss \(_{}\):

\[=_{}+_{1}_{}+_{2}_{},\] (9)

where \(_{1}\) and \(_{2}\) are the loss weights, and we set them to \(0.1\) in our experiments.

## 4 Experiments

### Datasets

We compare novel view synthesis results on LLFF  with density-field-based NeRF models. We follow the convention to use every 8th image as test images , while selecting the training views uniformly from the rest of the images . The selected training views and test views are the same across all methods. Three input views are used for training.

We compare surface reconstruction results on DTU  with SDF-based NeRF models. We follow the convention to use the same 15 scenes from DTU as previous works [21; 49]. The selected training views and test views are the same across all methods. Three input views are used for training.

Figure 4: **Correspondence loss of a ray pair \((_{q},_{s})\). \(_{q}\) and \(_{s}\) are the ray origins. \(_{q}\) and \(_{s}\) are the closest points along the two rays. \(_{q}\) and \(_{s}\) are the 3D points predicted by the NeRF model. \(_{q}(_{s})\) and \(_{s}(_{q})\) are the 2D projections of \(_{s}\) and \(_{q}\) on the other image. The correspondence losses are defined with the geometric properties as illustrated.**

### Evaluation Metrics

For the novel view synthesis task, we report photometric metrics, including PSNR, structural similarity index (SSIM) , and the learned perceptual metric LPIPS . In addition to the image metrics, we also report the mean absolute error (MAE) for depth prediction. We compute the pseudo ground truth depth using the baseline NeRF model given _all_ input views. The depth error is calculated as the mean absolute difference between the predicted depths and the pseudo ground truth depths. Both the predicted depths and the pseudo ground truth depths are in the normalized coordinates.

For the surface reconstruction task, we report the Chamfer distance with DTU's official evaluation code , where the Chamfer distance is computed in the unnormalized world coordinates for the foreground objects. We also report the PSNR, SSIM, and LPIPS metrics for the rendered images with (object) and without (image) the foreground masks.

### Novel View Synthesis

**Quantitative comparison.** For the novel view synthesis task, we compare our method with the baseline NeRF  as well as few-view optimized DS-NeRF  and RegNeRF . DS-NeRF uses external SfM module COLMAP  to generate sparse point clouds for depth supervision, while RegNeRF applies additional regularizations on depths and colors rendered from unobserved views. The experiments are performed on the LLFF dataset with 3 input views. Our method outperforms all other methods regarding photometric metrics and depth prediction. The results are shown in Table 1. For DS-NeRF, we observe that if COLMAP is only given sparse-view inputs with known camera poses for SfM (as opposed to providing all views for SfM and selecting visible points from sparse views), COLMAP does not generate a sufficient number of points for depth supervision. In contrast, our method relies on image correspondences as inputs that are computed according to various characteristics. Therefore, it can leverage much denser prior information for supervision, as shown in Fig. 3.

**Qualitative comparison.** Besides the quantitative comparison, we provide the qualitative comparison to demonstrate the effects of our approach in improving novel view synthesis performance. The visual samples are shown in Fig. 5. Compared with baselines, our results have sharper and more accurate details and fewer artifacts. Such an improvement in performance is most visually evident in challenging cases such as small or thin structures.

    & PSNR \(\) & SSIM \(\) & LPIPS \(\) & Depht MAE \(\) \\  NeRF  & 16.79 & 0.56 & 0.37 & 1.66 \\ DS-NeRF  & 17.09 & 0.57 & 0.38 & 1.64 \\ RegNeRF  & 19.08 & 0.59 & 0.34 & 1.02 \\ Ours & **19.83** & **0.70** & **0.29** & **0.91** \\   

Table 1: **Quantitative results on LLFF. We compare novel view synthesis on the LLFF dataset for 3 input views. Our method outperforms the baseline NeRF model and the sparse-view optimized models. Our model is built with the vanilla NeRF as the direct baseline.**

    &  &  &  & \)} \\  & Object & Image & Object & Image & Object & Image & Object \\  UNISURF  & 12.90 & 13.85 & 0.47 & 0.58 & 0.28 & 0.57 & 7.23 \\ VolSDF  & 15.68 & 15.44 & 0.58 & 0.65 & 0.21 & 0.47 & 4.44 \\ NeuS  & 16.06 & 16.37 & 0.59 & 0.66 & 0.21 & 0.46 & 6.16 \\ Ours & **20.58** & **18.23** & **0.77** & **0.76** & **0.13** & **0.33** & **2.63** \\   

Table 2: **Quantitative results on DTU. We compare novel view synthesis and surface reconstruction with SDF-based methods on the DTU dataset for 3 input views. All models are trained without foreground masks. For evaluation, we report photometric metrics for the masked foreground object and the full image. The Chamfer-\(L_{1}\) geometric metric is computed with the official DTU evaluation code with foreground masks applied. Our model is built with NeuS as the direct baseline, achieving the best results across photometric and geometric metrics.**

### Surface Reconstruction

**Quantitative comparison.** We apply the same correspondence loss terms to the SDF-based neural implicit field method NeuS  and compare the results with the baseline NeuS model as well as other two SOTA SDF-based surface reconstruction method, UNISURF  and VolSDF . The experiments are performed on the DTU dataset with 3 input views. The results are shown in Table 2. Our method outperforms all baseline models in terms of Chamfer-\(L_{1}\) distances, i.e., ours generates more accurate surfaces. Moreover, ours also is also significantly better than the baselines in terms of rendering metrics, including PSNR, SSIM, and LPIPS. This further demonstrates that our approach can be plugged into different types of network backbones (density-based and SDF-based) and can be applied to different tasks (novel view synthesis and surface reconstruction).

**Qualitative comparison.** The visual comparisons between our method and the baseline for surface reconstruction are shown in Fig. 6. It's apparent that our 3D reconstruction results have more precise geometrical shapes. Combined with the correspondence priors, our CorresNeRF achieves clean and high-quality reconstruction even when given sparse input views.

### Ablation Studies

We perform ablation studies of the correspondence generation process, correspondence loss terms, robustness to correspondence noise, and the effect of foreground masks.

**Correspondence generation process.** We compare the result with augmentation removed, with automatic filtering removed, and with the full pipeline. The results are shown in the first two rows in Table 3. The result shows that both the augmentation and the automatic outlier filtering process are able to improve the reconstruction quality measured in photometric and geometric metrics. The augmentation step can provide denser supervision signals, while the filtering step is able to remove the noisy correspondences to further improve the reconstruction quality.

**Correspondence loss terms.** We evaluate the effectiveness of the correspondence pixel reprojection loss and correspondence depth loss terms. We compare the results with only the reprojection loss removed, with only the depth loss removed, and the full pipeline. The results are displayed in Table 3. Our results show that both correspondence reprojection loss and the correspondence depth loss contribute significantly to the reconstruction quality, each adding \(\)1 dB PSNR compared to the vanilla NeRF. The combined loss term yields the best performance, adding \(\)3 dB PSNR.

**Robustness to correspondence noise.** To evaluate the robustness of CorresNeRF to noisy correspondences, we introduce Gaussian noise with standard deviations of 1, 2, and 4 pixels to both the \(x\) and \(y\) pixel coordinates. We measure the performance of CorresNeRF on LLFF dataset with 3 input views. The results are shown in Table 4. The automatic filtering process is able to remove the noisy correspondences, demonstrating the robustness of CorresNeRF to noisy correspondences.

**Correspondence with mask supervision.** For the surface reconstruction task, we evaluate the performance of CorresNeRF when the foreground mask is provided. When the foreground mask is available, the correspondences are filtered by the mask, allowing the model to focus on the foreground region. The results are shown in Table 5. Our model outperforms the NeuS model in both the with-mask and without-mask settings. Qualitative visualization results are shown in Fig. 8 in the supplementary material.

## 5 Conclusion

We presented CorresNeRF, a method that can utilize the image correspondence priors for training neural radiance fields with sparse-view inputs. We propose automatic augmentation and filtering methods to generate dense and high-quality image correspondences from sparse-view inputs. We design reprojection and depth loss terms based on the correspondence priors to regularize the neural radiance field training. Experiments show that our method can significantly improve the reconstruction quality measured in photometric and geometric metrics with only a few input images.

**Limitations and future work.** In CorresNeRF, convincing correspondences are obtained by using SOTA matching networks and further processed via adaptive augmentation and outlier removal. An
Figure 5: **Qualitative results from the LLFF dataset.**

Figure 6: **Qualitative results from the DTU dataset.**

admined number of correspondences can be acquired for most practical scenes (proved by experiments on various benchmarks). However, there are still some extreme cases where few correspondences are computed via matching networks (due to unreasonable camera positions or some specific textures of target scenes). Correspondence generation methods are needed for these cases to synthesize convincing correspondences along with the optimization of NeRF. Some recent works [53; 54] have proved that neural implicit representations can be utilized to learn correspondences in turn. Dealing with such extreme scenes will be our future work.