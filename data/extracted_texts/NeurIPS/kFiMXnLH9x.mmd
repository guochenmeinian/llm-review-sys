# Learning Over Molecular Conformer Ensembles:

Datasets and Benchmarks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MolecularR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules. In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models. Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.

## 1 Introduction

Recent years have seen the emergence of Molecular Representation Learning (MRL) as a promising approach for modeling molecules with machine learning. In the typical formulation, MRL maps discrete molecular objects to continuous features in a data-driven manner, encoding complex chemical structures into representation vectors that can subsequently be utilized in different downstream tasks. In particular, MRL now underpins a variety of biochemical applications spanning molecular property prediction to the design of novel drug candidates [1; 2; 3].

Traditional approaches often encode chemical compounds with fingerprints, such as extended-connectivity fingerprints [4; 5], which indicate the existence of certain substructures as binary bits in a fixed-length sequence. Such line-based representations are concise and efficient, but have limited expressive power and have difficulty in capturing 3D structural information such as bonding geometries and global shapes, which can be important for analyzing molecular properties and chemical reactivity [6; 7]. Recently, Graph Neural Networks (GNNs) have become an increasingly popular method of learning molecular representations by treating molecules as graph-structured objects. Existing GNN models for MRL can be broadly classified into two categories: 2D topological models [8; 9; 10; 11] and 3D geometric models [12; 13; 14; 15; 16; 17]. 2D GNNs typically model the molecular connectivity as a flat 2D graph with atoms as nodes and bonds as edges, learning representations of chemical environments by iteratively passing messages between neighboring atoms. Although powerful in the absence ofstructural information, 2D GNNs may fail to capture key conformational effects or stereochemical properties like chirality [18; 19], which is critical for modeling molecular interactions in areas such as drug design or chemical catalysis. Conversely, 3D GNNs are designed to model molecular conformers (conformations), which describe the structure of molecules in 3D space. Thus, these models have found widespread adoption for modeling electronic properties, predicting conformer energies and forces, and scoring interactions between ligands and proteins, amongst other applications.

In almost all applications, benchmarks, and demonstrations, 3D GNN models focus on encoding _individual_ conformer structures. It is critical to recognize that in reality molecules are not rigid, static objects; rather, thermodynamically-permissible rotations of chemical bonds, small vibrational motions, and dynamic intermolecular interactions cause molecules to continuously convert between different conformations . As a consequence, many experimentally observable chemical properties depend on the full distribution of thermodynamically-accessible conformers. For example, a molecule needs to be arranged into a particular pose to bind to a target protein, and this binding conformation changes depending on the dynamic interaction between the molecule and the target . Also, it is often challenging to determine _a priori_ the conformers that predominantly contribute to molecular properties without doing prohibitively expensive simulations. Therefore, a natural question arises: can we leverage the _collective_ power of many different conformer structures lying on the local minima of the potential energy surface, also known as the _conformer ensemble_, to improve MRL models?

As shown by the empirical evidence from various studies, learning from an explicit conformer ensemble can prove to be advantageous for many tasks, including property and energy prediction [22; 23; 24], key conformer pose identification , and RNA sequence design . However, these studies have been mostly confined to small-scale datasets, a limited set of tasks, and a restricted set of model architectures. As a result, it remains unclear (1) to what extent 2D GNNs can implicitly model molecular flexibility and (2) whether the _explicit_ encoding of conformer ensembles can improve the performance of 3D models that traditionally encode only one single conformer.

In this paper, we present the first MolecularARConformerEnsembleLearning (MARCEL) benchmark. As shown in Figure 1, MARCEL covers a diverse range of chemical space, which focuses on four chemically-relevant tasks for both molecules and reactions, with an emphasis on Boltzmann-averaged properties of conformer ensembles computed at the Density-Functional Theory (DFT) level. Our datasets encompass a variety of compounds with high-quality conformers, including organocatalysts and transition-metal catalysts, extending beyond the scope of conventional GNN benchmarks which are often restricted to drug-like molecules. Moreover, we implement a comprehensive benchmark suite that enables extensive empirical studies across representative 1D, 2D, and 3D MRL models. We further explore the advantages of leveraging conformer ensembles through two straightforward strategies: (1) augmenting training samples by randomly selecting one conformer from the ensemble for each molecule and (2) applying an explicit multi-instance ensemble learning layer, which aggregates individual conformer embeddings.

Our experimental results confirm the potential effectiveness of incorporating conformer ensembles in MRL, highlighting the improvements over conventional single-conformation 3D networks. However, it is important to understand the heterogeneity of outcomes based on different dataset characteristics, task objectives, and model choices. Our investigation yields three key findings: (1) Leveraging molecular conformers by incorporating explicit set encoders, as a part of conformer ensemble learning strategies, can improve single-conformer 3D MRL models performance. (2) Data augmentation through conformer sampling may offer potential benefits, evidenced by improved results in the BDE dataset, suggesting a method to increase model robustness against imprecise structures. (3) Model selection for MRL depends on dataset sizes and tasks, with traditional 1D fingerprints and 2D models preferred for smaller datasets and 3D models for larger or reaction-focused tasks.

Figure 1: We present a MARCEL benchmark that comprehensively evaluates the potential of learning on conformer ensembles across a diverse set of molecules, datasets, and models.

## 2 Problem Formulation

We represent a 2D molecular graph as a tuple \(=(,,,)\), where \(=\{v_{i}\}_{i=1}^{||}\) is the node set with each node corresponding to an atom, and \(\) is the edge set representing chemical bonds as edges between nodes. Further, \(^{d_{v}||}\) contains vector attributes for each node, and \(^{d_{w}||}\) contains attributes for each edge. When modeling chemical reactions, we represent a molecule-molecule complex as a pair of graphs \((_{1},_{2})\). In this case, the conformation describes the combined structure of the interacting molecules. For a given molecule or molecular complex, we assume that its geometry can be effectively characterized by a representative set of discrete, sampled conformers from the thermodynamically-accessible conformer distribution. Formally, this set can be denoted as \(=\{_{i}\}_{i=1}^{||}\), where \(_{i}^{|| 3}\) represents one conformer structure in 3D space. In reality, the conformer distribution is continuous; \(\) in our study contains representative samples of the infinite set. Each conformer in the sampled ensemble is associated with a statistical weight given by \(p_{i}=}{k_{B}T})}{_{j}(-}{k_{B}T})}\), which corresponds to its probability under experimental conditions. Here, \(e_{i}\) is the energy of the conformer \(_{i}\), \(k_{B}\) is the Boltzmann constant, and \(T\) is the temperature. Notably, \(p_{i}\) is not prior information to the models analyzed in this benchmark. Rather, we use a discrete approximation of \(p_{i}\) to compute the ground-truth labels for our regression tasks.

## 3 Datasets and Tasks

MARCEL contains four small-to-large-scale datasets involving nine regression tasks with considerably diverse chemistry. Drugs-75K and Kraken focus on molecular properties, while EE and BDE focus on reaction-centric properties. MARCEL includes molecules with high structural flexibility, evidenced by an average number of rotatable bonds exceeding 5. Table 1 summarizes the datasets.

**Drugs-75K** is a subset of the GEOM-Drugs  dataset, which includes 75,099 molecules with at least 5 rotatable bonds. For each molecule, we focus on three important quantum chemical descriptors: ionization potential, electron affinity, and electronegativity . The tasks are to predict the Boltzmann-averaged value of each property across the conformer ensemble \( y_{k_{B}}=_{_{i}_{i}}p _{i}y_{i}\), where \(y_{i}\) is a conformer-specific property. We are given each \(_{i}\), and the goal is to predict \( y_{k_{B}}\) from the molecular graph \(\), a single conformer \(_{i}\), or the set \(\).

**Kraken** is a dataset of 1,552 monodentate organophosphorus (III) ligands along with their DFT-computed conformer ensembles. In this study, we consider four 3D ligand descriptors exhibiting significant variance among conformers: Sterimol \(_{5}\), Sterimol L, buried Sterimol \(_{5}\), and buried Sterimol L. These descriptors quantify the steric features of each ligand in units of A and are often employed for Quantitative Structure-Activity Relationship (QSAR) modeling in catalyst design.

As in the Drugs-75K tasks, the goal is to predict the Boltzmann-averaged value of each property across the conformer ensemble from the molecular graph \(\), a single conformer \(_{i}\), or the set \(\).

**EE** is a dataset of 872 catalyst-substrate pairs involving 253 Rh-bound atropisomeric catalysts derived from chiral bisphosphine, with 10 enamides as substrates. The dataset includes conformations of catalyst-substrate transition state complexes in two separate pro-S and pro-R configurations. The task is to predict the Enantiomeric Excess (EE) of the chemical reaction involving the substrate. Unlike properties in Drugs-75K and Kraken, EE depends on the conformer ensembles of _each_ pro-R and pro-S complex. The goal is to predict EE from the graphs of the catalyst and substrate \((_{},_{})\), a conformer \(_{i}^{()}^{()}\) and \(_{i}^{()}^{()}\) for each complex, or the ensembles \(^{()}\) and \(^{()}\).

**BDE** is a dataset containing 5,915 organometallic catalysts \(_{1}_{2}\) consisting of a metal center coordinated to two flexible organic ligands. The data includes conformations of each unbound catalyst, as well as conformations of the catalyst when bound to ethylene and bromide after oxidative addition with vinyl bromide. Each catalyst has an electronic binding energy to be predicted. Although the binding energies are computed via DFT, the conformers provided for modeling are initially generated with Open Babel  followed by further geometry optimization, which ensures that the 3D structures are likely to be the global minimum energy conformers at the force field level . This dataset realistically represents the setting in which precise conformer ensembles are unknown at inference. The task is to predict the binding energy from the graphs of the unbound and bound catalyst, sampled conformers \(_{i}^{()}^{()}\) and \(_{i}^{()}^{()}\), or the ensembles \(^{()}\) and \(^{()}\).

[MISSING_PAGE_EMPTY:4]

We use atom types as the sole atom features for the 3D models. For both training and inference on Drug-75K, Kraken, and EE datasets, all the single-conformer 3D models encode the lowest-energy conformer of each conformer ensemble, which has the largest Boltzmann weight and hence provides the strongest model. Since imprecise conformers are encoded for the BDE task, we use a fixed, randomly sampled conformer for each unbound- and bound-catalyst during training and inference.

The 3D models also employ a two-tower architecture for the EE and BDE datasets. Two separate 3D GNNs are used to encode representations for each pro-S and pro-R complex in EE, or for each catalyst and bound complex in BDE, which are then concatenated to form the final representations.

We note that although using the lowest-energy conformer will yield the strongest performance, this setting can be unrealistic: it is often not possible to identify the lowest energy conformer without searching the entire conformer space. The lowest energy conformer can also depend on the force field used for geometry optimization, which may neglect experimental conditions such as solvents.

### Incorporating Conformer Ensembles into Molecular Representations

3D geometric models primarily focus on learning representations from individual 3D structures. Although some models preserve global symmetries such as SE(3)-equivariance, these models do not learn representations that capture conformational flexibility which is caused by internal degrees of freedom such as bond rotations. Here, we describe two straightforward strategies that model conformational flexibility by explicitly leveraging conformer ensembles.

#### 4.4.1 Strategy 1: Training-Time Data Augmentation via Conformer Sampling

A direct approach to modeling conformer flexibility is to simply enrich the training data by randomly sampling a conformer from the ensemble during each training epoch. Formally, for a given molecule G and its conformer ensemble \(\), we randomly select a conformer with uniform probability \(p=}{{}}\) while using the same training label for each conformer. Note that during inference, the lowest-energy conformer is used to evaluate the model performance. This strategy aligns with learning representations invariant to conformational changes, thus implicitly encoding the flexibility of molecular structures, and has been shown to be useful for learning chirality-sensitive 3D representations . When conformer ensembles are available, the strategy is computationally efficient as it maintains the same complexity as the base 3D model. Unlike the other ensemble methods, this strategy can be used if conformer ensembles are only available at training time. In Appendix C, we evaluate two alternative scenarios where conformer ensembles are also available during evaluation.

#### 4.4.2 Strategy 2: Ensemble Learning with Explicit Set Encoders

The second strategy utilizes a set encoder to simultaneously encode the entire conformer ensemble \(\) at both training and inference time. Inspired by the multi-instance learning framework , this strategy first employs 3D GNNs to generate individual conformer embeddings and then aggregates these embeddings using a set encoder, as illustrated in Figure 2.

Formally, for each conformer \(_{i}\), we obtain its corresponding embedding \(_{i}=f(,_{i})^{d}\), where \(f\) is a single-conformer 3D model and \(d\) is the embedding dimension. Note that the embedding \(\) is a (3D) graph-level representation resulting from a pooling function over the node-level embeddings after message passing. To further aggregate these embeddings \(=\{_{i}\}_{i=1}^{||}\) into a single molecular representation, we consider the following three set encoders:

* **Mean pooling** simply computes the mean of all the conformer embeddings.
* **DeepSets** utilizes a permutation-invariant function to process a set of inputs. It first applies a MultiLayer Perceptron (MLP) \(h\) to each conformer embedding and then aggregates the transformed embeddings using sum pooling followed by another MLP \(g\): \[^{}=g(_{i=1}^{||}h(_{i})).\] (1) This method retains more discernible information from individual embeddings compared to mean pooling at a cost of two non-linear functions.

* **Self-attention** further computes a weighted sum of the embeddings, where the weights are obtained by applying a softmax function to the dot product of the embeddings: \[^{}=_{i=1}^{||}_{i}, \ \ _{i}=g(_{j=1}^{||}_{ij}h(_{j})),\ \ _{ij}=h(_{i}))^{}(h(_{j})))}{_ {k=1}^{||}((h(_{i}))^{}(h(_{k})))}.\] (2) Here, \(^{d d}\) is a learnable weight matrix. This approach can capture conformer interactions.

By employing these set encoders, we can learn a model that is more sensitive to the full range of conformer variations present in the ensemble. After obtaining the ensemble embeddings, we further apply a linear projection head to generate the final prediction.

## 5 Experiments

### Experimental Configurations

Each dataset is partitioned randomly into three subsets: 70% for training, 10% for validation, and 20% for test. Each model is trained over 2,000 epochs using the Adam optimizer  with early stopping triggered if there is no improvement on the training loss over 200 epochs. For all nine regression targets, experiments are repeated three times, and the results reported correspond to the model that performs best on the validation set in terms of Mean Absolute Error (MAE).

The Boltzmann-averaged targets are computed over all available conformers. For ensemble learning models, we cap the number of encoded conformers per molecule to a maximum of 20, which empirically improves training stability and leads to better performance. To ensure a fair comparison, the hidden dimension size is uniformly set to 128 for all models. Other settings mostly follow the original configurations as described in the respective papers. We specify all hyperparameters and describe experimental environments in Appendix B.3.

### Results and Analysis

We summarize the performance of the 1D, 2D, and 3D MRL models in Table 2. Figure 3 reports the _performance changes_ in Mean Absolute Error (MAE) for each 3D model when applying the ensemble learning strategies. The raw performance data with standard deviation and the parameter size of each model can be found in Appendix D. In summary, although performance varies across the datasets, tasks, and models, the ensemble learning strategies improve upon 3D models that only encode one conformer in 48 out of 54 experiments across 9 tasks and 6 base models, demonstrating the effectiveness of conformer ensemble learning. Our analysis leads to the following key observations.

**Observation 1. The conformer ensemble learning strategy with explicit set encoders frequently yields improved performance.**

Figure 3 indicates that encoding conformer ensembles can substantially reduce test error, achieving improvements in 108 experiments across all 9 tasks, 6 base models, and 3 set encoders, most notably on the tasks in the smaller-sized Kraken dataset. This, however, does not always extend to larger datasets like Drugs-75K. We conjecture that for Drugs-75K, the computational burden of encoding all conformers in each ensemble alters the learning dynamics of the underlying model, making training more challenging. A similar finding was reported by Axelrod and Gomez-Bombarelli .

Among the three set encoders, DeepSets consistently demonstrates significant improvements in 42 out of 54 experiments across 9 tasks and 6 base 3D models. We conjecture that this superior performance

Figure 2: Conformer ensemble learning with explicit set encoders (Strategy 2). Individual conformer embeddings are first obtained via 3D GNN encoders. Then, a set encoder is employed to aggregate conformer embeddings. Finally, a linear projection head is used to generate the prediction.

is due to its ability of effectively modeling set objects at a relatively minor computational overhead of two non-linear transformations. On the other hand, the simple mean pooling approach loses discriminative power across the conformers in the ensemble, resulting in inferior performance. It is also noteworthy that the attention models exhibit mixed results compared to the vanilla 3D models, despite theoretically being the most powerful set encoders. This inconsistency might be attributable to the computational intricacy of the self-attention layer, which models the pairwise relationship among conformers in each ensemble and hence could require more sophisticated training strategies. Future research should consider developing better neural architectures that are specifically designed to more efficiently encode structural information from conformer ensembles.

**Observation 2. Sampling conformers at training time can improve performance, especially on imprecise conformer structures.**

We observe that data augmentation improves performance on 34 experiments, especially on the challenging BDE dataset, where the other ensemble learning strategies often do not help. Note that the conformers in the BDE dataset originate from Open Babel, as opposed to the golden-standard DFT-level conformers present in other datasets. This suggests that training with randomly sampled conformers might offer robustness to noise in the imprecise structures. On other tasks, randomly sampling the conformers at each epoch may help the model learn an invariance to conformational changes, but does not always increase performance for all 3D models. This might be because the sampling probability is uniform across the entire conformer set, which does not respect the underlying Boltzmann weight of each conformer. In future work, it may be worthwhile to investigate whether more physics-informed sampling strategies could lead to more consistent performance gains.

**Observation 3. No model consistently outperforms the rest, with substantial task dependencies.**

The results in Table 2 suggest that no single model outperforms the others across all tasks. Of the 1D models, LSTM outperforms Random Forest and Transformer models on Drugs-75K and BDE, demonstrating the effectiveness of SMILES-based representations of molecules on large-scale datasets. For small datasets such as Kraken and EE, Random Forests outperform sequence models at a lower computational cost, indicating that traditional models are superior in the low-data regime.

Amongst 2D models, GIN delivers the best performance on four tasks compared to all other models; GraphGPS also demonstrates strong performance on several tasks (B\({}_{5}\), L, and BurL). Surprisingly, the 2D models are also competitive with some 3D models on the large-scale Drugs-75K tasks. This is possibly due to the fact that the electronic properties in Drugs-75K are not as sensitive to conformational changes, thus explicitly modeling the structures in 3D may not be necessary. However, all 2D models perform worse as compared to the 3D models in the reaction datasets EE and BDE, indicating the important role of spatial interactions in determining reaction-related properties.

For 3D models, GemNet and LEFTNet excel in IP, EA, and \(\). The complexity of these two equivariant models may especially benefit from the large dataset size of Drugs-75K. For Kraken and the two reaction datasets, DimeNet++ -- an invariant model -- achieves promising performance, suggesting that highly-complex 3D models may be less useful for chemical applications with small-to-medium sized datasets. On EE, we observe that 3D models remarkably outperform 1D and 2D models, likely

    &  &  &  &  & BDE \\   & & IP & EA & \(\) & B\({}_{5}\) & L & BurB\({}_{5}\) & BurL & \\   & Random forest & 0.4987 & 0.4747 & 0.2732 & 0.4760 & 0.4303 & 0.2758 & 0.1521 & 61.2963 & 3.0335 \\  & LSTM & 0.4788 & 0.4648 & 0.2505 & 0.4879 & 0.5142 & 0.2813 & 0.1924 & 64.0088 & 2.8279 \\  & Transformer & 0.6617 & 0.5850 & 0.4703 & 0.9611 & 0.8389 & 0.4929 & 0.2781 & 62.0816 & 10.0771 \\   & GIN & 0.4354 & 0.4169 & 0.2260 & 0.3128 & 0.4003 & 0.1719 & 0.1200 & 62.3065 & 2.6368 \\  & GIN+VN & 0.4361 & 0.4169 & 0.2267 & 0.3567 & 0.4344 & 0.2422 & 0.1741 & 62.3815 & 2.7417 \\  & ChemProp & 0.4595 & 0.4417 & 0.2441 & 0.4850 & 0.5452 & 0.3002 & 0.1948 & 61.0336 & 2.6616 \\  & GraphGPS & 0.4351 & 0.4085 & 0.2212 & 0.3450 & 0.4363 & 0.2066 & 0.1500 & 61.6251 & 2.4827 \\   & SchNet & 0.4394 & 0.4207 & 0.2243 & 0.3293 & 0.5458 & 0.2295 & 0.1861 & 17.7421 & 2.5488 \\  & DimeNet++ & 0.4441 & 0.4233 & 0.2436 & 0.3510 & 0.4174 & 0.2097 & 0.1526 & 14.6414 & **1.4503** \\   & GemNet & 0.4069 & 0.3922 & **0.1970** & 0.2789 & 0.3754 & 0.1782 & 0.1635 & 18.0338 & 1.6530 \\   & PaN & 0.4505 & 0.4495 & 0.2324 & 0.3443 & 0.4471 & 0.2395 & 0.1673 & 20.2359 & 2.1261 \\   & ClotNet & 0.4393 & 0.4251 & 0.2378 & 0.4873 & 0.6417 & 0.2884 & 0.2529 & 33.9473 & 2.6057 \\   & LEFTNet & 0.4174 & 0.3964 & 0.2083 & 0.3072 & 0.4493 & 0.2176 & 0.1486 & 19.7974 & 1.5328 \\   

Table 2: Performance of 1D, 2D, and 3D baseline MRL models and the best results from ensemble learning strategies on 3D GNNs. The metric used is the Mean Absolute Error (MAE, \(\)). The **bold** indicates the best-performing model, while underlined denotes the second-best.

because enantioselectivity depends on subtle spatial interactions. When predicting binding energies, using 3D models also leads to modest improvements.

Overall, model performance varies substantially across tasks, even within the same dataset, emphasizing the diversity of the tasks in MARCEL. Generally, 1D and 2D models perform well on small-scale molecular datasets, while 3D models excel on large datasets and reaction-centric tasks. MARCEL also highlights the benefits of explicitly encoding multiple conformers to improve MRL.

## 6 Discussions and Conclusions

In this work, we present the first MoleculAR Conformer Ensemble Learning benchmark (MARCEL) to evaluate the potential of learning from a set of conformer structures. Through two conformer ensemble learning strategies, we discover performance improvements across various tasks. However, there are some limitations that require further consideration. First, our studied ensemble learning strategies do not universally improve performance across all tasks and datasets. This highlights the need for more tailored approaches that integrate with domain expertise to better model specific tasks and datasets of practical interest. Second, the computational cost of encoding all conformers within the ensembles, especially for larger datasets, suggests the need to further study the trade-offs between model complexity and efficiency. Finally, our datasets only contain regression tasks and do not cover all of the relevant chemical space, which might limit the generalization of our experimental findings.

Despite these challenges, we envision that our work will prompt further research in the geometric deep learning community on how to make use of conformer ensembles for molecular property prediction. For instance, future research could explore new model architectures that can efficiently encode ensemble-level information or more sophisticated conformer sampling strategies. We also hope that our work will stimulate collaborative research across the machine learning and chemistry fields, with the ultimate goal of pushing the boundaries of predictive molecular modeling and aligning algorithmic advancements with the practical needs of the chemistry community.

Figure 3: _Performance changes_ of four conformer ensemble learning strategies on the basis of six 3D graph models. Here, negative values (marked in batch patterns ) denote _reduced_ Mean Absolute Error (MAE), signifying a performance improvement due to the incorporation of conformer ensembles.