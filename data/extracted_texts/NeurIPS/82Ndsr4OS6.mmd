# Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning

Honghao Wei

Washington State University

honghao.wei@wsu.edu

&Xiyue Peng

ShanghaiTech University

pengxy2024@shanghaitech.edu.cn

&Arnob Ghosh

New Jersey Institute of Technology

arnob.ghosh@njit.edu

&Xin Liu

ShanghaiTech University

liuxin7@shanghaitech.edu.cn

###### Abstract

We propose WSAC (Weighted Safe Actor-Critic), a novel algorithm for Safe Offline Reinforcement Learning (RL) under functional approximation, which can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor's performance is inferior to the reference policy. In theory, we demonstrate that when the actor employs a no-regret optimization oracle, WSAC achieves a number of guarantees: \((i)\) For the first time in the safe offline RL setting, we establish that WSAC can produce a policy that outperforms **any** reference policy while maintaining the same level of safety, which is critical to designing a safe algorithm for offline RL. \((ii)\) WSAC achieves the optimal statistical convergence rate of \(1/\) to the reference policy, where \(N\) is the size of the offline dataset. \((iii)\) We theoretically show that WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism, indicating its practical robustness. Additionally, we offer a practical version of WSAC and compare it with existing state-of-the-art safe offline RL algorithms in several continuous control environments. WSAC outperforms all baselines across a range of tasks, supporting the theoretical results.

## 1 Introduction

Online safe reinforcement learning (RL) has found successful applications in various domains, such as autonomous driving (Isele et al., 2018), recommender systems (Chow et al., 2017), and robotics (Achiam et al., 2017). It enables the learning of safe policies effectively while satisfying certain safety constraints, including collision avoidance, budget adherence, and reliability. However, collecting diverse interaction data can be extremely costly and infeasible in many real-world applications, and this challenge becomes even more critical in scenarios where risky behavior cannot be tolerated. Given the inherently risk-sensitive nature of these safety-related tasks, data collection becomes feasible only when employing behavior policies satisfies all the safety requirements.

To overcome the limitations imposed by interactive data collection, offline RL algorithms are designed to learn a policy from an available dataset collected from historical experiences by some behavior policy, which may differ from the policy we aim to learn. A desirable property of an effective offline algorithm is the assurance of robust policy improvement (RPI), which guarantees that a learned policy is always at least as good as the baseline behavior policies (Fujimoto et al., 2019; Laroche et al., 2019;Kumar et al., 2019; Siegel et al., 2020; Chen et al., 2022a; Zhu et al., 2023; Bhardwaj et al., 2024). We extend the property of RPI to offline safe RL called safe robust policy improvement (SRPI), which indicates the improvement should be _safe_ as well. This is particularly important in offline safe RL. For example, in autonomous driving, an expert human driver operates the vehicle to collect a diverse dataset under various road and weather conditions, serving as the behavior policy. This policy is considered both effective and safe, as it demonstrates proficient human driving behavior while adhering to all traffic laws and other safety constraints. Achieving a policy that upholds the SRPI characteristic with such a dataset can significantly mitigate the likelihood of potential collisions and other safety concerns.

In offline RL, we represent the state-action occupancy distribution of policy \(\) over the dataset distribution \(\) using the ratio \(w^{}=d^{}/\). A commonly required assumption is that the \(_{}\) concentrability \(C^{}_{_{}}\) is bounded, which is defined as the infinite norm of \(w^{}\) for **all** policies (Liu et al., 2019; Chen and Jiang, 2019; Wang et al., 2019; Liao et al., 2022; Zhang et al., 2020). A stronger assumption requires a uniform lower bound on \((a|s)\)(Xie and Jiang, 2021). However, such all-policy concentrability assumptions are difficult to satisfy in practice, particularly for offline safe RL, as they essentially require the offline dataset to have good coverage of **all** unsafe state-action pairs. To address the full coverage requirement, other works (Rashidinejad et al., 2021; Zhan et al., 2022; Chen and Jiang, 2022; Xie et al., 2021; Uehara and Sun, 2021) adapt conservative algorithms by employing the principle of pessimism in the face of uncertainty, reducing the assumption to the best covered policy (or optimal policy) concerning \(_{}\) concentrability. Zhu et al. (2023) introduce \(_{2}\) concentrability to further relax the assumption, indicating that \(_{}\) concentrability is always an upper bound of \(_{2}\) concentrability (see Table 1 for detailed comparisons with previous works). While provable guarantees are obtained using single policy concentrability for unconstrained MDP as Table 1 suggests for the safe RL setting, all the existing studies (Hong et al., 2024; Le et al., 2019)_still_ require the coverage on **all** the policies. Further, as Table 1 suggests, the above papers do not guarantee robust safe policy improvement. Our main contributions are summarized below:

1. We prove that our algorithm, which uses average Bellman error, enjoys an optimal statistical rate of \(1/\) under partial data coverage assumption. _This is the first work that achieves such a result using only single-policy \(_{2}\) concentrability_.
2. We propose a novel offline safe RL algorithm, called Weighted Safe Actor-Critic (WSAC), which can robustly learn policies that improve upon any behavior policy with controlled relative pessimism. We prove that under standard function approximation assumptions, when the actor incorporates a no-regret policy optimization oracle, WSAC outputs a policy that never degrades the performance of a reference policy (including the behavior policy) for a range of hyperparameters (defined later). _This is the first work that provably demonstrates the property of SRPI in offline safe RL setting_.
3. We point out that primal-dual-based approaches Hong et al. (2024) may require **all**-policy concentrability assumption. Thus, unlike, the primal-dual-based appraoch, we propose a novel rectified penalty-based approach to obtain results using **single-policy** concentrability. Thus, we need novel analysis techniques to prove results.
4. Furthermore, we provide a practical implementation of WSAC following a two-timescale actor-critic framework using adversarial frameworks similar to Cheng et al. (2022); Zhu et al. (2023), and test it on several continuous control environments in the offline safe RL benchmark (Liu et al., 2023). WSAC outperforms all other state-of-the-art baselines, validating the property of a safe policy improvement.

Figure 1: Comparison between WSAC and the behavior policy in the tabular case. The behavior policy is a mixture of the optimal policy and a random policy, with the mixture percentage representing the proportion of the optimal policy. The cost threshold is set to 0.1. We observe that WSAC consistently ensures a safely improved policy across various scenarios, even when the behavior policy is not safe.

## 2 Related Work

**Offline safe RL:** Deep offline safe RL algorithms (Lee et al., 2022; Liu et al., 2023; Xu et al., 2022; Chen et al., 2021; Zheng et al., 2024) have shown strong empirical performance but lack theoretical guarantees. To the best of our knowledge, the investigation of policy improvement properties in offline safe RL is relatively rare in the state-of-the-art offline RL literature. Wu et al. (2021) focus on the offline constrained multi-objective Markov Decision Process (CMOMDP) and demonstrate that an optimal policy can be learned when there is sufficient data coverage. However, although they show that CMDP problems can be formulated as CMOMDP problems, they assume a linear kernel CMOMDP in their paper, whereas our consideration extends to a more general function approximation setting. Le et al. (2019) propose a model-based primal-dual-type algorithm with deviation control for offline safe RL in the tabular setting. With prior knowledge of the slackness in Slater's condition and a constant on the concentrability coefficient, an \((,)\)-PAC error is achievable when the number of data samples \(N\) is large enough \((N=}(1/^{2}))\). These assumptions make the algorithm impractical, and their computational complexity is much higher than ours. Additionally, we consider a more practical, model-free function approximation setting. In another concurrent work (Hong et al., 2024), a primal-dual critic algorithm is proposed for offline-constrained RL settings with general function approximation. However, their algorithm requires \(_{2}\) concentrability for **all** policies, which is not practical as discussed. The reason is that the dual variable optimization in their primal-dual design requires an accurate estimation of all the policies used in each episode, which necessitates coverage over all policies. Moreover, they cannot guarantee the property of SRPI. Moreover, their algorithm requires an additional offline policy evaluation (OPE) oracle for policy evaluation, making the algorithm less efficient.

## 3 Preliminaries

### Constrained Markov Decision Process

We consider a Constrained Markov Decision Process (CMDP) \(\), denoted by \((,,,R,C,,)\). \(\) is the state space, \(\) is the action space, \(:()\) is the transition kernel, where \(()\) is a probability simplex, \(R:\) is the reward function, \(C:[-1,1]\) is the cost function, \([0,1)\) is the discount factor and \(:\) is the initial state distribution. We assume \(\) is finite while allowing \(\) to be arbitrarily complex. We use \(:()\) to denote a stationary policy, which specifies a distribution over actions for each state. At each time, the agentobserves a state \(s_{t}\), takes an action \(a_{t}\) according to a policy \(,\) receives a reward \(r_{t}\) and a cost \(c_{t}\), where \(r_{t}=R(s_{t},a_{t}),c_{t}=C(s_{t},a_{t})\). Then the CMDP moves to the next state \(s_{t+1}\) based on the transition kernel \((|s_{t},a_{t})\). Given a policy \(,\) we use \(V_{r}^{}(s)\) and \(V_{c}^{}(s)\) to denote the expected discounted return and the expected cumulative discounted cost of \(\), starting from state \(s\), respectively.

\[V_{r}^{}(s):= [_{t=0}^{}^{t}r_{t}|s_{0}=s,a_{t} (|s_{t})] \] \[V_{c}^{}(s):= [_{t=0}^{}^{t}c_{t}|s_{0}=s,a_{t} (|s_{t})]. \]

Accordingly, we also define the \(Q-\)value function of a policy \(\) for the reward and cost as

\[Q_{r}^{}(s,a):= [_{t=0}^{}^{t}r_{t}|(s_{0},a_{0})=(s,a), a_{t}(|s_{t})] \] \[Q_{c}^{}(s,a):= [_{t=0}^{}^{t}c_{t}|(s_{0},a_{0})=(s,a), a_{t}(|s_{t})], \]

respectively. As rewards and costs are bounded, we have that \(0 Q_{r}^{}\), and \(- Q_{c}^{}\). We let \(V_{}=\) to simplify the notation. We further write

\[J_{r}():=(1-)_{s}[V_{r}^{}(s)], J_{c}(): =(1-)_{s}[V_{c}^{}(s)]\]

to represent the normalized average reward/cost value of policy \(\). In addition, we use \(d^{}(s,a)\) to denote the normalized and discounted state-action occupancy measure of the policy \(:\)

\[d^{}(s,a):=(1-)[_{t=0}^{}^{t}(s_ {t}=s,a_{t}=a)|a_{t}(|s_{t})],\]

where \(()\) is the indicator function. We also use \(d^{}(s)=_{a}d^{}(s,a)\) to denote the discounted state occupancy and we use \(_{}\) as a shorthand of \(_{(s,a) d^{}}[]\) or \(_{s d^{}}[]\)to denote the expectation with respect to \(d^{}\). Thus The objective in safe RL for an agent is to find a policy such that

\[\ J_{r}()\ J_{c}() 0. \]

_Remark 3.1_.: For ease of exposition, this paper exclusively focuses on a single constraint. However, it is readily extendable to accommodate multiple constraints.

### Function Approximation

In complex environments, the state space \(\) is usually very large or even infinite. We assume access to a policy class \((())\) consisting of all candidate policies from which we can search for good policies. We also assume access to a value function class \(([0,V_{}])\) to model the reward \(Q-\)functions, and \(([-V_{},V_{ }])\) to model the cost \(Q-\)functions of candidate policies. We further assume access to a function class \(\{w:[0,B_{w}]\}\) that represents marginalized importance weights with respect to the offline data distribution. Without loss of generality, we assume that the all-one function is contained in \(\).

For a given policy \(\), we denote \(f(s^{},):=_{a^{}}(a^{}|s^{})f(s^{},a ^{})\) for any \(s\). The Bellman operator \(_{r}^{}:^{} ^{}\) for the reward is defined as

\[(_{r}^{}f)(s,a):=R(s,a)+_{(s^{ }|s,a)}[f(s^{},)],\]

The Bellman operator \(_{c}^{}:^{} ^{}\) for the cost is

\[(_{c}^{}f)(s,a):=C(s,a)+_{(s^{ }|s,a)}[f(s^{},)].\]

Let \(\|\|_{2,}:=_{}[()^{2}]}\) denote the Euclidean norm weighted by distribution \(\). We make the following standard assumptions in offline RL setting (Xie et al., 2021; Cheng et al., 2022; Zhu et al., 2023) on the representation power of the function classes:

**Assumption 3.2** (Approximate Realizability).: Assume there exists \(_{1} 0,\) s.t. for any given policy \(,\) we have \(_{f}_{\,}\|f-T^{}_{r}f\|_{2,}^{2} _{1},\) and \(_{f}_{\,}\|f-T^{}_{c}f\|_{2,}^{2} _{1},\) where \(\) is the state-action distribution of any admissible policy such that \(\{d^{},\}.\)

Assumption 3.2 assumes that for any policy \(,\)\(Q^{}_{r}\) and \(Q^{}_{c}\) are approximately realizable in \(\) and \(\). When \(_{1}\) is small for all admissible \(,\) we have \(f_{r} Q^{}_{r},\) and \(f_{c} Q^{}_{c}.\) In particular, when \(_{1}=0,\) we have \(Q^{}_{r},Q^{}_{c}\) for any policy \(.\) Note that we do not need Bellman completeness assumption Cheng et al. (2022).

### Offline RL

In offline RL, we assume that the available offline data \(=\{(s_{i},a_{i},r_{i},c_{i},s^{}_{i})\}_{i=1}^{N}\) consists of \(N\) samples. Samples are i.i.d. (which are common assumptions in unconstrained Cheng et al. (2022), as well as constrained setting Hong et al. (2024)), and the distribution of each tuple \((s,a,r,c,s^{})\) is specified by a distribution \(()\), which is also the discounted visitation probability of a behavior policy (also denoted by \(\) for simplicity). In particular, \((s,a),r=R(s,a),c=C(s,a),s^{}(|s,a).\) We use \(a(|s),\) to denote that the action is drawn using the behavior policy and \((s,a,s^{})\) to denote that \((s,a),\) and \(s^{}(|s,a).\)

For a given policy \(,\) we define the marginalized importance weights \(w^{}(s,a):=(s,a)}{(s,a)}\) which is the ratio between the discounted state-action occupancy of \(\) and the data distribution \(.\) This ratio can be used to measure the concentrability of the data coverage (Xie and Jiang, 2020; Zhan et al., 2022; Rashidinejad et al., 2022; Ozdaglar et al., 2023; Lee et al., 2021).

In this paper we study offline RL with access to a dataset with limited coverage. The coverage of a policy \(\) is the dataset can be measured by the weighted \(_{2}\) single policy concentrability coefficient (Zhu et al., 2023; Yin and Wang, 2021; Uehara et al., 2024; Hong et al., 2024):

**Definition 3.3** (\(_{2}\) Concentrability).: Given a policy \(,\) define \(C^{}_{_{2}}=\|w^{}\|_{2,}=\|d^{}/\|_{2,}.\)

_Remark 3.4_.: The definition here is much weaker than the **all policy** concentrability used in offline RL (Chen and Jiang, 2019) and safe offline RL (Le et al., 2019; Hong et al., 2024), which requires the ratio \((s,a)}{(s,a)}\) to be bounded for all \(s\) and \(a\) and **all** policies \(.\) In particular, the all-policy concentrability assumption essentially requires the dataset to have full coverage of all policies ((nearly all the state action pairs). This requirement is often violated in practical scenarios. This requirement is even impossible to meet in safe offline RL because it would require collecting data from **every** dangerous state and actions, which clearly is impractical.

In the following lemma, we compare two variants of single-policy concentrability definition with the \(_{2}\) defined in Definition 3.3.

**Lemma 1** (Restate Proposition \(2.1\) in Zhu et al. (2023)).: _Define the \(_{}\) single policy concentrability (Rashidinejad et al., 2021) as \(C^{}_{_{}}=\|d^{}/\|_{}\) and the Bellman-consistent single-policy concentrability (Xie et al., 2021) as \(C^{}_{Bellman}=_{f}^{}f\|_{2,a^{ }}^{2}}{\|f-^{}f\|_{2,}^{2}}\) (\(\) could be \(_{r}\) or \(_{c}\) in our setting) Then, it always holds \((C^{}_{_{2}})^{2} C^{}_{_{}},C^{}_{_{2}} C^ {}_{_{}}\) and there exist offline RL instances where \((C^{}_{_{2}})^{2} C^{}_{Bellman},C^{}_{_{2}} C^{ }_{Bellman}.\)_

_Remark 3.5_.: It is easy to observe that the \(_{2}\) variant is bounded by \(_{}\) and \(C^{}_{Bellman}\) under some cases. There is an example (Example 1) in Zhu et al. (2023) showing that \(C^{}_{_{2}}\) is bounded by a constant \(\) while \(C^{}_{_{}}\) could be arbitrarily large. For the case when the function class \(\) is highly expressive, \(C^{}_{Bellman}\) could be close to \(C^{}_{_{}}\) and thus possibly larger than \(C^{}_{_{2}}.\) Intuitively, \(C^{}_{_{2}}\) implies that only \(_{d^{}}[w^{}(s,a)]\) is bounded, rather, \(w^{}(s,a)\) is bounded for all \((s,a)\) in \(_{}\) concentrability bound.

Given the definition of the concentrability, we make the following assumption on the weight function class \(\) and a single-policy realizability:

**Assumption 3.6** (Boundedness of \(\) in \(_{2}\) norm).: For all \(w\), assume that \(\|w\|_{2,} C^{*}_{_{2}}.\)

**Assumption 3.7** (Single-policy realizability of \(w^{}\)).: For some policy \(\) that we would like to compete with, assume that \(w^{}.\)

In this paper, we want to study the robust policy improvement on any reference policy, then we assume that we are provided a reference policy \(_{}.\) Note that in many applications (e.g., scheduling,networking) we indeed have a reference policy. We want that while applying a sophisticated RL policy it should do better and be safe as well. This is one of the main motivations behind this assumption.

**Assumption 3.8** (Reference Policy).: We assume access to a reference policy \(_{}\), which can be queried at any state.

In many applications such as networking, scheduling, and control problems, there are existing good enough reference policies. In these cases, a robust and safe policy improvement over these reference policies has practical value. If \(_{}\) is not provided, we can simply run a behavior cloning on the offline data to extract the behavior policy as \(_{}\) accurately, as long as the size of the offline data set is large enough. More discussion can be found in Section C in the Appendix.

## 4 Actor-Critic with Importance Weighted Bellman Error

Our algorithm design builds upon the constrained actor-critic method, in which we iteratively optimize a policy and improve the policy based on the evaluation of reward and cost. Consider the following actor-critic approach for solving the optimization problem (5):

**Actor:**\(^{*}_{}f_{r}^{}(s_{0},) s.t. f_{c} ^{}(s_{0},) 0\)

**Critic:**\(f_{r}^{}_{f}_{}[((f-_{r}f) (s,a))^{2}], f_{c}^{}_{f}_{}[(( f-_{c}f)(s,a))^{2}],\)

where we assume that \(s_{0}\) is a fixed initial state, and \(f_{r}(s,)=_{a}(a|s)f_{r}(s,a),f_{c}(s,)=_{a }(a|s)f_{c}(s,a).\) The policy is optimized by maximizing the reward \(q\) function \(f_{r}\) while ensuring that \(f_{c}\) satisfies the constraint, and the two functions are trained by minimizing the Bellman error. However, this formulation has several disadvantages. \(1)\) It cannot handle insufficient data coverage, which may fail to provide an accurate estimation of the policy for unseen states and actions. \(2)\)It cannot guarantee robust policy improvement. \(3)\) The actor training step is computationally intractable especially when the policy space is extremely large.

To address the insufficient data coverage issue, as mentioned in Xie et al. (2021) the critic can include a Bellman-consistent pessimistic evaluation of \(,\) which selects the most pessimistic function that approximately satisfies the Bellman equation, which is called absolute pessimism. Then later as indicated by Cheng et al. (2022), instead of using an absolute pessimism, a relative pessimism approach by considering competing to the behavior policy can obtain a robust improvement over the behavior policy. However, this kind of approach can only achieve a suboptimal statistical rate of \(N^{1/3},\) and fails to achieve the optimal statistical rate of \(1/,\) then later a weighted average Bellman error (Uehara et al., 2020; Xie and Jiang, 2020; Zhu et al., 2023) could be treated as one possible solution for improving the order. We remark here that all the discussions here are for the traditional _unconstrained_ offline RL. Regarding safety, _no existing efficient algorithms in safe offline RL have theoretically demonstrated_ the property of robust policy improvement with optimal statistical rate.

**Can Primal-dual based approaches achieve result using only single policy coverability?**: The most commonly used approach for addressing safe RL problems is primal-dual optimization (Efroni et al., 2020; Altman, 1999). As shown in current offline safe RL literature (Hong et al., 2024; Le et al., 2019), the policy can be optimized by maximizing a new unconstrained "reward" \(Q-\) function \(f_{r}^{}(s_{0},)- f_{c}^{}(s_{0},)\) where \(\) is a dual variable. Then, the dual-variable can be tuned by taking gradient descent step. As we discussed in the introduction, all these require **all** policy concentrability which is not practical especially for safe RL. Important question is whether all policy concentrability assumption can be relaxed. Note that primal-dual algorithm relies on solving the min-max problem \(_{}_{}f_{r}^{}(s_{0},)- f_{c}^{}(s_{0},)\). Recent result (Cui and Du, 2022) shows that single policy concentrability assumption is _not_ enough for offline min-max game. Hence, we _conjecture_ that using the primal-dual method we can not relax the all policy concentrability assumption. Intuitively, the primal-dual based method (Hong et al., 2024) rely on bounding the regret in dual domain \(_{k}(_{k}-^{*})(f_{c}^{_{k}}-0)\), hence, all the policies \(\{_{k}\}_{k=1}^{K}\) encountered throughout the iteration must be supported by the dataset to evaluate the dual value \(^{*}(f_{c}^{_{k}}-0)\) where \(^{*}\) is the optimal dual value.

**Our novelty**: In contrast, we propose an aggression-limited objective function \(f_{r}(s_{0},)-[f_{c}(s_{0},)]_{+}\) to control aggressive policies, where \(\{\}_{+}:=\{,0\}.\) The high-level intuition behind this aggression-limited objective function is that by appropriately selecting a \(\) (usually large enough), we penalize all the policies that are not safe. As a result, the policy that maximizes the objective function is the optimal safe policy. This formulation is fundamentally different from the traditional primal-dual approach as it does not require dual-variable tuning, and thus, does not require all policy concentrability. In particular, we only need to bound the primal domain regret which can be done as long as the reference policy is covered by the dataset similar to the unconstrained setup.

Combining all the previous ideas together provides the design of our main algorithm named WSAC (**W**eighted **S**afe **A**ctor-**C**ritic). In Section 5, we will provide theoretical guarantees of WSAC and discuss its advantages over existing approaches in offline safe RL. WSAC aims to solve the following optimization problem:

\[&^{*}*{arg\,max}_{} _{}(,f_{r}^{})-\{_{}(,f_{c}^{}) \}_{+}\\ & s.t.\ \ f_{r}^{}*{arg\,min}_{f_{r}} _{}(,f_{r})+_{}(,f_{r}), f_{c}^{ }*{arg\,min}_{f_{c}}\ -_{}( ,f_{c})+}_{}(,f_{c}), \]

where \(_{}(,f):=_{}[f(s,)-f(s,a)]\), and \(_{}(,f):=_{w}|_{}[w(s,a)((f- T_{r}^{}f)(s,a))]|,}_{}(,f):=_{w}|_{ }[w(s,a)((f-T_{c}^{}f)(s,a))]|\). This formulation can also be treated as a Stackelberg game (Von Stackelberg, 2010) or bilevel optimization problem. We penalize the objective function only when the approximate cost \(Q\)-function \(f_{c}^{}\) of the policy \(\) is more perilous than the behavior policy (\(f_{c}^{}(s,) f_{c}^{}(s,a)\)) forcing our policy to be as safe as the behavior policy. Maximization over \(w\) in for training the two critics can ensure that the Bellman error is small when averaged over measure \( w\) for any \(w\), which turns out to be sufficient to control the suboptimality of the learned policy.

In the following theorem, we show that the solution of the optimization problem (6) is not worse than the behavior policy \(\) in both performance and safety for any \( 0,>0\) than the policy \(\) under Assumption 3.2 with \(_{1}=0\).

**Theorem 4.1**.: _Assume that Assumption 3.2 holds with \(_{1}=0,\) and the behavior policy \(,\) then for any \( 0,>0\) we have \(J_{r}(^{*}) J_{r}(),\) and \(\{J_{c}(^{*})\}_{+}\{J_{c}()\}_{+}+.\)_

The result in Theorem 4.1 shows that by selecting \(\) large enough, for any \( 0,\) the solution can achieve better performance than the behavior policy while maintaining safety that is arbitrarily close to that of the behavior policy. The Theorem verifies the design of our framework which has the potential to have a robust safe improvement.

In the next section, we will introduce our main algorithm WSAC and provide its theoretical guarantees.

## 5 Theoretical Analysis of WSAC

### Main Algorithm

In this section, we present the theoretical version of our new model-free offline safe RL algorithm WSAC. Since we only have access to a dataset \(\) instead of the data distribution. WSAC solves an empirical version of (6):

\[&*{arg\,max}_{} _{}(,f_{r}^{})-\{_{ }(,f_{c}^{})\}_{+}\\ & s.t.\ \ f_{r}^{}*{arg\,min}_{f_{r}} _{}(,f_{r})+_{}(,f_{r} ), f_{c}^{}*{arg\,min}_{f_{c}}\ - _{}(,f_{c})+}_{}(,f_ {c}), \]

where

\[_{}(,f):=& _{}[f(s,)-f(s,a)]\\ _{}(,f):=&*{ arg\,max}_{w}|_{}[w(s,a)(f(s,a)-r- f(s^{ },))]|\\ }_{}(,f):=&_{w }|_{}[w(s,a)(f(s,a)-c- f(s^{}, ))]|. \]

As shown in Algorithm 1, at each iteration, WSAC selects \(f_{r}^{k}\) maximally pessimistic and \(f_{c}^{k}\) maximally optimistic for the current policy \(_{k}\) with a weighted regularization on the estimated Bellman error for reward and cost, respectively (Line \(4\) and \(6\)) to address the worse cases within reasonable range. In order to achieve a safe robust policy improvement, the actor then applies a no-regret policy optimization oracle to update the policy \(_{k+1}\) by optimizing the aggression-limited objective function compared with the reference policy (Line \(7\))\(f_{c}^{k}(s,_{})\}_{+}\). Our algorithm is very computationally efficient and tractable compared with existing approaches (Hong et al., 2024; Le et al., 2019), since we do not need another inner loop for optimizing the dual variable with an additional online algorithm or offline policy evaluation oracle. The policy improvement process relies on a no-regret policy optimization oracle, a technique commonly employed in offline RL literature (Zhu et al., 2023; Cheng et al., 2022; Hong et al., 2024; Zhu et al., 2023). Extensive literature exists on such methodologies. For instance, approaches like soft policy iteration (Pirotta et al., 2013) and algorithms based on natural policy gradients (Kakade, 2001; Agarwal et al., 2021) can function as effective no-regret policy optimization oracles. We now formally define the oracle:

**Definition 5.1** (No-regret policy optimization oracle).: An algorithm **PO** is called a no-regret policy optimization oracle if for any sequence of functions \(f^{1},,f^{K}\) with \(f^{k}:[0,V_{}], k[K]\). The policies \(_{1},,_{K}\) produced by the oracle **PO** satisfy that for any policy \(:\)

\[_{opt}^{}_{k=1}^{K}_{}[f^{k} (s,)-f^{k}(s,_{k})]=o(1) \]

There indeed exist many methods that can serve as the no-regret oracle, for example, the mirror-descent approach (Geist et al., 2019) or the natural policy gradient approach (Kakade, 2001) of the form \(_{k+1}(a|s)_{k}(a|s)( f^{k}(s,a))\) with \(=|}{2V_{}^{2}K}}\)(Even-Dar et al., 2009; Agarwal et al., 2021). In the following define \(_{opt}^{}\) as the error generated from the oracle **PO** by considering \(f_{r}^{k}(s,a)-\{f_{c}^{k}(s,a)-f_{c}^{k}(s,)\}_{+}\) as the sequence of functions in Definition 5.1, then we have the following guarantee.

**Lemma 2**.: _Applying a no-regret oracle_ **PO** _for \(K\) episodes with \((f_{r}^{k}(s,a)-\{f_{c}^{k}(s,a)-f_{c}^{k}(s,)\}_{+})\) for an arbitrary policy \(\), can guarantee_

\[_{k=1}^{K}_{}[f_{r}^{k}(s,)-f_{r}^ {k}(s,_{k})]_{opt}^{} \] \[_{k=1}^{K}_{}[\{f_{c}^{k}(s,_{k})-f _{c}^{k}(s,)\}_{+}]_{opt}^{}+}{}. \]

Lemma 2 establishes that the policy outputted by **PO** with considering the aggression-limited "reward" can have a strong guarantee on the performance of both reward and cost when \(\) is large enough., which is comparable with any competitor policy. This requirement is critical to achieving the performance guarantee of our algorithm and the safe and robust policy improvement. The detailed proof is deferred to Appendix B.2 due to page limit.

### Theoretical Guarantees

We are now ready to provide the theoretical guarantees of WSAC Algorithm 1. The complete proof is deferred to Appendix B.3.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]