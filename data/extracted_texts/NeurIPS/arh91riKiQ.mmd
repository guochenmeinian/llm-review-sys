# HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection

Theo King\({}^{2}\), Zekun Wu\({}^{1,2}\), Adriano Koshiyama\({}^{1}\)

**Emre Kazim\({}^{1}\)**, **Philip Treleaven\({}^{2}\)**

Corresponding Author: p.treleaven@ucl.ac.uk, zekun.wu@holisticai.com

\({}^{1}\) Holistic AI, \({}^{2}\)University College London

E. Kazim\({}^{1}\), **Philip Treleaven\({}^{2}\)**

Code available at https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection.

\({}^{1}\) Holistic AI, \({}^{2}\)University College London

###### Abstract

Stereotypes are generalised assumptions about societal groups, and even state-of-the-art LLMs using in-context learning struggle to identify them accurately. Due to the subjective nature of stereotypes, where what constitutes a stereotype can vary widely depending on cultural, social, and individual perspectives, robust explainability is crucial. Explainable models ensure that these nuanced judgments can be understood and validated by human users, promoting trust and accountability. We address these challenges by introducing HEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection), a framework that enhances model performance, minimises carbon footprint, and provides transparent, interpretable explanations. We establish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising 57,201 labelled texts across six groups, including under-represented demographics like LGBTQ+ and regional stereotypes. Ablation studies confirm that BERT models fine-tuned on EMGSD outperform those trained on individual components. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model using SHAP to generate token-level importance values, ensuring alignment with human understanding, and calculate explainability confidence scores by comparing SHAP and LIME outputs. An analysis of examples from the EMGSD test data indicates that when the ALBERT-V2 model predicts correctly, it assigns the highest importance to labelled stereotypical tokens. These correct predictions are also associated with higher explanation confidence scores compared to incorrect predictions. Finally, we apply the HEARTS framework to assess stereotypical bias in the outputs of 12 LLMs, using neutral prompts generated from the EMGSD test data to elicit 1,050 responses per model. This reveals a gradual reduction in bias over time within model families, with models from the LLaMA family appearing to exhibit the highest rates of bias.23

## 1 Introduction

The need to improve machine learning methods for stereotype detection is driven by the limitations of current approaches, particularly in the context of Large Language Models (LLMs). Although LLMs demonstrate superior language understanding and generation capabilities across many tasks , recent studies have shown that their accuracy in stereotype detection remains around 65% . This low performance underscores the potential value of fine-tuning smaller, specialised models for this domain. The subjectivity inherent in stereotypes, where definitions and perceptions can vary widely across different cultural, social, and individual contexts, further emphasises the importance ofrobust explainability in these models. Transparent and interpretable models are essential to ensure that stereotype detection aligns with human judgment and ethical standards.

To address these challenges, we introduce HEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection), which focuses on expanding the coverage of under-represented demographics in open-source composite datasets and developing explainable stereotype classification models. This work builds upon previous research that has aimed to establish frameworks for text stereotype detection . A significant application of HEARTS is the quantification of stereotypical bias in LLM outputs, a critical issue in Natural Language Processing (NLP). Numerous studies have identified statistically significant biases in LLM outputs , which can lead to harmful consequences when such models are used in decision-making processes, such as automated resume scanning in recruitment . Our research makes the following novel contributions:

1. The introduction of EMGSD, an Expanded Multi-Grain Stereotype Dataset, which includes labelled stereotypical and non-stereotypical statements covering gender, profession, nationality, race, religion, and LGBTQ+ stereotypes.
2. Development of a fine-tuned stereotype classification model based on ALBERT-V2, capable of achieving over 80% accuracy on EMGSD test data, while maintaining a minimal carbon footprint.
3. Implementation of an explainability system that produces rankings and confidence scores for token-level feature importance values, thereby enhancing the transparency and interpretability of stereotype classifiers.
4. Application of HEARTS to conduct a comparative analysis of stereotypical bias in LLM outputs, providing evidence of a gradual reduction in bias scores over time within individual model families.

**Social Impact Statement:** The tools developed through this research aim to improve the reliability and scalability of stereotypical bias detection, which, if deployed effectively, could mitigate risks associated with LLM usage. For example, by highlighting differences in the biases of models from different providers, users can make more informed decisions. This research contributes to the broader field of responsible AI by developing models that prioritise human well-being and align with societal values and ethical principles . Furthermore, HEARTS emphasises sustainability by focusing on model parameter size and carbon footprint management in the fine-tuning process, ensuring that the development of stereotype classification models adheres to high environmental standards.

## 2 Background and Related Work

HEARTS uses the classifier-based metrics approach to bias detection , by training an auxiliary model to benchmark an element of bias (stereotypical bias), which can in turn be applied to classify language, such as human or LLM-generated text. This is a common approach to bias evaluation in the domain of toxicity detection, which instead refers to offensive language that directly attacks a demographic, with notable examples including Jigsaw's Perspective API tool. There are fewer examples of open-source solutions in the domain of stereotype detection, where developing accurate detection models is a more challenging task, highlighting the need for explainable solutions. Some models have emerged in the Hugging Face community, such as the distilroberta-bias binary classification model trained on the wikirev-bias dataset and the Sentence-Level-Stereotype-Detector multi-class classification model trained on the original Multi-Grain Stereotype Dataset (MGSD) . These models are limited by either sub-optimal performance or lack of generalisability caused by training data that captures a relatively narrow set of stereotypes, which we seek to address by developing stereotype classification models on a more diverse dataset. In addition, previous research in the field of text stereotype detection has also placed little emphasis on model transparency, limited to anecdotal exploration of the use of explainability techniques such as SHAP  and LIME . We enhance these methodologies by making explainability a core component of HEARTS, incorporating a replicable system that includes confidence scores for token-level explanations.

Pure prompt-based and Q&A datasets such as BOLD , HolisticBias , BBQ  and UNQOVER  are not ideally suited to the task of fine-tuning a stereotype classification model, which requires labelled text instances consisting of stereotypical and non-stereotypical statements. The MGSD is a suitable composite dataset for stereotype classifier training , consisting of 51,867 observations covering gender, nationality, profession, and religion stereotypes, combining data from the previously established StereoSet  and CrowS-Pairs  datasets. This dataset does not provide coverage to some demographics such as LGBTQ+ communities, in addition to under-representingracial and national minorities, so we seek to expand it by incorporating data from other open-source datasets. Many other labelled datasets focus on binary gender and profession bias, such as BUG  and WinoBias , meaning their incorporation into MGSD would not significantly improve demographic diversity. The RedditBias  and ToxiGen  datasets cover multiple axes of stereotypes but have informal or conversational text structures that contrast sharply with the more formal nature of MGSD. In addition, datasets such as SHADR  focus on intersectional stereotypes that could be used to train multi-label classifiers, beyond the scope of our research. Therefore, our focus turns to the WinoQueer  and SeeGULL datasets , which respectively capture diverse LGBT+ and nationality stereotypes, from which we extract and augment data to combine with the MGSD.

## 3 Methodology

Our approach aims to improve the practical methods for text stereotype detection, by introducing HEARTS, an explainability-oriented framework, and deploying it to perform a downstream task of assessing stereotype prevalence in LLM outputs.

### Dataset Creation

We create the Expanded Multi-Grain Stereotype Dataset (EMGSD) by incorporating additional data derived from the WinoQueer and SeeGULL datasets. Before merging data sourced from each of these datasets into MGSD, we perform a series of filtering and augmentation procedures by leveraging LLMs, as shown in **Figure 1** below, with additional details including the full prompts used in A.1. This process includes a manual review of the final dataset. Our approach results in the creation of the Augmented WinoQueer (AWinoQueer) and Augmented SeeGULL (ASeeGULL) datasets and intends to align the structure of data with the original MGSD, which is equally balanced between text instances marked as "stereotype", "neutral" and "unrelated". We retain original instances of stereotypical text from each source dataset, which have been previously crowd-sourced and validated by human annotators in their creation. The final EMGSD has a sample size of 57,201, representing an increase of 5,334 (10.3%) compared to the original MGSD, with a full set of Exploratory Data Analysis (EDA) shown in A.2. The dataset is structured to support binary and multi-class sentence level stereotype classification. In order to validate the composition of EMGSD, we develop a series of binary sentence-level stereotype classification models. For this purpose, we divide the dataset into training and testing sets using an 80%/20% split, with stratified sampling based on binary categories.

### Dataset Validation and Model Training

Our proposed model for performing explainability and LLM bias evaluation experiments is the ALBERT-V2 architecture, primarily chosen over other BERT variants due to its lower parameter size. Using the CodeCarbon package , we estimate that fine-tuning an ALBERT-V2 model on the EMGSD leads to close to 200x lower carbon emissions compared to fine-tuning the original BERT model. We train four separate ALBERT-V2 models through the Hugging Face Transformers Library, with one model fine-tuned on each of the three components of the EMGSD (MGSD, AWinoQueer, ASeeGULL) in addition to its full version, to ascertain whether combining the datasets leads to the development of more accurate stereotype classifiers. Full model details, including hyperparameter choices, are shown in A.3. We also benchmark EMGSD test set performance of the fine-tuned ALBERT-V2 model against a series of other models. First, we consider fine-tuned DistilBERT and BERT models of larger parameter size, using the same training process. We also compare performance of these models against a general bias detector, distilroberta-bias, but do not test on the data used to develop this detector given it focuses on framing bias as opposed to stereotypical bias. In addition, we train two simple logistic regression baselines, the first vectorising features using Term Frequency - Inverse Document Frequency (TF-IDF) scores and the second using the pre-trained en_core_web_lg embedding model from the SpaCy library. CNN or RNN baselines are not explored given the extensive resources required for hyperparameter tuning, and their tendency to underperform BERT models in language understanding tasks . For each logistic regression model, we conduct hyperparemeter tuning by trialling a series of regularisation penalty types and strengths, with the hyperparameters achieving highest validation set macro F1 score shown in A.3. Finally, we compare performance to a set of state-of-the-art LLMs, focusing on the GPT series (GPT-4o, GPT-4o-Mini), using prompt templates that closely align with those used in the TrustLLM study , also shown in A.3. We do not explore fine-tuning of LLMs, given conventional XAI tools cannot be applied to them in a scalable manner.

### Token Level Explanations

To evaluate the predictions of our fine-tuned ALBERT-V2 classifier and calculate token-level importance values for test set model predictions, we apply established feature attribution methods, using SHAP to generate default feature importance values. We calculate a SHAP vector \(_{i}\) for each text instance \(i\) to rank tokens in accordance with their influence on model predictions, where a higher SHAP value indicates greater influence on stereotype classifier prediction probability. Formally, for token \(j\) in instance \(i\):

\[_{ij}=_{S N_{i}\{j\}}|-|S|-1)!}{|N _{i}|!}[f_{i}(S\{j\})-f_{i}(S)],_{i}=(_{i1},_{i2},, _{iN})\]

We next calculate a sentence-level explanation confidence score by generating a LIME vector \(_{i}\) for the same text instance and comparing pairwise similarities between SHAP and LIME values assigned to each token, using a custom regex tokeniser for consistency. The LIME vector is given by:

Figure 1: Overview of the dataset filtering and augmentation process for the WinoQueer and SeeGULL datasets. The WinoQueer dataset (91,080 sentences) undergoes filtering by removing duplicates, counterfactual statements, and overtly negative sentences, resulting in a refined set of 1,088 sentences. The SeeGULL dataset (6,781 phrases) is filtered to remove non-offensive and non-stereotypical sentences, yielding 690 phrases. Sentence generation using Mistral Medium expands these phrases to 690 sentences. Both filtered datasets are then augmented using GPT-4 to generate three categories: neutral, stereotypical, and unrelated sentences, contributing a total of 5,334 additional observations to the MGSD.

\[_{i}=_{}_{k=1}^{K}_{k}[f_{i}(x^{}_{k})-( _{0}+_{j N_{i}}_{j} x^{}_{kj})]^{2}, _{i}=(_{i1},_{i2},,_{iN})\]

Similarity scores are measured using cosine similarity, Pearson correlation and Jensen-Shannon divergence, with full definitions as follows:

1. Cosine Similarity:

\[CS(_{i},_{i})=_{i}}{\|_{i}\|\|_{i}\|} =^{N_{i}}_{ij}_{ij}}{^{N_{i}}_{ij} ^{2}}^{N_{i}}_{ij}^{2}}}\]

2. Pearson Correlation:

\[PC(_{i},_{i})=,_{i})}{_{_{i}}_ {_{i}}}=^{N_{i}}(_{ij}-})(_{ij}- })}{^{N_{i}}(_{ij}-})^{2}} {_{j=1}^{N_{i}}(_{ij}-})^{2}}}\]

3. Jensen-Shannon Divergence:

\[P_{j}=+|Min(_{i})|}{_{j=1}^{N_{i}}(_{ij}+|Min(_ {i})|)}, Q_{j}=+|Min(_{i})|}{_{j=1}^{N_{i}}( _{ij}+|Min(_{i})|)}\]

\[JSD(_{i},_{i})=_{j=1}^{N_{i}}P_{j}( }{P_{j}+Q_{j}})}+_{j=1}^{N_{i}}Q_{j} (}{P_{j}+Q_{j}})\]

In the Results section, we show examples of model predictions with corresponding token importance values and explanation confidence scores. We also show mean and standard deviation similarity metrics for SHAP and LIME vectors across a test set sample of size 1,005 in A.6, to demonstrate statistically significant alignment in the predictions of each method for our dataset.

### LLM Stereotype Assessment

The final stage of our research applies HEARTS to a practical use case, to evaluate stereotypical biases in LLM outputs, by processing neutral prompts through LLMs and evaluating their text continuations for stereotypical bias, using our fine-tuned ALBERT-V2 model. To create neutral prompts, we first select a sample of 35 stereotypical statements from the EMGSD test set, drawing five text instances for each group and dataset combination, then stem each sentence to remove the tokens with stereotypical meaning. To verify the prompt creation process, we apply the fine-tuned ALBERT-V2 model to each prompt to confirm it generates a neutral prediction. The full set of prompts is shown in A.7. Testing a subset of 12 LLMs in total, we batch process the prompts through each model with 30 independent iterations, using the template _"Please augment each of the following phrases into short sentences of up to 10 words_". This yields a sample of 1,050 responses for each model, to ensure statistical significance of results. To evaluate bias creation for each model, we count the number of statements labelled as stereotypical by our stereotype classifier to arrive at an estimated proportion \(P_{M}\) of model stereotype prevalence, where \(P_{M}=_{i=1}^{n}(}=1)\).

## 4 Results and Discussion

The full results of our ablation study are shown in **Table 1** below. Our intention in expanding the original MGSD is to improve its demographic coverage, without materially sacrificing performance of models trained on the dataset. The results appear to validate the composition of our dataset, with the dataset expansion generating performance improvements. The results show that the highest performing model for each dataset component, in terms of test set macro F1 score, is a BERT variant fine-tuned on the full EMGSD training data (DistilBERT for AWinoQueer and ASeeGULL, BERT for MGSD and EMGSD). The comparison of results across model architectures also indicates that the fine-tuned ALBERT-V2 model, which we select to perform explainability and bias evaluation experiments,shows similar performance to BERT variants of larger parameter size, whilst outperforming logistic regression and GPT baselines by a large margin. These outcomes indicate that the model is a reasonable choice for developing accurate stereotype classifiers with low carbon footprint. A further set of detailed results for the ALBERT-V2 model, decomposing performance by demographic, is displayed in A.4.

  
**Model Type** & **Emissions** & **Training Data** &  \\   & & & **MGSD** & **AWinoQueer** & **ASeeGULL** & **EMGSD** \\  DistilRoBERTa-Bias & Unknown & wikirev-bias & 53.1\% & 59.7\% & 65.5\% & 53.9\% \\ GPT-4o & Unknown & Unknown & 65.6\% & 47.5\% & 66.6\% & 64.8\% \\ GPT-4o-Mini & Unknown & Unknown & 60.7\% & 45.4\% & 54.2\% & 60.0\% \\ LR - TFIDF & \( 0\) & MGSD & 65.7\% & 53.2\% & 67.3\% & 65.0\% \\ LR - TFIDF & \( 0\) & AWinoQueer & 49.8\% & 95.6\% & 59.7\% & 52.7\% \\ LR - TFIDF & \( 0\) & ASeeGULL & 74.7\% & 56.7\% & 82.0\% & 58.3\% \\ LR - TFIDF & \( 0\) & EMGSD & 65.8\% & 83.1\% & 76.2\% & 67.2\% \\ LR - Embeddings & \( 0\) & MGSD & 61.6\% & 63.3\% & 71.7\% & 62.1\% \\ LR - Embeddings & \( 0\) & AWinoQueer & 55.5\% & 93.9\% & 66.1\% & 58.4\% \\ LR - Embeddings & \( 0\) & ASeeGULL & 53.5\% & 56.8\% & 86.0\% & 54.9\% \\ LR - Embeddings & \( 0\) & EMGSD & 62.1\% & 75.4\% & 76.7\% & 63.4\% \\ ALBERT-V2 & 2.88g & MGSD & 79.7\% & 74.7\% & 75.9\% & 79.3\% \\ ALBERT-V2 & 2.88g & AWinoQueer & 60.0\% & 97.3\% & 70.7\% & 62.8\% \\ ALBERT-V2 & 2.88g & ASeeGULL & 63.1\% & 66.8\% & 88.4\% & 64.5\% \\ ALBERT-V2 & 2.88g & EMGSD & 80.2\% & 97.4\% & 87.3\% & **81.5\%** \\ DistilBERT & 156.48g & MGSD & 78.3\% & 75.6\% & 73.0\% & 78.0\% \\ DistilBERT & 156.48g & AWinoQueer & 61.1\% & **98.1\%** & 72.1\% & 64.0\% \\ DistilBERT & 156.48g & ASeeGULL & 62.7\% & 82.1\% & **89.8\%** & 65.1\% \\ DistilBERT & 156.48g & EMGSD & 79.0\% & **98.8\%** & **91.9\%** & 80.6\% \\ BERT & 270.68g & MGSD & **81.2\%** & 77.9\% & 69.9\% & 80.6\% \\ BERT & 270.68g & AWinoQueer & 59.1\% & 97.9\% & 72.5\% & 62.3\% \\ BERT & 270.68g & ASeeGULL & 61.0\% & 78.6\% & 89.6\% & 63.3\% \\ BERT & 270.68g & EMGSD & **81.7\%** & 97.6\% & 88.9\% & **82.8\%** \\   

Table 1: Comparison of model macro F1 scores on each test set component of EMGSD. **Bold** indicates the highest, _bold italics_ the second-highest score in each column.

Figure 2: Evolution of test set F1 score by text length for ALBERT-V2 model trained on EMGSD. The results show an increase in F1 score variance as text length increases, with evidence of lower average F1 score for longer text lengths. Therefore, our model achieves more robust results when applied to short blocks of text, highlighting the need for new datasets featuring more complex text passages, to develop models capable of also achieving robust performance on longer text.

In **Table 2**, we show example output of HEARTS for a set of text instances from the EMGSD test set. Each of these examples, initially sourced from the StereoSet and CrowS-Pairs datasets, contains single tokens that were labelled as stereotypical (masked token that generates the stereotype) by human annotators when the datasets were initially created. For each example of correct ALBERT-V2 model predictions, the highest ranked token based on SHAP value aligns with the labelled stereotypical token, with the similarity metrics indicating a lower degree of confidence in model explanations for longer text instances and in cases where the model makes an incorrect prediction.

The full results from our comparative assessment of stereotypical bias in LLM outputs are shown in A.8. Of the models tested, Meta's LLaMA-3-70B-T has the highest bias score at 57.6%, whilst Anthropic's Claude-3.5-Sonnet has the lowest bias score at 37.0%. Focusing only on the most recent model iteration from each provider, Meta's LLaMA-3.1-405B-T also has the highest bias score at 50.7%, 8 percentage points higher than the next provider (42.5% for GPT-4o). In **Figure 5** below, we assess whether there is a discernible downward trend in prevalence of bias in LLM outputs over time, reflecting ongoing industry efforts to incorporate debias frameworks into LLM training processes. Considering general trends across the whole group of models, there appears to be limited evidence of a clear downward trend in bias scores, with recent releases such as LLaMA-3.1-405B-T exhibiting bias scores in excess of 50%. That said, within particular model families there is evidence of a gradual reduction in bias scores for later iterations, with the exception of the GPT family where bias scores are relatively constant, starting at a lower base level for the earliest iteration studied (GPT-3.5-Turbo).

## 5 Limitations and Future Work

A key limitation impacting the quality of our dataset and resultant stereotype classification models is the low availability of high-quality labelled stereotype source datasets, leading to sub-optimal linguistic structure and demographic composition of the EMGSD. For instance, despite extensive efforts to diversify the dataset, text instances referring to racial minorities account for approximately 1% of the sample. This issue leads to variation in performance of our fine-tuned ALBERT-V2 model across demographics. Ongoing efforts to produce diverse, crowd-sourced stereotype datasets are critical, which should also seek to capture intersectional stereotypes to allow the development of multi-label classifiers that can simultaneously identify multiple axes of stereotypes. In addition, our proposed token-level feature importance ranking framework relies on calculating explanation confidence levels based on a single pairwise comparison between SHAP and LIME vectors for a given text instance. To enhance the robustness of this approach, future research could incorporate additional feature importance tools, such as integrated gradients, to build more complex ensemble methods that could also be used to develop token-level classification frameworks.

Figure 5: Stereotype prevalence in LLM outputs by model release date. Stemmed text instances from the EMGSD test set (neutral prompts) are used to elicit 1,050 responses per model.