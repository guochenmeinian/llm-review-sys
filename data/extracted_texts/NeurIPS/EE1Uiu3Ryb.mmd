# Bandit Task Assignment

with Unknown Processing Time

 Shinji Ito

NEC Corporation, RIKEN AIP

i-shinji@nec.com

Daisuke Hatano

RIKEN AIP

daisuke.hatano@riken.jp

Hanna Sumita

Tokyo Institute of Technology

sumita@c.titech.ac.jp

&Kei Takemura

NEC Corporation

kei_takemura@nec.com

Takuro Fukunaga

Chuo University

fukunaga.07s@g.chuo-u.ac.jp

&Naonori Kakimura

Keio University

kakimura@math.keio.ac.jp

Ken-ichi Kawarabayashi

National Institute of Informatics, The University of Tokyo

k_keniti@nii.ac.jp

###### Abstract

This study considers a novel problem setting, referred to as _bandit task assignment_, that incorporates the processing time of each task in the bandit setting. In this problem setting, a player sequentially chooses a set of tasks to start so that the set of processing tasks satisfies a given combinatorial constraint. The reward and processing time for each task follow unknown distributions, values of which are revealed only after the task has been completed. The problem generalizes the stochastic combinatorial semi-bandit problem and the budget-constrained bandit problem. For this problem setting, we propose an algorithm based on upper confidence bounds (UCB) combined with a phased-update approach. The proposed algorithm admits a gap-dependent regret upper bound of \(O(MN(1/) T)\) and a gap-free regret upper bound of \(()\), where \(N\) is the number of the tasks, \(M\) is the maximum number of tasks run at the same time, \(T\) is the time horizon, and \(\) is the gap between expected per-round rewards of the optimal and best suboptimal sets of tasks. These regret bounds nearly match lower bounds.

## 1 Introduction

This paper introduces a new model of sequential decision-making that we refer to as the _bandit task assignment_ problem. The goal of this model is to determine which tasks to perform sequentially so that the total reward will be maximized, while estimating the distribution of rewards and processing time for each task. For each round \(t=1,2,,T\), we choose a set \(A_{t}\) of tasks to start. Each task \(i\) in \(A_{t}\) will be completed in the \((t+c_{ti})\)-th round, and we then obtain the reward \(r_{ti}\). The processing time \(c_{ti}\) and the reward \(r_{ti}\) follow unknown distributions independently for all \(t\), and they are observed only when the task has been completed, i.e., only bandit feedback is available. In all rounds, the set of processing tasks is required to satisfy a given combinatorial constraint. That is, in each round, the player can start a set of new tasks that, together with the tasks still in progress, satisfies the giventhe difference between the cumulative rewards obtained with an optimal policy and with the algorithm. Our model allows an arbitrary constraint for a set of tasks that is expressed by \(\{0,1\}^{N}\), where \(N\) is the number of tasks. Each vector \(a=(a_{1},a_{2},,a_{N})^{}\) is interpreted as a feasible set of tasks \(A=\{i[N] a_{i}=1\}\), where \([N]:=\{1,2,,N\}\).

Figure 1 illustrates an example of a feasible task assignment for the case of \(=\{a\{0,1\}^{3}\|a\|_{1} 2\}\), in which we choose which tasks to start from three candidates so that the number of processing tasks does not exceed two. Each arrow in this figure represents the interval of time to process a task. In this example, at round \(2\), two tasks (Tasks 1 and 2) are still processing, and hence, we cannot start any new task. At the beginning of round \(3\), we observe that Task 1 is completed, and it is then possible to start one new task. We note that the processing time \(c_{ti}\) and the reward \(r_{ti}\) of task \(i\) stated at time \(t\) are revealed only at the completion time \((t+c_{ti})\).

The model in this paper is a common generalization of stochastic combinatorial semi-bandit problems [25; 24; 34] and budget-constrained bandit problems [20; 15; 12; 37]. In fact, if the processing time \(c_{ti}\) is always equal to \(1\) for any task \(i\), the player can choose any set of \(\) in every round and can observe the feedback immediately afterward, which coincides with the stochastic combinatorial semi-bandit problem. On the other hand, if we suppose that \(=\{a\{0,1\}^{N}\|a\|_{1} 1\}\), i.e., if only a single task can be performed in every round, the problem corresponds to stochastic \(N\)-armed bandits with a budget constraint [20; 15]. Indeed, by regarding \(c_{ti}\) as the cost for choosing arm \(i\), we can interpret the model as a problem with the budget constraint that the cumulative cost is at most \(T\).

We note that our problem setting is similar to _blocking bandits_, in which each chosen arm is _blocked_, i.e., is unavailable for a certain period of time after it is chosen. However, they are different in the constraints on the actions in each round. For example, consider \(=\{a\{0,1\}^{N}\|a\|_{1} K\}\). Then, in the bandit task assignment problem, we cannot start a new task when \(K\) tasks are already running. On the other hand, in the blocking bandit problem, the player can choose any \(K\) arms that are not blocked. Blocking bandits have been extended to contextual bandit models , semi-bandit models [3; 28], and adversarial models , as well.

Applications of our model include the following examples:

**Example 1** (Matching constraint).: We consider the problem of assigning \(L\) jobs to \(K\) workers. Let \(V_{1}\) and \(V_{2}\) be the sets of workers and jobs, respectively. Let \(E V_{1} V_{2}\) be a set of pairs of a worker and a job such that \((u,v) E\) if and only if the worker \(u\) is qualified to do the job \(v\). In each round, we choose a subset \(M\) of \(E\) so that no two elements in \(M\) share the same worker or job, that is, \(M\) must satisfy the matching constraint for the bipartite graph \(G=(V_{1} V_{2},E)\). Each job has processing time to complete, and, when a job is assigned to a worker, the job-worker pair is occupied until the job is completed. Thus the occupied job-worker pairs have to be included in \(M\). We assume that the time required to complete a job and the quality of the results (rewards) follow unknown distributions that depend on the job-worker pair. This problem can be regarded as a special case of the bandit task assignment problem with \(N=|E|\). Choosing \(a_{t}\) corresponds to assigning a job \(v\) to a worker \(u\) for each \(e=(u,v)\) such that \(a_{te}=1\), and the worker \(u\) and the job \(v\) will then be occupied and unavailable until the task is completed. It would be worth mentioning that the matching constraint is often the subject of consideration in the context of combinatorial semi-bandits [18; 30].

**Example 2** (Knapsack constraint).: Consider the situation where we want to perform tasks (e.g., of data processing) under resource constraints (e.g., computational resources such as CPUs, GPUs, and memories). We assume that the processing time of each task varies stochastically, and is not revealed until it is completed. Formally, we are given \(N\) tasks and \(K\) resources. Assume that each task \(i[N]\) consumes \(b_{ki}_{ 0}\) amount of resource \(k[K]\) per unit time, and that the total

Figure 1: An example of a feasible task assignment.

amount of resource \(k\) available in unit time is \(f_{k}\). Then the player can choose a set of tasks from \(=\{a\{0,1\}^{N} b_{k}^{}a f_{k}(k[K])\}\), where \(b_{k}=(b_{k1},b_{k2},,b_{kN})^{}_{ 0}^{N}\) for each resource \(k\). Note that the set of _all running tasks_ must satisfy the constraint given by \(\), which means that we cannot use resources occupied by tasks in progress and we are restricted to choose a set of tasks that can be executed with remaining resources. We note that the class of problems with a single resource (\(K=1\)) includes the budget-constrained multi-armed bandit [20; 15] as a special case.

**Example 3** (Matroid constraint [35; 17]).: If \(\) consists of indicator vectors for independent sets of a matroid with the ground set \([N]\), we call this a matroid constraint. Matroid constraints capture a variety of problem settings of practical importance, including the situation in which the number of tasks that can be executed simultaneously is limited to at most \(K\), i.e., \(=\{a\{0,1\}^{N}\|a\|_{1} K\}\). This special case is called a uniform matroid.

### Contribution

Our main contribution is to present an algorithm that achieves a regret bound of \(O(}}{C_{1}^{2}}MN(1/+C_{}) T)\), where \(C_{}\) and \(C_{1}\) are upper and lower bounds for the processing times \(c_{ti}\), \(M\) is the maximum of \(\|a\|_{1}\) for \(a\), and \(>0\) is the gap between expected per-round rewards for the optimal and the best suboptimal actions. The proposed algorithm also enjoys a gap-free regret bound of \(O(}}MNT T})\). Our contributions include a nearly tight lower bound as well, i.e., we show that any algorithm suffers a regret at least \((}}MNT})\). This indicates that the proposed algorithm achieves nearly optimal regret bounds.

To design our algorithm, we first show that a nearly optimal policy can be characterized by expected per-round rewards for tasks (Proposition 2.2 in Section 2). This implies that, given estimation for expected per-round rewards and a linear optimization oracle for \(\), we can compute asymptotically optimal actions. This is in contrast to the blocking bandits, in which it is computationally hard to maximize the total reward even if all the distributions are known, as presented, e.g., in Corollary 3.2 by Basu et al. . That is why, for the blocking bandits, existing algorithms can achieve only approximate regret bounds, i.e., the performance is only guaranteed to be (close to) a constant-factor approximation for the optimal strategy.

Proposition 2.2 suggests us to balance exploration and exploitation with the aid of upper confidence bounds (UCB) on the expected per-round reward for each task, which can be computed efficiently. Such a UCB-based approach is similar to CombUCB1 for combinatorial semi-bandits . CombUCB1, however, does not directly apply to our problem setting due to the stochastically variable processing time. In the bandit task assignment problem, the set of tasks that the player can start in each round changes depending on the set of tasks still in progress, which makes the decision of the player restricted. To address this issue, we introduce a _phased-update_ approach, in which we divide the entirety of the rounds \([T]=\{1,2,,T\}\) into _segments_ of appropriate length. At the beginning of each segment \(s\), we compute a set of tasks \(A^{}_{s}\) based on the UCB for expected per-round rewards. We note that this can be done independently of the state of running tasks. Then, in the \(s\)-th segment, we basically continue to perform the same set of tasks \(A^{}_{s}\).

One main technical difficulty is how to determine the length \(l_{s}\) of the segment \(s\). At the start of a new segment \(s\), tasks in \(A^{}_{s}\) cannot be executed until all tasks performed in the previous segment have been completed, i.e., every switching point between segments causes _idle time_ with non-zero probability. Therefore, the larger the number of segments (i.e., the shorter the length of each segment), the greater the loss due to such waiting time. In particular, if the segment length is \(O(C)\), we cannot avoid a linear regret of \((T/C)\) due to idle time. On the other hand, setting too long segments also has a negative impact on performance as the longer the length of each segment, the less frequently the set \(A^{}_{s}\) will be updated, which may degrade the efficiency of exploration and exploitation. To address this dilemma, this study designs a way of determining the segment length \(l_{s}\) by which these potential losses will be well balanced and the overall regret will be bounded as desired.

Another nontrivial technique in the proposed algorithm is to employ _Bernstein-type confidence bounds_[4; 27] for the processing time. Thanks to this, we can construct a tight UCB estimator for the expected per-round reward for each task \(i\), with a width depending on the mean \(_{i}\) and variance \(_{i}^{2}\) of the processing time. This is essential to achieve the nearly optimal \((}}MNT})\)-regret bound. In fact, as we mention in Remark 4.2, an algorithm with standard confidence bounds will lead to regret upper bounds with additional \(C_{}/C_{1}\) factors, which do not match the lower bound.

Summary of contributionsThe contributions of this paper are summarized in the following four points: (i) We propose a novel problem setting of the bandit task assignment, which incorporates simultaneously combinatorial constraints among tasks, (unknown) processing time of each task, and the exploration-exploitation trade-off. The model includes various practical applications as described in Examples 1, 2 and 3. (ii) We show that a nearly optimal policy can be characterized with expected per-round rewards, which can be computed efficiently given only a linear optimization oracle. This contrasts with the computational difficulty in blocking bandits. (iii) We provide an algorithm with nearly optimal regret bounds. We handle the difficulties arising from the combinatorial constraints together with processing times by the phased-update approach. (iv) We present a regret lower bound that matches to the regret bound of the proposed algorithm, ignoring logarithmic factors. This means that the proposed algorithm achieves nearly optimal performance in terms of the worst-case analysis.

## 2 Problem Setup

Let \(N\) be the number of tasks. The player is given a family of feasible sets of tasks, which is expressed by \(\{0,1\}^{N}\). Here, each binary vector \(a\) in \(\{0,1\}^{N}\) corresponds to a set of tasks, and \(a\) means that the set of tasks \(A=\{i[N] a_{i}=1\}\) can be executed simultaneously. We assume that \(\) is closed under inclusion, i.e., \(a\) and \(b a\) together imply \(b\). Note here that, for any vectors \(a=(a_{1},,a_{N})^{T},b=(b_{1},,b_{N})^{T}^{N}\), the notation of \(b a\) means that \(b_{i} a_{i}\) holds for all \(i\{1,,N\}\). We denote \(M=_{a}\|a\|_{1}\). In each round \(t\), the player will choose a set of tasks \(a_{t}\) from \(\). The chosen set has to exclude all the processing tasks not yet completed. More precisely, \(a_{i}\) is constrained to satisfy \(a_{t}+b_{t}\), where \(b_{t}\) is the set of tasks still in progress at the \(t\)-th round (see (1) below). Each task \(i\) with \(a_{ti}=1\) is then started and will be completed at the beginning of the \((t+c_{ti})\)-th round, which yields the reward of \(r_{ti}\). The reward \(r_{ti}\) and the processing time \(c_{ti}\) will be revealed only after the task is completed. We assume that \(r_{ti}\) and \(c_{ti}\) are restricted so that \(r_{ti}\) and \(c_{ti}\{C_{1},C_{1}+1,,C_{u}\}\), where \(C_{1}\) and \(C_{u}\) are integers satisfying \(1 C_{1} C_{u}\). We also assume that \(((r_{ti},c_{ti}))_{i=1}^{N}\) follows an unknown distribution \(\) over \((\{C_{1},,C_{u}\})^{N}\), independently for \(t=1,2,,T\). Note that \(r_{ti}\) and \(c_{ti}\) may be dependent. From the above problem definition, a family \(_{t}\) of task sets available at the \(t\)-th round is expressed as follows:

\[b_{ti}=_{s=1}^{t-1}a_{si}[s+c_{si}>t](i[N]), _{t}=\{a a+b_{t}\}, \]

where the vector \(b_{t}=(b_{ti})_{i=1}^{N}\) corresponds to the set of uncompleted tasks that starts before the \(t\)-th round. The goal of the player is to maximize the sum of rewards earned by the \(T\)-th round, which can be expressed as \(_{t=1}^{T}r_{t}^{}a_{t}\).

_Remark 2.1_.: Stochastic combinatorial semi-bandits  and the multi-armed bandit problem with a budget constraint  are special cases of the bandit task assignment problem. In fact, if \(c_{ti}=1\) for all \(t\) and \(i\), the problem corresponds to a combinatorial semi-bandit problem with action set \(\). In addition, if \(=\{a\{0,1\}^{N}\|a\|_{1} 1\}\), the problem corresponds to an \(N\)-armed bandit with budget \(B=T\) and costs \(\{c_{ti}\}\).

In the problem, we assume that we are able to solve linear optimization over \(\) efficiently. More precisely, we suppose that we are given access to an _offline linear optimization oracle_ that returns a solution \(*{arg\,max}_{a}q^{}a\) for any input \(q^{N}_{>0}\). Note here that, for the problems with matching constraints or with matroid constraints (Examples 1 and 3, respectively), such an oracle can be implemented in polynomial time [23; 17]. For Example 2, linear optimization over \(\) is NP-hard, but can be solved practically fast in many cases, with the aid of dynamic programming or integer programming algorithms.

We define the _regret_\(R_{T}\) to be the expectation of the difference between the total rewards obtained with the optimal _proper policy_ and those obtained with the algorithm by the \(T\)-th round, where we call a policy for choosing \(a_{t}\)_proper_ if \(a_{t}\) satisfies the constraint defined by (1) and each \(a_{t}\) depends on information observed by the beginning of \(t\)-th round, but not on \(((r_{s},c_{s}))_{s=t}^{T}\). Any proper policy can be expressed as sequences of mapping \(=(_{t})_{t=1,2,}\), where each \(_{t}\) maps from the history \(h_{t}\), which consists of chosen actions \((a_{s})_{s=1}^{t-1}\) and feedback obtained by the beginning of the \(t\)-round, to action \(a_{t}\) in \(\). Let \(\) denote the set of all proper policies. The regret can then be expressed as

\[R_{T}=_{^{*}}[_{t=1}^{T}r_{t}^{}_{t}^{*}(h _{t}^{*})]-[_{t=1}^{T}r_{t}^{}a_{t}],\]

where \((h_{t}^{*})_{t=1,2,}\) denotes histories generated by the policy \(^{*}\). Intuitively, the total reward obtained with the optimal proper policy corresponds to the maximum performance achieved by an agent with unlimited computational resources who knows the distribution \(\) of processing times and rewards. Note that the regret \(R_{T}\) is defined on the basis of tasks _started by_ the \(T\)-th round. Although it may seem more natural to define the regret by the rewards for tasks _completed by_ the \(T\)-th round, the difference between these two alternatives is only \(O(M/C_{})\), which is almost negligible as it is independent of \(T\).

We can see that the performance of the optimal proper policy (and therefore also regret) can be characterized by using the expected per-round rewards. We define \(_{i}\) and \(_{i}\) to be the expectation of \(r_{ti}\) and \(c_{ti}\), respectively. We also denote \(q_{i}=_{i}/_{i}\). Define vectors \(^{N},[C_{},C_{}]^{N}\) and \(q[0,1/C_{}]^{N}\) by \(=(_{1},_{2},,_{N})^{}\), \(=(_{1},_{2},,_{N})^{}\) and \(q=(q_{1},q_{2},,q_{N})^{}=(_{1}/_{1},_{2}/_{2},,_{N}/_{N})^{}\). Note that \(q_{i}\) corresponds to the expected per-round reward for running each task \(i\). Then, the expected total reward for an optimal policy can be bounded as follows:

**Proposition 2.2**.: _For any proper policy \(=(_{t})_{t=1,2,}\), we have \([_{t=1}^{T}r_{t}^{}_{t}(h_{t})](T+C_{})_{a}\{q^{}a\}\), where \((h_{t})_{t=1}^{T}\) denote histories generated by \(\)._

Proposition 2.2 can be considered as a generalization of Lemma 1 by Ding et al. . From this proposition, the regret can be bounded as follows:

\[R_{T}(T+C_{})_{a}q^{}a-[ _{t=1}^{T}r_{t}^{}a_{t}][_{t=1}^{T}(q^{ }a^{*}-r_{t}^{}a_{t})]+}M}{C_{}}, \]

where we denote \(a^{*}*{arg\,max}_{a}q^{}a\) and the last inequality follows from the facts that \(q_{i} 1/C_{}\) holds for any \(i\) and that \(\|a^{*}\|_{1} M\).

## 3 Algorithm

The proposed algorithm employs a phased-update approach, in which we update a policy for each phase, and each phase continues to select the same set of tasks.

The procedure starts with the _initialization phase_ (or the zeroth phase), in which we execute each task \(B=(}}{C_{}} T)\) times. This is required to ensure that the UCB estimators \(_{i}(t)\) given in (3) are close enough to the true expected reward \(q_{i}\). Let \(t_{1} 1\) denote the round this initialization terminates, which is at most \(O(C_{}NB)\). As the regret per round is at most \(}}\), the total regret in the initialization phase is at most \(O(}MNB}{C_{}})=O(}^{2}}{C_{ }^{2}}MN T)\), which turns out to be negligible in the overall regret upper bound.

The \(s\)-th phase (\(s\{1,2,\}\)) consists of a segment of rounds with length \(l_{s}\), which implies that the first round \(t_{s}\) of the \(s\)-th phase is expressed as \(t_{s}=_{u=1}^{s-1}l_{u}+t_{1}\). At the beginning of each phase \(s\), we compute \(a^{}_{s}\) and the length \(l_{s}\) of the \(s\)-th phase, for which the details will be described later. Then, in the subsequent \(l_{s}\) rounds, we continue to choose the set \(A^{}_{s}=\{i[N] a^{}_{si}=1\}\). More precisely, in the \(t\)-th round, we choose \(a_{t}=a^{}_{s}-b_{t}\) if \(b_{t} a^{}_{s}\) and choose \(a_{t}=0\) otherwise, where we recall \(b_{t}\) is the set of tasks in progress at the \(t\)-th round (see (1)). Since \(a^{}_{s}\), \(a_{t}\) clearly belongs to \(_{t}\). In this procedure, intuitively, we repeatedly restart each task \(i\) in \(A^{}_{s}\) just after \(i\) is completed, in the \(s\)-th phase. Note here that all tasks executed in the \((s-1)\)-st phase are completed by the \((t_{s}+C_{})\)-th round as the processing times are at most \(C_{}\), which implies that \(b_{t} a^{}_{s}\) holds for any \(t[t_{s}+C_{},t_{s+1}-1]\). Hence, in any round \(t[t_{s}+C_{},t_{s+1}-1]\), it holds that \(b_{t}+a_{t}=a^{}_{s}\), which implies all tasks in \(A^{}_{s}\) are running in the phase. Furthermore, for each task \(i A^{}_{s}\), the number of times the task \(i\) is completed in the \(s\)-th phase will be at least \((l_{s}-C_{})/C_{}( l_{s}/C_{}-2)\) and at most \(l_{s}/C_{}+1\).

We next describe how to compute \(a^{}_{s}\). We use _upper confidence bounds (UCB)_ for \(q_{i}=_{i}/_{i}\), the expected reward per round. In our definition of the UCB, we use the empirical means \(_{i}(t)\) and \(_{i}(t)\)of rewards and costs, the empirical variance \(V_{i}^{c}(t)\) of costs, and the number \(T_{i}(t)\) of times that the task \(i\) has been completed before the \(t\)-th round. We define the UCB \(_{i}(t)\) on \(q_{i}\) by

\[_{i}(t)=_{i}(t)+d_{i}^{r}(t)\}}{\{C_{1},_{i}(t)-d_{i}^{c}(t)\}},d_{i}^{r}(t)=(t)}},\]

\[d_{i}^{c}(t)=^{c}(t) t}{T_{i}(t)}}+}-C _{}) t}{T_{i}(t)}. \]

We would like to note this definition includes an _empirical Bernstein bound_\(d_{i}^{c}(t)\) for the processing time \(c_{ti}\), which will turn out to be essential for tight regret upper bounds, as mentioned in Remark 4.2. We then have

\[[|_{i}(t)-_{i}| d_{i}^{r}(t)]},[|_{i}(t)-_{i}| d_{i}^{c}(t)] }. \]

These can be shown from Azuma-Hoeffding inequality and empirical Bernstein's inequality , of which details can be found in Section A.4 of the appendix. Hence, with probability \(1-6/t^{2}\), we have \(q_{i}_{i}(t)\). At the beginning of each phase \(s\), i.e., in the \(t_{s}\)-th round, we calculate these UCBs \((t_{s})=(_{1}(t_{s}),_{2}(t_{s}),,_{N}(t_{s} ))^{}\), and then call the linear optimization oracle to find \(a_{s}^{}_{a}(t_{s})^{}a\).

We set the length \(l_{s}\) of the \(s\)-th phase on the basis of \(_{i A_{s}^{}}T_{i}(t_{s})\), where we denote \(A_{s}^{}=\{i[N] a_{si}^{}=1\}\). We set \(l_{s}=C_{}_{i A_{s}^{}}T_{i}(t_{s})+2C_{}\). In the analysis, we assume that \(B\) is given by \(B= 90}}{C_{}} T\). We then have

\[T_{i}(t_{s}) 90C_{} T/C_{}, l_{s} 90C_{ } T, 90C_{}N T t_{1}=O(C_{}^{2}N T/C_{ }), \]

which will be used at several points in our analysis. These conditions and the construction of \(l_{s}\) lead to the bound on \(T_{i}(t_{s})\) as in the lemma below, which will be used in the regret analysis for the proposed algorithm.

**Lemma 3.1**.: _For any \(s 1\) and \(i A_{s}^{}\), we have \(l_{s} 2C_{}(T_{i}(t_{s+1})-T_{i}(t_{s}))\) and \(T_{i}(t_{s+1}) 4T_{i}(t_{s})\)._

All omitted proofs are given in the appendix. We also have the following bound on \(t_{s}\):

**Lemma 3.2**.: _For any \(s 1\), there exists \(i A_{s}^{}\) such that \(T_{i}(t_{s+1})(1+C_{}/C_{})T_{i}(t_{s})\). Consequently, we have \(t_{s} C_{}(1+C_{}/C_{})^{s/N-2}\) for any \(s 1\)._

From Lemma 3.2, we have \( t_{s} C_{}+(-2)(1+}}{C_{}}) C_{}+(-2) }}{2C_{}}\), where the last inequality follows from the fact that \((1+x) x/2\) for any \(x\). This implies that we have \(s N(}}{C_{}} t_{s}+2)\). Hence, the number of phases, denoted by \(S\), is bounded by \(S N(}}{C_{}} T+2)+1=O(}}{C_{}}N T)\), since the last phase contains \(T\).

The overall procedure of the proposed algorithm is summarized in Algorithm 1, which consists of arithmetic operations and offline linear optimization over \(\). The number of arithmetic operations in each round is bounded by \(O(N)\) since the update of each parameter in Step 7-10 of Algorithm 1 can be performed in \(O(1)\)-arithmetic operations. Furthermore the number of calls to the offline linear optimization oracle is at most \(S=O(}}{C_{}}N T)\). In fact, the offline linear optimization oracle is called only at the beginning of each phase. This means that Algorithm 1 is more efficient than standard UCB algorithms for combinatorial semi-bandits algorithms [24; 25] that require \(O(T)\) calls to the oracle. The space complexity, other than that required for the offline optimization oracle, is \(O(N)\) since the algorithm works by maintaining \(O(N)\) parameters.

## 4 Regret Analysis

We denote \(a^{*}_{a}q^{}a\). Let \(A^{*}=\{i[N] a_{i}^{*}=1\}\) and \(=[N] A^{*}\). We define _suboptimality gaps_\(_{a}\), \(_{i}\) and \(_{}\), respectively, by

\[_{a}=q^{}a^{*}-q^{}a(a),_{i}= _{a:a_{i}=1,_{a}>0}_{a}(i[N]), _{}=_{i}_{i}. \]We also denote the variance of the processing time \(c_{ti}\) by \(_{i}^{2}=[(c_{ti}-_{i})^{2}]\). We note that \(_{i}^{2}[(c_{ti}-C_{1})^{2}](C_{}-C_{1})\, [c_{ti}-C_{1}]=(C_{}-C_{1})(_{i}-C_{1})\).

The goal of this section is to show the following:

**Theorem 4.1**.: _The regret for Algorithm 1 is bounded as_

\[R_{T} O(M_{i}_{i} T}{ _{i}}+}^{2}}{C_{1}^{2}}NM T) O( (}+C_{})}NM T }{C_{1}^{2}}), \]

_where \(_{i} O(_{i}}(1+_{i}^{2} }(_{i}^{2}+}-C_{1})^{2}C_{}}{C_{ }}))) O(_{i}}(1+ }-C_{1}}{_{i}})) O(}}{_{i}C_{1}})\). Furthermore, for any distribution \(\), the regret is bounded as \(R_{T}=O(}}NMT T}+}^ {2}}{C_{1}^{2}}NM T)\)._

The specific definition of \(_{i}\) is given in (19) in the appendix.

_Remark 4.2_.: Bernstein-type confidence bounds used in (3) are essential to achieve the nearly optimal regret bound of \(R_{T}=O(}}NMT T})\). In fact, if we employ a standard confidence bound for \(_{i}\) instead of the Bernstein-type one given in (3), the parameter \(_{i}^{2}\) in the definition of \(_{i}\) will be replaced with \((C_{}-C_{1})^{2}\), which leads a suboptimal regret bound of \(R_{T}=O(}^{2}}{C_{1}^{3}}T T})\).

From Proposition 2.2, the regret can be bounded as follows:

\[R_{T}[_{t=1}^{T}(q^{}a^{*}-r_{t}^{ }a_{t})]+}}{C_{1}}M}\,[t_{1}]+R_{T}^{(1)}+R_{T}^{(2)}+}}{C_{1}}M, \]

where we define

\[R_{T}^{(1)}=[_{s=1}^{S}_{t=t_{s}}^{t_{s+1} -1}(q^{}a^{*}-q^{}a^{*}_{s})], R_{T}^{(2)}=[ _{s=1}^{S}_{t=t_{s}}^{t_{s+1}-1}(q^{}a^{}_{s}-r_{t}^{}a_ {t})]. \]

In these definitions of \(R_{T}^{(1)}\) and \(R_{T}^{(2)}\), the index \(S\) represents the phase that includes the \(T\)-th round, and we define \(t_{S+1}=T+1\) exceptionally, for notational simplicity.

Let us next show the upper bounds for \(R_{T}^{(1)}\) and \(R_{T}^{(2)}\) separately. We will show that

\[R_{T}^{(1)}=O(M_{i}_{i} T}{_{i}}+ }^{2}}{C_{}^{2}}NM), R_{T}^{(2)}=O( }^{2}}{C_{}^{2}}NM T). \]

These, together with (8) and (5), prove the gap-dependent regret bound (7) in Theorem 4.1.

We here discuss an upper bound on \(R_{T}^{(1)}\) while the analysis of \(R_{T}^{(2)}\) is given in the supplementary material. We denote \(_{s}=A_{s}^{} A^{*}\). Define \(d_{i}(t)\) by \(d_{i}(t)=}{c_{i}}(t)}}\), where \(_{i}\) is defined by (19) in the appendix so that \(_{i}=(_{i}}(1+_{i }^{2}}(_{i}^{2}+}-C_{})^{2}C_{}}{C_{}})))\). We then have \(_{i}(t)-q_{i}(t) d_{i}(t)\) with high probability. In fact, we can show the following lemma using concentration inequalities, of which proof can be found in the appendix.

**Lemma 4.3**.: _For any \(s\), with a probability at least \(1-6N/t_{s}^{2}\), we have \(_{a_{s}^{}}_{i_{s}}d_{i}(t_{s})\). Consequently, we have \(R_{T}^{(1)}[_{T}]+O(_{} t_{}})\), where we define \(_{T}=_{s=1}^{S}l_{s}_{a_{s}^{}}[_{ s}]\) with \(_{s}=\{_{a_{s}^{}}_{i_{s}}d_{i}(t_{s}), \;_{a_{s}^{}}>0\}.\)_

We can show an upper bound on \(_{T}\) in a way similar to what Kveton et al.  did, as follows:

**Lemma 4.4**.: \(_{T}\) _defined in Lemma 4.3 is bounded as \([_{T}]=O(M_{i}_{} T}{_{i}}).\)_

This bound on \(_{T}\) together with Lemma 4.3 and (5) leads to the first part of (10).

### Gap-Free Regret Bound

We can obtain the gap-free regret bound of \(R_{T}=(}}{C_{}^{2}}NMT})\) in Theorem 4.1 by modifying the analysis of \(_{T}\). For any \(>0\), \(_{T}\) can be bounded as \(_{T}_{s=1}^{S}(l_{s}_{a_{s}^{}}[ _{s},_{a_{s}^{}}>]+l_{s}_{a_{s}^{} }[_{s},_{a_{s}^{}}])=O( }}{C_{}^{2}}MN+ T ),\) where the last inequality follows from the same argument for showing Lemma 4.4. By setting \(=}MN T}{C_{}^{2}T}}\), we obtain \(_{T}=O(}}}MNT T})\). From this, (8), the second part of (10), and Lemma 4.3, we obtain the gap-free regret bound presented in Theorem 4.1. This completes the proof of Theorem 4.1.

### Regret Lower Bound

As shown in Theorem 4.1, the proposed algorithm achieves \(R_{T}=O(}}}NMT})\). This is tight up to a logarithmic factor in \(T\). In fact, we have the following lower bound:

**Theorem 4.5**.: _For any \(N,M,T\) such that \(N M\) and \(T=(C_{})\), there exists a problem instance for which any algorithm suffers regret of \(R_{T}=(}}\{}NMT},MT \})\)._

In the proof of this theorem, we first focus on the case of \(M=1\). If we consider the case of \(c_{ti}=C_{}\), our model is equivalent to the \(N\)-armed bandit with time horizon \(T^{}=(T/C_{})\), which leads to a lower bound of \((})=(}})\) with the aid of the lower bound for multi-armed bandit problems given in Theorem 5.1 by Auer et al. . When considering the case of \(r_{ti}=1\) and \(c_{ti}\{C_{},C_{}\}\), we can show a regret lower bound of \((}-C_{}}{C_{}C_{}}}NT})\), by using the proof technique by Badanidiyuru et al. . Combining these two bounds, we obtain an \((}}}NT})\)-lower bound for the case of \(M=1\). From this result and the technique used in Proposition 2 by Kveton et al. , we have an \((}}}MNT})\)-lower bound for \(M>1\).

## 5 Numerical Experiment

We conducted small experiments to evaluate the practical performance of the proposed algorithm using a synthetic dataset. We set the parameters for the dataset as follows:\(|A| M\)) with \((N,M)=(4,2)\), \(T=10000\), \(C_{ u}=6,C_{1}=1,=(0.5,0.5,0.5,0.5)\). For the expected processing times, we set \(=(1.5,1.5,2.0,2.0)\) as a problem instance with small \(\) or \(=(1.5,1.5,5.0,5.0)\) as a problem instance with large \(\). Each \(c_{ti}\) and \(r_{ti}\) are generated so that \(c_{ti}-1\) follows binomial distributions over \(\{0,1,,C_{ u}-1\}\) and \(r_{ti}\) follows Bernoulli distributions over \(\{0,1\}\). For comparison, we have added the results of applying UCB-BV1 by Ding et al.  and CombUCB1 by Kveton et al. . More detailed information on the experimental setup is provided in the supplementary material.

Figures 2 and 3 show the empirical means of regret computed by \(100\) repetitions of independent experiments. We depict \(2\)-standard deviations of the empirical regret by the shaded areas. These results imply that the proposed algorithm performs better experimentally than other algorithms. We note that UCB-BV1 and CombUCB1 cannot avoid a linear regret as they choose a set in \(\) and wait for them all to be completed, which causes idling time with non-zero probability every time. It is also worth mentioning that the empirical performance of the proposed algorithm is much better than Theorem 4.1 predicts. In fact, under the parameter settings of these experiments, the values in Theorem 4.1 can be approximated as: \(}NMT T} 2100\) and \(}{C_{1}}NM T 2650\).

## 6 Related Work

One of the most relevant studies to this work would be those on _combinatorial semi-bandits_, in which a family \(F 2^{[N]}\) of subsets of arms \([N]\) is given and the player sequentially chooses \(A_{t} F\) and then observes the obtained rewards \(r_{ti}\) for each \(i A_{t}\). Studies on combinatorial semi-bandits are classified into those on stochastic models [25; 24; 34], in which \(r_{t}\) are i.i.d. for \(t\), and those on adversarial models [33; 5], in which \(\{r_{t}\}\) are arbitrary sequences that may be chosen in an adversarial manner. As noted in the introduction, the stochastic combinatorial semi-bandits are special cases of bandit task assignment. The stochastic combinatorial semi-bandits are known to admit sublinear regret. Typical approaches to achieve sublinear regret include upper confidence bounds (UCB)-type algorithms [25; 24] and Thompson sampling algorithms .

Our problem is also relevant to bandit problems with budget constraints. Ding et al.  consider a stochastic multi-armed bandit (MAB) problem with a budget constraint, in which the player observes the reward \(r_{ti_{i}}\) and the cost \(c_{ti_{i}}\) for the chosen arm \(i_{t}\) in each round \(t\). The goal of the player is to maximize the cumulative rewards earned before cumulative costs reach a given total budget amount. This can be seen as a special case of our problem. As shown in Ding et al. , in the budget-constrained MAB problem, the reward of the optimal policy can be characterized by the expected reward-to-cost. This fact is generalized to our setting in Proposition 2.2, which will be used in our regret analysis. It should be mentioned that the budget-constrained MAB problem has been generalized to bandits with multiple plays , which differs from our model, as problems with processing times cannot be reduced to budget-constrained models in general. In addition to their work, there are various generalizations and variants, such as contextual bandit models , models with general distributions [11; 12], and models with multiple budgets [2; 31; 7].

While delayed feedback models [13; 38; 3] are similar to our model in that rewards earned will be revealed in later rounds, there are some essential differences. For example, in our model, the selected arm is occupied during the processing time and cannot be selected, while in the delayed feedback model, the arm can be selected while waiting for feedback. The former model can handle situationswhere the arm corresponds to a labor or computing resource, as in the examples shown in Examples 1 and 2.

Phased-update or lazy-update approaches were incorporated in other bandit problems and online learning problems such as a phased version of UCB for MAB (e.g., Exercise 7.5 of the book ) and the rarely switching OFUL algorithm for linear stochastic bandits . The motivation for using phased-update approaches is primarily to reduce the computational cost , to minimize switching cost , or to address batched settings . To our knowledge, our proposed algorithm is the first algorithm that applies such a phased-update approach to combinatorial semi-bandits. It allows us to reduce the number of oracle calls, and more importantly, to achieve the nearly tight regret bounds for the bandit task assignment. Our study reveals new usefulness of a phased-update approach.

Another line of research related to our problem is the _online matching_ or _online allocation problems with reusable resources_. The problem is formalized with a bipartite graph, and we choose edges for sequentially arriving vertices. The model is similar to ours in that each chosen edge will be unavailable for a certain period of time that follows a probability distribution. However, it is different in that the bipartite graph is unknown at first and all the distributions are given. Moreover, those papers adopt competitive ratio analyses rather than regret analyses, i.e., they evaluate the performance compared to a stronger agent who knows the sequence of arriving vertices. Thus, the existing papers are incomparable with our work.

As mentioned in the introduction, the problem setting of _blocking bandit_ appears to be similar to ours while the difficulties are significantly different. Considering unconstrained situations can highlight the differences in the problem. In the task assignment problem with no constraints, the optimal strategy will be trivial, in which we make each task always processing, i.e., in each round, we start every task that is completed at that round. On the other hand, as the reviewer commented, blocking bandits only allow us to play a single action in each round, which makes the optimal strategy nontrivial. In fact, finding the optimal policy can be an NP-hard problem even if the true distributions of \(r\) and \(c\) are given. This is an illustrative example of why bandit task assignment can be easier than blocking bandits.

## 7 Conclusion and limitation

In this paper, we introduced a new problem setting that we referred to as bandit task assignment, for which we proposed an algorithm with regret bounds. A limitation of this work is that we need to know the upper and lower bounds \(C_{},C_{}\) on the processing times, which may not be possible to assume in some real-world applications. To remove this assumption, it would be necessary to modify the framework of the problem. One possible modification would be to allow the player to suspend processing tasks. In this modified problem setup, even in cases where \(C_{}\) is incorrectly underestimated, it will be possible to immediately abort the task and correct \(C_{}\) when the underestimation is detected. We expect that this strategy and the appropriate parameter update rules for \(C_{}\) and \(C_{}\) allow us to remove the assumption of prior knowledge about processing times. Another future direction is to extend the problem to a decentralized setting, in which multiple agents collaborate to make task assignment decisions. We believe that such an extension of the problem provides insight into real-world applications where multiple entities perform task assignments, such as crowdsourcing and federated learning.