# Rethinking Bias Mitigation: Fairer Architectures

Make for Fairer Face Recognition

 Samuel Dooley\({}^{*}\)

University of Maryland, Abacus.AI

samuel@abacus.ai

&Rhea Sanjay Sukthanker\({}^{*}\)

University of Freiburg

sukthank@cs.uni-freiburg.de

John P. Dickerson

University of Maryland, Arthur AI

johnd@umd.edu

&Colin White

Caltech, Abacus.AI

crwhite@caltech.edu

Frank Hutter

University of Freiburg

fh@cs.uni-freiburg.de

&Micah Goldblum

New York University

goldblum@nyu.edu

* indicates equal contribution

###### Abstract

Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes. We release our code, models and raw data files at https://github.com/dooleys/FR-NAS.

## 1 Introduction

Machine learning is applied to a wide variety of socially-consequential domains, e.g., credit scoring, fraud detection, hiring decisions, criminal recidivism, loan repayment, and face recognition [78; 81; 61; 3], with many of these applications significantly impacting people's lives, often in discriminatory ways [5; 55; 114]. Dozens of formal definitions of fairness have been proposed , and many algorithmic techniques have been developed for debiasing according to these definitions . Existing debiasing algorithms broadly fit into three (or arguably four ) categories: pre-processing [e.g., 32; 93; 89; 110], in-processing [e.g., 123; 124; 25; 35; 83; 110; 73; 79; 24; 59], or post-processing [e.g., 44; 114].

Conventional wisdom is that in order to effectively mitigate bias, we should start by selecting a model architecture and set of hyperparameters which are optimal in terms of accuracy and then apply a mitigation strategy to reduce bias. This strategy has yielded little success in hard problems such as face recognition . Moreover, even randomly initialized face recognition models exhibit bias and in the same ways and extents as trained models, indicating that these biases are baked in to the architectures already . While existing methods for debiasing machine learning systems use a fixed neural architecture and hyperparameter setting, we instead, ask a fundamental question which has received little attention: _Does model bias arise from the architecture and hyperparameters?_ Following an affirmative answer to this question, we exploit advances in neural architecture search (NAS)  and hyperparameter optimization (HPO)  to search for inherently fair models.

We demonstrate our results on face identification systems where pre-, post-, and in-processing techniques have fallen short of debiasing face recognition systems. Training fair models in this setting demands addressing several technical challenges . Face identification is a type of face recognition deployed worldwide by government agencies for tasks including surveillance, employment, and housing decisions. Face recognition systems exhibit disparity in accuracy based on race and gender [37; 92; 91; 61]. For example, some face recognition models are 10 to 100 times more likely to give false positives for Black or Asian people, compared to white people . This bias has already led to multiple false arrests and jail time for innocent Black men in the USA .

In this work, we begin by conducting the first large-scale analysis of the impact of architectures and hyperparameters on bias. We train a diverse set of 29 architectures, ranging from ResNets  to vision transformers [28; 68] to Gluon Inception V3  to MobileNetV3  on the two most widely used datasets in face identification that have socio-demographic labels: CelebA  and VGGFace2 . In doing so, we discover that architectures and hyperparameters have a significant impact on fairness, across fairness definitions.

Motivated by this discovery, we design architectures that are simultaneously fair and accurate. To this end, we initiate the study of NAS for fairness by conducting the first use of NAS+HPO to jointly optimize fairness and accuracy. We construct a search space informed by the highest-performing architecture from our large-scale analysis, and we adapt the existing Sequential Model-based Algorithm Configuration method (SMAC)  for multi-objective architecture and hyperparameter search. We discover a Pareto frontier of face recognition models that outperform existing state-of-the-art models on both test accuracy and multiple fairness metrics, often by large margins. An outline of our methodology can be found in Figure 1.

We summarize our primary contributions below:

* By conducting an exhaustive evaluation of architectures and hyperparameters, we uncover their strong influence on fairness. Bias is inherent to a model's inductive bias, leading to a substantial difference in fairness across different architectures. We conclude that the implicit convention of choosing standard architectures designed for high accuracy is a losing strategy for fairness.
* Inspired by these findings, we propose a new way to mitigate biases. We build an architecture and hyperparameter search space, and we apply existing tools from NAS and HPO to automatically design a fair face recognition system.
* Our approach finds architectures which are Pareto-optimal on a variety of fairness metrics on both CelebA and VGGFace2. Moreover, our approach is Pareto-optimal compared to other previous bias mitigation techniques, finding the fairest model.

Figure 1: Overview of our methodology.

* The architectures we synthesize via NAS and HPO generalize to other datasets and sensitive attributes. Notably, these architectures also reduce the linear separability of protected attributes, indicating their effectiveness in mitigating bias across different contexts.

We release our code and raw results at https://github.com/dooleys/FR-NAS, so that users can easily adapt our approach to any bias metric or dataset.

## 2 Background and Related Work

Face Identification.Face recognition tasks can be broadly categorized into two distinct categories: _verification_ and _identification_. Our specific focus lies in face _identification_ tasks which ask whether a given person in a source image appears within a gallery composed of many target identities and their associated images; this is a one-to-many comparison. Novel techniques in face recognition tasks, such as ArcFace , CosFace , and MagFace , use deep networks (often called the _backbone_) to extract feature representations of faces and then compare those to match individuals (with mechanisms called the _head_). Generally, _backbones_ take the form of image feature extractors and _heads_ resemble MLPs with specialized loss functions. Often, the term "head" refers to both the last layer of the network and the loss function. Our analysis primarily centers around the face identification task, and we focus our evaluation on examining how close images of similar identities are in the feature space of trained models, since the technology relies on this feature representation to differentiate individuals. An overview of these topics can be found in Wang and Deng .

Bias Mitigation in Face Recognition.The existence of differential performance of face recognition on population groups and subgroups has been explored in a variety of settings. Earlier work [e.g., 57, 82] focuses on single-demographic effects (specifically, race and gender) in pre-deep-learning face detection and recognition. Buolamwini and Gebru  uncover unequal performance at the phenotypic subgroup level in, specifically, a gender classification task powered by commercial systems. Raji and Buolamwini  provide a follow-up analysis - exploring the impact of the public disclosures of Buolamwini and Gebru  - where they discovered that named companies (IBM, Microsoft, and Megvii) updated their APIs within a year to address some concerns that had surfaced. Further research continues to show that commercial face recognition systems still have socio-demographic disparities in many complex and pernicious ways .

Facial recognition is a large and complex space with many different individual technologies, some with bias mitigation strategies designed just for them . The main bias mitigation strategies for facial identification are described in Section 4.2.

Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO).Deep learning derives its success from the manually designed feature extractors which automate the feature engineering process. Neural Architecture Search (NAS) , on the other hand, aims at automating the very design of network architectures for a task at hand. NAS can be seen as a subset of HPO , which refers to the automated search for optimal hyperparameters, such as learning rate, batch size, dropout, loss function, optimizer, and architectural choices. Rapid and extensive research on NAS for image classification and object detection has been witnessed as of late . Deploying NAS techniques in face recognition systems has also seen a growing interest . For example, reinforcement learning-based NAS strategies  and one-shot NAS methods  have been deployed to search for an efficient architecture for face recognition with low _error_. However, in a majority of these methods, the training hyperparameters for the architectures are _fixed_. We observe that this practice should be reconsidered in order to obtain the fairest possible face recognition systems. Moreover, one-shot NAS methods have also been applied for multi-objective optimization , e.g., optimizing accuracy and parameter size. However, none of these methods can be applied for a joint architecture and hyperparameter search, and none of them have been used to optimize _fairness_.

For the case of tabular datasets, a few works have applied hyperparameter optimization to mitigate bias in models. Perrone et al.  introduced a Bayesian optimization framework to optimize accuracy of models while satisfying a bias constraint. Schmucker et al.  and Cruz et al.  extended Hyperband  to the multi-objective setting and showed its applications to fairness. Lin et al.  proposed de-biasing face recognition models through model pruning. However, they only considered two architectures and just one set of fixed hyperparameters. To the best of our knowledge,no prior work uses any AutoML technique (NAS, HPO, or joint NAS and HPO) to design fair face recognition models, and no prior work uses NAS to design fair models for any application.

## 3 Are Architectures and Hyperparameters Important for Fairness?

In this section, we study the question _"Are architectures and hyperparameters important for fairness?"_ and report an extensive exploration of the effect of model architectures and hyperparameters.

Experimental Setup.We train and evaluate each model configuration on a gender-balanced subset of the two most popular face identification datasets: CelebA and VGGFace2. CelebA  is a large-scale face attributes dataset with more than 200K celebrity images and a total of 10 177 gender-labeled identities. VGGFace2  is a much larger dataset designed specifically for face identification and comprises over 3.1 million images and a total of 9 131 gender-labeled identities. While this work analyzes phenotypic metadata (perceived gender), the reader should not interpret our findings absent a social lens of what these demographic groups mean inside society. We guide the reader to Hamidi et al.  and Keyes  for a look at these concepts for gender.

To study the importance of architectures and hyperparameters for fairness, we use the following training pipeline - ultimately conducting 355 training runs with different combinations of 29 architectures from the Pytorch Image Model (timm) database  and hyperparameters. For each model, we use the default learning rate and optimizer that was published with that model. We then train the model with these hyperparameters for each of three heads, ArcFace , CosFace , and MagFace . Next, we use the model's default learning rate with both AdamW  and SGD optimizers (again with each head choice). Finally, we also train with AdamW and SGD with unified learning rates (SGD with learning_rate=0.1 and AdamW with learning_rate=0.001). In total, we thus evaluate a single architecture between 9 and 13 times (9 times if the default optimizer and learning rates are the same as the standardized, and 13 times otherwise). All other hyperparameters are held constant fortraining of the model.

Evaluation procedure.As is commonplace in face identification tasks , we evaluate the performance of the learned representations. Recall that face recognition models usually learn representations with an image backbone and then learn a mapping from those representations onto identities of individuals with the head of the model. We pass each test image through a trained model and save the learned representation. To compute the representation error (which we will henceforth simply refer to as _Error_), we merely ask, for a given probe image/identity, whether the closest image in feature space is _not_ of the same person based on \(l_{2}\) distance. We split each dataset into train, validation, and test sets. We conduct our search for novel architectures using the train and validation splits, and then show the improvement of our model on the test set.

The most widely used fairness metric in face identification is _rank disparity_, which is explored in the NIST FRVT . To compute the rank of a given image/identity, we ask how many images of a different identity are closer to the image in feature space. We define this index as the rank of a given image under consideration. Thus, \(=0\) if and only if \(=0\); \(>0\) if and only if \(=1\). We examine the **rank disparity**: the absolute difference of the average ranks for each perceived gender in a dataset \(\):

\[_{}|}_{x_{}}(x)-_{}|}_{x _{}}(x).\] (1)

We focus on rank disparity throughout the main body of this paper as it is the most widely used in face identification, but we explore other forms of fairness metrics in face recognition in Appendix C.4.

Results and Discussion.By plotting the performance of each training run on the validation set with the error on the \(x\)-axis and rank disparity on the \(y\)-axis in Figure 2, we can easily conclude two main points. First, optimizing for error does not always optimize for fairness, and second, different architectures have different fairness properties. We also find the DPN architecture has the lowest error and is Pareto-optimal on both datasets; hence, we use that architecture to design our search space in Section 4.

We note that in general there is a low correlation between error and rank disparity (e.g., for models with error < 0.3, \(=.113\) for CelebA and \(=.291\) for VGGFace2). However, there are differences between the two datasets at the most extreme low errors. First, for VGGFace2, the baseline models already have very low error, with there being 10 models with error < 0.05; CelebA only has three such models. Additionally, models with low error also have low rank disparity on VGGFace2 but this is not the case for CelebA. This can be seen by looking at the Pareto curves in Figure 2.

The Pareto-optimal models also differ across datasets: on CelebA, they are versions of DPN, TNT, ReXNet, VovNet, and ResNets, whereas on VGGFace2 they are DPN and ReXNet. Finally, we note that different architectures exhibit different optimal hyperparameters. For example, on CelebA, for the Xception65 architecture finds the combinations of (SGD, ArcFace) and (AdamW, ArcFace) as Pareto-optimal, whereas the Inception-ResNet architecture finds the combinations (SGD, MagFace) and (SGD, CosFace) Pareto-optimal.

## 4 Neural Architecture Search for Bias Mitigation

Inspired by our findings on the importance of architecture and hyperparameters for fairness in Section 3, we now initiate the first joint study of NAS for fairness in face recognition, also simultaneously optimizing hyperparameters. We start by describing our search space and search strategy. We then compare the results of our NAS+HPO-based bias mitigation strategy against other popular face recognition bias mitigation strategies. We conclude that our strategy indeed discovers simultaneously accurate and fair architectures.

### Search Space Design and Search Strategy

We design our search space based on our analysis in Section 3, specifically around the Dual Path Networks architecture which has the lowest error and is Pareto-optimal on both datasets, yielding the best trade-off between rank disparity and accuracy as seen in Figure 2.

Hyperparameter Search Space Design.We optimize two categorical hyperparameters (the architecture head/loss and the optimizer) and one continuous one (the learning rate). The learning rate's range is conditional on the choice of optimizer; the exact ranges are listed in Table 6 in the appendix.

Architecture Search Space Design.Dual Path Networks  for image classification share common features (like ResNets ) while possessing the flexibility to explore new features  through a dual path architecture. We replace the repeating 1x1_conv-3x3_conv-1x1_conv block with a simple recurring searchable block. Furthermore, we stack multiple such searched blocks to closely follow the architecture of Dual Path Networks. We have nine possible choices for each of the three operations in the DPN block, each of which we give a number 0 through 8. The choices include a vanilla convolution, a convolution with pre-normalization and a convolution with post-normalization, each of them paired with kernel sizes 1\(\)1, 3\(\)3, or 5\(\)5 (see Appendix C.2 for full details). We thus have 729 possible architectures (in addition to an infinite number of hyperparameter configurations). We denote each of these architectures by XYZ where \(X,Y,Z\{0,,8\}\); e.g., architecture 180 represents the architecture which has operation 1, followed by operation 8, followed by operation 0.

Figure 2: (Left) CelebA (Right) VGGFace2. Error-Rank Disparity Pareto front of the architectures with lowest error (< 0.3). Models in the lower left corner are better. The Pareto front is denoted with a dashed line. Other points are architecture and hyperparameter combinations which are not Pareto-optimal.

Search strategy.To navigate this search space we have the following desiderata:

* **Joint NAS+HPO.** Since there are interaction effects between architectures and hyperparameters, we require an approach that can jointly optimize both of these.
* **Multi-objective optimization.** We want to explore the trade-off between the accuracy of the face recognition system and the fairness objective of choice, so our joint NAS+HPO algorithm needs to supports multi-objective optimization [84; 21; 71].
* **Efficiency.** A single function evaluation for our problem corresponds to training a deep neural network on a given dataset. As this can be quite expensive on large datasets, we would like to use cheaper approximations with multi-fidelity optimization techniques [98; 64; 31].

To satisfy these desiderata, we employ the multi-fidelity Bayesian optimization method SMAC3  (using the SMAC4MF facade), casting architectural choices as additional hyperparameters. We choose Hyperband  for cheaper approximations with the initial and maximum fidelities set to 25 and 100 epochs, respectively, and \(=2\). Every architecture-hyperparameter configuration evaluation is trained using the same training pipeline as in Section 3. For multi-objective optimization, we use the ParEGO  algorithm with \(\) set to 0.05.

### Empirical Evaluation

We now report the results of our NAS+HPO-based bias mitigation strategy. First, we discuss the models found with our approach, and then we compare their performance to other mitigation baselines.

Setup.We conducted one NAS+HPO search for each dataset by searching on the train and validation sets. After running these searches, we identified three new candidate architectures for CelebA (SMAC_000, SMAC_010, and SMAC_680), and one candidate for VGGFace2 (SMAC_301) where the naming convention follows that described in Section 4.1. We then retrained each of these models and those high performing models from Section 3 for three seeds to study the robustness of error and disparity for these models; we evaluated their performance on the validation and test sets for each dataset, where we follow the evaluation scheme of Section 3.

Comparison against timm models.On CelebA (Figure 3), our models Pareto-dominate all of the timm models with nontrivial accuracy on the validation set. On the test set, our models still Pareto-dominate all highly competitive models (with Error<0.1), but one of the original configurations (DPN with Magface) also becomes Pareto-optimal. However, the error of this architecture is 0.13, which is significantly higher than our models (0.03-0.04). Also, some models (e.g., VoVNet and DenseNet) show very large standard errors across seeds. Hence, it becomes important to also study

Figure 3: Pareto front of the models discovered by SMAC and the rank-1 models from timm for the _(a)_ validation and _(b)_ test sets on CelebA. Each point corresponds to the mean and standard error of an architecture after training for 3 seeds. The SMAC models Pareto-dominate the top performing timm models (\(Error<0.1\)).

the robustness of models across seeds along with the accuracy and disparity Pareto front. Finally, on VGGFace2 (Figure 4), our models are also Pareto-optimal for both the validation and test sets.

Novel Architectures Outperform the State of the Art.Comparing the results of our automatically-found models to the current state of the art baseline ArcFace  in terms of error demonstrates that our strategy clearly establishes a new state of the art. While ArcFace  achieves an error of 4.35% with our training pipeline on CelebA, our best-performing novel architecture achieves a much lower error of 3.10%. Similarly, the current VGGFace2 state of the art baseline  achieves an error of 4.5%, whereas our best performing novel architecture achieves a much lower error of 3.66%.

Novel Architectures Pareto-Dominate other Bias Mitigation Strategies.There are three common pre-, post-, and in-processing bias mitigation strategies in face identification. First, Chang et al.  demonstrated that randomly flipping labels in the training data of the subgroup with superior accuracy can yield fairer systems; we call this technique Flipped. Next, Wang and Deng  use different angular margins during training and therefore promote better feature discrimination for the minority class; we call this technique Angular. Finally, Morales et al.  introduced SensitiveNets which is a sensitive information removal network trained on top of a pre-trained feature extractor with an adversarial sensitive regularizer. While other bias mitigation techniques exist in face recognition, these three are the most used and pertinent to _face identification_. See Cherepanova et al.  for an overview of the technical challenges of bias mitigation in face recognition. We take the top performing, Pareto-optimal timm models from the previous section and apply the three bias mitigation techniques (Flipped, Angular, and SensitiveNets). We also apply these same techniques to the novel architectures that we found. The results in Table 1 show that the novel architectures from our NAS+HPO-based mitigation strategy Pareto-dominate the bias-mitigated models. In VGGFace2, the SMAC_301 model achieves the best performance, both in terms of error and fairness, compared to the bias-mitigated models. On CelebA, the same is true for the SMAC_680 model.

NAS+HPO-Based Bias Mitigation can be Combined with other Bias Mitigation Strategies.Additionally, we combined the three other bias mitigation methods with the SMAC models that resulted from our NAS+HPO-based bias mitigation strategy. More precisely, we first conducted our NAS+HPO approach and then applied the Flipped, Angular, and SensitiveNets approach afterwards. On both datasets, the resulting models continue to Pareto-dominate the other bias mitigation strategies used by themselves and ultimately yield the model with the lowest rank disparity of all the models (0.18 on VGGFace2 and 0.03 on CelebA). In particular, the bias improvement of SMAC_000+Flipped model is notable, achieving a score of 0.03 whereas the lowest rank disparity of any model from Figure 3 is 2.63, a 98.9% improvement. In Appendix C.6, we demonstrate that this result is robust to the fairness metric -- specifically our bias mitigation strategy Pareto-dominates the other approaches on all five fairness metrics.

Figure 4: Pareto front of the models discovered by SMAC and the rank-1 models from timm for the _(a)_ validation and _(b)_ test sets on VGGFace2. Each point corresponds to the mean and standard error of an architecture after training for 3 seeds. The SMAC models are Pareto-optimal the top performing timm models (Error<0.1).

Novel Architectures Generalize to Other Datasets.We observed that when transferring our novel architectures to other facial recognition datasets that focus on fairness-related aspects, our architectures consistently outperform other existing architectures by a significant margin. We take the state-of-the-art models from our experiments and test the weights from training on CelebA and VGGFace2 on different datasets which the models did not see during training. Specifically, we transfer the evaluation of the trained model weights from CelebA and VGGFace2 onto the following datasets: LFW , CFP_FF , CFP_FP , AgeDB , CALFW , CPLFW . Table 2 demonstrates that our approach consistently achieves the highest performance among various architectures when transferred to other datasets. This finding indicates that our approach exhibits exceptional generalizability compared to state-of-the-art face recognition models in terms of transfer learning to diverse datasets.

Novel Architectures Generalize to Other Sensitive Attributes.The superiority of our novel architectures even goes beyond accuracy-related metrics when transferring to other datasets -- our novel architectures have superior fairness properties compared to the existing architectures _even on datasets which have completely different protected attributes than were used in the architecture search_. Specifically, to inspect the generalizability of our approach to other protected attributes, we transferred our models pre-trained on CelebA and VGGFace2 (which have a gender presentation category) to the RFW dataset  which includes a protected attribute for race and the AgeDB dataset  which includes a protected attribute for age. The results detailed in Appendix C.7 show that our novel architectures always outperforms the existing architectures, across all five fairness metrics studied in this work on both datasets.

Novel Architectures Have Less Linear-Separability of Protected Attributes.Our comprehensive evaluation of multiple face recognition benchmarks establishes the importance of architectures for fairness in face-recognition. However, it is natural to wonder: _"What makes the discovered architectures fair in the first place?_ To answer this question, we use linear probing to dissect the

    &  &  \\
**Model** & **Baseline** & **Fipped** & **Angular** & **SensitiveNets** & **Model** & **Baseline** & **Fipped** & **Angular** & **SensitiveNets** \\  SMAC\_301 & **(3.66\_0.23)** & **(4.95\_0.18)** & (4.14\_0.25) & (6.20\_0.41) & SMAC\_000 & (3.25\_18) & **(5.20\_0.03)** & (3.45\_2.28) & (3.45\_2.18) \\ DPN & (3.56\_0.07) & (5.87\_0.32) & (6.06\_0.36) & (4.76\_0.34) & SMAC\_010 & (4.14\_27) & (127\_5.46) & (4.50\_2.50) & (3.99\_12.12) \\ RENet & (4.09\_0.27) & (5.73\_0.45) & (5.47\_0.26) & (4.75\_0.25) & SMAC\_600 & **(3.21\_96)** & (12.42\_4.50) & (3.80\_16) & (3.29\_20.09) \\ Swin & (5.47\_0.38) & (5.75\_0.44) & (5.23\_0.25) & (5.03\_0.30) & ArcFace & (11.30\_46) & (13.56\_2.70) & (9.90\_50.60) & (9.10\_3.00) \\   

Table 1: Comparison of bias mitigation techniques where the SMAC models were found with our NAS+HPO bias mitigation technique and the other three techniques are standard in facial recognition: Flipped , Angular , and SensitiveNets . Items in bold are Pareto-optimal. The values show (Error;Rank Disparity). Other metrics are reported in Appendix C.6 and Table 8.

  
**Architecture (trained on VGGFace2)** & **LFW** & **CFP\_FF** & **CFP\_FP** & **AgeDB** & **CALFW** & **CPLFW** \\    & 82.60 & 80.91 & 65.51 & 59.18 & 68.23 & 62.15 \\ DPN\_SGD & 93.0 & 91.81 & 78.96 & 71.87 & 78.27 & 72.97 \\ DPN\_AdamW & 78.66 & 77.17 & 64.35 & 61.32 & 64.78 & 60.30 \\ SMAC\_301 & **96.63** & **95.10** & **86.63** & **79.97** & **86.07** & **81.43** \\  
**Architecture (trained on CelebA)** & **LFW** & **CFP\_FF** & **CFP\_FP** & **AgeDB** & **CALFW** & **CPLFW** \\   DPN\_CosFace & 87.78 & 90.73 & 69.97 & 65.55 & 75.50 & 62.77 \\ DPN\_MagFace & 91.13 & 92.16 & 70.58 & 68.17 & 76.98 & 60.80 \\ SMAC\_000 & **94.98** & 95.60 & **74.24** & 80.23 & 84.73 & 64.22 \\ SMAC\_010 & 94.30 & 94.63 & 73.83 & **80.37** & 84.73 & **65.48** \\ SMAC\_680 & 94.16 & **95.68** & 72.67 & 79.88 & **84.78** & 63.96 \\   

Table 2: We transfer the evaluation of top performing models on VGGFace2 and CelebA onto six other common face recognition datasets: LFW , CFP_FF , CFP_FP , AgeDB , CALFW , CPLPW . The novel architectures found with our bias mitigation strategy significantly outperform other models in terms of accuracy. Refer Table 9 for the complete results.

intermediate features of our searched architectures and DPNs, which our search space is based upon. Intuitively, given that our networks are trained only on the task of face recognititon, we do not want the intermediate feature representations to implicitly exploit knowledge about protected attributes (e.g., gender). To this end we insert linear probes at the last two layers of different Pareto-optimal DPNs and the model obtained by our NAS+HPO-based bias mitigation. Specifically, we train an MLP on the feature representations extracted from the pre-trained models and the protected attributes as labels and compute the gender-classification accuracy on a held-out set. We consider only the last two layers, so k assumes the values of \(N\) and \(N-1\) with \(N\) being the number of layers in DPNs (and the searched models). We represent the classification probabilities for the genders by \(gp_{k}=softmax(W_{k}+b)\), where \(W_{k}\) is the weight matrix of the \(k\)-th layer and \(b\) is a bias. We provide the classification accuracies for the different pre-trained models on VGGFace2 in Table 3. This demonstrates that, as desired, our searched architectures maintain a lower classification accuracy for the protected attribute. In line with this observation, in the t-SNE plots in Figure 18 in the appendix, the DPN displays a higher degree of separability of features.

Comparison between different NAS+HPO techniquesWe also perform an ablation across different multi-objective NAS+HPO techniques. Specifically we compare the architecture derived by SMAC with architectures derived by the evolutionary multi-objective optimization algorithm NSGA-II  and multi-objective asynchronous successive halving (MO-ASHA) . We obesrve that the architecture derived by SMAC Pareto-dominates the other NAS methods in terms of accuracy and diverse fairness metrics Table 4. We use the implementation of NSGA-II and MO-ASHA from the sync-tune library  to perform an ablation across different baselines.

## 5 Conclusion, Future Work and Limitations

Conclusion.Our approach studies a novel direction for bias mitigation by altering network topology instead of loss functions or model parameters. We conduct the first large-scale analysis of the relationship among hyperparameters and architectural properties, and accuracy, bias, and disparity in predictions across large-scale datasets like CelebA and VGGFace2. Our bias mitigation technique centering around Neural Architecture Search and Hyperparameter Optimization is very competitive compared to other common bias mitigation techniques in facial recognition.

Our findings present a paradigm shift by challenging conventional practices and suggesting that seeking a fairer architecture through search is more advantageous than attempting to rectify an unfair one through adjustments. The architectures obtained by our joint NAS and HPO generalize across different face recognition benchmarks, different protected attributes, and exhibit lower linear-separability of protected attributes.

Future Work.Since our work lays the foundation for studying NAS+HPO for fairness, it opens up a plethora of opportunities for future work. We expect the future work in this direction to focus on

  
**NAS Method** & **Accuracy**\(\) & **Rank Disparity**\(\) & **Disparity**\(\) & **Ratio**\(\) & **Rank Ratio**\(\) & **Error Ratio**\(\) \\  MO-ASHA\_108 & 95.212 & 0.408 & 0.038 & 0.041 & 0.470 & 0.572 \\  NSGA-IL\_728 & 86.811 & 0.599 & 0.086 & 0.104 & 0.490 & **0.491** \\  SMAC\_301 & **96.337** & **0.230** & **0.030** & **0.032** & **0.367** & 0.582 \\   

Table 4: Comparison between architectures derived by SMAC and other NAS baselines

  
**Architecture (trained on VGGFace2)** & **Accuracy on Layer N**\(\) & **Accuracy on Layer N-1**\(\) \\  DPN\_MagFace\_SGD & 86.042\% & 95.461\% \\ DPN\_CosFace\_SGD & 90.719\% & 93.787\% \\ DPN\_CosFace\_AdamW & 87.385\% & 94.444\% \\ SMAC\_301 & **69.980**\% & **68.240**\% \\   

Table 3: Linear Probes on architectures. Lower gender classification accuracy is better studying different multi-objective algorithms [34; 60] and NAS techniques [67; 125; 115] to search for inherently fairer models. Further, it would be interesting to study how the properties of the architectures discovered translate across different demographics and populations. Another potential avenue for future work is incorporating priors and beliefs about fairness in the society from experts to further improve and aid NAS+HPO methods for fairness. Given the societal importance, it would be interesting to study how our findings translate to real-life face recognition systems under deployment. Finally, it would also be interesting to study the degree to which NAS+HPO can serve as a general bias mitigation strategy beyond the case of facial recognition.

Limitations.While our work is a step forward in both studying the relationship among architectures, hyperparameters, and bias, and in using NAS techniques to mitigate bias in face recognition models, there are important limitations to keep in mind. Since we only studied a few datasets, our results may not generalize to other datasets and fairness metrics. Second, since face recognition applications span government surveillance , target identification from drones , and identification in personal photo repositories , our findings need to be studied thoroughly across different demographics before they could be deployed in real-life face recognition systems. Furthermore, it is important to consider how the mathematical notions of fairness used in research translate to those actually impacted , which is a broad concept without a concise definition. Before deploying a particular system that is meant to improve fairness in a real-life application, we should always critically ask ourselves whether doing so would indeed prove beneficial to those impacted by the given sociotechnical system under consideration or whether it falls into one of the traps described by Selbst et al. . Additionally, work in bias mitigation, writ-large and including our work, can be certainly encourage techno-solutionism which views the reduction of statistical bias from algorithms as a justification for their deployment, use, and proliferation. This of course can have benefits, but being able to reduce the bias in a technical system is a _different question_ from whether a technical solution _should_ be used on a given problem. We caution that our work should not be interpreted through a normative lens on the appropriateness of using facial recognition technology.

In contrast to some other works, we do, however, feel, that our work helps to overcome the portability trap  since it empowers domain experts to optimize for the right fairness metric, in connection with public policy experts, for the problem at hand rather than only narrowly optimizing one specific metric. Additionally, the bias mitigation strategy which we propose here can be used in other domains and applied to applications which have more widespread and socially acceptable algorithmic applications .

#### Acknowledgments

This research was partially supported by the following sources: NSF CAREER Award IIS-1846237, NSF D-ISN Award #2039862, NSF Award CCF-1852352, NIH R01 Award NLM-013039-01, NIST MSE Award #20126334, DARPA GARD #HR00112020007, DoD WHS Award #HQ003420F0035, ARPA-E Award #4334192; TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215; the German Federal Ministry of Education and Research (BMBF, grant RenormalizedFlows 01IS19077C); the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 4179628283; the European Research Council (ERC) Consolidator Grant "Deep Learning 2.0" (grant no. 101045765). Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the ERC. Neither the European Union nor the ERC can be held responsible for them.