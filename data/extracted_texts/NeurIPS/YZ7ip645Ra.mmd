# Structured Prediction

with Stronger Consistency Guarantees

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehrvar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

We present an extensive study of surrogate losses for structured prediction supported by _\(\)-consistency bounds_. These are recently introduced guarantees that are more relevant to learning than Bayes-consistency, since they are not asymptotic and since they take into account the hypothesis set \(\) used. We first show that no non-trivial \(\)-consistency bound can be derived for widely used surrogate structured prediction losses. We then define several new families of surrogate losses, including _structured comp-sum losses_ and _structured constrained losses_, for which we prove \(\)-consistency bounds and thus Bayes-consistency. These loss functions readily lead to new structured prediction algorithms with stronger theoretical guarantees, based on their minimization. We describe efficient algorithms for minimizing several of these surrogate losses, including a new _structured logistic loss_.

## 1 Introduction

In most applications, the output labels of learning problems have some structure that is crucial to consider. This includes natural language processing applications, where the output may be a sentence, a sequence of parts-of-speech tags, a parse tree, or a dependency graph. It also includes image annotation, image segmentation, computer vision, video annotation, object recognition, motion estimation, computational photography, bioinformatics, and many other important applications.

Several algorithms have been designed in the past for structured prediction tasks, including Conditional Random Fields (CRFs) , StructSVMs , Maximum-Margin Markov Networks (M3N) , kernel-regression-based algorithms , Voted CRF and StructBoost , search-based methods  and a variety of deep learning techniques , see Appendix A for a more comprehensive list of references and discussion.

Structured prediction tasks inherently involve a natural loss function based on substructures, which could be the Hamming loss, the \(n\)-gram loss, the edit-distance loss, or some other sequence similarity-based loss or task-specific structured loss. Many of the algorithms previously mentioned overlook this inherent structured loss by simply minimizing the cross-entropy loss. In contrast, the surrogate loss functions minimized by algorithms such as CRFs , M3N , StructSVMs  or Voted CRF and StructBoost  do take into account the natural structured loss of the task. But are these structured prediction loss functions consistent? What guarantees can we rely on when minimizing them over a restricted hypothesis set that does not include all measurable functions? Can we derive non-asymptotic guarantees?This paper deals precisely with these theoretical problems in structured prediction.

**Previous work.** We include a detailed discussion on consistency in structured prediction in Appendix A. Here, we briefly discuss previous work by Osokin et al. (2017). To our knowledge, this is one of the only studies proving Bayes-consistency for a family of loss functions in structured prediction (see also (Nowak et al., 2020) and other related references in Appendix A). The surrogate losses the authors proposed are the following _quadratic losses_ (see also (Zhang, 2004)) defined for any function \(h\) mapping \(\) to \(\) and any loss function \(\) between output labels by

\[(x,y),^{}(h,x,y)=_{y^{}}[(y^{},y)+h(x,y^{})]^{2}.\] (1)

However, the authors only consider the hypothesis set of linear scoring functions. Moreover, the feature vector in their setting only depends on the input \(x\) and ignores the label \(y\). In many applications such as natural language prediction, however, it is critical to allow for features that depend both on the input sequence and the output sequence, parse tree, or dependency graph. Finally, in this formulation, the structured prediction problem is cast as a regression problem. Thus, as shown below, the loss function derived is non-standard, even in the binary classification case, where \(=_{0-1}\) is the zero-one loss and \(=\{y_{1},y_{2}\}\). In this simple case, \(^{}(h,x,y_{1})\) can be expressed as

\[^{}(h,x,y_{1})=_{y^{}}[_{0 -1}(y^{},y_{1})+h(x,y^{})]^{2}=h(x,y_{1})^{2}+(1+h(x,y_{2}))^{2}.\] (2)

This is not a typical formulation since it incorporates the magnitude of individual scores. In contrast, in standard binary classification scenario, only the difference between scores matters.

**Structure of the paper.** We present an extensive study of surrogate losses for structured prediction supported by \(\)_-consistency bounds_. These are recently introduced guarantees that are more relevant to learning than Bayes-consistency, since they are not asymptotic and since they take into account the hypothesis set \(\) used. We first show that no non-trivial \(\)-consistency bound or even Bayes-consistency can be derived for widely used surrogate structured prediction losses (Section 3). We then define several new families of surrogate losses, including _structured comp-sum losses_ (Section 4) and _structured constrained losses_ (Section 5), for which we prove \(\)-consistency bounds and thus Bayes-consistency. These loss functions readily lead to new structured prediction algorithms with stronger theoretical guarantees, based on their minimization. We also describe efficient gradient computation algorithms for several of these surrogate losses, including a new _structured logistic loss_ (Section 6).

## 2 Preliminaries

**Learning scenario.** We consider the standard structured prediction scenario with the input space \(\) and output space \(=\{1,,n\}\). The output space may be discrete objects with overlapping structures, such as sequences, images, graphs, parse trees, lists, or others. We assume that the output can be decomposed into \(l\) substructures. The substructures could represent words or tokens for example, or other subsequences along a sequence, resulting in the decomposition of the output space \(\) as \(=_{1}_{l}\). Here, each \(_{j}\) represents the set of possible labels or classes that can be assigned to the \(j\)-th substructure.

**Scoring function.** Structured prediction is typically formulated via _scoring functions_ that map \(\) to \(\), which assign a score to each possible class \(y\). Let \(\) be a family of such scoring functions. For any \(h\), we denote by \((x)\) its prediction for the input \(x\), which is the output \(y\) that maximizes the score \(h(x,y)\), that is, \((x)=*{argmax}_{y}h(x,y)\), with a fixed deterministic strategy to break ties in selecting the label with the highest score. For simplicity, we choose the label with the highest index under the natural ordering of labels as the tie-breaking strategy. We denote by \(_{}\) the family of all measurable scoring functions.

**Generalization error and target loss.** Given a distribution \(\) over \(\) and a loss function \(\), the _generalization error_ of a hypothesis \(h\) and the _best-in-class generalization error_ are defined as follows:

\[_{}(h)=*{}_{(x,y)}[ (h,x,y)]^{*}_{,}=_{h} _{}(h).\]

[MISSING_PAGE_FAIL:3]

Theorem 3.2], the minimizability gaps vanish \(_{}(_{})=0\) for the family of all measurable functions. More generally, the minimizability gaps vanish when the best-in-class error coincides with the Bayes-error, that is, \(^{*}_{}()=^{*}_{}(_{ })\)[Awasthi et al., 2022b, Mao et al., 2023h].

The following result characterizes the best-in-class conditional error and the conditional regret for a target loss \(\), which will be helpful for proving \(\)-consistency bounds in structured prediction. We denote by \((x)\) the set of all possible predictions on a input \(x\) generated by hypotheses in \(\): \((x)=\{(x) h\}\). The proof is given in Appendix B.

**Lemma 3**.: _The best-in-class conditional error and the conditional regret for a target loss \(\) in structured prediction can be expressed as follows:_

\[^{*}_{,}(x) =_{y^{}(x)}_{y}p(x,y) (y^{},y)\] \[_{,}(h,x) =_{y}p(x,y)((x),y)-_{y^{ }(x)}_{y}p(x,y)(y^{},y).\]

## 3 Structured max losses

In this section, we examine the loss functions associated to several prominent structured prediction algorithms. We show that, while they are natural, none of them is Bayes-consistent, which implies that they cannot be supported by \(\)-consistency bounds either. More generally, we consider the following family of surrogate loss functions proposed in [Cortes, Kuznetsov, Mohri, and Yang, 2016], which we refer to as _structured max losses_:

\[(x,y),^{}(h,x,y)= _{y^{} y}_{(y^{},y)}(h(x,y)-h(x,y^{})),\] (5)

where \(_{u}_{+}\) is an upper bound on \(v u_{v 0}\) for any \(u_{+}\). In this formulation, different choices of \(_{u}\) can lead to different structured prediction algorithms. Specifically, as shown by Cortes et al. (2016), the following choices of \(_{u}(v)\) recover many well-known algorithms:

* \(_{u}(v)=(0,u(1-v))\): _StructSVM_[Tsochantaridis et al., 2005b].
* \(_{u}(v)=(0,u-v)\): _Max-Margin Markov Networks (M3N)_[Taskar et al., 2003b].
* \(_{u}(v)=(1+e^{u-v})\): _Conditional Random Field (CRF)_[Lafferty et al., 2001b].
* \(_{u}(v)=ue^{-v}\): _StructBoost_[Cortes et al., 2016].

The following gives a general negative result for \(^{}\) that holds under broad assumptions.

**Theorem 4** (**Negative results of \(^{}\))**.: _Assume that \(n>2\) and that \(_{u}(v)\) is convex and non-increasing for \(u=1\). Then, the max structured loss \(^{}\) is not Bayes-consistent._

The proof is included in Appendix C. It is straightforward to see that the assumption of Theorem 4 holds for all the choices of \(_{u}\) listed above. Thus, the theorem rules out consistency guarantees for any of the loss functions associated to the structured prediction algorithms mentioned above: StructSVM, M3N, CRF, Structboost. Furthermore, Theorem 4 provides negative results for a broad and generalized family of loss functions, collectively referred to as structured max loss. This extends the scope of existing research, as previous works had only addressed the inconsistency of specific instances within the structured max loss category, such as that of M3N [Osokin et al., 2017, Ciliberto et al., 2016, Nowak et al., 2020].

## 4 Structured comp-sum losses

In this section, we first analyze the Voted CRF loss function, which incorporates the auxiliary loss function \(\) in the CRF loss and which has been used in several previous studies. Next, we introduce a new family of loss functions for structured predictions that we prove to admit strong consistency guarantees.

### Voted Conditional Random Field (VCRF)

We first study a family of surrogate losses called _Voted Conditional Random Field (VCRF)_, which corresponds to the structured prediction algorithm defined in (Cortes et al., 2016):

\[(x,y),\,^{}(h,x,y )=-\![}{_{y^{}}e^{h(x,y^{ })}+(y,y^{})}]=\![_{y^{}}e^{(y,y^{})+h(x,y^{})-h(x,y)}].\]

This loss function has also been presented as the softmax margin (Gimpel and Smith, 2010) or the reward-augmented maximum likelihood (Norouzi et al., 2016). It can be viewed as the _softmax variant_ of the M3N loss. Indeed, the loss function for M3N can be written as follows:

\[(h,x,y)=_{y^{}}(0,(y^{},y)+h(x,y^{})- h(x,y)).\] (6)

If we replace the maximum function with the softmax, we obtain

\[(h,x,y)=\![_{y^{}}e^{(0, (y^{},y)+h(x,y^{})-h(x,y))}]=\![_{y^ {}}\!(1,e^{(y^{},y)+h(x,y^{})-h( x,y)})]\!.\] (7)

Next, we show that, as with the loss function for M3N, the VCRF loss function \(^{}\) is inconsistent.

**Theorem 5** (Negative result of \(^{}\)).: _The Voted Conditional Random Field \(^{}\) is not Bayes-consistent._

The proof is included in Appendix D. The key observation in the proof is that the conditional error of VCRF loss function can be reduced to a specific form when the target loss function \(\) decouples, which can lead to a different Bayes classifier from that of the target loss function.

To the best of our knowledge, no prior studies in the literature have explored the consistency of the VCRF loss formulation. The most closely related discussions center around a specialized instance of the multi-class logistic loss (also referred to as Conditional Random Field in that context), in which \((y^{},y)\) disappears within the framework of the Voted Conditional Random Field. The previous works by Osokin et al. (2017); Ciliberto et al. (2016); Nowak et al. (2020) point out that the multi-class logistic loss cannot be consistent in structured prediction due to the absence of the target loss function within its formulation. Instead, our result shows that, even when integrating the target loss \((y^{},y)\) within its formulation, the Voted Conditional Random Field cannot be consistent.

Along with Theorem 4, these results rule out consistency guarantees for commonly used surrogate loss functions in structured prediction.

### Structured comp-sum loss functions

In this section, we define a family of new loss functions for structured prediction that are not only Bayes-consistent but also supported by \(\)-consistency bounds. These are loss functions that can be viewed as the generalization to structured prediction of loss functions defined via a composition and a sum, and that have been referred to as _comp-sum losses_ in (Mao et al., 2023). Thus, we will refer to them as _structured comp-sum losses_. They are defined as follows:

\[(x,y),^{}(h, x,y)=_{y^{}}(y^{},y)_{1}\!(_{y^{ }}_{2}(h(x,y^{})-h(x,y^{})) ),\] (8)

where \((y^{},y)=1-(y^{},y)\), \(_{1}\!:\!_{+}_{+}\) is a non-decreasing auxiliary function and \(_{2}\!:\!_{+}\) a non-decreasing auxiliary function. This formulation (8) can also be viewed as a weighted comp-sum loss, if we interpret \((,y)\) as a weight vector.

Specifically, we can choose \(_{2}(v)=e^{v}\) and \(_{1}(v)=(v)\), \(_{1}(v)=v-1\), \(_{1}(v)=1-},(0,1)\) and \(_{1}(v)=1-\), which leads to new surrogate losses for structured prediction defined in Table 1. These surrogate losses are novel strict generalization of their counterparts in the standard multi-class classification case where \(=_{0-1}\). More precisely, when \(=_{0-1}\), \(_{}^{}\) coincides with the _logistic loss_(Verhulst, 1838, 1845; Berkson, 1944, 1951); \(_{}^{}\) coincides with the _sum-exponential loss_(Weston and Watkins, 1998; Awasthi et al., 2022); \(_{}^{}\) coincides with thegeneralized cross-entropy loss_(Zhang and Sabuncu, 2018); and \(^{}_{}\) coincides with the _mean absolute error loss_(Ghosh et al., 2017).

We will show that these structured comp-sum losses benefit from \(\)-consistency bounds in structured prediction, when \(\) is a _symmetric_ and _complete_ hypothesis set. A hypothesis set \(\) is _symmetric_ if there exists a family \(\) of real-valued functions such that \(\{[h(x,1),,h(x,n)] h\}=\{[f_{1}(x),,f_{n}(x)]  f_{1},,f_{n}\}\) for any \(x\). Thus, the choice of the scoring functions does not depend on the order of the categories in \(\). A hypothesis set \(\) is _complete_ if it can generate scores that span \(\), that is, \(\{h(x,y) h\}=\) for any \((x,y)\). As shown by Awasthi et al. (2022b) and Mao et al. (2023), these assumptions are general and hold for common hypothesis sets used in practice, such as the family of linear hypotheses and that of multi-layer feed-forward neural networks, and of course that of all measurable functions.

**Theorem 6** (\(\)-consistency bound of \(^{}\)).: _Assume that \(\) is symmetric and complete. Then, for any target loss \(\), any hypothesis \(h\) and any distribution, we have_

\[_{}(h)-_{,}^{*} _{^{}}(h)-_{^{},}^{*}+_{^{}, }-_{,},\] (9)

_where \((t)=2\) when \(^{}=^{}_{}\) or \(^{}_{}\); \((t)=2t}\) when \(^{}=^{}_{}\); and \((t)=nt\) when \(^{}=^{}_{}\)._

Theorem 6 represents a consolidated result for the four structured comp-sum losses, with the proofs for each being presented separately in Appendix E. The key step of the proof is to upper bound the conditional regret of the target loss (Lemma 3) by that of a surrogate loss. To achieve this, we upper bound the best-in-class conditional error by the conditional error of a carefully selected hypothesis \(_{}}\). The resulting softmax \(}_{}\) of this hypothesis only differs from the original softmax \(\) corresponding to \(\) on two labels. Theorem 6 admits as special cases the \(\)-consistency bounds of Mao et al. (2023) given for standard multi-class classification (\(=_{0-1}\)) and significantly extends them to the general structured prediction scenario.

Let us emphasize that our proof technique is novel and distinct from the approach used in (Mao et al., 2023), which only applies to the special case where \(\) is the zero-one loss and cannot be generalized to any target loss \(\). In their proof, the authors choose \(_{}\) based on individual scores \((x,y)\), rather than the softmax. Consequently, when \(_{0-1}\), as is common in structured prediction, the resulting optimization problem of \(\) can be very intricate and a closed-form expression of the optimization solution cannot be derived. However, our new proof method overcomes this limitation. By viewing the softmax of hypothesis as a unit and introducing a pseudo-conditional distribution \(\), we are able to solve a simple constrained optimization problem on \(\) within structured prediction scenario.

By Steinwart (2007, Theorem 3.2), the minimizability gaps \(_{^{},}\) and \(_{,}\) vanish for the family of all measurable functions. Therefore, when \(=_{}\), the \(\)-consistency bounds provided in Theorem 6 imply the Bayes-consistency of these structured comp-sum losses.

**Corollary 7**.: _The structured comp-sum loss \(^{}\) is Bayes-consistent for \(^{}=^{}_{}\), \(^{}_{}\), \(^{}_{}\), and \(^{}_{}\)._

In fact, Theorem 6 provides stronger quantitative bounds than Bayes-consistency when the minimizability gaps vanish, which suggests that if the estimation error of the structured comp-sum loss \(_{^{}}(h)-_{^{},}^{*}\) is reduced to \(\), the estimation error of the target loss \(_{}(h)-_{,}^{*}\) is upper bounded by \(2\) for structured logistic loss and structured sum-exponential loss, \(2\,}\) for structured generalized cross-entropy loss, and \(n\,\) for structured mean absolute error loss.

   \(_{1}(v)\) & Name & Formulation \\  \((v)\) & Structured logistic loss & \(^{}_{}=-_{y^{} y}(y^{ },y)[)}}{_{y^{} y}e^{h (x,y^{})}}]\). \\ \(v-1\) & Structured sum-exponential loss & \(^{}_{}=_{y^{} y}(y ^{},y)_{y^{} y^{}}e^{h(x,y^{}) -h(x,y^{})}\) \\ \(1-}\) & Structured generalized cross-entropy loss & \(^{}_{}=_{y^{} y}(y ^{},y)1-)}}{_{y^{ } y}e^{h(x,y^{})}}^{}\) \\ \(1-\) & Structured mean absolute error loss & \(^{}_{}=_{y^{} y}(y ^{},y)1-)}}{_{y^{} y}e^{h (x,y^{})}}\). \\   

Table 1: A new family of surrogate losses for structured prediction: structured comp-sum losses.

## 5 Structured constrained loss functions

In this section, we introduce another new family of surrogate losses for structured prediction that we prove to admit \(\)-consistency bounds. We will present a novel generalization of the _constrained losses_(Lee et al., 2004; Awasthi et al., 2022b) to structured prediction. Thus, we refer to them as _structured constrained losses_ and define them as follows:

\[(x,y),^{ }(h,x,y)=_{y^{}}_{(y^{},y)}(-h(x,y^{})),\] (10)

with the constraint that \(_{y}h(x,y)=0\) and \(_{u^{}}_{+}\) is an upper bound on \(v u_{v 0}\) for any \(u_{+}\). In standard constrained loss formulation, a single-variable function \((v)\) that defines a margin-based loss is used. In (10), the single-variable function \((v)\) is generalized to being a function of two variables \(_{u}(v)\), which depends on both the target loss and the scores, to accommodate the structured prediction scenario. Specifically, we can choose \(_{u}(v)=ue^{-v}\), \(_{u}(v)=u\{0,1-v\}^{2}\), \(_{u}(v)=u\{0,1-v\}\), \(_{u}(v)=u\{\{0,1-v/\},1\}\), which lead to new surrogate losses for structured prediction defined in Table 2. These surrogate losses are novel generalization of their corresponding counterparts (Lee et al., 2004; Awasthi et al., 2022b) in standard multi-class classification, where \(=_{0-1}\). As with structured comp-sum losses, we will show that these structured constrained losses benefit from \(\)-consistency bounds in structured prediction as well, for any symmetric and complete hypothesis set \(\).

**Theorem 8** (\(\)**-consistency bound of \(^{}\)).: _Assume that \(\) is symmetric and complete. Then, for any target loss \(\), hypothesis \(h\) and any distribution, we have_

\[_{}(h)-_{,}^{*} _{^{}}(h)-_{^{}, }^{*}+_{^{ },}-_{,}.\] (11)

_where \((t)=2t}\) when \(^{}=^{}_{}\); \((t)=2\) when \(^{}=^{}_{-}\); and \((t)=t\) when \(^{}=^{}_{}\) or \(^{}_{}\)._

The proof is included in Appendix F. As for Theorem 8, the key part of the proof is to upper bound the conditional regret of the target loss (Lemma 3) by that of a surrogate loss. Here too, we introduce a pseudo-conditional distribution \(q\), which can be viewed as a weighted distribution of the original one, \(p(x)\), with weights given by the target loss function. Then, we upper bound the best-in-class conditional error by the conditional error of a carefully selected hypothesis \(_{}}\).

As shown by Steinwart (2007, Theorem 3.2), for the family of all measurable functions, the minimizability gaps vanish: \(_{^{},} =0\) and \(_{,}=0\). Therefore, when \(=_{}\), the \(\)-consistency bounds provided in Theorem 6 imply the Bayes-consistency of these structured constrained losses.

**Corollary 9**.: _The structured constrained loss \(^{}\) is Bayes-consistent for \(^{}=^{}_{}\), \(^{}_{- }\), \(^{}_{}\), and \(^{}_{}\)._

As with the cases of structured comp-sum losses, Theorem 8 provides in fact stronger quantitative bounds than Bayes-consistency. They show that that if the estimation error of the structured constrained loss \(_{^{}}(h)-_{^{},}^{*}\) is reduced to \(\), the estimation error of the target loss \(_{}(h)-_{,}^{*}\) is upper bounded by \(2}\) for \(^{}_{}\), \(2\) for \(^{}_{- }\) and \(\) for \(^{}_{}\) and \(^{}_{}\).

It is important to note that we can upper bound the minimizability gap by the approximation error, or finer terms depending on the magnitude of the parameter space as in (Mao et al., 2023h). Furthermore, our \(\)-consistency bounds (Theorems 6 and 8) can be used to derive finite sample learning bounds for a hypothesis set \(\). These bounds depend on the Rademacher complexity of the hypothesis set and the loss function, as well as an upper bound on the minimizability gap for the surrogate loss.

   \(_{u}(v)\) & Name & Formulation (\(_{}h(x,y)=0\)) \\  \(ue^{-v}\) & Structured constrained loss & \(^{}_{}=_{y^{ }}(y^{},y)\{0,1-h(x,y^{})\}^{2}\) \\ \(u\{0,1-v\}^{2}\) & Structured constrained squared-hing loss & \(^{}_{}=_{y^{ }}(y^{},y)\{0,1-h(x,y^{})\}\) \\ \(u\{0,1-v\}\) & Structured constrained hinge loss & \(^{}_{}=_{y^{ }}(y^{},y)\{0,1-h(x,y^{})\}\) \\ \(u\{\{0,1-\},1\}\) & Structured constrained \(\)-margin loss & \(^{}_{}=_{y^{ }}(y^{},y)\{\{0,1-)}{ }\},1\}\). \\   

Table 2: A new family of surrogate losses for structured prediction: structured constrained losses.

## 6 Optimization of \(_{}^{}\) and \(_{}^{}\)

In this section, we show that the gradient of the structured logistic loss \(_{}^{}\) can be computed efficiently at any point \((x_{i},y_{i})\) and therefore that this loss function is both supported by \(\)-consistency bounds and is of practical use. We similarly show that for \(_{}^{}\) in Appendix G.2.

Fix the labeled pair \((x_{i},y_{i})\) and \(h\). Observe that \(_{}^{}(h,x_{i},y_{i})\) can be equivalently rewritten as follows:

\[_{}^{}(h,x_{i},y_{i}) =_{y^{}}(y^{},y_{i}) \![_{y^{}}e^{h(x_{i},y^{}) -h(x_{i},y^{})}]\] \[=-_{y^{}}(y^{},y_{i} )h(x_{i},y^{})+[_{y^{}}(y^{ },y_{i})]\![_{y^{}}e^{h(x_ {i},y^{})}]\] \[=-_{y^{}}(y^{},y_{i} )h(x_{i},y^{})+_{i} Z_{h,i},\]

where \(_{i}=_{y^{}}(y^{},y_{i})\), and \(Z_{h,i}=_{y^{}}e^{h(x_{i},y)}\). Note that \(_{i}\) does not depend on \(h\) and can be pre-computed. Modulo normalization, this quantity is the average _similarity_ of \(y_{i}\) to \(\), if we interpret \(=1-\) as a similarity. While \(\) may be very large, this can be often computed straightforwardly for most loss functions \(\). For example, for the Hamming loss, for sequences of length \(l\), we have

\[|}_{i}= }\![_{k=1}^{l}(1-_{y^{}_{k}+y_{i,k}})]=_{k=1}^{l}}\![_{y^{}_{k} =y_{i,k}}]=_{k=1}^{l}=.\]

Thus, in this case, \(_{i}\) does not depend on \(i\) and is a universal constant. Similarly, \(_{i}\) can be shown to be a constant for many other losses.

**Hypothesis set.** For the remaining of this section, to simplify the presentation, we will consider the hypothesis set of linear functions \(=x(x,y)^ {d}}\), where \(\) is a feature mapping from \(\) to \(^{d}\). Note that a number of classical structured prediction algorithms adopt the same linear hypothesis set: StructSVM (Tschantaridis et al., 2005b), Max-Margin Markov Networks (M3N) (Taskar et al., 2003b), Conditional Random Field (CRF) (Lafferty et al., 2001b), Voted Conditional Random Field (VCRF) (Cortes et al., 2016). Our algorithms can also be incorporated into standard procedures for training neural network architectures (see (Cortes et al., 2018), Appendix B).

**Markovian features.** We will also assume Markovian features, as is common in structured prediction. Features used in practice often admit this property. Furthermore, in the absence of any such assumption, it is known that learning and inference in general are intractable. We will largely adopt here the definitions and notation from (Cortes et al., 2016) and will consider the common case where \(\) is a set of sequences of length \(l\) over a finite alphabet \(\) of size \(r\). Other structured problems can be treated in similar ways. We will denote by \(\) the empty string and for any sequence \(y=(y_{1},,y_{l})\), we will denote by \(y^{s^{}}_{s}=(y_{s},,y_{s^{}})\) the substring of \(y\) starting at index \(s\) and ending at \(s^{}\). For convenience, for \(s 0\), we define \(y_{s}\) by \(y_{s}=\).

We will assume that the feature vector \(\) admits a _Markovian property of order \(q\)_, that is it can be decomposed as follows for any \((x,y)\):

\[(x,y)=_{s=1}^{l}(x,y^{s}_{s-q+1},s).\] (12)

for some position-dependent feature vector function \(\) defined over \(^{q}[l]\). We note that we can write \(=_{k=1}^{p}_{k}\) with \(_{k}=(0,,_{k},,0)\). In the following, abusing the notation, we will simply write \(_{k}\) instead of \(_{k}\). Each \(_{k}\) corresponds to a Markovian feature vector based only on \(k\)-grams, \(p\) is the largest \(k\). Thus, for any \(x\) and \(y\), we have

\[(,y)=_{k=1}^{p}_{k}(x,y).\] (13)For any \(k[1,p]\), let \(_{k}\) denote the position-dependent feature vector function corresponding to \(_{k}\). Also, for any \(x\) and \(y^{l}\), define \(\) by \((x,y^{s}_{s-p+1},s)=_{k=1}^{p}_{k}(x,y^{s}_{s-k+1},s)\). Observe then that we can write

\[(x,y)=_{k=1}^{p}_{k}(x,y)=_{k=1}^{p}_{s=1}^{l}_{k}(x,y^ {s}_{s-k+1},s)=_{s=1}^{l}_{k=1}^{p}_{k}(x,y^{s}_{s-k+1},s)=_{s= 1}^{l}(x,y^{s}_{s-p+1},s).\]

**Gradient computation.** Adopting the shorthand \(\) for \(h\), we can rewrite the loss at \((x_{i},y_{i})\) as:

\[_{}^{}(,x_{i},y_{i})=- [_{y^{}}(y^{},y_{i})(x_{ i},y^{})]+_{i} Z_{,i}.\]

Thus, the gradient of \(_{}^{}\) at any \(^{d}\) is given by

\[_{}^{}() =-_{y^{}}(y^{},y_{i} )(x_{i},y^{})+_{i}_{y^{}} (x_{i},y)}}{_{y^{}}e^{ (x_{i},y^{})}}(x_{i},y)\] \[=-_{y^{}}(y^{},y_{i} )(x_{i},y^{})+_{i}}_{y }[(x_{i},y)],\]

where \(_{}\) is defined for all \(y\) by \(_{}(y)=(x_{i},y)}}{Z_{}}\) with \(Z_{}=_{y}e^{(x_{i},y)}\). Note that the sum defining these terms is over a number of sequences \(y\) that is exponential in \(r\) and that the computation appears to be therefore challenging. The following lemma gives the expression of the gradient of \(_{}^{}\) and helps identify the most computationally challenging terms.

**Lemma 10**.: _For any \(^{d}\), the gradient of \(_{}^{}\) can be expressed as follows:_

\[_{}^{}()=_{s=1}^{l}_{ ^{p}}_{i}_{}( ,s)-(,s)(x_{i},,s),\]

where \(_{}(,s)=_{y:y^{s}_{s-p+1}=} _{}(y)\) and \((,s)=_{y:y^{s}_{s-p+1}=}(y, y_{i})\).

Proof.: Using the decomposition of the feature vector, we can write:

\[_{y}(y,y_{i})(x_{i},y)=_{ y^{l}}(y,y_{i})_{s=1}^{l}(x_{i},y^{s}_{s- p+1},s)=_{s=1}^{l}_{^{p}}_{y:y^{s}_{s- p+1}=}(y,y_{i})(x_{i}, ,s)\] \[}_{y_{}}[(x_{i},y)]= _{y^{l}}_{}(y)_{s=1}^{l} (x_{i},y^{s}_{s-p+1},s)=_{s=1}^{l}_{ ^{p}}_{y:y^{s}_{s-p+1}=}_{}( y)(x_{i},,s).\]

This completes the proof. 

In light of this result, the bottleneck in the gradient computation is the evaluation of \(_{}(,s)\) and \((,s)\) for all \(s[l]\) and \(^{p}\). In previous work , it was shown that the quantities \(_{}(,s)\) can be determined efficiently, all together, by running two single-source shortest-distance algorithms over the \((+,)\) semiring on an appropriate weighted finite automaton (WFA). The overall time complexity of the computation of all quantities \(_{}(,s)\), \(^{p}\) and \(s[l]\), is then in \(O(lr^{p})\), where \(r=||\).

We now analyze the computation of \((,s)\) for a fixed \(^{p}\) and \(s[l]\). Note that, unlike \(_{}(,s)\), this term does not depend on \(\) and can therefore be computed once and for all, before any gradient computation. The sum defining \((,s)\) is over all sequences \(y\) that admit the substring \(\) at position \(s\).

**Rational losses.** In Appendix G.1, we also give an efficient algorithm for the computation of the quantities \((,s)\) in the case of Markovian losses. Here, we present an efficient algorithm for their computation in the important case of _rational losses_. This is a general family of loss functions based on rational kernels  that includes, in particular, \(n\)-gram losses, which can be defined for a pair of sequences \((y,y^{})\) as the negative inner product of the vectors of \(n\)-gram counts of \(y\) and \(y^{}\).

[MISSING_PAGE_FAIL:10]