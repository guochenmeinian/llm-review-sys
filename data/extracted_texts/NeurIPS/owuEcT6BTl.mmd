# Emergence of Hidden Capabilities:

Exploring Learning Dynamics in Concept Space

Core Francisco Park\({}^{*}{}^{1,2,3}\), Maya Okawa\({}^{*}{}^{1,3}\), Andrew Lee\({}^{4}\)

**Hidenori Tanaka\({}^{1}{}^{,}{}^{3}\), Ekdeep Singh Lubana\({}^{}{}^{1,}{}^{3}\)**

\({}^{1}\)CBS-NTT Program in Physics of Intelligence, Harvard University

\({}^{2}\)Department of Physics, Harvard University

\({}^{3}\)Physics & Informatics Laboratories, NTT Research, Inc.

\({}^{4}\)EECS Department, University of Michigan, Ann Arbor

equal contribution, \(\) equal advising (see detailed list). Email: corefranciscopark@g.harvard.edu, ajjl@umich.edu, {mayaokawa, ekdeeplubana, hidenori_tanaka}@fas.harvard.edu

###### Abstract

Modern generative models demonstrate impressive capabilities, likely stemming from an ability to identify and manipulate abstract concepts underlying their training data. However, fundamental questions remain: what determines the concepts a model learns, the order in which it learns them, and its ability to manipulate those concepts? To address these questions, we propose analyzing a model's learning dynamics via a framework we call the _concept space_, where each axis represents an independent concept underlying the data generating process. By characterizing learning dynamics in this space, we identify how the speed at which a concept is learned, and hence the order of concept learning, is controlled by properties of the data we term _concept signal_. Further, we observe moments of _sudden turns_ in the direction of a model's learning dynamics in concept space. Surprisingly, these points precisely correspond to the emergence of _hidden capabilities_, i.e., where latent interventions show the model possesses the capability to manipulate a concept, but these capabilities cannot yet be elicited via naive input prompting. While our results focus on synthetically defined toy datasets, we hypothesize a general claim on _emergence of hidden capabilities_ may hold: generative models possess latent capabilities that emerge suddenly and consistently during training, though a model might not exhibit these capabilities under naive input prompting.

## 1 Introduction

Modern generative models, such as text-conditioned diffusion models, show unprecedented capabilities [1; 2; 3; 4; 5; 6; 7; 8]. These abilities have led to use of such models in applications as valuable as training control policies for robotics [9; 10; 11] and models for weather forecasting , to as drastic as campaigning in democratic elections [13; 14]. Similar claims can be made for generative models of other modalities, e.g., large language models (LLMs) [15; 16; 17; 18], speech and audio models [14; 19], or even systems designed for enabling scientific applications such as drug discovery [20; 21]. Arguably, acquiring such general capabilities requires for models to internalize the data-generating process and disentangle the concepts (aka latent variables or factors of variation) underlying it [22; 23]. Flexibly manipulating these concepts can then enable generation of novel samples that are entirely out-of-distribution (OOD) with respect to the ones used for training [24; 25; 26; 27; 28].

As shown in prior work, modern generative models do exhibit signs of disentangling concepts underlying the data generating process and learning of corresponding capabilities to manipulate saidconcepts . At the same time, contemporary work has argued that models' capabilities can be unreliable, arbitrarily failing for a given input and succeeding on another . Thus, critical questions remain on how generative models acquire their capabilities (see Fig. 1): what determines whether the model will disentangle a concept and learn the capability to manipulate it; are all concepts and corresponding capabilities learned at the same time; and is there a structure to the order of concept learning such that, given insufficient time, the model learns capabilities to manipulate only a subset of concepts but not others?

**This work.** To address the questions above, we propose to analyze a model's _learning dynamics at the granularity of concepts_. Since performing such an analysis on realistic, off-the-shelf datasets can be challenging, we develop synthetic toy datasets of 2D objects with different concepts (e.g., shape, size, color) that give us thorough control over the data-generating process and allow for an exhaustive characterization of the model's learning dynamics (see Fig. 1). Our contributions are as follows.

* **Introducing Concept Space.** We propose to evaluate a model's learning dynamics in the _concept space_--an abstract coordinate system whose axes correspond to specific concepts underlying the data-generating process. We instantiate a notion of underspecification in concept space and establish its effects on a model's (in)ability to disentangle concepts.
* **Concept Signal Dictates Speed of Learning.** We find the speed at which a model disentangles a concept and learns the capability to manipulate it is dictated by the sensitivity of the data-generating process to changes in values of said concept--a quantity we call _concept signal_. We show concept signals shape the geometry of a learning trajectory and hence control the overall order of concept learning.
* **Sudden Transitions in Concept Learning.** We use analytical curves to explain the phenomenology of learning dynamics in concept space, showing the dynamics can be divided into two broad phases: (**P1**) learning of a _hidden capability_, whereby even if the model does not produce the desired output for a given input, there exist systematic latent interventions that lead the model to generate the desired output, and (**P2**) learning to generate the desired output from the input space.

Overall, while our results focus on a toy synthetic task and text-to-image diffusion models, we hypothesize a broader claim on _hidden emergence of latent capabilities_ holds true: generative models possess latent capabilities that are learned suddenly and consistently during training, but these capabilities are not immediately apparent since prompting the model via the input space may not elicit them, hence hiding how "competent" the model actually is . Empirically, signs of "hidden capabilities" have already been shown in generative models at scale , and our results help provide a formal framework to partially ground such results.

Figure 1: **Concept Learning Geometry underlies emergence. (a) Top: A multimodal model learns to generate the concepts in the order of “astronaut”, “horse”, and finally “riding” as it scales up (adapted from Yu et al.). Middle: “blue square apple” is generated in the order of “apple”, “blue”, and “square"" (adapted from Li et al.). Bottom: Despite its simplicity, our model trained on synthetic data shows _concept learning dynamics_ where it first learns “shape” and then “color”. (b) Concept space is an abstract coordinate space where individual axes correspond to different concepts and a given point corresponds to a “concept class”, i.e., a predefined collection of concepts (e.g., large blue circles on bottom left corner). Traversal along axes of the concept space yield change in a specific property of the sample (e.g., going from large blue circle to large red circle along object color axis). Trajectories show a model’s dynamics in concept space for learning to generate classes shown in-distribution (blue nodes) versus out of distribution (pink / red nodes). As we show, dynamics in concept space are highly interpretable, enabling precise comments on which concepts the model learns first, why, and what order it follows. (c) Measuring how accurately a model generates samples from a given concept class, showing an order of concept learning: first background color is learned, then size, and then object color.**

[MISSING_PAGE_EMPTY:3]

**Definition 2**.: _(**Capability.**) A concept class \(\) denotes the set of concept vectors \(z_{}\) such that a subset of dimensions of these vectors are fixed to predefined values. Classes \(\) and \(^{}\) are said to differ in the \(k^{}\) concept if \( z z_{}\), there exists \(z^{} z_{^{}}\) with \(z[k] z^{}[k]\) and \(z[i]=z^{}[i]\) for \(i k\). We say a model possesses the "capability to alter the \(k^{}\) concept" if for any class \(\) whose samples were seen during training, the model can produce samples from class \(^{}\) that differs from \(\) in the \(k^{}\) concept._

Intuitively, the definition above comments on whether the model can flexibly manipulate concepts of classes seen during training to produce samples from classes that were not seen, i.e., classes that are entirely out-of-distribution. As an example, consider a concept space with shape, color, and size as concepts. If shape and color are fixed to circle and blue respectively, we get the class of blue circles; i.e., \( z z_{}\), the first and second dimension respectively take on values that correspond to the shape circle and color blue. Then, given a conditional diffusion model that was shown blue circles during training, we will say this model possesses the capability to alter the concept color if it can produce samples from concept classes with the same shape as circles, but different colors (e.g., red or green circles). Analyzing learning dynamics in the concept space will thus provide a direct lens into the model's capabilities as they are acquired.

We also note that the definition above is not dependent on the precise manner via which the model is prompted to elicit an output, i.e., it need not be the case that the conditioning information \(h\) that is used for training the model is used to evaluate the model capability. In fact, in our experiments, we will try alternative protocols such as intervening on the latent representations to show that substantially before the model can be prompted using \(h\) to generate samples from an OOD concept class, it can generate samples from said class via such latent interventions. To this end, we also define a measure that assesses how much learning signal the data provides towards disentanglement of a concept, and hence learning of a capability to manipulate it.

**Definition 3**.: _(**Concept Signal.**) The concept signal \(_{i}\) for a concept \(z_{i}\) measures the sensitivity of the data-generating process to change in the value of a concept variable, i.e., \(_{i}(z)}}{{  z_{i}}}\)._

Intuitively, if the training objective is factorized at the granularity of concepts, concept signal indicates how much the model would benefit from learning about a concept. For example, consider a diffusion model trained using the MSE loss with conditioning \(h z\) to predict the noise added to an image \((z)\). \(_{i}\) is then merely the component of the loss representing how much change in conditioning \(h\) yields changes in concept \(z_{i}\), as it is represented in the image. Concept signal will thus serve as a crucial knob in our experiments to analyze learning dynamics in concept space and the order in which concepts are learned.

### Experimental and Evaluation Setup

Before proceeding further, we discuss our experimental setup that concretely instantiates the formalization above. Our data-generating process is motivated by prior work on disentanglement  and involves concept classes defined by three concepts, each with two values: color = {red, blue}, size = {large, small}, and shape={circle, triangle}. In Sec. 4.1 and Sec. 4.4, we use two attributes: color (with red labeled as 0 and blue as 1) and size (large labeled as 0 and small as 1). We generate a total of 2048 images for each class, with objects randomly positioned within each image. We train models on classes 00 (large red circles), 01 (large blue circles), and 10 (small red circles), shown as blue nodes in Fig. 2, and test using class 11(small blue circles), shown as pink nodes, to evaluate a model's ability to manipulate concepts and generalize OOD (see App. B.1 for further details). In Sec. 5, we will restrict to two attributes, shape={circle, triangle} and color = {red, blue}, and study the effect of noisy conditioning, i.e., what happens when concepts are correlated in the conditioning information \(h\) due to some non-linear mixing

Figure 2: **Concept spaces with different concept values see different concept signal.** Consider a concept space comprised of concepts size and color. (Left) The color separation between the classes is stronger than the size separation, resulting in a stronger concept signal in the color dimension. (right) The size separation between the classes is stronger, thus resulting in a stronger signal for size.

function. For all experiments, we use a variational diffusion model  to generate \(3 32 32\) images conditioned on vectors \(h\) (see App. B.2 for further training details).

Evaluation Metric.To assess whether a generated image matches the desired concept class without human intervention, we follow literature on disentangled representation learning [23; 91; 92; 93; 94; 54] and train classifier probes for individual concepts using the diffusion model's training data. The probe architecture involves a U-Net  followed by an average pooling layer and \(n\) MLP classification heads for the \(n\) concept variables. See App. B.2 for further details.

## 4 Learning Dynamics in Concept Space

### Concept Signal Determines Learning Speed

We first demonstrate the utility of concept signal as a tool to gauge at what rate the model learns a concept and the capability to manipulate it. To this end, we develop controlled variants of our data-generating process by changing the level of concept signal of each concept and train diffusion models conditioned with the latent concept vector \(z\) on them. We primarily focus on concepts color and size, altering their concept signal by respectively adjusting the RGB contrast between red and blue and the size difference between large and small objects (see App. B.1 for details). We define the speed of learning each concept as inverse of the number of gradient steps required to reach 80% accuracy for class 11, i.e., the OOD class that requires learning the capability to manipulate concepts as seen during training. Results comparing different concept signals are shown in Fig. 3. For both color and size, we observe that _concept signal dictates the speed at which individual concepts are learned_. We also find that when different concepts have varying strengths of concept signals, this leads to differences in the learning speed for each concept.

### Concept Signal Governs Generalization Dynamics

We next examine the model's learning dynamics in concept space under various levels of concept signal for the concepts color and size. For completeness, we evaluate a model's ability to generalize

Figure 4: **Concept signal governs generalization dynamics.** (a) Learning dynamics in the concept space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution (OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and 10 are shown to illustrate _concept memorization_. See App. D.3 for uncertainties.

Figure 3: **Concept signal determines learning speed.** The speed of concept learning as an inverse of the time in gradient steps when the separation in color (left) and size (right) between different classes increases. Concept learning is faster when pixel differences among concept class and hence concepts are larger.

both in-distribution (ID) and OOD, but only the latter is deemed learning of a capability to manipulate a concept. Results are shown in Fig. 4, with panel (a) showing dynamics for learning to generate samples from ID class 00 and panel (b) showing dynamics for OOD class 11. Results on OOD generalization show _concept memorization_, which we define as the phenomenon where _the model's generations on an OOD conditioning \(h\) are biased towards the training class that helps define the strongest concept signal_. For example, for the unseen conditioning 11, the generations are more alike 01 when the concept signal is stronger for size (e.g., blue curve in Fig. 4(b)) and alike 10 when the signal is stronger for color (e.g., red curve). Interestingly, we observe that for settings with high imbalance in concept signals, e.g., the blue curve in Fig. 4 (b), the endpoint of _concept memorization_ is very biased towards one training class, here 01, delaying its out-of-distribution (OOD) generalization. For the learning trajectories of all classes, see Fig. 15 in App. D.2.

Broadly, our results imply that an early stopped text-to-image model can witness _concept memorization_ and hence simply associate an unseen conditioning to the nearest concept class when asked to generate OOD samples (see Kang et al.  for a similar result in LLMs). However, given sufficient time, the model will disentangle concepts underlying the data-generating process and learn to generate entirely novel, OOD samples. For futher evidence in this vein, we also confirm our results across more general scenarios, including with the real-world CelebA dataset (App. D.4) and using three concept variables: color, background color, and size (App. D.5).

### Towards a Landscape Theory of Learning Dynamics

Fig. 4 indicates models undergo phases of understanding of concepts at different stages of training. In fact, an intriguing property of trajectories shown in Fig. 4 (b) is that there is a sudden turn in the learning dynamics from _concept memorization_ to _OOD generalization_ (e.g., see the top-left quadrant in Fig. 4 (b)). To investigate this further, we propose a minimal toy model that captures the geometry of model's learning trajectories shown in that figure. Specifically, we use the following dynamics equation, \(d(t)+(-))}}\), where \(\) is the target point we want to get to and \(\) is the initial, "biased" target. For example, consider the case with color and size concepts in Fig. 4 (b). The model's generated samples are more alike class 01 and biased towards \((_{1},_{2})=(0,1)\) when the size concept signal \(_{2}\) is stronger than \(_{1}\); and to 10, \((_{1},_{2})=(1,0)\) when the color concept signal \(_{1}\) is stronger than \(_{2}\). We define \((_{1},_{2})\) as the target values or directions we want the learning to head towards (e.g., \((1,1)\) for OOD generalization). Based on this framework, we can derive the following energy function.

\[}{dt}=-_{}L,\ L(z_{1},z_{2})= (d(t-_{1})-z_{1})^{2}+(1-z_{2})^{2}& _{1}>_{2},\\ (1-z_{1})^{2}+(d(t-_{2})-z_{2})^{2}& .\] (1)

Here, \(a\) is determined by the difference \(|_{1}-_{2}|\) and \(_{1}\) and \(_{2}\) denote the times when the model learns concepts \(z_{1}\) and \(z_{2}\), respectively. Fig. 5 illustrates the simulated trajectories for classes 00 and 11, based on Eq. 1. Panels (a) and (b) correspond to classes 00 and 11, respectively. We define the actual target points \((_{1},_{2})\) as \((0,0)\) for class 00 and \((1,1)\) for class 11. For the initial targets \((_{1},_{2})\), we set both values to \((0,0)\) for class 00. For class 11, the targets are set to \((1,0)\) when \(_{1}>_{2}\) and to \((0,1)\) when \(_{1}<_{2}\). _We find our toy model effectively captures the actual learning dynamics for both in-distribution (Fig. 3(b)) and out-of-distribution (OOD) concept classes (Fig. 3(c))_. Notably, our simulation accurately replicates the two types of curves: clockwise (blue trajectory in Fig. 3(b)) and counterclockwise (red trajectory).

An important conclusion that follows from the results above is that the network's learning dynamics can be precisely decomposed into two stages, hence yielding the sudden turns seen in trajectories in

Figure 5: **A Phenomenological Model of Learning Dynamics in Concept Space.** Using Eq. 1, we simulate the learning trajectory for concept class 00 in panel (a) and the OOD class 11 in panel (b). Initially, target values are set at \((0,1)\) or \((1,0)\) based on the concept signal strengths for color or size, respectively. As the model progressively learns each concept, the target values gradually shift towards \((1,1)\). This simple toy model accurately reproduces the observed curves in Fig. 3(c), which arise from _concept memorization_.

Fig. 4. _We hypothesize there is a phase change underlying this decomposition and the model acquires the capability to alter concepts at this point of phase change._ We investigate this next.

### Sudden Transitions in Concept Learning Dynamics

The concept space visualization of learning dynamics observed in Fig. 4 (b) and our toy models analysis in Fig. 5 indicate that there exists a phase in which the model departs from concept memorization and disentangles each of the concepts, but still produces incorrect images. We claim that at the point of departure, the model has in fact already disentangled concepts underlying the data-generating process and acquired the relevant capabilities to manipulate them, hence yielding a change of direction in its learning trajectory. However, naive input prompting is insufficient to elicit these capabilities and generate samples from OOD classes, giving the impression the model is not yet "competent". This then leads to the second phase in the learning dynamics, wherein an alignment between the input space and underlying concept representations is learned. We take the model corresponding to the second from left curve (the green curve) in Fig. 4 (b) to investigate this claim in detail. Specifically, given intermittent checkpoints along the model's learning trajectory, we use the following two protocols for prompting the model to produce images corresponding to the class 11 (blue, small). See App. C for further details.

1. **Activation Space: Linear Latent Intervention.** Given conditioning vectors \(h\), during inference we add or subtract components that correspond to specific concepts (e.g., \(h_{}\)).
2. **Input Space: Overprompting.** We simply enhance the color conditioning to values of higher magnitude, e.g. \((r,\ g,\ b)\) = (0.4, 0.4, 0.6) to (0.3, 0.3, 0.7).

Fig. 6 shows the accuracy for five independent runs under: (a) naive input prompting, (b) linear latent interventions, and (c) overprompting. In Fig. 6 (a), we observe that some runs can produce samples from the target concept class (blue, small) with \( 100\%\) accuracy after around 8,000 gradient steps, while other runs fail to do so. However, in Fig. 6 (b, c), we find alternative protocols for prompting the model can _consistently_ elicit the desired outputs much earlier than input prompting, e.g., at around as early as 6,000 gradient steps. _This indicates the model does possess the capability to alter concepts and generalize OOD!_ Furthermore, we note that different protocols enable elicitation of the capability _at approximately the same number of gradient steps, irrespective of the seed, and that this is precisely the point of sudden turn in the learning dynamics in Fig. 4!_ Interestingly, experiments with Classifier Free Guidance (CFG)  show that CFG only becomes effective after this transition (Fig. 21).

We further explore the second phase of learning in Appendix D.7 by patching the embedding module used for processing the conditioning information from the final checkpoint to an intermediate one. Our results show that when the final checkpoint does enable use of naive input prompting for eliciting a capability, the embedding module can be patched to an intermediate checkpoint and we can retrieve the desired output at approximately the same time that alternative prompting protocols start to work well. This suggests the second phase of learning primarily involves aligning the input space to intermediate representations that enable eliciting the model capabilities. Overall, our results above yield the following hypothesis.

Figure 6: **Emergence of hidden capabilities.** We plot accuracy as a function of gradient steps for five different runs, using three different protocols for prompting the model to generate outputs for OOD concept classes. (a) The baseline, naive prompting protocol; (b) linear latent intervention, applied in the activation space; and (c) overprompting, akin to an intervention on the input space.

**Hypothesis 1**.: _(Emergence of Hidden Capabilities.) Generative models possess hidden capabilities that are learned suddenly and consistently during training, but naive input prompting may not elicit these capabilities, hence hiding how "competent" the model actually is._

### Additional Results on Realistic Data

To provide further support for our hypothesis, we provide results in a more realistic and naturalistic data. Specifically, we use the CelebA dataset , which contains fine-grained attributes corresponding to concepts like Gender, With Hat, and Smiling, and analyze two settings.

* **Using concepts** Gender and Smiling. Results are in Fig. 18, where we find that concept learning dynamics discovered in the relatively simple setting in prior sections generalizes. We see that there is first a phase of concept memorization, wherein the model learns to generate images of (Female, Smiling). However, as training proceeds, there is a sudden turn in the learning dynamics and the model learns to generalize out-of-distribution.
* **Using concepts** Gender and With Hat. In this setting, given a compute budget of 2M gradient steps, we find that the model seem to never learn to generalize out-of-distribution (see Fig. 7). However, when we perform latent interventions on the model's representations, we are able to force the model to produce images from the class (Female, With Hat). These results reiterate that the model is indeed _capable_ of generalizing out-of-distribution, but is not _performant_ when naively prompted.

## 5 Effect of Underspecification on Learning Dynamics in Concept Space

In the results above, we use conditioning information that perfectly specifies concepts underlying the data-generating process, i.e., \(h=z\). In practice, however, instructions are underspecified and one can thus expect correlations between concepts in the conditioning information extracted from those instructions . For example, images of a strawberry are often correlated with the color red (see Fig. 9(a)). Correspondingly, unless a text-to-image model is shown explicit data stating "red strawberry" or images of non-red strawberries, the model's ability to disentangle the concept color from the concept strawberry may be impeded (see generations for "yellow strawberry" in Fig. 9). Motivated by this, we next investigate the effects of using underspecified conditioning information on a model's ability to learn concepts and capabilities to manipulate them.

**Experimental setup.** The data generation and evaluation process follows the protocol described in Sec. 3.1. We select

Figure 8: **Underspecification delays out-of-distribution (OOD) generalization. The number of gradient steps required to reach accuracy above 0.8, as the percentage of masked prompts increases. A higher proportion of masked prompts slows down the speed of concept learning.**

Figure 7: **Validating our Findings on CelebA. (a) Concept space dynamics of generated images concepts Gender and With Hat. We find the generalization class (Female, With Hat) ends up near (Female, No Hat). (b) Compositional Generalization Accuracy when performing latent interventions vs. naive prompting. This rise of accuracy near \(5 10^{5}\) gradient steps clearly demonstrates that the model is _capable_ of generalizing out-of-distribution, but is not _performant_ when evaluated via naive prompting.**color (red and blue) and shape (circle and triangle) as the concepts, drawing an analogy to the "yellow strawberry" example. To simulate underspecification, we randomly select training samples that have a specific combination of shape and color (e.g., "red triangle"). We then mask the token representing the color ("red") and train the model on three concept classes {00, 01, 10}, represented by blue nodes in Fig. 9, with some prompts masked. We test using class 11 (pink node) with no prompts masked to see if the model can generalize OOD.

**Underspecification Delays and Hinders OOD generalization.** Fig. 8 shows how underspecification (masked prompts) affects the speed of concept learning. We see that as the percentage of masked prompts increases, the speed of learning a concept decreases, suggesting that underspecification leads to slower learning of concepts. Further, Fig. 10 (a) shows models' learning dynamics in concept space at varying levels of underspecification. With 0% masking, the model accurately produces an image of blue triangle (see Fig. 9(b)). However, as the percentage of masking increases, the color of generated images shifts from blue to purple (middle), and finally red. This demonstrates that when prompts are masked, the model's understanding of shape triangle becomes intertwined with color red; even when blue is specified in the prompt, the dynamics are biased towards red.

**Toy Model of Learning Dynamics with Underspecification.** When prompts are masked (i.e., underspecification occurs), target values for the concept variables are shifted: e.g., in our setup, with no mask applied, the target directions for the class 11 is \((1,1)\). When the word "red" in "red triangle" is fully masked, the target shifts to \((0,1)\). Assuming this shift is linear with respect to the percentage of masked prompts \(\), we can derive the following energy function.

\[}{dt}=-_{}L,\ L(z_{1},z_{2})=(1-s )-z_{1}^{2}+1-z_{2}^{2}.\] (2)

Figure 10: **Underspecification hinders out-of-distribution (OOD) generalization.** (a) The learning dynamics with varying levels of prompt masking, from 0% to 100%, and the generated images. At 0% masking (top right image), the model correctly produces an image of blue triangle from the prompt “blue triangle.” As the masking increases (from right to left), the images gradually shift towards the incorrect color, red. (b) The simulation of the learning dynamics under underspecification in concept space based on Eq. 2. Our toy model replicates a trained network’s learning dynamics.

Figure 9: **Underspecification and Concept Learning.** (a) The state-of-the-art generative models  erroneously produces a red strawberry (top right corner) for the prompt “yellow strawberry”. (b) Without underspecification in the training data, a model \(F\) accurately learns the concepts of shape and color, successfully generalizes to the unseen node blue triangle (leftmost). As masks are applied to the word red for the prompt red triangle, concept signal for triangle increasingly starts to correlate with the concept red. This causes the output images to change from blue to purple as the level of masking increases (panels left to right). Eventually, the color dimension for triangle collapses, biasing the model towards generating solely red triangles (rightmost).

In the above, parameter \(s\) represents the impact of underspecification. Fig. 10 (b) shows the simulation of model behavior in the concept space according to Eq. 2 (\(s=0.01\)). As the masking level increases, the target directions shift from \(z_{2}=1\) in the top right corner to \(z_{2}=0\) in the top left corner. Our simulated dynamics thus match well with the model's learning dynamics shown in Fig. 10 (a).

Influence of Underspecification on Emergence of Hidden Capabilities.Following Sec. 4.4, we also explore whether it is possible to elicit the desired outputs from a model trained with underspecified data. Fig. 11 shows the accuracy results over five runs (a) without using any prompting method, and (b) using over-prompting. In both scenarios, the percentage of masking is set at 50%. Results clearly demonstrate that with over-prompting, the model achieves 100% accuracy after approximately 1,000 gradient steps, whereas without over-prompting, it fails in three out of five runs even after 2,000 gradient steps. These findings confirm that capability can develop prior to observable behavior, even in cases of underspecification.

## 6 Discussion

**Why study the concept space?** One might ask why concept space could be useful beyond synthetic datasets. In Fig. 14, we show the loss, accuracy, and training trajectory in concept space of a model from Fig. 6. The moment during training when the model acquires the capability to manipulate size and color independently is not evident from either the loss or accuracy curve. However, in concept space, one can directly see the divergence of the trajectory from _concept memorization_. Benchmarking generative models is a challenging task, still often involving humans in the loop [105; 106]. Our concept space framework suggests that benchmarking core generalization capabilities can potentially be reduced to monitoring the learning trajectory in concept space. Moreover, this information can be used to develop better training schemes as discussed in Ren et al. .

Concept learning vs. grokkingWe make a distinction between the potentially delayed elicitation of capabilities in our model versus grokking . The phenomenology of delayed increase in performance on the test set, as seen in Fig. 14, is shared. However, we deal with out-of-distribution evaluations, which is different from setups like modular arithmetic or polynomial regression in which grokking is usually studied [108; 109; 110; 111; 112]. Secondly, research on grokking using hidden progress measures indicates that the model gradually builds representations toward an ideal one [111; 113]; however, our results find that even at the level of latent representations, there is a sudden emergence whereby the model learns the capability to manipulate concepts underlying the data-generating process and generalize OOD.

Is concept learning a phase transition?In Sec. 4.4, we have demonstrated that before the capability to manipulate a concept is learned, the desired outputs cannot be generated regardless of the protocol used to prompt the model. Moreover, we showed that different protocols yield the desired outputs at the exact same time, which is also highly independent of model initialization (Fig. 6, Fig. 17). We thus hypothesize that: _Concept learning is a well-controlled phase transition in model capability. However, the ability to elicit this capability through predefined, single-input prompting can be delayed arbitrarily, depending on factors like the strength of the concept signal._

LimitationsOne limitation of our work is that the compositional setup used in this study is rather simple. Real world concepts are often hierarchical [114; 115] or relational . We also make assumptions that concepts are linearly embedded in the vector space \(\) in Sec. 3. However, this assumption seems to be at least partially justified as seen in Sec. D.8. Another limitation is that the findings are mainly drawn from synthetic data. While we show that our findings generalize to realistic data to some extent, as seen in Sec 4.5, Fig. 18 and Fig. 7, further studies are needed to investigate how our findings apply to real data in general.

Figure 11: **Underspecification and Hidden Capabilities. We use the over-prompting protocol from Sec. 4.4 to assess if the capabilities to enable OOD generalization are learned before naive input prompting. (a) Accuracy for OOD generalization across five different seed runs under naive prompting; (b) Accuracy under over-prompting, with a fixed masking percentage of 50%.**

## Contributions

Motivated by prior work from MO, ESL and HT , CFP and HT started investigating the order in which concepts are learned by a diffusion model, leading to CFP identifying the notion of concept signal and HT proposing to study the learning dynamics in concept space. This kickstarted the project. ESL and HT defined the formal setup (Sec. 3). ESL hypothesized the point of sudden turn marks the emergence of hidden abilities, hence defining the project narrative around competence versus performance. AL and CFP led the experimental verification of this hypothesis. AL led identification of linear representation of concepts and performed the latent and MLP intervention, while CFP contributed to the input space one. MO and HT led formulation of the landscape theory (Sec. 4.3), with inputs from CFP. MO led the underspecification picture of learning dynamics, with inputs from ESL and HT (Sec. 5). Paper writing was led by MO and ESL, with inputs from all authors. Visualizations were mainly led by MO, with inputs from HT and CFP. Experiments on CelebA learning dynamics were conducted by CFP in discussions with MO and corresponding latent intervention were led by AL.