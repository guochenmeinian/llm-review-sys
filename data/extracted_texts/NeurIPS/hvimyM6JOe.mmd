# Noise Aware Finetuning for Analog Non-Linear Dot Product Engine

Lei Zhao Luca Buonanno Aishwarya Natarajan Jim Ignowski Giacomo Pedretti

Hewlett Packard Labs - Artificial Intelligence Research Lab (AIRL)

{lei.zhao, luca.buonanno, aishwarya.natarajan, jim.ignowski,

giacomo.pedretti@hpe.com

###### Abstract

As interest in analog acceleration for deep neural networks (DNNs) grows, ReRAM-based Dot-Product Engines (DPEs) offer an energy-efficient solution for performing vector-matrix multiplications (VMMs) in the analog domain. However, DPEs require Analog-to-Digital Converters (ADCs), which contribute significantly to area and power overhead, and rely on digital logic for operations such as non-linear activations. This work presents an ADC-less DNN accelerator that leverages Analog Content Addressable Memory (ACAM) to replace ADCs and digital activation units. By training decision trees to approximate activation functions and programming them to ACAMs, the novel Non-linear DPE (NL-DPE) enables arbitrary activation to be implemented directly in the analog domain, supporting a broader range of future DNN architectures. Additionally, we explore the inherent noise present in real devices of both crossbars and ACAMs and propose noise-aware finetuning techniques that mitigate accuracy loss, demonstrating notable improvements.

## 1 Introduction

Deep neural networks (DNNs) have significantly grown in size and complexity, driving an increasing demand for memory bandwidth and computation. As DNN models continue to expand, the energy consumption associated with frequent data movement between memory and processing units has become a significant bottleneck, known as the _memory wall_. Such bottlenecks have led to the development of In-Memory Computing (IMC) accelerators, where computation occurs directly within the memory. Among the various IMC approaches, ReRAM-based analog computing stands out as one of the most promising solutions due to its potential for high energy efficiency and scalability. ReRAM cells can be organized in a crossbar structure to design Dot-Product Engines (DPEs), which perform vector-matrix multiplications (VMMs) in the analog domain in a single step, achieving low energy consumption and high parallelism .

However, DNNs consist of more than just VMMs; they also require non-linear activations, which are typically executed using digital logic. Therefore, DPE's analog outputs must be converted into digital signals via Analog-to-Digital Converters (ADCs). Unfortunately, ADCs are both energy and area-inefficient, significantly impacting the overall efficiency of ReRAM-based accelerators. ADCs can consume more than 30% of the chip area and account for over 50% of the total power consumption, creating a substantial bottleneck in achieving truly energy-efficient DNN accelerators .

Another challenge for ReRAM-based DNN accelerators is the inherent noise in analog computing. While many existing works tend to overlook or underestimate its effect, our experiments demonstrate that noise plays a crucial role in determining the accuracy of computations on real hardware. In fact, without careful attention to noise, accuracy can degrade to unacceptable levels, as noted also in otherstudies . Although some efforts have been made to address this issue , novel computation primitives may require to carefully handle new sources of noise.

In this work, we propose the Non-linear DPE (NL-DPE), an analog DNN accelerator design that eliminates the need for ADCs and digital activation units. We convert activation functions into decision trees (DTs), which are then mapped onto the programmable Analog Content Addressable Memory (ACAM) for analog computation. Since ACAM accepts analog inputs and produces digital outputs, ADCs are no longer required after the crossbars. To address the reliability issues of analog computation, we measure noise directly from real devices and develop detailed noise models that account for various sources, including programming inaccuracies and conductance fluctuations, etc. Noise models are integrated into a finetuning process to minimize the accuracy gap caused by noise.

## 2 Analog Computing with ReRAM

Fig. 1(a) shows an example VMM operation \((=)\). \(^{3}\), \(^{3}\) and \(^{3 3}\). Fig. 1(b) illustrates computing the VMM operation in the DPE with a 3\(\)3 crossbar. A ReRAM cell is placed at every intersection of the horizontal wires and vertical wires. Weight elements (\(W_{ij}\)) are programmed as ReRAM conductance (\(G_{ij}\)), and input elements(\(i_{i}\)) are represented by voltage (\(V_{i}\)) on horizontal wires. Following Kirchhoff's law and Ohm's law, the current (\(I\)) from vertical wires convey the dot-product result of the input vector and the weight matrix. The analog VMM operation is completed in a single step within the DPE, achieving high computing parallelism.

Recently, ReRAM has been used to build ACAMs, accelerating tree-based machine learning algorithms in the analog domain . Unlike digital CAM, which is limited to comparing a single input bit with a stored bit, an ACAM cell can compare an analog input value against a stored analog range. To map a trained DT of Fig. 1(c) onto an ACAM array, we traverse each leaf node back to the root, storing the feature thresholds along the path in a row of ACAM cells (Fig. 1(d)). For example, the last row in the ACAM array stores the feature thresholds corresponding to the highlighted path in the DT. Wildcard cells ('X') indicate that the particular feature is irrelevant along that path, thus the full range is programmed. Given an input feature vector during inference, if all feature values fall within the ranges stored in a row, the corresponding match line will be activated, retrieving the predicted class from an adjacent RAM. ACAMs have thus analog input, but digital output representing a match of mismatch.

## 3 Non-Linear Dot Product Engine

Fig. 2 shows a two-layer snippet from a larger neural network mapped to a conventional DPE (a) and the proposed NL-DPE (b). The first layer is followed by a Sigmoid activation function, while the second layer has no activation. Digital-to-Analog Converters (DACs) are required to convert the digital input into analog signals for computing VMM with the weights in the crossbar. In conventional

Figure 1: **(a) An example of VMM computing. (b) Computing the VMM in DPE. (c) An example of a DT. (d) Mapping the DT onto ACAM and performing an inference.**

DPE-based accelerators (Fig. 2(a)), activations are typically performed in the digital domain, thus ADCs are then needed to convert the analog output from the crossbars back into digital form. In our proposed NL-DPE (Fig. 2(b)), we use ACAM to compute activations. As ACAM accepts analog input, ADCs are no longer needed after the crossbars. The output of our ACAM is a digital signal, DACs are still needed before the next DPE. To replace the ADC, ACAM is programmed as an identity function, a special case of activation. For instance, in the figure, the second layer has no activation, so we use an ACAM implementing the identity function to replace the second ADC.

Fig. 3(a) demonstrates how ACAM is used to compute the Sigmoid activation with a 3-bit output. In this example, we assume 0 and 1 are quantized to \(000_{b}\) and \(111_{b}\) respectively, and the remaining 6 values are evenly distributed between 0 and 1, as shown by the y axis in Fig. 3(a). Note that any arbitrary output bit format can be used; the incremental progression from \(000_{b}\) to \(111_{b}\) is simply for illustration purposes. Each output bit (\(y_{2}\), \(y_{1}\), and \(y_{0}\)) is computed using a separate DT, treating each as a binary classification task. The training dataset contains only one feature (input \(x\)), with the output bit as the target. For example, to train the DT for the second most significant bit (MSB) (\(y_{1}\)), the target is set to 1 when the input \(x\) falls within the range of -0.91 to 0.28, or when \(x\) exceeds 1.79, as shown in Fig. 3(a). Fig. 3(b) depicts the trained DT and Fig. 3(c) its mapping onto an ACAM array. Unlike conventional DTs, which generalize to predict unseen inputs, the DTs here memorize the exact patterns from the training data, forcing overfitting. Note that the output of the ACAM already represents the bit value, there is no need to access to an attached memory as opposed to a Look Up Table (LUT) approach [Zhu et al. (2022)].

Since activations are typically monotonic functions, the less significant bits in the output tend to toggle more frequently between 0 and 1, leading to deeper DTs. To mitigate this, we propose an encoding scheme based on Gray code to reduce bit toggling. Fig. 3(a) also shows the Gray code for the output \(y\) on the left (denoted as \(g\)), where only one bit changes between consecutive values. Fig. 3(d) shows the DT trained using Gray code as the output format and Fig. 3(e) its mapping to ACAM. Theoretically, all DTs except for the MSB can be reduced to half their original size, leading to 50% resource usage reduction.

To enable subsequent computations, the Gray code output from the ACAM must be converted back to its original binary form. The conversion only needs simple XOR gates, as illustrated in the Appendix Section A.

Figure 3: **(a)** Analytical and 3-bit quantized sigmoid function using conventional binary (\(y\)) and Grey code (\(g\))format.**(b)** Trained DT to predict the second MSB of the activation with conventional binary representation (\(y_{1}\)) and **(c)** its mapping on ACAM. **(d)** Trained DT to predict the second MSB of the activation with Grey coding (\(g_{1}\)) and **(e)** its mapping on ACAM.

Figure 2: Mapping DNN on **(a)** converttional DPE and **(b)** NL-DPE.

Noise Aware Finetuning for ACAM

ReRAM stores information via its conductance but suffers from significant analog non-idealities, including noise during programming and reading phases. We developed a detailed noise model, calibrated with real-device measurements, to capture these effects. Additionally, given ACAM's more complex cell structure compared to the simple 1T1R crossbar cell, we also propose a noise model for ACAM's threshold values. The detailed noise models are described in Appendix B.

To address noise in crossbars, we adopt the recently proposed _Analog Slicing_ (AS) [Pedretti et al. (2021); Song et al. (2024)] method to map _unquantized_ DNN weights onto crossbars. Compared to the traditional _Bit Slicing_ (BS) [Shafiee et al. (2016)] approach, AS reduces the noise impact on crossbars to negligible levels. A detailed analysis of BS and AS can be found in Appendix C.

The remaining of this section will focus on the Noise Aware Finetuning (NAF) to mitigate the noise impact on ACAMs.

Fig. 4 illustrates the overall process of converting a pretrained DNN into a noise-resilient model. Given a pretrained DNN model, step 1 performs a small number of additional training iterations (typically fewer than 10) to mitigate the noise in the crossbar. During each iteration, each Convolutional and Linear layer's weight matrix is converted into a MSC matrix and LSC matrix (see Appendix C for details), and their corresponding error matrices are sampled based on the noise model described in Appendix B. Then, we use Eq. 9 to get the noise-injected weight matrix. This step finetunes the model to be resilient against noise inherent in crossbar operations.

Because activation functions will be implemented in ACAM, which produces fixed point output, we need to quantize the output of activation functions in step 1. The

Figure 4: Steps of converting a pretrained DNN into a noise-resilient model that will be deployed on NL-DPE.

Step 1 addresses noises in ACAM by performing NAF for each DT independently. Unlike step 1, this finetuning performs individual DTs rather than the entire DNN. Taking the sigmoid operation as an example, random input \(x\) is used to compute its ground truth output \(\). The same input \(x\) is processed through Algorithm 1, which performs a differential approximation of the ACAM computation Pedretti et al. (2022). In Algorithm 1, the DT thresholds are organized into trainable tensors, which can be updated using gradient-based optimization methods such as stochastic gradient descent (SGD). When computing Algorithm 1 in the forward pass, a random noise sampled from the noise model is injected into the DT thresholds. Finally, \(\) and \(y\) are used to compute a loss, which is used in the backward pass and update the DT thresholds.

Algorithm 1 demonstrates the differentiable method for computing an 8-bit output activation using ACAMs. In this setup, 8 DTs compute 1 bit each of the final output. We organize the lower and upper thresholds of each DT into tensors and treat these tensors as trainable parameters. To enable gradient computation, all operations on these tensors are made differentiable.

The for loop in lines 1-10 calculates each of the 8 output bits while incorporating noise into the thresholds. Since the noise model described in Appendix B is based on conductance, the loop first converts the DT thresholds into their corresponding target conductance values based on a threshold-to-conductance ration (\(g_{ration}\)) (lines 2-3). Noise sampled from the model is then added to the target conductance (lines 4). The noisy conductance values are subsequently converted back into DT thresholds (lines 5-6). In line 7, we replace the comparison operation between inputs and thresholds with a ReLU function. This ensures that \(_{i}\) is positive if the input \(x\) lies within the range defined by the lower and upper thresholds; otherwise, \(_{i}\) is zero. Line 8 uses a Sum() operation as a differentiable replacement for the OR gate in ACAM, ensuring \(m_{i}\) is positive if any value in \(_{i}\) is positive. Line 9 employs a division to quantize \(m_{i}\) to 0 or 1, with a small \(\) added to prevent division by zero. After the first loop, each \(m_{i}\) is a floating-point number very close to either 0 or 1, representing the Gray code output bit of an ACAM array.

The second loop, in lines 12-19, implements a differentiable version of the XOR-based decode logic. Here, we replace the XOR operation with a combination of subtraction and squaring to maintain differentiability.

As a result, all computations in Algorithm 1 are fully differentiable, allowing gradients of the threshold tensors to be computed during backpropagation.

## 5 Results

Table 1 shows the accuracy of various models at different stages of NAF. The balled numbers corresponds to the steps depicted in Fig. 4. A small error is introduced by the crossbars even in the presence of noise (step 1), thanks to the mitigation effect of AS. Using DT-based activations (step 2) doesn't introduce significant error as well, making the approach potentially generalizable to other accelerators that may efficiently perform inference of tree-based models 3. However, adding noise to the ACAM performing DT inference (step 1) has an enormous impact on the model accuracy due to the non-linear operation of the ACAM that exacerbates the noise effect, which can only be recovered thanks to NAF. Thus, results demonstrate that our DT-trained activations approach is feasible and does not impact accuracy, but NAF is needed for practical utilization with analog accelerators

NL-DPE completely removes the ADC and digital activation unit in conventional DPE accelerators. We compare the power and area between our proposed NL-DPE computing unit and conventional DPE-based computing unit (including ADC and activation units needed by DPE) in Table 2. We take

 
**Model** & **LeNet** & **ResNet-18** & **SENet** & **EfficientNet** & **ResNet-34** & **VGG11** & **ShuffleNet-v2** & **DenseNet121** \\ Dataset & MNIST & Cifar10 & Cifar10 & Cifar10 & Cifar10 & ImageNet & ImageNet & ImageNet \\  Baseline (FP32) & 99.02 & 92.59 & 95.4 & 91.17 & 73.302 & 70.38 & 69.356 & 74.438 \\   & 99.04 & 92.52 & 95.4 & 91.14 & 73.174 & 70.218 & 68.902 & 74.154 \\  & + NAF & 99.04 & 92.52 & 95.4 & 91.17 & 73.3 & 70.22 & 69.01 & 74.154 \\  & 99.03 & 92.48 & 94.67 & 90.81 & 73.154 & 69.962 & 68.542 & 72.394 \\   & 99.02 & 93.22 & 93.86 & 89.52 & 72.922 & 69.356 & 68.22 & 72.42 \\  & 10.36 & 71.12 & 61.92 & 54.06 & 41.944 & 50.53 & 61.696 & 53.231 \\   & + NAF & 99.01 & 91.8 & 93.23 & 89.05 & 72.066 & 67.4 & 68.05 & 71.34 \\  

Table 1: Accuracy of various stages in the proposed NAF.

the ADC data from [Shafiee et al. (2016)]. For digital activation unit, we take the data from FlexSFU [Reggiani et al. (2023)], a piecewise linear (PWL) based implementation to approximate arbitrary activation in digital domain. Because ACAM array produces the output bits directly, without the need of associated RAM (see Fig. 1(d)) as opposed to other ACAM-based approaches [Zhu et al. (2022)], ACAM's area is significantly smaller than its digital counterpart FlexSFU. As FlexSFU only accepts digital inputs, which requires power- and area-expensive ADCs, NL-DPE's area and power are both lower than that of DPE considering the whole unit.

Fig. 5 compares running the benchmarks on our proposed NL-DPE with two state-of-the-art ReRAM-based accelerators (ISAAC [Shafiee et al. (2016)] and RAELLA [Andrulis et al. (2023)]) as well as a NVIDIA H100 GPU. ISAAC and RAELLA use conventional DPEs for VMMs. Because ISAAC and RAELLA only supports simple activation functions (e.g., ReLU), we extend them with FlexSFU for more complex activations (e.g., SiLU, tanh, etc.). Notably, NL-DPE outperforms GPU in energy efficiency by a geometric mean factor of 4187 while being 292 \(\) faster. At the same time, NL-DPE beats ISAAC and RAELLA thanks to the improved energy efficiency of NL-DPE compared to conventional bulky ADCs and to digital activation unit for the case of all models tested. The latency improvement of NL-DPE compared to ISAAC and RAELLA is due to a drastic reduction of the digital operation needed in the post-processing of dot products.

## 6 Conclusion

We presented the NL-DPE, a novel ADC-less analog in-memory computing primitive. NL-DPE builds on previous works on crossbar arrays and ACAM, by connecting them in the analog domain, with the former accelerating dot products and the latter activation functions, effectively implementing \(f(xW)\) in the analog domain. Activations are approximated with DTs which the ACAM can efficiently accelerate. We propose a Noise Aware Finetuning (NAF) routing to increase accuracy in the presence of ReRAM analog noise and we benchmark the proposed accelerator against the conventional DPE and GPU, reaching significant improvements. We envision NL-DPE as a new building block for in-memory computing, opening up new possibilities for model design fully exploiting the programmable analog non-linearity operation.

   & **Params** & **Spec** & **Power (\(mW\))** & **Area (\( m^{2}\))** \\   \\   & array size & \(256 256\) &  &  \\  & number & 4 & & \\   & resolution & 8bits &  &  \\  & frequency & 1.2GSpks & & \\  & number & 256 & & \\  FlexSFU Reggiani et al. (2023) & bit width & 8bits & 6.6082 & 26431 \\  Total & & & 519.42 & 337956 \\    \\   & array size & \(256 256\) &  &  \\  & number & 4 & & \\   & array size & \(130 1\) &  &  \\  & number & 256 & & \\  Total & & & 41.77 & 11301 \\  

Table 2: Power and area breakdown of DPE and NL-DPE.

Figure 5: Speedup and normalized energy of running the benchmarks on different accelerators.