# Generating Origin-Destination Matrices in Neural Spatial Interaction Models

Ioannis Zachos\({}^{1*}\) Mark Girolami\({}^{1,2}\) Theodoros Damoulas\({}^{2,3}\)

\({}^{1}\)Department of Engineering, Cambridge University, Cambridge, CB2 1PZ.

\({}^{2}\)The Alan Turing Institute, London, NW1 2DB.

\({}^{3}\)Departments of Statistics & Computer Science, University of Warwick, Coventry, CV4 7AL.

iz230@cam.ac.uk

###### Abstract

Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology. In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations. Existing approaches resort to continuous approximations of this matrix and subsequent ad-hoc discretisations in order to perform ABM simulation and calibration. This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors. To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents' trip intensity through a neural differential equation that embeds spatial interactions. Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost. We demonstrate these benefits in large-scale spatial mobility ABMs in Cambridge, UK and Washington, DC, USA.

## 1 Introduction

High-resolution complex simulators such as agent-based models (ABMs) are increasingly deployed to assist policymaking in transportation [10; 21], social sciences [3; 8; 14; 34], and epidemiology [16; 22]. They simulate individual agent interactions governed by stochastic dynamic systems, giving rise to an aggregate, in a mean field sense, continuous emergent structure. This is achieved by computationally expensive forward simulations, which hinders ABM parameter calibration and large-scale testing of multiple policy scenarios . Considering ABMs for the COVID-19 pandemic  as an example, the continuous mean field process corresponds to the spatial intensity of the infections which is noisily observed at some spatial aggregation level, while the individual and discrete human contact interactions that give rise to that intensity are at best partially observed or fully latent. In transportation and mobility, running examples in this work, the continuous mean field process corresponds to the spatial intensity of trips arising from unobserved individual agent trips between discrete sets of origin and destination locations [19; 10].

The formal object of interest that describes the discrete count of these spatial interactions, e.g. agent trips between locations, is the origin-destination matrix (ODM). It is an \(I J\) (two-way) contingency table \(\) with elements \(T_{i,j}\) counting the interactions of two spatial categorical variables \(i,j_{>0}\), see Fig. 1. It is typically sparse due to infeasible links between origin and destination locations, and partially observed through summary statistics - such as table row and/or column marginals - due to privacy concerns, data availability, and data collection costs. Operating atthe discrete ODM level and learning this latent contingency table from summary statistics is vital for handling high-resolution spatial constraints and partial observations such as the total number of agents interacting between a pair of locations. It is also necessary for _population synthesis_ in ABMs , which is performed prior to simulation in order to reduce the size of the ABM's parameter space. Moreover, it avoids errors and biases due to ad-hoc discretisation required when working with continuous approximations of the underlying discrete ODM \(^{*}\).

Traditional ABMs, Fig. 1 (Left), simulate individual-level and spatially granular discrete ODMs at a high computational cost , which scales at least with \((M(M))\), where \(M I+J\) is the size of the agent interaction graph. These ODMs are then aggregated using a sum pooling operation to regional ones, \(^{(s)}\), whose summary statistics \(\) correspond to observed data. However, the lower-dimensional subspace of contingency tables \(\) satisfying constraints \(\) (e.g. row and column marginals), denoted by \(_{}\), is known to be combinatorially large  and therefore sampling and optimisation in that space is notoriously hard since it requires enumerating all elements of \(_{}\). This underlying challenge is the reason why prior art [35; 13; 41; 32; 27; 17] has been imposing a continuous relaxation of the ODM estimation problem, where the continuous approximation of the discrete contingency table restricts inference at the agent trip intensity level \(\). This leads to quantisations and inefficient rejection sampling, see Fig. 2.

Such intensity-level Spatial Interaction Models (SIMs) [45; 31], Fig. 1 (Right), are derived from entropy maximisation arguments, Sec. 2, with summary statistics constraints. These models are embedded in the Harris-Wilson (HW) system of differential equations  that describe the time evolution of location attraction and reflects the utility gained from reaching a destination. Coupling SIMs with the HW model introduces an inductive bias that regularises the continuous ODM and facilitates a mean-field ABM approximation. This approximation effectively acts as a cheap ABM surrogate or emulator, facilitating faster forward simulations to be run by practitioners . This has tangible benefits to ABM calibration allowing faster exploration of the parameter space. Our enhanced ODM reconstruction error demonstrates GeNSIT's ability to sufficiently approximate the ABM simulator at a fraction of the computational cost. However, if both the continuous ODM row and column marginals are fixed, the HW model becomes redundant and the SIM can either be greedily approximated through iterative proportional fitting  which is sensitive to initialisation or become unidentifiable as the number of parameters scales with both the number of origins and destinations. Furthermore, conditioning on individual continuous ODM cells creates discontinuities as the SIM becomes a piecewise function, also preventing its coupling into the HW model.

Figure 1: The ground truth discrete ODM (two-way contingency table) can be reconstructed through either multiple expensive ABM simulations  (ABM rectangle) or approximated by a continuous representation \(\) coupled with the Harris-Wilson SDE (SIM rectangle). In the latter, the ground truth can be reconstructed by sampling in the discrete combinatorial space of constrained ODMs conditioned on \(\) (GeNSIT rectangle). ABM simulations scale with \((M(M))\) compared to GeNSIT which scales with \((IJ)\), where \(M I+J\) is the size of the agent interaction graph.

Figure 2: The space \(_{}\) of \(3 3\) discrete ODMs with summary statistics \(_{T}\). Sampling on the continuous relaxation of \(_{}\) (\(\) level) with quantisation can lead to either large rejection rates, or poor exploration of the distribution over \(_{}\).

Competing approaches that operate directly on the discrete ODM space and are motivated by econometric arguments are discrete choice models . However, these cannot encode summary statistic constraints without introducing large rejection rates. The work of  leverages SIMs to sample discrete ODMs but removes intensity constraints through log-linearity assumptions and does not exploit the physics structure, effectively stripping SIMs of their advantages over other choice models. Markov Chain Monte Carlo routines have been devised to address these issues by learning the SIM parameters  and the associated discrete ODM table over its entire support . Such routines incur a computational overhead in the order of at least \(I\) or \(J\) due to the intractability of the Harris-Wilson model prior, rendering them prohibitive for large-scale applications. Neural Network (NN) parameter calibration of SIMs has been empirically shown to achieve up to ten-fold speed-ups during training  by using a numerical discretisation scheme for the Harris-Wilson model as a forward solver. Despite the significant advantages offered by NNs, they operate strictly in the continuous intensity level and cannot generate discrete agent-level ODMs.

In this paper, we introduce a computationally scalable framework named **G**enerating **N**eural **S**patial **I**nteraction **T**ables (GeNSIT) for exploring the constrained discrete ODM space using closed-form or Gibbs Markov Basis sampling while neurally calibrating the underlying physics-driven SIM parameters of its continuous representation, as shown in Fig. 3. Our framework scales linearly with the number of origin-destination pairs \(IJ\), which is at least \(I\) or \(J\) times faster than MCMC [13; 47]. We offer faster ODM reconstruction and enhanced uncertainty quantification compared to continuous approaches in seminal works [13; 17] and hybrid (discrete and continuous) approaches  in terms of the number of iterations required. It is the first framework that jointly explores the constrained continuous and discrete combinatorial ODM spaces in linear in the number of origin-destination pairs time. It does so while outperforming the prior art in terms of reconstruction error and ground truth table coverage while enabling the integration of a broader set of constraints, if these are available.

Our framework has merit beyond ODM sampling in ABMs. The challenge of learning discrete contingency tables constrained by their summary statistics extends to other fields. Contingency tables have been widely studied in multiple instance learning [33; 12; 48] and ecological inference [37; 36; 39]. In Neuroscience one estimates the efficiency, cost and resilience (equivalent to \(T_{ij}\)) of neural pathways between pairs of brain regions \((i,j)\) to understand communication and information processing ,. Epidemiology also investigates social contact matrices quantifying the number of contacts (\(T_{ij}\)) between types of individuals \((i,j)\) stratified by demographics, such as age .

## 2 Spatial Interaction Intensities and Contingency Tables

Consider \(A\) agents travelling from \(I\) residences (origins) to \(J\) workplaces (destinations). The expected number of trips (intensity) between origin \(i\) and destination \(j\) is \(_{ij}\) and is unobserved. The average number of agents starting (ending) their journey from each origin \(i\) (to each destination \(j\)) is:

\[_{i+}=_{j=1}^{J}_{ij},\ i=1,,I,_{+j} =_{i=1}^{I}_{ij},\ j=1,,J.\] (1)

The expected total number of agents travelling is assumed conserved:

\[_{++}=_{i=1}^{I}_{i+}=_{j=1}^{J}_{+j}=A.\] (2)

The family of models for intensities \(\) that assimilate any collection of the above constraints are called Spatial Interaction Models . The demand for each destination depends on its attractiveness denoted by \((z_{1},,z_{J})_{>0}^{J}\). In our example, this is the number of jobs available at each destination. Let the log-attraction be \(()\). Between two destinations of similar attractiveness, agents are assumed to prefer nearby destinations. Therefore, a cost matrix \(=(c_{i,j})_{i,j=1}^{I,J}\) is introduced to reflect travel impedance. These assumptions are justified by economic arguments  and establish the basis for the agents' utility function. The maximum entropy distribution of agent trips subject to \(_{++}=A\) yields a totally constrained SIM intensity:

\[_{ij}=( x_{j}-_{ij})}{_{k,m}^{I,J} ( x_{m}- c_{km})},\] (3)where \(_{}=\{_{++}\}\) is the set of summary statistic constraints on \(\). Henceforth, we set \(_{++}_{}\) unless otherwise stated. The vector \(=(,)\) contains the agents' two utility parameters controlling the effects of attractiveness and deterrence on the expected number of trips \(\). If \(\) grows larger relative to \(\) then agents gravitate towards destinations with higher job availability regardless of the travel cost incurred, and vice versa. Further, if we also fix the expected origin demand \(_{++}\), then we obtain the following singly (also known as production) constrained SIM intensity:

\[_{ij}=( x_{j}- c_{ij})}{_{m}^{J} ( x_{m}- c_{im})},\] (4)

where \(_{}\) is expanded to include \(_{+}\) in this case. Moreover, setting \(_{}=\{_{++},_{+},_{+}\}\) yields a doubly constrained SIM

\[_{ij}=_{i+}_{+j}( x_{j}- c_{ij})O(i)D(j),\] (5)

where \(O(i),D(j)\) are called balancing factors that ensure that \(_{}\) are satisfied. The balancing factors introduce \(I+J\) unknown parameters, rendering the intensity model unidentifiable. Alternatively, these factors are approximately recursively using iterative proportional fitting , which is sensitive to initialisation. Including individual cell constraints in \(_{}\) breaks the continuity of \(\) as a function of its parameters. For these reasons, the doubly and/or cell-constrained SIMs are prohibitive for use in statistical inference. See App. B.1.1 for more information on SIMs as a modelling choice.

We note that additional data at the origin, destination and origin-destination level can be assimilated into SIMs. This can be achieved by incorporating them as terms in the maximum entropy argument used to derive the \(\) functional forms in equations (3), (4), and (5). We note that the SIM's \(\) is equivalent to the multinomial logit , which generalises our \(\) construction to accommodate for more data. See App. B.1.2 for a guide on eliciting agent utility functions.

It has been shown that the destination attractiveness \(=()\) in families of SIMs is governed by the Harris-Wilson system of \(J\) coupled stochastic differential equations (SDEs) [20; 13]:

\[z_{j}}{t}= z_{j}(_{+j}- z_{j}+ )+ z_{j} B_{j,t},\ \ (0)=^{}\] (6)

where \(>0\) is a responsiveness parameter, \(>0\) is the number of agents competing for one job, \( 0\) is a parameter related to the smallest number of jobs a destination can have, \(>0\) is the standard deviation of SDE's noise, and \(_{t}\) is a \(J\)-dimensional Wiener process. The term \(_{+j}- z_{j}+\) in (6) reflects the net job capacity at destination \(j\). If more agents travel to \(j\) than there are jobs there (positive capacity), this may signify a boost in \(j\)'s local economy, which would trigger a rise in job availability, and vice versa. The diffusion term stochastically perturbs this trend to account for unobserved events, such as local government interventions affecting employment. By the HW-SDE, the SIM intensity is a stochastic and physics-driven quantity reflecting the agents' average number of trips \(\) between their residences and workplaces. However, the SIM intensity differs from the realised number of trips \(}\) agents make. The two notions are connected as follows:

\[T_{ij}|_{ij}(_{ij}),\] (7)

\(}\) is the \(I J\) discrete contingency table summarising the number of agents travelling from \(i\) to \(j\), and \(T_{ij} T_{i^{}j^{}}|_{ij},_{i^{}j^{ }}\ \ i i^{},j j^{}\). Any choice of model for \(T_{ij}|_{ij}\) (say Poisson or Binomial) becomes equivalent upon conditioning on sufficient summary statistics \(_{T}\). We note that the conditional independence of the \(T_{ij}\)'s given the \(_{ij}\)'s and that \(}\) inherits all constraints from \(\) such that every summary statistic constraint in \(\)-space is also applied in \(}\)-space, i.e. we always set \(_{++}=[T_{++}|_{T}]\) and \(_{++}=[T_{i+}|_{T}]\). The hard-coded constraints \(_{T}\) are realisations of the Poisson random variables \(T_{++}|\), \(T_{i+}|\), \(T_{+j}|\), and therefore are no longer random. They can be thought of as noise-free data on the discrete table space. Following the notational convention for \(\), we define \(T_{i+}=_{j=1}^{J}T_{ij}\), \(T_{+j}=_{i=1}^{J}T_{ij}\) and \(T_{++}=_{i,j=1}^{J,J}T_{ij}\), respectively. The aforementioned summary statistics become Dirac random variables upon conditioning on \(}\). The union of table and intensity constraints is summarised by \(\). We sometimes drop subscripts from \(\) for clarity. See App. A for more information on the notation used.

## 3 Neural Calibration of Spatial Interaction Models

We now introduce our framework1(GeNSIT). We estimate the parameters of the continuous SIM intensity by employing an ensemble of Neural Networks \(_{NN}:^{J}^{2}\). This allows us to bypass the computational challenges of solving the HW-SDE inverse problem within a Bayesian framework . Conditioned on those estimates and for every ensemble member \(e=1,,E\), we solve the HW-SDE to get estimates of the time-evolved log destination attraction \(}\) after \(\) time steps using an Euler-Maruyama numerical solver \(_{HW}:^{J}^{J}\). This allows us to incorporate the HW physics model into our parameter estimation without sampling from the SDE's intractable steady-state Boltzmann-Gibbs distribution, which was the case in .

Instead of a log-destination attraction data (\(\)) likelihood, we compute a loss operator \((\ ;\ ,)\) that can assimilate data \(\) from multiple sources on any transformation of \(},},}\). We note that \(\) is used to denote the loss hyperparameters (see Tab. 5). Then, the NN parameters (weights and biases) \(\) are updated using back-propagation using a suite of optimisation algorithms . This step requires derivatives of the loss with respect to the NN parameters \(_{}(\ ;\ ,)\) to be computed, which is achieved using off-the-shelf auto-differentiation libraries . This gradient is informed by \(}\) estimates and therefore by the dynamics of the HW-SDE in (6). Hence, the resulting SIM intensity is both stochastic and physics-driven. These steps are depicted in Fig. 2(b) by following the arrows from right to left and in steps 6 to 12 of Alg. 1. We note that our framework is divided into two sampling schemes: a _Joint_ and a _Disjoint_. The former passes \(\) information to the loss operator \(\), whereas the latter does not. The Joint scheme corresponds to a Gibbs sampler on the full posterior marginals \(|(,,,)\), \(|(,,,)\) and \(|(,,,)\). The Disjoint scheme corresponds to a collapsed Gibbs sampler where we sample from \(|(,,)\), \(|(,,)\) and then from \(|(,,,)\) by integrating out \(\) (see App. B.3.1).

The described optimisation routine yields a point estimate for \(\) for any given ensemble member \(e\) and iteration \(n\). We either increase the ensemble size \(E\) to obtain a distribution over \(\), or we can treat \(\) as a deterministic mapping of realisations of random variables \(,\) whose generalised posterior  is:

\[p(,|)(-(,\ ;\ ,))p(|)p(),\] (8)

where \(\) are loss-related hyperparameters. Each loss evaluation can be treated as a sample from a generalised likelihood (first term), while samples from the \(\) prior (second term) are obtained by forward solving \(_{HW}\). Finally, we can enforce a prior over \(\) by appropriately initialising the NN parameters \(\).

The continuous agent trip intensity \(\) (continuous ODM) samples need to be mapped to discrete agent trips \(\) (discrete ODM), which are required for agent population synthesis. We proceed by

Figure 3: GeNSIT: (a) successive iterations of Alg. 1 for a given ensemble member, (b) plate diagram for every iteration, ensemble member. We propose two schemes: a Joint and a Disjoint (see App. B.3.1 for details). Contrary to the latter, the former passes table \(\) information to the loss \(\) (see - - - - in (b)). We perform an optimisation step in the intensity \(\) space and a sampling step in \(\) space, with associated complexities \(( J+IJ)\) and \((IJ)\). The \(\) arises by the well-known family of SIMs (3),(4) coupled with the HW-SDE (6). The \(\) sampling step generates discrete \(_{T}\)-constrained ODMs contrary to , which only operate on the continuous mean-field level \(\).

introducing the necessary machinery to achieve this. We wish to sample from the target measure \(\) evaluated as \((|,)\).

### Constrained Table Sampling

Let the table cells be indexed by \(=\{(i,j):1 i I,1 j J\}\) such that \(T(x)=T_{ij}\) is the table value of cell \(x=(i,j)\). All non-negative two-way contingency tables \(\) are assumed to be members of a discrete space \(\). The \(k\)-th basis operator \(_{k}:_{k}\{0,1\}^{I+J}\) is defined to be

\[_{k}(x)=_{i},0,,0, _{I+j},0)}_{I+J}.\] (9)

Hence, we can define summary statistic operators \(_{k}:^{I+J}\) as linear combinations of the basis operator, that is \(_{k}()=_{x_{k}}(x)_{k}(x)\). A collection of such summary statistic operators is abbreviated by \(}()\).

```
1:Inputs: evidence: \(\), \(\), \(\), funcs.: \(\), \(_{NN}\), \(_{HW}\), \(\), \(\), hyperparams.: \(N\), \(E\), \(\), \(\), \(\), \(\), \(\), \(\).
2:Outputs: \(^{(1:N)},^{(1:N)},^{(1:N)}, ^{(1:N)}\).
3:for each ensemble member \(e=1,,E\)do
4: Initialise \(^{(0)}\), \(^{(0)}\).
5:for each iteration \(n=1,,N\)do
6:\(^{(n)}_{NN}(;^{(n-1)})\), with \(\). \(\) Forward solving Neural Net.
7:\(^{(n)}(^{(n)},,, ,)\).
8:\(^{(n)}_{t}^{(n-1)}_{t}\;\;\;t=1,,\).
9:for each time step \(t=1,,-1\)do
10:\(^{(n)}_{t+1}_{HW}(^{(n)}_{t},^{(n)})\). \(\) Solving Harris-Wilson SDE.
11:\(^{(n)}^{(n)}_{}\).
12:\(^{(n)}_{}(^{(n)}, ^{(n)},)\) using (3) or (4). \(\) Computing SIM intensity.
13:if Joint sampling scheme is used then
14:\(L^{(n)}(^{(n)},^{(n-1)},^{(n)}\;;\;,)\).
15:else
16:\(L^{(n)}(^{(n)}\;;\;\{\},)\).
17: Compute \(_{}^{(n)}\).
18: Update \(^{(n)}\) using back-propagation. \(\) Updating Neural Net Weights with SGD.
19:if\(\) is not tractable then
20: Sample \(_{l}\) uniformly at random from \(\).
21: Find \(\{\}\) such that \(^{(n-1)}+_{l} 0\).
22:\(^{(n)}(^{(n-1)}+_{l}|^{(n)},_{T})\).
23:else
24:\(^{(n)}(\;|^{(n)},_{T})\). ```

**Algorithm 1** : **Generating Neural Spatial Interaction Tables. \((NE( J+IJ))\)**

Constraints on \(\) can be expressed as fixed summary statistics \(_{T}=\{_{1},,_{K}\}\), where each \(_{k}\) is a fixed evaluation of \(_{k}()\) with respect to the basis operator. For example, constraint \(\{_{+},_{+}\}\) can be expressed in terms of the basis operator (9) over the entire cell set \(\).

**Definition 3.1**.: Let \(_{T}=\{_{1},,_{K}\}\) be a set of constraints based on summary statistics operators \(}\). A table \(\) is \(_{T}\)_-admissible_ if and only if \(}()=\{_{k}\}_{k=1}^{K}_ {T}\).

The subspace of \(\) containing all \(_{T}\)-admissible \(I J\) contingency tables is \(_{}=\{:}( )=_{T}\}\). This space contains all discrete ODMs consistent with the aggregate summary statistics \(_{T}\). Our goal is to efficiently sample from a measure \(\) on \(_{}\) subject to arbitrary \(_{T}\).

The target distribution is tractable, i.e. the entire table can be sampled directly in closed-form, if and only if the universe of summary statistic constraints contains at most one table marginal (row or col. sum). This covers the cases where \(_{T}{}^{}(\{T_{++},_{ +},\{_{_{l}}|_{l},l \}\})(\{T_{++},_{+},\{_{+},\{ _{_{l}}|_{l},l\}\})\})\). Note that the unconstrained case of \(_{T}=\) is handled by the construction in (7). This facilitates sampling \(^{(1:N)}\) in closed-form and in parallel as shown in step 22 of Alg. 1. The most notable cases of tractable distributions are (see App. B.2.1):

\[|,T_{++} (T_{++},/_{++});\] (10) \[|,_{+} _{i=1}^{I}(T_{i+},/ _{i+});\] (11) \[|,_{+} _{j=1}^{J}(T_{+j},/ _{+j}).\] (12)

Each of the distributions above can assimilate cell constraints of the form \(_{T}=\{_{_{l}}|_{l} ,l\}\) without violating \(\)'s tractability by limiting the support of \(|,\). If the constraint set \(_{T}\) contains at least both marginals, that is \(_{T}{}^{}(\{_{+}, _{+},\{_{_{l}}|_{l} ,l\}\})_{T}{}^{}\), then the target distribution becomes Fisher's non-central multivariate hypergeometric :

\[^{I}T_{i+}{_{j=1}^{J}T_{+j}!}}{T_{++}{_{i,j=1}^{I,J}T _{i}!}}_{i,j=1}^{I,J}(_{++}}{_{i+} _{+j}})^{T_{ij}}.\] (13)

Direct sampling without rejection from this distribution for arbitrary \(I,J\) is infeasible .

#### 3.1.1 Markov Basis MCMC

Therefore, we devise an MCMC proposal on \(_{}\). Using a suite of greedy deterministic Algorithms , we can initialise our MCMC with a \(^{(0)}_{}\). By virtue of definition 3.2 we guarantee that no proposed moves modify the summary statistics in \(_{k}\) (Condition 1) and that there exists a path between any two tables such that any table member of the path is \(_{k}\)-admissible (Condition 2). The collection \(\) of \(K\) constraints generates \(K\) sets of Markov bases \(_{1},,_{K}\). Our proposal mechanism consists of the universe \(=_{k=1}^{K}_{k}\). See App. B.2.2 for info on Markov Bases.

**Definition 3.2**.: : A _Markov basis_\(_{k}\) is a set of moves \(_{1},,_{L}:\) satisfying:

1. \(_{k}(_{l})=\{\}\)\(\)\(1 l L=|_{k}|\) and
2. for any two \(_{k}\)-admissible \(,^{}\) there are \(_{l_{1}},,_{l_{A}}\) with \(_{l}_{ 0}\) such that \(^{}=+_{m=1}^{A}_{l}_{l_{m}}\) and \(+_{m=1}^{a}_{l}_{l_{m}} 0\) for \(1 a A\).

In the case of \(I J\) ODMs constrained by \(_{T}{}^{}\), \(\) consists of functions \(_{1},,_{L}\) such that \(\)\(x=(i_{1},j_{1}),x^{}=(i_{2},j_{2})\) with \(i_{1} i_{2},j_{1} j_{2}\),

\[_{l}(x)=&,j_{1})$ or $x=(i_{2},j_{2})$};\\ -&,j_{2})$ or $x=(i_{2},j_{1})$};\\ 0&.\] (14)

A Gibbs Markov Basis (GMB) sampler can now be constructed (see steps 20 - 22 in Alg. 1).

**Proposition 3.1**.: (Adapted from ): Let \(\) be a probability measure on \(_{}\). Given a Markov basis \(\) that satisfies 3.2, generate a Markov chain in \(_{}\) by sampling \(l\) uniformly at random from \(\{1,,L\}\). Let \(\). If the chain is at \(_{}\), determine \(()\) such that \(+_{l} 0\). Choose

\[()_{x:_{l}(x) 0} ((T(x)+_{l}(x)))^{-1}\]

and move to \(^{}=+_{l}\) for the choice of \(\). An aperiodic, reversible, connected Markov chain in \(_{}\) is constructed with stationary distribution proportional to \(()\). Proof is found in .

We note that when individual cells are fixed (\(_{_{l}}\)), the summary statistics operator is applied over \(_{l}\) instead of \(\). Therefore, the size of \(\) is reduced to comply with Definition 3.2. This may shorten the diameter of the Markov Chain's state space, leading to better mixing times .

[MISSING_PAGE_FAIL:8]

#### 4.2.1 Cambridge, UK

In the Cambridge dataset, the ground truth ODM is a \(69 13\) contingency table with \(33,704\) agents. Tab. 1 shows that reconstruction error (SRMSE) and the % of ground truth cells covered by the 99% high probability region of the ODM samples (CP) are significantly improved when operating in \(\) level using our Joint scheme compared to SIM-MCMCand SIM-NN, which only operate on the \(\) level. We also outperform the competitive SIT-MCMC approach , which can operate in the discrete ODM level, since in the limit of \(_{T}\) our method can move to high \(\) probability regions much faster than MCMC due to the optimisation of the SIM \(\) parameters. This effectively deflates the \(}\) estimator variance relative to the variance of the \(\) samples in MCMC. Only in the absence of rich \(_{T}\) data does SIT-MCMC outperform our method (total constrained case) as it limits the support of \(\) to \(^{2}\) which acts as regularisation. Letting the support of \(\) span the entire \(^{2}\) allows information carried by stronger \(\) constraints (such as \(_{+}\)) to permeate to the \(\) space much faster in GeNSIT compared to SIT-MCMC. The plethora of sources of uncertainty ranging from the HW-SDE (6) to the combinatorial nature of \(_{}\) suggest that GeNSIT is faster in reconstructing ground truth ODMs compared to a fully Bayesian approach such as SIT-MCMC.

Fig. 6 supports this claim by shedding light on the convergence rate of running mean estimates of the ground truth \(^{*}\). The Joint GeNSIT scheme converges to a mean \(\) estimate much earlier than SIT-MCMC across all \(\) regimes. Mean \(\) estimates are also improved in the Joint GeNSIT compared to SIT-MCMC in confined \(\) spaces (doubly, doubly and 10% cell, doubly and 20% cell constrained ODMs). Under the same \(\) regimes CP does not improve significantly as \(N\) grows large, which suggests that the variance of \(\) samples appears stable as early as \(N=10^{4}\). In the Disjoint GeNSIT the information encoded in larger \(_{T}\) is not propagated to the \(\) updates. As a result, no SRMSE or CP improvements are detected in the course of \(N\).

#### 4.2.2 Washington, DC, USA

We apply our method to the Washington dataset, where the ground truth ODM is a \(179 179\) contingency table with \(200,029\) agents. Tab. 2 reports the reconstruction error (SRMSE) and the % coverage of the ground truth cells (CP) in \(\) and \(\). Comparisons against SIM-MCMCand SIT-MCMCwere infeasible due to their high computational complexity (300 hours to obtain 500 samples on a 32-core NVIDIA GPU). Instead, we leveraged GMEL which operates only in the

Table 1: Ground truth \(^{*}\) validation metrics comparing our method against  in the \(\) and \(\) levels across constraint sets \(\) and \(=0.141\) (best) for the Cambridge dataset. On a \(\) basis the best metric in \(\),\(\) spaces is _emphasised_ for each of the two and **highlighted** between the two. Inference on the discrete table space offers lower SRMSE and higher CP compared to inference in the continuous space. On an ODM basis we obtain the best reconstruction error (SRMSE) and ground truth coverage (CP) in \(\)-space in all but the totally constrained ODM. This is due to SIT-MCMC forcing \(^{2}\) instead of \(^{2}\), which has a regularisation effect. In the absence of substantial \(\) data, this effect is more pronounced. See Tab. 6 (App. E.1) for full table across multiple \(\) regimes.

continuous \(\) space by learning a mapping between a large feature space \(\) and \(_{}\). Tab. 2 shows that we outperform both GMEL and SIM-NN in terms of reconstruction error and coverage.

## 5 Concluding Remarks

We introduced GeNSIT, an efficient framework for jointly sampling the discrete combinatorial space of agent trips (\(\)) subject to summary statistic data and its continuous mean-field limit \(\). We surmount the limitations of methods which operate strictly on \(\) space  and of methods that incur a large computational cost . We accomplish this by introducing the first framework operating on both \(\), \(\) that scales linearly with the number of origin-destination pairs \(IJ\). We offer enhanced reconstruction error and coverage of the ground truth ODMs in Cambridge, UK and Washington, DC. Although NNs require a much larger number of internal parameters to be calibrated relative to MCMC, their embedding of physics models regularises this parameter space and prevents over-fitting. A remaining open problem on this front is the assimilation of more complex \(\) structures in agent population synthesis and simulation (see App. B.2.3), since ground truth data is typically partially observed. Our work also relies on the SIM's assumptions about the agents' decision-making process, which in practise is unobserved. An examination of different agent utility models could benefit the applicability of our framework. In terms of our work's social impact, policy decisions made from ABMs of social systems could negatively affect individuals, necessitating expert review and ethics oversight.

   & Data & Target & \(\) SRMSE & \(\) SSI & \(\) 99\% CP \\  & \(\) & \(\) & \((^{(1:N)},^{*})\) & \((^{(1:N)},^{*})\) & \((^{(1:N)},^{*})\) \\  \([\)GMEL\(]\) & \(,\) & \(\) & \(2.43 0.15\) & \(0.38 0.02\) & \(5\% 1\%\) \\  \([\)SIM-NN\(]\) & & \(\) & \(2.47 0.00\) & \(\) & \(24\% 3\%\) \\   & \(\) & \(2.47 0.00\) & \(\) & \(19\% 3\%\) \\  & \(}\) & \(2.39 0.09\) & \(0.43 0.01\) & \(\) \\   & \(\) & \(2.45 0.00\) & \(0.50 0.00\) & \(2\% 0\%\) \\  & \(\) & \(\) & \(0.45 0.01\) & \(44\% 1\%\) \\  

Table 2: Ground truth \(^{*}\) validation metrics (mean \(\) std. for \(E=10\) ensemble size) comparing our method against  in the \(\) and \(\) levels for \(=\{T_{++},_{}\}\) and \(=0.141\) (best) for the Washington dataset. Arrow \(\) indicates higher values are better, and vice versa. We achieve the best error (SRMSE) and ground truth coverage (CP) overall (see **bold cells**). The observed data \(\) leveraged to train GeNSIT, SIM-NN is a small subset of the data required to train GMEL (\(\) is a column vector of \(\)). See Tab. 7 (App. E.2) for full table across multiple \(\) regimes.