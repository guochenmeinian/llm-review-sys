# The Utility of "Even if..." Semifactual Explanation to Optimise Positive Outcomes

Eoin M. Kenny

Massachusetts Institute of Technology

Cambridge, MA, U.S.A.

ekenny@mit.edu

Contributed Equally.

Weipeng Huang

Tencent Security Big Data Lab

Shenzhen, Guangdong Province, China

fuzzyhuang@tencent.com

Code available at: https://github.com/EoinKenny/Semifactual_Recourse_Generation

###### Abstract

When users receive either a positive or negative outcome from an automated system, Explainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., _"If you earn 2k more, we will accept your loan application"_). Here, we instead focus on _positive_ outcomes, and take the novel step of using XAI to optimise them (e.g., _"Even if you wish to half your down-payment, we will still accept your loan application"_). Explanations such as these that employ "even if..." reasoning, and do not cross a decision boundary, are known as semifactuals. To instantiate semifactuals in this context, we introduce the concept of _Gain_ (i.e., how much a user stands to benefit from the explanation), and consider the first causal formalisation of semifactuals. Tests on benchmark datasets show our algorithms are better at maximising gain compared to prior work, and that causality is important in the process. Most importantly however, a user study supports our main hypothesis by showing people find semifactual explanations more useful than counterfactuals when they receive the positive outcome of a loan acceptance.

## 1 Introduction

Explainable AI (XAI) is broadly categorised into factual  and contrastive explanations . Within contrastive XAI, despite being neglected in comparison to counterfactuals, semifactuals are a major, fundamental part of human explanation, and have long been studied in psychology , philosophy , and lately computer science . They take the form of _"Even if \(x\) happened, \(y\) would still be the outcome"_. Such reasoning has many potential uses as demonstrated by these prior works, but here we are focused on how semifactuals can help optimise positive outcomes for users, which (to the best of our knowledge) remains completely unexplored.

Our definition of counterfactuals is in line with Wachter et al. , where a test instance classified as \(c\) must be mutated to cross a decision boundary into class \(c^{}\). Likewise, as established in the literature , we define a semifactual as an instance classified as \(c\), which must be modified in such a way as to _not_ cross a decision boundary (and hence remain class \(c\)) . In recourse , "negative outcomes" (e.g., a loan rejection) are generally mutated to produce "positive outcomes" (e.g., a loan acceptance) for users using counterfactuals. In our setting, we are assuming there was initially a positive outcome, and we are trying to mutate features to produce an even better situation for users, and in doing so _not_ cross the boundary into the negative outcome (i.e., using semifactuals).

Historically, counterfactuals have had obvious applications in computer science, such as explaining how to have a bank loan accepted rather than rejected, but applications for semifactuals as lessclear. As such, the usage of semifactuals has often inadvertently defaulted to copying counterfactual research by also explaining negative outcomes (e.g., _"Even if you double your savings, your loan will still be rejected"_). However, such an application for semiffactual explanation perhaps has two main issues. Firstly, it is debatable if these explanations convey useful information , whilst a counterfactual explaining how to cross a decision boundary and have a loan accepted has obvious utility . Secondly, such explanations make the user's situation seem helpless , in that they cannot possibly have their loan accepted, which raises ethical concerns . However, our proposed framework can be used to not only overcome both of these issues, but actively _contribute_ to fairness.

Firstly, to try offer useful information for users, we flip the usual recourse problem and consider the user starting from a positive (rather than a negative) outcome. In this setting, consider a user that has had their loan accepted, but might prefer to make a smaller down-payment on a loan application. In this situation, our framework could present an explanation such as _"Even if you half your down-payment, your loan will still be accepted"_, which seems to be more useful than explaining negative outcomes (see Section 6). Secondly, because we are starting from a positive outcome, there is no danger of manipulating people into accepting a negative outcome, which guarantees fairness is this regard. Now, with regards to optimising fairness even further, note that banks are not motivated to share such explanations even though they may help people, because (for example) larger down-payments are associated with lower risk on their behalf . So, the usage of semifactuals in this application has clear potential to actively _encourage_ fairness and transparency. As an aside, it is worth noting that although the focus of this paper is on financial applications, this research has broad impact on any domain for which the optimisation of a positive outcome is beneficial. For instance, in medical applications, our framework could present explanations of the form _"Even if you half your dose of drug \(x\), you will still be at a low risk for disease \(y\)"_. This is once again important for optimising fairness because people are frequently over-prescribed medicine with adverse side-effects , but due to profit Big Pharma has no incentive to actively encourage this type of transparency. Similar usage of semifactuals have also been proposed in smart agriculture to combat climate change .

Our main contributions are: (1) the first explicit exploration of how to optimise positive outcomes with XAI, (2) the problem formulation for this which involved augmenting current semifactual research with the concept of _Gain_ (see Section 3.3), and (3) the premiere user test in the XAI literature for semifactuals, showing a clear application in which users find them more useful than counterfactuals.

## 2 Literature Review

When using contrastive explanation to explain loan acceptance decisions, to the best of our knowledge, this has only been explored by McGrath et al. . Specifically, they suggest _positive counterfactuals_, which show "by how much" a user had their loan accepted to help inform them when making future financial decisions. While this is interesting information, we show that users find semifactual explanations more useful in loan acceptance situations than positive counterfactuals (see Section 6).

Semifactual explanation is growing in popularity , Kenny & Keane  first explored the idea, but focused only on images.2 Artelt & Hammer  used diverse semifactuals to explain why an AI system refuses to make predictions due to having an unacceptably low certainty, but ignore how to explain either positive or negative outcomes. Lu et al.  explain spurious patterns with semifactuals using a human-in-the-loop framework in NLP. Zhao et al.  proposed a class-to-class variational encoder (C2C-VAR) with low computational cost that can generate semifactual images. Vats et al.  used generative models to produce semifactual image explanations for classifications of ulcers. Lastly, for model exploration, Xie et al.  sampled semifactual images with a joint Gaussian mixture model, and Dandl et al.  proposed deriving semifactual explanations from interpretable region descriptors. In contrast to all these approaches, we are showcasing how semifactuals can be used to optimise positive outcomes for users (notably in causal settings).

From a user perspective, many have discussed the urgent need for comparative tests with semifactuals , with Aryal & Keane  pointing to the _'paucity of user studies'_ in the area. However, the only such tests we are aware of are in the psychological literature over two decades ago . Taking to this challenge, we conduct the first such test directly comparing semifactuals to counterfactuals in the XAI literature (see Section 6).

Our research is related to algorithmic recourse  in that we are trying to ensure users are treated fairly by automated systems . In this area, Mothil et al.  explored counterfactual diversity, in that we should be offering users several explanations. In addition, counterfactual robustness has been examined , which proposes that generated explanations should be robust to distributional shifts. Lastly, causality has been argued as essential to providing plausible recourse . We see these three facets as being important to our problem setting, and instantiate them in our framework. There are other areas in recourse such as sequential decision making [16; 42], fairness , and privacy , but we leave their exploration within semifactual explanation for future work.

As an aside, the literature on sufficiency could be conflated with semifactual explanation, as it describes a set of "sufficient" features for a prediction which, in the presence of the other features mutating, mostly doesn't affect the outcome [17; 46; 57]. However the techniques offer no insights for how to generate a meaningful semifactual. More importantly though, if the sufficient features are the only actionable ones, then by definition we can't modify them to create a semifactual.

## 3 Semifactual Framework

In this section, we describe the basic definitions and assumptions for our semifactual framework to optimise positive outcomes for users, before formalising it under the concept of _Gain_ (i.e., how much a user stands to benefit from the explanation) in a causal setting, neither of which has been considered before. As an aside, we also show how the established concepts of plausibility, robustness, and diversity can be made fit into the objective to offer better explanations. Finally, we reflect on the theoretical properties of the framework.

### Definitions

Let us denote an individual \(\) with \(k\)_mutable_ features \(=\{X_{1},,X_{k}\}\). Given the individual \(\), a set of actions can be applied to \(\) where each action \(a()\) is also a \(k\)-dim vector. As in prior work , we apply \(a()\) and \(a\) exchangeably, since the individual \(\) will always be fixed. We explicitly exclude features that are either _immutable_ or _non-actionable_. Adopting Pearl's \(do()\) operator , an action can be defined as \(a()=do()\), or simply \(do()\), to force a hard

Figure 1: Semifactual Explanation to Optimise Positive Outcomes: An individual \(\) has their loan accepted, but there are several semifactual explanations which can help optimise their outcome. Our algorithm produces a set of semifactual explanations which _maximise_ the distance between \(\) and the final explanation \((,a;)\). This allows the largest _Gain_ to be achieved so that the user gets the maximum benefit. In contrast, counterfactual algorithms are not suitable because they are designed to target the shortest path across a decision boundary. In addition, the semifactuals are robust to distributional shifts by constraining an \(\)-neighborhood between them and the decision boundary. Note \(\) is the Structural Causal Model (SCM), see Section 3.

intervention of replacing \(\) by \(\) where \(\). It implies that, for each feature, \(X_{i}_{i}\) for the individual \(\). If the action \(do()\) imposes no change, \(=\) holds. We further denote a set of human-constrained actionable ranges \(=\{a()=do():\}\). Note that the actions have to be mutable and explicitly exclude any action which keeps the individual in the same position.

The non-causal semifactual interaction between \(\) and \(a()\) is defined by \(:\). That is, the individual \(\) taking action \(a()\) will lead to another representation \(\) representing that person's recourse. Now, a structural semifactual is defined which considers the dependence between the related features [18; 24]. We denote the structural causal model (SCM) by \(=(,_{U})\) where \(\) are a set of structural equations and \(_{U}\) is the distribution over the exogenous variables \(U\). Consider that in a causal graph, there is a set of causal parents for each feature \(x_{i}\), denoted by \(_{i}\). We denote the structural equations as \(=\{x_{i} g_{i}(_{i},U_{i}):i=1,,k\}\) where \(g_{i}()\) is a deterministic function that describes the causal relationship for \(x_{i}\), and depends on the exogenous variable \(U_{i}\) alongside the corresponding parent set \(_{i}\). Hence, \(\) induces a mapping \(:^{*}\) and its inverse mapping \(^{-1}:\). Let \(f g(x)=f(g(x))\) which can be extended to more functions. Hence, we specify the SCM-processed semifactual by \((,do();)\) to denote the transition between the states by taking a certain action through an SCM \(\), where

\[^{}=(,do();) ^{-1}(,do();)\;.\] (1)

If all features are _independently manipulable_, we have \(^{}==(,do(); )=(,do())\). Therefore, \((,do();)\) is a more generalised formulation which covers the non-causal case. Lastly, we assume a binary model that generates the score for the users is \(h\), where \(h:\{0,1\}\) by which we can simply consider that \(1\) means a positive outcome (e.g., a loan acceptance) and \(0\) is a negative outcome (e.g., a loan rejection). We set a lower threshold \(\) that separates the decision boundary. For the form defined above, \(=0.5\) is a reasonable threshold that fits all situations well.

### Framework

We define our semifactual framework as one centering on gain (\(G\)) that is weighted by plausibility (\(P\)), regularization in the form of diversity (\(R\)), and hard constraints in the form of robustness (\(H\)), indexed by \(j\). All of the components are parameterized with \(\) and a subset of suggestions \(\{a_{1},,a_{m}\}\). Letting \(f()\) be a function composed by gain and some weighting (i.e., plausibility for us), the causal semifactual framework is defined as

\[_{a_{1},,a_{m}} _{i=1}^{m}f(G(,a_{i}),P(,a_{i }))+ R(\{_{1},,_{m}\})\] s.t. \[_{i}=(,a_{i};),H_{j} (_{i}) 0, i,j\] (2)

where the regularisation and hard constraints can be multiple and indexed with \(i\) and \(j\), respectively. One may define a similar formulation for the non-causal case (see Section 4.2). We defer all details of the components until Section 3.4.

### Optimising Positive Outcomes with _Gain_

For the core of the objective we appeal to the notion of gain. Note that _gain_ is similar to the idea of _cost_ commonly used in recourse , but there are three crucial differences. First, we are trying to _maximise_ gain, rather than _minimise_ cost . Second, gain ideally considers the causal dependencies between features in its function, whilst cost typically only considers the user's action(s) . Third, gain is further subdivided into positive and negative polarities. To elaborate on this last point, take the example of a user who has their loan application for buying a new house accepted. In this situation, if they desired to spend more time away from work with family, they would experience _positive gain_ if they could work less hours per week and still have their loan accepted (see Figure 1). Conversely, if this person increased the number of hours they worked, they would experience _negative gain_. Notably, positive/negative gain is not necessarily connected to the model's probabilities (see \(a_{1}\) in Figure 1 moving away from the decision boundary). Similar to actionability constraints which can offer individualised recourse , what is positive/negative gain must be manually defined for each individual. As prior work on semifactuals simply maximised the \(L_{2}\) distance between a test instance and explanatory one to define good explanations [1; 29], we introduced the concept of gain to make them more meaningful in application.

More formally, we define the gain function by \(G:\). By denoting \(=(,a;)\), we decompose the function as follows:

\[G(,a)_{SF}(,)= _{SF}(,(,a; ))\] (3)

where \((,)\) is an oracle function that computes the payoff based on the vectorised difference between \(\) and \(\), i.e., \(:^{k}\) which is a symmetrical difference function between the two feature representations. The subscript of \(_{SF}\) denotes a semifactual. In interpretation, the gain function compares two states, (1) the original feature vector \(\), and (2) the SCM-processed end state \(\) which was led to through \(\) taking action \(a\).

Why is _Gain_ not necessarily equivalent to _Cost_?Formally, to enable the comparison, we write the cost function (denoted by \(C(,a)\)) as

\[C(,a)=-_{CF}(,( ,a))\] (4)

which builds on the fact that cost solely considers the feature change. Note that \(\) is equivalent to the notion \(\) in , what makes our approach different is the consideration of positive outcomes and gain. Our finding is that gain in semifactuals (SFs) is not necessarily equivalent to cost in counterfactuals (CFs) where the equivalence ignores the sign of both quantities, as formally stated as follows.

**Theorem 3.1**.: _Even if \(_{SF}(,)_{CF}(,)\), gain and cost are not necessarily equivalent ignoring the sign._

Proof.: Note that SCMs are also considered in counterfactual recourse . However, in this prior research SCMs are typically applied for enforcing hard plausibility constraints, not in the computation of a user's cost. In contrast, our gain function takes the SCM-processed semifactual \(^{}\) as an input. We employ proof by contradiction here. Assume that cost and gain are equivalent ignoring sign so that, without loss of generality,

\[|G(,a)|=|C(,a)| |(,( ,a;))|=|(, (,a))|\] \[(,(,a; ))=(,(,a))\] (5)

holds. However, SCMs can result in possibly more features being changed since some features could be others' causal parents and those causal children will change their values accordingly. By denoting \(=(,a)\) and \(^{}=(,a^{})\), we consider the general case as follows:

\[|(,)|-|(,^ {})|=_{i}|(,)_{i}|-_{i}|( ,^{})_{i}|\\ =_{\{i:_{i}=_{i}^{}\}\{i:_{i} _{i}^{}\}}|(,)_{i}|-|(,^{})_{i}|=0+_{\{i:_{i}_{i}^{}\}}| (,)_{i}|-|(,^{})_ {i}| 0\,,\] (6)

which contradicts with Equation (5). Thus, even if the oracle function for calculating the payoff is the same, gain and cost are still not necessarily equivalent. Also, the equality in Equation (6) holds when all features are independently manipulable or the changed features are independently manipulable of the remaining features, so that \((,a)=(,a;)\). The proof completes here. 

### Semifactual Components

Here, we detail how to incorporate the concepts of plausibility, robustness, and diversity into our framework for maximising gain, because they are agreed upon as important in the literature and useful for evaluation. While plausibility and diversity have been explored in semifactual explanation , robustness and causality (and indeed an objective balancing all together) have not, yet we argue and show that the subtleties of "even if..." thinking are perhaps better captured in a casual setting.

Plausible GainWe define plausibility here as explanations which are within distribution. For example, an explanation saying a person could earn less and still have their loan accepted should change their "debt-to-income ratio" feature also, or it will lie outside the data manifold. Prior work on semifactuals has only considered euclidean distance to training data as a heuristic for this , in contrast we posit (similar to the counterfactual literature ) that this is better approached with SCMs. Hence, we define the plausibility for \(\) taking the action \(a\) by \(P(,a)=(a=do()|)\) where \(\) is fixed for an individual and \(()\) is a density function. In our non-causal tests, we use the \(L_{2}\) norm to training data to approximate plausibility (i.e., being in distribution, similar to ). However, this issue of plausibility is naturally taken care of in our causal tests thanks to the SCM ensuring plausible feature mutations, so we don't explicitly consider plausibility there going forward.

Robust GainContinuing with the example of a person who has a loan accepted to buy a house, the semifactual should sometimes be robust to distribution shifts. For example, if the person uses the semifactual explanation to triple their loan amount (recall Figure 1), they will likely need upwards of six months to locate a new house during which the semifactual should hold if the person e.g. gets an additional credit card. Hence, we define our semifactual robustness such that while taking action \(a\), any close neighbor of the generated semifactual \((,a;)\) can still receive a positive outcome. The \(\)-neighborhood of \(\) centering around an individual \(\) is

\[()=\{=(,a;):  a,(,)\}\] (7)

which covers all neighbors that can be reached from \(\) by taking an _actionable_ feature change \(a\) through the SCM \(\). By definition, \(\) is also a neighbor of itself since \(()\) holds given \((,)=0\). Let us represent \(((,a;))\) by \(_{s}(,a)\) for simplicity. Given the predictive model \(h()\) and an individual \(\), an action \(a\) is robust for individual \(\) if \(h()>,_{s}(,a)\), which is equivalent to \(_{_{s}(,a)}h()->0\). For instance, \(=0.5\) works for a binary model case. We hence denote the term related to the robustness by

\[H(,a)=_{_{s}(,a)}h()-\,,\] (8)

which will be useful for constructing the final objective.

Diverse GainIt is generally preferred to offer a number of suggested actions \(\{a_{1},,a_{m}\}\), rather than a single one . Like prior work in counterfactuals, we define diversity as the average pair-wise distance among a set of entities . We reuse the distance function \(\) and define the diversity objective within a set of SFs \(\{_{i}\}_{i=1}^{m}^{m}\) as

\[R(\{_{i}\}_{i=1}^{m})=_{i=1}^{m} _{j>i}^{m}L_{2}(_{i},_{j})&m>1\\ 0&m=1\] (9)

which represents a pairwise mean distance among the set of data points, based on the \(L_{2}\) norm. One may accommodate \(m=1\) for the case when only a single semifactual is desired.

#### 3.4.1 Semifactual Objective

The final objective may be constructed as follows.

**Definition 3.2** (Semifactual Objective).: We consider a simple composition multiplication function for \(f()\). Considering gain, plausibility, robustness, and diversity, the semifactual objective function is:

\[ _{i=1}^{m}P(,a_{i})G(,a_{i})+  R(\{(,a_{i};)\}_{i=1}^{m})\] (10) s.t. \[ i=1,,m:a_{i},H(,a_{i})>\,.\]

In optimisation , an adversarial interpretation from the perspective of a two-player zero sum game can further simplify Equation (10) to

\[_{_{1},,_{m} 0}\ _{a_{1},,a_{m}}_{i=1}^{m}P(,a _{i})G(,a_{i})+_{i}H(,a_{i})+ R(\{ (,a_{i};)\}_{i=1}^{m})\,,\] (11)

where \(H(,a)\) is the Lagrangian. The primal player tries to maximise the plausibility-weighted gain and diversity, with regard to \(a\), whilst the dual player tries to minimise regarding a set of \(\).

Since there are \(m\) suggestions, the constraints for robustness will be \(m\) times. Observing the objective, the robustness is a hard constraint, whilst the diversity can be regarded as regularisation. \(P\) can be seen as a scaling factor for \(G\) which helps to guarantee that high expected gain is only possible alongside high plausibility, simply adding them misses this special property.

### Properties of the Framework

Effective Solution Space.We discuss the set of meaningful solutions here and the result validates the re-formulation [i.e., Equation (11)] of the semifactual framework [i.e., Equation (10)]. First, we depict the lemma.

**Lemma 3.3**.: _Assume that the limit of the gain function and diversity term are finite. Also, assume that \(^{+}\{a:G(,a) 0\}\) is non-empty for an individual \(\). The semifactual objective \( 0\) when \( i=1,,m,a_{i}^{+}:H(,a_{i}) 0\), otherwise \(=-\)._

See Section A.1 for the proof. We can summarise that the action set which is able to provide the positive payoff can be defined by the named effective solution space for \(\): \(=\{a:H(,a) 0,G(,a)>0\}\). Hence, repeated suggestions will be produced when the number of actions in this space is smaller than the required \(m\). Otherwise, the solution will provide more versatile options. There are no suggestions to achieve an effective semifactual(s) (i.e., with positive gain) if this solution space is empty. However, similar situations exist for counterfactuals when they are also impossible to generate, assuming a similar set of actionable constraints are defined.

## 4 Implementation Details

We now introduce our methods to solve Equation (11), henceforth called Semifactual-recourse GENeration (S-GEN), for both causal and non-causal domains. In the following paragraphs, we use \(\) to denote an empirical approximation of \(G\), and likewise for \(P\), \(H\), \(R\), and \(\).

### Causal Case

Assuming the presence of a differentiable classifier \(h()\) and SCM \(\), (recall the latter guarantees plausibility), let \(_{i}()=\{(): _{s}(,a_{i})\}\) be the probability distribution over the \(\)-neighborhood of \((,a_{i};)\). Also, let \(_{i}\) represent a finite subset of \(_{s}(,a_{i})\) sampled according to \(_{i}()\). Our objective is:

\[_{a_{1},,a_{m}}_{_{1},,_{m}} _{i=1}^{m}-_{i}(h((,a_{i};),h())-_{i}|} _{_{i}_{i}}_{i}(h( _{i}),h()).\] \[. &+(,a_{i})( ,a_{i})+(\{(,a_{i};)\} _{i=1}^{m})\\ & i=1,,m:a_{i}, _{i}>0.\] (12)

where \(\) is the binary cross entropy loss. For robustness, we used Monte Carlo (MC) sampling with an epsilon \(\) robust hypersphere, and if either the instance or sampling return a _negative outcome_ with \(h()\), we use the prior optimisation step as the solution. For diversity, \(m\) is set to the number of actionable feature sets, and a solution is obtained for each. We utilise the causal recourse approach of Karimi et al.  for solving the maximin. The actionable bounds are clipped each iteration, and \(\) is iteratively decreased to put more emphasis on gain over time (see Algorithm 2).

### Non-Causal Case

For the non-causal case, we use a genetic algorithm [53; 58] which only assumes a binary predictive model \(h()\). This approach follows the standard design for genetic algorithms, with some minor alterations specifically for semifactual generation, see Appendix D for the pseudocode. Next we present the fitness function which optimises our objective.

#### 4.2.1 Fitness Function

For gain, the average distance between an individual \(\) and each semifactual \((,a)\) is measured as \((,a)=\|(,a)-\|_{2}\). For robustness, we relax it to two constraints: \(H_{p}\) is the probabilistic robustness for the neighbor points where the generated semifactuals for a query are randomly perturbed using MC simulation to make sure the surrounding neighborhood is robust, and \(H_{a}\) the absolute robustness for the individual \(\) (more detail in Section A.2). For the first constraint, a score of \(_{p}(,a)=_{i=1}^{n}\{h() =h(_{i})\}\) where \((_{s}(,a))\), is returned. Forthe second constraint, a score of \(_{a}(,a)=\{h()=h(,a)\}\) is returned. Hence, the solution is rewarded for (1) the neighborhood samples, and (2) the semifactuals themselves being classified as \(h()\). For plausibility, we take from prior work and directly use the training data . Specifically, considering the training data set \(\), we define the notion of plausibility using the distance of each semifactual generated to the nearest training data point. As the term must be maximised, we use a function which is monotonically decreasing with respect to the distance with \(P(,a)_{}(,a)=\{1/(_{ }\|(,a)-\|_{2}^{2}+_{p})\}\) where \(_{p}\) is to account for when a perfect match to the semifactual exists in the training data (thus the division is undefined), and \(_{}(,a)\) is an empirical approximation (based on \(\)) of the plausibility. Lastly, for diversity , we take the mean distance between all \(m\) generated semifactuals with \((\{(,a_{i})\}_{i=1}^{m})\) which precisely follows Equation (9). This objective collapses to 0 when \(m=1\).

Certain objectives need to be weighted individually based on the problem. For example, explanations which can be acted upon immediately perhaps don't need robustness. Notably, the multiplier \(\) in Equation (11) is split to \(_{p}\) and \(_{a}\), for \(H_{p}\) and \(H_{a}\) respectively. In this work, we treat them as hyperparameters. Also, they are used alongside \(\) to balance the objectives. Since \(\) and \(\) are selected as hyperparameters, they are removed under the min operator. Finally, the objective (fitness) function is defined as:

\[_{a_{1},,a_{m}^{+}}_{i=1}^{m}_{ }(,a_{i})(,a_{i})+_{p}_{ p}(,a_{i})+_{s}_{a}(,a_{i})+( \{(,a_{i};)\}_{i=1}^{m})\,.\]

We selected the hyperparameters via a grid search, see Appendix C. Crucially, we also weight the fitness function output by \(_{p}(,a)\) to encourage solutions with more semifactuals (see Algorithm 1).

## 5 Experiments & Results

Here we test S-GEN in both causal and non-causal settings. We show the effectiveness of our method in optimising a user's positive outcome compared to baselines and open source our code (see Appendix E). The actionability constraints are detailed in Appendix B. Baselines were modified to be appropriately compared, most importantly, we stopped counterfactual techniques before they crossed a decision boundary (thus generating semifactuals), and modified semifactual techniques to work on tabular data, Appendix G details the peripheral modifications.

In the non-causal setting, we consider three datasets, Loan Application , German Credit , and BCSC . All categorical variables are one hot encoded. Three models were used, a decision tree, logistic regression, and naive bayes, each with 30 random test data point explanation samples gotten by varying the random seed. Note that because the range varied on each dataset, the results were normalised and averaged for each, but Appendix F details each individual dataset for completeness. For baselines, we modify three techniques, DiCE by Mothilat et al.  (henceforth DiCE*), PIECE by Kenny & Keane  (henceforth PIECE*), and Diverse semifactual Explanations of Reject by Arttelt & Hammer  (henceforth DSER*). Plausibility is measured as the distance between a generated semifactual(s) and the nearest training example; thus, the smaller the better. Robustness is measured by MC sampling \(n=100\) single feature perturbations of each semifactual \(_{i}\), predicting their class, and returning a float between 0-1 of the success rate as described in Section 4.2.1.

In the causal setting, the Adult  and COMPAS  datasets are considered. The SCMs from Nabi & Shpitser  were used, and the structural equations from Dominguez et al. . All categorical features are treated as real-valued. We use the pre-trained MLP classifiers from Dominguez et al.  and take 30 averaged samples from 5 random seeds. As baselines we modify the technique of Karimi et al.  [henceforth Karimi et al.(2021)*], and Dominguez et al.  [henceforth Dominguez et al.(2022)*], the latter optimises with robustness in mind. We optimise the relevant techniques to be robust in an \(=0.1\) hypersphere, and the C&W adversarial attack by Carlini & Wagner  measures robustness by checking if the nearest adversarial attack is outside this radius.

For all tests, the main metric of concern is gain, that is, the mean distance between a query and its generated semifactual(s), the larger this number, the better. Diversity is also measured for all tests as the mean distance between all \(m\) generated semifactuals for an individual \(\), the higher the number, the better. To be in line with prior art, the \(L_{2}\) norm is used in non-causal tests , and the \(L_{1}\) in causal . Note for causal tests the SCM guarantees plausibility so this metric is not reported.

### Non-Causal Results

Our purpose here is to show that current methods are insufficient to meet the basic requirements for semifactual explanation discussed in Section 3.4. Specifically, a technique needs to optimise gain, while remaining plausible, robust, and offering diverse explanations.

Observing the average normalised results across all datasets (note robustness was not normalised since it is already 0-1 range), Figure 2 shows that S-GEN performed the best on all metrics for all values of \(m\) (1-10). The results demonstrate that traditional counterfactual approaches (DiCE*) are not suitable to achieve optimal gain, due to them focusing on minimising cost. Moreover, methods built for semifactual generation specifically (i.e., DSER* & PIECE*) that _do_ actually maximise gain somewhat, still fail to match the results of S-GEN. This shows that S-GEN is superior to existing semifactual methods (and popular counterfactual approaches appropriately modified) for maximising a user's gain in positive outcomes. Moreover, it does so while maintaining superior plausibility, robustness, and diversity in all tests.

### Causal Results

We evaluate our algorithm in a causal setting where the SCMs and structural equations are known. The primary purpose of this test is to demonstrate that the semantics of semifactual "even if" thinking is better captured in a causal setting due to dependencies being taken into account when calculating a person's gain. With regard to diversity, we fix \(m\) to the maximum number of feature sets available from the actionable features (so only one \(m\) value is tested).

Figures 2(a) and 2(b) show the initial gain achieved by a person after taking a certain action (i.e., the _Action Gain_), and how this gain transforms after considering the causal relationship between features (i.e., the _Causal Gain_). Firstly, the total gain achieved by S-GEN is much larger than the baselines in both datasets and hence consistent with our non-causal tests. More importantly however, the change in gain a person achieves after considering the causal relations in the adult dataset is significantly higher both in significance testing and effect size (\(0.055 0.001\) v. \(0.063 0.001\); t-test \(p<0.02\); Cohen's \(d=2.24\)), showing it is beneficial to consider causality when calculating a person's gain. The results of diversity put S-GEN first also (S-GEN \(=0.84 0.09\) v. Karimi \(=0.43 0.03\) v. Dominguez \(=0.34 0.03\)). In robustness, both S-GEN and Dominguez et al. (2022)* did reasonably well (S-GEN \(=87\%\) success v. Dominguez \(=54\%\) success), but Karimi et al. (2021)* did not (\(7.2\%\) success), likely due to the latter not being designed for this.

## 6 User Evaluation

The primary motivation behind this work is the hypothesis that semifactual explanation would be preferred by users over counterfactuals in positive outcome settings. To test this assumption, we design the first user test in XAI directly comparing the two. Specifically, we show users three materials in which a person has a bank loan accepted, and three in which they don't. Users were then shown both explanation types for each material, and asked to rate on a scale from 1-5 how useful each were. So, the study was a within-subjects design, and the condition was the explanation type. Note that although we are studying the effect of the explanation type on loan acceptance, the

Figure 2: Results: The ability of S-GEN to create semifactuals is compared to DiCE*, DSER*, and PIECE*. Overall, S-GEN does the best, achieving significantly better results to all baselines in all tests. Note we normalised all results before averaging because each dataset has different scaling. Standard error bars are shown.

loan rejection scenarios were also included to balance people's view of the problem setting, and as attention checks to verify that users were engaging with the materials and varying their scores accordingly. For analysis, each user's scores for counterfactuals and semifactuals were averaged in both loan acceptance and rejection materials into four decimal scores per user, thus allowing us to analyse the discrete Likert scores with t-tests . As is a popular approach , we don't explicitly define what "useful" means to users, but rather let them use their own natural interpretation, as the results returned were reasonably consistent across individuals, they appear to have converged on an common interpretation of this word. The null hypothesis is that people will find both explanation types not significantly different in loan acceptance. The alternative is that people will find semifactuals significantly more useful in loan acceptance.

A power analysis  of two dependent means with an effect size \(dz=0.8\), alpha \(=0.05\), and power (\(1-\) err prob)=0.9 informed a sample of 15 was appropriate for t-tests. Users were gathered from Prolific.com, 8 males, 7 females, aged 18+, native English speakers, and from the U.S. People were paid $12/hr, which totalled $35. The semifactuals were generated with S-GEN, and the counterfactuals with DiCE , notably these are equivalent to _positive counterfactuals_ by McGrath et al.  for explaining loan acceptance situations. The study obtained IRB approval from MIT.

All users engaged and changed their ratings significantly depending on whether a loan was accepted or rejected, so none were excluded. Figure 2(c) shows users find semifactuals significantly more useful in loan acceptance (S-GEN=3.60\(\)0.27 v. DiCE=2.33\(\)0.34; \(p<.005\)) compared to rejections when counterfactuals are preferred (S-GEN=2.6\(\)0.32 v. DiCE=4.53\(\)0.17; \(p<.0001\)). Hence we reject the null and lend credible evidence that semifactuals are more useful to explain positive outcomes.

## 7 Discussion

Although much XAI work has explored how to explain positive outcomes, to the best of our knowledge, no consideration has been given towards explaining how to _optimise_ them. Here, we have taken the novel step of exploring this, and showed how semifactuals are especially suited for the purpose. This required building on prior work in semifactuals by (1) introducing the concept of _Gain_, (2) re-framing them in a causal setting, and (3) conducting their first user test in XAI. Perhaps the notable limitation of our work is that although we have shown people do perceive semifactuals as being more useful in positive outcomes, we have not demonstrated this quantitatively, notably because of the difficulties acquiring an appropriate user base alongside the ethical considerations of such a study. Moreover, considering a casual formulation of semifactuals requires an SCM, which is not always realistic, but we have provided a non-causal algorithm for these situations. In future work, it would be interesting to formalise the utility of semifactuals for optimising positive outcomes in other domains such as robotics, which likely requires other considerations.

Figure 3: Causal Experiment & User Study Results: (a/b) show the gain achieved by all methods both before and after considering the causal dependencies. Firstly, note that S-GEN achieves significantly more gain than the alternatively proposed approaches. Most importantly however, (a) shows there is significantly more gain achieved on the Adult data by S-GEN after taking causal dependencies into account, showing the importance of a causal formalisation. (c) Shows the user study results, where people perceive semifactual explanation as being significantly more useful than counterfactuals in the positive outcome of having a loan accepted. Standard error bars are shown.