# Nonlinear dynamics of localization in

neural receptive fields

 Leon Lufkin

Yale University

leon.lufkin@yale.edu

&Andrew Saxe

Gatsby Unit & SWC, UCL

a.saxe@ucl.ac.uk

&Erin Grant

Gatsby Unit & SWC, UCL

erin.grant@ucl.ac.uk

###### Abstract

Localized receptive fields--neurons that are selective for certain contiguous spatiotemporal features of their input--populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints--a feed-forward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits.1

## 1 Introduction

A striking feature of peripheral responses in the animal nervous system is _localization_--that is, the linear receptive fields of simple-cell neurons often respond to contiguous regions much smaller than their full input domain. In vision, retinal ganglion cells approximate localized center-surround filters that tile the input , and simple cells downstream in primary visual cortex have localized filters that are selective for spatial frequency and orientation . In primary somatosensory cortex, neurons respond to stimulation of restricted regions of skin  and in primary auditory cortex, spatiotemporal receptive fields are typically localized in both time and frequency domains ; see Fig. 1 (left).

By contrast, artificial learning systems do not always learn localized filters. Principal component analysis tends to fit weights that span the entire input signal, as do unregularized autoencoder neural network architectures and restricted Boltzmann machines . This difference has prompted the search for artificial learning models that can learn localized receptive fields from naturalistic stimuli, the most notable of which are sparse coding  and independent component analysis . Sparse coding, ICA, and related compression methods that produce localized receptive fields from naturalistic data share a top-down approach--they find an efficient representation of the input signal by optimizing an explicit sparsity criterion, or an independence criterion that necessitates sparsity in a critically parameterized regime .

Though sparsity is appealing as a potentially unifying explanation for localization, localization also emerges naturally in networks trained to perform classification tasks without any explicit sparsity regularization [14; 15; 16; 17; 18]; see Fig. 1 (center) for an example. Ingrosso and Goldt  distilled such examples of emergent localization by demonstrating that localized receptive fields emerge in simple feedforward neural networks trained on a data model with properties meant to approximate natural visual input, in particular, locality structure (statistical independence of non-collocated dimensions) and non-Gaussianity (higher-order cumulants are non-null). In simulations, Ingrosso and Goldt  tie the dynamical emergence of localization to increased tuning to higher-order statistics of the input, and demonstrate that even a single neuron is sufficient to learn a localized receptive field in this setting.

In this work, we build on the demonstration of Ingrosso and Goldt  with the aim of describing the mechanisms behind the emergence of a localized receptive field in this minimal setting. The higher-order input statistics that drive localization are challenging to analyze with existing tools that exploit implied Gaussianity . By separating two stages of learning, we are able to derive equations for the effective early-time learning dynamics of the single neuron model that learns a localized receptive field from idealized naturalistic data. Our analytical model identifies a concise description of the higher-order statistics that drive emergence, and we validate both positive and negative predictions of this analytical model via simulations with many neurons; see Fig. 1 (right). These findings suggest an alternative path to account for the ubiquity of localization in early neural responses as resulting from the interaction of the nonlinear dynamics of learning in neural circuits and naturalistic data with higher-order statistical structure, rather than an explicit efficiency criterion.

## 2 Modeling approach

We extend the setting of Ingrosso and Goldt , a minimal example of a neural network that learns localized receptive fields from idealized naturalistic data. We analyze the dynamics of learning in this setting in Section 3 and validate our analytical model with simulations in Section 4.

### Neural network architecture and learning algorithm

We consider a two-layer feedforward neural network with nonlinear activation and scalar output. While simple, this architecture is highly expressive, capable of approximating arbitrary integrable univariate functions with appropriate scaling [12; 13], and exhibits rich feature learning dynamics that underlie the performance of models at scale , making this architecture the ongoing subject of theoretical neural network analyses [17; 11; 18]. We denote a two-layer network with \(N\)-dimensional input, \(M\) hidden units, and one-dimensional scalar output as

Figure 1: **(Left)** Localization in spatial receptive fields (RFs) measured from non-human primate (NHP) primary visual cortex [13, Fig. 2] and in spatiotemporal RFs measured from NHP [11, Fig. 2] and ferret [14, Fig. 2] primary auditory cortex. **(Center)** Half-slice of the localized first-layer kernels of AlexNet trained for ImageNet classification . **(Right)** Localized receptive fields learned from the task of Section 2.3 in 2-D using ICA  and the soft committee machine (SCM; M1 with fixed second-layer weights) of Section 2.1. _Localization—spatial and/or temporal selectivity—appears across settings, as measured by response maximization in biological systems (left) and by inspecting linear filters in artificial systems (center, right)._

**Model 1** (_many-neuron architecture_).

(M1) \[()=b^{(2)}+_{m=1}^{M}w_{m}^{(2)}(b_{m}^{(1)}+( _{m}^{(1)},))\]

where \(:\) is a pointwise nonlinearity such as the rectified linear unit (ReLU) or sigmoid function, \(_{m}^{(1)}^{N}\) and \(w_{m}^{(2)}\) are learnable weights, \(b_{m}^{(1)},b^{(2)}\) are learnable bias terms, and \(,\) denotes the standard Euclidean inner (dot) product on \(^{N}\). When the second-layer parameters are fixed, this model is known as a _soft-committee machine_, which  notes learns less noisy receptive fields but exhibits similar localization behavior. The many-neuron architecture in M1 is the focus of our **simulations** (Section 4), but the dynamics of this model are too complex to analyze directly, even for the idealized naturalistic data model considered here. In order to derive **analytical** results (Section 3), we consider the simplest neural network exhibiting the desired localization phenomenon, a single hidden neuron without bias and with rectified linear unit activation, written as

**Model 2** (_single-neuron architecture_).

(M2) \[()=(, )\]

where \((x)=(x,0)\), applied pointwise to vectorial input. As Ingrosso and Goldt  demonstrate, the localized receptive fields learned by the many- and single-neuron models defined in M1 and M2 are qualitatively similar up to spatial translation, which permits us to generalize insights from analyzing the learning dynamics of the single-neuron M2 to the many-neuron M1. For simulations, we initialize the weights and biases as independent draws from an isotropic Gaussian distribution with scaled variance, and train with batch gradient descent with a fixed learning rate on the mean-squared error (MSE) evaluated on input-output pairs from the task; see Section 2.3 for task sampling procedures.

### Stimulus properties

The data model of Ingrosso and Goldt  can be shown to satisfy three conditions that enable the analysis we give in Section 3. We consider several other data models that share the below properties but differ in generative mechanism in order to probe the effect of these properties on localization. In particular, we consider data \(\) sampled from distributions \(p\) on \(^{N}\) satisfying the following:

**Stimulus properties 1-3** (_idealization of natural images_).

(S1) (Positional) weak dependence: for any fixed \((0,1)\), as \(N\),

\[(N)_{A,B^{(1-) N}}|\,(X_{1} A,X_{> N} B)-(X_{1}  A)(X_{> N} B)| 0\,,\] (S2)

Translation invariance: \(p(=)=p(=)\) for all \(^{N}\), where \(\) is the circular shift operator, and

Sign symmetry: \(p(=)=p(=-)\) for all \(^{N}\).

Properties S1 and S2 are defining characteristics of natural image data . Property S3 can also be seen to hold for natural images after centering and is convenient analytically because it implies that \([]=0\). Property S1 assumes that \(p\) is implicitly parameterized by \(N\) in order to state that the statistical dependence between entries of \(\) vanishes as their separation increases.2

We denote the covariance of \(\) by \([]\), the square of principal-diagonal entries (the variance of each entry of \(\)) by \(^{2}\), and the \(i\)-th row by \(_{i}\). Weak dependence (S1) implies that entries far from the principal diagonal of \(\) will be 0, while translation-invariance (S2) implies that \(\) is circulant (_i.e._, entries along each diagonal are equal) and thus identifiable by a single row; see Fig. 2 (center).

### Lengthscale discrimination task

Ingrosso and Goldt  develop a minimal task for which localization emerges in a feedforward neural network: binary discrimination between inputs from two distributions that differ in the lengthscale of the correlations between their entries. This lengthscale discrimination task can be seen as a pretext task for self-supervised learning  of representations [_cf._ unsupervised: 11, 12]. More precisely, we generate data \((,Y)\) for supervised training according to

\[ Y=y p(;_{y})\;,\] (1)

where \(p\) is to be defined, \(_{y}\) are distinct covariance matrices for each \(y\), and we sample \(Y\) uniformly among a set of increasing _lengthscale correlation classes_\(y\{0,1,\}\), which correspond to the strength of correlation between distant positions. For instance, in the case of two classes (\(y=0,1\)), we take \(_{0}\) to be closer to \(^{2}_{N}\) than \(_{1}\), where \(_{N}\) is the \(N N\) identity matrix and \(\) is a fixed value. This construction isolates, via distinct covariance matrices per class, the second-order statistics, which we will see below enter into the learning dynamics separately from other properties of \(p()\), including, most critically, the implied marginal distributions, \(p(X_{i})\).

Ising.The first distribution we consider is the one-dimensional Ising model. It is of interest as a distribution that satisfies S1 to S3 with marginals \(p(X_{i})\) with extreme support on \(\{ 1\}\), making it the distribution that promotes localization most strongly, as we will see in Section 3. In the absence of an external field, the Ising distribution is

\[p_{}(=)=p_{}(X_{1}=x_{1}, ,X_{N}=x_{N})=e^{-_{i=1}^{N}Jx_{i}x_{i+1}}/,\] (2)

where \(J\) is a chosen pairwise interaction strength, \(\) is the normalizing constant, and we enforce a periodic boundary constraint via \(x_{N+1} x_{1}\). As \(J\) increases, the lengthscale of the correlations in \(\) also increases. For simulations, we sample from \(p_{}\) using a Gibbs sampler . Discrimination tasks in the simulations in Section 4 use \(J_{1}=0.7\) (for \(y=1\)) and \(J_{0}=0.3\) (for \(y=0\)).

Nlgp\((g)\).We also consider the data model used in Ingrosso and Goldt , the nonlinear Gaussian process (NLGP), which enables one to interpolate between distributions that do and do not yield localization via a single parameter, \(g\). A sample \( Y=y\) from the NLGP is constructed by first sampling a Gaussian \( Y=y(0,_{y})\) and then transforming it via

\[X_{i}(gZ_{i})/(g) 1 i N,\] (3)

where \(\) is the Gauss error function, \(\) is a normalization constant to ensure that the variances of \(X_{i}\) and \(Z_{i}\) are the same, and \(_{y}\) is a covariance matrix for \(\), where we use \((_{y})_{ij}=(-(i-j)^{2}/^{2})\) for a lengthscale parameter \(\). If \(g 0\) (where localization is _not_ observed), \(gZ_{i}\) will tend to lie in the linear regime of \(\), so \(Z_{i}\) will be untransformed, _i.e._, \(\) is Gaussian. However, as \(g\) (where localization _is_ observed), \(gZ_{i}\) will tend to saturate \(\), so \(X_{i}\) will have support on \(\{ 1\}\).

Figure 2: From left: Long- and short-lengthscale samples \(\), covariances \(\) for one lengthscale, and marginals \(p(X_{i})\) for the data models described in Section 2.3: Ising (with \(J=1.2,0.3\) for left, right samples), the nonlinear Gaussian process , and the controllable kurtosis model, \(\) (with \(=5,1\) for left, right samples). _Each model generates samples centered about zero and with covariances that can be constrained to be similar, but with differing higher-order statistics, as can be seen from the dimension-wise marginals._\((k)\).The final family we consider is chosen to give us flexibility over the kurtosis \(\) of the marginals \(p(X_{i})\). In the Ising model, the _excess_ kurtosis (\(-3\)) of the marginals is fixed at \(-2\), while in \((g)\), it varies from \(-2\) to \(0\). This family allows us to vary the excess kurtosis from negative through positive values. We sample \( Y=y\) from this family via inverse transform sampling to vary the marginals while enforcing dependence via Gaussian copulas. More concretely, we sample \( Y=y(0,_{y})\) and then transform it via

\[X_{i} f^{-1}((Z_{i}/))/, 1 i  N,\] (4)

where \(\) is the standard deviation of \(Z_{i}\), \(\) is the standard Gaussian cumulative distribution function (CDF), \(f\) is the CDF of the desired marginal distribution for \(X_{i}\), and \(\) is a normalization constant, which we compute numerically. We define \(_{y}\) as for \(\). We choose \(f\) to be the generalized _algebraic sigmoid_ function (see Appendix A.2) for \(k>0\) to make use of its tractable inverse, simplifying the procedure in Eq. (4). We denote the corresponding distribution by \((k)\). Though we are able to continuously vary excess kurtosis, we lack an explicit form; however, numerical computation shows that for \(k 5.8\), excess kurtosis is positive, while for \(k 5.9\), it is negative.

## 3 Theoretical results

We derive an analytical model for the localization dynamics of the single-neuron architecture in M2. This result establishes necessary and sufficient conditions for localization under Properties S1 to S3 for the minimal case of a binary response, _i.e._, \(y=0,1\). The conditions for localization in the single-neuron architecture in M2 are demonstrated in Section 4 to also hold empirically for the many-neuron architecture in M1. Further, we use this model to derive a negative prediction about localization, that the architectures in M1 and M2 fail to learn a localized receptive field on elliptical distributions despite their non-Gaussian--in particular, significantly positive kurtosis--statistics [_cf._ positive kurtosis as an objective or diagnostic for localization, HO00; IG22].

### An analytical model for the dynamics of localization in a single neuron

Previous approaches to obtain analytical dynamics in the architectures in M1 and M2 have studied the gradient flow under the assumption that the preactivation \(,\) is approximately Gaussian , but this assumption fails to capture the propagation of higher-order statistics through a neural network that promotes localization . Happily, the idealized visual input setting set out in S1 to S3 permits us some simplification. In particular, the translation-invariance of the data \(\) under S2 and the architecture of M2 allow us to work with the marginal distributions of each input dimension, \(X_{i}\) rather than the full joint distribution of \(\).

We now give the analytical simplifications that allow us to derive an analytical model for the localization dynamics of the single neuron architecture in M2, namely two assumptions on \( X_{i}\) for all \(i\{1,,N\}\) as a well as a mild condition on the weights that is satisfied at initialization. These are, where \(_{i}^{y}\) to denotes the \(i\)-th row of \(_{y}\):

**Analytical simplifications 1-3** (_early-time, limiting dynamics_).

* \([ X_{i}=x_{i},Y=y]=x_{i}_{i}^{y}\), _i.e._, the conditional mean scales linearly with \(x_{i}\).
* \([ X_{i}=x_{i},Y=y]=_{y}-_{i}^{y}_{i }^{y}\), _i.e._, the conditional covariance is smaller near \(i\), but independent of the exact value of \(x_{i}\).
* Lindeberg's condition holds for the sequence \(w_{1}X_{1},,w_{N}X_{N} X_{i}=x_{i}\) as \(N\) for all \(x_{i}\).

Our motivation for Assumptions A1 to A3 is that they replicate the kurtosis of the marginal distributions \(X_{i}\) (discussed further below) of two important and distinct limiting cases where localization does and does not appear, respectively: when \(\) has support on the vertices of the hypercube \(\{ 1\}^{N}\) (satisfied by Ising for any \(J\)), and when \(\) is Gaussian (satisfied by \(\) with \(g 0\)).

The gradient flow in Lemma 3.1 also relies on Assumption A3 that Lindeberg's condition holds for the sequence \(w_{i}X_{i}\), which ensures that no single term \(w_{i}X_{i}\) in the sequence can dominate. If this holds,then we can conclude that \(<,> X_{i}\) is approximately Gaussian. As we discuss in Appendix C.2, this is almost always satisfied for a Gaussian initialization of \(\), and for slight deviations therefrom, and is satisfied by the settings of Ingrosso and Goldt . Using this fact, we obtain an explicit form for the gradient flow early in training, stated in Lemma 3.1.

**Lemma 3.1**.: _Under Assumptions A1 and A2, the gradient flow for the single ReLU neuron in M2 early in training with \(y=0,1\) trained using MSE loss is_

\[}{t}=(}{,>}} )-(_{0}+_{1})+o_{N}(1),\] (5)

_where \(o_{N}(1)\) vanishes as \(N\), and where \(:(-1,1)\) is defined as_

\[(a)=_{X_{1} Y=1}[X_{1}(X_{1} ^{-1}(a)/)]\] (6)

_and \(^{-1}(x)=x/}\), the inverse of the algebraic sigmoid function \((x)=x/}\)._

Lemma 3.1 reduces the study of higher-order statistics to the marginal distributions, \(X_{1}\), where, by translation invariance, all marginals have the same distribution, so we refer to \(X_{1}\) without loss of generality. While Lemma 3.1 technically only holds early in training and breaks down if \(\) becomes localized due to violation of A3, the gradient flow in Eq. (5) holds sufficiently long to detect the emergence of localization in the weights \(\). In particular, numerically integrating Eq. (5) yields localized weights \(\) as \(t\). Moreover, the location of the peak of final weights from Eq. (5) corresponds closely to the actual peak of the weight, when we observe localization; see Section 4.2 for empirical validation of this fact. The primary difference observed is that the localized bump from Eq. (5) is less peaked than when computed exactly; see Fig. 3 for a comparison between experimentally observed localized receptive fields and theoretical predictions.

### Necessary and sufficient conditions for emergent localization

To establish an exact threshold at which localization emerges requires solving Eq. (5), which is not possible exactly for general nonlinear differential equations.3 Nevertheless, the form of Eq. (5) reveals that localization is driven solely by the first term. Indeed, the second term depends only on the second-order statistics of the data, and so can be held fixed as \(\) is varied from a distribution that induces localization to one that does not. Secondly, one can see that the first term in Eq. (5) does not change as \(\) is scaled, in contrast to the second term. As such, the second term in Eq. (5) serves to constrain the _scale_ of \(\), distinct from localization, while the first is primarily concerned with the _shape_ of \(\), and thus localization. This further motivates the first term, and thus \(\), which we will refer to as the _amplifier_ and which itself depends on properties of the data distribution \(p()\), as a focus of study for understanding localization.

We present an analysis of \(\) in Appendix B.1 that reveals the role of the marginal distribution of the data in driving localization. For each marginal, \((a)()a\) for \(a 0\). For larger \(a\), \(\) depends more strongly on the data distribution and can be super-linear (sub-linear), _i.e._, greater (smaller) than \(()a\). Super-linear \(\) encourage entries in \(\) that are large in some neighborhood to grow faster than those that are smaller, yielding localization. Linear and sub-linear \(\) are the opposite, encouraging oscillatory or flat weights by suppressing neighborhoods in \(\). However, super- and sub-linearity may not hold uniformly, as \(\) can be both over its domain (see Fig. 3, bottom row, black line). As an approximation, we consider a third-order Taylor expansion (red lines in Fig. 3, second column), which reveals that for the canonical setting of \(^{2}=1\), _negative excess kurtosis of the marginals yields super-linearity_, while _positive excess kurtosis yields sub-linearity_; see Appendix B.1. This leads us to the following claim, which is validated by our simulations in Section 4:

**Claim 3.2**.: _For sufficiently large \(N\), if the data \(^{N}\) satisfies conditions S1 to S3 and has marginal distributions with sufficiently negative excess kurtosis, then Model M2 will learn localized receptive fields. Conversely, if the excess kurtosis is sufficiently positive, it will not._

As a minimal positive example, the distribution with the most negative excess kurtosis is the symmetric Bernoulli, with a value of \(-2\). In our setting, this corresponds to a data vector \(\) with support on the vertices of the hypercube, \(\{ 1\}^{N}\). As mentioned above, it can be seen from the law of total covariance combined with sign-symmetry that A1 and A2 hold exactly. Note that \(\) is the same for all such distributions, which leads us to Claim 3.2 that _any_ distribution satisfying conditions S1 to S3 whose marginals are maximally concentrated will induce a localized receptive field in M2. Importantly, this claim includes the limiting case of Ingrosos and Goldt , \(g\) in NLGP. It also includes the Ising model as another example, corroborating an observation for restricted Boltzmann machines  that Ising data induces localization in a learning model. These claims are validated for the single-neuron model in Fig. 3 and in Section 4 for the many-neuron model.

### Case study: Elliptical distributions fail to produce localization

Above, we assume weak dependence (S1) as it enables a focus on how the marginals control localization. As a first investigation into departures from this regime, we consider data \(\) sampled from an elliptical distribution, where weak dependence may not hold. We specialize the definition of an elliptical distribution  to our setting of multiple class labels and sign-symmetry:

**Def 1**.: _Samples \(^{N}\) satisfy an elliptical distribution if we can write \( Y=y}{{=}}R_{y}_{y} _{y}\) where \(R_{y}\) is a nonnegative random variable, \(_{y}^{N D}\) is such that \(_{y}_{y}^{}=_{y}\), and \(_{y}\) is independent of \(R_{y}\) and uniformly distributed on the \(D\)-dimensional sphere._

The class of elliptical distributions is broad, imposing only the constraint that the contours of the density be ellipses; the multivariate Gaussian and Student-\(t\) distributions are examples. As such, they can vary greatly in measures of non-Gaussianity, including kurtosis, while maintaining enough structure for analytical convenience. Proposition 3.3 states that training on elliptical data _prevents_ localization in the single ReLU neuron model.

**Proposition 3.3**.: _Assume the data \(\) are sign-symmetric (S3), translation-invariant (S2), and follow an elliptical distribution such that the MSE on the task in Section 2.3 is always finite. If \(_{0},_{1}\) are such that the ratio of their \(i\)-th eigenvalues, \(_{i}(_{0})/_{i}(_{1})\), assumes a particular value for at most two distinct \(i\), then the steady states of the weight of \(M2\) are sinusoids, i.e., not localized._

The condition on the number \(i\) such that the ratio of the \(i\)-th eigenvalues are the same constrains the number of Fourier components that can be non-zero in the steady state of \(\). While opaque, this requirement seems to always hold in practice, as even slight increases in length-scale correlation can dramatically change the spectrum of \(_{y}\).

Figure 3: From left and for the same Ising, NLGP, and Kur data models as in Fig. 2: the marginals \(p(X_{i})\), the amplifier \(\) defined in Theorem 3.1 and kurtosis \(\), and the evolution of simulated receptive fields for the single-neuron model (M2) trained on its data, and lastly the receptive field given by numerically integrating Eq. (5) with \(\) expanded to a third-order Taylor approximation for the same data; training or evolution time is indicated by line color (blue for early-time; red for late-time). _See Section 4.1 for exposition._

The proposition is surprising because it reveals that the kurtosis of the preactivation is not an appropriate metric for explaining localization. Consider the example of the \(N\)-dimensional Student-\(t\) distribution with \(\) degrees of freedom, \(t_{N}()\). If \( t_{N}()\), then \( w, t_{1}()\). Note the kurtosis of \(t_{1}()\) is non-zero, and can be very large or even infinite for small \(\). This prediction is validated in Section 4.3. The condition also reveals that not all symmetries in the data (here, elliptical symmetry) induce structure in the trained model weights, if localization is to be seen as a sparsity more structured than oscillatory weights [_cf._ God+23]; indeed, translational symmetry (S2) is more relevant for localization than elliptical symmetry.

## 4 Experimental results

We describe experiments to validate the generalizability of the analytical results from Section 3. We run all experiments on a single CPU machine locally or on a compute cluster. Since all datasets are procedurally generated, training depends on both the model architecture and the complexity of sampling the data, but is between 10 and 60 minutes for any single simulation run.

### Validating Claim 3.2 with positive and negative predictions

In Fig. 5, we validate Claim 3.2 first via the single-neuron model (M2) with 30 initial conditions trained across a range of excess kurtoses for the \((g)\) and \((k)\) data models. We use the inverse participation ratio (IPR), defined in Appendix A.3. This measure, also used by Ingrosso and Goldt , is large when proportionally few weight dimensions "participate" (have large magnitude), and small when weight dimension magnitudes are more uniform. We see that when \(g\) and \(k\) assume values that yield a negative excess kurtosis, IPR is close to its maximum of \(1.0\), suggesting the weights are localized; if the excess kurtosis is positive, IPR is nearly zero, suggesting the weights are _not_ localized. The IPR is extremely consistent across random initializations, suggesting that localization is determined by data statistics and not initialization. The trend in IPR _vs._ excess kurtosis is very similar between the \((g)\) and \((k)\) data models, demonstrating that excess kurtosis is a primary driver of localization and localization is largely independent from other properties of the data distribution.

Figure 4 further validates Claim 3.2 with specific examples. We maintain constant initial conditions for our model and train on the Ising, \((g=0.01)\), and \((k=5)\) data models. The marginals of the Ising model have an excess kurtosis of \(-2\), the smallest possible value for any distribution. As a result, we see that the amplifier \(\) for Ising (top left) is super-linear (the dark line exceeds the dashed light line for larger \(a\)), which drives localization via its role in Eq. (5). Integrating Eq. (5) with \(\) expanded via a third-order Taylor approximation (red line) yields a similar localized receptive field to that from simulation (two right panels), validat

Figure 5: IPR _vs._ excess kurtosis for \(\) and \(\) data models, with mean and std. dev. across 30 re-initializations for the single-neuron model (M2); error bars are small and may not be visible.

Figure 4: Evolution of receptive fields learned by the single-neuron model (M2), along with sinusoids fit to final states (red dashes) when trained on data from three elliptical distributions: \(t_{40}(=3)\) (**left**), the surface of an ellipse (**middle**), and a custom elliptical distribution that places its mass near the outside of an ellipse (**right**). In all cases, the learned receptive field is oscillatory (a sinusoid), as predicted by Proposition 3.3. The \(_{2}\) distances between the fitted oscillatory weights and empirical RFs, as a ratio of the \(_{2}\) norm of the empirical RFs, are (left) 9.77%, (center) 3.75%, and (right) 4.14%. _See Section 4.3 for exposition._

For the remaining distributions (middle and bottom rows) that elicit oscillatory (sinusoidal) weights, Claim 3.2 is validated due to their positive excess kurtosis. The dynamical steady state (far right) assumes a more negative value than in the simulation (to the left), a difference that is the result of deviations of our _early-time_ gradient flow in Eq.5, but these deviations remain mild enough nevertheless to recover the qualitative structure of the learned receptive field.

### Validating Eq.5 with localization position prediction

The simulated and integrated receptive fields in Fig.3 demonstrate that our analytical model is able to meaningfully reproduce localization in receptive fields from neural network training. For the Ising model, we see that the integration even has a peak in the exact same position as the simulation (at index \(i=6\)), suggesting precision in our approximation. Indeed, we simulated the condition in Fig.3 for the Ising model under 28 different initial conditions (weight initializations), and found that in 26 of them (93%), the peaks of the integrated and simulated receptive fields matched exactly. In the two cases where the peaks differed, they did so substantially (see Fig.8 for an example).

### Validating Proposition3.3: Elliptical distributions fail to localize

Proposition3.3 claims that the single-neuron model (M2) trained on elliptical data will yield sinusoidal receptive fields, subject to a condition on the spectra of \(_{0}\) and \(_{1}\). We verify this claim in Fig.4 with three distinct elliptical distributions. The first, \(t_{40}(=3)\), gives preactivations \(,\) that have _infinite_ kurtosis, yet our theory predicts the final receptive field will be sinusoidal. This is confirmed in Fig.4, where the learned receptive field is indeed a sinusoid with period 1 and intercept at zero.

We also consider data sampled from the surface of an ellipse, which is done by fixing \(R_{y} 1\) in Definition1. Here, we observe that the learned receptive field is a near-constant function at \(-0.04\) (note that \((2 0 x) 1\) is a sinusoid, allowing nonzero intercepts and constant functions). Finally, we consider an unconventional elliptical distribution where the density of \(R\) is given by \(p_{R}(r)=(4e^{2r+4})/(e^{2r}+e^{4})^{2} 1(r 2)\). This particular density places most of its mass near \(r=2\) before rapidly falling off, imposing a minimum norm on \(\) and pushing support near the surface of an ellipse. This distribution, too, yields an oscillatory steady state, as shown in Fig.4 (right). We confirm our visual observations by fitting sinusoids to the final receptive fields and see the relative errors are quite low.

### Extensions to many-neuron model and ICA

All of our analysis thus far has concerned single-neuron models with ReLU activation and without hidden-to-output or bias terms, assumptions which were made to make our analysis tractable. Here, we depart from that regime by considering the SCM and the full two-layer network (Model M1). In Fig.6 (left) and (center), we train a SCM with 10 hidden units and sigmoid activation on the \((10)\) and \((4)\) datasets, which have excess kurtoses of \(-0.93\) and \(3.28\), respectively. So, based on our single-neuron analysis, we _do_ and _do not_ expect to see localization for these distributions. Indeed, this is precisely what we observe in Fig.6, where the receptive fields are sharply localized for the former distribution, while they look like low-frequency oscillations for the latter.

Figure 6: (**Left, Center)** Receptive fields learned by many-neuron (M1) soft committee machines (second-layer weights fixed at \(\)) trained on the \((10)\) and \((4)\) datasets, respectively. The models had \(N=40\) input units, \(K=10\) hidden units, and an initialization variance of \(0.1\). (**Right**) A random subset of 10 components from the 40 learned by the FastICA algorithm from scikit-learn [19; 20] on the \((3)\) dataset with length-scale correlation values of \(_{0}=1\) and \(_{1}=3\). _See Section4.4 for exposition.

In Fig. 7, we train many-neuron models with \(N=40\) input units and \(K=10\) hidden units, where all weights are learnable. In general, adding flexibility in the second layer leads to more varied structure in the first layer. We train on \(}(4)\) (top), which has an excess kurtosis of \(3.28\), and \(}(30)\) (bottom), which has an excess kurtosis of \(-1.17\). The receptive fields from the former are not localized, as in the single-neuron model; however, they appear more like high-frequency oscillations than low-frequency sinusoids. For \(}(30)\), where we expect localization, we see that the first three receptive fields exhibit localization, but less so than for a single neuron. Importantly, not all receptive fields are localized, a result of a variable second-layer weight effectively changing the variance \(^{2}\) in the third-derivative term in Lemma B.1.

We further compare these predictions against ICA, another framework that has been used to model receptive fields in visual cortex. We train on the \(}(3)\) dataset, which has marginals with excess kurtosis \(7.66\), fitting 10 components using the FastICA implementation from scikit-learn . We observe in Fig. 6 (right) that we learn localized receptive fields; this contrasts our neural network models, which require negative excess kurtosis. This stems from ICA's objective to maximize non-Gaussianity, regardless of how specifically it is done. The sign of the excess kurtosis is irrelevant, so long as it is nonzero. This deviation between our analytical model and ICA is an interesting avenue for future study, perhaps by validation with natural images.

## 5 Conclusions

We derive effective learning dynamics for the minimal example of emergent localization in a neural receptive field given by Ingrosso and Goldt . The analytical approach we take relies on the assumption that the _conditional_ preactivation is Gaussian, a refinement of previous work that assumes Gaussianity of the unconditioned preactivation as asserted by the _Gaussian equivalence property_ targeted by Ingrosso and Goldt . This approach may prove extensible beyond our specialized setting and may enable further analysis of how statistics of an input task drive emergent structure in neural network learning.

Emergence as an alternative mechanism to top-down constraints like sparsity is in line with recent work that reformulates data-distributional properties as a driver for complex behavior . Via these analytical effective dynamics, we observe that specific data properties--the covariance structure and the marginals--shape localization in neural receptive fields. Though we cannot capture dynamical interactions between neurons that may shape receptive fields in other settings with the single-neuron analytical model, our empirical validations with many neurons suggest that these interactions do not, in fact, play a significant role in shaping localization [_cf._ Har+20].

The data model we consider is a simplified abstraction of the task faced by early sensory systems, and, as a consequence, we do not yet capture certain features of receptive fields that are observed in early sensory systems. In particular, we do not observe orientation nor phase selectivity, features of simple-cell receptive fields in early sensory cortices and in artificial neural networks that can be seen in a subset of receptive fields in Fig. 1 (left and center, respectively). To capture orientation selectivity, it may be fruitful to follow the approach of Karklin and Simoncelli , who tie orientation selectivity in a population-based efficient-coding framework to the presence of noise. Furthermore, on-center-off-surround-filtering input data, including the idealized data, gives receptive fields with subfields in our simulations, but is difficult to analyze. Lastly, we do not yet look at the distribution of receptive field shapes and do not validate against other models of receptive field learning beyond a brief comparison with ICA [_cf._ Sax+11], but these are exciting avenues for future work.

Figure 7: Receptive fields learned by the many-neuron model (M1) with learnable second-layer weights, \(N=40\), \(K=10\). (**Top**) A random subset of 4 receptive fields from a model with sigmoid activation, trained on \(}(4)\) (positive excess kurtosis of \(3.28\)). As predicted by Claim 3.2, the receptive fields are _not_ localized, and appear as high-frequency oscillations. (**Bottom**) A random subset of 4 receptive fields from a model with ReLU activation, trained on \(}(30)\) (negative excess kurtosis of \(-1.17\)). Receptive fields are localized (**left three**) or exhibit low-frequency oscillations (**right**). _See Section 4.4 for exposition._