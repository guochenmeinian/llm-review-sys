# Characterization and Learning of Causal Graphs with Small Conditioning Sets

Murat Kocaoglu

School of Electrical and Computer Engineering

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer \(k\) for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of \(k\)-Markov equivalence: Two causal graphs are \(k\)-Markov equivalent if they entail the same conditional independence constraints where the conditioning set size is upper bounded by \(k\). We propose a novel representation that allows us to graphically characterize \(k\)-Markov equivalence between two causal graphs. We propose a sound constraint-based algorithm called the \(k\)-PC algorithm for learning this equivalence class. Finally, we conduct synthetic, and semi-synthetic experiments to demonstrate that the \(k\)-PC algorithm enables more robust causal discovery in the small sample regime compared to the baseline algorithms.

## 1 Introduction

Causal reasoning is a critical tool for machine learning and artificial intelligence research with benefits ranging from domain adaptation to planning, explainability and fairness . Estimating the effect of an action or an intervention from observational data is called causal inference. A very rich literature of causal inference algorithms have been developed to address this task in the literature . The function that is used to write an interventional distribution in terms of the observational distribution is called the estimand. Estimand depends on the causal relations between the system variables, which are represented in the form of a directed acyclic graph (DAG) called the causal graph. Thus, causal graph is required for solving most causal inference problems.

For small and well-studied systems, it might be possible to construct a causal graph using expert knowledge. However, in modern complex systems with changing structure, we need to learn causal graphs from data. This is called causal discovery. In most domains, we have access to plenty of observational data, but no interventional data. An important task then is to understand how much causal knowledge we can extract from observational data about a system.

The classical approach for addressing this problem is to use the conditional independence (CI) relations in the data to narrow down the space of plausible causal graphs . These are called constraint-based methods. Even though the number of causal graphs that entail the same set ofconditional independence relations are typically exponentially many, we can use a graphical characterization of _equivalence_ between causal graphs to compactly represent all of them using a single mixed graph called the _essential graph_. This notion of equivalence is called Markov equivalence.

Even though such causal discovery algorithms are consistent, i.e., they output the correct essential graph in the large sample limit, in practice, they struggle due to finite number of samples since not all CI tests can be performed accurately . For constraint-based algorithms, however, it is important for every test to be accurate since previously learned edge orientations are used to orient more edges due to their sequential nature. Thus, they may output very different causal graphs compared to the ground truth with just a few incorrect CI statements. Despite the efforts to stabilize PC output [2; 5], this fundamental issue still causes problems today. Furthermore, the existing Markov equivalence class characterization and causal discovery algorithms that rely on this characterization, such as the PC/IC algorithms [19; 22], require access to every CI relation, which is a significant practical limitation for causal discovery from data.

There are several alternatives to constraint-based causal discovery. For example, score-based approaches, such as GES [3; 4] optimize a regularized score function by greedily searching over the space of DAGs to output a graph within the Markov equivalence class. A line of works, such as NOTEARS  converts the graph learning problem to a continuous optimization problem by converting the acyclicity constraint to a continuous constraint via trace formulation. Note that our goal in this paper is not to beat the state-of-the-art causal discovery algorithm, but provide a theoretical basis to characterize what is learnable on a fundamental level by using conditional independence tests with restricted cardinality conditioning sets.

Variations of the causal discovery problem with limited-size conditioning sets have been considered in the literature. The special case of marginal dependence is considered in  and . The most related existing work is , where the authors aim learning the graph union of all causal graphs that are consistent with a set of conditional independence statements up to a fixed cardinality conditioning set. They propose a concise algorithm that modifies the steps of PC which they show recovers the union of all equivalent graphs. Similarly in the case with latent variables, AnytimeFCI  shows that one can stop FCI algorithm after exhausting all conditional independence tests up to a fixed cardinality conditioning set and the output of the algorithm is still sound for learning parts of the partial ancestral graph (PAG). We propose an alternative route: First, we formally define the equivalence class of causal graphs and propose a graphical object to capture this equivalence class called the \(k\)-closure graphs. We identify a necessary and sufficient graphical condition to test equivalence between \(k\)-closure graphs. Finally, we develop a learning algorithm that leverages the representative power of partial ancestral graphs (PAGs), which are typically used in the case with latents, to obtain a provably finer characterization of the set of equivalent causal structures than .

In this paper, we propose learning causal graphs using CI tests with bounded conditioning set size. This allows us to ignore CI tests that become unreliable with limited data, and avoid some of the mistakes made by constraint-based causal discovery algorithms. We call CI constraints where the conditioning set size is upper bounded by \(k\) as the degree-\(k\) CI constraints. We call two causal graphs \(k\)-Markov equivalent if they entail the same degree-\(k\) CI constraints. We propose \(k\)-closure graphs that entail the same degree-\(k\) CI relations as the causal graph, and show that we can characterize \(k\)-Markov equivalence via Markov equivalence between \(k\)-closure graphs. We then propose a constraint-based learning algorithm for learning this equivalence class from data, represented by the so-called \(k\)-essential graph. Finally, we demonstrate that our algorithm can help alleviate the finite-sample issues faced by the PC algorithm, especially while orienting arrowhead marks and for correctly learning the adjacencies in the graph. We also compare our algorithm with NOTEARS  and LOCI .

## 2 Background

In this section, we give some basic graph terminology as well as background on constraint-based causal discovery: \(An(x)\) represents the set of ancestors of \(x\) in a given graph that will be clear from context. \(De(x)\) similarly represents the descendants of \(x\). \(Ne(x)\) represents the neighbors of \(x\), i.e., the nodes that are adjacent to \(x\).

**Definition 2.1**.: A path in a causal graph is a sequence of nodes where every pair of consecutive nodes are adjacent in the graph and no node appears more than once.

**Definition 2.2** (Skeleton).: The skeleton of a causal graph \(D\) is the undirected graph obtained by making every adjacent pair connected via an undirected edge.

**Definition 2.3** ((Unshielded) Collider).: A path of three nodes \((a,c,b)\) is a collider if the edges adjacent to \(c\) along the path are into \(c\). A collider is called an unshielded collider if in addition the endpoints of the path \(a,b\) are non-adjacent.

**Definition 2.4** (d-connecting path).: A path \(p\) is called d-connecting given a set \(c\) iff every collider along \(p\) is an ancestor of some node in \(c\), and every non-collider is not in \(c\).

**Definition 2.5** (d-separation).: Two nodes \(a,b\) on a causal graph \(D\) are said to be d-separated given a set \(c\) of nodes iff there is no d-connecting path between \(a,b\) given \(c\), shown by \((a\!\!\! b\,|c)_{D}\), or \(a\!\!\! b\,|c\) if clear from context.

**Definition 2.6** (Causal Markov assumption).: A distribution \(p\) is called Markov relative to a graph \(D\!=\!(V,E)\), if each variable is independent from its non-descendants conditioned on its parents in \(D\).

There are local, global and pairwise Markov conditions that can all be shown as a consequence of the Causal Markov condition above . This gives us the following:

**Proposition 2.7** ().: _Let \(p\) be any joint distribution between varibles on a causal model with the graph \(D\). If \((a\!\!\! b\,|c)_{D}\) then \((a\!\!\! b\,|c)_{p}\), i.e., d-separation implies conditional independence under the Causal Markov condition._

**Definition 2.8** (Markov equivalence).: Two causal graphs \(D_{1},D_{2}\) are called Markov equivalent, shown by \(D_{1} D_{2}\), if they entail the same d-separation constraints.

**Definition 2.9** (Causal Faithfulness assumption).: A distribution \(p\) is called faithful to a causal graph \(D=(V,E)\) iff the following holds \( a,b V,c V(a\!\!\! b\,|c)_{p}(a \!\!\! b\,|c)_{D}\).

Constraint-based causal discovery is not possible without some form of faithfulness assumption, since otherwise CI statements do not inform us of the graph structure. We will later see that restricting ourselves to a small set of CI tests allow us to also weaken the faithfulness assumption we need for our causal discovery algorithm.

**Theorem 2.10** ().: _Two DAGs are Markov equivalent if and only if they have the same skeleton and the same unshielded colliders._

**Definition 2.11** (Degree-\(k\) d-separations).: The collection of d-separation statements entailed by a DAG where the conditioning set has size of at most \(k\) is called degree-\(k\) d-separation statements.

## 3 Markov Equivalence of Causal Graphs with Small Conditioning Sets

We are interested in characterizing the collection of causal graphs that entail the same d-separation constraints for all conditioning sets of size up to \(k\) for some \(k\). We call this \(k\)-Markov equivalence:

**Definition 3.1**.: Two DAGs \(D_{1},D_{2}\) are called \(k\)-Markov equivalent, shown by \(D_{1}_{k}D_{2}\) if they entail the same d-separation constraints \(a\!\!\! b\,|c\) for all \(c V:|c| k\).

The notion of \(k\)-Markov equivalence partitions the set of causal graphs into equivalence classes:

**Definition 3.2**.: \(k\)-Markov equivalence class of a DAG \(D\) is defined as the set of causal graphs that are \(k\)-Markov equivalent to \(D\), shown by \([D]_{k}\), i.e., \(D^{}\!_{k}D, D^{}[D]_{k}\).

We first discuss how Verma and Pearl's characterization fails when we cannot test all d-separation statements. Note that if two DAGs have the same skeleton and unshielded colliders, then they entail the same d-separation constraints by Theorem 2.10. Therefore, they also entail the same degree-\(k\) d-separation relations. However, the other direction is not true: Two graphs with different unshielded colliders or different skeletons may still entail the same degree-\(k\) d-separation relations. This is expected since we are checking less constraints, which increases the size of the equivalence class: More and more graphs become indistinguishable as we do not rely on certain d-separation constraints.

For example, consider the causal graphs in Figure 1 (a, b). In \(D_{1}\), even though \(a,b\) are non-adjacent, they cannot be made conditionally independent unless both \(c\) and \(d\) are conditioned on. Similarly in \(D_{2}\), \(c,b\) cannot be made independent unless both \(a,d\) are conditioned on. In both graphs, \(a,d\) are independent, and become dependent conditioned on \(c\) or \(b\). Therefore, \(D_{1},D_{2}\) are \(k\)-Markov equivalent for \(k=1\): They entail the same conditional independence relations for conditioning setsof size up to 1. Similarly in Figure 1 (c, d), the flipped edge between \(c,b\) induces different unshielded colliders in \(D_{1}\) and \(D_{2}\). However, the endpoints of these colliders \((f,c)\), \((d,c)\) and \((e,b)\), (\(a,b)\) are all dependent conditioned on subsets of size at most 1. Then it is easy to verify that \(D_{1},D_{2}\) entail the same degree-\(k\) d-separation relations for \(k=1\) despite having different unshielded colliders.

These examples show that different structures may induce the same degree-\(k\) d-separation relations. One might think that, if a collider actually changes the \(k\)-Markov equivalence class, then perhaps there is hope that a local characterization around all such equivalence class-changing colliders might be possible. However, this is not true. Our example in Section D.1 shows that a local characterization similar to Theorem 2.10 is not possible for \(k\)-Markov equivalence.

### \(k\)-closure Graphs

Our goal is to come up with a graphical representation of \(k\)-Markov equivalence class that captures the invariant causal information across equivalent DAGs. This will later be useful for learning identifiable parts of the causal structure from data by CI tests. First, we introduce the notion of a \(k\)-covered pair.

**Definition 3.3**.: Given a DAG \(D=(V,E)\) and an integer \(k\), a pair of nodes \(a,b\) are said to be \(k\)-covered if \( c V:|c| k\) and \(a\!\!\! b\,|c\).

Therefore, \(k\)-covered pairs are pairs of variables that cannot be made independent by conditioning on subsets of size at most \(k\). For example, \(a,b\) in Figure 1 are \(k\)-covered for \(k=1\). In any graphical representation of \(k\)-Markov equivalence class, \(k\)-covered pairs should be adjacent. This is because it is not always possible to distinguish if they are adjacent or not by degree-\(k\) d-separation relations.

We propose \(k\)-closure graphs as a useful representation to characterize \(k\)-Markov equivalence class.

**Definition 3.4**.: Given a DAG \(D=(V,E)\) and an integer \(k\), the \(k\)-closure of \(D\) is defined as the graph shown by \(_{k}(D)\) that satisfies the following:

1. If: \(a,b\) are \(k\)-covered in \(D\) \(i)\) if \(a An(b)\), then \(a b\) in \(_{k}(D)\), \(ii)\) if \(b An(a)\), then \(a b\) in \(_{k}(D)\), \(iii)\) else \(a b\) in \(_{k}(D)\)
2. Else: \(a,b\) are non-adjacent in \(_{k}(D)\)

The definition of \(k\)-closure trivially implies the following:

**Lemma 3.5**.: _Given a DAG \(D\) and an integer \(k\), the \(k\)-closure graph \(_{k}(D)\) is unique._

Furthermore, there is a straightforward algorithm one can employ to recover a \(k\)-closure from the DAG: Make the non-adjacent pairs that cannot be d-separated with conditioning sets of size at most \(k\) adjacent according to the definition. Even though this may not be a poly-time operation, it is unavoidable to characterize the \(k\)-Markov equivalence class.

The following lemma shows that \(k\)-closure graphs can be used to capture all the conditional independence statements with bounded-size conditioning sets imposed by a DAG.

**Lemma 3.6**.: \(k\)_-closure \(_{k}(D)\) of a DAG \(D\) entails the same degree-\(k\) d-separation statements as the DAG, i.e., \((a\!\!\! b\,|c)_{D}(a\!\!\! b\,|c)_{_{k}(D) }, c V:|c| k\)._

Figure 1: (a), (b): Both \(D_{1}\) and \(D_{2}\) entail the same degree-\(1\) CI relations although they have different skeletons, thus they are in the same \(k\)-Markov equivalence class. (c), (d): Both \(D_{3}\) and \(D_{4}\) entail the same degree-\(1\) CI relations although they have different unshielded colliders, thus they are in the same \(k\)-Markov equivalence class.

Proof Sketch.: The proof relies on two crucial lemmas. If \(a b\) in \(_{k}(D)\), then in \(D\), conditioned on any subset of size at most \(k\), there is a d-connecting path with arrowheads on both \(a\) and \(b\). Similarly, if \(a b\) in \(_{k}(D)\), then in \(D\), conditioned on any subset of size at most \(k\) there is a d-connecting path with an arrowhead at \(b\). Using these, we can show that any d-connecting path in \(_{k}(D)\) implies a d-connecting path in \(D\). The other direction is straightforward as the d-connection statements in \(D\) hold in \(_{k}(D)\) since it is obtained from \(D\) by adding edges. Please see Section A.1 for the proof. 

Inspired by ancestral graphs , \(k\)-closure graphs are mixed graphs with directed and bidirected edges, where \(k\)-covered pairs in the DAG are made adjacent in \(_{k}(D)\) based on their ancestrality.

**Definition 3.7**.: A mixed graph is a graph that contains directed edges \(a b,a b\) or bidirected edges \(a b\) where every pair of nodes is connected by at most one edge.

Due to bidirected edges, \(k\)-closure graphs are not DAGs. However, we can show that they are still acyclic. In fact, it is worth noting that \(k\)-closure graphs are a special class of ancestral graphs .

**Lemma 3.8**.: _For any DAG \(D\), the \(k\)-closure graph \(_{k}(D)\) is a maximal ancestral graph (MAG)._

MAGs have been successfully applied for learning causal graphs with latent variables . Similar to MAGs, \(k\)-closure graphs are simply graph objects to help us compactly represent the \(k\)-Markov equivalence class of causal DAGs, rather than expressing the underlying physical system directly. They do, however, represent ancestrality relations between variables by construction.

The relation between \(k\)-closure graphs and MAGs is not an if and only if relation. In a MAG, one can have a bidirected edge between any pair of nodes as long as it does not create a (almost) directed cycle. This is because they represent latent confounders and one might have latent confounders between any pair of observed nodes. However, bidirected edges in \(k\)-closure graphs represent \(k\)-covered pairs and cannot be added arbitrarily. Accordingly, there are MAGs that are not valid \(k\)-closure graphs. An example is given in Section D.2. We have the following characterization:

**Theorem 3.9**.: _A mixed graph \(K=(V,E)\) is a \(k\)-closure graph if and only if it is a maximal ancestral graph and for any bidirected edge \(a b E\) the following is true:_

* \( c V:|c| k,(a\!\!\! b\,|c)_{K^{}}\)_, where_ \(K^{}=(V,E-\{a b\})\)_._

The Markov equivalence between MAGs has been characterized in the literature. This relies on not only skeletons and unshielded colliders, but also colliders on discriminating paths being identical.

**Definition 3.10**.: A path \(p= a,z_{1}, z_{m},u,Y,v\) is called a discriminating path for \(u,Y,v\) if \(a,v\) are not adjacent and every vertex \(\{z_{i}\}_{i},u\) are colliders on \(p\) and parents of \(v\).

**Theorem 3.11**.: _[_15_]_ _Two MAGs \(M_{1},M_{2}\) are Markov equivalent if and only if \(i)\) They have the same skeleton, \(ii)\) They have the same unshielded colliders, \(iii)\) For any node \(Y\) for which there is a discriminating path \(p\), \(Y\) has the same collider status on \(p\) in \(M_{1},M_{2}\)._

One important observation for our characterization is that Markov equivalence of \(k\)-closure graphs do not rely on discriminating paths unlike arbitrary ancestral graphs.

**Lemma 3.12**.: _Suppose two \(k\)-closure graphs \(K_{1},K_{2}\) have the same skeleton and unshielded colliders. Then along any discriminating path \(p\) for a node \(Y\), \(Y\) has the same collider status in \(K_{1}\) and \(K_{2}\)._

The above lemma shows that discriminating paths, although may exist in \(k\)-closure graphs, do not alter the equivalence class by themselves. Hence, for the graphical characterization of the equivalence between \(k\)-closure graphs, we can drop the discriminating path condition.

**Corollary 3.13**.: _Two \(k\)-closure graphs \(K_{1},K_{2}\) are Markov equivalent if and only if_

1. _They have the same skeleton and_
2. _They have the same unshielded colliders._

In the next section, we will prove a \(k\)-Markov equivalence characterization based on Lemma 3.6, which will later be useful for learning, since we can employ algorithms devised for learning MAGs.

### \(k\)-Markov Equivalence

Our main result in this section is the following theorem that characterizes \(k\)-Markov equivalence:

**Theorem 3.14**.: _Two DAGs \(D_{1},D_{2}\) are k-Markov equivalent if and only if \(_{k}(D_{1})\) and \(_{k}(D_{2})\) are Markov equivalent._

Thus, \(k\)-Markov equivalence of two DAGs can be reduced to checking Markov equivalence of their \(k\)-closures, which can be checked locally using the equivalence condition in Corollary 3.13. Based on this result, it is clear that, just by using conditional independence tests, we can only hope to narrow down our search up to the equivalence class of \(k\)-closure graphs.

Note that when all the CI tests can be conducted, we can learn the arrowheads and tails that consistently appear in all Markov equivalent DAGs. By operating at the \(k\)-closure graph-level, we can attain a similar objective and hope to learn the invariant arrowheads and tails.

**Definition 3.15** (Edge union).: For our purposes, we define the edge union operation as follows:

\[a b a b= a}b,  a b a b a b= a}b= a}b\]

**Definition 3.16** (\(k\)-essential graph).: For any DAG \(D\), the edge union of all \(k\)-closure graphs that are Markov equivalent to \(_{k}(D)\) is called the \(k\)-essential graph1 of \(D\), shown by \(_{k}(D)\).

For example, among Markov equivalent \(k\)-closures, if \(a,b\) are adjacent only as \(a b\) the \(k\)-essential graph will have the edge \(a b\). Thus, the \(k\)-essential graph will preserve the invariant arrowhead and tail marks. The difference between \(a}b\) and \(a}o}b\) is significant from a causal perspective: In the former, two variables cause each other. In the latter, there is some \(k\)-Markov equivalent DAG where \(a,b\) do not cause each other and are simply not separable by conditioning sets of size at most \(k\).

For any edge type proposed in the edge union definition, there are relevant instances of \(k\)-essential graphs. In Figure 2, the edges \(a}b\), \(ao}c\), \(c b\) appear in the \(k\)-essential graph. Similarly, the \(k\)-essential graph of \(D\) in Figure 8 will contain the edge \(c}o}o\) since \(c d,c d,c d\) are all possible edges in different Markov equivalent \(k\)-closures. Figure 9 in Section D.3 shows an instance where \(\) appears in the \(k\)-essential graph. This example also demonstrates that our representation is strictly richer than the graph union that is recovered by LOCI , which orients \(d c,a c\), and hence cannot distinguish them from the possible edges between \(c b\) unlike our representation. Similarly, consider the simple graph \(u v\) with \(k=0\). Our representation yields \(u-v\) since there is no DAG where \(u,v\) are confounded in any way other than the direct edge. In a triangle graph \(w u,w v,u v\), our representation recovers \(uo}ov\) which shows there are graphs where \(u,v\) do not cause each other. LOCI cannot make such distinction as it uses \(u-v\) in both graphs.

Therefore, the variant edges that can be arrowheads or tails among different \(k\)-Markov equivalent graphs are replaced with circles and invariant arrowheads and invariant tails are preserved in the \(k\)-essential graph. Thus, \(k\)-essential graph captures the causal information that is preserved across all \(k\)-Markov equivalent causal graphs. Different from essential graphs where we can test all the

Figure 2: Two \(k\)-Markov equivalent DAGs for \(k=0\) with the same \(k\)-essential graph. \(D_{1}}D_{2}\) for \(k=0\). Thus, \(_{k}(D_{1})_{k}}(D_{2})}\). Thus, they have the same \(k\)-essential graphs \(_{k}(D_{1})_{k}(D_{2}) _{k}\), obtained as the edge union of their \(k\)-closures. Note that there are no Markov equivalent \(k\)-closures, where \(a,d\) are connected with a bidirected edge since removing that edge from the \(k\)-closure graph would make \(a,d\) separable by empty set, which means it would not be a valid \(k\)-closure graph by Theorem 3.9. Thus, \(a,d\) is connected via undirected edge. Similarly, there is no Markov equivalent \(k\)-closure where \(c b\) since \(c,b\) would not be \(k\)-covered in any Markov equivalent \(k\)-closure graph.

conditional independence relations, existence of an arrow \(a b\) in a \(k\)-essential graph does not mean \(a\) causes \(b\) in every \(k\)-Markov equivalent DAG. Rather, it means that in any \(k\)-Markov equivalent DAG where \(a,b\) are adjacent, \(a\) causes \(b\).

It is worth noting that \(k\)-essential graphs are in general more informative than partial ancestral graphs (PAGs), which are defined as the edge union of all Markov equivalent MAGs, where the union of \(,\) is defined to give \(o\)--\(o\) instead of the undirected edge. Since every Markov equivalent \(k\)-closure graph is a MAG but not every Markov equivalent MAG is a \(k\)-closure graph, in general \(k\)-essential graphs may have invariant arrowheads and tails where PAGs only have circles. We call any graph that contains arrowheads, tails, or circles as edge marks a partial mixed graph (PMG).

**Definition 3.17**.: For two partial mixed graphs \(A,B\) with the same skeleton, \(A\) is said to be a subset of \(B\), shown by \(A B\), iff the following conditions hold for any pair \(a,b\)

\[1.\ (a*b)_{A}(a*b)_{B }, 2.\ (a*b)_{A}(a*b)_{B}. \]

Note that asterisk \(*\) stands for a wild-card which can either be an arrowhead, tail, or circle. According to the definition, any circle mark in \(A\) is also a circle mark in \(B\). We have the following lemma that relates \(k\)-essential graphs to partial ancestral graphs of \(k\)-closures.

**Lemma 3.18**.: \(_{k}(D)}(_{k}(D))\)_._

Proof.: By Theorem 3.9, every \(k\)-closure graph is a MAG whereas every MAG is not a \(k\)-closure graph. Thus the set of Markov equivalent \(k\)-closure graphs form a subset of the set of Markov equivalent MAGs. Thus, the arrowheads and tails that appear in all Markov equivalent MAGs must also appear in all the Markov equivalent \(k\)-closure graphs. The result follows from Definition 3.17. 

In the next section, we propose a sound algorithm for learning \(k\)-essential graphs from data.

## 4 Learning \(k\)-essential Graphs

Constraint-based causal discovery algorithms use CI tests to extract all the causal knowledge that can be identified from data. In the previous section, we proposed a compact graphical characterization of what is learnable from such statistical tests. In this section, we propose a constraint-based learning algorithm. Since \(k\)-closure graphs are a special class of maximal ancestral graphs, we can use FCI algorithm that is devised for learning the invariant arrowheads and tails of a maximal ancestral graph.

FCI algorithm is sound and complete for learning PAGs: It can recover all invariant arrowheads and all invariant tails. One might think that FCI algorithm may also be sound and complete for our task. However, this is not true. Although sound, FCI is not complete for learning \(k\)-essential graphs.2

For soundness, we need the following lemma, which shows that discriminating paths do not carry extra information about the underlying causal structure.

**Lemma 4.1**.: _In any \(k\)-closure graph, if there is a discriminating path \(p\) for \( u,Y,v\) and \(u Y v\) is a collider along \(p\), then the orientations \(u\)\(\)\(\)\(Y\) and \(Y\)\( v\) can be learned by first finding all unshielded colliders, and then applying the orientation rules \(1\) and \(2\) of \(FCI\)._

The above lemma shows that the colliders that are part of discriminating paths can be learned by simply orienting the unshielded colliders and then applying the orientation rules of FCI. This is useful since we do not need to search for colliders along discriminating paths during learning.

Our constraint-based causal discovery algorithm, called \(k\)-PC, is given in Algorithm 1. It uses FCI Orient algorithm in Section B, with two extra rules specific for learning \(k\)-essential graph.

**Corollary 4.2**.: \(k\)_-PC without Step \(5\) is sound and complete for learning \((_{k}(D))\) of any DAG \(D\)._

Proof.: By Lemma A.18, any non-adjacent pair are separable by a set of size at most \(k\). Thus, a valid separating set for any non-adjacent pair will be found in Step \(1\), and will be used to learn the skeleton in Step \(2\), and to orient all unshielded colliders in Step \(3\).  proved arrowhead and tail completeness of FCI with orientation rules \(1\) to \(10\). By Lemma 4.1, colliders on discriminating paths will be oriented at the end of Step \(4\). Thus, \(4\) that is concerned with discriminating path colliders is not applicable. Similarly, \(5\), \(6\), \(7\) are only applicable in graphs with selection bias. Thus, they are not applicable. Since the rules that we omit are never applicable during the execution of the algorithm, Step \(4\) correctly returns the \((_{k}(D))\) since the algorithm at that point is identical to the FCI algorithm for learning PAGs. 

**Definition 4.3**.: A distribution \(p\) is said to be \(k\)-faithful to a causal graph \(D=(V,E)\), iff \((a\!\!\! b\,|c)_{p}\) implies \((a\!\!\! b\,|c)_{D}\) for all \(c V:|c| k\).

**Theorem 4.4**.: \(k\)_-PC algorithm is sound for learning \(k\)-essential graph given a conditional independence oracle under the causal Markov and \(k\)-faithfulness assumptions, i.e., if \(k\)-PC returns \(K\), we have \(_{k}(D) K(_{k}(D))\)_

Proof Sketch.: From Corollary 4.2, \(K\) obtained at the end of Step \(4\) is \((_{k}(D))\) and by Lemma 3.18 we know \(_{k}(D)(_{k}(D))\), i.e., every edge and tail orientation of \(K\) is consistent with \(_{k}(D)\). Thus, we only need to show that orientation rules \(11\), \(12\) are sound, i.e., there exists no \(k\)-closure graph in the Markov equivalence class that is inconsistent with these orientation rules. By Lemma A.2, we know that if \(a x\) for some \(x\), then conditioned on any subset of size \(k\), there exists a d-connecting path that starts with arrow at \(a\) and \(x\). This means that, irrespective of \(k\), \(a\) must have some incoming edge. The rules use this fact together with the fact that if there were two incoming edges from two non-adjacent neighbors, this would create an unshielded collider, which would change the Markov equivalence class. Please see Section A.8 for the proof. 

For two sample runs of the algorithm, please see Figure 5 and Figure 6 in Section C of Appendix. Note also that our algorithm can be seen as an improved version of AnytimeFCI algorithm  for causally sufficient systems, which the author shows can be stopped once every conditioning set of size at most \(k\) are tested. While FCI aims at learning arbitrary PAGs partially, \(k\)-PC learns \(k\)-closure graphs for causally sufficient systems. This allows extracting more causal information, evidenced by the additional orientation rules employed by \(k\)-PC. For more details on the differences and an example where AnytimeFCI is less informative than \(k\)-PC, please see Section D.7.

**Computational Complexity.** Suppose we are given two causal graphs. Our characterization gives an algorithmic way of testing \(k\)-Markov equivalence: Construct the \(k\)-closure graphs given the two DAGs and check whether they are Markov equivalent. The step to make two variables adjacent requires one to loop through all conditioning sets of size at most \(k\). This would take \((n^{k})\). This is the main time-consuming step. Afterward, one can test Markov equivalence using the existingapproaches. For example,  show that this can be done in \((ne^{2}+n^{2}e)\) time, where \(e\) is the number of edges. Thus, the overall algorithm will indeed be polynomial-time when \(k=(1)\).

The complexity of the learning algorithm, \(k\)-PC, will be similar to Anytime FCI, an early-stopped version of FCI . Although this is more complicated and would depend on other parameters in the graph, such as primitive inducing paths, and thus the number and location of unobserved confounders, we can also roughly bound this by \((n^{k+2})\) since for any pair, we will not be searching for separating sets beyond the \((n^{k})\) subsets of size at most \(k\). Further runtime improvements, such as RFCI by  might be possible, but this requires a further understanding of the structure of \(k\)-closure graphs.

## 5 Experiments

### Synthetic Data

In this section, we test our algorithm against the stable version of PC algorithm in causal-learn package. We randomly sample DAGs using the pyAgrum package. Variation of this experiment with a different sparsity level can be found in Section E.3 and gives similar results. All variables are binary with conditional probability tables filled randomly uniformly from the corresponding dimensional probability simplex, except the linear SCM experiments for comparing with NOTEARS. We observed similar results with a larger number of states, which are not reported. We report the \(F_{1}\) scores for identifying arrowheads, tails and skeleton correctly. The results are given in Figure 3. We observe that \(k\)-PC provides significant improvement to the arrowhead \(F_{1}\) score in the small-sample regime at little cost to the tail score. \(k\)-PC also helps improve skeleton discovery. Similar improvements are seen even for \(10\) samples whereas the advantage disappears for more than \(500\) samples (see Section E.3) since PC becomes reliable enough and \(k\)-PC does not use some of the informative CI tests in that high-sample regime. We report combined metrics such as the sum of all \(F_{1}\) scores in Section E.4.

In Section E.5, we compare with the conservative version of PC  and observe similar results. In Section E.2 we generate linear SCMs randomly and compare the performance of our algorithm with NOTEARS in addition to PC. Even though NOTEARS performs slightly better than PC in general, \(k\)-PC maintains an advantage over both in the small-sample regime. In Section E.1, we compare our algorithm with LOCI . As expected, the two algorithms perform similarly. We observe that our algorithm shows better arrowhead and skeleton \(F_{1}\) score performance. As a side note, we show that

Figure 3: Empirical cumulative distribution function of various \(F_{1}\) scores on \(100\) random DAGs on \(10\) nodes. For each DAG, conditional probability tables are independently and uniformly randomly filled from the corresponding probability simplex. Three datasets are sampled per model instance. The lower the curve the better. The maximum number of edges is \(15\). \(N\) is the number of data samples.

\(k\)-PC output is at least as informative as LOCI in Section D.6. See Section D for further discussions. The Python code is provided at [https://github.com/CausalML-Lab/kPC](https://github.com/CausalML-Lab/kPC).

### Semi-synthetic Data

We test our algorithm on the semi-synthetic _Asia_ dataset from bnlearn repository and compare it with PC algorithm. The dataset contains \(8\) binary variables. We randomly sample 500 datapoints from the observational distribution and run PC as well as \(k\)-PC for \(k=0\) and \(k=1\). \(k\)-PC for \(k=0\) correctly gets some of the causal and ancestral relations (Figure 4). PC, on the other hand, recovers a very sparse graph as it utilizes unreliable conditional independence tests with conditioning size of \(2\). \(k\)-PC with \(k=1\) recovers a graph in-between the two, still more informative about the structure than PC.

## 6 Discussion

A limitation of the method is that it assumes all independence statements up to degree \(k\) can be tested for some \(k\). In some cases, the set of available, or the set of reliable CI statements might not have such a structure. Our method is not directly applicable in such scenarios. Another limitation is that we assume that we can run CI tests for the tests that are deemed reliable. This is a non-trivial problem when the data is non IID, such as time-series data. We also make some other usual assumptions that are commonly made in causal discovery, such as acyclicity.

For more remarks, such as a demonstration of incompleteness of \(k\)-PC, please see Section D.

## 7 Conclusion

We proposed a new notion of equivalence between causal graphs called the \(k\)-Markov equivalence. This new equivalence allows us to learn parts of the causal structure from data without relying on CI tests with large conditioning sets that tend to be unreliable. We showed that our learning algorithm is sound, and demonstrated that it can help correct some errors induced by other algorithms in practice.