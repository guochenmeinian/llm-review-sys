# VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks

VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks

Jiannan Wu\({}^{2,1}\), Muyan Zhong\({}^{*3}\), Sen Xing\({}^{*3}\), Zeqiang Lai\({}^{*4}\), Zhaoyang Liu\({}^{*5,1}\), Zhe Chen\({}^{*6,1}\), Wenhai Wang\({}^{*7,1}\), \(^{3,8,1}\), \(^{8,1}\), Tong Lu\({}^{6}\), \(\) Luo\({}^{2}\), Yu Qiao\({}^{1}\), \(^{13,1}\)

\({}^{1}\)OpenGVLab, Shanghai AI Laboratory \({}^{2}\)The University of Hong Kong \({}^{3}\)Tsinghua University

\({}^{4}\)Beijing Institute of Technology \({}^{5}\)The Hong Kong University of Science and Technology

\({}^{6}\)Nanjing University \({}^{7}\)The Chinese University of Hong Kong \({}^{8}\)SenseTime Research

###### Abstract

We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope. It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing. To this end, we propose a new information transmission mechanism termed "super link", as a medium to connect MLLM with task-specific decoders. It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios. In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks. In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models. We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.

## 1 Introduction

Multimodal large language models (MLLMs)  have recently made significant progress, demonstrating outstanding performance across various vision-language tasks, even in scenarios requiring complex understanding and reasoning. However, _a notable limitation is that current MLLM outputs are in text form, which significantly constrains their capacity to represent structured or visual information._ Some researchers  have expanded the text-based output formats of MLLMs to better align with downstream tasks. While these efforts have shown promise, they have not fully addressed practical needs such as dense object detection, pose estimation, and image generation.

To overcome this limitation, a line of research  enhances the capabilities of MLLMs by transmitting task information to tools via text messages, as illustrated in Figure 1(a). Despite these advances, these text-based methods are restricted by the information that text can convey. They are not end-to-end, and the feedback gradient from the tools cannot be relayed back to the MLLM. This limitation has spurred another research direction  that employs learnable embeddings as intermediaries to connect MLLM with one specific task decoder (see Figure 1(b)). However, the naive embedding connection is difficult to scale to multi-task scenarios. A routing mechanism is needed to ensure the correct selection of tools, and the issue of task conflicts  arising from joint multi-task training is also a problem that needs to be considered. Therefore, _developing an end-to-end MLLM generalist for various vision and vision-language tasks beyond text output remains a significant challenge._

Given these challenges, developing an end-to-end generalist MLLM requires a more effective information transmission method than conventional text messages and naive embeddings. This method should ensure that task information and feedback gradients are accurately and flexibly communicated between the central MLLM and multi-task decoders while preventing task conflicts across various visual domains and input/output formats. In addition, multi-task datasets for generalist MLLMs need to be well-prepared. Despite the abundance of annotations in the community, the diverse and inconsistent formats of these annotations across different tasks make it challenging to develop a unified dataset that effectively supports multi-task learning.

In this work, we introduce VisionLLM v2, an end-to-end generalist MLLM designed for a wide array of vision and vision-language tasks. This model not only performs typical visual question answering but also extends to image generation, image editing, and open-ended object detection/instance segmentation/pose estimation across diverse image domains. To facilitate information transmission between the MLLM and multiple downstream task decoders, we introduce the **super link** technique, which consists of two components: (1) _Routing Token_: special tokens (_e.g._, [DET], [POSE], and [GEN]) added to the MLLM's vocabulary. Whenever the MLLM predicts a specific routing token, it triggers the selection of the appropriate decoder. (2) _Super-Link Queries_ randomly initialized learnable weights bound to the routing tokens. These queries are appended after the routing tokens and processed by the MLLM to extract task-specific information, which is then sent to the target decoder. This method enables flexible task information transmission, allows decoder gradients to backpropagate to the MLLM, and avoids task conflicts by ensuring the queries are bound to routing tokens and not shared across tasks.

Furthermore, we carefully collected and curated training data from hundreds of public vision and vision-language tasks to support various tasks. The data includes high-quality examples of visual question answering, visual perception, recognition, and understanding tasks from various sources such as natural scenes, remote sensing images, medical images, and industrial images. To ensure effective training with these extensive datasets, we also implemented a multi-stage joint training strategy, integrating new abilities and reaching a performance comparable to the expert models while maintaining the MLLM's foundational VQA capabilities.

These designs endow VisionLLM v2 with three distinct characteristics: (1) _Generality_. With one suit of parameters, our model can be generalized to different tasks using different text and visual prompts. To our knowledge, it is the first end-to-end model to support hundreds of vision-language tasks while achieving performance comparable to expert models. (2) _Openness_. By employing open-ended decoders, our model allows users to freely define tasks through multimodal prompts, breaking away from the constraints of closed-set models limited to predefined tasks or categories. Furthermore,

Figure 1: **Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a “super link” technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.**

users can flexibly combine various tasks into more complex ones through multi-round dialogue. (3) _Multimodal In-Context Ability_. With multimodal inputs and outputs, our model demonstrates extensive versatility and exhibits superiority over the previous in-context models with single-modal outputs [184; 8]. These features distinguish our model from previous approaches, and establish a leading foundational MLLM for various vision and vision-language applications.

In summary, our main contributions are listed as follows:

(1) We propose VisionLLM v2, the first end-to-end generalist MLLM model to accomplish hundreds of vision and vision-language tasks1, covering visual perception, understanding, and generation. It not only addresses the limitation of LLMs being confined to text outputs but also supports using textual, visual, and in-context instructions to flexibly combine tasks for real-world applications.

(2) We introduce the super-link technique, which integrates the MLLM with task-specific decoders. This integration facilitates end-to-end optimization across both linguistic and visual tasks. Additionally, we meticulously collect and re-organize data from a broad range of domains and develop an in-context learning dataset. These efforts lay a solid foundation for our progressive joint training process and enable the model to benefit from individual tasks.

(3) We comprehensively evaluate the proposed model on a wide range of vision and vision-language tasks, from visual perception to visual understanding, from weak interaction (_e.g._, closed-set) to strong interaction (_e.g._, visual prompt + language prompt), from common-seen domains to long-tailed domains (_e.g._, medical, remote-sensing, industry), as shown in the rightmost subfigure of Figure 1. In addition, with a generalist model, our method achieves comparable performance with the task-specialized models in various standard benchmarks.

## 2 Related Work

### Multimodal Large Language Model

**Conventional MLLMs**. With the advancement of large language models (LLMs) [145; 146; 21; 215; 171; 37; 172; 13; 170; 9; 103; 7; 59; 43; 22], multimodal large language models (MLLMs) have also gained significant momentum recently. Notable commercial models include GPT-4V , Gemini series [169; 150], Claude-3 , and Qwen-VL-Max , known for their outstanding performance. Early open-source MLLMs like InstructBLIP , LLaVA  and MiniGPT-4  fine-tune on instruction-following datasets. InternVL [34; 33] series models align a large-scale vision encoder with LLMs and perform comparably to commercial models. Efficient MLLMs [100; 228; 38] have also studied. However, these models only can output text, restricting their applications.

**Extension of MLLMs' Text Output**. To extend MLLMs to downstream tasks, models like Kosmo-2 , Shikra , VisionLLM , Ferret [201; 212], and All-Seeing V2  achieve this using specially-designed tokens or encoding coordinates as text tokens. Despite these advancements, using LLMs solely as visual decoders falls short of resolving the fine-grained visual context needed for precise detection and segmentation. The other line of works focus on broadening the modality scope. AnyGPT  builds a multimodal text-centric dataset for any-to-any multimodal generation (text, image, speech, music) with sequence modeling. Chameleon  uses fully token-based representations for both texts and images, capable of understanding and generating interleaved image-text sequences. CM3leon [5; 205] are autoregressive models for text-to-image and image-to-text tasks. All these works could unify image understanding and generation in one network. Our model can support more vision and vision-language tasks.

**MLLMs w/ Downstream Tools**. Recent works [116; 187; 166; 111; 117; 47; 17; 191; 68; 48] have integrated external tools for vision-centric tasks, transmitting task information to these tools via text messages. However, such text-based communication between LLMs and tools hinders end-to-end optimization. Another category of approaches [89; 148; 218; 83; 164; 163; 53; 54; 136; 44; 50; 69] feeds the output embeddings of LLMs into a special decoder and trains them end-to-end to enhance information communication. However, they only support semantic segmentation or image generation tasks. In this work, we target to develop an end-to-end MLLM generalist for diverse vision and vision-language tasks beyond text output.

### Vision Generalist Model

**Unified Vision Model.** The unified model approach integrates multiple visual tasks into a single framework, enhancing efficiency and reducing the complexity of deploying separate models for each task. Works such as Pix2Seq-D , SEEM , and Semantic-SAM  focus on unifying the segmentation interface, achieving promising results. Grounding-DINO  and VisionLLM  explore open-set detection grounded by language, while UniPose  excels in pose estimation. Additionally, pioneering works [227; 224; 95; 121; 229; 189] aim to design a unified model capable of solving multiple tasks, including detection, segmentation, captioning, _etc_. Their results demonstrate the feasibility of a single model performing diverse tasks.

**Visual Prompting.** Visual prompting has emerged as a novel paradigm by providing visual marks in the input instruction. It requires the model to pay attention to the specific region on the image when answering the question. Techniques like red circle , SoM , AutoVP , ILM-VP , and PIVOT  significantly reduce the need for textual prompt engineering, assisting models in focusing on relevant visual content. Similar to in-context learning in LLMs, Painter , DINO v2 , and SegGPT  leverage visual context to improve learning efficiency and adaptability, enabling models to adapt to new tasks with minimal input.

**Diffusion Model as Interface.** Diffusion models are a flexible interface between users and visual tasks, facilitating a more intuitive interaction paradigm. InstructCV  and InstructDiffusion  exemplify using of natural language instructions to guide visual generation and manipulation. Pix2Seq v2  showcases the potential of diffusion models in generating sequences of visual tokens, bridging the gap between vision and language.

Different from these works, our VisionLLM v2 integrating LLMs extends vision generalist to support a broader range of vision-language tasks and explore various visual prompting paradigms, thereby significantly broadening the scope of application.

## 3 VisionLLM v2

### Model Design

The overall architecture of VisionLLM v2 is depicted in Figure 2. It mainly consists of four parts: (1) an image encoder and a region encoder that encode the image-level and region-level information; (2) a large language model (LLM) that models the multimodal inputs and generates satisfactory textual responses; (3) a series of task-specific decoders for performing downstream tasks; (4) a super link that uses routing tokens and super-link queries for efficient and conflict-free information transmission. We detail each component in the following.

**Tokenization.** VisionLLM v2 is flexible for handling multimodal input. (1) _For text prompts_, we employ the text tokenizer to tokenize them into distinct vocabulary indices, which can be further processed by LLM and result in the text features \(F_{}^{L C}\), where \(L\) denotes the length of input text, and \(C\) is the channel dimension of LLM.

(2) _For an image input_, we utilize a pre-trained vision foundation model, such as CLIP , to extract image features. Recognizing that current vision models operate the images at a low resolution, we adopt the dynamic resolution approach  to process the input images. Specifically, the input image is first automatically matched to an optimal aspect ratio from a predefined ratio set. Subsequently, the image is scaled up to a higher resolution based on the selected aspect ratio and divided into \(P\) square patches, each whose resolutions are 336\(\)336. These local patches, along with a 336\(\)336 global image \(I_{}\), are processed by the image encoder to capture both holistic scenes and fine-grained details, resulting in image features \(F_{}^{576(P+1) C}\).

(3) _For a visual prompt_, we employ binary masks to flexibly represent the visual prompts, such as point, box, scribble, and mask. To extract the region embedding, we first concatenate the binary mask with the input image along the channel dimension and then process it with three convolutional layers to downsample by a factor of 14 (see appendix for more details). We further augment this feature map by adding the feature map of the global image \(I_{}\). Finally, grid sampling is used to extract features within the masked regions, and these features are averaged to form the features of the visual prompt \(F_{}^{1 C}\).

**Large Language Model.** Following previous works [107; 213; 60], both the images and visual prompts are projected to the feature space of the LLM. The LLM plays a central role in our modeland is used to model multimodal inputs, parse user instructions, and generate appropriate responses. In this work, we adopt the commonly used Vicuna-7B  as the LLM in our network.

**Task-specific Decoders.** To enhance the capacities of MLLM, we equip our model with several task-specific decoders. Specifically, we use Grounding DINO  for object-level localization. We additionally add a mask decoder upon it to obtain the segmentation ability. For pose estimation, we adopt UniPose  as the keypoint decoder. Moreover, we incorporate Stable Diffusion  and InstructPix2Pix  as the image decoders, endowing our model with the capability to generate and edit images. We discard these decoders' text encoders and link them with MLLM via the super link technique, which will be detailed explained in Section 3.2. In this way, the decoders can be trained end-to-end with the entire network, ensuring the effective transmission of task information and increasing the openness of these decoders.

### Super Link Technique

For the text-only output tasks, such as image-level and region-level VQA, we directly take the plain text generated by LLM as the final output. For visual perception and visual generation tasks, we propose the super link technique to tackle the challenge of selecting the appropriate decoder, avoiding task conflicts, and facilitating effective information transmission between the LLM and the decoders. The super link comprises two parts:

(1) _Routing Token._ We add the routing tokens, _e.g.,_ [DET], [POSE], [SEG], [GEN], [EDIT], as special tokens to the original LLM vocabulary. When the model intends to complete the downstream task using one of the decoders, LLM would include the corresponding routing token in its textual response. To enable the model to discern which tasks to perform and which routing tokens to output, we construct a series of instruction templates for different tasks using ChatGPT .

(2) _Super-Link Queries_. For each decoder, we define the super-link queries as a fixed set of embeddings denoted as \(Q_{}^{N C}\), where \(N\) is the number of queries. They are randomly initialized and serve as the bridge between LLM and task-specific decoders. Whenever the LLM predicts the routing token, the super-link queries would be automatically appended after the input embeddings of the routing token. We then extract their corresponding last-layer hidden states \(H_{}\) and apply an MLP projection to obtain \(_{}\). Finally, \(_{}\) is sent into the specific decoders as a condition to perform the downstream tasks. In the following, we illustrate how to integrate \(_{}\) into decoders for visual perception and generation, respectively.

**Visual Perception** covers a wide range of visual tasks, such as open-ended/closed-set object detection, instance segmentation, pose estimation, _etc._ VisionLLM v2 supports using both text and visual prompts to define these tasks. We list an example in the following figure. <image> and <region> are the placeholders that will be replaced by image and region embeddings before being fed into the LLM. Here, we take Example 1 of interactive segmentation for clarification. The user prompts the model to segment specific regions within a question. MLLM sequentially lists the region names followed by a routing token [SEG] in the response. Remember that the proposed method would automatically append the super-link queries after the routing token. In that way, we can obtain the

Figure 2: **Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.**

per-region representations by extracting the output hidden states of MLLM from corresponding super-link queries and pooling them into one embedding. These embeddings are fed into a segmentation decoder as the conditional feature, requiring only a single forward to produce segmentation results for all regions. In the following, we show a template example for interactive segmentation.

``` Example1: Text Prompt + Visual Prompt for Interactive Segmentation. USER: <image> Could you please segment all the corresponding objects according to the visual prompts as region1 <region>, region2 <region>? ASSISTANT: Sure, these objects are region1 [SEG], region2 [SEG]. ```

**Visual Generation** is also a wide topic covering a number of different tasks, such as generation, editing, variation, personalization, _etc_. In VisionLLM v2, we focus on two fundamental tasks, _i.e._, text-to-image generation and instruction-based image editing. We use Stable Diffusion v1.5 (SD) as our tool in the text-to-image generation task. We abandon its text encoder and use the output hidden states of the MLLM as the image generation condition for SD. Image editing task  can also be accomplished in the same paradigm by using both image and text prompts as inputs. In the following, we list a template example for text-to-image generation.

``` Example2: Text Prompt for Text-to-Image Generation. ASSISTANT: Of course, here it is [GEN]. ```

_Discussion._ Some previous works have used the special token or learnable queries independently. InstructBLIP , ep-ALM , and MAPL  use learnable queries (i.e., soft prompts) to connect the modality encoders and LLM. FROMAGe  uses a special token for image-text retrieval so as to handle multimodal outputs, where the images are not generated from the network end-to-end. However, these works still remain constrained to text-based outputs. The proposed super link is the seamless integration of the two techniques. Despite the simplicity of our method, it is able to extend MLLMs to handle hundreds of tasks by largely extending the output formats, _e.g._, box, mask, keypoint and image. Meanwhile, it can address several challenges when scaling up various tasks: (i) precise decoder invocation, (ii) mitigating task conflicts and (iii) efficient message transmission in an end-to-end manner.

### Training Strategy

Current MLLMs  face reduced conversational abilities when augmented with additional capacities. To create a generalist model capable of handling hundreds of tasks without compromising vision understanding, we propose a three-stage training strategy, where the first stage focuses on building an MLLM with strong image-level and region-level vision understanding. In the subsequent stages, we add task-specific decoders and continue training to equip the model with advanced capabilities.

**Stage-1: Multimodal Training.** In the first stage, we follow the training settings of LLaVA , comprising pre-training and instruction tuning phases. The pre-training phase aims to establish the image-level and region-level vision-language alignment, where only the region encoder and the projections for image embedding and region embedding are trained for efficiency. The instruction tuning phase unfreezes the LLM and trains the model on a wide range of high-quality instruction data. After the training in this stage, we can obtain a strong MLLM with excellent conversation ability, which we term as **VisionLLM v2-Chat**.

**Stage-2: Multi-capacity Fine-tuning.** At this stage, we integrate task-specific decoders into the model and perform multi-task joint training. In addition to the instruction data utilized in stage-1, we incorporate extensive visual datasets such as COCO , ADE20K  for their specific tasks. We construct a series of instruction templates for these visual datasets to perform instruction tuning, ensuring that the LLM can accurately invoke the downstream decoders. During this stage, the region encoder and all decoders undergo training, and we only finetune the input and output embeddings of the LLM to maximally preserve its original conversational ability.

**Stage-3: Decoder-only Fine-tuning.** Since the decoders cannot converge within a single epoch, we further train the decoders for 12 epochs using visual datasets while freezing all other components. It is noted that the super-link queries continue to be trained during this stage. After finishing the three-stage training, our model has diverse capacities for visual tasks while maintaining effectiveness in global vision understanding, named **VisionLLM v2**.

## 4 Experiments

### Implementation Details

**Dataset Details.** To support the joint training of our model, we meticulously collect and re-organize the datasets across a wide range of tasks from publicly available sources. For the first stage training, we utilize a substantial amount of high-quality instruction data for both image-level and region-level visual question answering, including ShareGPT4V , All-Seeing , VQAv2 , _etc._ In the last two stages, we further incorporate extensive visual datasets, _e.g._, COCO , RefCOCO/+/g , LIAION-Aesthetics , to enhance our model with numerous capacities. These datasets encompass multiple tasks such as object detection, pose estimation, image generation, and span various domains such as natural scenes, remote sensing images, medical images, _etc._ To facilitate the training of diverse datasets in our MLLM framework, we construct a series of instruction templates for different tasks, which are completely listed in the Appendix. Additionally, we also collect a multimodal dataset termed **MMIC** focusing on visual prompting and in-context learning. The data in our MMIC comes from various sources, including fine-grained visual recognition, object detection, instance segmentation, and pose detection. We elaborate on all datasets used in this work as well as the dataset construction of MMIC in the Appendix.

**Model Details.** We adopt the CLIP-L/14  as the image encoder and Vicuna-7B-v1.5  as the language model. Grounding-DINO  and UniPose  are selected as object decoder and keypoint decoder, respectively. And for these two decoders, we experiment with Swin-T  backbone. Additionally, image decoders are kept as Stable Diffusion v1.5  for image generation and InstructPix2Pix  for image editing. All these components load the pre-trained weights while the region encoder is randomly initialized. For visual perception and visual generation tasks, the number \(N\) of super-link queries is set to 4 and 64, respectively. During training, we adjust the dataloader so that each GPU processes samples from only one dataset. More training details are provided in the Appendix.

In the following subsections, we present the experimental results to cover as many tasks, interactive modes, and domains. It is noted that all the results of our method are reported using **a single generalist model** with the same parameters. More results can be found in the Appendix.

### Multimodal Benchmarks

**Multimodal Dialogue.** We first evaluate our models on academic-oriented VQA datasets and recent instruction-following datasets for MLLMs, as presented in Table 1. The results clearly demonstrate that our models outperform previous methods under the same parameter scale, particularly on the instruction-following datasets. For instance, VisionLLM v2-Chat surpasses LLaVA-NeXT-7B  by +9.7 and +7.0 points on MMBench-EN/CN , respectively. Additionally, we find that VisionLLM v2 achieves comparable performance to VisionLLM v2-Chat on these multimodal benchmarks and even performs better on some benchmarks, such as POPE , a popular benchmark for evaluating object hallucination. This phenomenon indicates that our framework effectively mitigates the issue of multi-task conflict and maintains proficiency in conversational ability.

  &  &  &  &  &  &  &  &  \\  & encoder & LLM & & & & & & & & & & \\  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  \\  InstructBlIP-7B  & EVA-g & Vicuna-7B & - & 49.2 & 34.5 & 60.5 & 50.1 & - & - & 36.0/2.7 & 53.4/ & 58.8/ 38.1 \\ InstructBlIP-13B  & EVA-g & Vicuna-13B & - & 49.5 & 33.4 & 63.1 & 50.7 & 78.9 & 1212.8 & - & -/ -/- \\ Shlrx  & CILP-1 & Vicuna-13B & 77.4 & - & - & - & - & - & - & 58.8/ - - & -/- \\ DEFICKS-80B  & CILP-1 & LLP-1 & LLP-3B & 68.0 & 45.2 & 36.0 & - & 30.9 & - & - & 54.5/ 38.1 & -/- 53.2/ - \\ Open-VL-Chat  & CILP-Q & Q-even-7B & 78.2 & 75.7 & 57.5 & 38.9 & 68.2 & 61.5 & - & 14875.6 & 60.6/ 56.7 & 58.2/ 65.4/ 37.8 \\ InstructVL-7B  & V7+6B & Vicuna-7B & 79.3 & 62.9 & 52.5 & 66.2 & 57.0 & 86.4 & 15251. & 64.6/ 57.6 & 60.2/ -/- \\ InstructVL-13B  & ViT-6B & Vicuna-13B & 80.2 & 63.9 & 54.6 & 70.1 & 58.7 & 87.1 & 1546.9 & 66.6/ 61.9 & 62.4/ -/- \\ LLaVA-15-7B  & CILP-1 & Vicuna-7B & 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 85.9 & 1510.7 & 64.3/ 58.3 & 58.6/ 66.1/ 37.3 \\ LLaVA-NeXT-7B  & CILP-1 & Vicuna-7B & 81.8 & 64.2 & 57.6 & 70.1 & 64.9 & 86.5 & 1519.0 & 67.4/ 60.6 & -/ 70.2/ - \\ LLaVA-NeXT-13B  & CILP-1 & Vicuna-13B & 82.8 & 65.4 & 60.5 & 73.6 & 67.1 & 86.2 & 1575.0 & 70.0/ 64.4 & -/ 71.9/ - \\ VisionLLM v2-Chat & CLIP-L & Vicuna-7B & 81.4 & 65.1 & 54.6 & 94.4 & 66.3 & 87.5 & 1515.2 & 77.1/ 67.6 & 65.4/ 71.7/ 41.6 \\ VisionLLM v2 & CLIP-L & Vicuna-7B & 80.8 & 65.1 & 54.8 & 94.2 & 64.7 & 88.8 & 1495.6 & 76.3/ 66.8 & 65.6/ 71.7/ 42.2 \\ 

Table 1: **Comparison with SoTA models on multimodal dialogue benchmarks.** The academic-oriented datasets include: VQAv2 test-dev , GQA test-balanced , VizWiz test-dev , ScienceQA test  and TextVQA val . The instruction-following datasets include: POPE , MME , MMBench-EN/CN , SEED-Bench (all/image/video) . \({}^{*}\)The training annotations of the dataset are observed during training.

**Region Recognition.** The region recognition task needs the model to identify the object category given the ground-truth bounding box. We compare our method with both feature-based and text-output approaches in Table 2(a). Feature-based methods, such as RegionCLIP  and ASM , compute similarity scores between region visual features and candidate category text features. In contrast, text-output methods  directly predict the category name using a single word or phrase, embracing the advantage of openness. As shown in the table, our models demonstrate the significant superior performance on COCO , long-tail LVIS  and part-level PACO .

**Visual Commonsense Reasoning.** Visual commonsense reasoning (VCR) requires the model to possess strong region-level question-answering and reasoning abilities, as it needs to select not only the correct answer but also the correct rationale behind it. We present the comparison results on the VCR dataset  in Table 2(b). Without task-specific fine-tuning, VisionLLM v2-Chat achieves an accuracy of 82.9% in the crucial Q\(\)AR task, which precedes the previous best model, ASMv2 , by +3.5 points. VisionLLM v2 also outperforms the previous methods for all the metrics, highlighting the promising common sense reasoning capability of our model.

### Visual Perception Tasks

**Object Detection and Instance Segmentation.** In Table 3, we compare the results of VisionLLM v2 with state-of-the-art methods on two fundamental vision tasks, _i.e._, object detection, and instance segmentation. As can be seen, using the lightweight backbone Swin-T, our generalist model achieves

 method &  \\   &  &  &  &  \\  ViLBERT  & 72.4 & 74.5 & 54.0 \\ Unicycle-V & 72.6 & 74.5 & 54.5 \\ VLBERT  & 75.5 & 77.9 & 58.9 \\ ERNIE-VL-L  & 78.5 & 83.4 & 65.8 \\ VILLA  & 78.5 & 82.6 & 65.2 \\ GPTRad-78  & 87.4 & 89.6 & 78.6 \\ ASMv2  & 87.8 & 88.8 & 78.4 \\ ASMv2  & 88.4 & 89.9 & 79.4 \\  VisionLLM v2-Chat & 90.0 & 91.9 & 82.9 \\ VisionLLM v2 & 89.8 & 91.7 & 82.5 \\ 

Table 4: **Comparison of pose estimation performance.**\({}^{*}\) indicates that the results rely on ground-truth bounding boxes for top-down methods.

 method &  &  &  \\  &  &  &  &  &  &  \\  CLIP  & 58.9 & - & - & - & - & - & - & - \\ RegionCLIP  & 58.3 & - & - & - & - & - & - & - \\ LLVA  & - & 40.0 & 49.9 & 19.8 & 42.2 & 14.6 \\ Shixra  & - & 53.9 & 49.7 & 19.8 & 43.6 & 11.4 \\ GPTRad  & - & 64.0 & 51.3 & 12.0 & 48.0 & 12.1 \\ ASM  & 69.3 & - & - & - & - & - & - \\ RegionGPT  & 70.0 & 80.6 & - & - & - & - & - \\ Ospecy  & - & - & - & 65.2 & 38.2 & 73.1 & 52.7 \\  VisionLLM v2-Chat & 81.9 & 90.5 & 67.3 & 42.7 & 63.8 & 36.3 \\ VisionLLM v2 & 81.9 & 90.4 & 73.0 & 51.3 & 70.9 & 47.6 \\ 

Table 2: **Comparison of region recognition and visual commonsense reasoning performance.** (a) SS and S-IoU represent semantic similarity and semantic IoU, which originated from . (b) Q, A, and R denote question, answer, and rationale, respectively. X\(\)Y means that the model needs to select option Y conditioned on X. \({}^{*}\)The model is finetuned on the dataset.

 method & type & backbone &  & instance seg. (COCO) &  \\  & & & AP & \(\%\) & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{50}\) & mMRL & Recall \\  Deformable-DETR  & ResNet50 & 46.2 & 65.2 & 50.0 & - & - & - & 89.1 & 50.0 & 95.3 \\ DDQ  & ResNet50 & 52.0 & 69.5 & 57.2 & - & - & 93.8 & 39.7 & 98.7 \\ ViTPC-B  & Specialist & VIT-B & 56.0 & - & - & 48.0 & - & - & - & - \\ Grounding DINO  & Swin-T & 57.2 & - & - & - & - & - & - & - \\ Mask2Former  & ResNet50 & - & - & 43.7 & - & - & - & - & - \\ Mask DINO  & ResNet50 & 51.7 & - & - & 46.3 & - & - & - & - \\  UniFGV  & ViT-B & - & - & - & - & - & - & 92.5 & - & - \\ Hulk  & ViT-L & - & - & - & - & - & - & 92.2 & - & - \\ Huk  & ViT-L & - & - & - & - & - & - & 93.0 & - & - \\ Pix2Seq \(\)Y  & ViT-B & 46.5 & - & 38.2 & - & - & - & - & - \\ VisionLLM  & ResNet50 & 44.8 & 64.1 & 48.5 & 25.2 & 50.6 & 22.4 & - & - & - \\ Uni-Perceiver-V2  & Swin-B & 58.6 & - & - & 50.6 & - & - & - & - & - \\ UNINET  & ResResNet50 & 51.3 & 68.4 & 56.2 & 44.9 & 67.0 & 48.9 & - & - \\ GLEE-Lite  & ResNet50 & 55.0 & - & - & 48.4 & - & - & - & - & - \\ GLEE-Plus  & Swin-T & 60.4 & - & 53.0 & - & - & - & - & - \\ VisionLLM v2 & Swin-T & 56.7 & 74.5 & 62.2 & 47.8 & 71.8 & 52.0 & 93.1 & 44.7 & 98.5 \\ 

Table 3: **Comparison of object detection and instance segmentation performance.** Instance seg. means instance segmentation segmentation. \({}^{*}\)The model is finetuned on the dataset.

[MISSING_PAGE_FAIL:9]

**Shared _vs._Unshared Super-Link Queries for Different Decoders.** To determine if one set of super-link queries is sufficient for all decoders, we conducted an ablation study by either using shared queries for all decoders or defining separate queries for each decoder. In this ablation, we only train the decoders and super-link queries while freezing all other components as the training setting of stage-3. In Figure 4, we plot the performance of box AP (using the object decoder) and keypoint AP (using the keypoint decoder) on COCO. We observe that the keypoint AP would decrease over training when using shared queries, which may be attributed to the fact that most data are used for object decoder. Besides, the box AP with shared queries is also inferior to decoupled ones. Therefore, we define separate super-link queries for each decoder in our model.

**Multi-Task Influence.** As indicated by previous works [225; 206], different tasks with shared parameters may cause conflict with each other. This is mainly due to inconsistent optimization in multi-task learning. To investigate the mutual influence of multi-task joint training in our framework, we start from the same checkpoint and train the model on a single task (image VQA, instance segmentation, or image generation) for 1000 iterations. We record the loss change for all three tasks in Table 6. In the table, a decrease in the loss value indicates beneficial training for the task, while an increase is detrimental. We can observe that training on image VQA is advantageous for all three tasks, which is reasonable as the conversation ability of MLLM is enhanced. Whereas training exclusively on instance segmentation or image generation leads to conflicts with other tasks. This aligns with the findings in Unipercirever-MoE .

**One-Stage _vs._Three-Stage Training.** Some previous generalist models [176; 15] train the model in one stage. Our model encompasses much more tasks and thus introduces a training conflict: the MLLM requires only 1 epoch of training on chat data to prevent overfitting, whereas the decoders need longer training epochs (e.g., Grounding-DINO need 12 epochs of training on visual data) to achieve convergence. One possible solution for one-stage training is to give a higher sample ratio for the visual data. In the following, we conduct the ablation to study the effect of one-stage v.s. three-stage training. We use image-level chat data, COCO, and COCO-Pose for image understanding, instance segmentation, and pose estimation, respectively. For one-stage training, we repeat the COCO and COCO-Pose datasets 12 times. As can be seen from Table 7, the conversation ability of the model is significantly decreased due to extreme data imbalance. And the performance of instance segmentation and pose estimation is also slightly reduced. These results prove the effectiveness of our three-stage training.

## 5 Conclusion & Limitation

In this paper, we presented VisionLLM v2, a comprehensive MLLM that unifies visual perception, understanding, and generation within a single framework. The proposed super link mechanism facilitates flexible information transmission between the MLLM and task-specific decoders, addressing training conflicts and enhancing gradient feedback. Experiments show that VisionLLM v2 achieves performance comparable to specialized models while maintaining broad applicability.

Regarding limitations, our model's training encompasses three stages, which are relatively complex. Moreover, the integration of downstream tools has only been preliminarily validated. Future work will further explore solutions to these issues, aiming to enhance the model's performance and efficiency.

**Broader Impact.** We envision that this work will further promote the fusion of visual and language tasks. In addition, since our work is built on open-source pre-trained vision foundation models and large language models, requiring low training resources, thus reducing the carbon footprint. We do not foresee obvious undesirable ethical/social impacts at this moment.

 Train / Test & Image VQA & Inst. Seg & Image Gen. \\  Image VQA & -0.01 & -0.11 & -0.04 \\ Inst Seg. & +0.04 & -0.12 & + 0.19 \\ Image Gen. & +0.03 & +0.02 & -0.04 \\ 

Table 6: **Ablation on the multi-task influence.** The numbers denote the loss change when the model is fine-tuned on a single task.

  & TextVQA & MME & MMB EN/CN & COCO & COCO-Pose \\  One-stage & 53.2 & 1284.4 & 61.9 / 51.4 & 54.9 / 44.6 & 74.1 \\ Three-stage & 66.2 & 1507.1 & 77.8 / 68.5 & 56.3 / 47.6 & 74.2 \\ 

Table 7: **Ablation on the one-stage and three-stage training.** We evaluate the models on image VQA, instance segmentation and pose estimation