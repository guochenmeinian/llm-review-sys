# Fantastic Robustness Measures:

The Secrets of Robust Generalization

 Hoki Kim

Seoul National University

ghr19613@snu.ac.kr

Jinseong Park

Seoul National University

jinseong@snu.ac.kr

Yujin Choi

Seoul National University

uznhigh@snu.ac.kr

Jaewook Lee

Seoul National University

jaewook@snu.ac.kr

###### Abstract

Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the \(L_{}\) norm and further validate our findings through an evaluation of more than 100 models from RobustBench  across CIFAR-10, CIFAR-100, and ImageNet. We hope this work can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.

## 1 Introduction

Deep neural networks have achieved tremendous success in various domains, but their vulnerability to subtle perturbations has been revealed through the existence of adversarial examples, which are not generally perceptible to human beings . To obtain robustness against these adversarial examples, numerous defense methods have been proposed, and among them, adversarial training has become a common algorithm because of its effectiveness and ease of implementation . However, researchers have recently found that adversarial training methods also suffer from the problem of overfitting , where an adversarially trained model shows high robustness on training examples, yet significantly reduced robustness on test examples. As this robust overfitting progresses, the robust generalization gap increases, resulting in poor robustness for unseen examples.

To prevent robust overfitting and achieve high robust generalization, researchers have analyzed the properties of adversarial training and demonstrated the usefulness of some measures, such as margin-based measures, flatness-based measures, and gradient-based measures . Researchers have used them to estimate the robust generalization gap of given models and further developed new training schemes for improving its robustness . While these measures offer significant insights into robust generalization, we find that the evaluation of some measures is often limited due to the lack of models or training setups. These limitations can potentially lead to misleading conclusions, which may include inaccurate estimations of the robust generalization gap and misguided directions for the further development of adversarial training methods.

Therefore, to gain a more precise understanding of when and how these measures correlate with robust generalization, we train over 1,300 models on CIFAR-10 under the \(L_{}\) norm across various training settings. We then investigate the relationships between a wide range of measures and their robust generalization gap. To further validate our findings, we also analyze over 100 models provided in RobustBench  across CIFAR-10, CIFAR-100, and ImageNet. Based on our large-scale study, we summarize our key findings as follows:

**Key findings.**

1. Due to the high sensitivity of the robust generalization gap to different training setups, the expectation of rank correlation across a wide range of training setups leads to high variance and may not capture the underlying trend.
2. Margin and smoothness exhibit significant negative correlations with the robust generalization gap. This suggests that, beyond a certain threshold, maximizing margin and minimizing smoothness can lead to a degradation in robust generalization performance.
3. Flatness-based measures, such as estimated sharpness, tend to exhibit poor correlations with the robust generalization gap. Rather, contrary to conventional assumptions, models with sharper minima can actually result in a lower robust generalization gap.
4. The norm of the input gradients consistently and effectively captures the robust generalization gap, even across diverse conditions, including fixed training methods and when conditioned on average cross-entropy.

To promote reproducibility and transparency in the field of deep learning, we have integrated the code used in this paper, along with pre-trained models, accessible to the public at https://github.com/Harry24k/MAIR. We hope that our findings and codes can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.

## 2 Related Work

The primary distinction between standard and adversarial training is that adversarial training aims to correctly classify not only benign examples but also adversarial examples as follows:

\[_{}_{\|^{adv}-\|}(f(^{ adv},),y),\] (1)

where \((,y)\) is drawn from the training dataset \(\) and \(f\) represents the model with trainable parameters \(\). Among the distance metrics \(\|\|\), in this paper, we focus on robustness with respect to the \(L_{}\) norm. Note that the loss function \(\) can also include \(f(,)\) to minimize the loss with respect to \(\). By optimizing (1), we hope to minimize the robust error on the true distribution \(\), defined as:

\[(;,)=_{(,y)}[_{\|^{adv}-\|}(f(^{adv},) y)],\] (2)

where \(( y)\) is an indicator function that outputs \(0\) if the prediction \(\) is same as the true label \(y\), and \(1\) otherwise.

The majority of researches have focused on optimizing (1) through the development of new loss functions or training attacks. For instance, vanilla adversarial training (AT)  minimizes the loss of adversarial examples generated by projected gradient descent (PGD)  with multiple iterations. Following AT, several variations, such as TRADES  and MART , have achieved significant reductions in robust errors on various datasets through the adoption of KL divergence and the regularization on probability margins, based on theoretical and empirical analyses.

However, recent works have revealed that the adversarial training framework has a challenging generalization problem, characterized by higher Rademacher complexity  and larger sample complexity . The overfitting problem in adversarial training has been observed as a common phenomenon across various settings , and it can even occur in a more catastrophic manner during single-step adversarial training [63; 31]. As robust overfitting progresses, the following robust generalization gap \(g()\) increases,

\[g()=(;,)-(; ,).\] (3)Thus, in order to minimize \((;,)\), we should not only focus on minimizing the training objective function \((;,)\) but also on reducing the robust generalization gap \(g()\).

To gain a deeper understanding of the robust overfitting and further reduce the robust generalization gap \(g()\), a line of work has theoretically and empirically investigated measures, such as boundary thickness , local Lipschitzness , and flatness . While these studies claim that these measures are reliable indicators of the robust generalization gap, the lack of consistency in experimental settings hinders us to identify their validity in practical scenarios. Therefore, in this work, we aim to investigate the clear relationship between these measures and the robust generalization gap by conducting a comprehensive analysis using a large set of models.

### Comparison to Jiang et al. 

The pioneering study  explored the empirical correlations between complexity measures and generalization with a primary focus on the standard training framework. Our main contribution is delving into the realm of measures within the adversarial training framework--a context having different generalization tendencies from those of the standard training framework [53; 66]. Indeed, we observe that the metric \(_{k}\) proposed in  has limitations in capturing the effectiveness of measures due to the high sensitivity of the robust generalization gap with respect to training setups. By introducing a new metric \(_{k}\), our work enhances the understanding of when and how robustness measures correlate with robust generalization. Furthermore, while Jiang et al.  employed customized parameter-efficient neural networks, we adopt widely-used model architectures such as ResNets, thereby providing insights that are not only relevant to recent research but also offer more practical implications.

## 3 Experimental Methodology

In the adversarial training framework, measures have played a crucial role by providing either theoretical upper bounds or empirical correlations with robust generalization. Previous works have leveraged these measures to propose new training schemes [61; 67; 64] and suggested directions to achieve high robustness [66; 56]. However, we discover that certain limitations and confusions exist when extending the findings of prior works to practical scenarios due to the use of a restricted set of models and training setups [67; 64; 66]. In order to gain a comprehensive understanding of the true efficacy of these measures, it is crucial to validate whether the effectiveness of measures remains valid in practical settings.

To address these challenges, our work aims to provide a comprehensive and accurate assessment of the effectiveness of measures for robust generalization in practical settings. Our objective is to address the fundamental question:

_Do measures remain effective in correlating with robust generalization in practical settings?_

_If so, how and when are measures correlated with robust generalization?_

To this end, in Sections 3.1 and 3.2, we carefully construct the training space that considers practical scenarios within the adversarial training framework and gather a wide range of measures from previous works. In Section 3.3, we define the evaluation metrics and introduce specific variations to accurately capture the correlation between measures and robust generalization in practical settings.

### Training Space

In the realm of adversarial training, various training procedures have been extensively explored to enhance adversarial robustness based on the development of AT, TRADES, and MART. Recently, to resolve the issue of robust overfitting, researchers have begun combining additional techniques, including commonly employed in the standard training framework, such as early-stopping , using additional data [9; 22], manipulating training tricks [48; 11], and adopting sharpness-aware minimization . By integrating these techniques into AT, TRADES, and MART, high adversarial robustness have been achieved, outperforming other variants of adversarial training methods .

Based on these prior works, to mimic practical scenarios, we have carefully selected eight training parameters widely used for improving robust generalization: (1) Model architecture {ResNet18 ,WRN28-10 , WRN34-10 ), (2) Training methods {Standard, AT , TRADES , MART }, (3) Inner maximization steps {1, 10}, (4) Optimizer {SGD, AWP }, (5) Batch-size {32, 64, 128, 256}, (6) Data augmentation {No Augmentation, Use crop and flip}, (7) Extra-data {No extra data, Use extra data }, and (8) Early-stopping {No early-stopping, Use early-stopping}. Additional training details can be found in Appendix C, providing a comprehensive overview of the training procedures.

In total, 1,344 models were trained using the CIFAR-10 dataset with \(=8/255\). Given these models, we evaluate their train/test robustness against PGD with 10 iterations (denoted as PGD10). While we acknowledge the existence of stronger adversarial attacks, such as AutoAttack , we primarily use PGD10 due to its prevalent use in adversarial training and the high computational demands of AutoAttack. Additionally, considering the usage of PGD in calculating specific measures, such as boundary_thickness and local_lip, ensures consistency in our analysis. For a detailed discussion, please refer to the Appendix A.4.

The statistics of the trained models are summarized in Figure 1. In the left plot, we can observe that the selected range of training parameters generates a diverse set of models, exhibiting robust generalization gaps ranging from 0% to 70%. Notably, certain models achieve 100% robustness on training data against PGD10, but their maximum robustness on the test set is only 65%, highlighting the importance of robust generalization gap. The right plot is a boxplot shows the distribution of robust generalization gaps for some training setups. As described in prior works [9; 52; 64], each training setup has a significant impact on robust generalization.

### Measures

Beyond the measures proposed under the adversarial training frameworks [67; 66], previous works [27; 17] have demonstrated that certain measures can effectively capture the generalization gap under the standard training framework. Therefore, in this paper, we have gathered diverse measures from both the standard and adversarial training frameworks and categorized them into five different types based on their origins and formulas: (i) weight-norm (7, 8, 9, 10), (ii) margin (12, 13, 14, 15), (iii) smoothness (16, 17), (iv) flatness (18, 19, 20, 21), and (v) gradient-norm (22, 23). We denote the chosen measures in teletype font (e.g., path_norm). While we briefly introduce the concepts of measures in each paragraph in Section 4, we refer the readers to Appendix B for the details of measures including their mathematical definitions due to the page limit.

Given these measures, we calculate their value on whole training examples for each trained model. This choice aligns with prior works, which argue that the most direct approach for studying generalization is to prove a generalization bound that can be calculated on the training set  and offer a caution against the oversimplified notion that maximizing (or minimizing) a measure value inherently leads to improved generalization .

Figure 1: (Left) Scatter plot of train robustness and test robustness. Bright color corresponds to high robust generalization gap, i.e., poor generalization. (Right) Boxplot of robust generalization gap for some training setups. All adversarial examples during training and testing are generated by PGD10 on CIFAR-10.

### Evaluation Metrics

To uncover the relationship between measures and robust generalization performance, we adopt the Kendall rank correlation coefficient following prior works [27; 36]. We begin by defining a search space \(=_{1}_{2}_{n}\). Each \(_{i}\) corresponds to a search space for each training parameter defined in Section 3.1. Given the search space \(\), we obtain the trained models \(f_{}()\) for \(\). For each model \(f_{}()\), we measure the robust generalization gap \(g(f_{}())\) and the corresponding measure value \((f_{}())\). For simplicity, we denote \(g():=g(f_{}())\) and \(():=(f_{}())\). We then calculate Kendall's rank coefficient  as follows:

\[()=|(||-1)}_{ }_{^{},^{}}(g()-g(^{})) {sgn}(()-(^{})),\] (4)

where \(||\) is the number of elements in \(\), and sgn(\(\)) is a sign function. The value of \(\) becomes \(1\) when the pairs have the same rankings and \(-1\) when the pairs have reversed order rankings. Therefore, a higher value of \(\) implies that as the value of a measure \(\) increases, the robust generalization gap \(g\) also increases.

As noted by , the measure may strongly correlate with the robust generalization gap only when a specific training setting is varied. Therefore, Jiang et al.  introduced the following metric:

\[_{k}()=_{_{1},,_{k-1},_{k+1}, ,_{n}}[(\{=(_{1},,_{n}), _{k}_{k}\})],\] (5)

which captures the robust generalization gap when only the hyper-parameter \(_{k}\) changes. However, we demonstrate that \(_{k}\) may not perform well in cases where Simpson's paradox exists. Simpson's paradox refers to a situation where each of the individual groups exhibits a specific trend, but it disappears (or reverses) when the groups are combined. Thus, when a parameter \(_{i k}\) heavily affects the robust generalization gap, \(_{k}\) becomes not effective as it captures the overall trends by taking expectation across all parameters including \(_{i k}\). In fact, within the adversarial training framework, the inner \(\) in (5) shows extremely high variance due to the high sensitivity of the robust generalization gap with respect to training setups, which will be discussed in Table 1.

Therefore, we propose a metric for capturing the robust generalization performance by fixing the hyper-parameter \(_{k}\) as follows:

\[_{k}()=_{_{k}}[(\{=(_ {1},,_{n}),\;_{i}_{i}\; i k\})].\] (6)

Here, \(_{k}()\) represents the effectiveness of a measure \(\) within a specific fixed training setup. This enables us to discover that some measures only work for specific training settings, e.g., uncovering the strong correlation between boundary_thickness and the robust generalization gap when AT is used as the training method. Furthermore, for measures exhibiting meaningful value of \(_{k}\), we additionally provide their scatter plots to mitigate the limitations of a correlation analysis.

## 4 Experimental Results

Based on the measures and trained models defined in Section 3, we calculate the measures for each model and their robust generalization gap. Notably, in the realm of adversarial training, models encounter two types of examples: benign examples and adversarial examples. Therefore, to gain a deeper understanding of the relationship between robust generalization and measures, we also calculate the values of example-dependent measures for both benign examples and PGD10 examples.

Table 1 summarizes the results of \(_{k}\) for each measure. Further, in order to consider the distributional correlation  and quantify the precision of \(_{k}\), we also report the corresponding standard deviation of the inner \(\) in Eq. (5). A higher value of \(_{k}\) indicates a stronger positive rank correlation, implying that as the measure value increases, the robust generalization gap increases. First of all, it is important to note that **none of the measures are perfect**. While certain measures show high \(_{k}\), all measures show high standard deviations. This observation indicates that no measure can provide a perfect estimation of the model's robust generalization gap. Moreover, with such high variances, it becomes challenging to clearly identify the underlying correlation of the measures.

To address this limitation, we conduct further analyses with our proposed metric, \(_{k}\) in Eq. (6). The benefit of \(_{k}\) is that it reveals the potential hidden relationship between the measures and the robust generalization gap by fixing training settings, which \(_{k}\) cannot captures. Thus, from now on, we will report \(_{k}\) of each measure and provide correlation analyses with the robust generalization gap, extending the connections beyond those presented in Table 1.

Norm-based measures requires fixed model architecture.In many prior works, researchers have demonstrated the effectiveness of norm-based measures in estimating the generalization gap [27; 17]. Among them, the weight norm-based measures, e.g., the product of Frobenius norm (log_prod_of_fro) , the product of spectral norm (log_prod_of_spec) , and the distance to the initial weight (euclid_init_norm) [27; 40], are built on theoretical frameworks such as PAC-Bayes [43; 47; 38]. path_norm is also often used to estimate the complexity of a neural network, which calculates the sum of outputs for all-ones input after squaring all parameters .

In Table 1, the most of norm-based measures exhibit a low correlation with the robust generalization gap for the metric \(_{k}\). However, by using the proposed metric \(_{k}\) in Table 2, we discover that log_prod_of_fro and euclid_init_norm exhibit strong correlations with low standard deviation when the model architecture is fixed. Intuitively, when the model architecture varies, the number of parameters and their corresponding values exhibit different ranges. Indeed, the range of log_prod_of_fro roughly shows \(\) for ResNet18, but shows \(\) for WRN28-10. Thus, comparing models with different architectures degrades the precision of weight norm-based measures. Under the fixation of the model architecture, log_prod_of_fro is **positively correlated with the robust generalization gap**, which consistents to the prior theoretical observations in PAC-Bayesian framework  or Lipschitz analysis . For log_prod_of_spec, we do not observe a strong correlation under any conditions.

Furthermore, we observe that path_norm shows some extent of correlation for all \(_{k}\) and \(_{k}\). Upon conducting a more in-depth analysis, we find that **the log of path_norm yields an almost linear relationship with the robust generalization gap** when conditioned with average_ce(PGD), resulting the total \(=0.68\) (detailed in Appendix A.2). This finding suggests that path_norm can be effectively utilized for estimating the robust generalization gap, with consistent to the prior works under the standard training framework [27; 17].

   & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & 
 Early \\ Stopping \\  & \(\) \\  num\_params (7) & - & -0.02\(\)0.04 & -0.01\(\)0.08 & -0.03\(\)0.01 & -0.02\(\)0.03 & -0.02\(\)0.02 & -0.03\(\)0.00 & -0.02\(\)0.01 \\ path\_norm (8) & 0.35\(\)0.05 & **0.27\(\)**0.13 & **0.24\(\)0.26** & **0.34\(\)0.03** & **0.37\(\)0.03** & **0.35\(\)0.05** & **0.35\(\)0.03** & **0.46\(\)0.14** \\ log\_prod\_of\_spec (9) & 0.03\(\)0.13 & -0.07\(\)0.07 & -0.00\(\)0.01 & -0.12\(\)0.03 & -0.11\(\)0.05 & -0.13\(\)0.01 & -0.13\(\)0.01 & -0.13\(\)0.06 \\ log\_prod\_of\_fro (10) & **0.37\(\)0.03** & 0.15\(\)0.11 & 0.09\(\)0.10 & 0.18\(\)0.01 & 0.18\(\)0.04 & 0.19\(\)0.00 & 0.19\(\)0.02 & 0.19\(\)0.04 \\ euclid\_init\_norm (11) & 0.32\(\)0.03 & 0.06\(\)0.03 & 0.09\(\)0.10 & 0.17\(\)0.00 & 0.20\(\)0.04 & 0.17\(\)0.01 & 0.17\(\)0.03 & 0.20\(\)0.11 \\  

Table 2: (Norm-based measures) Numerical results of \(_{k}\) and its corresponding standard deviation.

   & Model & Training & Steps & Optimizer & Batch-size & Aug & Extra-data & 
 Early \\ Stopping \\  & \(\) \\  num\_params (7) & - & -0.02\(\)0.04 & -0.01\(\)0.08 & -0.03\(\)0.01 & -0.02\(\)0.03 & -0.02\(\)0.02 & -0.03\(\)0.00 & -0.02\(\)0.01 \\ path\_norm (8) & 0.35\(\)0.05 & **0.27\(\)**0.13 & **0.24\(\)0.26** & **0.34\(\)0.03** & **0.37\(\)0.03** & **0.35\(\)0.05** & **0.35\(\)0.03** & **0.46\(\)0.14** \\ log\_prod\_of\_spec (9) & 0.03\(\)0.13 & -0.07\(\)0.07 & -0.00\(\)0.01 & -0.12\(\)0.03 & -0.11\(\)0.05 & -0.13\(\)0.01 & -0.13\(\)0.01 & -0.13\(\)0.06 \\ log\_prod\_of\_fro (10) & **0.37\(\)0.03** & 0.15\(\)0.11 & 0.09\(\)0.10 & 0.18\(\)0.01 & 0.18\(\)0.04 & 0.19\(\)0.00 & 0.19\(\)0.02 & 0.19\(\)0.04 \\ euclid\_init\_norm (11) & 0.32\(\)0.03 & 0.06\(\)0.03 & 0.09\(\)0.10 & 0.17\(\)0.00 & 0.20\(\)0.04 & 0.17\(\)0.01 & 0.17\(\)0.03 & 0.20\(\)0.11 \\  

Table 1: Numerical results of \(_{k}\) for each measure, along with its corresponding standard deviation. The total \(\) indicates the Kendallâ€™s rank coefficient for the entire pairs \((g,)\). * (PGD) indicates the same measure calculated on PGD10 examples for example-based measures.

Maximizing margin beyond a certain point harms robust generalization.Traditionally, the maximizing margin is considered as an ultimate goal in the adversarial training framework . Indeed, average cross-entropy loss  and margin-related losses  are frequently used in adversarial training methods. However, in both Table 1 and Table 3, average_ce(PGD) exhibit a high negative correlation with the robust generalization gap across all variations of training parameters. Similarly, \(_{k}\) and the total \(\) of prob_margin(PGD) are extremely high (0.79). This suggests that **lower cross-entropy on PGD examples (and higher margin in the probability space) leads to worse robust generalization.** Indeed, Fig. 2 shows a clear negative correlation between average_ce(PGD) and the robust generalization gap. Notably, high test robustness is observed within the range of average_ce(PGD) \([0.5,1.0]\), indicating that minimizing average_ce(PGD) beyond a certain point may harm the generalization performance as Ishida et al.  observed in the standard training frameworks.

Given this observation, we argue that the margin maximization in adversarial training methods should be carefully revisited. Recent studies have highlighted that maximizing the margin might not necessarily be the optimal objective in adversarial training due to intricate gradient flow dynamics  and the non-cognitive concept of using predicted probabilities . Additionally, in recent work , despite the similar robustness of TRADES and AT, their margin distributions on benign and adversarial examples are extremely different. This implies that the margin cannot be the sole determinant of adversarial robustness. Considering these findings and other recent studies , the margin maximization should be accompanied by a consideration of other factors such as weight regularization or gradient information.

The cross entropy and margin on benign examples, denoted as average_ce and prob_margin, also show some degree of correlation with the generalization gap. This correlation becomes particularly significant when using early stopping, where their correlations reach up to 0.65. Note that inverse_margin behaves differently because it uses the 10th-percentile of margins over the training dataset rather than the expectation.

Boundary thickness works well for models trained by AT.Yang et al.  introduced the concept of boundary thickness, which is an extended version of margin based on adversarial examples. They argue that a thin decision boundary leads to both poor adversarial robustness and the gap. Therefore, boundary_thickness should be negatively correlated with the robust generalization gap. However, as shown in Table 3, it does not correlate well with the robust generalization gap. The main difference between our experiments and those in  is that we also considered TRADES and MART, whereas Yang et al.  sorely compared models trained with AT. Thus, in Fig. 3, we plot the inner \(\) in \(_{k}\) for each training method. It is evident that the boundary thickness demonstrates a strong correlation with the robust generalization gap when the training method is fixed to AT. This suggests that boundary_thickness is more effective for comparing models trained with AT. Furthermore, in Appendix A.2, we also discover that boundary_thickness becomes more highly correlated with the robust generalization gap when the models are conditioned on average_ce(PGD). Thus, when using boundary_thickness as the sole determinant of robust generalization, we should carefully revisit the choice of training methods and the robust accuracy of models on train datasets.

Figure 2: Scatter plot for average_ce(PGD) and the gap. Bright color indicates a higher test robustness. For better visualization, we cutoff average_ce(PGD) \(>2\).

   & Model & Training & & & & & & \\  & Architecture & Methods & Steps & Optimizer & Batch-size & Aug & Extra-data & 
  \\  average\_ce(12) & -0.24\(\)0.03 & -0.28\(\)0.17 & -0.30\(\)0.27 & -0.23\(\)0.00 & -0.23\(\)0.04 & -0.24\(\)0.06 & -0.23\(\)0.06 & -0.34\(\)0.31 \\ inverse\_margin(13) & 0.07\(\)0.05 & -0.05\(\)0.23 & -0.18\(\)0.23 & 0.09\(\)0.06 & 0.08\(\)0.09 & 0.08\(\)0.13 & 0.09\(\)0.12 & 0.09\(\)0.12 \\ prob\_margin(14) & 0.24\(\)0.02 & 0.28\(\)0.16 & 0.31\(\)0.27 & 0.23\(\)0.01 & 0.23\(\)0.04 & 0.24\(\)0.06 & 0.23\(\)0.06 & 0.34\(\)0.31 \\ boundary\_thickness (15) & -0.02\(\)0.01 & -0.17\(\)0.15 & -0.19\(\)0.23 & -0.02\(\)0.01 & -0.02\(\)0.02 & -0.04\(\)0.06 & -0.02\(\)0.05 & -0.04\(\)0.27 \\  average\_ce(PGD) (12) & -0.78\(\)0.01 & -0.58\(\)0.39 & -0.47\(\)0.40 & **-0.79\(\)0.04** & **-0.79\(\)0.01** & **-0.80\(\)0.03** & **-0.79\(\)0.00** & **-0.78\(\)0.01** \\ inverse\_margin(PGD) (13) & 0.34\(\)0.02 & 0.26\(\)0.22 & 0.05\(\)0.19 & 0.37\(\)0.03 & 0.35\(\)0.03 & 0.34\(\)0.05 & 0.36\(\)0.13 & 0.38\(\)0.08 \\ prob\_margin(PGD) (14) & **0.79\(\)0.01** & **0.59\(\)0.38** & **0.48\(\)0.39** & **0.79\(\)0.04** & **0.79\(\)0.01** & **0.80\(\)0.03** & **0.79\(\)0.00** & **0.78\(\)0.02** \\  

Table 3: (Margin-based measures) Numerical results of \(_{k}\) and its corresponding standard deviation.

**Smoothness does not guarantee low robust generalization gap.** In the pursuit of achieving adversarial robustness, the smoothness between benign and adversarial examples is often considered as an indicative measure. For instance, TRADES  minimizes the kl_divergence between benign and adversarial logits. However, kl_divergence shows a negative correlation for both Table 1 and Table 4. This implies that, similar to average_ce(PGD), kl_divergence cannot serve as an indicator for robust generalization.

While Xu et al.  demonstrated that imposing local Lipschitzness (local_lip) leads to better generalization in linear classification, recent research  argued an opposing perspective, suggesting that within neural networks, local Lipschitzness might hurt robust generalization. However, this conclusion was built on fewer than 20 models and evaluated solely on test examples. In our large experiment, we cannot observe that local Lipschitzness itself negatively affects robust generalization. Rather, it is more efficient in predicting robust accuracy (detailed in Appendix A.1). These findings are consistent with , which highlighted the importance of model architecture or weight norms when evaluating models with local Lipshiftness.

**Flatness-based measures are not correlated well with the robust generalization gap.** Flatness-based measures have recently regarded as powerful indicators of generalization performance in both standard and adversarial training frameworks . This includes the maximum perturbation size in the weight space that do not dramatically changes the accuracy (pacbayes_flat) , the loss increment by adversarial weight perturbation (estimated_sharpness) , and its scale-invariant version (estimated_inv_sharpness) . However, our analysis reveals that flatness-based measures tend to exhibit poor correlations with the robust generalization gap. In both Table 1 and Table 4, the majority of flatness-based measures exhibit near-zero correlations or even negative values. Only pacbayes_flat(PGD) demonstrates a strong correlation with robust generalization because it effectively distinguishes between robust and non-robust models (refer to Appendix A.2).

Recently, Stutz et al.  demonstrated the importance of early stopping in the analysis of flatness. Similarly, we observe that, when early stopping is employed, the correlation of estimated_sharpness approaches zero. However, without early-stopping, we discover

   &  &  &  &  &  &  &  &  Early \\ Stopping \\  } \\  kl\_divergence (16) & -0.45\(\)0.01 & -0.35\(\)0.29 & -0.15\(\)0.19 & -0.46\(\)0.10 & -0.46\(\)0.08 & -0.43\(\)0.12 & -0.45\(\)0.02 & -0.37\(\)0.29 \\ local\_lip (17) & -0.24\(\)0.06 & -0.15\(\)0.19 & -0.01\(\)0.17 & -0.24\(\)0.10 & -0.25\(\)0.07 & -0.20\(\)0.05 & -0.23\(\)0.02 & -0.21\(\)0.30 \\ pacbayes\_flat (18) & 0.04\(\)0.09 & 0.04\(\)0.14 & -0.06\(\)0.14 & 0.06\(\)0.07 & 0.04\(\)0.08 & 0.10\(\)0.12 & 0.08\(\)0.10 & 0.07\(\)0.16 \\ estimated\_sharpness (19) & -0.07\(\)0.02 & -0.11\(\)0.24 & -0.22\(\)0.18 & -0.06\(\)0.17 & -0.08\(\)0.10 & -0.02\(\)0.13 & -0.04\(\)0.07 & -0.04\(\)0.16 \\ estimated\_inv\_sharpness (20) & 0.04\(\)0.02 & -0.04\(\)0.32 & -0.19\(\)0.14 & 0.05\(\)0.12 & 0.04\(\)0.06 & 0.10\(\)0.13 & 0.07\(\)0.10 & 0.03\(\)0.12 \\ average\_flat (21) & -0.36\(\)0.01 & -0.24\(\)0.21 & -0.10\(\)0.15 & -0.38\(\)0.07 & -0.37\(\)0.06 & -0.32\(\)0.13 & -0.35\(\)0.00 & -0.33\(\)0.25 \\  pacbayes\_flat (PGD) (18) & **0.95\(\)0.04** & **0.45\(\)0.16** & **0.39\(\)0.21** & **0.60\(\)0.08** & **0.59\(\)0.22** & **0.62\(\)0.22** & **0.63\(\)0.03** & **0.50\(\)0.07** \\ estimated\_sharpness (PGD) (19) & -0.22\(\)0.01 & -0.07\(\)0.12 & -0.06\(\)0.14 & -0.22\(\)0.10 & -0.23\(\)0.07 & -0.17\(\)0.07 & -0.19\(\)0.06 & -0.21\(\)0.32 \\ estimated\_inv\_sharpness (PGD) (20) & -0.24\(\)0.02 & -0.12\(\)0.14 & -0.10\(\)0.17 & -0.26\(\)0.10 & -0.26\(\)0.06 & -0.20\(\)0.08 & -0.23\(\)0.06 & -0.23\(\)0.35 \\  

Table 4: (Smoothness-based and Flatness-based measures) Numerical results of \(_{k}\) and its corresponding standard deviation.

Figure 3: Box plot of the inner \(\) in \(_{k}\) Eq. (6), where \(_{k}\) is training method. The text corresponds to the method for each outlier, e.g., boundary_thickness performs well when AT is used.

that estimated_sharpness exhibits a significant negative correlation for models have low average_ce(PGD) \( 1.5\). As shown in Fig. 4, models with low estimated_sharpness show high robust generalization gaps. This finding aligns with the concurrent work of , which demonstrates that flatter solutions generalize worse on out-of-distribution data. The additional results can be found in Appendix A.6.

In the case of average_flat, which is calculated with random weight perturbations and their worst-case losses, it demonstrates some degree of correlation. However, it is more efficient in predicting robust accuracy rather than the gap (refer to Appendix A.1). This result suggests that, as the concurrent work  observed in the standard training framework, flatness measures may not serve as reliable indicators of correlation in the adversarial training framework even they can be effectively used to achieving better performance.

The norm of gradient of inputs robustly captures the gap even for models with similar cross-entropy losses.Although some prior works  have argued that regularizing the input gradient norm might improve adversarial robustness, we observe that this cannot be argued as lower input gradient norm is better. Table 5 summarizes \(_{k}\) of the gradient norm of input (x_grad_norm) and the gradient norm of weight (w_grad_norm). Among these, x_grad_norm show a strong correlation with the robust generalization gap. The negative correlation of x_grad_norm indicates that models with a larger input gradient norm are more likely to show lower robust generalization gap.

Furthermore, even when comparing models having similar average_ce(PGD), x_grad_norm is the most robust indicator of the robust generalization gap. Previous works in the standard training framework  have argued that the cross-entropy loss is strongly correlated with the robust generalization gap, and thus, they used early stopping based on certain cross-entropy thresholds during training to remove the influence of varying cross-entropy loss. However, within the adversarial training framework, employing the same early stopping based on loss becomes challenging as TRADES and MART minimize different loss functions from AT. Therefore, we categorize the trained models into groups based on average_ce(PGD) values using a bin size of 0.1. This grouping reduces \(_{k}\) of average_ce(PGD) to \(-0.12\). The results are summarized in Table 6. Compared to all other measures, x_grad_norm**exhibits the highest rank correlation with the robust generalization gap even when conditioned on average_ce(PGD).** We believe this finding highlights the importance of the norm of input gradients as a valuable regularizer for improving model robustness and its generalization in practical settings.

 Measures & \(_{k}\) & Measures & \(_{k}\) \\ num\_params (7) & -0.23\(\)0.17 & estimated\_sharpness (19) & -0.12\(\)0.19 \\ path\_norm (8) & 0.25\(\)0.14 & estimated\_inv\_sharpness (20) & -0.13\(\)0.17 \\ log\_prod\_of\_spec (9) & 0.10\(\)0.13 & average\_flat (21) & -0.12\(\)0.17 \\ log\_prod\_of\_tr\_(10) & 0.11\(\)0.10 & x\_grad\_norm (22) & **-0.35\(\)0.16** \\ eucl\_init\_norm (11) & -0.11\(\)0.14 & w\_grad\_norm (23) & -0.06\(\)0.17 \\ average\_ce (12) & 0.03\(\)0.19 & inverse\_margin(PGD) (13) & 0.05\(\)0.19 \\ inverse\_margin (13) & -0.09\(\)0.19 & prob\_margin(PGD) (14) & -0.13\(\)0.16 \\ prob\_margin(14) & -0.02\(\)0.16 & peak\_base\_flat (PGD) (18) & -0.19\(\)0.20 \\ boundary\_thickness (15) & 0.06\(\)0.19 & estimated\_sharpness(PGD) (19) & 0.05\(\)0.19 \\ kl\_divergence (16) & -0.09\(\)0.14 & estimated\_inv\_sharpness(PGD) (20) & 0.01\(\)0.19 \\ local\_lap (17) & -0.11\(\)0.16 & x\_grad\_norm(PGD) (22) & -0.12\(\)0.14 \\ pacbayes\_flat (18) & -0.25\(\)0.21 & w\_grad\_norm(PGD) (23) & 0.13\(\)0.16 \\ 

Table 6: Numerical results of \(_{k}\) for each measure when \(_{k}\) is given by average_ce(PGD) values with a bin size of 0.1, along with its corresponding standard deviation.

Figure 4: Scatter plot for the robust generalization gap and estimated_sharpness. Conditioned on average_ce(PGD)\( 1.5\) without early-stopping.

## 5 Broader Impact with Benchmarks

RobustBench  provides a set of pre-trained models that achieve high robust accuracy across various datasets, including CIFAR-10, CIFAR-100, and ImageNet. Leveraging this benchmark, we extend our observations to diverse models including transformer-based architectures [3; 13] or trained on diffusion-generated datasets [51; 62]. As shown in Figure 5, we identify that some of our findings in Section 4 also can be applied to these models. Margin-based measures such as average_ce, prob_margin, average_ce(PGD), and prob_margin(PGD) consistently exhibit strong correlations with the robust generalization gap. Additionally, we observe that x_grad_norm consistently shows reliable performance in predicting the robust generalization gap, even when applied to models in the RobustBench across various datasets.

Though a deeper analysis is limited by the absence of training setting details, such as the use of early stopping, we additionally conduct an analysis with the model architecture by analyzing the pre-trained models. Table 7 summarizes \(_{k}\) for models with the same architecture. As we demonstrated in Section 4, norm-based measures exhibit a higher correlation when comparing models with identical architectures. Notably, for CIFAR-100, we find that path_norm shows a strong correlation with the robust generalization gap. Regarding the low correlation and high standard deviation of norm-based measures, we hypothesize that other training settings, such as the choice of activation functions (e.g., Swish and SiLU instead of ReLU) and training datasets, may affect the values of norm-based measures. Further exploration of these aspects is left to future work, as additional research and experiments can provide a more comprehensive understanding of these relationships.

## 6 Limitations and Future Work

While our study unveils the correlation between various measures and the robust generalization gap over 1,300 models, due to our computational constraints, we focused on ResNet models, CIFAR-10, and PGD with the \(L_{}\) norm. Thus, further investigations on a broader range of hyper-parameters and the use of stronger attacks may uncover new relationships beyond our analysis. We hope that future work would address these limitations.

## 7 Conclusion

Through large-scale experiments, we verified the underlying relationships between various measures and the robust generalization gap on CIFAR-10 under the \(L_{}\) norm. Our findings offer valuable insights into robust generalization and emphasize the need for caution when making statements such as,'model A is superior to model B because model A exhibits a better measure value than model B,' a frequently employed phrase in recent literature. We hope that our discoveries regarding diverse measures can contribute to further advancement in the field of adversarial robustness.

  CIFAR-10 & \(_{k}\) & Total \(\) & CIFAR-100 & \(_{k}\) & Total \(\) \\  path\_norm (8) & 0.24\(\)0.26 & 0.02 & path\_norm (8) & 0.55\(\)0.05 & 0.09 \\ log\_prod\_of\_fro (10) & 0.12\(\)0.40 & 0.10 & log\_prod\_of\_fro (10) & 0.30\(\)0.30 & 0.17 \\  

Table 7: Numerical results of \(_{k}\) for norm-based measures when \(_{k}\) is the model structure along with its corresponding standard deviation. The total \(\) is the same as in Fig. 5. For ImageNet, \(_{k}\) is not applicable due to the limited number of pre-trained models in RobustBench .

Figure 5: Experiment on RobustBench . For each dataset, we plot the total \(\) of each measure and highlight the robust measures with \(|| 0.25\) for all datasets with yellow background.