# A Structure-Aware Framework for Learning Device Placements on Computation Graphs

Shukai Duan

Center for Complex Particle Systems

University of Southern California

Los Angeles, USA

shukaidu@usc.edu

&Heng Ping

University of Southern California

Los Angeles, USA

hping@usc.edu

&Nikos Kanakaris

University of Southern California

Los Angeles, USA

kanakari@usc.edu

&Xiongye Xiao

Center for Complex Particle Systems

University of Southern California

Los Angeles, USA

xiongyex@usc.edu

&Panagiotis Kyriakis

Meta

pkyriakis@meta.com

&Nesreen K. Ahmed

Cisco Outshift

nesahmed@cisco.com

&Peiyu Zhang

University of Southern California

Los Angeles, USA

pzhang65@usc.edu

&Guixiang Ma

Intel Labs

guixiang.ma@intel.com

&Mihai Capota

Intel Labs

mihai.capota@intel.com

&Shahin Nazarian

University of Southern California

Los Angeles, USA

shahin.nazarian@usc.edu

&Theodore L. Willke

Intel Labs

ted.willke@intel.com

&Paul Bogdan

Center for Complex Particle Systems

University of Southern California

Los Angeles, USA

pbogdan@usc.edu

Equal contribution.

###### Abstract

Computation graphs are Directed Acyclic Graphs (DAGs) where the nodes correspond to mathematical operations and are used widely as abstractions in optimizations of neural networks. The device placement problem aims to identify optimal allocations of those nodes to a set of (potentially heterogeneous) devices. Existing approaches rely on two types of architectures known as grouper-placer and encoder-placer, respectively. In this work, we bridge the gap between encoder-placer and grouper-placer techniques and propose a novel framework for the task of device placement, relying on smaller computation graphs extracted from the OpenVINO toolkit. The framework consists of five steps, including graph coarsening, node representation learning and policy optimization. It facilitates end-to-end training and takes into account the DAG nature of the computation graphs. We also propose a model variant, inspired by graph parsing networks and complex network analysis, enabling graph representation learning and jointed, personalized graph partitioning,using an unspecified number of groups. To train the entire framework, we use reinforcement learning using the execution time of the placement as a reward. We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT. The robustness of the proposed framework is also highlighted through an ablation study. The suggested placements improve the inference speed for the benchmark models by up to \(58.2\%\) over CPU execution and by up to \(60.24\%\) compared to other commonly used baselines.

## 1 Introduction

The ability of intelligent agent systems (IASs) and cyber-physical systems (CPSs) to perceive and accurately interpret complex environments is crucial for artificial intelligence (AI). Recently, there has been a remarkable progress in machine learning and AI, due to the wide adoption of the transformer architecture and foundation models (FMs) [26; 30]. FMs have allowed both academia and industry to perform several data-demanding tasks, ranging from image and text analysis to multi-modal content generation and human-like visual perception [18; 20; 11; 4]. This is achievable due to the self-supervised nature of the FMs, their ability to easily generalize and the large amounts of data available online . The aforementioned properties make FMs distinguishable in specific general pre-training tasks such as next-word prediction, compared to traditional ML architectures that use supervised learning . This recent surge in using large FMs has led to increased demand for computing power, which is projected to grow even more in the next few years . This is due to the need for more advanced model training processes and the continuous expansion of model parameters [26; 6]. It is clear that as FMs become increasingly complex, they demand vast computational resources not only for training but also for fine-tuning and inference tasks. This surge in computational demand underscores the necessity for managing the available hardware more effectively.

In light of this, the concept of device placement has gained popularity lately as a manner to speed up and improve the inference time of deep learning models, including FMs, in systems with a mixture of heterogeneous devices, such as CPUs, GPUs and NPUs [31; 1; 33; 15; 21]. With neural networks evolving towards larger models, heterogeneous and multi-device computing has played a critical role in their implementation. Device placement emerges as a pivotal factor determining the performance of an implementation of a model. Strategically allocating neural networks across multiple devices can significantly reduce the runtime of a model and the overall energy consumption . The current process of device placement typically involves converting a neural network into a computation graph, where each node corresponds to an operation within the neural network. The computation graph is then partitioned and its nodes are allocated to the appropriate devices for processing. The effectiveness of device placement directly impacts the deployment performance of neural networks.

Early on device placement has been the main responsibility of human experts . Engineers with a substantial level of expertise and diligence were responsible for allocating each part of a model to the best-suited device. However, this rigorous task can be daunting, considering the rapid advancement in hardware, which leads to a serious increase in development time, bug fixing, and code optimization [3; 19; 23]. Deep reinforcement learning (DRL) has recently been proposed to provide effective device placements with full automation [31; 1; 21]. Two different DRL architectures for device placement currently exist in the literature: the 'grouper-placer' model that reduces the action space by merging operations into groups; and the 'encoder-placer' that encodes the features of the operations to capture the topological properties of the computational graph [15; 31].

Although existing approaches have been successful, there is still space for improvement. In particular, they demonstrate several shortcomings. To begin with, they disregard the directed and acyclic nature of computation graphs . Furthermore, they either follow a grouper-placer or an encoder-placer architecture [22; 21]. In addition, most of them are not designed to train all of their components simultaneously in an end-to-end fashion and they fail to capture higher-order interactions among the operations of a computation graph. Finally, they make use of large, fine-grained computation graphs, thereby exhibiting slow convergence and demanding a higher number of iterations during the learning process .

Considering the limitations mentioned above, this paper proposes a framework to optimize for device placement based on smaller, coarsened computation graphs produced by the OpenVINOtoolkit. Our framework consists of five steps: First, the neural network model is converted into a computation graph. Then, local and global structural features as well as positional, node-specific and fractal features are extracted to compose the initial node feature vectors. Following that, graph representation learning, graph partitioning and pooling are learned jointly, facilitating the fusion of the grouper-placer and encoder-placer models. Finally, we use the execution time of the suggested device placements as a reward to train the entire framework. In contrast with existing approaches, our framework allows for encoding and grouping operations of a computation graph jointly in an end-to-end fashion. Along with the proposed framework, through one of the variant models, we introduce a novel method tailored to computation graphs, for jointly learning node embeddings and performing personalized graph partitioning with an unspecified number of groups for further coarsening. The effectiveness and robustness of the proposed approach are demonstrated through multiple experiments with different benchmark models and a detailed ablation study.

**Contributions**. The main contributions of this paper are the following:

* To the best of our knowledge, this is the first flexible framework for the task of device placement capable of learning graph and node representations as well as graph partitions and pooling jointly in an end-to-end fashion. Even more, we introduce the concept of learning personalized graph partitions using an unspecified number of groups.
* We propose a structure-aware device placement framework that integrates graph coarsening, node representation learning, policy optimization and effectively combines the strengths of grouper-placer and encoder-placer models.
* Our framework is the first of its kind that encodes features from multifractal analysis, positional encodings, and node-specific features for the task of device placement through a model variant, and discusses the impact of incorporating different properties on the model.
* The proposed variant of the framework achieves a state-of-the-art performance improvement of up to \(58.2\%\) over CPU execution and \(60.24\%\) in comparison with other baseline models.

## 2 Proposed framework

In this section we introduce our framework titled Hierarchical Structure-Aware Device Assignment Graph (HSDAG). It consists of five steps as shown in Figure 1. Briefly, we first convert a neural network model into a computation graph. Then, we extract features for each node and edge of the computation graph. The next step enriches these features via graph representation learning techniques and simultaneously learns how to partition and pool the graphs. The learned features and groups of nodes are utilized to train a stochastic policy, which we use for assigning each node of a graph to the most appropriate device. We train the entire pipeline end-to-end with the objective of minimizing the inference time of the proposed device placement.

### Problem Formulation

**Definition 2.1**.: **(Computation graph)**. We denote a computation graph as \(G=(V,E)\). \(G\) is labeled, unweighted, directed and acyclic with a set of nodes \(V=\{v_{1},v_{2},...,v_{|V|}\}\) representing operations and a set of edges \(E V V\) representing their connections. Each graph \(G\) is associated with a binary asymmetric adjacency matrix as \(A\{0,1\}^{|V||V|}\). Each node \(v\) of \(G\) represents an operation applied to the input data and is associated with an operation type \(t_{v} T\). In the context of this paper, the terms node and operation are used interchangeably. An edge \(e=(v,u) E\) represents the flow of data or dependency among node \(v\) and node \(u\).

**Definition 2.2**.: **(Device placement)**. Given a list \(\) of the available devices, a placement \(P=\{p_{1},p_{2},...,p_{n}\}\) assigns each operation \(v\) of a computation graph \(G\) to a device \(p\), where \(p\{1,2,...,||\}\).

**Problem setup.** We focus on the problem of device placement in a heterogeneous computing system. Our goal is to assign each part of a computation graph to the most suitable device, such that the overall execution time during the inference of the model is minimized. Formally, given a computation graph \(G\), we learn a policy \(:G P\) that assigns a placement \(p\) for all \(v G\) such that

\[r^{*}_{,P}=_{,P}r(G;,P),\] (1)where \(r^{*}_{,P}\) is the reward by following policy \(\) and placement \(P\). We use a GNN (although any model can be employed) to learn the optimal policy denoted by \(^{s}_{}\) and its set of parameters \(\). Let \(l_{P}(G)^{+}\) denote the execution time of the computation graph \(G\) by following the placement \(P\). Our goal is to minimize execution time by learning the parameters

\[^{*}=_{,}l(G;,),\] (2)

for the policy \(\) that yields the best results.

### Graph construction

Given a set of neural network models \(C=\{c_{1},c_{2},...,c_{|C|}\}\), the first step is to decide a graph-based code representation \(repr\) that converts a structure of a neural network model \(c_{i}\) into a graph \(G_{i}\), \(repr:c_{i} G_{i}\). There is a list of different graph-based representations of neural network models including abstract syntax trees, contextual flow, control, data flow and LLVM IR graphs . Although such graphs may contain valuable information and capture the latent information flow of a program, they tend to add unnecessary complexity to the overall process. Instead, for the experiments of this paper, we opt for representing the code of a neural network model \(c_{i}\) as a computation graph, due to its expressiveness, simplicity, and practicality. A computation graph is generally smaller than its counterparts and could be easily allocated to specific devices. For instance, one can use several popular libraries to produce the computation graph \(G_{i}\) of a given neural network model \(c_{i}\). In this paper, we use the OpenVINO toolkit to generate the computation graphs as it generates smaller, already coarsened graphs compared to those of TensorFlow or PyTorch. Further information about the OpenVINO toolkit and examples of computation graphs are available in Appendix F. Even though it is optional, further coarsening can be performed using common co-locating operations or heuristics . Such co-locating heuristics eliminate certain execution failures due to placement rule violations. In our experiments, we apply a simple algorithm for co-location to further condense the model into a smaller computation graph . The graph construction step enables our approach to utilize graph representation learning techniques. Note that our framework is flexible and can

Figure 1: Overview of the proposed framework, HSDAG. **Graph construction.** We first convert a neural network model \(c\) into a computation graph \(G\), \(repr:c G\). **Feature extraction.** Then, we calculate the initial feature matrix \(^{(0)}\) capturing local and global connectivity information, node-aware features, information about the order of the nodes as well as features from fractal analysis. **Learning embeddings and groups jointly.** We further enrich node features \(X^{(0)}\) using a \(:G\) model and learn how to pool a graph \(G\) jointly using a graph parsing network. In that way, we bridge the gap between grouper-placer and encoder-placer methods for device assignment. **Device placement.** A learnable MLP model classifies the nodes \(V^{}\) of the coarsened graph \(G^{}=(V^{},E^{})\) to the available devices \(\). **Heterogeneous execution.** We map the device placement of \(V^{}\) to \(V\) based on the node assignment matrix \(\) and apply the placement of all the operations into the execution environment to measure the execution time with the corresponding reward. **End-to-end parameter update.** We update our policy \(\) parameters \(\), i.e. the combination of GNN and MLP, based on the reward and renew the node feature matrix \(\) with the current cluster information. The entire framework supports end-to-end parameter updates and training.

be coupled with any type of graph code representation for neural networks capable of producing a directed and acyclic graph \(G\), similar to those of the computation graph representation.

### Feature extraction

The proposed framework is also versatile as far as the initial node features are concerned. During our experimentation with various feature combinations (see Ablation studies in Section 3), we found that a mixture of features capturing local and global connectivity information, features from fractal analysis as well as node-specific features (e.g. topology, the order of a node and node type) leads to better results.

**Local structural features.** Specifically, the initial feature vector \(_{v}^{(0)}\) of a node \(v\) incorporates information about the node (operation) type \(t_{v}\), in-degree \(_{v}^{in}=|_{in}(v)|=_{u V}A(u,v)^{+}\) and out-degree \(_{v}^{out}=|_{out}(v)|=_{u V}A(v,u)^{+}\) of \(v\); here, \(_{}(v)\) and \(_{}(v)\) represent the sets of in-neighbors and out-neighbors of a node \(v\), respectively. Initially, we use a one-hot encoding to embed each unique operation type \(i\) into a tensor \(T_{i}\{0,1\}^{|T|}\), where \(|T|^{+}\) is the number of unique operation types among all the input models \(C\). Formally, for each operation type \(t T=\{1,,|T|\}\), the one-hot encoding is defined as

\[T_{i}=1&i=t\\ 0&,\;i\{1,,|T|\}\] (3)

Similarly, we one-hot encode each unique in-degree \(_{i}^{in}\) and out-degree \(_{j}^{out}\) values into the \(_{i}^{in}\{0,1\}^{|^{in}|}\) and \(_{i}^{out}\{0,1\}^{|^{out}|}\) tensors, where \(|^{in}|^{+}\) and \(|^{out}|^{+}\) is the number of unique in-degree and out-degree values, respectively.

**Global structural features.** Relying solely on local features might miss capturing important global properties of the network. To capture the multi-scale structural properties of the network, we calculate the fractal dimension for each node \(v V\). The fractal dimension \(D(v)\)[28; 30] of a node \(v\) is computed based on the mass distribution. Given the set of distances \(\{r_{1},r_{2},,r_{m}\}\) from node \(v\) to other nodes in the network, the fractal dimension \(D(v)\) is calculated as follows:

\[D(v)=^{m}((r_{k})-)((N(v,r_{k}))- ())}{_{k=1}^{m}((r_{k})-)^{2}}\] (4)

where \(r_{k}\) represents each distance in the set, \(N(v,r_{k})\) is the number of nodes within the distance \(r_{k}\), and \((r)\), \(()\) are the mean values of \((r_{k})\) and \((N(v,r_{k}))\), respectively.

**Positional features.** In an attempt to inject information about the order of the nodes, we associate each node \(v\) with an integer \(pos\) that encodes the topological order of the graph. To do so, we use a bijective mapping function \(id:V\{1,,|V|\}\). Formally, if \(v_{i}\) is the \(i\)-th node in the topological order, then \(id(v_{i})=i\). This kind of feature can be further enhanced using a function for positional encoding \(PE:\):

\[PE(pos,k)=(}})& k=2i\\ (}})&k=2i+1, i[0,}{2}]\] (5)

where \(d_{pos}\) is the size of the embedding of feature \(pos\).

**Node-specific features.** For each node \(v\), we define the padded, fixed-size output shape tensor \(S_{v}^{|S|}\), which is also provided as a piece of information in the original computation graph. Each digit of the output shape of a node \(v\) is represented as a new dimension in the tensor \(S_{v}\). We traverse the entire graph \(G\) and obtain the maximum data output shape \(|S|=_{v V}(v)\).

Finally, we concatenate all the information for each individual node \(v\) and form a node feature vector \(_{v}^{(0)}^{d}\), where \(d=T\|S\|^{in}\|^{out}\|D\|d_{pos}\). Building on top of the initial feature matrix \(^{(0)}^{|V| d}\), in the next steps, we extend the representation learning capabilities of our framework with recent techniques from the field of GNNs. While GNNs offer powerful feature extraction methods, this step is crucial as it provides a model with information about the nodes and structure of the graph, thereby accelerating the convergence of the process.

### Learning embedding and groups jointly

We further enrich node features \(^{(0)}\) and learn how to partition a given graph \(G\) into an unspecified varying number of groups by employing the Graph Parsing Network (GPN) . Existing grouper-placer methods typically operate with a predefined number of clusters during device placement exploration. They also employ non-trainable algorithms for graph partitioning or pooling relying mostly on human intuition and heuristics to group the nodes of a graph. This ad-hoc presetting of the group number leads to suboptimal solutions, which in turn inhibit the exploration and learning process of the overall framework. Instead, our framework treats both the number of node groups and the pooling algorithm as learnable parameters, which are trained in an end-to-end fashion. This step consists of three components: (1) graph and node encoding, (2) edge score matrix calculation and (3) graph partitioning and pooling.

**Graph and node encoding.** The graph and node encoding component is compatible with any neural network model and generates a node embedding \(_{v}^{d^{}}\) for each node \(v\), where \(d^{}\) is the dimension of the node feature vector. In practice, we use an embedding function \(:G^{|V| d^{}}\) as our main graph encoder; self-supervised techniques may also be employed to pre-train the embedding function GNN, which aids in the downstream task of device placement . As a result of using a GNN as an encoder, the learnable feature matrix \(^{|V| d^{}}\) captures both node- and structure-aware information about the graph \(G\). As we mentioned before, our framework is model-agnostic and allows for utilizing different GNN functions. In the interest of clarity, we formulate the representation learning step using a GCN  model with a single graph convolutional layer:

\[=(,A)=(}^{-1/2}}}^{-1/2}^{(0)})^{|V| d^{}}\] (6)

where \(}=+\{0,1\}^{|V||V|}\) denotes the adjacency matrix with self-loops and \(}_{ii}=_{j=0}}_{ij}\) is the corresponding diagonal degree matrix, \(^{d d^{}}\) is a matrix of learnable parameters, \(^{(0)}^{|V| d}\) is a matrix with the input features of each node \(v\) and \(()\) denotes an activation function such as \(()=(0,)\).

**Edge score matrix calculation.** This component accepts any differentiable neural network model to calculate an edge score matrix \(^{|V||V|}\). Given an edge \(e\) connecting two nodes \(v,u\) and their embeddings \(_{v},_{u}\), then the score \(_{v,u}=_{e}\) is calculated as follows:

\[_{v,u}=((_{v}_{u}))  s.t.= A\] (7)

where \((x)=(x)=}\) and \(\) can be any differentiable neural network. During our experimentation, we found that setting \(=\) yields good performance w.r.t. the task of device placement. The magnitude of an edge score \(_{e}\) quantifies the strength of the relationship between the connected nodes \(v\) and \(u\). A higher edge score \(_{e}\) implies a stronger relationship, increasing the probability for the nodes \(v\) and \(u\) to be grouped into the same partition \(_{i}\). Formally, this can be expressed as:

\[P(v_{i} u_{i})_{e}, e =(v,u)\] (8)

As a result, the higher the edge score \(_{e}\), the higher the probability that nodes \(v\) and \(u\) will be grouped into the same partition, reflecting their relational affinity.

**Graph partitioning and pooling.** The graph partitioning and pooling component uses the computed edge scores \(\) to partition the entire graph \(G\). Specifically, it iterates through each node \(v\) in the graph \(G\) and identifies the edge with the highest score among all edges connected to that node. In a graph with \(|V|\) nodes, this process may identify up to \(|||V|\) such edges. Only these \(||\) edges are retained and the remaining edges are discarded, automatically dividing the graph into multiple groups. The set of the remaining edges is then defined as:

\[=\{(v,u)|v V,u=_{u^{}_{(v)}} _{v,u^{}}\}\] (9)

This grouping method ensures that nodes within each group have stronger local connectivity and tighter relationships, making them more suitable for being assigned to the same device for execution.

The final step is to create a node assignment matrix \(^{|V||V^{}|}\) that maps each node \(v\) in the original graph \(G\) to a node \(v^{}\) in the coarsened graph \(G^{}=(V^{},E^{})\). To construct the node assignment matrix \(\) we use the graph parsing algorithm \(\), as proposed in :

\[=()\] (10)

The adjacency matrix \(A^{}\{0,1\}^{|V^{}||V^{}|}\) of the pooled graph \(G^{}\) is then defined as:

\[A^{}=^{T} A\] (11)

### Reinforcement learning for node-based device assignment

In this step, we combine the GPN from the previous component and an MLP to learn a policy \(:G^{} P^{}\). After we obtain the device placement \(P^{}\), we use the node assignment matrix \(\) to map each node \(v^{}\) of the coarsened graph \(G^{}\) to a node \(v\) of the original graph \(G\). In that way, we manage to assign a device \(p_{v}\) for each node \(v\) of the graph \(G\). At each RL episode, we infer the machine learning model with the updated operation device placement \(P^{}\) and get the inference latency \(l_{P^{}}(G^{})\). Our ultimate goal is to choose a reward function that maximizes the reward when the latency is low. Thus, we use the reward function \(r_{P^{}}(G^{})=}(G^{})}\). To find the proper stochastic group detection and placement policy parameters \(\), we maximize the objective function

\[J()=_{P(P|G^{};)}[r(P,G^{})]\] (12)

For each time step, we update the node embedding \(_{v}\) of a node \(v\) by summing up the embedding \(_{v^{}}\) of its corresponding coarsened node \(v^{}\): \(_{v}=_{v}+_{v^{}}\). We then form a new graph \(G^{}\) which can also be considered as the new state. We then run a new round of representation and group learning (Section 2.4) and device placement for the new graph \(G^{}\). We update our policy parameter gradient by REINFORCE  using the Adam  optimizer producing:

\[_{}J()=_{P(P|G^{};)}[r(P,G) _{} p(P G^{};)]\] (13)

We record \(x\) steps in the buffer and compute the reward of each device placement. After \(x\) steps, we update the policy parameter with the cumulative reward and loss

\[_{}J()-_{i=1}^{x}_{} p(P G ^{};)^{i} r(P_{i},G)\] (14)

where \(\) is the discount rate for the reward at the current time step to the previous time steps.

## 3 Experiments

### Benchmarks

To evaluate our approach we use the computation graphs created from three popular benchmarks: (1) **Inception-V3**: The Inception-V3 architecture  is extensively employed for image recognition and visual feature extraction . This neural network consists of multiple blocks, each comprising various branches of convolutional and pooling layers. These branches are capable of parallel execution and are concatenated to form inputs for the subsequent block. However, the depth of the network limits this parallelism since later blocks must wait for the completion of earlier ones; (2) **ResNet**: ResNet  is a widely-used model for image classification. It consists of multiple convolutional layers and uses residual connections to reduce the effects of the vanishing gradient problem. We use the ResNet-50, which is a 50-layer convolutional neural network; (3) **BERT**: BERT  is a language model relying on the transformer architecture. It pre-trains deep bidirectional representations on unlabeled data jointly. It can be used as the base to fine-tune models for a list of tasks, including question answering and language inference. Several versions of the BERT model exist. In this paper, we use the base uncased version. Important statistics and a more detailed description of the benchmark models are available in Table 1 and Appendix D.

### Setup

We implement the variant of HSDAG and baseline models with the PyTorch Geometric framework . The Adam  optimization algorithm is used for the optimization of the parameters of the models. We run our experiments on real hardware using the OpenVINO toolkit version 2023.3.0 2.

### Training and Training

We train the model on real hardware using the OpenVINO toolkit version 2023.3.0 3.0

**Devices.** The available devices for our experiments are the following: **(1)** CPU: 12th Gen Intel(R) Core(TM) i9-12900K, **(2)** GPU.0: Intel(R) UHD Graphics 770 (iGPU) and **(3)** GPU.1: Intel(R) Data Center GPU Flex 170 (dGPU). Our server has 64GB of memory.

### Baseline comparison

Aiming to test the performance of the proposed framework, we evaluate the proposed variant against a list of state-of-the-art baseline methods. The selected baseline models may differ from the variant of our framework in many ways, in terms of their architecture and the algorithms they employ to implement their components. Furthermore, they may ignore parts of the proposed framework or implement them differently (e.g., learn node embeddings and clusters separately). The main purpose of the baselines is the evaluation of our framework w.r.t. the task of device placement.

1. **CPU-only**. It assigns the entire computation graph to CPU. It does not include any part of the proposed approach, except for the device assignment component.
2. **GPU-only**. It assigns the entire computation graph to GPU. Similar to CPU-only, the device assignment part of our framework is the only one that is implemented.
3. **OpenVINO-CPU**. This baseline method lets the OpenVINO optimization toolkit decide whether the entire computation graph should be assigned to CPU or GPU, with CPU set as the first preference.
4. **OpenVINO-GPU**. A baseline similar to OpenVINO-CPU with GPU set as the first preference.
5. **Placeto**. It uses GNNs to learn features for any computation graph as proposed in . Therefore it enables the transfer of a learned device placement policy to new computation graphs without further re-training.
6. **RNN-based approach**. An RL framework trained to optimize device placement by utilizing a sequence-to-sequence LSTM model and a content-based attention mechanism.

Table 2 shows the performance of the compared models as far as the task of device placement and the reduction in execution (inference) time are concerned. On Inception-V3, our framework achieves a \(17.9\%\) speedup over the CPU-only baseline, reducing the inference time from \(0.0128\) seconds to \(0.0105\) seconds. This performance surpasses other baselines, such as GPU-only (\(6.25\%\) speedup) and Placeto (\(9.38\%\) speedup). Similarly, on ResNet, our framework delivers a \(52.1\%\) speedup, reducing the inference time to \(0.00766\) seconds, which is significantly better than the GPU-only (\(51.2\%\) speedup) and OpenVINO-GPU (\(45.3\%\) speedup) baselines. The most substantial improvement is observed on the BERT benchmark, where our framework achieves a \(58.2\%\) speedup, reducing the inference time to \(0.00267\) seconds, outperforming the GPU-only baseline (\(56.5\%speedup\)). These results highlight the efficiency and effectiveness of the proposed device placement approach, leveraging graph coarsening, node representation learning, and reinforcement learning to optimize computation graph execution on heterogeneous hardware environments.

### Ablation studies

To understand the impact of the components, steps and configurations of HSDAG, we conduct an ablation study. Various modifications to the framework were tested, such as removing graph structural features, output shape features, and node IDs. Overall, the results are shown in Table 3. indicate that each of these components plays a significant role in achieving optimal performance.

   Benchmark & \(|V|\) & \(|E|\) & \(\) \\  Inception-V3 & \(728\) & \(764\) & \(1.05\) \\ ResNet & \(396\) & \(411\) & \(1.04\) \\ BERT & \(1009\) & \(1071\) & \(1.06\) \\   

Table 1: Statistics of computation graphs of the benchmarks used in our experiments. \(|V|\): the number of nodes, \(|E|\): the number of edges, \(\): the average degree.

**No graph structural features.** For each node of \(v\) in the computation graph \(G\), we ignore features from fractal analysis, in-degree, and out-degree. Removing graph structural features clearly impacts the framework's ability to capture the global and local structural information of the computation graph. The results from this ablation show a decrease in performance, although not as important as other feature removals. For example, the inference time speedup on Inception-V3 drops to \(14.8\%\) from \(17.9\%\) when these features are excluded. This indicates the importance of these features in capturing the hierarchical and interconnected nature of computation graphs.

**No output shape features.** We assume that the output data shape of each operation reflects the computation requirement for the corresponding operation. We do not include the output data shape in the node feature vector to test its effectiveness. We observe a significant decrease in performance. The speedup for Inception-V3 drops from \(17.9\%\) to \(8.59\%\). This suggests that the features related to the output shape are crucial for understanding the computational load associated with each node in the graph. Without this information, the framework's ability to optimize device placement is compromised, leading to less efficient computation graph execution.

**No node ID.** In this case, we do not use the node ID to encode the topological sequence of the node \(v\) in a given computation graph \(G\). Omitting information about the node ID results in a significant performance drop. The speedup for Inception-V3 is reduced to \(8.59\%\). This highlights the critical role of node IDs in preserving the order and dependencies within the computation graph. The framework lacks essential information about the execution order of operations, leading to sub-optimal device placement and reduced overall efficiency.

### Downstream Model Performance and Runtime Complexity

As a sanity check for our method, we show the performance on downstream tasks is not affected. Theoretically, since the end-to-end training pipeline itself does not change, we do not expect any

   &  &  &  \\   & \(l_{P}(G)\) & Speedup \(\%\) & \(l_{P}(G)\) & Speedup \(\%\) & \(l_{P}(G)\) & Speedup \(\%\) \\  CPU-only & \(0.0128\) & \(0\) & \(0.0160\) & \(0\) & \(0.00638\) & \(0\) \\ GPU-only & \(0.0120\) & \(6.25\) & \(0.00781\) & \(51.2\) & \(0.00277\) & \(56.5\) \\ OpenVINO-CPU & \(0.0128\) & \(0\) & \(0.0234\) & \(-46.3\) & \(0.00657\) & \(-2.98\) \\ OpenVINO-GPU & \(0.0138\) & \(-7.81\) & \(0.00876\) & \(45.3\) & \(0.00284\) & \(55.5\) \\ Placeto & \(0.0116\) & \(9.38\) & \(0.00932\) & \(41.8\) & \(0.00651\) & \(-2.04\) \\ RNN-based & \(0.0128\) & \(0\) & \(0.00875\) & \(45.3\) & OOM & OOM \\ HSDAG & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 2: Evaluation results of different models on the device placement task. The **best results** for each baseline model across benchmarks are highlighted in **bold**. \(l_{P}(G)\) denotes the execution time (in seconds) for each model. Speedup % denotes the speedup with respect to the CPU-only baseline. On the execution time \(l_{P}(G)\) column, lower (\(\)) scores are better. On the Speedup % column, higher (\(\)) scores are better. To get accurate results, we measure the inference time with the same device displacement 10 times and take the average of the last 5 measurements. OOM: out of memory.

   &  &  &  \\   & \(l_{P}(G)\) & Speedup \(\%\) & \(l_{P}(G)\) & Speedup \(\%\) & \(l_{P}(G)\) & Speedup \(\%\) \\  CPU-only & \(0.0128\) & \(0\) & \(0.0160\) & \(0\) & \(0.00638\) & \(0\) \\  Original & \(0.0105\) & \(17.9\) & \(0.00766\) & \(52.1\) & \(0.00267\) & 58.2 \\  w/o output shape & \(0.0117\) & \(8.59\) & \(0.00768\) & \(52.0\) & \(0.00278\) & 56.4 \\ w/o node ID & \(0.0117\) & \(8.59\) & \(0.00768\) & \(52.0\) & \(0.00279\) & 56.4 \\ w/o graph structural features & \(0.0109\) & \(14.8\) & \(0.00766\) & 52.1 & \(0.00268\) & 58.2 \\  

Table 3: Results of the framework variants of the ablation study on the device placement task. \(l_{p}(G)\) denotes the execution time (in seconds) for each model. Speedup % denotes the speedup with respect to the CPU-only baseline. On the execution time \(l_{p}(G)\) column, lower (\(\)) scores are better. On the Speedup % column, higher (\(\)) scores are better. To get accurate results, we measure the inference time with the same device displacement 10 times and take the average of the last 5 measurements.

impact on the performance. Nonetheless, we show experimental results on 3 exemplar cases to empirically verify statement.

**Inception-V3**: We performed image classification inference on images depicting Samoyed dogs. All the parameters are directly derived from the torchvision pre-trained model. We did not change any configuration on the data type of the model. The classification accuracy of Inception-V3 using the best device placement is **82.77**%. For the GPU-only experiments the classification accuracy is **82.72**%. For the CPU-only experiments the classification accuracy is **82.33**%.

**ResNet**: Similarly, we performed image classification inference using the ResNet model on the same dataset. The classification accuracy with the best device placement is **45.37**%. For the GPU-only experiments the classification accuracy is **45.37**%. For the CPU-only experiments the classification accuracy is **45.44**%.

**BERT**: We evaluated the performance of the BERT model using the output embeddings from the different device placements. We calculated their mean squared error, cosine similarity and Euclidean distance (MSE: the lower the better, Cosine Similarity: the higher the better, euclidean distance: the lower the better). The results are shown on Table 4.

These experiments empirically demonstrate that HSDAG does not affect the performance of the model in the downstream tasks. All models have similar performance regardless of the running device (e.g. CPU, GPU or heterogeneous device). Finally, we also conducted an empirical runtime complexity estimation in order to show HSDAG's superiority in terms of execution time. We compare against the Placeto and the RNN-based device placement methods for the same 3 models. The results are shown in Table 5.

## 4 Conclusion

We introduce a flexible framework for the task of device placement. Our framework relies on smaller computation graphs and is divided into five steps such as graph coarsening, node representation learning and policy optimization using reinforcement learning. It supports end-to-end model training and is aware of the directed and acyclic nature of the computation graphs under consideration. Additionally, we propose a model variant that facilitates graph representation learning and personalized graph partitioning jointly with an unspecified number of node clusters. The experimental results highlighted that our framework is flexible, and robust and mitigates the shortcomings of the grouper-placer and encoder-placer models by capitalizing on the best aspects of the two worlds. The suggested placements improve the inference speed for the benchmark models compared to widely used baselines. One interesting future work direction is to explore different RL problem formulations as well as different reward structures, such as the incremental rewards used in . Another direction is to study the interplay of the reward and generalizability as well as potentially using reward models rather than measuring reward. This could unlock a more efficient algorithm as the current setup relies on measuring the inference latency, which has practical limitations.

**Limitations.** The latency measurements did not consider the temperature change of the environment of the system and surroundings. During our experiments, we allocate the CPU for both experiment and policy due to the limitation of the setup. At the same time, iGPU is not considered throughout the experiment because we consider it to be always slower than both CPU and GPU. We attempted to obtain the source code for the baseline methods, but they were not made available from the corresponding authors. As such, we implemented and reproduced the baseline methods with our best effort from their published papers.

   Comparison & MSE & CS & L2 norm \\  CPU vs GPU & 3.049e-05 & 0.999 & 0.432 \\
**CPU vs HSDAG** & **6.819e-07** & **0.999** & **0.064** \\ GPU vs HSDAG & 3.174e-05 & 0.999 & 0.441 \\   

Table 4: BERT performance on downstream tasks.

   Model & Inception-V3 & ResNet & BERT \\  Placeto & 2808s & 1162s & 4512s \\ RNN-based & 3706s & 1212s & OOM \\ HSDAG & **2454s** & **1047s** & **2765s** \\   

Table 5: Empirical runtime complexity comparison.