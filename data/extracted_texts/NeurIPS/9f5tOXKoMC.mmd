# A Bayesian Approach to Data Point Selection

Xinnuo Xu\({}^{}\)

Microsoft Research Cambridge

xinnuoxu@microsoft.com

&Minyoung Kim\({}^{}\)

Samsung AI Center Cambridge, UK

mikim21@gmail.com

&Royson Lee

Samsung AI Center Cambridge, UK

royson.lee@samsung.com

&Brais Martinez

Samsung AI Center Cambridge, UK

brais.mart@samsung.com

&Timothy Hospedales

Samsung AI Center Cambridge, UK

University of Edinburgh, UK

t.hospedales@ed.ac.uk

Part of this work was completed while Xinnuo was affiliated with Samsung AI Center Cambridge, UK.Equal Contribution.

###### Abstract

Data point selection (DPS) is becoming a critical topic in deep learning due to the ease of acquiring uncurated training data compared to the difficulty of obtaining curated or processed data. Existing approaches to DPS are predominantly based on a bi-level optimisation (BLO) formulation, which is demanding in terms of memory and computation, and exhibits some theoretical defects regarding minibatches. Thus, we propose a novel Bayesian approach to DPS. We view the DPS problem as posterior inference in a novel Bayesian model where the posterior distributions of the instance-wise weights and the main neural network parameters are inferred under a reasonable prior and likelihood model. We employ stochastic gradient Langevin MCMC sampling to learn the main network and instance-wise weights jointly, ensuring convergence even with minibatches. Our update equation is comparable to the widely used SGD and much more efficient than existing BLO-based methods. Through controlled experiments in both the vision and language domains, we present the proof-of-concept. Additionally, we demonstrate that our method scales effectively to large language models and facilitates automated per-task optimization for instruction fine-tuning datasets.

## 1 Introduction

Practical machine learning efficacy is heavily dependent on the choice, quality and quantity of training data, especially so in the case of neural networks that can easily fit every detail of the training set. This leads to challenges from how to learn reliably with imbalanced data , noisy data, noisy labels , and so on. Similarly there is often a key subset of data, which is most informative for a given learning problem, but buried among a much larger set of less relevant data. If the most salient data could be efficiently identified, learning could potentially be accelerated . All these challenges are only growing in the era of large scale training on web-scraped data, where curation and gold-standard quality control are not feasible.

Data Point Selection (DPS) algorithms aim to address these challenges by filtering or re-weighting the training data to reduce noise, imbalance, irrelevant background data and so on. The most establishedfamily of approaches [19; 15; 41; 45; 62] to DPS falls under the bi-level optimization or meta-learning umbrella, where one wraps the conventional learning problem with an outer loop that optimizes the dataset itself, so as to maximise performance on some validation set. These methods vary in the choice of their outer optimization variable (e.g., data point weights [41; 45; 19] vs mini-batch sampler ), the method of computing meta-gradients (e.g., reverse mode differentiation [45; 62] or reinforcement learning ), and the customization of their losses and other design parameters for the different scenarios (e.g., label-noise [45; 41], etc). However, all the BLO approaches are quite expensive in computation and/or memory, which limits their applicability to the most salient use case of large models trained on large web data.

In this paper we revisit the DPS problem from the perspective of Bayesian learning. Rather than constructing expensive nested optimization problems whose convergence is hard to analyse, we treat it as a problem of inferring the joint posterior over the main neural network parameters and instance-wise weights induced by a second weight-estimation neural network. This framework has several advantages in terms of being more efficient and scalable than typical BLO competitors and having a clear convergence guarantee. It is also able to address a variety of DPS-related problems - from noise and imbalance to data curation - within a single framework.

Our empirical results present proof of concepts for all these capabilities on a variety of learning tasks in vision and language. We also show a use case of automating Large Language Models (LLMs) instruction fine-tuning (IFT) data curation for specific downstream tasks. The available IFT datasets are large, diverse, and of varying quality. This means that a key activity for natural language processing (NLP) researchers and developers is often finding the right composition of IFT data sources to optimize particular use cases. Our framework can automatically resample and curate the wide array of available auxiliary IFT data to optimize performance for each NLP task of interest. To our knowledge, no BLO alternative has been demonstrated on billion-parameter scale LLMs. We name our approach **BADS** (**B**ayesian **D**ata Point **S**election).3

## 2 Our Approach

### Problem Setup

For the DPS problem, we assume that we are given two datasets: the _train_ set \(_{t}\!=\!\{z_{i}^{t}\}_{i=1}^{N_{t}}\) and the _meta_ set \(_{m}\!=\!\{z_{i}^{m}\}_{i=1}^{N_{m}}\). Each data point \(z_{i}\) can be an input-target pair \(z_{i}=(x_{i},y_{i})\) in the traditional (class-)labeled data scenarios. But in the autoregressive generative model scenarios (e.g., LLM), \(z_{i}\) is simply a sequence of tokens in which the inputs and targets are rather implicitly defined (e.g., all the tokens up until current time as input and the next token as target). We will use the notation \(l(z_{i};)\) for the loss of the model \(\) on the data point \(z_{i}\), which must be well-defined in both scenarios.

The meta dataset \(_{m}\) is considered as _in-domain_, meaning that the distribution of \(_{m}\) matches that of the downstream test task of interest. The size of \(_{m}\), denoted by \(N_{m}\), is typically small, due to the cost of curation/annotation processes in practice. The train dataset \(_{t}\) consists of _out-of-domain_ samples, possibly noisy, imbalanced, and uncurated, but the size \(N_{t}\) is usually large. The goal of DPS is to select a (soft weighted) subset of the train set \(_{t}\) with the guidance of the meta set \(_{m}\), so that the model trained on the selected train and meta dataset points performs well. Typical baselines include training with the meta-set alone, or union of meta and train sets.

Perhaps one of the most widely adopted DPS techniques is the bi-level optimisation (BLO) formulation of the problem [41; 58]. Letting \(w_{i}\) (\(\!0\)) be the weight (or importance) variable associated with the training data point \(z_{i}^{t}\), the main intuition is to find the weights \(w_{+}^{N_{t}}\) such that the model \(\) trained with the weighted train data with weights \(w\) yields the best performance on the meta set. More formally, this leads to the following BLO problem:

\[_{w_{+}^{N_{t}}}_{j=1}^{N_{m}}l(z_{j}^{m};^{*}(w)) ^{*}(w)=_{}_{i=1}^{N_{t}}w_{i}  l(z_{i}^{t};). \]

However, a critical drawback is that solving this difficult problem is costly in computation and/or memory, and unrealible due the practical heuristics required. Typical BLO solutions to obtain the hypergradient \(dl/d\) rely on approximate Hessian estimation or reverse mode differentiation with few-step SGD approximation of the inner optimisation. Aside from cost, for practical neural network implementations computed over _minibatches_, there is no theoretical guarantee for convergence.

### (Our Approach) Bayesian Data Point Selection (BADS)

We view the DPS problem from a completely different perspective, and tackle it via Bayesian learning. Our model's generative process, that is, the graphical model representation, is depicted in Fig. 1. The main neural network model parameters set \(\) is a random variable, which can generate data points in the meta dataset \(_{m}\) (precisely speaking, the backbone \(\) generates the target part of each data point). To make use of the train set \(_{t}\) in an appropriate way, we constrain \(\) to follow a prior distribution governed by the weighted train data with weights \(w_{+}^{N_{t}}\) which are also random variables. Before observing the meta set \(_{m}\), the weight vector \(w\) follows a prior distribution \(p(w)\) - a specific distributional choice for \(p(w)\) will be discussed later. Given \(w\) and \(_{t}\), our backbone \(\) has to be compatible with the weight data \(\{(w_{i},z_{i}^{t})\}_{i=1}^{N_{t}}\). This can be interpreted as placing a _weighted-data-driven prior_ on \(\), more specifically,

\[ p(|w,_{t}) p( )_{i=1}^{N_{t}}p(w_{i},z_{i}^{t}|) \]

where \(p()\) is a base prior (e.g., 0-centered Gaussian that amounts to weight decay regularisation), and \(p(w_{i},z_{i}^{t}|)\) can be defined from the loss, e.g., \((-w_{i} l(z_{i}^{t};))\), following the conventional tricks [36; 25]. Then given \(\), the meta data are generated following the likelihood defined as:

\[ p(_{m}|)_{j=1}^ {N_{m}}(-l(z_{j}^{m};)) \]

The equations (2) and (3) fully constitute the prior and likelihood for our Bayesian model. Our ultimate goal is to describe the distributions of \(\) and \(w\)_after_ observing all evidences \(_{t}\) and \(_{m}\), which boils down to the posterior inference \(p(,w|_{t},_{m})\). Formally, we have:

\[ p(,w|_{t},_{m})  p(w) p(|w,_{t}) p(_{m}|) \]

The detailed derivations for Eq. (4) can be found in Appendix A. However, it is widely known that (4) does not admit any closed-form expressions. One main difficulty arises from the intractable normalizing constant in (4).

Stochastic Gradient Langevin Dynamic Sampling.For computationally efficient posterior inference, we adopt the stochastic-gradient MCMC technique, specifically the Stochastic Gradient Langevin Dynamic (SGLD) sampling . Applied to our model, we can obtain samples from the posterior \(p(,w|_{t},_{m})\) by running the Langevin dynamic system (until convergence, i.e., mixing):

\[[,w]\;\;[,w]+_{,w} p(,w|_{t},_{m})+, (0,I) \]

where \(\) is a small (constant) step size. There are two critical benefits: i) Since we differentiate the log-posterior, the difficult normalizing constant in (5) will disappear; ii) The update (5) is essentially gradient descent with additive Gaussian noise, leading to a computationally efficient update.

Going one step further, even though the log-posterior involves the entire train data (and entire meta data), it is shown in  that the stochastic-gradient version (SGLD) that replaces the whole batch likelihood with a minibatched one, theoretically guarantees that the SGLD update converges to the posterior samples. More specifically, the SGLD update equations (one for \(\) and the other for \(w\)) can be written as follows:

\[ \;+_{} p( )-N_{t}_{i_{t}}w_{i} l(z_{i} ^{t};)-N_{m}_{j_{m}}l(z_{j }^{m};)+_{} \] \[w \;w+_{w} p(w)-N_{t} _{i_{t}}w_{i} l(z_{i}^{t};) +_{w} \]

Figure 1: Graphical model for _BADS_. Shaded nodes, representing curated (\(D_{m}\)) and uncurated (\(D_{t}\)) data, are evidence. Unshaded nodes, including model \(\) and instance weights \(w\), are random variables.

where \(_{t}\) and \(_{m}\) are minibatches from \(_{t}\) and \(_{m}\), respectively, and \(_{},_{w}(0,I)\) are independent Gaussian samples.

Repeating (6) and (7) for a sufficient amount of iterations (until we reach good mixing) leads us to posterior samples \((,w)\). There are several options to take these samples for a final model for test prediction. One option is to collect latest \(M\) samples (either consecutive collection or thinning to take every \(k\)th samples) from the iterations, and either take the average as _posterior means_ or perform full Bayesian treatment with the collected samples. Alternatively, we can just take the last single iterate \((,w)\) as a point representative for the posterior distribution. For simplicity, we take the latter approach, which also works well empirically.

### Interpretation and Benefits

InterpretationWe discuss several intuitions and implications of our proposed approach (Eq. 6-7). First, looking at the \(\) update Eq. (6), our model essentially updates \(\) in a way that it decreases the loss on the _combined data_ of the whole meta data points and the weighted train data points with the current weights. This is a fairly intuitive strategy provided that the weights are properly determined. Then the next question is how the weights are determined. If we inspect the \(w\) update (Eq. 7), and take the gradient of the expected loss term with respect to \(w\) directly, we see that: i) those train data points \(z_{i}^{t}\)s with _smaller_ losses at current backbone \(\) will get _higher_ weights \(w_{i}\)s; ii) those train data points \(z_{i}^{t}\)s with _larger_ losses at current backbone \(\) will get _lower_ weights \(w_{i}\)s. This essentially means that our model performs **loss alignment** for DPS - In the course of training/update, once the backbone \(\) enters a good regime in the parameter space such that \(\) can assign (valid) low loss values on the in-domain meta data points, then it starts putting high weights on those train data points that have low losses under the current backbone. In other words, the model will assign high weights to those train data points that are well-aligned with the meta data points in terms of loss.

Benefits over BLOOur Bayesian approach provides several benefits over BLO: (1) Efficiency. Our SGLD is efficient so does not require computationally demanding Hessian computations like implicit function theorem based methods (cf: ) or huge memory demand like reverse-mode differentiation methods [41; 19]. (2) Sparsity. Our method straightforwardly achieves sparsity on the \(w\) weights allowing efficient sample selection unlike . (3) Reliability. BLO-based methods rely on approximations (truncation, or Hessian approximations) for practical feasibility that make finding optimal solutions unreliable. Our straightforward Bayesian approach has reliable convergence properties thanks to being a standard application of SGLD.

Convergence of our SGLD algorithmIn Appendix C we provide a theorem showing that our SGLD algorithm converges to the true posterior. Our analysis is based on  where we make some adjustments for our case.

### Implementation Details

Choice of PriorsFor the base prior \(p()\), i.e., the prior before being driven by the weighted data, we adopt \(0\)-mean Gaussian, which amounts to adding the weight decay regularisation for \(\). For the weight prior \(p(w)\), we have made a careful design effort to come up with a viable sparsity inducing prior. Although encouraging sparsity in learned weights is ideal to avoid overfitting, during our initial experiments we have found that most of the weights eventually tend to vanish to \(0\), which is not what we actually want. We need to be able to impose both sparsity and a certain level of non-zero weights. To this end, we first introduce a hyperparameter \(\) (e.g., 0.01) for the target sparsity level that we want to attain. Roughly saying, among the \(N_{t}\) training data points, we aim to select \( N_{t}\). For the (soft) weights, we impose \(_{i_{t}}w_{i} N_{t}\), which can be encoded in the prior form as:

\[p(w) e^{-_{i}w_{i}- N_{t}^{2 }/2^{2}} \]

where \(\) controls the strength of the regularisation. One technical difficulty in directly plugging (8) into the weight update (7) is that we have to load the whole \(\{w_{i}\}_{i=1}^{N_{t}}\) in memory for backprop. To avoid this issue, we use the following fact:

\[_{i_{t}}w_{i}_{i_{t}}w_{i}+(N_{t}- |_{t}|) \]where \(\) represent historic running average of the entire weights. We basically build a computation graph only for the first term of batch \(_{t}\) weight sum, and regard the (historic) running average of the entire weights as constant during backprop. After each SGLD iteration, we update the running weight average with the new updated weights on the recent batch. We use the simple averaging scheme for the running average. To approximate the average weight \(\) precisely, we only conduct the average over the most recent \(s_{avg}\) step.

Introducing impact constantsIn the SGLD principle, we have the log-likelihood terms that are proportional to the sizes of the datasets. In particular, we have \(N_{t}\) and \(N_{m}\) in (6). However, this scheme does not properly capture our preference to the in-domain meta data set in contrast to the noisy, out-of-domain train data set. To this end, we introduce the impact constants (hyperparameters) in the update equations where we downweigh or upweigh the loss terms of train and meta sets.

Weight NetworkInstead of directly optimising individual weights \(w_{i}\), we can consider a weight network, \(w_{i}=w(z_{i}^{t};)\), a neural network with parameters \(\) that takes the train data point \(z_{i}^{t}\) as input and returns its weight \(w_{i}\) as output. We can then regard \(\) as random variables and the \(w\) update equation can be modified accordingly for \(\) update straightforwardly. This weight network approach can be useful for smoothing/regularising the output weights thanks to the smooth functional property of neural networks. Furthermore, if one needs to supplement the train dataset with extra new train samples after the model training, the learned weight network can be used for assigning weights or selecting samples from the new set, without retraining the whole model from the scratch. The posterior distribution similar to Eq. (4) is derived in full detail in Appendix B.

## 3 Experiments: Proof of Concept

In this section, we assess the effectiveness of the proposed _BADS_ method in three critical scenarios where DPS is essential: Data Balancing, Data Denoising, and Efficient Learning. We begin by introducing the baseline systems and then present the experimental results for each of these scenarios.

### Baselines

There are three types of DPS setups with different supervision signals:

* _Unsupervised DPS_ selects data without the guidance of a held-out meta set [47; 42]. Instead, it is guided by human-defined hypotheses, such as "challenging examples improve model performance". This approach aligns with curriculum learning. We include the online variant of AskLLM , i.e. **AskLLM-O**, in our baseline comparisons. It selects examples from training set by querying a pretrained OpenLLaMA 3B to obtain the sampling score for each training sample.4 * _Self-supervised DPS_ selects data with the guidance of a held-out meta set. However, the meta set does not share the same data distribution as the targeted test set [13; 4; 14; 49]. Typically, the examples in the meta set are selected from the training set based on specific hypotheses, such as "learnable examples enhance model performance". We include two approaches in our baseline comparisons: **Contrastive Data Selection (CDS)** is tailored for data denoising. The algorithm assigns weights to each data point in \(_{t}\) according to the difference between the _denoised_ and the _noisy_ log probability, predicted using a denoised and a noisy model trained on a clean dataset and an uncurated dataset, respectively. These weights are then used to sample data points from minibatches in the training of LM.5 Similar to CDS, **ClassAct** utilizes small proxy models trained on a limited portion of \(_{t}\) to calculate learnability scores for the remaining training data points.6 * _Meta-set guided DPS_ selects data with the guidance of a small meta set that shares the same distribution as the test set, aiming to train a model that excels specifically on the target test set. The test set may encompass one or multiple downstream domains or tasks. This DPS is closely related to meta learning, domain adaptation, and transfer learning. Current methods primarily rely on **Bilevel Optimization (BLO)** for purposes such as data denoising [19; 37], data balancing ,and efficient learning [28; 59; 29]. Considering both performance and code availability, we use the online BLO7[41; 19] as our baseline. Our approach, **BADS**, also falls under this category.

To ensure a fair comparison, we also introduce several baselines that train the backbone models using different combinations of the meta set and training set: **Mixing** trains the model using a combination of the _train_ set \(_{t}\) and the _meta_ set \(_{m}\). **Meta_Only** trains the model exclusively on \(_{m}\). **Random_Select** uses \(_{m}\) combined with a randomly selected subset from \(_{t}\). **Duplicate_Meta** utilize \(_{t}\) along with multiple copies of the _meta_ set, duplicating \(_{m}\) until it matches the size of \(_{t}\).

Note that, the selection ratio/sparsity level in _AskLLM-O_, _ClassAct_, _Random_Select_, _BLO_, and _CDS_ is the same as in _BADS_.8

### Scenario 1: Data Balancing (MNIST)

In this scenario, we assess the model's capability to manage an imbalanced _train_ set, \(_{t}\). Despite being trained on this imbalanced dataset, the models are expected to perform effectively on a balanced test set. Following the setup in , we use the standard MNIST handwritten digit classification dataset  to create a class-imbalanced binary classification task. A total of 5,000 images from classes 4 and 9 were selected as the _train_ set \(_{t}\), with class 9 dominating the training data distribution (4,975 examples) and class 4 having only 25 examples. A balanced _meta_ set \(_{m}\) is created by selecting another 25 examples from each of these two classes, ensuring no overlap between \(_{t}\) and \(_{m}\). The models are tested on the original MNIST test set, including only classes 4 and 9.

In this scenario, the loss function is defined using the standard binary cross-entropy loss. Following , all classification models are LeNet5 . The training is conducted on a single GPU, using SGD with a fixed learning rate of 1e-3 and a mini-batch size of 100, over a total of 15,000 steps. In _BADS_, the weight network is implemented as a single-layer Feedforward Neural Network (FNN) with a sigmoid activation function. It takes the top-layer image embeddings from LeNet5 as input

Figure 3: The MNIST test accuracy when trained with meta sets in varying sizes (x-aixs).

Figure 2: Proof-of-Concept experiment results. The **top** row displays the overall test performance across the three scenarios throughout the training phase, with x and y axis denote the training steps and the evaluation metrics, respectively. The **bottom** row visualizes the model-predicted weights of data points in each mini-batches in the final 2000 steps in WebNLG training (scenario 3). x and y axis show the training steps and average weights, respectively. Data points in **blue** color are expected to get higher weights compared to their counterparts (in **red** color).

and outputs a weight \(w_{i}\) for each image. The learning rate for the weight network is 1e-3 and the target sparsity level \(\) is 0.005. Other hyperparameters, including those in the baselines, are detailed in Table 3 (Appendix E).

#### 3.2.1 Experiment Results and Ablation Study

The classification accuracy is presented in the top-left plot in Figure 2. All approaches, except for _Mixing_ and _ClassAct_, achieve over 90% accuracy even when trained with a highly imbalanced _train_ set. _Meta_only_ demonstrates that training with a small amount of balanced data yields significantly better performance compared to training with a larger but imbalanced training set (_Mixing_). Both _BLO_ and _BADS_ outperform non-DPS baselines in terms of both accuracy and convergence speed, with _BADS_ further outperforming _BLO_ by a noticeable margin. _CDS_ underperforms all the non-DPS baselines, which we believe is due to the mini-batch-level discrete data selection. When dealing with an extremely imbalanced _train_ set, it is possible that the minority class might not be present in some mini-batches. Under these circumstances, the model is compelled to learn from the top examples from the majority class in each mini-batch, potentially leading to biased training outcomes. The top row in Figure 13 and the left plot in Figure 14 (Appendix F) visualizes the weights assigned to the examples in each mini-batch by DPS approaches. All methods, except for_ClassAct_, effectively assigns higher weights to the minority class than to the majority class, thereby directing the classifiers to focus more on the minority class in training.

We further evaluate the models' performance using _meta_ sets \(_{m}\) of various sizes, with 5, 10, 25, and 50 examples per class included.9 As illustrated in Figure 3, with a very limited number of meta examples (5 per class), only _BADS_ and _BLO_ achieve over 90% accuracy on the balanced test set. As the number of available meta data increases, _BADS_ consistently leads in performance. However, when sufficient meta data is provided, the gap between _BADS_ and the other approaches narrows.

### Scenario 2: Data Denoising (CIFAR)

In this scenario, we evaluate the model's ability to manage a noisy _train_ set \(_{t}\). Although the models are trained using a dataset with significant noise, they are anticipated to perform well on a clean test set. Our experiment utilizes the standard CIFAR 10-class classification dataset . Following the standard CIFAR train/validation set split ratio, we first create a clean and balanced _meta_ set \(_{m}\) by randomly sampling 1000 examples from each class in the training data. Then, we use the remaining 40,000 examples to create a noisy _train_ set \(_{t}\) by introducing noise based on the symmetric noise injection setup described in . We set the noise ratio to 0.5 and use the original CIFAR-10 test set in testing.

We use ResNet32  as the backbone classification model. Training is performed on a single GPU using SGD with a fixed learning rate of 1e-1 and a mini-batch size of 120 over 20,000 steps. The loss function is the standard multi-class cross-entropy loss. For BADS, the weight network structure retains the same as in scenario 1. However, in this scenario, data point weighting takes into account both the image and its associated label. We represent each label using a one-hot embedding, which is then concatenated with the top-layer image embeddings from ResNet32 and fed into the weight network. The learning rate for the weight network is set to 1e-4. Note that in Eq 7, the gradient of \(\) become large if \(N_{t}\) is big. Therefore, we reduce the learning rate of the backbone classification model to 1e-4. The target sparsity level \(\) is set to 0.8.10.

#### 3.3.1 Experiment Results and Ablation Study

The classification accuracy is shown in the top-middle plot in Figure 2. All methods, except for the _BLO_ approach11, manage to achieve over 60% accuracy, even using a _train_ set contaminated by 50%.

Figure 4: The CIFAR test accuracy when trained with 80% noisy data.

Notably, _CDS_, _ClassAct_ and _BADS_ deliver the highest three performances, with _BADS_ surpasses all other methods by a noticeable margin. The bottom row of Figure 13 and the middle plot of Figure 14 (Appendix F) visualizes the weights allocated to the examples in each mini-batch by DPS approaches. All methods, except for _ClassAct_, consistently gives lower weights to the noisy examples, guiding the classifiers to disregard the noisy examples during training. _ClassAct_ assigned lower weights to the noisy examples during the initial stages of training. However, these weights exceeded those of the clean examples in the later phases of training. Interestingly, this behavior did not significantly impact the model's performance.

If we raise the noise label ratio in the _train_ set to 80% (Figure 4), both _BLO_ and _BADS_ still lead the performance, with _BADS_ exceeds _BLO_ and non-DPS approaches by 15% and 20% in classification accuracy, respectively. _CDS_ does not exceed the performance of non-DPS approaches and starts to overfit on the noisy data after 15,000 training steps. When we lower the noise label ratio to 20% (Figure 5), all methods achieve a classification accuracy of around 80%. Although _BADS_ continues to outperform both DPS and non-DPS approaches, the lead is narrower.12

### Scenario 3: Efficient Learning (WebNLG)

In this scenario, we assess the model's ability to adapt to new domains with very few data points. To demonstrate the robustness of BADS across various research topics and backbone models, we focus on the Natural Language Generation (NLG) task in this section. NLG  aims to accurately generate textual descriptions from input tuples. An example is shown in Figure 7 (Appendix D). The English benchmark introduced in WebNLG 2020  includes a training set spanning 16 domains and a test set covering 3 different domains (for details, check Appendix D). We use the original training set (14,239 examples) as our _train_ set \(_{t}\), and create a single clean and balanced _meta_ set \(_{m}\) by randomly sampling 30 examples from the WebNLG 2020 validation set in each test domain.

All backbone models are the encoder-decoder T5-small . Training is on a single GPU using Adam with a fixed learning rate of 3e-5 and a mini-batch size of 20 over 40,000 steps. The loss function is the standard negative log likelihood loss. In BADS, the weight network structure remains the same as in scenario 1. Its input is the embedding of the input sequence, represented by the contextual embedding of the "[EOS]" token, and the learning rate is set to 1e-4. The sparsity level \(\) is 0.05.

#### 3.4.1 Experiment Result

The BLEU scores are displayed in the top-right plot of Figure 2. _BADS_ leads the performance, achieving a 2 BLEU score advantage over the second-best system, _Duplicate__Meta, and surpassing the remaining systems by more than 5 BLEU scores. The other three DPS approaches do not distinguish themselves from the non-DPS methods.

In this scenario, we opt for a more controlled examination of data selection effectiveness due to the big amount of domains in the _train_ set13. We select three occupation-centric domains--_Athlete_, _Politician_, and _Astronaut_--as our test domains. Additionally, we create a _meta_ set \(_{m}\) by randomly selecting 50 examples in each of these domains from their WebNLG 2020 validation set. We then choose two distinct domains, _Artist_ and _City_, for training. _Artist_, as another occupation-centric domain, shares a similar schema and vocabulary with the test domains, whereas _City_ does not. Ideally, DPS should prioritize the _Artist_ examples over those from _City_. The BLEU scores for text descriptions generated by the DPS methods are shown in Figure 12 (Appendix F). These results reinforce the findings from the original experimental setup, with _BADS_ outperforming both _BLO_ and _CDS_ by over 10 BLEU scores. The weights given to the

   Methods & **Mem (MB)** & **Time (s)** \\   Base & 9184.4 & 61.2 \\ BLO & 22361.3 & 113.48 \\ CDS & 9183.9 & 62.0 \\ -weight calc & – & +700.0 \\ ClassAct & 34821.36 & 269.81 \\ AskLLM-O & 32908.89 & 115.24 \\ -LLM call & – & +13932 \\  BADS & 14694.58 & 61.03 \\   

Table 1: The average GPU memory and time usage over 100 steps. “Base” represent all non-DPSs.

Figure 5: The CIFAR test accuracy when trained with 20% noisy data. We still show the weighting plots for the 16 domain in the bottom row of Figure 13 in Appendix F.

examples in each mini-batch are visualized in the bottom row of Figure 2. _BADS_ effectively prioritizes the examples in the _Artist_ domain. Instead, _BLO_ fails to differentiate between the two domains, and _CDS_ incorrectly weights the opposite domain higher. This illustrates that the effectiveness of DPS is linked to the overall performance of the models.

#### 3.4.2 Latency

We evaluate the average GPU memory and time consumption using the WebNLG task. Table 1 shows that the training time for _BADS_ and _CDS_ is similar to that of non-DPSs, while _BLO_ and _AskLLM-O_ takes nearly twice as long, and _ClassAct_ takes even longer. Note that _CDS_ and _AskLLM-O_ requires additional time to weight examples in the _train_ set. Given that the WebNLG 2020 _train_ set consists of 35,426 examples, we execute the weighting with a batch size of 20, _CDS_ takes a total of 700 seconds. For larger-scale tasks, such as foundation model fine-tuning (Section 4), the weighting process is estimated to consume approximately 22 hours per 1 million training examples. The offline scoring in _AskLLM-O_ takes around four hours in our setup on a single NVIDIA A40 GPU. In terms of memory usage, _CDS_ aligns with non-DPS approaches, while _BADS_ and _BLO_ require approximately 1.5 and 2.5 times more GPU memory, respectively. _ClassAct_ and _AskLLM-O_ takes even more GPU memory.

## 4 Use Case: Large Language Model Instruction Fine-tuning

Instruction Fine-tuning (IFT) for LLMs is a practical application where all three mentioned scenarios are encountered simultaneously. IFT data can be acquired through prompting LLMs , gathering existing Natural Language Processing (NLP) benchmarks , or employing human annotation . Noise is likely to accumulate during each of these data collection methods. Furthermore, since NLP benchmarks often vary greatly in size, the IFT data typically lacks balance. Additionally, the IFT data does not include data points from the downstream tasks, leading to domain shift in testing.

We use the same IFT data as  as our _train_ set \(_{t}\), which is a mix of FLAN V2 , COT , DOLLY , and OPEN ASSISTANT 1 . Following , we focus on four downstream tasks: MMLU , which consists of multiple-choice questions across 57 sub-tasks, ARC-challenge/-easy , and HellaSwag . Following , 5 examples were selected from each sub-task to create the _meta_ set \(_{m}\) for MMLU, totaling 285 examples. Additionally, following , for the other tasks, we randomly chose 25 examples from their validation set to create the respective _meta_ sets. To facilitate checkpoint selection, we additionally create a validation set of equivalent size to the _meta_ sets.

Due to limited computational resources, we use OpenLLaMA 3B 14 as the backbone model. Training uses one A40 GPU utilizing Adam with a fixed learning rate of 3e-5 and a mini-batch size of 3. In BADS, while the weight network remains the same as described in scenario 3, we modify the input to be the average contextual embedding of all tokens in the sequence. The sparsity level \(\) is 0.05. Results are presented in Table 2. We excluded _BLO_ from this experiment due to their prohibitive GPU memory usage. Given that the training data for the _Mixing_ baseline predominantly consists of IFT data points, it is considered as a standard IFT process. Table 2 indicates that while individual methods may excel in specific tasks, none of the non-DPS baselines nor the _CDS_ and _ClassAct_ consistently surpass the others across all downstream tasks. However, _BADS_ stands out by consistently outperforming all other baselines across every task, except for AskLLM-O, which, as indicated in Table 1, demands significantly more computational resources. According to Table 4 (Appendix F) BADS prefers the human-written-from-scratch, OPEN ASSISTANT 1 and DOLLY, over the rest two created using existing benchmarks.

  Methods & **MMLU** & **ARCc** & **ARCe** & **HellaSwag** \\   Mixing & 25.16 & 33.79 & 64.65 & 51.97 \\ Meta\_Only & 25.51 & 32.08 & 52.23 & 52.07 \\ Random\_Select & 25.62 & 28.92 & 66.75 & 51.83 \\ Duplicate\_Meta & 25.30 & 33.28 & 65.15 & 52.76 \\ CDS & 25.62 & 21.16 & 60.14 & 51.68 \\ ClassAct & 24.90 & 31.91 & 57.34 & 52.09 \\ AskLLM-O & 25.55 & **35.15** & 66.88 & **53.71** \\  BADS & **26.59** & 34.39 & **67.00** & 52.91 \\  

Table 2: Test accuracy of LLMs across four popular benchmarks in eval-harness . Checkpoint selection is using next token prediction accuracy as the selection metric. _Mixing_ represents standard IFT.

To facilitate the use of our proposed datapoint selection method, we provide comprehensive guidelines for hyperparameter tuning in Appendix E. We also include an in-depth discussion and ablation study on the influence of hyperparameters on the method's effectiveness.

## 5 Related Work

Recent works on DPS broadly fall into four categories: i) Approaches based on meta learning (or BLO), ii) Gradient-based methods, iii) Methods based on domain adaptation and transfer learning methods, and iv) General sample reweighing strategies.

\(\)**Meta learning (BLO) approaches.** The DPS can be formulated as a meta learning BLO problem where the outer loss is defined with the training data selection variables, and the inner optimisation is to minimize the model's loss with the selected data [19; 15; 41; 45; 62]. These methods vary in the choice of their outer optimization variables: either directly using the data point weights [41; 45; 19] or mini-batch samplers . To solve the BLO, most approaches rely on computing meta-gradients via reverse mode differentiation [45; 62] while some works utilised reinforcement learning techniques . All these BLO-based methods are computationally demanding with large memory footprint, hindering them from being applied to large-scale models/data.

\(\)**Gradient-based methods.** The key idea is to measure the importance of training data points based on the alignment scores between the loss gradients on training and meta data points. The rationale behind the gradient alignment can be theoretically underpinned by the BLO perspective. As shown in , the one-step inner loss update with zero initial weights in BLO reduces to the gradient alignment (cosine angle) between the train and meta data points . However, the method in  requires evaluating and storing gradients of the entire training data points, thus computationally expensive. Furthermore, the final solution may not be optimal since the gradient computation is done with an initial network that is warm-up trained with a random subset of the training data. In  the weights are optimised by the expected gradient norms during the network training, which serves as a proxy for the importance of data points. In  they find the subsets that closely match the gradient of the training and meta sets using an orthogonal matching pursuit algorithm. Under online continual learning setup , they formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning.

\(\)**Domain adaptation and transfer learning methods.** Selecting a subset of the train set that are best aligned with the in-domain meta set can be naturally seen as _domain adaptation_ and _transfer learning_. In the NLP community,  finds that the large language models implicitly learn sentence representations that cluster by domains without supervision, while in , they show the effectiveness of domain-adaptive pre-training with data augmented using simple data selection strategies. In  they proposed domain adaptive transfer learning which computes the importance weights on the target dataset from the ideas in domain adaptation and estimation of the label distribution. In multi-task transfer learning community, some previous works identify the detrimental effects of the gradients from different tasks, and propose strategies to mitigate the conflicting gradients issue [60; 11; 34].

\(\)**General sample reweighing strategies.** Since DPS essentially involves finding the optimal reweighed distribution with the underlying domain itself unchanged, several approaches aim to tackle the problem via importance sampling techniques. In  they derive a tractable upper bound to the per-sample gradient norm, and derive an estimator of the variance reduction achieved with importance sampling. The curriculum learning  is also closely related as one can design an optimal schedule of the successive training distributions. For instance, in  they introduce the so-called data parameters, associating samples and classes in the train data with learnable parameters, which governs their importance in the learning process within the curriculum learning framework.

## 6 Conclusion

We revisited the DPS problem from the perspective of Bayesian learning. By treating the neural network of interest, as well as an auxiliary weighting neural network as random variables and inferring their joint posterior using SGLD, we achieve a simple and effective approach to data reweighting that is more reliable and scalable than BLO alternatives. Our framework is straightforwardly applicable to learning with data imbalance, label noise, and automating auxiliary data curation. We demonstrate our framework can apply to automating curation of the wide variety of auxiliary instruction fine-tuning data available for billion-scale language models. Overall this demonstrates a promising new kind of approach to the growing need for data optimization in neural network learning.

## Limitations

Our proposed _BADS_ algorithm has the following limitations, which we plan to address as our future work.

1. There are several hyperparameters involved, which need to be carefully tuned for best performance. These include: the sparsity level parameter \(\), the relative impact constants \(\)'s of the objective terms in our SGLD update (detailed in App. E), and the standard hyperparameters (e.g., batch size, learning rates, weight decay). Some of these hyperparameters may be optimised via Bayesian model selection in a principled manner. For instance, the sparsity level \(\) can be regarded as a latent random variable (a part of the model) with a proper prior distribution imposed on it, and we can do posterior inference of \(\) (or marginalise it out) together with \(w\) and \(\) in our SGLD update equations. We will pursue this in our future study.
2. Although our method is computationally far more efficient than BLO and some other DPS approaches, its GPU memory footprint is demanding compared to non-DPS algorithms. This mainly originates from the entire weight variables or the whole weight network parameters loaded/maintained in the memory for frequent updates. One possible workaround is to load only the weight variables that are associated with the current minibatches. We will be investigating this code optimisation further in our ongoing future study.