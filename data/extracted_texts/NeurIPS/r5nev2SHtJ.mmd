# From Causal to Concept-Based

Representation Learning

Goutham Rajendran\({}^{1}\)

Equal Contribution

Simon Buchholz\({}^{2}\)

Equal ContributionMachine Learning Dept., Carnegie Mellon University, Pittsburgh, USA

Bryon Aragam\({}^{3}\)

Bernhard Scholkopf\({}^{2,4}\)

Pradeep Ravikumar\({}^{1}\)

\({}^{1}\)Machine Learning Dept., Carnegie Mellon University, Pittsburgh, USA

\({}^{2}\)Max Planck Institute for Intelligent Systems, Tubingen, Germany

\({}^{3}\)University of Chicago, Chicago, USA

\({}^{4}\) ELLIS Institute, Tubingen, Germany

###### Abstract

To build intelligent machine learning systems, modern representation learning attempts to recover latent generative factors from data, such as in causal representation learning. A key question in this growing field is to provide rigorous conditions under which latent factors can be identified and thus, potentially learned. Motivated by extensive empirical literature on linear representations and concept learning, we propose to relax causal notions with a geometric notion of concepts. We formally define a notion of concepts and show rigorously that they can be provably recovered from diverse data. Instead of imposing assumptions on the "true" generative latent space, we assume that concepts can be represented linearly in this latent space. The tradeoff is that instead of identifying the "true" generative factors, we identify a subset of desired human-interpretable concepts that are relevant for a given application. Experiments on synthetic data, multimodal CLIP models and large language models supplement our results and show the utility of our approach. In this way, we provide a foundation for moving from causal representations to interpretable, concept-based representations by bringing together ideas from these two neighboring disciplines.

## 1 Introduction

A key goal of modern machine learning is to learn representations of complex data that are human-interpretable and can be controlled. Although existing models are known to extract features that are useful and intuitive to humans (e.g. color, shape, size), the sensitivity of these models in capturing these features is notorious : The enormous capacity of contemporary models means that it is possible to reconstruct the input data with meaningless representations (e.g. posterior collapse ). Accordingly, understanding how and when useful features can be captured, along with providing assurances on the quality of learned representations, is an ongoing endeavor .

A natural approach to this problem is to model the input data \(X=(X_{1},,X_{d_{x}})\) as \(X=f(Z)\), where \(f\) is a nonlinear transformation that maps structured underlying latent generative factors \(Z=(Z_{1},,Z_{d_{z}})\) to \(X\), and then to attempt to recover the model parameters \(Z,f\) from \(X\). This is an appealing approach since it implies no restrictions on the data \(X\), and has the interpretation of recovering "ground truth" factors that generated the data. It is well-known that without additionalassumptions, this is impossible , a fact which has led to a long line of work on nonlinear ICA  and unsupervised disentanglement . One approach to resolve this limitation is to assume that \(Z\) has an intrinsic causal interpretation. This is known as Causal Representation Learning (CRL) , which endeavors to identify useful representations through the lens of causality. Structurally, the latent factors \(Z\) are assumed to have causal relationships among them, which enables us to reason about effects of interventions and conditioning on these latent factors. CRL studies this setting via an intricate interplay of ideas from causality, latent variable modeling and deep learning, with the main goal being to reconstruct the mixing function \(f\) and \(Z\), the true generative factors of data, by leveraging causal assumptions. Recent years have witnessed a surge of rigorous results on provably learning causal representations under different assumptions . For example, as long as we have access to interventions on each latent variable \(Z_{j}\) (a total of at least \(d_{z}\) interventions), under weak assumptions on \(Z\) and/or \(f\), the causal model over \(Z\) as well as the model parameters \((Z,f)\) can be uniquely identified .

While causal features are intrinsically desirable in many applications, the assumption that we can feasibly perform \((d_{z})\) interventions merits relaxing: Indeed, in complex models, the number of true generative factors \(d_{z}=(Z)\) might be intractably large (e.g. consider all of the latent factors that could be used to describe natural images, video, or text). At the same time, there are yet many other applications where the strict notion of causality may not be needed, and moreover it may not be necessary to learn the _full_ causal model over every causal factor. Is there a middle ground where we can simultaneously identify a smaller set of interpretable latent representations, without the need for a huge number of interventions?

In this paper, we study this problem in detail and provide an alternative setting under which latent representations can be provably recovered. Instead of recovering a (potentially huge) number of causal latents with interventions, we settle for recovering a smaller number of _interpretable_ concepts without strict interventions. The basic idea is to recover _projections_\(AZ\) of the generative factors \(Z\) that correspond to meaningful, human-interpretable concepts through _conditioning_ instead of intervention. The idea to model concepts as linear projections of the generative factors is derived from a growing body of literature (e.g. , see Section 3 for even more references) showing that the embeddings learned by modern, high-performant foundation models are not inherently interpretable, and instead capture interpretable concepts as linear projections of the (_apriori_) unintelligible embeddings. While this approach sacrifices causal semantics, it makes up for this with two crucial advantages: 1) Instead of strict interventions in the latent space, it suffices to _condition_ on the concepts, and 2) When there are \(n\) concepts of interest to be learned, only \(n+2 d_{z}\) such concept conditionals are needed.

We validate and utilize our theoretical ideas via experiments. First, we validate these theoretical insights on synthetic data, where we use a contrastive algorithm to learn such representations for a given collection of concepts. Moving ahead to real-world data, we probe our theory on embeddings learned by multimodal CLIP models . The training scheme for CLIP aligns with our theoretical setting and therefore, it's reasonable to ask whether they satisfy our observations. Indeed, we show that the concepts in the 3d-Shapes dataset approximately lie in hyperplanes, further supporting our theoretical results. Lastly, we show an effective application of our framework to alignment of large language models (LLMs) where we extend the alignment technique of  to make LLMs more truthful.

ContributionsIn summary, our contributions are:

1. We formalize the notion of distributions induced by abstract concepts in complex domains such as images or text (see Secion 2 for an overview and Section 4.2 for formal definitions). Our definition of concept conditional distributions allows both continuous and fuzzy concepts.
2. We prove near-optimal identifiability results for learning a collection of concepts from a diverse set of environments in Theorem 1. Thus our work can be interpreted as a new direction for identifiable representation learning in order to study when interpretable concepts can be recovered from data.
3. We then verify our guarantees via a contrastive learning algorithm on synthetic data. In addition in Section 6, we support our geometric definition of concepts and our identifiabilityresult by analysing image embeddings of CLIP-models and we utilize our ideas to improve alignment of LLMs to make them more truthful.

## 2 Overview

In this section, we describe our approach and put it in context of prior developments.

Defining concepts geometricallyOur starting point is a geometric notion that concepts live in linear directions in neural representation space, known as linearity of representations (see extensive references in Section 3).

To make this precise we assume that for observed data \(X\) that has an underlying representation \(Z\) with \(X=f(Z)\) where the latent variables \(Z\) follow an arbitrary distribution and \(f\) is a (potentially complicated) nonlinear underlying mixing map. We do not assume that \(f\) and \(Z\) correspond to a ground truth model or that the latent variables \(Z\) themselves are related to a causal model or are interpretable and instead only assume linearity of representations (well supported by prior works). In agreement with this hypothesis we define concepts as affine subspaces \(AZ=b\) of the latent space of \(Z\)s, i.e., to a concept \(C\) we assign an affine hyperplane \(H_{C}=\{Z^{d_{z}}:AZ=b\}\) in the embedding space and we say that \(X=f(Z)\) satisfies a concept \(C\) if \(Z H_{C}\). In this setting the usual goal of CRL to reconstruct \(f\) and \(Z\) seems to be unnecessary for many applications. Instead we focus on the more modest goal of identifying only a (small) set of _concepts we care about_, i.e., we want to be able to decide whether a datapoint \(X\) satisfies a concept \(C\). Our main result shows that it is possible to identify \(n\) concepts given access to \(n+2\) concept conditional distributions. We now compare natural assumptions on type of data for causal representation learning and the setting considered here.

From interventions to conditioningIt is worth contrasting here the difference between viewing a concept as a generic latent generative factor \(Z_{i}\) that non-linearly mixes together with other latent factors to yield the inputs \(X\), versus the geometric notion above, as specifying a linear subspace. In the former, the natural way to provide supervision, i.e. define concept distributions, is to simply intervene on a specific factor \(Z_{i}\) and set it to a particular value (see Section 3 for references).In the latter however, it is most natural to condition on the concept, i.e., \(Z H\).

This shift is aligned with the growing interest to relax the notion of interventions, and consequently dilute the notion of causality [15; 100; 4], although it is still open how to properly achieve this. Two key drivers of this trend are as follows. The first is that the number of additional datasets required is \(d_{z}\)[46; 71; 53; 14], which is infeasible in many settings 2. The second is that the various assumptions that go into these works are often difficult to achieve, such as requiring perfect interventions [111; 14]. Compared to interventional data, _conditional_ data is often easier to acquire, obtained by conditioning on particular values of the latent factors (see also Appendix B.2).

Concept conditional distributionsWe now formalize conditioning on a concept. The obvious approach to define concept conditional distributions is to simply condition on \(Z H_{C}\), so \(p_{C}(Z)=p(Z|Z H_{C})\) where \(p\) is a base distribution of \(Z\) on \(^{d_{z}}\). However, this suffers from the drawbacks that it is mathematically subtle to condition on sets of measure 0 and this does not account for inherent noise in the learned representations. Therefore we relax this strict conditioning by drawing inspiration from how data is collected in practice: We sample \(X\) from the base distribution and then keep it if it satisfies our concept \(C\). This leads us to define \(p_{C}(Z) p(Z)q(Z|C)\) where \(q\) is defined to be the probability that \(Z\) is _perceived_ to be in \(H\) by the data collector and can be chosen to incorporate noise in our data gathering scheme. Therefore, this can also be viewed from a Bayesian information gathering viewpoint, as well as a stochastic filter standpoint. This is the notion we study in this work

Figure 1: Concepts live in affine subspaces. The two subspaces in the figure correspond to the same concept but of different valuations.

(Definition 3) and we develop theoretical techniques to guarantee identifiability in this formulation. Depending on the specific setting other types of conditional distributions might be utilized to describe the available data and we discuss some options in Appendix C.

Connection to XAIOne important class of applications is human-in-the-loop (HIL) settings, where the hope is to recover human interpretable concepts. Apriori, there is no reason why the high-dimensional underlying latent \(Z\)s should be interpretable, particularly for large \(d_{z}\). To make the situation worse, there may be other potentially composite concepts that humans can conjure up; indeed, there is no limit to the number of concepts that humans can conjure, and these almost certainly are not going to correspond to the true \(Z\)s. Because of this mismatch between the "true" \(Z\)s and interpretable concepts, even if we could recover the \(Z\)s, they may not be interpretable to humans and therefore less useful for downstream HIL purposes. The second caveat, and which follows from the first as a soft corollary, is that interventions on these will be much harder to obtain, particularly perfect interventions which require us to isolate the generative mechanisms of the latent factor with the other factors. Furthermore, some concepts are not manipulable, e.g. psychological traits, biological mechanisms, so obtaining interventional data is not possible in these applications. Accordingly, there has been considerable recent work, especially in the explainable AI (XAI) community on extracting human-interpretable concepts from latent generative factors, or more generally from complex foundation model representations [55; 103; 18; 41; 137]. To do so in an identifiable and automated way with as little supervision as possible is a significant open problem.

## 3 Related work

Causal representation learning and concept discoveryCausal representation learning (CRL) [102; 101] aims to learn generative factors of high-dimensional data. This exciting field has seen significant progress in the last few years [53; 11; 106; 60; 78; 57; 114; 14; 36; 1; 127; 63]. A fundamental perspective in this field is to ensure that the model parameters we attempt to recover are identifiable [53; 25; 129]. We will elaborate more on the connection of our framework to CRL in Appendix B. Concept discovery is an important sub-field of machine learning which extracts human-interpretable concepts from pre-trained models. We do not attempt to list the numerous works in this direction, see e.g., [103; 18; 135; 74; 82; 99; 58; 104; 89; 114]. However, theoretical progress in this direction is relatively limited. The work  studies when concepts can be identified provided the non-linear model is known in advance, whereas we show concept identifiability for unknown non-linearity, while simultaneously allowing entangled concepts. Prior works have also attempted to formalize the notion of concepts [130; 85; 103; 61], however their definitions seem specific to the model and domain under consideration, e.g., [85; 52] focus on binary concepts via large language model representations of counterfactual word pairs, whereas our general concept definitions are applicable to all domains.

Linearity of representationsSometimes referred to as the linear representation hypothesis, it is commonly believed that well-trained foundation models in multiple domains learn linear representations of human-interpretable concepts, with experimental evidence going back at least a decade [77; 113; 5]. This has been experimentally observed in computer vision models [90; 94; 8; 31; 55; 19; 130], language models [77; 87; 5; 22; 117; 30], large language models [17; 118; 81; 79; 66; 85; 38; 52], and other intelligent systems [75; 103]. Various works have also attempted to justify why this happens [64; 5; 35; 3; 32; 105]. We take a different angle: Given that this phenomenon has been observed for certain concepts of interest, how does this enable recovery of the concepts themselves? Consequently, our model assumptions are well-founded and our theory applies to multiple domains of wide interest.

## 4 Setup

In this section, we provide a formal definition of concepts, which are high-level abstractions present in data. This allows us to develop a theoretical framework for associated data distributions and identifiability theory. For the sake of intuition, we can think of the data as images of different objects and the color of the object as a concept.

### Generative model

We assume that the observed data \(X\) lies in a space \(^{d_{x}}\) of dimension \(d_{x}\) and has an underlying representation \(X=f(Z)\) for latent variables \(Z\) that lie in a latent concept space \(^{d_{z}}\) of dimension \(d_{z}\). In contrast to most prior works we do not necessarily assume that \(Z\) represents the true underlying mechanism that generated the data. Instead we simply assume that the latent representation has the geometric property that it maps certain regions of the observation space to linear subspaces of the latent space (motivated by previous work; see Section 3). Our first assumption is standard:

**Assumption 1** (Mixing function).: _The non-linear \(f\) is injective and differentiable._

We make no additional assumptions on \(f\): The map from \(Z X\) can be arbitrarily non-linear.

We now define concepts living in the latent space \(^{d_{z}}\). Before presenting the general definition of multidimensional concepts, we outline the basic ideas in the simplified setting of a one-dimensional concept. Consider the color "red" as a concept. Different images have different levels of "redness" in them, so this concept is measured on a continuous scale, represented by a valuation \(b\). An (atomic) concept is then represented by a vector \(a^{d_{z}}\) such that \( a,Z= a,f^{-1}(X)\) encodes the "value" of the concept in \(X\), as measured in the latent space. More precisely, for a given valuation \(b\), the set of all observations \(X\) that satisfy this concept is given by \(\{X=f(Z)| a,Z=b\}\). For instance, for an object in an image \(X\), if \(a^{d_{z}}\) is the concept of red color, \(b\) could indicate the intensity; then all datapoints \(X\) satisfying this concept, i.e., all images with an object that has color red with intensity \(b\), can be characterized as \(X=f(Z)\) where \(Z\) satisfies \( a,Z=b\). For a 3D visualization, see Fig. 1 We make this intuition formal below.

**Definition 1** (Concepts).: _A concept \(C\) is a linear transformation \(A:^{d_{z}}^{d_{C}}\). The dimension of the concept will be denoted by \((C)=d_{C}\). A valuation is a vector \(b^{d_{C}}\) and we say that a datapoint \(X\) satisfies the concept \(C\) with valuation \(b\) if \(AZ=b\) where \(Z=f^{-1}(X)\)._

In this work, we are interested in learning a collection of \(m\) concepts \(C^{1},,C^{m}\) from observed data. By left multiplying by the pseudo-inverse \(A^{+}\), we can equivalently assume \(A\) is a projector matrix. However, the current definition is more suitable for embeddings of real models.

When we talk of learning concepts \(C\), we are in particular interested in learning the evaluation map \(Af^{-1}(x)\). This is a more modest objective than learning the entire map \(f\) which is the usual goal in, say, CRL. While the latter typically requires stringent assumptions, in particular \((d_{z})\) environments are necessary, our weaker identifiability results only need \(O(d_{C}) O(d_{z})\) environments. To simplify our analysis, we make use of the following definition:

**Definition 2** (Atoms).: _An atom (short for atomic concept) is any concept \(C\) with \((C)=1\)._

The idea is that we can view each concept as being composed of atomic concepts in the following sense: Atomic concepts are fundamental concepts that live in a space of co-dimension \(1\) in latent space, and thus are equivalently defined by vectors \(a^{d_{z}}\). For example, concepts such red color, size of object, etc., may be atomic concepts. Any generic concept is then composed of a collection of atomic concepts, e.g., the concept \(C\) of all small dark red objects will correspond to \((C)=2\) with row \(1\) corresponding to the atomic concept of red color with large valuation (dark red objects) and row \(2\) corresponding to the atomic concept of object size with low valuation (small objects).

### Data distributions

We now define the distributions of datasets over concepts. We will predominantly work with distributions of \(Z\) over \(^{d_{z}}\), as the resulting distribution of \(X=f(Z)\) over \(^{d_{x}}\) can be obtained via a simple change of variables.

To build intuition, consider the case where we first collect a base dataset with some underlying distribution and then collect concept datasets via filtering. For instance, we could first collect a set of images of all objects and then, to collect a dataset of dark red colored objects, we filter them to only keep images of dark red colored objects. We call the former the _base distribution_ and the latter the _concept conditional distribution_ corresponding to our concept.

Fix a nonlinearity \(f\). We assume that the base data distribution is the distribution of \(X=f(Z)\) with \(Z p\), where \(p\) is the underlying distribution on \(^{d_{z}}\). In what follows, we will abuse notation and use \(p\) for both the distribution and the corresponding probability density which we assume exists. Incontrast to existing work that puts assumptions on \(p\) (such as linearity, Gaussianity, etc.), we make no assumptions on \(p\) since we wish to model real-life datasets which could be very arbitrary.

We now define the concept conditional distribution, which is a distribution over \(X\) that is induced by noisy observations of a particular concept at a particular valuation. Formally, assume we want to condition on some atomic concept \(a^{d_{z}}\) with valuation \(b\). It is reasonable to assume that this conditioning is a noisy operation. For instance, humans are great at distilling concepts from noisy images, e.g., they recognize cars in a misty environment. We formalize this by assuming that data collection is based on a noisy estimate \(= a,z+\) where \(\) is independent of \(z\) and its density is a symmetric distribution with density \(q()\). Then we consider the distribution

\[ p_{C}(z)=p(z|=b)& p (=b|z)p(z)\\ &=q(b- a,z)p(z)\] (1)

where we used Bayes theorem in the last step. This definition directly extends to higher dimensional concepts which are concisely defined as follows.

**Definition 3** (Concept conditional distribution).: _For a concept \(C\) with associated linear map \(A\) and an arbitrary valuation \(b^{dim(C)}\), we define the concept conditional distribution to be the set of observations \(X\) respecting this concept, which is defined as the distribution of \(X=f(Z)\) where \(Z p_{C}\) with_

\[p_{C}(Z) p(Z)_{k=1}^{(C)}q((AZ-b)_{k}).\] (2)

This is by no means the only possible definition, and we present feasible alternate definitions in Appendix C. We remark that our formulation is related to the iVAE setting  and the auxiliary variable setting for identifiable ICA in Hyvarinen et al.  and we discuss the relation later. The majority of recent identifiability results relied on interventional data while we only consider conditional information here.

### Concept learning and identifiability

We are ready to define our main problem of interest.

**Problem 1**.: _We are given an observational dataset \(X^{0}=f(Z^{0})\) corresponding to the latent base distribution \(p\) along with datasets \(X^{1},,X^{m}\) from \(m\) environments corresponding to concept conditional datasets for different concepts \(C^{1},,C^{m}\) and corresponding valuations \(b^{1},,b^{m}\) over the same latent space \(^{d_{z}}\) with the same mixing \(f\). Under what conditions (and up to which symmetries) can we learn the concepts \(C^{1},,C^{m}\), which includes the linear maps \(A^{1},,A^{m}\), and the concept valuations \(A^{1}f^{-1}(x),,A^{m}f^{-1}(x)\)?_

Toward this end, a fundamental question is whether this problem is even possible, i.e., whether it is well-defined. This is known as the question of identifiability [53; 25; 129; 57; 43]. Therefore, we make the following definition. Informally, for the setting above, we say that the concepts \((C^{1},A^{1}),,(C^{m},A^{m})\) with associated nonlinearity \(f\) are identifiable (and thus learnable) if for any other collection of different parameters that fit the data, they are linearly related to the true parameters.

**Definition 4** (Identifiability).: _Given datasets \(X^{0},\)\(X^{1},,X^{m}\) corresponding to the observational distribution and \(m\) concepts \(C^{1},,C^{m}\) with underlying latent base distribution \(p\) on \(^{d_{z}}\), nonlinearity \(f\), linear maps \(A^{1},,A^{m}\) and valuations \(b^{1},,b^{m}\), we say the concepts are identifiable if the following holds: Consider any different collection of parameters \(,},\), concepts \((},}),,(},})\) and valuations \(},,}\) that also generate the same observations \(X^{0},X^{1},,X^{m}\). Then there exists a shift \(w^{d_{z}}\), permutation matrices \(P^{e}\) and invertible diagonal matrices \(^{e}\) such that for all \(e\) and \(x\),_

\[^{e}^{-1}(x)=^{e}P^{e}A^{e}(f^{-1}(x)+w),\] (3)

_i.e., we can evaluate the concept evaluations on the data up to linear reparametrizations. Moreover, there exists a linear map \(T:^{}}^{d_{z}}\) such that the concepts and their evaluations satisfy_

\[^{e}=P^{e}A^{e}T^{-1},^{e}=^{e}P^{e}(b^{ e}-A^{e}w).\] (4)Identifiability implies we can identify the nonlinear map \(f^{-1}\) within the span of the subspace of the concepts of interest, and therefore we can recover the concepts of interest from our data. That is, if certain concepts are identifiable, then we will be able to learn these concept representations up to linearity, even if they can be highly nonlinear functions of our data. Such concept discovery is useful because they can then be used for further downstream tasks such as controllable generative modeling.

We emphasize that in contrast to previous work we are not aiming to identify \(f\) completely and indeed, no stronger identifiability results on \(f\) can be expected. First, we cannot hope to resolve the linear transformation ambiguity because the latent space is not directly observed. In other words, a concept evaluation can be defined either as \( a,Z\) or as \( Ta,T^{-}Z\) for an invertible linear map \(T\). For the purposes of downstream tasks, however, this is fine since the learned concepts will still be the same. Second, we cannot expect to recover \(f^{-1}\) outside the span of the concepts because we do not manipulate the linear spaces outside the span therefore we do not learn this information from our observed data so this is also tight. The permutation matrix captures the fact that the ordering of the concepts does not matter. Therefore, this definition captures the most general identifiability guarantee that we can hope for in our setting and furthermore, this suffices for downstream tasks such as controllable data generation.

Because we will only be interested in recovering the set of concepts up to linear transformations, without loss of generality, we will fix the base collection of atomic concepts. That is, we assume that each concept \(C^{e}\) (where \(1 e m\) indexes the environment) corresponds to a linear map \(A^{e}\) whose rows are a subset of \(\), where \(=\{a_{1},,a_{n}\}\) is a set of atomic concepts that we wish to learn. Moreover, we assume that they are linearly independent, since we want them to encode distinct concepts. This is formalized as follows.

**Assumption 2**.: _There exists a set of atomic concepts \(=\{a_{1},,a_{n}\}\) of linearly independent vectors such that for each concept \(C^{e}\) under consideration the rows of the concept matrix \(A^{e}\) are contained in \(\), i.e., \((A^{e})^{t}e_{i}\). We denote the indices of the subset of \(\) that appear as rows of \(A^{e}\) by \(S^{e}\) and we assume that all concepts in \(\) appear in some environment \(e\) (where an environment corresponds to a concept conditional distribution), i.e., \(_{e}S^{e}=[n]\)._

**Remark 1**.: _Definition 4 implies that the atoms can be identified in the sense that there is a permutation \(_{n}\) and \(_{i} 0\) such that for \(T\) as in Definition 4 and some \(_{i}\)_

\[_{(i)}^{} =a_{i}^{}T^{-1}\] (5) \[_{(i)},^{-1}(x) =_{i}( a_{i},f^{-1}(x)+ a_{i}, w),\] (6)

_i.e., we can evaluate the valuations of the atomic concepts up to linear reparametrization._

## 5 Main Result

In this section, we present our main result on identifying concepts from data. The punchline is that when we have rich datasets, i.e., sufficiently rich concept conditional datasets, then we can recover the concepts. Crucially, we only require a number of datasets that depends only on the number of atoms \(n\) we wish to learn (in fact, \(O(n)\) datasets), and not on the underlying latent dimension \(d_{z}\) of the true generative process. This is a significant departure from many existing works, since the true underlying generative process could have \(d_{z}=1000\), say, whereas we may be interested to learn only \(n=5\) concepts, say. In this case, approaches based on CRL necessitate at least \( 1000\)_interventional_ datasets, whereas we show that \( n+2=7\)_conditional_ datasets are enough if we only want to learn the \(n\) atomic concepts. We will explain the connection to CRL in Appendix B. Let us now discuss our main assumptions.

**Assumption 3**.: _The noise distribution \(q\) is Gaussian, i.e. \(q N(0,^{2})\) for some \(^{2}>0\)._

We choose Gaussian noise since it is a conventional modeling choice. However, it would be feasible to consider other noise families and we expect similar results to hold (albeit with modified proof techniques). We now relate the concepts \(C^{e}\) to the atoms. Recall that we defined the index sets \(S^{e}=\{i[n]:a_{i}A^{e}\}\) of atomic concepts in environment \(e\).

We define the environment-concept matrix \(M^{m n}\) indexed by environments and atoms by

\[M_{ei}=}&i S^{e}\\ 0&\] (7)Similarly, we consider the environment-valuation matrix \(B^{m n}\) given by

\[B_{ei}=^{e}}{^{2}}&$ and row $k$ of $A^{e}$ is $a_{i}$,}\\ 0&\] (8)

Our first assumption ensures that the concept conditional distributions are sufficiently diverse.

**Assumption 4** (Environment diversity I).: _The environment-concept matrix \(M^{m n}\) has rank \(n\) and there is a vector \(v^{m}\) such that \(v^{}M=0\) and all entries of \(v^{}B\) are non-zero where \(B\) denotes the environment-valuation matrix._

We remark that this assumption can only hold for \(m n+1\) and indeed is satisfied under mild assumptions on the environments if \(m=n+1\), as the following lemma shows. Note that the condition on \(B\) ensures that the concept valuations \(b_{k}^{e}\) are not equal for all environments \(e\) which would prevent identifiability.

**Lemma 1**.: _Assumption 4 is satisfied almost-surely if there are \(n+1\) concept conditional distributions such that every \(n\) rows of the environment-concept matrix are linearly independent and the \(b^{e}\) are drawn independently according to a continuous distribution._

We also assume one additional diversity condition. To motivate this, observe if two concepts always occur together, it's information-theoretically impossible to distinguish them, e.g., if an agent only sees red large objects (i.e. all red objects are large and all large objects are red), it will be unable to disambiguate the "red" concept from the "large" concept. Therefore, we make the following assumption.

**Assumption 5** (Environment diversity II).: _For every pair of atoms \(a_{i}\) and \(a_{j}\) with \(i j\) there is an environment \(e\) such that \(i S^{e}\) and \(j S^{e}\)._

We remark that these are the only assumptions about the sets \(S^{e}\). In particular, we do not need to know the sets \(S^{e}\). In the proof, we will extract these sets based on a the signatures they leave on the datasets. We can now state our main result.

**Theorem 1**.: _Suppose we are given \(m\) context conditional datasets \(X^{1},,X^{m}\) and the observational dataset \(X^{0}\) such that Assumptions 1-5 hold. Then the concepts are identifiable as in Definition 4._

**Remark 2**.: _Assumption 4 can only be satisfied for \(m n+1\), i.e., the result requires at least \(n+2\) environments. On the other hand, Lemma 1 assures that \(n+2\) environments are typically sufficient. We expect that the result could be slightly improved by showing identifiability for \(n+1\) environments under suitable assumptions. However, this would probably require more advanced techniques from algebraic statistics  compared to the techniques we employ here._

As mentioned before, our setting somewhat resembles the iVAE setting in Khemakhem et al.  and therefore, their proof techniques can also be applied, with several modifications, to derive identifiability results in our setting (however our formulation and application are very different). However, this approach will require more environments because their main assumption is that the matrix \(=(M,B)^{m 2n}\) has rank \(2n\) so that \(2n+1\) environments are necessary. Moreover, this rank condition is much stronger than Assumption 4. For completeness and as a warm-up we prove this result in Appendix A. The full proof of Theorem 1 is fairly involved and is deferred to Appendix A.

## 6 Experiments

In this section, we present experiments to validate and utilize our framework. We first verify our results on synthetic data, via a contrastive learning algorithm for concept learning. Then, we focus on experiments involving real-world settings, in particular on image data using multimodal CLIP models and text data using large language models (LLMs).

End-to-end Contrastive learning algorithm and Synthetic experimentsWe validate our framework on synthetic data as follows. We sample the base distribution from a Gaussian Mixture model and experiment with both linear and nonlinear mixing functions (details deferred to Appendix G). The number of concepts \(n\) is intentionally chosen to be less than the ground truth dimension \(d_{z}\) and the number of concept condtional distributions is \(m=n+1\) as per our theory. Inspired by , we use a contrastive learning algorithm to extract the concepts. Here we sketch the key ideas of the algorithm and defer details to Appendix F.

The core idea for the algorithm is as follows. For each concept conditional distribution \(X^{e}\), we train a neural network to distinguish concept samples \(x X^{e}\) from base samples \(x X^{0}\). Under our assumptions the log-odds of these two distributions have the following simple parametric form in terms of the environment-concept matrix and the environment-valuation matrix

\[(p^{}(Z))-(p(Z))=_{i=1}^{n}(-M_{ei} a _{i},Z^{e}^{2}+B_{ei} a_{i},Z^{e})+c_{e}\] (9)

for some constants \(c_{e}\) (see Lemma 3 for the precise statement).

Then, to learn the \(n\) atomic concepts up to linearity, we build a neural architecture for this classification problem with the final layer mimicking the log-odds expression above, which can then be trained end-to-end. Because of the careful parametrization of the last layer, this will encourage the model to learn the representations as guaranteed by our results. We can assume without loss of generality that the concept vectors we learn are the first coordinate vectors because concepts are only identifiable up to linear transformations (see Definition 4). In other words, we consider an encoder neural network \(h^{}\) with parameters \(\) and the valuation of atomic concept \(i\) is \(h^{}_{i}\). Therefore, for each environment \(e\), we train classifiers of the form

\[g_{e}(X,^{e},^{e}_{k},^{e}_{k},)=^{e}-_{k=1} ^{n}^{k}_{e}(h^{}_{k}(X))^{2}+_{k=1}^{n}^{k}_{e}h^{ }_{k}(X)\] (10)

using standard cross-entropy loss, where \(^{e},^{e}_{k},^{e}_{k}\) are the parameter of the last layer and \(\) parametrizes the decoder.

In Table 1, we report the \(R^{2}\) and Mean Correlation Coefficient (MCC) metrics [53; 54] with respect to the ground truth concept valuations. In addition we provide results for larger values of \(d_{x}\) and \(d_{z}\) in Table 7 in Appendix G. There are no baselines since we are in a novel setting, but our metrics are comparable to and often surpass what's usually reported in such highly nonlinear settings [132; 14]. We remark that variations of the contrastive method can be designed for harder synthetic settings and different problems related to concept discovery. However, we will move onto real-life data experiments next.

Probing the theory on multimodal CLIP modelsA real world example that approximately matches the setting considered in this paper is the training of the multimodal CLIP models . They are trained by aligning the embeddings of images and their captions. We can view the caption as an indicator of the concepts present in the image. Thus the data provides access to several concept conditional distributions such as the collection of all images having the label 'A dog', but also to more complex distributions consisting of more than one atomic concept such as images labeled 'A red flower'. We embed images from the 3d-Shapes Dataset  with known factors of variation into the latent space of two different pretrained CLIP models. Using logistic regression we learn atomic concepts for each of the factors of variations (see Appendix D.1 for details) and then evaluate the concept valuations of the learned atomic concept on held out images. We show the results for the shape attribute in Figure 2 (further results are in Appendix D.2). The results show that there are indeed linear subspaces of the embeddings space that represent certain concepts. Moreover, the learned valuations for different models are approximately linearly related as predicted by Theorem 1. We emphasize that, while these observations highlight the relevance of the theory developed in this paper, they do not explain the behavior of CLIP models, as their training objective does not directly align with our theoretical framework.

Alignment of LLMsFinally, we show an application of our framework to interpret representations of LLMs and improve alignment techniques. In particular, we exploit our ideas to improve the

   Mixing (\(f\)) & \((n,d_{z},d_{x})\) & \(R^{2}\)\(\) & MCC\(\) \\  Linear & (2, 3, 4) & 0.98 \(\) 0.01 & 0.98 \(\) 0.03 \\ Nonlinear & (2, 3, 4) & 0.94 \(\) 0.06 & 0.96 \(\) 0.04 \\  Linear & (3, 4, 6) & 0.99 \(\) 0.01 & 0.86 \(\) 0.08 \\ Nonlinear & (3, 4, 6) & 0.97 \(\) 0.03 & 0.92 \(\) 0.07 \\  Linear & (4, 8, 10) & 0.97 \(\) 0.01 & 0.87 \(\) 0.06 \\ Nonlinear & (4, 8, 10) & 0.94 \(\) 0.03 & 0.87 \(\) 0.06 \\   

Table 1: Linear identifiability when number of concepts \(n\) is less than latent dimension \(d_{z}\) with observed dimension \(d_{x}\), averaged over \(5\) seeds.

Inference-Time Intervention technique  to promote LLMs to be more truthful, i.e. the downstream task is to take pre-trained LLMs and during inference, change the valuation of the truthfulness concept from _false_ to _true_, without affecting any other orthogonal concepts. Motivated by our framework, we propose to replace steering vectors by steering matrices for better alignment. Experiments on LLaMA  show an improvement of the TruthfulQA dataset  accuracy. Additional details, including a self-contained introduction to large language models (LLMs) and the Inference-Time Intervention (ITI) technique are deferred to Appendix E.

## 7 Conclusion

In this work, we study the problem of extracting concepts from data, inspired by techniques from causal representation learning. This is an approach to bridge identifiability theory in (causal) representation learning and the field of concept-based learning. For this, we geometrically define concepts as linear subspaces, well-supported via extensive empirical literature. With this formal definition of concepts, we study under what conditions they can be provably recovered from data. Our rigorous results show that this is possible under the presence of only conditional data,requiring far fewer distributions than the underlying latent dimension. Finally, synthetic experiments, multimodal CLIP experiments and LLM alignment experiments verify and showcase the utility of our ideas.

AcknowledgmentsWe acknowledge the support of AFRL and DARPA via FA8750-23-2-1015, ONR via N00014-23-1-2368, NSF via IIS-1909816, IIS-1955532, IIS-1956330, and NIH R01GM140467. We also acknowledge the support of the Tubingen AI Center, the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645, and the Robert H. Topel Faculty Research Fund at the University of Chicago Booth School of Business.