# Task-recency bias strikes back: Adapting covariances

in Exemplar-Free Class Incremental Learning

Grzegorz Rypesc

IDEAS NCBR

Warsaw University of Technology

grzegorz.rypesc@ideas-ncbr.pl &Sebastian Cygert

IDEAS NCBR

Gdansk University of Technology

sebastian.cygert@ideas-ncbr.pl &Tomasz Trzcinski

IDEAS NCBR

Warsaw University of Technology

Tooplocax &Bartlomiej Twardowski

IDEAS NCBR

Autonomous University of Barcelona

Computer Vision Center

Code: https://github.com/grypesc/AdaGauss

###### Abstract

Exemplar-Free Class Incremental Learning (EFCIL) tackles the problem of training a model on a sequence of tasks without access to past data. Existing state-of-the-art methods represent classes as Gaussian distributions in the feature extractor's latent space, enabling Bayes classification or training the classifier by replaying pseudo features. However, we identify two critical issues that compromise their efficacy when the feature extractor is updated on incremental tasks. First, they do not consider that classes' covariance matrices change and must be adapted after each task. Second, they are susceptible to a task-recency bias caused by dimensionality collapse occurring during training. In this work, we propose _AdaGauss_ - a novel method that adapts covariance matrices from task to task and mitigates the task-recency bias owing to the additional anti-collapse loss function. _AdaGauss_ yields state-of-the-art results on popular EFCIL benchmarks and datasets when training from scratch or starting from a pre-trained backbone.

## 1 Introduction

Continual learning (CL), an essential area of machine learning, focuses on developing algorithms that can learn progressively from a continuous stream of data and adapt to new tasks while retaining previously acquired knowledge. This paradigm is paramount for creating systems capable of lifelong learning, much like humans, and robust in dynamic environments where data distribution evolves over time. A significant challenge within CL is exemplar-free class incremental learning (EFCIL) , which requires the model to incorporate new classes without storing previous data samples (exemplars). This approach is especially relevant in scenarios with privacy constraints or limited storage capacity, as it compels the model to retain knowledge and prevent catastrophic forgetting  solely through internal mechanisms, such as knowledge distillation , parameter regularization , expanding neural architecture  or generative replay .

Recent state-of-the-art methods designed for EFCIL often represent classes as Gaussian distributions in the latent space of the feature extractor. That enables an inference using Bayes classifier  or training a linear classifier using pseudo-prototypes sampled from these distributions . However, we present in this work that these methods have multiple shortcomings and can be improved. First, they assume that covariance matrices of past classes are constant across incremental training.

However, as presented in Fig. 1, when the feature extractor is updated on incremental tasks (it is unfrozen), distributions of previous classes change and no longer match the memorized ones. Suitable methods must adapt both means and covariances. EFC  predicts drift (change) only of the distribution mean and points out that adapting covariances is an open question. Second, the methods suffer from a dimensionality collapse [30; 16], which is more significant in early tasks. That makes old classes' covariances to be of lower rank than those from recent tasks, which introduces errors while inverting the matrices for the classification, leading to increased task-recency bias. We explain this in detail in Sec. 3.2.

This work focuses on the challenging problems of adapting classes' covariances and overcoming dimensional collapse in EFCIL. We are the first to introduce a method that adapts the mean and covariance of memorized distributions, significantly reducing the error between memorized and ground truth distributions. We also overcome the dimensionality collapse of feature representations by introducing a novel anti-collapse loss, which alleviates the problem of task-recency bias. We dub the resulting method _AdaGauss_ - Adapting Gaussians. Our contributions are as follows:

* We analyze dimensionality collapse in EFCIL settings and explain that it leads to task-recency bias. We introduce a novel anti-collapse loss to prevent it.
* We show that knowledge distillation techniques in EFCIL provide different representation strengths of the feature extractor. We are the first to utilize knowledge distillation through a learnable projector network in EFCIL.
* Based on these findings, we propose _AdaGauss_, a novel method to adapt both means and covariances of memorized class distributions, which results in state-of-the-art results when the model is trained from scratch or starting from a pre-trained weights.

## 2 Related works

**Semantic drift.** We investigate offline, EFCIL setting  focusing on keeping the network size constant, where no task information is available at test time. Regularization-based approaches penalize changes to important neural network parameters [17; 6; 47; 22] or use distillation techniques to regularize neuron activations [21; 45; 51; 24]. However, even with knowledge distillation, the features from old classes will change, causing catastrophic forgetting [9; 27]. Therefore, few works tried to predict these changes by approximating their semantic drift [45; 24; 15; 36]. However, those strategies' limitations are that they adapt only prototypes, ignoring changes in covariance matrices, which we experimentally show is suboptimal. As predicting the drift is challenging, many methods focus on scenarios where the backbone is frozen after the first task [4; 31; 23; 29; 10]. However, this prevents the feature extractor to adapt to new tasks . We show that it is possible to change the feature extractor and adapt the covariance matrices of classes.

Figure 1: Latent space visualization, average accuracy after the last task, and symmetrical KL divergence between memorized and ground truth distributions for ResNet18 trained sequentially on ImagenetSubset dataset split into ten tasks. Freezing the feature extractor prevents changes in data distribution but results in inseparable classes. When the network is trained on incremental tasks (unfrozen), the ground truth distributions change and do not match the memorized ones. A suitable CL method should adapt the mean and covariance of distributions to retain valid decision boundaries.

**Task-recency bias.** Another challenge in CL is a task-recency bias, where the model is biased towards classifying classes from new tasks [13; 26; 50]. While some works approached this problem using exemplars [1; 43; 13; 48] the problem is amplified in an exemplar-free setting. Some works considered prototype replay, which maintains the decision boundary between classes [31; 51; 37; 39; 38; 53]. To improve this strategy, PASS  included prototype augmentation, and EFC  updates their prototypes after each task. In this work, we point out that the cause for task-recency bias in the EFCIL scenario is the dimensionality collapse of the feature extractor, leading to numerical instabilities when inverting covariance matrices.

**Dimensionality collapse.** Recent works revealed that supervised learning exhibits signs of neural collapse [30; 16], where a large fraction of features' variance is described only by a small fraction of their dimensions. Since then, several studies [5; 7; 46; 16] showed that utilizing additional MLP projector is a crucial component to alleviate the collapse of the representations and improve their transferability. Another implication of the neural collapse in CL is that it becomes challenging to invert covariance matrices. Existing methods add a constant value to the diagonal [35; 24; 51] of the covariance matrices or utilize shrinking  to prevent that. On the contrary, we propose an anti-collapse loss, which is more elegant and does not artificially alter covariance matrices.

## 3 Method

### Exemplar-Free Class-Incremental Learning (EFCIL)

Class-Incremental Learning (CIL) scenario considers a dataset split into \(T\) tasks, each corresponding to the non-overlapping set of classes \(C_{1} C_{2} C_{T}=C\) such that \(C_{t} C_{s}=\) for \(t s\). In Exemplar-Free CIL (EFCIL), during a training step \(t\), we have only access to current task data \(D_{t}=\{(x,y)|y C_{t}\}\) and we cannot store any exemplars from the previous steps. The objective is to train a model that discriminates between old (\(<t\)) and new classes combined. We assume a task-agnostic evaluation [41; 26], where the method does not know the task id during the inference.

### The three observations that motivate towards _AdaGauss_

In this section, we provide an insight into problems with current EFCIL methods. We train the standard ResNet18  network on the ImagenetSubset dataset divided into ten equal tasks. We point out that: **1.** covariance of class distributions during CL sessions change and must be adapted; **2.** the task recency bias comes from the differences in representational strength of the model; **3.** when training from scratch in EFCIL, the models are susceptible to dimensionality collapse.

**Observation 1**. As illustrated in Fig. 1, training the feature extractor on incremental tasks makes memorized distribution not match the ground truth (GT) ones. More specifically, the mean and covariance of GT change, and to keep valid decision boundaries, both memorized means and covariances must be adapted. That decreases symmetrical KL divergence between memorized and GT distributions, thus increasing average accuracy after the last task. However, existing state-of-the-art methods [36; 24; 51; 52] do not adapt covariance matrices, while others [31; 10; 55; 54] freeze the feature extractor after the initial task, which does not guarantee separability of classes from new tasks (first image in Fig. 1).

**Observation 2**. When training the feature extractor with different knowledge distillation methods (feature [53; 52; 45], logit [21; 33], projected ), representational strength of the feature extractor increases with each task, as presented in Fig. 2. That makes memorized covariance matrices of late tasks have a higher rank than those from early tasks, as presented in Fig. 3. When these matrices are inverted, the opposite happens - due to numerical instabilities, norms of inverted covariance matrices of early tasks will be greater. That causes task-recency bias as presented in Fig. 4. In the case of Bayes classification [35; 10], the Mahalanobis distance is much higher for early tasks, whereas in the case of sampling pseudo-prototypes [24; 51] the logits for recent tasks are higher, what skews classification towards recent tasks. This bias differs from already well-studied linear head bias [13; 43; 48], as it occurs at the level of the representations, where no linear head and no exemplars are utilized.

**Observation 3**. Fig. 3 also presents that feature extractor suffers from dimensionality collapse [30; 16] as ranks of covariance matrices are much lower than the latent space size (512 for ResNet18). That makes classes covariance matrices non-invertible. That, in turn, disallows the calculation of Mahalanobis distance, likelihood, and sampling from such collapsed distribution. In orderto overcome this issue, the existing methods utilize shrinking  or add a constant value to the diagonal [35; 24; 51] of the covariance matrices to prevent that. However, these techniques artificially alter classes' distributions, introducing additional hyperparameters and a new source of errors accumulating during long CIL sessions. A more elegant solution would directly prevent the dimensionality collapse of the feature extractor during training while preserving the class separability provided by cross-entropy.

### _AdaGauss_ method

Motivated by these three observations, we made the following decisions about _AdaGauss_. Based on the first observation, after training the feature extractor \(F\) on an incremental task, we train an auxiliary network (adapter), which we utilize to adapt the means and covariances of old classes to the latent space of the new feature extractor. To perform knowledge distillation and improve the representation strength of the feature extractor (second observation), we utilize feature distillation through a learnable projector. In order to overcome the dimensionality collapse and task-recency bias, showcased by the second and third observations, we utilize a novel anti-collapse loss that regularizes the features' covariance matrix and prevents dimensional collapse. _AdaGauss_ memorizes each class as a mean and covariance and performs Bayes classification as in [10; 35]. We provide a pseudo-code of our method in Alg. 1. Below, we explain the motivation and details of the method.

#### 3.3.1 Feature distillation through a learnable projector

Inspired by representational-learning , we utilize a feature distillation through a learnable projector to mitigate forgetting, which we refer to as projected distillation. As presented in Fig. 2, this distillation technique provides representations with a better eigenvalues distribution, thus decreasing the problem of task-recency bias compared to standard logit [21; 33] and feature [45; 53; 52] distillation techniques. As the projector, we utilize a 2-layer MLP network \(^{t t-1}:^{S}^{S}\) with hidden size \(d\) times bigger than the latent space \(S\). Following existing continual learning works [21; 45; 33], when training \(F_{t}\) on minibatch \(B\), we freeze \(F_{t-1}\) trained on the previous task. Finally, we calculate our knowledge distillation loss as follows:

\[L_{PKD}=_{i B}||^{t t-1}(F_{t}(x_{i}) )-F_{t-1}(x_{i})||^{2}.\] (1)

#### 3.3.2 Overcoming dimensionality collapse

As described in Sec. 3.2, existing methods for EFCIL that represent classes as Gaussian distributions suffer from dimensionality collapse, which leads to task-recency bias caused by the fact that ranks of covariance matrices are different amongst the tasks. To overcome the collapse, we encourage the feature extractor to produce features whose dimensions are linearly independent. Therefore, in each task, we directly optimize the covariance matrices of features produced by \(F_{t}\) to be positive-definitive by the diagonal of the Cholesky decomposition of covariance of each training minibatch to be positive. More precisely, let \(S\) be the size of the feature vectors and \(a_{i}\) be \(i\)-th element of the diagonal of a Cholesky decomposition of minibatch's covariance matrix. We formulate the anti-collapse loss \(L_{AC}\) in the form:

\[L_{AC}=-_{i=1}^{S}(a_{i},1)\] (2)

This loss forces Cholesky's decomposition of covariance of each minibatch to have diagonal entries greater than 1. Therefore, they are positive, and the covariance matrix is positive-definite due to the property of Cholesky decomposition. More on the definition of \(L_{AC}\) in Appendix, Sec. A.2).

#### 3.3.3 Training the feature extractor

In each task \(t\), we train all parameters of the feature extractor \(F_{t}\) together with additional projector \(\) used for knowledge distillation. Following most works [10; 35; 51; 31; 21], we utilize popular cross-entropy loss \(L_{CE}\) to discriminate between classes. The final loss function is:

\[L=L_{CE}+L_{AC}+ L_{PKD},\] (3)

where \(\) is a plasticity-stability trade-off hyperparameter, similar to .

After training the feature extractor, we represent classes \(C_{t}\) as multivariate Gaussian distributions in the latent space. More precisely, we represent any class \(c C_{t}\) as \((_{c},_{c})\).

#### 3.3.4 Adapting Gaussian distributions

After training of \(F_{t}\) is completed, representations of old classes drifted  (changed) and no longer match memorized Gaussians. Therefore, we update memorized Gaussians representing past classes to recover ground truth representations. To do that, we train an auxiliary adaptation network \(^{t-1 t}:^{S}^{S}\) (called adapter), which maps features from the old latent space to the new one. We use only the current data from task \(t\) for that. Training loss is:

\[L_{}=_{i B}||^{t-1 t}(F_{t-1}(x_{i}))-F_{t}(x_{i})|| ^{2}+L_{AC}.\] (4)

\(L_{AC}\) is the same anti-collapse loss as used during the training of the feature extractor. After training the adapter, for each old class \(c\), we sample from \((_{c},_{c})\) a set of \(N\) points: \(n_{1},n_{2},,n_{N}\), where \(N|S|\) and transform them through \(\) obtaining new set: \(\{(n_{1}),(n_{2}),,(n_{N})\}\). We calculate adopted mean \(_{c}^{new}\) and covariance \(_{c}^{new}\) using new sets of data and update the old distribution as follows: \((_{c},_{c})=(_{c}^{new},_{c}^{new})\) A pseudocode of the full _AdaGauss_ method is presented in Alg. 1.

## 4 Experiments

**Datasets and metrics.** We evaluate our method on several well-established benchmark datasets. CIFAR100  consists of 50k training and 10k testing images in resolution 32x32. TinyImageNet , a subset of ImageNet , has 100k training and 10k testing images in 64x64 resolution. ImagenetSubset contains 100 classes from ImageNet (ILSVRC 2012) . We split these datasets into 10 and 20 equal tasks. Thus, each task contains the same number of classes, a standard practice in EFCIL [21; 45; 35; 24]. We also evaluate our method on fine-grained datasets: CUB200  represents \(11,788\) images of bird species, and FGVCircraft  dataset consists of \(10,200\) images of planes. We split fine-grained datasets into 5, 10, and 20 tasks. As the evaluation metric, we utilize commonly used average accuracy \(A_{last}\), which is the accuracy after the last task, and average incremental accuracy \(A_{inc}\), which is the average of accuracies after each task [26; 24; 10].

**Baselines and hyperparameters.** We compare our method to multiple EFCIL baselines. Well-established ones, like EWC , LwF , PASS , IL2A , SSRE , and the most recent and strong EFCIL baselines: FeTrIL , FeCAM , DS-AL  and EFC . For the baselineresults on CIFAR100, TinyImageNet, and ImagenetSubset, we take the results reported in , while for FeCAM, we run its original implementation. For fine-grained datasets (CUB200, FGVCAircrafts), we run implementations provided in FACIL  and PyCIL  frameworks (if provided) or from the authors' repositories. We set default hyperparameters proposed in the original works. We utilize random crops and horizontal flips as data augmentation.

**Implementation details and reproducibility.** We utilize standard ResNet18  as a feature extractor \(F\) for all methods. We train it from scratch on CIFAR100, TinyImagenetSubset, and ImagenetSubset, while for experiments on fine-grained datasets, we utilize weights pre-trained on ImageNet. We implement our method in FACIL benchmark2. We set \(=10,N=10000,d=32\) and add a single linear bottleneck layer at the end of the \(F\) with \(S\) output dimensions, which define the latent space. When training from scratch, we set \(S=64\), while for fine-grained datasets, we decrease it to 32, as there are fewer examples per class. We use an SGD optimizer running for 200 epochs with a weight decay equal to \(0.0005\). When training from scratch, we utilize a starting learning rate (lr) of \(0.1\), decreased by ten times after 60, 120, and 180 epochs. We train the adapter using an SGD optimizer with weight decay of 0.0005, running for 100 epochs with a starting lr of \(0.01\); we decrease it ten times after 45 and 90 epochs.

We utilize a single machine with an NVIDIA RTX4080 graphics card to run experiments. The time for execution of a single experiment varied depending on the dataset type, but it was at most ten hours. We attach details of utilized hyperparameters in scripts in the code repository. We report all results as the mean and variance of five runs.

### Results

**Training from scratch.** We present the baseline results and _AdaGauss_ method when training from scratch in Tab. 1. We consider \(T=10\) and \(T=20\) equal tasks. We can see an improvement over the most recent state-of-the-art method - EFC . We improve its results by 3.7% and 6.8% points in terms of average accuracy on ImagenetSubset split into 10 and 20 tasks, respectively. This improvement is also consistent in terms of average incremental accuracy - 5.1% and 7.5% points and on the other datasets. This increase can be attributed to the fact that EFC does not adapt covariance matrices from task to task (just means), which, as we showed in Sec. 3.2, is required to improve the results. Older method - IL2A , which does not adapt their classes representations (means and covariance matrices) method at all, achieves much lower results than our approach - 23.4% and 25.1% points lower average accuracy on ImagenetSubset.

Methods such as FeTrIL , FeCAM  and DS-AL  overcome the problem of distribution drift by freezing the feature extractor on the first task. However, it cannot adapt well to the new incremental tasks, resulting in poor plasticity and worse results than _AdaGauss_ and EFC . FeTrIL achieves 14.9% and 16.0% points lower average accuracy on ImagenetSubset, while FeCAM - 12.4% and 13.6%.

**Training from pre-trained model.** We provide the baseline results and our method when training from a ImageNet pre-trained model in Tab. 2. Despite having a strong feature extractor from the very beginning, it still needs to be adapted to discriminate better between fine-grained classes. We report results for 5, 10, and 20 equal tasks. _AdaGauss_ achieves state-of-the-art results. It improves the average accuracy of the second-best method EFC  by 4.8% and 4.4% points on CUB200 and StanfordCars for \(T=10\), respectively. The results are consistent for other number of tasks.

**Ablation study.** We perform ablation of our method on CIFAR100 and ImagenetSubset datasets split into ten equal tasks in Table 3. First, we test our method with the nearest mean classifier (NMC) instead of the Bayes classifier to verify whether considering covariance improves the results. Without covariance matrices and with NMC  (1st row), we get worse results: 9.7% and 9.6% points lower average accuracy on CIFAR100 and ImagenetSubset, respectively. Memorizing covariances and sampling pseudo-prototypes to adapt means (2nd row) improves NMC results only slightly. Next, we utilize the Bayes classifier instead of NMC but assume that class distributions have diagonal covariance matrices (3rd row). That decreases the average accuracy of our method by 5.0% and 3.9%, respectively, proving that ground truth test distributions have non-zero off-diagonal. Then, we test our method without adapting means (5th row) like in IL2A  method. That severely hurts the performance - average accuracy decreases by 21.5% and 27.2 %. On the contrary, if we adapt means but not covariances like in EFC , we lose far less, 3.2% and 3.1%, respectively. Lastly, we check the performance of our method without the \(L_{AC}\) component. To allow covariance matrices to be invertible, we add a shrink value of 0.5, similarly to . This results in an average accuracy drop of 5.9% and 4.0%. The results are also consistent with the average incremental accuracies. This ablation proves our design choices and that all components are necessary to get the best results.

    &  &  \\   &  &  &  &  &  &  \\  & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) \\  EWC  & 21.6 & 38.2 & 15.8 & 32.6 & 12.3 & 27.2 & 24.3 & 44.0 & 14.3 & 34.5 & 10.9 & 27.9 \\ LwF  & 44.3 & 57.7 & 30.4 & 46.1 & 19.4 & 34.7 & 39.0 & 55.2 & 28.0 & 46.5 & 14.7 & 30.5 \\ PASS  & 34.5 & 48.6 & 27.0 & 42.3 & 18.1 & 36.9 & 33.3 & 48.9 & 26.4 & 41.0 & 13.9 & 28.1 \\ IL2A  & 36.9 & 51.3 & 29.4 & 45.5 & 20.8 & 35.1 & 39.4 & 49.1 & 27.3 & 45.1 & 14.2 & 28.7 \\ FeTrIL  & 41.9 & 53.2 & 36.9 & 48.2 & 34.6 & 45.3 & 46.0 & 58.5 & 40.5 & 53.4 & 32.5 & 43.3 \\ FeCAM  & 43.5 & 56.0 & 40.2 & 54.9 & 36.2 & 48.9 & 45.3 & 58.0 & 41.4 & 55.2 & 34.0 & 46.0 \\ DS-AL  & 49.4 & 61.9 & 45.8 & 59.1 & 41.4 & 53.8 & 50.6 & 62.7 & 42.6 & 56.4 & 34.2 & 46.7 \\ EFC  & 58.3 & 68.9 & 51.0 & 63.3 & 46.1 & 59.3 & 50.1 & 63.2 & 43.1 & 57.6 & 28.1 & 48.2 \\  AdaGauss & **60.4** & **69.2** & **55.8** & **66.2** & **47.4** & **60.6** & **53.3** & **64.0** & **47.5** & **58.5** & **34.8** & **48.6** \\   

Table 2: Average incremental and last accuracy in EFCIL fine-grained scenarios when utilizing a pre-trained feature extractor. We report the mean of 5 runs, while variances are reported in Tab. 6.

    &  &  &  \\   &  &  &  &  &  &  \\  & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) \\  EWC  & 31.2 & 49.1 & 17.4 & 31.0 & 17.6 & 32.6 & 11.3 & 26.8 & 24.6 & 39.4 & 12.8 & 27.0 \\ LwF  & 32.8 & 53.9 & 17.4 & 38.4 & 26.1 & 45.1 & 15.0 & 32.9 & 37.7 & 56.4 & 18.6 & 40.2 \\ PASS  & 30.5 & 47.9 & 17.4 & 32.9 & 24.1 & 39.3 & 18.7 & 32.0 & 26.4 & 45.7 & 14.4 & 31.7 \\ IL2A  & 31.7 & 48.4 & 23.0 & 40.2 & 25.3 & 42.0 & 19.8 & 35.5 & 27.7 & 48.4 & 17.5 & 34.9 \\ SSFE  & 30.4 & 47.3 & 17.5 & 32.5 & 22.9 & 38.8 & 17.3 & 30.6 & 25.4 & 43.8 & 16.3 & 31.2 \\ FeTrIL  & 34.9 & 51.2 & 23.3 & 38.5 & 31.0 & 45.6 & 25.7 & 39.5 & 36.2 & 52.6 & 26.6 & 42.4 \\ FeCAM  & 32.4 & 48.3 & 20.6 & 34.1 & 30.8 & 44.5 & 25.2 & 38.3 & 38.7 & 54.8 & 29.0 & 44.6 \\ DS-AL  & 40.8 & 54.9 & 31.7 & 43.2 & 33.6 & 47.2 & 26.5 & 41.6 & 46.8 & 58.6 & 36.7 & 48.5 \\ EFC  & 43.6 & 58.6 & 32.2 & 47.3 & 34.1 & 48.0 & 28.7 & 42.1 & 47.4 & 59.9 & 35.8 & 49.9 \\  AdaGauss & **46.1** & **60.2** & **37.8** & **52.4** & **36.5** & **50.6** & **31.3** & **45.1** & **51.1** & **65.0** & **42.6** & **57.4** \\   

Table 1: Average incremental and last accuracy in EFCIL when training the feature extractor from scratch. The mean of 5 runs is reported. Full results are in Tab. 5. We denote the best results **in bold**.

### Adaptation results

We verify how our adaptation method improves the quality of memorized class distributions on ImagenetSubset split into ten equal tasks. For this purpose, we measure the average distances between memorized and real classes after each task. More precisely, we measure the L2 distance between means and covariances as well as symmetrical Kulbach-Leibler divergence (\(D_{KL}\)) between memorized and real distributions. We utilize projected distillation (\(=10\)) and compare our method to a baseline that does not adapt distributions like in [51; 52] (No adapt) and to the prototype drift compensation introduced in EFC  that adapts only means. We provide results in Fig. 5. We can see that our approach allows us to better approximate ground truth distributions. More precisely, compared to EFC, it decreases the distance to real-mean by \(\)29%, to real-covariance by \(\)39% and \(D_{KL}\) distance by \(\)72%. We can also see that the EFC approach does not improve distance to real-covariance compared to no adaptation, which is a drawback of this method.

### Analysis of anti-collapse loss

We analyze the impact of anti-collapse \(L_{AC}\) regularization term on ImagenetSubset split to 10 equal tasks. After the last task, we verify how much \(L_{AC}\) improves the distribution of classes' covariance eigenvalues. We report results in Fig. 6. Without utilizing \(L_{AC}\), the largest eigenvalue is \( 1.2*10^{5}\) times greater than the lowest, showcasing the dimensionality collapse. However, with \(L_{AC}\), this difference equals \(\)84, proving that more eigenvectors contribute towards representations, and the collapse is greatly diminished.

Next, we measure the average rank of covariance matrices memorized in each task for different knowledge distillation methods and projected distillation with \(L_{AC}\). Here, we set \(S=64\). In Fig. 7 can see that without \(L_{AC}\), all distillation methods present in existing methods struggle to achieve class covariance equal to latent size \(S\), which according to Sec. 3.2 results in task-recency bias. Interestingly, when combining projected distillation with \(L_{AC}\), the rank of covariance matrices equals 64 for each task, proving that \(L_{AC}\) is a promising approach for combating dimensionality collapse when training from scratch.

    &  &  &  & \)} &  &  \\   & & & & & \(A_{last}\) & \(A_{inc}\) & \(A_{last}\) & \(A_{inc}\) \\  NMC & None & ✓ & ✓ & ✓ & 36.4 & 54.0 & 41.5 & 57.9 \\ NMC & Full & ✓ & ✓ & ✓ & 37.6 & 54.8 & 42.3 & 58.7 \\ Bayes & Diagonal & ✓ & ✓ & ✓ & ✓ & 41.1 & 56.3 & 45.5 & 61.3 \\  Bayes & Full & ✗ & ✗ & ✗ & 22.9 & 42.8 & 22.5 & 43.4 \\ Bayes & Full & ✗ & ✓ & ✓ & 24.6 & 44.7 & 23.9 & 44.9 \\ Bayes & Full & ✓ & ✗ & ✓ & 42.9 & 57.7 & 48.0 & 62.7 \\ Bayes & Full & ✓ & ✓ & ✗* & 40.2 & 56.2 & 46.7 & 57.1 \\  Bayes & Full & ✓ & ✓ & ✓ & **46.1** & **60.2** & **51.1** & **65.0** \\   

Table 3: Ablation of _AdaGauss_ indicating the contribution from the different components. * signifies that we utilized covariance matrix shrinking with the value of 0.5 (chosen on the validation set) instead of anti-collapse loss to overcome the covariance matrix singularity problem.

Figure 5: Distances from memorized distributions to the real ones in terms of distributions’ mean, covariance and KL divergence across 10 tasks on ImagenetSubset dataset. AdaGauss greatly reduces errors and allows for better adaptation than prototype drift compensation (EFC).

An alternative method for overcoming singularity in covariance matrices is shrinking . In Fig. 8, we present results for our method with different values of shrink performed when calculating covariance matrices on CIFAR100. Intuitively, increasing the shrink value decreases the method's efficacy, as it artificially alters the covariance to be different from the ground truth representation. Without using \(L_{AC}\) and without shrink, it is impossible to invert the matrices, resulting in the crash of the method. Nevertheless, the results are the highest when utilizing \(L_{AC}\) without shrink (60.2%).

### Different distillation techniques

We test the performance of the projected distillation against other distillation techniques in _AdaGauss_. We train from scratch on CIFAR100, ImagenetSubset and utilize the pre-trained model on CUB200. We split datasets into ten equal tasks and use hyperparameters from experiments in Tab. 1 and Tab. 2. We present the results in Fig. 9. Projected distillation achieves better average accuracy than logit distillation by 1.4%, 0.9%, and 4.0% points on CIFAR100, ImagenetSubset, and CUB200, respectively. Interestingly, the gap between projected distillation and not using knowledge distillation is much lower on CUB200, which we contribute to using a strong pre-trained model.

### Memory requirements

Our method does not increase the number of feature extractor's parameters. In addition, the adapter and distiller are discarded after the training, thus not increasing memory during the long CIL sessions and evaluations. \(AdaGauss\) requires \(S+\) parameters to memorize the mean and covariance of a class, where \(S\) is the latent space size. Therefore, the method requires the same number of parameters as FeCAM  and fewer weights than EFC  as we do not expand the linear classifier. Additionally, \(S\) can be decreased using linear bottleneck layer before the latent space.

### Time complexity of AdaGauss

We measure the training and inference time of popular EFCIL methods using their original implementations on a single machine with NVIDIA GeForce RTX 4060 and AMD Ryzen 5 5600X CPU. We repeat each experiment 5 times, train all methods for 200 epochs, use four workers, and have a batch size equal to 128. We test vanilla AdaGauss and AdaGauss, where the Bayes classifier is replaced with a trained linear head, where the classifier is trained on samples from class distributions (mean and cov. matrix). We utilize the FeTrIL version with a linear classification head.

We present results in Tab. 4. The inference of our method takes a similar amount of time as in FeCAM, as the feature extraction step is followed by performing Bayes classification. The inference

Figure 6: Distribution of eigen- Figure 7: Ranks of classes’ values of class representations covariance matrices with different values of class representations covariance matrices with different values of covariance shrinking, with and without anti-collapse regularization. Results without \(L_{AC}\) greatly reduces the difference between the most and least space size \(S=64\). \(L_{AC}\) makes significant eigenvalues, thus pre- covariance ranks to be equal to venting dimensional collapse. \(S\) in every task.

Figure 9: Average last task acc. of our method for different knowledge distillation techniques.

time of AdaGauss is slightly higher than that of methods with linear classification head (LwF, FeTrIL, AdaGauss with linear head) because Bayes classification requires an additional matrix multiplication when calculating the Mahalanobis distance.

The training time of AdaGauss is longer than for LwF, EFC, FeCAM, and FeTriL as we do not freeze the backbone after the initial task and additionally train the auxiliary adaptation network. Still, AdaGauss takes less time to train than its main competitor - EFC, and is much faster than SSRE. Our method does not increase the number of networks' parameters because the distiller and the adapter are disposed after training steps.

## 5 Conclusions and limitations

In this work, we analyze the impact of dimensionality collapse in EFCIL. We explain that it leads to differences across tasks in ranks of classes' covariance matrices, which in turn causes task-recency bias. We also present that due to distribution drift, means and covariance of classes change, and they should be adapted from task to task. Based on these findings, we propose the first EFCIL method to adapt both means and covariances, dubbed _AdaGauss_. It utilizes feature distillation through a learnable projector and a novel anti-collapse regularization term during training that prevents having degenerated, non-invertible features covariance matrices as class representations. That, in turn, alleviates the task-recency bias of the classifier in continual learning. With the series of experiments, we show that _AdaGauss_ achieves state-of-the-art results in common EFCIL scenarios, both when trained from scratch and when initialized from a pre-trained model.

The limitation of our method is that the cross-entropy separates classes only from the current task. However, when training the feature extractor, old classes can begin overlapping with each other and with new classes int he latent space causing forgetting. This problem is an open question in EFCIL. We speculate it can be alleviated wit a contrastive loss. Another problem arises when there is very little data representing a single class, making high-dimensional covariance matrix impossible to calculate. We tackle it by introducing a bottleneck layer at the very end of the feature extractor. However, it can limit its representational strength.