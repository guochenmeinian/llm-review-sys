# Explanation Shift

How Did the Distribution Shift Impact the Model?

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The performance of machine learning models on new data is critical for their success in real-world applications. However, the model's performance may deteriorate if the new data is sampled from a different distribution than the training data. Current methods to detect shifts in the input or output data distributions have limitations in identifying model behavior changes. In this paper, we define _explanation shift_ as the statistical comparison between how predictions from training data are explained and how predictions on new data are explained. We propose explanation shift as a key indicator to investigate the interaction between distribution shifts and learned models. We introduce an Explanation Shift Detector that operates on the explanation distributions, providing more sensitive and explainable changes in interactions between distribution shifts and learned models. We compare explanation shifts with other methods based on distribution shifts, showing that monitoring for explanation shifts results in more sensitive indicators for varying model behavior. We provide theoretical and experimental evidence and demonstrate the effectiveness of our approach on synthetic and real data. Additionally, we release an open-source Python package, skshift, which implements our method and provides usage tutorials for further reproducibility.

## 1 Introduction

ML theory provides means to forecast the quality of ML models on unseen data, provided that this data is sampled from the same distribution as the data used to train and evaluate the model. If unseen data is sampled from a different distribution than the training data, model quality may deteriorate, making monitoring how the model's behavior changes crucial.

Recent research has highlighted the impossibility of reliably estimating the performance of machine learning models on unseen data sampled from a different distribution in the absence of further assumptions about the nature of the shift [1; 2; 3]. State-of-the-art techniques attempt to model statistical distances between the distributions of the training and unseen data [4; 5] or the distributions of the model predictions [3; 6; 7]. However, these measures of _distribution shifts_ only partially relate to changes of interaction between new data and trained models or they rely on the availability of a causal graph or types of shift assumptions, which limits their applicability. Thus, it is often necessary to go beyond detecting such changes and understand how the feature attribution changes [8; 9; 10; 4].

The field of explainable AI has emerged as a way to understand model decisions [11; 12] and interpret the inner workings of ML models . The core idea of this paper is to go beyond the modeling of distribution shifts and monitor for _explanation shifts_ to signal a change of interactions between learned models and dataset features in tabular data. We newly define explanation shift as the statistical comparison between how predictions from training data are explained and how predictions on new data are explained. In summary, our contributions are:* We propose measures of explanation shifts as a key indicator for investigating the interaction between distribution shifts and learned models.
* We define an _Explanation Shift Detector_ that operates on the explanation distributions allowing for more sensitive and explainable changes of interactions between distribution shifts and learned models.
* We compare our monitoring method that is based on explanation shifts with methods that are based on other kinds of distribution shifts. We find that monitoring for explanation shifts results in more sensitive indicators for varying model behavior.
* We release an open-source Python package skshift, which implements our "_Explanation Shift Detector_", along usage tutorials for reproducibility.

## 2 Foundations and Related Work

### Basic Notions

Supervised machine learning induces a function \(f_{}:(X)(Y)\), from training data \(^{tr}=\{(x_{0}^{tr},y_{0}^{tr}),(x_{n}^{tr},y_{n}^{tr})\}\). Thereby, \(f_{}\) is from a family of functions \(f_{} F\) and \(^{tr}\) is sampled from the joint distribution \((X,Y)\) with predictor variables \(X\) and target variable \(Y\). \(f_{}\) is expected to generalize well on new, previously unseen data \(^{new}_{X}=\{x_{0}^{new},,x_{k}^{new}\}(X)\). We write \(^{tr}_{X}\) to refer to \(\{x_{0}^{tr},,x_{n}^{tr}\}\) and \(^{tr}_{Y}\) to refer to \(^{tr}_{Y}=\{y_{0}^{tr} y_{n}^{tr}\}\). For the purpose of formalizations and to define evaluation metrics, it is often convenient to assume that an oracle provides values \(^{new}_{Y}=\{y_{0}^{new},,y_{k}^{new}\}\) such that \(^{new}=\{(x_{0}^{new},y_{0}^{new}),,(x_{k}^{new},y_{k}^{new}) \}(X)(Y)\).

The core machine learning assumption is that training data \(^{tr}\) and novel data \(^{new}\) are sampled from the same underlying distribution \((X,Y)\). The twin problems of _model monitoring_ and recognizing that new data is _out-of-distribution_ can now be described as predicting an absolute or relative performance drop between \((^{tr})\) and \((^{new})\), where \(()=_{(x,y)}_{}(f_{ }(x),y)\), \(_{}\) is a metric like 0-1-loss (accuracy), but \(^{new}_{Y}\) is unknown and cannot be used for such judgment.

Therefore related work analyses distribution shifts between training and newly occurring data. Let two datasets \(,^{}\) define two empirical distributions \((),(^{})\), then we write \(()\!\!(^{})\) to express that \(()\) is sampled from a different underlying distribution than \((^{})\) with high probability \(p>1-\) allowing us to formalize various types of distribution shifts.

**Definition 2.1** (Data Shift).: We say that data shift occurs from \(^{tr}\) to \(^{new}_{X}\), if \((^{tr}_{X})\!\!(^{new}_{X})\).

Specific kinds of data shift are:

**Definition 2.2** (Univariate data shift).: There is a univariate data shift between \((^{tr}_{X})=(^{tr}_{X_{1}},, ^{tr}_{X_{p}})\) and \((^{new}_{X})=(^{new}_{X_{1}},, ^{new}_{X_{p}})\), if \( i\{1 p\}:(^{tr}_{X_{i}})\!\! (^{new}_{X_{i}})\).

**Definition 2.3** (Covariate data shift).: There is a covariate data shift between \(P(^{tr}_{X})=(^{tr}_{X_{1}},,^ {tr}_{X_{p}})\) and \((^{new}_{X})=(^{new}_{X_{1}},, ^{new}_{X_{p}})\) if \((^{tr}_{X})\!\!(^{new}_{X})\), which cannot only be caused by univariate shift.

The next two types of shift involve the interaction of data with the model \(f_{}\), which approximates the conditional \(^{tr})}{P(^{tr}_{X})}\). Abusing notation, we write \(f_{}()\) to refer to the multiset \(\{f_{}(x)|x\}\).

**Definition 2.4** (Predictions Shift).: There is a predictions shift between distributions \((^{tr}_{X})\) and \((^{new}_{X})\) related to model \(f_{}\) if \((f_{}(^{tr}_{X}))\!\!(f_{}( ^{new}_{X}))\).

**Definition 2.5** (Concept Shift).: There is a concept shift between \((^{tr})=P(^{tr}_{X},^{tr}_{Y})\) and \((^{new})=P(^{new}_{X},^{new}_{Y})\) if conditional distributions change, i.e. \((^{tr})}{(^{tr}_{X})}\! \!(^{new})}{(^{new}_{X})}\).

In practice, multiple types of shifts co-occur together and their disentangling may constitute a significant challenge that we do not address here [14; 15].

### Related Work on Tabular Data

We briefly review the related works below. See Appendix A for a more detailed related work.

**Classifier two-sample test:** Evaluating how two distributions differ has been a widely studied topic in the statistics and statistical learning literature [16; 15; 17] and has advanced in recent years [18; 19; 20]. The use of supervised learning classifiers to measure statistical tests has been explored by Lopez-Paz et al.  proposing a classifier-based approach that returns test statistics to interpret differences between two distributions. We adopt their power test analysis and interpretability approach but apply it to the explanation distributions.

**Detecting distribution shift and its impact on model behaviour:** A lot of related work has aimed at detecting that data is from out-of-distribution. To this end, they have created several benchmarks that measure whether data comes from in-distribution or not [22; 23; 24; 25; 26]. In contrast, our main aim is to evaluate the impact of the distribution shift on the model.

A typical example is two-sample testing on the latent space such as described by Rabanser et al. . However, many of the methods developed for detecting out-of-distribution data are specific to neural networks processing image and text data and can not be applied to traditional machine learning techniques. These methods often assume that the relationships between predictor and response variables remain unchanged, i.e., no concept shift occurs. Our work is applied to tabular data where techniques such as gradient boosting decision trees achieve state-of-the-art model performance [28; 29; 30].

**Impossibility of model monitoring:** Recent research findings have formalized the limitations of monitoring machine learning models in the absence of labelled data. Specifically [3; 31] prove the impossibility of predicting model degradation or detecting out-of-distribution data with certainty [32; 33; 34]. Although our approach does not overcome these limitations, it provides valuable insights for machine learning engineers to understand better changes in interactions resulting from shifting data distributions and learned models.

**Model monitoring and distribution shift under specific assumptions:** Under specific types of assumptions, model monitoring and distribution shift become feasible tasks. One type of assumption often found in the literature is to leverage causal knowledge to identify the drivers of distribution changes [35; 36; 37]. For example, Budhathoki et al.  use graphical causal models and feature attributions based on Shapley values to detect changes in the distribution. Similarly, other works aim to detect specific distribution shifts, such as covariate or concept shifts. Our approach does not rely on additional information, such as a causal graph, labelled test data, or specific types of distribution shift. Still, by the nature of pure concept shifts, the model behaviour remains unaffected and new data need to come with labelled responses to be detected.

**Explainability and distribution shift:** Lundberg et al.  applied Shapley values to identify possible bugs in the pipeline by visualizing univariate SHAP contributions. In our work we go beyond debugging and formalize the multivariate explanation distributions where we perform a two-sample classifier test to detect distribution shift impacts on the model. Furthermore, we provide a mathematical analysis of how the SHAP values contribute to detecting distribution shift.

### Explainable AI: Local Feature Attributions

Attribution by Shapley values explains machine learning models by determining the relevance of features used by the model [38; 39]. The Shapley value is a concept from coalition game theory that aims to allocate the surplus generated by the grand coalition in a game to each of its players . The Shapley value \(_{j}\) for the \(j\)'th player is defined via a value function \(:2^{N}\) of players in \(T\):

\[_{j}()=_{T N\{j\}}((T\{j\})-(T))\] (1)

In machine learning, \(N=\{1,,p\}\) is the set of features occurring in the training data. Given that \(x\) is the feature vector of the instance to be explained, and the term \(_{f,x}(T)\) represents the prediction for the feature values in \(T\) that are marginalized over features that are not included in \(T\):

\[_{f,x}(T)=E_{X|X_{T}=x_{T}}[f(X)]-E_{X}[f(X)]\] (2)

The Shapley value framework satisfies several theoretical properties [12; 40; 41; 42]. Our approach is based on the efficiency and uninformative properties:

**Efficiency Property.** Feature contributions add up to the difference of prediction from \(x^{}\) and the expected value:

\[_{j N}_{j}(f,x^{})=f(x^{})-E[f(X)])\] (3)

**Uniformativeness Property.** A feature \(j\) that does not change the predicted value has a Shapley value of zero.

\[ x,x_{j},x^{}_{j}:f(\{x_{N\{j\}},x_{j}\})=f(\{x_{N \{j\}},x^{}_{j}\}) x:_{j}(f,x)=0.\] (4)

Our approach works with explanation techniques that fulfill efficiency and uninformative properties, and we use Shapley values as an example. It is essential to distinguish between the theoretical Shapley values and the different implementations that approximate them. We use TreeSHAP as an efficient implementation for tree-based models of Shapley values [38; 12; 43], mainly we use the observational (or path-dependent) estimation [44; 45; 46], and for linear models, we use the correlation dependent implementation that takes into account feature dependencies .

LIME is another explanation method candidate for out approach [48; 49]. LIME computes local feature attributions and also satisfies efficiency and uninformative properties, at least in theoretical aspects. However, the definition of neighborhoods in LIME and corresponding computational expenses impact its applicability. In Appendix F, we analyze LIME's relationship with Shapley values for the purpose of describing explanation shifts.

## 3 A Model for Explanation Shift Detection

Our model for explanation shift detection is sketched in Fig. 1. We define it step-by-step as follows:

**Definition 3.1** (Explanation distribution).: An explanation function \(:F(X)^{p}\) maps a model \(f_{}\) and data \(x^{p}\) to a vector of attributions \((f_{},x)^{p}\). We call \((f_{},x)\) an explanation. We write \((f_{},)\) to refer to the empirical _explanation distribution_ generated by \(\{(f_{},x)|x\}\).

We use local feature attribution methods SHAP and LIME as explanation functions \(\).

**Definition 3.2** (Explanation shift).: Given a model \(f_{}\) learned from \(^{tr}\), explanation shift with respect to the model \(f_{}\) occurs if \((f_{},^{new}_{X})(f_{}, ^{tr}_{X})\).

**Definition 3.3** (Explanation shift metrics).: Given a measure of statistical distances \(d\), explanation shift is measured as the distance between two explanations of the model \(f_{}\) by \(d((f_{},^{tr}_{X}),(f_{}, ^{new}_{X}))\).

We follow Lopez et al.  to define an explanation shift metrics based on a two-sample test classifier. We proceed as depicted in Figure 1. To counter overfitting, given the model \(f_{}\) trained on \(^{}\), we compute explanations \(\{(f_{},x)|x^{}_{X}\}\) on an in-distribution validation data set \(^{}_{X}\). Given a dataset \(^{new}_{X}\), for which the status of in- or out-of-distribution is unknown, we compute its explanations \(\{(f_{},x)|x^{}_{X}\}\). Then, we construct a two-samples dataset \(E=\{(S(f_{},x),a_{x})|x^{}_{X},a_{x}=0\}\{(S (f_{},x),a_{x})|x^{}_{X},a_{x}=1\}\) and we train a discrimination model \(g_{}:R^{p}\{0,1\}\) on \(E\), to predict if an explanation should be classified as in-distribution (ID) or out-of-distribution (OOD):

\[=*{arg\,min}_{}_{x^{} _{X}^{}_{X}}(g_{}((f_{ },x)),a_{x}),\] (5)

where \(\) is a classification loss function (e.g. cross-entropy). \(g_{}\) is our two-sample test classifier, based on which AUC yields a test statistic that measures the distance between the \(D^{tr}_{X}\) explanations and the explanations of new data \(D^{new}_{X}\).

Explanation shift detection allows us to detect _that_ a novel dataset \(D^{new}\) changes the model's behavior. Beyond recognizing explanation shift, using feature attributions for the model \(g_{}\), we can interpret _how_ the features of the novel dataset \(D^{new}_{X}\) interact differently with model \(f_{}\) than the features of the validation dataset \(D^{val}_{X}\). These features are to be considered for model monitoring and for classifying new data as out-of-distribution.

## 4 Relationships between Common Distribution Shifts and Explanation Shifts

This section analyses and compares data shifts, prediction shifts, with explanation shifts. Appendix B extends this analysis, and Appendix C draws from these analyses to derive experiments with synthetic data.

### Explanation Shift vs Data Shift

One type of distribution shift that is challenging to detect comprises cases where the univariate distributions for each feature \(j\) are equal between the source \(_{X}^{tr}\) and the unseen dataset \(_{X}^{new}\), but where interdependencies among different features change. Multi-covariance statistical testing is a hard taks with high sensitivity that can lead to false positives. The following example demonstrates that Shapley values account for co-variate interaction changes while a univariate statistical test will provide false negatives.

**Example 4.1**.: _(**Covariate Shift**) Let \(D^{tr} N(_{1}\\ _{2},_{X_{1}}^{2}&0\\ 0&_{X_{2}}^{2}) Y\). We fit a linear model \(f_{}(x_{1},x_{2})=+a x_{1}+b x_{2}.\) If \(_{X}^{new} N(_{1}\\ _{2},_{X_{1}}^{2}&_{X_{1}}^{2} _{X_{2}}\\ _{X_{1}}_{X_{2}}&_{X_{2}})\), then \((_{X_{1}}^{tr})\) and \((_{X_{2}}^{tr})\) are identically distributed with \((_{X_{1}}^{new})\) and \((_{X_{2}}^{new})\), respectively, while this does not hold for the corresponding \(_{j}(f_{},_{X}^{tr})\) and \(_{j}(f_{},_{X}^{new})\)._

The detailed analysis of example 4.1 is given in Appendix B.2.

False positives frequently occur in out-of-distribution data detection when a statistical test recognizes differences between a source distribution and a new distribution, thought the differences do not affect the model behavior [28; 14]. Shapley values satisfy the _Uninformativeness_ property, where a feature \(j\) that does not change the predicted value has a Shapley value of \(0\) (equation 4).

**Example 4.2**.: _Shifts on Uninformative Features. Let the random variables \(X_{1},X_{2}\) be normally distributed with \(N(0;1)\). Let dataset \(^{tr} X_{1} X_{2} Y^{tr}\), with \(Y^{tr}=X_{1}\). Thus \(Y^{tr} X_{2}\). Let \(_{X}^{new} X_{1} X_{2}^{new}\) and \(X_{2}^{new}\) be normally distributed with \(N(;^{2})\) and \(,\). When \(f_{}\) is trained optimally on \(^{tr}\) then \(f_{}(x)=x_{1}\). \((_{X_{2}})\) can be different from \((_{X_{2}}^{new})\) but \(_{2}(f_{},_{X}^{tr})=0=_{2}(f_{},_{X}^{new})\)._

### Explanation Shift vs Prediction Shift

Analyses of the explanations detect distribution shifts that interact with the model. In particular, if a prediction shift occurs, the explanations produced are also shifted.

**Proposition 1**.: Given a model \(f_{}:_{X}_{Y}\). If \(f_{}(x^{}) f_{}(x)\), then \((f_{},x^{})(f_{},x)\).

Figure 1: Our model for explanation shift detection. The model \(f_{}\) is trained on \(^{tr}\) implying explanations for distributions \(_{X}^{val},_{X}^{new}\). The AUC of the two-sample test classifier \(g_{}\) decides for or against explanation shift. If an explanation shift occurred, it could be explained which features of the \(_{X}^{new}\) deviated in \(f_{}\) compared to \(_{X}^{val}\).

By efficiency property of the Shapley values  (equation ((3))), if the prediction between two instances is different, then they differ in at least one component of their explanation vectors.

The opposite direction does not always hold:

**Example 4.3**.: _(**Explanation shift not affecting prediction distribution**) Given \(^{tr}\) is generated from \((X_{1} X_{2} Y),X_{1} U(0,1),X_{2} U(1,2),Y=X_{1}+X_{2}+\) and thus the optimal model is \(f(x)=x_{1}+x_{2}\). If \(^{new}\) is generated from \(X_{1}^{new} U(1,2),X_{2}^{new} U(0,1), Y^{new}=X_{1}^{new}+X_{2} ^{new}+\), the prediction distributions are identical \(f_{}(^{tr}_{X}),f_{}(^{new}_{X}) U(1,3)\), but explanation distributions are different \(S(f_{},^{tr}_{X})\!\!\!S(f_{},^{new}_ {X})\), because \(_{i}(f_{},x)=_{i} x_{i}\)._

Thus, an explanation shift does not always imply a prediction shift.

### Explanation Shift vs Concept Shift

Concept shift comprises cases where the covariates retain a given distribution, but their relationship with the target variable changes (cf. Section 2.1). This example shows the negative result that concept shift cannot be indicated by the detection of explanation shift.

**Example 4.4**.: _Concept Shift Let \(^{tr} X_{1} X_{2} Y\), and create a synthetic target \(y_{i}^{tr}=a_{0}+a_{1} x_{i,1}+a_{2} x_{i,2}+\). As new data we have \(^{new}_{X} X_{1}^{new} X_{2}^{new} Y\), with \(y_{i}^{new}=b_{0}+b_{1} x_{i,1}+b_{2} x_{i,2}+\) whose coefficients are unknown at prediction stage. With coefficients \(a_{0} b_{0},a_{1} b_{1},a_{2} b_{2}\). We train a linear regression \(f_{}:^{tr}_{X}^{tr}_{Y}\). Then explanations have the same distribution, \(((f_{},^{tr}_{X}))=((f_{},^{new}_{X}))\), input data distribution \((^{tr}_{X})=(^{new}_{X})\) and predictions \((f_{}(^{tr}_{X}))=(f_{}( ^{new}_{X}))\). But there is no guarantee on the performance of \(f_{}\) on \(^{new}_{X}\)_

In general, concept shift cannot be detected because \(^{new}_{Y}\) is unknown . Some research studies have made specific assumptions about the conditional \(^{new})}{P(^{new}_{X})}\) in order to monitor models and detect distribution shift [7; 50].

In Appendix B.2.2, we analyze a situation in which an oracle -- hypothetically -- provides \(^{new}_{Y}\).

## 5 Empirical Evaluation

We perform core evaluations of explanation shift detection methods by systematically varying models \(f\), model parametrizations \(\), and input data distributions \(_{X}\). We complement core experiments described in this section by adding further experimental results in the appendix that (i) add details on experiments with synthetic data (Appendix C), (ii) add experiments on further natural datasets (Appendix D), (iii) exhibit a larger range of modeling choices (Appendix E), and (iv) include LIME as an explanation method (Appendix F). Core observations made in this section will only be confirmed and refined, but not countered in the appendix.

### Baseline Methods and Datasets

**Baseline Methods.** We compare our method of explanation shift detection (Section 3) with several methods that aim to detect that input data is out-of-distribution: _(i)_ statistical Kolmogorov Smirnov test on input data , _(ii)_ classifier drift , _(iii)_ prediction shift detection by Wasserstein distance , _(iv)_ prediction shift detection by Kolmogorov-Smirnov test, and _(v)_ model agnostic uncertainty estimation [10; 52]. Distribution Shift Metrics are scaled between 0 and 1. We also compare against Classifier Two-Sample Test  on different distributions as discussed in Section 4, viz. (vi) classifier two-sample test on input distributions (\(g_{}\)) and (vii) classifier two-sample test on the predictions distributions (\(g_{}\)):

\[=_{}_{x^{}_{X}^{new}_{X}}(g_{}(x)),a_{x})=_{} _{x^{}_{X}^{new}_{X}}(g_{}(f_{}(x)),a_{x})\] (6)

**Datasets.** In the main body of the paper we base our comparisons on the UCI Adult Income dataset  and on synthetic data. In the Appendix, we extend experiments to several other datasets, which confirm our findings: ACS Travel Time , ACS Employment , Stackoverflow dataset .

### Experiments on Synthetic Data

Our first experiment on synthetic data showcases the two main contributions of our method: \((i)\) being more sensitive than prediction shift and input shift to changes in the model and \((ii)\) accounting for its drivers. We first generate a synthetic dataset with a shift similar to the multivariate shift one (cf. Section 4.2). However, we add an extra variable \(X_{3}=N(0,1)\) and generate our target \(Y=X_{1} X_{2}+X_{3}\), and parametrize the multivariate shift between \(=r(X_{1},X_{2})\). We train the \(f_{}\) on \(^{tr}\) using a gradient boosting decision tree, while for \(g_{}:(f_{},_{X}^{val})\{0,1\}\), we use a logistic regression for both experiments. In Appendix E we benchmark other estimators and detectors.

Table 1 and Figure 2 show the results of our approach when learning on different distributions. In our sensitivity experiment, we observed that using the explanation shift led to higher sensitivity towards detecting distribution shift. This is due to the efficiency property of the Shapley values, which decompose \(f_{}(_{X})\) into \((f_{},_{X})\). Moreover, we can identify the features that are causing the drift by extracting the coefficients of \(g_{}\), providing global and local explainability.

The right image in Figure 2 compares our approach against Classifier Two Sample Testing for detecting multi-covariate shifts on different distributions. We can see how the explanations distributions have more sensitivity to the others. On the left image, the same experiment against other out-of-distribution detection methods such statistical differences on the input data (Input KS, Classifier Drift)[51; 4], which are model-independent; uncertainty estimation methods[52; 10; 56], whose effectiveness under specific types of shift is unclear; and statistical changes on the prediction distribution (K-S and Wasserstein Distance) [57; 58; 7], which can detect changes in model but lack sensitivity and accountability of the explanation shift. All metrics produce output scaled between 0 and 1.

**Detection Method** & **Covariate** & **Uninformative** & **Accountability** \\  Explanation distribution (\(g_{}\)) & ✓ & ✓ & ✓ \\ Input distribution(\(g_{}\)) & ✓ & ✗ & ✗ \\ Prediction distribution(\(g_{}\)) & ✓ & ✓ & ✗ \\ Input KS & ✗ & ✗ & ✗ \\ Classifier Drift & ✓ & ✗ & ✗ \\ Output KS & ✓ & ✓ & ✗ \\ Output Wasserstein & ✓ & ✓ & ✗ \\ Uncertainty & \(\) & ✓ & ✓ \\ 

Table 1: Conceptual comparison table over different detection methods over the examples discussed above. Learning a Classifier Two-Sample test \(g\) over the explanation distributions is the only method that achieves the desired results and is accountable. We evaluate accountability by checking if the feature attributions of the detection method correspond with the synthetic shift generated in both scenarios

Figure 2: In the left figure, we apply the Classifier Two-Sample Test on (i) explanation distribution, (ii) input distribution, (iii) prediction distribution. Explanation distribution shows highest sensitivity. Comparison of the sensitivity of the _Explanation Shift Detector_. The right figure, related work comparison of distribution shift methods, good indicators should follow a progressive steady positive slope, following the correlation coefficient \(\).

### Experiments on Natural Data: Inspecting Explanation Shifts

In the following experiments, we will provide use cases of our approach in two scenarios with natural data: \((i)\) novel group distribution shift and \((ii)\) geopolitical and temporal shift.

#### 5.3.1 Novel Covariate Group

The distribution shift in this experimental set-up relies on the appearance of a new unseen group at the prediction stage (the group feature is not present in the covariates). We vary the ratio of presence of this unseen group in \(_{X}^{new}\) data. As estimators, we use a gradient-boosting decision tree and a logistic regression(just when indicated); we use a logistic regression for the detector. We compare different estimators and detectors' performance in AppendixE.1 for a benchmark and Appendix E.2 for experiments varying hyperparameters.

#### 5.3.2 Geopolitical and Temporal Shift

In this section, we tackle a geopolitical and temporal distribution shift, for this, we train the model \(f_{}\) in California in 2014 and evaluate it in the rest of the states in 2018. The model \(g_{}\) is trained each time on each state using only the \(_{X}^{new}\) in the absence of the label, and a 50/50 random train-test split evaluates its performance. As models, we use a gradient boosting decision tree[59; 60] as estimator \(f_{}\), and using logistic regression for the _Explanation Shift Detector_.

We hypothesize that the AUC of the "Explanation Shift Detector" on new data will be distinct from on ID data due to the OOD model explanations. Figure 4 illustrates the performance of our method on different data distributions, where the baseline is a hold-out set of \(ID-CA14\). The AUC for

Figure 4: In the left figure, comparison of the performance of _Explanation Shift Detector_, in different states. In the right figure, strength analysis of features driving the change in the model, in the y-axis the features and on the x-axis the different states. Explanation shifts allow us to identify how the distribution shift of different features impacted the model.

Figure 3: Novel group shift experiment on the UCI Adult Income dataset. Sensitivity (AUC) increases with the growing fraction of previously unseen social groups. Left figure: The explanation shift indicates that different social groups exhibit varying deviations from the distribution on which the model was trained. Right figure: We vary the model \(f_{}\) to be trained by XGBoost (solid lines) and Logistic Regression (dots), and the model \(g\) to be trained on different distributions.

\(CA18\), where there is only a temporal shift, is the closest to the baseline, and the OOD detection performance is better in the rest of the states. The most disparate state is Puerto Rico (PR18).

Our next objective is to identify the features where the explanations differ between \(^{tr}_{X}\) and \(^{new}_{X}\) data. To achieve this, we compare the distribution of linear coefficients of the detector between ID and New data. We use the Wasserstein distance as a distance measure, where we generate 1000 in-distribution bootstraps using a \(63.2\%\) sampling fraction from California-14 and 1000 bootstraps from other states in 2018. In the right image of Figure 4, we observe that for PR18, the most crucial feature is the citizenship status1.

Furthermore, we conduct an across-task evaluation by comparing the performance of the "Explanation Shift Detector" on another prediction task in the Appendix D. Although some features are present in both prediction tasks, the weights and importance order assigned by the "Explanation Shift Detector" differ. One of this method's advantages is that it identifies differences in distributions and how they relate to the model.

## 6 Discussion

In this study, we conducted a comprehensive evaluation of explanation shift by systematically varying models (\(f\)), model parametrizations (\(\)), feature attribution explanations (\(\)), and input data distributions (\(_{X}\)). Our objective was to investigate the impact of distribution shift on the model by explanation shift and gain insights into its characteristics and implications.

Our approach cannot detect concept shifts, as concept shift requires understanding the interaction between prediction and response variables. By the nature of pure concept shifts, such changes do not affect the model. To be understood, new data need to come with labelled responses. We work under the assumption that such labels are not available for new data, nor do we make other assumptions; therefore, our method is not able to predict the degradation of prediction performance under distribution shifts. All papers such as [3; 10; 61; 31; 32; 62; 7] that address the monitoring of prediction performance have the same limitation. Only under specific assumptions, e.g., no occurrence of concept shift or causal graph availability, can performance degradation be predicted with reasonable reliability.

The potential utility of explanation shifts as distribution shift indicators that affect the model in computer vision or natural language processing tasks remains an open question. We have used Shapley values to derive indications of explanation shifts, but other AI explanation techniques may be applicable and come with their advantages.

## 7 Conclusions

Commonly, the problem of detecting the impact of the distribution shift on the model has relied on measurements for detecting shifts in the input or output data distributions or relied on assumptions either on the type of distribution shift or causal graphs availability. In this paper, we have provided evidence that explanation shifts can be a more suitable indicator for detecting and identifying distribution shifts' impact in machine learning models. We provide software, mathematical analysis examples, synthetic data, and real-data experimental evaluation. We found that measures of explanation shift can provide more insights than input distribution and prediction shift measures when monitoring machine learning models.

### Reproducibility Statement

To ensure reproducibility, we make the data, code repositories, and experiments publicly available 2. Also, an open-source Python package skshift3 is attached with methods routines and tutorials. For our experiments, we used default scikit-learn parameters . We describe the system requirements and software dependencies of our experiments. Experiments were run on a 4 vCPU server with 32 GB RAM.