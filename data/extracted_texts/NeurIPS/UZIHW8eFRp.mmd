# A Tractable Inference Perspective of Offline RL

Xuejie Liu\({}^{1,3}\), Anji Liu\({}^{2}\), Guy Van den Broeck\({}^{2}\), Yitao Liang\({}^{1}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)Computer Science Department, University of California, Los Angeles

\({}^{3}\)School of Intelligence Science and Technology, Peking University

xjliu@stu.pku.edu.cn, liuanji@cs.ucla.edu

guyvdb@cs.ucla.edu, yitaol@pku.edu.cn

Equal contributionCorresponding author

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)Computer Science Department, University of California, Los Angeles

\({}^{3}\)School of Intelligence Science and Technology, Peking University

xjliu@stu.pku.edu.cn, liuanji@cs.ucla.edu

guyvdb@cs.ucla.edu, yitaol@pku.edu.cn

###### Abstract

A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. In addition to obtaining accurate sequence models, this paper highlights that _tractability_, the ability to exactly and efficiently answer various probabilistic queries, plays an important role in offline RL. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes **Traffic** (**T**ractable **I**nference for **Offline** RL), which leverages modern tractable generative models to bridge the gap between good sequence models and high expected returns at evaluation time. Empirically, Traffic achieves \(7\) state-of-the-art scores and the highest average scores in \(9\) Gym-MuJoCo benchmarks against strong baselines. Further, Traffic significantly outperforms prior approaches in stochastic environments and safe RL tasks with minimum algorithmic modifications. 3

## 1 Introduction

Recent advancements in deep generative models have opened up the possibility of solving offline Reinforcement Learning (RL)  tasks with sequence modeling techniques (termed RvS approaches). Specifically, we first fit a sequence model to the trajectories provided in an offline dataset. During evaluation, the model is tasked to sample actions with high expected returns given the current state. Leveraging modern deep generative models such as GPTs  and diffusion models , RvS algorithms have significantly boosted the performance on various RL problems [1; 6].

Despite its appealing simplicity, it is still unclear whether expressive modeling alone guarantees good performance of RvS algorithms, and if so, on what types of environments. This paper discovers that many common failures of RvS algorithms are not caused by modeling problems. Instead, while useful information is encoded in the model during training, the model is unable to elicit such knowledge during evaluation. Specifically, this issue is reflected in two aspects: (i) _inability to accurately estimate the expected return_ of a state and a corresponding action sequence to be executed given near-perfect learned transition dynamics and reward functions; (ii) even when accurate return estimates exist in the offline dataset and are learned by the model, it could still _fail to sample rewarding actions_ during evaluation.4 At the heart of such inferior evaluation-time performance is the fact that highlynon-trivial conditional generation is required to stimulate high-return actions [32; 3]. Therefore, other than expressiveness, the ability to efficiently and exactly answer various queries (e.g., computing the expected returns), termed _tractability_, plays an equally important role in RvS approaches.

Having observed that the lack of tractability is an essential cause of the underperformance of RvS algorithms, this paper studies _whether we can gain practical benefits from using Tractable Probabilistic Models (TPMs) [35; 7; 23], which by design support exact and efficient computation of certain queries?_ We answer the question in its affirmative by showing that we can leverage a class of TPMs that support computing arbitrary marginal probabilities to significantly mitigate the inference-time suboptimality of RvS approaches. The proposed algorithm **Trifle** (**T**ractable **I**nference for **Offline RL) has three main contributions:

_Emphasizing the important role of tractable models in offline RL._ This is the first paper that demonstrates the possibility of using TPMs on complex offline RL tasks. The superior empirical performance of Trifle suggests that expressive modeling is not the only aspect that determines the performance of RvS algorithms, and motivates the development of better inference-aware RvS approaches.

_Competitive empirical performance._ Compared against strong offline RL baselines (including RvS, imitation learning, and offline temporal-difference algorithms), Trifle achieves the state-of-the-art result on \(7\) out of \(9\) Gym-MuJoCo benchmarks  and has the best average score.

_Generalizability to stochastic environments and safe-RL tasks._ Trifle can be extended to tackle stochastic environments as well as safe RL tasks with minimum algorithmic modifications. Specifically, we evaluate Trifle in 2 stochastic OpenAI-Gym  environments and action-space-constrained MuJoCo environments, and demonstrate its superior performance against all baselines.

## 2 Preliminaries

Offline Reinforcement Learning.In Reinforcement Learning (RL), an agent interacts with an environment that is defined by a Markov Decision Process (MDP) \(,,,,d_{0}\) to maximize its cumulative reward. Specifically, the \(\) is the state space, \(\) is the action space, \(:\) is the reward function, \(:\) is the transition dynamics, and \(d_{0}\) is the initial state distribution. Our goal is to learn a policy \((a|s)\) that maximizes the expected return \([_{t=0}^{T}^{t}r_{t}]\), where \((0,1]\) is a discount factor and \(T\) is the maximum number of steps.

Offline RL  aims to solve RL problems where we cannot freely interact with the environment. Instead, we receive a dataset of trajectories collected using unknown policies. An effective learning paradigm for offline RL is to treat it as a sequence modeling problem (termed RL via Sequence Modeling or RvS methods) [20; 6; 13]. Specifically, we first learn a sequence model on the dataset, and then sample actions conditioned on past states and high future returns. Since the models typically do not encode the entire trajectory, an estimated value or return-to-go (RTG) (i.e., the Monte Carlo estimate of the sum of future rewards) is also included for every state-action pair, allowing the model to estimate the return at any time step.

Tractable Probabilistic Models.Tractable Probabilistic Models (TPMs) are generative models that are designed to efficiently and exactly answer a wide range of probabilistic queries [35; 7; 37]. One example class of TPMs is Hidden Markov Models (HMMs) , which support linear time (w.r.t. model size and input size) computation of marginal probabilities and more. Probabilistic Circuits (PCs)  are a general class of TPMs. As shown in Figure 1, PCs consist of input nodes \(\) that represent simple distributions (e.g., Gaussian, Categorical) over one or more variables as well as sum \(\) and product \(\) nodes that take other nodes as input and gradually form more complex distributions. Specifically, product nodes model factorized distributions over their inputs, and sum nodes build weighted mixtures (mixture weights are labeled on the corresponding edges in Fig. 1) over their input distributions. Please refer to Appx. B for a more detailed introduction to PCs.

Figure 1: An example PC over boolean variables \(X_{1},,X_{4}\). Every nodeâ€™s probability given input \(x_{1}x_{2}}x_{4}\) is labeled in blue. \(p(x_{1}x_{2}}x_{4})=0.22\).

Recent advancements have extensively pushed forward the expressiveness of modern PCs [30; 31; 9], leading to competitive likelihoods on natural image and text datasets compared to even strong Variational Autoencoder  and Diffusion model  baselines. This paper leverages such advances and explores the benefits brought by PCs in offline RL tasks.

## 3 Tractability Matters in Offline RL

Practical RvS approaches operate in two main phases - training and evaluation. In the training phase, a sequence model is adopted to learn a joint distribution over trajectories of length \(T\): \(\{(s_{t},a_{t},r_{t},_{t})\}_{t=0}^{T}\).5 During evaluation, at every time step \(t\), the model is tasked to discover an action sequence \(a_{t:T}:=\{a_{}\}_{=t}^{T}\) (or just \(a_{t}\)) that has high expected return as well as high probability in the prior policy \(p(a_{t:T}|s_{t})\), which prevents it from generating out-of-distribution actions:

\[p(a_{t:T}|s_{t},[V_{t}] v):=p(a_{t: T}|s_{t})&_{V_{t} p(|s_{t},a_{t})}[V_{t}] v,\\ 0&,\] (1)

where \(Z\) is a normalizing constant, \(V_{t}\) is an estimate of the value at time step \(t\), and \(v\) is a pre-defined scalar chosen to encourage high-return policies. Depending on the problem, \(V_{t}\) could be the labeled RTG from the dataset (e.g., \(_{t}\)) or the sum of future rewards capped with a value estimate (e.g., \(_{=t}^{T-1}r_{}+_{T}\)) [13; 20].

The above definition naturally reveals two key challenges in RvS approaches: (i) _training-time optimality_ (i.e., "expressivity"): how well can we fit the offline trajectories, and (ii) _inference-time optimality_: whether actions can be unbiasedly and efficiently sampled from Equation (1). While extensive breakthroughs have been achieved to improve the training-time optimality [1; 6; 20], it remains unclear whether the non-trivial constrained generation task of Equation (1) hinders inference-time optimality. In the following, we present two general scenarios where existing RvS approaches underperform as a result of suboptimal inference-time performance. We attribute such failures to the fact that these models are limited to answering certain query classes (e.g., autoregressive models can only compute next token probabilities), and explore the potential of _tractable_ probabilistic models for offline RL tasks in the following sections.

Scenario #1We first consider the case where the labeled RTG belongs to a (near-)optimal policy. In this case, Equation (1) can be simplified to \(p(a_{t}|s_{t},[V_{t}] v)\) (choose \(V_{t}:=_{t}\)) since one-step optimality implies multi-step optimality. In practice, although the RTGs are suboptimal, the predicted values often match well with the actual returns achieved by the agent. Take Trajectory

Figure 2: RvS approaches suffer from inference-time suboptimality. **Left:** There is a strong positive correlation between the average estimated returns by Trajectory Transformers (TT) and the actual returns in 6 Gym-MuJoCo environments (MR, M, and ME denote medium-replay, medium, and medium-expert, respectively), which suggests that the sequence model can distinguish rewarding actions from the others. **Middle:** Despite being able to recognize high-return actions, both TT and DT  fail to consistently sample such action, leading to bad inference-time optimality; Trifle consistently improves the inference-time optimality score. **Right:** We substantiate the relationship between low inference-time optimality scores and unfavorable environmental outcomes by showing a strong positive correlation between them.

Transformer (TT)  as an example, Figure 2 (left) demonstrates a strong positive correlation between its predicted returns (x-axis) and the actual cumulative rewards (y-axis) on six MuJoCo  benchmarks, suggesting that the model has learned the "goodness" of most actions. In such cases, the performance of RvS algorithms depends mainly on their inference-time optimality, i.e., whether they can efficiently sample actions with high _predicted_ returns. Specifically, let \(a_{t}\) be the action taken by a RvS algorithm at state \(s_{t}\), and \(R_{t}:=[_{t}]\) is the corresponding estimated expected value. We define a proxy of inference-time optimality as the quantile value of \(R_{t}\) in the estimated state-conditioned value distribution \(p(V_{t}|s_{t})\).6 The higher the quantile value, the more frequent the RvS algorithm samples actions with high estimated returns.

We evaluate the inference-time optimality of Decision Transformers (DT)  and Trajectory Transformers (TT) , two widely used RvS algorithms, on various environments and offline datasets from the Gym-MuJoCo benchmark suite . As shown in Figure 2 (middle), the inference-time optimality is averaged (only) around \(0.7\) (the maximum possible value is \(1.0\)) for most settings. And these runs with low inference-time optimality scores receive low environment returns (Fig. 2 (right)).

Scenario #2Achieving inference-time optimality becomes even harder when the labeled RTGs are suboptimal (e.g., they come from a random policy). In this case, even estimating the expected future return of an action sequence becomes highly intractable, especially when the transition dynamics of the environment are stochastic. Specifically, to evaluate a state-action pair \((s_{t},a_{t})\), since \(_{t}\) is uninformative, we need to resort to the multi-step estimate \(V_{t}^{m}:=_{=t}^{t^{}-1}r_{}+_{t^{}}\) (\(t^{}>t\)), where the actions \(a_{t:t^{}}\) are jointly chosen to maximize the expected return. Take autoregressive models as an example. Since the variables are arranged following the sequential order \(,s_{t},a_{t},r_{t},_{t},s_{t+1},\), we need to explicitly sample \(s_{t+1:t^{}}\) before proceed to compute the rewards and the RTG in \(V_{t}^{m}\). In stochastic environments, estimating \([V_{t}^{m}]\) could suffer from high variance as the stochasticity from the intermediate states accumulates over time.

As we shall illustrate in Section 6.2, compared to environments with near-deterministic transition dynamics, estimating the expected returns in stochastic environments using intractable sequence models is hard, and Trifle can significantly mitigate this problem with its ability to marginalize out intermediate states and compute \([V_{t}^{m}]\) efficiently and exactly.

## 4 Exploiting Tractable Models

The previous section demonstrates that apart from modeling, inference-time suboptimality is another key factor that causes the underperformance of RvS approaches. Given such observations, a natural follow-up question is _whether/how more tractable models can improve the evaluation-time performance in offline RL tasks?_ While there are different types of tractabilities (i.e., the ability to compute different types of queries), this paper focuses on studying the additional benefit of _exactly_ computing _arbitrary_ marginal/condition probabilities. This strikes a proper balance between learning and inference as we can train such a tractable yet expressive model thanks to recent developments in the TPM community [9; 30]. Note that in addition to proposing a competitive RvS algorithm, we aim to highlight the necessity and benefit of using more tractable models for offline RL tasks, and encourage future developments on both inference-aware RvS methods and better TPMs. As a direct response to the two failing scenarios identified in Section 3, we first demonstrate how tractability could help even when the labeled RTGs are (near-)optimal (Sec. 4.1). We then move on to the case where we need to use multi-step return estimates to account for biases in the labeled RTGs (Sec. 4.2).

### From the Single-Step Case...

Consider the case where the RTGs are optimal. Recall from Section 3 that our goal is to sample actions from \(p(a_{t}|s_{t},[V_{t}] v)\) (where \(V_{t}:=_{t}\)). Prior works use two typical ways to approximately sample from this distribution. The first approach directly trains a model to generate return-conditioned actions: \(p(a_{t}|s_{t},_{t})\). However, since the RTG given a state-action pair is stochastic,7 sampling from this RTG-conditioned policy could result in actions with a small probability of getting a high return, but with a low expected return [32; 3].

An alternative approach leverages the ability of sequence models to accurately estimate the expected return (i.e., \([_{t}]\)) of state-action pairs . Specifically, we first sample from a prior distribution \(p(a_{t}|s_{t})\), and then reject actions with low expected returns. Such rejection sampling-based methods typically work well when the action space is small (in which we can enumerate all actions) or the dataset contains many high-rewarding trajectories (in which the rejection rate is low). However, the action could be multi-dimensional and the dataset typically contains many more low-return trajectories in practice, rendering the inference-time optimality score low (cf. Fig. 2).

Having examined the pros and cons of existing approaches, we are left with the question of whether a tractable model can improve sampled actions (in this single-step case). We answer it with a mixture of positive and negative results: while computing \(p(a_{t}|s_{t},[V_{t}]\!\!v)\) is NP-hard even when \(p(a_{t},V_{t}|s_{t})\) follows a simple Naive Bayes distribution, we can design an approximation algorithm that samples high-return actions with high probability in practice. We start with the negative result.

**Theorem 1**.: _Let \(a_{t}:=\{a_{t}^{i}\}_{i=1}^{k}\) be a set of \(k\) boolean variables and \(V_{t}\) be a categorical variables with two categories \(0\) and \(1\). For some \(s_{t}\), assume the joint distribution over \(a_{t}\) and \(V_{t}\) conditioned on \(s_{t}\) follows a Naive Bayes distribution: \(p(a_{t},V_{t}|s_{t}):=p(V_{t}|s_{t})_{i=1}^{k}p(a_{t}^{i}|V_{t},s_{ t})\), where \(a_{t}^{i}\) denotes the \(i^{th}\) variable of \(a_{t}\). Computing any marginal over the random variables is tractable yet conditioning on the expectation \(p(a_{t}|s_{t},[V_{t}]\!\!v)\) is NP-hard._

The proof is given in Appx. A. While it seems hard to directly draw samples from \(p(a_{t}|s_{t},[V_{t}]\!\!v)\), we propose to improve the aforementioned rejection sampling-based method by adding a correction term to the original proposal distribution \(p(a_{t}|s_{t})\) to reduce the rejection rate. Specifically, the prior is often represented by an autoregressive model such as GPT: \(p_{}(a_{t}|s_{t}):=_{i=1}^{k}p_{}(a_{t}^{i}|s_{t},a_{t}^{<i})\), where \(k\) is the number of action variables and \(a_{t}^{i}\) is the \(i\)th variable of \(a_{t}\). We propose to sample every dimension of \(a_{t}\) autoregressively following:

\[ i\{1,,k\}(a_{t}^{i}|s_{t},a_{t}^{<i};v):= {1}{Z} p_{}(a_{t}^{i}|s_{t},a_{t}^{<i}) p_{ }(V_{t} v|s_{t},a_{t}^{ i}),\] (2)

where \(Z\) is a normalizing constant and \(p_{}(V_{t}\!\!v|s_{t},a_{t}^{ i})\) is a correction term that leverages the ability of the TPM to compute the distribution of \(V_{t}\) given incomplete actions (i.e., evidence on a subset of action variables). Note that while Equation (2) is mathematically identical to \(p(a_{t}|s_{t},V_{t} v)\) when \(p=p_{}=p_{}\), this formulation gives us the flexibility to use the prior policy (i.e., \(p_{}(a_{t}^{i}|s_{t},a_{t}^{<i})\)) represented by more expressive autoregressive generative models.

As shown in Figure 2 (middle), compared to using \(p(a_{t}|s_{t})\) (as done by TT), the inference-time optimality scores increase significantly when using the distribution specified by Equation (2) (as done by Trifle) across various Gym-MuJoCo benchmarks.

###...To the Multi-Step Case

Recall that when the labeled RTGs are suboptimal, our goal is to sample from \(p(a_{t:t^{}}|s_{t},[V_{t}^{}]\!\!v)\), where \(V_{t}^{}\!:=\!_{=t}^{t^{}-1}r_{}\!+\!_{ t^{}}\) is the multi-step value estimate. However, as shown in the second scenario in Section 3, it is hard even to evaluate the expected return of an action sequence due to the inability to marginalize out intermediate states \(s_{t+1:t^{}}\). Empowered by PCs, we can solve this problem by computing the expectation efficiently as it can be broken down into computing conditional probabilities \(p(r_{}|s_{t},a_{t:t^{}})(t\!\!\!<\!t^{})\) and \(p(_{t^{}}|s_{t},a_{t:t^{}})\) (see Appx. C.2 for details):

\[V_{t}^{}=\!_{=t}^{t^{ }-1}_{r_{} p(:|s_{t},a_{t:t^{}})}r_{} +_{_{t^{}} p(:|s_{t},a_{t:t^{}})} _{t^{}}.\] (3)

We are now left with the same problem discussed in the single-step case - how to sample actions with high expected returns (i.e., \([V_{t}^{}]\)). Similar to Equation (2), we add correction terms that bias the action (sequence) distribution towards high expected returns. Specifically, we augment the original action probability \(_{=t}^{t^{}}p(a_{}|s_{t},a_{<})\) with terms of the form \(p(V_{t}^{}\!\!v|s_{t},a_{})\) This leads to:

\[(a_{t:t^{}}|s_{t};v)\!:=\!_{=t}^{t^{}} (a_{}|s_{t},a_{<};v),\]where \((a_{}|s_{t},a_{<};v)\!\!p(a_{}|s_{t},a_{<})\!\!p (V_{t}^{}\!\!v|s_{t},a_{})\), \(a_{<}\) and \(a_{}\) represent \(a_{t:-1}\) and \(a_{t:}\), respectively.8 In practice, while we compute \(p(V_{t}^{} v|s_{t},a_{})\) using the PC, \(p(a_{}|s_{t},a_{<})=_{s_{t+1:}}[p(a_{}|s_{}, a_{<})]\) can either be computed exactly with the TPM or approximated (via Monte Carlo estimation over \(s_{t+1:}\)) using an autoregressive generative model. In summary, we approximate samples from \(p(a_{t:t^{}}|s_{t},[V_{t}]\!\!v)\) by first sampling from \((a_{t:t^{}}|s_{t};v)\), and then rejecting samples whose (predicted) expected return is smaller than \(v\).

## 5 Practical Implementation

The previous section has demonstrated how to efficiently sample from the expected-value-conditioned policy (Eq. (1)). Based on this sampling algorithm, this section further introduces the proposed algorithm **Trifle** (**T**ractable **I**nference for **O**ffline** RL). The high-level idea of Trifle is to obtain good action (sequence) candidates from \(p(a_{t}|s_{t},[V] v)\), and then use beam search to further single out the most rewarding action. Intuitively, by the definition in Equation (1), the candidates are both rewarding and have relatively high likelihoods in the offline dataset, which ensures the actions are within the offline data distribution and prevents overconfident estimates during beam search.

Beam search maintains a set of \(N\) (incomplete) sequences each starting as an empty sequence. For ease of presentation, we assume the current time step is \(0\). At every time step \(t\), beam search replicates each of the \(N\) actions sequences into \(^{+}\) copies and appends an action \(a_{t}\) to every sequence. Specifically, for every partial action sequence \(a_{<t}\), we sample an action following \(p(a_{t}|s_{0},a_{<t},[V_{t}] v)\), where \(V_{t}\) can be either the single-step or the multi-step estimate depending on the task. Now that we have \( N\) trajectories in total, the next step is to evaluate their expected return, which can be computed exactly using the PC (see Sec. 4.2). The \(N\)-best action sequences are kept and proceed to the next time step. After repeating this procedure for \(H\) time steps, we return the best action sequence. The first action in the sequence is used to interact with the environment. Please refer to Appx. C for detailed descriptions of the algorithm.

Another design choice is the threshold value \(v\). While it is common to use a fixed high return throughout the episode, we follow  and use an adaptive threshold. Specifically, at state \(s_{t}\), we choose \(v\) to be the \(\)-quantile value of \(p(V_{t}|s_{t})\), which is computed using the PC.

## 6 Experiments

This section takes gradual steps to study whether Trifle can mitigate the inference-time suboptimality problem in different settings. First, in the case where the labeled RTGs are good performance indicators (i.e., the single-step case), we examine whether Trifle can consistently sample more

    &  &  &  &  & } & } & } &  &  \\   & & base & & & & & & & & & & & & & \\  Med-Expert & Half/Cheath & 95.0\(\)0.2 & **95.1\(\)**0.0 & 82.3\(\)6.1 & **89.9\(\)**4.6 & 86.8\(\)1.3 & **91.9\(\)**1.9 & 90.6 & 86.7 & 91.6 & 92.9 & 90.7 \\ Med-Expert & Hopper & 110.0\(\)2.2 & **113.0\(\)**0.4 & 74.7\(\)**6.3 & **78.5\(\)**6.4 & 107.6\(\)1.8 & / & 111.8 & 91.5 & 105.4 & 110.9 & 98.0 \\ Med-Expert & Walker2d & 101.9\(\)6.8 & **109.3\(\)**0.1 & 109.3\(\)1.3 & **109.6\(\)**0.2 & 108.1\(\)**0.2 & **108.6\(\)**0.3 & 108.8 & 109.6 & 108.8 & 109.0 & 110.1 \\  Medium & HalfCheath & 46.9\(\)0.4 & **95.2\(\)**0.0 & 87.3\(\)**0.3 & **48.3\(\)**0.3 & 42.6\(\)**0.4 & **44.2\(\)**0.4 & 41.4\(\)**0.4 & 42.5\(\)**0.7 & 44.4 & 42.5 & 48.3 \\ Medium & Hopper & 61.1\(\)0.4 & **67.1\(\)**0.3 & 55.2\(\)**0.3 & **57.8\(\)**0.3 & 67.6\(\)0.1 & / & 79.3 & 66.3 & 58.5 & 56.9 & 59.3 \\ Medium & Walker2d & 79.0\(\)2.8 & **83.1\(\)**0.3 & 82.2\(\)2.5 & **84.7\(\)**1.9 & 74.1\(\)**0.4 & **81.3\(\)**2.3 & 82.5 & 78.3 & 72.5 & 75.0 & 83.7 \\  Med-Replay HalfCheath & 41.9\(\)2.5 & **45.0\(\)**0.3 & 48.2\(\)0.4 & **48.9\(\)**0.3 & 36.6\(\)0.8 & **39.2\(\)**0.4 & 39.3 & 44.2 & 45.5 & 40.6 & 44.6 \\ Med-Replay Hopper & 91.5\(\)3.6 & **97.8\(\)**0.3 & 83.4\(\)5.6 & **87.6\(\)**6.1 & 82.7\(\)0.8 & / & 100.0 & 94.7 & 95.0 & 75.9 & 60.9 \\ Med-Replay Walker2d & 82.6\(\)6.9 & **88.3\(\)**3.8 & 84.6\(\)4.5 & **90.6\(\)**4.2 & 66.6\(\)**0.3 & **73.5\(\)**0.1 & 75.0 & 73.9 & 77.2 & 62.5 & 81.8 \\ 
**Average Score** & 78.9 & **83.1** & 74.3 & 77.4 & 74.7 & / & 81.8 & 77.0 & 77.6 & 74.0 & 75.3 \\   

Table 1: Normalized Scores on the standard Gym-MuJoCo benchmarks. The results of Trifle are averaged over 12 random seeds (For DT-base and DT-Trifle, we adopt the same number of seeds as ). Results of the baselines are acquired from their original papers.

rewarding actions (Sec. 6.1). Next, we further challenge Trifle in highly stochastic environments, where existing RvS algorithms fail catastrophically due to the failure to account for the environmental randomness (Sec. 6.2). Finally, we demonstrate that Trifle can be directly applied to safe RL tasks (with action constraints) by effectively conditioning on the constraints (Sec. 6.3). Collectively, this section highlights the potential of TPMs on offline RL tasks.

### Comparison to the State of the Art

As demonstrated in Section 3 and Figure 2, although the labeled RTGs in the Gym-MuJoCo  benchmarks are accurate enough to reflect the actual environmental return, existing RvS algorithms fail to effectively sample such actions due to their large and multi-dimensional action space. Figure 2 (middle) has demonstrated that Trifle achieves better inference-time optimality. This section further examines whether higher inference-time optimality scores could consistently lead to better performance when building Trifle on top of different RvS algorithms, i.e., combining \(p_{}\) (cf. Eq. (2)) with different prior policies \(p_{}\) trained by the corresponding RvS algorithm.

Environment setupThe Gym-MuJoCo benchmark suite collects trajectories in \(3\) locomotion environments (HalfCheetah, Hopper, Walker2D) and constructs \(3\) datasets (Medium-Expert, Medium, Medium-Replay) for every environment, which results in \(3 3=9\) tasks. For every environment, the main difference between the datasets is the quality of its trajectories. Specifically, the dataset "Medium" records 1 million steps collected from a Soft Actor-Critic (SAC)  agent. The "Medium-Replay" dataset adopts all samples in the replay buffer recorded during the training process of the SAC agent. The "Medium-Expert" dataset mixes 1 million steps of expert demonstrations and 1 million suboptimal steps generated by a partially trained SAC policy or a random policy. The results are normalized such that a well-trained SAC model hits 100 and a random policy has a 0 score.

BaselinesWe build Trifle on top of three effective RvS algorithms: Decision Transformer (DT) , Trajectory Transformer (TT)  as well as its variant TT(+Q) where the RTGs estimated by summing up future rewards in the trajectory are replaced by the Q-values generated by a well-trained IQL agent . In addition to the above base models, we also compare Trifle against many other strong baselines: (i) Decision Diffuser (DD) , which is also a competitive RvS method; (ii) Offline TD learning methods IQL  and CQL ; (iii) Imitation learning methods like the variant of BC  which only uses 10% of trajectories with the highest return, and TD3(+BC) .

Since the labeled RTGs are informative enough about the "goodness" of actions, we implement Trifle by adopting the single-step value estimate following Section 4.1, where we replace \(p_{}\) with the policy of the three adopted base methods, i.e., \(p_{}(a_{t}|s_{t})\), \(p_{(+)}(a_{t}|s_{t})\) and \(p_{}(a_{t}|s_{t})\).

Empirical InsightsResults are shown in Table 1.9 First, to examine the benefit brought by TPMs, we compare Trifle with three base policies, as the main algorithmic difference is the use of the improved proposal distribution (Eq. (2)) for sampling actions. We can see that Trifle not only achieves a large performance gain over TT and DT in all environments, but also significantly outperforms TT(+Q) where we have access to more accurate labeled values, indicating that Trifle can enhance the inference-time optimality of base policy reliably and benefit from any improvement of the training-time optimality. See Appx. E.1 for more results and ablation studies.

Moreover, compared with all baselines, Trifle achieves the highest average score of \(83.1\). It also succeeds in achieving \(7\) state-of-the-art scores out of \(9\) benchmarks. We conduct further ablation studies on the rejection sampling component and the adaptive thresholding component (i.e., selecting \(v\)) in Appx. F.

### Evaluating Trifle in Stochastic Environments

This section further challenges Trifle on stochastic environments with highly suboptimal trajectories as well as labeled RTGs in the offline dataset. As demonstrated in Section 3, in this case, it is even hard to obtain accurate value estimates due to the stochasticity of transition dynamics.Section 4.2 demonstrates the potential of Trifle to more reliably estimate and sample action sequences under suboptimal labeled RTGs and stochastic environments. This section examines this claim by comparing the five following algorithms:

(i) Trifle that adopts \(V_{t}=_{t}\) (termed single-step Trifle or **s-Trifle**); (ii) Trifle equipped with \(V_{t}=_{=t}^{t^{}}r_{}+_{t^{}}\) (termed multi-step Trifle or **m-Trifle**; see Appx. E.2 for additional details); (iii) TT ; (iv) DT  (v) Dichotomy of Control (DoC) , an effective framework to deal with highly stochastic environments by designing a mutual information constraint for DT training, which is a representative baseline while orthogonal to our efforts.

We evaluate the above algorithms on two stochastic Gym environments: Taxi and FrozenLake. Here we choose the Taxi benchmark for a detailed analysis of whether and how Trifle could overcome the challenges discussed in Section 4.2. Among the first four algorithms, s-Trifle and DT do not compute the "more accurate" multi-step value, and TT approximates the value by Monte Carlo samples. Therefore, we expect their relative performance to be DT \(\) s-Trifle \(<\) TT \(<\) m-Trifle.

Environment setupWe create a stochastic variant of the Gym-Taxi Environment . As shown in Figure 2(a), a taxi resides in a grid world consisting of a passenger and a destination. The taxi is tasked to first navigate to the passenger's position and pick them up, and then drop them off at the destination.There are \(6\) discrete actions available at every step: (i) \(4\) navigation actions (North, South, East, or West), (ii) Pick-up, (iii) Drop-off. Whenever the agent attempts to execute a navigation action, it has \(0.3\)_probability of moving toward a randomly selected unintended direction_. At the beginning of every episode, the location of the taxi, the passenger, and the destination are randomly initialized randomly. The reward function is defined as follows: (i) -1 for each action undertaken; (ii) an additional +20 for successful passenger delivery; (iii) -4 for hitting the walls; (iv) -5 for hitting the boundaries; (v) -10 for executing Pick-up or Drop-off actions unlawfully (e.g., executing Drop-off when the passenger is not in the taxi).

Following the Gym-MuJoCo benchmarks, we collect offline trajectories by running a Q-learning agent  in the Taxi environment and recording the first 1000 trajectories that drop off the passenger successfully, which achieves an average return of -128.

Empirical InsightsWe first examine the accuracy of estimated returns for s-Trifle, m-Trifle, and TT. DT is excluded since it does not explicitly estimate the value of action sequences. Figure 4 illustrates the correlation between predicted and ground-truth returns of the three methods. First, s-Trifle performs the worst since it merely uses the inaccurate \(_{t}\) to approximate the ground-truth return. Next, thanks to its ability to exactly compute the multi-step value estimates, m-Trifle outperforms TT, which approximates the multi-step value with Monte Carlo samples.

We proceed to evaluate their performance in the stochastic Taxi environment. As shown in Figure 2(c), the relative performance of the first four algorithms is DT \(<\) TT \(<\) s-Trifle \(<\) m-Trifle, which largely aligns with the anticipated results. The only "surprising" result is the superior performance of s-Trifle compared to TT. One plausible explanation for this behavior is that while TT can better estimate the given actions, it fails to efficiently sample rewarding actions.

Notably, Trifle also significantly outperforms the strong baseline DoC, demonstrating its potential in handling stochastic transitions. To verify this, we further evaluate Trifle on the stochastic FrozenLake

Figure 3: (a) Stochastic Taxi environment; (b) Stochastic FrozenLake Environment; (c) Average returns on the stochastic environment. All the reported numbers are averaged over 1000 trials.

environment. Apart from fixing the stochasticity level \(p=\),10 the experiment design follows the DoC paper . For data collection, we perturb the policy of a well-trained DQN (with an average return of \(0.7\)) with the \(\)-greedy strategy. Here \(\) is a proxy of offline dataset quality and varies from \(0.3\) to \(0.7\). As shown in Figure 3c, when the offline dataset contains many successful trials (\(=0.3\)), all methods perform closely to the optimal policy. As the rollout policy becomes more suboptimal (with the increase of \(\)), the performances of DT and TT drop quickly, while Trifle still works robustly and outperforms all baselines.

### Action-Space-Constrained Gym-MuJoCo Variants

This section demonstrates that Trifle can be readily extended to safe RL tasks by leveraging TPM's ability to compute conditional probabilities. Specifically, besides achieving high expected returns, safe RL tasks require additional constraints on the action or states to be satisfied. Therefore, define the constraint as \(c\), our goal is to sample actions from \(p(a_{t}|s_{t},[V_{t}] v,c)\), which can be achieved by conditioning on \(c\) in the candidate action sampling process.

Environment setupIn MuJoCo environments, each dimension of \(a_{t}\) represents the torque applied on a certain rotor of the hinge joints at timestep \(t\). We consider action space constraints in the form of "value of the torque applied to the foot rotor \( A\)", where \(A=0.5\) is a threshold value, for three MuJoCo environments: Halfcheetah, Hopper, and Walker2d. Note that there are multiple foot joints in Halfcheetah and Walker2d, so the constraint is applied to multiple action dimensions.11 For all settings, we adopt the "Med-Expert" offline dataset as introduced in Section 6.1.

Empirical InsightsThe key challenge in these action-constrained tasks is the need to account for the constraints applied to other action dimensions when sampling the value of some action variable. For example, autoregressive models cannot take into account constraints added to variable \(a_{t}^{i+1}\) when sampling \(a_{t}^{i}\). Therefore, while enforcing the action constraint is simple, it remains hard to simultaneously guarantee good performance. As shown in Table 2, owing to its ability to exactly condition on the action constraints, Trifle outperforms TT significantly across all three environments.

   Dataset & Environment & Trifle & TT \\  Med-Expert & Halfcheetah & **81.9\(\)**4.8 & 77.8\(\)5.4 \\ Med-Expert & Hopper & **109.6\(\)**2.4 & 100.0\(\)**4.2 \\ Med-Expert & Walker2d & **105.1\(\)**2.3 & 103.6\(\)**4.9 \\   

Table 2: Normalized Scores on the Action-Space-Constrained Gym-MuJoCo Variants. The results of Trifle and TT are both averaged over 12 random seeds, with mean and standard deviations reported.

Figure 4: Correlation between average estimated returns and true environmental returns for s-Trifle (w/ single-step value estimates), TT, and m-Trifle (w/ multi-step value estimates) in the stochastic Taxl domain. \(R\) denotes the correlation coefficient. The results demonstrate that (i) multi-step value estimates (TT and m-Trifle) are better than single-step estimates (s-Trifle), and (ii) exactly computed multi-step estimates (m-Trifle) are better than approximated ones (TT) in stochastic environments.

Related Work and Conclusion

In offline reinforcement learning tasks, our goal is to utilize a dataset collected by unknown policies to derive an improved policy without further interactions with the environment. Under this paradigm, we wish to generalize beyond naive imitation learning and stitch good parts of the behavior policy. To pursue such capabilities, many recent works frame offline RL tasks as conditional modeling problems that generate actions with high expected returns  or its proxies such as immediate rewards . Recent advances in this line of work can be highly credited to the powerful expressivity of modern sequence models, since by accurately fitting past experiences, we can obtain 2 types of information that potentially imply high expected returns: (i) transition dynamics of the environment, which serves as a necessity for planning in model-based fashion , (ii) a decent policy prior which acts more reasonably than a random policy to improve from .

While prior works on model-based RL (MBRL) also leverage models of the transition dynamics and the reward function , RvS approaches focus more on directly modeling the correlation between actions and their end-performance. Specifically, MBRL approaches focus on planning _only_ with the environment model. Despite being theoretically appealing, MBRL requires heavy machinery to account for the accumulated errors during rollout  and out-of-distribution problems . All these problems add a significant burden on the inference side, which makes MBRL algorithms less appealing in practice. In contrast, while RvS algorithms can mitigate this inference-time burden by directly learning the correlation between actions and returns, the suboptimality of labeled returns could degrade their performance. One potential solution is to combine RvS algorithms with temporal-difference learning to correct errors in the labeled returns .

While also aiming to mitigate the problem caused by suboptimal labeled RTGs, our work takes a different route -- by leveraging TPMs to mitigate the inference-time computational burden. Specifically, we identified major problems caused by the lack of tractability in the sequence models, and show that with the ability to compute more queries efficiently, we can partially solve both identified problems.

Limitations.One major limitation of Trifle is its dependency on expressive TPMs trained on sequential data -- if the TPMs are inaccurate, then Trifle will also have inferior performance. Another limitation is that current implementations of PCs are not as efficient as neural network packages, which could slow down the execution of Trifle.