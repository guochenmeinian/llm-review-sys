# SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data

Jialu Li1 Jaemin Cho1 Yi-Lin Sung Jaehong Yoon Mohit Bansal

UNC Chapel Hill

{jialuli, jmincho, ylsung, jhyoon, mbansal}@cs.unc.edu

https://selma-t2i.github.io

equal contribution

###### Abstract

Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fail to generate images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce **SELMA**: Skill-Specific **E**xpert **L**earning and **M**erging with **A**uto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (\(+2.1\%\) on TIFA and \(+6.9\%\) on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.

## 1 Introduction

Text-to-Image (T2I) generation models have shown impressive development in recent years [61; 59; 50; 30; 57; 85; 10]. Although these approaches can generate high-quality images based on textual inputs, they still struggle to capture all semantics in the given textual prompts, such as failing to compose multiple subjects [85; 22; 40] and generate correct spatial relationships .

Many recent works have been proposed to tackle these challenges in text-to-image generation, aiming to enhance the faithfulness of T2I models to textual inputs. One line of research focuses on supervised fine-tuning on high-quality image-text datasets with human annotations  or image-text pairs with re-captioned text prompts [65; 5], as shown in Fig. 1 (a). Another line of research is based on aligning T2I models with human preference annotations [82; 52; 21; 33; 76], as shown in Fig. 1 (b). Otherworks focus on introducing additional layouts or object grounding boxes to guide the generation process [37; 81; 84; 22; 16; 89]. Despite achieving significant improvements in aligning generated images with input textual prompts, the success of these approaches relies on the quality of the layouts created from the textual prompts, the collection of high-quality annotations with human efforts, or the existence of large-scale ground truth data, which involves expensive human annotation.

Motivated by LLMs' impressive text generation capability (given open-ended task instructions and in-context examples), and recent T2I models' capability in generating highly realistic photos (based on text prompts), we investigate an interesting question to further improve the faithfulness of state-of-the-art T2I models: "_Can we automatically generate multi-skill image-text datasets with LLMs and T2I models, to effectively and efficiently teach different image generation skills to T2I models?_" In this paper, we propose **SELMA**: **S**kill-Specific **E**xpert **L**earning and **M**erging with **A**uto-Generated Data, a novel paradigm for eliciting the pre-trained knowledge in T2I models for improved faithfulness based on skill-specific learning and merging of experts. SELMA consists of four stages: (1) collecting skill-specific prompts with in-context learning of LLMs, (2) self-generating image-text samples for diverse skills without the need of human annotation nor feedback from reward models, (3) fine-tuning the expert T2I models on these datasets separately, and (4) obtaining the final model by merging experts of each dataset for efficient adaptation to different skills and mitigation of knowledge conflict in joint training. We illustrate the SELMA pipeline in Fig. 1 (c).

In the first and second stages, we use the LLM and the T2I model to generate skill-specific image-text data. The skills include understanding common objects (_e.g._, puppy in a backyard), handling long prompts (_e.g._, an elegant room with floor-to-ceiling bookshelves, filled with an impressive collection of books of all genres. The cozy reading nook by the window invites anyone to curl up with a good book."), and displaying commonsense-defying scenes (_e.g._, cat flying over sky). We aim to teach diverse generation skills to the same T2I model (_i.e._, self-learning), so that they can handle different types of prompts. To generate image-text pairs for different skills, we first query GPT-3.5  for prompt generation by using only three skill-specific prompts as in-context examples, and filter the generated prompts with ROUGE-L score to maximize prompt diversity to collect 1K prompts in total (Sec. 3.1). Then we use Stable Diffusion models [59; 50] themselves to generate corresponding images from the prompts (Sec. 3.2). We find that our skill-specific training can help mitigate knowledge conflict when jointly learning multiple skills (see Table 2).

Figure 1: Comparison of different fine-tuning paradigms for text-to-image (T2I) generation models. **(a) Supervised Fine-tuning (SFT)**: a T2I model is trained with image-text pairs from existing datasets. **(b) Fine-tuning with Human Preference (_e.g._, RL/DPO)**: humans annotate their preferences on images by ranking/scoring in terms of text alignments, and a T2I model is trained to maximize the human preference scores. **(c) SELMA**: instead of collecting image-text pairs or human preference annotations, we automatically collect image-text pairs for desired skills with LLM and T2I model, and create a multi-skill T2I model by learning and merging skill-specific expert models.

In the third and fourth stages, we fine-tune a T2I model with the collected image-text pairs to teach different skills. However, updating the entire model weights can be inefficient; knowledge conflicts within mixed datasets may also lead to suboptimal performance . Thus, in the third stage, we fine-tune T2I models on these self-generated image-text pairs with parameter-efficient LoRA (low-rank adaptation) modules  to create skill-specific expert T2I models (Sec. 3.3). In the fourth stage, to build a joint multi-skill T2I model that can have faithful generations across different skills, we merge the skill-specific experts based on LoRA merging [66; 91] (Sec. 3.4).

We validate the usefulness of SELMA with public state-of-the-art T2I models - a family of Stable Diffusion - v1.4 , v2 , and XL  on two text faithfulness evaluation benchmarks (DSG  and TIFA ), three human preference metrics (Pick-a-Pic , ImageReward , and HPS ), and human evaluation. Empirical results demonstrate that SELMA significantly improves T2I models' faithfulness to input text prompts and achieves higher human preference metrics. Our final LoRA-Merging model achieves 6.9% improvements on DSG, 2.1% improvements on TIFA, and improves the human preference metrics by 0.4 on Pick-a-Pic, 0.39 on ImageReward, and 3.7 on HPS. Furthermore, we empirically show that the T2I models learned from the self-generated images achieve a performance similar to that of learning from ground-truth images (see Fig. 3). Lastly, we further show that fine-tuning with images from a weaker T2I model (_i.e._, SD v2) can help improve the faithfulness of a stronger T2I model (_i.e._, SDXL), suggesting promising weak-to-strong generalization in text-to-image models (see Table 3).

## 2 Related Work

Training Vision-Language Models with Synthetic Images.As recent denoising diffusion models [69; 25] have achieved photorealistic image synthesis capabilities, many works have studied using their synthetic images for training different models. Azizi _et al._, Sariyildiz _et al._, Lei _et al._, inter alia, study training image classification models with synthetic images. For image captioning, Caffagni _et al._ use diffusion models to generate images on the captioning data. For training CLIP  models, several works use diffusion models to generate images from existing captions  or text generated with language models . There is a recent research direction using synthetic images to train image generation models themselves, and we discuss more details in the following paragraph.

Training Text-to-Image Generation Models with Synthetic Images.A line of recent works train text-to-image (T2I) generation models with synthetic images generated by the same or other models annotated with human preference scores using reinforcement learning [33; 82; 79; 20; 17; 21] or direct preference optimization (DPO) [55; 76]. While these works show promising results in improving model behavior with human preferences, they require expensive human preference annotations. SPIN-Diffusion  proposes using self-play [62; 73], which was successfully adopted in Alphago Zero  and language models [13; 88], where the model itself becomes a judge and iteratively compares itself with previous iterations. However, self-play algorithm still relies on a set of ground truth image-text pairs as positive examples for supervision. Concurrent/independent to our work, DreamSync  trains a T2I model by first creating text prompts with LLMs, sampling multiple images by the T2I model itself, filtering out images with off-the-shelf scorers, and fine-tuning the model on the resulting synthetic image-text pairs . Unlike DreamSync that depends on image filtering (generating 8 images and taking at most one of them for each text prompt, SELMA generates 1 image for each prompt), significantly improving data generation efficiency by using only 2% of image-text pairs compared with DreamSync. Furthermore, we focus on learning multiple skills with T2I models by learning and merging skill-specific LoRA experts to mitigate knowledge interference across different skills, and we show this approach attains much stronger performance without adding any additional inference cost (see Table 2).

## 3 SELMA: Learning and Merging Text-to-Image Skill-Specific Experts with Auto-Generated Data

We introduce SELMA, a novel framework to teach different skills to a T2I generation model based on auto-generated data and model merging. As illustrated in Fig. 2, SELMA consists of four stages: (1) skill-specific prompt generation with LLM (Sec. 3.1), (2) image generation with T2I Model (Sec. 3.2), (3) skill-specific expert learning (Sec. 3.3), and (4) merging expert models (Sec. 3.4).

### Automatic Skill-Specific Prompt Generation with LLM

As shown in Fig. 2 (a), we automatically collect skill-specific prompts (that will be paired with images in Sec. 3.2) to fine-tune T2I models in two steps: (1) using large language models (LLMs) to generate prompts with brief skill descriptions and a few example prompts and (2) filtering the generated prompts to ensure their diversity. In the following, we explain the two steps in detail.

**Prompt Generation.** We leverage the in-context learning ability of LLMs to generate additional text prompts that follow similar writing styles (_e.g._, paragraph style) or acquire models' knowledge in the same domain (_e.g._, count capability). We manually collect three seed prompts with similar writing styles or acquire similar skills (_e.g._, spatial reasoning) to the target text prompts. Next, we use these seed prompts as in-context learning examples to query GPT-3.5 (GPT3.5-turbo-instruct) . We provide additional instructions that encourage diversity in generated prompts, including object occurrences, sentence patterns, and required skills for the T2I model to generate accurate prompts. The detailed prompt template can be found in the Appendix. During prompt generation, we keep expanding the seed prompts with the generated prompts, and always randomly sample three prompts as in-context learning examples from the seed prompts.

**Prompt Filtering.** To improve the diversity of the collected text prompts, we filter out prompts that are similar to already generated ones. As Taori _et al_.  demonstrate that instruction diversity is crucial for improving the instruction following capability of large language models, we follow the same intuition to create diverse text prompts. To ensure the diversity of generated prompts, we first receive a newly generated text prompt from the previous step. Then, we calculate its highest ROUGE-L  score with all the previously generated and filtered prompts. Following Taori _et al_. , we discard text prompts with ROUGE-L\(>\)0.8 to maximize the diversity of generated prompts.

### Automatic Image Generation with Text-to-Image Models

As illustrated in Fig. 2 (b), we generate corresponding images for each generated text prompt using the T2I model. We find that existing diffusion-based T2I models are highly effective in learning from their self-generated images, and even benefit from learning from images generated with weaker T2I models (Table 3). It is important to leverage the knowledge that already exists inside the T2I models (learned from web data during pre-training), and hence we aim to extract this knowledge for creating the skill-specific image-text pairs, and use them to improve T2I models' faithfulness (Sec. 3.3).

### Fine-tuning with Multiple Skill-Specific LoRA Experts

We efficiently adapt the T2I model to different skills by learning skill-specific Low-Rank Adaptation (LoRA)  experts. In LoRA fine-tuning, the updates to the original weights \(W_{0} R^{d d}\) is decomposed with two low-rank matrices: \(W_{0}+ W=W_{0}+BA\), where \(B R^{d r}\), \(A R^{r d}\) and \(r<<d\). For each new dataset, we fine-tune the T2I model with LoRA independently, and this introduces \(\) skill-specific LoRA experts (as shown in Fig. 2 (c)). In Sec. 5.2, we observe that learning and merging skill-specific experts is more effective than learning a single LoRA across all datasets, by helping the T2I model mitigate knowledge conflicts between different skills [42; 11]. However, using multiple skill-specific experts requires the model to know which expert to use for a given input, and this usually requires user annotations on the skill category of inputs. In the next section, we propose to merge skill-specific experts to efficiently construct a single multi-skill model.

Figure 2: Illustration of the four-stage pipeline of SELMA (Sec. 3).

### Merging LoRA Expert Models to Obtain a Multi-Skill Model

Recent work of model merging [29; 68; 2; 71; 83] proposes to merge multiple task-specific weights into one, while retaining the original task-specific performances. Moreover, model merging can help mitigate the knowledge conflicts between datasets because we only need to adjust the merging ratios without re-training the task-specific models [86; 56]. Due to these benefits, we extend model merging to learn a final T2I model that can handle multiple skills without knowledge conflicts. Concretely, given \(\) LoRA experts learned from Sec. 3.3, we merge all LoRA experts into one (\(A^{}=_{n}A^{n}\) and \(B^{}=_{n}B^{n}\)); the resulting single expert can handle all \(\) skills simultaneously (as shown in Fig. 2 (d)). With this approach, we can reach superior performance over standard multi-task LoRA training and even MoE-LoRA (learning a router with LoRA experts), as shown in Tables 2 and 5, and also eliminate the need to know the skill categories beforehand. Note that while ZipLoRA  has demonstrated the use of LoRA merging (merging 2 LoRA modules) in diffusion models, to the best of our knowledge, we are the first to show the effectiveness of LoRA merging on multiple diverse skills (from 5 datasets) in diffusion models.

## 4 Experimental Setup

### Evaluation Benchmarks

We evaluate models on two evaluation benchmarks that measure the alignment between text prompts and generated images: **DSG** and **TIFA**.

**DSG** consists of 1060 prompts from 10 different sources (160 prompts from TIFA , and 100 prompts from each of Localized Narratives , DiffusionDB , CountBench , Whoops , DrawText , Midjourney , Stanford Paragraph , VRD , PoseScript ). Among the ten DSG prompt sources, we mainly experiment with text prompts from five prompt sources that have (1) ground-truth image-text pairs (to compare the usefulness of auto-generated data with ground-truth data) and (2) measuring different skills required in T2I generation (_e.g._, following long captions, composing infrequent objects). Specifically, we use **COCO** for short prompts with common objects in daily life, **Localized Narratives** for paragraph-style long captions, **DiffusionDB** for human-written prompts that specify many attribute details, **CountBench** for evaluating object counting, and **Whoops** for commonsense-defying text prompts.

**TIFA** consists of 4,081 prompts from four sources, including COCO  for short prompts with common objects, PartiPrompts  / DrawBench  for challenging image generation skills, and PaintSkills  for compositional visual reasoning skills.

### Evaluation Metrics

We quantitatively evaluate the performance of T2I generation models in text faithfulness and human preference metrics. Specifically, to evaluate text faithfulness, we use VQA accuracy from TIFA  and DSG . To evaluate human preference score, we use the PickScore , ImageReward , and HPS  See also Sec. 5.6 for human evaluation. Details can be found in Appendix.

### Implementation Details

In the **prompt generation** stage (Sec. 3.1), we use gpt-3.5-turbo-instruct to generate text prompts. We collect 1K prompts for each of the five datasets (COCO , Localized Narratives , DiffusionDB , CountBench , and Whoops ). We refer to the resulting auto-generated datasets as Localized NarrativeSELMA, CountBenchSELMA, DiffusionDBSELMA, WhoopsSELMA, and COCOSELMA, and the resulting combination of 5K auto-generated dataset as DSGSSELMA-5K.

In the **image generation** stage (Sec. 3.2), we use the default denoising steps 50 for all models, and the Classifier-Free Guidance (CFG)  of 7.5. In the **LoRA fine-tuning** stage (Sec. 3.3), we use 128 as the LoRA rank. **During inference**, we uniformly merge the specialized LoRA experts into one multi-skill expert (Sec. 3.4). More details can be found in Appendix.

## 5 Results and Analysis

### Comparison with Different Alignment Methods for Text-to-Image Generation

We compare SELMA with different alignment methods for T2I generation, including training-free methods (SynGen , StructureDiffusion ), RL-based methods (DPOK , DDPO ), and DreamSync , a concurrent method based on automatic data generation. We experiment with three diffusion-based T2I models (_i.e._, SD v1.4, SD v2, and SDXL).

**SELMA outperforms other alignment methods for T2I generation.** As shown in Table 1, SELMA consistently improves faithfulness and human preference metrics for all three backbones. Specifically, on SD v1.4, SELMA improves the baseline by **2.9%** in TIFA, **4.0%** in DSG, **0.2** in PickScore, **0.58** in ImageNet, and **2.5** in HPS score. Furthermore, SELMA achieves significantly higher performance than other baselines, including the RL-based methods (DPOK/DDPO), which require annotated human preference data, and DreamSync, a concurrent/independent work based on a larger auto-generated dataset (_i.e._, 28K text prompts; SELMA uses 5K text training prompts in total), and image filtering (_i.e._, generating 8 images and taking at most one of them for each text prompt; SELMA only generates 1 image for each prompt). Besides, on SD v2 and SDXL, SELMA shows larger improvement in text faithfulness (_i.e._, **7.4%** improvement on DSG for SD v2, and **6.9%** on DSG for SDXL), demonstrating the effectiveness of SELMA.

### Effectiveness of Learning & Merging Skill-Specific Experts

We compare (1) separately learning multiple LoRA experts on different auto-generated datasets followed by merging and (2) training a single LoRA on a mixture of datasets. For this, we experiment with our five auto-generated image-text pairs: Localized NarrativeSELMA, CountBenchSELMA, DiffusionDBSELMA, WhoopsSELMA, and COCOSELMA (see Sec. 4.3 for details).

**Learning & merging skill-specific LoRA experts is more effective than single LoRA on multiple datasets.** Table 2 shows that the LoRA models trained separately on each of the five automatically generated datasets (_No.1_, to _No.5._) can improve the overall metric over the baseline SD v2 - 70.3%, while the degree of improvements is different for each metric (_e.g._, 76.4% for fine-tuning with Localized NarrativeSELMA, and 73.0% for fine-tuning with DiffusionDBSELMA). However, training multiple skills simultaneously with a single LoRA (_No.6._ to _No.7._) tends to degrade performance as more datasets are incorporated. This indicates that the T2I model struggles with LoRA to accommodate distinct skills and writing styles from different datasets. A similar phenomenon has been reported in LLAVA-MoLE , where the knowledge conflict between multiple datasets can degrade the performance. We're the first to show this knowledge conflict across different skills also

    &  &  &  \\   & & DSG\({}^{}\) & TIFA\({}^{}\) & PickScore \(\) & ImageNet \(\) & HPS \(\) \\   & Base model & 67.3 & 76.6 & 20.3 & -0.22 & 23.0 \\  & _(Training-free)_ & & & & & \\   & SyncGen  & 66.2 & 76.8 & 20.4 & -0.24 & 24.5 \\   & StructureDiffusion  & 67.1 & 76.5 & 20.3 & -0.14 & 23.5 \\   & _(RL)_ & & & & & \\   & DPOK  & - & 76.4 & - & -0.26 & - \\   & DDPO  & - & 76.7 & - & -0.08 & - \\   & _(Automatic data generation)_ & & & & & \\   & DreamSync  & - & 77.6 & - & -0.05 & - \\   & **SELMA (Ours)** & **71.3** & **79.5** & **20.5** & **0.36** & **25.5** \\   & Base model & 70.3 & 79.2 & 20.8 & 0.17 & 24.0 \\   & **SELMA (Ours)** & **77.7** & **83.2** & **21.3** & **0.72** & **27.5** \\   & Base model & 73.3 & 83.5 & 21.6 & 0.70 & 26.2 \\   & DreamSync  & - & 85.2 & - & 0.84 & - \\   & **SELMA (Ours)** & **80.2** & **85.6** & **22.0** & **1.09** & **29.9** \\   

Table 1: Comparison of SELMA and different text-to-image alignment methods on text faithfulness and human preference (see Sec. 5.1 for discussion). SELMA achieves the best performance in all five metrics when adapted on different base models (_i.e._, SD v1.4, SD v2, and SDXL). Best scores for each model are in **bold**.

exists in diffusion models. We find that merging multiple skill-specific LoRA experts (_No.8._ and _No.9._) achieves the best performance in both text faithfulness and human preference, demonstrating that merging LoRA experts can help mitigate the knowledge conflict between multiple skills.

### Effectiveness of Auto-Generated Data

In this section, we investigate the effectiveness of our automatically generated data by comparing them with ground truth data. We fine-tune SD v2 model using ground truth data from Localized Narratives, CountBench, DiffusionDB, Whoops, and COCO, sampling 1K image-text pairs from each dataset and fine-tuning specialized LoRA experts accordingly.

**Fine-tuning with auto-generated data can achieve comparable performance to fine-tuning with ground truth data.** As shown in Fig. 3, we observe that fine-tuning with either auto-generated or ground truth data improves from baseline SD v2 performance - 70.3%, when evaluated on the DSG benchmark. Surprisingly, fine-tuning with the generated data via SELMA outperforms the use of ground truth data in most cases, leading to a DSG accuracy improvement of **4.0%** with Localized Narrative style prompts, **1.0%** with CountBench style prompts, **1.9%** with DiffusionDB style prompts, and **0.9%** with COCO style prompts. In short, our approach results in an average improvement of 1.2% brought by fine-tuning only auto-generated data without any need for human-collected ground truth text-image pairs, suggesting that diffusion-based text-to-image models may benefit from the diversity of self-generated images. Furthermore, we investigate whether the improvement is brought by text prompt or image quality. We generate images with SD v2 based on 1K ground truth captions, and fine-tune specialized LoRA experts accordingly. We observe that in most cases, using generated images works better than ground truth images (_e.g._, Localized Narrative), suggesting T2I models can generate images with comparable alignment as ground truth images. Besides, learning from our LLM-generated captions achieves comparable performance with learning from ground truth captions, suggesting the effectiveness of our text prompt collection process. Lastly, we also show in Appendix that fine-tuning with LLaMA3  generated prompts also improve T2I models' faithfulness in generation, demonstrating that our proposed SELMA is compatible to different LLM-based prompt generator.

    & &  \\ 
**No.** & **Model** &  LN\({}^{}\) \\ (_Parse_) \\  &  CB\({}^{}\) \\ (_Gen_) \\  &  DB\({}^{}\) \\ (_Box_) \\  &  Whoops\({}^{}\) \\ (_Contr_) \\  &  COCO\({}^{}\) \\ (_Ols_) \\  &  DEC\({}^{}\) \\ (_Ols_) \\  &  TE\({}^{}\) \\ (_Ols_) \\  & 
 PriScore \\  & ImageReward & HPS \\ 
0. & SDv2 & & & & & & & 70.3 & 79.2 & 20.8 & 0.17 & 24.0 \\ 
1. & ✓ & & & & & & 76.4 & 81.4 & 20.9 & 0.56 & 26.2 \\
2. & & & ✓ & & & & 76.0 & 81.4 & 20.8 & 0.46 & 25.7 \\
3. & & & ✓ & & & & 73.0 & 81.2 & 20.9 & 0.46 & 25.8 \\
4. & + Single LoRA & & & & ✓ & & 73.0 & 80.7 & 20.8 & 0.44 & 25.3 \\
5. & & ✓ & ✓ & & & ✓ & 76.0 & 81.3 & 20.9 & 0.47 & 25.6 \\
6. & & ✓ & ✓ & & ✓ & 75.1 & 81.5 & 20.7 & 0.37 & 24.8 \\
7. & & ✓ & ✓ & ✓ & ✓ & 74.4 & 80.2 & 20.6 & 0.35 & 24.9 \\ 
8. & + LoRA Merging & ✓ & ✓ & ✓ & & & 26.9 & 82.9 & 21.2 & 0.65 & 22.3 \\
9. & & & ✓ & ✓ & ✓ & ✓ & **77.7** & **83.2** & **21.3** & **0.72** & **27.5** \\   

Table 2: Comparison of single LoRA and LoRA Merging (see Sec. 5.2 for discussion). We use SD v2 as our base model and train models with our automatically generated image-text pairs. DATASELMA. auto-generated image-text pairs where prompts are generated with LLMs with three prompt examples from DATA that are not included in DSG test prompts (see Sec. 4.3 for details). _LN: Localized Narratives; CB: CountBench; DDB: DiffusionDB._ Best/2nd best scores are **bolded**/underlined.

Figure 3: DSG accuracy of SD v2 fine-tuned with different image-text pairs.

### Weak-to-Strong Generalization

In previous experiments, we demonstrate the interesting self-improving capabilities of T2I models, where the training images were generated by the same T2I model. Here, we delve into the following research question: _"Can a T2I model benefit from learning with images generated by a weaker model?"_. The problem of _weak-to-strong_ generalization was initially explored in the context of LLMs [8; 60], referred to as superalignment, which involved training GPT-4  using responses generated by a weaker agent, such as GPT-2.

**Weaker T2I models can help stronger T2I models.** As shown in Table 3, fine-tuning SDXL with generated images from SD v2 (_No.4._) remarkably enhances performance over the SDXL baseline (_No.2._) in both text faithfulness and human preference. In addition, this approach achieves competitive performance compared with fine-tuning SDXL with SDXL-generated images (_No.5._), indicating a promising potential for weak-to-strong generalization in diffusion-based T2I generation models. To the best of our knowledge, this is the first work to find promising improvements in the weak-to-strong generalization for text-to-image diffusion models.

### Comparison with Prompt Generation with LLaMA3

In this section, we demonstrate that our proposed paradigm is compatible with different prompt generator LLMs. Specifically, we experiment with LLaMA3 (8B) , a publicly available open-source LLM and compare the results with GPT-3.5 (gpt-3.5-turbo-instruct) based setups described in the main paper. With both LLMs, we generate five sets of skill-specific prompts, and each set contains one thousand skill-specific prompts. As shown in Table 4, we find that fine-tuning SDXL with data generated with LLaMA3 achieves 78.6% on average on DSG, improving the baseline by 5.3%, closing the gap to GPT-3.5 based results. This demonstrates that SELMA is flexible and compatible with different prompt generator LLMs. Besides, we further experiment with fine-tuning SDXL with data generated with both a weaker image generator SDv2 and a weaker prompt generator LLaMA3. Our results show that this model achieves similar performance as the model fine-tuned with images generated with SDXL, demonstrating that weak-to-strong generalization holds with weaker data generators.

### Human Evaluation

In addition to automatic evaluation using text faithfulness benchmarks (DSG and TIFA) and human preference metrics (PickScore, ImageReward, and HPS), we further perform a human evaluation to compare the performance of SDXL and SDXL fine-tuned with SELMA on DSG\({}^{}\) (details in Sec. 4.3). We randomly select 200 prompts from DSG and ask three annotators to determine "Which image aligns with the caption better?" given the text prompt and generated images from both SDXL and SDXL+SELMA. We provide win/tie/lose options to the annotators, and we report the win _vs._ lose percentage in the following. The user interface, instructions, and the detailed statistics are provided in appendix.

  
**Model** & **Prompt Generator** & **Image Generator** & DSG\({}^{}\) \\  SDXL & - & - & 73.3 \\ SDXL & LLaMA3 & SDv2 & 78.0 \\ SDXL & LLaMA3 & SDXL & 78.6 \\ SDXL & GPT3.5 & SDv2 & 81.3 \\ SDXL & GPT3.5 & SDXL & 80.2 \\   

Table 4: DSG and TIFA accuracy of SDXL fine-tuned with prompt data generated with LLaMA3 and GPT-3.5.

    &  &  &  &  \\   & & & DSG\({}^{}\) & TIFA\({}^{}\) & PickScore \(\) & ImageNet\(\) & HPS \(\) \\ 
1. & SD v2 & - & 70.3 & 79.2 & 20.8 & 0.17 & 24.0 \\
2. & SDXL & - & 73.3 & 83.5 & 21.6 & 0.70 & 26.2 \\ 
3. & SD v2 & SD v2 & 77.7 & 83.2 & 21.3 & 0.72 & 27.5 \\
4. & SDXL & SD v2 & **81.3** & 83.8 & 21.5 & 0.78 & 28.8 \\
5. & SDXL & SDXL & 80.2 & **85.6** & **22.0** & **1.09** & **29.9** \\   

Table 3: Comparison of different image generators for creating training images. In addition to using the same model being trained as an image generator, we also experiment with using a smaller model as an image generator (No. 4.). SDXL is bigger/stronger than SD v2. See Sec. 5.4 for discussion.

SDXL+SELMA is preferred than SDXL in terms of text alignment.** Fig. 4 shows that on all five DSG splits, images generated with SDXL+SELMA are preferred **67.9%** of the time, compared to 32.1% for the baseline SDXL. Furthermore, on the five datasets fine-tuned with similar text prompts, SDXL+SELMA achieve a preference rate of **94.6%** on COCO split and **79.2%** on Localized Narratives. This substantial preference over the baseline model demonstrates the effectiveness of SELMA in enhancing T2I models.

### Training Method Ablations

We experiment with various training configurations for SELMA to validate our design choices for fine-tuning. As our current experiments are based on supervised fine-tuning with LoRA Merging, we additionally explore Direct Preference Optimization (DPO) [55; 76] as an alternative to supervised fine-tuning and employing Mixture of Lora Experts (MoE-LoRA)  instead of LoRA Merging. See the Appendix for the implementation details. Table 5 demonstrates that while fine-tuning with DPO and MoE-LoRA significantly improves the T2I models' text faithfulness and human preference (_No.2 & 3. vs. No.0._), simple inference-time LoRA merging achieves the best overall performance. In the end, we adopt LoRA merging and supervised fine-tuning as the default configuration in SELMA for its simplicity and efficiency.

### LoRA Expert Size Ablations

In this section, we demonstrate that simply scaling the LoRA size doesn't help mitigate the knowledge conflict between different skills. We experiment with using a single LoRA with different ranks (128 / 256 / 640) and compare them to our default LoRA merging (rank=128). Table 6 shows that increasing the rank of LoRA from 128 to 256 slightly improves the performance (i.e., 74.9 vs. 74.4), but further scaling the rank of the LoRA to 640 significantly drops the performance (i.e., 71.5 vs. 74.4). The performance drop when using LoRA with higher ranks (i.e., rank=640) is similar to the observation in Figure 3 in . This result indicates the effectiveness of our skill-specific learning and merging of LoRA experts.

  
**Model** & **LoRA Rank** & \(^{}\) \\  Base model (SD\( 2\)) & - & 70.3 \\ Single LoRA & 128 & 74.4 \\ Single LoRA & 256 & 74.9 \\ Single LoRA & 640 & 71.5 \\ LoRA Merging & 128 & 77.7 \\   

Table 6: Performance of scaling the LoRA ranks on DSG.

Figure 4: Human Evaluation on 200 sampled text prompts from DSG, where we show the win _vs_. lose percentages of SDXL and SDXL+SELMA (Ours).

    &  &  &  \\   & & \(^{}\) & \(^{}\) & PickScore \(\) & ImageNet \(\) & HPS \(\) \\ 
0. & SDv2 & 70.3 & 79.2 & 20.8 & 0.17 & 24.0 \\
1. & + LoRA Merging (SELMA) & **77.7** & **83.2** & **21.3** & **0.72** & **27.5** \\
2. & + LoRA Merging + DPO & 75.1 & 81.4 & 20.8 & 0.44 & 26.0 \\
3. & + MoE-LoRA & 77.2 & 83.0 & **21.3** & 0.68 & 27.2 \\   

Table 5: Comparison with different fine-tuning methods on SD v2 with our auto-generated data, in text faithfulness and human preference. See Sec. 5.7 for discussion.

### Qualitative Examples

We show some qualitative examples of images generated with SDXL fine-tuned with SELMA paradigm in Fig. 5. We find that fine-tuning with SELMA improves SDXL's capability in composing infrequently co-occurred attributes (_i.e_., "cube" and "denim" in the top-left image), composing multiple objects mentioned in the text prompts (_i.e_., "brown and white cake", "table", "red and yellow flower", and "fork" in the top-right image), following details in the long paragraph-style text prompts (_i.e_., "blue train with white stripes" and "long yellow line near train area" in the bottom-left image), and generating images that challenge commonsense (_i.e_., "Old man lifts a barbell" in bottom-right image). These qualitative examples demonstrate the effectiveness of SELMA in improving T2I models' text faithfulness and human preference.

## 6 Conclusion

We propose SELMA, an novel paradigm to improve state-of-the-art T2I models' faithfulness in generation and human preference by eliciting the pre-trained knowledge of T2I models. SELMA first collects self-generated images given diverse generated text prompts without the need for additional human annotation. Then, SELMA fine-tunes separate LoRA models on different datasets and merges them during inference to mitigate knowledge conflict between datasets. SELMA demonstrates strong empirical results in improving T2I models' faithfulness and alignments to human preference and suggests potential weak-to-strong generalization for diffusion-based T2I models.