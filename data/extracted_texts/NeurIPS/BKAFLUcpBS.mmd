# Outlier-Robust Gromov-Wasserstein for Graph Data

Lemin Kong

CUHK

lkong@se.cuhk.edu.hk &Jiajin Li

Stanford University

jiajinli@stanford.edu &Jianheng Tang

HKUST

jtangbf@connect.ust.hk &Anthony Man-Cho So

CUHK

manchoso@se.cuhk.edu.hk

###### Abstract

Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-world graph learning tasks, such as subgraph matching and partial shape correspondence.

## 1 Introduction

Gromov-Wasserstein distance (GW)  acts as a main model tool in data science to compare data distributions on unaligned metric spaces. Recently, it has received much attention across a host of applications in data analysis, e.g., shape correspondence , graph alignment and partition , graph embedding and classification , unsupervised word embedding and translation , generative modeling across incomparable spaces .

In practice, the robustness of GW distance suffers heavily from its sensitivity to outliers. Here, outliers mean the samples with large noise, which usually are far away from the clean samples or have different structures from the clean samples. The hard constraints on the marginals in the Gromov-Wasserstein distance require all the mass in the source distribution to be entirely transported to the target distribution, making it highly sensitive to outliers. When the outliers are weighted similarly as other clean samples, even a small fraction of outliers corrupted can largely impact the GW distance value and the optimal coupling, which is unsatisfactory in real-world applications.

To overcome the above issue, some recent works are trying to relax the marginal constraints of GW distance.  introduces a \(L^{1}\) relaxation of mass conservation of the GW distance. However, this reformulation replaces the strict marginal constraint that the transport plan should be a joint distribution with marginals as specific distributions by the constraint that only requires the transport plan to be a joint distribution, which can easily lead to over-relaxation. On another front,  propose a so-called partial GW distance (PGW), which only transports a fraction of mass from source distribution to target distribution. The formulation of PGW is limited to facilitating massdestruction, which restricts its ability to handle situations where outliers exist predominantly on one side. A formulation that allows both mass destruction and creation is proposed in  called unbalanced GW (UGW). The UGW relaxes the marginal constraint via adding the quadratic \(\)-divergence as the penalty function in the objective function and extends GW distance to compare metric spaces equipped with arbitrary positive measures. Additionally,  proved that UGW is robust to outliers and can effectively remove the mass of outliers with high transportation costs. However, UGW is sensitive to the penalty parameter as it balances the reduction of outlier impact and the control of marginal distortion in the transport plan. On the computational side, an alternate Sinkhorn minimization method is proposed to calculate the entropy-regularized UGW. Note that the algorithm does not exactly solve UGW but approximates the lower bound of the entropic regularized UGW instead. From a statistical viewpoint, these works do not establish a direct link between the reformulated GW distance and the GW distance in terms of uncontaminated levels.

In this work, we propose the robust Gromov-Wasserstein (RGW) to estimate the GW distance robustly when dealing with outliers. To achieve this, RGW simultaneously optimizes the transport plan and selects the best marginal distribution from a neighborhood of the given marginal distributions, avoiding contaminated distributions. Perturbed marginal distributions help to re-weight the samples and lower the weight assigned to the outliers. The introduction of relaxed distributions to handle outliers draws inspiration from robust OT techniques . Unlike robust OT, which is convex, RGW is non-convex, posing algorithmic challenges. This idea is also closely related to optimistic modelings of distribution ambiguity in data-driven optimization, e.g., upper confidence bound in the multi-armed bandit problem and reinforcement learning , data-driven distributionally robust decision-making with outliers , etc.

Moreover, inspired by UGW, RGW relaxes the marginal constraint via adding the Kullback-Leibler (KL) divergence between the marginals of the transport plan and the perturbed distributions as the penalty function in the objective function to lessen the impact of the outliers further. Instead of utilizing the quadratic KL divergence as employed in unbalanced GW, we opt for KL divergence due to its computational advantages. It allows for convex subproblems with closed-form solutions, as opposed to the linearization required for non-convex quadratic KL divergence, which could be challenging algorithmically. Furthermore, we leverage the convexity of KL divergence to establish the statistical properties of RGW, leading to an upper bound by the true GW distance with explicit control through marginal relaxation parameters and marginal penalty parameters. This statistical advantage is disrupted by the non-convex nature of quadratic KL divergence. Additionally, KL divergence aligns with our goal of outlier elimination and is less sensitive to outliers compared to quadratic KL divergence, which is more outlier-sensitive due to its quadratic nature. Overall, RGW combines the introduction of perturbed marginal distributions with the relaxation of hard marginal constraints to achieve greater flexibility, allowing control over marginal distortion through marginal penalty parameters and reduction of outlier impact using marginal relaxation parameters.

To realize the modeling benefits of RGW, we further propose an efficient algorithm based on the Bregman proximal alternating linearized minimization (BPALM) method. The updates in each

Figure 1: Visualization of Gromov-Wasserstein couplings between two shapes, with the source in blue and the target in orange. In (a), the GW coupling without outliers is shown. In (b), the coupling with 10% outliers added to the target distribution is depicted. The sensitivity of GW to outliers is evident from the plot. In (c), we present the coupling generated by our proposed RGW formulation, which effectively disregards outliers and closely approximates the true GW distance.

iteration of BPALM can be computed in a highly efficient manner. On the theoretical side, we prove that the BPALM algorithm converges to a critical point of the RGW problem. Empirically, we demonstrate the effectiveness of RGW and the proposed BPALM algorithm through extensive numerical experiments on subgraph alignment and partial shape correspondence tasks. The results reveal that RGW surpasses both the balanced GW-based method and the reformulations of GW, including PGW and UGW.

#### 1.0.1 Our Contributions

We summarize our main contributions as follows:

* We develop a new robust model called RGW to alleviate the impact of outliers on the GW distance. The key insight is to simultaneously optimize the transport plan and perturb marginal distributions in the most efficient way.
* On the statistical side, we demonstrate that the robust Gromov-Wasserstein is bounded above by the true GW distance under the Huber \(\)-contamination model.
* On the computational side, we propose an efficient algorithm for solving RGW using the BPALM method and prove that the algorithm converges to a critical point of the RGW problem.
* Empirical results on subgraph alignment and partial shape correspondence tasks demonstrate the effectiveness of RGW. This is the first successful attempt to apply GW-based methods to partial shape correspondence, a challenging problem pointed out in .

## 2 Problem Formulation

In this section, we review the definition of Gromov-Wasserstein distance and formally formulate the robust Gromov-Wasserstein. Following that, we discuss the statistical properties of the proposed robust Gromov-Wasserstein model under the Huber \(\)-contamination model.

For the rest of the paper, we will use the following notation. Let \((X,d_{X})\) be a complete separable metric space and denote the finite, positive Borel measure on \(X\) by \(_{+}(X)\). Let \((X)_{+}(X)\) denotes the space of Borel probability measures on \(X\). We use \(^{n}\) to denote the simplex in \(^{n}\). We use \(_{n}\) and \(_{n m}\) to denote the \(n\)-dimensional all-one vector and \(n m\) all-one matrix. We use \(^{n}\) to denote the set of \(n n\) symmetric matrice. The indicator function of set \(C\) is denoted as \(_{C}()\).

### Robust Gromov-Wasserstein

The Gromov-Wasserstein (GW) distance aims at matching distributions defined in different metric spaces. It is defined as follows:

**Definition 2.1** (Gromov-Wasserstein).: Suppose that we are given two unregistered complete separable metric spaces \((X,d_{X})\), \((Y,d_{Y})\) accompanied with Borel probability measures \(,\) respectively. The GW distance between \(\) and \(\) is defined as

\[_{(,)}|d_{X}(x,x^{})-d_{Y}(y,y^{})|^{2}d (x,y)d(x^{},y^{}),\]

where \((,)\) is the set of all probability measures on \(X Y\) with \(\) and \(\) as marginals.

As shown in the definition, the sensitivity to outliers of Gromov-Wasserstein distance is due to its hard constraints on marginal distributions. This suggests relaxing the marginal constraints such that the weight assigned to the outliers by the transport plan can be small. To do it, we invoke the Kullback-Leibler divergence, defined as \(d_{}(,)=_{X}(x)((x)/(x))dx\), to soften the constraint on marginal distributions. We also introduce an optimistically distributionally robust mechanism to perturb the marginal distributions and reduce the weight assigned to outliers. Further details on this mechanism will be discussed later.

**Definition 2.2** (Robust Gromov-Wasserstein).: Suppose that we are given two unregistered complete separable metric spaces \((X,d_{X})\), \((Y,d_{Y})\) accompanied with Borel probability measures \(,\) respectively. The Robust GW between \(\) and \(\) is defined as

\[^{}_{_{1},_{2}}(,) &_{(X),\;(Y) }\;F(,)\\ &\;d_{}(,)_{1},d_{}(,)_{2}, \]where \(F(,)=\)

\[_{^{+}(X Y)}|d_{X}(x,x^{})-d_{Y}(y,y^{ }))|^{2}d(x,y)d(x^{},y^{})+_{1}d_{ {KL}}(_{1},)+_{2}d_{}(_{2},),\]

and \((_{1},_{2})\) are two marginals of the joint distribution \(\), defined by \(_{1}(A)=(A Y)\) for any Borel set \(A X\) and \(_{2}(B)=(X B)\) for any Borel set \(B Y\).

The main idea of our formulation is to optimize the transport plan and perturbed distribution variables in the ambiguity set of the observed marginal distributions jointly. This formulation aims to find the perturbed distributions that approximate the clean distributions and compute the transport plan based on the perturbed distributions. However, incorporating the constraints of equal marginals between the transport plan \(\) and the perturbed distributions \(\) and \(\) directly poses challenges in developing an algorithm due to potential non-smoothness issues. Inspired by , we address this challenge by relaxing these marginal constraints and incorporating the KL divergence terms, denoted as \(d_{}(_{1},)\) and \(d_{}(_{2},)\), into the objective function as penalty functions. Different from , we use KL divergence instead of quadratic KL divergence due to its joint convexity, which is more amenable to algorithm development, as quadratic KL divergence is typically non-convex. Besides, transforming the hard marginal constraints into penalty functions can further lessen the impact of outliers on the transport plan.

Our new formulation extends the balanced GW distance and can recover it by choosing \(_{1}=_{2}=0\) and letting \(_{1}\) and \(_{2}\) tend to infinity. When properly chosen, \(_{1}\) and \(_{2}\) can encompass the clean distributions within the ambiguity sets. In this scenario, the relaxed reformulation closely approximates the original GW distance in a certain manner. Building on this concept, we prove that RGW can serve as a robust approximation of the GW distance without outliers, given some mild assumptions on the outliers.

### Robustness Guarantees

Robust Gromov-Wasserstein aims at mitigating the sensitivity of the GW distance to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. Specifically, RGW is designed to address the issue of the GW distance exploding as the distance between the clean samples and the outliers goes to infinity. In general, even a small number of outliers can cause the GW distance to change dramatically when added to the marginal distributions. To formalize this, consider the Huber \(\)-contamination model popularized in robust statistics . In that model, a base measure \(_{c}\) is contaminated by an outlier distribution \(_{a}\) to obtain a contaminated measure \(\),

\[=(1-)_{c}+_{a}. \]

Under this model, data are drawn from \(\) defined in (2).

Under the assumption of the Huber \(\)-contamination model, it can be demonstrated that by selecting suitable values of \(_{1}\) and \(_{2}\), the robust Gromov-Wasserstein distance ensures that outliers are unable to substantially inflate the transportation distance. For robust Gromov-Wasserstein, we have the following bound:

**Theorem 2.3**.: _Let \(\) and \(\) be two distributions corrupted by fractions \(_{1}\) and \(_{2}\) of outliers, respectively. Specifically, \(\) is defined as \((1-_{1})_{c}+_{1}_{a}\), and \(\) is defined as \((1-_{2})_{c}+_{2}_{a}\), where \(_{c}\) and \(_{c}\) represent the clean distributions, and \(_{a}\) and \(_{a}\) represent the outlier distributions. Then,_

\[^{}_{_{1},_{2}}(,) (_{c},_{c})+(0,_{1}- {_{1}}{d_{}(_{a},_{c})})_{1}d_{}( _{c},_{a})\] \[+(0,_{2}-}{d_{}( _{a},_{c})})_{2}d_{}(_{c},_{a}).\]

In Appendix B, we provide a proof that constructs a feasible transport plan and relaxed marginal distributions. By relaxing the strict marginal constraints, we can find a feasible transport plan that closely approximates the transport plan between the clean distributions and obtain relaxed marginal distributions that approximate the clean distributions.

The derived bound indicates that robust Gromov-Wasserstein provides a provably robust estimate under the Huber \(\)-contamination model. If the fraction of outliers is known, the upper bound for the robust GW is determined by the true Gromov-Wasserstein distance, along with additional terms that account for the KL divergence between the clean distribution and the outlier distribution for both \(\) and \(\). The impact of this factor is determined by the extent of relaxation in the marginal distributions \(_{1}\) and \(_{2}\). By carefully choosing \(_{1}=_{1}d_{}(_{a},_{c})\) and \(_{2}=_{2}d_{}(_{a},_{c})\), we can tighten the upper bound on the robust GW value, while still keeping it below the original GW distance (excluding outliers). Importantly, substituting these values of \(_{1}\) and \(_{2}\) yields \(^{}_{_{1},_{2}}(,)(_{c},_ {c})\), indicating that the robust GW between the contaminated distribution \(\) and \(\) is upper bounded by the original GW distance between the clean distribution \(_{c}\) and \(_{c}\).

**Remark 2.4**.: The following inequality for UGW under the Huber \(\)-contamination model can be derived using the same techniques as in Theorem 2.3:

\[(,)(_{c},_{c})+_{1}d_{} ^{}(_{c},)+_{2}d_{}^{}(_{c},).\]

We observe that the terms \(d_{}^{}(_{c},)\) and \(d_{}^{}(_{c},)\) cannot be canceled out unless \(_{1}\) and \(_{2}\) are set to zero, resulting in over-relaxation. However, RGW allows us to control the error terms \(d_{}(_{c},_{a})\) and \(d_{}(_{c},_{a})\) through the marginal relaxation parameters \(_{1}\) and \(_{2}\).

## 3 Proposed Algorithm

### Problem Setup

To start with our algorithmic developments, we consider the discrete case for simplicity and practicality, where \(\) and \(\) are two empirical distributions, i.e., \(=_{i=1}^{n}_{i}_{x_{i}}\) and \(=_{j=1}^{m}_{j}_{y_{j}}\). Denote \(D^{n}\), \(D_{ik}=d_{X}(x_{i},x_{k})\) and \(^{m}\) and \(_{jl}=d_{Y}(y_{j},y_{l})\). We construct a 4-way tensor as follows:

\[(D,)(|d_{X}(x_{i},x_{k})-d_{Y} (y_{j},y_{l})|^{2})_{i,j,k,l}.\]

We define the tensor-matrix multiplication as

\[( T)_{ij}(_{k,}_{i,j,k,}T_{k,})_{i,j}.\]

Then, the robust GW admits the following reformulation:

\[_{,,}& (D,),+_{1}d_{}( _{1},)+_{2}d_{}(_{2},)\\ & d_{}(,)_{1},d_{ }(,)_{2},\\ &^{n},^{m}, 0. \]

Here, \(_{1}=_{m}\) and \(_{2}=^{T}_{n}\).

### Bregman Proximal Alternating Linearized Minimization (BPALM) Method

As Problem (3) is non-convex and involves three variables, we employ BPALM [5; 2] to solve it. By choosing the KL divergence as Bregman distance, the updates of this algorithm are given by:

\[^{k+1}=*{arg\,min}_{ 0}\{(D, )^{k},+_{1}d_{}(_{1}, ^{k})+_{2}d_{}(_{2},^{k})+}d_{}(,^{k})\}, \]

\[^{k+1}=*{arg\,min}_{^{n }\\ d_{}(,)_{1}}\{d_{}( _{1}^{k+1},)+}d_{}(^{k},) \}, \]

\[^{k+1}=*{arg\,min}_{^{m}\\ d_{}(,)_{2}}\{d_{}( _{2}^{k+1},)+}d_{}(^{k},)\}. \]

Here, \(t_{k}\), \(c_{k}\), and \(r_{k}\) are stepsizes in BPALM.

In our algorithm updates, we employ distinct proximal operators for \(\) and \(\) (and \(\)). The use of \(d_{}(,^{k})\) in \(\)-subproblem (4) allows for the application of the Sinkhorn algorithm, while the introduction of \(d_{}(^{k},)\) in \(\)-subproblem (5) facilitates a closed-form solution, which we will detail in the following part.

To solve the \(\)-subproblem, we can utilize the Sinkhorn algorithm for the entropic regularized unbalanced optimal transport problem. This algorithm, which has been previously introduced in [13; 32], is well-suited for our needs. As for the \(\)-subproblem, we consider the case where \(_{1}\) is strictly larger than 0. Otherwise, when \(_{1}=0\), \(\) should simply equal \(\), making the subproblem unnecessary. To solve the \(\)-subproblem, we attempt to find the optimal dual multiplier \(w^{*}\). Specifically, consider the problem:

\[_{^{n}}d_{}(_{1}^{k+1},)+ }d_{}(^{k},)+w(d_{}(,)-_{1}). \]

Let \((w)\) represent the optimal solution to (7), and we define the function \(p:_{+}\) by \(p(w)=d_{}(,(w))-_{1}\). We prove the convexity, differentiability, and monotonicity of \(p\), which are crucial for developing an efficient algorithm for (5) later.

**Proposition 3.1**.: _Problem (7) has a closed-form solution_

\[(w)=_{m}+}^{k}+w}{ _{ij}_{ij}^{k+1}+}+w}.\]

_If \(w\) satisfies (i) \(w=0\) and \(p(w) 0\), or (ii) \(w>0\), \(p(w)=0\), then \((w)\) is the optimal solution to the \(\)-subproblem (5). Moreover, \(p()\) is convex, twice differentiable, and monotonically non-increasing on \(_{+}\)._

Given Proposition 3.1, we begin by verifying \(p(0) 0\). If this condition is not met, and given that \(p(0)>0\) while \(_{w+}p(w)=-_{1}<0\), it implies that \(p\) possesses at least one root within \(_{+}\). The following proposition provides the framework to seek the root of \(p\) by employing Newton's method, with the initialization set at 0. Hence, the \(\)-subproblem can be cast to search a root of \(p\) in one dimension, in which case it can be solved efficiently.

**Proposition 3.2**.: _Let \(p():I\) be a convex, twice differentiable, and monotonically non-increasing on the interval \(I\). Assume that there exists an \(\), \( I\) such that \(p()>0\) and \(p()<0\). Then \(p\) has a unique root on \(I\), and the sequence obtained from Newton's method with initial point \(x_{0}=\) will converge to the root of \(p\)._

Since the \(\)-subproblem shares the same structure as the \(\)-subproblem, we can apply this method to search for the optimal solution to the \(\)-subproblem.

### Convergence Analysis

To illustrate the convergence result of BPALM, we consider the compact form for simplicity:

\[_{,,}F(,,)=f()+q()+g_{1}(,)+g_{ 2}(,)+h_{1}()+h_{2}(),\]

where \(f()=(D,),\), \(q()=_{\{ 0\}}()\), \(g_{1}(,)=_{1}d_{}(_{m},)\), \(g_{2}(,)=_{2}d_{}(^{T}_{n},)\), \(h_{1}()=_{\{^{n},d_{}(,) _{1}\}}()\), and \(h_{2}()=_{\{^{m},d_{}(,) _{2}\}}()\).

The following theorem states that any limit point of the sequence generated by BPALM belongs to the critical point set of problem (3).

**Theorem 3.3** (Subsequence Convergence).: _Suppose that in Problem (1), the step size \(t_{k}\) in (4) satisfies \(0< t_{k}</L_{f}\) for \(k 0\) where \(\), \(\) are given constants and \(L_{f}\) is the gradient Lipschitz constant of \(f\). The step size \(c_{k}\) in (5) and \(r_{k}\) in (6) satisfy \(0< c_{k},r_{k}<\) for \(k 0\) where \(\), \(\) are given constants. Any limit point of the sequence of solutions \(\{^{k},^{k},^{k}\}_{k 0}\) belongs to the critical point set \(\), where \(\) is defined by_

\[\{ & 0 f()+ q()+_{}g_{1}( ,)+_{}g_{2}(,),\\ & 0_{}g_{1}(,)+ h_{1}(),\\ & 0_{}g_{2}(,)+ h_{2}(),\\ &(,,)^{n m}^{n} ^{m}\}.\]

For the sake of brevity, we omit the proof. We refer the reader to Appendix C for further details.

Experiment Results

In this section, we present comprehensive experimental results to validate the effectiveness of our proposed RGW model and BPALM algorithm in various graph learning tasks, specifically subgraph alignment and partial shape correspondence. Traditionally, balanced GW has been applied successfully in scenarios where the source and target graphs have similar sizes. However, in our approach, we treat the missing part of the target graph as outliers and leverage RGW for improved performance. All simulations are conducted in Python 3.9 on a high-performance computing server running Ubuntu 20.10, equipped with an Intel(R) Xeon(R) Silver 4214R CPU. Our code is available at [https://github.com/lmkong020/outlier-robust-GW](https://github.com/lmkong020/outlier-robust-GW).

### Partial Shape Correspondence

In this subsection, we first investigate a toy matching problem in a 2D setting to support and validate our theoretical insights and results presented in Section 2. Figure 2 (a) illustrates an example where we aim to map a two-dimensional shape without symmetries to a rotated version of the same shape while accounting for outliers in the source domain. Here, we sample 300 points from the source shape and 400 points from the target shape. Additionally, we introduce 50 outliers by randomly adding points from a discrete uniform distribution on \([-3,-2.5][0,0.5]\) to the source domain. The distance matrices, \(D\) and \(\) are computed using pairwise Euclidean distances.

Figures 2 provide visualizations of the coupling matrices and objective values for all the models, highlighting the matching results. In Figure 2(c), it is evident that even a small number of outliers has a significant impact on the coupling and leads to an increased estimated GW distance. While unbalanced GW and partial GW attempt to handle outliers to some extent, they fall short in achieving accurate mappings. On the other hand, our robust GW formulation, RGW, effectively disregards outliers and achieves satisfactory performance. Additionally, the objective value of RGW closely approximates the true GW distance without outliers, as indicated by Theorem 2.3, approaching zero.

### Subgraph Alignment

The subgraph alignment problem, which involves determining the isomorphism between a query graph and a subgraph of a larger target graph, has been extensively studied [16; 20]. While the

Figure 2: (a): 2D shape geometry of the source and target; (b)-(f): visualization of ground truth and the matching results of balanced GW, unbalanced GW, partial GW, and robust GW.

restricted quadratic assignment problem is commonly used for graphs of similar sizes, the GW distance provides an optimal probabilistic correspondence that preserves the isometric property. In the subgraph alignment context, the nodes in the target graph, excluding those in the source graph, can be considered outliers, making the RGW model applicable to this task. In our comparison, we evaluate RGW against various methods, including unbalanced GW, partial GW, semi-relaxed GW (srGW) , RGWD , and methods for computing balanced GW such as FW , BPG , SpecGW , eBPG , and BAPG .

Database StatisticsWe evaluate the methods on synthetic and real databases. In the synthetic database, we generate target graphs \(_{t}\) using Barabasi-Albert models with scales ranging from 100 to 500 nodes. The source graphs \(_{s}\) are obtained by sampling connected subgraphs of \(_{t}\) with a specified percentage of nodes. This process results in five synthetic graph pairs for each setup, totaling 200 graph pairs. The _Proteins_ and _Enzymes_ biological graph databases from  are also used, following the same subgraph generation routine. For the _Proteins_ database, we evaluate the accuracy of matching the overlap between two subgraphs with 90% overlap and between a subgraph and the entire graph, presented in the "Proteins-2" and "Proteins-1" columns, respectively. We compute the matching accuracy by comparing the predicted correspondence set \(_{}\) to the ground truth correspondence set \(_{}\), with accuracy calculated as \(=|_{}_{}|/|_{}| 100\%\). In addition, we also evaluate our methods on the _Douban Online-Offline_ social network dataset, which consists of online and offline graphs, representing user interactions and presence at social gatherings, respectively. The online graph includes all users from the offline graph, with 1,118 users serving as ground truth alignments. Node locations are used as features in both graphs. In line with previous works [18; 37], we gauge performance using the Hit@k metric, which calculates the percentage of nodes in set \(_{t}\) where the ground truth alignment includes \(_{s}\) among the top-k candidates.

    &  &  &  &  &  \\  & Acc & Time & Acc & Time & Acc & Time & Acc & Time & Hit@1 & Hit@10 \\  FW & 2.27 & 18.39 & 16.00 & 27.05 & 26.15 & 60.34 & 15.47 & 9.57 & 17.97 & 51.07 \\ SpecGW & 1.78 & 3.72 & 12.06 & 11.07 & 42.64 & 12.85 & 10.69 & 3.96 & 2.68 & 9.83 \\ eBPG & 3.71 & 85.31 & 19.88 & 1975.12 & 32.15 & 9645.05 & 21.58 & 12191.81 & 0.08 & 0.53 \\ BPG & 15.41 & 24.67 & 29.30 & 118.26 & 61.26 & 80.39 & 32.49 & 70.42 & 72.72 & 92.39 \\ BAPG & 48.89 & 27.95 & 30.98 & 122.13 & 66.84 & 16.49 & 35.64 & 16.41 & 72.18 & 92.58 \\  srGW & 1.60 & 152.01 & 21.30 & 63.00 & 12.08 & 172.48 & 24.13 & 19.68 & 4.03 & 11.54 \\ RGWD & 16.68 & 955.40 & 27.94 & 4396.41 & 59.69 & 3586.56 & 30.35 & 2629.00 & 4.11 & 16.46 \\ UGW & 89.88 & 176.24 & 25.72 & 4026.93 & 67.30 & 1853.82 & 43.73 & 1046.29 & 0.09 & 0.72 \\ PGW & 2.28 & 479.99 & 13.94 & 544.79 & 20.08 & 348.44 & 11.43 & 212.09 & 18.24 & 37.03 \\  RGW & **94.44** & 361.44 & **53.30** & 834.76 & **69.38** & 466.91 & **63.43** & 293.84 & **75.58** & **96.24** \\   

Table 1: Comparison of the average matching accuracy (%) and wall-clock time (seconds) on subgraph alignment of 50% subgraph on datasets Synthetic, Proteins and Enzymes and Hit@1 and Hit@10 of dataset Douban.

Figure 3: (a): 3D shape geometry of the source and target; (b)-(e): visualization of ground truth, initial point obtained from the partial functional map, and the matching results of PGW and RGW.

[MISSING_PAGE_FAIL:9]

subgraph to the entire graph. In Figure 5, the left side demonstrates that while keeping the ratio and \(\) fixed but varying \(\), accuracy diminishes when \(\) becomes either excessively large or too small. However, within the range of 0.05 to 1, accuracy remains resilient, highlighting the mitigating effect of marginal relaxation on outlier influence. Notably, accuracy significantly improves when \(\) falls within the range of 0.05 to 1 compared to the scenario with \(\) = 0, thus affirming the significance of marginal relaxation in our model. A similar trend is observed for \(\) on the right side of Figure 5, where maintaining a fixed ratio and \(\) while adjusting \(\) results in stable accuracy within the range of 0.1 to 1 but declining beyond this range. Consequently, in RGW computations, we can initially set \(\) to 0.2 and \(\) to 0.1 by default and subsequently adjust \(\) based on the outlier ratio: increasing it for a large ratio and decreasing it for a small ratio.

### Tightness of the Bound in Theorem 2.3

Our primary focus is on scenarios where the GW distance between clean samples is nearly zero or zero due to noise, such as in partial shape correspondence and subgraph alignment tasks. In such cases, it is possible to find an isometric mapping from the query subgraph to a portion of the entire graph. By appropriately selecting the value of \(\), as discussed in Section 2.2, the upper bound in RGW becomes the GW distance between clean samples, which is zero. As RGW is always nonnegative, this upper bound is tight in this context.

To empirically validate this, we conducted experiments on the toy example in Section 4.1, and Figure 6 (a) illustrates the function values of PGW, UGW, and RGW with varying outlier ratios. The results confirm that the value of RGW can remain close to zero as the ratio of outliers increases. Additionally, Figure 6 (b) shows the function value of RGW and its upper bound as \(\) varies. Both the RGW value and its upper bound decrease, converging to zero as \(\) increases. This observation provides empirical support for Theorem 2.3. Regarding UGW and its upper bound with changing \(\), we observed that both the UGW value and its upper bound increase as \(\) becomes larger, as shown in Figure 6 (c). Unlike RGW, UGW's \(\) must strike a balance between reducing outlier impact and preserving marginal distortion in the transport plan. This demands a careful balance and caution against setting \(\) excessively close to zero, which could lead to over-relaxation and potentially deteriorate the performance.

## 5 Conclusion

In this paper, we introduce RGW, a robust reformulation of Gromov-Wasserstein that incorporates distributionally optimistic modeling. Our theoretical analysis demonstrates its robustness to outliers, establishing RGW as a reliable estimator. We propose a Bregman proximal alternating linearized minimization method to efficiently solve RGW. Extensive numerical experiments validate our theoretical results and demonstrate the effectiveness of the proposed algorithm. Regarding the robust estimation of Gromov-Wasserstein, a natural question is whether we can recover the transport plan from the RGW model. On the computational side, our algorithm suffers from the heavy computation cost due to the use of the unbalanced OT as our subroutine, which limits its application in large-scale real-world settings. To address this issue, a natural future direction is to develop single-loop algorithms to leverage the model benefits of robust GW for real applications.

Figure 6: (a) Function values of PGW, UGW, and RGW for varying \(\); (b) Function value of RGW and its upper bound for different \(\); (c) Function value of UGW and corresponding upper bound for different \(\).