# Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework

Parsa Moradi

University of Minnesota

moradi@umn.edu

&Behrooz Tahmasebi

MIT CSAIL

bzt@mit.edu

&Mohammad Ali Maddah-Ali

University of Minnesota

maddah@umn.edu

###### Abstract

Coded computing has emerged as a promising framework for tackling significant challenges in large-scale distributed computing, including the presence of slow, faulty, or compromised servers. In this approach, each worker node processes a combination of the data, rather than the raw data itself. The final result then is decoded from the collective outputs of the worker nodes. However, there is a significant gap between current coded computing approaches and the broader landscape of general distributed computing, particularly when it comes to machine learning workloads. To bridge this gap, we propose a novel foundation for coded computing, integrating the principles of learning theory, and developing a framework that seamlessly adapts with machine learning applications. In this framework, the objective is to find the encoder and decoder functions that minimize the loss function, defined as the mean squared error between the estimated and true values. Facilitating the search for the optimum decoding and functions, we show that the loss function can be upper-bounded by the summation of two terms: the generalization error of the decoding function and the training error of the encoding function. Focusing on the second-order Sobolev space, we then derive the optimal encoder and decoder. We show that in the proposed solution, the mean squared error of the estimation decays with the rate of \((S^{3}N^{-3})\) and \((S^{}{{5}}}N^{-}{{5}}})\) in noiseless and noisy computation settings, respectively, where \(N\) is the number of worker nodes with at most \(S\) slow servers (stragglers). Finally, we evaluate the proposed scheme on inference tasks for various machine learning models and demonstrate that the proposed framework outperforms the state-of-the-art in terms of accuracy and rate of convergence.

## 1 Introduction

The theory of _coded computing_ has been developed to improve the reliability and security of large-scale machine learning platforms, effectively tackling two major challenges: (1) the detrimental impact of _slow workers (stragglers)_ on overall computation efficiency, and (2) the threat of _faulty or malicious workers_ that can compromise data accuracy and integrity. These challenges have been well-documented in the literature, including the seminal work  from Google. For instance,  reported that in a sample set of \(3000\) matrix multiplication jobs on AWS Lambda, while the median job time was \(40\) seconds, approximately \(5\%\) of worker nodes took \(100\) seconds to respond, and two nodes took as long as \(375\) seconds. Furthermore, coded computing has also been instrumental in addressing _privacy concerns_, a crucial aspect of distributed computing systems .

The concept of coded computing has been motivated by the success of coding in communication over unreliable channels, where instead of transmitting raw data, the transmitter sends a (linear) combination of the data, known as coded data. This redundancy in the coded data enables thereceiver to recover the raw data even in the presence of errors or missing values. Similarly, coded computing includes three layers  (see Figure 1(a)):

1. _The Encoding Layer_ in which a master node sends a (linear) combination of data, as coded data, to each worker node.
2. _The Computing Layer_, in which the worker nodes apply a predefined computation to their assigned coded data and send the results back to the master node.
3. _The Decoding Layer_, in which the master node recovers the final results from the computation results over coded data. In this layer, the decoder leverages the coded redundancy in the computation to recover the missing results of the stragglers and detect and correct the adversarial outputs.

The existing coded computing has largely built upon algebraic coding theory, drawing inspiration from the renowned Reed-Solomon code construction in communication , with proven straggler and Byzantine resiliency . However, the coding in communication is designed for the exact recovery of the messages, built on a foundation that is inconsistent with the computational requirements of machine learning. Developing a code that preserves its specific construction while composing with computation is extremely challenging, leading to significant restrictions. Firstly, current methods are mainly restricted to specific computation functions, such as polynomials and matrix multiplication . Secondly, rooted in algebraic error correction codes, existing approaches are tailored for finite field computations, leading to numerical instability when dealing with real-valued data . Furthermore, these methods are unsuitable for approximate, fixed-point, or floating-point computing, where exact computation is neither possible nor necessary, such as in machine learning inference or training tasks. Finally, these schemes typically have a recovery threshold, which is the minimum number of samples required to recover results from coded outputs of worker nodes . If the number of workers falls below this threshold, the recovery process fails entirely.

Several works have attempted to mitigate the aforementioned issues and transform the coded computing scheme into a more robust and adaptable one, applicable to a wide range of computation functions. These efforts include approximating non-polynomial functions with polynomial ones , refining the coding mechanism to enhance stability , and leveraging approximation computing techniques to reduce the recovery threshold and increase recovery flexibility . However, these attempts fail to bridge the existing gap between coded computing and general distributed computing systems. The root cause of these issues lies in the fact that they are grounded in coding theory, based on a foundation that is not compatible with the requirements of large-scale machine learning. Therefore, this paper aims to address the following objective:

**Objective:** The main objective of this paper is to develop a new foundation for coded computing, not solely based on _coding theory_, but also grounded in _learning theory_, that seamlessly integrates with machine learning applications, offering a more natural and effective solution for _general computing_.

In this paper, we establish a learning-theoretic foundation for coded computing, applicable to general computations. We adopt an end-to-end system perspective, that integrates an end-to-end loss function, to find the optimum encoding and decoding functions, focusing on straggler resiliency. We show that the loss function is upper-bounded by the sum of two terms: one characterizing the _generalization error_ of the decoder function and the other capturing the _training error_ of the encoder function. Regularizing the decoder layer, we derive the optimal decoder in the Reproducing Kernel Hilbert space (RKHS) of second-order Sobolev functions. This provides an explicit solution for the optimum decoder function and allows us to characterize the resulting loss of the decoding layer. The decoder loss appears as a regularizing term in optimizing the encoding function and represents the norm in another RKHS. Thus, the optimum solution for the encoding function can be derived, too. We address two noise-free and noisy computation settings, for which we derive the _optimal encoder and decoder_ and corresponding convergence rate. We prove that the proposed framework exhibits a faster convergence rate compared to the state-of-the-art and the numerical evaluations support the theoretical derivations (see Figure 1(b)).

**Contributions:** The main contributions of this paper are:

* We develop a new foundation for coded computing by integrating it with learning theory, rather than relying solely on coding theory. We define the loss function as the mean square error of the computation estimation, averaged over all possible sets of at most \(S\) stragglers (Section 3.1). To be able to find the best encoding and decoding functions, we bound the loss function with the summation of two terms, one characterizing the generalization error of the decoder function and the other capturing the training error of the encoder function (Section 3).
* Assuming that the encoder and decoder functions reside in the Hilbert space of second-order Sobolev functions, we use the theory of RKHSs to find the optimum encoding and decoding functions and characterize the convergence rate for the expected loss in both noise-free and noisy computation regimes (Section 4).
* We have extensively evaluated the proposed scheme across different data points and computing functions including state-of-the-art deep neural networks and demonstrated that our proposed framework considerably outperforms the state-of-the-art in terms of recovery accuracy (Section 5).

## 2 Preliminaries and Problem Definition

### Notations

Throughout this paper, uppercase and lowercase bold letters denote matrices and vectors, respectively. Coded vectors and matrices are indicated by a \(\) sign, as in \(},}\). The set \(\{1,2,,n\}\) is denoted as \([n]\) and symbol \(|S|\) denotes the cardinality of the set \(S\). Finally, we represent first, second and \(k\)-th order derivative of function \(f\) as \(f^{},f^{}\), and \(f^{(k)}\), respectively.

### Problem Setting

Consider a master node and a set of \(N\) workers. The master node is tasked with computing \(\{(x_{k})\}_{k=1}^{K}\) using a cluster of \(N\) worker nodes, given a set of \(K\) data points \(\{_{k}\}_{k=1}^{K},_{k}^{d}\). Here, \(:^{d}^{m}\) represents an arbitrary function, which could be a simple one-dimensional function or a complex deep neural network, and \(K,d,m\) are integers. A naive approach would be to assign the computation of \((_{k})\) to one worker node for \(k[K]\). However, some worker nodes may act as stragglers, failing to complete their tasks within the required deadline. To mitigate this issue, the master node employs coding and sends \(N\) coded data points to each worker node using an encoder function. Each coded data point is a combination of raw data points. Subsequently, each

Figure 1: Coded Computing: Each worker node processes a combination of data (coded data). The decoder recovers the final results, even in the presence of missing outputs from some worker nodes. Figure 1: The log-log plot of the expected error versus the number of workers (\(N\)) for the proposed framework (LeTCC) and the state-of-the-art BACC . LeTCC framework not only achieves a lower estimation error but also has a faster convergence rate.

worker applies the function \(()\) to the received coded data and sends the result, coded results, back to the master node. The master node's goal is to approximately recover \(}(_{k})(_{k})\) using a decoder function, even if some worker nodes appear to be stragglers. The redundancy in the coded data and corresponding coded results enables the master node to recover the desirable results, \(\{(_{k})\}_{k=1}^{K}\).

## 3 Proposed Framework: LeTCC

Here, we propose a novel straggler-resistant Learning-Theoretic Coded Computing (LeTCC) framework for general distributed computing. As depicted in Figure 2, our framework comprises two encoding and decoding layers, with a computing layer sandwiched between them. The framework operates according to the following steps:

1. **Encoding Layer:** The master node fits an encoder regression function \(_{}:^{d}\) at points \(\{(_{k},_{k})\}_{k=1}^{K}\) for fixed, distinct, and ordered values \(_{1}<_{2}<<_{K}\). Then, it computes the encoder function \(_{}()\) on another set of fixed, distinct, and ordered values \(\{_{n}\}_{n=1}^{N}\) where \(_{1}<_{2}<<_{N}\), with \(k[K]\) and \(n[N]\). Subsequently, the master node sends the coded data points \(}_{n}=_{}(_{n})^{d}\) to worker \(n\) for \(n[N]\). Note that each coded data point \(}_{n}\) is a combination of all initial points \(\{_{k}\}_{k=1}^{K}\).
2. **Computing Layer:** Each worker node \(n[N]\) computes \((}_{n})=(_{}(_{ n}))\) on its assigned input and sends the result back to the master node.
3. **Decoding Layer:** The master node receives the results \((}_{v})_{v}\) from the non-straggler worker nodes in the set \(\). Next, it fits a decoder regression function \(_{}:^{m}\) at points \((_{v},(}_{v}))_{v}=(_{v}, (_{}(_{v})))_{v}\). Finally, using the function \(_{}()\), the master node computes \(}(_{k}):=_{}(_{k})\) as an approximation of \((_{k})\) for \(k[K]\). Recall that \(_{}(_{k})(_{}( _{k}))(_{k})\).

As mentioned above, the master node selects and fixes the regression points, \(\{_{k}\}_{k=1}^{K}\) and \(\{_{n}\}_{n=1}^{N}\), which remain constant throughout the entire process. The encoder and decoder functions are the only components subject to optimization.

Note that the computational efficiency of the encoding and decoding layers is crucial. This includes the fitting process of the encoder and decoder regression functions, as well as the computation of these regression functions at points \(\{_{v}\}_{v}\) and \(\{_{k}\}_{k=1}^{K}\). If the master node's computation time is not substantially decreased compared to computing \(\{(_{k})\}_{k=1}^{K}\) by itself, then adopting this framework would not provide any benefits for the master node.

Figure 2: LeTCC framework.

### Objective

We view the whole scheme as a unified predictive framework that provides an approximate estimation of the values \(\{(_{k})\}_{k=1}^{K}\). We denote the estimator function of the LeTCC scheme as \(}_{,}[_{ },_{},]()\), where \(:=[_{1},,_{K}]^{T}\), \(:=[_{1},,_{N}]^{T}\), and \(:=\{i_{1},,i_{||}\}\) represents the set of non-straggler worker nodes.

Let us define a random variable \(F_{S,N}\) distributed over the set of all subsets of \(N\) workers with maximum \(S\) stragglers, \(\{:[N],|| N-S\}\). Also, suppose each worker node \(n[N]\) computes the function \(_{n}(x)=(x)+_{n}\), where \(_{n}\), \(n[N]\) are independent zero-mean noise vectors with covariance \(^{2}\).

This enables us to define the following loss function, which evaluates the framework's performance:

\[(}):=}_{,  F_{S,N}}[_{k=1}^{K}\|} (_{k})-(_{k})\|_{2}^{2}]=}_{, F_{S,N}}[ _{k=1}^{K}\|_{}(_{k})-(_{k})\|_{2}^{2}],\] (1)

where \(}():=}_{, }[_{},_{},]()\) to simplify the notation, \(\|\|_{2}\) represents the \(_{2}\)-norm, and \(=[_{1},,_ {N}]^{T}\). Our objective is to find \(_{}(.)\) and \(_{}(.)\) that minimize the objective function (1), which is very challenging, given that \(}(.)\) is a composition of \(_{}(.)\) and \(_{}(.)\) and the computation in the middle. Here, we take an important step to decompose these two, to gain a deeper understanding of interactions. Adding and subtracting \((_{}(_{k}))\) and utilizing inequality of arithmetic and geometric means (AM-GM), one can obtain an upper bound for (1):

\[(}) =}_{, F_{S, N}}[_{k=1}^{K}\|(_{}(_{k})- (_{}(_{k})))+((_{ {enc}}(_{k}))-(_{k}))\|_{2}^{2}]\] \[}_{,  F_{S,N}}[_{k=1}^{K}\|_{ }(_{k})-(_{} (_{k}))\|_{2}^{2}]}_{_{}(})}+_{k=1}^{K}\|( _{}(_{k}))-(_{k})\|_{2}^{2 }}_{_{}(})}.\] (2)

The right-hand side of (2) comprises two terms, which uncover an interesting interplay between the encoder and decoder regression functions. Let us elaborate on what each term corresponds to.

* \(_{}(})\) **- The expected generalization error of the decoder regression:** Recall that the master node fits a decoder regression function, \(_{}()\), at a set of points denoted as \(\{(_{v},(_{}(_{v})))\}_{v }\). \(_{}\) represents the \(_{2}\)-norm of the decoder regression function's error on a distinct set of points \(\{_{k}\}_{k=1}^{K}\), which are _different_ from its training data \(\{_{v}\}_{v}\). Consequently, this term provides an unbiased estimate of the decoder's generalization error. Given that the decoder regression function develops to estimate \((_{}())\), the generalization error of the decoder regression is inherently tied to the properties of \((_{}())\). This, in turn, is influenced by characteristics of both the \(()\) and \(_{}()\) functions, making the \(_{}(})\) a complex interplay of these two functions.
* \(_{}(})\) **- A proxy to the training error of the encoder regression:** Remember that the encoder regression is fitted at points \(\{(_{k},_{k})\}_{k=1}^{K}\). Consequently, the training error is calculated as \(_{k=1}^{K}\|_{}(_{k})- _{k}\|_{2}^{2}\). Therefore, \(_{}\) represents the encoder training error magnified by the effect of computing function \(()\). Specifically, if \(()\) is \(q\)-Lipschitz, then \(_{}(})\) can be upper bounded by: \[_{k=1}^{K}\|(_{}(_{k}))- (_{k})\|_{2}^{2}}{K}_{k=1}^ {K}\|_{}(_{k})-_{k}\|_{2}^{2}.\] (3)

## 4 Main Results

In this section, we examine the proposed framework from a theoretical standpoint. We provide a comprehensive explanation of the design process for the decoder and encoder functions and subsequently analyze the convergence rate. For simplicity, we present the results for a one-dimensionalfunction \(f:\). These results are generalizable to the case where \(f:^{m}\), as discussed in Appendix F.

Suppose the regression points, \(\{_{k}\}_{k=1}^{K},\{_{n}\}_{n=1}^{N}\), are confined to the interval \(:=(-1,1)\) and \(u_{},u_{}}^{2}(;)\), where \(}^{2}(;)\) is the reproducing kernel Hilbert space (RKHS) of second-order Sobolev functions on the interval \(\) induced with the norm \(\|f\|_{}^{2}(;)}^{2}:=_{ }(f^{}(t))^{2}\,dt+f(-1)^{2}+f^{}(-1)^{2}\) which is an equivalent norm on Sobolev space introduced by  (see (28) in Appendix A.1). The definition and properties of Sobolev spaces, along with their reproducing kernels and norms, are reviewed in Appendix A.1.

**Decoder Design:** Since \(_{}(})\) in the decomposition (2) characterizes the generalization error of the decoder function, we propose a regularized objective function for the decoder:

\[u_{}^{}=*{argmin}_{u}^{2} (;)}|}_{v} (u(_{v})-f(u_{}(_{v}) ))^{2}+_{}_{}(u^{}(t) )^{2}\,dt.\] (4)

The first term in (4) corresponds to the mean squared error, while the second term characterizes the smoothness of the decoder function on the interval \(\). Equation (4) represents a Kernel Ridge Regression problem (KRR). It can be shown that the solution of (4) has the following form :

\[d_{0}+d_{1}t+_{v=1}^{||}c_{v}_{0}(t,_{i_{v}}),\] (5)

where \(d_{0},d_{1}\), \(_{0}(,)\) is the kernel function of \(_{0}^{2}(;)\) (see Definition 2 and (44) in Appendix A.1), and \(=[c_{1},,c_{||}]^{T}^{||}\). Substituting (5) into the main objective (4), the coefficient vectors \(\) and \(:=[d_{0},d_{1}]^{T}\) can be efficiently computed by optimizing a quadratic equation . This solution is known as the _second-order smoothing spline_ function. The theoretical properties of smoothing splines are reviewed in Appendix A.2.

Let us define the following variables, which represent the maximum and minimum distances between consecutive data points in the decoder layer, \(\{_{n}\}_{n=1}^{N}\):

\[_{}:=_{n\{0\}[N]}\{_{n+1}-_{n}\}, _{}:=_{n[N-1]}\{_{n+1}-_{n}\},\] (6)

with \(_{0}:=-1\) and \(_{N+1}:=1\). The following theorems provide crucial insights for designing the encoder function as well as deriving the convergence rates.

**Theorem 1** (Upper bound for noiseless computation, \(_{0}=0\)).: _Consider the LeTCC framework with \(N\) worker nodes and at most \(S\) stragglers with \(_{d} N^{-4}\). Assume \(\{_{k}\}_{k=1}^{K}\) are arbitrary and distinct points in \(=(-1,1)\) and there is constant \(B\) such that \(}}{_{}} B\). If \(f()\) is a \(q\)-Lipschitz continuous function, then:_

\[() C_{1}()^{3}\|( f u_{})^{}\|_{L^{2}(; )}^{2}+}{K}_{k=1}^{K}(u_{}(_{k})-x_{k})^ {2},\] (7)

_where \(C_{1}\) is a constant._

The proof of Theorem 1 and the detailed expression for \(C_{1}\) can be found in Appendix B.1.

**Theorem 2** (Upper bound for noisy computation).: _Consider the LeTCC framework with \(N\) worker nodes and at most \(S\) stragglers and \(}_{d}_{0}\) for constant \(_{0}\). Assume each worker node computes \(f_{n}(x)=f(x)+_{n}\) with \([_{n}]=0\) and \([_{n}^{2}]_{0}^{2}\). Assume \(\{_{k}\}_{k=1}^{K}\) are arbitrary and distinct points in \(=(-1,1)\) and suppose there is constant \(B\) such that \(}}{_{}} B\). Assume \(f()\) is a \(q\)-Lipschitz continuous function. Then,_

\[() 4(^{2}}{N-S})^{}(C_{2} C(_{0}) p_{4}(S)\|(f u_{ })^{}\|_{L^{2}(;)}^{2} )^{}+}{K}_{k=1}^{K}(u_{}(_{k} )-x_{k})^{2},\] (8)_if_

\[}_{0}^{}}(  C(_{0}) p_{4}(S)\|(f u_{})^{(2)} \|_{L^{2}(;)}^{2}}{_{0}^{2}})^{} (N-S)^{}\] (9)

_where \(C_{2}\) is a constant, \(C(_{0})=(_{0}^{})\) is an increasing function of \(_{0}\), and \(p_{4}(S)\) is a degree-4 polynomial in \(S\) with positive constant coefficients._

The proof and expressions for \(C_{2}\) and \(C(_{0})\) are provided in Appendix B.2.

**Encoder Design:** The upper bounds established in Theorems 1, 2 hold for all \(u_{}}^{2}(;)\). However, they do not directly lead to a design for \(u_{}()\). To address this, we present the following theorem which bounds the \(\|f u_{}\|_{L^{2}(;)}^{2}\), enabling us to construct \(u_{}()\) without compromising the convergence rate.

**Theorem 3**.: _Consider a LetCC scheme. Assume computing function \(f()\) is \(q\)-Lipschitz continuous and \(\|f^{}\|_{L^{}(;)}\). Then:_

\[()}{K}_{k=1}^{K}(u_{}( _{k})-x_{k})^{2}+_{e}\|u_{}\| _{}^{2}(;)}^{2},\] (10)

_for some monotonically increasing function \(:^{+}^{+}\), where \(_{e}\) is depending on \((N,S,_{0},q,)\)._

The proof can be found in Appendix B.3. By applying the representer theorem , we can deduce that the optimal encoder \(u_{}()\), which minimizes the right-hand side of (10) takes the form \(u_{}()=_{k=1}^{K}z_{k}(_{k},)\), where \(^{K}\), and \(\) is the kernel function of \(}^{2}(;)\), as discussed in Appendix A.2 and in (46). However, due to the non-linearity of \(g()\), calculating the values of the coefficients \(\) is challenging. Nevertheless, we demonstrate that the coefficients can be efficiently derived under certain mild assumptions.

**Proposition 1**.: _In the noiseless case, there exists \(M\) that depends only on \(\{_{k}\}_{k=1}^{K}\) and \(\{x_{k}\}_{k=1}^{K}\), such that:_

* _If_ \(\|u_{}\|_{}^{2}(;)} ^{2} M\)_, then:_ \[()(u_{}),\] (11) _where_ \((u)\) _is defined as follows:_ \[(u):=}{K}_{k=1}^{K}(u(_{k})-x_{k})^{2}+ _{e}(m_{1}+m_{2}M)(M+_{}u^{}(t)^{2}\, dt),\] (12) _and_ \(m_{1},m_{2}\) _are constants._
* _If_ \(u^{*}()\) _is the minimizer of (_12_), then_ \(\|u^{*}\|_{}^{2}(;)}^{2}  M\)_._

See Appendix B.4 for the proof. Proposition 1 states that, under mild assumptions there exists a _smoothing spline_ that minimizes the upper bound given in (12) without changing in convergence rate.

**Convergence Rate:** Using Theorems 1, 2, and 3, we can derive the convergence rate of the proposed scheme as stated in the following theorem.

**Theorem 4** (Convergence rate).: _For LetCC scheme with \(N\) worker nodes and a maximum of \(S\) stragglers, \(()(S^{}N^{-})\) for the noisy computation, and \(()(S^{3}N^{-3})\) for the noiseless setting._

Refer to Appendix B.5 for the proof. Notably, the convergence rate yields from Theorem 4 surpasses the state-of-the-art Berrut coded computing approach upper bound  (Figure 1), both with respect to \(S\) and \(N\) (see Appendix C.1 for detailed comparison).

## 5 Experimental Results

In this section, we extensively evaluate the proposed scheme across various scenarios. Our assessments involve examining multiple deep neural networks as computing functions and exploring the impact of different numbers of stragglers on the scheme's efficiency. The experiments are run using PyTorch  in a single GPU machine. We evaluate the performance of the LeTCC scheme in three different model architectures:

* **Shallow model**: We choose LeNet5  architecture as a known shallow network with approximately \(6 10^{4}\) parameters, trained on the MNIST .
* **Deep model with low-dimensional output**: In this scenario, we evaluate the proposed scheme when the function is a deep neural network trained on color images in CIFAR-10  dataset. We use the recently introduced RepVGG  network with around \(26\) million parameters which was trained on CIFAR-\(10^{1}\).
* **Deep model with high-dimensional output**: Finally, we demonstrate the performance of the LeTCC scheme in a scenario where the input and output of the computing function are high-dimensional, and the function is a relatively large neural network. We consider the Vision Transformer (ViT)  as one of the state-of-the-art base neural networks in computer vision for our prediction model, with more than \(80\) million parameters (in the base version). The network was trained and fine-tuned on the ImageNet-1K dataset 2. 
We use the output of the last softmax layer of each model as the output.

**Hyper-parameters:** The entire encoding and decoding process is the same for different functions, as we adhere to a non-parametric approach. The sole hyper-parameters involved are the two smoothing parameters (\(_{}\), \(_{}\)) which are determined using cross-validation and greed search over different values of the smoothing parameters.

**Baseline:** We compare LeTCC with the Berrut approximate coded computing (BACC) introduced by  as the state-of-the-art coded computing scheme for general computing. The BACC framework is used in  for training neural networks and in  for inference. Although Berrut coded computing  is the only existing coded computing scheme for general functions, we include a comparison of the proposed framework with the Lagrange coded computing scheme  for polynomial computation in Appendix D.

**Interpolation Points:** We choose Chebyshev points of the first and second kind, \(\{_{i}\}_{k=1}^{K}=()\) and \(\{_{n}\}_{n=1}^{N}=()\), for fair comparison with .

**Evaluation Metrics:** We employ two evaluation metrics to assess the performance of the proposed framework: Relative Accuracy (RelAcc) and Root Mean Squared Error (RMSE). RelAcc is defined as the ratio of the base model's prediction accuracy to the accuracy of the estimated model on the initial data points. RMSE, on the other hand, is our main loss defined in (1) which measures the empirical average of the root mean square difference over multiple batches of and non-straggler set \(\), providing an unbiased estimation of expected mean square error, \(_{,}[_{k=1}^ {K}\|(_{k})-}(_{k})\| _{2}]\), for data distribution \(\).

  |)\)} &  &  &  \\   & \((100,20,60)\) &  &  \\ 
**Method** & RMSE & RelAcc & RMSE & RelAcc & RMSE & RelAcc \\  BACC & 2.55\(\) 0.43 & 0.92\(\) 0.04 & 2.44\(\) 0.38 & 0.83\(\) 0.05 & 0.68\(\) 0.13 & 0.90\(\) 0.07 \\ LeTCC & **2.18\(\) 0.51** & **0.94\(\) 0.04** & **2.04\(\) 0.42** & **0.87\(\) 0.05** & **0.62\(\) 0.11** & **0.94\(\) 0.06** \\  

Table 1: Comparison of the proposed framework (LeTCC) and the state-of-the-art (BACC) in terms of the Root Mean Squared Error (RMSE) and the Relative Accuracy (RelAcc).

**Performance Evaluation:** Table 1 presents both RMSE and RelAcc metrics side by side. The results demonstrate that LeTCC outperforms BAC across various architectures and configurations, with an average improvement of 15%, 17%, and 9% in RMSE for LeNet, RepVGG, and ViT architectures, respectively, and a 2%, 5%, and 4% enhancement in RelAcc.

In a subsequent analysis, we evaluate the performance of LeTCC in comparison to BAC across a variety of straggler scenarios. For each number of stragglers, \(S\), we randomly select \(S\) workers to act as stragglers. Both schemes are then run with the same input data points and straggler configurations, and the process is repeated \(20\) times. We record the average values of the RelAcc and RMSE metrics, along with their \(95\%\) confidence intervals. Figures 3 and 4 illustrate the performance of both schemes across three model architectures. As shown in both figures, the proposed scheme consistently outperforms BAC for nearly all straggler values. In Figure 3, where \(\) is relatively small-indicating a system design without excessive redundancy, which is more practical-the proposed scheme demonstrates even greater improvements in both metrics.

**Computational Complexity:** The calculation and inference of smoothing spline coefficients can be performed linearly in the number of regression points by leveraging B-spline basis functions . Consequently, the encoding and decoding processes in LeTCC, which involve evaluating new points and calculating the fitted coefficients, have computational complexities of \((K d)\) and \(((N-S) m)\), respectively, where \(d\) is the input dimension and \(m\) is the output dimension of the computing function \(()\). This complexity is comparable to that of BAC, which has complexities of \((K)\) and \((N-S)\) for its encoding and decoding layers . Table 2 in Appendix C.2 presents a comparison of the total end-to-end processing time statistics for the LeTCC and BAC schemes.

**Sensitivity Analysis:** We additionally investigate the sensitivity of the proposed schemes performance to the value of the smoothing parameter, as well as the sensitivity of the optimal smoothing parameter to the number of stragglers (or workers). The results are presented in Appendix E. As shown in Table 3 and Figure 6, the optimal smoothing parameters and the scheme's performance exhibit low sensitivity to the number of stragglers (or worker nodes) and to the smoothing parameters, respectively.

**Coded Points:** We also compare the coded points \(\{_{n}}\}_{n=1}^{N}\) sent to the workers in LeTCC and BAC schemes. The results, shown in Figure 7, demonstrate that BAC coded samples exhibit high-frequency noise which causes the scheme to approximate the original prediction worse than LeTCC.

Figure 3: Performance comparison of LeTCC and BAC with a \(95\%\) confidence interval across a diverse range of stragglers for different models in a low-redundancy regime (smaller \(\)).

## 6 Related Work

Coded computing was initially introduced to tackle the challenges of distributed computation, particularly the existence of stragglers or slow workers, and also faulty or adversarial nodes. Traditional approaches to deal with stragglers primarily rely on repetition [46; 47; 48; 49; 50; 1], where each task is assigned to multiple workers either proactively or reactively. Recently, coded computing approaches have reduced the overhead of repetition by leveraging coding theory and embedding redundancy in the worker's input data [51; 52; 53; 3; 26; 29; 54; 55]. This technique, which mainly relies on theory of coding, has been developed for specific types of structured computations, such as polynomial computation and matrix multiplication [56; 57; 58; 52; 3; 7; 13; 17; 52; 56]. Recently, there have been attempts to generalize coded computing for general computations [59; 4; 5; 29]. Towards extending the application of coding computing to machine learning computation, Kosaian et al.  suggest training a neural network to predict coded outputs from coded data points. However, the scheme of Kosaian et al.  requires a complex training process and tolerates only one straggler. In another work, Jahani-Nezhad and Maddah-Ali  proposes BACC, a model-agnostic and numerically stable framework for general computations. They successfully employed BACC to train neural networks on a cluster of workers, while tolerating a larger number of stragglers. Building on the BACC framework, Soleymani et al.  introduced ApproxIFER scheme, as a straggler resistance and Byzantine-robust prediction serving system. However, the scheme of BACC uses a reasonable rational interpolation (Berurt interpolation ), off the shelf, for encoding and decoding, without considering any end-to-end cost function to optimize. In contrast, we theoretically formalize a new foundation of coded computing grounded in learning theory, which can be naturally used for machine learning applications.

## 7 Conclusions and Future Work

In this paper, we developed a new foundation for coded computing based on learning theory, contrasting with existing works that rely on coding theory and use metrics like minimum distance and recovery threshold for design. This shift in foundations removes barriers to using coded computing for machine learning applications, allows us to design optimal encoding and decoding functions, and achieves convergence rates that outperform the state of the art. Moreover, the experimental evaluations validate the theoretical guarantees. While this work focuses on straggler mitigation, future work will extend our proposed scheme to achieve Byzantine robustness and privacy, offering promising avenues for further research.

Figure 4: Performance comparison of LeTCC and BACC with a \(95\%\) confidence interval across a diverse range of stragglers for different models in a high-redundancy regime (larger \(\)).

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant CIF-2348638. Behrooz Tahmasebi is supported by NSF Award CCF-2112665 (TILOS AI Institute) and NSF Award 2134108.