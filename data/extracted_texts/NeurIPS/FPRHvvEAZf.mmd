# Safe Decision Transformer with Learning-based Constraints

Ruhan Wang

Indiana University

ruhwang@iu.edu &Dongruo Zhou

Indiana University

dzl13@iu.edu

###### Abstract

In the field of safe offline reinforcement learning (RL), the objective is to utilize offline data to train a policy that maximizes long-term rewards while adhering to safety constraints. Recent work, such as the Constrained Decision Transformer (CDT) , has utilized the Transformer  architecture to build a safe RL agent that is capable of dynamically adjusting the balance between safety and task rewards. However, it often lacks the stitching ability to output policies that are better than those existing in the offline dataset, similar to other Transformer-based RL agents like the Decision Transformer (DT) . We introduce the Constrained Q-learning Decision Transformer (CQDT) to address this issue. At the core of our approach is a novel trajectory relabeling scheme that utilizes learned value functions, with careful consideration of the trade-off between safety and cumulative rewards. Experimental results show that our proposed algorithm outperforms several baselines across a variety of safe offline RL benchmarks.

## 1 Introduction

Recent studies have demonstrated the Transformer's  state-of-the-art performance across a range of applications, including natural language processing  and computer vision . When applied to the domain of Reinforcement Learning (RL), a recent trend is to treat the decision-making problem as a sequence modeling problem, using auto regressive models such as Transformer which maps the history information directly to the action  or the next state . Notably, the Decision Transformer (DT)  effectively extends the Transformer architecture to offline RL tasks, showcasing strong performance, particularly in sequential modeling. However, it is worth noting that while DT excels in maximizing long-term rewards, it may not always align with the complexities of real-world tasks. In practice, many tasks cannot be simplified solely into optimizing a single scalar reward function, and the presence of various constraints significantly narrows the spectrum of feasible solutions . Such a setting is called safe RL, which has been studied in lots of safety-related decision-making problems. For instance, it is crucial that robots interacting with humans in human-machine environments prioritize human safety and avoid causing harm. In the realm of recommender systems, it is imperative to avoid recommending false or racially discriminatory information to users. Similarly, when self-driving cars operate in real-world environments, ensuring safety is paramount .

Constrained Decision Transformer (CDT)  serves as a pioneering work which extends the Transformer-based RL to the safe RL regime, which builds upon the foundation of the DT while incorporating essential safety constraints. CDT's core objective is to acquire a safe policy from an offline dataset. Its distinguishing feature is the ability to maintain zero-shot adaptability across a range of constraint thresholds, rendering it highly suitable for real-world reinforcement learning applications burdened by constraints. While CDT demonstrates exceptional competitiveness in safe offline reinforcement learning tasks, it shares a common limitation with DT--an absence of the'stitching' capability. This crucial ability involves integrating sub-optimal trajectory segments to forma near-optimal trajectory, a pivotal characteristic highly desired in offline reinforcement learning agents. What is more challenging is that for RL with safety constraints, the agent not only needs to stitch trajectories to achieve better reward feedback but also needs to guarantee that the obtained policy is still _safe_ after stitching, not being affected by unsafe trajectories in the offline dataset. Thus, we raise the following question:

**Can we design DT-based algorithms that output safe policies with the stitching ability?**

We answer this question affirmatively. To better demonstrate our algorithm design, we propose a toy example involving finding the path that maximizes reward under cost constraints, as illustrated in Figure 1. The task's objective is to identify a path with the highest reward while adhering to a cost constraint (cost limitation = 10). The training data covers segments of the optimal path, but none of the training data trajectories encompass the entire optimal path. The agent must synthesize these fragmented trajectories to determine the optimal path within the cost constraint. For the existing DT-based RL algorithms such as Q-learning Decision Transformer (QDT) , they are able to stitch suboptimal trajectories into the optimal ones, but they are unable to maintain safety during the stitching phase. For the existing DT-based safe RL methods such as CDT, they can only obtain suboptimal policies while satisfying the safety guarantee, due to the lack of stitching ability. Thus, we propose the **Constrained Q-learning Decision Transformer (CQDT)** method to address the issues in existing methods, as shown in Figure 1. The main contributions are listed as follows.

* CQDT seeks to improve the quality of the training dataset by utilizing cost and reward critic functions from the Constraints Penalized Q-learning framework  to relabel the return-to-go (RTG) and cost-to-go (CTG) values in the training trajectories. This relabeled dataset is subsequently used to train a Decision Transformer-based safe RL method, such as CDT.
* We provide a comparative analysis of our CQDT against various safe RL benchmarks across different RL task settings. The results are summarized in Figure 2, demonstrate that CQDT consistently outperforms all existing benchmarks in several offline safe RL tasks.
* We also show that our proposed CQDT enjoys better stitching ability compared with CDT, which suggests that CQDT can better utilize suboptimal trajectories in the offline dataset. The results are summarized in Figure 3 and Figure 4.

## 2 Related Work

**Offline Reinforcement Learning.** Offline Reinforcement Learning refers to a data-driven approach to the classic RL problem . It focuses on deriving policies from pre-existing data without requiring further interaction with the environment. The practical applications of offline RL are extensive, spanning domains such as healthcare  and the acquisition of robotic manipulation skills . However, its inherent challenge lies in the potential disparity between the learned policy and the behavior that generated the initial offline data . Addressing this challenge has spurred the development of various methodologies, including constraining the learned policy close from the behavior policy [11; 21; 22; 44; 17; 20; 39; 32; 18]. There is a recent line of work aiming at providing a Transformer-based policy without explicitly constraining the distribution shift issue . Our work falls into that regime, which uses a Transformer as the policy model focusing on the safe RL setting.

**Offline Safe Reinforcement Learning.** Offline Safe Reinforcement Learning aims to acquire constrained policies from pre-collected datasets, ensuring adherence to safety requirements throughout the learning process. This approach amalgamates techniques from both offline RL and safe RL, leveraging methodologies from both domains . Certain methods tackle the constrained optimization problem through stationary distribution correction, employing Lagrangian constraints to ensure safe learning [41; 33]. Our work does not take the Lagrangian approach to learn a safe policy. Instead, our method explicitly treats the safe policy learning as a sequence modeling problem, similar to the previous CDT approach . Such an approach enjoys the simplicity regarding the algorithm design, as well as the robustness to the algorithm performance.

## 3 Preliminaries

**Safe Offline RL.** We formulate the environment as a Constrained Markov Decision Process (CMDP), a mathematical framework for addressing the safe RL problem . A CMDP comprises a tuple \((,,,r,c,_{0})\), where \(\) denotes the state space, \(\) signifies the action space, \(:\) represents the transition function, \(r:\) stands for the reward function, and \(_{0}:\) indicates the initial state distribution. In CMDP, \(c:[0,C_{}]\) quantifies the cost incurred for violating constraints, where \(C_{}\) denotes the maximum cost allowable.

We denote the policy by \(:\), while \(=\{s_{1},a_{1},r_{1},c_{1},,s_{T},a_{T},r_{T},c_{T}\}\) delineates the trajectory containing state, action, reward, and cost information throughout the maximum episode length \(T\). We use \(.s_{t},.a_{t},.r_{t},.c_{t}\) to denote the \(t\)-th state, action, reward and cost in trajectory \(\). For each time step \(t\), the action \(a_{t}\) is drawn following the distribution \((s_{t},)\), and the next state \(s_{t+1}\) is drawn following the transition function \((s_{t},a_{t},)\). The cumulative reward and cost for a trajectory \(\) are represented as \(R()=_{t=1}^{T}r_{t}\) and \(C()=_{t=1}^{T}c_{t}\). We also denote \(R_{t}=_{i=t}^{T}r_{i}\) by the return-to-go (RTG) at \(t\)-th step, \(C_{t}=_{i=t}^{T}c_{i}\) by the cost-to-go (CTG) at \(t\)-th step as well. For simplicity, we define \(Q_{}^{}(s,a)=_{}[R()|.s_{1}=s,.a_{1}=a]\) as the expected return of the policy starting from initial state \(s\) and action \(a\). Similarly, we denote \(Q_{e}^{}(s,a)=_{}[C()|.s_{1}=s,.a_{1}=a]\) as the expected cost. The agent's objective is to determine a policy \(^{}\) that maximizes the reward while ensuring that the cumulative cost for constraint violation remains below the threshold \(\):

\[^{}=_{}_{,.s_{1} _{0}}[R()],\] \[\ _{,.s_{1}_{0}}[C()].\] (1)

For safe offline RL, the agent learns the optimal safe policy purely from a static dataset \(\) that is previously collected with an unknown behavior policy (or policies). Specifically, \(\) consists of \(m\) episodes \(_{i}\) with the maximum length \(T\), which is \(:=\{_{1},,_{m}\}\).

**Constrained Decision Transformer.** Our algorithm builds on the Constrained Decision Transformer (CDT) . We briefly introduce the details of CDT here, and we leave more details in the appendix. CDT utilizes the return-conditioned sequential modeling framework to accommodate varying constraint thresholds during deployment, ensuring both safety and high return. CDT employs a stochastic Gaussian policy representation to generate the action at \(t\)-th time step, i.e., \(a_{t}_{}(|o_{t})=(_{}(o_{t}),_{ }(o_{t}))\), where \(o_{t}=\{R_{t-K:t},C_{t-K:t},s_{t-K:t},a_{t-K:t-1}\}\) represents the truncated history from step \(t-K\) to \(t\), \(K\{1,,t-1\}\) indicates the context length and \(\) denotes the CDT policy parameters. During the training phase, CDT generates the training set \(\{(o_{t},a_{t})\}\) by splitting trajectories in \(\) into shorter contexts with length \(K\), then it trains the policy by minimizing the prediction loss between \(a_{t}\) and \(_{}(|o_{t})\). During the inference phase, CDT selects the initial return-to-go \(R_{1}\) as well as the cost-to-go \(C_{1}\), then it selects the action \(a_{t}\) based on the current 

[MISSING_PAGE_FAIL:4]

in line 4 of Algorithm 1. In the subsequent steps, the \(_{r}\) and \(_{c}\) lists are utilized to relabel the RTG and CTG for each trajectory. For additional details, please refer to Appendix A.1.

**Second Step: Relabeling Trajectories.** Now we describe how to relabel a given trajectory \(\) using \(_{r}\) and \(_{c}\) in detail. The steps are summarized in Algorithm 2. To demonstrate that, recall that our goal is to learn \(^{}\) that maximizes the expected return under the \(\) constraint. Assume that for the trajectory \(\), the policy \(\) that generates \(\) satisfies the constraint \(\). Therefore, in order to further push the agent to learn \(^{}\) instead of only learning \(\), we generate a new trajectory \(^{}\) identical to \(\), with different \(^{}.R_{i}\), \(^{}.C_{i}\), to make \(^{}\) similar to a trajectory generated by \(^{}\). Our strategy is simple: we first replace the last RTG and CTG of \(^{}\) as 0. At the \(t\)-th step of \(^{}\), we would like to calculate \(^{}.R_{t-1}\) and \(^{}.C_{t-1}\). Then we either to use the existing RTG (\(^{}.R_{t}\)) and CTG (\(^{}.C_{t}\)) to update \(^{}.R_{t-1}\) and \(^{}.C_{t-1}\) (line 4 in Algorithm 2), or we use the learned reward critic and cost critic to update \(^{}.R_{t-1}\) and \(^{}.C_{t-1}\) (line 6 in Algorithm 2), if the reward critic and cost critic provide a more "aggressive" approximation, i.e., \(V_{c}^{}(.s_{t})^{}.C_{t}\) and \(V_{r}^{}(.s_{t})^{}.R_{t}\) (line 5 in Algorithm 2). Here \(V_{r}^{}\) and \(V_{c}^{}\) are learned reward and cost critics, and they are selected from \(_{r}\) and \(_{c}\) to make sure that the cost constraint estimation \(Q_{c}^{}\) is close to the true CTG \(C_{0}\) (line 8 in Algorithm 2). We summarize the relabeling process in Algorithm 2.

```
0: Trajectory \(\), reward critic \(Q_{r}^{}\) and cost critic \(Q_{c}^{}\)
1: Set \(T\) as the length of \(\), the new trajectory \(^{}=\), \(^{}.R_{T+1}=^{}.C_{T+1}=0\)
2:for\(t=T+1,,2\)do
3: Set \(V_{r}^{}(.s_{t})=Q_{r}^{}(.s_{t},.a_{t})\), \(V_{c}^{}(.s_{t})=Q_{c}^{}(.s_{t},.a_{t})\)
4:\(^{}.R_{t-1}.r_{t-1}+^{}.R_{t}\), \(^{}.C_{t-1}.c_{t-1}+^{}.C_{t}\)
5:if\(V_{c}^{}(.s_{t})^{}.C_{t}\) and \(V_{r}^{}(.s_{t})^{}.R_{t}\)then
6:\(^{}.R_{t-1}.r_{t-1}+V_{r}^{}(.s_{t})\), \(^{}.C_{t-1}.c_{t-1}+V_{c}^{}(.s_{t})\)
7:endif
8:endfor
8: Relabeled trajectory \(^{}\) ```

**Algorithm 2** Relabeling one trajectory

The most notable difference between our relabeling strategy and the previous one for offline RL  is that we relabel RTG and CTG _jointly and simultaneously_. If we only relabel each trajectory based on their RTG and CTG separately, we might obtain unsafe trajectories, which hurts the overall performance of CQDT. Instead, our strategy ensures that, each safe trajectory will still be safe after relabeling, which is crucial for the safe RL setting. Our experimental results in the later section suggest the effectiveness of our relabeling strategy.

**Third Step: Post-Processing Steps for the Final Trajectory.** Now, we have produced an augmented trajectory dataset \(\) which consists of the original trajectories \(\) and the new trajectories \(^{}\). Finally, we introduce some additional post-processing steps over \(\) from existing works [30; 42] for the further performance improvement of CQDT.

The first post-processing technique we adopt is to resolve the potential conflict between a high RTG and a low CTG. Due to the nature of safe RL, we would like to first guarantee the safety of our learned policy. Following , we use a Pareto Frontier-based data augmentation technique to further generate new trajectories and add them to \(\). The second post-processing technique aims to maintain the consistency of the RTG and CTG within the input sequence of CDT. Due to space limitations, we defer the detailed in Appendix A.2.

## 5 Experiment

In this section, we begin by outlining the fundamental settings of our experiment. We then show the performance of CQDT under a series of experiments, each addressed a key challenge as follows:

* How does CQDT compare with CDT and other offline safe reinforcement learning methods in terms of performance? Additionally, how does the choice of the value function affect CQDT's performance?
* Is CQDT capable of performing effective stitching?

[MISSING_PAGE_FAIL:6]

Compared to other baselines, our proposed CQDT method achieves maximum return while ensuring safety. CPQ, BCQ-Lag, and BEAR-Lag, three Q-learning-based safe reinforcement learning methods, encounter challenges in balancing safety and reward optimization. The BC-Safe method, grounded in imitation learning and trained on provided data, exhibits suboptimal performance during the test phase. This phenomenon may be attributed to the scarcity of safe data in the training dataset, indicating a lack of robustness. COptiDICE employs novel estimators to evaluate policy constraints and achieves suboptimal rewards, while facing challenges related to adhering to strict safety constraints.

The CQDT method presented in our work leverages additional value functions to relabel trajectories, enhancing the model's stitching capability and enabling it to achieve state-of-the-art performance. To demonstrate the effectiveness of the CQDT method, we conduct a comprehensive comparative analysis examining the impact of various value functions on algorithmic performance. BCQ-Lag and BEAR-Lag, both grounded in offline safe reinforcement learning and based on Q-Learning, are employed for this investigation. The \(Q_{r}\) and \(Q_{c}\) functions in these methodologies are employed to estimate the RTG and CTG values of the original trajectory. To signify the enhanced versions of these methods, we specifically refer to them as **BCQL-CDT** and **BEARL-CDT**, respectively. Refer Appendix C.2 for further details.

Our experimental results, detailed in Table 1, indicate that BCQL-CDT and BEARL-CDT perform similarly to CDT in the selected tasks, although they do not reach the performance of CQDT. The performance discrepancy between BCQ-Lag and BEAR-Lag compared to CPQ suggests suboptimality in relabeling the original trajectories using their respective value functions. This contributes to the varied performance among BCQL-CDT, BEARL-CDT, and CQDT.

We also conduct two ablation studies, Ablation 1 and Ablation 2, to assess the impact of various components within CQDT. The results of these experiments are provided in Table 1. For further details on the ablation studies, please refer to Appendix C.1. In addition, we evaluate the Zero-Shot Adaptation capability and robustness of CQDT. For more information, refer to Appendix C.4.

### The Stitching Capability of CQDT

We conduct an evaluation of CQDT's stitching capabilities for reward and cost by creating various suboptimal datasets for five tasks and comparing the performance of CQDT and CDT across these datasets.

Figure 5: Verification of stitching-reward ability with \(p=0.x\) values representing various suboptimal datasets. Suboptimal datasets were generated by removing safe trajectories that fall within the top x% of cumulative rewards. Higher \(p\)-values indicate the removal of more high-reward safe paths, degrading dataset quality. The first row illustrates cumulative rewards obtained by CQDT and CDT trained on these datasets for different tasks, while the second row shows the corresponding cumulative costs. The black dashed line denotes the cost limitation.

Figure 6: Verification of stitching-cost ability with \(p=0.x\) values corresponding to various suboptimal datasets. These datasets were created by excluding trajectories with low cumulative costs. As the \(p\)-value increases, fewer trajectories with small cumulative costs are retained, resulting in increasingly suboptimal datasets.

To evaluate the reward stitching ability, we remove the top X% of trajectories with the highest RTG from batches of trajectories. As the value of X increases, more high-return trajectories are excluded from the dataset. To create a suboptimal dataset for evaluating cost stitching capability, we group trajectories based on their RTG, then we remove the trajectories with lowest CTG in each group. In detail, we divide the trajectories into \(\) groups, where Max Return denotes the highest return among all trajectories. Within each group, we remove trajectories with the lowest X% CTG from each group. Such a setup allows us to detect the stitching ability for a safe offline RL agent, as we want her to learn safe and high-return policy from unsafe and low-return trajectories. We leave the detailed parameter setup and the visualization of these suboptimal datasets in the Appendix C.3.

We present the performance of CQDT on different suboptimal datasets in Figures 5 and 6. These experiments demonstrate that as the value of \(X\) increases, leading to the removal of higher-quality trajectories, the performance of policies generated by CQDT and CDT deteriorates. We can observe that the cumulative reward decreases. However, CQDT consistently outperforms CDT, demonstrating its superior stitching ability. Even when trained with suboptimal datasets, CQDT effectively utilizes these datasets to maximize performance by leveraging its stitching capabilities. Superior performance by CQDT highlights its unique ability to stitch suboptimal trajectories, a capability not present in CDT. This stitching ability enables CQDT to achieve better overall performance.

### Performance of CQDT in Sparse Reward Environment

The experiments in the previous sections demonstrate that CQDT performs well in dense reward environments. In this section, we evaluate and analyze the performance of CQDT in the sparse reward environment. Since there is no publicly available dataset or corresponding environment for sparse reward scenarios in the field of offline safe RL, we build our own environment based on existing datasets. Specifically, we select the existing DroneCircle environment to create a Sparse-Reward DroneCircle environment.

For any trajectory \(\) in DroneCircle, we aim to create a new trajectory \(^{}\) as follows. We consider each subsequence in \(\) with length 10, i.e., \(.r_{10k},.r_{10k+1},...,.r_{10k+9}\). We then set \(^{}=\), while it replaces \(^{}.r_{10k+9}\) with \(.r_{10k}+.r_{10k+1}++.r_{10k+9}\), and replaces \(^{}.r_{10k+i},0 i<9\) with 0. Such an operation keeps the total reward of \(^{}\) unchanged, while it makes \(^{}\) a trajectory with sparse rewards. Accordingly, we use the Sparse-Reward DroneCircle Offline Dataset to train CQDT and CDT. During testing, we adopt the similar strategy, where each agent encounters 0 reward in time step \(10k,10k+1,...,10k+8\), and she encounters \(.r_{10k}+.r_{10k+1}++.r_{10k+9}\) at time step \(10k+9\). We do not change the cost distribution.

Figures 8 and 8 show the performance comparison between CQDT and CDT under different target return and target cost settings in the Sparse-Reward DroneCircle environment. The results indicate that CQDT consistently outperforms CDT across various settings of target return and target cost. These experimental results demonstrate that CQDT maintains its superiority even in sparse-reward environments.

## 6 Conclusion and Future Work

In this work, we proposed the Constrained Q-learning Decision Transformer (CQDT) for safe offline RL. Our approach replaces reward-to-go and cost-to-go in the training data with dynamic programming-based learning-based reward return and cost return, which brings the stitching ability and addresses the weakness of the Constrained Decision Transformer (CDT). Our evaluation shows that our approach is able to outperform existing safe RL baseline algorithms. One potential future direction is to build a theoretical analysis to justify the effectiveness of our learning-based constraint approach to safe RL, similar to previous analyses applied to general goal-based RL algorithms .