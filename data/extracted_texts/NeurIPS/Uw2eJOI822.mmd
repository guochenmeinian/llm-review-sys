# Renovating Names in

Open-Vocabulary Segmentation Benchmarks

 Haiwen Huang\({}^{1,2,3}\) Songyou Peng\({}^{3,4,5}\) Dan Zhang\({}^{6}\) Andreas Geiger\({}^{2,3}\)

\({}^{1}\) Bosch IoC Lab, University of Tubingen \({}^{3}\) Tubingen AI Center

\({}^{3}\) Autonomous Vision Group, University of Tubingen \({}^{4}\) ETH Zurich

\({}^{5}\) MPI for Intelligent Systems, Tubingen \({}^{6}\) Bosch Center for Artificial Intelligence

###### Abstract

Names are essential to both human cognition and vision-language models. Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training. However, the precision of these names is often overlooked in existing datasets. In this paper, we address this underexplored problem by presenting a framework for "renovating" names in open-vocabulary segmentation benchmarks (RENOVATE). Our framework features a renaming model that enhances the quality of names for each visual segment. Through experiments, we demonstrate that our renovated names help train stronger open-vocabulary models with up to 15% relative improvement and significantly enhance training efficiency with improved data quality. We also show that our renovated names improve evaluation by better measuring misclassification and enabling fine-grained model analysis. We provide our code and relabelings for several popular segmentation datasets to the research community on our project page: https://andrehuang.github.io/renovate/.

## 1 Introduction

Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made.

-- Eleanor Rosch, _Basic Objects in Natural Categories_

We use names every day. Imagine wandering the trails of a national park - do you pause by a "lake" or simply a body of "water"? While driving through urban sprawl, do you see "trees" or "vegetation" along the road? Our instinctual use of terms like "lake" and "trees" illustrates the human propensity to categorize the world around us into "basic categories" -- rich in information and readily identifiable without unnecessary complexity . Such categorization is fundamental to human cognition and communication.

In stark contrast, recent advancements in open-vocabulary models  struggle to replicate this nuanced aspect of human categorization. While these models have made strides in generalizing to both familiar and novel categories via textual prompts, they are often hampered by the imprecise and sometimes wrong names provided in existing benchmarks. In fact, most datasets are labeled with class names that serve merely as identifiers to distinguish classes within a dataset, rather than descriptive labels aligning with the "basic categories" that match with the actual visual contents. As shown in Fig. 1, existing names are often inaccurate, too general, or lack enough context, leading to discrepancies between the model's outputs and the actual visual segments. This misalignment indicates a pressing need for a reassessment and refinement of name labeling practices. By adopting a naming scheme that aligns more closely with human categorization, we can pave the way for enhanced open-vocabulary generalization and more accurate model evaluation.

In this work, we focus on renovating the names for open-vocabulary segmentation benchmarks, as their dense annotations pose a greater challenge and offer broader applicability than other recognition tasks. The importance of names in open-vocabulary segmentation is often overlooked, with one exception, OpenSeg , which manually inspected and modified the class names of several segmentation benchmarks, resulting in considerable performance gains of their proposed model. While their work demonstrated the importance of choosing good names, a manual approach is subjective and difficult to scale. To date, there exists no systematic study on how to rename benchmarks in a scalable and principled way. Our work is the first attempt to tackle this challenge.

In this work, we introduce a scalable, simple, yet general method for renaming segmentation benchmarks that outperforms manual label efforts like OpenSeg . Our approach leverages foundation models to automate the renaming process, significantly reducing the manual labor traditionally required. Towards this goal, our method can be divided into two steps. First, we narrow down the name search space from the whole language space to a curated list of candidate names. This list is generated by leveraging the original class names with contextually relevant nouns extracted from visual contents via an image captioning model, thereby streamlining the search for names. Next, we employ a specially trained renaming model to identify and select the best-matching candidate name for every ground-truth segmentation mask. In this way, we match an individual name for each instance without any extra human annotations.

To demonstrate the value of RENOVATE names, we explore two practical applications. We first use our renovated names for training open-vocabulary models and show that they substantially enhance generalization performance across various benchmarks by up to 15%, indicating a more accurate alignment with visual segments. Our new names also significantly increase training efficiency thanks to their enriched textual information. Second, we apply our renovated names to improve evaluation of open-vocabulary segmentation. Specifically, we reveal that current pre-trained models are making many "benign" misclassifications, _i.e._, misclassified names that are semantically close to the ground-truth names. RENOVATE names enable more fine-grained model performance analysis, providing valuable information for further model enhancement and dataset curation.

In summary, our work makes the following contributions:

* We point out the naming problem in existing open-vocabulary segmentation benchmarks.
* We propose a scalable, simple, and general framework for automatic dataset renaming.
* We demonstrate that our RENOVATE names help to train models with stronger open-vocabulary capabilities and refine evaluation benchmarks, providing valuable insights for future research.

## 2 Related Work

**Open-Vocabulary Segmentation Methods.** Prior work in open-vocabulary segmentation [5; 6; 7; 8; 16; 17; 18; 19; 20] focused on adapting vision-language models (VLMs) [21; 22; 23] to the seg

Figure 1: **Problems of names in current segmentation benchmarks. We demonstrate examples from well-known datasets: MS COCO , ADE20K , and Cityscapes . Our renovated names are visually more aligned and help models to generalize better.**

mentation task without compromising pre-trained vision-language alignment. These efforts have concentrated mainly on the architecture design, exploring different backbones [24; 25], mask proposal branch designs [19; 5], and mask feature extraction techniques [7; 26; 27]. Our work differs from them in that we aim to adapt pre-trained vision-language representations for segment-wise name matching through our novel designs like candidate name generation techniques and our specialized renaming model.

**Names in Open-Vocabulary Segmentation Benchmarks.** Despite its apparent importance, the naming problem in open-vocabulary segmentation benchmarks has not received adequate attention. An exception is the work by , which proposes to learn class-specific word embeddings for improved model adaptation to new datasets. However, these learned embeddings are not in the language space and cannot be used by other models. In addition, some works [29; 30] propose to decompose the class names into a set of attributes or prototypes for open-vocabulary segmentation. To the best of our knowledge, OpenSeg  is the only work that proposes to directly improve the quality of the names. However, their approach relies on manual inspection of the datasets, which can be subjective and hard to scale. In this paper, we systematically explore the renaming challenge, culminating in a scalable, simple, yet general framework to overcome these limitations.

## 3 RENOVATE: Renaming Segmentation Benchmarks

Our method RENOVATE aims to rename the original names in a given dataset with ground-truth mask annotations. As shown in Fig. 2, we first generate a pool of candidate names, covering potential variants of the original class names. Then, we train a specially designed renaming model that can measure the alignment between segments in pixel space and names in language space. Equipped with the renaming model, we select the top-matching names from the candidate pool for each segment, thereby producing segment-level, refined names that offer enhanced accuracy and details in characterizing the dataset, see Fig. 3 and Fig. 4.

### Generating candidate names

We use GPT-4  for creating a pool of candidate names. A naive solution is to prompt GPT-4 with the original name and ask it to generate synonyms and hierarchical concepts. However, since the original names are often too general and ambiguous, GPT-4 does not have sufficient knowledge to generate high-quality candidates. Therefore, we propose to exploit the visual contents for generating some "context names" (Table 1) that assist GPT-4 in comprehending the category's meaning prior to generating candidate names.

As shown in Fig. 2, for each category, we use an image captioning method to process all training images that contain that specific category based on ground-truth annotations. From the generated captions, we further extract nouns by text parsing and filtering as done in CaSED . We observe that nouns appearing more frequently tend to offer deeper insights into the typical environments or traits associated with the category. Therefore, we construct _context names_ by selecting the top 10 most recurrent nouns for each category. We then use them as additional inputs alongside the original class names to prompt GPT-4.

   Original name & Context names & Candidate names \\   & lush, field, sky, green, &  \\  & grass, tree, road, & & sports field, grassland, grassy hillside \\  & hillside, grassy, rural & & \\   & room, stool, chair, floor, market, &  \\  & table, food, lamp, paper & & decorative box, display box, paper box, food box \\   & room, infant bed, nursery, dresser, &  \\  & carpet, bedroom, chair, bureau, & & baby cradle, infant cradle, wooden cradle \\   & lamp, armchair & & \\   

Table 1: **Examples of context names and generated candidate names** for three selected classes from ADE20K. Context names are key to comprehending general terms such as “field” and “box” and disambiguating polysemous terms like “cradle”, which, in this context, refers to a baby bed rather than a phone cradle or a mining tool.

Specifically, we instruct GPT-4 to generate two types of candidate names: (1) _Synonyms or subcategories_, which can expand the lexical scope of the overly general class name. For instance, the class name "grass" could yield variations like "lawn" or "turf". For this type, synonyms and subcategories are considered together, acknowledging their often interchangeable nature. (2) _Providing short, distinct contexts_ to polysemous names. As an example, the ambiguous name "fan" could be elaborated into "ceiling fan" or "floor fan", offering clearer specificity. As shown in Table 1, the 5-10 candidate names generated by GPT-4 are more precise than the original names, reflecting the context information encoded by context names. We provide detailed GPT-4 prompts in the supplement.

### Training for candidate name selection

Among the generated candidates, we will only keep those that are better aligned with the visual contents. Particularly, we train a model that can assess the alignment between each name and segment, using segmentation as the proxy task. Conceptually, a candidate name that well describes the segment in the image should help the model recover its ground-truth mask and make a correct classification. Therefore, the model should allow vision-language interaction, and the supervision signal should encourage to use textual information for mask prediction and classification.

Our renaming model is illustrated in Fig. 2. Its vision part starts from a CLIP-based vision backbone followed by a pixel decoder that gradually upscales the backbone features to generate per-pixel embeddings \(F_{}^{H W C}\). The candidate names of the categories present in the image are processed by the CLIP text encoder. The interaction of textual and visual information takes place at the transformer decoder. The initial queries \(X^{(0)}\) are from the text embeddings, and then updated by the transformer decoder consisting of \(L\) blocks using the multi-scale visual features from the pixel decoder. As output of each block, the queries \(X^{(l)}\) generate intermediate mask and class predictions:

\[^{(l)}=(X^{(l)}*F_{}),^{(l)}\;= [(X^{(l)})]\] (1)

where \(*\) refers to pixel-wise multiplication. The final predictions are made by the queries \(X^{(L)}\). Our pixel decoder, mask prediction and classification follow the design of Mask2Former , which handles panoptic, instance, and semantic segmentation in a unified manner. The key differences lie in 1) the transformer decoder design, and 2) the training strategy, which we detail subsequently.

Figure 2: **Overview of candidate name generation and renaming model training.** We generate candidate names based on the context names and train the renaming model to match them with the segments. For illustration clarity, we show only one segment. In practice, multiple segments are jointly trained, pairing with the text queries.

**Transformer Decoder.** Our transformer decoder uses text embeddings as input queries, unlike most open-vocabulary segmentation models that employ text embeddings solely as weights of the Linear layer in Eq. (1) for classification. This key change fosters an earlier integration of text and visual features at varying scales, essential for leveraging text information in segmentation tasks. As shown in Fig. 2, the text embedding of each candidate name from the segment's class "bicycle" feeds into the transformer decoder, generating their corresponding mask and class predictions.

Next, within each block of the transformer decoder, the masked cross-attention (CA) layer adopts ground-truth masks as the attention biases, and the self-attention (SA) layer  and feed-forward network (FFN) are the same as in Mask2Former. The queries interact with the visual features at the masked CA layers, which attend within the region defined by the ground-truth mask, i.e., the attention bias. In the example of Fig. 2, the visual features corresponding to the segment "bicycle" can therefore contribute more effectively to refining the query. To encourage the model to rely more on the textual information in prediction, we further randomly replace the ground-truth masks with predicted masks from the preceding block in the intermediate layers. Importantly, while ground-truth mask based attention biases augment the V-L interaction by focusing on the precise segment regions, they do not reduce the task to a straightforward use of ground-truth mask inputs for mask prediction, as the output queries remain a set of vectors. The final mask prediction \(^{(L)}\) is based on the correlation of these output queries \(X^{(L)}\) with the feature map \(F_{}\) generated by the pixel decoder as in Eq. (1).

**Training Strategy.** With the CLIP backbone kept frozen, we train the transformer decoder together with the pixel decoder. To recover the ground-truth mask \(M_{i}\) and class \(c_{i}\), the transformer decoder makes multiple mask and class predictions with candidate names from the class \(c_{i}\). The prediction with the highest Intersection over Union (IoU) with \(M_{i}\) is selected for loss computation:

\[_{i}=_{}(M_{i},_{})+ _{}(c_{i},_{}),\] (2)

where \(_{}\) and \(_{}\) are the mask localization and classification loss functions in Mask2Former. To provide extra supervision on the name quality, we append the candidate names with a "negative" name randomly selected from a different category than the ground-truth class \(c_{i}\). If a "negative" name scores the highest IoU, we supervise the predictions with an empty mask and the "void" class (\(c_{void}\)) which is one extra class in addition to the training classes:

\[_{i}=_{}(,_{})+ _{}(c_{void},_{}).\] (3)

The "void" class was also used in Mask2Former classification when the prediction mask is not matched to any ground-truth mask in the image. Having both the positive and negative supervision, we effectively incentivize the model to favor names of high quality and penalize those of low quality, thereby aiding in the accurate identification of the best-matching names for each segment.

### Obtaining renovated names

As illustrated in Fig. 2(a), to obtain the renovated names, we run the trained renaming model to associate each ground-truth mask with the candidate name. For each segment, our renaming model first

Figure 3: **Obtaining renovated names. In (a) we illustrate how we use the renaming model to obtain a renovated name for each segment. In (b) we demonstrate that the renaming results are helpful to dataset analysis with examples from “person” class.**ranks the names by the IoU scores between the predicted masks and the ground-truth mask, then selects the top-scoring name as the renovated name. While the IoU ranking scores are not probabilities, they are valuable to understand the relative and absolute model confidence in the names, i.e., how the chosen name stands against the other candidates and whether the selection is indeed confident. In the experiments (Section 5.3), we make use of this information to speed up human verification. We show some examples of renovated names in Fig. 4 and more in the supplements.

Our renovated names are readily usable for analyzing the dataset distribution and biases in a more fine-grained manner, as exemplified in Fig. 2(b). Note that our renovated names in each original class are not necessarily mutually exclusive because different visual segments may be matched to names at different hierarchies. For example, in Fig. 4, a person is named as "engineer" (second row), likely due to the clothing, while the other person is named as "woman" (third row), possibly from the physical appearance. But "engineer" and "woman" are not mutually exclusive names. An interesting future extension would be to further study the hierarchies of different names in a dataset.

## 4 Applications of RENOVATE Names

We further demonstrate the value of the renovated names with two application tasks: training stronger open-vocabulary segmentation models and improving existing evaluation benchmarks. The improvements in both tasks indicate the quality of RENOVATE names.

### Training with RENOVATE names

RENOVATE names offer precise and diverse text information. We exploit this to improve model training and achieve better generalization and data efficiency. Following prior works [24; 25; 35], we replace original ground-truth names with RENOVATE names to compute text embeddings. In addition, we employ the "negative sampling" technique, commonly used in natural language processing to enhance training with a large number of classes [36; 37; 38; 39]. For each segment, we first randomly select \(C-1\) negative classes and choose one RENOVATE name from each. These names are then concatenated with the ground-truth RENOVATE name to form the text prompt. This effectively constructs text embeddings of length \(C\) with semantically distinct names, improving training efficiency and generalization. For computing the classification loss, we use the cross entropy function on a per-segment basis, _i.e._, for an image with segments \(\{m_{i}\}_{i=1}^{N}\), \(_{}=_{i=1}^{N}(c_{i}, (v_{i}T_{i}^{T})),\) where \(c_{i}\) is the ground-truth label, \(v_{i}^{d}\) is the predicted

Figure 4: **Examples of renovated names on segments** from the validation sets of ADE20K and Cityscapes. For each segment, we show the original name below the image and the renovated name in the text box. See more visual results in the supplements.

visual embeddings and \(T_{i}^{C d}\) is the constructed text embeddings through negative sampling. Throughout our experiments, we use negative sampling in our model training unless specified.

### Improving evaluation with RENOVATE names

RENOVATE names also enable more fine-grained analysis of open-vocabulary models. For instance, for a misclassification from "car" to "van", we may distinguish between "SUV" to "minivan" versus "sedan" to "delivery van", which isn't possible with class-level annotations in current datasets. However, even with RENOVATE names, standard segmentation metrics obscure our desired fine-grained performance insights since they penalize all such misclassification cases equally. To this end, we adopt "open" evaluation metrics  that consider semantic similarity between names. Specifically, for a groundtruth mask pixel \(g_{i}\) with label \(c_{i}\) and a predicted mask pixel \(d_{j}\) with predicted label \(c_{j}\), we compute soft \(TP_{c_{i}}=S_{c_{i},c_{j}}\), soft \(FP_{c_{j}}=1-S_{c_{i},c_{j}}\), and soft \(FN_{c_{i}}=1-S_{c_{i},c_{j}}\), where \(S_{c_{i},c_{j}}\) is the semantic similarity between \(c_{i}\) and \(c_{j}\). These soft metrics are then used to compute panoptic quality (PQ), Average Precision (AP), and mean IoU (mIoU) in a standard way. In Section 5.3, we demonstrate how open metrics effectively reveal that there are indeed many "benign" misclassification scenarios using RENOVATE names.

While our renaming process is automated, it is essential to ensure exceptionally high-quality names for evaluation. Therefore, we introduce a rigorous verification process involving five human annotators who review the top three name suggestions and their confidence scores from our model to select the best match. If none of the top three names are suitable, verifiers examine the entire candidate list. In case of disagreements, verifiers discuss and decide collectively. Note that human verification is used only for evaluation sets, while automatically generated names are used for the much larger training sets. Future work could explore using multiple vision-language foundation models to automate this verification process as evaluation sets grow [41; 42; 43].

## 5 Experiments

### Obtaining renovated names

Setup.We renovate three panoptic segmentation datasets respectively: MS COCO , ADE20K , and Cityscapes . To generate context names, our default image captioning model is CaSED , which retrieves captions by matching vision embeddings with text embeddings of captions from a large-scale PMD dataset . We train a renaming model for 60k iterations with a batch size of 16 on the training set, then generate RENOVATE names for the entire dataset.

Results.Table 2 summarizes the statistics of RENOVATE names. Compared to the original class names, each dataset has approximately 5-6 times more distinct RENOVATE names, indicating that they provide more diverse and fine-grained semantic annotations to the visual segments. We show some visual examples in Fig. 4 and more in Appendix E.1. To further validate the quality of the names, we conduct a human preference study, see Appendix B.1. We also conduct an ablation analysis on the components of the renaming pipeline, see Appendix B.2.

### Training with renovated names

Setup.We train FC-CLIP  models on MS COCO with our renovated segment-matched names and compare them with models trained using other name sets on COCO, ADE20K, and Cityscapes. Specifically, "OpenSeg" names  were manually curated by previous researchers. "Synonym names" are generated by prompting GPT-4 based on the original class names. "Candidate names" are the outputs of the initial step in our renaming process, using image captioning to provide context for prompting GPT-4. A key difference between these names and RENOVATE names is that the former uses a set of names for a class of segments, whereas RENOVATE names provide a name for each individual segment. For model evaluation, we follow prior practices [5; 24; 25] to group

   & COCO & ADE & CS \\  \# Original classes & 133 & 150 & 19 \\ \# Segments/Class & 11,274 & 2,009 & 4,964 \\  \# RENOVATE names & 741 & 578 & 108 \\ \# Segments/Name & 1,871 & 521 & 873 \\  

Table 2: **Statistics of renovated datasets.**predictions based on the original class divisions. For a fair comparison, our test name set merges names from Original, OpenSeg, and RENOVATE names.

**Results.** Our results in Table 3 demonstrate that using RENOVATE names for training improves segmentation on both source dataset and target datasets. For example, compared to the Original names, the model trained with RENOVATE names improves by near 4 PQ on MS COCO and over 5% mIoU on ADE20K, showing significantly larger gains offered by OpenSeg names. This underscores the value of name refinement on per-segment level and indicates the high quality of RENOVATE names. We also note that models trained with Synonym names and Candidate names show inferior performance even to the Original names, reflecting the importance of high name quality.

The richer text information offered by RENOVATE names also improves training efficiency. In Fig. 5, we show that models trained with RENOVATE names can achieve comparable performance with significantly less data compared to those trained with the Original names. For instance, models trained with \(10^{4}\) images using RENOVATE names can match or exceed the performance of models trained with \(5 10^{4}\) images using the Original names. This corroborates previous findings that improving data quality can reduce the required quantity of training data [45; 46].

**Ablation on the negative sampling.** In Fig. 6, we study the effectiveness of negative sampling. Interestingly, we found that negative sampling is especially helpful to RENOVATE names while minorly impacting models trained with original names. This is because RENOVATE names have much more class names and the names are also more semantically close. Negative sampling effectively controls the number of classes used in cross entropy calculation and keeps the name embeddings relatively distinct, thus significantly improving training effectiveness for RENOVATE names.

### Improving evaluation with renovated names

**Setup.** We evaluate pre-trained FC-CLIP , MasQCLIP , and ODISE  models on ADE20K and Cityscapes using three different name sets with both standard and open metrics. For open metric computation, we need the similarity between the ground-truth and predicted class names. For Original and OpenSeg names, which use a set of names for a class of segments, we use the averaged pairwise name similarity between classes. For RENOVATE names, we can directly use

    &  &  \\   &  &  &  \\ 
**Train names** & PQ & AP & mIoU & PQ & AP & mIoU & PQ & AP & mIoU \\  Original names & 52.70 & 42.72 & 60.91 & 25.61 & 15.97 & 33.06 & 46.06 & 23.67 & 57.99 \\ OpenSeg names & 53.60 & 43.87 & 60.56 & 27.14 & 17.41 & 35.63 & 45.34 & 24.05 & 58.42 \\  Synonym names & 23.38 & 35.87 & 47.20 & 15.56 & 14.78 & 29.74 & 22.94 & 19.58 & 45.91 \\ Candidate names & 41.32 & 40.96 & 56.68 & 19.80 & 15.85 & 33.43 & 39.20 & 20.79 & 51.80 \\ RENOVATE & **56.62** & **45.50** & **65.48** & **27.98** & **17.94** & **38.50** & **46.10** & **27.99** & **61.25** \\   

Table 3: **Training with renovated names.** During inference, test names are merged from Original, OpenSeg, and RENOVATE names for fair comparison. Our results demonstrate that RENOVATE names can help train stronger open-vocabulary models.

the pairwise name similarity between renovated names. Fig. 7 shows PQ evaluation of pre-trained models on ADE20K. Full evaluation results, including mAP and mIoU and results on Cityscapes are provided in Appendix B.3.

**Results.** In Fig. 7, we first note that the discrepancy between standard and open metrics is significantly greater when evaluated using RENOVATE names. This indicates that many misclassification cases are indeed "benign", _i.e._, when evaluated using more fine-grained names, the misclassified names turned out to be semantically close to the ground-truth names. For example, when an "area rug" is predicted to be a "floor rug" (RENOVATE names), it will be penalized much less than from "rug" to "floor" (original names); but if an "area rug" is predicted as "concrete floor", the classification error will be even higher. This shows that current pre-trained models have learned relatively good semantic concepts, while still being sensitive to the text prompts at inference time.

RENOVATE names also enable a more fine-grained analysis of the models. In Fig. 7(a), we present a per-category IoU analysis on RENOVATE names within the ADE20K dataset. We can see that large, well-defined objects such as "sky" and "girl" have high IoU scores, while small, deformable, and rare classes like "boldard" and "ceiling light" are more challenging for current models. Note that most of these names do not exist in the original benchmarks. In addition, our analysis reveals model biases, evidenced by the disparity in IoU scores between "man" and "woman" as well as "girl" and "boy", suggesting an imbalance in the training dataset. In Fig. 7(b), we further illustrate how RENOVATE names facilitate a more detailed investigation of misclassifications. For example, among "building" segments misclassified as "wall", 32.7% "churches" from the "building" class are predicted to be "brick wall" from the "wall" class. Additionally, we observe a strong correlation between name similarity and misclassifications. This showcases that our RENOVATE names can help identify problematic sub-categories within the original class that models still struggle with.

Finally, we note that the open metrics are penalizing more on misclassified concepts with lower semantic similarity, e.g., "car/road" mistakes are penalized more than"car/truck" mistakes. In other use scenarios, different types of mistakes (e.g., outlier patterns) may be preferred to be penalized more and the evaluation metrics should be designed accordingly. However, regardless of the design of the evaluation metric, RENOVATE names make it possible to conduct fine-grained analysis on mistakes with different semantic distances. Without RENOVATE names, we can only see the coarse misclassifications without a fine-grained understanding of the model.

Figure 8: **RENOVATE names enable more fine-grained analysis on models.** (a) Per-category IoU with highlighted top/bottom-5 RENOVATE names and selected names from the “person” class in ADE20K. (b) Confusion matrix of frequently misclassified RENOVATE names from “building” to “wall”, showing misclassification proportions (numbers) and pairwise name similarity (color).

Figure 7: **Open-vocabulary evaluation on ADE20K with different names, metrics, and models.**

Conclusion and Limitations

In this work, we address the naming issues and show that renaming improves both model training and evaluation of open-vocabulary segmentation. While RENOVATE uncovers model biases, it could inadvertently propagate biases from the foundational models into the new names. To mitigate potential negative societal impacts, we advocate for verification of names in critical applications, as exemplified by our verification of names in evaluation sets. As the first attempt to propose a generalized renaming framework, we acknowledge that our exploration remains incomplete. In the future, we aim to further refine our method, exploring more design choices such as other model backbones , and scale it up to the publicly available large-scale datasets .