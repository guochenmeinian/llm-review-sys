# EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals

Xuan-Hao Liu\({}^{1}\)

Xuan-Hao Liu and Yan-Kai Liu contribute equally.

Yan-Kai Liu\({}^{1}\)1

Yansen Wang\({}^{2}\)2

Kan Ren\({}^{3}\)3

Hanwen Shi\({}^{1}\)

Zilong Wang\({}^{2}\)

Dongsheng Li\({}^{2}\)

Bao-Liang Lu\({}^{1}\)

Wei-Long Zheng\({}^{1}\)4

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Microsoft Research Asia \({}^{3}\)ShanghaiTech University

https://bcmi.sjtu.edu.cn/home/eeg2video

###### Abstract

Our visual experience in daily life are dominated by dynamic change. Decoding such dynamic information from brain activity can enhance the understanding of the brain's visual processing system. However, previous studies predominately focus on reconstructing static visual stimuli. In this paper, we explore to decode dynamic visual perception from electroencephalography (EEG), a neuroimaging technique able to record brain activity with high temporal resolution (1000 Hz) for capturing rapid changes in brains. Our contributions are threefold: Firstly, we develop a large dataset recording signals from 20 subjects while they were watching 1400 dynamic video clips of 40 concepts. This dataset fills the gap in the lack of EEG-video pairs. Secondly, we annotate each video clip to investigate the potential for decoding some specific meta information (_e.g._, color, dynamic, human or not) from EEG. Thirdly, we propose a novel baseline **EEG2Video** for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture. EEG2Video achieves a 2-way accuracy of 79.8% in semantic classification tasks and 0.256 in structural similarity index (SSIM). Overall, our works takes an important step towards decoding dynamic visual perception from EEG signals.

## 1 Introduction

Our visual experience are composed of continuously evolving scenes caused by the movement of objects and viewing perspective . The intricate and complex visual system in our brains which enables us to explore the wonderful and ever-changing visual world has been appealing interests from philosophers and scientists for centuries [2; 3; 4; 5; 6]. To investigate the mechanism of our visual system, various neuroimaging techniques have been used to analyze brain activities, especially non-invasive methods like functional Magnetic Resonance Imaging (fMRI) [2; 7; 8], magnetoencephalography(MEG) [9; 10; 11], and electroencephalography(EEG) [12; 13].

Compared to fMRI and MEG which need to be recorded by large and expensive medical devices, EEG is relatively low-cost and portable and thus has been applied across many human visual studies [14; 15; 16; 17; 18]. For instance, a recent work achieved an accuracy of 15.6% in 200-way zero-shot tasks on an EEG-image dataset , demonstrating the rich visual information in EEG signals. However, these studies adopt either artificial strobe (SSVEP) [14; 15] or static image [16; 17; 18] as stimulation, which is far different from the dynamic visual world and not suitable for studying brain activities in naturalistic paradigm. As far as we know, there is currently no research on decoding video from EEGsignals. Consequently, people have limited knowledge about **1)** whether can we decode video from EEG signals? **2)** if yes, what kind of visual information can we decode?

In order to fill the gap, we develop a large EEG dataset, called **SJTU EEG** Dataset for **D**ynamic **Vision** (SEED-DV) dataset, collected from 20 subjects while they were watching a series of natural videos belonging to 40 different concepts. Also, we annotate some meta information for each video clip to comprehensively explore the boundary of which visual information can be decoded from EEG signals, offering a benchmark containing various visual decoding tasks across object recognition, color/motion perception, and human/face detection.

Besides the fundamental classification tasks, reconstructing visual perceptions from corresponding brain signals helps to advance the understanding of our visual neural system. With the development of the representation learning and artificial intelligence generated content (AIGC), numerous works have reconstructed vivid images from brain activities [20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30], which utilize the text-to-image generation models pretrained on large amounts of visual-language pairs by aligning the brain signals with corresponding text embeddings. Recently, some works reconstruct high-quality two-second videos from a single fMRI data frame [31; 32]. However, limited by the low temporal resolution of fMRI, these video generation frameworks lack the ability of capturing high dynamic changes.

To this end, we propose a novel baseline, EEG2Video, for video reconstruction from EEG signals based on Seq2Seq architectures which extract continuous low-level dynamic visual perception such as color and position from the brain signals of high temporal resolution. Afterwards, a dynamic-aware noise-adding (DANA) method is adopted for the diffusion process according to the dynamic information decoded from EEG. At last, we adopt the inflated diffusion model  fine-tuned on our dataset for video generation using the semantic information predicted from EEG. Our method densely extract visual information from high temporal resolution brain signals thus can better recover fast changes. Overall, our video reconstruction results take an important step towards decoding dynamic visual perception from EEG.

In conclusion, our contribution are as follows:

* For the first time, we develop a large EEG dataset named SEED-DV dataset collected from 20 subjects, offering 1400 EEG-video pairs from 40 concepts for studying dynamic visual information in EEG signals.
* We annotate the meta information of each video clip for comprehensively analyzing the visual information in EEG, presenting the EEG-VP benchmark.
* We evaluate various EEG models on the EEG-VP benchmark to determine the decoding ability of different visual information in raw EEG signals and human-extracted features.
* We propose a novel framework named EEG2Video for video reconstruction from EEG signals based on Seq2Seq architecture to densely utilize the highly dynamic information.
* The ablation study showcases the effectiveness of Seq2Seq and DANA modules in EEG2Video, which are designed based on the decoding results of different visual information on the EEG-VP benchmark.

## 2 Related Work

### Decoding Static Visual Perception from Brain Activities

Researchers have been trying to decode low-level static visual perception (e.g., shape, color, and position) from brain activities for decades [34; 35; 36; 37], revealing the abundant visual information hidden in brain signals. Early approaches tried to generate hazy silhouette or higher-quality images with Deconvolutional Neural Networks (DeCNNs)[38; 39], Generative Adversarial Networks (GANs)[40; 41; 42; 43; 44], and Variational Autoencoders (VAEs) . Recent studies have obtained impressive results on decoding static visual stimuli from various brain activities like fMRI [20; 21; 22; 23; 24; 25; 26; 27], MEG , and EEG [29; 30] with the Latent Diffusion Models (LDMs), also named Stable Diffusion (SD) [45; 46; 47], as image generation module, which is pretrained on a large-scale image dataset to generate vivid images based on text prompts. Specifically, text prompts are embedded into a text-image sharing latent space by CLIP , a multi-modal encoder to align visual and language representations. By encoding brain signals into the same sharing space, these methods can decode static visual perception with high diversity and fidelity using LDMs.

Compared to the high spatial resolutions of brain signals recorded by large devices like fMRI (\(\) 100,000 voxels) and MEG (\(\) 300 sensors), the limited spatial resolution of EEG (\(\) 60 sensors) brings difficulties to decode accurate visual perception from EEG in both semantic and pixel levels. Although previous studies claimed to achieve over 60% of semantic decoding accuracy on an EEG dataset with 40 classes [29; 30], the dataset was blamed due to the block design . The follow-up rigorous experiment  by randomly arranging all images showed at most 7.0% classification accuracy of the EEG signals (chance level is 2.5%), exposing the fact that it is still challenging and insufficiently supported by appropriate datasets to decode visual perception from EEG.

### Decoding Dynamic Visual Perception from Brain Activities

Observing the great success in decoding static visual perception, many endeavours have been devoted to decoding dynamic visual perception from fMRI [52; 53; 31; 32; 31]. These works were conducted on an fMRI-video dataset collected from 3 female subjects while watching a series of videos, including animals, humans, and natural scenery . Due to the data sparsity, the DeCNN, VAE and GAN-based methods can only decode hazy perception of dynamic videos from fMRI [52; 53; 54]. To generate high-quality videos, MinD-Video and NeuroCine utilized an inflated SD model  for video generation, which incorporates network temporal inflation by adding temporal attention modules in the original SD to ensure the consistency between frames. The video SD is firstly fine-tuned using the text-video pairs in the training dataset (the text prompts were generated by an automatic image caption model called BLIP ), then the pre-trained fMRI encoder were co-trained with the video SD to enhance the fMRI guidance.

However, fMRI intrinsically lacks the ability of capturing dynamic visual perception due to the low temporal resolution, which is limited by the time scale of blood flow and results in a single fMRI frame every two seconds. MinD-Video and NeuroCine all decoded two-second videos from only one fMRI frame. Thus, they are unable to decode changes faster than 0.5 Hz. Instead, other neuroimaging techniques such as EEG with high temporal resolution up to thousands Hz can offer more appropriate alternatives. To the best of our knowledge, there is no such dataset studying decoding video stimuli from such signals, and we are the first to support this research direction with the dataset, benchmarks, and the decoding framework.

## 3 EEG Dynamic Vision Dataset and Benchmarks

In this section, we first describe how we construct the SJTU **EEG** Dataset for **D**ynamic **V**ision (SEED-DV). Then we introduce two benchmarks built on the SEED-DV dataset: EEG visual perception classification benchmark and video reconstruction benchmark. The purpose of building this new dataset is to answer the following research questions:

**RQ1**: : Whether can we decode dynamic visual information from EEG signals?
**RQ2**: : If yes, which visual information can be decoded?
**RQ3**: : To what extent can we reconstruct video from EEG signals?

Hence, we carefully selected video clips suitable for studying dynamic vision and annotated their meta information.

### Participants

Twenty healthy students from Shanghai Jiao Tong University (SJTU) participated (mean age: 21.75 STD: 2.05; 10 females, 10 males), all having normal or corrected-to-normal vision. All subjects were informed of the experimental process and signed informed consent forms before the experiment, then received monetary reimbursement after finishing. This study was approved by the ethical committee of SJTU Institutional Review Board for Human Research Protections.

### Visual Stimuli Selection

We elaborately selected 40 concepts of videos in our experiment to study. The number of concepts follows previous research on EEG-image pairs [51; 49]. It is worth noting that EEG-Things dataset (with 1854 object concepts) employed a rapid serial visual presentation (RSVP) paradigm [56; 57], which cannot be used for EEG-video experiment as video is a continuous stimulation. We further cluster the concepts into 9 coarser classes, as is demonstrated in Figure 1(A): {_land animal, water animal, plant, exercise, human, natural scene, food, musical instrument, transportation_}.

All video clips are collected from two online video websites, Bilibili1 and YouTube2. We selected 35 different two-second video clips for each concept, which are divided randomly into 7 groups, each group has 5 video clips. Afterwards, 40 groups of different concepts are arranged sequentially to form a single block, each block containing \(40 5=200\) video clips. Consequently, there are 7 blocks in our experiment, where the order of 40 categories of videos within each block is random and different from each other to mitigate the temporal bias of EEG signals.

### Experiment Protocol

To ensure the quality of the acquired EEG data, the experiments were conducted in a controlled laboratory environment to minimize noise and other environmental disturbances. 62-channel EEG signals were collected by an active AgCl electrode cap with an international 10-10 system. The EEG signals were acquired using the ESI NeuroScan System at a sampling rate of 1000 Hz. Besides EEG data, EOG and ECG signals were recorded simultaneously during the experiment. We also adopt a Tobii Pro Fusion eye tracker to collect eye movements at a sampling rate of 250Hz.

During the experiment, all subjects were instructed to watch a series of color video clips presented with the resolution of \(1980 1080\) (16:9) in full-screen mode on a 25-inch display. In each block, the 5 video clips with the same category were displayed continuously, and before playing these 5 same-class videos, there is a hint on the screen to inform the subjects what class they will see next, which will last for three seconds. Consequently, there are \(40(3+5 2)=520\) seconds, i.e., 8 min 40 s for each block.

There are 7 blocks of videos to watch in our experiment for a subject. After finishing watching a block (40 classes), the subject was required to rest for at least 30 seconds to mitigate fatigue and assesses his/her own attention level (ranging from 1 to 5. 1 means sleepy and 5 means very concentrating)

Figure 1: The meta information of video clips of 40 concepts, and experiment protocol. (A) Visualizations of the meta information for all video clips of 40 concepts, we plot the average of each meta information for each concept. (B) The data collecting environment. (C) Demonstration of a whole data collecting session. A session will contain 7 video blocks to be watched, and there are rest phases of at least 30 seconds each between blocks. (D) Demonstration of a video block, there is a 3-second hint before 5 different video clips of the same concept.

before starting the next block. As a result, the average attention level across all the subjects and blocks is \(4.01 0.83\), ensuring high-quality EEG signals with acceptable concentrations.

### Meta Information of Video Clips

Besides the 40 concepts and 9 coarser classes, we also labeled some other meta information for each video clip.

**Color**: The main color of the main object. There are 7 color categories: {_Red_, _Yellow_, _Blue_, _Green_, _White_, _Grey_, _Colorful_}. _Colorful_ indicates the color is too complex for identifying an accurate color from a single video clip.

**Optical Flow Score**: The optical flow score (OFS) of each 24 FPS two-second video clip obtained by averaging the length of the optical flow vectors, ranging from 0.008 (almost static) to 6.252 (rapidly changing). Further, based on the OFS, we divide all the video clips into 2 categories: {_Fast_, _Slow_}. We choose the median OFS of 1.799 as the threshold to make sure the label is balanced.

**Object Number**: The number of the main objects. There are 3 categories: {_One_, _Two_, _Many_}. _Many_ indicates the number of the main objects is equal to or more than three.

**Human**: If there are any humans appearing in the video, the label is \(1\), otherwise is \(0\).

**Human Face**: If there are any human faces appearing in the video, the label is \(1\), otherwise is \(0\).

We depict the average meta information of each concept in Figure 1(A):

### EEG Visual Perception Classification Benchmark

To better understand the visual information in EEG signals, some low-level and high-level visual perception tasks are investigated in our dataset, presenting an EEG Visual Perception (EEG-VP) benchmark. There are 7 EEG classification tasks based on the video label and meta information detailed in Section 3.4, whose statistics are presented in Figure 2:

* The 40-class classification of the fine-grained concept of the video clip.
* The 9-class classification of the course concept of the video clip.
* The 6-class classification of which color of the main object in the video clip. The data while watching _Colorful_ videos are discarded in this task.
* The binary classification of whether the video is _fast_ or _slow_ based on the OFS.
* The 3-class classification of the number of the main objects in the video clip.
* The binary classification of whether human appears in the video or not.
* The binary classification of whether any human face appears in the video or not. The data while watching videos without human appearance are discarded in this task.

### Video Reconstruction Benchmark

This task is to reconstruct two-second video clips from the corresponding EEG segments. Following the previous video reconstruction from fMRI studies [31; 32; 52; 53; 54], we utilize the same metrics to

Figure 2: Statistics of each meta information: (A) the fraction of human appearance. (B) the fraction of face appearance (only count the videos with humans). (C) the distribution of different object numbers. (D) the distribution of different object colors. (E) the histogram of OFS.

evaluate the quality of generated videos, roughly classified as frame-based metrics and video-based metrics. Definition of these metrics can be found in Appendix A.

## 4 Methodology

### EEG Encoder: Mixture of Global and Local Features.

EEG classifiers take into an EEG segment \(x^{C T}\), where \(\) denotes EEG channels and \(T\) denotes time samples, and decode the target information \(y\), which can be defined as \(f:\).

Previous studies on EEG classification always treat all channels equally [49; 58; 59]: \(f(x)=_{global}(x)\), where \(_{global}\) is a global encoder for decoding EEG. However, research in neuroscience indicates that human visual cortex are basically in the occipital lobe, as shown in Figure 3(A), treating all channels equally is unable to highlight the visual cortex features. To this end, we propose a simple yet effective network to combine the global features decoded from whole channels and local features decoded from the visual associated channels called Global Local Mixture Network (GLMNet). Depicted in Figure 3(B), GLMNet utilizes a local encoder \(_{local}\) to extract the vision-associated features, which can be denoted as \(f(x)=emb((_{global}(x),_{local}(x)))\).

### EEG2Video: High Temporal Resolution Brain Decoding Framework

Compared to the fMRI-video reconstruction framework [31; 32] which are constrained by the limited temporal resolution that results in decoding two-second dynamic videos from one fMRI data frame, EEG signals have higher temporal resolution for better capturing dynamic visual perception to facilitate video reconstruction. As demonstrated in Figure 3(C), we introduce **EEG2Video**, a novel framework which utilize a Seq2Seq model to densely reconstruct low-level visual perception from continuous EEG embeddings extracted by an overlapping sliding window, and decode the semantic

Figure 3: (A-B) GLMNet Encoders. (C-E) Overview of our EEG2Video framework. (A) The visual cortex, basically in the occipital lobe. (B) GLMNet architecture which combines the global and local embedding. (C)The framework of EEG2Video which predicts the latent variables \(_{0}\) and semantic guidance \(_{t}\) with Seq2Seq model and a predictor, respectively. A video diffusion model is then be adopted for generating videos using \(_{0}\) and \(_{t}\). (D) Dynamic-aware noise-adding process based on the decoded dynamic information \(\). (E) Using large amounts of video-text pairs to fine-tune the inflated diffusion UNet for video generation. The texts are obtained by BLIP.

and dynamic information with two other modules for guiding an inflated diffusion model to recreate videos.

**EEG Embeddings Extraction** For a two-second EEG segment \(x^{C T}\), we apply an overlapping sliding window for slicing it into shorter segments \(\{x_{1},x_{2},,x_{t}\}\), where \(t\) is the total number. EEG embeddings \(e_{eeg}=\{e_{e}^{1},e_{e}^{2},,e_{e}^{t}\}\) are then extracted from these shorter segments with an EEG encoder \(\) that \((x_{i})=e_{i}^{e},i=\{1,2,,t\}\).

**Seq2Seq Model** In contrast to video reconstruction from fMRI (generating several frames from a single data frame), it is essential to align the high temporal resolution brain signals with videos in video reconstruction from EEG (generating several frames from several EEG segments) for capturing rapid changes. The Seq2Seq models are naturally introduced for extracting the continuous visual information from high temporal resolution brain signals. We employ the Transformer architecture as the Seq2Seq model in our framework, which can be formulated as a stack of several blocks, each block containing a multi-head attention (MHA) layer and a feed-forward network (FFN) layer. Denoting the input of the \(i\)-th Transformer block as \(x_{ in}^{i}\), the calculation of the output \(x_{ out}^{i}\) is given by:

\[x_{ mid}^{i}=f_{ MHA}((x_{ in}^{i}))+x_{ in}^{i},\ \ x_{ out}^{i}=f_{ FFN}((x_{ mid}^{i}))+x_{ mid}^{i},\] (1)

where \(f_{ MHA}\) is the MHA layer, \(f_{ FFN}\) is the FFN layer, and \(\) is layer normalization. In our framework, the input of Transformer is the addition of EEG embeddings and position embeddings (PE): \(x_{ in}^{0}=e_{eeg}+PE\), and the output is the latent variables \(_{0}\) of the corresponding video frames. As depicted in Figure 3(E), the Ground Truth (GT) video frames are fed into the frozen VAE encoder to obtain the GT latent variables \(z_{0}\). We apply mean squared error (MSE) loss \(MSE(_{0},z_{0})\) for training the Seq2Seq model that densely predicts the continuous visual information of frames.

**Semantic Predictor** In order to utilize the pre-trained diffusion models for generating high-quality videos, we first generate the corresponding text description of each video by feeding the medium frame to a caption model called BLIP , then align the EEG signals with the text embeddings \(e_{t}^{77 768}\), which are acquired by the frozen CLIP text encoder as shown in Figure 3(E). An MLP is adopted as the semantic predictor to project EEG data into the same dimension to obtain \(_{t}\). Finally, the MSE loss \(MSE(_{t},e_{t})\) is employed for aligning EEG and text embeddings.

**Dynamic-Aware Noise-Adding Process** The frames in a video with high OFS are more diverse from each other than those in videos with low OFS. Based on the dynamic decoding results, we can roughly classify whether the video is high dynamic or not from EEG signals. Hence, we introduce the static noise \(_{s}\) and diverse noise \(_{d}\) into the diffusion process and balance them by the decoded dynamic information \(_{d}\). The diverse noise \(_{d}=\{_{d}^{1},_{d}^{2},,_{d}^{n}\}\) consists of \(n\) different noises, each \(_{d}^{i}(0,1)\). The static noise \(_{s}\) has the same noise \((0,1)\) replicated \(n\) times. The ratio of the static noise is smaller when the \(_{d}\) indicates the video is more dynamic. The diffusion process can be formulated as follows to acquire the noise \(z_{}\) at time steps \(\):

\[z_{}=_{}}_{0}+ {1-_{}}(_{s}+ _{d}),\] (2)

where \(_{}\) is the coefficient for adding noise directly to the noise at time steps \(\), \(\) is depended by the decoded dynamic information. Here, We set \(=0.2\) when the video is high dynamic and \(=0.3\) when the video is low dynamic.

**Video Diffusion Model** For reconstructing vivid videos from EEG signals, we utilize the Tune-A-Video technique which fine-tunes an inflated text-to-image diffusion model . The network inflation trick is adding a sparse temporal attention layer in the image generation model to ensure the consistency between frames, in which each frame is calculated with the the first frame and the frame before it. Using the same notations in , the attention is formulated as:

\[Q=W^{Q} z_{v_{i}},\ \ K=W^{K}[z_{v_{i-1}},z_{v_{1}}],\ \ V=W^{V}[z_{v_{i-1}},z_{v_{1}}],\] (3)

where \([]\) denotes concatenation operation, and \(z_{v_{i}}\) is the \(i\)-th frame. To fine-tune the video generation model in our framework, the corresponding text description of each video by feeding the medium frame to a caption model called BLIP . Afterwards, all video-text pairs in the training set are used for fine-tuning the Stable Diffusion Model V1-4 .

Experiment on SEED-DV Dataset

In this section, we evaluate the performance of our methods and other baselines on the proposed two benchmarks: the EEG-VP benchmark and the video reconstruction benchmark. The details of data pre-processing, model implementation and training can be found in Appendix B.

### EEG-VP Benchmark

#### 5.1.1 Quantitative Results

We present the overall accuracy of different EEG classifiers in Table 1. Besides raw EEG Signals, we also run experiments on the PSD features and the DE features  of 5 frequency bands.

From the result, we can see that (1) our GLMNet outperformed the baselines consistently across all the classification tasks, which indicates the importance of extracting vision-associated features for visual perception tasks. (2) Different types of EEG features yield similar results. (3) Different tasks have different difficulties. Some meta information are distinguishable via EEG signals, e.g., _color_, _dynamics_, while the statistical significance indicates that the number of main object and whether _human/face_ appear are difficult or even impossible to be classified. The difference may be attributed to different processing mechanism by our brains which can inspire research in neuroscience.

To conclude, we can now answer **RQ1** and **RQ2**: Yes, some of visual information can be decoded from EEG signals, including _category_, _color_, _dynamics_. However, the overall visual perception benchmark is rather challenging and calls for more advanced algorithms. Refer to Appendix C for the confusion matrix and analysis across subjects.

#### 5.1.2 Analysis of Brain Areas

To find electrodes or brain areas most associated with dynamic visual perception, we conduct a one-channel classification task to test the classification quality of each electrode. Due to the reason that only one channel is used, we simplify the task to binary classification: Human/Animal and Fast/Slow tasks, which is related to object recognition and dynamic perception, respectively. It can be observed from Figure 4(A) that the electrodes in the occipital area have higher accuracy on Human/Animal tasks, demonstrating the object recognition are related to the occipital area where the visual cortex is located, presenting a similar result as previous works [64; 19; 30]. However, not all dynamic visual perception are in the occipital region. Figure 4(B) reveals that the brain area associ

  Methods & 40\(\) top-1 & 40\(\) top-5 & 9\(\) top-1 & 9\(\) top-3 & Color & Fast/Slow & Numbers & Human Face & Human \\  Change level & 2.50 & 12.50 & 11.11 & 33.33 & 20.57 & 50.00 & 65.64 & 62.25 & 71.43 \\   \\  ShallowNet(62) & 5.59.27\({}^{*}\) & 16.934.65 & 21.40\(\)19.36 & 49.62\(\)23.42 & 77.02\(\)0.09\({}^{*}\) & 56.62\(\)71.74 & 66.15\(\)0.89 & 64.87\(\)51.54 & 73.21\(\)1.52 \\ DeepNet(62) & 4.64\(\)0.85\({}^{*}\) & 14.30\(\)25.25 & 20.27\(\)17.25 & 40.60\(\)5.91 & 26.37\(\)19.52 & 55.42\(\)0.95 & 55.71\(\)0.12 & 61.58\(\)5.93 & 72.26\(\)0.40 \\ EEGNet(58) & 4.64\(\)0.85\({}^{*}\) & 14.25\(\)0.87 & 19.63\(\)0.81 & 47.01\(\)4.52 & 26.46\(\)13.17 & 5.19\(\)0.20 & 64.67\(\)0.70 & 63.11\(\)3.17 & 23.80\(\)0.98 \\ Conformer(59) & 4.93\(\)0.57\({}^{*}\) & 15.36\(\)0.44 & 20.92\(\)0.98 & 49.25\(\)15.49 & **27.53\(\)13.7** & 55.02\(\)0.83 & 65.73\(\)0.26 & 64.96\(\)1.14 & 73.00\(\)0.85 \\ TSConv (19) & 4.92\(\)0.93\({}^{*}\) & 15.05\(\)1.31 & 20.01\(\)0.17 & 47.76\(\)1.51 & 26.89\(\)1.83 & 55.32\(\)0.99 & 65.39\(\)0.41 & 64.39\(\)1.74 & 27.68\(\)0.67 \\ GL_MNet (Ours) & **6.20\(\)0.02** & **17.75\(\)2.44** & **21.93\(\)1.87** & **50.01\(\)5.22** & 27.37\(\)1.45 & **57.35\(\)19.5** & 66.20\(\)19.14 & 63.81\(\)0.14 & 73.34\(\)**31.3** \\  SVM(63) & 5.19\(\)2.81 & & 19.02\(\)3.27\({}^{*}\) & 21.31\(\)0.97 & 53.36\(\)1.11\({}^{*}\) & 64.15\(\)12.22 & 58.94\(\)22.12 & 70.91\(\)3.84 \\ MLP & 6.20\(\)3.02\({}^{*}\) & 18.91\(\)5.94 & 21.59\(\)3.00\({}^{*}\) & 49.65\(\)7.38 & 22.02\(\)0.27 & 55.15\(\)1.20\({}^{*}\) & 64.48\(\)0.92 & 63.94\(\)1.13 & 71.74\(\)1.76 \\ GL_MNet (Ours) & **6.23\(\)2.91\({}^{*}\)** & **18.98\(\)6.22** & **12.69\(\)3.20** & **50.84\(\)10.26** & **26.40\(\)29.9** & **55.421\(\)32.2** & **64.68\(\)0.92** & **64.22\(\)1.43** & **72.77\(\)1.57** \\   SVM(63) & 4.87\(\)3.00\({}^{*}\) & - & 19.03\(\)0.39\({}^{*}\) & - & 21.07\(\)28.88 & 53.34\(\)27.58 & 63.62\(\)17.75 & 57.82\(\)3.00 & 70.25\(\)1.94 \\ MLP & 6.12\(\)0.38\({}^{*}\) & 19.02\(\)5.71\({}^{*}\) & - & 21.17\(\)3.24 & - & 25.91\(\)37.24 & 54.76\(\)12.5\({}^{*}\) & 64.07\(\)0.70 & 63.41\(\)5.17 & 71.34\(\)1.76 \\ GL_MNet (Ours) & **6.16\(\)3.18** & **19.12\(\)6.07** & **21.34\(\)3.34** & **49.55\(\)5.77** & **26.15\(\)32.4** & **56.06\(\)10.20** & **64.25\(\)0.74** & **63.63\(\)1.80** & **72.27\(\)1.58** \\  

Table 1: Average classification accuracy (%) and std across all subjects with different EEG classifiers on different tasks. Change level is the the largest class. The star symbol (\({}^{*}\)) represents the result is above chance level with statistical significance (two-sample t-test: \(p<0.05\)).

Figure 4: Spatial Analysis. (A-B). Topographies of each electrode’s accuracy for Human/Animal and Fast/Slow tasks. (C). Ablate electrodes of different brain regions.

[MISSING_PAGE_FAIL:9]

## 6 Conclusion

In this paper, we developed the large dataset, SEED-DV, to reconstruct videos from EEG signals, upon which we built the EEG Visual Perception Classification benchmark and the Video Reconstruction benchmark to support evaluating the advances of EEG-based video reconstruction. Moreover, we proposed a novel baseline EEG2Video for video reconstruction from EEG signals that can align visual dynamics with EEG based on the Seq2Seq architecture, and we presented vivid generated examples by training our framework on SEED-DV.

As the first attempt, we open a new possibility for BCI researchers to decode dynamic visual perception from EEG signals. Although the overall performance on SEED-DV are still in a preliminary stage, we hold the strong belief that the game-changing results for the BCI area can soon be discovered.

## 7 Broader Impacts

Reconstructing dynamic visual perception from brain activities helps to advance the understanding of our visual system in brains. EEG is a physiological signal widely used in clinical practice and brain-computer interfaces. Compared to non-portable and expensive neuroimaging techniques like fMRI and MEG, our work provides a convenient and cheap solution for decoding visual information from brain activities. This technique can be used for visualize our mind, offering a novel approach for listening the inner world of people patients with mental illnesses like autistic and depression.

However, every coin has two sides. Personal privacy may leak through our brain activities and be abused by malicious attackers for reading one's mind from EEG signals without acknowledgment. More strict regulations are supposed to be made for protecting the privacy of people's biological data by government and medical institutions.

## 8 Limitations

Our SEED-DV dataset currently records the EEG signals of each subject with one session, leading to the requirement for collecting more EEG-Video pairs with more sessions, which is significant for studying the stable neural patterns over time. Our framework is evaluated under the subject-dependent settings, the cross-subject ability remains unexplored due to individual variations. Future work can be focused on the transferability and stability of the video reconstruction framework by exploiting generalizable EEG encoders and Seq2Seq model.

Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., _Mountain_, _Beach_, _Face_) and high dynamics (e.g., _Skiing_, _Fireworks_, _Dancing_) across animals, scenes, persons, and activities can be correctly recovered.