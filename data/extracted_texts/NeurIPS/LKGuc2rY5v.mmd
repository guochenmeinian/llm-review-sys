# Efficient and Sharp Off-Policy Evaluation

in Robust Markov Decision Processes

 Andrew Bennett\({}^{*}\)

Morgan Stanley

andrew.bennett@morganstanley.com

&Nathan Kallus\({}^{*}\)

Cornell University

kallus@cornell.edu

&Miruna Oprescu\({}^{*}\)

Cornell University

amo78@cornell.edu

&Wen Sun\({}^{*}\)

Cornell University

ws455@cornell.edu

&Kaiwen Wang\({}^{*}\)

Cornell University

kw437@cornell.edu

Alphabetical order.

###### Abstract

We study the evaluation of a policy under best- and worst-case perturbations to a Markov decision process (MDP), using transition observations from the original MDP, whether they are generated under the same or a different policy. This is an important problem when there is the possibility of a shift between historical and future environments, _e.g._ due to unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that allows changes in the transition kernel densities up to a given multiplicative factor or its reciprocal, extending the classic marginal sensitivity model (MSM) for single time-step decision-making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model - _i.e._, the tightest possible bounds based on transition observations from the original MDP - and we study the estimation of these bounds from such transition observations. We develop an estimator with several important guarantees: it is semiparametrically efficient, and remains so even when certain necessary nuisance functions, such as worst-case Q-functions, are estimated at slow, nonparametric rates. Our estimator is also asymptotically normal, enabling straightforward statistical inference using Wald confidence intervals. Moreover, when certain nuisances are estimated inconsistently, the estimator still provides valid, albeit possibly not sharp, bounds on the policy value. We validate these properties in numerical simulations. The combination of accounting for environment shifts from train to test (robustness), being insensitive to nuisance-function estimation (orthogonality), and addressing the challenge of learning from finite samples (inference) together leads to credible and reliable policy evaluation.

## 1 Introduction

Offline policy evaluation (OPE) from historical data is crucial in domains where active, on-policy experimentation is costly, risky, unethical, or otherwise operationally infeasible. Relevant domains range from medicine to finance and recommendation systems. However, whenever historical data is used to study future behavior, there is a concern of non-stationarity - shift between the environment generating the data (training environment) and the environment in which a policy will be deployed (test environment). This may occur, _e.g._, due to general distributional shifts in the environment over time, unobserved confounding in the observed historical data, or adversarial elements of the environment (such as other agents) that may react when the agent is deployed. While standard OPE in offline reinforcement learning (ORL) accounts for the change between the logging and evaluation policies, it may overlook the fact that the Markov decision process (MDP) too has changed. Whilethis issue is particularly critical in high-stakes domains, it is broadly appealing to understand how value shifts across different environments in any application domain.

Robust MDPs  model unknown environments by allowing an adversary to choose from any one environment in a set. Therefore, they offer a natural model for unknown environment shifts by simply considering all environments to which we could possibly shift. A variety of work addresses questions such as planning in a known robust MDP  as well as online learning . Here we focus on a purely statistical estimation question: given observations of transitions from some unknown transition kernel, we wish to estimate the worst-case (or best-case) value of a given evaluation policy in a robust MDP, defined by a set of MDPs whose transition functions are centered around the observed transition kernel.

This setting captures the previously studied unconfounded robust OPE problem , where the observed transition kernel corresponds to an MDP, and the observed transitions are the result of applying some logging policy within this MDP. In such cases, the goal is to estimate policy values that are robust to future changes in the MDP dynamics. However, our setting is more general in that it also captures problems where the observed transitions are confounded by some unobserved variables, in which case they do _not_ correspond to observations from the transition kernel of an MDP. In this case, the robust MDP and the robust policy value estimates are designed to account for worst-case (or best-case) impact of this confounding bias. In either case, as in ORL, we emphasize that we do _not_ know the observational MDP, and can only access it via a sample of transitions. Furthermore, even in the simple case with no unmeasured confounding, in a notable departure from standard ORL, the problem can be difficult even if the logging and evaluation policies are the same (the usually easy on-policy setting), since the policy can induce very different visitation distributions in the original and perturbed MDPs.

Such robust offline evaluation from transition data was considered in recent work . We build on this recent work by focusing the question of statistically _efficient_ and _robust_ estimation of the _sharp_ bounds (_i.e._, the tightest possible given the data). Previous work focused on evaluation using only the Q-function under the worst-case environment (in some cases under a relaxation of the adversary, leading to loose bounds). Thus, any error in its estimation translates directly to error in evaluation. In other words, flexible nonparametric modeling of this function can mean slow rates for estimated bounds and a lack semiparametric efficiency. Moreover, without a clear understanding of the noise in the estimates, we cannot add confidence bands to the bounds, leading to bounds that are too tight.

We address these challenges by developing an orthogonalized estimation method that combines several nuisance functions: the worst-case \(Q\)-function, the state-visitation frequency in the worst-case environment, and a threshold function characterizing the worst-case transition kernel. Our first key result is that, to first order, our estimator behaves as a sample average using the true values of these functions without having to estimate them at all, provided we just estimate them at certain slow nonparametric rates. This ensures we not only have a \(\)-rate of estimation even when nuisances are estimated more slowly, but also that our estimator is asymptotically normal. This allows for the construction of confidence bands on the bounds, providing assurance that the true bound is captured. We further show that our asymptotic variance is in fact the minimum variance among all regular and asymptotically linear (RAL) estimators, ensuring semiparametric efficiency. Our second key result is that even if we do not estimate some of the nuisance functions correctly, we are still consistent to sharp or valid bounds. That is, even when we are biased due to misestimation of nuisances, our bias (if any) only enlarges our bounds, so they remain valid. We illustrate these guarantees numerically. Collectively, these guarantees lend substantial credibility to the bounds generated by our method.

Our contributions are summarized as follows:

1. We provide novel algorithms and analysis for learning robust \(Q\)-functions (Section3) and robust visitation density ratios (Section4) under the function approximation setting.
2. We derive the sharp and efficient estimator for the robust policy value, which is optimal in the local-minimax sense and is the gold standard in semiparametric estimation (Section5).
3. We empirically validate the efficiency and sharpness of our approach (Section6).

### Related Works

**Unobserved Confounding in Sequential Decision-Making.** OPE in robust MDPs is related to OPE bounds in confounded MDPs, where the behavior policy and the transition kernel are influenced by unobserved confounders. The constraint Eq. (1) that defines our target robust MDP aligns with the Marginal Sensitivity Model (MSM)  employed in sensitivity analysis for causal inference. Yet, unlike the MSM, which limits the ratio of policy densities, our approach directly constrains the ratio of the transition kernels. Our formulation can be viewed as a generalization of the MSM from traditional two-action no-horizon causal effects (where the constrains coincide) to multi-action infinite-horizon discounted MDPs, where the next state is the "potential outcome". In that sense, our model essentially serves as an outcome-based sensitivity model . This distinction is crucial as it enables our model to subsume the policy-based MSM in cases where the policy is confounded. Nonetheless, the reverse does not hold, and the policy-based MSM does not imply a transition kernel-based MSM for \(A>2\). This point is further corroborated by , who explore the policy-based MSM within confounded MDPs and obtain _non-sharp_ identification bounds when \(A>2\). In contrast, our approach yields _sharp_ identification in general, regardless of the number of actions and without placing assumptions on the behavior policy, which may or may not be confounded.

 also considered an MSM-like model in the transition kernel but their formulation assumes \(A=2\).  operates under the setting of  and required tabular states. We note that all these works including ours considers _i.i.d._ confounders at each step, which translates to a robust MDP with \((s,a)\)-rectangularity and ensures that the worst-case problem is still an MDP rather than a POMDP. The importance of this assumption was verified by , who showed that without it, the non-memoryless confounder can create exponential-in-horizon changes in value.

**Neyman Orthogonality and Semiparametric Efficient Estimation.** We leverage a body of research focusing on learning with nuisances functions (e.g., Q-functions) that we need to estimate from data but are not the primary target (e.g., policy value). Much of this research [7; 16; 17; 29; 64; 70], among others] aims to identify Neyman-orthogonal estimators, which are first order orthogonal (insensitive) to nuisance errors. This literature is tightly linked to the semiparametric efficient estimation literature since Neyman-orthogonal scores can arise naturally from efficient influence functions [33; 62]. Going beyond the no-horizon causal inference setting, some explore such estimators in off-policy sequential-decisions contexts [19; 38; 42; 48; 50]. Notably,  derive efficient influence functions and orthogonal estimation in standard, non-robust OPE in infinite-horizon RL, which coincides with our unconfounded no-uncertainty case (\(=1\)).

Moving beyond point-identified settings, some works explore orthogonality and efficiency for partial identification and sensitivity analysis. In the causal inference literature, efficient/orthogonal estimation in the no-horizon setting has been studied extensively under several sensitivity models [10; 18; 24; 58]. Closest to our work is  who provide an orthogonal estimator and convergence rates under the MSM , which coincides with our setting under \(=1\). In the sequential setting,  considers confounding at a single time step under the MSM, but their estimator is not orthogonal when the quantile function is unknown.  provide a fitted-Q-iteration learner with an orthogonalized loss function, but not orthogonal/efficient estimates of worst-case policy value.

## 2 Preliminaries

We consider an MDP with state space \(\), action space \(\), transition kernel \(P(s^{} s,a)\), reward function \(r(s,a)\) and initial state distribution \(d_{1}()\). We do not require \(\) or \(\) to be finite. We assume \(r\) and \(d_{1}\) are known for simplicity, and it is standard to extend our analysis to when they are unknown. We are given a dataset \(\) of \(n\)_i.i.d._ tuples \((s_{i},a_{i},r_{i},s^{}_{i})\) such that \((s_{i},a_{i})\), \(s^{}_{i} P( s,a)\) and \(r_{i}=r(s_{i},a_{i})\), where \(\) is an arbitrary data-generating distribution. For discount factor \([0,1)\), let the \(Q\) function be the discounted cumulative rewards under a policy \(:\), \(Q_{,P}(s,a)=_{,P}[_{t=0}^{}^{t}r_{t}(s_{t},a_{ t}) s_{1}=s,a_{1}=a]\). Similarly, define the value function as \(V_{,P}(s)=Q_{,P}(s,)\), where we use the notation \(f(s,):=_{a(s)}[f(s,a)]\) for any function \(f:\).

We are interested in estimating the value of a fixed target policy \(_{}\) (a.k.a. evaluation policy) in an unobserved MDP with a feasible perturbed transition kernel \(U\). We say \(U\) is a feasible perturbation of the observed, nominal kernel \(P\) if for all \(s,a,s^{}\): we have

\[^{-1}(s,a)U(s^{}|s,a)}{P(s^{ }|s,a)}(s,a)\] (1)

where \((s,a)[1,)\) is a sensitivity parameter chosen by the practitioner. On the extremes, \(=1\) corresponds to no-confounding (_i.e._, classic OPE setting) and \(=\) corresponds to maximal-confounding (_i.e._, worst or best outcome). We denote the set of all feasible perturbations of \(P\) by \((P)\), which is an \(s,a\)-rectangular set . We define the best- and worst-case \(Q\) functions of \(_{}\) as

\[Q^{+}(s,a):=_{U(P)}Q_{_{},U}(s,a); Q^{-} (s,a):=_{U(P)}Q_{_{},U}(s,a).\] (2)

Thus, the goal of this paper is to estimate the best- and worst-case value of \(_{}\) at the initial state,

\[V^{}_{d_{1}}:=(1-)_{s_{1} d_{1}}[V^{}(s_{1})].\] (3)

where \(V^{}(s)=_{a_{}(s)}[Q^{}(s,a)]\) and the \(\) symbol signals that an equation should be read twice, once with \(=+\) and once with \(=-\). For clarity, we focus the discussion in the main text on estimating the worst-case policy value, \(V^{-}_{d_{1}}\). We provide a similar analysis for policy values under best-case perturbations (\(V^{+}_{d_{1}}\)) in Appendix B.

Compared to standard OPE, robust OPE is more challenging since the best- and worst-case transition kernels \(U^{}\) are unobserved as our dataset \(\) is generated under \(P\). For example, standard OPE is easy in the on-policy case _i.e._, if \(\) were generated by \(_{}\), but our problem is still "off-data" and non-trivial.

**Discounted Visitation Distributions.** For any transition kernel \(U\), define the discounted visitation distribution of \(_{}\) under \(U\) as: \(d^{_{1},}_{d_{1},U}(s):=(1-)_{h=1}^{}^{h-1}d^{ _{h},h}_{d_{1},U}(s)\), where \(d^{_{h},h}_{d_{1},U}(s)\) is the probability of reaching state \(s\) in the Markov chain induced by \(U\) and policy \(_{}\) starting from \(d_{1}()\). We use \(d^{-,}\) as shorthand for \(d^{_{},}_{d_{1},U^{-}}\), where \(U^{-}\) denotes the worst-case kernel in \((P)\).

**Bellman-type Operators.** For any function \(f:\) and transition kernel \(U\), recall the Bellman operator is defined as \(_{U}f(s,a):=r(s,a)+_{U}[f(s^{},_{}) s,a]\). For robust OPE, we define the following robust analog \(^{+}_{}f(s,a):=r(s,a)+_{U(P)} _{U}[f(s^{},_{}) s,a]\) and \(^{-}_{}f(s,a):=r(s,a)+_{U(P)} _{U}[f(s^{},_{}) s,a]\). Moreover, we define \(_{U}f(s,a):=_{U}[f(s^{},_{})  s,a]-f(s,a)\). For any linear operator \(\), also let \(^{}\) denote its adjoint: that is, for all \(f,g L_{2}()\), \( f,g=^{}f,g\), where \(,\) is the inner product in \(L_{2}()\).

**Conditional Value-at Risk (CVaR).** For a random variable \(X\), its upper/lower CVaRs at level \(\) is defined as the average outcome of the upper/lower \(\)-fraction of cases, and are formally defined as follows :

\[^{+}_{}(X) :=_{b}\{b+^{-1}[(X-b)_{+}]\},\] \[^{-}_{}(X) :=_{b}\{b+^{-1}[(X-b)_{-}]\},\]

where \(y_{+}:=(0,y)\) and \(y_{-}:=(0,y)\) for \(y\). The optima are attained at the upper/lower \(\)-th quantile of \(X\) which we denote as \(^{+}_{}(X)/^{-}_{}(X)\), _i.e._,

\[^{+}_{}(X):=^{+}_{}(X)+^{-1}[(X- ^{+}_{}(X))_{+}],\ \ ^{-}_{}(X):=^{-}_{}(X)+^{-1}[(X-^{- }_{}(X))_{-}].\]

If \(X\) has a cumulative distribution function (CDF) which is differentiable at \(^{}_{}(X)\), its CVaRs simplify to \(^{+}_{}(X)=[X X^{+}_{}(X)]\) and \(^{-}_{}(X)=[X X^{-}_{}(X)]\). In the paper, \(\) will often be set to \((+1)^{-1}[0,0.5]\).

**Notations.** We use \(x y\) to mean that \(x Cy\) holds for some universal constant \(C\). The indicator function \([p]\) takes value \(1\) if \(p\) is true and \(0\) otherwise. For a measure \(\), we let \(\|f\|_{}:=(_{}|f(X)|^{2})^{1/2}\) denote the \(L_{2}\) norm of \(f\), provided it exists. When \(\) is clear from context, we also use \(\|f\|_{p}:=(|f(X)|^{p})^{1/p}\) to denote the \(L_{p}\) norm of \(f\) and \(\|f\|_{p,n}:=(_{n}|f(X)|^{p})^{1/p}\) to denote the empirical analog. For a data sample of size \(n\), we define the empirical mean as \(_{n}[f(X)]=_{i=1}^{n}f(x_{i})\). For a nuisance function \(f\), we reserve \(f^{*}\) as its true value and \(\) as the learned value from data. Moreover, we employ \(+\) and \(-\) to denote functions corresponding to best- and worst-case bounds, respectively. See Appendix A for a comprehensive notation table.

### Background: Non-robust OPE

We provide a quick primer on the double RL (DRL) estimator for classic OPE in non-robust MDPs , which combines estimates of the \(Q\)-function and density ratio \(w\) to achieve orthogonality, double robustness and semiparametric efficiency. This sets the stage for our orthogonal estimator in Section 5, which generalizes DRL to robust MDPs by incorporating the robust \(Q\)-function and density ratio in the worst-case MDP, as described in Section 3 and Section 4 respectively.

The DRL estimator involves two nuisances: (1) \(q\), for which the oracle (true value) is the \(Q\)-function of the target policy \(Q^{_{}}\), and (2) \(w\), for which the oracle is the density ratio of the target policy's visitation distribution and the data distribution \(w^{_{}}=d_{1,t}^{_{}}}}{{ }}/}}{{}}\). In this section, let \(=(w,q)\) denote the DRL nuisances (outside this section, we use \(\) to denote our robust estimator's nuisances) and let \(^{}=(w^{_{}},Q^{_{}})\) denote their true values, then the recentered efficient influence function (EIF) of \(V_{d_{1}}^{_{}}\) in non-robust MDPs is given by:

\[^{}(s,a,s^{};w,q)=V_{d_{1}}^{_{}}+w(s,a) (r(s,a)+ q(s^{},_{})-q(s,a)).\]

The DRL estimator uses cross-fitting to learn nuisances \(^{[k]}\) on all data excluding the \(k\)-th fold \(^{k}\), for \(k=1,2,,K\) and estimates the OPE value via:

\[_{d_{1}}^{}=_{k=1}^{K}_{(s,a,s^{ })^{k}}^{}(s,a,s^{};^ {[k]}).\]

As we will see, this paves the way for the EIF of the robust value (Theorem 5.1) and our orthogonal estimator (Algorithm 3). There are two main guarantees for DRL: double robustness and semiparametric efficiency. Let \(r_{n}^{u}\) and \(r_{n}^{q}\) be rate functions depending on \(n=||\) such that \(\|^{[k]}-Q^{_{}}\|_{2} r_{n}^{q}\) and \(\|^{[k]}-w^{_{}}\|_{2} r_{n}^{w}\). Then, DRL enjoys \(|_{d_{1}}^{}-V_{d_{1}}^{_{}}| O_{p}(n^{ 1/2}+r_{n}^{w}r_{n}^{q})\), which confers the algorithm double robustness properties. Moreover, if \(^{}\) is the efficiency bound (_i.e._, minimum achievable asymptotic variance among RAL estimators in nonparametric models for \((s,a,s^{})\)), then \((_{d_{1}}^{}-V_{d_{1}}^{_{}}) }{{}}(0,^{})\). We seek similar guarantees for our orthogonal robust estimator.

## 3 Robust \(Q\)-Function Estimation with Fitted-\(Q\) Evaluation

In this section, we identify the robust \(Q\)-function using the robust Bellman equation and then derive convergence rates for iteratively minimizing the robust Bellman error.

### Identification of the worst-case \(Q\)-function

The robust worst-case \(Q\)-function of \(_{}\), denoted as \(Q^{-}\), satisfies the robust Bellman equation \(Q^{-}(s,a)=_{}^{-}Q^{-}(s,a), s,a\) since the uncertainty set \((P)\) factorizes over \(s,a\). While these equations may seem intractable due to the \(\) in the definition of \(_{}^{-}\),  showed that \(_{}^{-}\) has a closed form solution in terms of the CVaR under the _observed_ kernel \(P\).

**Lemma 3.1**.: _Set \((s,a)=((s,a)+1)^{-1}\). Then, for any \(q:\),_

\[_{}^{-}q(s,a)=r(s,a)+^{-1}(s,a)[v( s^{}) s,a]+(1-^{-1}(s,a))_{(s,a)}^{ -}[v(s^{}) s,a],\]

_where \(v(s^{})=_{a^{}_{l}(s^{})}[q(s^{},a^{ })]\), and \(,_{}\) are under the observed kernel \(P( s,a)\)._

Lemma 3.1 implies that \(Q^{-}\) is identified via the following equation of observable distributions:

\[Q^{-}(s,a)=r(s,a)+^{-1}(s,a)[Q^{-}(s^{},_{ }) s,a]+(1-^{-1}(s,a))_{(s,a)} ^{-}[Q^{-}(s^{},_{}) s,a].\]

Under no confounding (\((s,a)=1\)), this recovers the classic Bellman equation.

### Estimating the Robust \(Q\)-Function with Robust FQE

In this section, we estimate \(Q^{-}\) via an iterative fitting algorithm based on fitted Q-evaluation (FQE) . Our algorithm RobustFQE (Algorithm 1) proceeds for \(M\) iterations with two main steps in each iteration \(i\). First, in Line 5, we estimate the lower-quantile of \(_{i-1}(s^{}) s,a\). Here, we assume access to an oracle QR for quantile regression, which is a well-established problem, allowing for the use of various existing algorithms. Second, in Line 6, we solve the tractable robust Bellman equation in Lemma 3.1 with the CVaR term estimated by its orthogonal estimating equation with the learned quantiles . By orthogonally estimating \(\), we achieve second-order dependence on the quantile estimation errors from the first step. Next, we minimize the mean squared error using a general function class, \([0,(1-)^{-1}]\).

To enable convergence guarantees, we make two assumptions. First, we assume that the quantile regression oracle has a specific convergence rate, which can be guaranteed under certain smoothness conditions . Distributional RL may also be modified to learn quantiles of the next state value and have shown benefits in practice  and in theory .

**Assumption 3.2** (QR Oracle).: For any \(v:[0,(1-)^{-1}]\), let the true \((s,a)\)-quantile of \(v(s^{}),s^{} P(s,a)\) be denoted by \(_{}^{v}(s,a)\). Given a dataset \(_{}\), we assume QR outputs estimates \(_{v}\) with bounded \(_{}\) error: for any \(\), w.p. \(1-\), \(\|_{q}-_{}^{q}\|_{}<_{}(| _{}|,)\).

The second assumption is completeness under the robust Bellman \(_{}^{-}\), Completeness is a standard assumption in algorithms based on temporal-difference learning and without it, fitted-Q can diverge or converge to suboptimal fixed points .

**Assumption 3.3** (Completeness).: For all \(q\), we have \(_{}^{-}q\).

We note that the current proofs of  require a stronger completeness: \(_{}q\) for all \(q\) and feasible \(\). We circumvent the need for the stronger "all-\(\)" completeness by bounding model misspecification of least squares regression with second order error in the quantile regression.

Finally, we express our bounds with the critical radius \(_{n}^{}\), a standard tool for deriving fast rates in statistics; see Appendix D.2 for a summary. Also, we denote the standard concentrability coefficient with \(C_{d_{1}}^{-}:=\|d_{}^{-}}}{{}{ }{}{}}\|_{}\), a standard and necessary quantity for OPE.

**Theorem 3.4**.: _Let \(_{n}^{}\) denote the critical radius of \(\). Under Assumptions 3.2 and 3.3, RobustFQE ensures that for any \((0,1)\), w.p. \(1-\),_

\[\|_{M}^{-}-Q^{-}\|_{d_{1}}(1-)^{-2}( }^{-}}_{n}^{}+_{}^ {2}(n/2M,/2M)),\;\;\] \[|(1-)_{d_{1}}[_{M}^{-}(s_{1})]-V _{d_{1}}^{-}|^{M}+(1-)^{-1}(}^{-}} _{n}^{}+_{}^{2}(n/2M,/2M )).\]

For parametric classes (_e.g._, finite or linear), the critical radius converges at the standard \(}(n^{-1/2})\) rate. Due to the orthogonal estimation of CVaR, we benefit from a favorable second-order dependence on \(_{}\) which allows for quantile regression to converge at slower \(}(n^{-1/4})\) rates. The main disadvantage of this direct approach is that it converges at a slow sub-\(\) rate if \(_{n}^{}\) converges at a sub-\(\), _e.g._, \(_{n}^{}\) converges at a \(}(n^{-1/4})\) rate if \(\) is nonparametric with metric entropy at most \(1/t^{2}\). In Section 5, we present an orthogonal estimator that is both robust to slower rates of \(Q\) and achieves semiparametric efficiency.

## 4 Robust \(w\)-Function Estimation with Minimax Learning

Before we present our orthogonal estimator, we study another essential nuisance function: the robust visitation density ratio, _i.e._, the robust \(w\)-function . In this section, we first identify the worst-case transition kernel \(U^{-}\) in our uncertainty set \((P)\). Then, we propose a minimax estimator  for the robust \(w\)-function, an important nuisance function for our orthogonal estimator in Section 5.

Identification of \(U^{-}\).The robust transition kernel \(U^{-}\) is defined as the feasible perturbed kernel that achieves the \(\) in the robust Bellman equation \(Q^{-}(s,a)=_{}^{}Q^{-}(s,a)\). Let\(P(V^{-}(s^{}) y s,a)\) be the next-state pushforward measure of the robust value function \(V^{-}\). Then, \(U^{-}\) is a convex combination of the nominal kernel \(P\) and a reweighting of \(P\) by an indicator function.

**Lemma 4.1**.: _Suppose \(F^{-}(_{}^{-}(s,a) s,a)=\), where \(_{}^{-}(s,a)\) is the lower \(\)-th quantile of \(F^{-}( s,a)\). Then,_

\[U^{-}(s^{} s,a)/P(s^{} s,a)=^{-1}(s,a)+(1-^ {-1})(s,a)^{-1}[(V^{-}(s^{})-_{}^{-}(s,a)) 0].\] (4)

The proof strategy decomposes \(U^{-}\) into its nominal and perturbed components, leveraging the primal solution of \(_{}\); we formalize this in Appendix E.2.

Identification of \(w^{-}\).Using the identification of \(U^{-}\) in Lemma 4.1, we can now identify the robust \(w\)-function based on the Bellman flow equations in the worst-case MDP. The Bellman flow in the robust MDP is given by \(d^{-,}(s)=(1-)d_{1}(s)+_{ d^{-, },_{}()}U^{-}(s ,)\). where \(d^{-,}(s)\) was defined in Section 2. Thus, the robust visitation density, defined as \(w^{-}(s):=d^{-,}(s)}}{{(s)}}\), satisfies the following moment condition for all \(f:\):

\[[w^{-}(s)f(s)]=(1-)_{d_{1}}[f(s_{1})]+ [w^{-}(s,a)_{s^{} U^{-}(s,a)}[f(s^{})]],\] (5)

where we relaxed notation and defined \(w^{-}(s,a):=w(s)_{}(a s)/(a s)\). As before, in the unconfounded base (\(=1\)), this result recovers the classic Bellman flow.

### Estimating \(w^{-}\) with Robust Minimax Indirect Learning

We now propose a penalized minimax estimator for \(w^{-}\) that generalizes the Minimax Indirect Learning (MIL) of  to our robust MDP setting. Our estimator, RobustML (Algorithm 2), leverages a general function class \(_{+}\) to approximately solve the moment equation in Eq. (5). It does so by minimizing the difference between the left- and right-hand sides of the equation across a sufficiently large set of adversaries \(f\) in a discriminator class \(\). Since \(U^{-}\) is unknown, we approximate it via Eq. (4) by plugging in a threshold \((s,a,s^{})\) in the indicator function to approximate the true threshold \(^{-}(s,a,s^{}):=V^{-}(s^{})-_{(s,a)}^{-}(s,a)\). This yields the minimax objective in Eq. (6), where we also allow for an optional regularization of the adversary's norm which can be useful for obtaining fast convergence rates.

We make the following assumptions for MIL . The first is a regularity condition that (i) our function class has bounded outputs and (ii) \(\) is continuously distributed around the threshold.

**Assumption 4.2** (Regularity).: (i) \(_{w\{w^{-}\}}\|w\|_{}<\); (ii) the marginal CDF of \(V^{-}(s^{})-^{-}(s,a)\), _i.e._, \(F(y)=P(V^{-}(s^{})-_{(s,a)}^{-}(s,a) y)\), is boundedly differentiable around \(0\).

If next-value distribution is discrete, we can use the discrete form of CVaR and (ii) can be removed.

The second is that the adversary class is rich enough to capture all projected errors under the adjoint of the operator \(_{U^{-}}f(s,a):=_{U^{-}}[f(s^{},_{ }) s,a]-f(s,a)\).

**Assumption 4.3** (\(w^{-}\)-realizability and completeness).: \(w^{-}\) and \(^{}_{U^{-}}(-w^{-})\).

We note that Assumption 4.3 is monotone in the function class size and can be satisfied by making the function class more expressive, _e.g._, increasing size of the neural net. Our algorithms are also robust to violations in Assumption 4.3, which we show in Appendix G.

We are now ready to state the main estimation result for \(w^{-}\) in terms of the critical radius (Appendix D.2) of the function class.

**Theorem 4.4**.: _Let \(_{n}^{}\) denote the maximum critical radii of the following classes:_

\[_{1} =\{(s,a,s^{})(f(s,a)- f(s^{},_{l})),f \},\] \[_{2} =(s,a,s^{})(w(s,a)-w^{-}(s,a))( f(s^{ },_{l})-f(s,a)),f,w}.\]

_Under Assumptions 4.2 and 4.3, RobustMIL ensures that for any \(\), w.p. \(1-\),_

\[_{U^{-}}^{}(-w^{-})_{2} _{n}^{}+\|^{-}-^{-}\|_{}+ {(1/)/n}.\]

As before, the critical radius \(_{n}^{}\) converges at an \(}(n^{-1/2})\) rate for parametric classes. Notably, our bounds degrade linearly w.r.t. the \(_{}\) error in \(^{-}\) for estimating \(^{-}\). For example, if \((s,a,s^{})=(s^{})-(s,a)\) where \(,\) are estimated with RobustFOE, then the \(\)-error can be bounded by \((\|-v^{-}\|_{}+\|-^{-}\|_{ })\). We present the full proof in Appendix G, where we also present a more general result that is robust to misspecifications to realizability and completeness (Assumption 4.3).

## 5 Orthogonal and Efficient Estimator for Robust Policy Value

In this section, we propose an orthogonal estimator that is robust against errors in the nuisances (exhibiting only second-order sensitivity), achieves semiparametric efficiency, and enables inference. Our estimator is based on the efficient influence function (EIF) of \(V_{d_{1}}^{-}\), which is the canonical gradient of a statistical estimand . The adoption of EIFs for developing efficient estimators is a broadly employed technique in causal inference [16; 43] and reinforcement learning [35; 39].

We define the collection of nuisance parameters by \(^{-}=(w^{-},q^{-},^{-})\). The notation \(\) indicates that these functions are estimated from data, while the notation \(\) denotes their true values.

**Theorem 5.1** ((Recentered) Efficient Influence Function).: _The (R)EIF of \(V_{d_{1}}^{-}\) is given by:_

\[(s,a,s^{};^{-})=V_{d_{1}}^{-}+w^{-}(s,a)r(s, a)+^{-}(s,a,s^{};v^{-},^{-})-q^{-}(s,a),\] \[^{-}(s,a,s^{};v^{-},^{-})=(s,a)^{-1}v^{-}( s^{})+(1-(s,a)^{-1})^{-}(s,a)+^{-1}(v^{-}(s^{ })-^{-}(s,a))_{-}.\]

_Remark 5.2_.: When \(=1\), there is no shift in the target environment, and the weight on the \(\) term is zero. The (R)EIF then reduces to the (R)EIF in  for regular OPE with an infinite horizon. As \(\), the \(\) term becomes predominant, with the quantile \(^{-}(s,a)\) taking extreme values. This yields the (novel) (R)EIF for the problem in , where the expected value term is replaced solely by a \(\) component in the Bellman equation.

The (R)EIF forms the basis of our orthogonal estimator. First, we note that \([(s,a,s^{};^{-})]\) is an unbiased estimator of \(V_{d_{1}}^{-}\). Furthermore, the expression for \((s,a,s^{};^{-})\) depends only on quantities \(w^{-},q^{-},^{-}\) which can be estimated from data. Thus, we can cast the expression \([(s,a,s^{};^{-})]\) as a statistical estimand to be learned from the observed sample. This suggests a natural two-stage estimator that we summarize in Algorithm 3. In the first stage, we estimate the nuisance parameters \(\) from the data with \(K\)-fold cross-fitting; in the second stage, these estimates are incorporated into the (R)EIF expression and we calculate the empirical average using the observed data. We summarize our procedure in Algorithm 3.

```
1:Input: Dataset \(\), number of splits \(K\).
2:for\(k=1,2,,K\)do
3: Use data \(_{k}\) to learn \((q^{-,[k]},^{-,[k]})\) with Algorithm 1 and \(w^{-,[k]}\) with Algorithm 2
4:for\(i=(k-1)n/K,, kn/K-1\)do
5:\(_{d_{1}}^{-}=_{i=1}^{n}_{i}^{-}\). ```

**Algorithm 3** Orthogonal Estimator for \(V_{d_{1}}^{-}\)

The nuisance estimation is detailed in Sections 3.2 and 4.1. The reliance on the EIF confers our estimator desirable statistical properties including a second order bias due to the nuisances, meaning the bias has a product structure with respect to the nuisance errors. Thus, this special structure orthogonalizes away the dependency on \(^{-}\) errors which now only appear in second order. Furthermore, our estimator is semiparametrically efficient in the sense that under mild consistency assumptions, it achieves minimum variance among all regular and asymptotically linear (RAL) estimators. We provide theoretical justifications for these properties in the next section.

### Theoretical Guarantees of the Orthogonal Estimator

We now characterize the theoretical properties of our orthogonal estimator. We consider the \(K\)-fold cross-fitted estimator in Algorithm 3 given by

\[_{d_{1}}^{-}=_{k=1}^{K}_{(s,a,s^{}) ^{k}}(s,a,s^{};^{[k]}),\]

where nuisances \(^{[k]},k[K]\) are trained on all data excluding the \(k\)th fold \(^{k}\). The following theorem outlines the theoretical guarantees of this estimator:

**Theorem 5.3** (Efficiency of \(_{d_{1}}^{-}\)).: _Let \(r^{w}_{n,p},r^{q}_{n,p},r^{}_{n,p}\) be functions of the same size \(n=||\) such that \(\|^{}_{U^{-}}(^{-,[k]}-w)\|_{p} r^{w}_{n,p},\| ^{-,[k]}-q\|_{p} r^{q}_{n,p}\), and \(\|^{-,[k]}-\|_{p} r^{}_{n,p}\) for any \(k[K]\). Furthermore, assume that the regularity conditions in Assumption 4.2 hold. Then:_

\[|_{d_{1}}^{-}-V_{d_{1}}^{-}| O_{p}(n^{-1/2})+O_{p}(r^{w}_{n,2}r^{q}_{n,2}+(r^{q}_{n,})^{2}+(r^{}_{n,})^{2})\] (Rates)

_Furthermore, if \(r^{w}_{n,2} r^{q}_{n,2}=o_{p}(1)\), \(r^{w}_{n,2}r^{q}_{n,2}=o_{p}(n^{-1/2})\), \(r^{q}_{n,}=o_{p}(n^{-1/4})\), and \(r^{}_{n,}=o_{p}(n^{-1/4})\), then \(_{d_{1}}^{-}\) satisfies:_

\[(_{d_{1}}^{-}-V_{d_{1}}^{-})(0, ),=((s,a,s^{};^{-})). \]

_Moreover, \(\) is the minimum achievable asymptotic variance among RAL estimators in the nonparametric model for \((s,a,s^{})\) (the efficiency bound)._

We provide the intuition along with a detailed proof in Appendix H. The first part of Theorem 5.3 implies that as long as we estimate the nuisances at rates faster that \(n^{-1/4}\), then we can learn \(_{d_{1}}^{-}\) at parametric rates. The second part of Theorem 5.3 states that under mild consistency assumptions, our estimator attains the efficiency bound and is asymptotically normal. That means, for example, we can construct asymptotically valid lower 95%-confidence bound on \(_{d_{1}}^{-}\) by simply subtracting 1.64 times \(}=(_{k=1}^{K}_{(s,a,s^{})^{k}}((s,a,s^{};^{[k]})-_{d_{1}}^{-})^{2}) ^{1/2}\). Then, we can be sure to have a bound on the worst-case RL policy value, accounting _both_ for potential environment shift and finite data. Finally, in Appendix J, we describe two settings when our orthogonal estimator remains valid even if some nuisances are _inconsistent_, which is a desirable guarantee for sensitivity analysis .

Bringing it all together.We can instantiate Theorem 5.3 with the nuisance estimators from the previous sections. First, use RobustFOE to estimate \(^{-}\) and \(^{-}\), ensuring \(\|^{-}-Q^{-}\|_{2}(_{n}^{}+ _{}^{2})\). Under smoothness conditions (Lemma D.2), the \(L_{2}\) guarantee for \(^{-}\) implies an \(L_{}\) guarantee for \(^{-}\), which also ensures an \(L_{}\) guarantee for \(^{-}\). This ensures \((\|^{-}-Q^{-}\|_{},\|^{-}-^{-}\|_{ })\) is well-controlled. Then, we can set \(^{-}(s,a,s^{})=^{-}(s^{},_{})- ^{-}(s,a)\) and run RobustML for estimating \(^{-}\). By Theorem 4.4, its projected-\(L_{2}\) error is \((_{n}^{}+\|^{-}-Q^{-}\|_{}+ \|^{-}-^{-}\|_{})\). Therefore, the final rate via Theorem 5.3 is \(((_{n}^{}+_{}^{2}) _{n}^{}+\|^{-}-Q^{-}\|_{}^{2}+\| ^{-}-^{-}\|_{}^{2})\).

## 6 Empirical Evaluation

We now provide a proof-of-concept empirical investigation to validate our theoretical findings. We experiment with our proposed methodology in a simple synthetic environment. First, we discuss our environment, followed by our approach for solving for the nuisances functions \(^{-}\). Then, we provide empirical results for our orthogonal estimator, and compare its performance to weighted or direct estimators using the \(Q^{-}\) or \(w^{-}\) nuisances only. The code for our experiments is open-sourced and available at https://github.com/CausalML/adversarial-ope/.

Experimental SetupWe consider a synthetic MDP with a one-dimensional state and two actions, modeled after a simple control problem with non-deterministic dynamics. The task is to estimate the worst-case policy value \(V_{d_{1}}^{-}\) of a fixed candidate policy \(_{}\), across four different constant values of the sensitivity parameter: \((s,a)\{1,2,4,8\}\).

We considered three methods for estimating the robust value \(V_{d_{1}}^{-}\):

1. \(\) (RobustFOE): Direct method using the estimated robust quality function \(^{-}\) only.
2. \(\) (RobustMIL): Importance-sampling method using the estimated robust density ratio \(^{-}\) only.
3. **Orth**: Our orthogonal estimator which combines the former two, as described in Algorithm 3.

We performed 10 replications of our experimental procedure, where for each replication we: (1) sampled a dataset of 20,000 tuples using a different fixed logging policy \(_{b}\); (2) fit the nuisance functions \(Q^{-}\), \(^{-}\), and \(w^{-}\) following the method outlined in Algorithms 1 and 2 for each \(\); and (3) estimated the corresponding robust policy value \(V_{d_{1}}^{-}\) for all estimators using the fitted nuisances.

ResultsWe summarize our results in Fig. 2. We note that all of our estimators are consistently valid for all values of \(\) in our experiment. Notably, **Orth** consistently has the lowest mean squared error for the true worst-case policy value. In particular, incorporating the robust importance-sampling weights improves the RobustFOE estimator \(\), even though these importance-sampling weights by themselves (as in \(\)) are much noisier estimators. This is consistent with our theory that the orthogonal estimator is semiparametrically efficient and insensitive to errors in the nuisance functions.

Full experimental details, including our MDP, target/logging policies, methodology for computing the true robust policy values \(V_{d_{1}}^{-}\), and nuisance estimation, are provided in Appendix K. Finally, we also performed an empirical evaluation in the real-world medical problem of sepsis management using the MIMIC-III dataset . We detail these results in Appendix L.

## 7 Conclusion

We consider the problem of infinite-horizon OPE in RL settings when there can be unknown, but bounded, shifts in the transition distribution compared to the transition distribution generating the data. This can arise due to unobserved confounding, where observed transitions do not reflect the true causal ones, non-stationarity in the environment, or adversarial environments. We propose a sensitivity model for such transition kernel shifts analogous to the classic MSM for static decision making, and provide theoretical guarantees for identifying and estimating the sharp (_i.e._, tightest possible) bounds on the best/worst-case policy value, as well as the corresponding robust \(Q\)-function and state density ratio functions. Our estimator for the best/worst-case policy value is orthogonal (insensitive to how the nuisance functions are estimated) and achieves semiparametric efficiency (attaining the best possible asymptotic variance). Finally, our estimator also supports inference, ensuring we can derive reliable bounds for the robust policy value even with finite data.

Figure 2: Results of our synthetic data experiments. We show results for our three estimators on all four \(\) values, over our 10 experiment replications. **Above:** Box plot summarizing range of policy value estimates for each combination of estimator and \(\), with Horizontal red dashed lines showing the true worst-case policy values \(V_{d_{1}}^{-}\). **Below:** Table summarizing the corresponding MSE of these estimators for the true worst-case policy value, along with one standard deviation errors.