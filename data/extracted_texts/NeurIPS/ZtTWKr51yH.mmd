# Our contributions can be summarized as follows:

Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data

 Thibault Simonetto

University of Luxembourg

 Luxembourg

thibault.simonetto@uni.lu

&Salah Ghamizi

LIST / RIKEN AIP

 Luxembourg

 salah.ghamizi@list.lu

&Maxime Cordy

University of Luxembourg

 Luxembourg

 maxime.cordy@uni.lu

###### Abstract

State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack. We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1% points and 21.9% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.

## 1 Introduction

Evasion attack is the process of slightly altering an original input into an _adversarial example_ designed to force a machine learning (ML) model to output a wrong decision. Robustness to adversarial examples is a problem of growing concern among the secure ML community, with over 10,000 publications on the subject since 2014 . Recent studies also report real-world occurrences of evasion attacks, which demonstrate the importance of studying and defending against this phenomenon .

While research has studied the robustness of deep learning models in Computer Vision (CV) and Natural Language Processing (NLP) tasks, many real-world applications instead deal with tabular data, including in critical fields like finance, energy, and healthcare. If classical "shallow" models (e.g. random forests) have been the go-to solution to learn from tabular data , deep learning models are becoming competitive . This raises anew the need to study the robustness of these models.

However, robustness assessment for tabular deep learning models brings a number of new challenges that previous solutions -- because they were originally designed for CV or NLP tasks -- do not consider. One such challenge is the fact that tabular data exhibit _feature constraints_, i.e. complex relationships and constraints across features. The satisfaction of these feature constraints can be a non-convex or even non-differentiable problem; this implies that established evasion attack algorithms relying on gradient computation do not create valid adversarial examples (i.e., constraint satisfying) . Meanwhile, attacks designed for tabular data also ignore feature type constraints  or, in the best case, consider categorical features without feature relationships [40; 41; 4] and are evaluated on datasets that exclusively contain such features. This restricts their application to other domains that present heterogeneous feature types.

The only published evasion attacks that support feature constraints are _Constrained Projected Gradient Descent_ (CPGD) and _Multi-Objective Evolutionary Adversarial Attack_ (MOEVA) . CPGD is an extension of the classical gradient-based PGD attack with a new loss function that encodes how far the generated examples are from satisfying the constraints. Although theoretically elegant and practically efficient, this attack suffers from a low success rate due to its difficulty to converge toward both model classification and constraint satisfaction . Conversely, MOEVA is based on genetic algorithms. It offers an outstanding success rate compared to CPGD and works on shallow and deep learning models. However, it is computationally expensive and requires numerous hyper-parameters to be tuned (population size, mutation rate, generations, etc.). This prevents this attack from scaling to larger models and datasets.

Overall, research on adversarial robustness for tabular machine learning in general (and tabular deep learning in particular) is still in its infancy. This is in stark contrast to the abundant literature on adversarial robustness in CV  and NLP tasks . Given this limited state of knowledge, the **objective** of this paper is to propose novel and effective attack methods for tabular models subject to feature constraints.

We hypothesize that gradient-based algorithms have not been explored adequately in  and that the introduction of dedicated adaptive mechanisms can outperform CPGD. To verify this, we design a new adaptive attack, named _Constrained Adaptive PGD_ (CAPGD), whose only free parameter is the number of iterations and that does not require additional parameter tuning (Section 4). We demonstrate that the different mechanisms we introduced in CAGPD contribute to improving the success rate of this attack compared to CPGD, by 81% points. Across all our datasets, the set of adversarial examples that CAPGD generates subsumes all the examples generated by any other gradient-based method. Furthermore, CAPGD is 75 times faster than MOEVA, while the latter reaches the highest success rate across all datasets.

These results motivate us to design _Constrained Adaptive Attack_ (CAA), an adaptive attack that combines our new gradient-based attack (CAPGD) with MOEVA for an increased success rate at a lower computational cost. Our experiments show that CAA reaches the highest success rate for all models/datasets we considered, except in one case where CAA is second-best. With this attack, we offer a strong baseline for future research on evasion attacks for tabular models, which should become the minimal test for robust tabular architectures and other defense mechanisms.1. We design a new parameter-free attack, CAPGD that introduces momentum and adaptive steps to effectively evade tabular models while enforcing the feature constraints. We show that CAPGD outperforms the other gradient-based attacks in terms of capability to generate valid (constraint-satisfying) adversarial examples.
2. We propose a new efficient and effective evasion attack (CAA) that combines gradient and search attacks to optimize both effectiveness and computational cost.
3. We evaluate CAA in a large-scale evaluation over four datasets, five architectures, and two training methods (standard and adversarial training). Our results show that CAA outperforms all other attacks and is up to 5 times more efficient.

## 2 Related work

### Tabular Deep Learning

Tabular data remains the most commonly used form of data , especially in critical applications such as medical diagnosis [38; 36], financial applications [18; 11; 8], user recommendation systems , cybersecurity [9; 1], and more. Improving the performance and robustness of tabular machine learning models for these applications is becoming critical as more ML-based solutions are cleared to be deployed in critical settings.

Borisov et al.  showed that traditional deep neural networks tend to yield less favorable results in handling tabular data when compared to more shallow machine learning methods, such as XGBoost. However, recent approaches like RLN  and TabNet  are catching up and even outperforming shallow models in some settings. We argue that DNNs for Tabular Data are sufficiently mature and competitive with shallow models and require therefore a thorough investigation of their safety and robustness. Our work is the first exhaustive study of these critical properties.

### Realistic Adversarial Examples

Initially applied to computer vision, adversarial examples have also been adapted and evaluated on tabular data. Ballet et al.  considered feature importance to craft the attacks, Mathov et al.  considered mutability, type, boundary, and data distribution constraints, Kireev et al.  suggested considering both the cost and benefit of perturbing each feature, and Simonetto et al.  introduced domain-constraints (relations between features) as a critical element of the attack.

While domain constraints satisfaction is essential for successful attacks, research on robustness for industrial settings (eg Ghamizi et al.  with a major bank) also demonstrated that imperceptibility remains important for critical systems with human-in-the-loop mechanisms, which could deflect attacks with manual checks from human operators. Imperceptibility is domain-specific, and multiple approaches have been suggested [3; 24; 16]. None of these approaches was confronted with human assessments or compared with each other, and in our study, we decided to use the most established \(L_{2}\) norm. Our algorithms and approaches are generalizable to further distance metrics and imperceptibility definitions.

Overall, except the work from Simonetto et al. , none of the existing attacks for tabular machine learning supports the feature relationships inherent to realistic tabular datasets, as summarized in Table 1. Nevertheless, we evaluate all the approaches that support continuous values and where a public implementation is available to confirm our claims: LowProFool, BF*, CPGD, and MOEVA.

## 3 Problem formulation

We formulate the problem of evasion attacks under constraints. We assume the attack to be untargeted (i.e. it aims to force misclassification in any incorrect class); the formulation for targeted attacks is similar and omitted for space reasons.

We denote by \(x^{d}\) an input example and by \(y\{1,,C\}\) its correct label. Let \(h:^{d}^{C}\) be a classifier and \(h_{c_{k}}(x)\) the classification score that \(h\) outputs for input \(x\) to be in class \(c_{k}\). Let \(^{d}\) be the space of allowed perturbations. Then, the objective of an evasion attack is to find a \(\) such that \(argmax_{c\{1,,C\}}h_{c}(x+) y\).

   Attack & Supported features &  \\  & & Categorical & Discrete & Relations \\ 
**LowProFool (LPF)** & Continuous & No & No & No \\ Cartella et al.  & Continous, Discrete, Categorical & Yes & Yes & No \\ Gressel et al.  & Continous, Discrete, Categorical & Yes & Yes & No \\ Xu et al.  & Categorical & Yes & No & No \\ Wang et al.  & Categorical & Yes & No & No \\ Bao et al.  & Categorical & Yes & No & No \\
**BF*/BFS**[26; 25] & Continous, Discrete, Categorical & Yes & Yes & No \\ Mathov et al.  & Continous, Discrete, Categorical & Yes & Yes & No \\
**CPGD, MOEVA** & Continous, Discrete, Categorical & Yes & Yes & Yes \\
**CAPGD, CAA (OURS)** & Continous, Discrete, Categorical & Yes & Yes & Yes \\   

Table 1: Evasion attacks for tabular machine learning. Attacks with a public implementation in bold.

In image classification, the set \(\) is typically chosen as the perturbations within some \(l_{p}\)-ball around \(x\), that is, \(_{p}=\{^{d},||||_{p}\}\) for a maximum perturbation threshold \(\). This restriction aims at preserving the semantics of the original input by assuming that small enough perturbations will yield images that humans perceive the same as the original images and would therefore classify the perturbed input into the same class (while the classifier predicts another class). This also guarantees that the example remains meaningful, that is, \(x+\) is not an image with random noise.

Tabular data are by nature different from images. They typically represent objects of the considered application domain (e.g. botnet traffic , financial transaction ). We denote by \(:Z^{d}\) the feature mapping function that maps objects of the problem space \(Z\) to a \(d\)-dimensional feature space defined by the feature set \(F=\{f_{1},f_{2},...f_{d}\}\). Each object \(z Z\) must inherently respect some natural condition to be valid (to be able to exist in reality). In the feature space, these conditions translate into a set of constraints on the feature values, which we denote by \(\). By construction, any input example \(x\) obtained from a real-world object \(z\) satisfies \(\), noted \(x\).

Thus, in the case of tabular data, we additionally require the perturbation \(\) applied to \(x\) to yield a valid example \(x+\) satisfying \(\), that is, \(_{p}(x)=\{^{d}:||||_{p} x+ \}\).

To define the constraint language expressing \(\), we consider the four types of constraint introduced by Simonetto et al. . These four constraint types cover all the constraints of the datasets in our empirical study. Hence, _immutability_ defines what features cannot be changed by an attacker; _boundaries_ defines upper / lower bounds for feature values; _type_ specifies a feature to take continuous, discrete, or categorical values; and _feature relationships_ capture numerical relations between features. Feature relationship constraints can be expressed with the following grammar:

\[ _{1}_{2}_{1}_{2} _{1}_{2}\] (1) \[  c f_{i}_{1}_{2} x_{i}\] (2)

Equation 1 means that a constraint formula \(\) can either be an intersection (\(\)), or a union (\(\)) of two other constraint formulae \(_{1}\), \(_{2}\), or \(\) can be a comparison operator \(\{<,,=,,,>\}\) between two values \(_{1}\) and \(_{2}\).

Equation 2 details the numeric expressions that are supported by the grammar. A numeric expression \(\) can be constant \(c\), an operation \(\{+,-,*,/\}\) between two other numerical expressions \(_{1}\) and \(_{2}\), or a specific feature \(f_{i}\), or the \(i\)-th feature of the clean sample. The difference between \(f_{i}\) and \(x_{i}\) is that \(f_{i}\) corresponds to the current value of the evaluated example and \(x_{i}\) corresponds to its original value in the clean example.

Let's consider one complex constraint from the LCLD credit scoring use case: the term of the loan can only be 36 or 60 months and the number of open accounts is lower than the number of allowed accounts for this client. Such a constraint can be formally written as:

\[_{1}=((f_{}=36)(f_{}=60))(f_{} f_{})\] (3)

We provide other examples in Appendix A.1.

### Constrained Projected Gradient Descent

Constrained Projected Gradient Descent (CPGD) is an extension of the PGD attack  to generate adversarial examples satisfying constraints in tabular machine learning. It integrates constraint satisfaction into the loss function that PGD optimizes. This is achieved by translating each constraint \(\) into a differentiable function \(penalty(x,)\) that values to zero if \(x\); otherwise, the function represents how far \(x\) is from satisfying \(\). We follow the definition of Table 5 in Appendix A.1 to translate each construct of the constraints grammar into a penalty function.

For instance, the penalty function of \(_{1}\) in Equation 3 is:

\[penalty(x,_{1})=min(|f_{}-36|,|f_{}-60|)+max(0,f_{ }-f_{})\] (4)

Based on this, CPGD produces adversarial examples from an initial sample \(x_{orig}\) classified as \(y\) by iteratively computing:\[x^{(k+1)}=R_{}P_{}x^{(k)}+^{(k)}(x^{(k)},y,h,)\] (5)

where \(x^{0}=x_{orig}\) (the original input), \(R_{}\) is a domain-specific repair operator , \(P_{}\) is the projection onto \(=\{x^{d},||x-x_{orig}||_{p}\}\), \(\) is the gradient of loss function \(\), defined as

\[(x,y,h,)=l(h(x),y)-_{_{i}}penalty(x, _{i}).\] (6)

In the original CPGD implementation, the step size \(^{(k)}\) follows a predefined decay schedule, \(^{(k)}= 10^{-(1+ k/ K/M))}\), with \(M=7\), and \(K=max(k)\). \(^{}(x)\) abbreviates \((x,y,h,)\).

### Experimental settings

Our experiments are driven by the following datasets, models, and attack parameters. More details about the datasets and models are given in Appendix A.5.

DatasetsTo conduct our study, we selected tabular datasets that present feature constraints from their respective application domain. **URL** is a dataset of legitimate and phishing URLs. With only 14 linear domain constraints and 63 features, it is the simplest of our empirical study. **LCLD** is a credit-scoring dataset with non-linear constraints. The **WiDS** dataset contains medical data on the survival of patients admitted to the ICU. It has only 30 linear domain constraints. The **CTU** dataset reports legitimate and botnet traffic from CTU University. The challenge of this dataset lies in its large number of linear domain constraints (360). We detail the datasets in the Appendix A.4.

ArchitecturesWe evaluate five top-performing architectures from a recent survey on tabular ML : **TabTransformer** and **TabNet** are transformer-based models. **RLN** uses a regularization coefficient to minimize a counterfactual loss. **STG** optimizes feature selection with stochastic gates, and **VIME** relies on self-supervised learning. These architectures achieve performance equivalent to XGBoost, the best shallow machine learning model for our use cases.

Perturbation parametersWe use the L2-norm to measure the distance between original and perturbed inputs, because this norm is suitable for both numerical and categorical features. We set \(\) to 0.5 for all datasets. Each dataset has a critical (negative) class, respectively phishing URLs, rejected loans, flagged botnets, and not surviving patients. Hence, we only attack clean examples from the critical class that are not already misclassified by the model and report robust accuracy of models.

Evaluation metricsWe measure the effectiveness of our attack using robust accuracy defined as the accuracy of valid examples generated by a given attack. If a clean example is misclassified, we do not perturb it. If the attack generates an invalid example, we consider it as correctly classified. We measure the efficiency of the attacks in computational time.

## 4 Our Constrained _Adaptive_ PGD

The relative lack of effectiveness of CPGD as reported in its original publication leads us to investigate the cause of these weaknesses. We investigate four factors that may affect the success rate of the attack: (1) we conjecture that the fixed step size and predefined decay in CPGD might be suboptimal because the choice of the step size is known to largely impact the effectiveness of gradient-based attacks ; (2) CPGD is unaware of the trend, i.e. it does not consider whether the optimization is evolving successfully and is not able to react to it; (3) CPGD does not check constraint satisfaction between the iterations, which could "lock" the algorithm into a part of the invalid data space; (4) CPGD starts with the original example, whereas classical gradient-based attacks often benefit from random initialization.

### CAPGD algorithm

We propose Constrained Adaptive PGD (CAPGD), a new constraint-aware gradient-based attack that aims to overcome the limitations of CPGD and improve its effectiveness. We detail CAPGD in Algorithm 1 in Appendix A.2, and summarize its components below.

Step size selectionWe introduce a step-size adaptation. We follow the exploration-exploitation principle by gradually reducing the gradient step . However, unlike CPGD, this reduction does not follow a fixed schedule but is determined by the optimization trend. If the value of the loss function grows, we keep the same step size; otherwise, we halve it. That is, we start with a step \(^{(0)}=2\), and we identify checkpoints \(w_{0}=0,w_{1},...,w_{n}\) at which we decide whether it is necessary to halve the size of the current step. We halve the step size if any of the following two conditions holds:

1. Since the last checkpoint, we count how many cases since the last checkpoint \(w_{j-1}\) the update step has successfully increased \(^{}\). The condition holds if the loss has increased for at least a fraction of \(\) steps (we set \(\) = 0.75): \[_{i=w_{j-1}}^{w_{j}-1}_{^{}(x^{(i+1)})> ^{}(x^{(i)})}<(w_{j}-w_{j-1}).\] (7)
2. The step has not been reduced at the last checkpoint and the loss is less or equal to the loss of the last checkpoint: \[^{(w_{j-1})}^{(w_{j})}_{}^{(w_{j-1})}_{}^{(w_{j})}\] (8) where \(^{}(x)\) is the loss function, \(_{}^{(w_{j})}\) is the highest loss value in the first \(j+1\) iterations.

Repair operatorWhile equality constraints are included in the penalty function, optimization alone does not achieve exact equality of feature values. Our new repair operator \(R_{}\) improved from  addresses this by setting the value of the left-hand side of an equation of the form \(f_{i}=\) to match the evaluation of the right-hand side in each iteration. It maintains other dataset constraints such as bounds, mutability, and feature types but does not ensure other relational constraints are met. The operator can violate maximum perturbation constraints, yet at each iteration, the perturbation is corrected back within the allowed maximum. This approach has been shown to improve the success rate of CAPGD, as demonstrated by our ablation study in Table 8. We provide the algorithm in Appendix A.2.

Initial stateAs for initialization, we apply the attack from two initial states: the original example \(x_{orig}\) and a random example sampled from \(\) (the Lp-ball around \(x_{orig}\)). The goal behind this second initialization is to reduce the risk of being immediately locked into local optima that encompass only invalid examples. Our experiments later reveal the complementary of these two initializations.

Gradient stepFinally, we introduce in CAPGD a momentum . Let \(^{(k)}\) be the step size at iteration \(k\), then we first compute \(z^{(k+1)}\) before the updated example \(x^{(k+1)}\).

\[z^{(k+1)} =P_{}x^{(k)}+^{(k)}(^{ }(x^{(k)})\] (9) \[x^{(k+1)} =R_{}P_{}x^{(k)}+(z^{(k +1)}-x^{(k)})+(1-)(x^{(k)}-x^{(k-1)})\]

where \(\) (we use \(=0.75\) following ) regulates the influence of the previous update on the current, and \(P_{}\) is the projection onto \(=\{x^{d},||x-x_{orig}||_{p}\}\).

### Comparison of CAPGD to gradient-based attacks

To evaluate the benefits of CAPGD, we compare it with CPGD as well as LowProFool, to the best of our knowledge, the only other public _gradient_ attack for tabular models that can be extended to support all types of features.

CAPGD is more successful than existing gradient attacks.In Table 2, we compare the robust accuracy across our four datasets and five architectures with CPGD, LowProFool, and CAPGD. CAPGD significantly outperforms CPGD and LowProFool. It decreases the robust accuracy on URL, LCLD, and WIDS datasets to as low as 10.9%, 0.2%, and 10.2% respectively.

The results also reveal that gradient attacks are ineffective on the CTU dataset. These results demonstrate that gradient-based attacks are not enough and motivate us to consider combining CAPGD with search-based attacks, as investigated in Section 5.

CAPGD subsumes all gradient attacks.We analyze in detail the original examples from which attacks could generate valid and successful adversarial examples. For each attack, we take the union of the sets of clean examples across 5 seeds. We generate the Venn diagram for CPGD, LowProFool, and CAPGD, for all datasets and model architectures. We sum the partition values in Figure 1. CAPGD generates adversarial examples for 6597 original examples from which none of the other gradient attacks could produce adversarial examples. In contrast, all successful adversarial examples by CPGD (132) and LowProFool (3) are also generated by CAPGD.

All components of CAPGD contribute to its effectiveness.We analyze in detail each of the components of CAPGD in Appendix B.1. These results and ablation studies confirm the complementarity of our new mechanisms and their contribution to the effectiveness of CAPGD.

## 5 CAA: an ensemble of gradient and search attacks

We next propose _Constrained Adaptive Attack_ (CAA), an effective and efficient ensemble of gradient- and search-based attacks. The idea underlying CAA is that gradient-based attacks for tabular data are more efficient but less successful than search-based attacks. Thus, CAA integrates the best search-based attacks from each family in a complementary way, such that we maximize the set of adversarial examples that can be generated.

### Design of CAA

Following our related work study, we consider the search attacks MOEVA and BF* Kulynych et al. , Kireev et al. . As a first step, we compare in Figure 2 these two attacks in terms of the original examples for which they could generate successful adversarial examples. We also include CAPGD in this comparison, since we have shown that this attack subsumes the other gradient-based attacks. Our results reveal that CAPGD and MOEVA together subsume BF* except for 5 examples. Additionally, CAPGD and MOEVA are complementary, with CAPGD generating 477 unique examples and MOEVA 953. Overall, the combination of CAPGD with MOEVA yields the strongest method including only one gradient-based

   DS & Model & Clean & LPF & CPGD & CAPGD \\   & TabTr. & \(93.6\) & \(93.6\) & \(91.9\) & \(\) \\  & RLN & \(94.4\) & \(94.4\) & \(92.8\) & \(\) \\  & VIME & \(92.5\) & \(92.5\) & \(90.7\) & \(\) \\  & STG & \(93.3\) & \(93.3\) & \(93.3\) & \(\) \\  & TabNet & \(93.4\) & \(93.4\) & \(88.5\) & \(\) \\   & TabTr. & \(69.5\) & \(69.2\) & \(69.5\) & \(\) \\  & RLN & \(68.3\) & \(68.3\) & \(68.3\) & \(\) \\  & VIME & \(67.0\) & \(67.0\) & \(67.0\) & \(\) \\  & STG & \(66.4\) & \(66.4\) & \(66.4\) & \(\) \\  & TabNet & \(67.4\) & \(67.4\) & \(67.4\) & \(\) \\   & TabTr. & \(\) & \(\) & \(\) & \(\) \\  & RLN & \(\) & \(\) & \(\) & \(\) \\  & VIME & \(\) & \(\) & \(\) & \(\) \\  & STG & \(\) & \(\) & \(\) & \(\) \\  & TabNet & \(\) & \(\) & \(\) \\   & TabTr. & \(75.5\) & \(75.5\) & \(75.2\) & \(\) \\  & RLN & \(75.7\) & \(77.5\) & \(77.3\) & \(\) \\   & VIME & \(72.3\) & \(72.3\) & \(71.5\) & \(\) \\   & STG & \(77.6\) & \(77.6\) & \(77.5\) & \(\) \\   & TabNet & \(79.7\) & \(79.7\) & \(76.0\) & \(\) \\   

Table 2: Robust accuracy against CAPGD and SOTA gradient attacks. A lower robust accuracy means a more effective attack (lowest in bold).

Figure 1: Visualization of the complementarity of CAPGD, CPGD, and LowProFool with the number of successful adversarial examples.

Figure 2: Visualization of the complementarity of CAPGD, MOEVA, and BF* with the number of successful adversarial examples.

[MISSING_PAGE_FAIL:8]

CAPGD and MOEVA fail to generate adversarial examples on CTU for 2 out of 5 models. In Appendix B.3, we provide a possible explanation for this dataset by evaluating our attacks on different sub-sets of constraints with varying complexity.

### Impact of attack budget

We study the impact of the attacker's budget on the effectiveness of CAA, in terms of (i) maximum perturbation \(\) and (ii) the number of iterations of its components. We focus on the CTU dataset, which models are the only ones to remain robust through our previous experiments. All the datasets are evaluated in Appendix B.2. Figure 3 reveals in (a) that the maximum perturbation distance \(\) has little impact on the effectiveness of the attack. Increasing the number of iterations for the gradient attack component (b) does not have an impact on the success rate of CAA. Increasing the budget of the search attack component (c) significantly impacts the robustness of some models. While TabTransformer and STG remain robust, the robust accuracy of RLM and VIME drops below 0.4 when doubling the number of search iterations to 200, and to zero with 1000 search iterations.

### Impact of Adversarial training

We evaluate the effectiveness of our attack against models made robust with Madry's adversarial training (AT) , using examples generated by the PGD attack. We consider this defense because adversarial training-based methods were shown to be the only reliable defense against evasion attacks [37; 6]. In Table 4, we show the clean accuracy and the robust accuracy (against CAA) of the adversarially trained models (big numbers in Table 4). We also show the accuracy difference with the models trained with standard training (small numbers). In Appendix B.4, we evaluate additional defenses based on adversarial training and data augmentations.

Adversarial training can degrade clean and robust performance.As a preliminary check, we investigate whether adversarial training degrades the clean performance of the models. This is important to ensure that a non-increase of robust accuracy does not originate from clean performance degradation (instead of being due to CAA's strength). Our evaluation shows that adversarial training significantly degrades clean performance of the STG and Tabnet architectures. The accuracy of STG models drops to 15.6% and 62.6% for LCLD and WIDS respectively. As for Tabnet models, the clean accuracy drops to 0.0% (LCLD) and 0.2% (CTU). In all other cases, clean accuracy remains stable.

   Dataset & Accuracy & TabTr. & RLN & VIME & STG & TabNet \\   & Clean & \(93.9_{+0.3}\) & \(95.2_{+0.8}\) & \(93.4_{+0.9}\) & \(94.3_{+1.0}\) & \(99.5_{+6.1}\) \\  & CAA & \(56.7_{+47.8}\) & \(56.2_{+45.4}\) & \(69.8_{+20.3}\) & \(90.0_{+32.0}\) & \(91.8_{+80.8}\) \\   & Clean & \(73.9_{+4.4}\) & \(69.5_{+1.2}\) & \(65.5_{-1.5}\) & \(15.6_{-50.8}\) & \(0.0_{-67.4}\) \\  & CAA & \(70.3_{+62.4}\) & \(63.0_{+63.0}\) & \(10.4_{+8.0}\) & \(12.1_{-41.5}\) & \(0.0_{-0.4}\) \\   & Clean & \(95.3_{+0.0}\) & \(97.3_{-0.5}\) & \(95.1_{+0.0}\) & \(95.1_{-0.2}\) & \(0.2_{-95.8}\) \\  & CAA & \(95.3_{+0.0}\) & \(97.1_{+3.0}\) & \(94.0_{+5.3}\) & \(95.1_{-0.2}\) & \(0.2_{+0.2}\) \\   & Clean & \(77.3_{+1.8}\) & \(78.0_{+0.5}\) & \(72.1_{-0.2}\) & \(62.6_{-15.1}\) & \(98.4_{+18.6}\) \\  & CAA & \(65.1_{+19.2}\) & \(66.6_{+5.7}\) & \(52.1_{+1.8}\) & \(45.2_{-18.6}\) & \(58.4_{+53.1}\) \\   

Table 4: CAA performances (XX+/-YY) against Madry adversarially trained model. XX refers to accuracy. YY is the difference between the accuracy of the adversarially trained model and standard training (cf. Table 3), such that ‘+’ means a higher accuracy for the adversarially trained model.

Figure 3: Impact of CAA budget on the robust accuracy for CTU dataset.

CAA remains effective against adversarial training for some architectures.The effectiveness of CAA against robust models is architecture- and dataset-dependent. The attack remains effective on VIME architecture applied to LCLD and WIDS, with robust accuracy as low as 10.1% and 52.2% respectively, as well as on RLN architecture on the WIDS dataset (66.6% robust accuracy and only +5.7% improvement compared to standard training). However, the robustness against CAA of Tabtransformer architecture is significantly improved on URL and LCLD datasets by respectively 47.8% and 62.4%, and marginally improved on WIDS dataset by 19.2%. Similarly, RLN robustness to CAA improves on URL (+45.4%) and LCLD (+63%).

## 6 Limitations

We identify three main limitations of our work.

_Marginal overhead of CAA:_ In scenarios where CAPGD struggles to attack tabular models, CAA can exhibit a computation overhead (<14%) compared to MOEVA. However, in 4 out of 5 evaluated settings, CAA is faster than MOEVA (up to 5 times).

_CAPGD effectiveness with complex constraints:_ CAPGD effectiveness drops when increasing the constraint's complexity in the number of constraints or the number of features involved in each constraint.

_Coherence of constraints:_ The mechanisms of CAA assume that the constraints definitions are sound. Incoherences between boundary constraints and feature relation constraints can lead to invalid adversarial examples with large \(\) budgets.

## 7 Broader Impact

Our work proposes CAA, the most effective evasion attack against constrained tabular ML. We also provided for each dataset at least one combination of architecture combined with AT where CAA can be mitigated. We expect that our work will have a more positive impact by leading to improved defenses in the scarcely explored field of robust tabular ML.

## 8 Conclusion

In this work, we first propose CAPGD, a new parameter-free gradient attack for constrained tabular machine learning. We also design CAA, a new Constrained Adaptive Attack that combines the best gradient-based attack (CAPGD) and the best search-based attack (MOEVA). We evaluate our attacks over four datasets and five architectures and demonstrated that our new attacks outperform all previous attacks in terms of effectiveness and efficiency. We believe that our work is a springboard for further research on the robustness of tabular machine learning and to open multiple research perspectives on constrained tabular ML. We hope that CAA will contribute to a faster development of adversarial defenses and recommend it as part of a standard evaluation pipeline of new tabular machine models.