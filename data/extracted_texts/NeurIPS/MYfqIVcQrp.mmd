# Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning

Xiaojun Guo\({}^{1}\) Yifei Wang\({}^{2}\) Zeming Wei\({}^{2}\) Yisen Wang\({}^{1,}\)\({}^{3}\)

\({}^{1}\)National Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{2}\)School of Mathematical Sciences, Peking University

\({}^{3}\)Institute for Artificial Intelligence, Peking University

Equal Contribution.Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn)

###### Abstract

With the prosperity of contrastive learning for visual representation learning (VCL), it is also adapted to the graph domain and yields promising performance. However, through a systematic study of various graph contrastive learning (GCL) methods, we observe that some common phenomena among existing GCL methods that are quite different from the original VCL methods, including 1) positive samples are not a must for GCL; 2) negative samples are not necessary for graph classification, neither for node classification when adopting specific normalization modules; 3) data augmentations have much less influence on GCL, as simple domain-agnostic augmentations (e.g., Gaussian noise) can also attain fairly good performance. By uncovering how the implicit inductive bias of GNNs works in contrastive learning, we theoretically provide insights into the above intriguing properties of GCL. Rather than directly porting existing VCL methods to GCL, we advocate for more attention toward the unique architecture of graph learning and consider its implicit influence when designing GCL methods. Code is available at https://github.com/PKU-ML/ArchitectureMattersGCL.

## 1 Introduction

Over the past few years, Self-Supervised Learning (SSL) has emerged as a promising approach towards utilizing abundant real-world data without costly human-annotated information . Among various SSL techniques, contrastive learning (CL) has established itself as the avant-garde framework for self-supervised visual representation learning . By pulling similar samples near and pushing dissimilar samples far apart, contrastive learning is able to learn semantically expressive representations and achieves huge success on multiple downstream tasks .

Inspired by the success of contrastive learning in the visual domain, contrastive learning is also extensively explored on graph data and achieves competitive performance to supervised ones . Despite technical varieties, most graph contrastive learning (GCL) methods share the same high-level skeleton as visual contrastive learning (VCL). Generally speaking, GCL applies augmentations to generate different views of the original graph, and learn node or graph representations by contrasting positive and negative data pairs. Though some works argue that GCL requires domain-specific designs for graph , the design of GCL generally obeys the same paradigm as VCL with three key components: data augmentations, positive pairs for feature alignment, and negative pairs for feature uniformity.

In this paper, we challenge the commonly held beliefs regarding GCL by revealing its distinct characteristics in comparison to VCL. Specifically, we perform a systematic study with a wide range of representative GCL methods on well-known benchmarks and find three intriguing properties: 1) for the positive pair part, we demonstrate that GCL can achieve competitive performance without adopting any positive pairs. This finding stands in stark contrast to VCL, which dramatically fails in the absence of positive samples. 2) for the negative pair part, we observe that for the graph classification task, GCL performs well without any special designs in the no-negative setting, which is a notable departure from VCL. 3) for the data augmentation part, for VCL, elaborate data augmentations are indispensable in boosting performance [5; 55]. However, we find that GCL is relatively robust under vanilla domain-agnostic data augmentations (_e.g.,_ random Gaussian noise).

To explain these intriguing properties of GCL, we delve deep into the model architecture, and uncover the interesting interplay between model components and graph contrastive objectives. **First**, we shed light on the implicit regularization mechanism of graph convolution in GCL. As is known, graph convolution encourages neighbor nodes to have similar features during propagation. We rigorously show that graph convolution implicitly minimizes a neighbor-induced alignment loss, which reveals its complementary relationship with the learning objective and elucidates the positive-free GCL. **Second**, we highlight the role of the projection head in GCL without negative samples or any specific designs, where we find for graph classification, the projection head implicitly selects a low-rank feature subspace to satisfy the loss. **Third**, for the node classification, by incorporating a normalization layer capable of driving nearby node features apart, we show that, without uniformity loss, a GCN encoder alone can prevent features from collapsing to a single point. We theoretically characterize this by connecting this normalization layer with a neighbor-induced uniformity loss.

These intriguing distinctive properties of GCL reveal that the design of contrastive learning can be very domain-specific. Importantly, as also shown in previous works [48; 37], the contrastive algorithm has an implicit interaction with the architecture. Therefore, experiences obtained from one domain (_e.g.,_ images) should not be directly considered universal. Instead, when designing graph self-supervised learning methods, we should consider the unique properties of graphs and graph-based models. We hope that our findings could inspire a better understanding of graph contrastive learning and pave the way for new approaches to self-supervised learning on graphs.

We summarize our main contributions below:

* We perform comprehensive evaluations of popular GCL methods across various benchmarks, and find intriguing and general properties of GCL compared with VCL: 1) GCL works without positive samples; 2) GCL works without negative samples on graph classification task; 3) GCL shows less dependence on delicately designed augmentations, where random Gaussian noise even works.
* We reveal the reasons behind the intriguing properties of GCL by theoretically uncovering the interplay between contrastive learning objectives and model architectures: 1) We shed light on the implicit regularization mechanism of graph convolution by establishing its connection with a neighbor-induced alignment objective; 2) We show the collapse-preventing effect of ContraNorm in the no-negative setting by theoretically characterizing its connection with a neighbor-induced uniformity loss.
* Our findings appeal to a re-examination of the real effectiveness of each component in GCL, and provide new insights for designing graph-specific self-supervised learning.

## 2 Related Work

**Contrastive Learning.** Contrastive learning has attracted intensive attention in self-supervised learning. Its primary objective is to learn a space where similar pairs are closely clustered while dissimilar pairs are far apart. The introduction of Information Noise Contrastive Estimation (InfoNCE)  propels contrastive learning to a new climax in the vision domain. Methods such as SimCLR  have achieved performance comparable to supervised methods on vision tasks by leveraging stronger augmentations, larger batch size, and a non-linear projection head. MoCo  employs a dynamic queue and a moving-averaged encoder to generate more negative samples. The heavy computational burden imposed by a large number of negative pairs prompts the search for alternatives in contrastive learning. BYOL  addresses this by enforcing diversity in representations through an asymmetric architecture and stop-gradient techniques. Barlow-Twins  proposes a feature regularization method that maximizes agreements between different views of a sample while eliminating redundancy. On the understanding of contrastive learning, Wang and Isola  highlight alignment and uniformity as key properties of contrastive loss, where alignment refers to the closeness of positive pairs and uniformity pertains to the uniform distribution of normalized features on the hypersphere. Wang et al.  establish the relationship between contrastive learning and graph neural networks. Zhuo et al.  analyze one kind of contrastive learning without negative samples like BYOL from the perspective of rank difference. Zhang et al.  reveal the connection between contrastive learning and masked image modeling. There are also other works studying how extra information helps contrastive learning .

**Graph Contrastive Learning.** Stimulated by the success of contrastive learning on images, assorted contrastive attempts are applied to graphs. Motivated by Deep InfoMax (DIM) , DGI  learns by maximizing mutual information between node representations and corresponding high-level summaries of graphs. InfoGraph  adopts the DIM principle on the graph classification task. Inspired by SimCLR, GRACE  maximizes the agreement of corresponding node representations in two augmented views for a graph. Similarly, GraphCL  learns graph-level representations by maximizing the global representations of two views for a graph. Building upon these pioneer efforts, a plethora of GCL methods have been proposed recently . Inspired by the sparsest cut problem, SCE  introduces a Laplacian smoothing trick and then proposes a model without positive samples but only using negative samples for training. It is noted that SCE adopts a specific design of the backbone network and learning objectives (details in Appendix A.2). Different from the above methods, our work does not focus on designing a specific graph contrastive method but generally discusses whether components like positive samples and negative samples are needed in the context of graph contrastive learning.

To relieve the learning from negative samples, some methods are also been proposed. Borrowing the idea of BYOL, BGRL  scales up graph contrastive learning to large-scale datasets. Along another line, Bielak et al. , Zhang et al.  add feature regularization objectives to make the cross-correlation matrix between two views close to an identity matrix, ensuring the two representations are distinguishable. However, these works are all proposed as a specific design that can not be straightforwardly transferred to other GCL methods.

In GCL, augmentations have also emerged as a subject of intensive research. Basic augmentations depend on the node features and topology information, _e.g.,_ node feature masking, edge perturbation, and subgraph sampling. Existing methods mostly adopt an empirical combination of basic graph augmentations , which is found to benefit more than augmentation of a single type . Adaptive selections of augmentations with learnable strategies have also been proposed . Moreover, some works infuse domain knowledge in finding proper augmentations , or design advanced augmentations from the spectral perspective . There are also works identifying limitations in existing task-irrelevant graph augmentations, and expect better practices in augmentations considering the graph-domain knowledge . However, our works are not intended to design stronger data augmentations, but we go another way that finding simple augmentations like random Gaussian noise also work on real-world graph datasets in GCL, while it causes a steep performance drop in VCL.

**Inductive Bias of Architecture to Contrastive Learning.** For the inductive bias of architecture, Saunshi et al.  proposes a general theoretical framework showing the importance of architecture inductive bias to standard contrastive learning. Trivedi et al.  presents comparable performances between GCL and untrained GCNs on some relatively simple benchmarks, showing the existence of inductive bias of GCL. In contrast, our work firstly uncovers what exactly the inductive bias of GNNs is by exploring the dynamic interplay between the GNN architecture and the contrastive optimization objective during training.

## 3 Preliminaries

Let \(=(,)\) be a graph with \(n\) nodes, where \(\) and \(\) are the node set and edge set. We denote \(^{n h}\) as the node attribute matrix with input dimension \(h\) and \(^{n n}\) as the adjacency matrix. Given augmentation functions \(_{1},_{2}\), where \(\) is the set of all possible augmentations defined on graphs, GCL generates two augmented views \(}_{1}=_{1}()\) and \(}_{2}=_{2}()\). Theseaugmented views are then embedded into representations via an encoder \(f\), which is often followed by a projection head \(g\). We denote the backbone features output by the encoder as \(=f()^{n m}\), and the projection output as \(=g()^{n d}\), where the dimension \(d\) is often smaller than \(m\). During the evaluation, the projection head is removed and only \(\) is used for downstream tasks. For graph-level tasks, a global representation can be further obtained by applying a pooling operation \(r\). The aim of GCL is to learn neural networks that embed a graph into representations by maximizing the representation consistency between different views of the input graph.

For the training objective, the InfoNCE loss  is widely used in GCL. Given an anchor view \(\), which can be a node representation \(=g(f(}))_{i}\) or the global representation of a graph \(=r(g(f(})))\), we denote its corresponding positive pair as \(\). Then, the InfoNCE loss of \(\) is defined as

\[_{}()=-, )/t)}{_{_{}}(s(,)/t)},\] (1)

where \(_{}\) is the set comprising negative samples of \(\), \(s(,)\) denotes the cosine similarity, and \(t\) is the temperature scale. The InfoNCE loss can be decoupled into two non-overlapping parts, each of which includes only positives or negatives, named as _alignment loss_ and _uniformity loss_:

\[_{}()=-s(,)/t, _{}()=_{_{ }}(s(,)/t).\] (2)

## 4 How GCL Works without Positive Samples

In this section, we investigate intriguing phenomena of GCL in the absence of positive samples, which are greatly different from the common understanding and practice in VCL. Specifically, we find that positive samples are not necessary for GCL. We highlight the importance of graph convolution in such discrepancy and provide theoretical insights for explaining the success of positive-free GCL. To ensure the validity of our conclusion, we choose seven highly cited GCL methods and evaluate on well-known benchmarks for both node classification and graph classification tasks. Following the convention of GCL, we use the linear-probing protocol for evaluation. We also report the results of the fine-tuning protocol in Appendix K. More details about the adopted methods, experimental settings, and benchmarks can be found in Appendix A. The proof of theorems is attached to Appendix F.

### Positive Samples Are NOT a Must in GCL

Upon examining existing approaches like SimCLR  and MoCo , it becomes evident that positive samples play a vital role in VCL. By maximizing the agreement between positive samples, the neural networks can effectively learn semantic information relevant to downstream tasks . It is widely recognized that without this alignment effect, learned representations may lose meaning and incur poor performance [53; 55]. To illustrate this, we conduct experiments on the CIFAR-10 dataset , comparing the InfoNCE loss (including positive samples) and the uniformity loss (excluding positive samples). As shown in Table 1, optimizing only the uniformity loss significantly degrades performance.

Considering the practice in VCL, one would naturally assume that positive samples are equally important and necessary in GCL. However, we unexpectedly find that many existing GCL methods achieve decent performance even without using any positive pairs. To demonstrate this, we conduct comprehensive experiments on both node classification and graph classification tasks. As shown in Table 2, the accuracy gap between the contrastive loss (_Contrast_) and the loss without positives (_NO Pos_) is relatively narrow across most node classification datasets. Similarly, in Table 3, we observe similar phenomena in graph classification, where using loss without positive samples sometimes even outperforms the contrastive loss. Additionally, we perform experiments on randomly initialized

  Loss & Accuracy (\%) \\  NO Training & 27.20 \(\) 0.9 \\ InfoNCE & 83.51 \(\) 0.3 \\ Uniformity & 27.51 \(\) 0.5 \\ Alignment & 29.67 \(\) 0.8 \\  

Table 1: Linear probing accuracy (%) of VCL with SimCLR on CIFAR-10: InfoNCE loss, uniformity loss (no positives), alignment loss (no negatives) and no training (random initialization).

[MISSING_PAGE_FAIL:5]

### The Implicit Regularization of Graph Convolution in GCL

The intriguing property of positive samples in GCL encourages us to explore the underlying reasons behind this phenomenon. It is worth noting that all the GCL methods analyzed above adopt message-passing graph neural networks (GNNs) like GCN  as backbone encoders. We aim to demonstrate that these GNNs inherently possess an implicit regularization effect that facilitates the aggregation of positive samples. This finding helps elucidate why GCL can achieve satisfactory performance without explicitly incorporating an alignment objective.

To simplify the explanation, we focus on the vanilla graph convolution module proposed in GCN as an illustrative example. At the \(l\)-th layer, node representations are aggregated through two interleaving steps:

\[\;}=} ^{(l)},\] (3) \[\;^{(l+1)}=(}^{(l)}),\] (4)

where \(\) is the activation function, and \(^{(l)}\) denotes the weight matrix. \(}=}^{-1/2}}}^{-1/2}\) is the symmetrically normalized version of the self-loop augmented adjacency matrix \(}=+\), where \(}\) is the diagonal degree matrix of \(}\). While various variants of GCN have been proposed, most include generalized graph convolution (GraphConv) operators that bring neighbor features closer through message passing . For comparison, we also consider a vanilla MLP encoder, which extracts features from each node individually and can be seen as only applying the feature transformation step.

Importantly, we notice that the utilization of GraphConv within encoders is the key for most existing GCL methods to generalize well in the absence of positive samples. To demonstrate this, we compare two backbones: GCN (with GraphConv) and MLP (without GraphConv), on the node-classification task. The results are presented in Table 4. Remarkably, we observe that the distinctive property of GCL disappears under the MLP backbone. Compared to the performance achieved with the standard contrastive loss (_e.g.,_ 67.72% on Cora), the MLP-based GCL exhibits significantly lower performance (56.10%) under the no-positive setting. This highlights the importance of the feature propagation process in GraphConv, which underlies the unique positive-free behavior observed in GCL.

Here we provide theoretical insights into this phenomenon by uncovering the implicit regularization mechanism of GraphConv in graph contrastive learning. Through formal connections established between GraphConv and a neighbor-induced alignment objective, we demonstrate that GraphConv has the capability to replace the positive alignment loss in GCL. Consequently, GCL attains favorable performance even in the absence of explicit positive sample training.

**Theorem 4.1**.: _Suppose the positive node pairs \((x,x^{+}) P_{}\) is drawn from the following distribution defined via the normalized connection weight of the graph \(\)_

\[_{}(x,x^{+})=}_{x,x^{+}}}{_{u,v }}_{u,v}}, x,x^{}[N],\] (5)

_then a step of GraphConv (Eq 3) will decrease the following feature alignment loss between positive samples (here \(_{x}\) refers to the \(x\)-th row of a feature matrix \(\)):_

\[}_{}(_{x})=-_{x,x^{+} _{}(x,x^{+})}[_{x}^{}_ {x^{+}}].\] (6)

  
**Method** & **Loss** & **Encoder** & Cora & CiteSeer & PubMed & Photo & Computers & **Avg** & **Avg p-value** \\   & Contrast & MLP & 67.72 \(\) 0.88 & 65.51 \(\) 2.63 & 83.29 \(\) 0.49 & 87.92 \(\) 0.59 & 80.89 \(\) 1.21 & 77.07 & - \\   & NO Training & MLP & 40.66 \(\) 2.49 & 42.81 \(\) 4.82 & 78.53 \(\) 0.90 & 62.12 \(\) 0.97 & 57.97 \(\) 1.13 & 56.42 & 0.0020 \\    & NO Pos & MLP & 56.10 \(\) 1.08 & 49.82 \(\) 3.76 & 81.32 \(\) 0.77 & 65.25 \(\) 1.13 & 61.37 \(\) 0.74 & 62.77 & 0.0020 \\    & NO Neg & MLP & 51.69 \(\) 3.04 & 50.36 \(\) 1.14 & 79.00 \(\) 0.63 & 61.33 \(\) 0.75 & 55.07 \(\) 0.99 & 59.49 & 0.0020 \\   

Table 4: Test accuracy (%) of node classification benchmarks using GRACE method with MLP encoder. We compare the performances of models trained with InfoNCE loss (Contrast), uniformity loss (NO Pos), alignment loss (NO Neg), and no training objective (NO Training). Mean accuracy with standard derivation is reported after 10 runs. Average accuracy across datasets is reported. We conduct significance testing using Wilcoxon Signed Rank Test , comparing the contrastive loss with other loss types. The p-value is averaged across datasets. A value below 0.05 denotes a significant accuracy difference (\(\)), while a value above 0.05 indicates insignificance (\(\)).

Theorem 4.1 reveals that GraphConv implicitly achieves feature alignment among neighborhood samples. This alignment process is particularly effective in homophilic graphs, where neighbors predominantly belong to the same class. In this context, the neighbors essentially act as high-quality positive samples. As a result, the neighbor-induced alignment loss can effectively cluster intra-class samples together, providing a plausible explanation for the success of positive-free GCL. Interestingly, the connection between graph convolution and the alignment objective can also provide a natural explanation for why Yang et al.  works, which applies graph convolution to a trained MLP and observes improved performance. This strategy, from our perspective, is amount to further training the MLP features with a neighbor-induced alignment loss for a few steps (thus no severe feature collapse), thus helps improve MLP's performance.

## 5 How GCL Works without Negative Samples

In this section, we investigate intriguing phenomena of GCL in the absence of negative samples. There are works showing contrastive learning can get rid of negative samples by specific designs on architectures [46; 80] or objective functions [71; 2]. However, we observe that negative samples are dispensable without any specific designs for the graph classification task, whereas in the node classification task, simply removing them may not be sufficient. From the perspective of feature collapse, we emphasize the significant role played by the projection head in the graph classification task. Building upon this insight, we address the collapse issue in the node classification task by modifying the backbone encoder with a specialized normalization technique, and further give a theoretical explanation. The experimental settings are identical to those in Section 4, and the proof of theorems is attached to Appendix F.

### Graph Classification: Both Negative Samples and Specific Designs Are Not Needed

In the context of VCL, it is widely recognized that the removal of negative samples alone leads to the failure of methods. This is primarily attributed to the fact that without negative samples, the alignment loss can be easily minimized by adopting a shortcut solution where the encoder generates a constant feature for all samples, _i.e.,_\(f(x)=c,\ x\). As a consequence, this collapsed feature lacks discriminative power for downstream tasks, resulting in a phenomenon referred to as feature collapse . To empirically demonstrate this issue, we present the evident performance degradation observed in pure no-negative VCL in Table 1.

In contrast to the VCL scenario, our findings in GCL for the graph classification task reveal a notable difference. Surprisingly, we discover that GCL can perform well by utilizing the vanilla positive alignment loss alone, without the need for negative samples or any modifications to the network architecture. As demonstrated in Table 3, models trained exclusively with positive pairs achieve comparable or even superior performance compared to those trained with the default contrastive loss.

Further investigation into the architecture reveals the intriguing role of the projection head in the no-negative setting. Specifically, we estimate the average similarity of the representations \(=f()\) output by the encoder and \(=g()\) output by the projection head (Figure 1(a)). The similarity of \(\) is close to 1, indicating that the projection head indeed learns a collapsed solution. However, the similarity of \(\) is much lower. It is worth noting that in common GCL practice, the projection head is removed after training, and downstream tasks employ \(\) for evaluation. Therefore, thanks to the projection head, although the model learns a collapsed solution for optimizing the alignment loss, the downstream results remain unaffected.

To gain further insights into the role of the projection head, we explore the mechanism from a spectral perspective. We compare the singular value distributions of representations before and after the projection head on the MUTAG dataset (Figure 1(b)). The distribution of singular value becomes more concentrated after the projection head, indicating a decrease in rank. To validate this, we estimate the ranks of \(\) and \(\), revealing that the rank of \(\) is noticeably lower than that of \(\) (Figure 1(c)). Inspired by Gupta et al. , we remove the projection head during training (where \(g()\) is an identity function, making \(\) equal to \(\)). Comparing the rank of resulting representation \(^{}\), we find that it falls between the ranks of \(\) and \(\). These observations indicate that the projection head implicitly selects a low-rank subspace of features to satisfy the alignment loss.

### Node Classification: Normalization in the Encoder Is Enough

When it comes to the node classification task, the absence of negative samples in GCL leads to a significant performance drop (Table 2), unlike the case in graph classification. Numerous factors may be responsible for the suboptimal performance. To confirm feature collapse is the underlying cause, we visualize the training process with alignment loss, where the average node similarities of \(}\) and \(\) both unite towards one at the end of training (see Appendix D).

Notably, different from graph classification, the representations learned by the encoder also collapse in the node classification task. One plausible conjecture is that learning a collapsed solution is relatively easier for the global graph representation, which can be achieved solely by the projection head. In this case, the encoder is preserved from collapse. However, for learning local node representations, the alignment loss requires each node to be collapsed, which often needs the encoder's involvement. We provide empirical insights in Appendix E, while a rigorous theoretical understanding remains a topic for future work.

Now, the question arises: how does GCL manage to work without negative samples for node classification? While previous solutions derived from VCL [11; 6; 69; 1], such as asymmetric architectures [46; 22] or feature decorrelation objectives [2; 71], exist, they are specific designs which cannot be easily generalized to other methods. Recalling that the feature collapse can be traced back to the encoder, a straightforward approach is directly changing the encoder to prevent collapse. Specifically, we find that just incorporating a normalization component called ContraNorm (CN)  into the encoder of GCL is enough.

ContraNorm is originally designed for alleviating the over-smoothing problem in GNNs and Transformers with the formulation:

\[()=-}.\] (7)

Here, \(\) is hidden representation, and \(\) is a hyper-parameter for scaling. \(\) is the diagonal degree matrix of \(}\), where \(}=(^{})\) computes the row-wise normalized similarity matrix between node features. Here, we use a degree-normalized variant of ContraNorm and are the first to introduce it as a novel extension to GCL. As seen from Table 5, by simply incorporating the normalization layer into the encoder, the collapse issue can be rooted out for the GRACE method. Importantly, this normalization layer can be easily adapted by other GCL methods in a plug-and-play

Figure 1: Metrics before and after the projection head. \(\) and \(\) denote the representations before and after the projection head, respectively. In Figure 1(c), we also report the rank of representations \(^{*}\) when training without the projection head. Experiments are conducted with GraphCL method.

  
**Method** & **Loss** & **Encoder** & Cora & CiteSeer & PubMed & Photo & Computers & **Avg** & **Avg p-value** \\   & Contrast & GCN & \(84.67 1.39\) & \(73.47 2.32\) & \(85.80 0.16\) & \(91.42 1.27\) & \(89.01 0.60\) & \(84.87\) & - \\    & NO Neg & GCN & \(29.85 1.45\) & \(20.42 2.26\) & \(39.63 0.81\) & \(25.10 1.74\) & \(36.84 1.30\) & \(30.37\) & \(0.0020\) \\    & NO Neg & GCN + CN & \(82.35 2.28\) & \(72.25 1.86\) & \(83.30 0.63\) & \(92.43 0.82\) & \(84.48 1.01\) & \(82.96\) & \(0.1621\) \\   

Table 5: Test accuracy (%) of node classification benchmarks using GRACE method. We compare the performances of models trained with InfoNCE loss (Contrast), alignment loss (NO Neg), and alignment loss with ContraNorm in encoder (GCN+CN). Mean accuracy with standard derivation is reported after 10 runs. Average accuracy across datasets is reported. We conduct significance testing using Wilcoxon Signed Rank Test , comparing the default setting (first line) with others. The p-value is averaged across datasets. A value below 0.05 denotes significant accuracy difference ( red), while a value above 0.05 indicates insignificance ( green).

manner. We validate the effectiveness of ContraNorm on multiple GCL methods and under different encoders. The results are provided in Appendix H due to space constraints. It is worth noting that our proposed approach maintains a symmetric model architecture with only the alignment loss, highlighting the ability of the normalization in the encoder for no-negative GCL to perform well.

### ContraNorm Performs Negative Uniformity Implicitly

In this part, we explain how ContraNorm prevents feature collapse without other special designs in the no-negative setting. Similar to the GraphConv case, we define a uniformity loss among all nodes in the same graph to promote feature diversity, namely the neighbor-induced uniformity loss. The following theorem proves that the update of ContraNorm layer leads to a decrease in this uniformity loss.

**Theorem 5.1**.: _Suppose the sample \(x\) is drawn from the same distribution in Theorem 4.1, the neighbor-induced uniformity loss is defined as_

\[}_{}=_{x P_{}(x)} [_{x^{{}^{}} P_{}(x)}(_{x}^ {}_{x^{{}^{}}})].\] (8)

_The gradient update of this uniformity loss with step size \(>0\) gives the ContraNorm update (Eq. 7)._

The derived update discussed in Guo et al.  suggests that the ContraNorm layer can implicitly promote the diversity among node features during the propagation process. This explains why combining GCL with ContraNorm can avoid feature collapse without explicitly relying on any negative samples.

By analyzing the roles of GraphConv and ContraNorm, we notice that in contrast to visual contrastive learning where the encoders primarily extract features from individual samples, the feature propagation layers in GNNs (GraphConv and ContraNorm) also capture the interaction between different samples. This property enables them to effectively replace the roles of inter-sample objectives like the alignment and uniformity losses. In other words, the feature propagation layers inherently encode the necessary information for learning meaningful representations without explicitly relying on inter-sample objectives. Specifically, by combining Theorem 4.1 and Theorem 5.1, we show that the joint update using graph convolution and ContraNorm implicitly optimizes a neighbor-induced contrastive learning loss:

**Theorem 5.2**.: _The joint update of GraphConv and ContraNorm, i.e.,_

\[_{}=(+})- }\] (9)

_corresponds to a gradient descent update of the following contrastive learning loss:_

\[}_{}& =}_{}+}_{ }\\ &=_{x,x^{+} P_{}(x,x^{+})}[_{ x}^{}_{x^{+}}]+_{x P_{}(x)}[_{x^{{}^{ }} P_{}(x)}(_{x}^{}_{x^{{}^{ }}})].\] (10)

## 6 Simple Augmentations Do Not Destroy GCL Performance

Data augmentation is the arguably most crucial component of VCL methods, since different kinds of data augmentations have a dramatic influence on its final performance. When examining Table 6, we observe a substantial degradation (83.51% \(\)30%) in VCL's performance when removing all augmentations or applying only random Gaussian noise. However, we find that data augmentations have a much smaller influence on GCL methods.

In GCL, basic augmentations are typically domain-specific, tailored to the node features and topology information, _e.g.,_ node feature masking, edge perturbation, and subgraph sampling. Here we choose the combination of node feature masking (FM) and edge perturbation (EP) as the baseline augmentation, which is widely adopted in GCL methods [77; 79; 46; 71].

   Augmentation & Accuracy(\%) \\  NO Aug & 28.29 \(\) 1.0 \\  Default Aug & 83.51 \(\) 0.3 \\ Gaussian & 36.56 \(\) 1.2 \\   

Table 6: Linear probing accuracy (%) of VCL with SimCLR on CIFAR-10 with different augmentation settings.

Taking the node classification as an example, following common practices when unknowing the data prior [33; 70], we further consider a simple augmentation: random Gaussian noise, where a random noise sample drawn from a Gaussian is directly added to node features. Formally, given a graph \(=(,)\), the random noise augmentation is defined as \(()=(,+),(0,^{2})\). In practice, we select the standard deviation \(\) selected from \([1e-4,5e-4,1e-5]\). For comparison, we also include the no augmentation setting. The results are shown in Table 7. Although independent of the graph structure and node attributes, the random noise augmentation still achieves a comparable performance compared to domain-specific augmentations, which is quite different from the observations in VCL (Table 6). For further verification, we also conduct experiments with MLP and under different loss settings, the details are shown in Appendix I.

The robustness of GCL to random noise augmentations highlights its flexibility and resilience in the absence of domain-specific augmentations. Recalling Section 4, graph contrastive learning is equipped with two kinds of alignment properties: alignment loss and graph convolution. Therefore, when the effect of alignment loss is weakened (corresponding to domain-agnostic augmentations) or even removed (corresponding to no augmentations), the performance of GCL is relatively slightly influenced with the graph convolution as backing.

## 7 Discussion and Conclusion

In this paper, we have shown that GCL exhibits many intriguing phenomena that are rather contradictory to those in VCL. Specifically, we have found that GCL can work in the absence of positive samples. Second, GCL works well without negative pairs for the graph classification task. Third, GCL can achieve comparable performance with domain-agnostic data augmentations like random Gaussian noise. We have made these observations via extensive experiments with a wide range of representative GCL methods. However, the empirical experiments cannot cover all of the GCL methods. We indeed find some exceptions and give a concrete discussion in Appendix G, where exceptional observations can be attributed to the individual property of methods.

Notably, we highlight the implicit mechanisms of architectures to contrastive learning. Theoretically, we build the connection between graph convolution and a neighbor-induced alignment loss, as well as the connection between ContraNorm and a neighbor-induced uniformity loss, giving explanations for the above unique properties of GCL. Overall, our method suggests that graph contrastive learning may behave quite differently from its visual counterpart, and more efforts should be brought in for designing graph-specific self-supervised learning.

Since the main goal of this work is to examine the roles of each component of GCL objectives, one limitation is that it does not propose a new GCL method. Nevertheless, we believe that the new findings in this work would be valuable for future GCL designs. Also, the paper does not examine other SSL paradigms on graph, like masked modeling, which would be an interesting direction to explore in the future.