# On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions

Yusu Hong

Center for Data Science

and School of Mathematical Sciences

Zhejiang University

yusuhong@zju.edu.cn

&Junhong Lin

Center for Data Science

Zhejiang University

junhong@zju.edu.cn

The corresponding author is Junhong Lin.

###### Abstract

In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise. We show that Adam with a specific hyper-parameter setup can find a stationary point with a \(}(1/)\) rate in high probability under this general noise model where \(T\) denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. Moreover, we show that under the same setup, Adam without corrective terms and RMSProp can find a stationary point with a \(}(1/T+_{0}/)\) rate which is adaptive to the noise level \(_{0}\). We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately.

## 1 Introduction

Since its introduction by , the Stochastic Gradient Descent (SGD): \(_{t+1}=_{t}-_{t}_{t}\) has achieved significant success in solving the unconstrained stochastic optimization problems:

\[_{^{d}}f(), f()= _{}[f_{}(,)],\] (1)

where \(\) is a random variable, \(_{t}\) is the stochastic gradients and \(_{t}\) is the step-size. From then on, numerous literature focused on the convergence behavior of SGD in various scenarios. Several studies focused on the non-convex smooth scenario where the stochastic gradient \(g()\) is unbiased with affine variance noise, i.e., for some constants \(_{0},_{1} 0\) and all \(^{d}\),

\[[\|g()- f()\|^{2}]_{0}^{2}+_{1}^{ 2}\| f()\|^{2}.\] (2)

Under the noise assumption (2),  provided an almost-sure convergence bound for SGD.  proved that SGD could reach a stationary point with a \((1/)\) rate when step-sizes are tuned by problem-parameters such as the smooth parameter \(L\). The theoretical result also revealed that the analysis of SGD under (2) is not essentially different from the bounded noise case .

In the popular field of deep learning, a range of variants based on SGD, known as adaptive gradient methods have emerged. These methods employ the past gradients to adaptively tune their step-sizes and are preferred to SGD for minimizing various objective functions due to their efficiency. Among these methods, Adam  has been one of the most effective methods empirically. Generallyspeaking, Adam absorbs some key ideas from previous adaptive methods such as AdaGrad (12; 37) and RMSProp (38) while adding more unique structures. It combines the exponential moving average mechanism from RMSProp and meanwhile adds the heavy-ball style momentum (30) and two unique corrective terms. This unique structure leads to a huge success for Adam in practical applications but at the same time brings more challenges to the theoretical analysis.

Considering the significance of affine variance noise and Adam in both theoretical and empirical fields, it's natural to question whether Adam can find a stationary point at a rate comparable to SGD under the same smooth condition and (2). Earlier researches (14; 41; 2) have shown that AdaGrad-Norm, a scalar version of AdaGrad, can find a stationary point at the same rate as SGD, not tuning step-sizes based on problem-parameters. Moreover, they addressed an essential challenge brought by the correlation of adaptive step-sizes and noise from (2) which does not appear in SGD's cases. However, since AdaGrad-Norm applies a cumulative step-sizes mechanism which is rather different from the exponential moving average step-sizes in Adam, the analysis for AdaGrad-Norm could not be trivially extended to Adam. Furthermore, the coordinate-wise step-size architecture of Adam, rather than the unified step-size for all coordinates in AdaGrad-Norm, brings more challenge when considering (2). In affine variance noise landscape, existing literature could only ensure the Adam's convergence with random-reshuffling scheme under certain parameter restrictions (53; 42), or deduce the convergence at the expense of requiring bounded gradient assumption and using problem-parameters to tune the step-sizes (18), both of which ignored the corrective terms. Some other works proved convergence to a stationary point by altering the original Adam algorithm such as removing certain corrective terms and modifying (2) to a stronger coordinate-wise variant (40; 20).

To the best of our knowledge, existing research has not yet fully confirmed the convergence of Adam under affine variance noise. To address this gap, we conduct an in-depth analysis and prove that Adam with the right parameter can find a stationary point in high probability. We assume a milder noise model (detailed in Assumption (A3)), covering almost surely affine variance noise, the bounded noise, and sub-Gaussian noise. We show that the convergence rate can reach at \((( T)/)\) matching the lower rate in (1) up to logarithm factors. Our proof employs the descent lemma over the introduced proxy iterative sequence and adopts techniques related to the new proxy step-sizes and error decomposition. Based on this, we are able to handle the correlation between stochastic gradients and adaptive step-sizes and transform the first-order term from the descent lemma into the gradient norm.

Finally, we apply the analysis to the \((L_{0},L_{q})\)-smooth condition (52). Several researchers have found empirical evidence of objective functions satisfying \((L_{0},L_{q})\)-smoothness but out of \(L\)-smoothness range, especially in large-scale language models (50; 39; 11; 8). Theoretical analysis of adaptive methods under this relaxed condition is more complicated and needs further nontrivial proof techniques. Also, prior knowledge of problem-parameters to tune step-sizes is needed, as indicated by the counter-examples from (41) for the AdaGrad. Existing works (13; 41) obtained a convergence bound for AdaGrad-Norm with (2), and (25) considered Adam with sub-Gaussian noise. In this

    & FCT & Grad. & Noise & Smooth & \(_{1},_{2}\) & \(\) & Conv. Rate & Conv. Type \\ 
 & ✗ & Bounded & Bounded & \(L\) & \(1-_{2} c^{2}\) & poly\(()\) & \(+^{2}\) & E \\
 & ✗ & Bounded & Bounded & \(L\) & \(_{1,K},_{1,B},_{2}=1-\) & \(}\) & E \\
 & ✗ & Bounded & - & \(L\) & \(_{1}=1-\) & poly\(()\) & \(}\) & E \\
 & ✗ & - & Finite Sum Affine & \(L\) & \(T(_{1},_{2}) 0\)1 & - & - & E \\
 & ✗ & Bounded & Bounded & \(L\) & \(_{1}<_{2},_{1}=1-\) & poly\(()\) & \(}\) & E \\
 & ✗ & Bounded & **Affine** & \(L\) & \(_{1}=1-}\) & poly\(()\) & \(}\) & E \\
 & ✓ & - & Finite Sum Affine & \(L\) & \(_{1}<},_{1}=1-\)1  & \(}\) & E \\
 & ✗ & - & Finite Sum Affine & \((,L_{1}})\) & \(_{1}<}\) & - & - & E \\
 & ✓ & - & Sub-Gaussian & \((,L_{1}})\) & \(_{1}=1-}\) & \(}\) & \(}\) & **w.b.p.** \\
 & ✗ & - & Coordinate-wise Affine & \(L\) & \(_{1}=b},_{2}=1-\) & poly\(()\) & \(}\) & E \\
 & ✓ & - & Coordinate-wise Affine & \(L\) & \(_{1}<_{2},_{2}=1-\) & poly\(()\) & \(}\) & **w.b.p.** \\
**Thm. 3.1** & ✓ & - & **Affine** & \(L\) & \(_{1}<_{2},_{2}=1-\) & poly\(()\) & \(}\) & **w.b.p.** \\
**Thm. 4.1** & ✓ & - & **Affine** & \((,L_{1}})\) & \(_{1}<_{2},_{2}=1-\) & poly\(()\) & \(}\) & **w.b.p.** \\   

\({}^{1}\) requires \(T(_{1},_{2})=(}{_{2}^{2}}( }{-_{1}}+1)) 0\), which seems could only achieve when \(_{1}=0\).

\({}^{2}\) Though not explicitly stated, the results in (Zhang et al., 2022) could imply convergence to the stationary point when with some calculations.

\({}^{3}\) “FCT” refers to “full corrective terms”. The “Conv. rate” column presents the convergence rate omitting logarithm factors.

Table 1: Comparison for existing Adam analyses with ours.

paper, we provide a probabilistic convergence result for Adam with the affine variance noise and the generalized smoothness condition.

We also refer readers to see the main contributions of our works and comparisons with the existing works in Table 1.

**Notations** We use \([T]\) to denote the set \(\{1,2,,T\}\) for any positive integer \(T\), \(\|\|\), \(\|\|_{1}\) and \(\|\|_{}\) to denote \(l_{2}\)-norm, \(l_{1}\)-norm and \(l_{}\)-norm respectively. \(a(b)\) and \(a(b)\) denote \(a=C_{1}b\) and \(a C_{2}b\) for some positive universal constants \(C_{1},C_{2}\), and \(a}(b)\) denotes \(a(b)( b)\). \(a b\) denotes \(a(b)\). For any vector \(^{d}\), \(^{2}\) and \(}\) denote coordinate-wise square and square root respectively. \(_{i}\) denotes the \(i\)-th coordinate of \(\). For any two vectors \(,^{d}\), we use \(\) and \(/\) to denote the coordinate-wise product and quotient respectively. \(_{d}\) and \(_{d}\) represent zero and one \(d\)-dimensional vectors respectively.

## 2 Problem set up and algorithm

We consider unconstrained stochastic optimization (1) over \(^{d}\) with \(l_{2}\)-norm. The objective function \(f:^{d}\) is differentiable. Given \(^{d}\), we assume a gradient oracle that returns a random vector \(g(,)^{d}\) dependent by the random sample \(\). The true gradient of \(f\) at \(\) is denoted by \( f()^{d}\).

Assumptions.We make the following assumptions throughout the paper.

* Bounded below: There exists \(f^{*}>-\) such that \(f() f^{*},^{d}\);
* Unbiased estimator: The gradient oracle provides an unbiased estimator of \( f()\), i.e., \(_{}[g(,)]= f(), {x}^{d}\);
* **(A3)** Generalized affine variance noise: The gradient oracle satisfies that there are some constants \(_{0},_{1}>0,p[0,4)\), \(_{}[(,)- f() \|^{2}}{_{0}^{2}+_{1}^{2}\| f()\|^{p}})] (1),^{d}\).

The first two assumptions are standard in the stochastic optimization. The third assumption provides a mild noise model that covers the almost surely bounded noise and sub-Gaussian noise. Moreover, it's more general than almost surely affine variance noise as follows

\[\|g(,)- f()\|^{2}_{0}^{2}+_{1}^{2}\|  f()\|^{2},a.s.,\] (3)

and enlarge the range of \(p\) to \([0,4)\). Assumption (**A3**) with \(p=2\) and (3) are also utilized in  to establish high probability results for AdaGrad-Norm. It represents a stronger condition than the expected version of (2) that is commonly employed for deriving the expected convergence of algorithms. However, almost surely assumption enables the derivation of stronger high-probability convergence guarantees for algorithms, while still ensuring expected convergence.

The affine noise variance assumption is important for machine learning applications with feature noise (including missing features) [15; 22], in robust linear regression , and generally whenever the model parameters are multiplicatively perturbed by noise (e.g., a multilayer network, where noise from a previous layer multiplies the parameters in subsequent layers). We refer interested readers to see e.g., [3; 46; 4; 14; 41; 2] for more discussions about the affine variance noise.

Adam.For the stochastic optimization problem, we study Algorithm 1, which is an equivalent form of Adam  with the two corrective terms for \(_{s}\) and \(_{s}\) included into \(_{s}\) for notation simplicity.

The iterative relationship in Algorithm 1 can be also written as for any \(s[T]\),

\[_{s+1}=_{s}-_{s}(1-_{1})_{s}}{_{s}}+_{s}}+_{1}(_{s-1}}+ {}_{s-1})}{_{s-1}(_{s}}+_{s})}(_{s}-_{s-1}),\] (4)

where we let \(_{0}=_{1}\) and \(_{0}=\). (4) plays a key role in the convergence analysis, showing that Adam incorporates a heavy-ball style momentum and dynamically adjusts its momentum through \(_{1}\) and \(_{2}\), along with adaptive step-sizes. This inspires us to learn from some classical analysis methods for algorithms with momentum and provides some new estimations to fit in with the adaptive property.

## 3 Convergence of Adam with smooth objective functions

In this section, we assume that the objective function \(f\) is \(L\)-smooth satisfying that for any \(,^{d}\),

\[\| f()- f()\| L\|-\|.\] (5)

We then show that Adam has the following high probability results.

**Theorem 3.1**.: _Let \(T 1\) and \(\{_{s}\}_{s[T]}\) be the sequence generated by Algorithm 1. If Assumptions (A1)-(A3) hold, and the hyper-parameters satisfy that_

\[0_{1}<_{2}<1,_{2}=1-c/T,=C_{0}},=_{0}},\] (6)

_for some constants \(c,C_{0}>0\) and \(_{0}>0\), then for any given \((0,1/2)\), it holds that with probability at least \(1-2\),_

\[_{s=1}^{T}\| f(_{s})\|^{2}\{G^ {2}(^{2}+_{1}^{2}G^{p}+G^{2}}{T}}+}{T})()\},\]

_where \(G^{2}\) is defined by the following order with respect to \(T,_{0},\):2_

\[G^{2}(^{\{2,\}}( })).\] (7)

Theorem 3.1 provides the nearly optimal convergence rate \((( T)/)\) to find a stationary point when setting the parameter probably: \(_{2}=1-(1/T)\). It's worth noting that the setting requires \(_{2}\) to be closed enough to \(1\) when \(T\) is sufficiently large, which roughly aligns with the typical setting in . For a more detailed comparison of our results to existing works, including assumptions, convergence rate, and dependency, we refer readers to Table 1.

Adam without corrective terms and RMSProp.We also consider a simplified version of Adam that drops two corrective terms as shown in Algorithm 2 in Appendix. Algorithm 2 with \(_{1}=0\) can be directly reduced to RMSProp . More importantly, the following result shows that the convergence rate of Algorithm 2 is adaptive to the noise level.

**Theorem 3.2** (informal version of Theorem C.2).: _Let \(T 1\) and \(\{_{s}\}_{s[T]}\) be generated by Algorithm 2 covering RMSProp. Following the assumptions and setup in Theorem 3.1, with probability at least \(1-2\), \(_{s=1}^{T}\| f(_{s})\|^{2}/T}(1/T+ _{0}/)\)._

## 4 Convergence of Adam with generalized smooth objective functions

In this section, we study the convergence behavior of Adam in the generalized smooth case. We first provide some necessary introduction to the generalized smooth condition.

### Generalized smoothness

We consider the following \((L_{0},L_{q})\)-smoothness condition: there exist constants \(q[0,2)\) and \(L_{0},L_{q}>0\), satisfying that for any \(,^{d}\) with \(\|-\| 1/L_{q}\),

\[\| f()- f()\|(L_{0}+L_{q}\| f()\|^{q}) \,\|-\|.\] (8)

The generalized smooth condition was originally put forward by  for any twice differentiable function \(f\) satisfying that

\[\|^{2}f()\| L_{0}+L_{1}\| f()\|.\] (9)

It has been proved that a lot of objective functions in experimental areas satisfy (9) but out of \(L\)-smoothness range, especially in training large language models, see e.g., Figure 1 in  and .

To better understand the theoretical significance of the generalized smoothness,  provided an alternative form in (8) with \(q=1\), only requiring \(f\) to be differentiable. They showed that (8) is sufficient to elucidate the convergence of gradient-clipping algorithms.

There are three key reasons for opting for (8). Firstly, considering our access is limited to first-order stochastic gradients, it's logical to only assume that \(f\) is differentiable. Second, as pointed out by Lemma A.2 in  and Proposition 1 in , (8) and (9) are equivalent up to constant factors when \(f\) is twice differentiable considering \(q=1\). Thus, (8) covers a broader range of functions than (9). Finally, it's easy to verify that (8) is strictly weaker than \(L\)-smoothness. A concrete example is that the simple function \(f(x)=x^{4},x\) does not satisfy any global \(L\)-smoothness but (8). Moreover, the expanded range of \(q\) to \([0,2)\) is necessary as all univariate rational functions \(P(x)/Q(x)\), where \(P,Q\) are polynomials and double exponential functions \(a^{(b^{x})}\) with \(a,b>1\) are \((L_{0},L_{q})\)-smooth with \(1<q<2\) (see [25, Proposition 3.4]). We refer interested readers to see  for more discussions of concrete examples of generalized smoothness.

### Convergence result

We then provide the high probability convergence result of Adam with \((L_{0},L_{q})\)-smoothness condition as follows.

**Theorem 4.1**.: _Let \(T 1\) and \((0,1/2)\). Suppose that \(\{_{s}\}_{s[T]}\) is a sequence generated by Algorithm 1, \(f\) is \((L_{0},L_{q})\)-smooth satisfying (8), Assumptions (A1)-(A3) hold, and the parameters satisfy_

\[0_{1}<_{2}<1,_{2}=1-c/T,= _{0}},=_{0}},\] \[_{0}\{E_{0},}{}, }{},(1-_{1})^{2}(1-_{1} /_{2})}{4L_{q}^{2}d}}\},\] (10)

_where \(c,_{0},E_{0},_{0}>0\) are constants, \(\) is controlled by \(((}))\)3, and \(H,,\) are defined as_

\[H :=L_{0}/L_{q}+(4L_{q})^{q}+(4L_{q} )^{}+(4L_{0})^{}+4L_{q}+(4L_{q})^{}+},\] \[ :=^{2}+_{1}^{2}H^{p}+H^{2})( )},:=L_{0}+L_{q}(H^{q}+H+}{L_{q}})^{q}.\] (11)

_Then it holds that with probability at least \(1-2\),_

\[_{s=1}^{T}\| f(_{s})\|^{2}\{ }{_{0}}(^{2}+_{1}^{2}H ^{p}+H^{2}}{T}}+}{T})() \}.\] (12)

Note that in the above theorem, the order of \( T\) in \(\) and the final convergence bound is better than the one in Theorem 3.1 under the same noise assumption. This better dependency comes from the expense of using problem parameters to tune step-size \(_{0}\). Since \(\) is logarithm orderof \(T\), \(H,,\) are both polynomial logarithm order of \(T\) and the final convergence rate in (12) is \((( T)/)\) order. Note that \(_{0}(1/( T))\) from (10) when \(T d\). Hence, when \(T\) is large enough, a possible optimal setting is that \(=c_{1}/(( T))\) for some constant \(c_{1}>0\), which roughly matches the typical setting as mentioned before.

Similarly, we also obtain a convergence bound that is adaptive to the noise level for Adam without corrective terms and RMSProp under generalized smoothness.

**Theorem 4.2** (informal version of Theorem E.1).: _Let \(T 1\) and \(\{_{s}\}_{s[T]}\) be generated by Algorithm 2 covering RMSProp. Following the assumptions and the same order of hyper-parameters in Theorem 4.1, with probability at least \(1-2\), \(_{s=1}^{T}\| f(_{s})\|^{2}/T}(1/T+ _{0}/)\)._

## 5 Related works

There is a large amount of works on stochastic approximations (or online learning algorithms) and adaptive variants, e.g., [5; 36; 48; 29; 12; 4; 6; 27; 55] and the references therein. In this section, we will discuss the most related works and make a comparison with our main results.

### Convergence with affine variance noise and its variants

We mainly list previous literature considering (2) over non-convex smooth scenario.  provided an asymptotic convergence result for SGD with (2). In terms of non-asymptotic results,  proved the convergence of SGD, illustrating that the analysis was non-essentially different from the bounded noise case from .

In the adaptive methods field,  studied convergence of AdaGrad-Norm with (2), pointing out that the analysis is more challenging than the bounded noise and bounded gradient case in . They provided a convergence rate of \(}(1/)\) without knowledge of problem parameters, and further improved the bound adapting to the noise level: when \(_{1}(1/)\),

\[_{t=1}^{T}\| f(_{t})\|^{2}}(}{}+).\] (13)

(13) matches exactly with SGD's case , showing a fast rate of \(}(1/T)\) when \(_{0}\) is sufficiently low. Later,  proposed a deep analysis framework obtaining (13) with a tighter dependency to \(T\) and not requiring any restriction over \(_{1}\). They further obtained the same rate for AdaGrad under a stronger coordinate-wise version of (2): for all \(i[d]\),

\[_{}|(,)_{i}- f()_{i}|^{2} _{0}^{2}+_{1}^{2}| f()_{i}|^{2}.\] (14)

 obtained a probabilistic convergence rate for AdaGrad-Norm with (3) using a novel induction argument to estimate the function value gap without any requirement over \(_{1}\) as well.

In the analysis of Adam, a line of works [35; 53; 42] considered Adam for finite-sum objective functions under different regimes while possibly incorporating natural random shuffling technique. They could ensure that this variant converged to a bounded region where

\[_{t[T]}[\{\| f(_{t})\|,\| f( _{t})\|^{2}\}]}+C_{1}_{0}\] (15)

under the affine growth condition which is equivalent to (2). Though not explicitly concluded, when setting \(_{2}=1-(1/T)\), 's work can also ensure a convergence rate of order \(}(1/)\) under certain settings. Besides, both  and  provided convergence bounds allowing for large heavy-ball momentum parameter that aligns more closely with practical settings. However, they relied on the assumption for step-sizes where \(C_{l}\|_{t}+_{t}}}\|_{} C_{u}, t [T]\).  and  used distinct methods to derive convergence bounds in expectation and high probability respectively, without relying on bounded gradients. Both studies achieved a convergence rate of the form in (13) for Adam ignoring the corrective terms.  further achieved a \(}(1/)\) rate for Adam. However, the two works only studied coordinate-wise affine variance noise.

In this paper, we derive a stronger high probability convergence rate for Adam with original corrective terms, relying on an almost surely noise assumption. The noise model is general enough to cover bounded noise, sub-Gaussian noise, and (coordinate-wise) affine variance noise. Although we consider a stronger almost surely assumption, our probabilistic convergence result is also stronger than the expected convergence.

### Convergence with generalized smoothness

The generalized smooth condition was first proposed for twice differentiable functions by  (see (9)) to explain the acceleration mechanism of gradient-clipping. This assumption was extensively confirmed in experiments of large-scale language models . Later,  further relaxed it to a more general form in (8) allowing for first-order differentiable functions. Subsequently, a series of works [31; 54; 33] studied different algorithms' convergence under this condition.

In the field of adaptive methods,  provided a convergence bound for AdaGrad-Norm assuming (2) and (8) with \(q=1\), albeit requiring \(_{1}<1\). Based on the same conditions,  improved the convergence rate to the form in (13) without restriction on \(_{1}\).  explored how Adam without corrective terms behaves under generalized smoothness with \(q=1\) and (2). However, they could only assert convergence to a bounded region as shown in (15).  showed that an Adam-type algorithm converges to a stationary point under a stronger coordinate-wise generalized smooth condition. Recently,  provided a novel framework to derive high probability convergence bound for Adam under the generalized smooth and sub-Gaussian noise case.

In this paper, we consider a more general noise setup and investigate Adam's convergence under the generalized smooth landscape. We prove that Adam is powerful enough to find a stationary point with properly tuned step-sizes even under these relaxed assumptions. Moreover, the convergence rate is not harmed by the relaxation of noise and smoothness, matching the optimal \((1/)\) rate up to logarithm factors.

### Convergence of Adam

Adam was first proposed by  with empirical studies and theoretical results on online convex learning. The original proof of convergence in  was later shown by  to contain gaps.  and the subsequent work  also showed that for a range of momentum parameters chosen independently with the problem instance, Adam does not necessarily converge even for convex objectives. Many works have focused on its convergence behavior in non-convex smooth fields. A series of works studied Adam ignoring corrective terms, all requiring a uniform bound for gradients' norm. Among these works,  demonstrated that Adam can converge within a specific region if step-sizes and decay parameters are determined properly by the smooth parameter.  proposed a convergence result to a stationary point and required all stochastic gradients must keep the same sign. To circumvent this requirement,  introduced a convergence bound only requiring hyper-parameters to satisfy specific conditions.  conducted a simple proof and further improved the dependency on the heavy-ball momentum parameter. Recently,  introduced Nesterov-like acceleration into Adam and AdamW  indicating their superiority in convergence over the non-accelerated versions. For Adam-related works under (2) or generalized smoothness, we refer readers to Sections 5.1 and 5.2.

We also want to highlight that a series of works [24; 45; 51] investigated the geometry of Adam from an \(l_{}\)-norm perspective.  and  studied the geometry of Adam by regarding it as a variant of SignSGD and  showed that full-batch Adam converges towards a linear classifier that achieves the maximum \(l_{}\)-margin when the training data are linearly separable.

## 6 Proof sketch under the smooth case

In this section, we provide a proof sketch of Theorem 3.1 with some insights and proof novelty. Our proof borrows some ideas from [44; 10; 14; 2; 40; 20]. The detailed proof is in Appendix B.

Preliminary.To start with, we let the stochastic gradient \(_{s}=(g_{s,i})_{i}\), the true gradient \( f(_{s})=}_{s}=(_{s,i})_{i}\) and \(_{s}=(_{s,i})_{i}=_{s}-}_{s}\). We also let \(_{s}=^{s}}\) and thus \(_{s}=_{s}_{d}\). For any positive integer \(T\) and \((0,1)\), we define \(_{T}=T/)}\). We denote the adaptive part of the step-size as

\[_{s}:=_{s}}+_{s}=_{s-1}+( 1-_{2})_{s}^{2}}+_{s}.\] (16)We define two auxiliary sequences \(\{_{s}\}_{s 1}\) and \(\{_{s}\}_{s 1}\),

\[_{1}=_{d},_{1}=_{1},_{s}= }{1-_{1}}(_{s}-_{s-1}),_{s}=_{s}+_{s}, s  2.\] (17)

We follow from  which was used to prove the convergence of SGD with momentum and later applied to handle many variants of momentum-based algorithms. Recalling the iteration of \(_{s}\) in (4), we reveal that \(_{s}\) satisfies

\[_{s+1}=_{s}-_{s}_{s}}{_{s}}+}{1-_{1}}(_{s-1}}{_{s-1}_{s}}- {1}_{d})(_{s}-_{s-1}).\] (18)

In addition, given \(T 1\), we define, \( s[T]\),

\[G_{s}=_{j[s]}\|}_{j}\|,_{T}(s)=_{T} ^{2}+2_{1}^{2}G_{s}^{p}+2G_{s}^{2}},_{T}= _{T}^{2}+2_{1}^{2}G^{p}+2G^{2}},\] (19)

where \(G\) is as in Theorem 3.1. Both \(G_{s}\) and \(_{T}(s)\) will serve as upper bounds for gradients' norm before time \(s\). We will verify their importance in the later argument.

Starting from the descent lemma.We fix the horizon \(T\) and start from the standard descent lemma of \(L\)-smoothness. Then, for any given \(t[T]\), combining with (18) and summing over \(s[t]\),

\[f(_{t+1})  f(_{1})+^{t}-_{s}  f(_{s}),_{s}}{_{s}}}_{} +}{1-_{1}}_{s=1}^{t}_{ s}(_{s}-_{s-1}), f(_{s})}_{}}\] \[+_{s=1}^{t}\|_{s}_{s}}{_{s}}-}{1-_{1}}(_{s}(_ {s}-_{s-1}))\|^{2}}_{}},\] (20)

where we let \(_{s}=_{s-1}}{_{s-1}_{s}}-_{d}\) and use \(_{1}=_{1}\) from (17). In what follows, we will estimate \(\), \(}\), and \(}\) respectively.

Probabilistic estimations.To proceed with the analysis, we next introduce two probabilistic estimations showing that the norm of the noises and a related summation of martingale difference sequence could be well controlled with high probability. We show that with probability at least \(1-2\), the following two inequalities hold simultaneously for all \(t[T]\):

\[\|_{t}\|^{2}_{T}^{2}(_{0}^{2}+_{1}^{2 }\|}_{t}\|^{p}),\] (21)

\[-_{s=1}^{t}_{s}}_{s},_{s}}{ _{s}}_{T}(t)}{4_{T}}_{s=1}^{t }_{s}\|}_{s}}{_{s}}}\|^{2}+D_{1} _{T},\] (22)

where \(D_{1}\) is a constant defined in Lemma B.7 and \(_{s}\) will be introduced later. In what follows, we always assume that (21) and (22) hold for all \(t[T]\) and carry out our subsequent analysis with some deterministic estimations.

Estimating A.We first decompose \(}\) as

\[}=^{t}-_{s}}_{s },_{s}}{_{s}}}_{}+^{t}_{s}}_{s}- f(_{s}),_{s}}{_{s}}}_{}.\]

Due to the correlation of the stochastic gradient \(_{s}\) and the step-size \(_{s}/_{s}\), the estimating of \(}\) is challenging, as also noted in the analysis for other adaptive gradient methods, e.g., . To break this correlation, the so-called proxy step-size technique is introduced and variants of proxy step-size have been introduced in the related literature. However, to our best knowledge, none of these proxy step-sizes could be used in our analysis for Adam considering potential unbounded gradients under the noise model in Assumption (A3). In this paper, we construct a proxy step-size \(_{s}/_{s}\), with \(_{s}\) relying on \(_{T}(s)\) in (19), defined as for any \(s[T]\),

\[_{s}=_{s-1}+(1-_{2})(_{T}(s) _{d})^{2}}+_{s}.\] (23)

With the so-called proxy step-size technique over \(_{s}/_{s}\) and \(_{s}=_{s}-}_{s}\), we decompose **A.1** as

\[=-_{s=1}^{t}_{s}\|}_{s}}{_{s}}}\|^{2}^{t}_{s}} _{s},_{s}}{_{s}}}_{}+ ^{t}_{s}}_{s},( {_{s}}-_{s}})_{s}}_{}.\]

In the above decomposition, the first term serves as a descent term. **A.1.1** is now a summation of a martingale difference sequence which could be estimated by (22). **A.1.2** is regarded as an error term when introducing \(_{s}\). However, due to the delicate construction of \(_{s}\), the definition of local gradients' bound \(_{T}(t)\), and using some basic inequalities, we show that

\[_{s=1}^{t}_{s}\|}_{ s}}{_{s}}}\|^{2}+_{T}(t)}} {1-_{1}}_{s=1}^{t}\|_{s}}{_{s}}\|^{2}.\]

The first RHS term can be eliminated with the descent term while the summation of the last term can be bounded by

\[_{s=1}^{t}\|_{s}}{_{s}}\|^{2}_{s=1} ^{t}\|_{s}}{_{s}}\|^{2}_{s=1}^{t}\| _{s}}{_{s+1}}\|^{2}_{s=1}^{t}\|}_{s}}{_{s}}\|}( ^{T}}),\] (24)

due to the step-size's adaptivity, the iterative relationship of the algorithm, the smoothness of the objective function, as well as (21). Here, \(}_{s}=_{s}}{1-_{1}^{2}}\).

Estimating B and C.The key to estimate **B** is to decompose **B** as

\[=}{1-_{1}}_{s=1}^{t} _{s}(_{s}-_{s-1}),}_{s}}_{ }+}{1-_{1}}_{s=1}^{t} _{s}(_{s}-_{s-1}), f(_{s})-}_{s }}_{}.\]

To estimate **B.1**, we use the updated rule and further decompose \(_{s}(_{s}-_{s-1})\) as the following terms

\[-(}{_{s}}-}{_{s}}) _{s-1}-(}{_{s}}-}{_{s-1}} )_{s-1}-(_{s}-_{s-1})_{s-1}}{ _{s-1}},\]

and upper bound the three related inner products. Using some basic inequalities, the smoothness, (24), and some delicate computations, one can estimate the three related inner products, **B.2** and **C**, and thus get that

\[+_{s=1}^{t}_{s}\|}_{s}}{_{s}}}\|^{2}+(b_{1}_{T}(t)+b_{2}) (^{T}}),\]

where \(b_{1}\) and \(b_{2}\) are positive constants determined by \(_{1},_{2},d,L,\).

Bounding gradients through induction.The last challenge comes from the potential unbounded gradients' norm. Plugging the above estimations into (20), we obtain that

\[f(_{t+1}) f(_{1})+(_{T}(t)}{4 _{T}}-)_{s=1}^{t}_{s}\|}_{s}}{ _{s}}}\|^{2}+c_{1}_{T}+(c_{2}_{T}(t)+c _{3})(^{T}}),\] (25)

where \(c_{1},c_{2},c_{3}\) are constants determined by \(_{1},_{2},d,L,\). Then, we will first show that \(G_{1} G\) and suppose that for some \(t[T]\),

\[G_{s} G, s[t]_{T}(s) _{T}, s[t].\] (26)It's then clear to reveal from (25) and the induction assumption that \(f(_{t+1})\) is restricted by the first-order of \(_{T}\). Moreover, \(f(_{t+1})-f^{*}\) could be served as the upper bound of \(\|}_{t+1}\|^{2}\) since

\[\|}_{t+1}\|^{2} 2\| f(_{t+1})\|^{2}+2\|}_{t +1}- f(_{t+1})\|^{2} 4L(f(_{t+1})-f^{*})+2\|}_{t +1}- f(_{t+1})\|^{2},\] (27)

where we use a standard result \(\| f()\|^{2} 2L(f()-f^{*})\) in smooth-based optimization. We also use the smoothness to control \(\|}_{t+1}- f(_{t+1})\|^{2}\) and combine with (26) and (27) to derive that

\[\|}_{t+1}\|^{2}_{1}+_{2}(_{1}G^{p/2}+G),\]

where \(_{1},_{2}\) are constants that are also determined by hyper-parameters and restricted by \(( T-T_{2})\) with respect to \(T\). Then, using Young's inequality,

\[\|}_{t+1}\|^{2}}{2}+_{1}+ p ^{}(_{1}_{2})^{}+( _{2})^{2}.\]

Thus, combining with a proper construction \(G^{2}\) (detailed in (53)), we could prove that

\[G^{2}=2_{1}+ p^{}(_{1} _{2})^{}+2(_{2})^{2},\]

which leads to \(\|}_{t+1}\|^{2} G^{2}\). Combining with the induction argument, we deduce that \(\|}_{t}\|^{2} G^{2}, t[T+1]\).

Final estimation.Following the induction step for upper bounding the gradients' norm, we also prove the following result in high probability:

\[L_{s=1}^{T}}{\|_{s}\|_{}}\|}_{s}\|^{ 2} L_{s=1}^{T}_{s}\|}_{s}}{_{s}} }\|^{2} G^{2}.\]

We could rely on \(_{T}\) to prove that \(\|_{s}\|_{}_{T}^{*}}+_{s},  s[T]\), and then combine with \(_{s}\) in Algorithm 1 to further deduce the desired guarantee for \(_{s=1}^{T}\|}_{s}\|^{2}/T\).

## 7 Conclusion

In this paper, we investigate the convergence of the Adam optimization algorithm on non-convex smooth problems under certain relaxed conditions. We begin by considering a mild noise assumption that encompasses several noise types, particularly the almost surely affine variance noise. Under this noise condition, we demonstrate that Adam can find a stationary point at a rate of \((( T)/)\) with high probability. Within our framework, we introduce a novel proxy step-size to manage the entanglement of stochastic gradients and adaptive step-sizes, and we employ a new decomposition method to estimate the errors introduced by the proxy step-size, the momentum, and the corrective terms in Adam.

We also extend our analysis to the convergence of Adam when the objective function is generalized smooth. This relaxed assumption is empirically validated to be more realistic in practical applications. Our results indicate that, with appropriate hyper-parameter tuning, Adam can find a stationary point at the same order of convergence rate as in the smooth case.

Limitations.Our study has several limitations that warrant further exploration. First, it would be advantageous to provide experimental results to validate the hyper-parameter settings in our results. Second, the convergence bound is not strictly tight compared to the lower bound, leaving a gap involving logarithmic factors, which may be improved in future work.