# Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function

Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function

 Chenyi Zhuang\({}^{1}\) Ying Hu\({}^{1}\) Pan Gao\({}^{1,2}\)

\({}^{1}\) Nanjing University of Aeronautics and Astronautics

\({}^{2}\) Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education

{chenyi.zhuang,ying.hu,pan.gao}@nuaa.edu.cn

Corresponding author

###### Abstract

Text-to-image diffusion models particularly Stable Diffusion, have revolutionized the field of computer vision. However, the synthesis quality often deteriorates when asked to generate images that faithfully represent complex prompts involving multiple attributes and objects. While previous studies suggest that blended text embeddings lead to improper attribute binding, few have explored this in depth. In this work, we critically examine the limitations of the CLIP text encoder in understanding attributes and investigate how this affects diffusion models. We discern a phenomenon of attribute bias in the text space and highlight a contextual issue in padding embeddings that entangle different concepts. We propose **Magnet**, a novel training-free approach to tackle the attribute binding problem. We introduce positive and negative binding vectors to enhance disentanglement, further with a neighbor strategy to increase accuracy. Extensive experiments show that Magnet significantly improves synthesis quality and binding accuracy with negligible computational cost, enabling the generation of unconventional and unnatural concepts. Code is available at https://github.com/I2-Multimedia-Lab/Magnet.

## 1 Introduction

Recently, Text-to-Image (T2I) diffusion models  have drawn considerable attention from both the research community and industry. Among these models, Stable Diffusion (SD)  uses the CLIP text encoder  to encode the given prompt, which is relatively lightweight than other diffusion models that adopt T5 . Unfortunately, generating text-aligned images is still challenging for SD and requires multiple runs to achieve the desirable results. Several works  have pointed out that the blended context by the CLIP text encoder causes improper binding. However, few have analyzed in detail how the text encoder affects the generation of the diffusion model.

Step back and refocus on the CLIP text encoder--an integral part of the Vision-Language Model (VLM). Prior studies  have observed VLMs lacking compositional understanding and investigated them on image-to-text retrieval benchmarks. In this work, we are motivated to answer **how the text encoder understands attribute**, and **how it affects the attribute binding of T2I diffusion models**. Upon closer inspection, we observe a phenomenon of attribute bias and discern a contextual problem in padding embeddings, leading to a well-known T2I issue--concept bleeding .

Based on the observation, we introduce the binding vector, which is applied to the text embedding of each object. With positive and negative binding vectors, each object can pull target attributes and push unrelated attributes to distinguish them from each other. We further introduce a neighbor strategy to ensure an accurate estimation of the binding vector. Our manipulation is performed strictly in the textual space, without training, fine-tuning, or additional datasets and inputs. Overall, the main contributions of this work are: (1) We highlight a contextual issue in the text encoder, which impacts diffusion-based image generation. (2) We propose a novel training-free method to address the binding problem. (3) Extensive experiments are conducted to verify the effectiveness of Magnet.

## 2 Analysis of the CLIP text encoder and the diffusion model

In this section, we aim to recognize the pattern of the CLIP text encoder for understanding attributes, then go deep into the diffusion model to analyze the underlying reason for improper attribute binding.

The CLIP text encoder uses the causal mask mechanism to produce a unidirectional context, i.e., each word can only consider the words to their left [13; 14]. It performs contrastive learning on a specific _End of Text_ ([EOT]) embedding without word-level supervision, while prior studies [15; 16; 17] suggest each _word_ has a semantic effect on the generated image. In this case, we categorize two types of embedding as **word** and **[EOT]** for fine-grained analysis. Consider a prompt encoded to text embeddings \(c=\{c_{SOT},c_{p_{1}},...,c_{p_{N}},c_{EOT}\}\) consisting \(N\) word embeddings. Different from the [EOT] embedding \(c_{EOT}\), word embeddings \(c_{p_{1}},...,c_{p_{N}}\) are unsupervised during training. We skip the embedding of _Start of Text_ ([SOT]) \(c_{SOT}\) for simplicity. To study how the two types of embedding understand attributes, we select 60 familiar objects and 7 common colors to obtain text embeddings \(c=\{c_{object},c_{EOT}\}\) and \(c^{}=\{c^{}_{color},c^{}_{object},c^{}_{EOT}\}\), i.e., without and with the color context, respectively. We aim to compare: (1) contextualized word embedding \(c^{}_{object}\) with the original \(c_{object}\) ; (2) contextualized [EOT] embedding \(c^{}_{EOT}\) with the original \(c_{EOT}\).

**How do two types of embedding understand attributes?** Fig. 1 (a) compares the Euclidean distance and the cosine similarity between embeddings with and without the color context. The pattern varies between cases or objects. As to the word embedding, the similarity curve of _"chair"_ is relatively smooth, but that of _"sheep"_ has a large gap between _"black"_ and _"white"_. Similarly, _"blue apple"_ diverges from others. The above phenomenon, which we call _attribute bias_, describes the tendency of an object to _favor_ certain attributes over others. We compare the attribute bias per object for two embedding types in Fig. 1 (b). If the object has a natural composition in human knowledge, it presents a serious attribute bias (e.g., _"yellow banana"_ v.s. _"blue banana"_). Meanwhile, the word embedding is more _volatile_ than the [EOT] embedding, which shows _less dramatic_ change. Our hypothesis is the absence of word-level supervision during CLIP training, as well as the bags-of-words behavior of VLMs [11; 12]: **the [EOT] embedding is trying to _remember_ all important words in the given prompt**, including adjectives and nouns. However, it leads to an inaccurate textual representation of the [EOT] embedding, affecting the interaction between the image latent and semantic word embeddings. We have provided more analysis and examples in Appendix A.1 (see Fig. 12, 13).

**How do two types of embedding affect SD?** In practice, SD pads the input prompt to a fixed length \(L=77\) using additional [EOT] embeddings (i.e., the padding token is initialized by the symbol [EOT]), denoted as \(c_{padi_{l}}\), where \(l=1,...,L-N-2\). To study the attribute binding during generation, we modify the above two text embeddings \(c\) and \(c^{}\) with \(L-N-2\) padding token embeddings, then design 4 fine-grained cases: (1) standard generation conditioned on \(c^{}\) where all text embeddings contain the color context; (2) only replace \(c^{}_{object}\) with \(c_{object}\) to eliminate the context on the word embedding; (3) only replace \(c^{}_{EOT}\) with \(c_{EOT}\), as well as padding embeddings \(c^{}_{pad_{l}}\) with \(c_{pad_{l}}\); (4) eliminate the color context on word, [EOT], and padding embeddings. Fig. 2 (a) presents 3 examples,

Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.

including a natural concept _"red chair"_, unnatural concepts _"black sheep"_ and _"blue apple"_. Cases 1-2 can be observed in all examples with less realistic and painting-like images. **Note that the used SD is trained to generate photo-realistic images**. These results indicate a deviation from the learned distribution. Conversely, cases 3-4 produce realistic images, but neglect the target color when the concept is unnatural. This suggests that the context in [EOT] and padding embeddings do have a significant impact on attribute generation. In Appendix A.2, we describe the above 4 cases in detail and provide more examples, as well as 3 additional cases (see Fig. 14).

**Why improper binding?** We posit that the first [EOT] has a close-knit context with word embeddings. The padding embedding, however, may deviate due to the causal attention mechanism. We then compute the cosine similarity between [EOT] and each padding embedding \(l=1,...,L\), and dive into their cross-attention activations on single- and multi-concept scenarios. Fig. 2 (b) studies a prompt with only one object. The curve drops more drastically on the unnatural concept _"blue apple"_ than the natural one _"red chair"_. Cross-attention shows that _"apple"_ overlaps with padding embeddings (e.g., \(pad_{73}\)) rather than the [EOT] embedding. **It is like these padding embeddings have _forgotten_ part of the context remembered in the first [EOT]_. Without interference from other concepts, the inaccurate context in these padding embeddings causes out-of-distribution, or the binding of another attribute if the model learns an underlying bias in the training dataset (e.g., _"apple"_ prefers _"red"_). Fig. 2 (c) further studies the context issue on multiple concepts. For two objects _"bird"_ and _"car"_, even though the activations of their word embeddings do not overlap, cross-attention shows obvious entanglement in these padding embeddings. This multi-concept context issue in padding embeddings, i.e., entangled concept representations, explains color leakage and object sticking. We refer the reader to Appendix A.3 for a comprehensive analysis of this context issue.

**How to disentangle different concepts?** Prior studies [8; 18] prove that these padding embeddings are essential for image quality and can not simply be removed. Also, it is impossible to manipulate one single concept in these padding embeddings due to their entangled property. On the other hand, these naturally separated word embeddings show editability. For instance, Fig. 2 (a) _"black sheep"_ from case 2 to case 1 changes only the word embedding of _"sheep"_ while encouraging the desired attribute. We are inspired to manipulate the word embedding of each object, therefore strengthening the binding within each concept and enhancing the distinction between concepts.

## 3 Magnet: disentangling concepts with the binding vector

Our approach is based on two key observations: the context issue of the padding embedding, and the controllability of the word embedding. We introduce the **binding vector**, which can be applied on the object embedding to attract the target attribute and repulse other attributes, analogous to a **Magnet**.

**Preliminary:** Given a prompt \(\), we use Stanza's dependency parsing module  to extract each _concept_, denoted \(A\&E\), where \(E\) is the object word and its target attribute as \(A\). The dependency

Figure 2: (a) Fine-grained study through our designed embedding swapping experiment. The context issue in padding embeddings for (b) single-concept scenario, and (c) multi-concept scenario.

set with \(M\) concepts is \(=\{A_{1}\&E_{1},...,A_{M}\&E_{M}\}\). Detailed dependency extraction is given in Appendix B.1. Then the pre-trained CLIP text encoder \(\) is applied to map \(\) to the text embedding \(c=\{c_{SOT},c_{A_{1}},c_{E_{1}},...,c_{A_{M}},c_{E_{M}},c_{EOT},c_{pad_{1}},..., c_{pad_{L-N-2}}\}\). For simplicity, we omit the linking words. We treat the diffusion model as a black box and leave its background in Appendix B.2.

For each object \(E_{i}\) in \(\) with the word embedding \(c_{E_{i}}\), we aim to estimate its positive binding vector \(v_{i}^{pos}\) to pull the target attribute \(A_{i}\), and its negative binding vector \(v_{i}^{neg}\) to push other attributes.

### Apply the binding vector on the object embedding

Instinctively, the binding vector can be estimated by the object itself. To be specific, we compose new concepts _out of the current context_ of \(\), which are: (1) unconditional concepts, \(}_{i}^{uc}=\{\&E_{i}\}\), where \(\) is a blank text ""; (2) positive concepts, \(}_{i}^{pos}=\{A_{i}\&E_{i}\}\); (3) negative concepts, \(}_{i}^{neg}=\{A_{j}\&E_{i}|j=1,...,M,j i\}\). The positive and negative binding vectors are estimated by:

\[v_{i}^{pos} =(E_{i},}_{i}^{pos})-(E_{ i},}_{i}^{uc})\] (1) \[v_{i}^{neg} =(E_{i},}_{i}^{neg})-(E_ {i},}_{i}^{uc})\] (2)

where \(()\) extracts the word embedding of the object \(E_{i}\) in a specific _decontextualized_ prompt. Note each object has \(M-1\) negative concepts, resulting in \(M-1\) negative binding vectors to punish all unrelated attributes \(A_{j},j=1,...,M,j i\). Note that these positive and negative attributes are prompt-dependent 2. We introduce the unconditional concept as a pivot to avoid the need for manual definition or semantic contrast between positive and negative attributes.

Based on our analysis of the context issue in the padding embedding in Section 2, we hypothesize an association between the attribute bias and the strength. Intuitively, unnatural concepts (e.g., _"blue banana"_) suffer more attribute bias and their padding embeddings are more tend to _forget_ the concept. In this case, we need to manipulate the word embedding significantly to ensure strong binding. We introduce the **adaptive strength** of the binding vector for each object \(E_{i}\):

\[_{i}=e^{-_{i}},_{i}=1-_{i}^{2}, _{i}=cos((}_{i}^{pos}),( }_{i}^{pos}))\] (3)

where \(()\), \(()\) extract the first [EOT] embedding and the last padding embedding in text embeddings \((}_{i}^{pos})\), respectively. \(\) is a positive constant. Please refer to Appendix B.3 for the inspiration of the formula, and the statistical analysis for the choice of the hyperparameter \(\).

Finally, the object embedding \(c_{E_{i}}\) in the initial text embeddings \(c=()\) is modified by:

\[_{E_{i}}=c_{E_{i}}+_{i} v_{i}^{pos}-_{i} v_{i}^{ neg}\] (4)

### Neighbor-guided vector estimation

In practice, we find that using a single object to estimate the binding vector can be inaccurate and fail to disentangle concepts (see Fig. 7 and Fig. 19). In this case, we introduce the **neighbor strategy**

Figure 3: Overview of the proposed Magnet. We manipulate the object embedding with the positive and negative binding vectors, which are estimated with the guidance of neighbor objects.

to ensure an accurate estimation. These neighbor objects should have similar representations to the target object in the learned textual space. We define a candidate set \(=\{B_{1},...,B_{R}\}\) with \(R\) objects that has pre-processed to \(\{c_{B_{1}},...,c_{B_{R}}\}\), which is the collection of the word embedding \(c_{B_{r}}\) in \((B_{r})=\{c_{SOT},c_{B_{r}},c_{EOT},...,r=1,...,R\). The top-\(K\) neighbor objects for the target object \(E_{i}\) are determined by \(d(c_{B_{r}},(E_{i},}_{i}^{uc}))\), where \(B_{r}\), \(d()\) denotes the cosine similarity.

In Appendix B.4, we describe this neighbor strategy in detail, and further discuss a way to predict semantic neighbors using pre-trained large language models.

With the selected neighbors of the target object \(E_{i}\), denoted \(\{B_{k}^{(i)}\}_{k=1}^{K}\), we compose the unconditional concepts \(}_{k}^{uc}=\{\&B_{k}^{(i)}\}\), positive concepts \(}_{k}^{pos}=\{A_{i}\&B_{k}^{(i)}\}\), and negative concepts \(}_{k}^{neg}=\{A_{j}\&B_{k}^{(i)}|j=1,...,M,j i\}\). The estimation of the binding vector is then rewritten as:

\[v_{i}^{pos} =_{k=1}^{K}((B_{k}^{(i)},}_{k}^{pos})-(B_{k}^{(i)},}_{k}^{uc}))\] (5) \[v_{i}^{neg} =_{k=1}^{K}((B_{k}^{(i)},}_{k}^{neg})-(B_{k}^{(i)},}_{k}^{uc}))\] (6)

### Overall workflow

Fig. 3 depicts the workflow of Magnet. The target text embedding \(\) can be obtained after replacing all object embeddings \(c_{E_{i}}\) with \(_{E_{i}},i=1,...,M\). To generate the image, the pre-trained U-Net denoises the latent \(z_{t-1}=_{}(z_{t},t,)\), where timesteps \(t=T,...,1\). We set the hyperparameters \(=0.6\), \(K=5\). Please refer to Appendix C for implementation details.

## 4 Experiments

### Datasets

We evaluate our proposed Magnet on two existing benchmarks:

**(1) Attribute Binding Contrast set (ABC-6K) .** This dataset consists of natural compositional prompts from MS-COCO , each prompt includes at least two concepts (e.g., _"a bathroom with a tan sink and white toilet"_, _"a brown cow standing in a lush green field"_). We randomly sample 600 prompts from this dataset and generate 5 images per prompt to compare all methods.

**(2) Concept Conjunction 500 (CC-500) .** The dataset contains prompts that conjunct two concepts, each with one color attribute. Following , objects are divided into two types: living (i.e., animals and plants) and other non-living nouns. Prompts type is categorized into (1) two living objects, (2) one living object and one non-living object, and (3) two non-living objects. We adopt 80 prompts for each case to avoid bias and maintain fairness. In total, we have used 240 prompts and generated 10 images for each prompt to compare all methods.

Both datasets are augmented using contrast settings . The position of attribute words for different objects is swapped (e.g., "a red chair and a blue cup" \(\) "a blue chair and a red cup").

### Metrics

We mainly rely on human evaluation since the common metrics (e.g. CLIP text-image similarity) are unreliable for assessing attribute binding, which is discussed in Appendix D.

**Coarse-grained comparison**. We assess the generated image for image quality and concept disentanglement on two adopted datasets. To measure _image quality_, human evaluators were asked "Which image is more realistic or visually appealing?". The evaluation of concept disentanglement is divided into two types: (1) _object disentanglement_ by asking "Which image shows different objects more clearly?"; (2) _attribute disentanglement_ by asking "Which image shows different attributes more clearly?". If all images are equally good or bad, evaluators can indicate "no winner". We randomly sample one image for each prompt from ABC-6K and two images for each prompt from CC-500 to conduct this coarse-grained comparison.

**Fine-grained comparison**. This comparison is conducted on the CC-500 dataset based on two key criteria: (1) _object existence_, counting the target objects in the generated images; (2) _attribute alignment_, concerning the correct binding between the object and its attribute. We ask annotators to identify the object mentioned in the prompt per generated image. Take prompt _"a red car and a yellow cat"_ as an example, each image will be indicated the number two (show both objects), one (show either _"car"_ or _"cat"_), or zero (no distinct object). Attribute alignment is assessed by counting whether the generated object presents the desired attribute (maximum to the number of the generated objects). All generated images on CC-500 are used for this fine-grained comparison. In addition, we adopt the phrase grounding model GrondingDINO  to detect the target objects automatically. Note that this automatic detection can not reflect the proper binding.

### Quantitative comparison

**Coarse-grained comparison.** In Tab. 1, we present the human evaluation results of Magnet compared to three baseline methods: SD V1.4 , Structure Diffusion  and Attend-and-Excite . Note that Magnet and Structure Diffusion are both training-free. The ABC-6K benchmark has more complicated and challenging prompts. In this case, all methods may fail to include all objects and attributes, resulting in a higher number of _no winner_. Overall, Magnet achieves the best scores in terms of image quality and attribute disentanglement on both datasets.

**Fine-grained comparison.** As shown in Tab. 2, Magnet alleviates the missing problem more than Structure Diffusion on both automatic and manual evaluation, with 3.8% (Det.) and 4.6% (Obj.) improvement. We are inferior to the optimization method, Attend-and-Excite in object existence. In attribute alignment (Manual Attr.), Magnet outperforms all baseline methods. In addition, we compare the runtime and memory used for generation. The data is obtained by generating 100 prompts each with two images. Obviously, Attend-and-Excite requires more resources which affects efficiency. Conversely, Magnet only adds 2.9% to runtime and 6.5% to memory.

**Evaluation on image quality metric.** We also evaluate Magnet on the commonly used metric FID  for two SD versions (V1.4  and V2.1 ). We follow the standard evaluation process and generate 10k images from randomly sampled MS-COCO  captions. SD V1.4 gets \(19.04\), with Magnet \(18.92\); SD V2.1 gets \(19.76\), with Magnet \(19.20\), the lower the better. This shows that Magnet will not deteriorate the image quality while improving the text alignment.

    &  &  \\   & Image & Disentanglement & Image & Disentanglement \\  & Quality & Object & Attribute & Quality & Object & Attribute \\  Magnet (Ours) & 26.57 & 25.71 & 27.14 & 25.43 & 24.86 & 29.43 \\ Attend-and-Excite & 15.43 & 21.43 & 19.71 & 22.86 & 26.29 & 18.57 \\ Structure Diffusion & 12.28 & 7.14 & 10.29 & 12.29 & 6.86 & 11.14 \\ Stable Diffusion & 10.29 & 6.57 & 8.57 & 11.14 & 7.71 & 13.42 \\ No Winner & 35.43 & 39.15 & 34.29 & 28.28 & 34.28 & 27.44 \\   

Table 1: Coarse-grained comparison on the ABC-6k and CC-500 datasets for image quality, object disentanglement, and attribute disentanglement. Values are normalized to sum to 100.

    &  &  & Runtime & Memory Usage \\  Method & Det. & Conf. & Obj. & Attr. & (s) & (GB) \\  Stable Diffusion & 71.5 & 56.4 & 65.8 & 59.1 & 6.62 & 6.1 \\ Structure Diffusion & 72.1 & 56.0 & 64.0 & 63.9 & 7.94 (+20.0\%) & 7.0 (+14.7\%) \\ Attend-and-Excite & 84.3 & 62.6 & 84.6 & 66.2 & 13.4 (+102.4\%) & 15.6 (+155.7\%) \\ Magnet (Ours) & 76.5 & 59.8 & 68.6 & 74.0 & 6.81 (+2.9\%) & 6.5 (+6.5\%) \\   

Table 2: Fine-grained comparison on the CC-500 dataset. For reference, we provide the average confidence (Conf.) of GroundingDINO  to detect the object (Det.). Manual evaluation concerns the object existence (Obj.) and the attribute alignment (Attr.).

### Qualitative comparison

Fig. 4 shows the qualitative comparison of the ABC-6K and CC-500 datasets. The results demonstrate that baselines suffer from the entanglement of objects and attributes.

**Object entanglement** includes the neglect of the object or sticking structures. In columns 1-2, baselines struggle to be faithful to the complex prompt with 4 objects, missing _"fries"_ or _"tile"_. In columns 5-6, the objects _"banana"_ and _"stickers"_ are indistinguishable. Similarly, SD presents blended objects _"dog"_ and _"chair"_ in columns 7-8 and neglects the target object _"green apple"_ in columns 9-10. Note that the results of Structure Diffusion resemble that of SD. On the other hand, the optimization of Attend-and-Excite encourages the attendance of objects but leads to out-of-distribution results, showing strong artifacts (e.g., _"green apple"_ in columns 9-10).

**Attribute entanglement** includes the generation of incorrect attributes or the leakage of attributes. For instance, for the prompt _"a pink cake with white roses on silver plate"_ with three colors in columns 3-4, SD and Structure Diffusion generate _"white cake"_ and _"pink roses"_. In columns 7-8, they generate _"chair"_ with mixed colors _"yellow"_ and _"red"_. On the other hand, Attend-and-Excite may produce less aesthetic images, which can be attributed to the over-optimized image latent.

Notice that baselines fail to produce **unnatural concepts** like _"blue banana"_ in columns 5-6 in Fig. 4. Instead, they generate _"yellow banana"_, which is a natural concept learned as the prior knowledge. Conversely, Magnet is capable of disentangling different concepts and hence generating unnatural concepts, which we call the _anti-prior_ ability. Fig. 5 displays the results on prompts with anti-prior concepts. We skip Structure Diffusion for its limited improvement over SD.

Figure 4: Qualitative comparison using prompts from ABC-6K and CC-500 datasets. For each prompt, we show the image generated by each method under the same seed.

Figure 5: Prompts with unnatural concepts. Baselines generate exchanged colors (row 1) or unwanted artifacts (row 2) while Magnet demonstrates the anti-prior ability with high-quality outputs.

### Ablation study

**Hyperparameter \(\).** We study the effect of \(\) in Fig. 6. When setting \(=0\), \(,\) are still positive numbers but the manipulation is in relatively low strength. In this case, concepts are still entangled: _"roses"_ appear in shades of _"white"_ and _"pink"_. When setting \(=1\), the result presents artifacts: distorted _"plate"_ and watermarked background. We find using \(=0.6\) can achieve the balance between concept disentanglement and image quality based on the statistic analysis in Fig. 16.

**Selection strategy of the neighbor strategy.** The effectiveness of the neighbor strategy is shown in Fig. 7. The neighbors improve the estimation accuracy and the disentanglement of concepts. In Tab. 3, we ask human evaluators to evaluate both settings using the disentanglement criteria. Evaluators indicate the generated images using the neighbor strategy more disentanglement. This verifies the effectiveness of the neighbor-guided vector estimation.

**Effectiveness of the binding vector.** In Fig. 8, we verify the effectiveness of the binding vector by manually changing \(,\) instead of adaptively calculating by Eq. (3). The value of \(,\) changed from positive to negative shows a swapped binding between objects and attributes. This is because that the context problem in padding embeddings has caused the entanglement of concepts. Our proposed binding vectors can improve the discrimination between objects and lead to designated attributes.

We have conducted additional ablation experiments for the hyperparameter \(K\) (Appendix E.1, Fig. 19), and the importance of using both positive and negative binding vectors (Appendix E.2, Fig. 20).

### Extensions

**Incorporate with optimization-based methods.** Manipulated in the textual space, Magnet can be readily integrated with Attend-and-Excite. Fig. 9 (a) compares the optimization loss of Attend-and-Excite with and without Magnet. The loss can start at a lower value with Magnet to strengthen the distinction between concepts. Fig. 9 (b) shows vanilla Attend-and-Excite with strong artifacts or inaccurate colors, which should be attributed to the entangled concept representations in padding embeddings. More examples are displayed in Fig. 23 in the Appendix.

**Different text encoders.** In Fig. 10 (a) and (b), we assess Magnet on three T2I models with different text encoders to SD V1.4. Specifically, SD V2.1  adopts CLIP ViT-H/14, SDXL  combines

Figure 6: Ablation study on the hyperparameter \(\) given the prompt ”a pink cake with white roses on silver plate”. A small value of \(\) can not well disentangle different concepts, while a large value causes artifacts in the generated image (best viewed zoomed in). We empirically set \(=0.6\).

    &  \\  & Object & Attribute \\  w/ neighbor & **27.1** & **28.6** \\ w/o neighbor & 9.1 & 6.4 \\ Stable Diffusion & 2.3 & 2.1 \\ No winner & 61.5 & 62.9 \\   

Table 3: Ablation study. Human evaluators were asked to indicate which image can better separate attributes or objects.

Figure 7: Ablation study. The neighbor strategy improves the binding vector estimation, separating different attributes (_“cup”_ is purely _“blue”_) and objects (_“backpack”_ and _“apple”_ are distinguishable).

multiple CLIP text encoders, and PixArt  uses the T5 encoder . We use the same setting of all hyperparameters and equations for all CLIP-based models while using fixed strength for PixArt. The redesign of the strength formula for the adaptation of T5 is a matter for future work.

**Incorporate with T2I controlling modules.** In Fig. 10 (c) and (d), we investigate the plug-and-play nature of Magnet. Magnet shows compatibility when integrated with existing controlling modules: (1) layout-guidance , which constrains the image layout by bounding boxes and intervenes cross-attention layer, and (2) ControlNet  conditioned on Depth Map  to add spatial control.

**Image editing.** In Fig. 11, we compare the image editing ability of Magnet to Prompt-to-Prompt (P2P) , which edits the generated image by manipulating the cross-attention layers. Given the source prompt _"a car on the side of the street"_, we aim to change the attribute of the object _"car"_ or _"street"_. In column 1, Magnet applies a positive binding vector \(v^{pos}\) (here, the strength \(\) is stated manually) on the word embedding \(c_{E_{cor}}\), toward the attribute _"old"_. With no control of the attention maps, Magnet surprisingly edits the image with fewer changes in the background than P2P.

## 5 Related work

**Text-to-image diffusion models**. Diffusion models that  pioneered, have emerged with great improvement in both unconditional [31; 32] or conditional [28; 33] image generation, together with the advance in synthesis quality [34; 35] and sampling speed [36; 37; 38]. However, the semantic flaw of the text encoder affects the performance of the diffusion models [7; 10; 39]. In this work, we discern the attribute bias and the context issue, providing novel insights about attribute binding.

**Attribute binding**. The binding problem occurs when the model blends improper concepts. To tackle complicated prompts,  collaborates different pre-trained diffusion models.  suggests word embeddings with blended context and manipulate cross-attention features. In contrast, we highlight the entanglement of the padding embedding and modify solely the text embedding.  optimizes the latent to guarantee the attendance of each object. Yet, the optimization may lead to out-of-distribution and require more resources to generate images. Other works [40; 41; 42] introduce layout constraints in the attention layers. Magnet differs from the above approaches in that it can be executed entirely in the textual space. This distinguishes it as a more efficient solution.

It is noteworthy that a line of works [15; 43] achieves image editing on specific visual aspects. However, none have gone as far as this paper in exploring the contextual influences on SD from the perspective of text embedding. Most are subject to a subset of attributes (e.g., texture ), control the global object rather than fine-grained attributes [45; 46], or depend on a predefined text pair , requiring a learning process or additional datasets. Conversely, our method enhances binding towards arbitrary attributes without the need for new inputs to the standard pipeline.

Figure 8: Ablation study on the effectiveness of the binding vector.

Figure 9: Magnet can be combined with the optimization method, Attend-and-Excite . (a) Magnet improves the loss during optimization. (b) Magnet improves the disentanglement of concepts.

## 6 Limitations

While we have demonstrated improvement in the synthesis quality and text alignment, Magnet is still subject to a few limitations (see Fig. 21). First, it still suffers from the missing problem. In some cases, the manipulation may be overstrength and cause artifacts. An interesting phenomenon is that Magnet generates the correct concepts while rendering errors in positional relations. Finally, it is still challenging to generate an unnatural concept when the object is strongly biased towards one specific attribute. (e.g., _"broccoli"_). We have described the limitations of Magnet in detail in Appendix F.

## 7 Conclusion

In this work, we propose a novel training-free method, Magnet, to tackle the attribute binding issue. First, we conduct a fine-grained analysis of the CLIP text encoder. We observe the phenomenon of attribute bias and point out the context issue of padding embeddings, where the representations of different concepts are entangled, and hence provide potential explanations for existing T2I issues. Second, we introduce the positive and negative binding vectors to enhance the binding within the concept and strengthen the distinction between concepts. Further with the neighbor strategy, the vector estimation can be more accurate. Evaluated in various ways, Magnet shows the ability to disentangle different attributes and generate anti-prior concepts. Performed in the textual space, Magnet improves the synthesis quality and text alignment, with an impressively low increase in computational cost. We sincerely hope that this work will motivate the exploration of generative diffusion models and the discovery of other interesting phenomena.