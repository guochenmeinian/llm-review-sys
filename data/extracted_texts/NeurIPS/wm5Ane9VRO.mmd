# Maximization of Average Precision for

Deep Learning with Adversarial Ranking Robustness

 Gang Li

Texas A&M University

College Station, USA

gang-li@tamu.edu

&Wei Tong

General Motors

Warren, USA

wei.tong@gm.com

&Tianbao Yang

Texas A&M University

College Station, USA

tianbao-yang@tamu.edu

###### Abstract

This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial robustness, an area that has not been extensively explored to the best of our knowledge. AP maximization for deep learning has widespread applications, particularly when there is a significant imbalance between positive and negative examples. Although numerous studies have been conducted on adversarial training, they primarily focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. However, this type of adversarial robustness is insufficient for many applications, as minor perturbations on a single example can significantly impact AP while not greatly influencing the accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that combines an AP surrogate loss with a regularization term representing adversarial ranking robustness, which maintains the consistency between ranking of clean data and that of perturbed data. We then devise an efficient stochastic optimization algorithm to optimize the resulting objective. Our empirical studies, which compare our method to current leading adversarial training baselines and other robust AP maximization strategies, demonstrate the effectiveness of the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by more than 4% in terms of robust AP against PGD attacks while achieving 7% higher AP on clean data simultaneously on CIFAR10 and CIFAR100. The code is available at: https://github.com/GangLii/Adversarial-AP

## 1 Introduction

AP measures the precision of a model at different recall levels, offering a more nuanced understanding of the trade-offs between precision and recall. Optimizing AP for deep learning is of vital importance, especially in cases with highly imbalanced datasets. In such situations, accuracy alone can be misleading, as a model may perform well on the majority class but struggle with the minority class, thereby offering a superficially high accuracy score. In contrast, AP serves as a ranking metric that is particularly attuned to errors at the top of the ranking list, which makes it a more appropriate metric for applications dealing with highly imbalanced datasets. For example, deep AP maximization has been crucial in enhancing molecular property prediction performance, contributing to a winning solution in the MIT AICures challenge .

However, existing approaches of AP maximization are not robust against adversarial examples. It is notoriously known that deep neural networks (DNN) are vulnerable to adversarial attacks, where small, carefully-crafted perturbations to the input data can cause the model to produce incorrect predictions [46; 16]. These perturbations are often imperceptible to humans but can significantly impact the model's performance. Tremendous studies have been conducted to improve the adversarial robustness of DNN. A popular strategy to achieve adversarial robustness is through adversarial training [27; 59; 53; 45; 21; 2; 35; 44; 47; 15], which injects adversarial examples into the trainingthat are generated by various attack methods. Nevertheless, almost all existing methods focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. This type of adversarial robustness is insufficient for many applications with highly imbalanced datasets, as minor perturbations on a single example can significantly impact AP while not greatly influencing the accuracy of the prediction system (cf. an example in Figure 1). This presents a significant challenge for adversarially robust AP maximization.

In this paper, we conduct a comprehensive study on how to imbue AP maximization with adversarial robustness. There are several technical and practical concerns in the design of adversarial training methods for AP maximization to enjoy three nice properties: (i) capability to trade off between AP on a set of clean data and the robustness of the model on the perturbed data; (ii) robustness in terms of ranking performance instead of accuracy against adversarial perturbations; (ii) consistency of attacks between training and inference. The first property is obvious in light of existing works on adversarial training . The importance of the second property has been explained in the previous paragraph. The third property is tricky as it does not exist in existing adversarial training methods. The adversarial attack is usually applied to an individual data during the inference phase. Hence, we expect that maintaining the consistency between the attacks generated in training process and that in the inference phase will help boost the performance. However, this will cause a dilemma for achieving pointwise attack and listwise robustness in a unified framework.

To acquire these properties in a unified framework, we draw inspiration from prior adversarial training methods through robust regularization, and integrate two distinct design elements. We examine robust objectives that combine an AP surrogate loss on the clean data and a regularization term depending on the perturbed data. The two unique design features are (i) a new listwise adversarial regularization defined by a divergence between two distributions that represent the top one probabilities of a set of clean data and their perturbed versions; (ii) a non-zero-sum game approach, which integrates pointwise adversarial attacks with the proposed listwise adversarial regularization. This will ensure the attack consistency between training and inference. Our contributions are summarized below.

* We propose a non-zero-sum game optimization formulation for AP maximization with adversarial ranking robustness, which achieves listwise defense against pointwise attacks.
* We propose an efficient stochastic algorithm for solving the resulting objective, which integrates traditional adversarial sample generation methods with a state-of-the-art deep AP maximization algorithm without requiring a large batch size.
* We conduct extensive experiments to compare with existing leading adversarial training baselines, and ablation studies to compare different approaches shown in Table 1 for adversarial AP maxi

  Approaches & Objective & Regularization & Optimization & Ranking Robustness & Trade-off & Consistent Attack \\  Adap\_MM & MinMax AP Loss & No & Zero-sum Game & Yes & No & No \\  Adap\_ZZ & AP Loss + Reg. & Pointwise & Zero-sum Game & No & Yes & Yes \\  Adap\_LZ & AP Loss + Reg. & Listwise & Zero-sum Game & Yes & Yes & No \\  Adap\_LN & AP Loss + Reg. & Listwise & Non-zero-sum Game & Yes & Yes & Yes \\  Adap\_LPN & AP Loss + Reg. & Listwise + Pointwise & Non-zero-sum Game & Yes & Yes & Yes \\  

Table 1: Comparison of different approaches for Adversarial AP Maximization (AdAP). Red indicates new features proposed in this paper.

Figure 1: Top are predictions on clean images of a robust model trained by a state-of-the-art adversarial training method TRADES  and our method on CIFAR-10 data for detecting trucks. Bottom are predictions on the same set of images with only one example adversarially perturbed, which is generated by PGD following a black-box attack. The results of TRADES (left) indicate that slightly changing one example could dramatically impact AP but not on the accuracy. The results of our approach (right) demonstrate that our solution is more robust to adversarial data in terms of ranking and AP score. The dashed blue line indicates the decision boundary at score 0.5.

mization. We conclude that AdAP_LN and AdAP_LPN achieve the best performance among all methods corroborating the effectiveness of the proposed techniques.

## 2 Related Work

**Adversarial Robustness.** To safeguard deep neural networks (DNNs) from adversarial attacks, a variety of adversarial defense techniques have been proposed in the literature, including (1) detection for defense [37; 48; 38; 24]; (2) input transformations for defense [25; 17; 39; 18; 58]; (3) adversarial training [1; 47; 15; 4]. Among these techniques, adversarial training has been demonstrated to be one of the most effective approaches [27; 59; 41; 45; 21; 14; 53]. Among these,  is the first to theoretically study and justify adversarial training by solving a min-max formulation for training adversarially robust models for deep learning.  presents an objective function that strike a balance between accuracy and robustness in light of their theoretical tight upper bound on the robust error. However, these previous methods focus on how to improve the robustness concerning accuracy, which is not sufficient for highly imbalanced data.  considers adversarial training for imbalanced data by combining a minimax weighted loss and a contrastive loss of feature representations.  considers adversarial ranking in the context of retrieval and proposes maximum-shift-distance attack that pushes an embedding vector as far from its original position as possible and uses it in a triplet loss for optimization.  presents a study on adversarial AUC optimization by reformulating the original tightly coupled objective as an instance-wise form for adversarial training. Nevertheless, none of these methods enjoy three nice properties simultaneously, i.e., adversarial ranking robustness, trade-off between AP and robustness, and consistent attacks between training and inference.

**Average Precision Optimization.** For imbalanced classification and information retrieval problems, AP optimization has attracted significant attention in the literature [13; 36; 31; 5; 10; 29; 28; 55]. To maximize the AP score for big data, some works employ stochastic optimization with mini-batch averaging to compute an approximate gradient of the AP function or its smooth approximation [6; 36; 42; 4]. These methods typically rely on a large mini-batch size for good performance. In contrast,  proposes a novel stochastic algorithm that directly optimizes a surrogate function of AP and provides theoretical convergence guarantee, without the need for a large mini-batch size. Then  further improve the stochastic optimization of AP by developing novel stochastic momentum methods with a better iteration complexity of \(O(1/^{4})\). However, these approaches of AP maximization are vulnerable to adversarial examples created by introducing small perturbations to natural examples. The question of how to boost model's AP under adversarial perturbations while maintaining AP on clean data is still unresolved.

## 3 Preliminaries

For simplicity of exposition, we consider binary classification problems. However, the discussions and algorithms can be easily extended to mean AP for multi-class or multi-label classification problems. Let \(=\{(_{i},y_{i})\}_{i=1}^{n}\) denote the set of all training examples with \(_{i}\) being an input data and \(y_{i}\{-1,1\}\) being its associated label. Denoted by \(h()=h_{}()\) the predictive function (e.g., a deep neural network), whose parameters are \(^{d}\). Denote by \(()\) an indicator function of a predicate. Denoted by \(\|\|=\|\|_{p}\) the \(L_{p}\)-norm where \(p(1,]\). Denoted by \((0,)=\{x:\|x\|\}\) the \(L_{p}\)-norm ball centered at 0 with radius \(\). Let \(_{+}\) and \(_{-}\) be the subsets of \(\) with only positive examples and negative examples, respectively. Let \(n_{+}=|_{+}|\) denote the number of positive examples. Denote by \(r(_{i},)=_{_{j}}(h _{}(_{j}) h_{}(_{i}))\) the rank of \(_{i}\) in a set \(\) according to the prediction scores in descending order, i.e., the number of examples that are ranked higher than \(_{i}\) including itself. Let \((h_{}(),y)\) denote a pointwise loss function, e.g., the cross-entropy loss.

### AP Maximization

According to its definition, AP can be expressed as: \(}_{_{i}_{+}}_{i},_{+})}{r(_{i},)}\). Many different approaches have been developed for AP maximization [9; 36; 13; 4; 34]. We follow a recent work , which proposes an efficient stochastic algorithm for AP maximization based on differentiable surrogate loss minimization. In particular, the rank function is approximated by using a surrogate loss of the indicator function \((h_{}(_{j}) h_{}(_{i}))\), yielding the following problem:

\[_{}P()=-}_{_{i} _{+}}^{n}(y_{s}=1)(; _{s};_{i})}{_{s=1}^{n}(;_{s};_{i})},\] (1)where \(()\) denotes a smooth surrogate loss function, e.g., the squared hinge loss \((;_{s};_{i})=(\{m-(h_{}(_{i})-h_{}(_{s})),0\})^{2}\), where \(m\) is a margin parameter.

To tackle the computational complexity of computing the stochastic gradient of \(P()\),  has formulated this problem as a finite-sum coupled compositional optimization of the form \(}_{_{i}_{+}}f((; _{i}))\), where \((;_{i})=[_{1}(;_{i}), _{2}(;_{i})]\), \(_{1}(;_{i})=_{s=1}^{n}(y_{s}=1) (;_{s};_{i})/n\), \(_{2}(;_{i})=_{s=1}^{n}(;_{s};_{i})/n\), \(f([_{1},_{2}])=-_{1}/_{2}\). To compute a stochastic gradient estimator, their algorithm maintains two moving average estimators \(_{_{i}}^{1}\) and \(_{_{i}}^{2}\) for \(_{1}(;_{i})\) and \(_{2}(;_{i})\). At iteration \(t\), two mini-batches are sampled \(^{+}_{+}\) and \(\). The two estimators \(_{_{i}}^{1}\) and \(_{_{i}}^{2}\) are updated by Equation (2) for \(_{i}^{+}\):

\[_{_{i}}^{1}&=(1- _{1})_{_{i}}^{1}+_{1}|} _{_{j}}(_{t};_{j}, _{i})(y_{j}=1)\\ _{_{i}}^{2}&=(1-_{1}) _{_{i}}^{2}+_{1}|}_{ _{j}}(_{t};_{j},_{i }),\] (2)

where \(_{1}(0,1)\) is a moving average parameter. With these stochastic estimators, an stochastic estimate of \( P()\) is given by Equation (3)

\[_{_{t}}P(_{t})=^{+}| }_{_{i}^{+}}_{_{j}} _{_{i}}^{1}-_{_{i}}^{2}(y_{j}=1))(_{t};_{j},_{i})}{| |(_{_{i}}^{2})^{2}}.\] (3)

Then the model parameter can be updated similar to SGD, momentum-based methods, or Adam.

### Pointwise Attacks and Pointwise Adversarial Regularization

An adversarial attack is usually applied to a specific example \(\) such that its class label is changed from its original label (non-targeted attack) or to a specific label (targeted attack). We refer to this kind of attack as pointwise attack. Various pointwise attacking methods have been proposed, including, FGSM , Basic Iterative Method , PGD , JSMA , DeepFool , and CW attack . While our approach is agnostic to any pointwise attacks, we restrict our discussion to optimization-based non-target attacks, \(=_{\|\|}G(,+,y)\), where \(G\) is an appropriate function, e.g., \(G(,+,y)=-(h_{}(+),y)\). A classic adversarial training method is to solve a robust optimization problem that integrates the training loss with the adversarial attack, e.g.,

\[_{}_{i=1}^{n}_{\|_{i}\|} (h_{}(_{i}+_{i}),y_{i}).\] (4)

A deficiency of this approach is that it may not capture the trade-off between natural and robust errors . To address this issue, robust regularization methods have been proposed, whose objective consists of a regular surrogate loss of error rate on clean data and a robust regularization term that accounts for the adversarial robustness. Different robust regularizations have been proposed [59; 45; 21]. A state-of-the-art approach is TRADES , whose objective is formulated by:

\[_{}_{i=1}^{n}\{(h_{}(_{i}),y_{i})+_{\|_{i}\|}L(h_{}(_{i}),h_{ }(_{i}+_{i}))\},\] (5)

where \(>0\) is a regularization parameter and \(L(,)\) is an appropriate divergence function, e.g., cross-entropy loss between two predicted probabilities .

## 4 Adversarial AP Maximization

First, we discuss two straightforward approaches. The first method (referred to as AdAP_MM in Table 1) is to replace the loss in (4) as an AP surrogate loss yielding the following:

\[_{}_{\|\|}-}_{ _{i}_{+}}^{n}(y_{s}=1)(, _{s}+_{s},_{i}+_{i})}{_{s=1}^{n}( ,_{s}+_{s},_{i}+_{i})}.\] (6)

The second method (referred to as AdAP_PZ in Table 1) is to simply replace the first term in (5) with an AP surrogate loss:

\[_{}P()+_{i=1}^{n}_{\|_{i }\|}L(h_{}(_{i}),h_{}(_{i}+ _{i})).\] (7)The limitations of the first method are that it may not capture the trade-off between AP and robustness and the adversarial attacks during the training are inter-dependent, which is not consistent with pointwise attacks generated during the inference phase. While the second method is able to trade off between AP and robustness, the pointwise regularization is not suitable for tackling imbalanced datasets, as an perturbation on a single data point from a minority class does not change the pointwise regularization too much but could degrade the AP significantly.

### Listwise Adversarial Regularization

To address the deficiencies of the above straightforward approaches, we draw inspiration from TRADES and propose a listwise adversarial regularization to replace the pointwise regularization. The key property of the listwise adversarial regularization is that it should capture the divergence between the ranking result of the clean data and that of the perturbed data. To this end, we leverage the top-one probability proposed in the literature of learning to rank .

**Definition 1**: _The top one probability of an data \(_{i}\) represents the probability of it being ranked on the top, given the scores of all the examples i.e., \(p_{t}(_{i})=}(_{i}))}{_{j=1} (h_{}(_{j}))}\)._

With the definition of top-one probability, we define the listwise adversarial regularization as the divergence between two listwise probabilities, i.e., \(\{p_{t}(_{i})\}_{i=1}^{n}\) for the clean examples and \(\{p_{t}(_{i}+_{i})\}_{i=1}^{n}\) for the perturbed examples. Different metrics can be used to measure the divergence between two distributions, e.g., KL divergence between \(\{p_{t}(_{i})\}_{i=1}^{n}\) and \(\{p_{t}(_{i}+_{i})\}_{i=1}^{n}\), KL divergence between \(\{p_{t}(_{i}+_{i})\}_{i=1}^{n}\) and \(\{p_{t}(_{i})\}_{i=1}^{n}\), and their symmetric version Jensen-Shannon divergence. For illustration purpose, we consider the KL divergence:

\[R(,,)=_{i=1}^{n}p_{t}(_{i})  p_{t}(_{i})-p_{t}(_{i}) p_{t}(_{i}+ _{i}).\] (8)

To further understand the above listwise regularization, we conduct a theoretical analysis similar to  by decomposing the robust error into two components. The difference from  is that we need to use the misranking error. We consider the misranking error as \(_{nat}=_{_{i}_{+}}\{h (_{i})_{_{j}_{-}}h(_{j})\}\), which measures how likely a positive example is ranked below a negative example . To characterize the robust ranking error under the attack of bounded \(\) perturbation, we define \(_{rob}=_{_{i}_{+}}\{ _{i},_{j}(0,),h(_{i}+ _{i})_{_{j}_{-}}h(_{j}+_{j})\}\). It is worth noting that \(_{nat}_{rob}\) is always satisfied, and in particular, \(_{nat}=_{rob}\) if \(=0\). We show that the \(_{rob}\) can be decomposed into \(_{nat}\) and \(_{bdy}\) in Theorem 1.

**Theorem 1**: \(_{rob}=_{nat}+_{bdy}\)_, where the second term is called the boundary error \(_{bdy}=_{_{i}_{+}}\{h (_{i})>_{_{j}_{-}}h(_{j})\} \{_{i},_{j}(0,),h(_{i}+_{i})_{_{j}_{-}}h(_{j}+ _{j})\}\)._

**Remark:** It is clear that the boundary error measures the divergence between two ranked list \(\{h_{}(_{i})\}_{i=1}^{n}\) and \(\{h_{}(_{i}+_{i})\}_{i=1}^{n}\), which provides an explanation of the divergence between the top-one probabilities on the clean data and on the perturbed data.

Since we are optimizing AP, we use an AP surrogate loss as a surrogate of the misranking error \(_{nat}\) on the clean data. Finally, it yields in a robust objective \(P()+R(,,)\). A question remained is how to generate the perturbations. A simple strategy is to use the robust optimization approach that solves a zero-sum game: \(_{_{}^{n}}P()+R(,, )\) (referred to as AdAP_LZ in Table 1). However, since \(R(,,)\) is a listwise regularization, the resulting perturbations \(\{_{i}\}_{i=1}^{n}\) are inter-dependent, which is not consistent with the pointwise attacks generated in the inference phase. To address this issue, we decouple the defense and the attack by solving a non-zero-sum game:

\[_{} P()+ R(,,)\] (9)

\[_{\|_{i}\|}_{i=1}^{n}G(, _{i}+_{i},y_{i}),\]

where the attacks are pointwise attacks generated for individual examples separately, e.g., FGSM, PGD. We refer to the above method as AdAP_LN. Finally, we experiment with another method by adding a pointwise regularization into the objective for optimizing the model parameter \(\), i.e.,

\[_{} P()+(R(,,) +_{i=1}^{n}L(h_{}(_{i}),h_{}( _{i}+_{i})))\] (10)

This method referred to as AdAP_LPN imposes a stronger adversarial regularization on the model.

```
1:Initialize \(,_{}^{1},_{}^{2},,_{ 1},_{2}\)
2:for\(t=1,,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(_{+}\).
4: Draw a batch of \(B\) samples denoted by \(\).
5:for\(_{i}\)do
6: Initialize \(_{i}(0,1)\)
7:for\(m=1,,M\)do
8: Update \(_{i}=_{\|\|}(_{i}+_{2} sign(_{ _{i}}G(_{t},_{i}+_{i},y_{i})))\), where \(_{}()\) is the projection operator.
9:endfor
10:endfor
11: For each \(_{i}_{+}\), update \(_{_{i}}^{1}\) and \(_{_{i}}^{2}\) by Equation (2)
12: Update \(\) by Equation (11) and compute \(_{_{t}}R(_{t},,)\) by Equation (12)
13: Compute stochastic gradient estimator \(_{_{t}}=_{_{t}}P(_{t})+ _{_{t}}R(_{t},,)\)
14: Update \(_{t+1}\) by using SGD, momentum-methods or Adam.
15:endfor ```

**Algorithm 1** Stochastic Algorithm for Solving Adap_LN in (9)

### Algorithms for Adversarial AP Maximization

Below, we will discuss efficient algorithms for adversarial AP maximization employing different objectives. Due to limit of space, we focus attention on solving (9). Equal efforts have been spent on solving other Adap formulations with algorithms presented in the appendix.

Since there are two players one for minimizing over \(\) and one for maximizing over \(\), we adopt the alternating optimization framework that first optimizes \(\) for generating attacks of sampled data and then optimizes for \(\) based on the sampled clean data and perturbed data. The optimization for \(\) is following the existing methods in the literature. We use the PGD method for illustration purpose. The major technical challenge lies at computing a gradient estimator of the listwise regularization \(R(,,)\) in terms of \(\). We consider \(\) to be fixed and rewrite \(R(,,)\) as:

\[^{n}(h(_{i}))(h(_{i})-h(_{i }+_{i}))}{_{j=1}^{n}(h(_{j}))}-_{j=1}^{n}( h(_{j}))+_{j=1}^{n}(h(_{j}+_{j})).\]

Note that calculating the gradient of \(R(,,)\) in terms of \(\) directly, which includes the prediction scores of all samples, is not feasible. To tackle this challenge, we cast \(R(,,)\) into a compositoal function and borrow the technique of stochastic compositional optimization. To this end, we define:

\[g()=[g_{1}(),g_{2}(),g_{3}()]^{}\] \[=_{i=1}^{n}(h(_{i}))(h( _{i})-h(_{i}+_{i})),_{i=1}^{n}(h (_{i})),_{i=1}^{n}(h(_{i}+_{i}) )^{}.\]

Let \(F(g)=}{g_{2}}- g_{2}+ g_{3}\). Then we write \(R(,,)=F(g())=()}{g_{ 2}()}- g_{2}()+ g_{3}()\). The gradient of \(R(;,)\) is given by:

\[_{}R(,,)=_{}g( )^{} F(g())=_{}g()^{ }()},()-g_{2}( )}{(g_{2}())^{2}},()}^{ }.\]

The major cost for computing \(_{}R(,,)\) lies at evaluating \(g()\) and its gradient \(_{}g()\), which involves passing through all examples in \(\). In order to develop a stochastic algorithm, we will approximate these quantities by using a mini-batch of random samples. The gradient \(_{}g()\) can be simply approximated by the stochastic gradient, i.e.,

\[_{}g()=( _{_{i}}(h(_{i}))((h(_{i}) -h(_{i}+_{i})+1)_{}h(_{i})-_{ }h(_{i}+_{i}))\\ _{_{i}}(h(_{i}))_{ }h(_{i})\\ _{_{i}}(h(_{i}+_{i}) )_{}h(_{i}+_{i})),\]

where \(\) denote a set of \(B\) random samples from \(\). Due to the non-linear dependence of \(_{}R(,,)\) on \(g()\), we cannot simply use their mini-batch estimator in the calculation as it will yield a large optimization error . To reduce the optimization error [34; 33], we will maintain a vector \(=[u_{1},u_{2},u_{3}]\) for tracking \([g_{1}(),g_{2}(),g_{3}()]\). The vector \(\) is updated by Equation (11), where \(_{2}(0,1)\) is a parameter. An estimate of \(_{}R(,,)\) is given by Equation (12):

\[=(1-_{2})+_{2}( _{_{i}}(h(_{i}))(h(_{i})-h( _{i}+_{i}))\\ _{_{i}}(h(_{i}))\\ _{_{i}}(h(_{i}+_{i}) ))\] (11)

\[_{}R(,,)=_{}g()^{}},-u_{2}} {(u_{2})^{2}},}^{}.\] (12)

Finally, we are ready to present the detailed steps of the proposed algorithm for solving AdAP_LZ in (9) in Algorithm 1, which employs PGD for generating adversarial attacks and stochastic compositional optimization techniques for updating the model parameter \(\).

## 5 Experiments

In this section, we perform extensive experiments to evaluate the proposed approaches against white-box and black-box attacks on diverse imbalanced datasets. In order to provide a comprehensive understanding of our methods, we conduct experiments from three other perspectives: (1) trade-off between robustness and average precision; (2) a close look at the effect of the adversarial perturbations; (3) ablation study of different strategies for adversarial AP maximization.

**Datasets.** We conduct experiments on four distinct datasets sourced from various domains. These encompass CIFAR-10 and CIFAR-100 datasets , CelebA dataset  and the BDD100K dataset . For CIFAR-10 and CIFAR100, we adopt one versus-all approach to construct imbalanced classification tasks. It should be noted that even for clean data, the task of distinguishing between 1 positive class and 99 negative classes in CIFAR100 is challenging. Hence, in our experiments, we employ the 20 superclass labels, of which we choose the first 10 superclasses for constructing 10 one-vs-all binary classification tasks to verify the adversarial robustness on the CIFAR100 dataset. We split the training dataset into train/validation sets at 80%/20% ratio, and use the testing dataset for testing. CelebA dataset is a large-scale face attributes dataset that contains over 200,000 images of celebrities, each of which is annotated with 40 facial attributes. In our experiments, we choose two attributes with the highest imbalanced ratio, i.e., Gray_hair and Mustache, to show the superiority of our method. We adopt the recommended training/validation/testing split. BDD100K dataset is a large-scale diverse driving video dataset, which also has collected image-level annotation on six weather conditions and six scene types. In our experiments, we choose two kinds of weather conditions, i.e., Rainy and Cloudy, and two kinds of scene types, i.e., Tunnel and Residential, which are more imbalanced than others. Since the official testing dataset is not handy, we take the official validation set as the testing data, and split the training dataset into train/validation sets at 80%/20% ratio. The statistics for each task in datasets are presented in Table 6 in the appendix.

**Baselines.** In all experiments, we compare our methods (AdAP_LN, AdAP_LPN) with the following baseline methods: (1) PGD , which solves a MiniMax objective directly to enhance the adversarial robustness in terms of accuracy; (2) TRADES , which considers the trade-off between robustness and accuracy by minimizing a form of regularized surrogate loss; (3) MART , which explores the effect of misclassified examples to the robustness of adversarial training. Furthermore, we include two normal training methods, namely CE minimization and AP maximization by , as references.

**Experimental Details.** We employ the ResNet18  as the backbone network in our experiments. This choice is based on the fact that all the tasks involved in our study are binary classifications, and ResNet18 is considered to be expressive enough for the purpose. For all methods, with mini-batch size as 128, we tune learning rate in {1e-3,1e-4,1e-5} with standard Adam optimizer. We set the weight decay to 2e-4 for the CIFAR10 and CIFAR100 datasets and 1e-5 for the CelebA and BDD100k datasets. In the case of the CIFAR10 and CIFAR100 datasets, we run each method for a total of 60 epochs. For the CelebA and BDD100k datasets, we run each method for 32 epochs. The learning rate decay happens at 50% and 75% epochs by a factor of 10. For MART, AdAP_LN and AdAP_LPN, we tune the regularization parameter \(\) in {0.1, 0.4, 0.8, 1, 4, 8, 10}. For TRADES, we tune the regularization parameter in {1, 4, 8, 10, 40, 80, 100}, since they favor larger weights to obtain better robustness. In addition, for AdAP_LN and AdAP_LPN, we tune its moving average parameters \(_{1},_{2}\) in {0.1, 0.9}. Similarly, we tune the moving average parameters \(_{1}\) for AP maximization in {0.1, 0.9}. We set margin parameter in the surrogate loss of AP as 0.6 for all methods that use the AP surrogate loss. For all adversarial training methods, we apply 6 projected gradient ascent steps to generate adversarial samples in the training stage, and the step size is 0.01. We choose \(L_{}\) norm to bound the perturbation within the limit of \(=8/255\), as it is commonly used in the literature.

All the models in our experiments are trained from scratch, except direct AP maximization which is fine-tuned on a pretrained CE model as in . For each experiment, we repeat three times with different random seeds, then report average and standard deviation.

### Robustness against White-box Attacks

In this part, we evaluate the robustness of all models against PGD and APGD  white box attacks that have full access to model parameters. Specifically, we utilized 10-step PGD and APGD attack to generate adversarial perturbations constrained by the same perturbation limit \(=8/255\). Given that APGD is a step size-free method, we set the step size for PGD to 0.007. For the adversarial training methods, hyperparameters and models are chosen based on the robust average precision (AP) metric on validation datasets, and their corresponding performances on test datasets are reported. For normal training methods, models are chosen based on the clean average precision. The results are presented in Table 2 and 3. Since we run all the classes of CIFAR10 and the first 10 classes of CIFAR100 to verify the effectiveness of our method, we report the mean average precision over the ten classes in the tables, and the performance on each class is shown in the appendix.

From Table 2 and 3, we can observe that our proposed methods outperform other baselines consistently by a large margin while maintaining notably higher clean AP scores. It is striking to see that our methods improve robust AP by \(2 7\) percent on all datasets, compared with adversarial training methods which are concerning accuracy. The results also show that normal training methods (CE min. and AP max.) are vulnerable to adversarial attacks, while they can achieve pretty high clean AP. This is consistent with the observation from other literature [46; 16]. When comparing results in Table 3 with that in Table 2, we can see that APGD exhibits stronger attacks than PGD, as demonstrated in . However, the superiority of our proposed methods still remains evident.

### Robustness against Black-box Attacks

In this part, we examine all the models against adversarial black-box attacks. To achieve this, we utilize the model trained with CE loss minimization on clean datasets as an attacker model. So we do not include the performance of models of CE loss minimization here. With the well-trained model from CE loss minimization, we craft adversarial test images by PGD attack with 20 iterations and step size is 0.003, under perturbation limit \(=8/255\). The models evaluated here are the same model evaluated in 5.1. The results are summarized in Table 4. Results show that our method exhibits significantly superior robustness against black-box attacks on all tasks, with evident advantages against white-box attacks shown in 5.1. From Table 4, we can also observe that the normal training method, i.e., AP maximization, is also susceptible when confronting adversarial black-box attacks.

### Trade-off between Robustness and AP

As argued in various research studies [59; 40; 56], there exists a trade-off between accuracy and adversarial robustness. That is saying when one model boosts its adversarial performance, it may result in a decrease in its clean performance. In this part, we aim to study the trade-off between robustness and average precision and provide clear visualizations of the phenomenon. To accomplish this, we tune the weight of the regularization for the regularization-based approaches, i.e. TRADES, MART, AdAP_LN and AdAP_LPN, then show how their robust AP and clean AP changes. We evaluate the models at the last epoch to ensure that all methods reach a convergence point. For TRADES, AdAP_LN and AdAP_LPN, we tune the weight introduced in Experimental Details part, as well as 0.01, to better illustrate the trade-off trend. We also include non-regularization-based approaches as a point in this graph.

Based on Figure 2, we can observe that for TRADES, AdAP_LN and AdAP_LPN, as the weight of regularization increases, the clean AP decreases while the robust AP increases at first, which is consistent with the observation in . But for MART, the trend is not clear as it is not a trade-off based approach. However, as the weight of regularization continues increasing, both the clean AP and robust AP decrease. This is because when the model places excessive emphasis on the regularization term, it may overlook the actual objective. Notably, our proposed methods place more towards the upper-right than other baselines, which indicates that our method is able to achieve better both robust and clean AP simultaneously.

### Visualizing the Behavior after Adversarial Perturbations

To gain a deeper understanding of our approach, we have a close look at how defense models' predictions change after introducing adversarial perturbations. To be more specific, we compare our AdAP_LN method with another robust baseline, TRADES, to examine how their predictions are 

[MISSING_PAGE_FAIL:9]

run all these five approaches on Cifar10, CIFAR100 and BDD100K datasets. The experimental settings and the hyperparameters for AdAP_LN and AdAP_LPN are the same as those in 5.1. The hyperparameters we tune for AdAP_LZ are the same as those for AdAP_LN. For AdAP_PZ, we tune the weight parameter in {1, 4, 8, 10, 40, 80, 100}. For AdAP_MM, we tune \(_{1}\) in {0.1,0.9}. The results are presented in Table 5. We can see that (i) robust regularization approaches are better than the minimax approach (AdAP_MM) (ii) the non-zero-sum game approaches (AdAP_LPN, AdAP_LN) are usually better than the zero-sum game approach (AdAP_LZ); (iii) combining listwise and pointwise adversarial regularization in AdAP_LPN could bring significant boost in term of both robust AP and clean AP.

**Insensitivity to Batch Size.** We investigate the proposed approximation method for \(g()\) by varying the mini-batch size for AdAP_LN algorithm and report results in Figure 4 in the appendix. We can see that AdAP_LN does not require a very large batch size and is generally not sensitive to the mini-batch size, which implies the effectiveness of our approximation method.

**Sensitivity to \(_{1},_{2}\).** We study the sensitivity of the hyper-parameters \(_{1},_{2}\) for proposed AdAP_LN algorithm. From the results in Table 8 in the appendix, we can observe that \(_{1},_{2}\) have a significant impact on the performance. However, when we tune \(_{1},_{2}\) in {\(0.1,0.9\)}, it is able to achieve relatively good performance but not always the optimal one.

**Empirical convergence analysis.** We report the convergence curves of proposed AdAP_LN algorithm in Figure5 in the appendix, which demonstrates the convergence of the algorithm.

**Time efficiency.** We compare the training time efficiency of the proposed method with different algorithms in Table 7 in the appendix. We can observe that (i) adversarial training methods are generally more time-consuming than natural training; (ii) our proposed AdAP_LN and AdAP_LPN methods cost a little more time than traditional PGD method but much less time than TRADES.

## 6 Conclusions

In this paper, we have proposed a novel solution for maximizing average precision with adversarial ranking robustness. The proposed formulation is robust in terms of listwise ranking performance against individual adversarial perturbations and is able to trade off between average precision on clean data and adversarial ranking robustness. We have developed an efficient stochastic algorithm to solve the resulting objective. Extensive experimental results on various datasets demonstrate that the proposed method can achieve significantly improved adversarial robustness in terms of AP, compared with other strong adversarial training baselines. It would be interesting to extend our method to construct more robust systems with average precision as the objective, such as object detection systems and medical diagnosis systems.