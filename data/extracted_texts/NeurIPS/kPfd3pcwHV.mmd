# Online Ad Allocation with Predictions

Fabian Spaeh

Department of Computer Science

Boston University

fspaeh@bu.edu

&Alina Ene

Department of Computer Science

Boston University

aene@bu.edu

###### Abstract

Display Ads and the generalized assignment problem are two well-studied online packing problems with important applications in ad allocation and other areas. In both problems, ad impressions arrive online and have to be allocated immediately to budget-constrained advertisers. Worst-case algorithms that achieve the ideal competitive ratio are known, but might act overly conservative given the predictable and usually tame nature of real-world input. Given this discrepancy, we develop an algorithm for both problems that incorporate predictions and can thus improve the performance beyond the worst-case. Our algorithm is based on the work of Feldman et al. (2009a) and similar in nature to Mahdian et al. (2007) who were the first to develop an algorithm with predictions for the related, but more structured Ad Words problem. We use a novel analysis to show that our algorithm is able to capitalize on a good prediction, while being robust against poor predictions. We experimentally evaluate our algorithm on synthetic and real-world data on a wide range of predictions. Our algorithm is consistently outperforming the worst-case algorithm without predictions.

## 1 Introduction

Advertising on the internet is a multi-billion dollar industry with ever growing revenue, especially as online retail gains more and more popularity. Typically, a user arrives on a website which fills an empty advertising spot (called an impression) by allocating it instantly to one of many advertisers. Advertisers value users differently based on search queries or demographic data and reveal their valuations in auctions or through contracts with the website. Formulations of increasing complexity have been studied to capture this problem, creating a hierarchy of difficulty (Mehta, 2013). The most basic is online bipartite matching, where each vertex on one side arrives online with all its adjacent edges and has to be matched immediately to one of the vertices on the other side, which were supplied offline. The problem and all generalizations admit a hard lower bound of \(1-\) due to the uncertainty about future vertices. Motivated by online ad exchanges, where advertisers place bids on impressions, Mehta et al. (2007) introduced the Ad Words problem, which is a generalization of online bipartite matching where we charge each advertiser for the amount they bid. Going beyond the Ad Words setting, Feldman et al. (2009a) considered the more expressive problems Display Ads and the generalized assignment problem (GAP), and proposed algorithms for these settings with worst-case guarantees.

Classic algorithms that defend against the worst case of \(1-\) are often overly conservative given that the real world does not behave like a contrived worst-case instance. Recently, researchers have thus been trying to leverage a prediction about some problem parameter to go beyond the worst-case (Mitzenmacher and Vassilvitskii, 2022). In the context of ad allocation, a prediction can be the keyword distribution of users on a certain day, or simply the advertiser allocation itself. Such a prediction is readily obtainable in practice, for example through learning on historic data. Two opposing properties are important: The algorithm has to be consistent, meaning that its performanceshould improve with the prediction quality. Simultaneously, we want the algorithm to be robust against a poor prediction, i.e. not to decay completely but retain some form of worst-case guarantee. This is particularly important in the advertising business as much revenue is extracted from fat tails containing special events that are difficult or impossible to predict, but extremely valuable for the advertising business (e.g. advertising fan merchandise after a team's victory). To this end, Mahdian et al. (2007) developed an algorithm with predictions for the Ad Words problem and Medina and Vassilvitskii (2017) show how to use bid predictions to set reserve prices for ad auctions. Inspired by their work, we develop an algorithm with predictions for Display Ads and GAP.

**Our Contributions:** We design the first algorithms that incorporate predictions for the well-studied problems Display Ads and GAP. The two problems are general online packing problems, that capture a wide range of applications. Our algorithm follows a primal-dual approach, which yields a combinatorial algorithm that is very efficient and easy to implement. It is able to leverage predictions from any source, such as predictions constructed from historical data. Using a novel analysis, we show that the algorithm is robust against bad predictions and able to improve its performance with good predictions. In particular, we are able to bypass the strong lower bound on the worst-case competitive ratio for these problems. We experimentally verify the practical applicability of our algorithm under various kinds of predictions on synthetic and real-world data sets. Here, we observe that our algorithm is able to outperform the baseline worst-case algorithm due to Feldman et al. (2009a) that does not use predictions, by leveraging predictions that are obtained from historic data, as well as predictions that are corrupted versions of the optimum allocation.

### Preliminaries

**Problem Definition:** In this work, we study the Display Ads problem and its generalization, the generalized assignment problem (GAP) (Feldman et al., 2009a). In Display Ads, there are advertisers \(a\{1,,k\}\) that are known ahead of time, each of which is willing to pay for at most \(B_{a}\) ad impressions. A sequence of ad impressions arrive online, one at a time, possibly in adversarial order. When impression \(t\) arrives, the values \(w_{at} 0\) for each advertiser \(a\) become known. These values might be a prediction of click-through probability or any valuation from the advertiser, but we treat them as abstractly given to the algorithm. We have to allocate \(t\) immediately to an advertiser, or decide not to allocate it at all. The goal is to maximize the total value \(_{a,t}x_{at}w_{at}\) subject to \(_{t}x_{at} B_{a}\) for all \(a\), where \(x_{at}=1\) if \(t\) is allocated to \(a\) and \(x_{at}=0\), otherwise. GAP is a generalization of Display Ads where the size that each impression takes up in the budget constraint of an advertisers is non-uniform. That is, each impression \(t\) has a size \(u_{at} 0\) for each advertisers \(a\), and advertiser \(a\) is only willing to pay for a set of impressions whose total size is at most \(B_{a}\). More precisely, we require that \(_{a,t}x_{at}u_{at} B_{a}\).

**Free Disposal Model:** In general, it is not possible to achieve any competitive ratio for the online problems described above. Motivated by online advertising, Feldman et al. (2009a) introduced the free disposal model which makes the problem tractable: when a new impression arrives, the algorithm allocates it to an advertiser \(a\); if \(a\) is out of budget, we can decide to dispose of an impression previously allocated to \(a\). The motivation for this model is that advertisers are happy to receive more ads, as long as they are only charged for the \(B_{a}\) most valuable impressions. We refer the reader to the paper of Feldman et al. (2009a) for additional motivation of this model. In this work, we consider both Display Ads and GAP in the free disposal model.

**Related Problems:** Display Ads and GAP are significant generalizations of well-studied problems such as online bipartite matching and Ad Words. In online bipartite matching, all values are \(1\). In Ad Words, values and sizes are identical. The latter setting allows for more specialized algorithms that exploit the special properties of this problem, as we discuss in more detail later.

**Algorithms with Predictions:** The algorithms we study follow under the umbrella of algorithms that leverage predictions to obtain improved performance. These were studied in an extensive line of work, see e.g. the survey of Mitzenmacher and Vassilvitskii (2022). Following this established research, we use two important measures for the performance of the algorithm: The _robustness_\(/\) indicates how well the algorithm's objective value \(\) performs against the optimum solution \(\); the _consistency_\(/\) measures how close the algorithm gets to the prediction's objective value \(\). Most algorithms with predictions, including the one presented in this work, allow to control the trade-off between robustness and consistency with a parameter \(\).

### Related Work

**Online Ad-Allocation with Predictions:** To the best to our knowledge, we are the first to study Display Ads and GAP with predictions. Related problems were considered in the work by Mahdian et al. (2007) and Medina and Vassilvitskii (2017) for Ad Words, and by Lattanzi et al. (2020) for online capacitated bipartite matching. Medina and Vassilvitskii (2017) use bid predictions to set reserve prices for ad auctions. Lavastida et al. (2021) incorporate predictions of the dual variables into the proportional-weights algorithm (Agrawal et al., 2018; Karp et al., 1990; Lattanzi et al., 2020) for online capacitated bipartite matching. Chen and Indyk (2021) analyze an algorithm that uses degree predictions by matching arriving vertices to vertices with minimum predicted degree. Jin and Ma (2022) provide an algorithm for batched allocation in bipartite matching under predictions, based on the model of Feng et al. (2021). As noted above, Ad Words and bipartite matching have additional structure, which is exploited in these prior works. In particular, the algorithms proposed in these works are not applicable to the more general problems Display Ads and GAP. Our algorithm builds on the approach of Mahdian et al. (2007) for the Ad Words problem, but substantial new ideas are needed in the algorithm design and analysis, as discussed in more detail in Section 2.

There has been further extensive work in the design of worst-case algorithms and under random input models without predictions, which we now summarize.

**Worst-Case Algorithms:** The design of worst-case algorithms has been the focus of a long line of work which can, for instance, be found in the survey of Mehta (2013). A large focus has been on Ad Words. Several combinatorial algorithms have been proposed, based on the work of Karp et al. (1990). The combinatorial approach is tailored to the structure of to these special cases. The primal-dual approach is a more general approach that can handle more complex problems such as Display Ads and GAP (Buchbinder et al., 2007; Feldman et al., 2009a). In this work, we build on the primal-dual algorithm of Feldman et al. (2009a) and show how to incorporate predictions into their framework. The worst-case guarantee for online bipartite-matching, and therefore for all generalizations, is \(1-\)(Karp et al., 1990).

**Stochastic Algorithms:** The lower bound of \(1-\) can be circumvented under distributional assumptions. This has been extensively studied for online bipartite matching (Karande et al., 2011; Feldman et al., 2009b; Jin and Williamson, 2022). Further work has been done for the Ad Words problem (Devanur and Hayes, 2009; Devanur et al., 2012) with generalizations due to Feldman et al. (2010) for a more general stochastic packing problem.

## 2 Our Algorithm

In order to illustrate the algorithmic ideas and analysis, we consider the simpler setting of Display Ads in this section. Our algorithm for GAP is a generalization of this algorithm and we include it in Appendix B. For simplicity, we assume that our prediction is a solution to the problem, which is as in prior work (Mahdian et al., 2007). However, due to our general analysis framework, we also consider our algorithm a starting point towards incorporating weaker predictors, such as partial solutions or predictions of the supply, which we leave for future work.

**Prediction:** We assume that we are given access to a prediction, which is a fixed solution to the problem, given as an allocation of impressions to advertisers. With each impression \(t\), we also receive the advertiser \((t)\) to which the prediction allocates \(t\). In particular, this means that the prediction does not have to be created up front, but can be adjusted on the fly based on the observed impressions.

Given a solution to the problem, we could also consider the following random-mixture algorithm: For some parameter \(q\), run the worst-case algorithm; with probability \(1-q\), follow the prediction exactly. This algorithm achieves a robustness of \(q(1-)\) and consistency of \(q(1-)+1-q\). However, this is only in expectation and against a weak adversary that is oblivious to the algorithm's random choices. In contrast, Algorithm 1 is designed to obtain its guarantees deterministically against the strongest possible adversary that can adapt to the algorithm's choices, which is identical to the setup in Mahdian et al. (2007). We observe that Algorithm 1 clearly outperforms this random-mixture algorithm in our experiments (cf. Section 4) which shows that our stronger setting is indeed valuable in practice. Furthermore, the random-mixture algorithm cannot be adapted to different predictors that are not solutions, such as the ones mentioned above.

**Algorithm Overview:** Our Algorithm, shown in Algorithm 1, incorporates predictions in theprimal-dual algorithm of Feldman et al. (2009a). The algorithm is based on the primal and dual LP formulations in Figure 1. The algorithm constructs both a primal integral solution which is an allocation of impressions to advertisers as well as a dual solution, given explicitly by the dual variables \(_{a}\). Analogously to other algorithms with predictions, the algorithm takes as parameter the value \(\); a larger value of \(\) means that we trust the prediction more. Similarly to the worst-case algorithm of Feldman et al. (2009a), the dual variables \(_{a}\) play an important role in the allocation of impressions. When an impression \(t\) arrives, we evaluate the discounted gain \(w_{at}-_{a}\) for each advertiser. The worst-case algorithm allocates the impression to the advertiser \(a_{()}\) maximizing the discounted gain and only allocates if the discounted gain is positive, i.e. the value exceeds the threshold \(_{a}\). Our algorithm modifies this base algorithm to incorporate predictions as shown in Line 7 and it follows the prediction \(a_{()}\) if its discounted gain is a sufficiently high fraction of the discounted gain of \(a_{()}\). We refer the reader to the discussion below for more intuition on the choice of update. After selecting the advertiser to which to allocate the impression \(t\), we remove the least valuable impression currently assigned to \(a\) to make space for \(t\), and then allocate \(t\) to \(a\). Another crucial part of the algorithm is the update rule for \(_{a}\) in Line 14, which is updated in a novel way based on the parameter \(\). More precisely, we set \(_{a}\) as an exponential averaging of the values of impressions currently allocated to \(a\). Compared to the worst-case algorithm, we assign higher weight to impressions with less value which is essential for leveraging predictions.

To simplify the algorithm description and analysis, we initially allocate \(B_{a}\) impressions of zero value to each advertiser \(a\). Furthermore, we assume that there exists a "dummy" advertiser with large budget that only receives zero value impressions. Instead of not allocating an impression explicitly (either in the algorithm or the prediction), we allocate to the dummy advertiser, instead.

Figure 1: Primal and dual of the Display Ads LP

**Intuition for our Algorithm:** As noted above, we make two crucial modifications to the worst-case algorithm to incorporate predictions: The advertiser selection (Line 7) and the update of \(_{a}\) (Line 14). We now provide intuition for both choices.

First, let us illustrate the difficulties in incorporating predictions in the advertiser selection. Based on the worst-case algorithm, a natural way to incorporate predictions is to allocate to the prediction if the distorted gain \(w_{a_{()}t}-_{a_{()}}\) exceeds the maximum discounted gain. However, this approach does not work as shown by the following example: Consider a scenario where the prediction suggests a constant advertiser \(a_{()}\). Impressions are split into two phases: Phase 1 contains \(B_{a_{()}}\) impressions \(t\) where only \(a_{()}\) can derive a value of \(w_{a_{()}t}=1\) and \(w_{at}=0\) for \(a a_{()}\). The algorithm allocates all these impressions to \(a_{()}\) and at the end of phase 1 has \(_{a_{()}}=1\). In phase 2, a large amount of impressions with \(w_{a_{()}t}=1\) and \(w_{at}=\) for \(a a_{()}\) arrive. Since the distorted gain \(w_{a_{()}t}-_{a_{()}}=\) exceeds the discounted gain \(w_{at}-_{a}=\) for \(a a_{()}\), the algorithm allocates to \(a_{()}\) which yields \(0\) gain. However, we forfeit an unbounded amount of potential value derived from allocating to advertisers \(a a_{()}\). An important takeaway of this example is the crucial observation that the algorithm should never allocate to the predicted advertiser if its discounted gain is \(0\). The selection rule in our algorithm is designed to meet this important consideration.

Second, we need to change the update rule for \(_{a}\). We update \(_{a}\) using a carefully selected exponential average of the values of impressions currently assigned to \(a\), that incorporates our confidence in the prediction parameterized by \(\). In contrast to the worst-case algorithm, we weigh less valuable impressions more. This lowers the threshold for the addition of new impressions, which allows us to exploit more potential gain from the predicted advertiser.

**Theorem 1**.: _Let \(B_{a}B_{a}\) and \(e_{B}(1+)^{B}\). Let \(\) and \(\) be the values of the optimal and predicted solutions, respectively. For any \( 1\), Algorithm 1 obtains a value of at least_

\[\{R(),C()\}\]

_where the robustness is_

\[R()^{}-1}{Be_{B}^{}(e_{B}^{/ B}-1)}-1}{ e^{}}(B )\]

_and the consistency is_

\[C() (1+^{}-1}\{ }(e_{B}^{}-^{}-1}{_{B}}),(e_{ B}^{})\})^{-1}\] \[(1+-1}\{(e^{}--1}{}),\} )^{-1}(B).\]

Figure 2 shows the above trade-off between consistency and robustness. We can observe that the guarantee rapidly improves as the minimum advertiser budget \(B\) increases, which is very beneficial both in theory and in practice. The trade-off is comparable to the one obtained by Mahdian et al. (2007) for the more structured Ad Words problem.

Figure 2: We illustrate the consistency-robustness trade-off of Algorithm 1 for various values of \(\) and budgets \(B\).

**Comparison to Prior Work:** The work most closely related to ours is the work of Mahdian et al. (2007) which incorporates predictions into the algorithm of Mehta et al. (2007). These works are for the related but different Ad Words problem, and do not apply to Display Ads and GAP. In the Ad Words problem, the value of an impression is equal to the price that the advertiser pays for it, i.e., the size of the impression in the budget constraint. In contrast, in Display Ads and GAP, the values and the sizes are independent of each other (e.g., in Display Ads an impression takes up \(1\) unit of space in the advertiser's budget but it can accrue an arbitrary value). Thus there is no longer any relationship between the total value/profit of the impressions and the amount of the advertiser's budget that has been exhausted. The Ad Words algorithms of Mehta et al. (2007); Mahdian et al. (2007) crucially rely on this relationship both in the algorithm and in the analysis. Due to the special structure of the problem, these algorithms do not dispose of impressions and consider only the fraction of the advertiser's budget that has been filled up in order to decide the allocation and to incorporate the prediction. Moreover, the algorithm and the analysis do not need to account for the loss incurred by disposing impressions. These crucial differences require a new algorithmic approach and analysis, which was given by Feldman et al. (2009a) using a primal-dual approach. Since we build on their framework as opposed to the work of Mehta et al. (2007), we also need a new approach for incorporating predictions, as described above in the intuition for our algorithm. Further, the primal-dual framework only helps in establishing the robustness but not the consistency of our algorithm, and we develop a novel combinatorial analysis for proving the consistency.

## 3 Analysis

In the following, we outline the analysis of Algorithm 1 to prove Theorem 1. Specifically, we show separately that \(/ R()\) (robustness) and \(/ C()\) (consistency).

**Notation:** We denote with superscript \((t)\) the value of variables after allocating impression \(t\). E.g. \(a^{(t)}\) is the algorithm's choice of advertiser for impression \(t\) and \(_{a}^{(t)}\) is the dual variable after allocating \(t\). Let

\[_{a}t:a^{(t)}=a}_{a}t:a^{(t)}_{()}=a}\]

be the impressions that were assigned to \(a\) and potentially disposed of, and the impressions that the prediction recommended for assignment to \(a\), respectively. We set \(I_{a}|_{a}|\) and \(_{a}|_{a}_{a}|\) as the size of the overlap. Let also \(T\) be the last impression and \(_{a}\) is the final allocation, i.e., the \(B_{a}\) impressions allocated to \(a\) at the end of the algorithm. Finally, let \(\) and \(\) be the total value of the solution created by the algorithm and the prediction, respectively.

**Robustness:** Our proof for robustness closely follows the analysis in Feldman et al. (2009a) by using the primal-dual formulation of the problem, with some additional care that is needed not to violate dual feasibility whenever we follow the prediction. We defer the full proof to Appendix A.1.

**Consistency:** We now show the consistency, i.e. that \(\) is bounded by a multiple of \(\). The complete analysis can be found in Appendix A.2, while we only give a high-level overview here.

A common approach in the analysis of online primal-dual algorithms is to employ a local analysis where, in each iteration, we relate the change in the value of the primal solution to the change in the dual solution (Buchbinder and Naor, 2009). However, it is not clear how to employ such a strategy in our setting due to the complications arising from our algorithm following a mixture of the worst-case and predicted solutions. We overcome this challenge using a novel global analysis that relates the final primal value to the prediction's value.

We now provide a high level overview of our global analysis. We start by noting that the objective value of our algorithm and the prediction is the sum of the impression values allocated to each advertiser, i.e.

\[=_{a}_{t_{a}}w_{at} =_{a}_{t_{a}}w_{at}\]

However, note that \(\) contains values of impression that do not appear in \(\) since we ignore the prediction in some iterations, or already disposed of the impression. It is further unclear which advertiser to "charge" for an impression that does not agree with the prediction.

Consider an impression for which we followed the worst-case choice \(a_{()}\) that maximizes the discounted gain instead of the prediction. Due to our selection rule, the reason for this departure isdue to the discounted gains satisfying the following key inequality:

\[w_{a_{}}}(w_{a_{}}-_{a_{ }})+_{a_{}}.\] (1)

By using this important relationship, we upper bound the value \(\) of the prediction using a linear combination of the values of impressions allocated by the algorithm (but possibly disposed of) and the thresholds. By grouping the impression values and dual variables by advertiser in the resulting upper bound, we are able to correctly charge each impression for which we deviated from the prediction to a suitable advertiser, thus overcoming one of the challenges mentioned above. To summarize, using (1) we obtain a bound \(_{a}_{a}\) where each \(_{a}\) is a linear combination of impression values and dual variables for advertiser \(a\), and we want to compare this quantity to \(_{a}_{t S_{a}}w_{at}\).

Let us now consider a fixed advertiser \(a\), and relate \(_{a}\) to \(_{a}\). At this point, a key difficulty is that the amount \(_{a}\) that we charged to advertiser \(a\) involves the threshold \(_{a}\). By definition, the threshold is a convex combination of the values of the impressions in the algorithm's allocation. This gives us that \(_{a}\) is a linear combination of only the weights, but this cannot be readily compared to \(_{a}\) due to the complicated structure of the coefficients in the former. To bridge this gap, we show a useful structural property (Lemma 4) that gives us the following upper bound on \(_{a}\): If we define \(t_{i}\) as the \(i\)-th impression allocated to \(a\), we have

\[_{a}_{i=I_{a}-B_{a}+1}^{I_{a}-_{a}}_{i}w_{at_ {i}}+_{i=I_{a}-_{a}+1}^{I_{a}}_{i}w_{at_{i}}+w_{at_{I_{a}-B_{a}}} _{a}\] (2)

for appropriate coefficients \(_{i}\), \(_{i}\), and \(_{a}\). The RHS of this inequality accounts for the value as follows: the first sum is for the impressions that agree with the prediction; the second sum is for the impressions that disagree with the prediction; the final term accounts for the values of all impressions that were disposed. As this is (almost) a linear combination over impression values that all appear in \(\) (except \(w_{at_{I_{a}-B_{a}}}\)), we could bound the ratio \(_{a}/_{a}\) by the maximum coefficient in (2). However, this does not lead to a constant competitive ratio. We therefore need a delicate analysis (Lemma 10) to balance the coefficients as uniformly as possible among all values, where we use properties of the coefficients \(_{i}\), \(_{i}\), and \(_{a}\) and the structural property we derived in Lemma 4.

In Appendix B, we discuss how to generalize Algorithm 1 and its analysis to GAP, where impressions can take up arbitrary size \(u_{at}\) in the budget constraints. This generalization uses a gain \(w_{at}-u_{at}_{a}\) that considers the size of each impression, and the comparison of the worst-case and predicted choices are updated correspondingly. We also change the update of \(_{a}\) to consider the impression density \(w_{at}/u_{at}\). We also note that our technique can be used to achieve a slight improvement in the guarantees, but we omit this in the interest of simplicity and conciseness.

## 4 Experimental Evaluation

We now evaluate the practical applicability of Algorithm 1. We use multiple baseline algorithms on different kinds of predictions, which we describe below. We showcase results on real-world and synthetic data, with further experimental results in Appendix C.

**Algorithms:** We compare Algorithm 1 to the random-mixture algorithm described in Section 2 (Random Mixture) and the worst-case algorithm without predictions due to Feldman et al. (2009a) (Worst-Case Baseline). Additionally, we consider two modifications of the worst-case algorithm: One allocates to the impression of maximum value (Greedy Baseline) and the other to the impression with maximum gain after disposal (Discounted Greedy Baseline).

**Predictors:** We consider variations of the following predictors. Recall that each predictor is a fixed allocation of impressions to advertisers that is revealed online.

(1) _Optimum Solution (\(OPT\)):_ The optimum solution is obtained by solving the problem optimally offline using an LP solver. To evaluate our algorithm's robustness, we also consider a version of the optimum solution where a random \(p\)-fraction of the allocations has been corrupted. Under a _random corruption_, we corrupt by reallocating to randomly chosen advertisers. For a _biased corruption_, we sample a random permutation offline and corrupt by reallocating according to this permutation, generating a more adversarial corruption.

(2) _Dual Base:_ We generate a solution using the algorithm of Devanur and Hayes (2009). Here, we sample the initial \(\)-fraction of all impressions and optimally solve a scaled version of the dual LP to obtain the dual variables \(\{_{a}\}_{a}\). We get a primal allocation for all future impressions \(t\) by allocating to the advertiser \(a\) that maximizes the discounted gain \(w_{at}-_{a}\), but do not update \(_{a}\).

(3) _Previous Day:_ We look at all impressions from the previous day and optimally solve the dual LP offline to obtain dual variables \(\{_{a}\}_{a}\). To get a prediction for today's impressions, we use the same algorithm as above and allocate to the advertiser maximizing the discounted gain.

**Real-World Instances:** We generate two instances for Display Ads based on the real-word datasets iPinYou (Zhang et al., 2014) and Yahoo (Yahoo, 2011).

Figure 4: Performance of our algorithm on the iPinYou (top) and Yahoo (bottom) datasets for \(=5\) with predictions of varying quality obtained as follows: We vary the sample fraction \(\) for the dual base algorithm and \(p\) for random and biased corruptions. The prediction competitiveness is defined as \(\).

Figure 3: Experimental results on iPinYou (top) and Yahoo datasets (bottom) using different predictions for varying \(\). The solid lines show our algorithm and the dashed lines the random-mixture algorithm. We run the algorithms 5 times and report average for both algorithms and the standard deviation only for our algorithm, to avoid clutter. For the robustness, the black line shows the performance of the worst-case algorithm without predictions due to Feldman et al. (2009a). For each predictor, we also include in parentheses the average competitive ratio over 5 runs (e.g. PreviousDay (0.66) indicates that the average competitive ratio for the solution of the Previous Day prediction was \(0.66\)). We run the random-mixture algorithm for each prediction and \(q 1/\).

(1) _iPinYou:_ The iPinYou dataset contains real-time bidding data from the iPinYou online advertisement exchange. This dataset contains 40372 impressions over 7 days and 301 advertisers. Each advertiser places multiple bids for an impression. We use this bid data to construct the dataset. Specifically, we set the maximum of those bids as the advertiser's valuation. We assume a constant budget for each advertiser of 10 impressions as it makes for an interesting instance.

(2) _Yahoo:_ We replicate the experimental setup of Lavastida et al. (2021) who generated an instance of online capacitated bipartite matching based on a Yahoo dataset (Yahoo, 2011). Capacitated online bipartite matching is a special case of Display Ads where all impression values are \(1\). Based on this dataset, we create an instance of capacitated online bipartite matching with around 2 million impressions and 16268 advertisers for 123 days. We defer the details to Section C.1.

**Synthetic Instances:** We obtain random synthetic data for a fixed set of \(T\) impressions and \(k\) advertisers as follows. We first generate a set of impression types, whereas each type is meant to model a group of homogeneous users (e.g. similar demographic or using similar keywords) and advertisers value users from the same group identically. We sample an advertiser's valuation for each impression type from an exponential distribution. To represent a full day of impressions, we assume that display times of impressions from a certain type are distributed according to a Gaussian with some uniformly random mean in \(\) and a fixed standard deviation \(\). We then sample the same number of impressions from each type along with display times, and order them in increasing display time. Finally, we equip each advertiser with some fixed budget that makes for a difficult instance.

**Results:** Figure 3 and Figure 5 show results for real-world and synthetic instances, respectively. For each predictor, we show the consistency (left) and robustness (right) for varying \(\). Figure 4 shows results for \(=5\) with predictions of different quality, as described in the figure caption.

**Discussion:** We make several observations. On real-world instances, there is only a single prediction for which the performance of our algorithm drops below the worst-case algorithm, even for heavily corrupted predictions. E.g., on the iPinYou dataset, our algorithm is still able to leverage a prediction with a corruption rate as high as \(p=50\%\), and improve upon the worst-case algorithm (see the green and black lines in the top right plot of Figure 3). Moreover, for higher performing predictors, the improvement over the worst-case algorithm is significantly higher in both datasets. See for example, the Previous Day predictions on the iPinYou dataset (the purple and black lines in the top right plot of Figure 3) or Dual Base predictions on the Yahoo dataset (the orange and black lines in the bottom right plot of Figure 3). The increase in robustness is surprising as it does not follow the behavior of our theoretical bounds (cf. Figure 2) which hold against an adversarial prediction. This suggests that on real-world data, our algorithm is able to exploit "good suggestions" from the prediction while ignoring "bad suggestions". Second, as we can see in Figure 3, the consistency of our algorithm for predictors except the optimum is always above 1, and is significantly high for artificially corrupted predictions. The robustness of our algorithm remains high in almost all cases, even for the most heavily corrupted predictions (cf. the right side of Figure 4). On synthetic instances, we observe that our algorithm is robust against both random and biased corruption, as the robustness does not drop to the prediction's low competitiveness of \( 0.7\). Furthermore, our algorithm performs well in combination with the dual base prediction for \(=0.1\) (the orange line in Figure 5), even though the first \(200\) impressions are not representative of all impressions. On all instances, we clearly outperform the random-mixture algorithm which merely interpolates between the objective values of the worst-case algorithm and the prediction.

Figure 5: Experimental results for varying values of \(\) on synthetic data with \(12\) advertisers and \(2000\) impressions of \(10\) types, where we report the same quantities as in Figure 3. We use different predictors with \(=1.5\). We conducted experiments with significantly more or less impressions, where we obtain similar results, and we therefore omit these.

## Conclusion

We introduce a novel algorithm with predictions for Display Ads and GAP with free disposal. Our algorithm is based on the primal-dual approach and can be efficiently implemented in practice. We show its robustness using the primal-dual method similar to Feldman et al. (2009a) and use a novel combinatorial proof to show its consistency. Finally, our experiments show the applicability of our algorithm, which is able to improve beyond the worst-case performance using readily available predictions. An interesting direction left open by our work is to understand the optimal trade-off between robustness and consistency for Display Ads and GAP. This challenging direction has been explored recently in the works (Jin and Ma, 2022; Wei and Zhang, 2020) for a related problem.

**Limitations:** Our algorithm requires a strong prediction that is a solution to the problem. We leave weaker predictions, such as partial solutions or predictions of the supply, for future work.