# SA3DIP: Segment Any 3D Instance with Potential 3D Priors

Xi Yang1, Xu Gu1, Xingyilang Yin1, Xinbo Gao2

1Xidian University, 2Chongqing University of Posts and Telecommunications

yangx@xidian.edu.cn, {ryangu,yxyl}@stu.xidian.edu.cn, gaoxb@cqupt.edu.cn

Corresponding author.

###### Abstract

The proliferation of 2D foundation models has sparked research into adapting them for open-world 3D instance segmentation. Recent methods introduce a paradigm that leverages superpoints as geometric primitives and incorporates 2D multi-view masks from Segment Anything model (SAM) as merging guidance, achieving outstanding zero-shot instance segmentation results. However, the limited use of 3D priors restricts the segmentation performance. Previous methods calculate the 3D superpoints solely based on estimated normal from spatial coordinates, resulting in under-segmentation for instances with similar geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D space suffers from over-segmentation due to SAM's inherent part-level segmentation tendency. To address these issues, we propose SA3DIP, a novel method for Segmenting Any 3D Instances via exploiting potential 3D Priors. Specifically, on one hand, we generate complementary 3D primitives based on both geometric and textural priors, which reduces the initial errors that accumulate in subsequent procedures. On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process. Furthermore, we notice a considerable portion of low-quality ground truth annotations in ScanNetV2 benchmark, which affect the fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth labels and supplement additional instances for 3D class-agnostic instance segmentation. Experimental evaluations on various 2D-3D datasets demonstrate the effectiveness and robustness of our approach. Our code and proposed ScanNetV2-INS dataset are available HERE.

## 1 Introduction

3D instance segmentation is a fundamental task pivotal to 3D understanding across various domains such as autonomous driving, robotics navigation, and virtual reality applications. State-of-the-art methods  are predominantly supervised and rely heavily on precise 3D annotations for training, thus constraining their applications in open-world scenes. Compared to scarce 3D labeled data, the acquisition and annotation of 2D images are more convenient. Recently, 2D foundation models  trained on large-scale annotated 2D data show impressive performance and strong generalization capabilities in zero-shot scenarios. Recent efforts have sought to leverage Segment Anything Model (SAM) by lifting its class-agnostic 2D segmentation results to 3D tasks . Specifically, some methods  propose a pipeline that decomposes the 3D scene into geometric primitives and leverages 2D multi-view masks from SAM to calculate pairwise similarity scores as merging guidance. Further well-designed algorithms or Graph Neural Networks (GNNs) are included to ensure multi-view consistency.

However, the geometric rudimentary pre-segmentation initialization impedes their ability to group superpoints on points with highly similar normals, such as boards on walls and books on tabletops. As shown in Fig. 1 bottom left, the blackboard and the wall are wrongly allocated within the same superpoint using previous methods. Owing to the coarse-to-fine pattern of the pipeline, errors at this stage propagate to subsequent stages, which the sophisticated merging algorithms fail to rectify. Furthermore, current approaches heavily rely on 2D foundation models and design algorithms or GNNs within 2D space, neglecting the inherent 3D priors of the data. Part-level segmentation in the generated 2D masks by SAM transfers to 3D space and leads to over-segmented 3D instances. As illustrated in Fig. 1 bottom right, the sofa and chairs are segmented at the part level in 2D space, causing over-segmentation in the final results. These limitations primarily stem from the under-exploitation of 3D priors: (1) Complete point cloud data encompasses not only spatial coordinates but also color channels; (2) Constraints provided by 3D space prior to the merging process cannot be neglected.

In this paper, we present **SA3DIP** (**Segment Any 3D Instance with potential 3D Priors), a novel method for segmenting high-quality 3D instances. Specifically, we observe that distinct instances with similar normals often exhibit different colors. Therefore, we incorporate both geometric and textural priors to generate finer-grained complementary primitives. As shown in Fig. 1 top right, our method identifies the boundary between the blackboard and the wall clearly. In this way, the initial errors are minimized, which reduces error accumulation in the subsequent process. Moreover, we exploit the 3D prior at the merging stage to provide constraints on the over-segmented 3D instances, which is implemented by incorporating a 3D detector. This additional 3D prior enables rectification on the over-segmented 3D instances, while preserving the capability in handling fine-grained objects. Therefore, the sofa and chairs maintain their integrity in 3D space by our approach, which is illustrated in Fig. 1 top right. Additionally, we notice that the widely-used benchmark, ScanNet , contains a

Figure 1: Comparison of our **SA3DIP** with other methods. Methods like SAI3D (bottom) fail to distinguish instances with similar normals when computing superpoints, which accumulate to the final segmentation. Moreover, the part-level 2D segmentation transfers to 3D space, resulting in over-segmented 3D instances. We present a novel pipeline for segmenting any 3D instances, which overcomes the limitations by exploiting additional 3D priors, specifically by incorporating both geometric and textural prior on superpoints computing, and supplementing 3D space constraint provided 3D prior by utilizing a 3D detector.

considerable portion of low-quality ground truth annotations for instance segmentation, which leads to biases in assessing model performance. Thus, we propose ScanNetV2-INS, a point-level enhanced version tailored for 3D class-agnostic instance segmentation. The revised dataset contains fewer incomplete labels and fewer missing instances, which better showcases real-world scenarios.

Our contributions are three-fold: (1) We present **SA3DIP**, a novel pipeline for segmenting any 3D instances by exploiting potential 3D priors, which includes incorporating both geometric and color priors on computing 3D superpoints, and introducing constraints provided by 3D prior at the merging stage; (2) We propose a point-level enhanced version of ScanNetV2, specifically for 3D class-agnostic instance segmentation by rectifying incomplete annotations and incorporating more instances; (3) Extensive experiments are conducted on ScanNetV2, ScanNetV2-INS and ScanNet++  datasets, and the competitive results demonstrate the effectiveness and robustness of our method.

## 2 Related Work

Close-set 3D segmentation.3D semantic segmentation aims to classify each point into a specific semantic class . 3D instance segmentation, on the other hand, assigns unique masks to each distinct instance within the same semantic category . Prior research, categorized as Grouping-based , Kernel-based , and Transformer-based  methods, has primarily relied on labeled datasets in a supervised manner. Mask3D  proposes the first Transformer-based model for 3D semantic instance segmentation that uses instance queries and Transformer decoders. Spherical Mask  achieves state-of-the-art 3D instance segmentation performance on the ScanNetV2 dataset by leveraging a novel coarse-to-fine approach  based on spherical representation. However, they all necessitate a significant corpus of annotated 3D data for network training, which is financially burdensome and poses challenges for extending the method to open-world scenarios featuring novel objects from unobserved categories.

Open-set 3D segmentation.2D foundation models  have exhibited remarkable efficacy across various tasks. Training on the SA-1B dataset with 11 million images and 1.1 billion masks, Segment Anything model (SAM) serves as a cornerstone for image segmentation, allowing strong zero-shot transfer ability and diverse prompts such as points, boxes, and texts to generate high-quality segmentation masks. Inspired by the generalization capabilities of foundation models, certain works  explore the feasibility of bridging the gap between 2D and 3D, enabling various open-vocabulary 3D tasks. OpenScene  and OpenMask3D  both rely on transferring knowledge from CLIP, where the former infers CLIP features for each 3D point and classifies them embeddings of class labels, and the latter uses additional pre-trained 3D segmentation model to produce class-agnostic 3D proposals and classifies them based on CLIP scores. SAM3D  pioneers the extension of SAM into 3D perception by transferring segmentation information from 2D images to 3D space. SAMPro3D  attempts to locate 3D points as natural prompts to align their projected pixel prompts across frames, while its segmentation quality heavily relies on the accuracy of 2D segmentation results. Several other works  follow the idea that segments each frame individually and devises a merging algorithm or graph neural network (GNN) to guide the merging process of pre-segmented superpoints. However, the limited use of 3D priors restricts the performance in both superpoints computing and region growth, which results in substandard 3D segmentation. In this paper, we present **SA3DIP**, which incorporates more priors and constraints to minimize error accumulation and over-segmentation, by a thorough exploitation of 3D priors.

## 3 Methodology

### Sa3dip

Overview.Our overall pipeline is shown in Fig. 2. Given a point cloud \(P^{N 6}\) and corresponding 2D data \(\{I_{m},D_{m},K_{m},E_{m}\}_{m=1}^{M}\), which denote the RGB, depth images, camera intrinsic and extrinsic parameters, respectively, the masks of 3D instances in the scene are desired as output. First, we generate complementary 3D primitives of the given point cloud via performing 3D over-segmentation on both geometric and textural priors. Then, we construct the superpoint graph by treating the 3D primitives and their relations as nodes and edges of the graph, respectively. Leveraging the 2D masks generated by 2D foundation segmentators like SAM, we create the affinity matrix which contains the node features and edge weights. Finally, we perform affinity- and distance-aware region growing to merge the 3D primitives. Further merging is introduced by considering the supplemental prior from 3D space, which is implemented using a 3D detector.

Complementary primitives generation.Following the graph-cut algorithm in , we calculate complementary primitives by employing over-segmentation on both geometric and textural priors. Previous methods [7; 8] only consider the geometry information during the primitive generation process. As shown in the middle example in Fig. 2-A, under-segmentation occurs in regions where the door, wall, and board exhibit similar normals. Errors at this initial stage propagate and accumulate, adversely affecting the final segmentation. In contrast, we propose to incorporate additional textural prior, as illustrated in the right example in Fig. 2-A, which leads to finer-grained primitives. Specifically, for a 3D scene \(P\), we first treat each point \(p_{a} P\) as a node \(v_{a}\) and calculate the edge weights \(w(v_{a},v_{b})\) for each pair of nodes \(v_{a}\) and \(v_{b}\). We begin by estimating the normal \(_{a}\) for all \(p_{a}\) using corresponding 3D coordinates \(f_{a}\). Next, we extract the additional color information \(c_{a}\), which previous methods fail to exploit. Note that the combination of \(f_{a}^{3}\) and \(c_{a}^{3}\) represents the complete point \(p_{a}^{6}\). We compute the cosine similarity between normals \(_{a}\) and \(_{b}\), and normalized Euclidean distance between colors \(c_{a}\) and \(c_{b}\). The final edge weights \(w(v_{a},v_{b})\) are obtained by a weighted sum of these two dissimilarities:

\[w(v_{a},v_{b})=w_{n}_{a}_{b}}{\| _{a}\|\|_{b}\|}+w_{c}^{3}(c_{ak}-c_{bk})^{2}}. \]

Subsequently, we cluster points to the finer-grained primitives \(\{U_{i}\}_{i=1}^{N_{U}}\) based on each pair of \(w(v_{a},v_{b})\).

Scene graph construction.As shown in Fig. 2-B, we follow the paradigm to construct a superpoint graph for the given scene. The generated primitives serve as nodes and the affinity scores obtained through a matching algorithm serve as edge weights. Specifically, we first obtain the 2D projection \(U_{i}^{m}^{H W 2}\) of the \(i\)-th 3D primitive \(U_{i}^{N_{i} 3}\) on the \(m\)-th image by utilizing the common pinhole camera matrix:

\[(U_{i}^{m},1)^{T}=K_{m} E_{m}(U_{i},1)^{T}. \]

Figure 2: **Overall pipeline.** Our approach first integrates both geometric and textural priors for grouping 3D primitives (step A). Corresponding posed masks are generated using SAM. An affinity matrix is then computed based on these 2D-3D results serving as edge weights (step B). Region growing and instance-aware refinement are conducted on the constructed scene graph, utilizing 3D box constraint to address over-segmentation while maintaining the fine-grained outcomes (step C).

We feed the \(m\)-th RGB-image \(I_{m}\) into the 2D foundation segmentator, e.g. SAM, to obtain its masks \(S_{m}\). The primitive-mask matching algorithm is then performed on 2D projections \(U_{i}^{m}\) and the masks \(S_{m}\) for computing affinity scores. To be specific, we calculate a normalized histogram vector \(_{i,m}\) to collect the 2D masks in \(S_{m}\) covered by rendered \(U_{i}^{m}\), since multiple labels may be covered due to the ambiguity or inaccuracy in 2D masks. The affinity score between \(i\)- and \(j\)-th superpoints on the \(m\)-th frame is obtained by computing the cosine similarity of their histogram vectors:

\[A_{i,j}^{m}=_{i,m}_{j,m}}{\|_{i,m}\| \|_{j,m}\|}. \]

Traversing all \(M\) images yields all affinity scores between \(U_{i}\) and \(U_{j}\). However, primitives may not be visible in all frames, which leads to invalid affinity scores. To address this, we apply a visibility-based filter on the obtained scores. The visibility \(_{i}^{m}(0,1)\) is defined as the ratio of the visible point number of \(U_{i}\) on \(m\)-frame to the total point number of that in the scene. Thus, the final affinity score \(A_{ij}\) is calculated in a weighted-sum manner as:

\[A_{i,j}=^{M}(_{i,j}^{m}A_{i,j}^{m})}{_{m= 1}^{M}_{i,j}^{m}}, \]

where the weight \(_{i,j}^{m}\) is calculated as the product of \(_{i}^{m}\) and \(_{j}^{m}\). Iteratively processing through all superpoints and 2D frames yields the adjacency matrix \(A^{N_{U} N_{U}}\). Thus, the superpoint graph of the scene is constructed with primitives \(\{U_{i}\}_{i=1}^{N_{U}}\) as nodes and adjacency matrix \(A\) as edge weights.

```
1:Input: bounding boxes \(\{bb_{i}\}_{i=1}^{N_{bb}}\) in ascending order of volume, primary 3D segmentation masks \(l^{N}\), threshold \(_{2}\)
2:Output: Updated 3D segmentation \(l^{}\)
3:\(O_{i}\), \(l^{} l\)
4:\(L\) {max instance ID in \(l\)} + 1
5:for each \(bb_{i}\) in \(\{b_{i}\}_{i=1}^{N_{bb}}\)do
6:\(O_{i} l bb_{i}\)
7:for each instance ID \(O_{i}^{}\) in \(O_{i}\)do
8:\(_{i}O_{i}^{}}{\{l^{ }=O_{i}^{}\}}\)
9:if\(_{i}>_{2}\)then
10:\(\{l^{}=O_{i}^{}\} L\)
11:endif
12:endfor
13:\(L L+1\)
14:endfor
```

**Algorithm 1** Instance-aware refinement

Region growing and instance-aware refinement.We perform affinity- and distance-aware region growing on the constructed graph. Previous methods inherit the part-level segmentation tendency from 2D masks output by SAM, often leading to over-segmentation in 3D space. For instance, the chair of the primary segmentation shown in Fig. 2-C is segmented as two distinct parts. To address this issue, we propose to exploit supplemental prior from 3D space by the integration of a 3D detector  for further merging. As shown in Fig. 2-Output, the constraint provided by additional 3D prior rectifies the over-segmented instances while preserving the capability to handle detailed objects.

At the primary merging stage, we incorporate not only the affinity scores \(A_{i,j}\) but also the Euclidean distances \(_{i,j}\) between nodes \(U_{i}\) and \(U_{j}\), thus to introduce a certain level of global awareness. Dynamic thresholds \(_{1}^{N_{t}}\) are applied to reduce the initial erroneous merges and subsequent error accumulation. Specifically, we multiply \(A_{i,j}\) with decay factor \(_{}\) to get the merging confident score \(_{i,j}\) between \(U_{i}\) and \(U_{j}\):

\[_{i,j}=_{} A_{i,j}=_{i,j}}  A_{i,j}, \]

where \(_{}\) is the reciprocal of the Euclidean distance. Then we compare the confident score \(_{i,j}\) with the threshold \(_{1}\) to judge whether to merge the nodes \(U_{i}\) and \(U_{j}\). Therefore, the primary segmentation results are obtained through iterating all pairs of nodes \(N_{t}\) times.

We further introduce supplemental prior from 3D space by employing a detection-based instance-aware refinement. As shown in Algorithm 1, we gather all points \(O_{i}\) within the bounding box \(bb_{i}\), and assess the proportion \(_{i}\) of points belong to instance ID \(O_{i}^{}\) in \(O_{i}\) to that of the entire scene \(P\). If the ratio \(_{i}\) exceeds a specified threshold \(_{2}\), it indicates high confidence that the points with instance ID \(O_{i}^{}\) represent a portion of an over-segmented instance. We assign a new label to all points that exceed the threshold, thereby rectifying the over-segmented instance. However, there is a possibility that smaller objects which are in close proximity to or situated on larger objects are false-corrected. Tomitigate this issue, we opt to pre-sort the bounding boxes based on size. In this way, the corrections are performed in descending order of bounding box size to ensure that smaller objects retain their independence in the final output.

### ScanNetV2-INS

ScanNetV2 has served as a standard benchmark for evaluating model performance. However, it includes a notable proportion of low-quality ground truth labels, potentially leading to misleading results. To address this issue, we introduce a refined version of the dataset, termed ScanNetV2-INS, wherein annotations are enhanced at the point level.

Imperfection in vanilla ScanNetV2.The original ScanNetV2 exhibits imperfections in its ground truth annotations, primarily manifesting in two aspects. Firstly, certain obvious instances remain unmarked. For example, as illustrated in Fig. 3-a top row, the board on the wall and papers on the desk are neglected. Secondly, some instances are incompletely annotated. For instance, doors and boards in Fig. 3-a bottom row that are clearly visible in clean point clouds feature large areas of black (indicating "unlabeled") in the annotations. The prevalence of these significantly impacts the accuracy of evaluation metrics and leads to erroneous estimations of model performance. Thus, corrective measures are imperative.

Revision of ScanNetV2.With the aid of a recently released annotation tool AGILE3D proposed in , we perform point-level updates on the ground truth annotations for all 312 scenes in the validation set efficiently. The revision primarily addresses two aforementioned deficiencies, as shown in Fig. 3-a right column. Firstly, we re-label the instances where the ground truth was obscured by unlabeled black points, such as the door and boards. Secondly, we assign class-agnostic labels to certain instances that were clearly discernible to the human perception but were not originally annotated in the ground truth, such as the papers on the desk and the poster on the wall.

Figure 3: Overview of our proposed ScanNetV2-INS. We present the new benchmark for 3D class-agnostic instance segmentation, which rectifies incomplete annotations and incorporates more instances based on ScanNetV2. Row (a) shows the comparison before and after revision, and row (b) illustrates the object counts per scene between the two benchmarks.

a result, the instance count of our new dataset, as shown in Tab. 2, significantly increases. Therefore, it better reflects and poses more challenges on the model performance. However, our dataset consists of only the revised version of 312 scenes in the validation set, focusing on the evaluation use of 3D class-agnostic instance segmentation in the context of no-training methods.

## 4 Experiments

In this section, we quantitatively evaluate our SA3DIP on ScanNet series (including the vanilla ScanNetV2 , our ScanNetV2-INS, and more challenging ScanNet++ ), Matterport3D  and Replica  datasets to demonstrate its effectiveness and robustness in 3D instance segmentation. Qualitative visualizations for ScanNet series datasets are also provided for a more intuitive comparison with other methods.

### Experiment settings

Datasets.ScanNet  integrates a comprehensive array of 2D and 3D data sourced from indoor environments, facilitated by an iPad application in tandem with depth sensors. This dataset includes RGB and depth images, along with 3D point cloud data, all meticulously annotated with semantic and instance labels. It encompasses an extensive collection of over 2.5 million views derived from more than 1500 scans. In contrast, ScanNet++  represents a recently introduced indoor dataset exhibiting a similar composition to ScanNet but boasting higher-resolution 3D geometry and more detailed data annotations. ScanNet++ data is captured using advanced equipment, including the Faro Focus Premium laser scanner, iPhone 13 Pro, and a DSLR camera equipped with a fisheye lens. Our proposed ScanNet-INS encompasses a revised version of all 312 validation scenes in ScanNetV2, while maintaining consistency with ScanNetV2 in terms of data and label format. It provides a more accurate metric and fairer comparison between methods.

Parameter settings.We conduct all experiments on a single RTX4090. The weights for geometry and texture used in the complementary primitives generation are set as \(w_{n}=0.96\) and \(w_{c}=0.04\). This is because texture prior such as RGB values are not robust enough when being used solely due to lighting conditions, reflections, shadows, and noise collected by sensors. We conduct detailed ablation study on the choice of the two weights in the later section. The threshold \(_{1}\) in the region growing is empirically set as \([0.9,0.8,0.7,0.6,0.5]\) for ScanNetV2 and ScanNetV2-INS, \([0.9,0.8,0.7]\) for ScanNet++, and the threshold \(_{2}\) in instance-aware refinement is set as \(0.75\) experimentally.

Evaluation metrics.We evaluate the quantitative results with the widely used Average Precision score. Following , we report AP with thresholds of 25% and 50% (denoted as \(_{25}\), \(_{50}\)) and averaged over all overlaps between [50% and 95%] at 5% steps (\(\)). Since the 2D foundation segmentation model produces class-agnostic masks, we ignore semantic class labels in the evaluation and consider only the accuracy of the instance masks themselves.

    &  \\   & 500 & 500-1000 & 1000-2000 & 2000-5000 & 5000-10000 & 10000 \\  ScanNetV2 & 252 & 452 & 1119 & 1690 & 567 & 284 \\ ScanNetV2-INS & 692 & 748 & 1366 & 1873 & 626 & 291 \\   

Table 1: Instance number within varying point range of ScanNetV2 and ScanNetV2-INS dataset.

    &  \\   & Min & Max & Avg & Total \\  ScanNetV2 & 2 & 47 & 14 & 4364 \\ ScanNetV2-INS & 2 & 54 & 17 & 5596 \\   

Table 2: Instance count of ScanNetV2 and ScanNetV2-INS dataset.

Baselines.We compare our approach with both closed-vocabulary and open-vocabulary methods. Mask3D  trained on ScanNetV2 serves as the closed-vocabulary baseline. Recent methods based on leveraging 2D foundation models, including SAM3D  (with and without ensemble process), SAM-graph , SAI3D , and SAMPro3D  are compared as open-vocabulary methods. In addition, we compare with the traditional point grouping method proposed by Felzenszwalb .

### Results on ScanNet series

Tab. 3 shows the quantitative results of our approach in comparison with other methods on ScanNetV2, ScanNetV2-INS, and ScanNet++ datasets. Our method achieves the best performance among all three datasets, showing the effectiveness of our approach. Specifically, our SA3DIP outperform 7.9% **mAP**, 8.4% **AP\({}_{50}\)**, and 6.0% **AP\({}_{25}\)** on ScanNetV2, 3.6% **mAP**, 3.8% **AP\({}_{50}\)**, and 2.9% **AP\({}_{25}\)** on ScanNetV2-INS. On the challenging ScanNet++, our method still obtains 2.5% **mAP**, 2.7% **AP\({}_{50}\)**, and 2.0% **AP\({}_{25}\)** gain. Note that all methods except SAM3D experience a drop in precision on ScanNetV2-INS dataset compared with the vanilla ScanNetV2. This indicates that our proposed ScanNetV2-INS poses more challenges in identifying fine-grained objects and yields fairer metrics. SAM3D, due to its limited segmentation capability, tends to produce a significantly higher number of instances than the actual objects in the scene. Consequently, its metrics do not show noticeable changes on finer-grained ScanNetV2-INS.

We also present qualitative results in Fig. 4. The visual comparison further proves the effectiveness of our method. As shown in the first two rows of Fig. 4, our method maintains a better instance awareness and is capable of identifying the tables as a whole. Moreover, by utilizing more accurate 3D primitives, our approach is the only one able to segment the door out from the wall, as shown in the third row of Fig. 4. This showcases the significance of exploiting the potential 3D priors.

### Ablation studies

We conduct detailed ablation studies on prior with varying weights \(w_{n}\) and \(w_{c}\). We report the metrics in Tab. 4. We assigned several weights for geometry and texture to test their contribution. Specifically, we conduct one with configuration of \(w_{n}=0.4\) and \(w_{c}=0.6\) which yields the similar number of 3D primitives as the primitives used in SAM3D, SAI3D and others, for a fair comparison.

It can be observed that texture prior are not robust enough when being used solely due to the influence of shadows, reflection and so on. Therefore, we choose to assign less weight to the textural prior, thus to exploit it while minimizing its negative impact. The experiments show that the setting with \(w_{n}=0.96\) and \(w_{c}=0.04\) suits best for our approach.

However, it is noticed that incorporating only complementary primitives yields a slight drop in average precision on both datasets. It is related to the definition of the metric AP (ratio of correctly

    &  &  &  \\   & **mAP** & **AP\({}_{50}\)** & **AP\({}_{25}\)** & **mAP** & **AP\({}_{50}\)** & **AP\({}_{25}\)** & **mAP** & **AP\({}_{50}\)** & **AP\({}_{25}\)** \\   \\ Mask3D  & 31.1 & 44.9 & 58.0 & 29.1 & 43.9 & 56.3 & 9.9 & 17.3 & 25.8 \\   \\ Felzenszwalb  & 5.0 & 12.7 & 38.9 & 2.8 & 6.5 & 24.0 & 4.1 & 9.2 & 25.3 \\ SAM3D  & 12.4 & 28.7 & 57.4 & 12.5 & 28.9 & 57.8 & 1.1 & 4.5 & 15.4 \\ (w/o ensemble) & 20.1 & 33.3 & 52.1 & 20.0 & 33.2 & 52.2 & 7.2 & 14.2 & 29.4 \\ (w/ ensemble) & 24.1 & 40.3 & 65.9 & 23.1 & 39.5 & 64.1 & 12.9 & 25.3 & 43.6 \\ SAI3D  & 30.8 & 50.5 & 70.6 & 28.9 & 49.2 & 69.7 & 17.1 & 31.1 & 49.5 \\ SAMPro3D  & 33.7 & 56.2 & 75.3 & 32.5 & 54.8 & 73.4 & 18.9 & 33.7 & 51.6 \\  SA3DIP (ours) & **41.6** & **64.6** & **81.3** & **36.1** & **58.6** & **76.3** & **21.4** & **36.4** & **53.6** \\   

Table 3: Class-agnostic 3D instance segmentation comparison on ScanNetV2, ScanNetV2-INS, and ScanNet++ datasets.

identified instances to the total number of identified instance). This AP metric is more in favor of under-segmentation rather than over-segmentation, since the former yields high precision and fewer false positive cases, while the latter gives fewer precision and higher recall.

### More experiments

We conduct further experiments and corresponding ablation studies on both Matterport3D  and Replica  dataset to test the robustness and generalization ability of our method. Matterport3D dataset contains 194,400 RGB-D images of 90 building-scale indoor scenes and exhibits more view changes on its 2D frames compared to ScanNet. Replica dataset incorporates 18 highly photo-realistic 3D indoor scene reconstructions with dense geometry, high resolution and dynamic range textures. Our method clearly gives better quantitative results, as shown in Tab. 5.

   \)} & \)} &  &  &  \\   & & & **mAP** & \(_{50}\) & \(_{25}\) & **mAP** & \(_{50}\) & \(_{25}\) \\ 
1 & 0 & × & 30.8 & 50.5 & 70.6 & 28.9 & 49.2 & 69.7 \\
0 & 1 & × & 10.4 & 18.1 & 32.5 & 9.5 & 17.0 & 31.1 \\
0.4 & 0.6 & × & 27.3 & 47.4 & 69.8 & 25.6 & 46.3 & 69.4 \\
0.96 & 0.04 & × & 29.3 & 49.2 & 70.5 & 27.4 & 48.3 & 70.4 \\ 
1 & 0 & ✓ & 40.8 & 63.6 & 80.7 & 35.9 & 57.8 & 75.4 \\
0 & 1 & ✓ & 12.7 & 22.1 & 37.2 & 11.0 & 19.7 & 34.1 \\
0.4 & 0.6 & ✓ & 39.1 & 62.7 & 80.2 & 33.5 & 56.3 & 75.0 \\
0.96 & 0.04 & ✓ & **41.6** & **64.6** & **81.3** & **36.1** & **58.6** & **76.3** \\   

Table 4: Ablation studies on prior we exploit with varying weights.

Figure 4: Visual comparison between our method with SAM3D , SAMPro3D , and SAI3D  on ScanNetV2, ScanNetV2-INS, and ScanNet++ dataset. Among all datasets, our method shows the most robust and accurate segmentation.

### Limitations

Due to the trade-off between efficiency and accuracy, we choose to compute 3D superpoints based on only 3D priors. This yields an extremely short time of execution within a few seconds, while it may lead to an overwhelming number of superpoints which introduces challenges in the merging process. Moreover, for high-resolution point clouds with vivid light and shade effects, the superpoints generated based on geometric and texture is not enough yet. One approach is to design a more sophisticated pre-segmentation model with semantic awareness. Besides, though constraints provided by 3D prior are introduced, the affinity matrix based on 2D masking still relies heavily on the accuracy of 2D foundation segmentators. Designing a more robust merging algorithm or better leveraging various 2D foundation models shows promise in the future.

## 5 Conclusion

In this paper, we introduce a novel method for segmenting any 3D instances by exploiting the potential 3D priors. The key idea is to incorporate more 3D priors into the 2D foundation model guided pipeline and leverage not only knowledge transferred from 2D space but also features in 3D space. We first generate complementary 3D superpoint primitives based on both geometric and textural priors to reduce the initial errors that accumulate in subsequent procedures. Then we introduce supplemental constraints from the 3D space by using a 3D detector. Along with the constructed affinity matrix by using 2D masks, the region growing and refinement process is performed on the 3D primitives. Furthermore, we propose ScanNetV2-INS with complete ground truth labels and supplement additional instances for 3D class-agnostic instance segmentation, which produces unbiased metrics on comparing different methods. Experimental evaluations on ScanNetV2, ScanNetV2-INS, and ScanNet++ datasets demonstrate the effectiveness of our approach. We believe that we pioneer at exploiting the importance of 3D priors in the 2D foundation model guided pipeline, and it should draw attention toward future research that methods trying to extend 2D foundation models into 3D space should not overlook the role of inherent 3D priors.