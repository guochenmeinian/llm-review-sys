# Toxicity Detection for Free

Zhanhao Hu  Julien Piet  Geng Zhao  Jiantao Jiao  David Wagner

University of California, Berkeley

{huzhanhao,julien.piet,gengzhao,jiantao,daw}@berkeley.edu

###### Abstract

Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.

## 1 Introduction

Significant progress has been made in recent large language models. LLMs acquire substantial knowledge from wide text corpora, demonstrating a remarkable ability to provide high-quality responses to various prompts. They are widely used in downstream tasks such as chatbots  and general tool use . However, LLMs raise serious safety concerns. For instance, malicious users could ask LLMs to write phishing emails or provide instructions on how to commit a crime .

Current LLMs have incorporated safety alignment  in their training phase to alleviate safety concerns. Consequently, they are generally tuned to decline to answer toxic prompts. However, alignment is not perfect, and many models can be either overcautious (which is frustrating for benign users) or too-easily deceived (e.g., by jailbreak attacks) . One approach is to supplement alignment tuning with a toxicity detector , a classifier that is designed to detect toxic, harmful, or inappropriate prompts to the LLM. By querying the detector for every prompt, LLM vendors can immediately stop generating responses whenever they detect toxic content. These detectors are usually based on an additional LLM that is finetuned on toxic and benign data.

Current detectors are imperfect and make mistakes. In real-world applications, toxic examples are rare and most prompts are benign, so test data exhibits high class imbalance: even small False Positive Rates (FPR) can cause many false alarms in this scenario . Unfortunately, state-of-the-art content moderation classifiers and toxicity detectors are not able to achieve high True Positive Rates (TPR) and very low FPRs, and they struggle with some inputs.

Existing detectors also impose extra costs. At training time, one must collect a comprehensive dataset of toxic and benign examples for fine-tuning such a model. At test time, LLM providers must also query a separate toxicity detection model, which increases the cost of LLM serving and can incur additional latency. Some detectors require seeing both the entire input to the LLM and the entire output, which is incompatible with providing streaming responses; in practice, providers deal with this by applying the detector only to the input (which in current schemes leads to missing some toxic responses) or applying the detector once the entire output has been generated and attempting toerase the output if it is toxic (but by then, the output may already have been displayed to the user or returned to the API client, so it is arguably too late).

In this work, we propose a new approach to toxicity detection, Moderation Using LLM Introspection (MULI), that addresses these shortcomings. We simultaneously achieve better detection performance than existing detectors and eliminate extra costs. Our scheme, MULI, is based on examining the output of the model being queried (Figure 1). This avoids the need to apply a separate detection model; and achieves good performance without needing the output, so we can proactively block prompts that are toxic or would lead to a toxic response.

Our primary insight is that there is information hidden in the LLMs' outputs that can be extracted to distinguish between toxic and benign prompts. Ideally, with perfect alignment, LLMs would refuse to respond to any toxic prompt (e.g., "Sorry, I can't answer that..."). In practice, current LLMs sometimes respond substantively to toxic prompts instead of refusing, but even when they do respond, there is evidence in their outputs that the prompt was toxic: it is as though some part of the LLM wants to refuse to answer, but the motivation to be helpful overcomes that. If we calculate the probability that the LLM responds with a refusal conditioned on the input prompt, this refusal probability is higher when the prompt is toxic than when the prompt is benign, even if it isn't high enough to exceed the probability of a non-refusal response (see Figure 2). As a result, we empirically found there is a significant gap in the probability of refusals (PoR) between toxic and benign prompts.

Calculating PoR would offer good accuracy at toxicity detection, but it is too computationally expensive to be used for real-time detection. Therefore, we propose an approximation that can be computed efficiently: we estimate the PoR based on the logits for the first token of the response. Certain tokens that usually lead to refusals, such as _Sorry_ and _Cannot_, receive a much higher logit for toxic prompts than for benign prompts. With this insight, we propose a toxicity detector based on the logits of the first token of the response. We find that our detector performs better than state-of-the-art (SOTA) detectors, and has almost zero cost.

At a technical level, we use sparse logistic regression (SLR) with lasso regularization on the logits for the first token of the response. Our detector significantly outperforms SOTA toxicity detection models by multiple metrics: accuracy, Area Under Precision-Recall Curve (AUPRC), as well as TPR at low FPR. For instance, our detector achieves a \(42.54\%\) TPR at \(0.1\%\) FPR on ToxicChat , compared to a \(5.25\%\) TPR at the same FPR by LlamaGuard. Our contributions include:

* We develop MULI, a low-cost toxicity detector that surpasses SOTA detectors under multiple metrics.
* We highlight the importance of evaluating the TPR at low FPR, show current detectors fall short under this metric, and provide a practical solution.

Figure 1: Pipeline of MULI. Left: Existing methods use a separate LLM as a toxicity detector, thus having up to a 2x overhead. Right: We leverage the original LLM’s first token logits to detect toxicity using sparse logistic regression, incurring negligible overhead.

* We reveal that there is abundant information hidden in the LLMs' outputs, encouraging researchers to look deeper into the outputs of the LLM more than just the generated responses.

## 2 Related work

Safety alignment can partially alleviate safety concerns: aligned LLMs usually generate responses that are closer to human moral values and tend to refuse toxic prompts. For example, Ouyang et al.  incorporate Reinforcement Learning from Human Feedback (RLHF) to fine-tune LLMs, improving alignment. Yet, further improving alignment is challenging [24; 27].

Toxicity detection can be a supplement to safety alignment to further improve the safety of LLMs. Online APIs such as the OpenAI Moderation API , Perspective API , and Azure AI Content Safety API  can be used to detect toxic prompts. Also, Llama Guard is an open model that can be used to detect toxic/unsafe prompts .

## 3 Preliminaries

### Problem Setting

Toxicity detection aims to detect prompts that may lead a LLM to produce harmful responses. One can attempt to detect such situations solely by inspecting the prompt, or by inspecting both the prompt and the response. According to , both approaches yield comparable performance. Therefore, in this paper, we focus on detecting toxicity based solely on the prompt. This has a key benefit: it means that we can block toxic prompts before the LLM produces any response, even for streaming APIs and streaming web interfaces. We focus on toxicity detection "for free", i.e., without running another classifier on the prompt. Instead, we inspect the output of the existing LLM, and specifically, the logits/softmax outputs that indicate the distribution over tokens.

### Evaluation metrics

We measure the effectiveness of a toxicity detector using three metrics:

**Balanced optimal accuracy:** The accuracy indicates the proportion of the examples in which the predictions agree with the ground truth labels. Balanced optimal prediction accuracy is evaluated on a balanced dataset where the proportion of negatives and positives is roughly equal.

**Area Under Precision-Recall Curve (AUPRC):** In real-world applications, there is significant class imbalance: benign prompts are much more common than toxic prompts. The Precision-Recall Curve plots precision against the recall across various TPR to FPR tradeoffs, without assuming balanced classes. AUPRC is a primary metric in past work, so we measure it in our evaluation as well.

**True Positive Rate (TPR) at low False Positive Rate (FPR):** Because most prompts are benign, even a modest FPR (e.g., 5%) is unacceptable, as it would cause loss of functionality for many benign users. In practice, we suspect model providers have an extremely low tolerance for FPR when applying the detection method. Therefore, we measure the TPR when FPR is constrained below some threshold of acceptability (e.g., 0.1%). We suspect this metric might be the most relevant to practice.

Figure 2: Illustration of the candidate responses and the starting tokens.

## 4 Toy models

To help build intuition for our approach, we propose two toy models that help motivate our final approach. The first toy model has an intuitive design rationale, but is too inefficient to deploy, and the second is a simple approximation to the first that is much more efficient. We evaluate their performance on a small dataset containing the first \(100\) benign prompts and \(100\) toxic prompts from the test split of the ToxicChat  dataset. Llama2  is employed as the base model.

### Probability of refusals

Current LLMs are usually robustly finetuned to reject toxic prompts (see Figure 3). Therefore, a straightforward idea to detect toxicity is to simply check whether the LLM will respond with a rejection sentence (a refusal). Specifically, we evaluate the probability that a randomly sampled response to this prompt is a refusal.

To estimate this probability, we randomly generate 100 responses \(r_{i}\) to each prompt \(x\) and estimate the probability of refusal (PoR) using a simple point estimate:

\[(x)=_{i=1}^{100}[r_{i}],\] (1)

Following , we treat a response \(r\) as a refusal if it starts with one of several refusal keywords. As shown in Figure 3(a), there is a huge gap between the PoR distribution for benign vs toxic prompts, indicating that we can accurately detect toxic prompts by comparing the PoR to a threshold. We hypothesize this works because alignment fine-tuning significantly increases the PoR for toxic prompts, so even if alignment is not able to completely prevent responding to a toxic prompt, there are still signs that the prompt is toxic in the elevated PoR.

Figure 4: (a) LLMs have a high probability of refusing to respond for most toxic prompts (_Positives_) and a low probability for benign prompts (_Negatives_). (b) The logit for “Sorry” appearing as the first token of the response tends to be higher for positives than negatives. (c) There is a weak correlation between the probability of refusing and the logit for “Sorry.”

Figure 3: Typical prompts and responses.

However, it is completely infeasible to generate 100 responses at runtime, so while accurate, this is not a practical detection strategy. Nonetheless, it provides motivation for our final approach.

### Logits of refusal tokens

Since calculating the PoR is time-consuming, we now turn to more efficient detection strategies. We noticed that many refusal sentences start with a token that implies refusal, such as _Sorry_, _Cannot_, or \(I\) (_I_ usually leads to a refusal when it is the first token of the response); and sentences that start with one of these tokens are usually a refusal. Though the probability of starting with such a token could be quite low, there can still be a huge gap between negative and positive examples. Therefore, instead of computing the PoR, we compute the probability of the response starting with a refusal token (PoRT). This is easy to compute:

\[(x)=_{t}(t),\] (2)

where \(t\) ranges over all refusal tokens, and \((t)\) denotes the estimated probability of \(t\) at the start position of the response for prompt \(x\). This allows us to detect toxic prompts based on the softmax/logit values at the output of the model, without any additional computation or classifier.

We build two toy toxicity detectors, by comparing PoR or PoRT to a threshold, and then compare them by constructing a confusion matrix for their predictions (Table S1 in the Appendix). In this experiment, we used _Sorry_ as the only refusal token for PoRT, and we computed the classification threshold as the median value of each feature over the \(200\) examples from the small evaluation dataset. We found a high degree of agreement between these two approaches, indicating that toxicity detection based on PoRT is built on a principled foundation.

### Evaluation of the toy models

We evaluated the performance of the toy models on the small evaluation dataset. We estimated PoR with \(1\), \(10\), or \(100\) outputs, and calculated PoRT with three refusal tokens (_Sorry_, _Cannot_ and \(I\); tokens 8221, 15808, and 306). In practice, we used the logits for PoRT since it is empirically better than using softmax outputs. We evaluate the performance with balanced prediction accuracy Acc, AUPRC, and TPR at low FPR (TPR@FPR\({}_{}\)). For TPR@FPR\({}_{}\), we set the FPR to be \(10\%,1\%,0.1\%\), respectively.

Results are in Table 1. All toy models achieve accuracy around \(80\%\), indicating they are all decent detectors on a balanced dataset. Increasing the number of samples improves the PoR detector, which is reasonable since the estimated probability will be more accurate with more samples. PoR struggles at low FPR. We believe this is because of sampling error in our estimate of PoR: if the ground truth PoR of some benign prompts is close to \(1.0\), then after sampling only 100 responses, the estimate \(_{100}\) might be exactly equal to \(1.0\) (which does appear to happen; see Figure 3(a)), forcing the threshold to be \(1.0\) if we wish to achieve low FPR, thereby failing to detect any toxic prompts. Since the FPR tolerance of real-world applications could be very low, one may need to generate more than a hundred responses if the detector is based on PoR.

In contrast, PoRT-based detectors avoid this problem, because we obtain the probability of a refusal token directly without any estimate or sampling error. These results motivate the design of our final detector, which is based on the logit/softmax outputs for the start of the response.

    & Acc\({}_{}\) & AUPRC & TPR@FPR\({}_{10\%}\) & TPR@FPR\({}_{1\%}\) & TPR@FPR\({}_{0.1\%}\) \\  PoR\({}_{1}\) & 78.0 & 71.4 & 0.0 & 0.0 & 0.0 \\ PoR\({}_{10}\) & **81.0** & 77.1 & 0.0 & 0.0 & 0.0 \\ PoR\({}_{100}\) & 80.5 & 79.3 & **50.0** & 0.0 & 0.0 \\ Logits\({}_{}\) & **81.0** & 76.5 & 30.0 & 9.0 & 5.0 \\ Logits\({}_{}\) & 75.5 & 79.3 & 45.0 & 13.0 & 10.0 \\ Logits\({}_{}\) & 78.5 & **83.8** & 47.0 & **31.0** & **24.0** \\   

Table 1: Effectiveness of the toy modelsMULI: Moderation Using LLM Introspection

Concluding from the results of the toy models, even the logit of a single specific starting token contains sufficient information to determine whether the prompt is toxic. In fact, hundreds of thousands of tokens can be used to extract such information. For example, Llama2 outputs logits for \(36,000\) tokens at each position of the response. Therefore, we employ a Sparse Logistic Regression (SLR) model to extract additional information from the token logits in order to detect toxic prompts.

Suppose the LLM receives a prompt \(x\); we extract the logits of all \(n\) tokens at the starting position of the response, denoted by a vector \(l(x)^{n}\). We then apply an additional function \(f:^{n}^{n}\) on the logits before sending to the SLR model. We denote the weight and the bias of the SLR model by \(^{n}\) and \(b\) respectively, and formulate the output of SLR to be

\[(x)=^{T}f(l(x))+b.\] (3)

In practice, we use the following function as \(f\):

\[f^{*}(l)=(((l))-(1-(l))),\] (4)

is the estimated re-scaled probability by applying the Softmax function across all token logits. \(()\) is a normalization function, where the mean and standard deviation values are estimated on a training dataset and then fixed. \(f^{*}\) can be understood as computing log-odds for each possible token and then normalizing these values to a fixed mean and standard deviation. The parameters \(,b\) in Equation (3) are optimized for the following SLR problem with lasso regularization:

\[_{,b}_{\{x,y\}}(((x)),y)+\|w\|_{1}\,.\] (5)

In the above equation, \(\) indicates the training set, each example of which consists of a prompt \(x\) and the corresponding toxicity label \(y\{0,1\}\), \(()\) denotes the Binary Cross-Entropy (BCE) Loss, \(\|\|_{1}\) denotes the \(_{1}\) norm, and \(\) is a scalar coefficient.

## 6 Experiments

### Experimental setup

**Baseline models.** We compared our models to LlamaGuard  and the OpenAI Moderation API  (denoted by OMod), two current SOTA toxicity detectors. We also queried GPT-4o and GPT-4o-mini  (the prompt can be found in Appendix A.7) for additional comparison. For LlamaGuard, we use the default instructions for toxicity detection. Since it always output either _safe_ or _unsafe_, we extracted the logits of _safe_ and _unsafe_ and use the feature \(_{}=_{}- _{}\) for multi-threshold detection. For OpenAI Moderation API, we found that directly using the toxicity flag as the indicator of the positive leads to too many false negatives. Therefore, for each prompt we use the maximum score \(c(0,1)\) among all \(18\) sub-categories of toxicity and calculate the feature \(_{}=(c)-(1-c)\) for multi-threshold evaluation.

**Dataset.** We used the prompts in the ToxicChat  and LMSYS-Chat-1M  datasets for evaluation, and included the OpenAI Moderation API Evaluation dataset for cross-dataset validation . The training split of ToxicChat consists of \(4698\) benign prompts and \(384\) toxic prompts, the latter including \(113\) jailbreaking prompts. The test split contains \(4721\) benign prompts and \(362\) toxic prompts (the latter includes \(91\) jailbreaking prompts). For LMSYS-Chat-1M, we extracted a subset of prompts from the original dataset. We checked through the extracted prompts, grouped all the similar prompts, and manually labeled the remaining ones as toxic or non-toxic. We then randomly split them into training and test sets without splitting the groups. The training split consists of \(4868\) benign and \(1667\) toxic examples, while the test split consists of \(5221\) benign and \(1798\) toxic examples.

**Evaluation metrics and implementation details.** We measured the optimal prediction accuracy Acc\({}_{}\), AUPRC and TPR at low FPRPR\(\)FPR\({}_{}\). For TPR@FPR\({}_{}\), we set the FPR \(\{10\%,1\%,0.1\%,0.01\%\}\). The analysis is based on llama-2-7b except otherwise specified. For llama-2-7b, we set \(=1 10^{-3}\) in Equation (5) and optimized the parameters \(\) and \(b\) for \(500\) epochs by Stochastic Gradient Descent with a learning rate of \(5 10^{-4}\) and batch size \(128\). We released our code on GitHub1.

[MISSING_PAGE_FAIL:7]

We further display the logarithm scale plot of TPR versus FPR for different models in Figure 5. We also include one of the toy models \(_{}\). On ToxicChat, MULI outperforms all other schemes, achieving significantly better TPR at all FPR scales. On LMSYS-Chat-1M, MULI is comparable to the OpenAI Content Moderation API and outperforms all others. Even the performance of toy model \(_{}\) is comparable to that of LlamaGuard and the OpenAI Content ModerationAPI on ToxicChat, even though the toy model is zero-shot and almost zero-cost.

### MULI based on different LLM models

We built and evaluated MULI detectors based on different models [26; 19; 30; 32; 13; 9; 22; 7]. See Table S2 in the Appendix for the results. Among all the models, the detectors based on llama-2-7b and llama-2-13b exhibit the best performance under multiple metrics. For instance, the detector based on llama-2-13b obtained \(46.13\%\) TPR at \(0.1\%\) FPR. It may benefit from the strong alignment techniques, such as shadow attention, that were incorporated during training of Llama2. Performance drops heavily when Llama models are quantized. The second tier includes Llama3, Vicuna, and Mistral. They all obtained around \(30\%\) TPR at \(0.1\%\) FPR.

We further investigated the correlation between the security of base LLMs and the performance of the MULI detectors. We collected the Attack Success Rate (ASR) of the human-generated jailbreaks evaluated by HarmBench and computed the security score of the model by \(_{}=100\%-\). See Figure 6 for the scatter plot for different LLMs. The correlation is clear: the more secure the base LLM is against jailbreaks and toxic prompts (the stronger the safety alignment), the higher the performance that our detector can achieve. Such findings corroborated our motivation at the very beginning, which was that well-aligned LLMs already provide sufficient information for toxicity detection in their output.

### Dataset sensitivity

Figure 7 shows the effect of the training set size on the performance of MULI (see Table S3 in the Appendix for additional results). Even training on just ten prompts (nine benign prompts and only one toxic prompt) is sufficient for MULI to achieve \(76.92\%\) AUPRC and \(13.81\%\) TPR at \(0.1\%\) FPR, which is still better than LlamaGuard and the OpenAI Content Moderation API.

Table 4 shows the robustness of MULI when used on a different data distribution than it was trained on. In cross-dataset scenarios, the model's performance tends to be slightly inferior compared to its performance on the original dataset. Yet, it still surpasses the baseline models on ToxicChat, where the TPRs at \(0.1\%\) FPR of LlamaGuard and OMod are \(5.25\%\) and \(6.08\%\), respectively. In addition,

   TrainingTest &  AUPRC \\  & 
 TPR@\(_{0.1\%}\) \\  \\  ToxicChat & 91.29 & 95.86 & 42.54 & 31.31 \\ LMSYS-Chat-1M & 79.62 & 98.23 & 33.43 & 66.85 \\   

Table 4: Cross-dataset performance

Figure 6: Security score of different models versus (a) AUPRC; (b) TPR@\(_{0.1\%}\).

we also evaluated both detectors and baseline models on the OpenAI Moderation API Evaluation dataset. The results are in Table 5. The TPR at \(0.1\%\) FPR of MULTI trained on ToxicChat / MULTI trained on lmsys1m / LlamaGuard / OpenAI Moderation API are \(24.90\%/25.86\%/14.56\%/15.13\%\), respectively. Even when MULIs is trained on other datasets, its performance significantly exceeds SOTA methods.

### Interpretation of the failure cases

We inspected some failure cases of MULTI. As shown in Figure S1, the MULTI logits of most negative examples in Toxic Chat are below \(3\), while that of most positive examples are above \(0\). We found that the failure cases all seem to be ambiguous borderline examples. Some high-logit negative examples contain sensitive words. Some low-logit positive examples are extremely long prompts with a little bit of harmful content or are related to inconspicuous jailbreaking attempts. See the examples in Appendix A.8.

### Interpretation of the SLR weights

In order to find how MULTI detects toxic prompts, we looked into the weights of SLR trained on different training sets. We collected five typical refusal starting tokens, including _Not_, _Sorry_, _Cannot_, \(I\), _Unable_, and collected five typical affirmative starting tokens, including _OK_, _Sure_, _Here_, _Yes_, _Good_. We extracted their corresponding weights in SLR and calculated their ranks (see Table S4 in the Appendix). The rank \(r\) of a weight \(w\) is calculated by \(r(w)=\{v W|v>w\}|W|\), where \(W\) is the set of all weight values. A rank value near \(0\) (resp. \(1\)) suggests the corresponding token is associated with benign (resp. toxic) prompts, and more useful tokens for detection have ranks closer to \(0\) or \(1\). Note that since the SLR is sparsely regularized, weights with ranks between \(0.15-0.85\) are usually very close to zero. Refusal tokens generally seem more useful for toxicity detection than affirmative tokens, as suggested by the frequent observations of ranks as low as \(0.01\) for the refusal tokens in Table S4. Our intuition is the information extracted by SLR could be partially based on LLMs' intention to refuse.

### Ablation study

We trained MULTI with different function \(f\) in Equation (3) and different sparse regularizations. See Table 6 for the comparison. The candidate functions include \(f^{*}\) defined in Equation (4), \(

    & Acc\({}_{}\) & AUPRC & TPR@FPR\({}_{10\%}\) & TPR@FPR\({}_{1\%}\) & TPR@FPR\({}_{0.1\%}\) & TPR@FPR\({}_{0.01\%}\) \\  MULTI\({}_{}\) & 86.85 & 85.84 & 78.54 & 37.16 & 24.90 & **22.80** \\ MULTI\({}_{}\) & 86.61 & **87.52** & 77.78 & **41.38** & **25.86** & 18.01 \\ LlamaGuard & 85.95 & 84.74 & 75.86 & 34.87 & 14.56 & 12.64 \\ OMod & **88.15** & 87.03 & **82.38** & 31.99 & 15.13 & 11.69 \\   

Table 5: Results on OpenAI Moderation API Evaluation dataset

Figure 7: Results of MULTI with different training set sizes on ToxicChat by (a) AUPRC; (b) TPR@FPR\({}_{0.1\%}\). The dashed lines indicate the scores of LlamaGuard and OMod.

outputting the logits of the tokens, \(\) outputting the probability of the tokens, and \(()\) outputting the logarithm probability of the tokens. The candidate sparse regularization inlude \(_{1}\), \(_{2}\), and \(\) for no regularization. We can see that \(f^{*}\) and \(()\) exhibit comparable performance. The model with function \(f^{*}\) has the highest AUPRC score, as well as TPR at \(10\%\) and \(1\%\) FPR. The model trained on the logits achieved the highest TPR at extremely low (\(0.1\%\) and \(0.01\%\)) FPR.

## 7 Conclusion

We proposed MULI, a low-cost toxicity detection method with performance that surpasses current SOTA LLM-based detectors under multiple metrics. In addition, MULI exhibits high TPR at low FPR, which can significantly lower the cost caused by false alarms in real-world applications.

MULI only scratches the surface of information hidden in the output of LLMs. We encourage researchers to look deeper into the information hidden in LLMs in the future.

LimitationsMULI relies on well-aligned models, since it relies on the output of the LLM to contain information about harmfulness. MULI's ability to detect toxic prompts was shown to be correlated with the strength of alignment of base LLMs, so we expect it will work poorly with weakly-aligned or unaligned LLMs. MULI has also not been tested under scenarios where a malicious user fine-tunes an LLM to remove the safety alignment or launches adversarial attacks. In such scenarios, running MULI based on a separate LLM may be required, which could incur an additional inference cost. Moreover, we didn't evaluate whether MULI remains equally effective across demographic subgroups [11; 8], which could be a topic for future work. Training MULI requires a one-time training cost, to run the base LLM on the prompts in the training set, so while MULI is free at inference time, it does require some upfront cost to train. If training cost is an issue, even training MULI on just ten examples suffices to achieve performance superior to SOTA detectors, as shown in Section 6.4.