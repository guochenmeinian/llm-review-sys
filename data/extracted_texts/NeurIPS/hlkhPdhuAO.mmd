# Global-correlated 3D-decoupling Transformer

for Clothed Avatar Reconstruction

 Zechuan Zhang\({}^{1}\), Li Sun\({}^{1}\), Zongxin Yang\({}^{1}\), Ling Chen\({}^{2}\), Yi Yang\({}^{1}\)

\({}^{1}\) ReLER, CCAI, Zhejiang University

\({}^{2}\) AAII, University of Technology Sydney

###### Abstract

Reconstructing 3D clothed avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present **G**lobal-correlated 3D-decoupling **T**ransformer for clothed **A**vatar reconstruction (**GTA**), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0 datasets illustrate that our method outperforms state-of-the-art approaches in both geometry and texture reconstruction, exhibiting high robustness to challenging poses and loose clothing, and producing higher-resolution textures. Codes are available at https://github.com/River-Zhang/GTA.

+
Footnote â€ : \(\): the corresponding author.

## 1 Introduction

As virtual worlds and metaverse technology gain popularity, the demand for advanced techniques to reconstruct 3D clothed human avatars from single images is rapidly increasing. These techniques  are employed across various areas, such as AR/VR, social telepresence, virtual try-on, or the movie industry. However, in-the-wild images often present challenges, such as loose clothing and complex poses, which are not typically found in training data. As a result, there is a pressing need for models that can effectively generalize to these scenarios and reconstruct accurate, animatable, and high-resolution 3D human avatars.

In light of the significant progress made in 3D clothed human avatar reconstruction, existing models still face two main limitations: (i) _Overreliance on 2D image features_. Sole dependence on 2D CNN-based features compromises the accuracy of 3D object reconstructions due to the lack of global correlation. Despite the integration of 3D features from human body priors in methods like , their inconsistent performance with loose clothing and challenging poses (See Fig. 2) indicates insufficient integration. Additionally, optimization-based methods  can be complex and prone to errors, reducing reliability. (ii) _Inconsistent query methods_. Current strategies for querying features differ and have drawbacks. The pixel-aligned method  directlyprojects query points on feature maps but lacks guidance from human body prior, while the prior-guided strategy  integrates features on a human body prior but may lead to loss of detailed information on the image and result in 3D avatar reconstructions with an increased level of fuzziness.

Considering the limitations discussed above, we propose that 2D feature maps are insufficient for 3D reconstruction tasks, while global-correlated 3D feature representations offer a more effective solution. Traditional 3D representations are space-intensive and inefficient, necessitating alternatives such as the memory-conserving tri-plane representation . However, generating global-correlated 3D representations from monocular images remains challenging due to difficulties in obtaining orthogonal plane feature maps. Our approach employs learnable embeddings and cross-attention mechanisms to effectively model intricate cross-plane relationships, enabling robust and precise 3D feature extraction. Furthermore, it is important to develop a strategy that synergizes various query methods while maintaining simplicity and efficiency. By combining existing strategies for 3D features, our method leverages localized spatial features and prior knowledge of human body structure, resulting in a balanced feature extraction process that improves reconstruction performance.

In response to the identified challenges, we present **GTA** (**G**lobal-correlated 3D-decoupling **T**ransformer for clothed **A**vatar reconstruction), employing a novel global-correlated 3D-decoupling transformer and a hybrid prior fusion strategy for comprehensive 3D geometry and texture reconstruction. Our vision transformer-based encoder extracts global-correlated features from the input image, while our unique 3D-decoupling decoder disentangles tri-plane 3D features using learnable embeddings as queries. This integration of global-correlated encoding and 3D-decoupling decoding effectively captures the 3D avatar structure from a single image. To further enhance feature fusion, our hybrid prior fusion strategy combines spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. This efficient and accurate integration strategy achieves state-of-the-art performance in single-view human avatar reconstruction.

Our proposed model, trained on THuman2.0 , outperforms state-of-the-art(SOTA) methods in geometry and texture reconstruction. We achieve a significant reduction in Chamfer distance on CAPE-FP  test dataset, below 0.8cm for the first time, and demonstrate superior side-view normal performance, illustrating our method's efficacy in reconstructing accurate 3D clothed human avatars. Our model excels in handling complex poses and loose clothing, and attains state-of-the-art texture reconstruction with higher PSNR scores. Moreover, it can be extended to animation and virtual try-on applications, showcasing its wide-ranging real-world potential. Our main contributions include:

* We introduce a novel global-correlated 3D-decoupling transformer that effectively disentangles tri-plane features, thereby substantially enhancing the reconstruction of clothed avatars from 2D images. To the best of our knowledge, our approach is the pioneering application of transformers in 3D feature decoupling for monocular human avatar reconstruction tasks.
* We put forward an innovative hybrid prior fusion strategy for feature query, combining spatial query's localization capabilities with prior-enhanced query's ability to incorporate knowledge of the human body prior, ultimately leading to improved geometry and texture reconstruction performance.
* Our proposed model achieves state-of-the-art performance in both clothed human geometry and texture reconstruction, outperforming previous models and exhibiting enhanced side-view normal performance.

Figure 1: Given a monocular image as input, GTA reconstructs the full 3D geometry and texture of the subject portrayed, allowing for various applications such as virtual try-on and animation.

## 2 Related Work

**Monocular Human Reconstruction** has been an active area of research for many years. This task is inherently ill-posed due to the lack of 3D information, requiring additional assumptions or prior knowledge to recover the full 3D structure. Previous research has proposed effective parametric human prior models [17; 18; 19; 20; 21], which employs statistical methods to reduce the variations in human body shape and pose to a compact set of parameters. By leveraging this model, subsequent research has proposed novel methods to estimate or regress the model parameters from a single RGB image [22; 23; 24; 25; 26]. However, the human prior models can only capture a minimally clothed body without complex details like garments, adornments, or hairstyles. To address this limitation, some researchers add offsets on the top of prior body vertices to simulate outfits [27; 28; 29; 30; 31]. While these methods can effectively represent clothing close to the body surface and use blending weights of surface vertices to drive the clothing, they are not suitable for geometry topology far from the human body, such as robes and dresses.

In order to overcome the constraints imposed on reconstruction by clothing shape and type, researchers have explored various alternative representations for the human body, including voxels [32; 33], visual hulls , double depth maps [11; 12; 13; 4], and UV maps translation . Among these diverse methods, implicit function-based methods  have shown the most remarkable performance. Saito et al. introduced PIFu , which firstly incorporates implicit functions into the problem of human body reconstruction. The method leverages a CNN-based neural network to extract features from 2D images and uses implicit functions to express the spatial geometry field, such as signed distance functions (SDF)  and occupancy fields . While implicit function-based methods [1; 39; 6] can accurately reconstruct the complex topology of clothed human body surfaces, they may generate non-anatomical shapes for out-of-distribution poses due to the lack of regularization.

To improve pose robustness, recent research [3; 2; 4; 5; 7] has utilized the prior knowledge to guide implicit function representation. These methods have shown promising results in enhancing the quality and accuracy of reconstruction geometry, particularly for challenging poses. However, these methods, like previous ones, still rely on 2D features extracted from CNN-based networks, even though some of them incorporate 3D features obtained from human body prior. In the reconstruction process, the feature obtained by 2D projection may result in incomplete reconstructions from other viewpoints and diminish overall reconstruction accuracy. Our method extracts global-correlated 3D-aware feature to efficiently represent the clothed human avatar.

**Transformers in Vision.** The transformer architecture, initially proposed by Vaswani et al. , has achieved immense success across domains like NLP, speech recognition, and multimodal applications. Inspired by this, many studies have attempted to adapt the transformer architecture to the field of computer vision. Among these explorations, the Vision Transformer (ViT) proposed by Dosovitskiy et al.  has shown impressive performance in 2D visual tasks. Meanwhile, transformer's ability to model global and long-range correlation is also suitable for 3D vision tasks. Therefore, we leverage a ViT-based 3D transformer with cross-plane attention to efficiently extract global-correlated 3D features for better human reconstruction.

**Generative 3D-aware Feature.** Recent studies  have proposed the tri-plane 3D feature representation method, which efficiently extracts features from objects in three orthogonal orientations. Tri-plane representation has been demonstrated high efficacy in generating 3D objects , particu

Figure 2: GTA vs. SOTA. SOTA methods (red) are vulnerable to challenging poses and loose clothing, leading to artifacts such as non-human shapes (PIFu , PaMIR ), incomplete clothing reconstruction (ICON ), and erroneous stitching (ECON ). GTA deals with these challenges and produces high-quality results (blue).

larly 3D human bodies [43; 44; 45]. Thus, the tri-plane representation method holds potential for application in human body reconstruction. Nevertheless, the challenge of establishing a reasonable relationship between the input monocular image and the three planes of features remains an unresolved problem. In our method, we introduce learnable embeddings to represent features of spatial planes that are not directly visible, and utilize cross-attention mechanisms to establish relationships between the input image and other planes.

## 3 Method

We introduce an implicit function-based framework for reconstructing 3D clothed human models from a single image (See Fig. 3). Our model employs a global-correlated 3D-decoupling transformer to disentangle tri-plane features and harnesses a hybrid prior fusion strategy for reconstructing the full 3D geometry and texture of the clothed avatar. In the following sections, we will discuss the preliminaries of **GTA** in Sec. 3.1, the global-correlated 3D-decoupling transformer in Sec. 3.2, and the hybrid prior fusion strategy in Sec. 3.3.

### Preliminary

**SMPL.** The Skinned Multi-Person Linear (SMPL) model  is a widely-used parametric human body model. The SMPL model utilizes shape parameters \(^{10}\) and pose parameters \(^{3 K}\) to parameterize the deformation of the human body mesh \(\):

\[(,):^{3 N}\] (1)

where \(K=24\) joints and \(N=6890\) vertices. Shape parameter \(\) describes the body's overall size and proportions and pose parameter \(\) defines the positions and orientations of the joints relative to their default positions. SMPL enables effective representation and manipulation of human body shape and pose in various applications.

**Implicit Function.** Implicit function is a powerful tool for modeling complex geometries with neural networks. Our implicit function maps an input point to a scalar value that represents the spatial field including occupancy field and color field. The occupancy field takes a point in space as input and outputs a binary value indicating whether the point is inside or outside the human surface. Our reconstructed human surface can be represented as \(_{}\):

\[_{}=\{^{3}()= (o,)\}\] (2)

where occupancy \(o\) = 0.5, color \(^{3}\), and \(\) represents the implicit function.

Figure 3: GTA Overview. GTA has two key modules: (1) the global-correlated 3D-decoupling transformer and (2) the hybrid prior fusion strategy. The former module extracts a latent \(\) from the input image and integrates it with learnable embeddings \(\) via 3D-decoupling decoders, producing disentangled tri-plane features. The latter module employs spatial (red) and prior-enhanced (green) queries to merge features from the two tri-planes, thus enabling geometry and texture reconstruction.

### Global-correlated 3D-decoupling Transformer

Directly extracting 3D information from a single 2D image is not feasible, this is largely due to the fact that 3D features include information from planes orthogonal with the image plane (also noted as \(xy\)-plane, principle-plane). To effectively decouple cross-plane features from a monocular image input, it is crucial to have additional guiding information. This is where the idea of learnable embeddings and the use of a cross-attention mechanism prove valuable. Inspired by this, we devise a novel transformer-based architecture including a global-correlated encoder and a 3D-decoupling decoder to disentangle 3D features from single input images.

**Global-correlated Encoder.** In our method, we employ a vision transformer to encode the input image and capture global correlations in the image, resulting in high-dimensional global-correlated image features. Our encoder module processes the input image by dividing it into non-overlapping \(n n\) patches and subsequently mapping them to image features through transformer blocks. This procedure generates a latent \(\) for a image \(I\).

**3D-decoupling Decoder.** To decode 3D tri-plane features from the encoder output, we propose to use two types of decoders: the principle-plane decoder and the cross-plane decoder. Our principle-plane decoder generates \(xy\)-plane features, which share the same plane as the input image. This decoder effectively reverses the encoding operation, leveraging a self-attention mechanism on the encoder output and converting the image features into a principal feature map \(F_{xy}^{H W C}\).

In order to generate plane features orthogonal with the principle plane while preserving global correlation with the principle plane, we employ the cross-plane decoder to decode \(yz\) and \(xz\) plane features from input image features. To guide the decoder in decoding features from different planes, we introduce a learnable embedding \(\) that supplies additional information for decoupling new planes. The learnable embedding \(\) is first processed through self-attention encoding. It is then used as a query in a multi-head cross-attention mechanism with the output image latent \(\) from the encoder stack. The image features are converted into keys and values for the cross-attention mechanism,

\[(,)=( ())(W^{K})^{T})}{})(W^{V})\] (3)

where \(W^{Q}\), \(W^{K}\), and \(W^{V}\) are learnable parameters and \(d\) is the scaling coefficient. Following the original transformer architecture , our model employs residual connections  and layer normalization  after each sub-layer. The entire decoder consists of multiple identical layers, and we use two such decoders to produce feature maps \(F_{yz}^{H W C}\) and \(F_{xz}^{H W C}\).

**Principle-plane Refinement.** In accordance with the approach demonstrated in , higher-resolution feature maps play a crucial role in producing detailed geometry and sharper textures. Hence, we use both the original image and the principle-plane feature map, to produce a higher-resolution feature map. The original image is initially down-convoluted to match the principle-plane size and then concatenated along the channel dimension. Subsequently, they are fed into a streamlined Hourglass network and a super-resolution module for refinement. This process generates a higher-resolution feature map \(F_{xy}^{refine}^{2H 2W C}\).

\[F_{xy}^{refine}=(((I) _{xy}))\] (4)

where \(\) means concatenation operation. The resulting \(xy\) plane (principle-plane) exhibits a higher resolution than the \(yz\) and \(xz\) planes and incorporates more information from the original image, leading to higher fidelity reconstruction.

**Tri-plane Division.** After obtaining each plane, we evenly divide the plane features along the channel dimension into two groups, creating two tri-planes. For one group, we perform a spatial query to acquire features for query points, while for the other group, we utilize a prior-enhanced query to integrate the human body prior. Please refer to the next section for a detailed introduction to our novel hybrid prior fusion strategy.

### Hybrid Prior Fusion Strategy

In previous works, two primary methods have been utilized for acquiring query point features, each with significant limitations as previously discussed in Sec. 1. To address this, we propose a hybrid prior fusion strategy that combines the strengths of both spatial query and prior-enhanced query.

**Spatial Query.** Following , we extend the pixel-aligned query into 3D space, denoted as spatial query. This method projects query points onto the \(xy\), \(yz\), and \(xz\) planes of a tri-plane group, producing localized features that capture important details for reconstruction. We combine the \(F_{yz}\) and \(F_{xz}\) features by summation and concatenate the result with \(F_{xy}^{refine}\) to generate the spatial query feature \(F_{SQ}()\):

\[F^{SQ}()=F_{xy}^{SQ}()(F_{yz}^{SQ}()+F_{xz}^{SQ}( {x}))\] (5)

where \(F_{xy}^{SQ}()\), \(F_{yz}^{SQ}()\), \(F_{xz}^{SQ}()\) are extracted from \(F_{xy}^{refine}\), \(F_{yz}\), \(F_{xz}\), and \(\) is concatenation.

**Prior-enhanced Query.** For the other tri-plane, we project the human body prior [18; 19] mesh vertices onto the three planes similar to the spatial query above to obtain the feature \(F^{PQ}()\), \(\), where \(\) is the body prior mesh. For each query point \(\), we find the nearest triangular face \(t_{}=[_{0},_{1},_{2}]^{3 3}\) and use barycentric interpolation to integrate features for \(\) (See Fig. 3), denoted as \(F_{PQ}()\):

\[F^{PQ}()=uF^{PQ}(_{0})+vF^{PQ}(_{1})+wF^{PQ}(_{2})\] (6)

where \([u,v,w]\) represents the barycentric coordinates of the query point \(\) projected onto triangle \(t_{}\).

**Hybrid Prior Fusion Strategy.** Spatial query projects the query points directly onto tri-plane features, providing detailed information but lacking prior knowledge. On the other hand, the prior-enhanced query merges body prior information but may causes an increased level of fuzziness. Therefore, we concatenate these two query features to capitalize on each method's strengths and compensate for their weaknesses. Furthermore, we also include the signed distance between query point and human prior mesh \(_{Prior}()\) and pixel-aligned normal feature \(F_{}()\) as input to the implicit function for predicting occupancy and color. Consequently, the reconstructed human surface \(_{}\) can be represented as:

Figure 4: Qualitative 3D human reconstruction for real images showcasing diverse poses and clothing variations. For each example, we show the input image along with two views of the reconstructed geometry and front view of the reconstructed texture. Our approach is robust to challenging poses and loose clothing, and contains detailed geometry and texture. See SupMat. for more results.

\[_{}=\{^{3}(F^{SQ}(), F^{PQ}(),_{Prior}(),F_{}())=(o,)\}\] (7)

where occupancy \(o\) = 0.5, color \(^{3}\), and \(\) represents the implicit function.

**Training Objectives.** For each 3D scan, we consider two sets of points as training data, denoted as \(G_{o}\) and \(G_{c}\). \(G_{c}\) is sampled uniformly with a slight perturbation along the normals of the mesh surface, whereas \(G_{o}\) is sampled according to the same strategy as in PIFu , where points are sampled near the mesh surface and throughout the entire space.

For the points in \(G_{o}\), we employ the following loss function:

\[_{o}=|}_{ G_{o}}BCE(_{}-o_ {})\] (8)

where \(_{}\) denotes the model's predicted occupancy, while \(o_{}\) signifies the ground truth occupancy. For the sampled points in \(G_{c}\), we apply the following loss function:

\[_{c}=|}_{ G_{c}}|}_{}- _{}|\] (9)

where \(}_{}\) represents the predicted color at location \(\) by the model, while \(_{}\) indicates the true color of the mesh at \(\). The overall loss function is expressed by:

\[_{GTA}=_{o}+_{c}\] (10)

## 4 Experiments

**Strategy for Point Sampling.** In the context of each training subject, our approach involves obtaining 2048 points for occupancy, denoted as \(G_{o}\), and 2048 points for color, symbolized as \(G_{c}\). The method for occupancy point sampling is aligned with the strategy illustrated in . Color points are sampled uniformly, with a minor Gaussian disturbance, expressed as \((0,)\), wherein our experiment \(\) is set at 0.1 cm. This disturbance occurs along the normals of the mesh surface. We obtain labels for the ground truth geometry, which specify whether a point is inside or outside the surface, through the application of Kaolin  to ascertain if a point lies within the ground truth mesh. The source of the ground truth color labels is the UV texture map of the 3D meshes.

**Model Structure.** To generate global-correlated latent features, we utilize a Vision Transformer (ViT)  model of depth 6, functioning as our global-correlated encoder, and generating an output of size \(1024 256\). Our 3D-decoupling decoder incorporates both cross-plane and principal-plane decoders, each with a depth of three. The cross-plane decoder is initialized with learnable embeddings that experience a Gaussian perturbation to align cohesively with the encoder's output shape. The configuration of the cross-plane decoder corresponds with the structure described in , while the principal-plane decoder emulates the global-correlated encoder. Each decoder outputs a feature map \(F^{128 128 64}\). During refinement, a 2-stack hourglass and a transpose convolution module are integrated to generate a higher resolution principal-plane feature map \(F_{xy}^{refine}^{128 128 64}\). Following the feature acquisition through our hybrid prior fusion method, two identical Multilayer

    &  &  &  &  \\   & & Chamler \(\) & P2S\(\) & Normals\(\) & Chamler \(\) & P2S\(\) & Normals\(\) & Chamler \(\) & P2S\(\) & Normals\(\) \\  PIFu  & THuman2.0  & 2.458 & 2.117 & 0.094 & 1.786 & 1.639 & 0.071 & 1.586 & 1.530 & 0.088 \\ PIFu  & Renderpeople  & 2.973 & 2.940 & 0.111 & 2.100 & 2.093 & 0.091 & - & - & - \\ PIFuHD  & Renderpeople  & 3.767 & 3.591 & 0.123 & 2.302 & 2.335 & 0.090 & - & - & - \\ Paffu  & THuman2.0  & 1.603 & 1.429 & 0.068 & 1.502 & 1.291 & 0.064 & 1.276 & 1.247 & 0.080 \\ PMLR  & Renderpeople  & 1.431 & 1.321 & 0.063 & 1.225 & 1.206 & 0.055 & - & - & - \\ ICON  & THuman2.0  & 1.096 & 1.085 & 0.046 & 0.969 & 0.987 & 0.041 & 1.249 & 1.368 & 0.076 \\ ICON  & Renderpeople  & 1.070 & 1.013 & 0.059 & 1.202 & 1.170 & 0.055 & - & - & - \\ ECON  & THuman2.0  & 0.942 & 0.933 & **0.035** & 0.904 & 0.984 & **0.033** & 2.120 & 1.807 & 0.074 \\ ECON  & THuman2.0  & 0.926 & **0.917** & 0.037 & - & - & - & - & - & - \\ 
**Ours** & THuman2.0  & **0.911** & **0.917** & 0.042 & **0.763** & **0.763** & 0.035 & **0.814** & **0.862** & **0.055** \\   

Table 1: Quantitative comparison on geometry against other methods. *: obtained from .

Perceptrons (MLPs) are employed for separate predictions of occupancy and color, each with layer sizes of . In the inference phase, we utilize Rembg  for background subtraction in in-the-wild images. The Marching Cubes algorithm  is employed for generating 3D meshes, while off-the-shelf models from ICON  are leveraged for the production of normal maps. This normal map is further processed through a 2-stack hourglass to achieve a size of \(128 128 6\). Besides the front/back normal maps are also used as input into the encoder with the image. The model, implemented in PyTorch Lightning , is trained for 10 epochs with a learning rate of 1e-4 and a batch size of 4, over a span of 2 days on a single NVIDIA GeForce RTX 3090 GPU.

**Datasets.** Our model was trained on the THuman2.0 , featuring 526 high-quality human scans, with 505 designated for training and 21 for evaluation. Testing was primarily conducted on the CAPE  and THuman2.0, with the former divided into "CAPE-FP" and "CAPE-NFP" subsets to examine model generalization on different pose types. Further dataset and implementation details are available in SupMat.

**Metrics.** We employ Chamfer and point-to-surface (P2S) distances, capturing significant geometric errors, to evaluate the accuracy of the reconstructed meshes. We assess the quality of local details and the efficacy of 3D features via the L2 error between normal images of reconstructed and ground-truth meshes from six views. Finally, the quality of texture prediction is measured using Peak Signal-to-Noise Ratio (PSNR), comparing images rendered from both reconstructed and ground-truth surfaces across different views.

### Evaluation

**Evaluation of Geometry.** We compare our **GTA** model with body-agnostic methods like PIFu , PIFuHD , and body-aware methods such as PaMIR , ICON , and ECON . Our evaluation is thorough, involving training and testing these models ourselves and incorporating testing results from  for a comprehensive comparison. As depicted in Tab. 1, **GTA** excels in terms of Chamfer and P2S distances on images with out-of-distribution (OOD) poses and diverse clothing. Notably, **GTA** is the first to reduce the Chamfer distance to less than 0.8 cm on CAPE-FP. On par with ECON for normals on CAPE, **GTA** sets a new state-of-the-art on THuman2.0. Fig. 7(a) visually underlines our model's superior performance on the THuman2.0 benchmarks.

**Quantitative Evaluation of Side-face Reconstruction.** In our novel quantitative evaluation of side-face reconstruction, we spotlight the advantages of 3D features in crafting plausible side-faces and accurate thickness. Using six virtual camera angles, we render normal images of the reconstructed human and compute the normal difference for each face on the THuman2.0  dataset. As shown in Tab. 2, **GTA** surpasses other models in 5 out of 6 views, matching ECON only in the front view, thereby underscoring our model's prowess in capturing inherent 3D structures within an image. For visual results, refer to Fig. 2 and SupMat.

**Evaluation of Texture.** In evaluating texture reconstruction, we compare **GTA** with color-predicting models like PIFu , ARCH , ARCH++ , PHORHUM , and S3F . By rendering textured meshes from multiple angles and calculating the PSNR with respect to ground truth images, we find **GTA** outperforms other models on THuman2.0. As Fig. 5 demonstrates, **GTA** exceeds the state-of-the-art S3F  by 22% in PSNR. Notably, our model provides superior textures on the front side and accurately predicts invisible regions, as seen in Fig. 7(b), highlighting the effectiveness of 3D features.

### Ablation Study

**Ablation Details.** To ensure a fair and unbiased experimental setup, we employed an approach wherein the implicit functions of each ablation experiment were augmented with front and back

    &  &  \\    & Front & Left & & Back & Right & Above & Below \\  PIFu  & 0.053 & 0.143 & 0.046 & 0.109 & 0.067 & 0.066 & 0.081 \\ PAMIR  & 0.051 & 0.124 & 0.068 & 0.077 & 0.051 & 0.054 & 0.071 \\ ICON  & 0.074 & 0.091 & 0.064 & 0.076 & 0.044 & 0.044 & 0.066 \\ ECON  & **0.043** & 0.123 & 0.045 & 0.083 & 0.050 & 0.041 & 0.064 \\ 
**Ours** & 0.048 & **0.069** & **0.044** & **0.061** & **0.035** & **0.040** & **0.050** \\   

Table 2: Normal Evaluation of Different Views on THuman2.0 . These views are obtained by positioning a virtual camera at the front, left, back, right, above, and below the reconstructed human.

Figure 5: Quantitative Evaluation of Texture on THuman2.0 .

normal features and signed distances from the human prior body. For the ablation experiments pertaining to different network architectures, we replaced the corresponding components of our network structure with three identical UNet  and Vision Transformer  decoders solely based on self-attention, respectively. Moreover, for the ablations on 2D features, we generated a 2D feature map of size \(256 256\) with UNet. Additionally, for the ablations on the hybrid prior fusion strategy, we employed a single tri-plane for the spacial query and the prior-enhanced query, respectively.

**A. Different Networks for 3D Feature Decoupling.** We evaluate alternative architectures by modifying our global-correlated encoder and 3D-decoupling decoder, confirming the strength of our proposed transformer. We experiment with UNet , also used in , as a convolution filter representative, and employ three separate UNets to compose the tri-plane. We also test a transformer encoder-decoder with only self-attention and without the refinement module or additional learnable embeddings. Results (see Tab. 3 and Fig. 6) suggest that a purely convolution-based network struggles to decouple 3D features due to limited correlation and receptive field constraints. While the refinement module shows minor geometric improvements, it is crucial for texture reconstruction. Additional learnable embeddings and cross-attention blocks notably enhance geometry results.

**B. 2D Features vs. 3D Features.** We analyze the effectiveness of 3D features by conducting an ablation study using solely 2D feature maps for reconstruction. Utilizing UNet  to generate high-dimensional 2D features, we compare spatial query (SQ), prior-enhanced query (PQ), and our hybrid prior fusion strategy. Results highlight the inferiority of 2D features in producing accurate reconstructions (Tab. 3), emphasizing the importance of 3D features.

**C. Hybrid Prior Fusion Strategy vs. Others.** We evaluate our hybrid prior fusion strategy against individual use of spatial query (SQ) and prior-enhanced query (PQ). Results show that spatial query surpasses prior-enhanced query in both geometry and texture quality due to its provision of localized features for detailed reconstruction. However, combining both methods optimizes geometry performance while preserving texture quality, demonstrating the effectiveness of our hybrid strategy.

### Applications

**Reconstruction of Images in-the-wild.** The **GTA** model demonstrates significant prowess in reconstructing 3D human meshes from unconstrained, real-world images (refer to Fig. 4 and SupMat.), addressing the complexities posed by varied poses and clothing styles. This capability of recon

Table 3: Ablation study of several of our designs on CAPE  dataset.

Figure 6: Texture change of different ablation settings.

Figure 7: Application of our model in animation and virtual try-on.

structing high-fidelity 3D models from in-the-wild images paves the way for extensive applications, notably in virtual and augmented reality.

**Animation and Virtual Try-On.** We present a robust approach for generating novel poses of 3D clothed human meshes, catering to applications in animation and virtual try-on (See Fig. 7). We extend the S3F  model by employing estimated body shape and pose parameters to derive tri-plane features, facilitating realistic deformations. For a single-image clothed human reconstruction, our method excels by only needing the target pose, thus overcoming the limitations of previous deep learning methods. Additionally, our model supports virtual try-on by enabling feature replacement across body parts of different parametric bodies. By selectively interchanging these features, we can simulate changes in clothing on the target image. Our method, therefore, provides a versatile solution for both animation and virtual try-on applications, merging the strengths of previous methods while alleviating their weaknesses. More technical details and results are available in SupMat.

## 5 Conclusion

In conclusion, we present the **GTA** model, a cutting-edge approach for reconstructing 3D clothed human from single images. Our global-correlated 3D-decoupling transformer effectively extracts latent representations from input images and integrates them with learnable embeddings through 3D-decoupling decoders, generating disentangled tri-plane features. Moreover, our hybrid prior fusion strategy integrates the benefits of spatial query's localization capabilities with the prior-enhanced query's ability to incorporate knowledge of the human body prior, ultimately leading to improved geometry and texture reconstruction performance. We demonstrate that our proposed model outperforms state-of-the-art methods in geometric and texture reconstruction, exhibiting resilience against challenging poses and loose clothing, enabling a wide range of applications.

**Acknowledgements.** This work was supported in part by the Fundamental Research Funds for the Central Universities (No. 226-2023-00048) and the Natural Science Foundation of Zhejiang Province (DT23F020008).