# _SparseLLM_: Towards Global Pruning of Pre-trained Language Models

Guangji Bai\({}^{1}\)  Yijiang Li\({}^{2}\)  Chen Ling\({}^{1}\)  Kibaek Kim\({}^{2}\)  Liang Zhao\({}^{1,*}\)

\({}^{1}\) Emory University, Atlanta, GA, USA

\({}^{2}\)Argonne National Laboratory, Lemont, IL, USA

\({}^{*}\)Corresponding Author

{guangji.bai,chen.ling,liang.zhao}@emory.edu

{yijiang.li,kimk}@anl.gov

###### Abstract

The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose _SparseLLM_, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. _SparseLLM_'s approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes, surpassing current state-of-the-art methods. Our source code is publicly available at https://github.com/BaiTheBest/SparseLLM.

## 1 Introduction

Large language models (LLMs) [1; 2] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks [3; 4; 5]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs [6; 7], including pruning, quantization, knowledge distillation, and low-rank factorization. _Pruning_, by introducing _sparsity_, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy.

Model pruning has a long history  and has proven effective in applications related to vision and smaller language models . However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU [10; 11], become impractical for today's LLMs due to their vast size. Recently, several _local pruning_ methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by "stitching together" the individually compressed layers. SparseGPT , an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda , introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations. Despite its efficiency gains, local pruning only aims to minimize the local error for each specific layer under sparsity constraints, resulting in a _suboptimal_ solution for the overall model.

This is because local pruning _over-aligns_ the intermediate layers' activations, leading to suboptimal performance, especially in high-sparsity regimes [11; 14].

To address these challenges and achieve global pruning with low memory consumption, we propose _SparseLLM_ that decomposes the global pruning objective into multiple subproblems, each of which can be solved with low resources and coordinate to achieve the global pruning objective. More specifically, we first formulate LLMs as a composite function where the output of one module is the input of the next. Based on this formulation, we reformulate the global pruning goal into an equivalent form with auxiliary variables that facilitate its decomposition and coordination of the subproblems. Then we propose an alternating optimization algorithm to efficiently solve the subproblems, achieving computational resource efficiency and global optimality, due to the close-form solution of each subproblem. Empirically, we find that _SparseLLM_ can consistently improve the performance of local pruning methods, particularly in high sparsity regimes (\(>60\%\)), where the perplexity can be significantly decreased by up to around 80% as compared to the state-of-the-art methods.

Furthermore, our SparseLLM framework can be readily applicable to enhance the performance of most existing local pruning solvers, such as SparseGPT and Wanda, with marginal additional computational overhead. This adaptability ensures that our framework can be seamlessly integrated into a wide range of LLMs and pruning methods, making it a versatile tool and useful baseline for future research exploiting the sparsity of LLMs.

## 2 Related work

_Pruning_, a pivotal concept in machine learning that introduces sparsity into neural networks, dates back to the 1980s . It gained renewed attention in the late 2010s, especially for deep neural networks, under the banner of reducing inference costs . LLM pruning techniques can broadly be categorized into _structured_ and _unstructured_ prunings.

Unstructured pruning [16; 17] looks at simplifying the complexity of LLMs by removing certain parameters _regardless_ of the model's inherent structure. This approach typically involves setting a threshold to nullify parameters below it, leading to a model with a non-uniform sparse structure. SparseGPT , an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves up to 60% parameter reduction with minimal performance loss. A novel pruning criterion is introduced in Wanda , which evaluates weights by considering both magnitude and related input activations. This approach is beneficial in linear layers of LLMs, helping to identify and remove less significant weights. Tuli and Jha  proposed DynaTran, a dynamic inference scheme for pruning activations at runtime, supported by a specially designed ASIC architecture, AccelTran, to enhance transformer inference throughput.

On the other hand, structured pruning involves the selective removal of groups of weights, where "group" might mean blocks of weights, filters, attention heads, or other structures conducive to hardware acceleration. Ma et al.  introduced the LLM-Pruner, a framework designed for structured pruning of LLMs, which utilizes a combination of first-order data and Hessian information for effective importance estimation. This aids in identifying crucial groups for pruning. Li et al.  proposed LoSparse, a novel approach combining low-rank and sparse matrix approximations to balance pruning and expressive power. Tao et al.  extended this concept to pruning hidden dimensions in LLMs, including embedding layers and attention heads. ZipLM , a structured pruning method for LLMs, is proposed to optimize for compression and accuracy while considering specific hardware constraints. More recently, Xia et al introduced _LLM-shearing_, a structured pruning method that scales down LLaMA models by selectively pruning layers, heads, and dimensions.

Figure 1: _SparseLLM_ decomposes the global pruning of LLMs into manageable subproblems by leveraging the chain of modules and auxiliary variables while maintaining dependencies.

This approach, combined with dynamic data batching, reduces pre-training compute costs while maintaining competitive performance, outperforming similar open-source models on key tasks.

Our work falls in the category of unstructured pruning of LLMs, where existing methods such as SparseGPT and Wanda only consider an _entirely local_ pruning algorithm and suffer from _suboptimal_ performance. We discuss the limitations and challenges of entirely local pruning in Sec. 3.

## 3 Background and notation

### Global pruning

Given a pre-trained neural network \(f\) with parameter \(\) and inputs \(\), global pruning aims to find a global sparsity mask \(\) and possibly updated weights \(}\) to minimize the _global loss_\(\) between the final outputs of the uncompressed and compressed model:

\[_{,}}\ \ f(; }),f(;),\] (1)

where \(\) denotes the _element-wise_ multiplication. In addition to NP-hardness , however, a critical challenge in solving Eq. 1 is the huge memory cost, as one needs to store the entire model in a single GPU, rendering this method impractical for modern billion-scale LLMs.

### Local pruning

Local pruning circumvents the memory issue mentioned above by dividing the full model compression into subproblems for each layer and constructing a _local loss_ to measure the \(_{2}\)-error between the outputs of the uncompressed and compressed layers. Hence, the local pruning can be formulated by

\[_{_{},}_{}}_{} _{}-(_{}}_{}) _{}_{2}^{2}.\] (2)

Although smaller than the global pruning, the local pruning still needs to optimize both the mask \(_{}\) and the remaining weights \(}_{}\) and thus remains NP-hard. Therefore, exactly solving it for larger layers is unrealistic, leading all existing methods to resort to approximations.

Mask selection & weight reconstruction.A particularly popular approach is to separate the problem into _mask selection_ and _weight reconstruction_[25; 26]. Concretely, this means first choosing a pruning mask \(\) according to some salient criterion, like the weight magnitude , and then optimizing the remaining unpruned weights while keeping the mask unchanged. Importantly, once the mask is fixed, Eq. 2 turns into a _linear regression_ problem that can be easily optimized.

Existing solvers.Early work  applied iterated linear regression to small networks. Recently, the AdaPrune approach  has shown good results for this problem on modern models via magnitude-based weight selection, followed by applying SGD steps to reconstruct the remaining weights. Follow-up works demonstrate that pruning accuracy can be further improved by removing the strict separation between mask selection and weight reconstruction. More recently,  developed SparseGPT, an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieving up to 60% parameter reduction with minimal performance loss.  introduced a novel pruning criterion in Wanda, which evaluates weights by considering both magnitude and related input activations.

### What is wrong with local pruning?

As shown in Eq. 2, local pruning focuses on minimizing the error for each specific layer \(\) subject to sparsity constraints. This results in a suboptimal solution with respect to the global pruning problem. While the primary goal of pruning is to ensure that the input and output of the pruned model align closely with those of the original models, the local pruning overly constrains the activations of all the intermediate layers between the two models, leading to performance degradation.

## 4 _SparseLLM_: Towards global pruning for LLMs

We present our proposed method SparseLLM that can address the drawbacks of existing pruning methods by achieving a global pruning with low memory consumption. SparseLLM decomposes the global pruning objective into many subproblems, each of which can be solved using low resources and can coordinate each other toward the global pruning objective. An overview of SparseLLM on the OPT and LlaMA configurations are shown in Figure 2.

### Motivation

The development of SparseLLM is motivated by the observation: LLMs can be formulated as a composite function such that the output of one module is the input of the next. This allows us to reformulate the global pruning goal into its equivalent form with auxiliary variables that enable the decomposition into multiple subproblems, as detailed in Sec. 4.2. Then we develop a resource-efficient algorithm that achieves the alternating optimization of the subproblems with global optimality, thanks to the close-form solution of each subproblem, as illustrated in Sec. 4.3.

### A unified formulation of pruning

In this section, we present the reformulation of the global pruning problem into an equivalent one by introducing auxiliary variables. This reformulation provides a more flexible form and enables the decomposition of the problem into many manageable subproblems.

The key idea behind our formulation is to decouple the densely parametric parts (linear layers) from non-parametric parts (activation function, self-attention, layer norm, etc) using a splitting technique. Rather than feeding the output of the dense linear layer \(_{}\) directly into the non-parametric and potentially nonlinear layer \(_{}\), we store the output of layer \(\) in a new variable \(_{}=_{}_{-1}\)1. We also represent the output of the non-parametric layer as a vector of activations \(_{}=_{}(_{})\). We then solve the following problem:

\[_{\{}_{}\},\{_{}\}, \{_{}\},\{_{}\}}(_{L}, ),\] (3) \[_{}=(_{} }_{})_{-1},\ \ [L],\] \[_{}=_{}(_{}),\ \ ,\] \[_{},_{}=_{}^{pre}, _{}^{pre},\ \ [L-1],\]

where \(L\) represents the total number of dense (linear) layers and \([L]=\{1,2,,L\}\). \([L-1]\) denotes the complement set of \(\). We use \(_{}^{pre}\), \(_{}^{pre}\) to denote the corresponding intermediate variables' values of the original dense (i.e., _without_ pruning) pre-trained model. \(y\) denotes the ground-truth final output of the dense pre-trained model.

In our proposed formulation above, its unified nature lies in the interpretation and application of the set \(\), which denotes the indices of layers subject to the pruning process. Intuitively, \(\) measures how "global" the pruning is. The bigger the set of \(\) is, the more layers are connected via the second constraint, and the pruning is more towards the global extreme, and vice versa. The generality and versatility of our formulation is illustrated in the following remark:

**Remark 4.1** (Generality and flexibility of Eq. 3).: _Given an LLM formulated as a composite function with dense layers \(l\{1,2,,L-1\}\), where \(L\) is the total number of dense layers and \(\) denotes the set of layers subject to the pruning process. Our formulation can seamlessly treat both global and local pruning as special cases under certain conditions. Specifically:_

* _When_ \(=\{1,2,,L-1\}\)_, solving our pruning formulation is equivalent to global pruning, accounting for inter-layer dependencies across the entire network._
* _When_ \(=\)_, the formulation simplifies to local pruning, considering each layer independently (the last constraint dominates and "cuts" all layer dependencies with pre-trained values.)_

The ability to shift between these two extremes, and potentially any intermediate configurations, demonstrates the flexibility and comprehensiveness of our formulation. By adjusting \(\), one can seamlessly transition from a global perspective to a local perspective. This flexibility not only caters to a wide range of pruning strategies but also provides a unified framework to compare and contrast the effectiveness of different pruning methods under a consistent mathematical lens.

### Algorithm design

In this section, we introduce the algorithm design of _SparseLLM_, which alternatively optimizes the subproblems associated with the corresponding variables. This approach is resource-efficient and achieves global optimality, attributed to the closed-form solutions that each subproblem yields.

The key idea of our algorithm lies behind the flexibility of \(\) in our Eq. 3, as we want to find a better trade-off between completely global (memory bottleneck) and completely local (suboptimal performance) pruning. Naively applying SparseLLM to prune all layers globally is impractical. On the other hand, recent work shows that the feed-forward network (FFN) module in each decoder layer accounts for more than _two-thirds_ of the total parameters in an LLM . Therefore, our SparseLLM prioritizes the global pruning of the FFN module, while still adhering to a local pruning strategy for the multi-head attention (MHA) module (see Figure 2). This strategy strikes a balance between the computational feasibility of pruning large-scale models and the effectiveness of the pruning process, adhering to the limitations and practices of state-of-the-art LLM pruning frameworks.

Formally speaking, rather than trying to solve Eq. 3 directly, we first relax the constraints by adding an \(_{2}\)-penalty function to the objective and attack the unconstrained problem:

\[(_{L},)+_{[L]}\| _{}-(_{}}_{}) _{-1}\|_{2}^{2}+_{_{}}\|_{}-_{}(_{})\|_{2}^{2},\] (4)

where \(\), \(\) are hyperparameters for controlling the weight of each constraint. \(_{}\) denotes the set of indexes for the linear layers in the FFN module of each decoder layer, i.e., linear layers from the same FFN module are pruned globally. For simplicity, the superscript "pre" of \(_{}\) and \(_{}\) in the third constraint in Eq. 3 is omitted here, i.e., for \(_{}\) the \(_{}\) and \(_{}\) are fixed and equal to the pre-trained model's intermediate value in the second term of Eq. 4. In the following subsections, we illustrate how we approach the pruning of FFN and MHA modules, respectively.

#### 4.3.1 _SparseLLM_ on OPT models

For each decoder layer in a pre-trained LLM, our Eq. 4 instantly simplifies to globally pruning the corresponding FFN module within that decoder layer as:

\[\|_{+1}^{pre}-(_{+1}}_{+1})_{}\|_{2}^{2}+\|_{}- _{}(_{})\|_{2}^{2}+\|_{}-(_{}}_{})_{-1}^{pre}\|_{2}^{2},\] (5)

where layers \(\) and \(+1\) correspond to the up-projection and down-projection linear layers.

In this work, we consider the _alternating_ method to optimize our Eq. 5, i.e., optimize each variable while keeping the rest fixed. The careful and elaborate design of our Eq. 5 allows us to derive a _closed-form_ solution to every subproblem as shown below.

**Pruning weight.** First consider optimizing Eq. 5 with respect to \(_{}\) and \(}_{}\). For each linear layer \(\) in a FFN module, the optimal solution minimizes \(\|_{}-(_{}}_{}) _{-1}\|_{2}^{2}\). To solve it, the first step is to decompose \(_{}\) to \(_{}_{-1}\), where \(_{}=_{}_{-1}^{}\) (\(\) denotes the pseudo-inverse.) Plug decomposed \(_{}\) back in original loss and we get \(\|}_{}_{-1}-(_{} }_{})_{-1}\|_{2}^{2}\), which aligns with the pruning objective of Eq. 2 and can be analytically solved by existing pruning solver e.g., SparseGPT. The superscript of "\(pre\)" for \(_{-1}\) is omitted in this section for simpler notation.

Figure 2: Illustration of _SparseLLM_ on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., \(\)) allow _SparseLLM_ to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are _analytically_ solvable and enjoy fast convergence.

Updating activation.Minimization for \(_{}\) is a simple least-squares problem similar to weight pruning. However, in this case, the matrix \(_{-1}\) appears in two penalty terms in Eq. 5, so we must minimize \(\|_{+1}^{pre}-(_{+1}} _{+1})_{}\|_{2}^{2}+\|_{}-_{}(_ {})\|_{2}^{2}\) for \(_{}\), holding all other variables fixed. By following a very similar idea to Ridge regression, the new value of \(_{}\) is given by:

\[(_{+1}^{}_{+1}+)^{- 1}(_{+1}^{}_{+1}^{pre}+ (_{})),\] (6)

where \(_{}\) denotes the updated weight matrix after pruning, i.e., \(_{}_{}}_{}\).

Updating output.The update for \(_{}\) requires minimizing the following loss:

\[\|_{}-(_{})\|_{2}^{2}+\| _{}-(_{}}_{}) _{-1}^{pre}\|_{2}^{2}.\] (7)

This problem is non-convex and non-quadratic (because of the non-linear function ReLU). Fortunately, because the ReLU function works entry-wise on its argument, the entries in \(_{}\) are de-coupled. Solving Eq. 7 is particularly easy for the case of ReLU, as it can be solved in closed form followed by a simple if-then logic. Specifically, one only needs to compute two solutions of a quadratic equation:

\[_{}^{(1)}=(_{}}_{}) _{-1}^{pre},_{}^{(2)}=(+)^{-1} _{}+_{}^{(1)},\] (8)

where the first solution corresponds to those entries of \(_{}\) that are negative (reduced to zero by ReLU), and the second solution corresponds to those entries of \(_{}\) that are non-negative.

#### 4.3.2 _SparseLLM_ on LlaMA models

In this section, we introduce how _SparseLLM_ decomposes global pruning into subproblems and solves them iteratively on LlaMA model families. The model architecture of LlaMA can be found in Figure 2. Overall, _SparseLLM_ operates similarly on both LlaMA and OPT models, with the main difference being that LlaMA includes an additional dense linear layer, known as the gate projection layer, and uses the SiLU activation function instead of ReLU.

Pruning weight.In this part, _SparseLLM_ functions almost identically to its operation on OPTs.

Updating activation \(_{}\).Similarly, for updating \(_{}\), _SparseLLM_ works nearly the same as on OPT. The minimization for \(_{}\) is a simple least-squares problem, akin to weight pruning. However, in this case, the matrix \(_{-1}\) appears in two penalty terms in Eq. 5, necessitating the minimization of:

\[\|_{+1}^{pre}-(_{+1} }_{+1})_{}\|_{2}^{2}+\|_{}-(_{})_{}\|_{2}^{2},\] (9)

for \(_{}\), with all other variables held fixed. Following a concept similar to Ridge regression, the updated value of \(_{}\) is:

\[_{+1}^{}_{+1}+^{-1}_{+1}^{}_{+1}^{ pre}+(_{})_{} ,\] (10)

where \(_{}\) denotes the updated weight matrix after pruning, i.e., \(_{}_{}}_{}\).

Updating output \(_{}\).Updating \(_{}\) is somewhat simpler in LlaMA since the activation function applies over the gate projection layer. The update requires minimizing the loss:

\[\|_{}-(_{})_{}\|_{ 2}^{2}+\|_{}-(_{}}_{ })_{-1}^{pre}\|_{2}^{2}.\] (11)

This problem is quadratic when solving for \(_{}\) with other variables fixed. Through mathematical manipulations, the analytical solution for \(_{}\) is found by solving a quadratic equation:

\[_{}^{}=_{}}_{ })_{-1}^{pre}+(_{})_{}} {(_{})^{2}+},\] (12)

where the division is element-wise and \(\) denotes the all-one matrix.

Updating gate projection output \(_{}\).Updating \(_{}\) involves minimizing:

\[\|_{}-(_{})_{}\|_{ 2}^{2}+\|_{}-(_{s}}_{s}) _{-1}^{pre}\|_{2}^{2},\] (13)

where \(_{s}\) and \(}_{s}\) denote the mask and layer weights for the gate projection layer. This problem is non-convex and non-quadratic due to the non-linear SiLU function. However, since SiLU operates entry-wise, the entries in \(_{}\) are decoupled. Despite LlaMA lacking a simple closed-form solution as in OPT (which uses ReLU), the problem can still be solved quickly and analytically using a lookup table of pre-computed solutions, since each element in \(_{}\) depends on only three variables.

**Remark 4.2** (Global convergence of SparseLLM).: _Consider the objective function given by Eq. 5, under the condition that the activation function \(\) is ReLU. Notice that (1) the objective function is convex with respect to each variable when all others are fixed, and (2) given that closed-form solutions exist for the subproblems in the alternating optimization scheme, the proposed algorithm resembles multiblock ADMM which has been shown to converge to in many applications._

#### 4.3.3 Pruning of MHAs

SparseLLM also prunes other linear layers besides those in FFNs. By following Eq. 4, for each linear layer out of FFN modules, the pruning objective simplifies to \(\|_{+1}^{pre}-(_{+1} }_{+1})_{}^{pre}\|_{2}^{2}\), which is equivalent (with some simple math) to that of completely local pruning as shown in Eq. 2. Existing LLM pruning solvers such as SparseGPT and Wanda are applicable here.

### Time complexity analyses

The proposed _SparseLLM_ consists of three main steps, with the overall time complexity being the sum of the complexities of these steps. In the weights pruning step, the complexity is dominated by the pseudo-inverse computation of matrix \(_{}\) (dimensions \(n h\)), which is \(O(nh^{2})\). Using SparseGPT as the solver, the exact pruning step has a complexity of \(O(h^{3})\). The second step, updating activations, involves matrix inversion of the weight matrix \(_{}\) (size \(h h\)) with a complexity of \(O(h^{3})\). The third step, updating outputs, has a lower complexity. Thus, the overall algorithm complexity is bounded by \(O(h^{3})\), therefore making our method's per-epoch time complexity comparable to SparseGPT.

## 5 Experiments

**Experiments setup.** We implemented _SparseLLM_ in PyTorch  and use the HuggingFace Transformers library  for handling models and datasets. All pruning experiments are conducted on NVIDIA A100 GPUs. For calibration data, we follow  and use 128 2048-token segments, randomly chosen from the first shard of the C4  dataset. This represents generic text data crawled from the internet and ensures our experiments are zero-shot as no task-specific data is seen during pruning. _We followed existing work [12; 13] and pruned all linear layers (in FFN and MHA) to the target sparsity._

**Models, datasets & evaluation.** We consider the OPT model family  and LlaMA-2 model family  in our experiments as well as the most recent LlaMA-3 model. We show results on different sizes of models to provide a broader picture for the performances of _SparseLLM_. In terms of metrics, we mainly focus on perplexity, which is known to be a challenging and stable metric that is well-suited for evaluating the accuracy of compression methods [34; 35]. We consider the test sets of raw-WikiText2  (WT2) and PTB  as well as a subset of the C4 validation data, all popular benchmarks in LLM compression literature [34; 38; 12; 13]. For additional interpretability, we also provide zero-shot accuracy results following the same setup of , which is based on the popular EleutherAI-eval harness .

**Comparison methods.** We compare against three baselines, magnitude pruning  applied locally, and two other state-of-the-art local pruning methods, SparseGPT  and Wanda .

### Results and analyses

**Pruning vs. model sizes.** We begin by exploring the pruning capabilities of _SparseLLM_ across various model sizes in comparison to baseline methods. For each model, we consider unstructured sparsity ranging from 70% to 90% with a 10% increment, as well as a 3:4 semi-structured sparsity. The 3:4 semi-structured sparsity is inspired by our preliminary results that suggest good performance _SparseLLM_ at high sparsity regimes. However, note that two of our baselines, Magnitude and Wanda, are unable to be configured to this sparsity out-of-box. We conduct a sensitivity study on the calibration sample sizes (see Appendix A.3) and use calibration sample sizes between 32 and 64 for all experiments. Moreover, we prune the first 50% of the Transformer decoder layers in each model to achieve a balance between the computation resources and the performances. Detailed results can be found in Table 1 and Table 2 as well as Table 8 in Appendix A.5. Note that in Table 2 for LlaMA-3 model, we only compare SparseGPT to the proposed _SparseLLM_. The perplexity results of the dense models are reported next to the names of the models.

From the tables, it shows a general trend of increasing perplexity with increasing sparsity. Moreover, we observe a trend of decreasing perplexity for SparseGPT and _SparseLLM_ at the same sparsity

[MISSING_PAGE_EMPTY:8]

with increasing model sizes. However, such a trend is not obvious for Magnitude and Wanda. We also observe that SparseGPT and _SparseLLM_ consistently outperform Magnitude and Wanda by a significant margin. For smaller sparsity, _SparseLLM_ achieves comparable perplexity to SparseGPT. As we increase the sparsity, _SparseLLM_ starts to demonstrate noticeable improvements over SparseGPT. In numerous instances for the OPT model family, _SparseLLM_ achieves perplexity reductions of more than 50% compared to SparseGPT. We also see that performance improvements from _SparseLLM_ over SparseGPT are more significant for the OPT model family than the LlaMA-2 model family.

We provide additional set of perplexity results for a 2:4 semi-structured sparsity for a few OPT models in Table 3. We see that _SparseLLM_ and SparseGPT generally outperform Magnitude and Wanda while _SparseLLM_ has comparable if not slightly better performances compared to SparseGPT with the 2:4 semi-structured sparsity. Note that a 2:4 semi-structure sparsity is considered to be in low sparsity regime.

**Zero-shot experiments.** To further conclude the evaluations and discussions, we show results for several zero-shot tasks in Table 4 and Table 5 as well as Table 9 in Appendix A.5, comparing SparseGPT and _SparseLLM_. These evaluations are known to be relatively noisy , but more interpretable. We also report the results for zero-shot tasks from the dense models in the "Dense" row. We see that the accuracy of both methods decreases with increasing sparsity, which is expected, as more parameters are pruned. A similar trend of increasing accuracy with increasing model size is observed too. Across all the tasks, OBQA and ARC-c remain the most challenging ones as the accuracy for both methods is 30% or below 30% while both methods perform well for BoolQ, RTE, WinoGrande, and ARC-e. In general, _SparseLLM_ is able to achieve higher accuracy in the majority of tasks across the models of different sizes in both OPT and LlaMA-2 model families.

**Training loss vs. epochs in _SparseLLM_.** Figure 3 illustrates the change in training loss over epochs for _SparseLLM_, with the training loss plotted on a scale of \(10^{3}\) for clarity. We observe that the training loss decreases rapidly during the initial epochs, highlighting the efficiency of _SparseLLM_ in achieving effective global pruning within a short period. This rapid convergence is largely due to the closed-form solutions employed by _SparseLLM_ for various subproblems, which streamline the pruning process and ensure optimal layer-wise pruning without extensive iterative computations. These analytical solutions enable _SparseLLM_ to perform precise pruning operations quickly, making it a powerful tool for optimizing large-scale models like LlaMA, significantly reducing model size while maintaining high accuracy.

## 6 Conclusion

Our work presents _SparseLLM_, a cutting-edge framework poised to redefine the compression of LLMs through sparsity. By adeptly circumventing the scalability issues of global pruning and optimizing the local suboptimality of existing methods, _SparseLLM_ stands as a significant advancement in the field. Our empirical results affirm its efficacy, particularly in high-sparsity environments. It achieves a notable reduction in perplexity, thereby setting a new precedent for model compression. The versatility and minimal computational overhead of _SparseLLM_ complement its integration

Figure 3: **Fast convergence of _SparseLLM_. Training loss per epoch for pruning layer 3 of OPT-125m at 80% sparsity (Left) and layer 6 of LlaMA-2 13b at 70% sparsity (Right).**

    &  &  &  &  \\  Dataset & WT2 & PTB & C4 & WT2 & PTB & C4 & WT2 & PTB & C4 & WT2 & PTB & C4 \\  Magnitude & 96.68 & 133.92 & 48.08 & 272.34 & 308.55 & 267.70 & 64.11 & 92.23 & 82.67 & 67.07 & 110.77 & 52.61 \\ Wanda & 15.63 & 24.04 & 18.23 & 13.66 & 21.67 & 16.10 & 11.86 & 18.54 & 14.77 & 10.33 & 15.35 & 12.54 \\ SparseGPT & 15.11 & 23.71 & 17.88 & 12.62 & 19.28 & 15.12 & 11.30 & 16.90 & 13.51 & 10.20 & 15.14 & 12.48 \\ _SparseGPT_ & 14.97 & 23.40 & 17.67 & 12.62 & 19.28 & 15.12 & 11.07 & 16.73 & 13.42 & 10.20 & 15.14 & 12.41 \\   

Table 3: Perplexity of 2:4 sparsity; the lower the perplexity, the better.

with current pruning technologies, underscoring its potential as a universal tool for enhancing the performance and accessibility of LLMs.