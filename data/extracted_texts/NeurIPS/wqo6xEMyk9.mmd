# ProG: A Graph Prompt Learning Benchmark

Chenyi Zi\({}^{1}\)

Haihong Zhao\({}^{1}\)

Xiangguo Sun\({}^{2,}\)

Yiqing Lin\({}^{3}\)

Hong Cheng\({}^{2}\)

Jia Li\({}^{1}\)

\({}^{1}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\)Department of Systems Engineering and Engineering Management,

and Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong

\({}^{3}\)Tsinghua University

corresponding author: xiangguosun@cuhk.edu.hk

The first two authors contributed equally to this work. Listing order is random.

###### Abstract

Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional 'Pre-train & Fine-tune' paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integrates **SIX** pre-training methods and **FIVE** state-of-the-art graph prompt techniques, evaluated across **FIFTEEN** diverse datasets to assess performance, flexibility, and efficiency. We also present 'ProG', an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.

## 1 Introduction

Recently, artificial general intelligence (AGI) on graphs has emerged as a new trend in various applications like drug design [59; 38], protein prediction , social analysis [49; 64; 52], etc. To achieve this vision, one key question is how to learn useful knowledge from non-linear data (like graphs) and how to apply it to various downstream tasks or domains. Classical approaches mostly leverage the 'Pre-train & Fine-tune' paradigm, which first designs some pretext with easily access data as the pre-training task for the graph neural network model, and then adjusts partial or entire model parameters to fit new downstream tasks. Although much progress has been achieved, they are still not effective and efficient enough. For example, adjusting the pre-trained model will become very time-consuming with the increase in model complexity [25; 24; 48; 23]. In addition, a natural gap between these pretexts and downstream tasks makes the task transferring very hard, and sometimes may even cause negative transfer [57; 30; 29]. These problems are particularly serious in few-shot settings.

To further alleviate the problems above, Graph Prompt Learning [50; 28] has attracted more attention recently. As shown in Figure 1, graph prompts seek to manipulate downstream data by inserting an additional small prompt graph and then reformulating the downstream task to the pre-training task without changing the pre-trained Graph Neural Network (GNN) model. Since a graph prompt is usually lightweight, tuning this prompt is more efficient than the large backbone model. Comparedwith graph 'pre-training & fine-tuning', which can be seen as a model-level retraining skill, graph 'pre-training & prompting' is more customized for data-level operations, which means its potential applications might go beyond task transferring in the graph intelligence area like data quality evaluation, multi-domain alignment, learnable data augmentation, etc. Recently, many graph prompt models have been proposed and presented significant performance in graph learning areas [47; 46; 33; 8; 66; 5], indicating their huge potential towards more general graph intelligence. Despite the enthusiasm around graph prompts in research communities, three significant challenges impede further exploration:

* **How can we unify diverse graph prompt models given their varying methodologies?** The fragmented landscape of graph prompts hinders systematic research advancement. A unified framework is essential for integrating these diverse methods, creating a cohesive taxonomy to better understand current research and support future studies.
* **How do we evaluate the quality of graph prompts?** Assessing the efficiency, power, and flexibility of graph prompts is crucial for understanding their impact and limitations. Currently, there is no standardized benchmark for fair and comprehensive comparison, as existing works have inconsistent experimental setups, varying tasks, metrics, pre-training strategies, and data-splitting techniques, which obstruct a comprehensive understanding of the current state of research.
* **How can we make graph prompt approaches more user-friendly for practical comparison and selection?** Despite numerous proposed methods, the lack of an easy-to-use toolkit for creating graph prompts limits their potential applications. The implementation details of existing works vary significantly in programming frameworks, tricks, and running requirements. Developing a standardized library for various graph prompt learning approaches is urgently needed to facilitate broader exploration and application.

In this work, we wish to push forward graph prompt research to be more standardized, provide suggestions for practical choices, and uncover both their advantages and limitations with a comprehensive evaluation. To the best of our knowledge, this is the first benchmark for graph prompt learning. **To solve the first challenge**, our benchmark treats existing graph prompts as three basic components: prompt tokens, token structure, and insert patterns. According to their detailed implementation, we categorize the current graph prompts into two branches: graph prompt as an additional graph, and graph prompt as tokens. This taxonomy makes our benchmark approach compatible with nearly all existing graph prompting approaches. **To solve the second challenge**, our benchmark encompasses 6 different classical pre-training methods covering node-level, edge-level, and graph-level tasks, and 5 most representative and state-of-the-art graph prompt methods. Additionally, we include 15 diverse datasets in our benchmark with different scales, covering both homophilic and heterophilic graphs from various domains. We systematically investigate the effectiveness, flexibility, and efficiency of graph prompts under few-shot settings. Compared with traditional supervised methods and 'pre-training & fine-tuning' methods, graph prompting approaches present significant improvements. We even observe some negative transfer cases caused by 'pre-training & fine-tuning' methods but effectively reversed by graph prompting methods. **To solve the third challenge**, we develop a unified library for creating various graph prompts. In this way, users can evaluate their models or datasets with less effort. The contributions are summarized as follows:

* We propose the first comprehensive benchmark for graph prompt learning. The benchmark integrates 6 most used pre-training methods and 5 state-of-the-art graph prompting methods with

Figure 1: An overview of our benchmark.

15 diverse graph datasets. We systematically evaluate the effectiveness, flexibility, and efficiency of current graph prompt models to assist future research efforts. (Detailed in Section 5)
* We offer 'ProG'2, an easy-to-use open-source library to conduct various graph prompt models (Section 3). We can get rid of the distraction from different implementation skills and reveal the potential benefits/shortages of these graph prompts in a more objective and fair setting. * We propose a unified view for current graph prompt methods (Section 2). We decompose graph prompts into three components: prompt tokens, token structure, and insert patterns. With this framework, we can easily group most of the existing work into two categories (prompt as graph, and prompt as tokens). This view serves as the infrastructure of our developed library and makes our benchmark applicable to most of the existing graph prompt works.

## 2 Preliminaries

**Understand Graph Prompt Nature in A Unified View.** Graph prompt learning aims to learn suitable transformation operations for graphs or graph representations to reformulate the downstream task to the pre-training task. Let \(t()\) be any graph-level transformation (e.g., "modifying node features", "augmenting original graphs", etc.), and \(^{*}()\) be a frozen pre-trained graph model. For any graph \(\) with adjacency matrix \(\{0,1\}^{N N}\) and node feature matrix \(^{N d}\) where \(N\) denotes the node number and \(d\) is feature number. It has been proven [47; 8] that we can always learn an appropriate prompt module \(\), making them can be formulated as the following formula:

\[^{*}((,,,_{inner}, _{cross}))=^{*}(t(,))+O_{}\] (1)

Here \(^{K d}\) is the learnable representations of \(K\) prompt tokens, \(_{inner}\{0,1\}^{K K}\) indicate token structures and \(_{cross}\{0,1\}^{K N}\) is the inserting patterns, which indicates how each prompt token connects to each node in the original graph. Prompt module \(\) takes the input graph and then outputs an integrated graph with a graph prompt. This equation indicates that we can learn an appropriate prompt module \(\) applied to the original graph to imitate any graph manipulation. \(O_{}\) denotes the _error bound_ between the manipulated graph and the prompting graph w.r.t. their representations from the pre-trained graph model, which can be seen as a measurement of graph prompt flexibility.

**Task Definition (Few-shot Graph Prompt Learning).** Given a graph \(\), a frozen pre-trained graph model \(^{*}\), and labels \(\), the loss function is to optimize the task loss \(_{Task}\) as follows:

\[_{}=_{Task}(^{*}((,,,_{inner},_{cross}))),)\]

During the few-shot training phase, the labeled samples are very scarce. Specifically, given a set of classes \(C\), each class \(C_{i}\) has only \(k\) labeled examples. Here, \(k\) is a small number (e.g., \(k 10\)), and the total number of labeled samples is given by \(k|C|=||\), where \(|C|\) is the number of classes. This setup is commonly referred to as \(k\)-shot classification . In this paper, our evaluation includes both node and graph classification tasks.

**Evaluation Questions.** We are particular interested in the **effectiveness**, **flexibility**, and **efficiency** of graph prompt learning methods. Specifically, **in effectiveness**, we wish to figure out how effective are different graph prompt methods on various tasks (Section 5.1); and how well graph prompting methods overcome the negative transferring caused by pre-training strategies (Section 5.2). **In flexibility**, we wish to uncover how powerful different graph prompt methods are to simulate data operations (Section 5.3). **In efficiency**, we wish to know how efficient are these graph prompt methods in terms of time and space cost (Section 5.4).

## 3 Benchmark Methods

In general, graph prompt learning aims to transfer pre-trained knowledge with the expectation of achieving better positive transfer effects in downstream tasks compared to classical 'Pre-train & Fine-tune' methods. To conduct a comprehensive comparison, we first consider using a supervised method as the baseline for judging positive transfer (For example, if a 'pre-training and fine-tuning' approach can not beat supervised results, one negative transfer case is observed). We introduce various pre-training methods to generate pre-trained knowledge, upon which we perform fine-tuningas the comparison for graph prompts. Finally, we apply current popular graph prompt learning methods with various pre-training methods to investigate their knowledge transferability. The details of these baselines are as follows (Further introduction can be referred to Appendix A):

* **Supervised Method:** In this paper, we utilize Graph Convolutional Network (GCN)  as the baseline, which is one of the classical and effective graph models based on graph convolutional operations [50; 67; 32; 69] and also the backbone for both 'Pre-train & Fine-tune' and graph prompt learning methods. We also explore more kinds of graph models like GraphSAGE , GAT , and Graph Transformer , and put the results in Appendix E.
* **'Pre-train & Fine-tune' Methods:** We select 6 mostly used pre-training methods covering node-level, edge-level, and graph-level strategies. For **node-level**, we consider **DGI** and **GraphMAE**, where DGI maximizes the mutual information between node and graph representations for informative embeddings and GraphMAE learns deep node representations by reconstructing masked features. For **edge-level**, we introduce **EdgePreGPT** and **EdgePreG-prompt**, where EdgePreGPT calculates the dot product as the link probability of node pairs and EdgePreGprompt samples triplets from label-free graphs to increase the similarity between the contextual subgraphs of linked pairs while decreasing the similarity of unlinked pairs. For **graph-level**, we involve **GCL**, **SimGRACE**, where GCL maximizes agreement between different graph augmentations to leverage structural information and SimGRACE tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph.
* **Graph Prompt Learning Methods:** With our proposed unified view, current popular graph prompt learning methods can be easily classified into two types, 'Prompt as graph' and 'Prompt as token' types. Specifically, the **'Prompt as graph'** type means a graph prompt has multiple prompt tokens with inner structure and insert patterns, and we introduce **All-in-one**, which aims to learn a set of insert patterns for original graphs via learnable graph prompt modules. For the **'Prompt as token'** type, we consider involving **GPPT**, **Gprompt**, **GPF** and **GPF-plus**. Concretely, GPPT defines graph prompts as additional tokens that contain task tokens and structure tokens that align downstream node tasks and link prediction pretext. GPprompt focuses on inserting the prompt vector into the graph pooling by element-wise multiplication. GPF and GPF-plus mainly add soft prompts to all node features of the input graph.

**ProG: A Unified Library for Graph Prompts.** We offer our developed graph prompt library ProG as shown in Figure 2. In detail, the library first offers a **Model Backbone** module, including many widely-used graph models, which can be used for supervised learning independently. Then, it designs a **Pre-training** module involving many pre-training methods from various level types, which can pre-train the graph model initialized from the model backbone to preserve pre-trained knowledge and further fine-tune the pre-trained graph model. Finally, it seamlessly incorporates different graph prompt learning methods together into a unified **Prompting** module, which can freeze and execute prompt tuning for the pre-trained graph model from the pre-training module. For a fair comparison, it defines an **Evaluation** module, including comprehensive metrics, batch evaluator, and dynamic dispatcher, to ensure the same settings across different selected methods. Many essential operations are fused into the **Utils** module for repeated use, and all the data-related operations are included in the **Data** module for user-friendly operations to use the library. Additionally, the **Tutorial** module introduces the key functions that ProG offers.

## 4 Benchmark Settings

### Datasets

To figure out how adaptive existing graph prompts on different graphs, we conduct our experiments across 15 datasets on node-level or graph-level tasks, providing a broad and rigorous testing ground

Figure 2: An overview of ProG

for algorithmic comparisons. As shown in Table 1, we include 7 node classification datasets, covering homophilic datasets (Cora, Citeseer, PubMed, ogbn-arxiv) , heterophilic datasets (Wisconsin, Texas, Actor) , and large-scale datasets (ogbn-arxiv). Additionally, we consider 8 graph classification datasets from different domains, including social networks (IMDB-B, COLLAB) , biological datasets (ENZYMES, PROTEINS, DD) , and small molecule datasets (MUTAG, COX2, BZR) . More details on these datasets can be found in Appendix C.

### Implementation Details

**Data Split.** For the node task, we use 90% data as the test set. For the graph task, we use 80% data as the test set. To ensure the robustness of our findings, we repeat the sampling process five times to construct k-shot tasks for both node and graph tasks. We then report the average and standard deviation over these five results.

**Metrics.** Following existing multi-class classification benchmarks, we select Accuracy, Macro F1 Score, and AUROC (Area Under the Receiver Operating Characteristic Curve) as our main performance metrics . The Accuracy results are used to provide a straightforward indication of the model's overall performance in Section 5. F1 Score and AUROC results are provided in Appendix D.

**Prompt Adaptation to Specific Tasks.** We observe that certain prompt methods, such as GPPT, are only applicable to node classification, while GPF and GPF-plus are only suitable for graph classification. To this end, we design graph task tokens to replace the original node task tokens to make GPPT also applicable in graph-level tasks. Inspired by All-in-one and GPrompt, we utilize induced graphs in our benchmark to adapt GPF and GPF-plus for the node task. The concrete adaptation designs are shown in Appendix A.

**Hyperparameter Optimization.** To manage the influence of hyperparameter selection and maintain fairness, the evaluation process is standardized both with and without hyperparameter adjustments. Initially, default hyperparameters as specified in the original publications are used. To ensure fairness during hyperparameter tuning, **random search** is then employed to optimize the settings. In each trial across different datasets, a set of hyperparameters is randomly selected from a predefined search space for each model. For additional details on metrics, default hyperparameters, search spaces, and other implementation aspects, please see Appendix D.

## 5 Experimental Results

### Overall Performance of Graph Prompt Learning

In Table 2 and Table 3, we present the optimal experimental results of the supervised methods, classical 'Pre-train & Fine-tune' methods, and various graph prompt methods on 1-shot node/graph

   Dataset & Graphs & Avg.nodes & Avg.edges & Features & Node classes & Task (N / G) & Category \\  Cora & 1 & 2,708 & 5,429 & 1,433 & 7 & N & Homophilic \\ Pubmed & 1 & 19,717 & 88,648 & 500 & 3 & N & Homophilic \\ CiteSeer & 1 & 3,327 & 9,104 & 3,703 & 6 & N & Homophilic \\ Actor & 1 & 7600 & 30019 & 932 & 5 & N & Heterophilic \\ Wisconsin & 1 & 251 & 515 & 1703 & 5 & N & Heterophilic \\ Texas & 1 & 183 & 325 & 1703 & 5 & N & Heterophilic \\ ogbn-arxiv & 1 & 169,343 & 1,166,243 & 128 & 40 & N & Homophilic \& Large scale \\   Dataset & Graphs & Avg.nodes & Avg.edges & Features & Graph classes & Task (N / G) & Domain \\  MUTAG & 188 & 17.9 & 19.8 & 7 & 2 & G & Small Molecule \\ IMDB-BINARY & 1000 & 19.8 & 96.53 & 0 & 2 & G & Social Network \\ COLLAB & 5000 & 74.5 & 2457.8 & 0 & 3 & G & Social Network \\ PROTEINS & 1,113 & 39.1 & 72.8 & 3 & 2 & G & Proteins \\ ENZYMES & 600 & 32.6 & 62.1 & 3 & 6 & G & Proteins \\ DD & 1,178 & 284.1 & 715.7 & 89 & 2 & G & Proteins \\ COX2 & 467 & 41.2 & 43.5 & 3 & 2 & G & Small Molecule \\ BZR & 405 & 35.8 & 38.4 & 3 & 2 & G & Small Molecule \\   

Table 1: Statistics of all datasets. N indicates node classification, and G indicates graph classification.

classification tasks across 15 datasets, where the evaluation metric is accuracy (Complete results can be referred to Appendix E). Here the 'optimal' results mean that the best result is shown from six classical 'Pre-train & Fine-tune' methods or from six pretraining-based variants of each graph prompt method. This presentation style can help reflect the upper bounds of classical 'Pre-train & Fine-tune' methods and graph prompt methods, better highlighting the advantages of graph prompts.

From these results, we can observe consistent advantages of various graph prompt learning methods. Almost all graph prompt methods consistently outperform the Supervised method on both node-level and graph-level tasks, demonstrating positive transfer and superior performance. Specifically, graph prompt methods generally surpass classical 'Pre-train & Fine-tune' methods across most datasets, showcasing enhanced knowledge transferability. In node-level tasks, GPF-plus, which focuses on modifying node features via learnable prompt features, achieves the best results on 4 out of 7 datasets with significant effects, attributed to its inherent ability to easily adapt to different pre-training methods . In contrast, All-in-one, which concentrates on modifying graphs via learnable subgraphs, achieves the best results on 7 out of 8 datasets on graph-level tasks.

Similar observations can also be found in 3-shot and 5-shot results in the optimal and complete versions, which can be checked in Appendix E. Additionally, for comprehensive, we also offer the curve of the shot number from 1 to 10 and the accuracy in Appendix E.

### Impact of Pre-training Methods

From Table 2, we further find that on the Wisconsin dataset, the classical Pre-train & Fine-tune method consistently exhibited negative transfer, whereas most graph prompt methods effectively transferred upstream pre-training knowledge, achieving significant results and positive transfer. Therefore, it is crucial to explore the effectiveness of various pre-training methods, especially for graph prompt learning. Table 4 and Table 5 show the experimental results of six variants of GPF-plus/All-in-one and the corresponding 'Pre-train & Fine-tune' methods. Concretely, we have the following key observations:

**Consistency in Pretext and Downstream Tasks.** In node classification tasks, the pre-trained knowledge learned by the node-level pre-training method (e.g., GraphMAE) shows superior transferability to downstream tasks using GPF-plus, achieving better results compared to traditional fine-tuning methods. Conversely, in graph classification tasks, the graph-level pre-training method (e.g., SimGRACE) demonstrates better transferability with All-in-one, resulting in more significant outcomes.

  
**Methods/Datasets** & **Cora** & **Cliteseer** & **Pubmed** & **Wisconsin** & **Texas** & **Actor** & **Ogbn-arxiv** \\  Supervised & 26.56\({}_{ 5.55}\) & 21.78\({}_{ 7.52}\) & 39.37\({}_{ 15.34}\) & 41.60\({}_{ 3.10}\) & 37.97\({}_{ 5.80}\) & 20.57\({}_{ 4.47}\) & 10.99\({}_{ 3.19}\) \\  Pre-train \& Fine-tune & 52.61\({}_{ 1.73}\) & 35.05\({}_{ 4.37}\) & 46.74\({}_{ 11.89}\) & 40.69\({}_{ 1.43}\) & 46.88\({}_{ 8.49}\) & 20.74\({}_{ 1.22}\) & 16.21\({}_{ 3.82}\) \\  GPPT & 43.15\({}_{ 9.44}\) & 37.26\({}_{ 6.17}\) & 48.31\({}_{ 7.72}\) & 30.40\({}_{ 6.81}\) & 31.81\({}_{ 15.33}\) & 22.58\({}_{ 1.97}\) & 14.65\({}_{ 3.07}\) \\  All-in-one & 52.39\({}_{ 10.17}\) & 40.41\({}_{ 2.80}\) & 45.17\({}_{ 6.45}\) & 78.24\({}_{ 16.68}\) & 65.49\({}_{ 7.06}\) & 24.61\({}_{ 2.80}\) & 13.16\({}_{ 6.98}\) \\  Gprompt & 56.66\({}_{ 33.12}\) & 53.21\({}_{ 10.94}\) & 39.74\({}_{ 15.35}\) & 83.80\({}_{ 2.44}\) & 33.25\({}_{ 10.11}\) & 25.26\({}_{ 13.10}\) & 75.72\({}_{ 4.95}\) \\  GPF & 38.57\({}_{ 5.41}\) & 31.16\({}_{ 8.05}\) & 49.99\({}_{ 8.86}\) & 88.67\({}_{ 5.78}\) & 87.40\({}_{ 3.30}\) & 28.70\({}_{ 3.35}\) & 78.37\({}_{ 5.67}\) \\  GPF-plus & 55.77\({}_{ 10.30}\) & 59.67\({}_{ 11.82}\) & 46.64\({}_{ 18.97}\) & 91.03\({}_{ 4.11}\) & 95.83\({}_{ 4.19}\) & 29.32\({}_{ 8.56}\) & 71.98\({}_{ 12.23}\) \\   

Table 2: Performance on 1-shot node classification. The best results for each dataset are highlighted in bold with a dark red background. The second-best are underlined with a light red background.

  
**Methods/Datasets** & **IMDB-B** & **COLLAB** & **PROTEINS** & **MUTAG** & **ENZYMES** & **COX2** & **BZR** & **DD** \\  Supervised & 57.30\({}_{ 0.598}\) & 47.23\({}_{ 0.61}\) & 56.36\({}_{ 7.97}\) & 65.20\({}_{ 6.70}\) & 20.58\({}_{ 2.60}\) & 27.08\({}_{ 1.95}\) & 25.80\({}_{ 6.53}\) & 55.33\({}_{ 6.22}\) \\  Pre-train \& Fine-tune & 57.75\({}_{ 1.22}\) & 48.10\({}_{ 6.23}\) & 63.44\({}_{ 3.56}\) & 65.47\({}_{ 5.89}\) & 22.21\({}_{ 2.79}\) & 76.19\({}_{ 5.41}\) & 34.69\({}_{ 8.50}\) & 57.15\({}_{ 4.32}\) \\  GPPT & 50.15\({}_{ 6.07}\) & 47.18\({}_{ 5.93}\) & 60.92\({}_{ 2.47}\) & 60.40\({}_{ 15.43}\) & 21.29\({}_{ 3.79}\) & 78.23\({}_{ 1.98}\) & 59.32\({}_{ 11.22}\) & 57.69\({}_{ 6.59}\) \\  All-in-one & 60.07\({}_{ 4.81}\) & 51.66\({}_{ 6.26}\) & 66.49\({}_{ 6.26}\) & 29.78\({}_{ 7.54}\) & 23.96\({}_{ 10.56}\) & 76.14\({}_{ 5.51}\) & 79.20\({}_{ 6.05}\) & 59.72\({}_{ 11.92}\) \\  Gprompt & 54.75\({}_{ 2.43}\) & 48.25\({}_{ 13.64}\) & 59.17\({}_{ 11.20}\) & 73.60\({}_{ 7.66}\) & 22.29\({}_{ 3.50}\) & 54.64\({}_{ 9.94}\) & 55.43\({}_{ 13.69}\) & 57.81\({}_{ 2.68}\) \\  GPF & 59.65\({}_{ 5.66}\) & 47.42\({}_{ 11.22}\) & 63.91\({}_{ 3.26}\) & 68.40\({}_{ 5.09}\) & 22.00\({}_{ 1.25}\) & 65.79\({}_{ 17.72}\) & 71.67\({}_{ 44.71}\) & 59.36\({}_{ 118}\) \\  GPF-plus & 57.93\({}_{ 1.62}\) & 47.24\({}_{ 10.29}\) & 62.92\({}_{ 2.78}\) & 65.20\({}_{ 6.04}\) & 22.92\({}_{ 11.66}\) & 33.78\({}_{ 1.52}\) & 71.17\({}_{ 14.92}\) & 57.62\({}_{ 12.42}\) \\   

Table 3: Performance on 1-shot graph classification. The best results for each dataset are highlighted in bold with a dark red background.

[MISSING_PAGE_FAIL:7]

graph and the manually manipulated graph restored by graph prompt methods. These manipulations include dropping nodes, dropping edges, and masking features. Table 6 presents the specific results. The experimental results indicate that **all graph prompt methods significantly reduced the error compared to the original error across all types of manipulations**. Notably, All-in-one and Gprompt achieved the lowest error bounds, demonstrating their flexibility in adapting to graph transformations. Specifically, All-in-one achieved a RED% of 95.06, indicating a 95.06% reduction in original error, underscoring its comprehensive approach to capturing graph transformations. These findings highlight the importance of flexibility in prompt design for effective knowledge transfer. Flexible prompts that can adapt to various graph transformations enable more powerful graph data operation capability, thereby improving the performance of downstream tasks. Moreover, this flexibility is inspiring for more customized graph data augmentation, indicating more potential for other applications.

### Efficiency Analysis

We evaluate the efficiency and training duration of various graph prompt methods across tasks, focusing on node classification in Cora and Wisconsin and graph classification in BZR and DD. **GPF-plus** achieves **high accuracy** with **relatively low training times** despite its **larger parameter size** on Cora and Wisconsin. In contrast, **All-in-One**, with **a smaller parameter size**, has **slightly larger training times** than GPF-plus due to similarity calculations. Among all the graph prompt methods, GPPT shows lower performance and the longest training duration because of its k-means clustering update in every epoch. Besides, we **compare the tunable parameters of graph prompt methods** during the training phase with **classical 'Pre-train & Fine-tune' methods** with various backbones for further analyzing the advantages of graph prompt learning. Consider an input graph has \(N\) nodes, \(M\) edges, \(d\) features, along with a graph model which has \(L\) layers with maximum layer dimension \(D\). **Classical backbones (e.g., GCN) have \(O(dD+D^{2}(L-1))\)** learnable parameter complexity. Other more complex backbones (e.g., graph transformer) may have larger parameters. In the **graph prompt learning framework**, tuning the prompt parameters with a frozen pre-trained backbone makes training converge faster. For **All-in-One**, with \(K\) tokens and \(m\) edges, **learnable parameter complexity is \(O(Kd)\). Other graph prompt methods also have similar tunable parameter advantages, with specific sizes depending on their designs (e.g., Gprompt only includes a learnable vector inserted into the graph pooling by element-wise multiplication).

   & Drop & Drop & Mask &  \\  & Nodes & Edges & Feature & \\  ori\_error & 0.4862 & 0.0713 & 0.6186 & \\  All-in-one & 0.0200 & 0.0173 & 0.0207 & 95.06 \(\) \\ Gprompt & 0.0218 & 0.0141 & 0.0243 & 94.88 \(\) \\ GPF-plus & 0.0748 & 0.0167 & 0.1690 & 77.85 \(\) \\ GPF & 0.0789 & 0.0146 & 0.1858 & 76.25 \(\) \\  

Table 6: Error bound between different prompt methods, RED(%): average reduction of each method to the original error.

Figure 3: Head map of GPF-plus and All-in-one on node-level and graph-level tasks across various pre-training methods and datasets. The green interval shows that the graph prompt learning method performs worse than ’Pre-train & Fine-tune’. The yellow-to-red interval shows that the graph prompt learning method performs better. **An upward arrow** indicates that the ’Pre-train & Fine-tune’ method experiences **negative transfer**, while the graph **prompt** learning method can **alleviate it significantly**.

### Adaptability of Graph Prompts on Different Graphs.

Besides the main observations mentioned before, we here further discuss the adaptability of graph prompts on various kinds of graphs. Specifically, considering graph prompts performance on **various domains** as shown in Table 3, we can find that the adaptability to various domains of graph prompt methods, particularly those of the Prompt as Token type, is not always good, where graph prompt methods even may encounter negative transfer issues. GPPT, in particular, faces significant challenges, likely because first-order subgraphs fail to comprehensively capture graph patterns, introducing noise and affecting transferability. For **Large-scale Datasets** like node-level dataset ogbn-arsiv in Table 2, the transferability of graph prompt methods involving subgraph operations, such as GPPT and All-in-one, requires further enhancement. Furthermore, regarding the relatively large graph-level dataset DD in Table 3, all graph prompt methods achieve positive transfer and outperform Pre-train & Fine-tune. Despite this success, the magnitude of improvement indicates that the transferability on graph-level large-scale datasets also requires further enhancement. For **Homophilic & Heterophilic Datasets** in Table 2, most graph prompt methods can handle both homophilic and heterophilic graph datasets effectively, with the exception of GPPT, possibly due to instability caused by the clustering algorithms of GPPT. For **Pure Structural Dataset** like IMDB-BINARY and COLLAB in Table 3, the ability to handle pure structural datasets of graph prompt methods needs further refinement. In detail, some 'Pre-train & Fine-tune' methods can achieve positive transfer on these datasets, while some variants of the All-in-one method even exhibit negative transfer. Further detailed analysis can be referred to Appendix E.

## 6 Conclusion and Future Plan

In this paper, we introduce ProG, a comprehensive benchmark for comparing various graph prompt learning methods with supervised and 'Pre-train & Fine-tune' methods. Our evaluation includes 6 Pre-training methods and 5 graph prompt learning methods across 15 real-world datasets, demonstrating that graph prompt methods generally outperform classical methods. This performance is attributed to the high flexibility and low complexity of graph prompt methods. Our findings highlight the promising future of graph prompt learning research and its challenges. By making ProG open-source, we aim to promote further research and better evaluations of graph prompt learning methods.

As the empirical analysis in this paper, graph prompt learning shows its significant potential beyond task transferring, though our current evaluation focuses on node/graph-level tasks. We view ProG as a long-term project and are committed to its continuous development. Our future plans include expanding to a wider range of graph tasks, adding new datasets, etc. Ultimately, we aim to develop ProG into a robust graph prompt toolbox with advanced features like automated prompt selection.