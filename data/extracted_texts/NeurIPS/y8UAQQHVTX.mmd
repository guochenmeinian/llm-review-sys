# Private Everlasting Prediction

Moni Naor

Department of Computer Science and Applied Math, Weizmann Institute of Science. moni.naor@weizmann.ac.il. Inclument of the Judith Kleeman Professorial Chair. Research supported in part by grants from the Israel Science Foundation (no.2686/20), by the Simons Foundation Collaboration on the Theory of Algorithmic Fairness and by the Israeli Council for Higher Education (CHE) via the Weizmann Data Science Research Center.

Kobbi Nissim

Department of Computer Science, Georgetown University. kobbi.nissim@georgetown.edu. Research supported in part by by NSF grant No. CNS-2001041 and a gift to Georgetown University.

Uri Stemmer

Blavatnik School of Computer Science, Tel Aviv University, and Google Research. u@uri.co.il. Partially supported by the Israel Science Foundation (grant 1871/19) and by Len Blavatnik and the Blavatnik Family foundation.

Chao Yan

Department of Computer Science, Georgetown University. cy399@georgetown.edu. Research supported in part by by a gift to Georgetown University.

Learning often happens in settings where the underlying training data is related to individuals and privacy-sensitive. For legal, ethical, or other reasons, the learner is then required, to protect personal information from being leaked in the learned hypothesis \(h\). Private learning was introduced by Kasiviswanathan et al. (2011) as a theoretical model for studying such tasks. A _private learner_ is a PAC learner that preserves differential privacy with respect to its training set \(S\). That is, the learner's distribution on outcome hypotheses must not depend too strongly on any single example in \(S\). Kasiviswanathan et al. showed that any finite concept class can be learned privately and with sample complexity \(n=O(|C|)\), a value that is sometimes significantly higher than the VC dimension of the concept class \(C\).

It is now understood that the gap between the sample complexity of private and non-private learners is essential - an important example is private learning of threshold functions (defined over an ordered domain \(X\) as \(C_{thresh}=\{c_{t}\}_{t X}\) where \(c_{t}(x)=1_{x I}\)), which requires sample complexity that is asymptotically higher than the (constant) VC dimension of \(C_{thresh}\). In more detail, with _pure_ differential privacy, the sample complexity of private learning is characterized by the representation dimension of the concept class (Beimel et al., 2013). The representation dimension of \(C_{thresh}\) (hence, the sample complexity of private learning thresholds) is \((|X|)\)(Feldman and Xiao, 2015). With _approximate_ differential privacy, the sample complexity of learning threshold functions is \((^{*}|X|)\)(Beimel et al., 2013, Bun et al., 2015, Alon et al., 2019, Kaplan et al., 2020, Cohen et al., 2022). Hence, whether with pure or with approximate differential privacy, the sample complexity of privately learning thresholds grows with the cardinality of the domain \(|X|\) and no private learner exists for this task over infinite domains. In contrast, non-private learning is possible with constant sample complexity (independent of \(|X|\)).

Privacy preserving (black-box) prediction.Dwork and Feldman (2018) proposed privacy-preserving prediction as an alternative for private learning. Noting that "[i]t is now known that for some basic learning problems [...] producing an accurate private model requires much more data than learning without privacy," they considered a setting where "users may be allowed to query the prediction model on their inputs only through an appropriate interface". That is, a setting where the learned hypothesis is not made public and may be accessed only in a _black-box_ manner via a privacy-preserving query-answering prediction interface. The prediction interface is required to preserve the privacy of its training set \(S\):

**Definition 1.1** (private prediction interface (Dwork and Feldman, 2018) (rephrased)).: A prediction interface \(\) is \((,)\)-differentially private if for every interactive query generating algorithm \(Q\), the output of the interaction between \(Q\) and \((S)\) is \((,)\)-differentially private with respect to \(S\).

Dwork and Feldman focused on the setting where the entire interaction between \(Q\) and \((S)\) consists of issuing a single prediction query and answering it:

**Definition 1.2** (Single query prediction (Dwork and Feldman, 2018)).: Let \(\) be an algorithm that given a set of labeled examples \(S\) and an unlabeled point \(x\) produces a label \(v\). \(\) is an \((,)\)-differentially private prediction algorithm if for every \(x\), the output \((S,x)\) is \((,)\)-differentially private with respect to \(S\).

W.r.t. answering a single prediction query, Dwork and Feldman showed that the sample complexity of such predictors is proportional to the VC dimension of the concept class.

### Our contributions

We extend private prediction to answering a sequence - _unlimited in length_ - of prediction queries. We refer to this as _private everlasting prediction_ (PEP). Our goal is to present a generic private everlasting predictor with low training sample complexity \(|S|\).

#### 1.1.1 Private prediction interfaces when applied to a large number of queries

We begin by examining private everlasting prediction under the framework of Definition 1.1. We prove:

**Theorem 1.3** (informal version of Theorem 3.3).: _Let \(\) be a private everlasting prediction interface for concept class \(C\) and assume \(\) bases its predictions solely on the initial training set \(S\), then there exists a private learner for concept class \(C\) with sample complexity \(|S|\)._

This means that everlasting predictors that base their prediction solely on the initial training set \(S\) are subject to the same complexity lowerbounds as private learners. Hence, to avoid private learning lowerbounds, private everlasting predictors need to rely on more than the initial training sample \(S\) as a source of information about the underlying probability distribution and the labeling concept.

In this work, we choose to allow the everlasting predictor to rely on the queries made - which are unlabeled points from the domain \(X\) - assuming the queries are drawn from the same distribution the initial training \(S\) is sampled from. This requires modifying the privacy definition, as Definition 1.1 does not protect the queries issued to the predictor.

#### 1.1.2 A definition of private everlasting predictors

Our definition of private everlasting predictors is motivated by the observations above. Consider an algorithm \(\) that is first fed with a training set \(S\) of labeled points and then executes for an unlimited number of rounds, where in round \(i\) algorithm \(\) receives as input a query point \(x_{i}\) and produces a label \(_{i}\). We say that \(\) is an everlasting predictor if, when the (labeled) training set \(S\) and the (unlabeled) query points are coming from the same underlying distribution, \(\) answers each query points \(x_{i}\) by invoking a good hypothesis \(h_{i}\) (i.e., \(h_{i}\) has low generalization error), and hence the label \(_{i}\) produced by \(\) is correct with high probability. We say that \(\) is a _private_ everlasting predictor if its sequence of predictions \(_{1},_{2},_{3},\) protects both the privacy of the training set \(S\)_and_ the query points \(x_{1},x_{2},x_{3},\) in face of any adversary that chooses the query points adaptively.

We emphasize that while private everlasting predictors need to exhibit average-case utility - as good prediction is required only for the case where the initial training set \(S\) and the queries \(x_{1},x_{2},x_{3},\) are selected i.i.d. from the same underlying distribution - our privacy requirement is worst-case, and holds in face of an adaptive adversary that chooses each query point \(x_{i}\) after receiving the prediction provided for \((x_{1},,x_{i-1})\), and not necessarily in accordance with any probability distribution.

#### 1.1.3 A generic construction of private everlasting predictors

We show a reduction from private everlasting prediction of a concept class \(C\) to non-private PAC learning of \(C\). Our Algorithm GenericBBL, presented in Section 6, executes in rounds.

Initialization.The input to the first round is a labeled training set \(S\) where \(|S|=O(((C))^{2})\), assumed to be labeled consistently with some unknown target concept \(c C\). Denote \(S_{1}=S\) and \(c_{1}=c\).

Rounds.Each round begins with a collection \(S_{i}\) of labeled examples and ends with a newly generated collection of labeled examples \(S_{i+1}\) that feeds as input for the next round. The size of these collections grows by a constant factor at each round, to allow for the accumulated error of the predictor to converge. The construction ensures that each labeled set \(S_{i}\) is consistent with some concept in \(c_{i} C\), which is not necessarily the original target concept \(c\), but has a bounded generalization error with respect to \(c\). In more detail, the construction ensures that \(_{x D}[c_{i+1}(x) c_{i}(x)]\) decreases by a factor of two in every round, and hence, by the triangle inequality, \(_{x D}[c_{i}(x) c(x)]\) is bounded.

We briefly describe the main computations performed in each round of GenericBBL.5

* **Round initialization:** At the outset of a round, the labeled set \(S_{i}\) is partitioned into sub-sets, each with number of samples which is proportional to the VC dimension (so we have \(|}{(C)}\) sub-sets). Each of the sub-sets is used for training a classifiernon-privately, hence creating a collection of classifiers \(F_{i}=\{f:X\{0,1\}\}\) that are used throughout the round.
* **Query answering:** Queries are issued to the predictor in an online manner. A query is first labeled by each of the classifiers in \(F_{i}\). Then the predicted label is computed by applying a privacy-preserving majority vote on these intermediate labels. By standard composition theorems for differential privacy, we could answer roughly \(|F_{i}|^{2}(|}{(C)})^{2}\) queries without exhausting the privacy budget.
* **Generating a labeled set for the following round:*
* A round ends with the preparation of a collection \(S_{i+1}\) of labeled samples that is to be used in the initialization of next round. We explore alternatives for creating \(S_{i+1}\):
* As at the end of a round the predictor has already labeled all the queries presented during the round's execution, one alternative is to let \(S_{i+1}\) consist of these queries and the labels provided for them. Note, however, that it is not guaranteed that the majority vote would result in a set of labels that are consistent with some concept in \(C\), even if \(S_{i}\) is consistent with some concept in \(C\). Hence, following this alternative would require the use of non-private _agnostic_ learners instead. Furthermore, the error introduced by the majority vote can be larger than the generalization error of the classifiers in \(F_{i}\) by a constant factor greater than one. This may prevent the generalization error of the classifiers used in future rounds from converging.6 * To overcome this problem, we use Algorithm LabelBoost - a tool developed by Beimel et al. (2021) in the context of private semi-supervised learning. LabelBoost takes as input the sample \(S_{i}\) (labeled by a concept \(c_{i} C\)) and the (unlabeled) queries made during the round. It labels them with a concept \(c_{i+1} C\) where the error of \(c_{i+1}\) with respect to \(c_{i}\), i.e., \(_{x D}[c_{i+1}(x) c_{i}(x)]\) is bounded.
* **Controlling the generalization error:** Let \(S_{i+1}\) denote the (re)labeled query points obtained in the \(i\)th round. This is a collection of size \(|S_{i+1}|(|}{(C)})^{2}\). Hence, provided that \(|S_{i}|((C))^{2}\) we get that \(|S_{i+1}|>|S_{i}|\). This allows to lower the accuracy parameters of the non-private learners in each round, and hence ensure that the total error converges.

**Theorem 1.4** (informal version of Theorem 6.1).: _For every concept class \(C\), Algorithm GenericBBL is a private everlasting predictor requiring an initial set of labeled examples which is (upto polylogarithmic factors) quadratic in the VC dimension of \(C\)._

### Related work

Beyond the work of Dwork and Feldman (2018) on private prediction mentioned above, our work is related to private semi-supervised learning and joint differential privacy.

Semi-supervised private learning.As in the model of private semi-supervised learning of Beimel et al. (2021), our predictors depend on both labeled and unlabeled samples. Beyond the difference between outputting a hypothesis and providing black-box prediction, a major difference between the settings is that in the work of Beimel et al. (2021) all samples - labeled and unlabeled - are given at once at the outset of the learning process whereas in the setting of everlasting predictors the unlabeled samples are supplied in an online manner. Our construction of private everlasting predictors uses tools developed for the semi-supervised setting, and in particular Algorithm LabelBoost of of Beimel et al.

Joint differential privacy.Kearns et al. (2015) introduced joint differential privacy (JDP) as a relaxation of differential privacy applicable for mechanism design and games. For every user \(u\), JDP requires that the outputs jointly seen by all _other_ users would preserve differential privacy w.r.t. the input of \(u\). Crucially, in JDP users select their inputs ahead of the computation. In our settings, the inputs to a private everlasting predictor are prediction queries which are chosen in an online manner, and hence a query can depend on previous queries and their answers. Yet, similarly to JDP, the outputs provided to queries not performed by a user \(u\) should jointly preserve differential privacy w.r.t. the query made by \(u\). Our privacy requirement hence extends JDP to an adaptive online setting.

Additional works on private prediction.Bassily et al. (2018) studied a variant of the private prediction problem where the algorithm takes a labeled sample \(S\) and is then required to answer \(m\) prediction queries (i.e., label a sequence of \(m\) unlabeled points sampled from the same underlying distribution). They presented algorithms for this task with sample complexity \(|S|\). This should be contrasted with our model and results, where the sample complexity is independent of \(m\). The bounds presented by Dwork and Feldman (2018) and Bassily et al. (2018) were improved by Dagan and Feldman (2020) and by Nandi and Bassily (2020) who presented algorithms with improved dependency on the accuracy parameter in the agnostic setting.

### Discussion and open problems

We show how to transform any (non-private) learner for the class \(C\) (with sample complexity proportional to the VC dimension of \(C\)) to a private everlasting predictor for \(C\). Our construction is not polynomial time due to the use of Algorithm LabelBoost, and requires an initial set \(S\) of labeled examples which is quadratic in the VC dimension. We leave open the question whether \(|S|\) can be reduced to be linear in the VC dimension and whether the construction can be made polynomial time. A few remarks are in order:

1. While our generic construction is not computationally efficient, it does result in efficient learners for several interesting special cases. Specifically, algorithm LabelBoost can be implemented efficiently whenever given an input sample \(S\) it is possible to efficiently enumerate all possible dichotomies from the target class \(C\) over the points in \(S\). In particular, this is the case for the class of 1-dim threshold functions \(C_{thresh}\), as well as additional classes with constant VC dimension. Another notable example is the class \(C_{thresh}^{enc}\) which intuitively is an "encrypted" version of \(C_{thresh}\). Bun and Zhandry (2016) showed that (under plausible cryptographic assumptions) the class \(C_{thresh}^{enc}\) cannot be learned privately and efficiently, while it can be learned efficiently non-privately. Our construction can be implemented efficiently for this class. This provides an example where private everlasting prediction can be done efficiently, while (standard) private learning is possible but necessarily inefficient.
2. It is now known that some learning tasks require the produced model to memorize parts of the training set in order to achieve good learning rates, which in particular disallows the learning algorithm from satisfying (standard) differential privacy (Brown et al., 2021). Our notion of private everlasting prediction circumvents this issue, since the model is never publicly released and hence the fact that it must memorize parts of the sample is not of a direct privacy threat. In other words, our work puts forward a private learning model which, in principle, allows memorization. This could have additional applications in broader settings.
3. As mentioned above, in general, private everlasting predictors cannot base their predictions solely on the initial training set, and in this work we choose to rely on the _queries_ presented to the algorithm (in addition to the training set). Our construction can be easily adapted to a setting where the content of the blackbox is updated based on _fresh unlabeled samples_ (whose privacy would be preserved), instead of relying on the query points themselves. This might be beneficial to avoid poisoning attacks via the queries.

## 2 Preliminaries

### Preliminaries from differential privacy

**Definition 2.1** (\((,)\)-indistinguishability).: Let \(R_{0},R_{1}\) be two random variables over the same support. We say that \(R_{0},R_{1}\) are \((,)\)-indistinguishable if for every event \(E\) defined over the support of \(R_{0},R_{1}\),

\[[R_{0} E] e^{}[R_{1} E]+\ \ \ \ [R_{1} E] e^{}[R_{0} E]+.\]

**Definition 2.2**.: Let \(X\) be a data domain. Two datasets \(x,x^{} X^{n}\) are called _neighboring_ if \(|\{i:x_{i} x_{i}^{}\}|=1\).

**Definition 2.3** (differential privacy (Dwork et al., 2006)).: A mechanism \(M:X^{n} Y\) is \((,)\)-differentially private if \(M(x)\) and \(M(x^{})\) are \((,)\)-indistinguishable for all neighboring \(x,x^{} X^{n}\).

In our analysis, we use the post-processing and composition properties of differential privacy, that we cite in their simplest forms.

**Proposition 2.4** (post-processing).: _Let \(M_{1}:X^{n} Y\) be an \((,)\)-differentially private algorithm and \(M_{2}:Y Z\) be any algorithm. Then the algorithm that on input \(x X^{n}\) outputs \(M_{2}(M_{1}(x))\) is \((,)\)-differentially private._

**Proposition 2.5** (composition).: _Let \(M_{1}\) be a \((_{1},_{1})\)-differentially private algorithm and let \(M_{2}\) be \((_{2},_{2})\)-differentially private algorithm. Then the algorithm that on input \(x X^{n}\) outputs \((M_{1}(x),M_{2}(x)\) is \((_{1}+_{2},_{1}+_{2})\)-differentially private._

**Theorem 2.6** (Advanced composition Dwork et al. (2010)).: _Let \(M_{1},,M_{k}:X Y\) be \((,)\)-differentially private algorithms. Then the algorithm that on input \(x X\) outputs \((M_{1}(x),,M_{k}(x))\) is \((^{},k+^{})\)-differentially private, where \(^{}=)}\) for every \(^{}>0\)._

**Definition 2.7** (Laplace mechanism (Dwork et al., 2006)).: For \(f:X^{n}\) Let \(_{f}=(f(x)-f(x^{}))\), where the maximum is taken over all neighboring \(x,x^{} X^{n}\). The Laplace mechanism is \(M(x)=f(x)+Y\), where \(Y\) is sampled from the laplace distribution \((_{f}/)\). The Laplace mechanism is \((,0)\)-differentially private.

**Definition 2.8** (Exponential mechanism (McSherry and Talwar, 2007)).: Let \(q:X^{n} Y\) be a score function defined over data domain \(X\) and output domain \(Y\). Define \(=(|q(x,y)-q(x^{},y)|)\) where the maximum is taken over all \(y Y\) and all neighbouring databases \(x,x^{} X^{n}\). The exponential mechanism is the \((,0)\)-differentially private mechanism which selects an output \(y Y\) with probability proportional to \(e^{}\).

**Claim 2.9** (Privacy amplification by sub-sampling (Kasiviswanathan et al., 2011)).: _Let \(\) be an \((^{},^{})\)-differentially private algorithm operating on a database of size \(n\). Let \( 1\) and let \(t=(3+(^{}))\). Construct an algorithm \(\) operating the database \(D=(z_{i})_{i=1}^{t}\). Algorithm \(\) randomly selects a subset \(J\{1,2,,t\}\) of size \(n\), and executes \(\) on \(D_{J}=(z_{i})_{i J}\). Then \(\) is \((,(^{})}^{ })\)-differentially private._

### Preliminaries from PAC learning

A concept class \(C\) over data domain \(X\) is a set of predicates \(c:X\{0,1\}\) (called concepts) which label points of the domain \(X\) by either \(0\) or \(1\). A learner \(\) for concept class \(C\) is given \(n\) examples sampled i.i.d. from an unknown probability distribution \(\) over the data domain \(X\) and labeled according to an unknown target concept \(c C\). The learner should output a hypothesis \(h:X\) that approximates \(c\) for the distribution \(\). More formally,

**Definition 2.10** (generalization error).: The _generalization error_ of a hypothesis \(h:X\) with respect to concept \(c\) and distribution \(\) is defined as \(_{}(c,h)=_{x}[|h(x)-c(x)|]\).

**Definition 2.11** (PAC learning (Valiant, 1984)).: Let \(C\) be a concept class over a domain \(X\). Algorithm \(\) is an \((,,n)\)-_PAC learner_ for \(C\) if for all \(c C\) and all distributions \(\) on \(X\),

\[[(x_{1},,x_{n})^{n}\ ;\ h((x_{1},c(x_{1})),,(x_{n},c(x_{n}))\ ;\ _{}(c,h)] 1-,\]

where the probability is over the sampling of \((x_{1},,x_{n})\) from \(\) and the coin tosses of \(\). The parameter \(n\) is the _sample complexity_ of \(\).

See Appendix A for additional preliminaries on PAC learning.

### Preliminaries from private learning

**Definition 2.12** (private PAC learning (Kasiviswanathan et al., 2011)).: Algorithm \(\) is a \((,,,,n)\)-private PAC learner if (i) \(\) is an \((,,n)\)-PAC learner and (ii) \(\) is \((,)\) differentially private.

Kasiviswanathan et al. (2011) provided a generic private learner with \(O((|X|))\) labeled samples.7Beimel et al. (2013) introduced the representation dimension and showed that any concept class \(C\) can be privately learned with \(((C))\) samples. For the sample complexity of \((,)\)-differentially private learning of threshold functions over domain \(X\), Bun et al. (2015) gave a lower bound of \((^{*}|X|)\). Recently, Cohen et al. (2022) gave a (nearly) matching upper bound of \((^{*}|X|)\).

## 3 Towards private everlasting prediction

In this work, we extend private prediction to answering any sequence of prediction queries - unlimited in length. Our main goal is to present a generic private everlasting predictor with low training sample complexity \(|S|\).

**Definition 3.1** (everlasting prediction).: Let \(\) be an algorithm with the following properties:

1. Algorithm \(\) receives as input \(n\) labeled examples \(S=\{(x_{i},y_{i})\}_{i=1}^{n}(X\{0,1\})^{n}\) and selects a hypothesis \(h_{0}:X\{0,1\}\).
2. For round \(r\), algorithm \(\) gets a query, which is an unlabeled element \(x_{n+r} X\), outputs \(h_{r-1}(x_{n+r})\) and selects a hypothesis \(h_{r}:X\{0,1\}\).

We say that \(\) is an \((,,n)\)-_everlasting predictor_ for a concept class \(C\) over a domain \(X\) if the following holds for every concept \(c C\) and for every distribution \(\) over \(X\). If \(x_{1},x_{2},\) are sampled i.i.d. from \(\), and the labels of the \(n\) initial samples \(S\) are correct, i.e., \(y_{i}=c(x_{i})\) for \(i[n]\), then \([ r 0_{}(c,h_{r})>]\), where the probability is over the sampling of \(x_{1},x_{2},\) from \(\) and the randomness of \(\).

Applying the Dwork-Feldman notion of a private prediction interface to everlasting predictors we get:

**Definition 3.2**.: An algorithm \(\) is an \((,,,,n)\)-everlasting differentially private prediction interface if (i) \(\) is a \((,)\)-differentially private prediction interface \(M\) (as in Definition 1.1), and (ii) \(\) is an \((,,n)\)-everlasting predictor.

As a warmup, consider an \((,,,,n)\)-everlasting differentially private prediction interface \(\) for concept class \(C\) over (finite) domain \(X\) (as in Definition 3.2 above). Assume that \(\) does not vary its hypotheses, i.e. (in the language of Definition 3.1) \(h_{r}=h_{0}\) for all \(r>0\).8 Note that a computationally unlimited adversarial querying algorithm can recover the hypothesis \(h_{0}\) by issuing all queries \(x X\). Hence, in using \(\) indefinitely we lose any potential benefits to sample complexity of restricting access to \(h_{0}\) to being black-box and getting to the point where the lower-bounds on \(n\) from private learning apply. A consequence of this simple observation is that a generic private everlasting predictor should answer all prediction queries with a single hypothesis - it should modify its hypothesis over time as it processes new queries.

We now take this observation a step further, showing that a private everlasting predictor that answers prediction queries solely based on its training sample \(S\) (and randomness, but not on the queries) is subject to the same sample complexity lowerbounds as private learners.

Consider an \((,<1/8,,,n)\)-everlasting differentially private prediction interface \(\) for concept class \(C\) over (finite) domain \(X\) that upon receiving the training set \(S(X\{0,1\})^{n}\) selects an infinite sequence of hypotheses \(\{h_{r}\}_{r 0}\) where \(h_{r}:X\{0,1\}\). Formally, we can think of \(\) as composed of three mechanisms \(=(M_{0},M_{1},M_{2})\) where \(M_{0}\) is \((,)\)-differentially private:

* On input a labeled training sample \(S(X\{0,1\})^{n}\) mechanism \(M_{0}\) computes an initial state and an initial hypothesis \((_{0},h_{0})=M_{0}(S)\).
* On a query \(x_{n+r}\) mechanism \(M_{1}\) produces an answer \(M_{1}(x_{n+r})=h_{i}(x_{n+r})\) and mechanism \(M_{2}\) updates the hypothesis-state pair \((h_{r+1},_{r+1})=M_{2}(_{r})\).

Note that as \(M_{0}\) and \(M_{2}\) do not receive the sequence \(\{x_{n+r}\}_{r 0}\) as input, the sequence \(\{h_{r}\}_{r 0}\) depends solely on \(S\). Furthermore as \(M_{1}\) and \(M_{2}\) post-process the outcome of \(M_{0}\), i.e., the sequence of queries and predictions \(\{(x_{r},h_{r}(x_{r}))\}_{r 0}\) preserves \((,)\)-differential privacy with respect to the training set \(S\). In Appendix B we prove:

**Theorem 3.3**.: \(\) _can be transformed into a \((O(),O(),,,O(n(1/))\)-private PAC learner for \(C\)._

## 4 Private everlasting prediction - definition

Theorem 3.3 requires us to seek private predictors whose prediction relies on more information than what is provided by the initial labeled sample. Possibilities include requiring the input of additional labeled or unlabeled examples during the lifetime of the predictor, while protecting the privacy of these examples.

We choose to rely on the queries for updating the predictor's internal state. This introduces a potential privacy risk for these queries as sensitive information about a query may be leaked in the predictions following it. Furthermore, we need take into account that a privacy attacker may choose their queries adversarially and adaptively.

**Definition 4.1** (private everlasting black-box prediction).: An algorithm \(\) is an \((,,,,n)\)-private everlasting black-box predictor for a concept class \(C\) if

1. **Prediction:**\(\) is an \((,,n)\)-everlasting predictor for \(C\) (as in Definition 3.1).
2. **Privacy:** For every adversary \(\) and every \(t 1\), the random variables \(^{0}_{,t}\) and \(^{1}_{,t}\) (defined in Figure 1) are \((,)\)-indistinguishable.

## 5 Tools from prior works

We briefly describe tools from prior works that we use in our construction. See Appendix C for a more detailed account.

Algorithm LabelBoost[Beimel et al., 2021]:Algorithm LabelBoost takes as input a partially labeled database \(S T(X\{0,1,\})^{*}\) (where the first portion of the database, \(S\), contains examples by some concept \( C\)) and outputs a similar database where both \(S\) and \(T\) are (re)labeled by a concept \(h C\) such that \(_{D}(c,h)\) is bounded. We use the following lemmata from Beimel et al. (2021):

**Lemma 5.1** (privacy of Algorithm LabelBoost).: _Let \(\) be an \((,)\)-differentially private algorithm operating on labeled databases. Construct an algorithm \(\) that on input a partially labeled database \(S T(X\{0,1,\})^{*}\) applies \(\) on the outcome of LabelBoost\((S T)\). Then, \(\) is \((+3,4)\)-differentially private._

**Lemma 5.2** (Utility of Algorithm LabelBoost).: _Fix \(\) and \(\), and let \(S T\) be s.t. \(S\) is labeled by some target concept \(c C\), and \(s.t.\)\(|T|(C)((C)} )-|S|\). Consider the execution of LabelBoost on S\( T\), and let \(h\) denote the hypothesis chosen by LabelBoost to relabel \(S T\). With probability at least \((1-)\) we have that \(_{S}(h)\)._

## 6 A Generic Construction

Our generic construction Algorithm GenericBBL transforms a (non-private) learner for a concept class \(C\) into a private everlasting predictor for \(C\). The theorem below follows from Theorem 6.2 and Claim 6.3 which are proved in Appendix E.

**Theorem 6.1**.: _Given \(,,<1/16,<1\), Algorithm GenericBBL is a \((4,4,,,n)\)-private everlasting predictor, where \(n\) is set as in Algorithm GenericBBL._

**Theorem 6.2** (accuracy of algorithm GenericBBL).: _Given \(,,<1/16,\ <1\), for any concept \(c\) and any round \(r\), algorithm GenericBBL can predict the label of \(x_{r}\) as \(h_{r}(x_{r})\), such that \([error_{}(c(x_{r}) h_{r}(x_{r})) 4] 1-4\)._

**Claim 6.3** (privacy of algorithm GenericBBL).: _GenericBBL is \((,)\)-differentially private._

**Remark 6.4**.: _For simplicity, we analyzed GenericBBL in the realizable setting, i.e., under the assumption that the training set \(S\) is consistent with the target class \(C\). Our construction carries over to the agnostic setting via standard arguments (ignoring computational efficiency). We refer the reader to [Beimel et al., 2021] and [Alon et al., 2020] for generic agnostic-to-realizable reductions in the context of private learning._

### Improving the sample complexity dependency on accuracy

We briefly sketch how to improve the sample complexity of Algorithm GenericBBL from \(n=(^{2}(C)}{^{2}^{2 }})\) to \(n=(^{2}(C)}{^{2}})\) by modifying steps 3(d)ii and 3(d)iii of Algorithm GenericBBL. To simplify the description, we consider a constant \(\), we ignore the privacy amplification by subsampling occurring in steps 2 and 3f and illustrate how the modified algorithm would execute by considering round \(i=1\) of Algorithm GenericBBL.

Note that \(S_{1}=S\) where \(n=|S_{1}|=T_{1}_{1}\) and \(_{1}=((C)/_{1})\) is the sample complexity needed such that each of the hypotheses computed in Step 2(b) has error \(_{1}\) except for

Figure 1: Definition of \(^{0}_{,t}\) and \(^{1}_{,t}\).

probability \(_{i}\). Applying advanced composition, we get that \((T_{1}^{2})\) noisy majority queries implemented in steps steps 3(d)ii and 3(d)iii can be performed, i.e., \(R_{1}=(T_{1}^{2})\). Finally, to feed the next phase with \(|S_{2}|=(|S_{1}|)\) labeled samples, we need that \(R_{1}=(T_{1}_{1})=(T_{1}(C)/)\) and hence \(T_{1}=(VC(C)/)\) resulting in \(n=(VC^{2}(C)/^{2})\).

However, when queries are made from the same underlying distribution \(S\) was selected, we expect that most of them would exhibit a clear majority in steps 3(d)ii and 3(d)iii, except for a fraction of \(O()\). Hence a natural way to improve on the number of majority queries that can be performed is to replace steps 3(d)ii and 3(d)iii with a decision based on the sparse vector technique of Dwork et al. (2009). In particular, we can use the BetweenThresholds mechanism of Bun et al. (2016) to get an improved \(R_{1}=(T_{1}^{2}/)\) and hence get \(T_{1}=(VC(C))\) and \(n=(VC^{2}/)\).

A final important wrinkle is that the above calculation is based on the queries coming from the same underlying distribution \(S\) was selected, while our worst-case privacy requirement allows for an adversarial choice of queries that may in turn cause the execution of BetweenThresholds to halt too often and hence exhaust the privacy budget for the phase. It is hence important to estimate the number of times BetweenThresholds halts within a phase and to stop the execution of Algorithm GenericBBL when the estimate crosses a threshold. This estimate needs to be done in an online manner and should preserve differential privacy with respect to the queries.9 This can be done, e.g., using the _private counter_ algorithm of Dwork et al. (2010), which preserves differential privacy under continual observation.