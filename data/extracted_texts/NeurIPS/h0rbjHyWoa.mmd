# Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts

Zhitong Gao\({}^{1,2}\)   Bingnan Li\({}^{1}\)   Mathieu Salzmann\({}^{2}\)   Xuming He\({}^{1,3}\)

\({}^{1}\)ShanghaiTech University  \({}^{2}\) EPFL

\({}^{3}\)Shanghai Engineering Research Center of Intelligent Vision and Imaging

{gaozht, libn, hexm}@shanghaitech.edu.cn

mathieu.salzmann@epfl.ch

###### Abstract

In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor out-of-distribution (OOD) detection or domain generalization performance. In this work, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that recalibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts. Our method achieves state-of-the-art performance across all benchmarks for both OOD detection and domain generalization. Code is available at [https://github.com/gaozhitong/MultiShiftSeg](https://github.com/gaozhitong/MultiShiftSeg).

## 1 Introduction

Semantic segmentation, a fundamental task in computer vision, has become indispensable in various real-world applications, such as autonomous driving . Recent progress in deep learning-based semantic segmentation has exhibited promising results under the assumption of consistent distributions between the training and testing data. However, these models often falter when faced with distributional shifts. Consequently, research on semantic segmentation under distributional shifts has garnered significant attention in recent years. Some studies approach this challenge from a generalization perspective, aiming to train networks to adapt to data with covariate distribution shifts, such as novel domains . Another line of research focuses on training models to discern (or detect) test data exhibiting semantic distributional shifts, such as anomalies or unfamiliar objects, to ensure reliable predictions . In real-world situation, both types of distribution shifts often occur jointly. This leaves us with the question: _Can a model jointly handle both kinds of distribution shift?_

To address this question, we assess the ability of current domain generalization techniques  to detect unknown objects and that of out-of-distribution detection techniques  to generalize to unknown domains. Interestingly, we find that models trained using domain generalization techniques, such as domain randomization or whitening transformation, often fail to identify unknown objects, and sometimes even perform worse than the baseline without domain generalization. Furthermore, we observe that models trained using out-of-distribution detection techniques struggle to generalize to unknown domains, exhibiting overly high uncertainty towards objects experiencing domain shifts compared to baseline methods without OOD training. While one intuitive approach isto combine existing anomaly segmentation and domain generalization techniques during training, we note that current domain generalization strategies primarily address image-level shifts, whereas anomaly segmentation focuses on object-level semantic differences. Consequently, the resulting models tend to generalize well to image-level variations, such as changes in weather but struggle with object-level shifts. They often misinterpret any object-level distribution shift as a semantic anomaly, assigning high uncertainty scores to known objects that exhibit covariate changes, such as color variations in cars or changes in pedestrian attire, as demonstrated in Fig. 1. These experiments underscore the challenge of differentiating and jointly handling different types of distribution shifts.

In this work, we jointly study both semantic and covariate distribution shifts. That is, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that re-calibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts 1.

Specifically, we first introduce a novel data augmentation technique that employs a semantic-to-image generation model to create data that encompasses both covariate and semantic shifts at various levels, allowing the model to learn the essential differences between the shift types. Additionally, we introduce a learnable, semantic-exclusive uncertainty function trained using a relative contrastive loss. We adopt a two-stage training paradigm designed to balance the integration of these enhancements while minimizing their potential interference. A noise-aware training strategy further complements this approach, employing online, pixel-wise selection to mitigate noise in the generated images. Altogether, our approach not only boosts the model's generalization across domain shifts but also ensures a high level of uncertainty in response to semantic shifts.

We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts, including RoadAnomaly , SMIYC , ACDC-POC  and MUAD  benchmarks. Our results demonstrate that our method achieves state-of-the-art performance across all benchmarks, employing different segmentation backbones for both OOD detection and known class segmentation.

In summary, our contributions are: (1) We study semantic segmentation under both semantic and domain shifts, revealing limitations in methods focused on a single shift; (2) We introduce a coherent-generative augmentation method that augments training data with both shifts; (3) We propose a two-stage, noise-aware training pipeline to optimally leverage augmented data, learning a semantic-exclusive uncertainty function while aligning features for domain shifts.

## 2 Related Work

Anomaly Segmentation(a.k.a. dense out-of-distribution detection). The task aims to detect anomalies or unknown objects by producing pixel-level uncertainty maps. One approach uses generative

Figure 1: We study semantic segmentation with both **semantic-shift** and **covariate-shift** regions. (a) Training for _Out-of-distribution (OOD) detection_ alone  yields high uncertainty for both types of shifts, whereas training for _domain generalization (DG)_ alone  tends to produce low uncertainty for both. Our method effectively differentiates between the two, generating high uncertainty only for semantic-shift regions. (b) We achieve strong performance in both OOD detection and domain-generalized semantic segmentation. (c) This is achieved by coherently augmenting original images (first row) with both covariate and semantic shifts (second row).

models to learn training distributions and detect anomalies through reconstruction differences [30; 48], but this often requires additional networks, resulting in slower inference. Other methods use auxiliary OOD data to train models to distinguish known from unknown instances [46; 6; 31; 42; 36; 18]. Among these, Entropy Maximization  uses entire images from COCO  as OOD proxies, maximizing softmax entropy on these samples. PEBAL  improves upon this by cutting out OOD object instances, pasting them into training images, and using an energy function as the uncertainty score. To reduce artifacts in the pasted OOD region,  proposes using a style transfer model to align the pasted region with the background. RPL  further regularizes embedding similarity between COCO background pixels and training images. Beyond improvements in OOD proxies and uncertainty functions, recent methods explore the use of the Mask2Former architecture , such as RbA , Mask2Anomaly , and EAM . Our method follows this second approach, generating OOD data with a semantic-to-image model to reduce artifacts and further introducing a learnable uncertainty function to enhance both OOD detection and known class segmentation. It is architecture-agnostic, compatible with both pixel-based and mask-based segmentation backbones.

Domain Generalization for Semantic SegmentationThe task aims to train a model on one or more source domains that can perform well on unseen target domains. Existing techniques focus either on introducing specialized model architectures, such as those incorporating normalization  or whitening transformations [9; 41; 27], or on designing domain randomization techniques [45; 50; 23]. Most domain randomization methods rely on image transformation rules or style transfer [45; 50]. Recently, Jia et al.  proposed a semantic-to-image model that generates images across diverse domains. Orthogonally, Bi et al.  explore architectural changes with Mask2Former. Our approach belongs to the domain randomization category, generating images with both domain and semantic shifts simultaneously to improve the model's ability to distinguish between these shifts.

Segmentation Under Multiple Distribution ShiftsEarly works [51; 2] demonstrated the necessity and feasibility of addressing both semantic segmentation under domain shifts and anomaly segmentation. However, these problem settings remain in their early stages (e.g., image-level anomalies) and may not fully capture the true challenges. More recent benchmarks, such as RoadAnomaly , SMIYC , and MUAD , include domain and semantic shifts that better reflect real-world scenarios. Some recent studies  have explored the effects of domain shifts on anomaly segmentation benchmarks and proposed a test-time adaptation pipeline to address the problem. In this work, we aim to further bridge the gap by investigating the core challenges of adopting domain generalization techniques and simultaneously enhancing model performance in both areas.

Generative-based Data AugmentationThis technique is widely used to expand training datasets and prevent overfitting [14; 38; 20; 23; 12]. Unlike rule-based augmentation methods, which focus on image-level changes, generative methods can introduce more object-level variations. Among these works, [12; 33] are the most related to ours, using a text-guided inpainting pipeline to generate anomalies or novel objects. However, this local generation process risks creating inconsistencies between the patch and its background. Additionally, they either focus on generating novel objects within the same domain  or use separate pipelines for domain and semantic shifts . In contrast, our method generates multiple distribution shifts in a single process, preserving the global context of the image and ensuring a more natural integration of novel objects.

## 3 Method

### Problem Formulation and Method Overview

We consider the problem of _semantic segmentation under multiple distribution shifts_. Formally, we define the training distribution as \(P_{XY}\) in \(_{in}^{H W}\), where \(=^{3 H W}\) represents the three-dimensional input space of images with \(H W\) pixels, and \(_{in}=[1,C]\) denotes the semantic label space at each pixel. The test distribution is denoted as \(Q_{XY}_{test}^{H W}\). There are two common types of distribution shifts: _covariate shifts_--where the input distribution changes (\(Q_{X} P_{X}\)) but the label space remains the same --and _semantic shifts_, which involve alterations to the label space, including the introduction of novel categories (\(_{test}_{in}\)). We consider the possibility of both types of distribution shifts occurring during testing.

Our goal is to learn a model capable of jointly identifying semantic-shift regions and generalizing well under covariate shifts. This involves two primary challenges: (1) Enabling the model to distinguish between the two types of distribution shifts, and (2) ensuring the model responds appropriately to each type. To address the first challenge, we introduce a novel generative-based data augmentation strategy that supplements training data with both covariate and semantic shifts in a coherent manner. To tackle the second challenge, we propose a semantic-exclusive uncertainty function with a decoupled training strategy. This encourages the model to learn invariant features for covariate-shift regions while maintaining high uncertainty for semantic-shift regions. Below, we first introduce our generative-based augmentation strategy (Section 3.2), followed by the model training pipeline (Section 3.3).

### Coherent Generative-based Augmentation

To distinguish between covariate and semantic shifts, we design a coherent generative-based data augmentation (CG-Aug) pipeline that enriches the training data with realistic and diverse distribution shifts. The pipeline consists of two stages: The first stage uses zero-shot semantic-to-image generation to create a variety of synthetic data, while the second stage automatically filters out low-quality synthetic data. We describe the details of each stage below.

Zero-Shot Semantic-to-Image Generation.To generate more realistic and diverse OOD data for segmentation, we propose a generation process that first cut-and-pastes the semantic mask of novel objects to the training labels and then leverages a semantic-to-image generation model to create corresponding augmentation images. By exploiting powerful image generation models, this process is able to produce images with a wide range of covariate shifts and augments the training images with both covariate and semantic shifts in a coherent manner. We detail our process below.

Formally, given training set \(^{tr}\{(x_{n},y_{n})\}_{n=1}^{N_{t}}\) with \((x_{n},y_{n}) P_{XY}\), we introduce an auxiliary OOD set \(^{o}\{y_{m}^{o}\}_{m=1}^{N_{a}}\) with object masks \(y_{m}^{o}_{out}^{H W}\). Subsequently, using a pretrained semantic-to-image generative model \(:(_{in}_{out})^{H W}^{ H W}\), we generate an augmented image as

\[x^{aug}=(y^{aug},t) y^{aug}=y y^{o}, \]

where \(t\) is a text prompt and \(\) denotes the pasting operation. Here we adopt a pretrained ControlNet  as \(\) to instantiate the semantic-to-image generation process. Thanks to the powerful prior encoded in Stable Diffusion , this process allows us to generate images with more diverse styles than a task-specific semantic-to-image generation model, therefore creating rich covariate shifts. Moreover, we leverage the text prompt \(t\) to produce more diversity in the augmented images by specifying the space, time, and weather, and to enhance the OOD object generation by indicating the class of the pasted objects, via a set of templates (see Appendix A.1 for details).

Figure 2: Method Overview: (a) A novel generative-based data augmentation strategy that supplements training data with both covariate and semantic shifts in a coherent manner. (2) A semantic-exclusive uncertainty function with two-stage noise-aware training to encourage invariant feature learning for covariate-shift regions while maintaining high uncertainty for semantic-shift regions.

Auto-Filtering.While generative-based augmentation can produce more diverse and realistic distribution shifts than rule-based augmentation, we observed it to often yield inaccurate or noisy rendering for the OOD objects. This might be caused by the fact that these objects appear rarely, or their cut masks are inconsistent with the surroundings. To cope with this, we design an automatic filtering process that identifies generation failures where no object is generated, or a known-category object is incorrectly generated. To achieve this, we leverage pretrained segmentation models to check the region size or its semantic class, and assign a quality score to each generated image. We then filter out the images with low-quality scores (see Appendix A.2 for details).

We perform the above image generation process offline before model training, and the resulting synthetic data is used alongside standard augmentation strategies, such as mixup and AnomalyMix , during training. Below, we denote our augmented dataset as \(D^{aug}=\{(x_{n},y_{n},x_{n}^{aug},y_{n}^{aug})\}_{n=1}^{N_{t}}\).

### Model Training

Given the augmented dataset, we aim to train a segmentation model with recalibrated uncertainty output, generating high OOD scores for semantic-shift regions and performing robustly under covariate shifts. To achieve this, we propose a learnable uncertainty function and develop a stage-wise learning strategy that initializes the uncertainty function before fine-tuning the entire model. Our training process integrates a relative contrastive loss and a noise-aware data selection scheme, enabling the model to effectively align both the feature space and the OOD output scores. We note that our method is generic and can be applied to pixel-wise models (e.g. DeepLabv3+ ) or mask-wise models (e.g. Mask2Former ).

#### 3.3.1 Semantic-Exclusive Uncertainty Recalibration

Learnable Uncertainty Function.Suppose we have a neural network with its feature extractor \(f(x)^{M F}\), where \(M\) is the number of pixels (or masks), and \(F\) is the feature dimension. We introduce a learnable linear projection \(W^{o}^{F C}\), with \(W^{o}_{c}\) denotes \(W^{o}[:c]\) for short. For a pixel-wise prediction model, we adopt the energy function form and parameterize it into a learnable uncertainty function

\[u(x)=_{c} f(x)W^{o}_{c}. \]

For a mask-based segmentation network, we use the adapted maximum softmax probability (MSP) defined in  as the uncertainty function, and parameterize it with \(W^{o}\), leading to

\[u(x)=_{c}((f(x)W^{o}_{c})^{T} g(x) ). \]

Here \(g(x)(0,1)^{M H W}\) is the sigmoid output of the mask head. For both cases, we initialize the projection function \(W^{o}\) as the class weight \(W^{in}\) of the pretrained segmentation network. The corresponding uncertainty score corresponds to the original energy score (or MSP score).

Relative Contrastive Loss.We train the uncertainty function using a novel relative contrastive loss, which encourages higher uncertainty in unknown-class regions compared to known-class regions, while ensuring that regions with and without covariate shifts exhibit similar levels of uncertainty.

Formally, for each batch of data \(\{(x_{n},y_{n},x_{n}^{},y_{n}^{})\}_{n=1}^{B}\), where \(B\) is the batch size, we define the following pixel index sets: \(^{}=\{i:y(i)_{}\}\), representing _inlier_ pixel indices from the original training images; \(^{}=\{i:y^{}(i)_{}\}\), representing inlier pixel indices from the _augmented_ training images (covariate-shift set); and \(^{}=\{i:y(i)_{}\}\{i:y^{}(i)_{}\}\), representing _outlier_ pixel indices from both original and augmented images (semantic-shift set). Here, \(y(i)\) (or \(y^{}(i)\)) denotes the label of pixel \(i\). Our contrastive loss is defined as

\[L_{}=_{o^{},i^{}}_{ _{1}}(u_{o}-u_{i})+_{o^{},c^{} }_{_{2}}(u_{o}-u_{c})+_{c^{},i^{ }}m_{c,i}_{_{3}}(-(u_{c}-u_{i})), \]

where \(_{}(x)=(-x,0)\) is the margin-based contrastive loss, which encourages the input value to exceed \(\). The first two terms promote larger uncertainty gaps between unknown-class and known-class regions, while the third term encourages smaller uncertainty gaps between covariate-shifted and original data. For the third term, we calculate gaps only between pairs of original and augmented images, with \(m_{c,i}\{0,1\}\) indicating whether pixel \((c,i)\) is paired in the dataset. The three margin values (\(_{1},_{2},_{3}\)) introduce priors on the uncertainty gaps, and are set based on the initial average distance. Our method remains robust across a wide range of margin values (cf. Table 6).

Compared to existing OOD losses that either maximize uncertainty only for unknown data  or supervise known and unknown data separately [46; 42], our loss supervises the relative distance between them, making it more robust to hyperparameters and simpler to train (cf. Sec.4.5).

#### 3.3.2 Two-Stage Noise-Aware Training

We now present our two-stage training procedure, which sequentially learns the uncertainty function and the feature extractor \(f(x)\) of the segmentation network. Specifically, we first freeze the pre-trained segmentation network and learn the semantic-exclusive uncertainty function using the relative contrastive loss defined in Eq. 4. We then fine-tune the feature extractor with both the contrastive and standard segmentation loss to improve the feature representations of both known and OOD classes.

Despite the offline filtering process, the generated images may still contain regions that are inconsistent with the label masks. To address this, we introduce a pixel-wise sample selection scheme during training, based on the'small loss' criterion . Specifically, we compute and rank the cross-entropy loss for each pixel, selecting pixels with smaller losses for backpropagation while ignoring those with larger losses. Formally, our _selective cross-entropy loss_ is defined as

\[L_{}(y,p,)=_{i}_{i}_{c}y_{i}^{c} p_{i}^{c}\, \]

where \(p_{i}\) and \(y_{i}\) represent the pixel-wise softmax score and one-hot label, respectively, and \(_{i}\{0,1\}\) indicates whether a pixel is selected for backpropagation. We determine the percentage of selected pixels per batch by visualizing the selection map of a small number of samples, ensuring that visibly incorrect patterns are excluded (see Figure 4 for an example). For models using the Dice loss, such as mask-based ones, we use a similar scheme to remove pixels with a large loss (cf. Appendix A.3).

For the original data, which we assume to be noise-free, we set \(_{i}=1\) for all pixels. This corresponds to using the _standard cross-entropy loss_. We denote the segmentation loss for the original data as \(L_{}^{}\) and for the generated augmentation data as \(L_{}^{}\). The overall loss function can be written as

\[L=L_{}+_{1}L_{}^{}+_{2}L_{}^ {}. \]

Here, \(_{1}\) and \(_{2}\) ensure that the three loss terms are on the same scale.

In summary, our semantic-exclusive uncertainty function, trained through a decoupled parameter training approach and relative contrastive loss, enables the model to fully leverage the generated distribution-shift data. Our noise-aware learning strategy enhances the model's robustness against generation errors. Together, these components of our training pipeline equip the model to effectively learn both domain generalization and accurate OOD detection, ensuring robust performance in dynamic open-world scenarios.

## 4 Experiments

In this section, we evaluate our method's performance in jointly handling anomaly segmentation and domain generalization using several datasets that include both domain and semantic shifts: RoadAnomaly , SMIYC , ACDC-POC , and MUAD . We first introduce the datasets in Sec.4.1 and describe the experimental setup in Sec.4.2. The results are presented in Sec.4.3 and Sec.4.4, followed by an ablation study in Sec. 4.5.

### Datasets

Following the literature [5; 31; 8], we train our model on the Cityscapes dataset  and evaluate its performance on the test sets described below. Based on the evaluation goals, we divide these datasets into two groups. Examples from each dataset are shown in Fig. 3.

**Anomaly Segmentation Datasets:** (a) _The Road Anomaly dataset_ includes 60 images of real-world road anomalies such as animals, rocks, and obstacles, featuring various driving conditionsand covariate shifts. (b) _The SMIYC benchmark_ consists of RoadAnomaly21 (10 validation, 100 test images) and RoadObstacle21 (30 validation, 327 test images), with anomaly objects and domain shifts. These datasets provide masks for anomaly objects, allowing us to evaluate our method's performance on anomaly segmentation under distribution shifts.

**Joint Anomaly Segmentation and Domain Generalization Datasets:** (a) The _ACDC-POC dataset_ is based on the original ACDC Validation set  with generated anomaly objects via inpainting . It contains 200 images with domain shifts including various weather and night scenes. (b) The _MUAD dataset_ is a synthetic dataset containing various driving environments and anomaly objects. We use the challenge test set as in , which contains 240 images with domain shifts at both object and image levels, and anomaly objects such as animals and trash cans.2 These two datasets contain both known-class annotations and unknown object masks, enabling us to evaluate our method jointly for anomaly segmentation and domain generalization.

### Experimental Setup

**Performance Measure:** For evaluation of anomaly segmentation, we use the Area Under the Receiver Operating Characteristics curve (AUROC), the Average Precision (AP), and the False Positive Rate at a True Positive Rate of 95% (FPR95). For evaluation of known class segmentation, we use the mean intersection-over-union (mIoU) and the mean accuracy (mAcc).

**Implementation Details:** We build our method on two segmentation backbones: (a) DeepLabv3+  and (b) Mask2Former . We maintain the network architecture, pretrained models, segmentation loss, and training pipeline the same as in previous work  to make a fair comparison. We use the SMIYC validation set for model selection and maintain the same model for evaluation across all test sets. We refer the reader to Appendix A for other training details.

### Results on Anomaly Segmentation Benchmarks

We present the performance of our method on anomaly segmentation benchmarks, including RoadAnomaly and SMIYC (RA21 and RO21). As shown in Table 1, our method achieves state-of-the-art performance on both DeepLabv3+ and Mask2Former-based models. With the same backbone, it outperforms RPL  by 3% on RoadAnomaly and 5% on SMIYC, and surpasses Mask2Anomaly  by 10% on RoadAnomaly and 3% on SMIYC. Recent methods, M2F-EAM  and RbA , use a more powerful Swin Transformer backbone, while ours uses ResNet-50, as Mask2Anomaly. M2F-EAM also uses Mapillary Vistas  as additional dataset for training. Despite these unfair comparisons, our method still outperforms both on most metrics, demonstrating its effectiveness.

    & &  &  &  \\  Method & Backbone & AUC \(\) & AP \(\) & FPR\({}_{95}\) & AP\(\) & FPR\({}_{95}\) & AP \(\) & FPR\({}_{95}\) \\  Maximum softmax  & & 67.53 & 15.72 & 71.38 & 27.97 & 72.05 & 15.72 & 16.60 \\ ODIN  & & - & - & - & 33.06 & 71.68 & 22.12 & 15.28 \\ Mahalanobis  & & 62.85 & 14.37 & 81.09 & 20.04 & 86.99 & 20.90 & 13.08 \\ Image resynthesis  & & - & - & - & 52.28 & 25.93 & 37.71 & 4.70 \\ SynBoost  & 81.91 & 38.21 & 64.75 & 56.44 & 61.86 & 71.34 & 3.15 \\ Maximized entropy  & DeepLabv3+ & - & 48.85 & 31.77 & 85.47 & 15.00 & 85.07 & 0.75 \\ PEBAL  & & 87.63 & 45.10 & 44.58 & 49.14 & 40.82 & 4.98 & 12.68 \\ Dense Hybrid  & & - & 31.39 & 63.97 & 77.96 & 9.81 & 87.08 & **0.24** \\ RPL+CoroCL  & & 95.72 & 71.61 & 17.74 & 83.49 & 11.68 & 85.93 & 0.58 \\ Ours & & **96.40** & **74.60** & **16.08** & **88.06** & **8.21** & **90.71** & 0.26 \\  Mask2Anomaly  & & - & 79.70 & 13.45 & 88.7 & 14.60 & 93.3 & 0.20 \\ RbA  & Mask2Former & - & 85.42 & 6.92 & 90.90 & 11.60 & 91.80 & 0.50 \\ M2F-EAM  & & - & 69.40 & 7.70 & **93.75** & **4.09** & 92.87 & 0.52 \\ Ours & & **97.94** & **90.17** & **7.54** & 91.92 & 7.94 & **95.29** & **0.07** \\   

Table 1: **Results on anomaly segmentation benchmarks:** RoadAnomaly, SMIYC-RA21 and SMIYC-RO21. Our method achieves the best results under both backbones (Best results in Bold).

In Fig. 3, we visualize the uncertainty map output by our method using the DeepLabv3+ architecture. Compared to the previous state-of-the-art method, RPL , our model assigns higher uncertainty scores to anomalous objects and lower uncertainty scores to covariate shifts. This highlights the efficacy of our method in distinguishing between domain shifts and semantic shifts.

### Results on ACDC-POC and MUAD

We then extend our evaluation to the ACDC-POC and MUAD datasets, assessing both anomaly segmentation performance and known-class domain generalization performance. For a comprehensive comparison, we include both previous state-of-the-art OOD detection techniques and domain generalization techniques [9; 45]. Additionally, we trained a DG+OOD combination method by combining naive OOD training with contrastive loss and rule-based data augmentation (denoted as OOD+RuleAug). A DeepLabv3+ model with standard training is used as a baseline method. 3

The results are shown in Table 2, where our model achieves the best results for both out-of-distribution detection and domain generalization, demonstrating its capacity in jointly handling both types of distribution shifts. By comparison, previous methods fall short in either known class segmentation or OOD detection. Specifically, we find that: (a) Previous works that mainly focus on **domain generalization** (RobustNet , RuleAug ) generally improve the known class segmentation results, but their performance in OOD detection is affected, sometimes worse than the baseline. (b) Previous works that mainly focus on **OOD detection** (such as PEBAL ) perform poorly

   Method & Backbone & Technique &  &  \\   & & OOD & DG & AP\(\) & FPR\(_{5}\)\(\) & mIoU\(\) & mAcc\(\) & AP\(\) & FPR\(_{5}\)\(\) & mIoU\(\) & mAcc\(\) \\  Baseline  & & - & - & 3.92 & 55.50 & 46.89 & 78.57 & 1.34 & 72.78 & 29.47 & 68.63 \\ RuleAug  & & - & ✓ & 2.09 & 72.79 & 86.80 & 81.79 & 0.99 & 81.08 & 29.42 & 69.22 \\ RobustNet  & & - & ✓ & 4.39 & 62.65 & 47.41 & 82.41 & 2.27 & 58.64 & **32.18** & 72.02 \\ PEBAL  & DeepLabv3+ & ✓ & - & 20.67 & 14.35 & 45.59 & 81.28 & 7.81 & 47.56 & 29.08 & 66.41 \\ RPL  & & ✓ & ✓ & 77.84 & 1.20 & 46.35 & 78.96 & 27.70 & 24.45 & 29.86 & 71.60 \\ OOD + RuleAug  & & ✓ & ✓ & 80.65 & 1.30 & 46.76 & 73.08 & 20.97 & 20.37 & 27.83 & 63.02 \\ Ours & & ✓ & **82.41** & **1.01** & **54.12** & **85.07** & **36.08** & **18.74** & 31.33 & **73.13** \\  Mask2Anomaly  & & & - & 73.77 & 3.60 & 47.32 & 83.10 & 39.32 & 41.24 & 23.43 & 61.91 \\ OOD + RuleAug  & Mask2Former & ✓ & ✓ & 82.82 & 0.79 & 50.36 & 82.83 & 25.43 & 41.15 & 26.27 & 67.51 \\ Ours & & ✓ & ✓ & **90.42** & **0.46** & **51.75** & **83.16** & **45.65** & **24.70** & **28.44** & **73.77** \\   

Table 2: **Results on ACDC-POC and MUAD**. Our model achieves the best performance in both anomaly segmentation (AP\(\), FPR\(\) ) and domain-generalized segmentation (mIoU\(\), mAcc\(\) ). Anomaly segmentation methods typically perform worse than the baseline for known class segmentation, while domain generalization methods fall below the baseline on OOD detection. (Best results are in bold; results below baseline are in blue.)

Figure 3: **Comparison of Uncertainty Maps. Our method robustly detects anomalies under covariate shifts across five datasets (first five columns) and generated data (last column). The previous method RPL  failed to distinguish domain from semantic shifts, producing high uncertainty in both cases.**

on domain generalization, sometimes worse than the baseline. Furthermore, their OOD detection performance may also be affected by the domain shift. (c) Previous works that jointly handle **image-level DG and OOD** (RPL  and OOD+RuleAug) may not fully distinguish object-level domain shifts. Our method leveraging diverse augmentations and a dedicated decoupled training strategy enables the model to jointly handle OOD detection and domain generalization.

In Appendix C.1, we provide additional results on individual domain shifts (fog, rain, snow, night) and per-class evaluation. Furthermore, we compare our method with other DG methods on the original ACDC dataset in Appendix C.4, where we show superior domain generalization performance.

### Analysis and Ablation Study

We conduct ablation studies to evaluate the design of our components. We begin by analyzing the effectiveness of our proposed modules: the coherent generative-based augmentation (CG-Aug) and our model training strategy. We then proceed with a detailed examination of each module's design.

Impact of CG-Aug and Training StrategyWe evaluate the decoupled contributions of our data augmentation and training strategies in Table 3. Starting with a recent anomaly segmentation method, Mask2Anomaly , we first replace its original OOD data, which utilizes cut-and-pasted COCO images, with our proposed CG-Aug. As shown in Row #2, this substitution results in consistent performance improvements across all datasets. This demonstrates the efficacy of introducing data with both semantic and domain shifts in a coherent way. Next, we replace their training strategy with ours, leading to further gains in performance. This indicates that our training strategy is more effective in leveraging the generated data. Additionally, we present and discuss similar experiments using RPL  as a baseline. For more details, please refer to Appendix Table 7.

Ablation Study of our CG-AugThe proposed CG-Aug generate semantic-shift and domain-shift jointly in a coherent way. To evaluate the design, we compare with three variations: (1) _Semantic-Shift Only (SS)_: Generate images with semantic shift using POC . (2) _Domain-shift or Semantic-shift (DS or SS)_: Create a mixed dataset with either domain shifts (DS) using our semantic-mask-to-image process or semantic shifts (SS) using POC. (3) _Domain-shift and Semantic-shift (DS and SS)_: First generate DS data, then inpaint unknown objects. The second and third methods can be seen as applying  to our problem in two ways. Results in Table 4 show that: adding domain shift data significantly improves performance over semantic-shift-only data. Jointly generating DS and SS in one image yields better results than generating them separately. Our method, which generates both DS and SS in one step, achieves the best performance, ensuring more coherence without artifacts and outperforming the two-step approach. We include more comparison results with POC  in Appendix C.3.

    & AUC\(\) & AP\(\) & FPR\({}_{95}\)\(\) \\  POC  (SS) & 95.43 & 83.66 & 10.33 \\ DS or SS & 95.90 & 87.64 & 9.28 \\ DS and SS & 96.47 & 89.08 & 8.16 \\  CG-Aug (Ours) & **97.94** & **90.17** & **7.54** \\   

Table 4: **Ablation Study of CG-Aug**. Generating data with both Semantic-shift (SS) and Domain-shift (DS) in a coherent manner achieves better results than other variations. The experiments were conducted using the Mask2Former backbone and evaluated on the RoadAnomaly dataset.

    & & RoadAnomaly &  &  \\  Training & Aug. & AP\(\) & FPR\({}_{95}\)\(\) & AP\(\) & FPR\({}_{95}\)\(\) & AP\(\) & FPR\({}_{95}\)\(\) \\  M2A  & Default & 79.70 & 13.45 & 94.50 & 3.30 & 88.60 & 0.30 \\ M2A  & Ours & 85.47 & 22.38 & **97.96** & 1.55 & 89.80 & **0.12** \\ Ours & Ours & **90.17** & **7.54** & 97.31 & **1.04** & **93.24** & 0.14 \\   

Table 3: **Impact of CG-Aug and Training Strategy. The proposed coherent generative-based augmentation consistently enhances the previous OOD method, Mask2Anomaly  (M2A for short). Our fine-tuning strategy makes better use of the data and further boosts the performance.**Ablation study of our Training DesignWe evaluate our training design in Table 5. In Row #1, we replace our _Learnable Uncertainty Function_ (Learnable-UF) with a fixed energy function. In Row #2, we substitute our _Relative Contrastive Loss_ (RelConLoss) with an absolute contrastive loss , which directly supervises the uncertainty score value rather than the relative gap between two uncertainty scores. In Row #3, we remove the _Sample Selection_ module. Compared to our complete method, presented in the final row, these modifications result in decreased performance in both OOD detection and domain generalization, highlighting the effectiveness of our module design. A visualization of the sample selection process is shown in Figure 4 (a).

In Figure 4(b), we evaluate the effectiveness of our _Stage-wise Training_ pipeline. Starting from a pre-trained baseline model, our first stage--fine-tuning only the learnable uncertainty function--doubles the performance on SMIYC-RA/RO datasets, demonstrating that the initial uncertainty function is often sub-optimal and can be significantly improved using fixed features . A second-stage feature fine-tuning further boosts performance. Additionally, our two-stage approach outperforms single-stage fine-tuning with a learnable uncertainty function, showing that training directly with uncalibrated uncertainties can disrupt feature learning and degrade OOD detection performance.

We also demonstrate the robustness of our method under a range of hyperparameters (loss margins and selection ratio) in Appendix B.1, and evaluate the impact of generated dataset size in Appendix B.2.

## 5 Conclusion

In this work, we have studied semantic segmentation under multiple distribution shifts, finding that prior methods focusing separately on domain generalization and anomaly segmentation may not effectively handle these complex shifts. To tackle this, we have introduced a coherent generative data augmentation approach that enriches training data with both domain and semantic shifts. Additionally, we have proposed a learnable uncertainty function, trained in a stage-wise manner, to fully utilize the data and produce uncertainty scores specifically for semantic shifts. One limitation of our method is its reliance on the quality of the generative model. While we mitigate generation failures through offline autofiltering and online sample selection, some impact remains, such as lower performance for classes the generative model struggles with and potential limitations in scaling up the generated data (see Appendix E for details).

Figure 4: (a) **Visualization of Our Selection Maps. Our selection strategy effectively identifies and removes generation errors (highlighted with boxes). (b) **Analysis of Our Two-Stage Training.** The first stage of training the uncertainty function boosts baseline performance, and second-stage fine-tuning further improves performance, achieving better results than single-stage training.

   Learnable-UF & RelConLoss & Selection & SMIYC-RA & Val & SMIYC-RO & Val &  &  \\   & & & & APT & FPR\({}_{50}\)\(\) & APT & FPR\({}_{50}\)\(\) & APT & FPR\({}_{50}\)\(\) & mIoU\(\) & mAcc\(\) & APT & FPR\({}_{50}\)\(\) & mIoU\(\) & mAcc\(\) \\  ✗ & ✓ & ✓ & 89.34 & 7.51 & 94.96 & 0.21 & 24.10 & 25.73 & 31.67 & 72.16 & 77.92 & 2.00 & 53.43 & 84.46 \\ ✓ & ✗ & ✓ & 91.01 & 5.78 & 94.95 & 0.20 & 20.49 & 24.58 & 32.68 & **73.86** & 75.67 & 1.60 & 53.22 & 84.32 \\ ✓ & ✓ & ✗ & 91.64 & 4.18 & **96.07** & **0.15** & 20.24 & 22.57 & 31.73 & 71.88 & 77.99 & 1.27 & 54.26 & **85.30** \\  ✓ & ✓ & ✓ & **93.82** & **3.94** & 95.20 & 0.19 & **36.08** & **18.74** & **31.33** & 73.13 & **82.41** & **1.01** & **54.12** & 85.07 \\   

Table 5: **Ablation Study of Our Training Pipeline: Learnable Uncertainty Function (Learnable-UF), Relative Contrastive Loss (RelConLoss), and Noise-aware Sample Selection (Selection). Experiments are conducted under DeepLabv3+ architecture.**