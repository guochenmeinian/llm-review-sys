# On Memorization of Large Language Models in

Logical Reasoning

Chulin Xie\({}^{}\)  Yangsibo Huang\({}^{,@paragraphsign}\)  Chiyuan Zhang\({}^{}\)

Da Yu\({}^{}\)  Xinyun Chen\({}^{}\)  Bill Yuchen Lin\({}^{@sectionsign}\)  Bo Li\({}^{}\)  Badih Ghazi\({}^{}\)  Ravi Kumar\({}^{}\)

\({}^{}\)Google \({}^{}\)University of Illinois Urbana-Champaign  Princeton University  Allen Institute for AI

###### Abstract

Large language models (LLMs) show good performance on some complicated reasoning tasks, yet could also make the most basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar benchmark problems accidentally leaked into the training data. In this paper, we systematically investigate this problem with a measurement of memorization in reasoning tasks inspired by human behaviors, and a dynamically generated logical reasoning benchmark based on Knights and Knaves puzzles. We found that LLMs could interpolate the training puzzles (achieving \( 100\%\) accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that LLMs learn to reason while interpolating the training set. At higher level of memorization, the model not only solves more unseen test puzzles, but also solves them relatively robustly under perturbation. This phenomenon suggests that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities, and reveals an interesting direction for future research. Our code and data are available at https://memkklogic.github.io/.

## 1 Introduction

Modern Large Language Models (LLMs) show impressive reasoning capabilities that allow them to solve a wide range of problems including commonsense reasoning and mathematical reasoning. In the meantime, LLMs also make mistakes on some of the most basic problems (e.g., comparing which number is bigger - 13.11 or 13.8 , and counting the number of sisters that Alice's brother have ).

Their contrast of both superhuman reasoning capabilities and dumb mistakes is puzzling when it comes to understanding how exactly LLMs perform reasoning tasks. This question is important both scientifically and practically: understanding how LLMs reason could shed light on our understanding of learning and generalization behaviors of LLMs; and is crucial for real-world applications where robust reasoning is required to mitigate safety and trustworthiness concerns .

One hypothesis is that LLMs could be relying on _memorization_ when solving those reasoning tasks, especially when measured by popular benchmark datasets that could be accidentally leaked into the massive internet-crawled pre-training datasets. Previous work  show that LLMs could indeed memorize the training data. However, most of those studies focus on analyzing memorization from the perspective of privacy  or copyright  concerns. Other papers focus on designing dynamic benchmarks  or alternative evaluation protocols  that could mitigate the issue of benchmark saturation potentially due to memorization. In this paper, we takea direct approach to quantify memorization in reasoning tasks and analyze the interplay between memorization and reasoning. Specifically, we summarize our contributions below:

* To quantify memorization in reasoning tasks, we define a memorization metric based on the notions of interpolation and the performance inconsistency under local perturbation that are inspired by human behaviors.
* To facilitate the measurement, we propose a new logical reasoning benchmark based on the _Knights and Knaves_ (K&K)  puzzles, that support the automatic generation of new puzzles with different difficulty levels and local perturbations of existing puzzles.
* We show that K&K puzzles are challenging and only the most advanced LLMs could consistently solve them. The generally low accuracy observed across most off-the-shelf models indicates that K&K puzzles are likely uncommon in internet-based training data. However, our analysis suggests that certain models exhibit signs of memorization to solve the puzzles.
* By fine-tuning on K&K samples, we confirm that modern LLMs are capable of memorizing a large collection of puzzles and their solutions when seen during training. Interestingly, when measuring accuracy on the unseen test puzzles, we found that the models' reasoning capabilities _grow_ with the amount of memorization as the models _interpolate_ the training set1. Additionally, these enhanced reasoning abilities transfer across different levels of puzzle difficulty. 
## 2 How to Measure Memorization in Reasoning Tasks

### Memorization Metrics for Reasoning Tasks

Memorization of LLMs has been studied in various contexts such as privacy, copyright , and solving knowledge intensive tasks . In this paper, we are specifically interested in measuring the level of memorization when solving reasoning tasks. This kind of behaviors can be observed on humans. For example, when preparing for an exam / interview, people may not be able to fully digest the underlying principles due to various reasons or constraints. But when (luckily) facing the same problem one has prepared for, they would still be able to solve it. The key characteristics of this type of memorization are: (A) high accuracy on observed problems; (B) low accuracy on unseen but similar problems, due to the lack of deep understanding.

Based on this intuition, for a dataset \(\) of reasoning puzzles, we measure the following two quantities:

1. To measure (A), we use the accuracy \((f;)\) to measure the percentage of the puzzles in \(\) that \(f\) can solve. We are especially interested in measuring on the set of _observed puzzles_, i.e. the training set, \((f;)\). We say \(f\)**interpolates** the training puzzles if \((f;) 100\%\).
2. To measure (B), we measure a _consistency ratio_\((f;)\) between the number of _consistently solved puzzles_ after some _local perturbations_, and the number of solved puzzles (without perturbation). We are interested in local perturbations that makes minimal change to the puzzle and maintain the difficulty level (to be specified in SS 2.2).

We combine the two factors to define a **Local Inconsistency based Memorization Score**:

\[(f;)=(f;)(1- (f;)).\]

When there is no ambiguity, we simply call it the memorization score. \((f;)\%\) and a larger score provide a stronger evidence of memorization. In our empirical study, we say \(f\) solves \(\)**via memorization** if \((f;)>10\%\); otherwise we say \(f\) solves \(\)**via reasoning**. Specifically, a high \((f;)\) matches the characteristic behavior of human memorizing observed puzzles, and in this case we say \(f\)**memorized the training puzzles**. Furthermore, we also measure \((f;)\) on test examples, to study if the generalization accuracy is due to reasoning or memorization.

In order to effectively measure memorization score \((f;)\), we need a principled way to (1) perform local perturbation that changes the problem while maintaining its difficulty level; (2) compute the new answer after perturbation, which should be different from the original answer. Towards this goal, we design and implement a functional dataset based on the Knights and Knaves puzzles .

### Knights and Knaves Logical Reasoning Benchmark

Knights and Knaves (K&K) is a type of logic puzzle where some characters can only answer questions truthfully, and others only falsely. The goal is to infer each person's identity. For example: _A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 2 inhabitants: Samuel, and Isabella. Samuel told you that Isabella is a knight. Isabella said that Samuel is a knave and Isabella is a knight. So who is a knight and who is a knave?_ The ground-truth answer is that _(1) Samuel is a knave and (2) Isabella is a knave._

Based on the K&K puzzle, we design a _dynamic_ benchmark that supports generating new problems and perturbing existing problems. Our library automatically solves the K&K puzzles and generates solutions for evaluation and training. Specifically, our benchmark consists of two components:

**The abstract problem sampler** generates random K&K puzzles in an abstract format (see details in SS B). Specifically, it takes as input the problem specification \((N,D,W)\) that determines the difficulty level. It then generates a problem with \(N\) persons, and for each person, a statement that consists of a random tree of maximum width \(W\) and depth \(D\). The leaf nodes can be a claim that a specific person is lying (i.e., kn naive) or telling the truth (i.e., knight), whereas the branching node can be _and_, _or_, _not_, _if_, and _if-and-only-if_. The problem sampler also has two subcomponents: the **Solver** finds all possible solutions to a given puzzle, which is used to guarantee that we generate only problems with a unique solution; the **Perturber** which, given a problem, generates a locally perturbed version, by replacing a leaf node or an entire statement of a random person's statement with a different one. Perturber only keeps the perturbed problems that have a different solution than the original problem. Comparison between the original sample and the leaf/statement-perturbed samples is provided in Tab. 1.

**The natural language generator** takes an abstract K&K problem and formats it in natural language. The formatting is template-based, but we support a variety of different (common and uncommon) person names, role names (e.g., knight & knaves, angels & devils), and different styles of making each person's claim.

We create disjoint sets of \(n_{}\) training problems and \(n_{}\) testing problems for each \(N\)-person task. Here, \(n_{}=100\), \(n_{}=1,000\) for \(N>2\), and \(n_{}=200\) for \(2\)-person tasks due to limited combinations. Then, we generate perturbed versions for each problem.

## 3 Quantifying LLM Memorization of Reasoning Tasks

To quantify LLMs' memorization of the logical reasoning task, we employ the metrics proposed in the previous section. Since the training data for the off-the-shelf models is unknown, we will delay the measurement of the interpolation to fine-tuned models in SS 3.2 and focus on the memorization score under local perturbation \((f;)\) here. As shown in Fig. 1, the test accuracy is relatively low for most cases, suggesting K&K-related problems are probably rare in the Internet and in the training sets of these models. However, we also note that some specific models have large gaps under local perturbation as shown in Fig. 5, such as GPT4o and Claude-3.5-Sonnet on 3-person problems under logical statement perturbation, indicating signs of memorization when solving these puzzles.

Figure 1: Under 0-shot direct prompting, test accuracy of off-the-shelf models drops significantly with increasing puzzle complexity.

### Fine-tuned Models

Here, we study the model's memorization capacity when directly fine-tuned on K&K problems. We take Llama3-8B and GPT4o-mini and run _supervised fine-tuning_ (SFT) on a set of K&K training problems disjoint from the test set. Specifically, during SFT, the model observes the concatenation of the question and the answer for each problem, but the loss is only computed on the answer part.

**LLMs interpolate K&K training problems**. We fine-tune 50 epochs for Llama3-8B and GPT4o-mini for 5 epochs (due to budget constraints) via the OpenAI Finetune API (see details in SS C). From Fig. 3 (clean), we observe high \((f;)\), and GPT4o-mini fine-tuned on \(3\)-person puzzles reach interpolation (\((f;)=100\%\)).

**Interpolating LLMs have large \((f;)\)**. In Fig. 3, we report the consistent accuracy \((f;)\) under perturbation, defined as the ratio of samples correctly solved in both their original and perturbed forms. We observe significant gaps under math problem perturbations (e.g., statement and leaf) on training samples, suggesting that models have large \((f;)\) and may rely on memorization to solve the training samples.

## 4 Large Language Interpolators Learn to Reason

The studies in SS 3 show that both off-the-shelf and fine-tuned models exhibit some level of memorization in solving K&K reasoning tasks. However, does it mean that those models do not have reasoning capabilities at all? It turns out that the models seem to do both, and the reasoning capability actually improves as the memorization level increases. Next, we present evidence that support this hypothesis.

**The generalization performance increases with memorization level**. As shown in Fig. 3, the accuracy of fine-tuned models on the test set continues to increase as \((f;)\) increases, despite that \(\) on training samples also increases (i.e., stronger memorization).

**The \((f;)\) on test samples are smaller than \((f;)\) on train samples** in Fig. 3, particularly for more challenging cases (e.g., 5-person puzzles). This suggests that models are more likely to use reasoning when solving unseen test samples.

**The fine-tuned model generalizes across different difficulty levels**. By fine-tuning on the \(M\)-person problem and testing on the \(N\)-person problem, we study LLMs' transferability. The \(N M\) test accuracy improvement grid in Fig. 4 shows that 1) training on any \(M\)-person problem generally enhances accuracy on unseen \(N\)-person test problems for any \(N\), indicating enhanced reasoning ability on both easier and harder problems; 2) extending the training epochs generally yields better results, particularly for Llama3-8B; 3) test accuracy improvement is larger when \(N 6\), and improving performance on more challenging tasks remains possible but more difficult.

Conclusion

In this paper, we designed a K&K puzzle-based logical reasoning benchmark and local perturbation-based metrics to quantify LLMs' memorization in reasoning tasks. Our results reveal an intriguing interplay between memorization and reasoning: while models heavily rely on memorization to solve challenging K&K puzzles, models trained to have a higher level memorization also solve more unseen test puzzles, and solve them relatively robustly (in contrast to the memorized training puzzles).