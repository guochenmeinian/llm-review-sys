# Adapting Neural Link Predictors

for Data-Efficient Complex Query Answering

 Erik Arakelyan\({}^{*1}\) &Pasquale Minervini\({}^{* 2}\)&Daniel Daza\({}^{3,4,5}\)

&Michael Cochez\({}^{3,5}\) &Isabelle Augenstein\({}^{1}\)

\({}^{1}\)University of Copenhagen \({}^{2}\)University of Edinburgh \({}^{3}\)Vrije Universiteit Amsterdam

\({}^{4}\)University of Amsterdam \({}^{5}\)Discovery Lab, Elsevier, The Netherlands

{erik.a,augenstein}@di.ku.dk p.minervini@ed.ac.uk {d.dazacruz,m.cochez}@vu.nl

###### Abstract

Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Prior work in the literature has proposed to address this problem by designing architectures trained end-to-end for the complex query answering task with a reasoning process that is hard to interpret while requiring data and resource-intensive training. Other lines of research have proposed re-using simple neural link predictors to answer complex queries, reducing the amount of training data by orders of magnitude while providing interpretable answers. The neural link predictor used in such approaches is not explicitly optimised for the complex query answering task, implying that its scores are not calibrated to interact together. We propose to address these problems via \(^{}\), a parameter-efficient score _adaptation_ model optimised to re-calibrate neural link prediction scores for the complex query answering task. While the neural link predictor is frozen, the adaptation component - which only increases the number of model parameters by \(0.03\%\) - is trained on the downstream complex query answering task. Furthermore, the calibration component enables us to support reasoning over queries that include atomic negations, which was previously impossible with link predictors. In our experiments, \(^{}\) produces significantly more accurate results than current state-of-the-art methods, improving from \(34.4\) to \(35.1\) Mean Reciprocal Rank values averaged across all datasets and query types while using \( 30\%\) of the available training query types. We further show that \(^{}\) is data-efficient, achieving competitive results with only \(1\%\) of the complex training queries, and robust in out-of-domain evaluations. Source code and datasets are available at https://github.com/EdinburghNLP/adaptive-cqd.

## 1 Introduction

A Knowledge Graph (KG) is a knowledge base representing the relationships between entities in a relational graph structure. The flexibility of this knowledge representation formalism allows KGs to be widely used in various domains. Examples of KGs include general-purpose knowledge bases such as Wikidata (Vrandecic and Krotzsch, 2014), DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007); application-driven graphs such as the Google Knowledge Graph, Microsoft's Bing Knowledge Graph, and Facebook's Social Graph (Noy et al., 2019); and domain-specific ones such as SNOMED CT (Bodenreider et al., 2018), MeSH (Lipscomb, 2000), and Hetionet (Himmelstein et al., 2017) for life sciences; and WordNet (Miller, 1992) for linguistics.

Answering complex queries over Knowledge Graphs involves a logical reasoning process where a conclusion should be inferred from the available knowledge.

Neural link predictors (Nickel et al., 2016) tackle the problem of identifying missing edges in large KGs. However, in many domains, it is a challenge to develop techniques for answering complex queries involving multiple and potentially unobserved edges, entities, and variables rather than just single edges.

Prior work proposed to address this problem using specialised neural networks trained end-to-end for the query answering task (Hamilton et al., 2018; Daza and Cochez, 2020; Ren et al., 2020; Ren and Leskovec, 2020; Zhu et al., 2022), which offer little interpretability and require training with large and diverse datasets of query-answer pairs. These methods stand in contrast with Complex Query Decomposition (CQD, Arakelyan et al., 2021; Minervini et al., 2022), which showed that it is sufficient to re-use a simple link prediction model to answer complex queries, thus reducing the amount of training data required by orders of magnitude while allowing the possibility to explain intermediate answers. While effective, CQD does not support negations, and fundamentally, it relies on a link predictor whose scores are not necessarily calibrated for the complex query answering task. Adapting a neural link predictor for the query answering task while maintaining the data and parameter efficiency of CQD, as well as its interpretable nature, is the open challenge we take on in this paper.

We propose \(^{}\), a lightweight _adaptation_ model trained to calibrate link prediction scores, using complex query answering as the optimisation objective. We define the adaptation function as an affine transformation of the original score with a few learnable parameters. The low parameter count and the fact that the adaptation function is independent of the query structure allow us to maintain the efficiency properties of CQD. Besides, the calibration enables a natural extension of CQD to queries with atomic negations.

An evaluation of \(^{}\) on three benchmark datasets for complex query answering shows an increase from \(34.4\) to \(35.1\) MRR over the current state-of-the-art averaged across all datasets while using \( 30\%\) of the available training query types. In ablation experiments, we show that the method is data-efficient; it achieves results comparable to the state-of-the-art while using only \(1\%\) of the complex queries. Our experiments reveal that \(^{}\) can generalise across unseen query types while using only \(1\%\) of the instances from a single complex query type during training.

## 2 Related Work

Link Predictors in Knowledge GraphsReasoning over KGs with missing nodes has been widely explored throughout the last few years. One can approach the task using latent feature models, such as neural link predictors (Bordes et al., 2013; Trouillon et al., 2016; Yang et al., 2014; Dettmers et al., 2018; Sun et al., 2019; Balazevic et al., 2019; Amin et al., 2020) which learn continuous representations for the entities and relation types in the graph and can answer atomic queries over incomplete KGs. Other research lines tackle the link prediction problem through graph feature models (Xiong et al., 2017; Das et al., 2017; Hildebrandt et al., 2020; Yang et al., 2017; Sadeghian et al., 2019), and Graph Neural Networks (GNNs, Schlichtkrull et al., 2018; Vashishth et al., 2019; Teru et al., 2020).

Complex Query AnsweringComplex queries over knowledge graphs can be formalised by extending one-hop atomic queries with First Order Logic (FOL) operators, such as the existential quantifier (\(\)), conjunctions (\(\)), disjunctions (\(\)) and negations (\(\)). These FOL constructs can be represented as directed acyclic graphs, which are used by embedding-based methods that represent the queries

Figure 1: Given a complex query \(\), \(^{}\) adapts the neural link prediction scores for the sub-queries to improve the interactions between them.

using geometric objects (Ren et al., 2020; Hamilton et al., 2018) or probabilistic distributions (Ren and Leskovec, 2020; Zhang et al., 2021; Choudhary et al., 2021) and search the embedding space for the answer set. It is also possible to enhance the properties of the embedding space using GNNs and Fuzzy Logic (Zhu et al., 2022; Chen et al., 2022). A recent survey (Ren et al., 2023) provides a broad overview of different approaches. Recent work (Daza and Cochez, 2020; Hamilton et al., 2018; Ren and Leskovec, 2020) suggests that such methods require a large dataset with millions of diverse queries during the training, and it can be hard to explain their predictions.

Our work is closely related to CQD (Arakelyan et al., 2021; Minervini et al., 2022), which uses a pre-trained neural link predictor along with fuzzy logical t-norms and t-conorms for complex query answering. A core limitation of CQD is that the pre-trained neural link predictor produces scores not calibrated to interact during the complex query-answering process. This implies that the final scores of the model are highly dependent on the choice of the particular t-(co)norm aggregation functions, which, in turn, leads to discrepancies within the intermediate reasoning process and final predictions. As a side effect, the lack of calibration also means that the equivalent of logical negation in fuzzy logic does not work as expected.

With \(^{}\), we propose a solution to these limitations by introducing a scalable adaptation function that calibrates link prediction scores for query answering. Furthermore, we extend the formulation of CQD to support a broader class of FOL queries, such as queries with atomic negation.

## 3 Background

A Knowledge Graph \(\) can be defined as a set of subject-predicate-object \( s,p,o\) triples, where each triple encodes a relationship of type \(p\) between the subject \(s\) and the object \(o\) of the triple, where \(\) and \(\) denote the set of all entities and relation types, respectively. A Knowledge Graph can be represented as a First-Order Logic Knowledge Base, where each triple \( s,p,o\) denotes an atomic formula \(p(s,o)\), with \(p\) a binary predicate and \(s,o\) its arguments.

First-Order Logical QueriesWe are concerned with answering logical queries over incomplete knowledge graphs. We consider queries that use existential quantification (\(\)) and conjunction (\(\)) operations. Furthermore, we include disjunctions (\(\)) and atomic negations (\(\)). We follow Ren et al. (2020) by transforming a logical query into Disjunctive Normal Form (DNF, Davey and Priestley, 2002), i.e. a disjunction of conjunctive queries, along with the subsequent extension with atomic negations in (Ren and Leskovec, 2020). We denote such queries as follows:

\[[A]&?A: V _{1},,V_{m}.(e_{1}^{1} e_{n_{1}}^{1}) (e_{1}^{d} e_{n_{d}}^{d}),\\ &e_{i}^{j}=p(c,V),V\{A,V_{1},,V_{m}\},c ,p,\\ &e_{i}^{j}=p(V,V^{}),V,V^{}\{A,V_{1},,V_{m}\},V V^{},p .\] (1)

In Equation (1), the variable \(A\) is the _target_ of the query, \(V_{1},,V_{m}\) denote the _bound variable nodes_, while \(c\) represent the _input anchor nodes_, which correspond to known entities in the query. Each \(e_{i}\) denotes a logical atom, with either one (\(p(c,V)\)) or two variables (\(p(V,V^{})\)).

The goal of answering the logical query \(\) consists in finding the answer set \(\) such that \(a\) iff \([a]\) holds true. As illustrated in Figure 1, the _dependency graph_ of a conjunctive query \(\) is a graph where nodes correspond to variable or non-variable atom arguments in \(\) and edges correspond to atom predicates. We follow Hamilton et al. (2018) and focus on queries whose dependency graph is a directed acyclic graph, where anchor entities correspond to source nodes, and the query target \(A\) is the unique sink node.

**Example 3.1** (Complex Query).: Consider the question "_Which people are German and produced the music for the film Constantine?_". It can be formalised as a complex query \(?T:(,T)(,T)\), where _Germany_ and _Constantine_ are anchor nodes, and \(T\) is the target of the query, as presented in Figure 1. The answer \(\) corresponds to all the entities in the knowledge graph that are German composers for the film Constantine.

Continuous Query DecompositionCQD is a framework for answering EPFO logical queries in the presence of missing edges (Arakelyan et al., 2021; Minervini et al., 2022). Given a query \(\), CQDdefines the score of a target node \(a\) as a candidate answer for a query as a function of the score of all atomic queries in \(\), given a variable-to-entity substitution for all variables in \(\).

Each variable is mapped to an _embedding vector_ that can either correspond to an entity \(c\) or to a _virtual entity_. The score of each of the query atoms is determined individually using a neural link predictor (Nickel et al., 2016). Then, the score of the query with respect to a given candidate answer \([a]\) is computed by aggregating all of the atom scores using t-norms and t-conorms - continuous relaxations of the logical conjunction and disjunction operators.

Neural Link PredictorsA neural link predictor is a differentiable model where atom arguments are first mapped into a \(d\)-dimensional embedding space and then used to produce a score for the atom. More formally, given a query atom \(p(s,o)\), where \(p\) and \(s,o\), the score for \(p(s,o)\) is computed as \(_{p}(_{s},_{o})\), where \(_{s},_{o}^{d}\) are the embedding vectors of \(s\) and \(o\), and \(_{p}:^{d}^{d}\) is a _scoring function_ computing the likelihood that entities \(s\) and \(o\) are related by the relationship \(p\). Following Arakelyan et al. (2021); Minervini et al. (2022), in our experiments, we use a regularised variant of ComplEx (Trouillon et al., 2016; Lacroix et al., 2018) as the neural link predictor of choice, due to its simplicity, efficiency, and generalisation properties (Ruffinelli et al., 2020). To ensure that the output of the neural link predictor is always in \(\), following Arakelyan et al. (2021); Minervini et al. (2022), we use either a sigmoid function or min-max re-scaling.

T-norms and NegationsFuzzy logic generalises over Boolean logic by relaxing the logic conjunction (\(\)), disjunction (\(\)) and negation (\(\)) operators through the use of t-norms, t-conorms, and fuzzy negations. A _t-norm_\(:\) is a generalisation of conjunction in fuzzy logic (Kleinert et al., 2000, 2004). Some examples include the _Godel t-norm_\(_{}(x,y)=\{x,y\}\), the _product t-norm_\(_{}(x,y)=x y\), and the _Lukasiewicz t-norm_\(_{}(x,y)=\{0,x+y-1\}\). Analogously, _t-conorms_ are dual to t-norms for disjunctions - given a t-norm \(\), the complementary t-conorm is defined by \((x,y)=1-(1-x,1-y)\). In our experiments, we use the Godel t-norm and product t-norm with their corresponding t-conorms.

Fuzzy logic also encompasses negations \(n:\). The _standard_\(n_{}(x)=1-x\) and _strict cosine_\(n_{}=(1+( x))\) are common examples of fuzzy negations(Kruse and Moewes, 1993). To support a broader class of queries, we introduce the _standard_ and _strict cosine_ functions to model negations in \(^{}\), which was not considered in the original formulation of CQD.

Continuous Query DecompositionGiven a DNF query \(\) as defined in Equation (1), CQD aims to find the variable assignments that render \(\) true. To achieve this, CQD casts the problem of query answering as an optimisation problem. The aim is to find a mapping from variables to entities \(S=\{A a,V_{1} v_{1},,V_{m} v_{m}\}\), where \(a,v_{1},,v_{m}\) are entities and \(A,V_{1},,V_{m}\) are variables, that _maximises_ the score of \(\):

\[&*{arg\,max}_{S}(,S) =*{arg\,max}_{A,V_{1},,m}e_{1}^{1}  e_{n_{1}}^{1}e_{1}^{d}  e_{n_{d}}^{d}\\ &e_{i}^{j}=_{p}(_{c},_{V}), V\{A,V_{1},,V_{m}\},c,p\\ &e_{i}^{j}=_{p}(_{V},_{V^{ }}),V,V^{}\{A,V_{1},,V_{m}\},V V^{},p ,\] (2)

where \(\) and \(\) denote a t-norm and a t-conorm - a continuous generalisation of the logical conjunction and disjunction, respectively - and \(_{p}(_{s},_{o})\) denotes the neural link prediction score for the atom \(p(s,o)\).

Complex Query Answering via Combinatorial OptimisationFollowing Arakelyan et al. (2021); Minervini et al. (2022), we solve the optimisation problem in Equation (2) by greedily searching for a set of variable substitutions \(S=\{A a,V_{1} v_{1},,V_{m} v_{m}\}\), with \(a,v_{1},,v_{m}\), that maximises the complex query score, in a procedure akin to _beam search_. We do so by traversing the dependency graph of a query \(\) and, whenever we find and atom in the form \(p(c,V)\), where \(p\), \(c\) is either an entity or a variable for which we already have a substitution, and \(V\) is a variable for which we do not have a substitution yet, we replace \(V\) with all entities in \(\) and retain the top-\(k\) entities \(t\) that maximise \(_{p}(_{c},_{t})\) - i.e. the most likely entities to appear as a substitution of \(V\) according to the neural link predictor. As we traverse the dependency graph of a query, we keep a beam with the most promising variable-to-entity substitutions identified so far.

**Example 3.2** (Combinatorial Optimisation).: Consider the query "_Which musicians \(M\) received awards associated with a genre \(g\)?_, which can be rewritten as \({}^{}M: A.(g,A)(A,M)\). To answer this query using combinatorial optimisation, we must find the top-\(k\) awards \(a\) that are candidates to substitute the variable \(A\) in \((g,A)\). This will allow us to understand the awards associated with the genre \(g\). Afterwards, for each candidate substitution for \(A\), we search for the top-\(k\) musicians \(m\) that are most likely to substitute \(M\) in \((A,M)\), ending up with \(k^{2}\) musicians. Finally, we rank the \(k^{2}\) candidates using the final query score produced by a t-norm. \(\)

## 4 Calibrating Link Prediction Scores on Complex Queries

The main limitation in the CQD method outlined in Section 3 is that neural link predictors \(\) are trained to answer simple, atomic queries, and the resulting answer scores are not trained to interact with one another.

**Example 4.1**.: Consider the running example query "_Which people are German and produced the music for the film Constantine?_" which can be rewritten as a complex query \(^{}T:(,T)(,T)\). To answer this complex query, CQD answers the atomic sub-queries \(_{1}=(,T)\) and \(_{2}=(,T)\) using a neural link predictor, and aggregates the resulting scores using a t-norm. However, the neural link predictor was only trained on answering atomic queries, and the resulting scores are not calibrated to interact with each other. For example, the scores for the atomic queries about the relations country and producerOf may be on different scales, which causes problems when aggregating such scores via t-norms. Let us assume the top candidates for the variable \(T\) coming from the atomic queries \(_{1},_{2}\) are \(_{1}\) and \(_{2}\), with their corresponding neural link prediction scores \(1.2\) and \(8.9\), produced using \(_{}\) and \(_{}\). We must also factor in the neural link prediction score of the candidate \(_{1}\) for query \(_{2}\) at \(7.4\) and vice versa at \(0.5\). When using the Godel t-norm \(_{}(x,y)=\{x,y\}\), the scores associated with the variable assignments \(_{1},_{2}\) are computed as, \((8.0,0.5)=0.5\)\((7.4,1.2)=1.2\). For both answers \(_{1}\) and \(_{2}\), the scores produced by \(_{}\) for \(_{1}\) are always lower than the scores produced with \(_{}\) for \(_{2}\), meaning that the scores of the latter are not considered when producing the final answer. This phenomenon can be broadly observed in CQD, illustrated in Figure 2. \(\)

To address this problem, we propose a method for adaptively learning to calibrate neural link prediction scores by back-propagating through the complex query-answering process. More formally, let \(_{p}\) denote a neural link predictor. We learn an additional adaptation function \(_{}\), parameterised by \(=\{,\}\), with \(,\). Then, we use the composition of \(_{}\) and \(_{p}\), \(_{}_{p}\), such that:

\[_{}(_{p}(_{V},_{V^{}}))=_{p}( _{V},_{V^{}})(1+)+.\] (3)

Here, the function \(\) defines an affine transformation of the score and when the parameters \(==0\), the transformed score \(_{}(_{p}(_{V},_{V^{}}))\) recovers the original scoring function. The parameters \(\) can be conditioned on the representation of the predicate \(p\) and the entities \(V\) and \(V^{}\), i.e. \(=(_{V},_{p},_{V^{}})\); here, \(\) is an end-to-end differentiable neural module with parameters \(\). \(_{V}\), \(_{p}\), \(_{V^{}}\) respectively denote the representations of the subject, predicate, and object of the atomic query. In our experiments, we consider using one or two linear transformation layers with a ReLU non-linearity as options for \(\).

The motivation for our proposed adaptation function is twofold. Initially, it is monotonic, which is desirable for maintaining the capability to interpret intermediate scores, as in the original formulation of CQD. Moreover, we draw inspiration from the use of affine transformations in methodologies such as Platt scaling (Platt et al., 1999), which also use a linear function for calibrating probabilities and have been applied in the problem of calibration of link prediction models (Tabacof and Costabello, 2020). Parameter-efficient adaptation functions have also been applied effectively in other domains, such as adapter layers Houlsby et al. (2019) used for fine-tuning language models in NLP tasks.

TrainingFor training the score calibration component in Equation (3), we first compute how likely each entity \(a^{}\) is to be an answer to the query \(\). To this end, for each candidate answer \(a^{}\), we compute the _answer score_ as the complex query score assuming that \(a^{}\) is the final answer as:

\[(,A a^{})= _{S}(,S),A a^{} S.\] (4)Equation (4) identifies the variable-to-entity substitution \(S\) that 1) maximises the query score score \((,S)\), defined in Equation (2), and 2) associates the answer variable \(A\) with \(a^{}\), i.e. \(A a^{} S\). For computing \(S\) with the additional constraint that \(A a^{} S\), we use the complex query answering procedure outlined in Section 3. We optimise the additional parameters \(\) introduced in Section 4, by gradient descent on the likelihood of the true answers on a dataset \(=\{(_{i},a_{i})\}_{i=1}^{||}\) of query-answer pairs by using a _1-vs-all_ cross-entropy loss, introduced by Lacroix et al. (2018), which was also used to train the neural link prediction model:

\[()=_{(_{i},a_{i})}-(_{i},A a_{i})+[_{a^{}} ((_{i},A a^{}))].\] (5)

In addition to the _1-vs-all_(Ruffinelli et al., 2020) loss in Equation (5), we also experiment with the binary cross-entropy loss, using the negative sampling procedure from Ren and Leskovec (2020).

## 5 Experiments

DatasetsTo evaluate the complex query answering capabilities of our method, we use a benchmark comprising of 3 KGs: FB15K (Bordes et al., 2013), FB15K-237 (Toutanova and Chen, 2015) and NELL995 (Xiong et al., 2017). For a fair comparison with previous work, we use the datasets of FOL queries proposed by Ren and Leskovec (2020), which includes nine structures of EPFO queries and 5 query types with atomic negations, seen in Figure 3. The datasets provided by Ren and Leskovec (2020) introduce queries with _hard_ answers, which are the answers that cannot be obtained by direct graph traversal; in addition, this dataset does not include queries with more than 100 answers, increasing the difficulty of the complex query answering task. The statistics for each dataset can be seen in Table 1. Note that during training, we only use _2i_, _3i_, _2in_, and _3in_ queries, corresponding to \( 30\%\) of the training dataset, for the adaptation of the neural link predictor. To assess the model's ability to generalise, we evaluate it on all query types.

  
**Split** & **Query Types** & **FB15K** & **FB15K-237** & **NELL995** \\ 
**Train** & 1p, 2p, 3p, 2i, 3i & 273,710 & 149,689 & 107,982 \\  & 2in, 3i, 1ip, pin, pin & 27,371 & 14,968 & 10,798 \\ 
**Valid** & 1p & 59,078 & 20,094 & 16,910 \\  & Others & 8,000 & 5,000 & 4,000 \\ 
**Test** & 1p & 66,990 & 22,804 & 17,021 \\  & Others & 8,000 & 5,000 & 4,000 \\   

Table 1: Statistics on the different types of query structures in FB15K, FB15K-237, and NELL995.

Figure 3: Query structures considered in our experiments, as proposed by Ren and Leskovec (2020) – the naming of each query structure corresponds to _projection_ (**p**), _intersection_ (**i**), _union_ (**u**) and _negation_ (**n**), reflecting how they were generated in the BetaE paper (Ren and Leskovec, 2020). An example of a **pin** query is \(?T: V.p(a,V),q(V,T), r(b,T)\), where \(a\) and \(b\) are anchor nodes, \(V\) is a variable node, and \(T\) is the query target node.

Figure 2: The distributions of two atomic scores \(_{1}\) and \(_{2}\), and the aggregated results via \(_{}-\) the scores from \(_{2}\) dominate the final scores.

Evaluation ProtocolFor a fair comparison with prior work, we follow the evaluation scheme in Ren and Leskovec (2020) by separating the answer of each query into _easy_ and _hard_ sets. For test and validation splits, we define _hard_ queries as those that cannot be answered via direct traversal along the edges of the KG and can only be answered by predicting at least one missing link, meaning _non-trivial_ reasoning should be completed. We evaluate the method on non-trivial queries by calculating the rank \(r\) for each hard answer against non-answers and computing the Mean Reciprocal Rank (MRR).

BaselinesWe compare \(^{A}\) with state-of-the-art methods from various solution families in Section 2. In particular, we choose GQE (Hamilton et al., 2018), Query2Box (Ren et al., 2020), BetaE (Ren and Leskovec, 2020) and ConE (Zhang et al., 2021) as strong baselines for query embedding methods. We also compare with methods based on GNNs and fuzzy logic, such as FuzzQE (Chen et al., 2022), GNN-QE (Zhu et al., 2022), and the original CQD (Arakelyan et al., 2021; Minervini et al., 2022), which uses neural link predictors for answering EPFO queries without any fine-tuning on complex queries.

Model DetailsOur method can be used with any neural link prediction model. Following Arakelyan et al. (2021), Minervini et al. (2022), we use ComplEx-N3 (Lacroix et al., 2018). We identify the optimal hyper-parameters using the validation MRR. We train for \(50,000\) steps using Adagrad as an optimiser and 0.1 as the learning rate. The beam-size hyper-parameter \(k\) was selected in \(k\{512,1024,,8192\}\), and the loss was selected across _1-vs-all_(Lacroix et al., 2018) and binary cross-entropy with one negative sample.

Parameter EfficiencyWe use the query types _2i_, _3i_, _2in_, _3in_ for training the calibration module proposed in Section 4. We selected these query types as they do not require variable assignments other than for the answer variable \(A\), making the training process efficient. As the neural link prediction model is frozen, we only train the adapter layers that have a maximum of \(^{2 2d}\) learnable weights. Compared to previous works, we have \( 10^{3}\) times fewer _trainable_ parameters, as shown in Table 3, while maintaining competitive results.

  
**Model** & **avg\({}_{p}\)** & **avg\({}_{n}\)** & **1p** & **2p** & **3p** & **2i** & **3i** & **pi** & **ip** & **2u** & **up** & **2in** & **3in** & **inp** & **pin** & **pi** \\   \\  GQE & 28.0 & - & 54.6 & 15.3 & 10.8 & 39.7 & 51.4 & 27.6 & 19.1 & 22.1 & 11.6 & - & - & - & - & - \\ Q2B & 38.0 & - & 68.0 & 21.0 & 14.2 & 55.1 & 66.5 & 39.4 & 26.1 & 35.1 & 16.7 & - & - & - & - & - \\ BetaE & 41.6 & 11.8 & 65.1 & 25.7 & 24.7 & 55.8 & 66.5 & 43.9 & 28.1 & 40.1 & 25.2 & 14.3 & 14.7 & 11.5 & 6.5 & 12.4 \\ CQD-CO & 46.9 & - & **89.2** & 25.3 & 13.4 & 74.4 & 78.3 & 44.1 & 33.2 & 41.8 & 21.9 & - & - & - & - & - \\ CQD-Bean & 68.4 & - & **89.2** & 65.3 & 29.7 & 76.1 & 79.3 & 70.6 & 70.6 & 72.3 & 59.4 & - & - & - & - & - \\ ConE & 49.8 & 14.8 & 73.3 & 33.8 & 29.2 & 64.4 & 73.7 & 50.9 & 37.5 & 55.7 & 31.4 & 17.9 & 18.7 & 12.5 & 9.8 & 15.1 \\ GNN-QE & **72.8** & 38.6 & 88.5 & **69.3** & **58.7** & **79.7** & **83.5** & 69.9 & 70.4 & **74.1** & **61.0** & 44.7 & 41.7 & **42.0** & 30.1 & **34.3** \\ CQD\({}^{A}\) & 70.4 & **42.8** & **89.2** & 64.5 & 57.9 & 76.1 & 79.4 & **70.0** & **70.0** & **70.6** & 68.4 & 57.9 & **54.7** & **47.1** & 37.6 & **35.3** & 24.6 \\   \\  GQE & 16.3 & - & 35.0 & 7.2 & 5.3 & 23.3 & 34.6 & 16.5 & 10.7 & 8.2 & 5.7 & - & - & - & - \\ Q2B & 20.1 & - & 40.6 & 9.4 & 6.8 & 29.5 & 42.3 & 21.2 & 12.6 & 11.3 & 7.6 & - & - & - & - \\ BetaE & 20.9 & 5.5 & 39.0 & 10.9 & 10.0 & 28.8 & 42.5 & 22.4 & 12.6 & 12.4 & 9.7 & 5.1 & 7.9 & 7.4 & 3.5 & 3.4 \\ CQD-CO & 21.8 & - & **46.7** & 9.5 & 6.3 & 31.2 & 40.6 & 23.6 & 16.0 & 14.5 & 8.2 & - & - & - & - \\ COD-Bean & 25.3 & - & **46.7** & 13.3 & 79.7 & 34.4 & 48.3 & 27.1 & 20.4 & 17.6 & 11.5 & - & - & - & - & - \\ ConE & 23.4 & 5.9 & 41.8 & 12.8 & 11.0 & 32.6 & 47.3 & 25.5 & 14.0 & 14.5 & 10.8 & 5.4 & 8.6 & 7.8 & 4.0 & 3.6 \\ GNN-QE & **26.8** & 10.2 & 42.8 & **14.7** & **11.8** & **38.3** & **54.1** & **31.1** & 18.9 & 16.2 & **13.4** & 10.0 & **16.8** & **9.3** & 7.2 & **7.8** \\ CQD\({}^{A}\) & 25.7 & **10.7** & **46.7** & 13.6 & 11.4 & 34.5 & 48.3 & 27.4 & **20.9** & **17.6** & 11.4 & **13.6** & **16.8** & 7.9 & **8.9** & 5.8 \\   \\  GQE & 18.6 & - & 32.8 & 11.9 & 9.6 & 27.5 & 35.2 & 18.4 & 14.4 & 8.5 & 8.8 & - & - & - & - \\ Q2B & 22.9 & - & 42.2 & 14.0 & 11.2 & 33.3 & 44.5 & 22.4 & 16.8 & 11.3 & 10.3 & - & - & - & - \\ BetaE & 24.6 & 5.9 & 53.0 & 13.0 & 11.4 & 37.6 & 47.5 & 24.1 & 14.3 & 12.2 & 8.5 & 5.1 & 7.8 & 10.0 & 3.1 & 3.5 \\ COD-CO & 28.8 & - & **60.4** & 17.8 & 12.7 & 39.3 & 46.6 & 30.1 & 22.0 & 17.3 & 13.2 & - & - & - & - \\ CQD-Bean & 31.8 & - & **60.4** & 22.6 & 13.6 & 42.6 & 52.0 & 31.2 & 25.6 & 19.9 & 16.7 & - & - & - & - \\ ConE & 27.2 & 6.4 & 53.1 & 16.1 & 13.9 & 40.0 & 50.8 & 26.3 & 17.5 & 13.3 & 5.7 & 8.1 & 10.8 & 3.5 & 3.9 \\ GNN-QE & 28.9 & 9.7 & 53.3 & 18.9 & **14.9** & 42.4 & 52.5 & 30.8 & 18.9 & 15.9 & 12.6 & 9.9 & 14.6 & 11.4 & 6.3 & 6.3 \\ CQD\({}^{A}\) & **32.3** & **13.3** & **60.4** & **22.9** & **16.7** & **43.4** & **52.6** & **32.1** & **26.4**

### Results

Complex Query AnsweringTable 2 shows the predictive accuracy of \(^{}\) for answering complex queries compared to the current state-of-the-art methods. Some methods do not support queries that include negations; we leave the corresponding entries blank. We can see that \(^{}\) increases the MRR from \(34.4\) to \(35.1\) averaged across all query types and datasets. In particular, \(^{}\) shows the most substantial increase in predictive accuracy on NELL995 by producing more accurate results than all other methods for all query types. \(^{}\) can achieve these results using \( 30\%\) of the complex query types during training while maintaining competitive results across each dataset and query type. For queries including negations, \(^{}\) achieves a relative improvement of \(6.8\%\) to \(37.1\%\), which can be attributed to the fact that the adaptation is completed with query types _2in_ and _3in_ that include negation, which allows for learning an adaptation layer that is robust for these types of queries. In our experiments, we found that calculating the neural adaptation parameters \(\) of the adaptation function \(_{}\) in Equation (3) as a function of the predicate representation yields the most accurate results followed by computing \(\) as a function of the source entity and predicate representation, which is strictly more expressive. In Appendix A, we show the impact of the adaptation layers on the neural link prediction scores.

The adaptation process does not require data-intensive training and allows the model to generalise to query types not observed during training. This prompts us to investigate the minimal amount of data samples and query types required for adaptation.

Data EfficiencyTo analyse the data efficiency of \(^{}\), we compare the behaviour of the pre-trained link predictors tuned with \(1\%\) and \(100\%\) of the training complex query examples in FB15K-237, presented in Table 4. For adapting on \(1\%\) of the training complex queries, we used the same hyper-parameters we identified when training on the full dataset. Even when using \(1\%\) of the complex training queries (\(3290\) samples) for tuning, the model still achieves competitive results, with an average MRR difference of \(2.2\) compared to the model trained using the entire training set. \(^{}\) also produces higher test MRR results than GNN-QE with an average MRR increase of \(4.05\).

We can also confirm that the adaptation process converges after \( 10\%\) of the training epochs as seen in Figure 4. The convergence rate is not hindered when using only \(1\%\) of the training queries. This shows that \(^{}\) is a scalable method with a fast convergence rate that can be trained in a data-efficient manner.

Out-of-Distribution GeneralisationTo study the generalisation properties of \(^{}\), we trained the adaptation layer on all atomic queries and only \(1\%\) of samples for _one_ training query type _2i_, one of the simplest complex query types. We see in Table 4 that \(^{}\) can generalise to other types of complex queries not observed during training with an average MRR difference of \(2.9\) compared to training on all training query types. \(^{}\) also produces significantly higher test MRR results

Figure 4: Average test MRR score (\(y\)-axis) of \(^{}\) using \(1\%\) and \(100\%\) of the training queries from FB15K-237 throughout the training iterations (\(x\)-axis).

    \\  ^{}\)} & FB15K & FB15K-237 & NELL \\   & \(}_{}\) & \(}_{}\) & \(}_{}\) \\  & \(+4 10^{3}\) & \(+4 10^{3}\) & \(+4 10^{3}\) \\ BetaE & \(1.3 10^{7}\) & \(1.3 10^{7}\) & \(6 10^{7}\) \\ Q2B & \(1.2 10^{7}\) & \(1.2 10^{7}\) & \(6 10^{7}\) \\ GNN-QE & \(3 10^{6}\) & \(3 10^{6}\) & \(3 10^{6}\) \\ ConE & \(1.2 10^{7}\) & \(1.2 10^{7}\) & \(6 10^{7}\) \\ GQE & \(1.5 10^{7}\) & \(1.5 10^{7}\) & \(7.5 10^{7}\) \\   

Table 3: Number of parameters used by different complex query answering methods – values for GNN-QE are approximated using the backbone NBFNet (Zhu et al., 2021), while the remaining use their original studies.

than GNN-QE, with an average increase of \(5.1\) MRR. The greatest degradation in predictive accuracy occurs for the queries containing negations, with an average decrease of \(2.7\). This prompts us to conjecture that being able to answer general EPFO queries is not enough to generalise to the larger set of queries, which include atomic negation. However, our method can generalise on all query types, using only \(1\%\) of the _2i_ queries, with \(1496\) overall samples for adaptation.

Fine-Tuning All Model ParametersOne of the reasons for the efficiency of CQD\({}^{}\) is that the neural link predictor is not fine-tuned for query answering, and only the parameters in the adaptation function are learned. We study the effect of fine-tuning the link predictor using the full training data for CQD and CQD\({}^{}\) on FB15K-237. We consider several variants: 1) CQD\({}_{F}\), where we Fine-tune all neural link predictor parameters in CQD; 2) CQD\({}_{F}^{}\), where we fine-tune all link predictor parameters in CQD\({}^{}\), 3) CQD\({}_{}\), where we learn a transformation for the entity and relation embeddings and we use it to **Re**place the initial entity and relation representations, and 4) CQD\({}_{}\), where we learn a transformation for the entity and relation embeddings, and we **Concatenate** it to the initial entity and relation representations.

It can be seen from Table 5 that CQD\({}^{}\) yields the highest test MRR results across all query types while fine-tuning all the model parameters produces significant degradation along all query types, which we believe is due to catastrophic forgetting (Goodfellow et al., 2013) of the pre-trained link predictor.

## 6 Conclusions

In this work, we propose the novel method CQD\({}^{}\) for answering complex FOL queries over KGs, which increases the averaged MRR over the previous state-of-the-art from \(34.4\) to \(35.1\) while using \( 30\%\) of query types. Our method uses a single adaptation layer over neural link predictors, which allows for training in a data-efficient manner. We show that the method can maintain competitive predictive accuracy even when using \(1\%\) of the training data. Furthermore, our experiments on training on a subset (\(1\%\)) of the training queries from a single query type (\(2i\)) show that it can generalise to

  
**Dataset** & **Model** & **1p** & **2p** & **3p** & **2i** & **3i** & **pi** & **ip** & **2u** & **up** & **2in** & **3in** & **inp** & **pin** & **pni** \\   & CQD\({}^{}\) & **46.7** & **11.8** & **11.4** & **33.6** & 41.2 & **24.82** & **17.81** & **16.45** & **8.74** & **10.8** & **13.36** & 5.93 & **5.38** & **14.82** \\  & GNN-QE & 36.82 & 8.96 & 8.13 & 33.02 & **49.28** & 24.58 & 14.18 & 10.73 & 8.47 & 4.89 & 12.31 & **6.74** & 4.41 & 4.09 \\  & Beta & 36.80 & 6.89 & 59.42 & 28.44 & 34.34 & **17.12** & 87.92 & 9.23 & 56.46 & 4.41 & 6.14 & 5.18 & 2.54 \\   & CQD\({}^{}\) & **46.7** & **11.8** & **11.2** & **30.35** & 40.75 & **23.36** & **18.28** & **15.85** & **8.96** & **9.36** & **10.25** & **5.17** & **4.46** & **4.44** \\  & GNN-QE & 34.81 & 5.40 & 5.17 & 30.12 & **48.88** & 23.06 & 12.65 & 9.85 & 5.26 & 4.26 & 12.5 & 4.43 & 0.71 & 1.98 \\   & Beta & 37.99 & 5.62 & 4.48 & 23.73 & 35.25 & 15.63 & 7.96 & 9.73 & 4.56 & 0.15 & 0.49 & 0.62 & 0.10 & 0.14 \\   

Table 4: Comparison of test MRR results for queries on FB15K-237 using the following training sets – FB237, 1% (resp. FB237 i, 1%) means that, in addition to all 1p (atomic) queries, only 1% of the complex queries (resp. _2i_ queries) was used during training. As CQD\({}^{}\) uses a pre-trained link predictor, we also include all _1p_ queries when training GNN-QE for a fair comparison.

  
**Model** & **2p** & **2i** & **3i** & **pi** & **ip** & **2u** & **up** & **2in** & **3in** & **inp** & **pin** & **pni** \\  CQD & **13.2** & 34.5 & 48.2 & 26.8 & 20.3 & 17.4 & 10.3 & 5.4 & 12.4 & 6.1 & 3.2 & 4.6 \\ CQD\({}_{}\) & 9.3 & 22.8 & 34.9 & 19.8 & 14.5 & 13.0 & 7.2 & 7.4 & 7.1 & 4.9 & 3.9 & 3.8 \\ CQD\({}_{}\) & 9.5 & 23.9 & 39.0 & 19.8 & 14.5 & 14.2 & 7.2 & 8.4 & 9.7 & 4.9 & 4.2 & 3.6 \\ CQD\({}_{}\) & 10.9 & 33.7 & 47.3 & 25.6 & 18.9 & 16.4 & 9.4 & 7.9 & 12.2 & 6.6 & 4.2 & 5.0 \\ CQD\({}_{}\) & 6.4 & 22.2 & 31.0 & 16.6 & 11.2 & 12.5 & 4.8 & 4.7 & 5.9 & 4.1 & 2.0 & 3.5 \\ CQD\({}^{}\) & **13.2** & **35.0** & **48.5** & **27.3** & **20.7** & **17.6** & **10.5** & **13.2** & **14.9** & **7.4** & **7.8** & **5.5** \\   

Table 5: Test MRR results for FOL queries on FB15K-237 using the following CQD extensions: CQD from Arakelyan et al. (2021); Minervini et al. (2022) with the considered normalisation and negations; CQD\({}_{}\), where we fine-tune all neural link predictor parameters in CQD; CQD\({}_{F}^{}\), where we _fine-tune all link predictor parameters_ in CQD\({}^{}\); CQD\({}_{}\), where we learn a _transformation_ for the entity and relation embeddings and we use it to _replace_ the initial entity and relation representations; and CQD\({}_{}\), where we learn a transformation for the entity and relation embeddings, and we _concatenate_ it to the initial entity and relation representations.

new queries that were not used during training while being data-efficient. Our results provide further evidence for how neural link predictors exhibit a form of compositionality that generalises to the complex structures encountered in the more general problem of query answering. CQD\({}^{A}\) is a method for improving this compositionality while preserving computational efficiency. As a consequence, rather than designing specialised models trained end-to-end for the query answering task, we can focus our efforts on improving the representations learned by neural link predictors, which would then transfer to query answering via efficient adaptation, as well as other downstream tasks where they have already proved beneficial, such as clustering, entity classification, and information retrieval.