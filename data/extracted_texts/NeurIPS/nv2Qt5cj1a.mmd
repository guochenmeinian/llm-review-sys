# Membership Inference Attacks against

Large Vision-Language Models

 Zhan Li Yongtao Wu Yihang Chen1 Francesco Tonin Elias Abad Rocamora Volkan Cevher

LIONS, EPFL

[first name].[last name]@epfl.ch

Equal contribution.Corresponding author. Email: yhangchen@cs.ucla.edu.

###### Abstract

Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRenyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.

## 1 Introduction

The rise of large language models (LLMs) [9; 60; 45; 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4  and Gemini . These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning , image question answering [13; 35], and image knowledge extraction . However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3; 56; 63]. To mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23; 53], where attackers seek to detect whether a particular data record is part of the training dataset [23; 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24; 19; 44].

When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size  and multi-modality of the training data, and the diverse VLLMs training pipelines [66; 35; 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs.

Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is , which performs MIAs on multi-modal CLIP  by detecting whether an image-text pair is in the training set. However, in practice, it is more commonto detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as Min-K% and Perplexity, mostly are _target-based_ MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only _target-free_ MIAs  can be directly applied.

Therefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61; 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM's output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way.

Overall, the contributions and insights can be summarized as follows.

* We release the first benchmark tailored for the detection of training data in VLLMs, called **V**ision **L**anguage **MIA** (**VL-MIA**) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 , LLaVA 1.5  and LLaMA-Adapter V2 .
* We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM's output logits (Figure 1 and Section 5.1).
* We propose a target-free MIA metric, MaxRenyi-K%, and its modified target-based ModRenyi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs.

## 2 Related work

**Membership Inference Attack (MIA)** aims to classify whether a data sample has been used in training a machine learning model . Keeping training data confidential is a desired property for

Figure 1: **MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRenyi-K% metric: we first get the Rényi entropy of each token position, then select the largest \(k\%\) tokens and calculate the average Rényi entropy.**

machine learning models, since training data may contain private information about an individual [62; 24]. Popular MIA methods can be divided into metric-based and shadow model-based MIAs . Metric-based MIAs [62; 48; 57; 52] determine whether a data sample has been used for training by comparing metrics computed from the output of the target model with a threshold. Shadow model-based MIAs need shadow training to mimic the behavior of the target model [53; 48], which is computationally infeasible for LLMs. Therefore, we focus on the metric-based methods in this work.

MIAs have been extensively researched in various machine learning models, including classification models [38; 58; 10], generative models [20; 22; 7], and embedding models [54; 40]. With the emergence of LLMs, there are also a lot of work exploring MIAs in LLMs [42; 17; 52; 41]. Nevertheless, MIAs for multi-modal models have not been fully explored. [30; 25] perform MIAs using the similarity between the image and the ground truth text, which detects the image-text pair instead of a single image or text sequence. However, detecting an individual image or text is more practical in real-world scenarios and poses additional challenges. To the best of our knowledge, we are the first to perform the individual image or text MIA on VLLMs.

In this paper, we also consider a more difficult task, to detect the pre-training data from the fine-tuned models, that is, to detect the base LLM pertaining data from VLLMs. First, compared with detecting fine-tuning data [55; 51], pretraining data come from a much larger dataset and are used only once, reducing the potential probability for a successful MIA [27; 32]. In addition, compared to the detection of pretraining data from the pre-trained models [53; 52], catastrophic forgetting [29; 28; 16] in the fine-tuning stage also makes it harder to detect the pre-training data from downstream models. To the best of our knowledge, we are the first to perform pre-training data MIAs on fine-tuned models.

**Large Vision-Language Models (VLLMs)** incorporate visual preprocessors into LLMs [9; 60; 45] to manage tasks that require handling inputs from text and image modalities. A foundational approach in this area is represented by CLIP , which established techniques for aligning modalities between text and images. Further developments have integrated image encoders with LLMs to create enhanced VLLMs. These models are typically pre-trained on vast datasets of image-text pairs for feature alignment [33; 64; 39; 2], and are subsequently instruction-tuned for specific downstream tasks to refine the end ability. MiniGPT [66; 8], LLaVA [36; 35], and LLaMA Adapter [65; 18] series have demonstrated significant capabilities in understanding and inference in this area.

## 3 Problem setting

In this section, we introduce the main notation and problem settings for MIAs.

**Notation.** The token set is denoted by \(\). A sequence with \(L\) tokens is denoted by \(X:=(x_{1},x_{2},,x_{L})\), where \(x_{i}\) for \(i[L]\). Let \(X_{1} X_{2}\) be the concatenation of sequence \(X_{1}\) and \(X_{2}\). An image token sequence is denoted by \(Z\). In this work, we focus on the VLLM, parameterized by \(\), where the input is the image \(Z\) followed by the instruction text \(X_{}\), and the output is the description text \(X_{}\). We use \(_{}\) and \(_{}\) to represent the description training set and image training set, respectively. Detailed notations are summarized in Table 5 of the appendix.

**Attacker's goal.** In this work, the purpose of the attacker is to detect whether a given data point (image \(Z\) or description \(X_{}\)) belongs to the training set. We formulate this attack as a binary classification problem. Let \(_{}(Z;):\{0,1\}\) and \(_{}(X;):\{0,1\}\) be two binary classification algorithms for image and description respectively, which are implemented by comparing the metric \((Z X_{} X_{};)\) with some threshold \(\).

When detecting image \(Z\), we feed the model with the target image with a fixed instruction prompt such as "Describe this image in detail", denoted as \(X_{}\). The model then generates the description text \(X_{}^{}\). The algorithm \(_{}(Z;)\) is defined by

\[_{}(Z;)=1&(Z_{}),(Z X_{} X_{}^{};)< ,\\ 0&(Z_{}),(Z X_{ } X_{}^{};).\] (1)

When detecting a description sequence \(X_{}\), we feed the model with an all-black image, denoted as \(Z_{}\), as the visual input, followed by an empty instruction \(X_{}\). The algorithm \(_{}(X_{};)\) is defined by

\[_{}(X_{};)=1&(X_{} _{}),(Z_{} X_{ } X_{};)<,\\ 0&(X_{}_{}),(Z_{ } X_{} X_{};).\] (2)

**Attacker's knowledge.** We assume a grey-box setting on the target model, where the attacker can query the model by a custom prompt (including an image and text) and have access to the tokenizer, the output logits, and the generated text. The attacker is unaware of the training algorithm and parameters of the target model.

## 4 Dataset construction

We construct a general dataset: **V**ision **L**anguage **MIA** (**VL-MIA**), based on the training data used for popular VLLMs, which, to our knowledge, is the first MIA dataset designed specifically for VLLMs. We present a takeaway overview of VL-MIA in Table 1. We also provide some examples in VL-MIA, see Table 16 in the appendix. The prompts we use for generation can be found in Table 6.

**Target models.** We perform MIAs open-source VLLMs, including MiniGPT-4 , LLaVA-1.5 , and LLaMA-Adapter V2.1 . The checkpoints and training datasets of these models are provided and public. The training pipeline of a VLLM encompasses several stages. Initially, an LLM undergoes pre-training using extensive text data such as LLaMA . Meanwhile, a vision preprocessor, e.g., CLIP , is pre-trained on a large number of image-text pairs. Subsequently, a VLLM is constructed based on the LLM and the vision preprocessor and is pre-trained using image-text pairs. The final stage involves instruction-tuning the VLLM, which can be performed using either image-text pairs or image-based question-answer data. In the instruction tuning stage of a VLLM, every data entry contains an image, a question, and the corresponding answer to the image. We use the answer text as member data and GPT-4 generated answers under the same question and same image as non-member data. Specifically, for LLaVA 1.5 and LLaMA-Adapter v2, we use the answers in LLaVA 1.5's instruction tuning as member data.

**VL-MIA/DALL-E.** MiniGPT-4, LLaVA 1.5, and LLaMA-Adapter V2 use images from LAION , Conceptual Captions 3M , Conceptual 12M  and SBU captions  datasets (collectively referred to as LAION-CCS) as pre-training data. BLIP  provides a synthetic dataset with image-caption pairs for LAION-CCS used in MiniGPT-4 and LLaVA 1.5. We first detect the intersection of the training images used in these three VLLMs. From this intersection, we randomly select a subset to serve as the member data for our benchmark. For the non-member data, we use the corresponding BLIP captions of the member images as prompts to generate images with DALL-E 23. This process yields one-to-one corresponding pairs of the generated images (non-member) and the original member images. Consequently, our dataset comprises an equal number of member images from the LAION-CCS dataset and non-member images generated by DALL-E, allowing us to evaluate MIA performance comprehensively. We have 592 images in VL-MIA/DALL-E in total.

**VL-MIA/Flickr.** MS COCO  co-occurs as a widely used dataset in the training data of the target models, so we use the images in this dataset as member data. Given the fact that such member data are collected from Flickr4, we filter Flickr photos by the year of upload and obtain new photos from January 1, 2024, as non-member data, which are later than the release of the target models. We additionally prepare a set of corrupted versions, where the member images are deliberately corrupted

  
**Dataset** & **Modality** & **Member data** & **Non-member data** & **Application** \\   &  &  &  & LLaVA 1.5 \\  & & & & & MiniGPT-4 \\  & & & & LLaMA-adapter v2 \\   &  & MS COCO (from Flickr) &  & LLaVA 1.5 \\  & & & & MiniGPT-4 \\  & & & & LLaMA\_adapter v2 \\   &  & LLaVA v1.5 instruction-tuning text & GPT-generated answers & LLaVA 1.5 \\  & & & MiniGPT-4 instruction-tuning text & GPT-generated answers & MiniGPT-4 \\   

Table 1: **Overview of VL-MIA dataset**: VL-MIA covers image and text modalities and can be applied for dominant open-sourced VLLMs.

to simulate real-world settings. More results of the corrupted versions are discussed in Section 6.5. This dataset contains 600 images.

**VL-MIA/Text.** We prepare text MIA datasets for the VLLMs instruction-tuning stage. LLaVA 1.5 and LLaMA Adapter v2.1 both use the LLaVA-Instruct-150K  in instruction-tuning, which consists of multi-round QA conversations. We first select entries with descriptive answers of 64 words. Next, we feed the corresponding questions and images into GPT-4\({}^{3}\), and ask GPT-4 to generate responses of the same length, treating these generated responses as non-member text data. In addition, since MiniGPT-4 employs long descriptions of images for instruction-tuning, typically beginning with "the image", we prompt GPT to generate descriptions based on the MS COCO dataset, starting with "this image" to ensure similar data distributions. We also prepare different versions of the datasets by truncating the text into different word lengths such as 16, 32, and 64. Each text dataset contains 600 samples.

**VL-MIA/Synthetic** We synthesize two new MIA datasets: **VL-MIA/Geometry** and **VL-MIA/Password**. The image in the VL-MIA/Geometry consists of a random 4x4 arrangement of geometrical shapes, and the image in the VL-MIA/Password consists of a random 6x6 arrangement of characters and digits from MNIST  and EMIIST . The associated text for each image represents its content (e.g., specific characters, colors, or shapes), ordered from left to right and top to bottom. Half of the dataset can be selected as the member set for VLLM fine-tuning, with the remainder as the non-member set. This partition ensures that members and non-members are independently and identically distributed, avoiding the latent distribution shift between the members and non-members in current MIA datasets . Our synthetic datasets are ready to use, and can be applied to evaluate any VLLM MIA methods through straightforward fine-tuning. We provide some examples of this dataset in Figure 5 in the Appendix C.

## 5 Method

### A cross-modal pipeline to detect image

VLLMs such as LLaVA and MiniGPT project the vision encoder's embedding of the image into the feature space of LLM. However, a major challenge for image MIA is that we do not have the ground-truth image tokens. Only the embeddings of images are available, which prevents directly transferring many target-based MIA from languages to images. To this end, we propose a token-level image MIA which calculates metrics based on the output logit of each token position.

This pipeline consists of two stages, as demonstrated in Figure 1. In _generation stage_, we provide the model with an image followed by an instruction to generate a textual sequence. Subsequently, in _inference stage_, we feed the model with the concatenation of the same image, instruction, and generated description text. During the attack, we correspondingly slice the output logits into image, instruction, and description segments, which we use to compute various metrics for MIAs. Our pipeline considers the information from the image, the instructions and the descriptions following the image. In practice, even if there is no access to the logits of the image feature and instruction slice, we can still detect the member image solely from the model generation. We visually describe a prompt example with different slice notations presented in Appendix A.2.

Our pipeline operates on the principle that VLLMs' responses always follow the instruction prompt , where the images usually precede the instructions and then always precede the descriptions. For causal language models used in VLLMs that predict the probability of the next token based on the past history , the logits at text tokens in the sequence inherently incorporate information from the preceding image.

### MaxRenyi MIA

We propose our MaxRenyi-K%, utilizing the Renyi entropy of the next-token probability distribution on each image or text token. The intuition behind this method is that if the model has seen this data before, the model will be more confident in the next token and thus have smaller Renyi entropy.

Given a probability distribution \(p\), the Renyi entropy  of order \(\), is defined as \(H_{}(p)=(_{j}(p_{j})^{}),0< <, 1\). \(H_{}(p)\) is further defined at \(=1,\), as \(H_{}(p)=_{}H_{}(p)\) by,* \(H_{1}(p)=-_{j}p_{j} p_{j}\),
* \(H_{}(p)=- p_{j}\).

To be more specific, given a token sequence \(X:=(x_{1},x_{2},,x_{L})\), let \(p^{(i)}()=(|x_{1},,x_{i})\) be the probability of next-token distribution at the \(i\)-th token. Let Max-K%\((X)\) be the top K% from the sequence \(X\) with the largest Renyi entropies, the MaxRenyi-K% score of \(X\) equals

\[(X)=(X)|}_{i(X)}H_{}(p^{(i)}).\]

When \(K=0\), we define the MaxRenyi-K% score to be \(_{i[L-1]}H_{}(p^{(i)})\). When \(K=100\), the MaxRenyi-K% score is the averaged Renyi entropy of the sequence \(X\).

In our experiments, we vary \(=,1,2\), and \(+\); \(K=0,10,100\). As \(\) increases, the top percentile of distribution \(p\) will have more influence on \(H_{}(p)\). When \(=1\), \(H_{1}(p)\) equals the Shannon entropy , and our method at \(K=100\) is equivalent to the Entropy . When \(=\), we consider the most likely next token probability . In contrast, Min-K%  only deals with the target next token probability. When the sequence is generated by the target model deterministically, i.e., when the model always generates the most likely next token, our MaxRenyi-K% at \(=\) is equivalent to the Min-K%.

We also extend our MaxRenyi-K% to the target-based scenarios, denoted by ModRenyi. We first consider linearized Renyi entropy, \(_{}(p)=(_{j}(p_{j})^{}-1 ),0<<, 1\). \(_{}(p)\) is also further defined at \(=1\), as \(_{1}(p)=_{ 1}_{}(p)=H_{1}(p)\). Assuming the next token ID is \(y\), recall that a small entropy value or a large \(p_{y}\) value indicates membership, we want our modified entropy to be monotonically decreasing on \(p_{y}\) and monotonically increasing on \(p_{j},j y\). Therefore, we propose the modified Renyi entropy on a given next token ID \(y\), denoted by \(_{}(p,y)\):

\[_{}(p,y)=-((1-p_{y})p_{y}^{| -1|}-(1-p_{y})+_{j y}p_{j}(1-p_{j})^{|-1|}-p_{j}).\]

Let \( 1\), we have \(_{1}(p,y)=_{ 1}_{}(p,y)=-_{j y }p_{j}(1-p_{j})-(1-p_{y}) p_{y}\), which is equivalent to the Modified Entropy. In addition, our more general method does not encounter numerical instability in Modified Entropy as \(p_{j} 0,1\) at \( 1\). For simplicity, we let the ModRenyi score be the averaged modified Renyi entropy of the sequence.

## 6 Experiments

In this section, we conduct MIAs across three target models using various baselines, MaxRenyi-K%, and ModRenyi. Experiment setup is provided in Section 6.1. The results on text MIAs and image MIAs are present in Section 6.2 and Section 6.3, respectively. In Section 6.4, we show that the proposed MIA pipeline can also be used in GPT-4. Ablation studies are present in Section 6.5. The versions and base models of VLLMs we use are listed in Table 7 of the appendices.

### Experimental setup

**Evaluation metric.** We evaluate different MIA methods by their AUC scores. AUC score is the area under the receiver operating characteristic (ROC) curve, which measures the overall performance of a classification model in all classification thresholds \(\). The higher the AUC score, the more effective the attack is. In addition to the average-case metric AUC, we also include the worst-case metric, the True Positive Rate at 5% False Positive Rate (TPR@5%FPR) in Appendix D suggested by .

**Baselines.** We take existing metric-based MIA methods as baselines and conduct experiments on our benchmark. We use the MIA method from , which compares the feature vectors produced by the original image with the augmented image. We use KL-divergence to compare the logit distributions and term it Aug-KL in this paper. We also use Loss attack , which is perplexity in the case of language models. Furthermore, we consider ppl/zlib and ppl/lowercase, which compare the target perplexity to zlib compression entropy and the perplexity of lowercase texts respectively.  proposes Min-K% method, which calculates the smallest \(K\%\) probabilities corresponding to the ground truth token. Min-K% is currently a state-of-the-art method to detect pre-training data of LLMs. For both Min-K%, and MaxRenyi-K%, we vary \(K=0,10,100\). In addition, we consider \(K=20\) for Min-K% as suggested in . We further include the max_prob_gap metric that can represent the extreme confidence in certain tokens by the model. That is, we subtract the second largest probability from the maximum probability in each token position and calculate the mean as metric.

### Image MIA

We first conduct MIAs on images using VL-MIA/Flickr and VL-MIA/DALL-E in three VLLMs. For the image slice, it is not possible to perform target-based MIAs, because of the absence of ground-truth token IDs for the image. However, our MIA pipeline presented in Figure 1 can still handle target-based metrics by accessing the instruction slice and description slice.

As demonstrated in Table 2, MaxRenyi-K% surpasses other baselines in most scenarios. An \(\) value of 0.5 yields the best performance in both VL-MIA/Flickr and VL-MIA/DALL-E. As \(\) increases, performance becomes erratic and generally deteriorates, though it remains superior to all target-based metrics. Overall, target-free metrics outperform target-based metrics for image MIAs. Another interesting observation is that instruction slices result in unstable AUC values, sometimes falling below 0.5 in target-based MIAs. This can be partially explained by the fact the model is more familiar with the member data. As a result, after encountering the first word "Describe", the model is more inclined to generate the description directly than generating the following instruction of \(X_{}\), i.e., "this image in detail". This is an interesting phenomenon that we leave to future research.

The performance of the image MIA model is influenced by its training pipelines. Recall that MiniGPT-4 only updates the parameters of the image projection layer in image training, and LLaMA Adapter v2 applies parameter-efficient fine-tuning approaches. In contrast, LLaVA 1.5 training updates both the parameters of the projection layer and the LLM. The inferior performance of MIAs on MiniGPT-4 and LLaMA Adapter compared to LLaVA 1.5 is therefore consistent with  that more parameters' updates make it easier to memorize training data.

We find that VL-MIA/DALL-E is a more challenging dataset than VL-MIA/Flickr, reflected in the AUC being closer to 0.5. In VL-MIA/DALL-E, each non-member image is generated based on the description of a member image. Therefore, member data have a one-to-one correspondence with non-member data and depict a similar topic, which makes it harder to discern.

### Text MIA

Text member data might be used in different stages of VLLM training, including the base LLM model pre-training and the later VLLM instruction-tuning. We hypothesize that after the last usage of the member data in its training, the more the model changes, the better the target-free MIA methods compared to target-based ones, and vice-versa. The heuristic is that if the model's parameters have changed a lot, target-free MIA methods, which use the whole distribution to compute statistics, are more robust than target-based methods, which rely on the probability at the next token ID. On the other hand, if the member data are seen in recent fine-tuning, the next token will convey more causal relations in the sequence remembered by the model, and thus target-based ones are better.

    &  &  \\   & 32 & 64 & 32 & 64 & 128 & 256 \\  Perplexity\({}^{*}\) & 0.779 & 0.988 & 0.542 & 0.505 & 0.553 & 0.582 \\ Perplexity\({}^{*}\)/plink\({}^{*}\) & 0.609 & 0.986 & 0.56 & 0.537 & 0.581 & 0.603 \\ Perplexity\({}^{*}\)/provenase\({}^{*}\) & **0.962** & 0.977 & 0.493 & 0.518 & 0.503 & 0.583 \\ Min\_0\% Prob\({}^{*}\) & 0.522 & 0.522 & 0.455 & 0.451 & 0.425 & 0.448 \\ Min\_10\% Prob\({}^{*}\) & 0.461 & 0.883 & 0.468 & 0.487 & 0.526 & 0.534 \\ Min\_20\% Prob\({}^{*}\) & 0.603 & 0.890 & 0.505 & 0.498 & 0.549 & 0.562 \\ Max\_Prob\_Gap & 0.461 & 0.545 & 0.574 & 0.544 & 0.565 & 0.629 \\  ^{*}\)} & \(=0.5\) & 0.809 & 0.979 & 0.557 & 0.500 & 0.536 & 0.567 \\  & \(=1\) & 0.808 & **0.993** & 0.544 & 0.503 & 0.546 & 0.567 \\   & \(=2\) & 0.779 & 0.963 & 0.559 & 0.497 & 0.529 & 0.560 \\   & Max\_0\% & 0.506 & 0.514 & 0.541 & 0.515 & 0.489 & 0.571 \\   & Max\_10\% & 0.458 & 0.776 & 0.518 & 0.525 & 0.608 & 0.65 \\   & Max\_10\% & 0.564 & 0.535 & 0.555 & 0.531 & 0.6 & 0.631 \\   & Max\_0\% & 0.552 & 0.579 & 0.566 & 0.571 & 0.603 & 0.668 \\   & Max\_10\% & 0.566 & 0.809 & 0.553 & 0.541 & 0.623 & 0.65 \\   & Max\_100\% & 0.554 & 0.750 & 0.544 & 0.523 & 0.588 & 0.621 \\   & Max\_0\% & 0.589 & 0.625 & 0.594 & 0.606 & 0.659 & 0.657 \\   & Max\_10\% & 0.607 & 0.787 & 0.583 & 0.556 & 0.629 & 0.663 \\   & Max\_100\% & 0.553 & 0.709 & 0.592 & 0.576 & 0.568 & 0.649 \\   & Max\_0\% & 0.600 & 0.638 & **0.607** & **0.615** & **0.688** & **0.669** \\   & Max\_10\% & 0.618 & 0.763 & 0.586 & 0.548 & 0.627 & 0.667 \\   & Max\_100\% & 0.557 & 0.694 & 0.546 & 0.527 & 0.584 & 0.634 \\   

Table 3: **Text MIA. AUC results on LLaVA.**

**MIA on VLLM instruction-tuning texts** We detect whether individual description texts appear in the VLLMs instruction-tuning. We use the description text dataset VL-MIA/Text of lengths (32, 64), constructed in Section 4. We present our results in the first column of Table 3. We observe that target-based MIA methods are significantly better than target-free ones, confirming our hypothesis.

**MIA on LLM pre-training texts.** We use the WikiMIA benchmark , which leverages the Wikipedia timestamp to separate the early Wiki data as the member data, and recent Wiki data as the non-member data. The early Wiki data are used in various LLMs pre-training . We use WikiMIA of different lengths (32, 64, 128, 256), and expect the membership of longer sequences will be more easily identified. We present our results in the second column of Table 3. We observe that on LLaVA, our target-free MIA methods on large \(\) consistently outperform target-based MIA methods, which

    &  \\   &  &  &  \\   & img & inst & desp & inst+desp & img & inst & desp & inst+desp & inst & desp & inst+desp \\  Perplexity\({}^{*}\) & N/A & 0.378 & 0.667 & 0.559 & N/A & 0.414 & 0.649 & 0.497 & 0.380 & 0.661 & 0.425 \\ Min.0\% Prob\({}^{*}\) & N/A & 0.357 & 0.651 & 0.357 & N/A & 0.272 & 0.569 & 0.274 & 0.462 & 0.566 & 0.463 \\ Min.10\% Prob\({}^{*}\) & N/A & 0.357 & 0.669 & 0.390 & N/A & 0.272 & 0.603 & 0.265 & 0.437 & 0.591 & 0.438 \\ Min.20\% Prob\({}^{*}\) & N/A & 0.374 & 0.670 & 0.370 & N/A & 0.293 & 0.628 & 0.303 & 0.437 & 0.611 & 0.424 \\ Aug.\&KL & & 0.596 & 0.539 & 0.492 & 0.508 & 0.462 & 0.458 & 0.438 & 0.435 & 0.428 & 0.422 & 0.427 \\ Max.Prob.Gap & & 0.577 & 0.601 & 0.650 & 0.650 & 0.650 & 0.664 & 0.695 & 0.609 & 0.626 & 0.475 & 0.671 & 0.661 \\  ^{*}\)} & \(=0.5\) & N/A & 0.368 & 0.651 & 0.614 & N/A & 0.483 & 0.636 & 0.592 & 0.430 & 0.662 & 0.555 \\  & \(=1\) & N/A & 0.359 & 0.659 & 0.502 & N/A & 0.371 & 0.635 & 0.417 & 0.394 & 0.646 & 0.423 \\  & \(=2\) & N/A & 0.370 & 0.645 & 0.611 & N/A & 0.492 & 0.636 & 0.605 & 0.434 & 0.665 & 0.579 \\   & Max.\% & 0.515 & 0.689 & 0.687 & 0.689 & 0.437 & 0.624 & 0.542 & 0.626 & 0.497 & 0.570 & 0.499 \\  & Max.10\% & 0.557 & 0.689 & 0.691 & 0.719 & 0.493 & 0.624 & 0.592 & 0.707 & 0.432 & 0.573 & 0.622 \\  & Max.100\% & **0.702** & **0.726** & **0.713** & 0.728 & **0.671** & **0.795** & **0.664** & 0.724 & **0.633** & 0.674 & **0.697** \\   & Max.\% & 0.503 & 0.708 & 0.688 & 0.725 & 0.429 & 0.645 & 0.579 & 0.652 & 0.517 & 0.602 & 0.517 \\  & Max.10\% & 0.623 & 0.708 & 0.698 & **0.743** & 0.489 & 0.645 & 0.627 & 0.710 & 0.456 & 0.610 & 0.565 \\  & Max.100\% & 0.702 & 0.720 & 0.702 & 0.721 & 0.626 & 0.776 & 0.662 & **0.741** & 0.597 & **0.678** & 0.687 \\   & Max.\% & 0.583 & 0.682 & 0.673 & 0.705 & 0.444 & 0.697 & 0.587 & 0.665 & 0.580 & 0.617 & 0.581 \\  & Max.100\% & 0.682 & 0.682 & 0.685 & 0.725 & 0.482 & 0.697 & 0.621 & 0.734 & 0.499 & 0.615 & 0.584 \\  & Max.100\% & 0.682 & 0.694 & 0.683 & 0.703 & 0.627 & 0.785 & 0.656 & 0.735 & 0.572 & 0.670 & 0.668 \\   & Max.\% & 0.588 & 0.646 & 0.651 & 0.674 & 0.462 & 0.693 & 0.569 & 0.657 & 0.604 & 0.566 & 0.603 \\  & Max.100\% & 0.593 & 0.646 & 0.669 & 0.699 & 0.488 & 0.693 & 0.603 & 0.704 & 0.506 & 0.591 & 0.584 \\  & Max.100\% & 0.669 & 0.673 & 0.677 & 0.687 & 0.632 & 0.769 & 0.649 & 0.725 & 0.564 & 0.661 & 0.659 \\    \\   &  &  &  \\   & img & inst & desp & inst+desp & img & inst & desp & inst+desp & inst & desp & inst+desp \\  Perplexity\({}^{*}\) & N/A & 0.338 & 0.564 & 0.448 & N/A & 0.356 & 0.517 & 0.421 & 0.491 & 0.577 & 0.506 \\ Min.0\% Prob\({}^{*}\) & N/A & 0.482 & 0.559 & 0.482 & N/A & 0.422 & 0.494 & 0.421 & 0.448 & 0.554 & 0.448 \\ Min.10\% Prob\({}^{*}\) & N/A & 0.482 & 0.563 & 0.425 & N/A & 0.422 & 0.495 & 0.462 & 0.447 & 0.556 & 0.455 \\ Min.20\% Prob\({}^{*}

again confirms our hypothesis since the base LLM model of LLaVA has full parameter fine-tuning from LLaMA-2 and thus changed a lot.

### Image MIA on GPT-4

In this section, we demonstrate the feasibility of image MIAs on the closed-source model GPT-4. Our experiments use two image datasets: VL-MIA/Flickr and VL-MIA/DALL-E, detailed in Section 4. We choose GPT-4-vision-preview API, which was trained in 2023 and likely does not see the member data in either dataset. We randomly select \(200\) images per dataset and prompt GPT-4 to describe them in 64 words. We then apply MIAs based on the generated descriptions. Since GPT-4 can only provide the top-five probabilities at each token position, we can not directly use the proposed MaxRenyi-K% that requires the whole probability distribution. To address this issue, we assume the size of the entire token set is \(32000\) and the probability of the remaining \(31995\) tokens are uniformly distributed. The AUC results are present in Table 4. We omit the result of perplexity and Min-K% since they are equivalent to MaxRenyi-K% with \(=\) in the greedy-generated setting, as discussed in Section 5.2. Surprisingly, we observe that in VL-MIA/DALL-E, the best-performed method MaxRenyi-K% (\(=0.5\)) can achieve an AUC of \(0.815\). This indicates a high level of effectiveness for MIAs on GPT-4, demonstrating the potential risks of privacy leakage even with closed-source models.

### Ablation study

**Does the length of description affect the image MIA performance?** We conduct ablation experiments on LLaVA 1.5 targeting the length of generated description texts with MaxRenyi-10%. In the generation stage, we restrict the max_new_tokens parameter of the generate function to (32, 64, 128, 256) to obtain description slices of different lengths. As presented in Figure 1(a), when the length of the description increases, the AUC of the MIA becomes higher and enters a plateau when max_new_tokens reaches 128. This may be because a shorter text contains insufficient information about the image, and in an excessively long text, words generated later tend to be more generic and not closely related to the image, thereby contributing less to the discriminative information that helps discern the membership.

**Can we still detect corrupted member images?** The motivation is to detect whether sensitive images are inappropriately used in VLLM's training even when the images at hand may get corrupted. We leverage ImageNet-C  to generate corrupted versions of member data in VL-MIA/Flickr: Snow, Brightness, JPEG, and Motion_Blur, with the parameters in Table 8. The corrupted examples and corresponding model output generations are demonstrated in Appendix C Table 17 and Table 18. We take MaxRenyi-K% (\(\) = 0.5) as the attacker and the results of LLaVA are presented in Figure 1(b). Corrupted member images make MIAs more difficult, but can still be detected successfully. We also observe that reducing model quality (JPEG) or adding blur (Motion_Blur) degrade MIA performance more than changing the base parameter (Brightness) or overlaying texture (Snow).

**Can we use different instructions?** We conduct image MIAs on VL-MIA/Flickr with LLaVA through three different instruction texts: "Describe this image concisely.", "Please introduce this painting.", and "Tell me about this image.". We present our results in Table 14 of the appendix. Our pipeline successfully detects member images on every instruction, which indicates robustness across different instruction texts.

    & **VL-MIA/ VL-MIA/** \\  & DALL-E & Flickr \\  ^{*}\)Max\_Prob\_Gap} & 0.807 & 0.520 \\  & 0.516 & 0.486 \\   & 0.697 & 0.571 \\  & 0.749 & 0.604 \\  & Max\_100\% & **0.815** & 0.605 \\   & 0.688 & 0.572 \\  & Max\_100\% & 0.747 & 0.591 \\  & Max\_100\% & 0.790 & **0.630** \\   & 0.678 & 0.572 \\  & Max\_100\% & 0.723 & 0.574 \\   & Max\_100\% & 0.786 & 0.595 \\   & 0.685 & 0.561 \\   & Max\_100\% & 0.708 & 0.549 \\   & Max\_100\% & 0.781 & 0.583 \\   

Table 4: **Image MIA on GPT-4.**

## 7 Conclusion

In this work, we take an initial step towards detecting training data in VLLMs. Specifically, we construct a comprehensive dataset to perform MIAs on both image and text modalities. Additionally, we uncover a new pipeline for conducting MIA on VLLMs cross-modally and propose a novel method based on Renyi entropy. We believe that our work paves the way for advancing MIA techniques and, consequently, enhancing privacy protection in large foundation models.