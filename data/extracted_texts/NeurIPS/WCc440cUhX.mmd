# Understanding Transformers via \(N\)-gram Statistics

Timothy Nguyen

Google DeepMind

timothycnguyen@google.com

###### Abstract

Transformer based large-language models (LLMs) display extreme proficiency with language yet a precise understanding of how they work remains elusive. One way of demystifying transformer predictions would be to describe how they depend on their context in terms of simple template functions. This paper takes a first step in this direction by considering families of functions (i.e. rules) formed out of simple \(N\)-gram based statistics of the training data. By studying how well these rulesets approximate transformer predictions, we obtain a variety of novel discoveries: a simple method to detect overfitting during training without using a holdout set, a quantitative measure of how transformers progress from learning simple to more complex statistical rules over the course of training, a model-variance criterion governing when transformer predictions tend to be described by \(N\)-gram rules, and insights into how well transformers can be approximated by \(N\)-gram rulesets in the limit where these rulesets become increasingly complex. In this latter direction, we find that for 79% and 68% of LLM next-token distributions on TinyStories and Wikipedia, respectively, their top-1 predictions agree with those provided by our \(N\)-gram rulesets.

## 1 Introduction

This paper is an attempt to answer the following

**Question:**_How does a transformer-based large language model (LLM) make use of its context when predicting the next token?_

Our approach proceeds via studying the statistical properties of training data. This is perhaps the most natural place to start even though it is not exhaustive (e.g. it does not include in-context learning ). The reasons to understand LLM behavior in terms of the statistics of their training data are plenty. First, the functional form of how LLMs use their training data is not well-understood (though there has been progress on understanding memorization ). Second, the over-reliance of LLMs on training data statistics leads to brittleness (e.g. the "reversal curse" ) and the perpetuation of dataset biases . Understanding the nature of this statistical dependence can lead to improved and more informed dataset curation and training methods. Finally, in various scenarios, the performance of LLMs on downstream tasks are found to be correlated with frequency of relevant training data . A better understanding of this phenomenon would allow better steering of models towards desired performance levels.

We can think of the complexity of an LLM next token prediction (regarded as a probability distribution over tokens) along two axes: form and selection. Form refers to the functional form of the prediction as a function of the context, e.g. whether the prediction is some explicit function of associated training data statistics (see Figure 1). Selection refers to which functional form, chosen from a set of functional templates, suitably describes the transformer prediction (supposing the choice set is sufficiently rich). As a first nontrivial step, one might hope that an approximate model for an LLM is that each of its next token predictions can be roughly described by simple statistical rules fromthe context (simple form) even if the mechanism for its rule selection remains hidden (complex selection)1. This paper is an attempt to see how far this perspective can be pushed, and fortuitously we obtain additional insights for understanding LLM behavior along the way. The statistical rules we consider, which are based on \(N\)-grams, are defined in Section 4, with Figure 1 showing some examples.

We perform our main investigations on the TinyStories  dataset, with supporting experiments on Wikipedia to confirm our results remain robust at larger scales. The use of TinyStories is for practical reasons: its small size makes training models and aggregating \(N\)-gram statistics computationally efficient, yet it is complex enough to capture basic natural language statistics (those occurring in simple children's stories).

Below is a summary of our observations and contributions:

1. (Approximation Criterion) We observe that next token LLM predictions tend to be well-approximated by \(N\)-gram rules when the predictions have low variance across different training runs2. In particular, this includes predictions conditioned on contexts with sufficiently high count in the training data. (Section 5) 2. (Curriculum Learning Dynamics) By grouping our \(N\)-gram rulesets in terms of complexity (as measured by the amount of context they use), we discover the various ways in which the learning dynamics of LLMs implement a statistical type of curriculum learning, in which easier rules are eventually supplanted by more complex ones. (Section 6.1) 3. (Overfitting Criterion) Based on our analysis of approximating LLM predictions by \(N\)-gram rules, we propose a simple and novel procedure for detecting overfitting of LLMs during training. The procedure makes no use of holdout data and it makes quantatively precise the intuition that overfitting corresponds to a model memorizing long context at the expense being able to generalize through making use of subcontext. (Section 6.2) 4. (Approximation Strength) We study how well LLM predictions can be approximated by our \(N\)-gram rulesets, noting that significant gains in top1-accuracy occur as we increase ruleset

Figure 1: **Illustration of rule approximation. Given a context, different \(N\)-gram based rules formed out of the context will yield different next-token predictive distributions. In the above example, the context consists of three tokens. The first rule uses all three tokens of the context and makes a prediction based on the corresponding \(4\)-gram rule derived from the training data; the second rule uses only the first and last tokens to form a corresponding \(3\)-gram rule (and so the next token “slept” will be assigned less weight than the first rule since the “tired” token is ignored); and the third rule makes a prediction using the \(N\)-gram statistics obtained from aggregating over three token contexts from the training data where the second token is arbitrary (i.e. the second token is marginalized). Given a list of such rules, one can ask which rule’s predictive distribution best matches that of the transformer.**

complexity and diversity, whereby we achieve up to 79% top1-accuracy on TinyStories (Tables 2 and 5). We also visually ground these approximations with concrete examples (Figure 5), which may form the basis for dataset attribution methods in future work. Corresponding experiments on Wikipedia are shown in the Appendix. (Section 7)

We also open source our training datasets and related \(N\)-gram statistics so that others can verify and build upon our work.3

## 2 Related Work

Rule extraction methods for neural networks have been studied in quite different settings, e.g. [15; 21]. Some recent works have performed \(N\)-gram analyses for large-language models in the setting of in-context learning [1; 23] and associative recall . The "infini-gram" model  compares LLM predictions with the single \(N\)-gram rule given by retrieving the largest possible matching context from the training data. Our work uses shorter but more sophisticated \(N\)-gram rules. In , an approach to understanding how LLMs process \(N\)-grams is carried out at the level of individual neurons. This complements our dataset-based work, which treat models as a black box. See also  which studies how transformers can represent \(N\)-gram models. In , the evolution of the type of \(N\)-gram statistics that transformers learn during training is analyzed in the setting of synthetic Markov chain data, in contrast to our natural language setting. Other works studying the learning trajectory of language models include [7; 8]. There is a large literature on building more sophisticated \(N\)-gram models, e.g. [18; 13]. Such models could have been incorporated into our set of rules, but for simplicity we choose not to include them.

## 3 Experimental Setup

We train standard decoder-only transformer models on the TinyStories  dataset (480M tokens) consisting of children's stories synthetically generated from GPT-4 using templated prompts. The value of this dataset lies in its linguistic simplicity, whereby it is possible to model language well on the dataset using very small models. Unless stated otherwise, our experiments use a 160M parameter model trained for 4 epochs, which achieves a loss of around 1.11 nats on the validation set. We train for 4 epochs since we use learning rate warmup and cosine learning rate decay and we want to ensure all datapoints receive updates with a high learning rate (this way all \(N\)-gram statistics have a fair chance of being learned during training). For overfitting experiments in Section 6.2, we train a 1.4B model for 10 epochs. In the Appendix, we include additional corresponding experiments on Wikipedia (from MassiveText ) with a single epoch of training in order to validate that our results are of a general nature and extend to more complex datasets. For a fixed dataset, the only source of randomness among different runs are different dataset shuffles. Full experimental details are described in the Appendix.

## 4 \(N\)-Gram Rules

The attention layer within a transformer is in essence a soft context-selection mechanism. The \(N\)-gram rules we consider will be loosely modeled on this mechanism. Namely, given a context we will form a derived context in which each token will either be kept, discarded, or marginalized, which is meant to mimic positive attention, no attention, and semantic invariance4, respectively. More formally, we proceed as follows:

Given a regular expression5\(\), all contexts from the training data can be retrieved which match the regular expression. This allows us to define a corresponding rule that defines for us a distribution over tokens \(t\):

\[R_{}(t)=}{\#\{*\}}\] (1)

where the numerator and denominator are the counts for the \(N\)-grams from the training data matching the concatenated regular expressions \( t\) and \(*\), respectively, where \(*\) is wildcard (single) character match6. (Thus the \(N\)-grams in the numerator end with \(t\) while those in the denominator can end with any token.) Observe that the next-token predictions of a vanilla \(N\)-gram model are obtained by letting \(R_{}(t)\) vary over all ordinary token sequences \(\) of length \(N-1\).

Given \(\), a symbol from the the alphabet \(\{*,-,+\}\), consider the following operation which maps a token \(t\) to a regular expression:

\[S_{}(t)=t&=+\\ *&=*\\ &=-\] (2)

where \(\) is the empty regular expression. Given now a sequence \(=_{-N}_{-2}_{-1}\), define \(S_{}\) on a sequence of tokens \(C=C_{-N} C_{-2}C_{-1}\) by tokenwise application of (2) and concatenation7:

\[S_{}(C)=S_{_{-N}}(C_{-N}) S_{_{-2}}(C_{-2})S_{_ {-1}}(C_{-1}).\] (3)

Thus (3) defines a regular expression which we can think of as fuzzy matching for a subset of a context \(C\) (the fuzziness arising from the presence of wildcard matches). For notational convenience, we assume \(\) is left padded with \(-\) symbols, so that we can define \(S_{}(C)\) for \(()<(C)\). Finally, define

\[R_{}(t|C)=R_{S_{}(C)}(t)\] (4)

for \(C\) with \((C)()\). The collection of (4) for various \(\) defines our \(N\)-gram rules under consideration8. Each such rule is a function which maps a context \(C\) to a next token distribution. We refer to \(S_{}(C)\) as the rule context for \(R_{}(t|C)\).

As concrete examples, let \(=+-*+\). If \(C=C_{-5}C_{-4}C_{-3}C_{-2}C_{-1}\), then \(S_{}(C)=C_{-4}*C_{-1}\) and

\[R_{+-*+}(t|C)=*C_{-1}t\}}{\#\{C_{-4}*C_{-1}*\}}\] (5)

is a rule which yields a next token distribution based on a particular combination of \(4\)-gram model statistics: it retrieves all three token contexts in the training data whose first token is \(C_{-4}\) and last token is \(C_{-1}\) and marginalizes over the second token. Likewise, the rules

\[R_{++--}(t|C)=C_{-3}t\}}{\#\{C_{-4}C_{-3}*\}} R_{++**}= C_{-3}**t\}}{\#\{C_{-4}C_{-3}***\}}\] (6)

are respectively a trigram model with context \(C_{-4}C_{-3}\) (all other tokens receiving a \(-\) are dropped) and a model which uses four tokens of context but marginalizes over the two most recent ones.

When \(\) consists of all \(+\) symbols, we get vanilla \(N\)-gram rules derived from the suffix of \(C\). When \(\) consists of \(\) symbols, we get vanilla \(N\)-gram rules derived from subsets of \(C\). Varying the length and the entries of \(\) yields the following rulesets9:

\[_{M}^{} =\{R_{}(t|):|| M,_{i}=+\}\] (7) \[_{M}^{} =\{R_{}(t|):|| M,_{i}=\}\] (8) \[_{M}^{} =\{R_{}(t|):|| M\}.\] (9)

The parameter \(M\) controls how much of the context is being used for the rules.

## 5 Approximating Transformer Predictions with Rules

Let \(p(t|C)\) denote the next-token distribution of an LLM conditioned on the context \(C\) and for notational similarly, write \(p_{r}(t|C)\) for \(r(t|C)\), where \(r\) is one of the rules defined in Section 4. We wish to measure how similar these distributions are (higher similarity corresponds to a better rule description). To that end, we use the variational distance to measure the difference of two distributions (we discuss our choice and others in the Appendix):

\[d(p,q)=_{}|p_{}-q_{}|,\] (10)

where the summation is over the vocabulary index (i.e. the components of the probability vectors). Since variational distance may be lacking in concrete interpretability, we will sometimes use top1-accuracy to measure similarity, defined by

\[(p,q)=(p)(q)|}{|(p)(q)|}\] (11)

(in general, the argmax of a probability distribution is a set due to potential ties among maximal probabilities). When the argmaxes in (11) are singletons, top1-accuracy just measures agreement between greedy predictions.

Given a context \(C\), we want to understand how \(d(p(t|C),p_{r}(t|C))\) varies with different rules \(r\) and in particular if it can be made small. To that end, we introduce some terminology:

We are interested in determining the optimal rule \(p_{r}(t|C)\) (as defined in Table 1) and if it has small optimal rule distance then we regard the rule as being a good description of the corresponding transformer predictions.10 As a first step, note there is a distinguished rule

\[p_{}(t|C)=}{\#\{C*\}}\] (12)

whose rule context is the full unmodified context \(C\).11 This is because (roughly) the language-modeling objective aims to make \(p(t|C)\) similar to \(p_{}(t|C)\).12 All other rules in our rulesets are "subleading" in that they drop or marginalize over tokens in the context \(C\). Our goal is to quantify which rules, either (12) or subleading ones, are optimal rules and what their optimal rule distances are.

One of our main findings is an _approximation criterion_: contexts that have low model-variance tend to have low optimal rule distance. In particular, this includes contexts with sufficiently high frequency in the training data. The latter situation is to be expected: the more often a context \(C\) occurs in the

  _optimal rule distance_: the minimum (possibly averaged over runs) distance between LLM predictions and rule predictions \\  _optimal rule_: a rule achieving the optimal distance & \(}{}\ _{t}d(p^{(i)}(t|C),p_{r}(t|C))\) \\  _model variance_: the average of the pairwise distance between LLM predictive distributions over different runs & \(}\ d(p^{(i)}(t|C),p^{(j)}(t|C))\) \\  

Table 1: **Terminology associated to a context \(C\)**. Here \(\) is some reference ruleset under consideration. The superscript on \(p^{(i)}(t|C)\) is meant to denote the predictions of the \(i\)th model. In Section 5, we consider rules that are fixed across model runs (where we have five models) whereas elsewhere we will only have a single model (and thus optimal rules will be model specific).

training data, the more the minimization of the cross entropy loss objective encourages the network to make predictions close to \(p_{}(t|C)\).

The novel aspect of our approximation criterion is the _sufficiency_ of low model-variance situation even in cases when the context is rare.13 We present the case of \(7\)-gram contexts in Figure 2 to corroborate the approximation criterion, with additional examples relegated to the Appendix. We sample around six-thousand \(7\)-grams from the training data, sampling from logarithmically spaced buckets based on counts, and plot various relations between counts, model variances, and rule distances. For simplicitly, we consider the ruleset \(=_{7}^{}\) to limit the number of rules under consideration. Our analysis of Figure 2 can be summarized as follows:

Plot (a) shows how with increasing count of the number occurrences of the context \(C\) in the training data, LLM predictions become nearer to \(p_{}(t|C)\), which in this case, is the vanilla \(8\)-gram rule. Nevertheless, for all but the highest of counts, we have a large spread of distances: even for unique \(7\)-gram contexts, some predictions are well-approximated by \(p_{}(t|C)\) while others are close to having disjoint-support (distance equal to \(1\)). Plot (c) also shows that while model variance decreases with count of the context (as expected) we have a large spread of model variances for contexts with intermediate or low counts. Since the contexts whose predictions have high model variance can be regarded as "noise", one can ask whether those contexts with low model variance have some structure. Plots (b) and (c) address this question. While for (b), we see that the 8gram-rule has a large spread when plotted against model variance, there is a significant reduction in outliers in (d) when the y-axis is the optimal distance to rules in \(_{7}^{}\). Concretely, the transition from (b) to (d) is a way of visualizing LLMs performing back-off, whereby LLMs rely on statistics from subsets of the context. Moreover, the lower left portion of (d) is a manifestation of our approximation criterion: contexts that yield consistent predictions across model runs (i.e. sufficiently low model variance) are indicative of rule-like behavior (in this case, good approximation with a suffix \(N\)-gram rule formed out of the context).

We believe our approximation criterion and its corresponding analyses have significance beyond the experiments carried out here since they (i) highlight that naive count-based statistics do not provide the strongest signal in terms of how LLMs leverage dataset statistics (since as Figure 2(a) shows, high count can still have high model variance) (ii) suggest that LLM predictions that have low-variance are likely the ones that are amenable to description (or even explanation) by some underlying dataset statistic (with high-variance predictions being regarded as noise). We leave a more systematic exploration of (ii) to future work.

## 6 Learning Dynamics

### Curriculum Learning

We can track how well LLM predictions are described by our \(N\)-gram rules over the course of training by tracking optimal rule distance as a function of train step. Here optimal rule distance is defined as in Table 1 with \(\) any of the rulesets (7)-(9), and we will measure how optimal rule distances vary with maximum context length \(M\) (the resulting analyses are similar for "all", "subgram", and "suffix" rules so we show our analysis for "all").

Figure 3 summarizes our results. Early in training, LLM predictions acquire structure and thus become approximable by rule predictors. However, with further training, LLM predictions eventually diverge from simpler rules (small context length) while continuing to increase in similarity with more complex rules (larger context length). Moreover, the rightmost plot of Figure 3 shows that top1-acc\((p_{}(t|C),p_{r}(t|C))\) increases over the course of training for optimal \(r_{M}^{}\) (for \(M>1\)), where \(p_{}\) is the ground-truth distribution regarded as a one-hot distribution, showing that the rule selection improves with time. Altogether, this shows that LLMs undergo a curriculum style learning, in which their predictions gradually move away from simpler rules to more complex and effective rules.

### Early Stopping Criterion

Our investigations of approximating LLMs with rules given by limited contexts naturally lead us to consider LLMs with limited context. The latter have predictive distributions given by

\[p_{n}(t|C)=p(t|C_{-n} C_{-1})\] (13)

where \(n\) is the maximum context length. In Figure 4, we plot the loss of an LLM trained to overfit (train loss decreases while validation loss increases) along with its limited context versions for \(1 n 7\). For the limited context models with \(n>1\), we see that on _both_ the train and validation set, the two respective loss curves track each other closely and both eventually go up. This suggests the following picture: an overfitting LLM is spending capacity to minimize train loss by memorizing the full context at the expense of using capacity to learn statistics of subcontext, i.e. the reduced context in (13). This manifests itself both during training (where subcontext arises from a subset of a larger memorized context) and during validation (where subcontext arises from the partial overlap between novel context and the train set).

Our discovery suggests a simple and computationally inexpensive early stopping criterion: during training, evaluate the transformer on train data consisting of short contexts and when this quantity begins increasing, stop training. Significantly, this method involves no holdout set and is a training dataset intrinsic criterion.

Figure 2: **TinyStories \(7\)-grams**. Every point in the above plots represents a \(7\)-gram context. Shaded regions are obtained by bucketing along the x-axis and computing one standard deviation within the mean along the y-axis. Slope and \(R^{2}\) values of plots are with respect to the linear fit of the data. Optimal rule distances and model variances are computed with respect to five model runs. _(a)_: \(d(p(t|C),p_{}(t|C))\) vs count of \(C\). _(b)_: \(d(p(t|C),p_{}(t|C))\) vs model variance. _(c)_: model variance vs count of \(C\). _(d)_: similar to (b) but now the y-axis is optimal rule distance of the optimal rule from \(_{7}^{}\). Model size: 160M.

## 7 Rule Peformance

Finally, addressing our main question from the introduction, we track how well our rulesets describe LLM predictions (in the sense of Section 5) as a whole at inference time. Here, the utility of our \(N\)-gram rules defined in Section 4 becomes apparent, since on a holdout set, there will be novel contexts and being able to drop or marginalize context tokens aid in being able to retrieve or aggregate corresponding training dataset statistics. In Table 2, we show the average top1-accuracy between the optimal rule from our various rulesets and LLM predictions on 100 random stories from the validation set. Here, we include as baseline backoff\({}_{M}\), the single rule given by the predictive model which performs "stupid backoff"  using \(M\) tokens of context.14

We see significant gains in accuracy at large \(M\) when adding additional types of rules. In the end, we are able to obtain 78% top1-accuracy between the per-prediction optimal rule and the LLM predictions, averaged over all tokens. This is perhaps a remarkably high figure, considering that the top1 accuracy of the model with respect to the ground truth on the validation set is 69.6%. At minimum, we have provided a precise quantification of structure in LLM next-token predictions: they are often matched (as measured by top token prediction) by some simple \(N\)-gram rule derived from

Figure 4: **Overfitting Detection. We plot both train loss (solid lines) and validation loss (dashed lines) for the full transformer and limited context length transformers (the latter are marked with an “x” for emphasis) on TinyStories. Unlike the full transformer which overfits, those with limited context length have train and validation loss curves closely following each other. Model size: 1.4B.**

Figure 3: **Training Dynamics.**_Left:_ Models reach their lowest distance to more complex rules later in training. For rules with four tokens of context or less, the variational distance eventually starts increasing later in training. For six and seven tokens of context, the variational distance continues to decrease. _Center & Right:_ The optimal rule selected always has nonincreasing distance and nondecreasing top1-accuracy relative to the ground truth (interpreted as a one-hot distribution \(p_{}\)), despite distances to model predictions eventually increasing or plateauing for rules with less than six tokens of context. This shows that the optimal rule selection is improving with additional training even if the optimal rule distance with respect to model predictions is not improving. (One can imagine the rule predictions as a mesh in probability space, with LLM predictions navigating this space through training. The distance to the mesh may plateau but which rule is closest can continue to change.) Model size: 160M.

the training data. See Section D.1 for some supplementary analysis. Using the \(L^{}\) distance instead of the variational distance gives us a slightly higher result of 79%, see Table 5.

To ground our rule optimization procedure, we provide Figure 5 which shows side-by-side how LLM predictions compare with ground truth and optimal rule predictions in an example heldout story.

Figure 5: **Rule selection for a TinyStories validation sequence.** The above is a sequence from a heldout story. In the second and third columns are the ground truth, token by token, along with the rule context (as defined in Section 4) associated to the optimal rule from \(^{}_{7}\). The heatmap indicates the variational distance between optimal rule and LLM next token distributions at the given token. The first column shows at most two tokens, which are chosen as follows: If the LLM top-1 prediction disagrees with the ground truth, the LLM prediction is shown. If in addition, the rule selected makes a different top-1 prediction from the transformer, that token is shown as the second token and the corresponding ground truth token is colored red. Thus red tokens are precisely the locations of disagreement between LLM and optimal rule greedy predictions. The last column shows the number of contexts supporting the optimal rule. Model size: 160M.

For instance, for the target token "climb" in "... Roxy loved to climb", both the LLM and optimal rule \(R_{}\) predict "play", where \(=\) "... * loved to". For target token "climb" in "... She climbed", the LLM predicts "would" whereas the ground truth and \(R_{}\) predict "climb", where \(=\) "loved to climb * She". In general, optimal rules provide the closest statistical match from the training data to the given LLM predictive distributions (from amongst our rulsets), and their top1-predictions can agree or disagree agree (as indicated by target token color). Additional examples, including those from Wikipedia, are shown in Section D. For interpretability purposes, we re-emphasize that our optimal rules currently only provide descriptions, not explanations. We leave the possibility of the latter for future work.

## 8 Conclusions and Limitations

Our work provides quantitative measures of how well the predictions of transformer-based LLMs are described (i.e. approximated) by simple \(N\)-gram rules. Such rules were motivated by the simplest token-level operations applied to the context (keep, ignore, or marginalize). The results we obtained in Section 7 imply that, at least on simple datasets like TinyStories and Wikipedia, LLM predictions contain much quantifiable structure insofar that they often can be described in terms of our simple statistical rules. Along the way, we also obtained novel discoveries into the statistical nature of overfitting, the occurrence of curriculum learning, and the relation between model-variance and approximability by \(N\)-gram rules. Altogether then, our work provides various avenues of progress in understanding how simple dataset statistics are reflected in LLM behavior.

On the other hand, it is intuitively clear that current state-of-the-art LLMs go well beyond invoking \(N\)-gram rules. A typical request to perform a nontrivial task (e.g. "Write a thirty line poem about mathematics that rhymes") requires a high-level conceptual understanding of language that goes beyond simple literal token-level associations between the context and the training data that we consider here. Nevertheless, one can speculate that an analogue of our work could still apply: in general, an LLM might be performing some high-level rule application, whereby statistics formed out of distributional categories  instead of individual tokens are leveraged from the context. Formulating a correct and parsimonious set of rules, if it is at all possible, would be a nontrivial challenge to overcome and one which we leave to future work. Addressing such a challenge and being able to promote the descriptive approximations provided here to explanatory ones would provide a next step towards understanding how LLMs work.