# \(p\)-value Adjustment for Monotonous, Unbiased, and Fast Clustering Comparison

Kai Klede\({}^{1}\)   Thomas Altstidl\({}^{1}\)   Dario Zanca\({}^{1}\)   Bjorn Eskofier\({}^{1,2}\)

\({}^{1}\)Machine Learning and Data Analytics (MaD) Lab

Friedrich-Alexander Universitat Erlangen-Nurnberg

\({}^{2}\)Translational Digital Health Group

Institute of AI for Health, Helmholtz Zentrum Munchen

{kai.klede; thomas.r.altstidl; dario.zanca; bjoern.eskofier}@fau.de

###### Abstract

Popular metrics for clustering comparison, like the Adjusted Rand Index and the Adjusted Mutual Information, are type II biased. The Standardized Mutual Information removes this bias but suffers from counterintuitive non-monotonicity and poor computational efficiency. We introduce the \(p\)-value adjusted Rand Index (\(_{2}\)), the first cluster comparison method that is type II unbiased and provably monotonous. The \(_{2}\) has fast approximations that outperform the Standardized Mutual information. We demonstrate its unbiased clustering selection, approximation quality, and runtime efficiency on synthetic benchmarks. In experiments on image and social network datasets, we show how the \(_{2}\) can help practitioners choose better clustering and community detection algorithms.

## 1 Introduction

Clustering is fundamental to unsupervised learning, and practitioners can choose from many algorithms to partition a dataset into homogeneous clusters. Therefore it is common to annotate parts of an otherwise unlabeled dataset and select the clustering algorithm that best reproduces the annotations . A good selection crucially depends on the clustering comparison method, like the Mutual Information (MI)  or the Rand Index (RI) . The importance of the comparison method further increases with the advent of deep learning methods for applications such as community detection, in which they serve as components of loss functions during network training [10; 12; 33]. Some use cases admit multiple clustering solutions [23; 38], and clustering comparison can help identify qualitatively different clustering solutions for a single dataset. Other applications of clustering comparison include categorical feature selection or consensus clustering [5; 17].

The MI and RI are biased towards clusterings with particular cluster size distributions  (type I bias). For example, the MI favors larger clusters for any reference clustering . The Adjusted Rand Index (ARI)  and Adjusted Mutual Information (AMI)  achieve a constant baseline value by subtracting the expected value under random permutation of the cluster labels. However, they still exhibit a bias when multiple clusterings are compared via a fixed ground truth (type II bias) , as opposed to comparing two random clusterings with each other. This type II scenario typically arises when selecting the best algorithm for a given task on a labeled subset of the data.

Romano et al.  showed via the Tsallis entropy, that the AMI and ARI are special cases of generalized information-theoretic clustering comparison measures \(_{q}\) and proposed standardization to resolve both types of biases. However, the runtime for standardization generally exhibits a substantial time complexity of \((N^{3}k_{A}(k_{A},k_{B}))\), where \(N\) represents the number of data points and \(k_{A}\) and \(k_{B}\) denote the respective number of clusters. This complexity is prohibitive for many applications . Gosgens et al.  found that the Standardized Mutual Information (SMI) does notincrease monotonically as one clustering is adjusted to match a reference and therefore reject the SMI entirely.

This work presents the \(p\)-value of the \(_{q}\) (denoted \(_{q}\)) as a provably monotonous type II bias correction. We formally define type II bias and prove that the \(_{q}\) does not suffer from it. For the \(_{q}\), there is only empirical evidence [29; 30]. We show that the \(_{q}\) is monotonous for \(q 2\). This includes the \(p\)-value of the RI, but for the MI, the \(p\)-value is not monotonous. When normalized with the normal CDF, the \(_{q}\) approximates the \(_{q}\), which we confirm via Monte Carlo simulation. We reduce the runtime of the \(_{2}\) from \((N^{3}k_{A}(k_{A},k_{B}))\) to a much more practical \((k_{A}k_{B})\) by a reformulation of the variance term. We demonstrate the impact of type II unbiased algorithm selection for community detection on a social network dataset and clustering on images of handwritten digits and human faces.

## 2 Generalized information theoretic clustering comparison measures

A clustering \(A\) of \(N\) data points is a partition of the set \(\{1,,N\}\) into disjoint subsets \(A=\{A_{1},,A_{k_{A}}\}\). \(A_{i}\) denotes the set of points in the \(i\)-th cluster of size \(a_{i}|A_{i}|\) and \(k_{A}\) is the number of clusters in \(A\). Clustering comparison measures quantify the similarity between two clusterings \(A\) and \(B\), and can be expressed as a function of the contingency table (Table 1).

While many clustering comparison methods exist in the literature [2; 9], many well-known methods like the Variation of Information, Mirkin Index, or Rand Index belong to the family of generalized information-theoretic clustering comparison measures [30; 31]. When adjusted for chance, these measures reduce to the mutual information with Tsallis \(q\)-entropy , which will be the focus of this work.

**Definition 2.1** (Tsallis \(q\)-entropy).: Let \(q_{+}\), and \(A\) be a clustering. Then, the _Tsallis \(q\)-entropy_ is

\[H_{q}(A)=-_{i=1}^{k_{A}}}{N}^{q}_{q}}{N},\] (1)

with the \(q\)-logarithm \(_{q}(y)(y^{1-q}-1)/(1-q)\) if \(q 1\) and the natural logarithm for \(q=1\), where \(x_{1}(x)=0\) for \(x=0\).

The generalized mutual information is defined in analogy to the mutual information but with Tsallis \(q\)-entropy replacing the Shannon entropy.

**Definition 2.2** (Generalized mutual information).: Let \(A,B\) be two clusterings of the set \(\{1,,N\}\) and \(q_{+}\), then the _generalized mutual information_ is

\[_{q}(A,B)=H_{q}(A)+H_{q}(B)-H_{q}(A,B).\] (2)

Here \(H_{q}(A,B)\) denotes the joint \(q\)-entropy \(H_{q}(\{A_{i} B_{j}\,|\,A_{i} A B_{j} B\})\).

## 3 Adjustment for chance

The bare \(_{q}\) has limited value as a clustering comparison measure. When comparing two clusterings directly with one another, it is biased towards larger or smaller clusters, depending on the value of

   &  &  &  &  &  \\   & \(b_{1}\) & \(\) & \(b_{j}\) & \(\) & \(b_{k_{B}}\) \\  \(a_{1}\) & \(n_{11}\) & \(\) & \(\) & \(\) & \(n_{1k_{B}}\) \\ \(\) & \(\) & & \(\) & \(\) & \(\) \\ \(a_{i}\) & \(\) & & \(n_{ij}\) & \(\) \\ \(\) & \(\) & & \(\) & & \(\) \\ \(a_{k_{A}}\) & \(n_{k_{A}1}\) & \(\) & \(\) & \(\) & \(n_{k_{A}k_{B}}\) \\  

Table 1: Contingency table for clusterings \(A,B\) with \(k_{A}\) and \(k_{B}\) clusters respectively. Lower case \(a_{i}\) and \(b_{j}\) denote the cluster size of the \(i\)-th and \(j\)-th clusters in \(A\) and \(B\), while \(n_{ij}\) represents the size of their overlap. Clustering comparison measures can be expressed in terms of the elements of this contingency table.

\(q\) (type I bias) [19; 25]. But even after adjusting for type I bias, there is another, more subtle bias when multiple clusterings are compared via a single ground truth  (Figure 1). In Section 3.2, we introduce the \(p\)-value as an adjustment to the latter type II bias.

### Type I bias

It is well known throughout the literature that the \(_{1}\) is biased towards smaller clusters in direct clustering comparisons [19; 25]. To make this precise, Gosgens et al.  defined a family of clustering distributions for which the expected similarity to a reference clustering should be constant:

**Definition 3.1** (Element-symmetric distribution).: A distribution over clusterings \(\) is _element-symmetric_ if every two clusterings \(B\) and \(B^{}\) with the same cluster sizes have the same probability.

An example of an element-symmetric distribution is the uniform distribution over all clusterings of \(N\) elements into \(k\) clusters. If the clustering is random, a comparison measure should not favor one particular \(k\) over another. If it does, we call it _type I biased_.

**Definition 3.2** (Type I unbiased).: A clustering measure \(V\) is _type I unbiased_ if there is a constant \(c\), such that for any clustering \(A\) with \(1<k_{A}<N\) and every element-symmetric distribution \(\) the expected value \(_{B}[V(A,B)]=c\) is constant.

In other words, type I unbiased means that when comparing a fixed clustering \(A\) to all permutations of any clustering \(B\), the average metric value is the same for all \(A\). As the \(_{q}\) has this type I bias, it is commonly adjusted by subtracting its expected value under random permutation, yielding a type I unbiased measure .

\[_{q}(A,B)_{q}(A,B)-_{  S_{N}}[_{q}(A,(B))]}{(H_{q}(A)+H_{q}(B))- _{ S_{N}}[_{q}(A,(B))]}.\] (3)

\(S_{N}\) denotes the symmetric group and \((H_{q}(A)+H_{q}(B))\) is an upper bound to the \(_{q}\) such that the \(_{q}\) is normalized to \(c=0\) for random clusterings and upper bounded by \(1\)[25; 30]. Adjustments with respect to other random models are possible [8; 18]. However, the random permutation model remains the most popular and is the focus of this work. Due to the generalization using Tsallis entropy, the \(_{2}\) corresponds to the Adjusted Rand Index [11; 30].

### Type II bias

However, in a typical external validation scenario, a single absolute value of the \(_{q}\) is of little help. While it is easy to understand that an \(_{q}\) of zero means a clustering algorithm is no better than random and a value of one means optimal agreement, the scale of the range in between is unclear. Therefore, the \(_{q}\) values of multiple candidate solutions with a reference are typically compared against each other to find the best algorithm for a given dataset.

As a toy model for this scenario, we uniformly generate \(5000\) clusterings of \(N=500\) elements for each number of clusters \(k_{B}\{2,6,10,14,18,22\}\). We compare them to a fixed clustering \(A\) with \(k_{A}=10\) evenly sized clusters and plot the selection probabilities for the \(,\), and \(_{q}\) for \(q\{1,2\}\) (Figures 0(a), 0(b), 0(d) and 0(e)). The bare \(\) and \(\) and their adjusted variants \(_{q}\) are biased towards certain values of \(k_{B}\).

We generalize and formalize this observation by demanding a clustering comparison measure to favor no element-symmetric distribution over another.

**Definition 3.3** (Type II unbiased).: Let \(V\) be a clustering comparison measure and \(,^{}\) be element-symmetric clustering distributions. \(V\) is _type II unbiased_ if

\[_{B,B^{}^{}}[(V(A,B) -V(A,B^{}))]=\] (4)

for any clustering \(A\) with \(1<k_{A}<N\), where \(\) denotes the Heaviside step function with \((0)=1/2\).

Intuitively, Type I bias means that certain cluster sizes receive higher metric values. Type II bias, on the other hand, gives a higher relative rank to certain cluster sizes when multiple clusterings are compared with a ground truth.

Romano et al.  introduced standardization to correct for type II bias

\[_{q}(A,B)_{q}(A,B)-_{ S _{N}}[_{q}(A,(B))]}{_{ S_{N}}[ _{q}(A,(B))]^{2}-_{ S_{N}}[_{q} (A,(B))^{2}]}}.\] (5)

They observed in numerical simulations that in the toy model above (Figure 1), the \(_{q}\) selects each \(k_{B}\) with approximately equal probability .

However, the \(_{q}\) is not normally distributed under random permutation (Figure 2a), and standardization is only an approximation to the true \(p\)-value (Figure 2b). The \(p\)-value quantifies what percentage of all permutations of the data would have led to higher mutual information, and we propose to use it for clustering comparison.

**Definition 3.4** (\(p\)-value adjusted, generalized mutual information).: Let \(A,B\) be two partitions of the set \(\{1,,N\}\), \(q_{+}\). Assuming the random permutation model, the \(p\)-value adjusted, generalized mutual information is

\[_{q} _{ S_{N}}[(_{q}(A,B)- _{q}((A),B))].\] (6)

Note that as the marginal entropies are independent of the permutation, \(_{q}=_{ S_{N}}[(H_{q}((A),B)-H_{q} (A,B))]\). For \(q=1\), this is the \(p\)-value of the mutual information and the variation of information by definition. For \(q 1\) the \(_{q}\) further simplifies to \(_{ S_{N}}[(n^{ q}_{ij}-n^{ q}_{ij})]\) for \(q>1\) and \(_{ S_{N}}[(n^{q}_{ij}-n^{ q}_{ij})]\) for \(q<1\) with \(n^{}_{ij}\) being the elements of the contingency table for \((A),B\). In a sense, the details of the generalized mutual information don't matter under \(p\)-value adjustment, except the exponent \(q\) of the contingency matrix elements. In fact, the \(p\)-value of the Rand Index is equivalent to the \(_{2}\) by a very similar argument.

In the experiment in Figure 1, we observe that the \(_{1}\) and \(_{2}\) select each \(k_{B}\) with approximately equal probability.

**Proposition 3.1**.: The \(_{q}\) is type I and type II unbiased.

We formally prove this result in Appendix A.

Figure 1: We compare a fixed reference clustering with \(k_{A}=10\) even clusters, to random clusterings with \(k_{B}\{2,6,10,14,18,22\}\) clusters. The plot shows the selection probabilities of each \(k_{B}\) for the \(\) and \(\) and its adjusted (\(_{q}\)) and our \(p\)-value adjusted (\(_{q}\)) variants after \(5000\) repetitions. The \(,\), and \(_{q}\) are type II biased, while our \(_{q}\) selects each cluster size with equal probability.

## 4 Monotonicity

When one clustering is changed to resemble another more, any clustering similarity measure should increase. A major drawback of the \(_{1}(A,B)\) is its non-monotonous behavior as the number of pairs of elements that agree in \(A\) and \(B\) increases . We show that the \(_{q}\), on the other hand, is monotonous for \(q 2\).

### Definition of monotonicity

The atomic operations that add new pairs of agreeing elements in two clusterings are the _perfect split_ and the _perfect merge_.

**Definition 4.1** (Perfect split).: \(B^{}\) is a perfect split of \(B\) with respect to \(A\) if it splits a single cluster \(B_{1} B\) into \(B^{}_{1}\), \(B^{}_{2} B^{}\) such that for all \(i\) either \(A_{i} B_{1} B^{}_{1}\) or \(A_{i} B_{1} B^{}_{2}\).

**Definition 4.2** (Perfect merge).: \(B^{}\) is a perfect merge of \(B\) with respect to \(A\) if \(B^{}\) is obtained by merging two clusters \(B_{1},B_{2} B\) with \(B_{1},B_{2} A_{i}\) for some \(i\).

Gosgens et al.  require that any clustering similarity measure increases monotonically for any combination of perfect splits and perfect merges.

**Definition 4.3** (\(A\)-consistent improvement).: \(B^{}\) is an \(A\)_-consistent improvement_ of \(B\) iff there exists a series of perfect splits and perfect merges that change \(B\) into \(B^{}\).

**Definition 4.4** (Monotonicity).: A symmetric clustering comparison measure \(V\) is monotonous if for every \(A,B\) with \(1<k_{A}<N\) and any \(A\)-consistent improvement \(B^{}\) of \(B\), \(V(A,B^{})>V(A,B)\).

We show that the \(_{q}\) is monotonous for \(q 2\), making it the first known clustering comparison measure to be both type II unbiased and monotonous. The case \(q=2\) is particularly interesting as it corresponds to the well-known Rand Index.

### Proof of monotonicity for \(_{q}\) with \(q 2\)

The proof can be broken down into monotonicity under perfect splits and perfect merges. We first show that the joint \(q\)-entropy increases under any split that is not perfect.

Figure 2: The probability of obtaining a particular \(_{2}\) under random permutation for two fixed clusterings \(A,B\) each of 100 elements. Our \(_{2}\) (blue bars in a)) takes the true distribution of the \(_{2}\) into account, whereas the \(_{2}\) (shaded blue region in b)) is based on a continuous normal approximation. However, when normalized with the normal CDF \(\), the \(_{2}\) is a good approximation of the \(_{2}\) as shown in Figure c). Here, we sampled \(1000\) pairs of clusterings uniformly at random for different numbers of elements \(N\). We plot the absolute difference between Monte Carlo estimates of the \(_{2}\) and normalized \(_{2}\) values as a function of the two-sided \(p\)-value. The larger the dataset size \(N\), the better \((_{2})\) approximates the true \(_{2}\).

**Lemma 4.1**.: Let \(A,B\) be clusterings with \(1<k_{A}<n\) and \(B^{}\) be obtained by splitting a cluster \(B_{j} B\) into non-empty clusters \(B^{}_{j_{1}},B^{}_{j_{2}}\). Then \(H_{q}(A,B^{})=H_{q}(A,B)+ H_{q}\) with \( H_{q} 0\) and equality iff the split is perfect with respect to \(A\).

This statement is a direct consequence of the subadditivity of the \(q\)-entropy . In particular \(h_{q}:p p^{q}_{q}p\) is a strictly convex function with \(h_{q}(0)=0\) and hence strictly superadditive, i.e. \(h_{q}(p_{1}+p_{2}) h_{q}(p_{1})+h_{q}(p_{2})\) for any \(p_{1},p_{2} 0\) with equality iff \(p_{1}=0 p_{2}=0\).

Proof of Lemma 4.1.: We express \( H_{q}\) as

\[ H_{q} =_{i}[(}{N})^{q}_{q}( }{N})-(_{ij_{1}}}{N})^{q}_{q }(_{ij_{1}}}{N})-(_{ij_{2}}}{ N})^{q}_{q}(_{ij_{2}}}{N})]\] \[=_{i}h_{q}(}{N})-h_{q}(_{ij_{1}}}{N})-h_{q}(_{ij_{2}}}{N}).\] (7)

From \(n_{ij}=n^{}_{ij_{1}}+n^{}_{ij_{2}}\) and the strict superadditivity of \(h_{q}\) follows \( H_{q} 0\) with equality iff \(n^{}_{ij_{1}}=0 n^{}_{ij_{2}}=0\), i.e. when the split is perfect. 

Conversely, a perfect merge maximizes the difference in joint entropy.

**Lemma 4.2**.: Let \(A,B,B^{}\) and \( H_{q}\) be as in Lemma 4.1, then for \(q 2\), \( H_{q}\) is maximal iff \(B\) is a perfect merge of \(B^{}\) with respect to \(A\).

Proof of Lemma 4.2.: When \(B\) is a perfect merge of \(B^{}\) with respect to \(A\), then

\[ H_{q}^{}=h_{q}(}{N})-h_{q}( _{j_{1}}}{N})-h_{q}(-b^{}_{j_{1}} }{N}).\] (8)

To show that \( H_{q}^{}\) is superadditive as a function of \(b_{j}\), we take its second derivative

\[^{2}}{b_{j}^{2}} H_{q}^{}= {q}{N^{q}}(b_{j}^{q-2}-(b_{j}-b^{}_{j_{1}})^{q-2}) 0q 2 b_{j}>b^{}_{j_{1}}>0.\] (9)

For \(q>2\), it is strictly convex and thus strictly superadditive. For \(q=2\) and \(b_{j}=0\), the difference \( H_{q}^{}=-b_{j_{1}}^{2}\) is negative and thus \( H_{2}^{}\) is also strictly superadditive. Now consider \(\) such that \(B\) is not a perfect merge with respect to \(\). Then at least two \(_{i_{1}},_{i_{2}}\) have non-vanishing overlap \(_{i_{1}j},_{i_{2}j}>0\) with \(B_{j}\) such that

\[ H_{q}^{}>_{i}h_{q}(_{ij}}{N} )-h_{q}(_{j_{1}}}{N})-h_{q}(_{ij}-b^{}_{j_{1}}}{N})q 2.\] (10)

With the superadditivity of \(h_{q}\) follows \(H_{q}^{}>H_{q}^{}\) (Compare Eq. 7). 

Now that we know how the joint entropy behaves under perfect splits and perfect merges, we can put together the proof of the monotonicity of the \(_{q}\).

**Theorem 4.3**.: Let \(A,B\) be clusterings with \(1<k_{A}<n\) and \(B^{}\) an \(A\)-consistent improvement of \(B\). Then \(_{q}(A,B^{})>_{q}(A,B)\) for \(q 2\).

Proof of Theorem 4.3.: It suffices to show monotonicity for \(B^{}\) a perfect split or perfect merge since any \(A\)-consistent improvement of \(B\) can be obtained by a sequence of perfect splits and perfect merges.

**Case 1.**\(B^{}\) is a perfect split.

Since \(A\) is not a singleton cluster, a permutation \(\) exists such that \(B^{}\) is not a perfect split with respect to \((A)\) and with Lemma 4.1 it follows

\[_{q}(A,B^{})>_{ S_{N}}[(H_{q}( (A),B)-H_{q}(A,B^{}))].\] (11)However, \(B^{}\) is a perfect split of \(B\) with respect to \(A\) and equality holds in Lemma 4.1

\[_{q}(A,B^{})>_{ S_{N}}[(H_{q}(( A),B)-H_{q}(A,B))]=_{q}(A,B).\] (12)

**Case 2.**\(B^{}\) is a perfect merge.

Let \(b_{1},b_{2} B\) denote the merged clusters that form \(b_{1}^{} B^{}\). Using Lemma 4.2, we find

\[_{q}(A,B^{})=_{ S_{N}}[(H_{q}( (A),B)-H_{q}(A,B)+ H_{q}^{(A)}- H_{q}^{A})]>_{q}(A,B),\] (13)

as there is at least one permutation \(\) for which the merge is not perfect.

## 5 Approximations and runtime

A limitation of the \(_{q}\) is its computational complexity. Its exact calculation is intractable even for small datasets, as it requires a sum over all contingency tables with given marginals. To mitigate this limitation, we propose two approximation schemes:

1. **Standardized approximation (\(\)):** We approximate the true, discrete distribution of \(_{q}\) with a continuous normal distribution that matches its first and second statistical moments (See Figure 2a and b). While this approximation is particularly fast for \(q=2\), it does not preserve the theoretical guarantees of the \(_{q}\).
2. **Monte Carlo approximation:** Given two clusterings \(A\) and \(B\), we sample contingency tables with the same cluster sizes. The fraction of tables with \(_{q}\) lower than \(_{q}(A,B)\) approximates the true \(p\)-value. In this approach, the theoretical guarantees hold up to a tunable approximation error at the cost of higher runtime.

### The standardized Rand Index

We approximate the \(_{q}\) with the \(_{q}\), normalized with the normal CDF \(\) (Figure 2b). This can be seen as a truncated, second-order Gram Charlier A series of the \(_{q}\), and while this could be continued for higher statistical moments, it is difficult to find an exact error term . A more cautious

Figure 3: Runtime of the Monte Carlo \(_{2}\) and the \(_{2}\) for random clusterings of a) \(N\) elements into \(k_{A}=k_{B}=10\) clusters and of b) random clusterings with \(N=1000\) and varying approximation error \(a\). The \(_{1}\) calculation, as proposed in , is prohibitively expensive for medium-sized datasets. Our exact reformulation of the \(_{2}\) and the Monte Carlo \(_{2}\) maintain practical runtimes for high \(N\). The \((_{2})\) is faster, while the Monte Carlo \(_{2}\) allows for higher accuracy.

normalization permits the lower bound \(_{q}(A,B) 1-_{q}(A,B))^{2}}\) for \(_{q}(A,B)>0\), but has little practical significance in the context of this work . Therefore, we evaluate the approximation quality experimentally1 on \(1000\) pairs of clusterings drawn uniformly from the set of all clusterings with \(N\{50,100,200,500,1000\}\) using a method described in . We compare \((_{2})(1+(_{2}\,/))/2\) with a Monte Carlo estimate of the \(_{2}\) with approximation error \(0.001\) in Figure 1(c). The values are highly correlated (\(r_{}=0.9983\) for \(N=50\)), and the approximation improves with larger values of \(N\) (\(r_{}=0.9995\) for \(N=1000\)). So although \((_{q})\) itself is not monotonous, it closely matches the \(_{q}\), which is monotonous for \(q 2\).

While \((_{q})\) is a simplification over the \(_{q}\), its computational complexity \((N^{3}k_{A}(k_{A},k_{B}))\) for general \(q\) is far from practical . In this work, we contribute a novel algorithm for the special case \(q=2\) that improves the computational complexity.

**Proposition 5.1**.: The computational complexity of \(_{2}\) is \((k_{A}k_{B})\).

The proof is in Appendix C. This special case \(q=2\) is of particular interest because of the correspondence with the well-known Rand Index and the monotonicity of \(_{2}\). Our improved algorithm for the \(_{2}\) allows comparisons of moderately sized clusterings \(N 10,\!000\) that are computationally out of reach for, e.g., the \(_{1}\) (Figure 2(a)).

### Monte Carlo approximation

The standardized approximation has two limitations:

* It is computationally inefficient only for \(q 2\).
* There is no guarantee that it preserves the desirable theoretical properties of the \(_{2}\).

We address both of these limitations by introducing a Monte Carlo approximation at the cost of increased runtime. For two clusterings \(A,B\), the method samples contingency tables uniformly from all tables with their respective cluster sizes \(a_{1},,a_{k_{A}};b_{1},,b_{k_{B}}\) (Compare Table 1), using the algorithms proposed in . The fraction of samples with \(_{q}\) lower than \(_{q}(A,B)\) is an unbiased estimator of the \(_{q}\). The sampling procedure terminates when a given approximation error \(a\) is reached. This way, the theoretical properties of the \(_{q}\) are preserved up to the tunable approximation error.

However, lower approximation errors require more samples:

**Proposition 5.2**.: The computational complexity of the Monte Carlo \(_{q}\) is \(((N,k_{A}k_{B} N)/a^{2})\), with the desired approximation error \(a\).

The proof is in Appendix B, and Figure 3 shows an experimental study of the runtime compared to the standardized approximation. The Monte Carlo approach is computationally more expensive, especially for larger datasets. Therefore, the standardized approach is for choice when \(q=2\) and moderate approximation quality is acceptable. The Monte Carlo method should be used if \(q 2\) or theoretical guarantees are required.

## 6 Algorithm selection on real-world datasets

### \(k\)-means clustering on image datasets

As a first example, we mimic the synthetic experiment in Figure 1. For several numbers of clusters \(k\), we apply \(k\)-means clustering  with \(1000\) different random seeds. We select the clustering with the highest \(\), \(_{2}\), and \(_{2}\), approximated by \((_{2})\) and denote the corresponding number of clusters \(k_{}\). Figure 3(a) shows the selection probabilities of \(k_{}\) when compared with a ground truth with \(k_{}\) clusters for a handwritten digit dataset  and Figure 3(b) for a dataset of human faces . Naturally, all measures favor solutions where \(k_{}>k_{}\), as a higher number of clusters increases the chances of \(k\)-means matching the reference decision boundaries. However, the \(\) and \(_{2}\) additionally suffer from type II bias, leading to a higher overestimation of \(k_{}\)(compare with Figure 1). The difference between the \(}\) and the \(}\) is subtle, but this is expected. Type II bias correction is just one step forward in clustering comparison and does not turn existing assessments based on metrics like the \(}\) on its head. In practice, much wider ranges of \(k_{}\) can arise from different clustering algorithms which could potentially amplify the effect. Two additional experiments with spectral clustering instead of \(k\)-means can be found in Appendix E.

### Community detection in social networks

In social networks, detecting communities is ubiquitous and can help to detect fraud, deliver personalized content, or target ads . As a result, many community detection algorithms are known in the literature . However, community detection is inherently unsupervised, and it is a priori unclear which algorithm with which parameters will perform best for a given application. In practice, human experts often annotate a subset of the dataset, and an unsupervised algorithm is selected via a clustering comparison measure on that subset.

We simulate this procedure on a network of email conversations between European research institutions, where each institution is a ground truth cluster . We select a connected subset with \(k_{}=30\) institutions and detect communities using Degree Ordered Label Propagation, Label Propagation, Leiden, Louvain, and Louvain Map Equation  with 22 parameter configurations (Appendix D). We then select the most similar algorithm to the ground truth using \(\), \(}\), and \(}(})\). This process is repeated for \(100\) subsets per dataset, and the resulting probabilities are shown in Figure 4c. Label propagation is a fast but inaccurate method  and overestimates \(k_{}\) by almost an order of magnitude in our experiment. During algorithm selection, \(\) was the only metric to choose Label Propagation in some cases. The \(}\) differs from the \(}\) in that it selected Leiden more frequently over the Louvain Map Equation, both of which are improvements over the original Louvain method . However, Leiden comes closer to the true number of clusters \(k_{}\), and in that sense, \(}\) led to a better choice of algorithm.

Figure 4: We apply \(k\)-means clustering  with varying \(k\) to a) the UCI handwritten digit dataset  and b) the Olivetti faces dataset  and select the solution with the highest similarity to the ground truth. We repeat this experiment \(1000\) times with different random seeds and plot the selection probability under the \(\), \(}\), and normal approximation of the \(}\). The \(}\) selects candidates where \(k_{}\) is closer to the true number of clusters \(k_{}\) on average (dashed lines) compared to the \(\) and \(}\). In c), we select a connected subset of \(k_{}=30\) communities from the email EU core dataset  and detect communities using five algorithms with 22 parameter configurations. The \(\), \(}\), and \(}\) select the best solution, and we plot the selection probability for \(k_{}\) after \(100\) repetitions. The \(}\) prefers the Leiden algorithm, which produces \(k_{}\) on the order of \(k_{}\). The \(}\) gives a higher probability for Louvain Map Equation, and the \(\) sometimes selects low-quality Label Propagation results.

## 7 Conclusion and outlook

Table 2 summarizes our findings for the \(_{q}\) and compares its theoretical properties to 17 clustering comparison measures from the systematic review by Gosgens et al. . We introduce the first type II unbiased and monotonous cluster comparison method, the \(p\)-value adjusted Rand Index (\(_{2}\)). Existing methods that addressed type II bias, namely the Standardized Mutual Information (\(_{1}\)) and the Standardized Rand Index (\(_{2}\)) are not monotonous, meaning clusterings closer to the ground truth can score worse. In addition, the \(_{1}\) has high computational complexity, making it unsuitable in practice. For the \(_{2}\) we showed that an efficient algorithm exists and we leverage this algorithm for an efficient approximation of the proposed \(_{2}\). However, our analysis of the errors in this standardized approximation is limited to experimental observations, leaving a theoretical analysis for future work. We devised a Monte Carlo approximation for the \(_{2}\) with tunable approximation error, for when theoretical guarantees are required. To validate our theoretical findings, synthetic experiments confirm that the presented \(_{2}\) selects different cluster sizes with equal probability and is not subject to type II bias. In practice, the \(_{2}\) chooses better clustering algorithms from a set of candidates when a ground truth reference is available. Thanks to its monotonicity and computational efficiency, the \(_{2}\) is a practical candidate for evaluating cluster similarity without type II bias. While we investigated \(p\)-value adjustment for the family of generalized information-theoretic clustering comparison measures, further research is required to understand if other comparison measures, like the Jaccard Index, could benefit from a similar adjustment.