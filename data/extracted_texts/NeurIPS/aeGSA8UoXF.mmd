# Symmetry-Informed Governing Equation Discovery

Jianke Yang

UCSD

Wang Rao\({}^{*}\)

Tsinghua University

Nima Dehmamy

IBM Research

Robin Walters

Northeastern University

Rose Yu

UCSD

###### Abstract

Despite the advancements in learning governing differential equations from observations of dynamical systems, data-driven methods are often unaware of fundamental physical laws, such as frame invariance. As a result, these algorithms may search an unnecessarily large space and discover less accurate or overly complex equations. In this paper, we propose to leverage symmetry in automated equation discovery to compress the equation search space and improve the accuracy and simplicity of the learned equations. Specifically, we derive equivariance constraints from the time-independent symmetries of ODEs. Depending on the types of symmetries, we develop a pipeline for incorporating symmetry constraints into various equation discovery algorithms, including sparse regression and genetic programming. In experiments across diverse dynamical systems, our approach demonstrates better robustness against noise and recovers governing equations with significantly higher probability than baselines without symmetry. Our codebase is available at https://github.com/Rose-STL-Lab/symmetry-ode-discovery.

## 1 Introduction

Discovering governing equations that can describe and predict observational data from nature is a primary goal of science. Compared to the black-box function approximators common in machine learning, symbolic equations can reveal a deeper understanding of underlying physical processes. In this paper, we consider a first-order dynamical system governed by an autonomous ordinary differential equation (ODE):

\[}(t)=((t))\] (1)

where \(t T\) denotes time, \((t) X\) is the system state at time \(t\), \(X^{d}\) is the phase space of the ODE, \(}(t)\) the time derivative, and \(:X^{d}\) the vector field describing the evolution dynamics of the system. Our goal is to discover the function \(\) in a symbolic form from the observed trajectories of the system.

Traditionally, discovering governing equations has been a difficult task for human experts due to the complexity of the underlying physical systems and the large search space for equations. Recently, many new algorithms have been proposed for automatically discovering equations from data. When applied to an ODE, these methods first estimate the label, i.e. the time derivative, by numerically differentiating the trajectory. A variety of techniques including sparse function basis regression (Brunton et al., 2016; Champion et al., 2019) and genetic programming (Cranmer et al., 2019) are then used to fit the function between the measurement and the estimated derivative. Another class of methods considers the differential equation's variational form to eliminate the need for derivative estimation and improve the robustness against noise (Messenger and Bortz, 2021; Qian et al., 2022).

An important aspect of equation discovery is the principle of parsimony (Saber, 1981) - _the scientific principle that the most acceptable explanation of a phenomenon is the simplest_. It promotes the scientific goal of identifying a model that offers robust predictive capabilities while maintaining simplicity. Existing methods that fail to consider these principles may end up searching in anunnecessarily large space of equations and discover equations that are overly complex or do not conform to fundamental physical laws.

In this work, we advocate for using the principle of symmetry to guide the equation discovery process. Symmetry has been widely used in machine learning to design equivariant networks (Bronstein et al., 2021; Finzi et al., 2021; Wang et al., 2021), augment training data (Benton et al., 2020), or regularize the model (Akhound-Sadegh et al., 2023; Otto et al., 2023), leading to improved prediction accuracy, sample efficiency, and generalization.

Specifically, we derive the equivariance constraints from the symmetries of ODEs and solve the constraints explicitly to compress the search space of equations. Alternatively, we can also use symmetry as a regularization loss term to guide the equation discovery process and improve its robustness to measurement noise. In practice, a dynamical system may possess symmetries that are unknown or difficult to describe in simple terms. To account for these situations, we also incorporate a method for learning symmetries with nonlinear group actions (Yang et al., 2023). We then establish a pipeline capable of learning unknown symmetries from data and subsequently discovering governing equations that conform to the discovered symmetry. To summarize, our main contributions include:

* We establish a holistic pipeline to use Lie point symmetries of ODEs to improve the accuracy and robustness of equation discovery algorithms.
* We derive the theoretical criterion for symmetry of time-independent ODEs in terms of equivariance of the associated flow map.
* From this criterion, we solve the linear symmetry constraint explicitly for compressing the equation search space in sparse regression and promoting parsimony in the learned equations.
* For general symmetry, we promote symmetry by a symmetry regularization term when the symmetry constraint cannot be explicitly solved.
* In experiments across many dynamical systems with substantial noise, our symmetry-informed approach achieves higher success rates in recovering the governing equations and better robustness against noise.

## 2 Related Works

### Governing Equation Discovery

There are numerous methods for discovering governing equations from data. Genetic programming has been successful in searching exponentially large spaces for combinations of mathematical operations and functions (Gaucel et al., 2014). Recent works have applied genetic programming to distill graph neural networks into symbolic expressions (Cranmer et al., 2019, 2020). Udrescu and Tegmark (2020) introduce physical inductive biases to expedite the search. The main limitation of genetic programming arises from computational complexity due to the combinatorially large search space. Also, genetic programming can suffer from noisy labels (Agapitos et al., 2012), which are common in discovering ODEs due to measurement noise.

Another branch of research involves sparse regression. Originating from SINDy (Brunton et al., 2016), these methods assume that the equation can be written as a linear combination of some predefined functions and optimize the coefficients with sparsity regularization. Subsequently, Champion et al. (2019) combines the sparse regression technique with an autoencoder network to simultaneously discover coordinate systems and governing equations. Weak SINDy (Messenger and Bortz, 2021) uses an alternative optimization objective based on the variational form of the ODE, which eliminates the need for estimating time derivatives and improves the robustness against measurement noise. Other works improve upon SINDy by incorporating physical priors, such as dimensional analysis (Xie et al., 2022; Bakarji et al., 2022), physical structure embedding (Lee et al., 2022), and symmetries (Loiseau and Brunton, 2018; Guan et al., 2021).

### Symmetry Prior in Machine Learning

Symmetry plays a crucial role in machine learning. There has been a vast body of work on equivariant neural networks (Cohen and Welling, 2016; Weiler and Cesa, 2019; Finzi et al., 2020, 2021; Bronsteinet al., 2021; Wang et al., 2021). These network architectures enforce various symmetries in different data types. Symmetry can also be exploited through data augmentation (Benton et al., 2020) and canonicalization (Kaba et al., 2023). There are already some works that exploit symmetries for learning in dynamical systems governed by differential equations through data augmentation (Brandstetter et al., 2022), symmetry loss (Huh et al., 2020; Akhound-Sadegh et al., 2023) and contrastive learning (Mialon et al., 2023). However, works using symmetry in recovering underlying equations from data are scarce, and existing examples only consider symmetries specific to a particular system, e.g. reflections and permutations in proper orthogonal decomposition (POD) coefficients (Guan et al., 2021), rotations and translations in space (Ridderbusch et al., 2021; Baddoo et al., 2021), etc. In this work, we provide solutions for dealing with general matrix Lie group symmetries.

While symmetry has proved an important inductive bias, for the ODEs considered in this work, the underlying symmetries are often not known a priori. Some recent works have aimed to automatically discover symmetries from data (Liu and Tegmark, 2022; Yang et al., 2023, 2023; Otto et al., 2023; van der Oudera et al., 2024). When there is no available prior knowledge about symmetry, We use the adversarial framework in Yang et al. (2023) to learn unknown symmetries from data and discover the equations subsequently using the learned symmetry.

## 3 Symmetry of ODEs

Our goal is to discover the governing equations of a dynamical system (1) in symbolic form. Our dataset consists of observed trajectories of the system, \(\{_{0:T}^{(i)}\}_{i=1}^{N}\), where \(_{t}^{(i)} X^{d}\) denotes the observation at timestep \(t\) in the \(i\)th trajectory. We assume the observations are collected at a regular time interval \( t\). We omit the trajectory index \(i\) and consider one trajectory, \(_{0:T}\) for simplicity. To leverage symmetry for better equation discovery, we first define symmetries in ODEs and provide the theoretical foundation for our methodology. In particular, we show that a time-independent symmetry of the ODE is equivalent to the equivariance of its flow map. This equivariance property can then be conveniently exploited to enforce symmetry constraints in the equation discovery process.

### Lie Point Symmetry and Flow Map Equivariance

The Lie point symmetry of an ODE is a transformation that maps one of its solutions to another. The symmetry transformations form the symmetry group of the ODE. In this paper, we focus on symmetry transformations that act solely on the phase space \(X\), without changing the independent variable \(t\). We refer to these as _time-independent_ symmetries, formally defined as follows.

**Definition 3.1**.: A _time-independent_ symmetry group of the ODE (1) is a group \(G\) acting on \(X\) such that whenever \(=(t)\) is a solution of (1), \(}=g(t)\) is also a solution of the system.

We refer the readers to Appendix A for background on Lie groups, group actions, and their applications on differential equations. We consider time-independent symmetries in Definition 3.1 and demonstrate how to use these symmetries to derive equivariance constraints on equations.

Given a fixed time \(\), the flow map associated with the ODE (1) is denoted \(_{}:X X\), which maps a starting point in a trajectory \((t_{0})\) to an endpoint \((t_{0}+)\) after moving along the vector field \(\) for time \(\). Formally, \(_{}(_{0})=()\), where \((t)\) is the solution of the initial value problem

\[}(t)=(),\ (0)=_{0},\ t[0,].\] (2)

**Proposition 3.2**.: _Let \(G\) be a group that acts on the phase space \(X\) of the ODE (1). \(G\) is a symmetry group of the ODE (1) in terms of Definition 3.1 if and only if for any \( T\), the flow map \(_{}\) is equivariant to the \(G\)-action on \(X\)._

As our dataset consists of trajectories measured at discrete timesteps, consider the discretized flow map \(_{t+1}=(_{t})\), where \(_{ t}\) depends on the sampling step size \( t\) of the data. From Proposition 3.2, this function of moving one step forward in the trajectory is \(G\)-equivariant, i.e.

\[_{t+1}=(_{t}) g_{t+1}=(g _{t}).\] (3)

### The Infinitesimal Formulae

We can use the equivariance condition (3) to constrain the equation learning problem. Consider the Lie group of symmetry transformations \(G\) and the associated Lie algebra \(\). We have the following equality constraints.

**Theorem 3.3**.: _Let \(G\) be a time-independent symmetry group of the ODE (1). Let \(v\) be an element in the Lie algebra \(\) of the group \(G\). Consider the flow map \(\) defined in (2) for a fixed time interval. Denote \(J_{}\) and \(J_{g}\) as the Jacobian of \(()\) and \(g\) w.r.t \(\). For all \( X\) and \(g=( v) G\), the following equalities hold:_

\[(g)-g() =0; (4) J_{}()v()-v(()) =0; (5)\] \[J_{g}()()-(g) =0; (6) J_{}()v()-J_{v}()() =0. (7)\]

To understand Theorem 3.3, (4) follows directly from Proposition 3.2, and the other equations are the infinitesimal equivalent of this equivariance condition. For example, the Jacobian-vector product in (5) reveals how much \(\) changes when the input is infinitesimally transformed by \(v\), and \(v(())\) indicates the amount of change caused by the infinitesimal action on the output of \(\). 2 The difference between these two terms vanishes if the function is equivariant. On the other hand, as both \(\) and \(v\) can be viewed as vector fields on \(X\), their roles are interchangeable in the equivariance formula. Therefore, we can also express the equivariance in terms of the flow \(\) and a finite group element \(g=( v)\). Finally, we can also consider the fully infinitesimal formula (7), which indicates that \(\) and \(v\) commute as vector fields. The proof of Theorem 3.3 may be found in Appendix B.1.

Theorem 3.3 provides the theoretic basis for our symmetry-based equation discovery methods in Section 4. We can enforce the equality constraints in Theorem 3.3 by solving the constraints explicitly (Section 4.1) or adding them as regularization terms (Section 4.2).

## 4 Symmetry-Informed Governing Equation Discovery

Next, we discuss how to leverage the knowledge of symmetry in equation discovery. Depending on the nature of different symmetries, we develop a holistic pipeline for incorporating symmetries into various equation discovery algorithms, shown in Figure 1. We introduce the components in the following subsections.

### Equiv-c: Solving the Linear Symmetry Constraint

We start from the case of linear symmetries. Many ODEs exhibit symmetries with linear actions on the state variable \(\). For example, in classical mechanics, the motion of a particle under a central force is invariant under rotations about the center. For such linear symmetries, if the governing equations can be written as linear combinations of basis functions as in SINDy sparse regression (Brunton et al., 2016), the infinitesimal formula (7) becomes a linear constraint. The constraint can be explicitly solved to construct an equivariant model for \(\).

In particular, suppose the governing equations can be written in terms of \(()^{p}\) as:

\[()=W()\] (8)

where \(W^{d p}\) is the learnable coefficient matrix. In SINDy, the function library \(()\) is pre-defined. For example, \(\) can be the set of all polynomials up to second order for a 2-dimensional system, i.e. \((x_{1},x_{2})=[1,x_{1},x_{2},x_{1}^{2},x_{1}x_{2},x_{2}^{2}]^{T}\).

Our goal is to solve for the parameter \(W\) that makes (7) hold for any \(\). To this end, we define the symbolic map \(M_{}:(^{d})^{p}\), which maps a function to its coordinate in the function space spanned by \(\). Specifically, \(M_{}(f_{j})=\) for \(f_{j}()=^{T}()\), \(^{p}\). For a multivariate function \(f:^{d}^{n}\), we compute \(M_{}\) for each of its components and stack them into \(M_{}(f)^{n p}\). A concrete example is provided in Appendix B.2. We have the following proposition.

**Proposition 4.1**.: _Let \(G\) be a time-independent symmetry group of the ODE \(}=W()\) with linear actions. Let \(v\) be an Lie algebra element with action \(X\) by \(v()=L_{v}\), \(L_{v}^{d d}\). Then, \(L_{v}W=WM_{}(J_{}()L_{v}())\), where \(J_{}\) denotes the Jacobian of \(\)._Proof.: Substituting (8) into (7), we obtain \(J_{v}()W()=WJ_{}()v(),\  X\). As \(v\) acts on \(X\) linearly, i.e. \(v()=L_{v}\), the constraint is equivalent to \(L_{v}W()=WJ_{}()L_{v}\).

We then apply \(M_{}\) to both sides of the equation. Obviously, \(M_{}()=I_{p}\). Since (7) is true for all \(\), we have \(L_{v}=WM_{}(J_{}()L_{v}())\). 

To calculate \(M_{}\), we need to ensure that \(J_{}()L_{v}\) is still within the span of the functions in the given \(()\). This is true when \(()\) is the set of all polynomials up to a certain degree, which we prove in Appendix B.2.

**Proposition 4.2**.: _The components of \(J_{}()L_{v}^{p}\) can be written as linear combinations of the terms in \(()\) if \(()\) is the set of all polynomials up to degree \(q^{+}\)._

Once we have the infinitesimal action \(L_{v}\) and an appropriate function library \(\), \(M_{}\) can be computed in a purely symbolic way using sympy(Meurer et al., 2017). Denoting \(M_{,L_{v}} M_{}(J_{}()L_{v}())\) and vectorizing the matrix \(W\), we can rewrite the equation in Proposition 4.1 as the following linear constraint on \(\):

\[(-M_{,L_{v}}^{T}L_{v})(W)=0\] (9)

where \(\) is the Kronecker sum: \(AB=A I+I B\). We concatenate the constraints for all representations \(L_{v_{i}}\) of the Lie algebra basis \(\{v_{i}\}_{i=1}^{c}\) into a single matrix \(C\) and solve the constraint using the singular value decomposition:

\[C\,(W) =-M_{,L_{v_{1}}}^{T}L_{v_{1}}\\ \\ -M_{,L_{v_{c}}}^{T}L_{v_{c}}(W)\] \[=U&0\\ 0&0P^{T}\\ Q^{T}(W)=0.\] (10)

The coefficient matrix resides in the \(r\)-dimensional nullspace of \(C\) and can be parametrized as \((W)=Q,\,^{r}\). This equivariant parametrization significantly reduces the number of free parameters from \(d p\) to \(r\), leading to parsimonious equations and easier training in a compressed parameter space. We name this approach as EquivSINDy-c, where c stands for constraint.

Figure 1: Pipeline for incorporating symmetries into equation discovery via solving linear symmetry constraint (Section 4.1), regularization (Section 4.2) and symmetry discovery (Section 4.3). Given the trajectory data from the dynamical system, we first identify its symmetry based on prior knowledge or symmetry discovery techniques. We then enforce the symmetry by solving a set of constraints when possible and otherwise promote the symmetry through regularization.

The above procedure uses linear symmetries to inform sparse regression. However, when the data is high-dimensional, such as videos, linear symmetries can become too restrictive. For these scenarios, SINDy autoencoder (Champion et al., 2019) maps the data to a latent space to discover the coordinate system and the governing equation under that system. We show in the experiment section that our model can also be used in this case by enforcing the latent equation to be equivariant. With the inductive bias of symmetry, the resulting coordinate and equation can achieve better long-term prediction accuracy.

### Equiv-r: Regularization for General Symmetry

The above procedure, EquivSINDy-c, applies to linear symmetry with a proper choice of function library \(\). Solving the equality constraint in Proposition 4.1 for arbitrary group action is generally more challenging. Technically, when the infinitesimal action \(v\) is expressed in a closed form, we can still apply \(M_{}\) to both sides of the equation and obtain \(M_{}(v(W()))=WM_{}(J_{}()v())\). However, the function library \(\) needs to be carefully chosen based on the specific action \(v\) to ensure that \(M_{}\) is evaluated on functions within the span of \(()\). Moreover, as we will discuss in Section 4.3, we may rely on symmetry discovery methods to learn unknown symmetries parametrized by neural networks. Without a closed-form symmetry, calculating \(M_{}\) is intractable.

We propose an alternative approach that is universally applicable to any symmetry and equation discovery algorithm. We use the formula from Theorem 3.3 as a regularization term to promote symmetry in the learned equation. This symmetry loss is added to the equation loss from the base equation discovery algorithm, e.g. \(L_{2}\) error between the estimated time derivative and the prediction from the learned equation.

We consider the following relative loss based on infinitesimal group action \(v\) and a finite-time flow map \(_{}\) of the equation as in (5):

\[_{}=_{}[_{v B( )}_{}}()v()-v(_{}())\|^{2}}{ \|J_{_{}}()v()\|^{2}}]\] (11)

where \(B()\) denotes the basis of the Lie algebra, \(_{}\) is obtained by solving the initial value problem (2), and \(J_{_{}}\) denotes the Jacobian of \(_{}\).

We use a relative loss because the scale of both terms in the numerator and their difference is subject to the specific \(_{}\). In the extreme case when \(=\), both terms are zero because \(_{}()\) is constant. As we are optimizing \(\) under this objective, we compute the relative error to eliminate the influence of the level of variations in the function itself on the symmetry error.

We can also introduce other forms of symmetry regularizations based on the other equations in Theorem 3.3. Empirically, these regularization terms perform similarly, but their implementations have subtle differences. For instance, some loss terms require integrating the learned equations to get the flow map \(_{}\) and thus incur additional computational cost; some involve computing higher-order derivatives for infinitesimal symmetries, which amplifies the numerical error when the symmetry is not exactly accurate. We list these different options of symmetry regularization and compare them in more detail in Appendix C.2.

The use of symmetry regularization encourages equation discovery models to achieve lower symmetry error, instead of explicitly constraining the parameter space as in Section 4.1. However, it is a general approach that can be applied to various equation discovery algorithms, e.g. sparse regression and genetic programming. Similar to the works that utilize the variational form of ODEs (Messenger and Bortz, 2021; Qian et al., 2022), our symmetry regularization does not require estimating the time derivative, so it is more robust to noise. We name this approach as EquivSINDy-r (or EquivGP-r for genetic programming-based discovery algorithms), r standing for regularization.

### Equation Discovery with Unknown Symmetry

The knowledge of ODE symmetries is often not accessible when we do not know the equations. In this case, we can use symmetry discovery methods (Dehmamy et al., 2021; Liu and Tegmark, 2022; Yang et al., 2023, 2023) to first learn the symmetry from data and then use the discovered symmetries to improve equation discovery.

In our experiments, we use LaLiGAN (Yang et al., 2023a) to learn nonlinear actions of a Lie group \(G(k;)\) as \(( v)x=(( L_{v}))(x)\), where the networks \(\) and \(\) define an autoencoder and \(L_{v}^{k k}\) is the matrix representation of \(v\). Our dataset consists of ODE trajectories sampled at a fixed rate \( t\), each given by \(_{0:T}\). We can learn the equivariance of \(:_{t}_{t+1}\) by feeding the input-output pairs \((_{i},_{i+1})\) into LaLiGAN. We extract the infinitesimal action as

\[v()=(( L_{v}))() _{=0}=J_{}()L_{v}\] (12)

where \(=()\) is the latent mapping via encoder network, and the Jacobian-vector product can be obtained by automatic differentiation. This infinitesimal action is then used to compute the regularization (11).

Alternatively, we can also discover equations in the latent space of LaLiGAN by solving the equivariance constraints in Section 4.1, as the latent dynamics \(_{t}_{t+1}\) has linear symmetry \(L_{v}\). Discovering governing equations in the latent space can be helpful when the dataset consists of high-dimensional observations instead of low-dimensional state variables.

We should note that our method has a completely different goal from LaLiGAN. Our method aims to discover equations using symmetry as an inductive bias. LaLiGAN aims to discover unknown symmetry. Only when we do not know the symmetry a priori, we use LaLiGAN as a tool to discover the symmetry first and use the discovered symmetry to regularize equation learning.

## 5 Experiments

The experiment section is organized as follows. In Section 5.1, we will demonstrate how to solve the linear symmetry constraints (Section 4.1) on some synthetic equations. In Section 5.2, we will apply the same equivariant model to discover equations in a symmetric latent space for some high-dimensional observations. In Section 5.3, we consider two well-studied benchmark problems for dynamical system inference under significant noise levels, where we use the regularization technique (Section 4.2) based on learned symmetries.

Data generation.For each ODE system, we sample \(N\) random initial conditions \(_{0}\) from a uniform distribution on a specified subset of \(X^{d}\). Starting from each initial condition, we integrate the ODE using the 4th-order Runge-Kutta (RK4) method and sample with a regular step size \( t\) to get the discrete trajectory \(_{0:T}\). We use different \(N\), \(T\) and \( t\) for each system, as specified in Appendix D.

Then, unless otherwise specified, we add white noise to each dimension of the state \(\) at a noise level \(_{R}\). The scale of the noise depends on the variance in the data within each state dimension \(x_{i}\): \(_{i}=_{R}(x_{i})\). We will report the noise level \(_{R}\) for each system. After adding the noise, we apply a Gaussian process-based smoothing procedure on the noisy data, similar to Qian et al. (2022).

Evaluation metrics.We run each algorithm multiple times and report the _success probability_ of discovering the correct function form. Specifically, assume the true equation is expanded as \(_{f_{i}}f_{i}()\), where \(_{f_{i}}\) is a nonzero constant parameter and \(f_{i}\) is a function of \(\). Also, we expand the discovered equation as \(_{f_{i}}_{i}()\), where \(_{f_{i}} 0\). The discovery is considered successful if all the terms match, i.e. \(\{f_{i}\}=\{_{i}\}\). The probability is computed as the proportion of successful runs. As each dynamical system consists of multiple equations (one for each dimension), we evaluate the success probability of discovering each individual equation (e.g. "Eq. 1", "Eq. 2" columns in Table 1) and the joint probability of successfully discovering all the equations (e.g. "All" column in Table 1). We argue that this is the most important metric for evaluating the performance of an equation discovery algorithm, since an accurate equation form reveals the key variables and their interactions, and thus the underlying structure and relationships governing the system.

The following metrics are also considered:

* _RMSE_ for parameter estimation \(-}^{(k)}\|^{2}/K}\), where \(K\) is the number of runs, \(=(_{f_{1}},_{f_{2}},...)\) is the constant parameters in true equation terms as defined above, and \(}^{(k)}\) the estimated parameters of the corresponding terms from the \(k\)th run. If \(f_{i}\) is not in the learned equation, \(_{f_{i}}=0\). We evaluate this metric on (1) all runs and (2) successful runs, referred to as _RMSE (all)_ and _RMSE (successful)_. Also, for each system, this metric is computed for each individual equation and all equations.
* _Long-term prediction error_. We integrate the learned ODE from an initial condition sampled from the test set to predict the future trajectory. We evaluate the error between the predicted and the true trajectory at chosen timesteps.

Algorithms and Baselines.For baseline comparisons, we include SINDy sparse regression (Brunton et al., 2016) and Genetic Programming (GP) with PySR package (Cranmer, 2023), and their respective variants using the variational formulation, i.e. Weak SINDy (WSINDy) (Messenger and Bortz, 2021) and D-CODE (Qian et al., 2022). Based on different experiment setups and the guidelines in Figure 1, we apply our methods referred to as EquivSINDy / EquivGP, where the suffix c stands for solving the linear symmetry constraint, and \(\) stands for using the symmetry regularization.

### Equivariant SINDy for Linear Symmetries

In this section, we consider two dynamical systems which possess some linear symmetries. These symmetries are commonly observed across many systems and their presence can be easily detected. Therefore, we assume we know these symmetries and focus on symmetry-informed equation discovery by solving the symmetry constraints.

Damped Oscillator.This is a two-dimensional system with rotation symmetry, characterized by the infinitesimal generator \(v=x_{2}_{1}-x_{1}_{2}\) (i.e. \(0&1\\ -1&0\) in matrix terms). It is governed by (13).

\[_{1}=-0.1x_{1}-x_{2}\\ _{2}=x_{1}-0.1x_{2}\] (13)

Growth.This system exhibits a scaling symmetry under \((x_{1},x_{2})(a^{2}x_{1},ax_{2})\). The corresponding infinitesimal generator is \(v=2x_{1}_{1}+x_{2}_{2}\). It is governed by (14).

Figure 2 visualizes the parameter spaces for these two systems under the equivariance constraint. The original parameter space is \(W^{2 6}\) as we build the function library \(\) with up to second-order polynomials. For (13), the equivariance constraint reduces the parameter space to two dimensions. The equivariant basis \(Q_{1}=Qe_{1}\) and \(Q_{2}=Qe_{2}\) are visualized. For (14), the parameter space is reduced to three dimensions. Each equivariant basis component only affects one equation term, visualized in three distinct colors. We can observe that the equivariant models have much fewer free parameters than non-equivariant ones.

Table 1 displays the success probability of different methods. With the reduced weight space, our equivariant model almost always recovers the correct equation forms. Full results, including the parameter estimation error and the prediction error with discovered equations, as well as the experiment on a higher-dimensional system, are deferred to Appendix C.1. Overall, these results fully demonstrate the advantage of applying symmetry to compress the parameter space.

   &  &  \\  & & Eq. 1 & Eq. 2 & All \\   & GP & 0.00 & 0.70 & 0.00 \\  & D-CODE & 0.00 & 0.00 & 0.00 \\   & SINDy & 0.35 & 0.38 & 0.15 \\   & WSINDy & 0.06 & 0.07 & 0.00 \\   & EquivSINDy-c & **0.93** & **0.97** & **0.90** \\    & GP & 0.00 & **1.00** & 0.00 \\   & D-CODE & 0.00 & 0.65 & 0.00 \\   & SINDy & 0.26 & 0.13 & 0.03 \\   & WSINDy & 0.00 & 0.00 & 0.00 \\   & EquivSINDy-c & **1.00** & **1.00** & **1.00** \\  

Table 1: Success probability of equation discovery on the damped oscillator (13) at noise level \(_{R}=20\%\) and the growth system (14) at \(_{R}=5\%\), computed from 100 runs for each algorithm. See Appendix C.1 for full results.

Figure 2: Solutions for equivariant constraints of (13, 14). In (13), the 2D parameter space is spanned by \(Q_{1}\) and \(Q_{2}\). In (14), the 3D parameter space is spanned by \(Q_{1,2,3}\), each marked with a different color.

### Equivariant SINDy in Latent Space

We also show an example of discovering equations in the latent space with an equivariant model. We consider a \(-\) reaction-diffusion system visualized in Figure 3. The system is sampled at a \(100 100\) spatial grid, yielding observations in \(^{10000}\). See Appendix D.1 for more details.

We apply LaLiGAN (Yang et al., 2023a) to discover a latent space \(^{2}\) where the dynamics \(_{t}_{t+1}\) is equivariant to a linear group action. Meanwhile, we use equivariant SINDy with the learned symmetry to discover the equation for the latent dynamics. We compare our approach with SINDy Autoencoder (Champion et al., 2019) without any symmetry and LaLiGAN + SINDy, i.e. learning the equations in the LaLiGAN latent space but without equivariance constraint.

Figure 3 shows the prediction error on this system using the latent equations. We find that the discovery result of SINDy Autoencoder is unstable, and the simulation can quickly diverge. Meanwhile, both approaches with symmetry discover equations that accurately simulate the dynamics in the input space. Interestingly, even with similar latent spaces discovered by LaLiGAN, our approach that enforces the equivariance constraint can further reduce prediction error compared to non-constrained regression.

### Symmetry Regularization

We demonstrate the use of symmetry regularization in the following ODE systems. These systems do not possess any linear symmetries for solving the constraint explicitly.

**Lotka-Volterra System** describes the interaction between a predator and a prey population. We consider its canonical form as in Yang et al. (2023a), where the state is given by the logarithm population densities of the prey and the predator.

\[_{1}=-e^{x_{2}}\\ _{2}=-1+e^{x_{1}}\] (15)

   &  &  &  &  \\  & & Eq. 1 & Eq. 2 & All & Eq. 1 & Eq. 2 & All & Eq. 1 & Eq. 2 \\   & SINDy & 0.40 & **0.64** & 0.24 & 1.01 (0.26) & 0.56 (0.21) & 0.79 (0.16) & 4.01 (2.50) & **3.24** (3.67) \\  & WISNDy & 0.18 & 0.22 & 0.06 & **0.59** (0.38) & **0.32** (0.23) & **0.26** (0.16) & 1.13 (1.32) & **1.86** (1.88) \\  & EquivSNDy-r & **0.54** & 0.58 & **0.36** & 1.00 (0.21) & 0.45 (0.20) & 0.79 (0.15) & **3.16** (2.46) & 3.83 (4.01) \\    & GP & **1.0** & 0.0 & 0.0 & **2.44** (0.89) & N/A & N/A & **2.44** (0.89) & 13.20 (3.20) \\  & D-CODE & 0.0 & 0.0 & 0.0 & N/A & N/A & N/A & 10.38 (0.11) & 9.08 (1.88) \\  & EquivGP-r & **1.0** & **0.8** & **0.23** (1.39) & **0.51** (0.98) & **1.58** (1.62) & **2.43** (1.39) & **1.76** (0.31) \\    & SINDy & 0.30 & 0.56 & 0.14 & 0.87 (0.15) & 0.32 (0.13) & 0.67 (0.10) & 15.86 (17.52) & 12.71 (18.32) \\  & WISNDy & 0.06 & 0.14 & 0.04 & **0.11** (0.10) & 0.59 (0.23) & **0.34** (0.04) & 2.36 (2.943) & 2.163 (2.763) \\   & EquivSNDy-r & **0.40** & **0.70** & **0.28** & 0.92 (0.22) & **0.30** (0.13) & 0.71 (0.16) & **9.97** (8.07) & **7.29** (17.27) \\    & GP & 0.0 & 0.0 & 0.0 & N/A & N/A & N/A & **7.57** (4.65) & 14.88 (23.50) \\   & D-CODE & 0.0 & 0.4 & 0.0 & N/A & 0.27 (0.11) & N/A & 14.37 (15.66) & 8.90 (16.84) \\   & EquivGP-r & **0.1** & **0.6** & **0.1** & **0.67** (N/A) & **0.22** (0.38) & **0.40** (N/A) & 8.59 (12.10) & **3.43** (6.69) \\  

Table 2: Equation discovery statistics on the Lotka-Volterra system (15) at noise level \(_{R}=99\%\) and the glycolytic oscillator (16) at noise level \(_{R}=20\%\). The RMSE is scaled by \( 10^{-1}\) for (15) and \( 10^{-2}\) for (16). The success probability is computed from 50 runs for sparse regression-based algorithms and 10 runs for genetic programming-based algorithms. The success probabilities of recovering individual equations (Eq. 1 & Eq. 2) and simultaneously recovering both equations (All) are reported. The RMSE (all) refers to the parameter estimation error over all runs. The RMSE (successful) refers to the parameter estimation error over successful runs, which is missing for algorithms with zero success probability.

Figure 3: Reaction-diffusion system and the prediction error in the high-dimensional input space using equations from different methods. The means and standard deviations (shaded area) of errors over 3 random runs are plotted.

**Glycolytic Oscillator**(Sel'Kov, 1968) is a biochemical system of two ODEs with complex interactions in cubic terms. We use the same constants in the equations as in Qian et al. (2022).

\[_{1}=0.75-0.1x_{1}-x_{1}x_{2}^{2}\\ _{2}=0.1x_{1}-x_{2}+x_{1}x_{2}^{2}\] (16)

Since we assume no knowledge of the symmetries here, we first run LaLiGAN (Yang et al., 2023a) to discover symmetry from data for each system. We then extract the group action and the induced infinitesimal action from LaLiGAN and use them to compute symmetry regularization. Table 2 shows the results of different discovery algorithms with and without symmetry. Comparison is made within each class of methods, i.e. sparse regression and genetic programming. For genetic programming, we use an alternate form of symmetry regularization (37), which will be discussed in Appendix C.2.

Informed by symmetries through regularization terms, equation discovery algorithms have a much higher success probability of discovering the correct equation forms. Also, models with symmetry regularization generally have the lowest parameter estimation errors averaging over all random experiments.

In Appendix C.2, we provide additional results to show that the equations discovered by our method also achieve lower prediction error. We also compare the different regularization options based on Theorem 3.3.

## 6 Conclusion

We propose to incorporate symmetries into equation discovery algorithms. Depending on whether the action of symmetry is linear, we develop a pipeline (Figure 1) with various techniques, e.g. solving the equivariance constraint and using different forms of symmetry regularization, to perform symmetry-informed equation discovery. Experimental results show that our method can reduce the search space of equations and recover true equations from noisy data.

Our methodology applies to time-independent symmetries for autonomous ODEs. These symmetries have relatively simple forms and can be either obtained as prior knowledge or learned by existing algorithms. In theory, our method can be generalized to other settings, such as time-dependent symmetries and non-autonomous ODEs. We provide a more general problem definition and discuss these potential generalizations in Appendix A.2. However, in these problems, it is less likely that we know the symmetries before discovering the equations. Extending our symmetry-informed methodology to these scenarios would require further breakthroughs in learning symmetries for general differential equation systems, which could be an interesting future research direction.