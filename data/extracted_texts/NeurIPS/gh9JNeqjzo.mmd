# Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form Color Estimation Method

Qihang Fang1,2,*  Yafei Song3,*  Keqiang Li1,2  Liefeng Bo3

1State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation,

Chinese Academy of Sciences

2School of Artificial Intelligence, University of Chinese Academy of Sciences

3Alibaba Group

1,2{fangqihang2020,likeqiang2020}@ia.ac.cn

3{huiazhang.syf,liefeng.bo}@alibaba-inc.com

Both authors contributed equally to this work.

###### Abstract

A neural radiance field (NeRF) enables the synthesis of cutting-edge realistic novel view images of a 3D scene. It includes density and color fields to model the shape and radiance of a scene, respectively. Supervised by the photometric loss in an end-to-end training manner, NeRF inherently suffers from the shape-radiance ambiguity problem, _i.e.,_ it can perfectly fit training views but does not guarantee decoupling the two fields correctly. To deal with this issue, existing works have incorporated prior knowledge to provide an independent supervision signal for the density field, including total variation loss, sparsity loss, distortion loss, _etc_. These losses are based on general assumptions about the density field, _e.g._, it should be smooth, sparse, or compact, which are not adaptive to a specific scene. In this paper, we propose a more adaptive method to reduce the shape-radiance ambiguity. The key is a rendering method that is only based on the density field. Specifically, we first estimate the color field based on the density field and posed images in a closed form. Then NeRF's rendering process can proceed. We address the problems in estimating the color field, including occlusion and non-uniformly distributed views. Afterwards, it is applied to regularize NeRF's density field. As our regularization is guided by photometric loss, it is more adaptive compared to existing ones. Experimental results show that our method improves the density field of NeRF both qualitatively and quantitatively. Our code is available at https://github.com/qihangGH/Closed-form-color-field.

## 1 Introduction

A neural radiance field (NeRF)  is a cutting-edge technique in computer graphics and computer vision that enables the synthesis of realistic novel view images of a 3D scene from a collection of posed 2D images. It represents a 3D scene with a density and a color field, which describe the scene's shape and radiance, respectively. Despite its impressive novel-view synthesis capability, the NeRF suffers from an inherent shape-radiance ambiguity problem . To be more specific, a family of degradation solutions exist, which can perfectly recover training views but produce incorrect shape. As a consequence, poor novel views would be rendered.

The shape-radiance ambiguity exists because a NeRF-based method usually renders an image using the Max volume rendering algorithm . It integrates the density and color fields closely. As shown in Fig. 1(a), a pixel is rendered as the sum of products of density and color values. In general, such a rendering process is supervised by the photometric loss in an end-to-end training manner, where thedensity and color fields do not receive independent supervision signals. As a result, NeRF can either modify its density or color field to reduce the loss. Unfortunately, performing the latter may well fit the training views, but neglect the error in shape.

To reduce the ambiguity, previous work focuses on different aspects. A group of methods utilizes additional assumptions, _e.g._, sparse or dense depth priors [5; 25; 22; 29], to bypass the influence of the color field and directly supervise the density field. A group of other methods improves NeRF's representation, _e.g._, by modeling the surface normal , to inherently reduce the ambiguity. Besides, a group of methods incorporates prior belief to the original NeRF's representation for regularization. Typical ones are volume-based loss, such as sparsity [10; 7] and total variation (TV) loss [26; 7], and ray-based loss, such as background entropy loss [15; 27], ray-entropy loss , and distortion loss . These losses directly supervise the density field with reasonable prior knowledge, so that the shape-radiance ambiguity can be alleviated. We typically focus on the third group of methods and their effects on explicit NeRF models, which are well-known for their training and rendering efficiency. Despite the efficacy of learning a better density field, most existing regularizations are not aware of the geometry of a scene. They are based on some general assumptions, _e.g._, the space tends to be sparse, the density should distribute smoothly in the space, and it should distribute compactly along a ray. Thus, such regularizations may be less adaptive or too aggressive.

In this paper, we propose a more adaptive method to regularize the density field, which elegantly breaks the entanglement of the shape and radiance in NeRFs. Specifically, we devise a rendering method that is _only based on the density field_ as shown in Fig. 1(c). The key is to first estimate the color field given the density field and posed training images by using predefined rules. Then, the NeRF's rendering process can proceed as usual. Notably, the color field is not learnable here, and so the rendering results solely depend on the density field. In this case, the photometric loss can well supervise the scene shape. Calculating the color field from a density field, however, faces several hurdles, such as occlusion and non-uniformly distributed views. We tackle them by weighting the observed colors with transmittance and using a residual color estimation scheme, respectively. Overall, our regularization is more adaptive to specific scene geometry compared with existing volume and ray-based ones, as it is guided by the photometric loss. The results of two typical explicit NeRFs, including Plenoxels  and Direct Voxel Grid Optimization (DVGO) [27; 28], on the NeRF Synthetic , LLFF , and DTU datasets  demonstrate the superiority of our method. Overall, our contributions are summarized as follows:

1. We propose a closed-form color field estimation method given the density field and posed images. We deal with problems that include occlusion and non-uniformly distributed views. Experimental results show that the estimated color field has acceptable rendering quality.
2. We use the estimated color field for regularization, and reduce shape-radiance ambiguity accordingly. Our method is aware of scene shape and so more adaptive. Experimental results show that our loss improves the shape both qualitatively and quantitatively.
3. We implement our regularization method with CUDA kernels. Thus, the computational cost for training a scene is still acceptable.

## 2 Related Work

**Novel-view synthesis**. The NeRF  has revolutionized the novel-view synthesis technology  with its impressive rendering quality. It uses fully-connected neural networks to map a 3D location and viewing direction to a density and view-dependent color. Then a novel view is synthesized by using the MAX volume rendering algorithm . Besides the implicit neural network representation,

Figure 1: Graphical representations of the NeRF and its regularizations.

explicit NeRFs [14; 7; 27; 28; 21; 2] are proposed to accelerate both training and rendering speed. The density and color field are represented by voxel grids, which store the density values or the color features. In this way, a lightweight multilayer perceptron (MLP) should suffice for predicting the view-dependent colors. Furthermore, researchers have paid great effort to optimize the rendering quality in challenging scenarios [1; 16; 18], training speed [14; 7; 27; 28; 21; 31], and inference speed [11; 10; 3; 23; 8; 32]. Despite these improvements, the NeRF's representation of a scene inherently suffers from the shape-radiance ambiguity problem . A complex color field can well fit the training images while leaving an error in geometry. Although Zhang _et al._ analyzes that the NeRF relieves the problem by using an MLP with limited capacity and low-frequency positional encoding components, the shape-radiance ambiguity problem needs to be solved more systematically.

**Regularization methods for the density field**. The regularization methods can be divided into volume-based and ray-based ones. The most simple volume-based regularization is the sparsity loss [10; 32; 7]. It is based on the fact that most of the space in a scene is empty. Another effective regularization is the TV loss [26; 7], which assumes that the density values of adjacent voxels should not change drastically. It helps increase the smoothness of the density field. Ray-based regularization methods, on the other hand, impose prior beliefs on how the density along a ray should distribute. The background entropy loss [15; 27] forces the final transmittance of a ray to be either \(1\) or \(0\), which means that the ray can only belong to the background or the foreground. This can effectively sharpen the contour of an object. The ray-entropy loss  is an improved version of the background entropy loss. It argues that the density values along a ray should have a low entropy. This is achieved by minimizing the Shannon entropy of the normalized ray density. Such a regularization helps produce better geometry than the vanilla NeRF. The distortion loss  shares a similar idea with the ray-entropy loss. It encourages the density along a ray to distribute as compactly as possible. This is achieved by optimizing the non-empty intervals to be narrow and near to each other. Experimental results show that the distortion loss effectively removes the floaters in the free space.

Overall, we notice that the above volume and ray-based methods are all based on general assumptions. They are directly applied to the density field, and so may be less adaptive. In this paper, we propose a more adaptive method to regularize the density field, which is guided by the photometric loss.

## 3 Preliminaries

A radiance field is composed of a density field \(^{}\) and a color field \(^{c}\). The density \(^{}_{}\) of a point \(^{3}\) indicates the differential probability of a ray hitting a particle at \(\), and \(^{c}_{,}^{3}\) represents the color emitted by \(\) along the direction \(\). The NeRF uses volume rendering to render the color \(()\) of a ray \((t)=+t\) that starts at a camera's original point \(\) following the Max  rendering equation

\[()=_{t_{n}}^{t_{f}}T(t)^{}_{ (t)}^{c}_{(t),}\;dt,\] (1)

where \(t_{n}\) and \(t_{f}\) are near and far bounds for the integral, and the \(T(t)\) is the transmittance defined as

\[T(t)=(_{t_{n}}^{t}-^{}_{(s)}ds).\] (2)

To deal with the integral, the NeRF assumes that the density and color are piece-wise constant along the ray \(\). Thus, the integrals above become summations.

Given a collection of images and their corresponding camera poses. A photometric loss \(_{p}\) is applied to minimize the difference between a rendered pixel color \(()\) and its ground-truth pixel value \(C()\) as follows,

\[_{p}=|}_{}| |C()-()||_{2}^{2},\] (3)

where \(\) is a mini-batch of rays.

In such a formulation, let \(^{wc}_{(t),}=T(t)^{}_{(t)}^{c}_{(t),}\) denotes the transmittance-density-weighted color field, the photometric loss in effect optimizes \(^{wc}_{(t),}\) to fit training views. An optimal \(^{wc}_{(t),}\) learned on training views, however, does not guarantee a correct density field, since it can be achieved by infinitely possible combinations of density and color fields. This raises the shape-radiance ambiguity problem.

Method

The method is divided into two parts. First, we elaborate on the color field estimation method given a density field and posed images. Then we use the estimated color field for regularization.

### Closed-Form Color Field Estimation

The key problem in this subsection is to estimate a color field \(}^{c}_{,}\) as accurately as possible given a density field \(^{}_{}\) and posed observation images.

**Spherical harmonics representation for the color field**. We use spherical harmonics (SH) to represent the color of each point. The SH includes a series of orthogonal and complete basis functions defined on the surface of the unit sphere \(\). Following , a linear combination of SH basis functions is used to approximate the color of a point \(\) observed from a direction \(\) as

\[^{c}_{,}=_{=0}^{L}_{m=-}^{ }h_{}^{m}Y_{}^{m}(),\] (4)

where \(L\) is the degree of the SH basis, \(Y_{}^{m}()\) is the SH function with a degree \(\), order \(m\), and \(h_{}^{m}\) is its coefficient. Note that \(h_{}^{m}\) is a function of \(\). We omit its notation for brevity. Under this formulation, the color estimation problem is equivalent to estimating the SH coefficients.

Similar to Fourier series, the coefficient of a specific frequency band can be obtained by integrating the product of the color and the corresponding SH function as follows,

\[ h_{}^{m}&=_{ }^{c}_{,}Y_{}^{m}( )d\\ &=_{}p_{}() {^{c}_{,}Y_{}^{m}()}{p_{}()}d\\ &_{k=1}^{K}^{c}_{,_{k}}Y_{}^{m}(_{k})}{p_{}(_{k})}, \ \ _{k} p_{}(),\] (5)

where \(p_{}()\) is the probability density function of directions, and the integral is approximated by Monte Carlo sampling. In our case, we assume that the direction should be uniformly distributed and \(p_{}()=\). However, we cannot randomly select the direction since its color could not be observed. Actually, we only know the samples \(_{k}\) from posed images.

**Transmittance-weighted color for occlusion handling**. The posed images provide a set of \(\{_{k},^{c}_{,_{k}}\}\) for Eq. (5). We use a colored rectangle as an example scene as shown in Fig. 2, where \(\) is a point on its surface. Specifically, there are four cameras with centers \(_{k},k=1,2,3,4\), and the direction \(_{k}=_{k}-}{\|_{k}-\|}\) is a normalized vector that connects \(_{k}\) and \(\). Assume that the projection matrix of the \(k^{th}\) camera is \(_{k}\), the \(^{c}_{,_{k}}\) is the observation color at \(_{k}\) of the image plane. Ideally, if there is no occlusion, the radiance emitted by \(\) can be directly captured by a camera (\(_{2}\) and \(_{3}\)). Then, the radiance is the same as the corresponding observation color (\(^{c}_{,_{2}}\) and \(^{c}_{,_{3}}\)). However, it is possible that the point \(\) is occluded (\(_{0}\) and \(_{1}\)), in which case the observation color (\(^{c}_{,_{0}}\) and \(^{c}_{,_{1}}\)) is not the radiance emitted by \(\), but the radiance of a surface point that is closest to the camera center. These observations should be treated differently when estimating the SH coefficients. We resort to the transmittance \(T_{,k}\) between \(\) and \(_{k}\), _i.e._,

\[T_{,k}=(_{0}^{\|_{k}-\|}-^{}_{(s;,_{k})}\ ds),\] (6)

Figure 2: Observation colors and transmittance.

where \((s;,_{k})=+s_{k}\). As \(T_{,k}\) exponentially accumulates the density along a ray, an occluded ray has a very low transmittance. Thus, we use it as a weight factor and modify Eq. (5) as follows,

\[_{}^{m}=^{K}T_{,k}}_{k=1}^{K},k}_{,_{k}}^{}Y_{}^{m}( _{k})}{p_{}(_{k})}.\] (7)

**Residual estimation for non-uniformly distributed views**. The estimation given by Eq. (7) is strongly affected by non-uniformly distributed sample directions. To calculate the coefficient of a frequency band, the integral in Eq. (5) relies on the orthogonality of the SH basis that

\[_{}Y_{}^{m}()Y_{i}^{j}()d =1,&i=j=m,\\ 0,&\] (8)

When we discretize the integral with samples that are non-uniformly distributed on the sphere \(\), Eq. (8) may be violated. For simplicity, we use the product of two trigonometric functions \( 0x x\) as an example. As shown in Fig. 3, the positive and negative values in uniform samples better compensate each other to approach a zero-value summation, while the non-uniform case generates a large bias.

To reduce the estimation bias, we propose a residual estimation scheme. The basic idea is to subtract the terms that integrate to zero in the integral to reduce their influences. Specifically, we further modify Eq. (7) as follows,

\[_{}^{m}=^{K}T_{,k}}_{k=1}^{K} {T_{,k}}_{k} Y_{}^{m}( _{k})}{p_{}(_{k})},\] (9)

with \(}_{k}\) defined as

\[}_{k}=_{,_{k}}^{c}-_{i=0} ^{}_{j=-i}^{}_{i}^{j} Y_{i}^{j}(_{k}).\] (10)

where \(=m\) if \(i=\), else \(=i\). This scheme can effectively reduce estimation biases. Please refer to the Appendix A for more explanations.

### Regularization with the Estimated Color Field

We use the estimated color field \(}_{,}^{c}\) to reduce the shape-radiance ambiguity. Specifically, we can render a closed-form pixel \(C_{cf}()\) with \(}_{,}^{c}\) and \(_{}^{}\) following the volume rendering equation as per Eq. (1). Then, a closed-form photometric loss (CF loss for short) can be calculated as follows,

\[_{cf}=_{cf}|}_{r _{cf}}\|C()-C_{cf}()||_{2}^{2},\] (11)

where \(_{cf}\) is a mini-batch of rays. With a weight factor \(\), the regularization term \(_{cf}\) is added to the training loss for reducing the shape-radiance ambiguity.

## 5 Experiments

### Experimental Settings

**Datasets**. To evaluate our method, we conduct experiments on DTU , NeRF synthetic , and LLFF datasets . Each DTU scene has \(49\) or \(64\) images. We evenly select \(5\) or \(6\) images

Figure 3: Non-uniform samples generate a larger bias.

from them as testing ones. Besides, we downsample the \(1600 1200\) images to be \(800 600\) ones. For NeRF synthetic and LLFF datasets, we follow the conventional train-test split used in previous studies. By default, we use a black background for DTU and NeRF synthetic datasets, as it makes the shape-radiance ambiguity more severe on some scenes.

**Baselines**. We choose two explicit NeRFs, including Plenoxels  and DVGO , as baselines. Among them, the Plenoxels has been trained with the sparsity loss and TV loss, and DVGO has been trained with the background entropy loss.

**Metrics**. Besides the commonly applied peak signal-to-noise ratio (PSNR) metric of images, we calculate the PSNR of depth for the NeRF Synthetic dataset to evaluate the geometry, since it provides the ground truth depth. Depth values are normalized into the range \(\) before calculating the PSNR. Additionally, we adopt the metric named inverse mean residual color (IMRC)  to evaluate the geometry of a scene. It is defined as the transmittance-density-weighted residual color in dB as follows

\[=-10_{10}}_{k}T_{ ,k}(1-(-_{}^{} _{}))(}_{,k} )^{2}}{_{}_{k}T_{,k}(1- (-_{}^{}_{}) )},\] (12)

where \(T_{,k}\) is the transmittance defined in Eq. (6), \(}_{,k}\) is the final residual color defined in Eq. (10), and \(_{}\) is the half-width of a voxel as the step size. To get deeper insights into Eq. (12), it is a weighted mean squared error of the radiance emitted from all directions \(_{k}\) of all points \(\). The error or residual color \(}_{,k}\) of a point \(\) at a direction \(_{k}\) is the difference between the ground truth observation color and recovered radiance based on the geometry. The better geometry should better recover the color field. Thus, the higher the IMRC, the lower the residual color, the better.

**Implementation details**. The codes of Plenoxels and DVGO are borrowed from the official release. A uniform probability distribution \(p_{}()\) of directions is applied. In other words, \(p_{}()\) is a constant value. Please refer to Appendix B.1.1 for more discussions about the probability density function of directions. We implement the CF loss with CUDA kernels. For explicit NeRFs, the 3D space is divided into voxel grids. We need to estimate the color field for the voxels. Specifically, the block number is set as the number of voxels. Each block has 128 threads and each thread handles a ray that connects the voxel and a camera. Following , we set the SH degree to \(2\), _i.e._, there are \(3*(2+1)^{2}=27\) SH coefficients per voxel. For Plenoxels, the number of rays \(|_{cf}|\) is set to \(25\) for all datasets. The weight factor \(\) is set to \(10\), \(0.1\), and \(0.5\) for the DTU, NeRF synthetic, and LLFF dataset, respectively. For DVGO, the number of rays \(|_{cf}|\) is set to \(10\) for all datasets. The weight factor \(\) is set to \(1\) and \(0.1\) for the coarse and fine stage of the DTU dataset, \(2\) and \(0.1\) for the coarse and fine stage of the NeRF synthetic dataset, and \(0.1\) for the LLFF dataset. All the experiments are run on a single NVIDIA Tesla A100 GPU.

### Results of the Closed-Form Color Field Estimation Given Trained Density Fields

In this subsection, we verify the effectiveness of the closed-form color estimation. Specifically, we train Plenoxels and DVGO with their original implementations without adding the CF loss. Then, the density fields of these trained models are used to estimate color fields without any training process.

The occlusion handling and residual estimation play significant roles. We conduct an ablation study here. The results based on Plenoxels' density fields are reported in Table 1. In Fig. 4, we additionally showcase an example based on a DVGO's density field. Specifically, when we directly estimate SH coefficients according to Eq. (7) without using the residual estimation scheme, the rendered images look bad as shown by the first two sub-figures. This is because estimating the SH coefficient of one frequency band is affected by other frequency components if we cannot uniformly sample directions from the 2D sphere. Since the images of each DTU scene are approximately taken from a half sphere, the views are non-uniformly distributed. As a result, the estimation of SH coefficients suffers from biases. On the other hand, when we estimate without occlusion handling, _i.e._, without using transmittance to weight an observation, the estimation result is blurred as shown by the third sub-figure. This is because in the estimation, \(_{,_{k}}^{c}\) of different \(_{k}\) is not guaranteed to be the radiance emitted from the same point \(\), but possibly that of a point

   Occlusion handling & Residual estimation & PSNR \\   & & 14.61 \\ ✓ & & 13.57 \\  & & ✓ \\ ✓ & ✓ & **26.49** \\   

Table 1: Ablation study on DTU scenes based on Plenoxels’ density.

nearer to a camera. When both occlusion handling and residual estimation are applied, as shown in the fourth sub-figure, the rendering result is visually acceptable, and quantitatively, has a much higher PSNR that approaches the trained one. To summarize, both the table and figure show that the residual estimation improves the PSNR considerably. With only occlusion handling, the PSNR decreases, but when it is combined with the residual color estimation, the PSNR is further improved.

The estimation results are not only affected by the algorithm, but also by the quality of the density field. Specifically, the estimation is based on an assumption that, if a point is not occluded, its radiance along a direction is directly determined by the corresponding observation color in the image. This assumption is true for the ideal case, where the density is sharp enough and the color reduces to the surface light field . By using this assumption, the closed-form color field becomes sensitive to false geometry. If the geometry or the density field is not correct or sharp enough, which breaks this assumption, the color estimation results will be bad. In Fig. 5, we show the estimation results based on the density field of DVGO and Plenoxels, respectively. We also visualize the depth map of the scene and render the residual color. The Plenoxels suffers a sharper PSNR drop from the trained image (\(34.06\)) to estimated one (\(29.09\)). This is because it has an inferior density field as qualitatively shown by the depth maps. The errors in the density field affect the estimation process and result in poor rendering quality. In contrast, a better density field given by the DVGO permits a higher PSNR (\(33.08\)) of the estimated color field. Note that the closed-form color field is more sensitive to the quality of density field than the trained one, as it produces a larger PSNR difference (\(3.99>1.35\)) between two methods. By using the closed-form photometric loss, the rendering errors caused by an inferior density field can be back-propagated to improve the density field.

### Results of Regularization and Comparison

In this subsection, we analyze the regularization results. Specifically, we apply our regularization to Plenoxels and DVGO and train from scratch. Based on DVGO, we additionally add a fine-tuned distortion loss for comparison. Table 2 reports the PSNR, IMRC, and PSNR of depth.

For Plenoxels, the CF loss improves all metrics on all datasets. It effectively improves the geometry of a scene. As shown in Fig. 6, the floaters in the free space (marked by red circles) are penalized,

Figure 4: Ablation study on DTU scan 122 based on DVGO’s density.

Figure 5: Color field estimation results of DTU scan 63.

and the density field becomes sharper (marked by green circles). This is reasonable because the observation colors of the points in wrong geometry, _e.g._, floaters or a thick surface, tend to be very high frequency. The SH of degree \(2\) fails to fit such a high-frequency color variation, and the estimated color field may produce poor rendering results, as represented by a high CF loss. During back-propagation, the CF loss can guide to a better density field.

For DVGO, the best results for all metrics on all datasets are obtained when the CF loss is added or both CF loss and distortion loss are added. These two losses complement each other's advantages, but the CF loss is more sensitive to specific geometric errors. We visualize the rendered images, depth, and residual color of several scenes for better comparison. As shown in Fig. 7, the DVGO fails to recover two details of a house as marked by blue and red rectangles. The distortion loss relieves the problems, but the windows in the red rectangle are still distorted. We try to further increase the weight factor of the distortion loss but this does not help. In contrast, our loss significantly improves the quality of the density field as shown by the depth maps and residual colors. Two more examples are shown in Fig. 8. It is noteworthy that the scene with a mic is trained with abundant views (100 views) uniformly distributed in the upper hemisphere. Under a black background, however, it severely suffers from the shape-radiance ambiguity problem. A part of the wire is confused with the background, and lots of floaters exist around the wire. The distortion loss makes no difference as it is not aware of the geometry of the scene. The CF loss successfully distinguishes the wire and background, producing a better density field. The right scene in Fig. 8 is another example. The DVGO, with or without distortion loss, mistakenly reconstructs the upper surface of the bottom rock. This mistake is avoided by our method. Overall, the CF loss effectively improves the geometry of a scene qualitatively and quantitatively.

   Method & DTU & NeRF synthetic & LLFF \\  Plenoxels  & 31.86 / 15.88 & 29.83 / 16.55 / 22.70 & 26.30 / 19.15 \\ Plenoxels  + CF loss & **32.08 / 16.66** & **29.84 / 16.59 / 23.08** & **26.40 / 20.02** \\  DVGO  & 32.15 / 18.19 & 31.58 / 17.25 / 25.46 & 26.23 / 18.87 \\ DVGO  + Distortion loss  & 32.20 / 18.47 & 31.50 / 17.93 / 25.68 & 26.33 / 20.76 \\ DVGO  + CF loss & 32.23 / 18.51 & **32.24 / 17.61 / 26.36 & 26.26 / 19.66 \\ DVGO  + Distortion loss  + CF loss & **32.26 / 18.80** & 32.23 / **18.90 / 26.68** & **26.34 / 20.92** \\   

Table 2: Comparison of PNSR \(\)/IMRC \(\)/PSNR of depth\(\)) on the three datasets (**Best**).

Figure 6: Our loss improves the geometry of a scene by making the density field sharper and removing the floaters in the free space. The two scenes are DTU scan 37 and 105 from left to right.

### Computational Cost

In this subsection, we analyze the computational cost. Specifically, in each training step, a batch of \(B\) rays are used. Then, only the colors of the voxels intersected with these rays need to be estimated. Suppose that each ray involves the order of \(N\) voxels, there are \(M=BN\) voxels. To estimate the color of a voxel, the density values along the ray from the voxel to \(K\) source cameras need to be calculated. Then the total number of voxels involved is of order \(KMN=KBN^{2}\). In our implementation, each CUDA block handles one of the \(M\) voxels, and each thread handles one of the \(K\) cameras. As the threads are executed in parallel, in theory, this

Figure 8: Comparison of distortion loss and CF loss on the mic from the NeRF synthetic dataset and DTU scan 40.

   \(B\) & DTU & NeRF synthetic & LLFF \\ 
0 & 17.7 & 16.0 & 24.2 \\
10 & 32.7 & 20.5 & 29.4 \\
25 & 43.0 & 25.5 & 34.6 \\   

Table 3: Training time (minutes) with different batch sizes (\(B\)).

Figure 7: Comparison of distortion loss and CF loss on DTU scan 24.

reduces the computation overhead from the order of \(KBN^{2}\) to \(N\). In practice, we observe that sometimes the computational overhead is affected by the batch size. This is due to the hardware limit of the maximum CUDA threads. When an excessive number of threads are used, they are not guaranteed to be executed in parallel. In Table 3, we report the computational cost of training Plenoxels by using different batch sizes for regularization. The number of source cameras \(K\) used are \(44\) or \(58\) for the DTU, \(100\) for NeRF synthetic, and from \(17\) to \(54\) for LLFF dataset. The DTU dataset requires longer training time because it is trained for \(12\) epochs, while the NeRF synthetic dataset is trained for \(9\) epochs. For \(9\) epochs, the DTU dataset costs about \(24\) and \(32\) minutes for \(10\) and \(25\) rays, respectively. Compare these with those of NeRF synthetic dataset, we can observe that the large number of \(K=100\) cameras does not obviously increase the computational burden.

## 6 Conclusion

The shape-radiance ambiguity is a significant problem of NeRFs. Even with a simple background and abundant training images, wrong geometry can be learned and poor novel-views can be rendered. In this paper, we propose a new regularization method to break the entanglement of density and color field. The key is a closed-form color estimation method, which can recover the color field of a scene given a density field and a set of posed training images. We overcome the difficulties in estimating the color fields by handling the occlusion and non-uniformly distributed views. The image quality rendered by our closed-form color field approaches the trained one. Then, we use the photometric loss derived from the estimated color field to provide an independent supervision signal for the density field. Both the PSNR and IMRC of two explicit NeRFs, including Plenoxels and DVGO, are improved. Experimental results show the capability of the CF loss to correct geometric errors compared with existing volume and ray-based losses.

**Limitations and future work**. The closed-form color field has difficulties to perfectly recover highly reflective objects. As shown in Fig. 9, the surface of the scissors is less reflective compared to the ground truth. Although increasing the SH degree can relieve this problem to some extent as shown in Appendix B.1.2, it needs to be solved more systematically. For example, the probability density function \(p_{}()\) of directions can be better estimated, and a rigorous proof of the bias reduction strategy needs to be further studied. Besides, a well-estimated color field has the potential to get rid of the parameterized color field in a NeRF, and then only the density field needs to be trained and stored.