# HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution

Eric Nguyen\({}^{*1}\), Michael Poli\({}^{*,1}\), Marjan Faizi\({}^{2,*}\),

Armin W. Thomas\({}^{1}\), Callum Birch Sykes\({}^{3}\), Michael Wornow\({}^{1}\), Aman Patel\({}^{1}\),

Clayton Rabideau\({}^{3}\), Stefano Massaroli\({}^{4}\), Yoshua Bengio\({}^{4}\), Stefano Ermon\({}^{1}\),

Stephen A. Baccus\({}^{1,}\), Christopher Re\({}^{1,}\)

Equal contribution. \(\) Equal senior authorship. \({}^{1}\)Stanford University. \({}^{2}\)Harvard University. \({}^{3}\)SynTensor. \({}^{4}\)Mila and Universite de Montreal. \({}^{2}\)On benchmarks from Nucleotide Transformer, HyenaDNA uses a model with 1500x fewer parameters (2.5B vs 1.6M) and 3200x less pretraining data (3202 vs 1 human reference genome).

###### Abstract

Genomic (DNA) sequences encode an enormous amount of information for gene regulation, protein synthesis, and numerous other cellular properties. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution (i.e. DNA "characters") where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, **a genomic foundation model** pretrained on the human reference genome with **context lengths of up to 1 million tokens at the single nucleotide-level** - an **up to 500x increase** over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), **uses single nucleotide tokens**, and has **full global context at each layer**. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On a long-range species classification task, HyenaDNA is able to effectively solve the challenge by increasing the context length to 1M without downsampling. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.2 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification. Code available at https://github.com/HazyResearch/hyena-dna.

## 1 Introduction

Understanding and learning from DNA sequences has long been a goal of biologists and deep learning researchers, as its "language" encodes instructions essential for all living things . The mapping from DNA instructions, genotypes, to observable function and traits, phenotypes, remains ongoing research effort. Towards this goal, researchers have proposed using foundation models (FMs) in genomics to learn generalizable features from unstructured whole genome data that can then be fine-tuned for a number of tasks including predicting the location and function of genes, identifying regulatory elements, and analyzing the evolution of species . In contrast to protein sequences, which have had successes in protein language models , DNA sequences are orders of magnitudes longer (e.g. the human genome is 3.2B nucleotides) with long-range dependencies and interactions that span over 100k+ nucleotides in length . Overcoming the long-range limitations of current generation models could help drive the next wave of innovations in AI-powered drug discovery and therapeutics, and enable genomic FMs to understand and learn in-context whole patient genomes in a personalized way.

Limitations of current modelsPrevious genomic FM approaches have relied on attention-based Transformers , but face a number of challenges unique to DNA sequences. The attention mechanism scales quadratically in sequence length, with current genomic FMs pretraining on only 512 to 4,096 tokens as context , <0.001% of the human genome. Also prevalent is the reliance on fixed k-mers, akin to DNA "words", and tokenizers to aggregate meaningful DNA units. However, single nucleotide alterations represent physical analogs where, for example, single nucleotide polymorphisms (SNPs) and mutations can have a profound impact on biological properties including regulatory activity . In contrast, natural language semantics can often be conserved when single character or word changes occur over very long contexts. Therefore, having both **long-range context** and **single nucleotide resolution** simultaneously is critical, and remains a particular challenge in genomics.

Figure 1.1: HyenaDNA recipe for long-range foundation models in genomics. The HyenaDNA architecture is a simple stack of Hyena operators  trained using next token prediction. (See Fig. 1.3 for block diagram of architecture). We introduce a new sequence length scheduling technique to stabilize training, and provide a method to leverage the longer context length to adapt to novel tasks without standard fine-tuning by filling the context window with learnable soft prompt tokens.

Figure 1.2: Pretraining on the human reference genome using longer sequences leads to better perplexity (improved prediction of next token).

Toward longer context modelsRecently, Hyena , a large language model based on implicit convolutions, was shown to match attention in quality while reducing computational time complexity, thereby allowing a longer context to be processed. Hyena uses a parameter-efficient **global convolutional filter** along with a **data-controlled gating** mechanism, which enables a context-specific operation over every token. Indeed, Hyena showed that for simple associative recall tasks using _synthetic_ data, a shallow 2 layer model could effectively process context lengths at 131k tokens. We hypothesize that Hyena's core operations can unlock the potential to capture both the long-range and single nucleotide resolution of _real_ genomic sequences over attention-based approaches. To test this, we explore two questions: **(i.) Can a convolutional long-context model be used effectively at single nucleotide resolution? (ii.) What new capabilities could long-context genomic foundations models enable?**

HyenaDNAThe result of our investigation is, a genomic FM pretrained on the human reference genome at **context lengths up to 1 million tokens at single nucleotide resolution** - an up to **500x increase** over existing genomic FMs using dense-attention. **HyenaDNA** scales sub-quadratically in sequence length (training up to 160x faster than attention at sequence length 1M), uses single nucleotide tokens, and has a global receptive field at each layer. Our contributions include a "full-stack" recipe for building genomic FMs, including architecture design, a warm-up schedule to speed up training on ultralong sequences, and an efficient downstream adaptation procedure based on soft prompting and in-context learning.

Full-stack genomics modelingWe start with a decoder-only Hyena architecture pretrained using next nucleotide (token) prediction. We forego standard aggregating tokenizers, using a single-character tokenizer and a minimal DNA vocabulary of 4 nucleotides (plus special tokens). Training stability becomes an issue at ultralong sequences (200k+). To overcome this issue, we introduce a sequence length warm-up scheduler that gradually increases sequence length in stages. At sequence length 450k, training time is reduced by 40%, while boosting accuracy by 7.5 accuracy points on a species classification task. Furthermore, we design downstream adaptation procedures to leverage longer context windows, as simpler and more flexible alternatives to standard fine-tuning in genomics. This includes a novel soft prompt technique where learnable tokens (up to 32k) are injected directly into the input sequence itself, enabling competitive downstream results without the need to update a pretrained model.

Genomic downstream tasksWe apply our pretrained models to 29 diverse downstream genomic tasks to showcase its long-range ability as well as fine-grain resolution. On fine-tuned benchmarks from the Nucleotide Transformer , achieves state-of-the

Figure 1.3: HyenaDNA block architecture. A Hyena operator is composed of long convolutions and element-wise gate layers. The gates are fed projections of the input using dense layers and short convolutions. The long convolutions are parameterized _implicitly_ via an MLP that produces the convolutional filters. The convolution itself is evaluated using a Fast Fourier Transform convolution with time complexity \((L_{2}L)\).

art (SotA) on 12 of 18 datasets while using a model with orders of magnitude less parameters and pretraining data (see Tab. 4.2). On the GenomicBenchmarks , HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer function identification. On a novel species classification task, HyenaDNA effectively solves the challenge by increasing the context length to 1 million tokens. In a challenging chromatin profile experiment, a 919-way multi-task, HyenaDNA performs competitively against a larger SotA sparse-attention BigBird Transformer . Finally, we analyze the learned embeddings of a pretrained HyenaDNA model by clustering sequences by biotype (gene or transcription type) and compare the results with existing genomic FMs, showing that HyenaDNA can serve as an effective universal featurizer in genomics.

## 2 Preliminaries and Related Work

### Transformers and Attention

Powering many recent _foundation models_ is the _attention_ mechanism. Given a length-\(L\) sequence \(x^{L D}\), a (single-headed) layer of _scaled self-attention_[2; 51] is a map from \(^{L D}\) to \(^{L D}\) which performs the following operations:

\[(x)=(x_{q}_{k}^{}x^{}), y= (x)x_{v}\] (2.1)

where \(D\) is the embedding dimension, \(_{q},_{k},_{v}^{D D}\) are learnable linear maps and \(\) indicated row-wise softmax (and optional scaling). Attention computes all pair-wise comparison for every token, and scales as \((L^{2})\) in sequence length. This allows a global context at high resolution, but limits the size of the context on current hardware.

Previous methods to reduce the quadratic cost of attention have used specialized methods to approximate full dense attention . In sparse attention, elements attend only to a subset of all other positions. Alternatively, linear attention methods construct approximations to \((u)\) that can be evaluated in subquadratic time. Both of these classes of methods, however, trade lower time complexity (allowing longer sequences) for loss in expressivity.

### Long Context Strategies in Genomics

To achieve longer context, genomic models have relied on two strategies: i. tokenization and ii. dilation and downsampling. Tokenization is a necessary step in masked language modeling (MLM) with bidirectional Transformer architectures (BERT) , a common model in genomics. These tokenizers use fixed k-mers (short overlapping sequences of length k) or frequency-based byte pair encoding (BPE), that attempt to aggregate DNA into meaningful units [25; 55]. Consequently, these aggregation techniques create large new vocabularies (compared to the natural vocabulary of 4 nucleotides) that are less generalizable . The second strategy uses dilated convolutions and downsampling, both of which essentially average or skip elements between weights . A canonical example is the Enformer, which uses dilation and downsampling to reach context lengths of 100k nucleotides to predict gene expression tracks . Common across tokenization, dilation, and downsampling is the sacrifice of single nucleotide resolution to reach longer context.

### Large Convolutional Models

A discrete convolution between an input \(x\) of length \(L\) and a (learnable) filter \(h\) is given by:

\[y_{t}=(h*x)_{t}=_{t^{}=0}^{L-1}h_{t-t^{}}x_{t^{}}  y=x.\] (2.2)

where \(^{L L}\) is the Toeplitz matrix corresponding to the convolution. Historically, convolutions have played an important role in deep learning and more broadly signal processing. More recently, it has been shown that by stacking \(k\) long convolution layers, where \(k\) is parametrized through a function \(_{}\) i.e. \(k:=_{}(L)\), one can achieve state-of-the-art performance on a variety of benchmarks involving long sequences, for example the Long Range Arena (LRA) [48; 24; 47; 19]. Different \(_{}\) have been proposed in the literature: state-space models [24; 19], and implicit parametrizations via neural fields [45; 44; 37]. On language, the \(\)-family of implicit convolution language models,H3 and Hyena,  used long convolutions and gating to match Transformer performance in \((L_{2}L)\) time, notably lower than the \((L^{2})\) of attention-based models.

HyenaDNA takes inspiration from these approaches, showing that attention-free, long-context causal models can achieve high performance on downstream genomic tasks. These extended long-range capabilities enable us to explore new paradigms in genomics, such as in-context learning to easily adapt to new tasks without updating pretrained models.

## 3 HyenaDNA Long-Range Genomic Foundation Models

In this section, we introduce the HyenaDNA approach to long-range genomic sequence modeling. We start with a description of the model architecture, then discuss sequence length warm-up and soft prompting techniques for downstream adaptation.

### The HyenaDNA Model

The HyenaDNA model is a decoder-only, sequence-to-sequence architecture defined by a stack of blocks consisting of a Hyena operator , followed by a feed-forward neural network (see Fig. 1.3).

Given an input \(x^{L}\) (\(L\) denotes sequence length), a Hyena3 operator can be defined as:

\[(x_{1},x_{2},v)&(x_{ 1},x_{2})v\\ (x_{1},x_{2})&=_{x_{1}}_{h}_{x_{1}}\] (3.1)

where \(x_{1}\), \(x_{2}\), \(v\) are projections of the input, and \(_{h}^{L L}\) is the Toeplitz matrix constructed from a learnable long convolution filter produced as the output of a neural network, \((_{h})_{ij}=h_{i-j}\). The convolution filter values themselves are obtained through a small neural network \(_{}\) taking as input the time (position) index and optionally positional encodings, \(h_{t}=_{}(t)\), which enable the operator to process very long sequences without growing linearly in the number of parameters. Further, the matrices \(_{x_{1}},_{x_{2}}^{L L}\) are constructed with \(x_{1},x_{2}\) on the diagonals, and evaluated as element-wise gating. The projections are obtained by applying a dense linear layer and short convolution to the input sequence, as shown in Figure 3.1.

**Proposition 3.1**.: _A Hyena operator can be evaluated in \((L_{2}L)\) time._

Efficient evaluation is crucial on settings involving extremely long sequences such as genomics. In the general case where the embedding dimension \(D>1\) and \(x^{L D}\), the linear projections \(_{x_{1}},_{x_{2}},_{v}^{D D}\) are right multiplied to \(x\), and \(D\) independent Hyena operators are then applied to each dimension.

### Training Long Sequence Models

TokenizationThe subquadratic cost of HyenaDNA in sequence length allows the model to process ultralong sequences directly at the single nucleotide level without the need for frequency-based aggregation tokenizers. This enables fine-grain resolution for both short and long sequences, critical for detecting single nucleotide polymorphisms or mutations and modeling long-range dependencies in gene expression.

We use the natural DNA vocabulary and refer to each nucleotide as a token. The tokens include "A", "G", "C", "T", and "N" (a non-specific nucleotide) and special character tokens for padding, separation, and unknown characters. Tokens are mapped to embedding dimension \(D\).

Sequence length warm-up for ultralong sequencesDirectly training on long sequences can affect training stability as the variance in gradient increases . Training on shorter sequences initially (followed by longer sequences) was used by  to train small scale Transformers and reduce training time, while  used sequence length warm-up to address stability on up to 2k tokens.

Figure 3.1: The Hyena operator is a combination of long convolutions \(\) and data-controlled gating \(\), and can be a drop-in replacement for attention.

For ultralong sequences (200k+), we develop a new warm-up schedule that gradually increases the sequence length in stages to improve both stability and decrease training time.

Our sequence length schedule starts at \(L_{1}=64\), then doubles the window at each stage while keeping the global batch size constant. By doing so, iterations at each consecutive stage will include more tokens, ensuring the scheduler can also act as a form of batch size warm-up. In Fig. 3.2, we observe sequence length scheduling to be particularly important at sequence lengths greater than \(450\)k, where at this length training time is reduced by 40% and improving ultimate accuracy by 7.5% points for a species classification task described later in section 4.4.3.

### Downstream Adaptation

**Tuneable prompting for long-context models** Prompts have been traditionally used to guide the output of a FM  by prepending additional context to an input. Expanding on this approach, _soft_ tuneable prompting was introduced to inject _learnable_ tokens (as weights) into the input directly  as an alternative to model fine-tuning.

With an extended context length (\(L\)), we're able to explore new paradigms in adapting FMs after pretraining. Given a downstream task with prompts \(x_{p}^{T}\) and corresponding labels \(y_{p}\), we prepend \(N L-T\) trainable parameters \(\) of dimension \(D\) after the embedding step:

\[x[(x_{p}),], x^{L(T+N)}\] (3.2)

The resulting sequences \(x\) are then processed by the model, and \(\) is optimized on a loss function involving the input sequence's label \(y_{p}\). Crucially, soft prompting requires utilization of a small subset of prompt and label pairs to optimize \(\).

During soft prompting, \(\) only optimizes the parameters of the prompt in the input sequence while keeping all other model parameters fixed. Soft prompting thereby provides a flexible and computationally efficient approach to adapting genomic FMs to new downstream tasks.

## 4 Experiments

In 4.1, we start with pretraining \(\) on the human reference genome . We then evaluate \(\) on existing short-range (<5k nucleotides) downstream benchmarks in 4.2 to assess the performance of single nucleotide resolution. In 4.3, we explore what new capabilities emerge with longer range genomic modeling in the form of in-context learning. Finally, we push the limits of ultralong context performance in 4.4.

### Pretraining on the Human Genome

We pretrain \(\) on the human reference genome  using next nucleotide (token) prediction. Starting with a stack of decoder-only Transformer blocks, we swap attention for the Hyena operator, and compare against a baseline Transformer (GPT) with Flash Attention . We add gradient checkpointing to \(\) to decrease the memory footprint by 3x on longer sequences ( > 160k). We then scale \(\) along dimensions of model depth (2 to 8 layers), width (128 to 256 dimensions), and sequence length (1024 to 1M). At sequence length 1M, \(\) is 160x faster than its Transformer counterpart as shown in Fig. 4.1.

As shown in Fig. 1.2, we observe that as context length increases, perplexity improves during pretraining. However, this improvement comes at the expense of more training time and tokens. For models too shallow to effectively process longer context, perplexity can begin to degrade (increase), observing inflection points with longer sequences. In this way, increasing context can serve as a

Figure 3.2: Sequence length warm-up reduces the training time of \(\) at sequence length 450k by 40% and boosts accuracy by 7.5 points on species classification.

[MISSING_PAGE_FAIL:7]

ProcedureIn both variants, we use the GenomicBenchmarks in 4.2, and a HvenaDNA model pretrained on sequence length 160k from 4.1.

In the first experiment, we evaluate a soft prompting approach by prepending a sequence of soft tuneable tokens (2 to 32k) directly in the input sequences. We include a brief tuning phase (\(<20\) epochs), updating the soft tokens only, to provide HvenaDNA with the ability to indicate the target classes. To denote classes, we repurpose HvenaDNA's fixed vocabulary: for binary classification, for example, we indicate the two classes with the letters "A" and "N".

In the second experiment, we evaluate a few-shot learning approach to in-context learning  by prepending, consecutively, \(k\) (2 to 32) demonstrations of each class and its sequence into the prompt. As before, we encode class labels by the use of individual letters of HvenaDNA's existing vocabulary. We additionally perform a brief instruction-tuning period  for each dataset to familiarize HvenaDNA with this task structure by tuning the pretrained model on a small subset of the dataset.

ResultsIn Fig. 4.2, HvenaDNA's performance on novel tasks improves as more tuneable tokens are added into the input sequences, and saturates close to baseline performance (Tab. 4.1; with the exception of the Human Regulatory dataset). By contrast, we find that increasing \(k\)-shot demonstrations to the input does not necessarily improve performance. A higher number of tuning samples is needed before \(k\)-shot demonstrations start to boost accuracy as shown in Tab. A.1. See A.3 for experiment details.

### Ultralong-Range Genomics

In our final experimental section, we focus on pushing the limits of using long context effectively in genomics. In 4.4.1, we tackle a challenging 919 binary multi-task against a sparse-attention baseline. In 4.4.2 we analyze the learned embeddings HvenaDNA and its use in clustering long sequences by functional annotation, and in 4.4.3 we showcase a novel ultralong-range species classification task.

#### 4.4.1 Chromatin Profile Prediction

   Model & NT & NT & NT & HvenaDNA \\ Params & 500M & 2.5B & 2.5B & 1.6M \\ \# of Genomes & 1 & 3,202 & 850 & 1 \\  Enhancer & 53.5 & 59.3 & 58.0 & **62.6** \\ Enhancer types & 48.5 & 50.0 & 47.4 & **55.7** \\ H3 & 73.7 & 77.6 & 81.4 & **81.7** \\ H3K4me1 & 35.8 & 44.5 & 55.9 & **57.1** \\ H3K4me2 & 28.1 & 30.0 & 32.6 & **53.9** \\ H3K4me3 & 26.3 & 28.1 & 42.1 & **61.2** \\ H3K9ac & 46.2 & 50.8 & 57.5 & **65.1** \\ H3K14ac & 37.7 & 47.1 & 55.0 & **66.3** \\ H3K36me3 & 46.7 & 53.3 & 63.2 & **65.3** \\ H3K79me3 & 57.7 & 59.2 & 64.2 & **71.6** \\ H4 & 76.2 & 78.9 & **82.2** & 79.6 \\ H4ac & 34.4 & 42.3 & 50.1 & **63.7** \\ Promoter all & 95.4 & 96.6 & **97.4** & 96.5 \\ Promoter non-TATA & 95.6 & 96.9 & **97.7** & 96.6 \\ Promoter TATA & 94.8 & 95.8 & 96.4 & **96.7** \\ Splice acceptor & 96.5 & 98.5 & **99.0** & 96.6 \\ Splice donor & 97.2 & 98.2 & **98.4** & 97.3 \\ Splice all & 97.2 & 97.8 & **98.3** & 97.9 \\   

Table 4.2: **Nucleotide Transformer (NT) Benchmarks** The Matthews correlation coefficient (MCC) is used as the performance metric for the enhancer and epigenetic marks dataset, and the F1-score is used for the promoter and splice site dataset.

Figure 4.2: **Filling long-context with soft tuneable tokens.** HvenaDNA is able to learn new tasks in-context when adding a sequence of tuneable tokens to the input sequences. Longer sequences of tuneable tokens lead to better performance.

The prediction of chromatin profiles and epigenetic markers from DNA sequences is an important and challenging task to quantify the functional effects of non-coding variants. These variants include single nucleotide changes in DNA that can affect the downstream expression of genes . The DeepSEA dataset  is compiled from 919 chromatin features including transcription factor (TF) binding profiles, DNase I-hypersensitive sites (DHS) and histone mark (HM) profiles. For a given sequence, the task is to jointly predict 919 labels corresponding to the chromatin profile (similar to peak detection) of a central region of the sequence, indicating the presence of such functional effects. The input also includes flanking regions that provide broader contextual information needed to incorporate long-range interactions. We fine-tune our pretrained HryenaDNA models from 4.1 and perform competitively against a DeepSea CNN and the SotA sparse attention BigBird  baselines using 5-30\(\) fewer parameters. See A.4 for experiment details.

#### 4.4.2 Biotype Embeddings

Next, we analyze the pretrained embeddings from HryenaDNA and compare them with DNABERT  and the Nucleotide Transformer . We encode sequences of human genes corresponding to different biological function annotations obtained from the Ensembl dataset known as biotypes . In cases where the length of the input exceeds the context window of the encoder, the sequence is chunked (by the max length of the encoder) and averaged.

We fit the embeddings using an XGBoost  classifier on the 10 most frequent biotypes, and apply t-SNE  for visualization. As shown in 4.3, distinct clusterings emerge visually, while quantitatively, HryenaDNA produces the highest F1 score in biotype classification (with a much smaller model), indicating that during pretraining, HryenaDNA learns informative features related to biological function.

   Model & Params & Len & F1 \\  DNABERT & \(110\) M & 512 & 64.6 \\ NT & \(500\) M & 6k & 66.5 \\  HryenaDNA & \(7\) M & 160k & \(\) \\   

Table 4.4: **Embedding quality** Weighted F1 classification score on \(10\) biotypes.

Figure 4.3: **Embedding visualisation.** t-SNE of the embeddings generated by DNABERT, Nucleotide Transformer and HryenaDNA coloured by Ensembl biotype annotations.

   Model & Params & Len &  \\  & & & TF & DHS & HM \\  DeepSEA & \(40\) M & \(1\)k & \(95.8\) & \(92.3\) & \(85.6\) \\ BigBird & \(110\) M & \(8\)k & \(96.1\) & \(92.1\) & \(88.7\) \\  HryenaDNA & \(7\) M & \(1\)k & \(\) & \(\) & \(86.3\) \\  & \(3.5\) M & \(8\)k & \(95.5\) & \(91.7\) & \(\) \\   

Table 4.3: **Chromatin profile prediction** Median AUROC computed over three categories: Transcription factor binding profiles (TF), DNase I-hypersensitive sites (DHS) and histone marks (HM).

#### 4.4.3 Species Classification

The majority of the genome is conserved across species - humans and non-human primates, for example, have <10% sequence divergence , making them difficult to discriminate. This allows us to to design an ultralong-range sequence modeling task to test whether a model can determine the source species of a random genetic sequence. To train, we randomly sample DNA sequences from 5 different species, and fine-tune pretrained \(\) and Transformer models from 4.1 to predict the species label. We observe in Tab. 4.5 that both models struggle on shorter sequences of length \(1024\), but performance improves with longer contexts as the distinct mutational profile of each species becomes more evident. \(\) effectively solves the task by using a context length of \(450\)k to \(1\) million, where Transformer cannot due to infeasible training time limitations. See A.6 for experiment details.

## 5 Conclusion

SummaryWe presented \(\), a genomic foundation model pretrained on the human reference genome with context lengths up to 1 million tokens at single nucleotide resolution - an up to 500x increase over previous genomic FMs using dense-attention. \(\) is able to learn generalizable features that can then be fine-tuned for tasks including identifying regulatory elements and on a 919-way chromatin profile prediction task. We also explored the first use of in-context learning in genomics to enable simpler adaptation to downstream tasks without any updates to pretrained weights.

Limitations and Future WorkWhile demonstrating competitive results and introducing novel capabilities, it is worth noting that \(\) was pretrained on only one human reference genome. Incorporating genomes of multiple humans and species could increase generalizability in learned features and reduce bias. Furthermore, our current focus in this study was exclusively on DNA sequences. Extending our framework to incorporate other biological or chemical sequences, such as proteins and drug molecules, has the potential to unlock multi-modal capabilities similar to those observed in natural language and vision FMs [39; 40; 54].

With respect to model size, \(\) is significantly smaller than previous genomic FMs and was pretrained using up to 8 Nvidia A100 (80GB) GPUs. We expect increasing model size, and compute, may lead to additional long-range capabilities. Notably, with model parallelism, it becomes feasible to extend the context length by orders of magnitude beyond this current work, and leave that open to future research.

Furthermore, beyond discriminative applications, the use of long context models in generative tasks unlocks exciting prospects for the design of synthetic regulatory elements, genes and protein complexes. In conclusion, the continued advancements of long-range sequence models with single nucleotide resolution hold great promise in driving innovation in genomic research and unraveling the complexities of biological systems.