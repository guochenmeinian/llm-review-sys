# Neural Network Reparametrization for Accelerated Optimization in Molecular Simulations

Nima Dehmamy

IBM Research

Nima.Dehmamy@ibm.com

Csaba Both

Northeastern University

both.cfmorthestern.edu

Jeet Mohapatra

MIT CSAIL

jeetmo@mit.edu

Subhro Das

IBM Research

subhro.das@ibm.com

Tommi Jaakkola

MIT CSAIL

tommi@csail.mit.edu

###### Abstract

We propose a novel approach to molecular simulations using neural network reparametrization, which offers a flexible alternative to traditional coarse-graining methods. Unlike conventional techniques that strictly reduce degrees of freedom, the complexity of the system can be adjusted in our model, sometimes increasing it to simplify the optimization process. Our approach also maintains continuous access to fine-grained modes and eliminates the need for force-matching, enhancing both the efficiency and accuracy of energy minimization. Importantly, our framework allows for the use of potentially arbitrary neural networks (e.g., Graph Neural Networks (GNN)) to perform the reparametrization, incorporating CG modes as needed. In fact, our experiments using very weak molecular forces (Lennard-Jones potential) the GNN-based model is the sole model to find the correct configuration. Similarly, in protein-folding scenarios, our GNN-based CG method consistently outperforms traditional optimization methods. It not only recovers the target structures more accurately but also achieves faster convergence to the deepest energy states. This work demonstrates significant advancements in molecular simulations by optimizing energy minimization and convergence speeds, offering a new, efficient framework for simulating complex molecular systems. 1

Scientific simulations, particularly in molecular dynamics (MD), face fundamental challenges in finding optimal configurations. The energy landscapes of these systems are characterized by numerous saddle points and local minima, making it difficult for traditional optimization methods to discover the most stable states. This complexity stems from the interplay between different scales of interactions, from strong covalent bonds to weak van der Waals forces, leading to slow convergence in gradient-based methods and often suboptimal results. For instance, in protein folding, the strong peptide bonds create steep energy barriers while weak hydrophobic interactions guide the overall folding process, creating a hierarchy of energy scales that is challenging to optimize simultaneously.

To address these challenges, coarse-graining (CG) methods have emerged as a popular approach, reducing computational complexity by decreasing the number of degrees of freedom (DOF) and clustering them into collective modes. While these methods have shown success (Pak & Voth, 2018; Hollingsworth & Dror, 2018), they face significant limitations. Traditional CG approaches require cumbersome procedures such as back-mapping (returning to the original DOF) and force-matching (finding the forces experienced by CG modes) (Jin et al., 2022), which can limit their efficiency and scalability. Moreover, the strict reduction of DOF in conventional CG can sometimes oversimplify the system, losing important fine-grained details necessary for accurate energy minimization.

In this paper, we introduce an innovative alternative that overcomes these limitations through neural reparametrization. Instead of strictly reducing DOF as in conventional CG, our approach leverages an overparametrized neural ansatz to represent fine-grained (FG) modes as functions of CG modes. This reparametrization concept, similar to techniques such as Deep Image Priors (Ulyanov et al., 2018), enables the neural network to dynamically represent the FG system while maintaining continuous access to FG modes and eliminating the need for force-matching. The overparametrization provides additional flexibility in navigating the energy landscape - while the physical system has \(n d\) degrees of freedom (\(n\) particles in \(d\) dimensions), our neural representation can use a higher-dimensional latent space to find paths around energy barriers that might be difficult to traverse in the original space.

A key innovation in our approach is the incorporation of Graph Neural Networks (GNN) with a structure informed by'slow modes'-inherently stable collective modes identified through spectral analysis of the system's dynamics. We show that these modes arise naturally from the structure of physical Hessians, which are Laplacian matrices over particle indices for a broad class of potential energies. By focusing on these slow modes, which typically cause convergence bottlenecks in traditional optimization, we can significantly accelerate the learning process. The GNN architecture allows us to safely increase learning rates without stability issues, resulting in both faster dynamics progression and the discovery of lower energy states compared to direct optimization methods.

The effectiveness of our approach is demonstrated through experiments on both synthetic systems and real molecular structures. In particular, for protein folding with weak Lennard-Jones interactions, where traditional methods often struggle with the shallow energy landscape, our GNN-based model consistently finds deeper energy minima. This success can be attributed to two key factors: the ability of the overparametrized representation to explore the energy landscape more effectively, and the incorporation of physically meaningful slow modes into the neural architecture, which helps guide the optimization toward stable configurations.

The main contributions of this work are:

1. **CG via reparametrization:** A new paradigm that circumvents traditional challenges like force-matching and back-mapping.
2. **Robust slow modes:** Effective identification and utilization of stable modes across various systems.
3. **MD simulations:** Demonstrated improvements in efficiency and depth of energy exploration in protein dynamics.
4. **Overparametrization benefits:** Evidence that an overparametrized framework can outperform traditional DOF reduction in terms of convergence speed and energy minimization.
5. **Data-free optimization:** Our method modifies the optimization landscape without the need for training data, enhancing its applicability and efficiency.

## 1 Background

Traditional optimization in physics-based models, like (MD), faces unique challenges due to the shallow nature of these models, where physical DOF are the trainable weights. Additionally, the interactions occur at multiple scales, from strong covalent bonds to weak van der Waals forces, leading to slow convergence in gradient-based methods.

To address these challenges, conventional strategies include preconditioning with methods like adaptive gradient (Duchi et al., 2011; Kingma and Ba, 2014) or quasi-Newton (Fletcher, 2013), and CG, which simplifies the system by truncating DOF to focus on collective modes. However, both approaches have limitations: preconditioning methods struggle with cost and inefficacy due to non-diagonal Hessians in physics problems, and CG can be restrictive and require intensive back-mapping and force-matching steps (Jin et al., 2022).

In contrast, our approach utilizes neural network reparametrization to dynamically adjust system complexity, which may involve overparametrization. This method allows for flexible system representation, which can simplify the optimization process. It can help avoid local minima and accelerates convergence by exploring the configuration space more efficiently.

Neural Reparametrization in PracticeOur neural reparametrization approach is not limited to reducing DOF but can also increase them when beneficial, offering an adaptive solution to the specific needs of a simulation. This flexibility is crucial for addressing the hierarchy of interactions in molecular systems, where different forces operate at vastly different scales.

Implementation and Comparison to CGWhile CG methods focus on predefined collective modes and often involve laborious optimization steps like force-matching and back-mapping, our neural reparametrization approach defines modes based on the spectrum of a canonical Hessian, directly incorporating these into the neural network's architecture. This not only bypasses the need for traditional CG steps but also enhances the adaptability and speed of the optimization process.

Advantages Over Traditional MethodsOur method diverges from traditional data-driven ML approaches that require extensive datasets, which are often unavailable or costly to produce in molecular and material design. By not relying on training data, our approach provides a robust framework for tackling complex optimization problems, from molecular dynamics to protein folding, with improved efficiency and without the constraints of data availability.

### Traditional Coarse-graining

Let \(X^{n d}\) represent the degrees of freedom (DOF), such as particle positions or bond angles, and let \(:\) denote the energy or potential function. The objective is to simulate the dynamics of the system or to find high-likelihood configurations \(X^{*}\) that represent deep local minima of \(\). Given that \(n\) is typically large and \(\) is a steep non-convex function, computations can be slow. Traditional coarse-graining (CG) maps \(X\) to a reduced space of CG variables, \(^{k d}\), where \(k n\). Implementing dynamics using CG modes requires determining the inter-mode forces ("force-matching") and how to revert to \(\) ("back-mapping").

Force-matching.The fine-grained (FG) energy function, \(_{FG}:\), needs an approximate potential \(_{CG}:\) such that for \(X\),

\[:,_{CG}((X)) _{FG}(X). \]

The process of finding \(_{CG}\) is called force-matching, traditionally solved analytically but increasingly with machine learning for enhanced accuracy Jin et al. (2022); Majewski et al. (2023).

Back-mapping.The map \(^{k d}\) is not unique, often resulting in multiple possible \(X\) for a given \(Z\). Back-mapping typically involves sampling or optimization to find physically plausible \(X\) configurations, avoiding scenarios like overlapping atoms or high energies. This can be complex when many \(X\) map to the same \(Z\), with current methods ranging from geometric reconstruction Lombardi et al. (2016) to refinement with molecular dynamics Badaczewska-Dawid et al. (2020); Roel-Touris and Bonvin (2020) and data-driven approaches Yang and Gomez-Bombarelli (2023); Wang et al. (2022).

### Neural Reparametrization as an Alternative to Coarse-graining

Instead of traditional CG, which reduces DOF through a mapping to a reduced space, our approach reparametrizes the DOF \(X\) as a function of CG-like modes. This reparametrization, given by \(X=(Z)\), where \(:\), offers a flexible, reversible mapping that inherently includes benefits such as direct access to fine-grained modes and elimination of force-matching and back-mapping needs:

\[ X=(Z),: \]

1. **Flexible parametrization:** Leveraging neural overparametrization and architecture design.
2. **Direct access to fine-grained modes:**\(X=(Z)\) avoids the need for back-mapping.
3. **Simplified energy computation:** The energy for CG-like modes is \(_{CG}(Z)=((Z))\).

While this method can be computationally intensive as \(_{CG}(Z)\) is computed using \(X\), the efficiency gains in optimization speed and depth of energy minimization can offset the costs.

Neural Architectures for ReparametrizationThe reparametrization function \(\) can range from simple linear projections to complex neural networks. Initially, we employ a linear projection onto identified slow modes:

\[X=(Z)=Z^{T}_{}_{i}Z_{i}^{T} _{i} \]

More generally, \(\) may be a deep neural network (DNN), similar to the approach taken in prior work suggesting neural priors (e.g. Deep Image Priors Ulyanov et al. (2018)).

Graph Neural Networks (GNN) for Dynamic Reparametrization:Extending beyond linear models, we explore the use of GNNs, inspired by recent advancements in graph-based optimizations Both et al. (2023). Here, the GNN reparametrizes node states and was shown to find both lower energy states and exhibit faster convergence. Our idea is to use a "Hessian backbone" as a graph, which acts as a weighted graph adjacency matrix for a GNN. In our experiments, we observe this GNN to have significant advantages over the direct as well as linear reparametrization equation 3. The details of our GNN architecture are discussed in Section 3. Next, we derive the properties of the slow modes for a large class of energy functions important in molecular systems.

### The role of the Hessian

The success of optimization in molecular systems is fundamentally limited by the disparity in evolution rates along different modes of the system. Near any configuration \(X\), the dynamics of

Figure 1: Overview of the neural reparametrization method. **Top:** Architectures used for reparametrization. In linear reparametrization, \(X=Z^{T}_{}\). In the GNN case, we use the slow modes to construct a graph with adjacency \(A=_{}_{}^{T}\) and use it in GCN layers to obtain \(X=(Z)\). **Left:** Flowchart showing the key steps of the method. **Right:** Detailed algorithm for implementation.

gradient-based optimization can be understood through the eigendecomposition of the Hessian \(=\). The eigenvectors of \(\) define the natural modes of the system, with their eigenvalues determining how quickly these modes evolve under gradient descent. Modes with large eigenvalues (fast modes) evolve rapidly but constrain the learning rate to ensure stability, while modes with eigenvalues close to zero (slow modes) evolve orders of magnitude more slowly, leading to extremely slow convergence, particularly near saddle points.

This disparity presents a fundamental challenge: To maintain numerical stability, the learning rate must be small enough to handle the fastest modes, but this makes the slow modes evolve at a glacial pace. Traditional approaches like adaptive gradient methods attempt to address this by approximating a diagonal preconditioner, but they struggle with the strongly coupled nature of physical systems where the Hessian is far from diagonal. While conventional coarse-graining partially addresses this by eliminating fast modes, it introduces other challenges such as force-matching and back-mapping.

Our approach takes a different perspective: instead of eliminating modes, we seek to identify and directly incorporate slow modes into our optimization process. However, this raises two key challenges. First, as the system evolves, the Hessian changes, potentially altering which modes are slow. Second, even if we can identify slow modes, we need a way to modify the optimization to preferentially explore these directions. The next section addresses the first challenge by proving that slow modes of physical Hessians are remarkably robust, arising from fundamental symmetries of the underlying interactions. We then show how these robust slow modes can be effectively utilized through neural reparametrization.

## 2 Properties of Physical Hessians

We will now show that the Hessian of potential energies important in physics and molecular systems enjoy certain properties that lead to the robustness of slow modes. In short, if we find a stable backbone for Hessians of different configurations \(X\), then the slow modes of the Hessian at \(X\) are close to the slow modes derived from the backbone.

Invariant potentials.In systems of interacting particles in physics, leading interactions are often pairwise and involve relative features, \(_{ij} X_{i}-X_{j}\) (distance vector, relative angle, etc). These interactions are invariant under global symmetries, such as Euclidean symmetries (translation and rotation) or Lorentz symmetry (relativistic particles). These symmetries maintain the invariance of certain norms, \(v^{2}=\|\|_{}^{T}\), where \(\) may be the Euclidean metric \(=(1,1,1)\) or the Minkowski metric \(=(-1,1,1,1)\). For example, the Euclidean norm \(^{T}\) in \(d\) dimensions is invariant under rotations \( g\), where \(g SO(d)\).

Energy function structure.Let \(r\) denote the matrix of distances with \(r_{ij}=\|_{ij}\|_{}\). Any function of \(r_{ij}\) is invariant under symmetries that keep \(\|\|_{}\) invariant. Assuming additivity, the energy function can be written as:

\[(X)=_{ij}f_{ij}(r_{ij}) \]

where \(f_{ij}(z)=f_{ji}(z)\). For example, the Coulomb potential between particles \(i\) and \(j\) with charges \(q_{i}\) and \(q_{j}\) respectively, is given by \(f_{ij}(z)=kq_{i}q_{j}/z\). The Lennard-Jones potential \(f_{ij}(z)=A_{ij}/z^{1}2-B_{ij}/z^{6}\) in molecular systems is also of this form.

### Hessian of invariant potentials

The Hessian of potentials of the form equation 4 has the special property that it is the graph Laplacian of a weighted graph which depends on \(X\), as we show now (see appendix E for details). This will play a crucial role in our argument about the robustness of the slow modes.

Hessian as a graph Laplacian.Recall the Laplacian of an undirected graph with adjacency matrix \(A\) is defined as \(L=(A)=D-A\), where \(D\) is the degree matrix with elements \(D_{ij}=_{ij}_{k}A_{ik}\). The components of Laplacian can also be written as \(L_{ij}=_{k}A_{ik}(_{ij}-_{jk})\). We show that the Hessian of \(\) in equation 4 is a Laplacian. Let \(_{i}/ X_{i}\) and let \(=/r\) be the dual unit vector of \(\). First, observe that \(_{i}r_{jk}=_{jk}(_{ij}-_{ik})\) where \(_{jk}\) is the unit vector of \(_{jk}\) and \(_{ij}\) is the Kronecker delta (1 if \(i=j\), 0 otherwise). Let \([g]\) denote the Hessian of a function \(g\). We find that (app. E)

\[[](X)_{ij}=_{i}_{j}(X)=_{ k}(_{ij}-_{jk})_{ik}(X)=()_{ij} \]

where \(_{ik}(X)=[f_{ik}](r_{ik})\). Note that \(\) has four indices, with components \(_{ij}^{}\), having two particle indices \(i,j\) and two spatial indices \(,\). Thus, for every pair of spatial indices \(,\), the Hessian \(^{}\) is a Laplacian over particle indices. The Hessian being Laplacian has an important effect on its null eigenvectors. To show this we make use of the incidence matrix.

We are interested in the eigenvalues and eigenvectors of \(\), as these characterize the slow and fast modes of the system. First, given a weighted adjacency matrix \(A\) of a graph, let \(\) and \(\) be the "unweighted" adjacency and Laplacian matrices, where \(_{ij}=1\) if \(A_{ij} 0\) and zero otherwise. It follows that the null spaces of \(L\) and \(\) are shared:

**Theorem 2.1** (Null Space of the Laplacian).: _Let \([M]\) denote the null space of a symmetric real matrix \(M\). The null space of the unweighted Laplacian \(\) is contained within the null space of the weighted Laplacian \(L\), i.e., \([][L]\)._

Sketch of proof.: For any vector, \(^{n}\), \(^{T}(A)=_{ij}A_{ij}(v_{i}-v_{j})^{2}\). Since \(_{ij}=0\) yields \(A_{ij}=0\), but not necessarily vice versa, null vectors of \([][L]\). See appendix for full proof. 

**Definition 2.1** (Slow manifold).: _Let \(L\) be a graph Laplacian (undirected, weighted or unweighted), with spectral expansion \(L=_{i=1}^{n}_{i}_{i}_{i}^{}}\). Let \( 1\) and \(_{}=\{_{i}\}\) be the largest eigenvalue of \(L\). We define the slow manifold as_

\[_{}[L]=_{i}||_{i}|< ^{2}_{}} \]

**Theorem 2.2** (Slow modes of weighted Laplacians).: _Let \(A\) be the adjacency matrix of a weighted graph and \(\) be its unweighted counterpart. Let \(L=(A)\) and \(=()\). Then \(_{}[L]\) overlaps with \(_{}[]\) up to \(O(^{2})\) corrections from the rest of the modes._

The sketch of the proof relies on relating the spectra of the weighted and unweighted Laplacians using the incidence matrix \(C\), as \(L=CWC^{T}\) and \(=CC^{T}\). For a random configuration \(X\) the edge weights \(W\) will be random, as they arising from derivatives of \(f_{ij}(r_{ij})\) in equation 20 (unless \(f_{ij}\) is quadratic which makes \(W\) constant). Then, using the assumption of randomness on the weights \(W\), we can show the slow modes of \(L\) are perturbations of order \(^{2}\) on slow modes of \(}\). See Appendix D for proof.

Implications for Coarse-Graining.The identification of slow modes in the Hessian is crucial for coarse-graining, as these modes capture the essential dynamics of the system at larger scales. By focusing on these slow modes, we can develop reduced models that retain the key physical properties while being computationally more efficient.

Coarse-Graining via Slow Modes.The identification of slow modes in the Hessian enables an effective coarse-graining approach, where fast dynamics are averaged out, retaining only the slow, relevant dynamics. This method is particularly advantageous in reducing computational complexity while preserving critical structural information.

### Hessian Backbone and Robust Slow Modes

The slow modes of the Hessian \([](X)=((X))\) can dynamically change during optimization. To ensure the robustness of these modes, we need a proxy for the unweighted adjacency matrix \(\). To this end, we aggregate Hessians from perturbed configurations \((X)=\{X^{}=X+ X\}\):

\[_{ij}=_{X^{}(X)} \|H_{ij}(X^{})\|^{2} \]This aggregation helps identify consistently significant components across configurations, aiding in the extraction of reliable slow modes that remain effective over extended periods of optimization. In equation 7, \(i,j_{n}\) are the particle indices and the Frobenius norm \(\|H_{ij}\|^{2}=_{,}(H_{ij}^{})^{2}\) sums over the feature indices (note that \(X_{i}^{}\) has a particle index \(i\) and a feature index \(\{1, d\}\)). Then, we extract the slow modes of the backbone, by doing a spectral expansion \(=_{i}_{i}_{i}_{i}^{T}\) and picking \(_{i}\) with \(|_{i}|<^{2}_{j}[_{j}]\), for some small \(<1\). The intuition behind equation 7 is to identify the components in the sampled Hessians which have consistently high magnitudes. If we had taken a simple mean we could get very small values, because the components can fluctuate randomly. Also, if we had taken the variance instead of the norm, we would get zero for quadratic \(\), where \(H\) is constant and has no variance. As we discussed above, the slow modes of the backbone \(\) approximate the slow modes of sampled \(H(X^{})\) up to \(O(^{2})\) errors.

## 3 Experiments

We apply our method to protein folding using classical MD forces.

**Settings:** We use gradient descent to minimize \((X)\). All experiments (both CG and baseline) use the Adam optimizer with a learning rate \(10^{-2}\) and early stopping with \(||=10^{-6}\) tolerance and \(5\) steps patience. We ran each experiment four times.

**Baseline:** we use gradient descent (GD) with Adam optimizer on the MD energy as baseline.

Figure 3: **Synthetic loop folding (\(n=1000\)). Lower means better for both energy and time. In Bond+LJ (left), a quadratic potential \(_{i}(r_{ii+1}-1)^{2}\) attracts nodes \(i\) and \(i+1\). A weak LJ potential attracts nodes \(i\) and \(i+10\) to form loops. In LJ loop (right) both the backbone \(i,i+1\) and the 10x weaker loop are LJ. Orange crosses denote the baseline GD, green is GNN and blue is CG. The dots are different hyperparameter settings (LR, Nr. CG modes, stopping criteria, etc.) with error bars over 5 runs. In Bond+LJ, CG yields slightly better energies but takes longer, while GNN can converge faster to GD energies. In pure LJ, using CG and GNN can yield significantly better energies.**

Figure 2: **Synthetic loop experiments. Example runs of the synthetic loop experiments with \(n=400\) nodes. On the left (Bond+LJ), the potential is the sum of a quadratic bond potential \(E_{bond}\) and a weak LJ (12,6) \(E_{LJ}\). The bonds form a line graph \(A_{bond}\) connecting node \(i\) to \(i+1\), and a 10 weaker \(A_{loop}\) connecting node \(i\) to \(i+10\) via the LJ potential. To the right (Pure LJ) where the interactions are all LJ, but with a coupling matrix \(A=A_{bond}+0.1A_{loop}\). In Bond+LJ, GD already finds good energies and the configuration is reasonably close to a loop, though flattened. Both linear CG reparametrization (CG Rep) and GNN also find a good layout. The pure LJ case is much more tricky. But in most runs, GD almost gets the layout, but some nodes remain far away. The CG Rep fails to bring all the pieces together. Only GNN succeeds in finding the correct layout.**

CG model:We use four different choices for the fraction of the eigenvectors to use in CG equation 3: \(3(\#)\), \(30\%\), \(50\%\), and \(70\%\). We use a two stage process. First, we use CG as in equation 3\(X=(Z)=Z^{T}_{}\) and minimize \(_{CG}(Z)=((Z))\) over \(Z\). After convergence to \(X_{0}=(Z_{0})\), we add \( X\) to \(X_{0}\) and optimize the fine-grained \( X\), starting with \( X=0\).

GNN model:We use a GNN consisting of a graph convolution (GCN) layer with self-loops and one node-wise MLP layer, projecting the GNN output to 3D to get particle positions. The GCN takes \(Z_{h_{0}}^{n h_{0}}\) as input, with \(h_{0}>3\) and has weights \(W_{G}^{h_{0} h_{1}}\). Then, GCN output gets a Tanh activation and is passed to the MLP layer to yield \(X\). The CG parameters in this case are \(Z_{h},W_{G}\) and the weights and biases of the MLP.

Synthetic coil:We use quadratic and LJ potentials to make synthetic systems whose minimum energy state should be a coil (looping every 10 nodes), inspired by MD potentials. Figure 3 shows many experiments using GD, CG, and GNN. In the quadratic Bond+LJ case, GNN yields a good

Figure 4: **Protein folding simulations** Figure (a) shows the energy improvement factor (FG energy / GNN energy) in the function of the speedup factor (FG time / GNN time) for the six selected proteins marked with different colors (c). In all cases, the GNN parameterization leads to speed improvement while it converges higher energy. (b) However, the higher energy in some cases, 2JOF and 1UNC proteins, results in a slightly lower RMSD value, which measures how close the final layout is to the PDB layout. The data points are averaged over ten simulations per protein.

Figure 5: **2JOF (Trp-Cage) protein folding.** Figure (a) shows the RMSD value evolution of the 2JOF protein as it goes from an unfolded to a folded stage. At every step, we calculated the RMSD of the current layout compared to the PDB layout. We ran the OpenMM simulations at \(298K\) temperature with 2fs timestep for 800000 steps, while the GNN and GD simulations were performed for 400000 steps with various hidden dimensions (10, 100, 300, 500). The black curves show the stochastic nature of protein folding using OpenMM. (b) The first figure shows the PDB (red) and unfolded (blue) layout; the second one is the GNN 500 final layout (blue), while the third is one of the OpenMM layouts, corresponding to the black curve.

speedup, while CG yields better energies. The benefit of CG and GNN become more apparent in the harder pure LJ problem, where GD fails to find good energies, while CG finds much deeper energies, followed by GNN (Fig. 2).

Protein folding with classical MD:We implement a simplified force-field with implicit solvent (i.e. water molecules are not modeled and appear as hydrogen-bonding and hydrophobicity terms; app. A). In protein folding our energy function consists of five potential energies: bond length \(E_{bond}\), bond angles \(E_{angle}\), van der Waals \(E_{vdW}\), hydrophobic \(E_{hp}\) and hydrogen bonding \(E_{H}\) Ceci et al. (2007). Figure 7 shows an example of these coupling matrices for the Enkephalin (1PLW) protein. To evaluate the effect of our CG model, we run experiments on four small proteins: Chignolin (5AWL), Trp-Cage (2JOF), Cyclotide (2MGO) and Enkephalin (1PLW).

Protein Folding with Classical MD Using AMBER Force FieldIn the updated simulation approach, we incorporate the AMBER force field, known for its accurate representation of molecular interactions, particularly in proteins. This force field is implemented using the parameters from OpenMM Eastman et al. (2017), and it comprehensively models the following interactions:

* Bond lengths \(E_{bond}\) and bond angles \(E_{angle}\)
* Torsional angles \(E_{torsion}\)
* Non-bonded interactions including van der Waals \(E_{vdW}\) and electrostatic \(E_{elec}\) forces

We utilize the functional forms and parameters specified in the AMBER force field:

\[E_{bond} =_{bonds}k_{bond}(r-r_{0})^{2} E_{angle} =_{angles}k_{angle}(-_{0})^{2} \] \[E_{torsion} =_{torsions}V_{n}[1+(n-)] E_{vdW} =_{i<j}}{r_{ij}^{12}}-}{r_{ij}^{6}}\] (9) \[E_{elec} =_{i<j}q_{j}}{4_{0}_{r}r_{ij}} \]

Here, \(r\) and \(\) represent the bond lengths and angles, respectively, with \(r_{0}\) and \(_{0}\) as their equilibrium values. The torsional term \(E_{torsion}\) includes a sum over all torsion angles \(\), with periodicity \(n\)

Figure 6: **Learning rate and initialization in protein folding for pdb 2JOF:** We conducted a sweep of the learning rate to see how robust the advantage of GNN over direct GD is. In **a** and **b** we show the energy achieved by GD and GNN vs the number of iterations and wallclock time. GNN1 and GNN2 use one and two GCN layers, respectively. We used early stopping which generally stopped the runs after 3-5k steps. The grey star shows the OpenMM results after 5k steps, which has a worse (higher) energy than our GD and GNN runs, but it takes a fraction of the time (it has many efficiency tricks that our code doesnâ€™t have). The dashed line shows the energy achieved by OpenMM after 10k steps. As we see, some of our GNN models reach energies close to the 10k steps of openMM in a fraction of the steps. All experiments show the best energy among three runs. **c** shows the effect of initialization on the GD runs. We do find the protein converges to significantly different conformations based on the init.

amplitude \(V_{n}\), and phase \(\). The Lennard-Jones potential in \(E_{vdW}\) is characterized by parameters \(A_{ij}\) and \(B_{ij}\), and \(E_{elec}\) is calculated using the Coulombic potential with partial charges \(q_{i}\), \(q_{j}\) and the relative permittivity \(_{r}\).

In this simulation, we exclude the modeling of solvent effects entirely, focusing solely on the protein in vacuum. This approach simplifies the computational model while emphasizing the direct interactions within the protein.

The overall energy of the system is then given by:

\[(X)=E_{bond}+E_{angle}+E_{torsion}+E_{vdW}+E_{elec} \]

Figure 7 shows the interaction matrices for the Enkephalin (1PLW) protein. Our framework has been extended to efficiently compute these energies and gradients, facilitating the simulation of protein folding dynamics in our coarse-grained model. We test our model on several small proteins including Chignolin (5AWL), Trp-Cage (2JOF), Cyclotide (2MGO), and Enkephalin (1PLW) to evaluate the effectiveness of our approach.

Protein results:Denoting the final energy and run time of the GNN model by \(E_{GNN}\) and \(t_{GNN}\), and baseline by \(E_{0}\) and \(t_{0}\), we compute the energy improvement factor \(=E_{0}/E_{GNN}\) and speedup factor \(=t_{0}/t_{GNN}\), to plot different proteins together. Figure 4a shows the mean of \(\) vs \(\) over the 10 runs for GNN the model with hidden dimensions 300 (error bars are 1 STD). Overall, we find that all GNN models outperform the baseline in terms of run time and, eventually, also with energy improvement. To measure the folding quality, we use RMSD, comparing the final layouts to the PDB structure.

Figure 5 shows the RMSD value evolution using different methods. While, in most cases, OpenMM reaches a deeper RMSD value, our models could serve as a good initializer for accelerating molecular dynamics. To evaluate the robustness of these results, we ran sweeps over the learning rate, varied the number of GNN layers (one or two layers), and varied the initialization.

Figure 6 shows the results of these tests for the protein 2JOF. We used early stopping for switching from CG to FG in our GNN models and GD, resulting in 3-5k iteration steps. Compared with 5k steps of OpenMM simulations, both our GNN models and GD with Adam reach significantly deeper energies with fewer steps (a), with the lowest energies being all GNN. However, OpenMM takes less wall-clock time (b). Nevertheless, the depth of the energies achieved by GNN at 3-5k steps is close to 10k steps with OpenMM. More efficient implementations of our GNN may further improve these results.

## 4 Discussion

We showed preliminary evidence that CG through reparametrization can yield some improvements over non-CG baseline in protein folding, both in terms of run time as well as energy. This method has the advantage that it does not require force-matching or back-mapping. However, more experiments are needed to compare it against traditional CG methods. In fact, using ML to learn force-matching might provide further advantage by removing the need to evaluate \(_{CG}(Z)=(X)\) via the fine-grained modes \(X\). Also, while our canonical slow modes are derived for physical Hessians, the reparametrization approach to CG is general and could be applied to other ML problems.