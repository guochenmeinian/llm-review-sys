# SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning

Benjamin Ellis\({}^{1}\) Jonathan Cook\({}^{1}\) Skander Moalla\({}^{1,5}\) Mikayel Samvelyan\({}^{2,3}\)

**Mingfei Sun\({}^{4}\) Anuj Mahajan\({}^{1}\) Jakob N. Foerster\({}^{1}\) Shimon Whiteson\({}^{1}\)**

\({}^{1}\)University of Oxford \({}^{2}\)University College London \({}^{3}\)Meta AI \({}^{4}\)University of Manchester

\({}^{5}\)EPFL

Correspondence to benellis@robots.ox.ac.uk.Code is available at https://github.com/oxwhirl/smacv2

###### Abstract

The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for the centralised training with decentralised execution paradigm. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex _closed-loop_ policies (i.e., those that condition on the observation). In particular, we show that an _open-loop_ policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings during evaluation.2 We show that these changes ensure the benchmark requires the use of _closed-loop_ policies. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our website.

## 1 Introduction

In many real-world cases, control policies for cooperative multi-agent reinforcement learning (MARL) can be learned in a setting where the algorithm has access to all agents' observations, e.g., in a simulator or laboratory, but must be deployed where such centralisation is not possible. Centralised training with decentralised execution (CTDE)  is a paradigm for tackling these settings and has been the focus of much recent research . In CTDE, the learning algorithm sees all observations during training, as well as possibly the Markov state, but must learn policies that can be executed without such privileged information.

As in other areas of machine learning, benchmarks have played an important role in driving progress in CTDE. Examples of such benchmarks include the Hanabi Learning Environment , Google football , PettingZoo , and Multi-Agent Mujoco . Perhaps the most popular one is the StarCraft Multi-Agent Challenge (SMAC) , which focuses on decentralised micromanagement challenges in the game of StarCraft II. Rather than tackling the full game with centralised control , SMAC tasks a group of learning agents, each controlling a single army unit, to defeat the units of theenemy army controlled by the built-in heuristic AI. SMAC requires learning several complex joint action sequences such as focus fire,3 and kitting enemy units,4. For these reasons, many prominent MARL papers  rely on SMAC to benchmark their performance.5

However, there is significant evidence that SMAC is outliving its usefulness . Recent work reports near-perfect win rates on most scenarios  and strong performance without using the centralised state via independent learning . This suggests that, due to ceiling effects, SMAC may no longer reward further algorithmic improvements and that new benchmarks are needed.

In this paper, we present new, more fundamental problems with SMAC and hence reason for new benchmarks. In particular, we show that an open-loop policy, which conditions only on the timestep and ignores all other observations, performs well on many SMAC scenarios. We also extend this analysis by examining the estimates of the joint \(Q\)-function learnt by QMIX. We demonstrate that, even with all features masked, regression to the joint \(Q\)-function is possible with less than 10% error in all but two scenarios from just timestep information.

Together these results suggest that SMAC is not stochastic enough to necessitate complex closed-loop (i.e. conditioned on the observation) control policies on many scenarios. Therefore, although SMAC scenarios may require difficult to discover action sequences such as focus fire, agents do not need to adapt to a diverse range of situations, but can largely repeat a fixed action sequence with little deviation. Additionally, _meaningful partial observability_ (see Section 4) is minimal in SMAC due to large fields-of-view. For partial observability to be meaningful, one agent must observe information that is relevant to the current or future action selection of another agent, unknown to that other agent, and uninferrable from the other agent's observation. _Meaningful partial observability_ is crucial to decentralisation and the NEXP-completeness of Dec-POMDPs .

To address these shortcomings, we propose _SMACv2_, a new benchmark that uses procedural content generation (PCG)  to address SMAC's lack of stochasticity. In SMACv2, for each episode we randomly generate team compositions and agent start positions. Consequently, it is no longer sufficient for agents to repeat a fixed action sequence, but they must learn to coordinate across a diverse range of scenarios. To remedy the lack of meaningful partial observability, we introduce the extended partial observability challenge (EPO), where we only allow the first agent to spot an enemy to see it for certain when it is in range. This requires agents to implicitly communicate enemy information to prioritise targets. Additionally, we update the sight and attack ranges to increase the diversity of the agents and complexity of scenarios.

Our implementation is extensible, allowing researchers to implement new distributions easily and hence further expand on the available SMACv2 scenarios. We also analyse SMACv2 and demonstrate that the added stochasticity prevents an open-loop policy from learning successfully, but instead requires policies to condition on ally and enemy features.

We evaluate several state-of-the-art algorithms on SMACv2 and show that they struggle with many scenarios, confirming that SMACv2 poses substantial new challenges. Finally, we perform an ablation on the new observation features to demonstrate how each contributes to the difficulty, highlighting the specific areas on which future MARL research should focus.

Figure 1: Screenshots from SMACv2 showing agents battling the built-in AI.

Related Work

The MARL community has made significant use of games for benchmarking cooperative MARL algorithms. The Hanabi Learning Environment  tasks agents with learning to cooperate in the card-game Hanabi. Here, each agent observes the cards of its teammates but not its own, which must be implicitly communicated via gameplay. Hanabi is partially observable and stochastic, but only features teams of 2 to 5 players, which is fewer than all but the smallest SMACv2 scenarios. Kurach et al.  propose Google Football as a complex and stochastic benchmark, and also have a multi-agent setting. However, it assumes fully observable states, which simplifies coordination.

Peng et al.  propose a multi-agent adaptation of the MuJoCo environment featuring a complex continuous action space. Multi-Particle Environments (MPE)  feature simple communication-oriented challenges where particle agents can move and interact with each other using continuous actions. In contrast to both multi-agent MuJoCo and MPE, SMACv2 has a discrete action space, but challenges agents to handle a wide range of scenarios using procedural content generation. A plethora of work in MARL uses grid-worlds [25; 23; 54; 48; 35; 24] where agents move and perform a small number of discrete actions on a 2D grid, but these tasks have much lower dimensional observations than SMACv2. OpenSpiel  and PettingZoo  provide collections of cooperative, competitive, and mixed sum games, such as grid-worlds and board games. However, the cooperative testbeds in either of these suites feature only simple environments with deterministic dynamics or a small number of agents. Neural MMO  provides a massively multi-agent game environment with open-ended tasks. However, it focuses on emergent behaviour within a large population of agents, rather than the fine-grained coordination of fully cooperative agents. Furthermore, none of these environments combines meaningful partial observability, complex dynamics, and high-dimensional observation spaces, whilst also featuring more than a few agents that need to coordinate to solve a common goal.

StarCraft has been frequently used as a testbed for RL algorithms. Most work focuses on the full game whereby a centralised controller serves as a pupperer issuing commands for the two elements of the game: _macromanagement_, i.e., the high-level strategies for resource management and economy, and _micromanagement_, i.e., the fine-grained control of army units. TorchCraft  and TorchCraftAI  provide interfaces for training agents on _StarCraft: BroodWar_. The StarCraft II Learning Environment (SC2LE)  provides a Python interface for communicating with the game of _StarCraft II_ and has been used to train AlphaStar , a grandmaster-level but fully centralised agent that is able to beat professional human players. SMAC and SMACv2 are built on top of SC2LE and concentrate only on decentralised unit micromanagement for the CTDE setting.

One limitation of SMAC is the constant starting positions and types of units, allowing methods to memorise action sequences for solving individual scenarios (as we show in Section 5.1), while also lacking the ability to generalise to new settings at test time, which is crucial for real-world applications of MARL . To address these issues, SMACv2 relies on procedural content generation [PCG; 36; 18] whereby infinite game levels are generated algorithmically and differ across episodes. PCG environments have recently gained popularity in single-agent domains [7; 6; 17; 21; 38] for improving generalisation in RL  and we believe the next generation of MARL benchmarks should follow suit. Iqbal et al.  and Mahajan et al.  consider updated versions of SMAC by randomising the number and types of the units, respectively, to assess the generalisation in MARL. However, these works do not include the random start positions explored in SMACv2, analyse the properties of SMAC to motivate these changes, or address SMAC's lack of meaningful partial observability. They also do not change the agents' field-of-view and attack ranges or provide a convenient interface to generate new distributions over these features.

## 3 Background

### Dec-POMDPs

A partially observable, cooperative multi-agent reinforcement learning task can be described by a _decentralised partially observable Markov decision process (Dec-POMDP)_. This is a tuple \((n,S,A,T,,O,R,)\) where \(n\) is the number of agents, \(S\) is the set of states, \(A\) is the set of individual actions, \(T:S A^{n}(S)\) is the transition probability function, \(\) is the set of joint observations, \(O:S A^{n}()\) is the observation function, \(R:S A^{n}()\) is the reward function, and \(\) is the discount factor. We use \((X)\) to denote the set of probability distributions over a set \(X\). Ateach timestep \(t\), each agent \(i\{1,,n\}\) chooses an action \(a A\). The global state then transitions from state \(s S\) to \(s^{} S\) and yields a reward \(r\) after the joint action \(\), according to the distribution \(T(s,)\) with probability \((s^{}|s,)\) and \(R(s,)\) with probability \((r^{}|s,)\), respectively. Each agent then receives an observation according to the observation function \(O\) so that the joint observation \(\) is sampled according to \(O(s,)\) with probability \((|s,)\). This generates a trajectory \(_{i}( A^{n})^{*}\). The goal is to learn \(n\) policies \(_{i}:(_{i} A)^{*}(A)\) that maximise the expected cumulative reward \([_{i}^{i}r_{t+i}]\).

### StarCraft Multi-Agent Challenge (SMAC)

Rather than tackling the full game, the StarCraft Multi-Agent Challenge (Sandam, 2017, SMAC) benchmark focuses on micromanagement challenges where each military unit is controlled by a single learning agent. Units have a limited field-of-view and no explicit communication mechanism when at test time. By featuring a set of challenging and diverse scenarios, SMAC has been used extensively by the MARL community for benchmarking algorithms. It consists of 14 micromanagement scenarios, which can be broadly divided into three different categories: _symmetric_, _asymmetric_, and _micro-trick_. The symmetric scenarios feature the same number and type of allied units as enemies. The asymmetric scenarios have one or more extra units for the enemy and are often more difficult than the symmetric scenarios. The micro-trick scenarios feature setups that require specific strategies to counter. For example, 3s_vs_5z requires the three allied stalkers to kite the five zealots, and corridor requires six zealots to block off a narrow corridor to defeat twenty-four zerglings without being swarmed from all sides. Two state-of-the-art algorithms on SMAC are MAPPO (Sandam, 2017) and QMIX (Sandam, 2017). We provide background to these methods in Section 2 of the appendix.

## 4 Partial Observability and Stochasticity in CTDE

If the initial state and transition dynamics of a Dec-POMDP are deterministic, then open-loop policies suffice, i.e., there exists an optimal joint policy that ignores all information but the timestep and agent ID. Such a policy amounts to playing back a fixed sequence of actions. **One way of determining whether SMAC has sufficient stochasticity therefore is to measure how strongly the learned policies condition on their observations**. For example, in a mostly deterministic environment, a policy may only be required to condition on a small subset of the observation space for a few timesteps, whereas a more stochastic environment might require conditioning on a large number of observation features. We use this concept in section 5.2 to evaluate SMAC's stochasticity.

Since open-loop policies do not rely on observations, the observation function and any partial observability it induces only become relevant when the environment is stochastic enough to require closed-loop policies. However, even in stochastic environments not all partial observability is meaningful. If the hidden information is not relevant to the task, or can be inferred from the observation, it is resolvable by a single agent alone, and if it is unknown and undiscoverable by any of the agents, then it cannot contribute to solving the task. **Without meaningful partial observability, a task cannot be considered truly decentralised because a policy can either ignore or infer any unknown information without the intervention of other agents**. Meaningful partial observability is essential to the difficulty (and NEXP-completeness) of Dec-POMDPs (Brock et al., 2017). While SMAC is technically partially observable, there is limited hidden information because large fields-of-view render most relevant observations common among agents.

## 5 Limitations of SMAC

In this section, we analyse stochasticity in SMAC by examining the performance of open-loop policies and the predictability of Q-values given minimal observations. We show that SMAC is insufficiently stochastic to require complex closed-loop policies.

### SMAC Stochasticity

We use the observation that if the initial state and transition functions are deterministic, there always exists an optimal deterministic policy that conditions solely on the timestep information to investigate stochasticity in SMAC. In particular, we use a range of SMAC scenarios to compare policies conditioned only on the timestep (which we refer to as _open-loop_) to policies conditioned on the full observation history (i.e., all the information typically available to each agent in SMAC), which we refer to as _closed-loop_. The open-loop policies observe no ally or enemy information, and can only learn a distribution of actions at each timestep. If stochasticity in the underlying state is a serious challenge when training policies on SMAC scenarios, then policies without access to environment observations should fail. However, if the open-loop policies can succeed at the tasks, this demonstrates that SMAC is not representing the challenges of Dec-POMDPs well and so would benefit from added stochasticity.

We use MAPPO and QMIX as our base algorithms because of their widespread adoption and very strong performance and train open- and closed-loop versions of each. We train the _open-loop_ policies on SMAC, but only allow the policies to observe the agent ID and timestep, whereas the closed-loop policies are given the usual SMAC observation as input with the timestep appended. The open-loop policies still have access to the central state as usual during training since this is only used in CTDE to aid in training and not during execution. The QMIX open-loop policy was trained only on the maps which the MAPPO open-loop policy was not able to completely solve. For QMIX, the hyperparameters used are the same as in  and for MAPPO the hyperparameters are listed in Appendix C.

Figure 2 shows the mean win rates and standard deviation of both policies on selected scenarios. The full results are available in Figure 8 in the appendix. Overall, the open-loop policies perform significantly worse than their closed-loop counterparts. However, this performance difference varies widely across scenarios and algorithms. For MAPPO, some scenarios, such as bane_vs_bane, 3s5z, 1c3s5z and 2s3z achieve open-loop performance indistinguishable from that of closed-loop. In other scenarios, such as Bm_vs_9m and 27m_vs_30m, there are large performance differences, but the open-loop policy still achieves high win rates. There are also scenarios, such as corridor and 2c_vs_64zg, where the MAPPO open-loop policy does not learn at all, whereas the closed-loop policies can learn successfully. QMIX also achieves good performance on 2c_vs_64zg, MMM2, 27m_vs_30m and 8m_vs_9m, although for the latter maps the closed-loop policy strongly outperforms the open-loop QMIX. Altogether, there are only four SMAC maps where the open-loop approach cannot learn a good policy at all: 3s5z_vs_3s6z, corridor, 6h_vs_8z and 5m_vs_6m. Overall, open-loop policies perform well on a range of scenarios in SMAC. The _easy_ scenarios show no difference between the closed-loop and the open-loop policies, and some _hard_ and _very hard_ scenarios show either little difference or non-trivial win rates for the open-loop policy. These successes are striking given the restrictive nature of the open-loop policy. This suggests that stochasticity is not a significant challenge for a wide range of scenarios in SMAC.

These results highlight an important deficiency in SMAC. Stochasticity is either not evaluated or not part of the challenge for the vast majority of SMAC maps. Not testing stochasticity is a major flaw for a general MARL benchmark because without stochasticity the policy only needs to take optimal actions along a single trajectory. Additionally, without stochasticity, there can be no meaningful partial observability. Given widespread use of SMAC as a benchmark to evaluate algorithms for Dec-POMDPs, this suggests that a new benchmark is required to evaluate MARL algorithms.

### SMAC Feature Inferrability & Relevance

In this section, we look at stochasticity from a different perspective. We mask (i.e. "zero-out") all features of the state and observations on which QMIX's joint Q-function conditions. If the Q-values can still be easily inferred via regression, then SMAC does not require learning complex closed-loop

Figure 2: QMIX (left) and MAPPO (right) open-loop and closed-loop results.

policies as Dec-POMDPs in general do, but is solvable by ignoring key aspects of StarCraft II gameplay. If the central \(Q\)-value could be inferred only from timestep information, this would also imply the task is not decentralised because all agents implicitly know the timestep and so could operate with the same central policy.

We also extend this experiment to masking subsets of the observation and state features. By only masking a subset of features, we measure the impact of different features on the learned policy. The more observation features the \(Q\)-function conditions on, the more confident we can be that SMAC requires understanding decentralised agent micromanagement rather than learning action sequences. We measure this conditioning by regressing to Q-values of a trained policy when masking different sets of features. The observation features hidden by each mask are in Table 1 in the appendix. Since we evaluate the regression performance of the joint Q-function, we always mask features in the state as well as the observation. More details of the regression procedure are in Appendix C.

Some interesting trends can be observed in Figure 3, which shows Q-value regression loss for each mask and scenario against steps in the RL episode. First, the root-mean-squared error of masking _everything_ is low, reaching about \(15\%\) of the mean \(Q\)-value at peak, and 5-10% for most of the episode. Mean root-mean-squared error as a proportion of mean \(Q\)-value, as well as other metrics, is given in the appendix in Table 2. For all scenarios except gm_vs_6m this value is below \(0.12\). These are not much higher than the baseline _nothing_ masks, suggesting the observation features do not really inform the Q-value. These results highlight the lack of stochasticity and meaningful partial observability in SMAC and the necessity of novel benchmarks to address these shortcomings.

## 6 SMACv2

As shown in the previous section, SMAC has some significant drawbacks. To address these, we propose three major changes: random team compositions, random start positions, and increasing diversity among unit types by using the true unit attack and sight ranges. These changes increase stochasticity and so address the deficiency discovered in the previous section.

Teams in SMACv2 are procedurally generated. Since StarCraft II does not allow units from different races to be on the same team, scenarios are divided depending on whether they feature Protoss, Terran or Zerg units. In scenarios with the same number of enemy and ally units, the unit types on each team are the same. Otherwise the teams are the same except for the extra enemy units, which are drawn identically to the other units. We use three unit types for each race, chosen to be the same unit types that were present in the original SMAC. We generate units in teams algorithmically, with each unit type having a fixed

Figure 4: Examples of the two different types of start positions, _reflect_ and _surround_. Allied units are shown in _blue_ and enemy units in _dark red_.

Figure 3: Comparison of the loss for different feature masks when regressing to a trained QMIX policy. The x-axis plots steps within an episode. ‘Episodes Running’ is the number of episodes that have not terminated by the given timestep. _everything_ masks the entire observation and _nothing_ masks no attributes. _team_all_ masks all attributes of the team. _last_action_only_ masks the last actions of all the aliles in the state. The _all_except_actions_ masks all the allied information _except_ the last actions in the state. The mean is plotted for each mask and standard deviation across 3 seeds used as error bars. The low error rates for the everything mask imply that Q-values can be effectively inferred given only the timestep.

probability. The probabilities of generating each unit type are given in Table 9 in the appendix. These probabilities are the same at test time and train time.

Random start positions in SMACv2 come in two different flavours. In _reflect_ scenarios, the allied units are spawned uniformly at random on one side of the scenario. The enemy positions are the allied positions reflected in the vertical midpoint of the scenario. This is similar to how units spawned in the original SMAC benchmark but without clustering them together. In _surround_ scenarios, allied units are spawned at the centre and surrounded by enemies stationed along the four diagonals. Figure 4 illustrates both flavours of start position.

The final change is to the sight range and attack range of the units. In the original SMAC, all unit types had a fixed sight and attack range, allowing short- and long-ranged units to choose to attack at the same distance. This did not affect the real attack range of units within the game - a unit ordered to attack an enemy while outside its striking range would approach the enemy before attacking. We change this to use the values from SC2, rather than the fixed values. However, we impose a minimum attack range of \(2\) because using the true attack ranges for melee units makes attacking too difficult.

Taken together, these changes mean that before an episode starts, agents no longer know what their unit type or initial position will be. Consequently, by diversifying the range of different scenarios the agents might encounter, they are required to understand the observation and state spaces more clearly, which should render learning a successful open-loop policy not possible. We also define a convenient interface for defining distributions over team unit types and start positions. By only implementing a small Distribution class one can easily change how these are generated.

Akin to the original version of SMAC , we split the scenarios into _symmetric_ and _asymmetric_ groups. In the symmetric scenarios, allies and enemies have the same number and type of units. In the asymmetric scenarios, the enemies have some extra units chosen from the same distribution as the allies. There are 5 unit, 10 unit and 20 unit symmetric scenarios, and 10 vs 11 and 20 vs 23 unit asymmetric scenarios for each of the three races.

### Extended Partial Observability Challenge

We now introduce the Extended Partial Observability (EPO) challenge, where we make additional modifications to SMACv2 to ensure an extension of SMAC with meaningful partial observability. While SMAC is clearly partially observable in the sense that agents do not observe the global state, this partial observability is not particularly meaningful. SMAC certainly occludes information that would be relevant to each agent through the use of fixed sight ranges. However, the ranges of attack actions are always less than sight ranges and so ally teams can do without communication by moving within firing range and attacking any observed enemies. Finding enemies is not itself a significant challenge, as ally and enemy units spawn closely together. Therefore, much of the information that is relevant to the action selection of a given agent is within its observation.

To create a benchmark setting with meaningful partial observability, we introduce two further changes to SMACv2. First, enemy observations are stochastically masked for each agent. Once an enemy has been observed for the first time within an episode, the agent that was first to see it is guaranteed to observe it as normal. If two or more agents observe an enemy that had not yet been observed on the same timestep, there is a random tie break. As soon as this initial sighting is made, a random binary draw occurs for all other agents, which determines whether an agent will be able to observe that enemy for the remainder of the episode if it enters the agent's sight range. Agents for which the draw is unsuccessful have their observations masked such that they cannot observe any information about that enemy for the rest of the episode, even if it enters their sight range. The draw has a tunable probability \(p\) of success. We suggest \(p=0\) in the challenge. Figure 5 illustrates this setup.

Figure 5: Example EPO enemy sighting. Allied units that do not observe the enemy are shown in _blue_, those that do are shown in _green_ and the enemy unit in _dark red_. Initially, an ally spots an enemy. Later (right), when the enemy is within all allied sight ranges, only the first ally to observe the enemy and the ally for which the draw was successful can see it.

In SMAC, the environment provides an available action mask that guarantees agents can only choose attack actions if the corresponding enemy is within shooting range. In this setting only, we remove this available actions mask. This makes deciding on a target more difficult because agents cannot rely on the available actions to infer which enemies they can attack. Hence they must either infer or communicate this information. If an invalid action is chosen, it becomes a no-op. We also found that without removing the available actions, there was little separation between the \(p=1\) (i.e. all allies can target all enemies) and \(p=0\) (i.e. only the first ally to see an enemy can target it) cases, as shown in Figure 13 in the appendix. We believe that this is because the available actions mask encourages heavy weight to be placed on attack actions, knowing that they will not be chosen unless available. In EPO, because of the increased difficulty, 6 allied agents battle against 5 enemy units. We found that this increased the gap between the \(p=1\) and \(p=0\) settings. We believe this is because in the more difficult 5_vs_5 setting, agents are too preoccupied with other more basic challenges for target prioritisation to be a large issue. Results for the 5_vs_5 setting are shown in Figure 14 in the appendix and for the 6_vs_5 setting in Figure 7. In EPO, there are 3 maps, one for each race, with 6 allies and 5 enemies per team. All other settings such as unit types and start positions are identical to SMACv2. We recommend a setting of \(p=0\).

## 7 SMACv2 Experiments

In this section, we present results from a number of experiments on SMACv2. We first train state-of-the-art SMAC baselines on SMACv2 to assess its difficulty. Currently, both value-based and policy optimisation approaches achieve strong performance on all SMAC scenarios [55; 13]. As baselines we use QMIX , a strong value-based method and MAPPO , a strong policy optimisation method. We also investigate the effect of the new SMACv2 environment features with ablations. These results are available in Figure 12 in the appendix.

### SMACv2 Baseline Comparisons

First we compare the performance of MAPPO, QMIX, QPLEX and IPPO and an open-loop policy, based on MAPPO, on SMACv2. We run each algorithm for 10M training steps and 3 environment seeds. For MAPPO, we use the implementation from Sun et al.  and for QMIX the one from Hu et al. . The results of the runs of the best hyperparameters are shown in Figure 6.

These results shown in Figure 6 reveal a few trends. First, QMIX generally performs better than MAPPO across most scenarios. QMIX strongly outperforms MAPPO in two Protoss scenarios (protoss_20_vs_20 and protoss_10_vs_11), but otherwise final performance is similar, although QMIX is more sample efficient. However, QMIX is memory-intensive because of its large replay buffer and so requires more compute to run than MAPPO. Additionally, MAPPO appears to still be

Figure 6: Comparison of the mean test win rate of QMIX, MAPPO, IPPO, QPLEX and an open-loop policy on SMACv2. Plots show the mean and standard deviation across 3 seeds.

increasing its performance towards the end of training in several scenarios, such as the 10_vs_11 and 20_vs_20 maps. It is possible that MAPPO could attain better performance than QMIX if a larger compute budget were used. MAPPO and IPPO perform nearly identically across the maps, suggesting MAPPO gains little benefit from the centralised state. QPLEX performs worse than QMIX across a number of maps, particularly the 20_vs_20 maps.

Roughly even win rates are attained on the symmetric maps. By contrast, on the asymmetric maps, win rates are very low, particularly on the 20_vs_23 scenarios. Additionally, there is no real change in difficulty as the number of agents scales. However, the maps do not seem uniformly difficult, with all algorithms struggling with the Zerg scenarios more than other maps. Reassuringly the open-loop method cannot learn a good policy on any maps - even those where QMIX and MAPPO both achieve high win rates. This is strong evidence that SMAC's lack of stochasticity has been addressed.

### EPO Baselines

We now compare QMIX and MAPPO on EPO. A comparison between the \(p=1\), \(p=0\) and \(p=0.5\) settings is shown in Figure 7. In this setting, \(p=0\) struggles to attain even modest performance, whereas \(p=1\) comes close to solving the task in a number of instances. This leaves a lot of room for the development of algorithms that learn to use implicit communication as a means of overcoming meaningfully restrictive partial observability and reduce the performance gap between the \(p=1\) and \(p=0\) settings. The performance in the \(p=0.5\) case is similar to the \(p=0\) case. This suggests that any communication protocol would have to be reasonably robust to have an impact on performance. This makes sense - an additional 50% chance to observe an enemy would still make it difficult to perform group target prioritisation. In general QMIX has a smaller gap between the \(p=0\) and \(p=1\) cases, suggesting it is perhaps better able to use the global state to resolve partial observability differences than MAPPO.

### SMACv2 Feature Inferrability & Relevance

In this section we repeat the experiments from Section 5.2 on the 5 unit SMACv2 scenarios. We use three previously trained QMIX policies to generate 3 different datasets for training, and compute error bars in the regression across these three policies. The results are given in Table 3 and Figure 11 in the appendix.

When comparing results between SMAC and SMACv2, we focus on the difference between the _everything_ (all features not visible) and _nothing_ (no features hidden) masks. This is to account for some scenarios having much higher errors in the _nothing_ mask than others. For example, 5m_vs_6m has an error as a proportion of the mean Q-value in the _nothing_ mask of \(0.14\), compared to \(0.018\) for 2c_vs_64zg. The SMACv2 scenarios have higher errors than any of the SMAC scenarios when subtracting \(}}{Q}\) for the _everything_ and _nothing_ masks. For example, this value is \(0.12\) for 5_gen_protoss, which has the lowest value for any tested SMACv2 map. This is significantly higher than the \(0.07\) for 5m_vs_6m, which has the highest value for any SMAC map. Tables 2 and 3 in the appendix list the full values for SMAC and SMACv2. These results suggest that SMACv2 is significantly more stochastic than SMAC. Second, examining stepwise results in Figure 11 in the appendix shows that SMACv2 maps have similar patterns of feature importance to SMAC maps. Enemy health is the most important individual feature and is important throughout an episode for all scenarios tested. Importantly, both the ally and enemy all masks reach significant thresholds of the

Figure 7: Comparison of mean test win rate when \(p=0,0.5,1\), with 6 agents against 5 enemy units, for QMIX (left) and MAPPO (right). Here, the available actions mask has been removed.

average \(Q\)-value, suggesting that it is important in SMACv2 for methods to attend to environment features.

## 8 Conclusion

Using a set of novel experimental evaluations, we showed that the popular SMAC benchmark for cooperative MARL suffers from a lack of stochasticity and meaningful partial observability. We then proposed SMACv2 and repeated these evaluations, demonstrating significant alleviation of this problem. We also demonstrated that baselines that achieve strong performance on SMAC struggle to achieve high win rates on the new scenarios in SMACv2. We performed ablations and discovered that the stochasticity of the unit types and random start positions combine to explain the tasks' difficulty. We also introduced and evaluated current methods on the extended partial observability (EPO) challenge and demonstrated that meaningful partial observability is a significant contributor to the challenge in this environment.

Gorsane et al.  found that it was most common for MARL papers accepted at top conferences to only evaluate on one benchmark, relying on the multiple scenarios within that benchmark to provide sufficient variety. However, in addition to the issues shown in that paper with cherry-picking of scenarios within a benchmark, this evaluation protocol is vulnerable to issues in environment dynamics, such as SMAC's lack of stochasticity, and community-wide overfitting. Given the difficulty in designing both benchmarks and sufficiently diverse scenarios within them, we recommend that evaluating on _multiple benchmarks_ should become the norm where possible to help avoid issues.

We hope that SMACv2 can contribute to MARL research as a significantly challenging domain capturing practical challenges.