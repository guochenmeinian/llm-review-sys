# On Differentially Private U Statistics

Kamalika Chaudhuri

University of California San Diego

kamalika@ucsd.edu

&Po-Ling Loh

University of Cambridge

pll28@cam.ac.uk

&Shourya Pandey

University of Texas at Austin

shouryap@utexas.edu

&Purnamrita Sarkar

University of Texas at Austin

purna.sarkar@austin.utexas.edu

###### Abstract

We consider the problem of privately estimating a parameter \([h(X_{1},,X_{k})]\), where \(X_{1}\), \(X_{2}\),..., \(X_{k}\) are i.i.d. data from some distribution and \(h\) is a permutation-invariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even \((1/n)\) rather than \(O(1/n^{2})\) in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using _local Hdjek projections_. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics.

## 1 Introduction

A fundamental task in statistical inference is to estimate a parameter of the form \([h(X_{1},,X_{k})]\), where \(h\) is a possibly vector-valued function and \(\{X_{i}\}_{i=1}^{n}\) are i.i.d. draws from an unknown distribution. U-statistics are a well-established class of estimators for such parameters and can be expressed as averages of functions of the form \(h(X_{1},,X_{k})\). U-statistics arise in many areas of statistics and machine learning, encompassing diverse estimators such as the sample mean and variance, hypothesis tests such as the Mann-Whitney, Wilcoxon signed rank, and Kendall's tau tests, symmetry and uniformity testing , goodness-of-fit tests , counts of combinatorial objects such as the number of subgraphs in a random graph , ranking and clustering , and subsampling .

U-statistics are a natural generalization of the sample mean. However, little work has been done on U-statistics under differential privacy, in contrast to the sizable body of existing work on private mean estimation . Ghazi et al.  and Bell et al.  consider U-statistics in the setting of local differential privacy , while we are interested in privacy guarantees under the central model. Moreover, existing work on private U-statistics focuses on discrete data, and relies on simple privacy mechanisms (such as the Laplace mechanism ) which are usually optimal in these settings.

Many U-statistics converge to a limiting Gaussian distribution with variance \(O(k^{2}/n)\) when suitably scaled. This is commonly used in hypothesis testing . However, there are also examples of non-degenerate U-statistics, which often arise in a variety of hypothesis tests , where the statistic is degenerate at the null hypothesis (in which case the U-statistic converges to a sum of centered chi-squared distributions ). Another interesting U-statistic arises in subgraph counts in random geometric graphs . When the probability of an edge being present tends to zero with \(n\), creating a private estimator by simply adding Laplace noise with a suitable scale may not be effective.

#### Our contributions

1. We present a new algorithm for private mean estimation that achieves nearly optimal private and non-private errors for non-degenerate U-statistics with sub-Gaussian kernels.

2. We provide a lower bound for privately estimating non-degenerate sub-Gaussian kernels, which nearly matches the upper bound of our algorithm. We also derive a lower bound for degenerate kernels and provide evidence that the private error achieved by our algorithm in the degenerate case is nearly optimal. A summary of the utility guarantees of our algorithm and adaptations of existing private mean estimation methods is presented in Table 1.

3. The computational complexity of our first algorithm scales as \(\). We generalize this algorithm and develop an estimator based on subsampled data, providing theoretical guarantees for a more efficient version with \(O(n^{2})\) computational complexity.

The paper is organized as follows. Section 2 reviews the background on U-statistics and key concepts in differential privacy. Section 3 introduces an initial set of estimators based on the CoinPress algorithm for private mean estimation . Section 4 presents our main algorithm, which leverages what we term _local Hdjek projections_. Section 5 discusses applications of our algorithm to private uniformity testing and density estimation in sparse geometric graphs. Section 6 concludes the paper.

## 2 Background and Problem Setup

Let \(n\) and \(k\) be positive integers with \(k n\). Let \(\) be an unknown probability distribution over a set \(\), and let \(h:^{k}\) be a known function symmetric in its arguments1. Let \(\) be the distribution of \(h(X_{1},X_{2},,X_{k})\), where \(X_{1},X_{2},,X_{k}\) are i.i.d. random variables. We are interested in providing a \(\)-differentially private confidence interval for the estimable parameter \(=[h(X_{1},X_{2},,X_{k})]\), which is the mean of the distribution \(\), given access to \(n\) i.i.d. samples from \(\); we use \(X_{1},X_{2},,X_{n}\) to denote these \(n\) samples. The kernel \(h\), the degree \(k\), and the estimable parameter \(\) are allowed to depend on \(n\); we omit the subscript \(n\) for the sake of brevity.

We consider bounded kernels \(h\) and unbounded kernels \(h\) where the distribution \(\) is sub-Gaussian. We write \(Y()\) if \([((Y-Y))](^{2}/2)\) for all \(\). The quantity \(\) is called a _variance proxy_ for the distribution \(\) and satisfies the inequality \(^{2}\). Throughout the paper, we assume that the privacy parameter \(=O(1)\). We also use the notation \(()\) in error terms, which hides poly-logarithmic factors in \(n/\).

### U-Statistics

Let \([n]\) denote \(\{1,,n\}\), and let \(_{n,k}\) be the set of all \(k\)-element subsets of \([n]\). Denote the \(n\) i.i.d. samples by \(X_{1},X_{2},,X_{n}\). For any \(S_{n,k}\), let \(X_{S}\) be the (unordered) \(k\)-tuple \(\{X_{i}:i S\}\). The U-statistic associated with the data and the function \(h\) is

\[U_{n}:=}_{\{i_{1},,i_{k}\}_{n,k}} h(X_{i_{1}},,X_{i_{k}}).\] (1)

The function \(h\) is the _kernel_ of \(U_{n}\) and \(k\) is the _degree_ of \(U_{n}\). While U-statistics can be vector-valued, we consider scalar U-statistics in this paper. The variance of \(U_{n}\) can be expressed in terms of the conditional variances of \(h(X_{1},X_{2},,X_{n})\). For \(c[k]\), we define the conditional variance

\[_{c}:=([h(X_{1},,X_{k})|X_{1},, X_{c}]).\] (2)

Equivalently, \(_{c}=(h(X_{S_{1}}),h(X_{S_{2}}))\) where \(S_{1},S_{2}_{n,k}\) and \(|S_{1} S_{2}|=c\). The number of such pairs of sets \(S_{1}\) and \(S_{2}\) is equal to \(\), which implies

\[(U_{n})=^{-1}_{c=1}^{k} _{c}.\] (3)Since \([U_{n}]=\), \(U_{n}\) is an unbiased estimate of \(\). Moreover, the variance of \(U_{n}\) is a lower bound on the variance of any unbiased estimator of \(\). (cf Lee [45, Chapter 1, Theorem 3]). We also have the following inequality from Serfling  (see Appendix A.2 for a proof):

\[_{1}}{2}}{3}}{k}.\] (4)

**Infinite-order U-Statistics:** Classical U-statistics typically have small, fixed \(k\). However, important estimators that appear in the contexts of subsampling  and Breiman's random forest algorithm  have \(k\) growing with \(n\). These types of U-statistics are sometimes referred to as _infinite-order_ U-statistics ). U-statistics also frequently appear in the analysis of random geometric graphs . The difference between this setting and the examples above is that the conditional variances \(\{_{c}\}\) vanish with \(n\) in the sparse setting. (See Section 5.)

**Degenerate U-statistics:** A U-statistic is _degenerate_ of _order_\( k-1\) if \(_{i}=0\) for all \(i[]\) and \(_{+1}>0\) (if \(_{k}=0\), the distribution is almost surely constant). Degenerate U-statistics arise in hypothesis tests such as Cramer-Von Mises and Pearson tests of goodness of fit  and tests for uniformity . They also appear in tests for model misspecification in econometrics . For more examples of degenerate U-statistics, see .

### Differential privacy

The main idea of differential privacy  is that the participation or non-participation of a single person should not affect the outcome significantly. A (randomized) algorithm \(M\), that takes as input a dataset \(D^{n}\) and outputs an element of its range space \(\), satisfies \(\)-differential privacy if for any pair of adjacent datasets \(D\) and \(D^{}\) and any measurable subset \(S\) of the range space \(\), \((M(D) S) e^{}(M(D^{}) S)\). A dataset is \(D:=(X_{1},X_{2},,X_{n})\) from some domain \(\), for some \(n\) which is public. Two datasets \(D\) and \(D^{}\) are adjacent if they differ in exactly one index. An important property of differentially private algorithms is composition. We defer composition theorems for differentially private algorithms to Appendix A.2.

**Basic DP algorithms.** One way to ensure an algorithm satisfies differential privacy is through the Laplace Mechanism . The global sensitivity of a function \(f:^{n}\) is

\[GS(f)=_{|D D^{}|=1}|f(D)-f(D^{})|,\] (5)

where \(D D^{}:=|\{i:D_{i} D^{}_{i}\}|\) A fundamental result in differential privacy is that we can achieve privacy for \(f\) by adding noise calibrated to its global sensitivity.

**Lemma 1**.: _(Laplace mechanism ) Let \(f:^{n}\) be a function and let \(>0\) be the privacy parameter. Then the algorithm \((D)=f(D)+()^{2}\) is \(\)-differentially private._

The global sensitivity of a function is the worst-case change in the function value and may be high on atypical datasets. To account for the small sensitivity on "typical" datasets, the notion of _local sensitivity_ is useful. The local sensitivity of a function \(f:^{n}\) at \(D\) is defined as

\[LS(f,D)=_{|D D^{}|=1}|f(D)-f(D^{})|.\] (6)

Unfortunately, adding noise proportional to the local sensitivity does not ensure differential privacy, because variation in the magnitude of noise itself may leak information. Instead,  proposed the notion of a smooth upper bound on \(LS(f,D)\). A function \(SS(f,)\) is said to be an \(\)-smooth upper bound on the local sensitivity of \(f\) if (i) \(SS(f,D) LS(f,D)\) for all \(D\), and (ii) \(SS(f,D) e^{}SS(f,D^{})\) for all \(|D D^{}|=1\). Intuitively, (i) ensures that enough noise is added, and (ii) ensures that the noise itself does not leak information about the data.

**Lemma 2**.: _(Smoothed Sensitivity mechanism ) Let \(f:^{n}\) be a function, \(>0\), and \(SS(f,\ \ )\) be an \(\)-smooth upper bound on \(LS(f,\ \ )\). Then, the algorithm \((D)=f(D)+SS(f,D)/ Z\), where \(Z\) has density \(h(z)}\), is \(/10\)-differentially private._

## 3 Lower Bounds and Application of Off-the-shelf Tools

We start with a simple, non-private estimator involving an average of independent quantities. Let \(m= n/k\), and define \(S_{j}=\{(j-1)k+1,(j-1)k+2,,jk\}\) for all \(j[m]\). Define the naive estimator \(_{}:=_{j=1}^{m}h(X_{S_{j}})/m\). Directly applying existing private mean estimation algorithms  to our setting yields an error bound of3

\[((_{})}+k/(n ))=(/n}+k/(n) ),\] (7)

since \((_{})=k_{k}/n\). Note that this variance is larger than the dominant term \(k^{2}_{1}/n\) of \((U_{n})\) (see Lemma A.1 and Eq 4); indeed, \(_{}\) is a suboptimal estimator of \(\).

In Algorithms A.2 and A.3, we present a general extension of the CoinPress algorithm , which is then used to obtain a private estimate of \(\) with the non-private error term matching \((U_{n})}\). For completeness, we present the algorithms and their proofs in Appendix A.3.

**Definition 1** (All-tuples family).: _Let \(M=\) and let \(_{}=\{S_{1},S_{2},,S_{M}\}=_{n,k}\) be the set of all \(k\)-element subsets of \([n]\). Call \(_{}\) the "all-tuples" family._

**Proposition 1**.: _Suppose \([-R,R]\). Let \(_{}\) be the all-tuples family in Definition 1. Then Wrapper 1, with \(f=\) all, failure probability \(\), and \(=\) U-StatMean (Algorithm A.2) returns an estimate \(_{}\) of the mean \(\) such that, with probability at least \(1-\),_

\[|_{}-| O((U_{n_{ }})})+(}{n_{}} ),\] (8)

_as long as \(n_{}=(})\). Moreover, the algorithm is \(\)-differentially private and runs in time \(((1/)(k+})}{k})\)._

**Remark 1**.: _While Lemma 1 recovers the correct first term of the deviation, the private error term is a \(\) factor worse. Moreover, we need \(k^{2}/n=o(1)\) for the private error to be asymptotically smaller than the non-private error. Note, however, that existing concentration  or convergence in probability results  only require \(k=o(n)\) (see Lemmas A.1 and A.3 in the Appendix)._

**Remark 2** (Degenerate and sparse settings).: _While Lemma 1 improves over the naive estimator, the private error can overwhelm the non-private error for degenerate and sparse U-statistics (see Section 5). We show that for uniformity testing, using this estimator can lead to suboptimal sample complexity if the distribution is already close to uniform._

Proposition 1 improves over the naive estimator at the cost of computational complexity. We can trade off the computational and statistical efficiencies using a different family \(\) parameterized by the size \(M\) of \(\). In Appendix 3, we show a result similar to 1 for the subsampled family.

 
**Algorithm** &  &  \\   & Private error & Is non-private error & Private error & Is non-private error \\  & \(O((U_{n}))\)? & & \(O((U_{n}))\)? \\  Naive (Proposition A.1) & \((}{n})\) & No & \(()\) & No \\  All-tuples (Proposition 1) & \((}{n})\) & Yes & \(()\) & No \\  Main algorithm & \((}{n})\) & **Yes** & \((C}{n^{3/2}})\) & **Yes** \\  & Corollary 1 & & Corollary 2 & \\  
 Lower bound \\ for private algorithms \\  & \((}{n}})\) & & \((C}{n^{3/2}})\) & \\  & Theorem 1 & & Theorem 3 \\  

Table 1: We compare our application of off-the-shelf tools to Algorithm 1. We only provide the leading terms in the private error. The non-private lower bound on \((-h(X_{1},,X_{k}))^{2}\) for all unbiased \(\) is \((U_{n})\), which our private algorithms nearly match.

**Definition 2** (Subsampled Family).: _Draw \(M\) i.i.d. samples \(S_{1},,S_{M}\) from the uniform distribution over the elements of \(_{n,k}\), and let \(_{ss}:=\{S_{1},,S_{M}\}\). Call \(_{}\) the "subsampled" family._

The next result shows a nearly optimal dependence on \(n\) and \(\) in the bounds for \(_{}\) and \(_{}\). In particular, the dependence of the modified Coinpress algorithm (Lemma 1) on \(k\) is suboptimal.

**Theorem 1** (Lower bound for non-degenerate kernels).: _Let \(n\) and \(k\) be positive integers with \(k<n/2\) and let \(=(k/n)\). Let \(\) be the set of all sub-Gaussian distributions over \(\) with variance proxy \(1\), and let \(\) be the output of any \(\)-differentially private algorithm applied to \(n\) i.i.d. observations from \(\). Then, \(_{h,:}|(X_ {1},,X_{n})-[h(X_{1},,X_{k})]|=(}).\)_

Among unbiased estimators, \(U_{n}\) is the best non-private estimator [35; 45]. The most widely used non-private estimators are \(U\)- and \(V\)-statistics, which share similar asymptotic properties . The above lower bound also has a log factor arising from an optimal choice of the sub-Gaussian proxy for Bernoulli random variables . Proofs are deferred to Appendix A.4.

### Boosting the error probability via median-of-means

If we use Algorithm A.2 as stated with failure probability \(\), then the error in the algorithm has a \(O(1/)\) factor, which is undesirable. Instead, we use the Algorithm with a constant failure probability (say, \(0.25\)) and then boost this failure probability to \(\) via a median-of-means procedure. We incorporate the median-of-means in all of our theoretical results.

**Wrapper 1** (MedianOfMeans(\(n\), \(k\), Algorithm \(\), Parameters \(\), Failure probability \(\), Family type \(f\{,\}\))).: _Divide \([n]\) into \(q=8(1/)\) independent chunks \(I_{i},i[q]\) of roughly the same size. For each \(i[q]\), run Algorithm \(\) with subset family \(_{i}:=_{f}(I_{i})\), Dataset \(\{h(X_{S})\}_{S_{i}}\), and other parameters \(\) for \(\) to output \(_{i},i[q]\). Return \(=(_{1},,_{q})\)._

In the above wrapper, \(_{f}(D_{i})\) simply creates the appropriate family of subsets for the dataset \(D_{i}\). For example, if \(D_{i}=\{X_{1}, X_{n_{}}\}\), \(f=\), then \(_{}(D_{i})\) is \(\{h(X_{S})\}_{S_{n_{},k}}\). If \(f=\), then \(_{}(D_{i})\) is \(\{h(X_{S})\}_{S_{i}}\), where \(_{i}\) is the set of \(M\) subsampled subsets of \(D_{i}\). Here, \(n_{}:=\). Wrapper 1 can be used to boost the failure probability from constant to \(\) by splitting the data into \(O((1/))\) chunks, applying the algorithm on each of the chunks, and taking the median output. The expense of this procedure is the reduction in the effective sample size to \(n_{}=(n/(1/))\). Details and proofs on the median-of-means procedure can be found in Section A.3.2.

## 4 Main Results

In Section 3, we showed that off-the-shelf private mean estimation tools applied to U-statistics either achieve a sub-optimal non-private error (see Remark 1) or a sub-optimal private error. If the U-statistic is degenerate of order \(1\), the non-private and private errors (assuming \(=(1)\)) are \((1/n)\). We now present an algorithm that achieves nearly optimal private error for sub-Gaussian non-degenerate kernels. Our algorithm can be viewed as a generalization of the algorithm proposed in Ullman and Sealfon  for privately estimating the edge density of an Erdos-Renyi graph. We provide strong evidence that, for bounded degenerate kernels, we achieve nearly optimal non-private error. All proofs for this section can be found in Section A.5.

### Key intuition

Our key insight is to leverage the Hajek projection [58; 45], which gives the best representation of a U-statistic as a linear function of the form \(_{i=1}^{n}f(X_{i})\):

\[_{n}}{{=}}_{i=1}^{n}[T_{n}|X_ {i}]-(n-1)[T_{n}]}{{=}}_ {i=1}^{n}[h(X_{S})|X_{i}]-(n-1).\]

Equality (i) gives the form of the Hajek projection for a general statistic \(T_{n}\), whereas (ii) gives the form when \(T_{n}\) is a U-statistic. Let \(_{n,k}^{(i)}=\{S_{n,k}:i S\}\). In practice, one uses the estimates

\[}[h(X_{S})|X_{i}]:=}_{S _{n,k}^{(i)}}h(X_{S}),\] (9)which we call _local Hajek projections_. In some sense, this is the U-statistic when _viewed locally_ at \(X_{i}\). When the dataset is clear from context, we write \(_{_{n,k}}(i)\), or simply \((i)\), for \(}[h(X_{S})|X_{i}]\).

### Proposed algorithm

Consider a family of subsets \(_{n,k}\) of size \(M\). Let \(_{i}=\{S:i S\}\) and \(M_{i}=|_{i}|\), and suppose \(M_{i} 0\) for all \(i[n]\). Assume also that \(\) satisfies the inequalities

\[M_{i}/M 3k/n\ \ \ \ M_{ij}/M_{i} 3k/n\] (10)

for any distinct indices \(i\) and \(j\) in \([n]\) (one such family is \(=_{n,k}\), for which \(M_{i}/M=k/n\) and \(M_{ij}/M_{i}=(k-1)/(n-1) k/n\), but there are other such families). Define

\[A_{n}():=_{S}h(X_{S}),_{}(i):=}_{S_{i}}h(X_{S }), i[n].\] (11)

\(U_{n}\) in Eq (1) and \(}[h(X_{S})|X_{i}]\) in equation (9) are the same as the quantities \(A_{n}(_{n,k})\) and \(_{_{n,k}}(i)\).

A standard procedure in private estimation algorithms is to clip the data to an appropriate interval [10; 41] in such a way that the sensitivity of the overall estimate can be bounded. Similarly, we use the concentration of the local Hajek projections to define an interval such that each \(i\) can be classified as "good" or "bad" based on the distance between \(_{}(i)\) and the interval. The final estimator is devised so the contribution of the bad indices to the estimator is low and the estimator has low sensitivity.

Let \(\) and \(C\) be parameters to be chosen later; they will be chosen in such a way that with high probability, (i) \(|(i)-|\) for all \(i\), and (ii) each \(h(X_{S})\) is at most \(C\) away from \(\). Define

\[L_{}:=*{arg\,min}_{t_{>0}}(| \{i:|_{}(i)-A_{n}()|>+6kCt/n \}| t).\] (12)

In other words, \(L_{}\) is the smallest positive integer \(t\) such that at most \(t\) indices \(i[n]\) satisfy \(|_{}(i)-A_{n}()|>+\) (such an integer \(t\) always exists because \(t=n\) works). Define

\[():=\{i:|_{}(i)-A_{n}( )|+6kCL_{}/n\},( ):=[n].\] (13)

For each index \(i[n]\), define the weight of \(i\) with respect to \(\) as

\[_{}(i):=(0,1-(_{}(i)-A_{n},[--6kCL_{}/n,+6kCL_{ }/n])).\] (14)

Here, \(\) is the privacy parameter and \((x,I)\) is the distance between \(x\) and the interval \(I\).

Based on whether a datapoint is good or bad, we will define a weighting scheme that reweights the \(h(X_{S})\) in equation (1). For each \(S\), let

\[_{}(S):=_{i S}_{}(i),  g_{}(X_{S}):=h(X_{S})_{}(S)+A_{ n}()(1-_{}(S)).\]

In particular, if \(_{}(S)=1\), then \(g_{}(X_{S})=h(X_{S})\); and if \(_{}(S)=0\), then \(g_{}(X_{S})=A_{n}()\). Finally, define the quantities

\[_{n}():=_{S}g_{}(X_ {S}),_{}(i):=}_{S_{i}}g _{}(X_{S})\ \  i[n].\] (15)

To simplify notation, we will drop the argument \(\) from \(L\), \(A_{n}\), \(_{n}\), \(\), \(\), Good, and Bad.

**Idea behind the algorithm:** If all \((i)\)'s are within \(\) of the empirical mean \(A_{n}\), then \(=\) and \(_{n}=A_{n}\). Otherwise, for any set \(S\) containing a bad index, we replace \(h(X_{S})\) by a weighted combination of \(h(X_{S})\) and \(A_{n}\). This averaging-out of the bad indices allows a bound on the local sensitivity of \(_{n}\). We then provide a smooth upper bound on the local sensitivity characterized by the quantity \(L\), which can be viewed as an indicator of how well-concentrated the data is. The choice of \(\) will be such that \(L=1\) with high probability and that the smooth sensitivity of \(_{n}\) at \(\) is small. This ensures that a smaller amount of noise is added to \(_{n}\) to preserve privacy.

**Theorem 2**.: _Algorithm 1 is \(10\)-differentially private for any \(\). Moreover, suppose \(h\) is bounded with additive range \(C\),4 and with probability at least \(0.99\), we have \(_{i}|_{}(i)-A_{n}|\). Run Wrapper 1 with \(f=\) all, and \(=\). With probability at least \(1-\), the output \(\) satisfies \(|-|=O((U_{n_{}})}+ {n_{}}+(}{n_{}^{2}^{2}}+}{n_{}^{3}^{3}})C).\)_

**Connections to Ullman and Sealfon :** Ullman and Sealfon  estimate the edge density of an Erdos Renyi graph using strong concentration properties of its degrees. This idea can be loosely generalized to a broader setting of U-statistics: consider a hypergraph with \(n\) nodes and \(\) edges, where the \(i^{th}\) node corresponds to index \(i\). An edge corresponds to a \(k\)-tuple of data points \(S_{n,k}\), and the edge weight is given by \(h(X_{S})\). The natural counterpart of a degree in a graph becomes a local Hajek projection, defined as in equation (9). In degenerate cases and cases where \(k^{2}_{1} k_{k}\), the local Hajek projections are tightly concentrated around the mean \(\). We exploit this fact and reweight the edges (\(k\)-tuples) so that the local sensitivity of the reweighted U-statistic is small.

### Application to non-degenerate and degenerate kernels

Algorithm 1 can be extended from bounded kernels to sub-Gaussian\(()\) kernels. First, split the samples into two roughly equal halves. The first half of the samples will be used to obtain a coarse estimate of the mean \(\). For this, we can use any existing private mean estimation algorithm to obtain an \(/2\)-differentially private estimate \(_{}\) such that with probability at least \(1-\), \(|_{}-|=(/n}+k/(n)).\) By a union bound, with probability at least \(1-\), \(|h(X_{S})-|\) is within \(4/)}\) for all \(S_{n,k}\), and therefore also within \(c\) of the coarse estimate \(_{}\), for some universal constant \(c\), as long as \(=(/n)\).

Define the projected function \((X_{1},X_{2},,X_{k})\) to be the value \(h(X_{1},X_{2},,X_{k})\) projected to the interval \([_{}-c,_{ }+c]\). The final estimate of the mean \(\) is obtained by applying Algorithm 1 to the other half of the samples, the function \(\), and theprivacy parameter \(/2\). The following lemma shows that \(\) is a valid choice of the concentration parameter \(\) for sub-Gaussian, non-degenerate kernels.

**Lemma 3**.: _If \(\) is sub-Gaussian\(()\), the local Hajek projections \((i)\) are also sub-Gaussian\(()\). In particular, with probability at least \(1-\), we have \(_{1 i n}|(i)-|\)._

Combining these parameters with Theorem 2 gives us the following result:

**Corollary 1** (Non-degenerate sub-Gaussian kernels).: _Suppose \(h\) is sub-Gaussian\(()\) and the privacy parameter \(=(k^{1/2}/n)\). Split the samples into two halves and compute a private estimate of the mean by applying the naive estimator on the first half of the samples with privacy parameter \(/2\) to obtain \(_{}\). Let \(\) be the projection of of the function \(h\) onto the interval \(_{} O()\). Run Wrapper I on the remaining half of the samples with \(f=\) all, failure probability \(/2\), algorithm \(\) = PrivateMeanLocalHajek (Algorithm 1), \(C=\), \(=\), function \(\), and privacy parameter \(/20\). Then, the output \(\) is \(\)-differentially private. Moreover, with probability at least \(1-\),_

\[|-|=O((U_{n_{}})})+ (}{n_{}}+}{n_{}^{2}^{2}}+}{n_{}^{3} ^{3}}).\]

From our lower bound on non-degenerate kernels in Theorem 1, we see that the above corollary is optimal in terms of \(k,n,\) (up to log factors). In contrast, Lemma 1 is suboptimal in \(k\).

Many degenerate U-statistics (e.g., all the degenerate ones in Section 5) have bounded kernels. For these, we see that the local Hajek projections concentrate strongly around the U-statistic.

**Lemma 4**.: _Suppose \(\) is bounded, with additive range \(C\). Let \(i[n]\) be an arbitrary index and \(S_{i}_{n,k}\) be a set containing \(i\), and suppose \(x_{i}\) is some element in the support of \(\). With probability at least \(1-\), conditioned on \(X_{i}=x_{i}\), we have_

\[|}[h(X_{S})|X_{i}=x_{i}]-[h(X_{S_{i}}) |X_{i}=x_{i}]| 2_{i}()}+(),\] (16)

_where \(}[h(X_{S})|X_{i}=x_{i}]=}h(X_{S_{i}}) }{()}\), and \(_{i}^{2}=(h(X_{S_{i}})|X_{i}=x_{i})\)._

For bounded kernels with additive range \(C\), \(_{i} C/2\) by Popoviciu's inequality . Moreover, for degenerate kernels, \(_{1}=0\). That is, the conditional expectation \([h(X_{S_{i}})|X_{i}=x_{i}]\) is equal to \(\) for all \(x_{i}\), because the variance of this conditional expectation is \(_{1}\). Based on this, we can show that the choice of \(=(Ck^{1/2}/n^{1/2})\) satisfies the requirement that the local Hajek projections are within \(\) of \(\) with probability at least \(1-\).

**Corollary 2** (Degenerate bounded kernels).: _Suppose \(h\) is bounded with additive range \(C\) and the kernel is degenerate \(_{1}=0\). Let \(=(k^{1/2}/n)\) be the privacy parameter. Run Wrapper I with \(f=\) all, failure probability \(\), and algorithm \(\) = PrivateMeanLocalHajek (Algorithm 1) with \(=O(C(n/))\), to output \(\). With probability \(1-\), we have_

\[|-|=O((U_{n_{}})} )+(}{n_{}^{1.5}}C+} {n_{}^{2}^{2}}C+}{n_{}^{3}^{3}}C),\]

Obtaining a result for sub-Gaussian degenerate kernels poses difficulties on bounding the concentration parameter \(\). However, for bounded kernels, we see that the above result obtains better private error than the application of off-the-shelf methods (Lemma 1). In the next subsection, we provide a lower bound for degenerate bounded kernels, which, together with Corollary 2, gives strong indication that our algorithm achieves optimal private error for private degenerate kernels.

### Lower bound

To obtain a lower bound of the private error, we construct a dataset and kernel function such that the local Hajek projections are \(1/\) concentrated around the corresponding U-statistic. This is one way of characterizing a degenerate U-statistic. The proof of the following theorem is in Appendix A.4.

**Theorem 3**.: _For any \(n,k\) with \(k n\), \(=((k/n)^{1-1/2(k-1)})\), and \(\)-differentially private algorithm \(:^{n}\), there exists a function \(h:^{k}\{0,1\}\) and dataset \(D\) such that \(|(i)-U_{n}|\) (where \((i)\) and \(U_{n}\) are computed on \(D\)) for every \(i[n]\) and \(|(D)-U_{n}|=(}{n^{3/2}})\), where the expectation is taken over the randomness of \(\)._

**Remark 3**.: _The above lower bound is in some sense informal because we created a deterministic dataset and \(h\) that mimics the property of a degenerate U statistic; the local Hajek projections concentrate around \(U_{n}\) at a rate \(\). However, it gives us a strong reason to believe that the private error cannot be smaller than \(O(k^{3/2}/n^{3/2})\) for degenerate U statistics of order \(k\). Note that for bounded kernels, Corollary 2 does achieve this bound, as opposed to Lemma 1._

### Subsampling estimator

We now focus on subsampled U-statistics. Previous work has shown how to use random subsampling to obtain computationally efficient approximations of U-statistics [38; 52; 17], where the sum is replaced with an average of samples (drawn with or without replacement) from \(_{n,k}\).

Recall Definition 2. Let \(:=\{S_{1},,S_{M}\}\) denote the subsampled set of subsets, let \(_{i}:=\{S:i S\}\), and let \(M_{i}:=|_{i}|\). The proof of Theorem 2 with \(=_{n,k}\) (cf. Appendix A.5) uses the property of \(_{n,k}\) that \(M_{i}/M=k/n\) and \(M_{ij}/M_{i}=(k-1)/(n-1)\), so the inequalities (10) certainly hold. Indeed, we can show that for subsampled data (cf. Lemma A.11), the following inequalities hold with probability at least \(1-\), provided \(M=(n^{2}/k^{2}(n/))\):

\[M_{i}/M 3k/n\ \ \ \ \ M_{ij}/M_{i} 3k/n.\] (17)

Algorithmically, we check if the bounds (17) hold for \(\), and output \(\) if not. Privacy is not compromised because the check only depends on \(\) and is agnostic to the data.

**Theorem 4**.: _Let \(M_{n}=((n^{2}/k^{2}) n)\). Then Algorithm 1, modified to output \(\) if the bounds (17) do not hold, is \(10\)-differentially private. Moreover, suppose that with probability at least \(0.99\), we have \(_{i}|_{}(i)-A_{n}|\) and \(|h(S)-| C\) for all \(S_{n,k}\). Run Wrapper 1 with \(f=ss\), failure probability \(\), and \(=\). With probability at least \(1-\), we have_

\[|()-|=O((U_{n_{}})}+ }{M_{n_{}}}}+}+( C}{n_{}^{2}^{2}}+C}{n_{}^{3} ^{3}})(k,)).\]

**Remark 4**.: _If the kernel is non-degenerate and the number of times we subsample (for each run of the algorithm) is \((n_{}^{2}/k^{2})\), then Theorem 4 nearly achieves the same error as Algorithm 1 with \(=_{n,k}\) with a better computational complexity for \(k 3\). The lower-order terms have an additional \((k,1/)\) factor, which can be removed with \((n^{3})\) subsamples._

## 5 Applications

We apply our methods to private uniformity testing and estimation in random networks. For more applications, see Appendix A.6.4.

**1. Uniformity testing:** A fundamental task in distributional property testing [6; 7] is deciding whether a discrete distribution is uniform on its domain, called the problem of _uniformity testing_. Let \(X_{1},X_{2},,X_{n}\) be \(n\) i.i.d. samples from a discrete distribution with support \([m]\), characterized by the probability masses \(p_{1},p_{2},,p_{m}\) on the atoms. Given an error tolerance \(>0\), the task is to distinguish between approximately uniform distributions \(\{p:_{2}(p,U)/\}\) and far-from-uniform distributions \(\{p:_{2}(p,U)/\}\).

Without the constraint of privacy, Diakonikolas et al.  perform this test by rejecting the uniformity hypothesis whenever the test statistic \(U_{n}:=_{i<j}(X_{i}=X_{j})/>(1+3^{2}/4)/m\), and show that this test succeeds with probability \(0.9\) as long as \(n=(m^{1/2}/^{2})\). As detailed in Algorithm A.4 in the appendix, instead of using \(U_{n}\), we use the private estimate \(_{n}\) using Algorithm 1.

For our algorithm to work, we require the distributions to satisfy \(p_{i} 2/m\) for all \(i\). Let \(p_{i}=(1+a_{i})/m\) for all \(i\), with \(a_{i}[-1,1]\). Under \(H_{1}\), we have\((1+^{2})/m\), where \(\|a\|\) is the \(_{2}\) norm of \((a_{1},a_{2},,a_{m})\). Under \(H_{0}\), the mean is \(1/m\). The difference between the threshold \((1+^{2}/2)/m\) and the mean (under either of the two hypotheses) is at least \(^{2}/(2m)\). Moreover,  shows that the standard deviation of \(U_{n}\) is much smaller than the difference in the means under \(H_{0}\) and \(H_{1}\) as long as \(n=(m^{1/2}/^{2})\). However, we must also account for the noise added to ensure privacy. In Appendix A.6.1, we show that the choice of \(=(1/m+1/n)\) works and establish the following result:

**Theorem 5**.: _Let \(p_{i}=(1+a_{i})/m\) with \(a_{i}[-1,1]\), \(_{i=1}^{m}a_{i}=0\). Let \(\{X_{j}\}_{j=1}^{n}\) be i.i.d. multinomial random variables such that \(P(X_{1}=i)=p_{i}\), for all \(i[m]\). There exists an algorithm that distinguishes between \(}{m^{2}}}{m}\) from \(}{m^{2}}<}{2m}\) with probability at least \(1-\), as long as \(n_{}=(}{^{2}}+}{( )}+(m/)}{^{1/2}}+}{^{2}/3}+})\), and is \(10\)-differentially private._

The non-private error term of Theorem 5 is the same as in Theorem 1 of  and is optimal . Proposition 1 shows that Algorithm A.2 with the all-tuples family leads to a private error bounded by \((1/n)\). This private error is \(O(^{2}/m)\) only when \(n=(m/^{2})\). In comparison, Algorithm A.4 has error \(O(^{2}/m)\) for \(n=(m^{1/2}/(^{2},))\), which is quadratically better in \(m\).

**Remark 5** (Comparison with existing algorithms).: _Existing results for private uniformity testing  distinguish between the uniform distributions (\(_{1}(p,U)=0\)) and distributions away from uniform in TV-distance (\(_{1}(p,U)\)). Our algorithm considers the alternative hypothesis to be \(_{2}(p,U)/\). Hence, our results are not strictly comparable. One caveat is that we restrict ourselves to distributions \(p\) such that \(_{}(p,U) 1/m\). Our algorithm also allows some tolerance in the null hypothesis, similar to  and other collision-based testers. That is, can allow some slack and take the null hypothesis \(H_{0}:_{2}(p,U)/\) instead of \(_{2}(p,U)=0\)._

**2. Sparse graph statistics:** The geometric random graph (see ) has edges \(h(X_{i},X_{j}):=(\|X_{i}-X_{j}\|_{2} r_{n})\), where \(r_{n}\) governs the average degree. Under a suitable distribution for \(X_{1},,X_{n}\), the subgraph counts show normal convergence for a large range of \(r_{n}\). Typically, we only observe the graph and do not know the underlying distribution of the latent variables \(X_{i}\) or the radius \(r_{n}\). This is why estimates of the network moments are of interest since they reveal information about the underlying unknown distribution and parameters.

Let the \(X_{i}\)'s be uniformly distributed on the three-dimensional sphere to ignore boundary conditions. For edge density, \(h(X_{i},X_{j}) r_{n}^{2}\). For any distinct indices \(i,j,k\) and a given \(X_{i}\), the random variables \(h(X_{i},X_{j})\) and \(h(X_{i},X_{k})\) are independent. Therefore, \(_{1}=(h(X_{i},X_{j})h(X_{i},X_{k}))=0\). We have \(_{2}=[h(X_{i},X_{j})]=O(r_{n}^{2})\), so the non-private error is \(O(r_{n}/n)\). In Appendix A.6.2, we provide Algorithm A.5 that uses Algorithm 1 to obtain a private estimate of the edge density of a graph \(\{h(X_{i},X_{j})\}_{1 i<j n}\). Note that the \(X_{i}\)'s themselves can be unknown. Our methods can also be used for private triangle density estimation. See the extended version  for details.

**Theorem 6**.: _Let \(r_{n}=(n^{-1/2})\) and \(=(1/nr_{n}^{2})\). Let \(\{X_{1},,X_{n}\}\) be i.i.d. latent positions such that \(X_{i}\) is distributed uniformly on \(^{2}\). Let the observed geometric network have adjacency matrix \(\{A_{ij},1 i<j n\}\) where \(A_{ij}=1(\|X_{i}-X_{j}\| r_{n})\). There exists a \(10\)-differentially private algorithm that estimates the edge density \(\) of the geometric graph. With probability at least \(1-\), the output \(\) satisfies \(|-|=(}{n_{}}+^{2}^{2}}+^{2}^{3}})\)_

**Remark 6**.: _By Lemma 1, the all-tuples estimator (1) satisfies \(|_{}-|(r_{n}/n+/n )\) with probability \(1-\), where \(\) is the variance proxy of the distribution. Since \(=(1)\), the private error overpowers the main variance term in sparse settings where \(r_{n}=o(1)\)._

## 6 Discussion

We have considered the problem of estimating \(:=h(X_{1},,X_{k})\) for a broad class of kernel functions \(h\). The best non-private unbiased estimator is a U statistic, which is widely used in estimation and hypothesis testing. While existing private mean estimation algorithms can be used for this setting, they can be suboptimal for large \(k\) or for non-degenerate U statistics, which have \(O(1/n)\) limiting variance. We provide lower bounds for both degenerate and non-degenerate settings. We analyze bounded degenerate kernels motivated by typical applications with degenerate U statistics. To extend this to the subgaussian setting is part of future work. We propose an algorithm that matches our lower bounds for sub-Gaussian non-degenerate kernels and bounded degenerate kernels. We also provide applications of our theory to private hypothesis testing and estimation in sparse graphs.