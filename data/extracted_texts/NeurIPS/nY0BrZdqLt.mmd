# Time-Reversal Provides

Unsupervised Feedback to LLMs

 Varun Yerram

Equal Contribution.

Google DeepMind

&Rahul Madhavan

Work done as part of Google Research

&Sravanti Addepalli

Work done as a Student Researcher at Google Research

&Arun Suggala

Google DeepMind

&Karthikeyan Shanmugam

Google DeepMind

&Prateek Jain

Google DeepMind

Equal Contribution.

Work done as part of Google Research

&Work done as a Student Researcher at Google Research

&Correspondence to: vyerram@google.com, sravantia@google.com, karthikeyanvs@google.com

###### Abstract

Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to \(5\%\) improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to _augment_ or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.

## 1 Introduction

Large Language Models (LLMs) trained on a large corpora of text are able to accomplish a wide variety of downstream tasks such as summarization, open-ended/ context-based question answering, document retrieval, and citation generation . While the generations from pre-trained and instruction-tuned models already show significant promise, alignment techniques such as Reinforcement Learning via Human Feedback (RLHF) , Ouyang et al. (2022) are widely used to improve the quality of their generations further. However, these methods rely heavily on additional supervision to construct preference data, which can be expensive to acquire, or noisy for training. This brings up a natural question - _Can we generate useful feedback on LLM generations without additional supervised data?_A recent line of work aims at _specially prompting_ LLMs to review their own generations and generate meaningful natural language feedback, which can subsequently be used to refine them [Madaan et al., 2024]. This process can be repeated to improve the generations iteratively. The success of such methods serves as an evidence that it is indeed possible to obtain better responses without additional supervision. However, such methods rely on the superior instruction following and reasoning abilities of LLMs, which may not necessarily hold for low capacity models. Further, these methods involve sequential processing of the generated responses, and thus increase inference time significantly.

In this work, we propose a natural method of enabling LLMs to _look backwards_ in order to obtain meaningful unsupervised feedback during inference. Towards this, we introduce a class of models that we call _Time Reversed Language Models_ (TRLMs), which operate in the reversed direction of a regular LLM, or the _time-reversed_ direction. Rather than predicting (or scoring) in the standard query response direction, time reversed language models predict (or score) in the response query direction. We first introduce TRLM-Fo - a TRLM variant based on forward models, which are _prompted_ to operate in the time-reversed direction using a prompt such as "Generate a question that would result in the following answer: <response>". Further, we extend the reversal to _token_-level granularity by pre-training LLMs from scratch in a reversed token direction, rather than the standard forward token direction. We call this as TRLM-Ba where Ba stands for Backward. Note that the inputs and outputs of such a model are in the reversed language order. Pre-training TRLM-Ba on reversed text exposes the model to a completely different world model where the conventional order of information is flipped. Introductions _follow_ conclusions, questions _follow_ answers, logical precedents _follow_ their antecedents. Hence, such a model may not only develop representations that are distinct from those of a regular LLM - despite being trained on the same pre-training corpus - but may also be better suited to score/ generate in the reverse direction, i.e. conditional on the response.

We show in several use-cases that scoring and generation in this reverse direction can produce non-trivial feedback on the responses generated by forward LLMs. We consider three classes of tasks to showcase the scoring and generating capability of TRLM, viz. a) Reranking answers in open ended question answering b) Citation and retrieval tasks and c) Amplifying existing safety filters through query generation in the reverse.

**Our Contributions:**

**a)** We propose time reverse language models - TRLM-Fo, TRLM-Ba and TRLM-FoBa, all of which score and generate queries given responses, enabling their use in obtaining unsupervised feedback on LLM generations. TRLM-Fo is a forward model prompted to predict in reverse, while TRLM-Ba is pre-trained in the reverse token order, enabling reverse prediction naturally. TRLM-FoBa is pre-trained in both reverse and forward token orders and can be used to predict in forward or reversed language.

**b)** We demonstrate significant improvements when best-of-N reranking is applied to multiple LLM generations by using TRLM scores. Specifically, we show up to a \(5\%\) improvement over self-reranking using TRLM-Ba, in LC win-rates (0.98 Pearson correlation with human preferences) against a GPT4-1106-Preview reference model. We show multiple ablations on this study.

**c)** We demonstrate that the reverse direction of scoring (response\(\)query) is highly significant, as it improves citation attribution accuracy by 44.15% when compared to the forward baseline on the CNN-Daily Mail dataset. Further, we improve the NDCG@10 metric by \(44.13\) points on the NF-Corpus medical information retrieval benchmark, and obtain similar improvements on MS-Marco as well.

**d)** We show that the reverse generation capability of the TRLM models - specifically TRLM-Ba, can be used to improve False Negative rate (FNR) of input safety filters with negligable impact on FPR. We show significant improvements on several attacks submitted to the Jailbreakbench benchmark, and on a Human Annotated dataset from JailbreakBench.

We complement these results with theoretical arguments using a bipartite graph model between queries and responses, to show that RLHF done with TRLM-Ba scores induces a non trivial distribution shift in answers, mitigating primitive forms of "hallucination" under the defined conditions.

## 2 Related Work

**Reverse Direction in Language Modeling:** Classical work [Serdyuk et al., 2017] showed how sequence to sequence models can regularize the current word token embedding based on the ability of the future tokens to be able to predict the current token. Such bi-directional (forward and reverse) consistency checks have been used to improve forward models. Golovneva et al.  train an LLMin the forward direction first, followed by the reverse token direction, and show that this alleviates the reversal curse identified by Berglund et al. (2023). This work is closely related to ours in that we also consider a variant of combining reverse and forward token order during training. Our key models differ from this, and are trained in either forward (TRLM-Fo )/ reverse (TRLM-Ba ) token order, using which we demonstrate improvements in a wide range of applications such as long form question answering, citations, retrieval and augmenting input filters for defending against toxic questions. Yang et al. (2023) use question generation from a given answer combined with access to external databases to determine hallucination. Another recent work (Guo et al., 2024) also explores a different pre-training order. While their focus is to correct causal ordering bias, our work instead is focused on the value that scoring and generation of these models bring to downstream tasks.

**Reversed scoring:** Several prior works (Li et al., 2016; Zhang et al., 2018, 2020) have proposed to improve the diversity of generated responses by optimizing the _mutual information_ between the responses and the respective queries. These works motivate the need for better decoding strategies based on scores in both, \(\) and \(\) directions. We theoretically show that reverse scoring alone, when used with forward generations, will achieve this naturally using a formal RLHF based argument (Lemma 2), and present strong empirical results across a wide range of tasks to support the same.

**Controlling Decoding through feedback:** A broad line of works align a pre-trained model to a reward model trained on human feedback by using Reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO) (Stiennon et al., 2020; Ouyang et al., 2022; Korbak et al., 2022), (Identity policy optimization) IPO (and \(\)) (Azar et al., 2024), Direct Preference Optimization (Rafailov et al., 2024) and offline RL (Snell et al., 2022). Zhao et al. (2022) and Zhao et al. (2023) calibrate likelihood of generated responses on a dataset with desired responses or human preference feedback.(Krause et al., 2020; Yang and Klein, 2021; Qin et al., 2022) control the generation of an LLM at test time by specifying constraint functions or discriminators that operate in the token or logit space, encouraging certain attributes in the output. Using preference feedback, Mudgal et al. (2023) train a prefix scorer model that acts as a value function over partial completions consistent with the preference rewards. Yang et al. (2024) investigate the relation between best-of-N-reranking and KL regularized RL objective. An observation made by Yang et al. (2024) is that best-of-N-reranking dominates/ competes very well with most RL based alignment methods. Under certain assumptions, authors show formally that best-of-N-reranking approximates the optimal solution to the regularized RL objective. We take inspiration from this and use best-of-N-reranking to evaluate generations through unsupervised feedback by the reverse LLMs. Our work differs from all these in that they rely on external feedback to control generation, while our method does not.

**Self Play and Self Tuning:**Chen et al. (2023) explore how an LLM can be prompted to self-debug based on an explanation of the code produced by the LLM during code generation and the execution output on test cases. Welleck et al. (2022) use a corrector model that is trained to prefer a new corrected answer if the corrected answer has higher value that a default generation. They require access to a value function for this determination. All these approaches use an external feedback to align the model in their pipeline.

Fu et al. (2023) explore LLM agents initialized as buyers and sellers to play a negotiating game of setting the price of a transaction. A critic LLM provides feedback to both the buyer and seller agents to improve. Madaan et al. (2024) propose a self refining loop where the same model is prompted to provide feedback and further use the feedback to refine and regenerate. Both these works use very powerful and large models from the Claude, GPT-4, GPT-3.5 family to use self generated language feedback. Madaan et al. (2024) remark that the self refining approach does not work well with weaker models. In contrast, we focus on improving generation quality of much smaller models using unsupervised scalar feedback. Other prior works relating to self play are reviewed in the survey article by Amini et al. (2022).

## 3 TRLM - Time Reversed Language Models

We introduce our primary contribution - TRLM (**T**ime **R**eversed **L**anguage **M**odels), a class of language models that operate in the \(\) direction during scoring and generation. This is achieved by either (a) [TRLM-Ba ] reversing the token order and effectively utilizing previous token prediction instead of next token prediction during pre-training, scoring, and generation, or (b)[TRLM-Fo ] maintaining the standard token order during pre-training but reversing the direction of generation through appropriate prompts during inference (scoring and generation).

We show that TRLM provides non-trivial unsupervised feedback that could be used by pre-trained, fine-tuned, and instruction tuned models, for various downstream tasks like reranking to improve open-ended long-form question answering, generating citations, and retrieval. We demonstrate that the ability of TRLM to score in the reverse direction - scoring query based on the response - is essential to achieve the requisite gains. Further, TRLMs that are pre-trained in the reverse direction (TRLM-Ba ) provide an additional boost in most cases. We further leverage the generative ability of TRLM in reverse (generating query from a response) to amplify the effectiveness of input safety filters as well.

We propose four variants of the TRLM class - TRLM-Ba, TRLM-Fo, TRLM-FoBa (Reverse) and TRLM-FoBa (Forward) - based on how they are pre-trained and fine-tuned. TRLM models can be considered to have three functions: TRLM.Pretrain, TRLM.Score, and TRLM.Generate, which we describe for each of the four variants in Table 1. We further outline these functions for different TRLM models in Algorithms 1, 2, 3, & 4. For this work, we consider two baselines, which are trained in forward token order, and score in the conventional order of response given the query. The first of these uses self-scoring based on the model's own perplexity. The second (Forward Baseline) is a forward model that we train, whose training corpus and model class are identical to TRLM.

TRLM Model Training:The pre-training setup for all TRLM models is identical to that of PALM2-Otter models described by Anil et al. [2023b], except for the token orders specified by our TRLM.pretrain methods for TRLM-Fo, TRLM-Ba and TRLM-FoBa respectively. We fine-tune them on the FLaN dataset [Longpre et al., 2023] using the TRLM-xx.pretrain function. Where xx can refer to Fo, Ba or FoBa based on the model being fine-tuned. Let Instruction, Question, Answer

} 
**Model** & **Description** \\  TRLM-Ba & Pre-trained in the reverse token order for previous token prediction (Alg. 1 in the supplement). Instruction-tuned variant is FLaN fine-tuned [Longpre et al., 2023] in reverse token order. Scores the reversed question given a reversed answer combined with suitable prompts. Generates questions in the reverse direction when conditioned on answers in the reverse direction. \\  & **Scoring:**\(_{}\)Reverse(Scoring Prompt+Query) \(\) Reverse(Conditioning Prompt + Answer)\(\) (Alg. 2 in the supplement). \\  & **Generation:**\(_{}\ \ \ \ |\ \ \ \) \\  TRLM-Fo & Pre-trained in the usual forward token order. Scores Question given Answer using the prompt. Generates from the conditional distribution of an answer. \\  & **Scoring:**\(_{}\)Query \(\) Answer + Conditioning Prompt \(\) (Alg. 3 in the supplement) **Generation:**\(_{}\ \ \ \ |\ \ \ \) \\  TRLM-FoBa (Reverse) & Pre-trained both in forward and reverse token order (Alg. 4 in the supplement). Understands text in both directions. Reverse version scores and generates identically to TRLM-Ba. \\  & **Scoring:** Scores identically to TRLM-Ba. \\  & **Generation:** Generates identically to TRLM-Ba. \\  TRLM-FoBa (Forward) & Pre-trained both in forward and reverse token order. Forward version scores and generates identically to TRLM-Fo. \\  & **Scoring:** Scores identically to TRLM-Fo. \\  & **Generation:** Generates identically to TRLM-Fo. \\  Self Scoring & The model that is used for generating a given response is also used for scoring responses given queries in the conventional forward scoring direction. \\  & **Scoring:** We use the model’s own perplexity scores as feedback to select the responses. \\  Forward Baseline & A conventional forward model trained for next-token prediction on the same training corpus and model class as TRLM. \\  & **Scoring:** While self-scoring used the perplexity obtained from the generator model, in this setting, we use perplexity of a different forward model. \\  

Table 1: Description of different TRLM model variants.

denote instruction, question and answer respectively. Before calling the pretrain function during fine tuning, we merge Instruction + Question to be the new question.

## 4 Scoring in Reverse

In this section, we provide formal results on TRLM and the benefit of using pre-training in the reverse direction. Let us denote by \(_{}(A|Q)\) the conditional distribution of a forward LLM. Similarly, denote \(P_{}(Q|A)\) to be the conditional distribution of the Time Reversed Language Model. For simplicity, we merge the instruction and question together.

### Formal Results on Reverse LLM based Alignment

In this subsection, we focus on the distribution shift encountered while using a reverse model based scorer on forward generations. Specifically, we conclude that while reranking using Forward Baseline is equivalent to temperature scaling (Yang et al., 2024b), reranking using TRLM induces a distribution shift that is not equivalent to temperature scaling.

Consider the _alignment_ problem of learning a new forward LLM - \(}_{}(|)\). A very popular framework is the KL constrained optimization objective with respect to a reward oracle \((,)\), for some threshold \(\):

\[_{}_{}}*{}_{ \\ _{}(| )}[(,)]\;\;D_{}(}_{}\|_{})\] (1)

**Log-perplexity of the forward model used as reward:** In general, for long form question answering where an explicit reward model is not available, a typical method is to use log-perplexity of the forward model i.e. \(_{}\) as a reward. Then, we have the following corollary of Lemma 1 in Yang et al. (2024b),

**Lemma 1** (Corollary of Lemma 1 in Yang et al. (2024b)).: The new LLM policy \(}_{}\) that optimizes (1) is given by: \(}_{}(|) _{}^{1+}(|)\) where \(\) is chosen appropriately depending on the threshold \(\) when reward \(R()\) is set to \(\) perplexity of the forward model \(_{}\).

A policy obtained post the constrained KL-alignment procedure is akin to temperature re-scaled forward model, since \(p^{1+}\) is equivalent to _temperature rescaling_\(^{(1+) p}\).

**Log-perplexity of the** TRLM-Ba.score **used as reward:** Suppose \(R()\) is set to output of TRLM-Ba.score computed on the the question given the answer, then we have:

**Lemma 2** (Corollary of Lemma 1 in Yang et al. (2024b)).: The new LLM policy \(}_{}\) that optimizes (1) is given by: \(}_{}(|) _{}(|) _{}^{}(|)\) where \(\) is chosen appropriately depending on \(\) when reward \(R()\) is set to \(\) perplexity of the reverse model \(_{}\).

Optimal distribution after alignment using TRLM scores results in a non-trivial distribution that is not simply temperature re-scaling. While we have not used TRLM for alignment using KL constraints in our experiments, the distribution shift that is induced by reverse token training is indeed non-trivial even with Best-of-N-re-ranking, which we adopt in our experiments.

## 5 Experimental Results

In this section, we explore the effectiveness of time reversed language models on different downstream tasks, by utilizing unsupervised feedback to improve upon existing forward model generations. Broadly, these applications fall into two categories - first, where we utilize the scoring capacity of TRLM (three use cases), and second where we utilize the generative capacity of TRLM for generating queries given a response.

### Best-of-N reranking

The best-of-N reranking task involves outputting the best response out of \(N\) model responses to a user query. Specifically, given \(N\) LLM outputs to a user query, a reranking algorithm finds the best response based on scalar scores assigned to each response. Prior works (Rafailov et al., 2023; Mudgal et al., 2023a) aim to improve LLM performance on this task by using feedback-based RLHF algorithms and training on KL-regularized alignment objectives. Yang et al. (2024a) show that best-of-N reranking is the most effective way to approximate these RL objectives, and further, it is empirically observed to outperform them.

In this work, we consider several best-of-N reranking based algorithms based on TRLM.Score, for evaluating a base model response. The methods considered rely on nothing more than the pre-training (or instruction-tuning) corpus to achieve alignment of response to the user query. We further note that such scores from TRLM may be used within RL objectives as well, but we leave the exploration of such rewards to future work.

#### 5.1.1 Alpaca Leaderboard Evaluation

**Benchmark and Evaluation:** The AlpacaEval leaderboard (Dubois et al., 2024) is a widely used benchmark to evaluate the capability of language models. In this benchmark, there are 805 questions from the _AlpacaFarm_ evaluation set - consisting of questions ranging from general writing, chat ability, and reasoning to general knowledge. The goal is to output a response that is better than a base model's response, as judged by an annotator model. Both base model and annotator model are set as GPT4-1106-Preview on the AlpacaEval leaderboard as on May 10, 2024, and hence we use the same for our evaluations. The evaluation benchmark computes various metrics including winrates, discrete winrates and length-controlled winrates (Dubois et al., 2024). The length-controlled winrates are calculated using a debiasing algorithm that removes the length bias that is otherwise preferred by GPT4-1106-Preview.

Formally, we define the task for TRLM as follows -- Given a query \(Q\) from the dataset and \(N\) model responses \(=\{A_{1} A_{N}\}\) from a generator model, we wish to use TRLM.score to output the highest scoring response \(a_{i}\), which is further evaluated against an answer from GPT4-1106-Preview.

In our experiment, we consider outputs from a generator model that is Gemini-Pro-1.0 (Anil et al., 2023a). We generate 16 responses using a temperature \(=0.8\) to ensure diversity of answers. We then rerank the responses using different variants of TRLM from the PALM2-Otter family of models (TRLM training details in the supplement). We further consider two baselines, Self scoring and Forward Baselines, as described in Table 1. Scoring prompts and Conditioning prompts used with various TRLM variants for this task are described in the Table 7 of Appendix C.1.

**Discussion of Results:** In Table 8, we see that TRLM-Ba scores the highest length controlled win rate which is \(5\%\) over the self scoring baseline of Gemini-Pro-1.0 with \(16\) generations against the GPT4-1106-Preview judge. Further, it registers an \(8\%\) increase over the reported number for single generations in the benchmark leaderboard. We note that scoring Response->Query seems to bring out some improvements as TRLM-Fo improves over Forward Baseline. Further, TRLM-Ba outperforms TRLM-Fo indicating the impact of reverse token pre-training. This demonstrates that time reversed scoring provides an intrinsic unsupervised feedback that could help improve the performance of even larger capacity models. We note that pre-training in both forward and reverse directions (TRLM-FoBa models) and scoring in the reverse direction is better than TRLM-Fo variant.

We present further results where the generations of a Mixtral model (Jiang et al., 2024b) are reranked and compared against GPT4-1106-Preview, and the generations of a smaller Mixtral model are reranked and compared against a larger Mixtral model. These results are presented in the Appendix C.2. We note a 4% improvement over Forward Baseline with the proposed TRLM-Ba.Score method of reranking.

**Key Takeaway:** Through empirical justifications, we show that TRLM variant models can be used as effective re-rankers of generations from multiple classes of models (Gemini-Pro-1.0, Mixtral8x22B, Mixtral8x7B), and improve the instruction following capability of the model as a whole. This is consistent with the 1 considering the fact that we outperform generation model's self-log perplexity score. While other methods of re-ranking exists, to the best of our knowledge none of them provide unsupervised feedback for effective reranking with just a pre-trained model.

### Citation Attribution

In this section, we describe applications of reverse scoring to the task of producing citations to original passages that can _corroborate_ the sentences in an already produced summary. Summaries are created from long form articles, and one often wants to know which part of the article a given summary sentence is derived from (Benjamin Cohen-Wang (2024)).

**Dataset and Evaluation:** For this task, we take the CNN Daily Mail Dataset [CNN] which consists of pairs of news articles and their respective highlights. Our goal is to identify which sentence (or groups of sentences) within a given news article provides the most direct corroboration for a specific article highlight given as a query. We evaluate the attributed citations using various relevancy metrics. We use cosine similarity on the embeddings of the Gecko model (Lee et al., 2024), cosine similarity on TF-IDF features, BLEU score and ROUGE score to compute metrics. We score and choose the best pairing using all the models from the TRLM PALM2-Otter family trained in the forward, reverse and forward-reverse directions as outlined in Section 5.1.1.

**Algorithms:** Different search algorithms, Linear Search, Binary Search and Exclusion Search are coupled with using TRLM.score to find the attribution. We outline these in Algorithms 7, 8 and 9 along with details in the supplement. The number of inference calls is \(O( N)\) where \(N\) is the number of article sentences for Binary Search, and this method produces multiple sentences as a citation. The other methods require \(O(N)\) calls to produce the citation for a sentence.

Our results shown in Table 3, demonstrate the efficacy of TRLM for the attribution task. Specifically, we show 44% gains over the baseline in the linear search method, 39% gains in the binary search method and 34% gains in the exclusion search method as measured through gecko cosine similarity.

**Key Takeaway:** Through our results on CNN-Daily Summarization dataset we present multiple methods of citation attribution and demonstrate significant gains with TRLM model variants. We note that a direction of _low_ information to _high_ information (summary -> article) is harder to reason upon and select among a given set of texts. Further, we highlight the importance of binary **selection based** approach over log-perplexity based **exclusion based** search. We show 9% improvement using TRLM-Ba on Gecko embedding-based metric using only \(O( N)\) inference calls to the main model.

   } &  &  &  &  &  &  \\    &  & & & **LC** & & **Reg** & & **Discrete** & & **Error** \\  TRLM-Ba & Response -> Query & 32.44 & 24.35 & 24.04 & 1.27 & 192 & 610 & 3 \\ TRLM-FoBa (backward) & Response -> Query & 31.18 & 22.72 & 21.99 & 1.24 & 176 & 627 & 2 \\ TRLM-FoBa (forward) & Response -> Query & 30.55 & 22.85 & 22.48 & 1.25 & 180 & 623 & 2 \\ TRLM-Fo & Response -> Query & 29.19 & 22.68 & 21.30 & 1.24 & 170 & 632 & 3 \\ One Generation & - & 24.38 & 18.18 & 17.08 & 1.16 & 135 & 665 & 5 \\ Self & Query -> Response & 27.05 & 17.66 & 17.14 & 1.15 & 136 & 665 & 4 \\ Forward Baseline & Query -> Response & 24.27 & 17.13 & 15.78 & 1.12 & 126 & 677 & 2 \\   

Table 2: The best re-ranked response is compared with a single response of GPT4-1106-Preview. The setting is identical to the AlpacEval Alp Leader board. TRLM-Fo, that scores in the backward direction, fares better than the conventional forward baseline. Scoring using TRLM-Ba (pretrained in reverse) gets even a higher (LC) win rate.

    &  &  &  &  \\  & **Direction** & Gecko & TF-IDF & ROUGE & Gecko & TF-IDF & ROUGE & Gecko & TF-IDF & ROUGE \\  TRLM-Ba & A->S & 53.16 & **55.45** & 49.12 & **45.09** & **50.93** & **42.11** & 36.33 & 46.34 & 36.13 \\ TRLM-PoBa (Rev.) & A->S & **53.48** & 53.22 & **49.67** & 40.74 & 45.04 & 39.81 & 32.40 & 40.84 & 33.88 \\ TRLM-FoBa (Forward.) & A->S & 50.65 & 52.21 & 45.24 & 43.81 & 49.84 & 40.60 & **38.67** & **48.16** & **38.11** \\ TRLM-Fo & A->S & 45.00 & 49.40 & 37.66 & 43.14 & 49.65 & 39.22 & 37.90 & 47.83 & 37.98 \\ Forward Baseline & S->A & 9.33 & 9.54 & 11.06 & 5.88 & 6.66 & 6.69 & 4.66 & 7.53 & 7.00 \\ Backward Baseline & S->A & 7.62 & 8.23 & 9.18 & 5.47 & 6.23 & 6.32 & 4.11 & 5.02 & 5.11 \\   

Table 3: Tabulates the citation Attribution results through Re-ranking on the CNN-Daily Mail dataset. \(A\) denotes article whereas \(S\) denotes the corresponding summary. The ease of scoring a summary given the article instead of _reverse_ is clearly highlighted in all of the search methods.

### Document Retrieval

In this section, we study the performance of TRLM in retrieving relevant passages from a corpus to answer a specific question. Our goal is to show the efficacy of TRLM based reverse scoring over doing it in the forward direction. The task is as follows: Given a question, the goal is to retrieve relevant documents from the given corpus. We retrieve \(k\) documents from the corpus and compute various information-retrieval metrics to calculate performance w.r.t. the golden set of documents.

We experiment with two retrieval-based datasets from MTEB benchmark [Muennighoff et al., 2023] as shown in Table 4. Metrics are precision, recall, normalized discounted cumulative gain (NDCG) and mean reciprocal rank (MRR) (details in Appendix E.1). We show our results in Table 5. TRLM reverse scoring algorithms along with respective prompts used are presented in Algorithms 11, 10 of the Supplement. As Table 5 suggests, results favor TRLM based reverse scoring methods. For example, we see a 22.48% improvement in recall at \(K=4\) for MS-MARCO dataset. TRLM-Ba model dominates across metrics. For NF-Corpus, we see that the conventional forward scoring algorithm (query -> document) has a very poor performance. We attribute this to the fact that, in this inference direction, we are scoring a highly complex medical document using a simple natural language query. We see a gain of 44.2 points in NDCG at \(K=10\) with TRLM-Fo compared to Forward Baseline. The results in both these datasets suggest that TRLM can show greater gains when the complexity of documents in the corpus differs significantly from the complexity of queries.

**Key takeaways:** We experiment with two information retrieval-based benchmarks MS-MARCO and NF-CORPUS and compute multiple metrics to compare TRLM variant models with standard Forward Baseline and unconventional Backward Baseline. We show a gain of 8.49 points in NDCG@10 on MS-MARCO and 44.19 points in NDCG@10 on NF-CORPUS. Aligning with the results in citation, the results from this task also accurately demonstrate the importance of going from a _high_ information direction to a _low_ information direction. The massive difference between the directions is evident in the NF-CORPUS dataset.

### Defending against Jailbreak attacks

We next aim to leverage the generative ability of TRLM to augment toxicity filters that are used to improve the safety of LLMs. Prior works show that LLMs (and their input filters) can be jailbroken using crafted adversarial attacks [Zou et al., 2023], while output filters tend to have a high false negative rate due to the sensitivity to the presence of toxic words, despite being in a neutral context (See Table-10). We propose to combine the benefits of input and output filters by projecting the output response of LLMs to the input query space using the reverse generative capability of TRLM, and further detecting the toxicity of the generated queries to block/ pass the response to the original

  
**Dataset** & **Description** \\ 
**MS-Marco** & Contains 101.09k examples in its public dev split. Each example consists of a simple question along with 10 relevant passages. [Bajaj et al., 2016] \\
**NF-Corpus** & Medical information retrieval dataset with 323 queries in its test split and 3.6k total documents in the corpus. Queries are in simple English, and documents are extracted from PubMed with a fair amount of medical terminology. [Boteva et al., 2016a, Pub] \\   

Table 4: Summary of MS-Marco and NF-Corpus Datasets

    & & &  &  \\ 
**Method** & **Inference** &  &  &  &  &  &  \\  & **Direction** & K=1 & K=4 & K=1 & K=4 & **@10** & K=10 & K=20 & K=10 & K=20 & **@10** \\  TRLM-Ba & D -> Q & **28.4** & **18.54** & **27.22** & **70.29** & **61.49** & 15.7 & 11.38 & 10.68 & **13.08** & 43.23 \\ TRLM-FoBa (Reverx) & D -> Q & 24.9 & 17.38 & 23.85 & 65.85 & 58.84 & 19.08 & 10.91 & 10.01 & 12.76 & 41.65 \\ TRLM-FoBa (Forward) & D -> Q & 21.16 & 15.58 & 20.25 & 59.08 & 55.46 & **17.86** & **12.6** & **11.11** & 13.5 & 48 \\ TRLM-Fo & D -> Q & 20.37 & 14.9 & 19.45 & 56.39 & 54.46 & 17.31 & 12.38 & 9.74 & 11.76 & **48.08** \\ Forward Baseline & Q -> D & 21.05 & 13.82 & 18.42 & 47.81 & 53 & 0.87 & 0.87 & 0.17 & 0.31 & 3.89 \\ Backward Baseline & Q -> D & 16.8 & 14.04 & 15.99 & 53.13 & 25.07 & 1.11 & 0.79 & 0.21 & 0.29 & 3.95 \\   

Table 5: Tabulates the results of various reranking algorithms with two inference directions. \(Q\) denotes Queries, while \(D\) denotes Documents. TRLM outperforms Forward Baseline and Backward Baseline significantly, which highlights the importance of inference direction in this task.

query based on a pre-specified criteria. We thus effectively amplify input safety filters, i.e. reduce False Negative Rate (FNR) with marginal/ no impact on False Positive Rate (FPR).

**Key Idea:** Consider TRLM. Generate(Response) that generates queries that could have produced a given response. The insight is that, the reverse generative ability of TRLM allows the projection of a candidate (jailbreak) query that could bypass the input filter back to the (naive) query space observed during training. These projected questions can thus be rightly classified using the same input filter.

**Defense Strategy:** We propose a defense strategy where i) a query is passed through the input filter, ii) if the input filter rejects the query, we return reject as well, iii) if the input filter allows the query, we take the Response produced by the model and generate multiple queries using TRLM.Generate(Response). If the number of generated queries rejected exceeds a threshold, we reject the query as "unsafe". Otherwise, we declare it as safe, and output the response corresponding to the input query. An elaborate description is provided in Algorithm 12 of the Supplement.

**Datasets:** We consider a human annotated (HA) dataset provided as part of the JailbreakBench benchmark [HAd] for evaluating the performance of toxicity classifiers. This contains \(100\) questions annotated by humans, of which \(43\) are annotated as toxic based on a majority vote across 3 annotators. We introduce a GPT-4 based filter, that considers the prompt-response pair to judge their toxicity (Details in Appendix-F2), and has 0 FNR on this HA dataset, which is ideal for defense evaluation. We further consider a gpt-3.5-turbo-1106 based input toxicity filter for the empirical evaluation of the proposed defense, which has an FNR of 25.58% on this dataset. These unblocked questions form our new-HA dataset for the experiments. In addition to this, we use the following datasets for evaluation: JBB dataset that contains jailbreak questions (that are toxic as per the GPT-4 judge, but are safe as per the GPT-3.5 filter we augment) corresponding to different attacks on JailbreakBench, E dataset that contain safe and easy questions and H dataset that contains safe questions that are hard to classify as safe. We discuss more details on these datasets in Appendix-F1.

In the two toxic datasets (HA and JBB), the gpt-3.5-turbo-1106 based input filter does not block any of the questions, and our defense strategy aims at lowering the False Negative rate on the toxic questions (JBB dataset and new-HA dataset), while ensuring a low false positive rate on the safe questions as well (E and H datasets). We present the improvements in FNR rates for Algorithm 12 when combined with the gpt-3.5-turbo-1106 input filter and various TRLM variants in Table-6. We further present the impact of varying the threshold in Fig.4 of the Appendix.

**Results:** We firstly note that the proposed TRLM defense strategy improves the FNR of the gpt-3.5-turbo-1106 input filter across all settings considered. Further, the TRLM-Ba pre-trained model improves FNR by more than 70% on the HA dataset and around 35% on the JBB dataset, and outperforms other variants with negligible impact on FPR.

We note that the proposed defense outperforms existing perplexity thresholding based defenses  and Smooth-LLM  on the JailbreakBench attacks  owing to the integration with an input filter that already outperforms them on the same. Hence, we do not compare with them. Further, these defenses operate only in the input space, while the proposed defense aims at augmenting the input space with feedback from the response. Hence, the proposed defense is orthogonal to such methods, and can thus be integrated with them as well.

    &  &  &  \\   & **FNR-HA** & **FNR-JBB** & **FPR (H)** & **FPR (E)** & **FNR-HA** & **FNR-JBB** & **FPR (H)** & **FPR (E)** & **FNR-HA** & **FNR-JBB** & **FPR (H)** & **FPR (E)** \\  TRLM-Fa (PT) & 0.00 & 36.11 & 17.00 & 2.00 & 36.36 & 55.6 & 12.00 & 0.00 & 45.45 & 70.83 & 6.00 & 0.00 \\ TRLM-Ba (PT) & 18.18 & 52.78 & 0.00 & 8.00 & 72.27 & 65.28 & 0.00 & 20.00 & 27.27 & 69.44 & 0.00 & 2.00 \\ TRLM-Fa (PT) & 54.55 & 55.56 & 3.00 & 0.00 & 63.64 & 72.22 & 1.00 & 0.00 & 63.64 & 81.94 & 1.00 & 0.00 \\ TRLM-Ba (PT) & 18.18 & 59.72 & 0.00 & 8.00 & 18.18 & 70.83 & 0.00 & 4.00 & 27.27 & 79.17 & 0.00 & 2.00 \\   

Table 6: Performance of the proposed defense strategies across different thresholds, evaluated on the human annotated and jailbreakbench toxic responses. TRLM-Ba achieves significant gains over all other approaches. Notations: PT [Pretrained], IT[Instruction-finetuned], FNR[False Negative Rate], FPR[False Positive Rate], new-HA [new HA Dataset], JBB[JBB Dataset], (H) [Hard], (E) [Easy]Conclusions

In this work, we explore the capabilities of TRLM for scoring and generation of queries, when conditioned on responses. Our study points to the importance of the response\(\ \ \) direction in LLMs. When deploying TRLM models for reverse scoring, we show improvements on \(\) leaderboard, Citation attribution and retrieval tasks. We further show that generations from TRLM can augment safety filters effectively.

## 7 Limitations

We note that the assumptions made for our theoretical results in Section 4 are stylized, and may not hold true in practice, as the space of all answers to questions may not be adequately captured by assumptions in that section. Given this assumption, one may wish to explore other models for hallucination that are more general and provide results about reverse scoring. We leave such a theoretical exploration to future work.

Further, TRLM benefits have thus far been explored on tasks related to short form queries that have long answers. One may wish to understand and demonstrate the effects of reverse scoring on other tasks. For instance, one might pose the question - does TRLM provide possible benefits for a broader set of tasks that language models are used for. We leave the exploration of such settings in which the reverse scoring direction of response\(\)query is better than the forward scoring direction, along with obtaining an understanding on the reason behind such an advantage, as part of future work.

## 8 Acknowledgements

We are grateful to Kathy Meier-Hellstern and Krishnamurthy Dvijotham for the helpful discussions regarding defending against Jailbreak attacks. We sincerely thank Roman Novak and Abhishek Kumar for their inputs on early versions of our work.