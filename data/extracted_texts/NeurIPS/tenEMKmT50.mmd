# Safe and Sound: Evaluating Language Models for Bias Mitigation and Understanding

Shaina Raza

Vector Institute

Toronto, ON, Canada

&Oluwanifemi Bampshose

Vector Institute, ServiceNow

Toronto, ON, Canada

&Shardul Ghuge

Vector Institute, Amazon

Toronto, ON, Canada

&Deval Pandya

Vector Institute

Toronto, ON, Canada

Corresponding author: shaina.raza@vectorinstitute.ai

###### Abstract

Large Language Models (LLMs) have demonstrated remarkable capabilities in Natural Language Processing (NLP) tasks, but they often generate text that perpetuates societal biases and produces unsafe content. While existing approaches to mitigate these issues have shown some success, they frequently come at the cost of reduced knowledge retention and language understanding. This study investigates a method to produce safe, unbiased outputs from LLMs without compromising their core capabilities. To address this challenge, we trained already-safe LLMs on a specialized dataset containing examples of unsafe content paired with safer alternatives. Our results demonstrate that this approach enhances the model's ability to generate safe content while maintaining its language understanding capabilities. The findings of this study have significant implications for the development of more responsible and ethical AI systems. To promote transparency and facilitate further research in this area, we have made our code and dataset publicly available on GitHub.

## 1 Introduction

LLMs have shown remarkable capabilities in various NLP tasks . However, these models are not without their challenges, particularly in terms of bias and ethical matters. Research [4; 27; 5] has shown that LLMs can perpetuate and even amplify biases and stereotypes related to gender, race, and other demographic factors. Various strategies address bias in LLMs throughout their pipeline. Preprocessing techniques include data augmentation and balanced sampling , and counterfactual data generation . During training, methods like Reinforcement Learning from Human Feedback (RLHF) [2; 37; 38] and adversarial debiasing  can to improve fairness. Post-processing approaches involve guardrails  and prompt engineering [5; 39]. Evaluation strategies such as red teaming  and adversarial demonstrations  also help identify biases.

Despite efforts to mitigate bias in LLMs, challenges persist in balancing bias reduction with maintaining language understanding. Research indicates that excessive bias mitigation can lead to loss of contextual understanding , likely due to overfitting. To address this, we introduce a Safe and Responsible Large Language Model (\(}\)) that is designed to identify and transform biased or harmful content into safe, unbiased versions while preserving knowledge integrity. While recent works [28; 50; 29; 43] have demonstrated that prompting is an effective method for debiasing due to its simplicity, we believe that additional data and training could provide further improvements. Inthis study, we explore how fine-tuning already safe LLMs (e.g., Llama, Mistral) for debiasing can enhance their value. Our contributions include:

* A curated dataset of social media content with unsafe (biased) texts and their safe (debiased) counterparts as ground truth labels.
* Fine-tuning of already safe models on this custom data, based on the intuition that exposure to diverse examples of safe content transformations enhances the model's ability to generate unbiased outputs.
* Consideration of compute-efficient techniques for training and deploying \(_{}\) in production environments.

Our approach acknowledges the ethical implications of content modification while developing a methodology for producing safe LLM outputs. Our long-term goal is to create systems that consistently generate unbiased and ethically sound content, thereby fostering public trust on news media.

## 2 Method

Our approach is shown in Figure 1 and explained next.

Dataset PreparationWe selected 8,500 records from our large-scale data source (_Anonymous link_), based on diverse bias aspects and text length \(>\)100 words, containing original texts with bias labels. GPT-4 Turbo, configured in a 2-shot setting (Appendix A.1), is use to generate debiased versions. Six reviewers from diverse backgrounds verified the LLM-generated texts for bias removal and context retention. Disagreements were resolved through consensus. The annotated dataset schema is ID, Original Text, Bias Indicator, and Debiased Variation (Appendix A.1).

The dataset details is given below in Table 1.

ModelAt the core of our method are instruction-following models, specifically such as Llama 2/3 , Mistral (v0.2, v0.3) . We chose these models for their open-source nature and built-in safety features. To create \(_{}\), we enhanced these base models through instruction fine-tuning

 
**Attribute** & **Value** \\  Datapoints & 20,000 \\  Classes & Bias, Toxicity, Sentiment, Harm. \\  Class Distribution & Bias: No (14,227) / Yes (5,772); Toxicity: No (12,040) / Mild (5,293) / High (2,666); Sentiment: Neg (9,028) / Neu (8,370) / Pos (2,601); Harm: Low (14,151) / Med (3,932) / High (1,915); Unsafe(10,359) /Benign (9,640) \\  Split & Train 13,999 / Dev 1,999 / Test 4,001 \\  

Table 1: Dataset Details.

Figure 1: \(_{}\) architecture

(IFT) using our custom dataset. We adapted our dataset to the Alpaca instruction dataset format for compatibility. The specific prompts and data used for IFT are detailed in Appendix A.2.

We explored various parameter-efficient methods  and primarily utilized Quantized Low-Ranked Adaptation (QLoRA)  with 4-bit quantization to achieve compute efficiency. This approach allowed us to fine-tune LLMs with significantly reduced memory requirements and computational costs, while maintaining performance comparable to full fine-tuning. In this method, we employ QLoRA with 4-bit quantization of the base model, which is combined with low-rank adapters, and enable us to fine-tune LLMs on consumer-grade GPUs. After training, we merged the adapter weights with the base model for stability and deployment. This merging process combines the specialized knowledge captured in the adapters with the original model, and creating a unified model that retains the benefits of fine-tuning while being more efficient for inference.

## 3 Experimental Setup

Our experiments address the following research question: _Can we reduce unsafe content generation while preserving the model's knowledge and language understanding?_

**Baselines:** We utilized instruct-versions of Llama2/3 and Mistral v0.2/v0.3 for prompting and for IFT. Fine-tuning is mainly performed through parameter efficient method like QLoRA. Hyperparameters are given in Table A.3.

**Evaluation Metrics:** Our metrics assess model accuracy, fairness, and output diversity as shown below and detailed in Appendix A.4.

_Safety and Toxicity:_ We use the OpenAI moderation API  (threshold: 0.5), Toxigen-RoBERTa  toxicity scores, and an LLM-based scoring for bias and toxicity based on GPT-4 Turbo .

_Bias and Language Understanding:_ StereoSet  provides metrics such as LMS (where 100 represents full retention), SS (where 50 represents neutrality, with bias indicated by deviations), and ICAT, which combines LMS and SS.

_Knowledge Retention:_ The LLM-based Knowledge Retention metric is used as described in .

_Content Diversity and Style:_ We adapt the CLEN metric from HolisticBias  to detect sentiment and style, measuring sentence length entropy.

_Statistical Validation:_ For statistical significance, we use a T-test for significance  and a One-Sample T-Test  to compare safety classification pre- and post-safety intervention.

**Evaluation Data**: We mainly used our our testset of 6,000 entries, and Toxigen v2 , a refined version of the Toxigen dataset , with 430 examples across various demographics. We also used Stereoset  to evaluates stereotype biases with 8,498 entries across multiple demographics.

**Hardware and Runtime**: The experiments were performed on a single NVIDIA A100 GPU with support from 4 CPU cores. The total memory usage was approximately 100GB, and the total runtime was around 50 minutes for QLoRA method. We used a batch size of 16 for training and 8 for evaluation. Training was constrained to 1 epoch for QLoRA (with trials up to 5 epochs; more epochs led to overfitting, as noted in the Llama2 paper ) and 5 epochs for prefix-tuning. For QLoRA, we set the LoRA rank (\(r\)) to 64, \(\) to 16, used a dropout rate of 0.2, and applied 4-bit NF4 quantization with nested quantization enabled.

### Results

This section presents our results.

### Evaluating Bias, Toxicity and Knowledge Retention

We test different LLMs (Llama 2/3 and Mistral v0.2/0.3) on \(_{}\) in prompt settings and IFT for bias and toxicity reduction, as well as content moderation; we also evaluate knowledge retention after debiasing the texts through our approach.

The results in Table 2 demonstrate that our debiasing approach effectively reduces bias and toxicity while preserving language understanding. The IFT method outperformed prompting techniques, with few-shot prompting (2 demonstrations) proving more effective than only zero-shot. This indicates that providing examples with instruction guides the model towards safer, less biased outputs.

In the next set of experiments, we primarily focused on \(_{}\) using the best-performing Llama-3.1-8B-Instruct\({}_{IFT}\) model, which demonstrated superior performance in our previous experiments.

### Language Understanding Evaluation

We assessed \(_{}\) on StereoSet data  to evaluate bias across four demographics. This experiment (Table 3) tested our method and confirmed its ability to reduce biases while maintaining language understanding.

We observed in the results in Table 3 that our approach using \(_{}\) effectively reduces stereotypical biases while preserving language modeling capabilities, as indicated by high ICAT scores (close to 100) that demonstrate language understanding. We are able to maintain SS scores (close to 50), which indicate a neutral stance, while deviations from 50 suggest a bias toward stereotype or anti-stereotype terms. Additionally, high ICAT scores (above 50) reflect language competence and bias neutrality

### Evaluation on Text Style Features

We evaluated \(_{}\) using the ParAI style classifier [32; 44] to analyze text styles before and after safety interventions. We conducted a one-sample t-test  on our training dataset of 16,602 samples and utilized CLEN scores from HolisticBias  to assess the styles before and after debiasing. This approach allowed us to quantitatively measure the effectiveness of our debiasing techniques and their impact on the overall style and content of the text.

 
**Models** &  &  &  &  \\   & **Our** & **Toxigen** & **Our** & **Toxigen** & **Our** & **Toxigen** & **Our** & **Toxigen** \\  Original text & 43.18 & 49.78 & 32.21 & 38.34 & 40.09 & 48.39 & - & - \\ Debiased text & 23.05 & 24.53 & 23.83 & 28.92 & 20.29 & 21.32 & 79.35 & 77.91 \\  Llama-2-7b-chat\({}_{P}\) & 22.81 & 24.28 & 23.58 & 28.60 & 20.28 & 21.10 & 79.35 & 77.91 \\ Llama-2-7b-chat\({}_{PS}\) & 21.92 & 23.24 & 22.42 & 27.39 & 19.20 & 20.00 & 83.66 & 81.49 \\ Llama-2-7b-chat\({}_{FT}\) & 14.57 & 17.13 & 17.32 & 19.12 & 12.72 & 13.26 & 85.67 & 82.69 \\  Llama-3-8B-Instruct  & 12.98 & 15.69 & 15.70 & 18.31 & 11.22 & 12.21 & 84.09 & 83.31 \\ Llama-3-8B-Instruct\({}_{PS}\) & 12.20 & 14.91 & 14.91 & 17.39 & 10.65 & 12.11 & 86.70 & 84.34 \\ Llama-3-8B-Instruct\({}_{IFT}\) & 06.64 & 07.68 & 06.55 & 07.61 & 09.16 & 06.27 & 89.88 & 82.00 \\  Llama-3.1-8B-Instruct & 12.49 & 16.17 & 16.04 & 17.83 & 10.74 & 12.60 & 84.63 & 82.44 \\ Llama-3.1-8B-Instruct\({}_{PS}\) & 12.45 & 15.39 & 14.39 & 17.66 & 10.99 & 11.53 & 85.86 & 83.09 \\ Llama-3.1-8B-Instruct\({}_{IFT}\) & **06.03** & **07.10** & **05.25** & **05.93** & **08.03** & **06.32** & **89.98** & **88.45** \\  Mistral-7B-Instruct-v0.2\({}_{P}\) & 12.33 & 15.39 & 14.39 & 17.66 & 10.99 & 11.53 & 84.86 & 83.09 \\ Mistral-7B-Instruct-v0.2\({}_{PS}\) & 11.96 & 14.93 & 13.96 & 17.13 & 10.66 & 11.19 & 86.43 & 84.58 \\ Mistral-7B-Instruct-v0.2\({}_{IFT}\) & 11.60 & 14.48 & 13.54 & 16.62 & 10.34 & 10.85 & 87.16 & 87.12 \\  Mistral-7B-Instruct-v0.3\({}_{P}\) & 11.25 & 14.04 & 13.13 & 16.12 & 10.03 & 10.52 & 82.92 & 82.70 \\ Mistral-7B-Instruct-v0.3\({}_{PS}\) & 10.91 & 13.62 & 12.73 & 15.63 & 09.72 & 10.20 & 83.72 & 84.33 \\ Mistral-7B-Instruct-v0.3\({}_{IFT}\) & 10.26 & 12.81 & 11.98 & 14.70 & 09.12 & 09.59 & 88.42 & 87.82 \\  

Table 2: \(_{}\) evaluation on our test set and Toxigen v2 . LLM based metrics: Content Moderation (Mod.) , Bias, Toxicity, and Knowledge Retention . Abbreviations: FT (fine-tuning), P (zero-shot prompt), PS (2-shot prompt), IFT (Instruction fine-tuning). Best scores in **bold**.

   Demographic & LMS \(\) & SS (\(\)) & ICAT \(\) \\  Gender & 92.82 & **53.90** & 80.88 \\ Profession & 90.58 & 54.78 & 88.89 \\ Race & **96.59** & 57.92 & **90.28** \\ Religion & 93.68 & 58.93 & 88.84 \\   

Table 3: Performance of \(_{}\) on StereoSet Intrasentence test, utilizing metrics Stereotype Score (SS) (**Closer to 50 is better**), Language Modeling Score (LMS), and Idealized CAT Score (ICAT) (**Higher \(\) the better**, closer to 100).

Results in Figure2 showed higher consistency in positive styles post-debiasing, indicating effective bias reduction while maintaining content diversity. The analysis revealed a statistically significant change in linguistic style post-intervention (p < 0.00001, t-statistic = 28.17), which reflect much shift towards safer and more inclusive traits.

### Evaluation on Demographics Post Safety Intervention

We evaluated \(_{}\) for mitigating toxicity across various demographics using the Toxigen test set. Toxicity levels were assessed with ToxiGen-RoBERTa , a model trained for this dataset. The toxicity scores, presented as average probabilities in percentage form in Table 4, showed a reduction in toxicity by up to 97% with our debiasing approach.

Qualitative AnalysisWe conducted a human evaluation to assess \(_{}\) ability to reduce biases while maintaining language understanding. Five evaluators performed a blind assessment of 100 examples generated by four model variants of \(_{}\) : Safe_PEFT-1_epoch (default \(_{}\)), Safe_PEFT-5_epoch, Safe_Dense-fine-tuning, Safe_prefix-tuning (prefix tuning offers a lightweight alternative to full fine-tuning, allowing for efficient adaptation ). The evaluation criteria focused on **safety** (freedom from bias, toxicity, and prejudice) and **language understanding** (maintaining original content integrity) on a Likert scale (1-5).

Table 5 summarizes the key findings from the human evaluation, with detailed examples provided in Table A.2. Our evaluation demonstrates that the current model setup with IFT for one epoch (Safe_PEFT-1_ep) achieves the best balance between bias reduction and language understanding while being computationally efficient. This finding supports our approach to developing a safer language model without compromising its understanding capabilities.

  
**Demographic** & **Women** & **Ment. Disability** & **LGBTQ** & **Black** & **Chinese** & **Asian** & **Nat. Amer.** \\ 
**Orig. Toxicity** & 92.6 & 90.5 & 86.6 & 90.5 & 86.5 & 99.2 & 98.3 \\
**SR\({}_{}\)** & 3.2 & 1.2 & 1.9 & 1.0 & 1.0 & 1.4 & 1.9 \\ 
**Demographic** & **Mid. East.** & **Muslim** & **Phys. Disability** & **Mexican** & **Jewish** & **Latino** & \\ 
**Orig. Toxicity** & 91.5 & 94.5 & 82.8 & 87.5 & 82.0 & 84.8 & \\
**SR\({}_{}\)** & 1.9 & 1.9 & 1.1 & 1.2 & 2.8 & 2.2 & \\   

Table 4: Reducing Toxicity for Demographic Groups on the Toxigen Test Set

Figure 2: One-Sample t-Test results.

## 4 Related Works

Ensuring safety in LLMs is a critical concern, and various strategies have been developed to address biases and produce safe outputs.

_Debiasing Methods:_ Debiasing techniques often modify embedding spaces or involve post-processing with minimal fine-tuning [25; 47]. Subtraction-based methods neutralize embedding spaces by equalizing distances between non-gendered and gendered word pairs . Data augmentation techniques replace gendered terms in training data with their opposites . Fine-tuning approaches reduce bias by adjusting a small portion of model parameters . Recent research highlights that LLMs can unintentionally perpetuate stereotypes related to gender, race, and other demographics [8; 13; 19]. Strategies like Red-teaming evaluate robustness against biases , and RLHF and context distillation further refine model outputs [2; 37].

_Prompt-Based Approaches:_ Prompt-tuning and self-diagnosis techniques reduce biases during generation [34; 41]. Methods such as Co2PT mitigate biases during prompt tuning , while causal prompting leverages causal inference to address bias . Though effective, prompt-based debiasing can vary across models and bias types, necessitating ongoing research .

_Evaluation and Datasets:_ Appropriate datasets and metrics are crucial for assessing debiasing methods. Commonly used datasets include RedditBias , WinoBias , and HolisticBias , with metrics like WEAT  and StereoSet  employed for evaluation.

_LLM-Based Annotations:_ LLMs like GPT-4 can act as annotators, performing on par with or better than humans in tasks such as tweet annotation and medical information extraction [14; 57]. These models are integrated into active learning loops for labeling large datasets . _LLM-Based Scoring:_ LLMs are also being used as evaluators to ensure alignment with human preferences on accuracy, toxicity, and relevance [54; 55]. Instruction-tuned models show great potential, but debates on their safety and competitiveness persist .

As research on LLMs advances, it is critical to explore how LLMs can handle biased instructions while maintaining context and knowledge retention, which is our study's primary focus.

## 5 Discussion

### Social Impact

Our research on bias mitigation in LLMs has significant societal implications, particularly for the development and deployment of AI systems that interact with diverse populations. This work aims to promote fairness and inclusivity in NLP applications, with potential positive impacts including improved fairness, reduced discrimination, enhanced transparency around biases, and advancement of techniques for creating representative datasets . The proposed approach can increase awareness of ethical considerations in AI development. However, we acknowledge potential negative impacts, such as overreliance on automated debiasing techniques, unintended introduction of new biases, privacy concerns related to demographic data collection, and potential misuse of fine-tuning techniques .

To mitigate these risks, we open-source our dataset and methodology, ensuring transparency and enabling further enhancements for studying ethics in AI . We recognize the potential for data

 
**Model** & **Safety** & **Language** & **Key Insights** \\  & **Score** & **Score** & \\  Safe\_PEFT-1\_ep & 5/5 & 4.99/5 & Effectively removed biases, promoted \\  & & & & inclusivity \\  Safe\_PEFT-5\_ep & 4/5 & 3.5/5 & Introduced multilingual elements, \\  & & & potentially limiting inclusivity \\  Safe\_Dense & 3.4/5 & 3.8/5 & Addressed diversity but left responses \\ finetuning & & & incomplete \\  Safe\_prefix tuning & 4.5/5 & 4.8/5 & Broadened demographic narrative, \\  & & & unnecessary apology \\  

Table 5: Human evaluation results for \(}}\) variants. PEFT is for parameter efficient fine-tuning.

misuse by malicious actors exploiting unintended model functionalities. Model developers must implement appropriate safeguards to mitigate these risks.

This research aligns with the broader goal of creating trustworthy AI systems that respect human rights, protect user privacy, and promote social good. With our focus on bias mitigation in LLMs, we aim to reduce the perpetuation of harmful stereotypes and discrimination that can arise from biased training data. We encourage further research to build upon our work, emphasizing the importance of diverse perspectives and stakeholder engagement in the development of ethical AI solutions. As LLMs continue to shape various aspects of society, from healthcare to finance and beyond, our findings contribute to the important task of ensuring these powerful tools are deployed responsibly and equitably.

#### Limitations

Just like any studies, this work has some limitations too.

_Subjective nature of bias:_ While we examined multiple aspects of bias, we acknowledge that bias is inherently subjective and context-dependent. Our use of GPT-4 for labeling, while experimentally driven, may be cost-prohibitive for some researchers.

_Evaluation constraints:_ We lacked access to log-probabilities for chat and instruct models, limiting their evaluation to generative responses . LLM-based scoring methods is based on confidence scores provided by these models that can produce non-deterministic results and may exhibit inherent biases, such as position bias. This non-determinism arises from factors like sampling techniques, model initialization variations, and hardware differences, leading to potential inconsistencies across evaluations. Additionally, while we demonstrated performance for language generation task, areas such as question answering, machine ethics, and translation still need to be addressed.

_Limited prompting methods:_ Our evaluations focused on straightforward prompts with some demonstration, which might have impacted the full functionality of debiasing and jailbreaking the systems. This approach, while providing valuable insights, may not fully capture the complex interactions between prompts and LLM responses. Future work could explore adversarial or misleading prompts to further test and enhance model detection capabilities. Despite careful review, the potential for human errors and residual subjectivity in our gold standard dataset remains. This limitation highlights the inherent challenges in creating more reliable unbiased benchmarks for evaluating LLMs.

_Methodological limitations:_ Our research faces significant challenges related to computational requirements and accessibility, which may impact its reproducibility and widespread adoption. High computational requirements and specialized knowledge for model deployment and optimization present barriers to widespread adoption, particularly for smaller research groups.

Evaluation that relies on platforms such as Content Moderation and DeepEval is not accessible to all, thereby limiting reproducibility of the research to some extent. Also, our annotated dataset spans various media types but may not be fully representative of global demographics or media coverage across all regions. This could limit the generalizability of identified demographic techniques. To mitigate these limitations and promote transparency, we have shared all code and data used in our research, provided comprehensive documentation of our experimental setup and procedures, and encouraged researchers to adapt our methods to their available resources and platforms.

_AI safety and unforeseen challenges:_ The rapid advancement of LLMs introduces new complexities that may be difficult to fully anticipate and address. Our efforts to cover a broad range of safety risks and bias aspects may not encompass all potential issues.

## 6 Conclusion

In this study, we introduced \(_{}\) for safe language generations, this approach is trained on our custom dataset of instructions featuring original texts (potentially unsafe) and their benign variations to ensure safe language generation. This model offers reduced inference and deployment costs. It has proven competitive on three test sets. We have detailed the methods and techniques to develop our models, emphasizing their adherence to safety and language understanding principles. Committed to transparency and safety, we plan to enhance the model and data in future work. Despite achieving good results, further safety evaluation using jailbreaking techniques remains necessary. The modelscan also be tested on other downstream tasks like retaining summarization, translation capabilities.Its worthwhile to explore other parameter-efficient methods.