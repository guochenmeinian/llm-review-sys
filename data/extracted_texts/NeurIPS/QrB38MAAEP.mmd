# Towards a Unified Framework of Contrastive Learning for Disentangled Representations

Stefan Matthes, Zhiwei Han, Hao Shen

fortiss GmbH, Munich, Germany

{matthes,han,shen}@fortiss.org

###### Abstract

Contrastive learning has recently emerged as a promising approach for learning data representations that discover and disentangle the explanatory factors of the data. Previous analyses of such approaches have largely focused on individual contrastive losses, such as noise-contrastive estimation (NCE) and InfoNCE, and rely on specific assumptions about the data generating process. This paper extends the theoretical guarantees for disentanglement to a broader family of contrastive methods, while also relaxing the assumptions about the data distribution. Specifically, we prove identifiability of the true latents for four contrastive losses studied in this paper, without imposing common independence assumptions. The theoretical findings are validated on several benchmark datasets. Finally, practical limitations of these methods are also investigated.

## 1 Introduction

Learning to disentangle the explanatory factors of the observed data is valuable for a variety of machine learning (ML) applications, as it allows for a more compact, interpretable, and manipulable data representation. Consequently, this can improve sampling efficiency  and predictive performance [35; 11; 13] for downstream tasks, and promote fairness [33; 8]. Among the many efforts to develop a theoretically grounded approach to learning such representations, contrastive learning (CL) has emerged as a particularly promising technique.

Intuitively, contrastive methods learn a function that maps observations into a representation space, such that related examples (e.g., augmentations of the same sample) are mapped close to each other and negative pairs are mapped far apart [6; 48]. In recent years, this intuition has been refined and extended by different theories. One of these theories is that contrastive methods (approximately) invert the data generating process [49; 24] and thus recover the generative factors. We build upon this line of work and extend their theoretical findings to a wider range of contrastive objectives.

In this study, we aim to develop a unified framework of statistical priors on the data generating process to improve our understanding of CL for disentangled representations. Two important issues we will discuss are how to deal with nonuniform marginal distributions of the latent factors and situations when these factors are conditionally dependent to some degree. We demonstrate how this could be accomplished with a simple modification of common contrastive objectives. We also show that without these modifications, contrastive objectives make implicit assumptions about the data generating process and investigate when these assumptions are justified. We empirically verify our theoretical claims and show practical limitations of the proposed framework.

In short, the contributions of our study can be summarized as follows:

* We extend and unify theoretical guarantees of disentanglement for a family of contrastive losses under relaxed assumptions about the data generating process.

* The theoretical findings are empirically validated on several benchmark datasets and we quantitatively compare the disentanglement performance of four contrastive losses.
* We analyze the impact of partially violated assumptions and investigate practical limitations of the proposed framework.

## 2 Related Work

**Disentangled Representation Learning** The concept of learning representations whose components correspond to the explanatory factors of the data can be traced back to the principles of blind source separation  and discovering factorial codes . In contrast to earlier work, however, the research focus has gradually shifted to applications where the relationship between data and underlying factors is nonlinear.

The difficulty of this task is that for independent and identically distributed (i.i.d.) data that depend nonlinearly on the latent factors, it is fundamentally impossible to identify these factors without labels or assumptions about the data generating process [23; 34; 26].

A few studies showed how the underlying generative factors can be recovered if they are mutually independent (with at most one of them being Gaussian) and their relation to the observed data exhibits certain regularities, such as a linear mixture followed by an element-wise nonlinearity , and local isometries . Gresele et al.  also showed that conformal maps rule out some spurious solutions.

Most approaches, however, depart from the i.i.d. assumption and utilize co-observed dependent variables, such as a time index  or prior observations  in a time-series, observations obtained from interventions , data augmentation  or different views . These approaches mainly differ in how they model the statistical and causal dependencies between the latent factors and their relation to the auxiliary variable.

Hyvarinen et al.  modeled the distribution of the latent factors with the exponential family, where the key assumptions are that, for a given auxiliary variable, the latent factors are conditionally independent and that the effect of the auxiliary variable on their distributions is sufficiently diverse. Their theory was later extended to the Variational Autoencoder (VAE ) framework  and to energy-based models  with further relaxed independence assumptions. Other conditional distributions that have been considered are the Laplace distribution  and the von Mises-Fisher distribution . In this work, we extend these results to distance-based conditional distributions and arbitrary marginals of the latents.

Another research direction considers pairs of observations whose underlying latents differ only in some of the factors and share the others, for example, through interventions or actions. In [41; 32] the shared factors or intervention targets are assumed to be known. Locatello et al.  showed how the ground-truth latents can be recovered when the pairs of observations share at least one latent factor, which was extended to multi-dimensional factors in , but their approaches additionally require that the latent factors are mutually independent. In contrast, [30; 1; 4] proved identifiability for more general causal dependencies between the latent factors by exploiting sparse transitions.

**Contrastive Learning** In recent years, contrastive methods have shown a remarkable ability to learn useful representations for downstream tasks [37; 39; 6; 18; 17]. A number of works attribute this success primarily to their tendency to maximize mutual information (MI) between latent representations [39; 19; 2; 43]. However, it has been observed that contrastive objectives with tighter MI bounds do not necessarily enhance downstream performance [45; 44].

More recently, by exploiting the alignment and uniformity properties  of InfoNCE , Zimmermann et al.  showed that InfoNCE approximately inverts the data generating process, leading to the identification of the true latent factors. We extend this theory to a broader family of contrastive methods [39; 16; 17; 38].

## 3 Contrastive Learning for Disentangled Representations

In this section, we present our theoretical framework of contrastive methods for learning disentangled representations (Figure 1). We begin with the formalization of our assumptions on the underlying data generating process.

### Data Generating Process

In this work, we adopt the common assumption that the observations are generated by a smooth and invertible generator \(g:\), where \(^{n}\) denotes the space of latent factors and \(^{m}\) the space of observations. Here, we assume \(n m\). The goal is to find a function \(f:^{n}\) that inverts \(g\) up to element-wise transformations or other simple mappings such as linear functions.

We do not impose any constraints on the ground-truth marginal distribution of the latents, \(p()\), except that the support must be connected. Let \( p()\) and \((,})\) be a latent positive pair. We model the dependency between \(\) and \(}\) with the following conditional distribution

\[p(}|)=})}{Z()} \,e^{-d(,})} Z()=_{}Q(})\,e^{-d( ,})}\,},\] (1)

where \(Q\) and \(Z\) are scalar-valued functions and \(d\) is a distance function, which we will specify below. It is worth noticing that here the latent factors \(s_{i}\) are not assumed to be statistically independent or conditionally independent given auxiliary observations (\(Q\) cannot necessarily be factorized).

Two related observations (e.g., consecutive frames in a sequence, multiple views or augmentations of the same data point) are then generated by \(=g()\) and \(}=g(})\). For the sake of simplicity, we assume that the related instances are of the same modality, though it is straightforward to extend this framework to different modalities, such as image and audio coming from different generators.

### Contrastive Learning Approaches

Contrastive methods generally have in common that they employ certain mechanisms to measure similarity between data points. The most frequent choice is the cosine similarity , while \(_{p}\) distances [7; 49] and even learnable functions [22; 24] have also been used. Inspired by the work in [22; 24], which relies on specific models of learnable similarity measures, we propose the following generic dissimilarity measure

\[(,})=(,}) +()+(}),\] (2)

where \(\) and \(\) are learnable scalar-valued functions, e.g., neural networks, and \(\) is a fixed expression that describes the interaction between related examples. In the ideal case, \(\) matches \(d\) in Eq. (1).

Let us now recall some common contrastive methods. The first approach we investigate is noise-contrastive estimation (NCE) , which has been used for disentanglement in [22; 24]. Using our dissimilarity measure, the loss can be formulated as follows

\[_{}(f,)=,}) p_{}}{}-[(-(f( ),f(})))]-,  p}{}[1-(-(f(),f( ^{})))],\] (3)

where sig is the sigmoid function, \(p_{}\) is the distribution of positive pairs, and \(p\) is the distribution of all observations. 1

Figure 1: Overview of our CL framework for disentanglement. The unknown generator \(g\) maps latent pairs \((,}) p(,})\) to observation pairs \((,})\), which are encoded by \(f_{}\). After computing the dissimilarity measure according to Eq. (2), the representation is optimized with one of the contrastive losses.

[MISSING_PAGE_EMPTY:4]

The central idea of the proof, which can be found in the supplementary material, is to show that the learned representation preserves the distance from the latent space, i.e., \(d(,})=d(h(),h(}))\). Further, by constraining the dimension and connectivity of the latent space, such a mapping must be affine. As a byproduct of the derivation, we obtain for the global optimum that

\[ h= p- Q\] (7)

and, except for \(_{}(f,;K)\),

\[ h= Z.\] (8)

We verify this with a simple example in Figure 2. This means that when we optimise the InfoNCE objective with \(\) set to zero, we implicitly assume that \(Q=p\). This is true, for example, when the marginal distribution is uniform and \(Q\) is constant. Thus, our theorem is consistent with the perspective of uniformity and alignment  and can be seen as a generalization of Theorem 5 from . In particular, we extend their results to other contrastive losses, nonconvex latent spaces, nonuniform marginal distributions, and even to the case where the latent factors are not conditionally independent.

Next we show strong identifiability when \(d\) in the conditional distribution of the latent factors is further restricted. The proof can be found in the supplementary material.

**Theorem 2** (Strong identifiability).: _Assume that all conditions in Theorem 1 are satisfied. Let the function \(d\) from Eq. (1) be defined by_

\[d(,})=_{i}(|s_{i}-_{i}|/_{i})^ {},\] (9)

_with \((0,2)(2,)\) and \(_{i}>0\) for all \(i\), then \(h=f g\) is a generalized permutation matrix, i.e., a composition of a permutation and element-wise scaling and sign flips._

This result is similar to earlier theorems [29; 49], but relaxes some conditions. In particular, it also holds for nonuniform marginals and \((0,1)\). For the case \(=2\), the conditional distribution is quasi-Gaussian  and, as with linear Independent Component Analysis (ICA) for normally distributed sources, strong identifiability is then provably impossible.

In contrast to , we show identifiability for the generalized normal distribution (excluding \(=2\)) and do not assume some regularity conditions, e.g., that the encoder is twice differentiable and invertible. However, we do not claim to generalize their results, since the exponential family contains distributions that we do not consider here.

Theorem 2 also has an interesting connection to some recently proposed disentanglement methods that rely on sparsity [30; 1]. This situation is similar to the case when \( 0\) in Eq. (9) and the conditional \(p(}|)\) becomes sparse. However, this is a topic for future work.

## 4 Experiments

In this section, we conduct a comprehensive quantitative evaluation of our theoretical framework. We largely adopt the experimental setup and evaluation protocol of  with one major difference. During training, we additionally optimize for \(\) and \(\) in Eq. (2), both of which are parameterized by three-layer neural networks, respectively. In practice, both functions are normalized to a mean of zero and are jointly trained with a learnable offset \(c\), i.e., \((,})=(,}) +()+(})+c\). In the case of \(\)-INCE, we use \((,})=(,} )+(})\) instead. Further details on the implementation can be found in the supplementary material.

As in previous research [21; 22; 49], we test for weak identifiability, i.e., up to affine transformations, by fitting a linear regression model between the ground-truth and recovered latents and compute the coefficient of determination (\(R^{2}\)). For strong identifiability, i.e., up to generalized permutations and element-wise transformations, we report the mean correlation coefficient (MCC).

### Validation of Theoretical Findings

To validate our theoretical claims, we adopt a generative process similar to . However, to demonstrate that identifiability can be achieved for nonconvex latent spaces, nonuniform marginals,and without conditionally independent latents, we investigate additional configurations of these components.

In this section, we consider latent spaces with \(n=10\) dimensions. We generate pairs of source signals \((,})\) by first sampling from a marginal distribution \(p()\) and subsequently from a conditional distribution

\[p(}|)=})}{Z()}\,e ^{-_{i}(-z_{i}|}{})^{}}.\] (10)

The observations \((,})\) are then generated by passing \(\) and \(}\) individually through an (approximately) invertible multilayer perceptron (MLP).

We analyze the following four scenarios where we vary \(\), \(p()\), \(Q\) and \(\):

* Box (simple): \(=^{n}\), \(p()\) is uniform, \(Q\) is constant. This is our baseline.
* Box (complex): \(=^{n}\), \(p()\) is a normal distribution with block-diagonal correlation matrix such that the odd dimensions are correlated with the even dimensions, \(Q\) has a checkerboard pattern (i.e., has value 1 on "white squares" and 0.1 on "black squares"), so the latent factors are not conditionally independent.
* Hollow ball: \(=\{^{n} r<\|\|<R,0<r<R\}\) with inner radius \(r\) and outer radius \(R\), \(p()\) is uniform over the radius and angle (but not uniform in Euclidean coordinates), \(Q\) is constant. Note that here \(_{i}\) and \(_{j}\) are not conditionally independent given \(\), because the support is not rectangular.
* Cube grid: \(=\{[-1,1]^{n} i:s_{i}<-b s_{i}>b,0<b<1\}\) (i.e., \(\) is disconnected), \(p()\) is uniform, \(Q\) is constant. Although our current analysis makes no predictions about disconnected spaces, we investigate this configuration to empirically probe potential limits of our framework.

To fit our hardware setup, we use a smaller batch size of 5120. Also, we use an encoder network with residual connections and batch normalization, which has been shown to be more stable. We generally leave the output space unrestricted, i.e., \(=^{n}\).

Table 1 provides a summary of the MCC scores. The corresponding \(R^{2}\) values can be found in the supplementary material. In each scenario, all contrastive losses achieve near-optimal disentanglement, with the exception of a few outliers. In particular, \(\)-SCL appears to be numerically less stable than its counterparts, which we will examine in more detail in later sections. Additional experiments in the supplementary material compare \(\)-SCL with the original SCL and show that \(\)-SCL generally yields higher disentanglement scores. However, we would like to point out that one should not draw direct conclusions from these results about the performance of the original SCL on other downstream tasks.

To verify that the learned functions \(\) and \(\) converge to the solution given in Eq. (7) and Eq. (8), we construct a simple example where the ground truth is known. We choose \(==^{2}\) with a uniform marginal distribution. The positive examples are sampled from a truncated Laplace

   Scenario & \(\) & \(\)-NCE & \(\)-INCE & \(\)-SCL & \(\)-NWJ \\  Box (simple) & 1/2 & 99.68 \(\) 0.04 & 99.56 \(\) 0.18 & 87.08 \(\) 1.40 & 99.80 \(\) 0.02 \\ Box (simple) & 1 & 99.91 \(\) 0.01 & 99.90 \(\) 0.01 & 94.33 \(\) 2.02 & 99.87 \(\) 0.01 \\ Box (simple) & 3 & 99.66 \(\) 0.20 & 96.97 \(\) 2.47 & 98.29 \(\) 0.14 & 99.71 \(\) 0.05 \\ Box (simple) & 5 & 99.79 \(\) 0.02 & 96.56 \(\) 2.44 & 98.63 \(\) 0.36 & 99.74 \(\) 0.00 \\  Box (complex) & 1 & 99.13 \(\) 0.41 & 99.87 \(\) 0.05 & 83.73 \(\) 2.49 & 93.14 \(\) 4.97 \\ Box (complex) & 3 & 99.90 \(\) 0.00 & 99.84 \(\) 0.00 & 93.64 \(\) 0.22 & 99.84 \(\) 0.00 \\  Hollow ball & 1 & 99.73 \(\) 0.11 & 99.70 \(\) 0.08 & 90.27 \(\) 6.48 & 99.73 \(\) 0.08 \\ Hollow ball & 3 & 95.82 \(\) 4.65 & 98.05 \(\) 0.08 & 97.65 \(\) 0.00 & 99.03 \(\) 0.06 \\ Hollow ball & 5 & 98.52 \(\) 0.21 & 96.25 \(\) 0.43 & 97.57 \(\) 0.06 & 98.67 \(\) 0.16 \\  Cube grid & 1 & 99.87 \(\) 0.02 & 99.66 \(\) 0.04 & 96.92 \(\) 1.88 & 99.79 \(\) 0.01 \\ Cube grid & 5 & 97.42 \(\) 0.12 & 82.26 \(\) 7.06 & 96.17 \(\) 0.07 & 97.96 \(\) 0.06 \\   

Table 1: Identifiability on synthetic data. MCC [%] mean \(\) standard deviation over 2 random seedsdistribution. Figure 2 shows the results. Details and additional experimental setups can be found in the supplementary material.

Since we learn the sum of \(\) and \(\), they can only be identified up to some offset. In Figure 2, we have added the learned bias to \(\) to confirm that the total offset is correct. All losses reach the global optimum except for \(\)-SCL, which appears to be stuck at a local minimum.

### Evaluation under Partially Violated Assumptions

We also report results for the case where \(\) and \(\) are set to a learnable constant (see Table 2). Interestingly, we do not observe that this change significantly affects the outcome. However, we do observe a drastic performance drop in \(\)-NCE, \(\)-SCL and \(\)-NWJ when this offset is fixed to a value which is far from the ground truth. Specifically, setting the bias to zero causes a drop in the MCC score by more than 20% on average when \(=1\). For larger \(\) we do not observe different behaviour. This is likely due to the fact that the optimal bias is far from 0 when \(\) is smaller, which can distort the learned representation.

### Evaluation on Kitti Masks

The KITTI Masks dataset  consists of segmentation masks of pedestrians extracted from the autonomous driving benchmark KITTI-MOTS . We compare the presented loss functions with

   Scenario & \(\) & \(\)-NCE & \(\)-INCE & \(\)-SCL & \(\)-NWJ \\  Box (simple) & 1/2 & 99.57 & 99.81 & 82.69 & 89.22 \\ Box (simple) & 1 & 99.85 & 99.91 & 94.00 & 99.81 \\ Box (simple) & 3 & 99.77 & 99.40 & 99.78 & 99.76 \\ Box (simple) & 5 & 99.82 & 98.86 & 94.84 & 99.82 \\  Box (complex) & 1 & 99.70 & 99.82 & 76.63 & 99.68 \\ Box (complex) & 3 & 99.69 & 99.75 & 89.78 & 99.51 \\  Hollow ball & 1 & 98.49 & 99.37 & 87.31 & 97.94 \\ Hollow ball & 3 & 98.59 & 97.23 & 96.39 & 98.64 \\ Hollow ball & 5 & 98.52 & 96.26 & 98.49 & 98.52 \\  Cube grid & 1 & 99.82 & 99.39 & 98.21 & 99.75 \\ Cube grid & 5 & 96.53 & 84.73 & 97.50 & 97.42 \\   

Table 2: MCC [%] scores on synthetic data for \(()=(})=c\)

Figure 2: Learned functions \(\) (top row) and \(\) (bottom row) for a simple example described in the text. The ground truth is in the right column. In the case of \(\)-INCE, \(\) is set to zero.

the current state of the art for this dataset [29; 49]. Except for the additional parameters in \(\), we use the same settings and follow their evaluation protocol.

As noted by Klindt et al. , the transitions of the latents are mostly sparse. It is therefore not surprising that all contrastive losses consistently perform better when a Laplace conditional is assumed as opposed to a normal conditional. Considering the large variance across different random seeds, all contrastive losses perform similarly well, with the largest outlier being \(\)-SCL, which could already be observed on the synthetic dataset in the previous section.

Overall, there are only small differences between the different embedding spaces, with about half of the methods performing better on each setup. Consistent with previous observations , we see for all models slightly better performance for larger time intervals, \(\), between consecutive frames. One possible reason is that a larger concentration of the conditional leads to a larger range of \(\) values, causing numerical instability. We investigate this phenomenon in more detail in Section 4.5.

### Evaluation on 3DIdent

We additionally evaluate our framework under partially violated assumptions on 3DIdent , a recently proposed benchmark dataset. The dataset is composed of rendered images showing a complex object in different positions, rotations and under different lighting conditions. Positive pairs \((,})\) are obtained by randomly selecting \(\) and matching sampled \(}^{}\) from the conditional \(p(}^{}|)\) to the closest \(}\) with a rendered image. We consider a normal (\(=2\)) and a Laplace conditional (\(=1\)). In both cases we set \((,})=\|-}\|_{2}^ {2}\) where we either leave the output space unbounded (\(=^{n}\)) or restrict it to an adjustable box (\(=[0,b]^{n}\), where \(b\) is a learnable parameter). The latter

    & & \)} &  \\  \(\) & Loss & Unbounded & Box & Unbounded & Box \\ 
2 & \(\)-contr  & 96.43 \(\) 0.03 & 96.73 \(\) 0.10 & 54.94 \(\) 0.02 & 98.31 \(\) 0.04 \\
2 & \(\)-INCE & 97.87 \(\) 0.07 & 98.08 \(\) 0.03 & 56.09 \(\) 0.10 & 98.98 \(\) 0.09 \\
2 & \(\)-NCE & 98.05 \(\) 0.04 & 97.91 \(\) 0.02 & 55.34 \(\) 0.11 & 98.95 \(\) 0.08 \\
2 & \(\)-SCL & 71.07 \(\) 3.76 & 74.92 \(\) 2.28 & 49.88 \(\) 1.20 & 76.02 \(\) 3.13 \\
2 & \(\)-NWJ & 94.03 \(\) 1.35 & 97.67 \(\) 0.02 & 53.82 \(\) 0.46 & 98.79 \(\) 0.02 \\ 
1 & \(\)-contr  & & 96.87 \(\) 0.08 & & 98.38 \(\) 0.03 \\
1 & \(\)-INCE & 98.03 \(\) 0.06 & 97.45 \(\) 0.04 & 55.79 \(\) 0.09 & 98.63 \(\) 0.05 \\
1 & \(\)-NCE & 97.83 \(\) 0.04 & 97.33 \(\) 0.03 & 56.35 \(\) 0.08 & 98.71 \(\) 0.02 \\
1 & \(\)-SCL & 78.59 \(\) 3.92 & 93.13 \(\) 1.83 & 52.28 \(\) 1.98 & 95.77 \(\) 2.58 \\
1 & \(\)-NWJ & 94.02 \(\) 1.28 & 97.64 \(\) 0.09 & 54.87 \(\) 0.33 & 98.82 \(\) 0.03 \\   

Table 4: 3DIdent. \(R^{2}\) [%] and MCC [%] mean \(\) standard deviation over 2 random seeds

    & &  &  \\  \(\) & Loss & Unbounded & Box & Unbounded & Box \\ 
0.05s & SlowVAE  & 66.1 \(\) 4.5 & & & \\ – – – – & \(\)-contr  & 77.1 \(\) 1.0 & 74.1 \(\) 4.4 & 58.3 \(\) 5.4 & 59.9 \(\) 5.5 \\ – – – – & \(\)-INCE & 77.3 \(\) 1.5 & 75.9 \(\) 1.7 & 68.1 \(\) 5.0 & 68.3 \(\) 4.8 \\ – – – – & \(\)-NCE & 77.1 \(\) 1.0 & 75.4 \(\) 2.4 & 60.2 \(\) 4.9 & 68.0 \(\) 1.5 \\ – – – – & \(\)-SCL & 75.8 \(\) 3.9 & 71.5 \(\) 7.5 & 66.2 \(\) 10.7 & 66.5 \(\) 4.0 \\ – – – – – & \(\)-NWJ & 77.3 \(\) 0.7 & 77.7 \(\) 0.9 & 67.5 \(\) 5.3 & 69.2 \(\) 5.3 \\ 
0.15s & SlowVAE  & 79.6 \(\) 5.8 & & & \\ – – – – – & \(\)-contr  & 79.4 \(\) 1.9 & 80.9 \(\) 3.8 & 60.2 \(\) 8.7 & 68.4 \(\) 6.7 \\ –is achieved by applying a sigmoid function in the last layer of the network and multiplying the result by \(b\). Table 4 shows the results for weak and strong identifiability.

With the exception of \(\)-SCL, the examined contrastive losses have \(R^{2}\) values close to the theoretical optimum. However, \(\)-NWJ shows less robustness when we do not restrict the latent space compared to \(\)-NCE and \(\)-INCE. This demonstrates that incorporating knowledge about the latent space can stabilize training and lead to strong identifiability, even in the case of a (truncated) Gaussian conditional distribution.

### Investigation of Limitations

In this section, we discuss practical and theoretical limitations of the proposed framework. We first examine how changes in the number of latent factors and the concentration of the conditional distribution affect disentanglement. We use the same setup as in Section 4.1, except that we choose a slightly smaller batch size of 4096 and train for \(8 10^{5}\) iterations to ensure convergence when we consider more dimensions. The results are shown in Figure 3.

First, we note that all methods have difficulties when the number of latent factors is increased (see Figure 3, left column). In particular, \(\)-SCL shows little robustness for larger dimensions.

Furthermore, we see that the performance of all methods declines with increasing concentration (\(1/\)) of the conditional and is exacerbated by a larger shape parameter (\(\)). The work in  noted that InfoNCE has difficulty disentangling the latent factors when the conditional distribution becomes too flat relative to the marginal one, which, they pointed out, makes positive examples indistinguishable from negative ones. We observe here the same problem in the opposite case.

Consider the following example. Suppose \(=^{n}\) and \(p()\) is uniform, then the range of \(\) at the global optimum is \([0,n/^{}]\). For larger \(\) and smaller \(\), this quickly leads to vanishing values when calculating the exponential in the contrastive losses. This could also be one of the reasons why we see worse performance for smaller \(\) in KITTI Masks.

It should be noted that employing learnable functions in the similarity measure may entail certain drawbacks, such as the need to store additional parameters and a slight increase in training time. Furthermore, we have seen in Section 4.2 that in some cases the underlying data distribution can be adequately approximated by constant \(\) and \(\). Although not observed in this work, it is also plausible that the inclusion of these functions for more complex datasets may increase the risk of getting stuck in poor local minima during training, as incorrect shapes of these functions may promote poor representations (the shape of \(\) and \(\) affects the gradient of \(f\) and vice versa).

Finally, we want to emphasize that the mechanism that governs the observation pairs is highly dependent on the application and that the relationship between the latents may not be adequately

Figure 3: MCC (top) and \(^{2}\) scores (bottom) for different numbers of latent factors (left) and for different concentrations (\(1/\)) of the conditional distribution for \(=1\) (center) and \(=3\) (right).

described by a distance-based conditional as assumed in this work (Eq. (1)). However, we leave the exploration of methods suitable for other mechanisms to future work.

## 5 Conclusion

In this work, we propose a new theoretical framework of contrastive methods for learning disentangled representations. In contrast to previous research, our framework accounts for nonuniform marginal distributions of the factors of variation, allows for nonconvex latent spaces, and does not assume that these factors are statistically independent or conditionally independent given an auxiliary variable.

Our theory unveils that contrastive methods implicitly encode assumptions about the data generating process. We show empirically that even when these assumptions are partially violated, these methods learn to recover the true latent factors. This study provides further evidence that contrastive methods learn to approximately invert the underlying generative process, which may explain their remarkable success in many applications.

## 6 Acknowledgements

This work has been carried out as part of the AuSeSol-AI Project which is funded by Federal Ministry for the Environment, Nature Conservation, Nuclear Safety and Consumer Protection (BMUV) under the Grant 67KI21007F.