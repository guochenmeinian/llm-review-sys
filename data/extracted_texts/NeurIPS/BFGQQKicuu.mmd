# Score-based Source Separation with Applications to Digital Communication Signals

Tejas Jayashankar\({}^{1}\) Gary C.F. Lee\({}^{1}\) Alejandro Lancho\({}^{1,2}\) Amir Weiss\({}^{1}\)

&Yury Polyanskiy\({}^{1}\) Gregory W. Wornell\({}^{1}\)

\({}^{1}\)Massachusetts Institute of Technology \({}^{2}\)Universidad Carlos III de Madrid

###### Abstract

We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by _maximum a posteriori_ estimation with an \(\)_-posterior_, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling. The project webpage is available at https://alpha-rgs.github.io.

## 1 Introduction

The problem of single-channel source separation (SCSS) is ubiquitous and arises in many different applications, ranging from the cocktail party problem in the audio domain to interference mitigation in the digital communications domain. Broadly speaking, the goal in SCSS is to decompose a mixture \(=_{1}_{1}++_{K}_{K}^{N}\) into its \(K\) constituent components, \(\{_{i}\}_{i=1}^{K},_{i}_{i}^{N}\). Motivated by applications in modern engineering, e.g., intelligent communication systems, we are particularly interested in the SCSS problem for heterogeneous sources with _underlying discrete structure_.

Prior art that uses deep learning for source separation problems has been well-studied in the audio and image domain by leveraging domain-specific structures. For example, recent end-to-end methods in the audio domain leverage spectrogram masking techniques to separate sparse time-frequency representations [1; 2], while in the visual domain, natural images may be separable by exploiting local features and "smoothness" assumptions in the color space [3; 4]. More recent efforts have tried to solve this problem by leveraging independently trained statistical priors using annealed Langevin dynamics sampling [5; 6; 7]. However, these methods have demonstrated shortcomings on discrete sources that exhibit intricate temporal structures with _equiprobable multimodal distributions_.

These additional challenges motivate us to develop a _novel general Bayesian framework_ for source separation that relies on statistical priors over sources with underlying discreteness. Specifically, we focus on learning data-driven priors for two main reasons--i) **Unknown system parameters**, e.g., the underlying signal generation model may not be available to create hand-crafted priors; and ii) **Automation**, facilitated by learning methods to create plug-and-play priors for versatile use.

In this paper, we study the SCSS problem involving a mixture of two signals, \(\) and \(\), which interfere at a different relative scale to produce the mixture signal \(=+\), where \(_{+}\) is the scaling coefficient. We additionally impose restrictions on access to paired training data samples \((,,)\) during training. Our motivation for this restriction arises from the following consideration: given \(n\) sources, the number of two-component source separation models leveraging joint-statistics grows as \((n^{2})\). Any changes to the training data--even for a single source--would require \((n)\) model updates. In contrast, solutions that leverage independently trained priors over the sources need to _update only a single model_ (i.e., \((1)\)). Additionally, these priors can also be used to solve more generalized source-separation problems, e.g., with \(K>2\) components or with a different mixture model, _without requiring any additional training_.

We demonstrate the successful operation of our method in the digital radio-frequency (RF) setting. Signals in the RF domain are often discrete in nature as their randomness arises from bits/symbols that modulate a continuous waveform. As such, source separation performance is measured not only through continuous reconstruction metrics, e.g., the mean squared error (MSE), but also (and primarily) by the fidelity of digital communication, commonly measured by the bit error rate (BER).

Broader Impact.The crux of our work is motivated by challenges arising in the next generation of wireless systems. The proliferation of intelligent wireless devices--utilizing finite spectrum resources--has called for better interoperability strategies, so as to ensure reliable operations in a rapidly growing _heterogeneous wireless networking ecosystem_. Interference from other sources operating in the same channel, e.g., 5G waveforms or WiFi signals, can lead to deterioration in quality of service. While conventional approaches treat such interference as Gaussian noise, there could be significant performance gains if we were able to learn and exploit intricate statistical structures of these co-channel signals . We take an important step towards this goal by developing data-driven _score-based SCSS solutions_ that demonstrate significant gains over modern and conventional approaches, thereby unveiling novel intelligent and effective interference mitigation strategies. We further elaborate on the broader impact of our work in Appendix A.

Contributions.We start from first principles and propose a new Bayesian-inspired method for SCSS. Our main contributions are as follows.

* We present a new method for SCSS that leverages the (approximate) score from pre-trained diffusion models to extend MAP estimation using generalized Bayes' theorem with an \(\)-posterior  across different levels of Gaussian smoothing. Our method, termed as \(\)**-RGS** (\(\)-posterior with **R**andomized **G**aussian **S**moothing) (see SS3), is easy to implement by randomizing the predetermined training noise levels without cumbersome tuning of a special annealing noise schedule, as in Langevin-dynamics-based approaches .
* We show that our formulation is a multi-source extension of the recently proposed score distillation sampling (SDS) objective . Our analysis (see SS4) shows that despite randomizing across multiple noise levels, the local extrema of our loss (and hence of SDS) asymptotically approach the modes of the underlying unsmoothened source distribution(s) (see SS3.2.1).
* We demonstrate through experimental results (see SS5) that \(\)-RGS outperforms classical signal processing and annealed Langevin-dynamics-based approaches for RF source separation, with a 96% and 94.5% improvement in BER (averaged across different levels of interference) over traditional and existing learning-based methods, respectively. While score-based methods have been adopted to solve problems such as symbol detection  and channel estimation , to the best of our knowledge, this is the first work that applies score-based models for the task of source separation in the RF domain.

Paper Organization.The paper is organized as follows. The necessary prerequisites are introduced in SS2. SS3 meticulously develops our proposed method, starting from basic principles. We provide analytical characterizations of our method in SS4, with additional details available in Appendix B. In SS5, we introduce our problem in the context of digital communication signals and describe our experiments along with results. A detailed summary of our results is available in Appendix G. A primer on digital communication signals and details about our datasets are available in Appendix C. Details related to diffusion models in the RF context, a primer on conventional baselines, and a description of score-based separation baselines can be found in Appendices D, E and F, respectively. We conclude with SS6, where we comment on future work and on the broader impact of our method.

Prerequisites

### Finite Alphabet Signal Processing

In many engineering systems, signals commonly exhibit continuous magnitudes. However, in particular cases, these signals may possess discrete properties, leading to a finite (but possibly large) number of possible realizations. Such signals are often expressed as,

\[s(t)=_{p=-}^{}c_{p}\;g_{p}(t-t_{p}),\] (1)

where \(c_{p}\) are discrete symbols drawn from a finite set \(\) (or \(\)) and \(g_{p}()\) is a continuous filter that "carries" contributions from the symbols. For example, in optical or RF communications, the symbols could correspond to complex-valued mappings of the underlying bits , while in the discrete tomography domain, the symbols might correspond to measurements obtained from different materials with a finite number of phases or absorption values . In this work, we will address the challenges associated with finding the global optimum within the optimization landscape formulated for separating a superposition of such discrete sources.

### Diffusion Models

Diffusion models are a class of generative models introduced by Sohl-Dickstein et al.  based on the principles of thermodynamic diffusion. The model is parametrized by two Markov chains of length \(T\)--i) the forward chain \(q(_{t}|_{t-1})\), in which noise is gradually added to the data \(_{0}\) such that at the end of the forward process, \(q(_{T})=(_{T};0,)\); and ii) the learned reverse chain \(p_{}(_{t-1}|_{t})\), in which noise is gradually removed (i.e., denoising) in order to generate new samples.

These models are trained by minimizing a variational lower bound on the log-likelihood. Recent work by Ho et al.  in the image domain shows that this optimization problem translates to learning a denoiser \(r_{}(_{t},t)\) parametrized as a neural network with weights \(\) that estimates the noise term in \(_{t}}_{0}+} \), \((0,)\) across different timesteps \(t(\{1,,T\})\),

\[^{*}=*{arg\,min}_{}_{,, }[\|r_{}(_{t},t)-\|_{2}^{2}],\] (2)

where \(\{1-_{i}\}_{i=1}^{T}\) defines an increasing signal _variance-preserving_ noise schedule with \(0_{i} 1\). The works in [21; 22; 23] show that objective (2) is equivalent to minimizing a loss whose minimizer is an estimate of the marginal score \(S_{_{t}}(_{t})=_{_{t}} q(_ {t})\) such that,

\[S_{_{t}}(_{t})-(1-_{t})^{-}{{ 2}}}r_{}(_{t},t).\] (3)

Other works have extended diffusion models to continuous time using stochastic differential equations , while others have used them for signals in the audio domain  and forecasting problems .

## 3 A Bayesian Framework for Source Separation

We now develop the Bayesian framework underlying our proposed source separation method. We are particularly motivated in developing a generalized framework that relies on statistical priors _only_, without any other form of domain-specific regularization.

### Maximum a Posteriori (MAP) Source Separation

Signal statistical structure.Let \(^{d}\) and \(^{d}\) be two _statistically independent_ complex-valued vector sources, where \(\) is a countable set of all realizations \(\). We assume that \(\) has PMF \(P_{}\), and we let \(\) be an arbitrary source (potentially discrete with some noise) with PDF \(p_{}\). We assume that the latter distributions are multimodal, where the probability is generally (close to) zero except at the modes. Additionally, we seek to be robust to the challenging setting where the distributions have _multiple equiprobable modes_, so as to develop novel methods that can tackle the finite alphabet source separation problem as is motivated in SS1. We emphasize that these assumptions do not completely characterize the often complicated fine-grained statistical source structure, and we therefore rely on generative models to learn such unknown structures from data.

MAP problem formulation.Consider a mixture composed of two superimposed sources,

\[=+,\] (4)

where \(_{+}\) is a relative scaling coefficient between the two signals.

Given \(\) and assuming \(\) is known, in order to separate the sources, it is sufficient to estimate \(\) since \(=(-)/\). The MAP estimate of \(\) is then given by,

\[}=}{}\,\,p_{ |}(|).\] (5)

Using Bayes' theorem, (5) can be equivalently expressed as,

\[}{}\,\,p_{| }(|) =}{}\,\,p_{| }(|)P_{}()\] (6a) \[=}{}- P_{}()- p_{}((-)/),\] (6b)

where the likelihood \(p_{|}(|)=p_{}((-)\,/)\) under the constraint (4), and we convert to negative log probabilities in (6b). Due to the underlying discreteness of \(\), and the potential underlying discreteness of \(\) as well, the objective function in (6b) is not differentiable. Hence, in its current form, gradient-based techniques cannot be used to solve this problem, and we must instead resort to combinatorial methods, which are computationally infeasible even for moderate dimension size \(d\).

### Proposed Method (\(\)-RGS)

To overcome the computational complexity of combinatorial-based methods, our goal is to develop new gradient-based SCSS solutions that leverage diffusion models trained on discrete sources. To this end, we propose to use multiple levels of Gaussian smoothing with an extended MAP framework with an \(\)-posterior, such that the optimization landscape of the resulting objective function is smoothened.

#### 3.2.1 The Smoothing Model and \(\)-posterior Generalized Bayes'

Surrogate distribution.One can almost perfectly approximate \(P_{s}\) (in some well-defined sense) with the _surrogate distribution_\(p_{}\), where \(}=+_{s}\) for \(_{s}(0,_{s}^{2})\), \(_{s} 0\). While now theoretically amenable to optimization via gradient descent, the sharp modes, which constitute numerical pitfalls, often cause gradient-based methods to get stuck in local extrema.

Gaussian Smoothing Model.We adopt a _variance-preserving_ smoothing model, with adjustable noise levels, based on the formulation of the stochastic process proposed in . Let \(\{1-_{t}\}_{t=1}^{T}\) be a sequence of noise levels such that \(_{i}>_{i+1}\) and \(_{i}(0,1]\). Define the "smoothened sources" as

\[}_{t}(}) }}+} _{s},\] (7a) \[}_{u}(},) }(-})/ +}_{b},\] (7b)

where \(t,u(\{1,,T\})\) and \(_{s},_{b}(0,_{d})\). Observe that \(}_{t}\) and \(}_{u}\)--the continuous-valued proxies for s and b, respectively--have PDFs rather than PMFs, and in particular, their PDFs have infinite support, and they are differentiable. More importantly, a direct consequence is that it smoothens the optimization landscape and helps in preventing gradient-based algorithms from getting stuck at spurious local minima.

Generalized Bayes' with an \(\)-posterior.We found it useful to replace the likelihood in (6a) with a distribution proportional to \(p_{|}(|)^{},\,>1\). Intuitively, under the constraint (4), this sharpens the distribution of \(\) and gives a higher weight to the modes of \(p_{}\) relative to the natural weighting that arises from the MAP criterion. This is beneficial, for example, when \(p_{}\) is more complicated and has many more modes than \(P_{s}\).

This aforementioned reweighting has been used as an implementation trick in diffusion sampling with classifier conditioning , but we recognize this as MAP estimation with an \(\)-posterior1 where \(=\) which is expressed through the generalized Bayes' rule  as,

\[=}{p_{| }(|;)}}{p_{| }(|)^{}}\,\,}{P_{ }()}.\] (8)In practice, using an \(\)-posterior has demonstrated increased learning speeds  and could potentially help in resolving model mismatch arising from the use of approximate densities or scores (rather than exact ones) during optimization via gradient descent.

Single noise level estimation loss.Let \(}()=\) be our estimate of \(\).2 Motivated by the form in (6b), we generalize using the smoothing (7a)-(7b) in conjunction with the \(\)-posterior with \(=\) to define a new _single noise level estimation loss_,

\[_{t,u}()- p_{_{t}} }_{t}()-  p_{_{u}}(}_{u}( {},)).\] (9)

While departing from the original MAP (6b), the newly-defined approximated loss (9) thereof facilitates gradient-based methods, which is key to our solution approach. Particularly, by varying the level of smoothing, (9) is more easily explored in regions between the modes via gradient descent.

#### 3.2.2 Estimation Rule Across Multiple Noise Levels

Intuitively, larger noise variances allow us to move between modes during gradient descent. In contrast, at lower noise levels the modes are sharper, which is beneficial in resolving the solution, assuming the iterative procedure starts at the basin of attraction of the correct local extremum point, or, alternatively, another "good" local extremum point. We propose a new estimation rule that uses (9) across multiple noise levels. Randomizing over multiple levels of Gaussian smoothing, our proposed gradient update asymptotically (in the number of iterations) takes the form,

\[_{}() _{,_{t}}[}\,S _{}_{t}}(}_{t}())]+_{, _{u}}[}S_{}_{u}}( ,)]}_{_{t,_{t}} [_{}_{t,u}( )]},\] (10)

where \(t,u(\{1,,T\})\), and the true score is defined as

\[S_{}_{t}}}_{t}()_{} p_{_{t}} ()_{=}_{t}( )},\] (11)

and similarly for \(S_{}_{u}}\).

Our proposed updates can be implemented using stochastic gradient descent as shown in Algorithm 1. We use _pre-trained diffusion models_ (SS2.2) to approximate the score when deriving the analytical score is not possible. Using (3), we relate the learned score to the denoiser available from the diffusion model, and also use a zero mean noise corrected estimate,

\[_{_{s}}[r_{_{}} }_{t}(),t]= _{_{s}}[r_{_{}} }_{t}(),t-_ {s}],\] (12)

in our updates as shown in Algorithm 1. The above rule is motivated to introduce numerical stability and reduces the variance of the updates, since the diffusion models were trained to minimize the squared value of the same error term in (2). The state-of-the-art method of score distillation sampling (SDS)  that uses diffusion models as critics, also uses the _same term_ in their gradient update step.

### Relation to Other Works

BASIS separation.Jayaram and Thickstun  introduced the BASIS separation algorithm that leverages the score from a generative model to perform source separation using annealed Langevin dynamics. Unlike our method, the BASIS algorithm relies on a specially tuned noise schedule for separation that is distinct from the diffusion model training noise schedule. We circumvent the challenges associated with tuning such a schedule and instead _re-use the pre-determined training noise schedule_ in a randomized fashion. As such, _the only parameter that we tune is the learning rate_. Our results (SS5.3) show that we outperform BASIS in the context of RF source separation.

Score Distillation Sampling.Our work can be viewed as an extension of the recently proposed Dreamfusion architecture , which uses pre-trained image diffusion models as critics to guide the generation of novel 3D visual content. Given a generated sample, \(g()\), Score Distillation Sampling (SDS) updates the 3D object realization using gradient descent with a gradient given by,

\[_{}_{}(,}=g())=_{}[w(t)(r_{}(}\,g()+},t)-)],\] (13)

where \(w(t)\) is a scaling related to the noise variance. Our updates contain the same gradient terms in (13) and hence it can be viewed as a _multi-source extension of SDS_, shedding light on its use in applications to problems beyond sampling with interactions between numerous individual priors.

Diffusion models as priors.More recently diffusion priors have been used in inverse problems such as MRI reconstruction [29; 30], image restoration/colorization [23; 31; 32; 33; 34] and for developing general inversion methods [32; 35]. A majority of these works solve a MAP problem with domain-specific likelihood constraints, thus resembling our formulation. However, they either use annealed Langevin dynamics or a noise contracting reverse diffusion process to sample from the posterior distribution, necessitating the tuning of a special annealing schedule. In our work, we demonstrate convergence to the same modes using a simpler scheme based on randomized levels of Gaussian smoothing (see SS4). The \(\)-posterior parameter can be interpreted as a weighting of the regularization term, thus shedding light on the potential applicability of our method to general inverse problems in the future.

## 4 Characterization of \(\)-Rgs

In this section, we characterize the behavior of our objective function through a simple yet intuitive example. We first present a sufficient condition for perfect signal separation in the context of discrete sources that follow the signal generation model in (1).

**Proposition 1**.: _Let \(s(t)\) and \(b(t)\) be two sources following (1) with underlying symbols \(c_{p}^{s}\) and \(c_{p}^{b}\) respectively. Assume that the symbols are obtained as,_

\[c_{p}^{s}=f(\{u_{i}\}_{i=p}^{L+p}) c _{p}^{b}=h(\{v_{i}\}_{i=p}^{L+p}),\] (14)

_where \(f:^{L}\) and \(h:^{L}\) are mappings from a sequence of length \(L\) over the discrete alphabets \(\) and \(\) respectively3. If the mapping between the discrete representation and the symbol representation of the sources is unique, perfect recovery is possible._

Figure 1: **Left:** Two discrete sources, with infinitesimal additive noise, superimposed to produce a joint distribution with 8 equiprobable modes. An observed mixture \(\), imposes a linear constraint in this space. **Middle:** Extending vanilla MAP (\(=1\)) to multiple noise levels still has a relatively large local minima. **Right:** By using \(=^{2}\), we are able to accentuate the correct mode and smooth the landscape even further. Colored curves correspond to (15) evaluated with \(T=1\) and \(t=u\).

We now argue that under such conditions, our method in Algorithm 1 asymptotically approaches a local extremum corresponding to the following loss function,

\[()-_{,_{s}} [ p_{}(}_{t}(}_ {t}())]-_{,_{u}}[ p_{}_{u}}(}_ {u}(,))].\] (15)

To see this, let \(\) and \(\) be two discrete sources with equiprobable modes at \(\{-1,+1\}\) and \(\{ 6/, 2/\}\) respectively. Figure 1 shows the contours of the negative log joint probability, with the sources augmented with an infinitesimally small amount of Gaussian noise (see SS3.2.1). An observed mixture \(y\), adds a linear constraint in the (\(s\), \(b\)) plane. The goal of Algorithm 1 is to pick out the constraint-satisfying mode at \((1,}{{}})\). If \(=1\), as shown in the middle plot, and given a poor initialization \(^{(0)}\) closer to \(-1\), the gradients in this region may prevent the estimate from escaping the suboptimal local minimum. If instead, we use \(=^{2}>1\) (\(=15.85\)), as shown in rightmost plot, the optimization landscape is better conditioned in the same (absolute) neighborhood around \(=-1\) with the mode at \(+1\) much more accentuated. Specifically, if the algorithm is now initialized at \(^{(0)}=-1\), after enough iterations, gradient steps at larger noise levels would lead the solution towards the direction of \(=1\). Thus, on average (black curve), after enough iterations, the solution will approach \(+1\). In constrast, Langevin-dynamics-based approaches without the \(\)-posterior weighting , if initialized poorly, could potentially get stuck at local extrema due to sharpening of the optimizing landscape as the level of noise decreases.

The estimate of the discrete source \(\) can be obtained by mapping the solution returned by Algorithm 1 to the closest point (in the Euclidean sense) in the discrete alphabet \(\). This relies on the extremum \(^{*}\) of (15) being sufficiently close to the desired mode of \(p_{}(|;)\), so that \(^{*}\) can be mapped to the correct point in \(\) with high probability. In the context of the above example, a key observation in achieving this desired behavior is that the mode at \((1,}{{}})\) is still prominent at large noise levels. Thus, randomizing across different noise levels helps balance the exploration between modes and the resolution of the estimate. This mainly requires a suitable noise level range (i.e., a lower bound on \(_{T}\)), to ensure that the modes are sufficiently resolved. In our experiments, we show that _no additional tuning is required_ and that the training noise levels from the pre-trained diffusion models can be re-used, provided that the models have learned the source's structure sufficiently well.

A more detailed characterization of our algorithm's convergence to the modes can be found in Appendix B. We underscore that in this work we focus on general signal types and mixtures, where perfect separability is not guaranteed and hence performance bounds are not analytically tractable.

## 5 Applications and Experiments

### Digital Communication Signals

Digital communication pipeline.At a high level, digital communications deals with the transmission of bits by modulating a so-called "carrier signal". Groups of bits, from which the underlying discreteness of these sources originates, are first mapped to symbols \(c_{p}\) via the _digital constellation_--a mapping between groups of bits and a finite set of complex-valued symbols. These symbols are subsequently aggregated via (1) to form a _complex-valued continuous waveform_. In the RF domain, \(g()\) is known as the _pulse shaping_ filter and helps limit the signal's bandwidth [36, Sec 4.4.3]. The constellation is chosen (among other considerations) by the number of bits modulated simultaneously. Common schemes include modulating two bits at a time (Quadrature Phase Shift Keying, or QPSK), or one bit at a time (Binary Phase Shift Keying, or BPSK). Figure 2 illustrates a representative modulation pipeline. To recover the bits at the receiver, one may adopt _matched filtering_ (MF) [18, Sec 5.8] before the estimation of the underlying symbols, and thereafter decode them back to bits. For commonly used pulse shaping functions, such as the root-raised cosine

Figure 2: The single-carrier digital modulation pipeline with an intelligent decoder that performs a pre-processing stage of source separation. Illustrated is an example with a QPSK constellation and a root-raised cosine (RRC) pulse shaping function.

(RRC) shown in Figure 2, the matched filter and pulse shaping filter coincide. We refer readers to [18; 36; 37] for a more thorough exposition of the topic.

Interference mitigation.Mitigating co-channel interference is a challenging problem, especially in heterogeneous networks [9; 10; 11]. If the system parameters and signal generation model are known ahead of time, one can leverage this knowledge to devise hand-crafted priors. However, one often deals with interference from more complicated wireless sources, for which the signal model is unknown. In such scenarios, data-driven methods that learn priors from background recordings can be useful. Our results demonstrate that \(\)-RGS, which leverages these priors, sets a new state-of-the-art in SCSS for RF mixtures. We envision that our SCSS solution could help mitigate such interference prior to decoding the signal, as shown in the right hand (receiver) side of Figure 2.

Deep learning for RF systems.Recently deep learning methods have demonstrated the potential to reap significant gains over handcrafted model-based methods in RF applications [38; 39]. Some works have studied the problem of symbol detection [40; 41], where they assume that the channel is stationary. Other works, such as DeepSIC , use deep learning for interference cancellation in the multi-user setting within the same channel. In contrast, we deal with the more challenging setting of non-stationary interference, thereby requiring efficient exploitation of intricate temporal structures. While the latter works consider the superposition of independent and identically distributed sources (same technology), we assume unknown additive interference (cross technology), a hard problem to solve with naive decoding methods in the absence of explicit prior knowledge about the interference. Our problem formulation is closer to recent work in . However, they learn an end-to-end estimate of the signal from paired data samples. As motivated in SS1, we instead assume restricted access to joint data, with a focus on capturing properties of the components through independent priors.

### Experimental Setup

We now detail the setup for training diffusion models on RF signals. We subsequently explain how these models are used in the implementation of our method at inference time (i.e, for separation).

RF SCSS formulation.We are interested in recovering \(\), the signal of interest (SOI), from a mixture \(=+\), where \(\) is assumed to be a co-channel interference with unknown system parameters. We evaluate performance using two metrics--i) the MSE, that measures the distortion between the estimated SOI and the ground truth; and ii) the BER of the decoded discrete representation, which is obtained from the estimated SOI by extracting the underlying bits (see Appendix C). The latter measure is particular to digital communication signals as it captures the fidelity of the estimated representation that is only partially reflected in the MSE criterion.

Datasets.We train diffusion models on different RF datasets --i) synthetic QPSK signals with RRC pulse shaping (see SS5.1), ii) synthetic OFDM signals (BPSK and QPSK) with structure similar to IEEE 802.11 WiFi signals; and iii) signals corresponding to "CommSignal2" from the RF Challenge , which contains datasets of over-the-air recorded signals. All synthetic datasets were created using the NVIDIA Sionna toolkit . All datasets contain between 150k - 500k samples and we use a 90-10 train-validation split during training. Additional details are in Appendix C.

Diffusion model training.We adopt the Diffwave  architecture for our experiments, with a minor changes (see Appendix D) to accommodate the complex-valued nature of our signals. Our models are trained in the waveform domain on inputs of length \(2560\) with the real and imaginary components concatenated in the channel dimension. We train unconditional diffusion models and assume no access to knowledge about the signal generation model. We use noise variance levels in the range \((1,0.72)\) discretized into \(50\) levels. We train for 500k steps with early stopping on 2 x NVIDIA 3090 GPUs. Detailed hyperparameters for our training setup are provided in Appendix D.

Source separation setup.We consider three different mixtures in our experiments, all using an RRC-QPSK signal as the SOI \(\). The interference signal \(\) is one of OFDM (BPSK), OFDM (QPSK) or a windowed recording from the CommSignal2 dataset. Our proposed algorithm uses \(=^{2}\) and is initialized with the MF solution given the mixture \(\) (see SS5.1). Note that \(\) can be equivalently described as the signal to interference ratio (\(:=1/^{2}_{}[\|\|_{2}^{2}]/_{}[\| \|_{2}^{2}]\)).

We assume that \(\) is known4 and use \(N=20,000\) with a cosine annealing learning rate schedule . The OFDM mixtures use \((_{},_{})=(5,1)\) and the CommSignal2 mixture uses \((_{},_{})=(2,1)\). Importantly, we _re-use the training noise levels_ from the diffusion models and randomize over all but the smallest noise level resulting in a noise variance range of \((1-_{1}=1.2,1-_{T}=0.72)\) discretized into \(T=49\) levels. We test performance across SIR levels ranging from \(-24\) dB to \(-3\) dB ("strong interference" regime), by changing the value of \(\) in the mixture. Each set of separation experiments was conducted on a single NVIDIA V100 GPU.

Baselines.We compare our proposed method against baselines that also leverage independent statistical or structural priors over the sources. The simplest baseline, which nevertheless is still commonly used in most communication systems, is the matched filtering solution, which treats the interference as white Gaussian noise. The linear minimum mean square error (LMMSE) solution, a commonly used technique for signal estimation, is another baseline that leverages (up to) second-order statistics of the underlying source distributions. More details on these methods are in Appendix E.

We also compare with the BASIS separation algorithm (see SS3.3), which is the closest learning-based method that resembles our problem formulation. Applying their method as is yielded poor results, and hence we modified the original hyperparamters by tuning the annealing schedule to the best of our abilities for a fair comparison. For more details on this, refer to Appendix F.

To study the fidelity of our learned score models, we derive the analytical score function of the QPSK SOI in the symbol domain (i.e., before pulse shaping). We use this analytical score as another comparison to demonstrate the performance of our method if the score was known perfectly. This formulation is closer to a learning-based interference mitigation setting, where we assume perfect knowledge about the SOI model, and rely on a learned interference model.

### Source Separation Results

Figure 3 shows the source separation results for the three different mixture types in SS5.2, obtained by averaging 100 independent trials per SIR. Our proposed method (analytical or trained SOI score) significantly outperforms MF, LMMSE and BASIS in terms of both BER and MSE. As expected, our best results are obtained by leveraging the prior knowledge, in the form of the analytical score

Figure 3: **Left:** Source separation results for a mixture with RRC-QPSK SOI and OFDM (QPSK) interference. All curves are obtained by averaging 400 different mixtures per SIR and using \(=^{2}\). Our proposed method significantly outperforms traditional and learning-based baselines (BASIS) in terms of BER and MSE across all noise levels. **Middle:** Similar comparisons only in BER for mixtures with OFDM (BPSK) and CommSignal2 interference, respectively. BER is slightly higher for the CommSignal2 mixture since it contains background noise that is amplified for large \(\). The **black dotted line** in the bottom figure denotes the (presumed) BER lower bound assuming the background noise is an additive white Gaussian noise. **Right:** BER and MSE versus \(/^{2}\) for different SIR levels. Clearly a good choice in this setting, in the sense of minimum BER and MSE, is \(=^{2}\).

for the SOI. Nevertheless, our learned SOI score can nearly mimic this performance, and despite the slight degradation still outperforms all baselines in terms of BER as well. It should be noted that CommSignal2 contains small amount of background noise, which is amplified at low SIR (high \(\)). This noise constrains the minimal achievable BER even under the assumption of only having the residual AWGN present, illustrated by the black dotted line at the bottom middle plot in Figure 3. We also outperform the baselines in terms of MSE across all SIR levels, an example of which is shown at the bottom left of Figure 3. More detailed results are included in Appendix G.

Characterizing the choice of \(\).We numerically verify that \(=^{2}\) is a good choice for the \(\)-posterior term. To this end, we find a suitable order of magnitude for \(\), by varying \(/^{2}\) between \(10^{-2}\) and \(10^{2}\) and computing the resulting BER and MSE for four representative SIR levels using a held-out _hyperparameter validation set_ of 400 mixtures. The right panel of Figure 3 shows that \(=^{2}\) achieves the minimum BER and MSE on average. More characterizations are in Appendix G.

## 6 Concluding Remarks

In this work, we propose \(\)-RGS, a method that extends MAP estimation with an \(\)-posterior across randomized levels of Gaussian smoothing, which stems from a new objective function, whose extrema points correspond to the modes of the underlying discrete distribution of interest. Our method relies only on pre-trained diffusion models as priors via a simple randomized algorithm that does not require cumbersome tuning of a special annealing schedule, as is done in existing Langevin-dynamics-based works. Through simple analytical illustrations, we demonstrate the favorable mode-preserving nature of our objective, and show that \(\)-RGS is a generalized extension of the recently proposed SDS loss. Experiments on RF sources demonstrate superior separation performance, with gains of up to 95% in terms of both BER and MSE over classical and existing score-based SCSS methods.

Limitations and future work.We believe there is a potential to use our algorithm to develop a general toolkit of algorithms for source separation and inverse problems. However, we do list some _limitations_ of our method that will be a focus of future work: 1) inference time (\( 5\) min per mixture) requires speed-up in real-time systems; 2) studying settings with more than two mixture components or other mixture models; 3) developing a systematic approach to choose \(\) for practitioners; and 4) addressing the robustness of our method for applications to signals with new properties, e.g., gravitational waves. We further elaborate on the broader impact of our work in Appendix A.