# Active Bipartite Ranking

James Cheshire Stephan Clemencon

Telecom ParisTech

first.last@telecom-paris.fr

Vincent Laurent

ENS Paris Saclay

first.last@ens-paris-saclay.fr

###### Abstract

In this paper, we develop an active learning framework for the bipartite ranking problem. Motivated by numerous applications, ranging from supervised anomaly detection to credit-scoring through the design of medical diagnosis support systems, and usually formulated as the problem of optimizing (a scalar summary of) the \(\) curve, bipartite ranking has been the subject of much attention in the passive context. Various dedicated algorithms have been recently proposed and studied by the machine-learning community. In contrast, active bipartite ranking rule is poorly documented in the literature. Due to its global nature, a strategy for labeling sequentially data points that are difficult to rank w.r.t. to the others is required. This learning task is much more complex than binary classification, for which many active algorithms have been designed. It is the goal of this article to provide a rigorous formulation of such a selective sampling approach. We propose a dedicated algorithm, referred to as active-rank, which aims to minimise the distance between the ROC curve of the ranking function built and the optimal one, w.r.t. the \(\) norm. We show that, for a fixed confidence level \(\) and probability \(\), active-rank is \((,)\). In addition, we provide a problem dependent upper bound on the expected sampling time of active-rank and also demonstrate a problem dependent lower bound on the expected sampling time of any \((,)\) algorithm. Beyond the theoretical analysis carried out, numerical results are presented, providing strong empirical evidence of the performance of the algorithm proposed, which compares favorably with more naive approaches.

## 1 Introduction

In bipartite ranking, the statistical framework is exactly the same as that in standard binary classification, the flagship problem in statistical learning theory. One observes \(n 1\) independent copies \(_{n}=\{(X_{1},Y_{1}),\ ,\ (X_{n},Y_{n})\}\) of a generic random pair \((X,Y)\) with (unknown) distribution \(P\), where \(Y\) is a binary random label, valued in \(\{-1,\ +1\}\) say, and \(X\) is a high dimensional random vector, taking its values in \(^{d}\) with \(d 1\), that models some information hopefully useful to predict \(Y\). In contrast to binary classification, the goal pursued is of global (and not local) nature. It is not to assign a label, positive or negative, to any new input observation \(X\) but to rank any new set of (temporarily unlabeled) observations \(X^{}_{1},\ ,\ X^{}_{n^{}}\) by means of a (measurable) scoring function \(s:\), so that those with positive label appear on top of the list (_i.e._ are those with the highest scores) with high probability. More formally, the accuracy of any scoring rule can be evaluated through the \(\) curve criterion or its popular scalar summary, the \(\) (standing for the Area Under the \(\) Curve), and, as expected, optimal scoring functions w.r.t. these performance measures can be shown to be increasing transforms of the posterior probability \((x)=\{Y=+1 X=x\}\)\(x\). Though easy to formulate, this problem encompasses many applications, ranging from credit risk screening to the design of decision support tools for medical diagnosis through (supervised) anomaly detection. Hence, motivated by a wide variety of applications, bipartite ranking has received much attention these last few years. Many approaches to this global learning problem (_i.e._ the problem of learning a preorder on the input space \(\) based on a binary feedback) have been proposed and investigated. In Clemencon and Vayatis (2009), optimization of the (empirical) \(\) curve in \(\) norm is considered via nonlinear approximation techniques, while this functional optimization problem is viewed as a superposition of cost-sensitive binary classification problems in Clemencon and Vayatis (2008) so as to propose an alternative method. In Clemencon et al. (2008) (see also Agarwal et al. (2005)), empirical maximization of the empirical \(\) criterion is considered, bipartite ranking being viewed as a pairwise classification problem, a _plug-in_ approach to bipartite ranking is developed in Clemencon and Robbiano (2011) and scalar performance criteria other than the \(\) have been recently reviewed in Menon and Williamsson (2016). Whereas the vast majority of dedicated articles consider the _batch_ situation solely, where the learning procedure fully relies on a set \(_{n}\) of training examples given in advance, the goal of this paper is to develop an _active learning_ framework for bipartite ranking, in other words to investigate this problem in an iterative context, where the learning procedure can formulate queries in a sequential manner, so as to observe the labels at new data points in order to refine progressively the scoring/ranking model. Precisely, the challenge consists in determining an incremental experimental design to label the data points in \(\) that would permit to improve the \(\) curve progressively, with statistical guarantees.

Our contributionsWe describe an algorithm, active-rank, which sequentially queries points of the feature space \(\). Given a confidence level \(>0\) and probability \(>0\), the goal of active-rank is, in a few queries as possible, to output a ranking of \(\), such that the induced ROC curve of said ranking is within \(\) of the optimal ROC curve, in terms of the sup norm, with probability greater than \(1-\). We restrict our selves to dimension 1, i.e. \(=\) and make a single key assumption that the posterior \(\) is piecewise constant on a grid of size \(K\). Theorem 3.2 then shows that active-rank satisfies the above statistical guarantee and furthermore, provides an upper bound on it's expected total number of queries. In Theorem 3.3 we provide a lower bound on the expected number of queries for any possible policy, which satisfies a confidence level \(\) with probability greater than \(1-\). The aforementioned bounds are _problem dependent_, in the sense that they depend on features of the posterior \(\). Finally we conduct a practical analysis of active-rank on synthetic data, comparing it to several naive approaches.

The article is structured as follows. In Section 2 we formally define our setting as well recalling some key notions related to bipartite ranking and \(\) analysis. In Section 2 we also cover some of the existing literature in active learning, of relevance to bipartite ranking. We then describe the active-rank algorithm in Section 3.1. Following this our theoretical results are presented in Section 3.2. Lastly, the experiments are displayed in Section 4. We then conclude and discuss some perspectives for future research in Section 5. Technical details and proofs are deferred to the Supplementary Material.

## 2 Background and preliminaries

### Notation

Here we introduce several dedicated notions that will be extensively used in the subsequent analysis. For any integer \(n 1\), we set \([n]:=\{1,\ ,\ n\}\), denote by \(_{n}\) the symmetric group of permutations on \(\{1,\ ,\ n\}\), by \(_{n}\) the identity map of \(_{n}\). By \(\) is meant the Lebesgue measure on \(\). Given two probability distributions \(P\) and \(Q\) on a measurable space \((,)\), we write \(P Q\) when \(P\) is absolutely continuous w.r.t. \(Q\). For any \(a,\ b\) in \(\), \((a)\) refers to the Bernoulli distribution with mean \(a\) and \((a,b)\) to the Kullback Leibler divergence between the Bernoulli distributions \((a)\) and \((b)\). For any \(a,b\), the Chernoff Information between the distributions \((a)\) and \((b)\) is defined as, \(^{*}(a,b)=(x^{*},a)=(x^{*},b)\), where \(x^{*}\) is the unique \(x\) such that \((x,a)=(x,b)\). The indicator function of any event \(\) is denoted by \(\{\}\), the Dirac mass at any point \(x\) by \(_{x}\), and the pseudo-inverse of any cdf \((u)\) on \(\) by \(^{-1}(t)=\{v:\ (v) t\}\).

### Setting

The bipartite ranking problemA rigorous formulation of bipartite ranking involves functional performance measures. Let \(\) be the set of all scoring functions, any \(s\) defines a preorder \(_{s}\) on \(\): for all \((x,x^{})^{2}\), \(x_{s}x^{} s(x) s(x^{})\). From a quantitative perspective, the accuracy of any scoring rule can be evaluated through the \(\) curve criterion, namely the PP-plot \(t(1-H_{s}(t),\ 1-G_{s}(t))\), where \(H_{s}(t)=\{s(X) t Y=-1\}\) and \(G_{s}(t)=\{s(X) t Y=+1\}\), for all \(t\). The curve can also be viewed as the graph of the cad-lag function \((0,1)(s,)=1-G_{s} H_{s}^{-1}(-1-)\). The notion of \(\) curve defines a partial order on the set of all scoring functions (respectively, the set of all preorders on \(\)): \(s_{1}\) is more accurate than \(s_{2}\) when \((s_{2},)(s_{1},)\) for all \((0,1)\). As can be proved by a straightforward Neyman-Pearson argument, the set \(^{*}\) of optimal scoring functions is composed of increasing transforms of the posterior probability \((x)=\{Y=+1 X=x\}\), \(x\). We have \(^{*}=\{s:\ (x,\ x^{})^{2},\ \ (x)< (x^{}) s^{*}(x)<s^{*}(x^{})\}\) and

\[(s,s^{*})^{*},\ (0,1),\ \ (s,)^{*}():=(s^{*},).\]

The ranking performance of a candidate \(s\) can be thus measured by the distance in \(\)-norm between its \(\) curve and \(^{*}\), namely \(d_{}(s,s^{*}):=_{(0,1)}\{^{*}()-(s,)\}\). An alternative convention to represent the ROC of a scoring function \(s\), which we will use for the remainder of this paper, is to consider the broken line \(}(s,.)\), which arises from connecting the PP-plot by line segments at each possible jump of the cdf \(H_{s}\). **From here on out when referring to the \(\) of a scoring function \(s\), we refer to the broken line \(}(s,.)\).**

The active learning settingWhereas in the batch mode, the construction of a nearly optimal scoring function (_i.e._ a function \(s\) such that \(d_{}(s,s^{*})\) is'small' with high probability) is based on a collection of independent training examples given in advance, the objective of an _active learner_ is to formulate queries in order to recover sequentially the optimal preorder on the feature space \(\) defined by the supposedly unknown function \(\). That is, the active learner plays a game with multiple time steps, where, at time each step \(n\), they must choose a point \(a_{n}\) to query, so as to observe the random label \(Y_{n}((a_{n}))\) and refine the scoring model incrementally. After a sufficient number of rounds has elapsed, chosen at the learner's discretion, a final scoring function \(\), is output.

Piecewise constant scoring functionsHere we consider the simplest scoring functions, measurable functions that are constant on pieces of the input space \(\) forming a partition. As shown in Clemencon and Vayatis (2009) (see subsection 2.3 therein), when smooth enough, \(^{*}\) can be accurately approximated by the (stepwise) \(\) curve of a piecewise constant scoring function. Because the goal of this paper is to highlight the nature of active bipartite ranking rather than treating the problem in full generality, various simplifying assumptions are made in the subsequent analysis. For simplicity we suppose that \(=[0,1)\) and introduce the grid points \(\{G_{1},...,G_{K}\}=\{i/K:\ \ i=1,\ ,\ K-1\}\), where \(K 1\). A preorder on \(\) can be then naturally defined by means of a permutation \(_{K}\). Consider indeed the scoring function

\[s_{}(x):=_{i=1}^{K}i\{x[G_{(i)},\ G_{( i+1)})\}.\] (1)

We denote by \(_{K}\) the set of all functions of type (1). To avoid dealing with model bias here, we assume that the optimal preorder, that induced by \((x)\) namely, can be defined by a scoring function in \(_{K}\).

**Assumption 2.1**.: There exist a permutation \(_{K}\) and distinct constants \(_{1},\ ,\ _{K}\) in \((0,1)\) such that

\[(x)=_{i=1}^{K}_{i}\{x[G_{(i)},\ G_{(i+ 1)})\}\ \ x[0,1)\.\]

We write \(p=_{i[K]}_{i}\). We point out that, as \(\) may remain constant over multiple sections of the grid, the permutation \(\) satisfying assumption 2.1, is not necessarily unique. In the subsequent analysis, the parameter \(K\) is supposed to be known, in contrast with the \(_{i}\)'s, which have to be learned by means of an active strategy. As we assume no structure between the grid points, one can easily consider Assumption 2.1 in higher dimensions, simply consider the \(d\) dimensional grid of size \(K^{d}\). As such, all results presented in this paper **extend trivially to higher dimensions**.

**Policies and fixed confidence regime.** We denote the outputted scoring function of the learner \( S_{K}\). The way the learner interacts with the environment - i.e. their choice of points to query, how many samples to draw in total and their choice of \( S_{K}\), we term the _policy_ of the learner. We write \(\) for the set of all possible policies of the learner. For a policy \(\) and problem \(\) we denote random variable \(^{}_{}\) as the stopping time of policy \(\). We write \(^{}_{}\) for the scoring function outputted by policy \(\) on problem \(\). Where obvious we may drop the dependency on \(,\) in the notation, referring to the scoring function outputted by the learner as simply \(\). We write \(_{,}\) as the distribution on all samples gathered by a policy \(\) on problem \(\). We similarly define \(_{,}\).

For the duration of this paper we will work in the _fixed confidence regime_. For a confidence level \(\), define, \(S^{}_{K}:=\{s S_{K}:d_{}(s,)\}\). A policy \(\) is said to be \((,)\) (probably approximately correct), on the class of problems \(\), if, \(,_{,}[ S^{}_{K}]  1-\). The goal of the learner is to then obtain a \((,)\) policy \(\), such that the expected stopping time in the worst case, \(_{}_{,}[^{}_{}]\), is minimised.

**Defining problem complexity** The expected minimum number of samples a policy must draw on a certain problem, \(\), to be \((,)\) is a quantity which depends upon the features of \(\), specifically, the shape of the posterior \(\). When defining our measure of problem complexity we must capture this dependence as succinctly as possible. To build intuition for our definition we first explore a naive strategy and introduce some informative Lemmas. A naive approach to the active bipartite ranking problem, is to treat each pair of points on the grid, \(i,j[K]\) as a separate classification problem. To correctly distinguish the situations, \(^{i,j}_{0}:=_{i}>_{j}\), \(^{i,j}_{1}:=_{i}<_{j}\), with probability greater than \(1-\), it is well known, see e.g. Kaufmann et al. (2014), that for small \(\), the minimum number of samples required is of the order \(^{*}(_{j},_{i})}\), where we remind the reader \(^{*}\) is the the Chernoff Information, closely related to the \(\) divergence, see Section 2.1. Thus, if the learner wished to output a scoring function in \(S^{*}\), the sample complexity would be of the order, \(_{i[K]}(^{*}(_{j}, _{i}))}\). Of course, distinguishing between \(^{i,j}_{0}\) and \(^{i,j}_{1}\) is impractical when \(_{i}\) and \(_{j}\) are very close, or even equal. However, in our regime, the learner is not required to correctly rank every pair of points \(i,j[K]\), only to output a scoring function existing in \(S^{}_{K}\). Intuition indicates that the learner may be irreventent to the ranking within certain groups of points on the gird, as long as there posterior values are sufficiently close. For instance, consider a partition of \(\), \(=\{C_{1},C_{2},C_{3}\}\), and increasing sequence \((_{1},_{2},_{3})^{3}\) with

\[(x)=_{i=1}^{3}(x C_{i})_{i},(x)= (x C_{1})+2(x\{C_{2} C_{3}\})\;,\] (2)

Where the scoring function function \(\) essentially, treats all points of \(C_{2},C_{3}\) of the same rank. See Figure 1 for the ROC curves of \(\) and \(\). Via simple calculation, we have the following, \(d_{}(,)=)}{p}-_{2}) /2}{1-(_{3}+_{2})/2}\), which suggests that, whether or not there exists a scoring function in \(S^{}_{K}\), which treats the groups \(C_{2},C_{3}\) of the same rank, depends upon two things, the size of the groups and also their position on the ROC curve. Specifically, if \(_{3}-_{2}+_{2})/2)}{ (C_{3})}\), then \( S^{}_{K}\). The following lemma formalises this intuition, the proof follows via a direct generalisation of the above example.

**Lemma 2.2**.: _Let \(>0\), \(i[K]\) and define \(S^{(i,)}_{K}\) as the set of scoring functions such that, for all \(s S^{(i,)}_{K}\), one has that \( j:|_{j}-_{i}|,\ (s(i)-s(j))=(_{i}-_{j})\). There exist a \( S^{(i,)}_{K}\), \(\), such that on problem \(\), \(d_{}(,)-_{j}|)|}{p( 1-_{i})}\)._

Lemma 2.2 suggests that the for \(i[K]\) the learner must be _at least_ able to distinguish \(^{i,j}_{0}\) vs \(^{i,j}_{1}\), for all \(j:|_{i}-_{k}|_{i}\), where, \(_{i}:=\{x>0:_{i j}x|_{i}- _{j}| x<K p(1-_{i})\}\).

The following Lemma shows that \(_{i}\) is not only an upper bound on the necessary order of the confidence level around \(_{i}\) but also a lower bound, the proof of which can be found in the proof of Theorem 3.2.

**Lemma 2.3**.: _For a problem \(\), let \(s S_{K}\) be a scoring function such that the following holds, \( i[K]\),_

\[ j:|_{j}-_{i}|_{i}/4,\ (s(i)-s(j))= (_{i}-_{j})\]_then \(s S_{K}^{e}\)._

Under the conditions where \(K p>1\) and for a grid point \(i[K]\), \(_{i}\) is close to one, a certain phenomenon occurs. Specifically, when \(_{i} 1-_{i}\), for the learner to know the value of \(_{i}\) up to an error of \(_{i}\) is no longer sufficient, as in such a case, to the best of the learners knowledge, the value of \(_{i}\) may be arbitrarily close to 1, and thus have an arbitrarily large effect on the regret. With this in mind, we define \(_{i}\) as follows,

\[_{i}:=\{x>0:_{i j}x|_{i}-_{j}| x <K p(1-_{i})\}(1-_{i})\]

Note that in the case where \(K p<1\), we have \(_{i}=_{i}\). In the case where \(_{i}_{i}\), we thus define the complexity at a point \(i[K]\) as \(H_{i}^{(1)}=(_{i},_{i}+_{i}) (_{i},_{i}-_{i})}.\) If \(_{i}_{i}\), \(H_{i}^{(1)}=(_{i},_{i}+_{i})}\). For a problem \(\), the total problem complexity is then given as \(_{i[K]}H_{i}^{(1)}\). As an illustrative example, see Figure 2 for the complexity of \(i[K]\) with \(\) defined as in Scenario 1 of the experiments, see section 4.

There is a natural comparison to multi armed bandits, where the problem complexity is typically given as the summation across the individual complexity of each arm. However, in most multi armed settings the complexity of a single arm is dependent upon its distance to a single other arm, e.g. the optimal arm, whereas in our setting the complexity of a single grid point \(i[K]\) has a more complex dependency on the shape of the posterior around \(G_{i}\).

### Performance of the passive approach

At this point we can consider how a uniform sampling strategy would perform, that is, the learner simply draws a sample from each section of the grid in turn. This would be essentially a _passive approach_, in line with the classical batch setting. For a uniform/passive sampling strategy to be PAC\((,)\) one would have to draw samples until the width of the confidence interval at all points \(i\) is less than \(_{i}\). Therefore, a uniform sampling strategy, with an appropriate stopping rule, would have the following tight upper bound on its expected sampling time, up to log terms, \(cK_{i[K]}(_{i},_{i}+_{i})},\) for some absolute constants \(c,c^{}>0\). The improvement we hope to gain in the active setting is to replace the \(\) with a weighted summation across the grid. Thus, in settings where the \(_{i}\) are relatively constant across large sections of the grid, then the theoretical performance of a passive approach can be close to optimal. In contrast, in cases where a very small section of the interval is hard to rank and the rest is easy - i.e. \(K_{i[K]}(_{i},_{i}+_{i})}\) is much greater than \(_{i[K]}(_{i},_{i}+_{i})}\), a passive approach will fail. Such settings involve large \(K\) where the gaps \(_{i}\) on the majority of cells are large, with a relatively small number of cells with small gaps \(_{i}\). Incidentally, we point out that this corresponds to many situations of interest in practice (ininformation retrieval, for a specific request, the vast majority of the documents are equally irrelevant, while the ranking of a very small fraction of relevant documents is challenging; the same phenomenon is also observed in credit-risk screening). In these cases the benefit to the practitioner, will be that they quickly focus on the interesting sections of the feature space.

### Related literature in active learning

While, to the best of our knowledge, Bipartite Ranking has not yet been considered under active learning, there are several related settings. Firstly, it is important to note, that as we assume the posterior \(\) is piecewise constant on the grid of size \(K\), we can view our problems as a \(K\) armed bandit. In the case where \(K=2\) the bipartite ranking problem becomes akin to best arm identification (BAI) for the two armed bandit, also known as A/B-Testing. In BAI for the two armed bandit, the learner sequentially draws samples from two distributions \(_{A},_{B}\) with respective means \(_{A},_{B}\). Their objective is then carry out the hypothesis test, \(_{0}:=(_{A}_{B})\), \(_{1}:=(_{a}>_{B})\) in as few samples as possible. A/B Testing is considered in an active, fix confidence regime, in Kaufmann and Kalyanakrishnan (2013), wherein they prove a lower bound on the expected sampling time of any PAC\((,)\) as of the order \(}[}(_ {A},_{B})}\). Note that, in the case where \(K=2\) active-rank matches said lower bound up to logarithmic terms. In the fixed confidence regime, the BAI problem has also been generalised for larger \(K>2\) - see Garivier and Kaufmann (2016),Jamieson and Talwalkar (2016) along with the TopM problem, where the learner must output the \(M\) best arms - see Kaufmann et al. (2014), Kalyanakrishnan et al. (2012), however, for \(K>2\) both BAI and TopM problems are no longer comparable to our setting.

Aside from BAI and the TopM problem, there are several other settings in active learning that, while not directly comparable to our own, are worth mentioning. The first is active clustering. Several works have considered clustering in an online framework, see Choromanska and Monteleoni (2012), Liberty et al. (2016), Cohen-Addad et al. (2021) and Khaleghi et al. (2012). In the above works new observations from certain arms become available to the learner at each time step, however the learner does not actively choose which arms to pull and therefore the flavour of the above literature is very different to our setting. Much closer is the work of Yang et al. (2022) in which the authors consider a active clustering problem, represented as \(K\) armed \(d\) dimensional bandit, where the arms are split into \(M\) clusters. They work in a PAC\(()\) setup where their goal is to recover the entire clustering of the arms, with probability greater than \(1-\) in as few samples as possible. Comparing to our setting, if one is to view a section of the grid on which \(\) is constant as a single cluster, by retrieving the clustering of the arms one can then easily do ranking. Their results differ to our own in several key ways though. Firstly their algorithm takes the number of clusters \(M\) as a parameter, this highlights the main difference between their setting and our own. In the Bipartite ranking problem, assuming \(\) is not very small, one does not have to recover exactly all the clusters to ensure regret under the \(d_{}\) norm is less than \(\). Therefore we do not need to know the number of clusters and our algorithm must be able to exploit larger \(\) to achieve smaller stopping times. The second key difference is that the results of Yang et al. (2022) are hold only in the asymptotics, that is as \( 0\). Their algorithm employs a forced exploration phase, which ensures each arm is pulled at at least a sub linear rate. Essentially, this means that in such an asymptotic setting, _the means of the arms are known to the learner_, which naturally drastically changes the nature of their results. Extension to bounds for fixed \(>0\) would be none trivial, noted as potential future work in Yang et al. (2022), and essential if one were to compare to our confidence setting.

Also of note is active multi class classification. In Krishnamurthy et al. (2017) they consider a cost sensitive classification problem, where the learner receives input examples \(x\) and cost vectors \(c^{K}\), where \(c(y)\) is the cost of predicting label \(y\) on \(x\). For each input example received the learner is able to query a subset of labels. The objective is to then train a classifier with minimal expected loss in as few queries as possible. The results of Krishnamurthy et al. (2017) cannot be directly compared to our own, as in our setting there is no such thing as the cost of a classifier at a given point \(x\), as the cost miss ranking a section of \(\) is dependent on our ranking of the entire feature space.

## 3 Our results

### The active-rank Algorithm

Our algorithm active-rank maintains an active set of grid points across several rounds. At the beginning of each round active-rank draws a sample, uniformly, from all points of the grid in the active set and at the end of each round, eliminates points from the active set based on a specific criterion. We track the empirical mean of all samples drawn from the \(i\)th point of the grid, \([G_{i},G_{i+1})\) up to round \(t\) as \(_{i}^{t}\). At the beginning of each round \(t\), for each grid point \(i[K]\), we will maintain an upper and lower confidence bound, on \(_{i}\), which we term the UCB and LCB index respectively. At time \(t\), for each grid point \(i[K]\) and exploration parameter \((t,):_{+}\), remaining in the active set, we then define the LCB index,

\[(t,i):=q0,_{i}^{t} :_{i}^{t},q}\;,\] (3)

and the UCB index,

\[(t,i):=q_{i}^{t},1 :_{i}^{t},q}\;.\] (4)

Let \(S_{t}\) denote the active set at the beginning of round \(t\), via careful choice of exploration parameter the following Lemma holds, the proof of which can be found in Section A of the supplementary material.

**Lemma 3.1**.: _We have that, the event_

\[=_{t}_{i[S_{t}]}\{_{k} [(t,i),(t,i)]\}\;,\]

_occurs with probability greater than \(1-\)._

For \(i[K]\), time \(t\) and \(z\) define, \(U_{i,t}(z):=j S_{t}:j i,|_{i}^{t}-_ {j}^{t}| z}\). Following Lemma 2.2, our intuition would be to then remove a point \(i\) from the active set if, \(U_{i,t}(i,t)-(i,t)  p(1-_{i,t})}{\|(i,t)-(i,t)\|}\), for some well chosen constant \(c>0\). However, due to the technical difficulty of the proof, we make the following concession. For \(t>0\), let \(S_{t}\) be the list \(S\) at time \(t\). At time \(t\) let \(_{(t)}=_{i S_{t}}((t,i)-(t,i))\). Furthermore, at time \(t\) define the set of grid points,

\[_{t}:=i[K]:_{(t)} _{t}}{|U_{i,t}(6_{(t)})|} 1 (1-_{i}^{t})}\;.\] (5)

If a point exists in \(_{t}\), we remove it from the active set. Note that active-rank does not take the average of the posterior \(p\) as a parameter. Instead we show it is possible to use an estimate \(_{t}\) which updates round by round.

Elimination algorithms such as active-rank have seen wide usage in the literature for BAI, see In Paulson (1964),Mannor and Tsitsiklis (2004), Even-Dar et al. (2002) and Even-Dar et al. (2006). However, closer to our work is the Racing algorithm Kaufmann and Kalyanakrishnan (2013), designed for the TopM problem, where, as in our approach, the confidence bounds used are based on the kl divergence as opposed to Hoeffdings. Their elimination criterion, however, differs considerably to our own. For simplicity let us consider the Top1 problem, that is BAI - the following arguments can be extended in the case of TopM. The racing algorithm of Kaufmann and Kalyanakrishnan (2013) eliminates an arm \(i[K]\) from the active set, at time \(t\), when, the positive gap the lower confidence bound around the highest empirical mean and the upper confidence bound at point \(i\) is greater than \(\). However, due to the global nature of the ranking problem, in our setting, the decision to remove a point from the active set is not made based on the distance to another single point. We rather consider a condition on the local smoothness of the posterior around the point \(i\). An additional difficulty that arises here is that the local smoothness around a point can potentially depend upon points no longer in the active set and once a point is no longer in the active set, we essentially have no control of the width of its confidence interval.

**Proving** active-rank **is PAC\((,)\) and upper bounding the expected sampling time** Theorem 3.2 demonstrates that our algorithm active-rank is PAC\((,)\) and provides a problem dependent upper bound on it's expected sampling time. The proof can be found in Section A of the supplementary material. Theorem 3.2 makes no assumption on the posterior \(\), aside from it being piecewise constant on the grid of size \(K\), i.e. Assumption 2.1. For \(i[K]\), set \(H_{i}^{(2)}=_{j[K]}}(_{j}, _{j}+_{i}/8)}}(_{j},_{j} -_{i}/8)}\).

**Theorem 3.2**.: _For \(,>0,>480/(K)\), with \((t,)=c_{}(t^{2}K^{2}/)\) where \(c_{}\) is a constant depending only on \(\), on all problems \(\), on execution of active-rank, with output \(\), we have that,_

\[d_{}(,)\,\]

_with probability greater than \(1-\). Furthermore, the expected stopping time of active-rank is upper bounded by the following,_

\[c_{}^{}_{i[K]}H_{i}^{(2)}c_{}^{ }H_{i}^{(2)}K/\,\]

_where \(c_{}^{}\), \(c_{}^{}\), are constants depending only on \(\)._

Lower boundTheorem 3.3 provides a problem dependent lower bound on the expected sampling time of any PAC\((,)\) policy. The proof of Theorem 3.3 can be found in Section B of the supplementary material.

**Theorem 3.3**.: _Let \([0,1/4),0<<1-(-1/8)\) and \(\) such that \(K p<1/8\). For any PAC\((,)\) policy \(\), there exists a problem \(\) such that, for all \(i[K]\), \(_{i}_{i}/2\) where \(_{i}\) is the gap of the \(i\)th grid point on problem \(\), where the expected stopping time of policy \(\) on problem \(\) is bounded as follows,_

\[_{,}_{}^{} c^{ }_{i[K]}_{i}^{(1)}\,\]

_where \(c^{}>0\) is an absolute constant and \(_{i}^{(1)}\) is the complexity of point \(i\) on problem \(\). Furthermore we have that,_

\[_{i[K]}_{i}^{(1)} c_{i[K]}H_{i}^{(1)}\,\]

_where \(c>0\) is an absolute constant._

The proof of Theorem 3.3 follows from a novel application of a Fano type inequality on a well chosen set of problems.

Gap between upper and lower boundThere are essentially two components in the gap between the bounds of Theorems, 3.3 and 3.2. The first is the additional logarithmic dependency upon \(K\) present in our upper bound. Despite being logarithmic this dependence is potentially significant, as in practical situations, the size of grid needed, for the assumption that the posterior \(\) is piecewise constant, may be very large. The second component, in the gap between upper and lower bounds is the difference in the \(H_{i}^{(1)}\) and \(H_{i}^{(2)}\) terms. The reason \(H_{i}^{(2)}\) appears in Theorem 3.2 is that, the decision to remove a point \(i[K]\) from our active set is made based on the minimum width of confidence interval across the entire grid, \(_{(t)}\) as opposed to the local width at \(i\), \((i,t)-(i,t)\), see Equation (5). As we are dealing with Bernoulli distributions and kl divergence based confidence bounds, for a fixed number of samples, points close to zero or one will have tighter confidence boundsand thus may be sampled more than is necessary. If one were to assume that the posterior \(\) exists solely in the interval \([,1-]\) for some \(>0\), then for all \(i[K]\), \(H_{i}^{(2)}\) and \(H_{i}^{(1)}\) will be with a constant factor of each other, with that constant depending on \(\).

In authors opinion, both the logarithmic dependency on \(K\) and usage of \(_{(t)}\) may be removed. However, this would require several non trivial modifications to the proof of Theorem 3.2, see the discussion in Section A of the supplementary material for details. As, to the best of our knowledge, this is the first work to consider the bipartite ranking problem in an active learning setting, we present Theorem 3.2 as it stands and leave the aforementioned improvements for future works.

## 4 Experiments

In this section we discuss practical cases based on synthetic data. For all experiments \(\) is fixed at \(0.01\) and the constants used are smaller than their theoretical counterparts, which are typically overestimated, furthermore \(\) is calculated with all previous samples. As represented in Figure 3, each cell \(i\) is assigned a level value \(_{i}\) so that \(\) follows the Assumption 2.1. Without loss of generality, we assume that \(\) can be described as an increasing family \((_{i})_{i[K]}\). Our study scenarios are then as follows:

* Scenario 1: \((_{i})_{i[K]}=(0,0.28,0.3,0.38)\) and \(K=16\);
* Scenario 2: \((_{i})_{i[K]}=(0.8((i-1)/K)^{4}+0.1)_{i[K]}\) and \(K=64\);
* Scenario 3: \((_{i})_{i[K]}\) is sub-sampled (with replacement) of \(((i-1)/K)^{4})_{i}\) and \(K=64\)
* Scenario 4: \(_{i}=0.8((i-1)/K)+0.1\, i[K]\{7,8\}\) with \(_{7}=0.8(6/K)+0.3\), \(_{8}=0.8(7/K)-0.1\) and \(K=16\)

The objective of these scenarios is to evaluate the capacity of the algorithm on different cases. As shown in Figure 3, scenarios 1 and 3 will have variable jumps and cell sizes.

Competing algorithmsTo our knowledge there is no algorithm dealing with the active learning for bipartite ranking problem, we thus compare to the following naive approaches. **Passive rank**: each new point \(a_{t}\) is drawn uniformly on \(\). **Naive rank**: each new point \(a_{t}\) is sampled in \(P_{i_{t}}\) s.t. \(i_{t}:=*{arg\,max}_{i[K]}(t,i)- (t,i)\), this algorithm reduces the bias in an undifferentiated way without considering the problem as global (requiring peer-to-peer comparison). **Active classification**: for this algorithm we consider binary classification with threshold \(0.5\). The set of active cells \(S\), as defined in Algorithm 1 is then \(S_{t}=\{i[K];0.5[_{i}^{t}-(t,i);_{i }^{t}+(t,i)]\}\{i;}( |0.5-_{i}^{t}|)\}\). As the competing algorithms do not output a stopping time, for a single \(\), algorithm active-rank is run across several values of \(\), following a geometric sequence of common ratio \(0.99\) and initial value \(1\). The values of \(\) then plotted against the respective stopping times of active-rank.

Interpretation of resultsOn scenario 1, the simplest case, and to an extent scenario 2, active-rank and passive suffer near identical regret for larger sample sizes. On all other scenarios active-rank outperforms all competitors. The fact that active-rank does not reach extremely small values of regret suggests that analysis for much higher sample size may be interesting, however this creates issues in computation time, the same goes for larger \(K\). Also, the uniform

Figure 3: Different scenarios chosen for the experiments

approach still performs relatively well, and appears difficult to fool. Some more work may be needed to find a setting in which uniform sampling suffers considerably.

## 5 Conclusion

To the best of our knowledge we have developed the first rigorous framework for the active bipartite ranking problem and our algorithm, active-rank, is the first to tackle said problem. Our upper bound on performance of active-rank matches our lower bound up to logarithmic terms, in the case where the posterior \(\) is not very close to 0 or 1 at any point. As well as theoretical guarantees we have demonstrated good practical performance of active-rank, on synthetic data, in various settings. We conclude with some perspectives for future research.

An obvious path for future research, is to replace the Assumption 2.1 with a smoothness assumption on the posterior \(\), e.g. a Holder condition. The setting would then be equivalent to a continuous armed bandit as opposed to a finite armed bandit. Assuming the learner has knowledge of the Holder coefficient, a standard approach in continuous armed bandits is to first discretise and then apply classic techniques from finite armed bandits, carefully choosing the discretisation level to balance the discretisation error and classical regret. It is of our opinion that such an approach would not be sufficient in our case. We conjecture that to achieve optimal or near optimal performance the learner must vary the level of discretisation across the feature space, based on the flatness of the posterior function \(\) and placement on the ROC curve. Also, under such a Holder condition, the extension to higher dimensions would no longer be immediate.