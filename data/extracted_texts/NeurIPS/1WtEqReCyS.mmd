# Multilingual Diversity Improves Vision-Language Representations

Thao Nguyen\({}^{1}\)1 &Matthew Wallingford\({}^{1}\) &Sebastin Santy\({}^{1}\) &Wei-Chiu Ma\({}^{1,2}\)

&Sewoong Oh\({}^{1}\) &Ludwig Schmidt\({}^{1}\) &Pang Wei Koh\({}^{1,2}\) &Ranjay Krishna\({}^{1,2}\)

\({}^{1}\)University of Washington \({}^{2}\)Allen Institute for Artificial Intelligence

###### Abstract

Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large.

## 1 Introduction

Today, the predominant pre-training paradigm for vision-language models relies on large quantities of image-text pairs scraped from the web . As raw web data contains a significant amount of noise, automatic data filtering approaches are designed to curate a high-quality subset and maximize the performance of a model trained on this subset on standard computer vision benchmarks (e.g., ImageNet). However, these benchmarks typically only evaluate in English, and many of them have been shown to be geographically biased: for instance, ImageNet images are mostly sourced from North America and Western Europe . Consequently, it is possible that we are designing data curation algorithms that propagate a monolingual bias, i.e., filtered datasets are increasingly dominated by English image-text pairs. In fact, a lot of highly cited work--including CLIP , ALIGN  and BASIC --relies exclusively on English data. Using more multilingual data for training is often only a deliberate design decision when non-English tasks are involved .

Multilingual data enriches any monolingual data distribution; multilingual data brings attention to culturally salient concepts and introduces new perspectives and annotations for the same visual category . As illustrated in Figure 1, there are certain native concepts, e.g. 'kiji' (the national bird of Japan), that are more likely to be conveyed in Japanese (non-English) captions compared to English ones. Even in the case of a common everyday object ('stove'), the non-English and English images look very different. Despite the diversity present in multilingual data, it is disproportionately excluded from existing large-scale pre-training corpora.

In this paper, we investigate the counterfactual: _can we improve on English vision tasks by diversifying the cultural and linguistic backgrounds of the training data?_

Our investigation is motivated by a dichotomy: English image-text pairs constitute a minority of any random web crawl (in our estimate, one-third); yet, they form a majority in popular pre-training datasets such as LAION-5B , DataComp , and DFN .

It is common for web-scraped corpora to remove "low-quality" data by using a high-performing model (e.g., OpenAI CLIP) to compute image-text alignment and rank the raw data samples. However, this process often disproportionately favors English data if the filtering model also has an English bias . In addition to discarding many potentially useful non-English image-text pairs, this can also negatively impact the geographical and cultural representation of the resulting dataset, and consequently, the model's performance on certain underrepresented populations .

Our key observation is that the diversity present in multilingual data can be confounded by the language the data is in, making it difficult to observe the empirical benefits of using such data in model training. To offer a more systematic study of the effectiveness of multilingual data--in contrast to English-only or English-dominated datasets--we fix the language medium, translate all captions from DataComp's 128M-sample web crawl  to English with an advanced translation model. We then re-filter the data pool and train a CLIP model on this translated multilingual data. We focus on two types of evaluations: (i) on standard English vision tasks including ImageNet, MSCOCO and Flickr retrieval, and (ii) on geographically diverse images, e.g. from GeoDE , which contains images of common objects across different geographical locations. We acknowledge that translation can sometimes be too literal, subject to losing the intent and richness of the original

Figure 1: **Multilingual image-text data adds diversity to the English data distribution in various, significant ways** (a) We show some examples of culturally salient concepts that would not exist in ”high-quality” English data (as determined by CLIP score), such as “bamboo steamer”, “kiji” (the national bird of Japan) and yalt (a traditional architecture style for Turkish waterside houses) (b) Even for a common everyday object (‘stove”), non-English and English images portray very different visual representations.

phrasing. Nevertheless, we hope findings from our work provide a starting point for studying how to leverage the diversity of multilingual data more effectively.

Our contributions are as follows:

* We demonstrate that with translation, non-English data does benefit English vision tasks. In particular, training on more samples of non-English origins leads to better performance on ImageNet, ImageNet distribution shifts and image-English-text retrieval. On the DataComp benchmark with a fixed compute budget, our best-performing approach that leverages translated multilingual captions outperforms training on just filtered raw captions by 2.0% on ImageNet and 1.1 percentage points on average across 38 tasks. When training for longer (which mimics the number of epochs large-scale multimodal models are often trained for), these performance gaps increase to 4.2% and 2.1 percentage points respectively.
* On a geographically diverse task such as GeoDE, training on translated multilingual data leads to 4.2% boost in accuracy on average compared to training on filtered raw data, with performance improvement observed for all regions, especially for Africa where the increase is 5.5%.
* We analyze in detail the differences between English and (translated) non-English image-text pairs. We quantitatively show that they capture distinct distributions, both in text and image space, even after they are converted to the same language medium. Consequently, it is beneficial to combine high-quality data from both sources as much as possible, since they are inherently complementary.

In summary, despite the abundance of "sufficiently useful" English data, existing data curation techniques can always do better in the data diversity axis by being more deliberate about including data from other language and cultural backgrounds. This way of enhancing diversity in turn leads to a better vision-language model _in general_, offering performance benefits beyond non-English vision tasks or tasks involving geographically diverse images. We will release the raw captions and the corresponding English translations for the 128M image-text pairs used in our experiments.

## 2 Related Work

Existing data collection and filtering approaches induce Western bias in downstream datasets and models; benchmarks that seek to capture this bias still receive relatively little attention. Consequently, despite evidence showing cultural and geographical limitations in popular vision datasets, the use of multilingual data is mostly intended for pre-training and fine-tuning multimodal models to do well on non-English tasks. Our work seeks to include more image-text pairs of non-English origins in the pre-training dataset, and shows that this process can improve performance, even on English-centric vision tasks.

Western bias of existing models and datasetsSeveral papers have studied biases in popular datasets, especially biases that correlate with culture and geographic locations. Notably, Shankar et al.  find that ImageNet and OpenImages exhibit substantial US-centric and eurocentric representation bias. In NLP, Santy et al.  find that existing datasets align predominantly with Western and White populations. It is not only the data collection process that leads to a Western bias, but also the data preprocessing pipeline. For instance, automated data filtering with scores output by a model, e.g., OpenAI CLIP, has been commonly adopted as a way to discard low-quality web-crawled samples. Little is known about the potential biases induced by this approach. Hong et al.  recently show that CLIP score filter is more likely to include data related to Western countries compared to that of non-Western countries. In  (Figure 24), the authors offer evidence that CLIP filtering implicitly performs some English filtering, as the top CLIP score examples are increasingly dominated by English image-text pairs. Consequently, all these dataset biases translate to performance disparity, as demonstrated by existing work showing that the accuracy of vision systems drops significantly on non-Western inputs [10; 49; 38; 36], or low-resource languages .

Improving the availability of non-English data in multimodal datasetsTranslation has been a popular technique to address the limited availability of large-scale and high-quality non-English data in training and evaluation [45; 47; 6; 32; 11; 2; 16]. In addition to translating English captions into the language of interest, previous work also uses a curated list of common words in the native language to scrape image-text pairs from the web [26; 17]. COCO-CN  extends the MSCOCO dataset  with manually written Chinese captions.

Most closely related to our setup is the LAION-Translated dataset , which translates 3B samples of LAION-5B from many languages into English using the M2M100 model . Compared to this dataset construction, we (i) use a more advanced translation model, NLLB , that covers twice as many languages, (ii) work with mostly raw data while LAION was heavily filtered and thus could contain a biased representation of multilingual data. To the best of our knowledge, no existing work has experimented with the LAION-Translated dataset.

Adapting CLIP post-training for multilingual tasksGeigle et al.  translate high-quality English data into 95 languages, and use these translated samples to re-align an image encoder previously trained on English data to a multilingual language model. Similarly, Chen et al.  propose re-training OpenAI CLIP on a mix of Chinese and English data to enhance its multilingual representation. In , the authors explore adaptation without any image data, solely fine-tuning the text encoder with English captions from MSCOCO, Google Conceptual Captions, and VizWiz translated to other languages. Visheratin  replace the text encoder of OpenAI CLIP with the text encoder from the NLLB model, and fine-tune the new model on multilingual image-text pairs obtained from translating LAION-COCO's English captions  into 200 languages. In contrast to these papers that employ multilingual data for the purpose of adapting to non-English tasks, we focus on using multilingual data to do better on common vision tasks that are in English.

Using multilingual data significantly enhances data diversityOur study is partly inspired by findings from Ye et al. , who show that multilingual synthetic captions obtained from existing image captioning systems provide higher semantic coverage than monolingual ones, over 3.6K images. Our experiments instead use raw web-crawled data and explore the performance benefits of embracing cultural and linguistic diversity in (mostly) human-generated captions _at scale_.

## 3 Experimental Setup

Given a starting pool of raw image-text pairs scraped from the web, many of which contain non-English captions, we experiment with ways to preprocess and filter this pool into a high-quality dataset. The quality of the dataset is measured by the zero-shot performance of a CLIP model trained on it from scratch.

DataWe experiment with the medium pool of the DataComp benchmark , which consists of 128M image-text pairs randomly sampled from Common Crawl dumps between 2014 and 2022, and deduplicated. Unlike other heavily filtered corpora such as LAION , DataComp applies minimal data preprocessing, involving only NSFW filtering, deduplication of evaluation sets, and face blurring. This allows the candidate pool to stay close to the natural distribution of the raw web data as much as possible, in addition to enabling maximum flexibility in dataset design.

Translation modelTo detect language and translate the raw captions from DataComp into English, we use the No Language Left Behind (NLLB) translation model , which is considered state-of-the-art. NLLB is the first to translate across 200 languages, including low-resource ones that are not currently supported by common translation tools. We use the 600M-parameter model publicly available on HuggingFace to allow for fast inference on our large data corpus. All 128M captions from DataComp are translated to English; examples could be found in Appendix A. We provide some quantitative analysis of the translation quality in Appendix C.

TrainingAfter translating the captions of all samples in the raw data pool, we filter them based on cosine similarity between image and text embeddings. We experiment with using OpenAI CLIP-ViT-L14  and the _public_ Data Filtering Network (DFN) from  to obtain the embeddings, and subsequently, the cosine similarities. The DFN, specifically designed to filter data for subsequent model training, was trained on three public datasets deemed as high-quality--Conceptual Caption 12M , Conceptual Captions 3M , and Shutterstock 15M . We find that indeed the public DFN is better at data filtering compared to OpenAI CLIP, as measured by the performance of CLIP trained on the corresponding filtered datasets (see Appendices E and G).

We pretrain a CLIP model  on each filtered subset with ViT-B/32 as the image encoder, and follow DataComp's hyperparameters; details can be found in Appendix B. Unless specified otherwise, all models are trained for the same compute budget (128M steps) as determined by DataComp. For some select baselines, we also experiment with training for 10\(\) longer. The fixed architecture, compute and hyperparameter setup allow us to isolate data quality as the main factor influencing performance.

EvaluationWe perform zero-shot evaluation of trained CLIP models using the 38 tasks from DataComp. These tasks involve recognition and classification of a wide range of domains (e.g., texture, scene, metastatic tissue, etc.) in addition to image-text retrieval and commonsense association. Among them, we pay particular attention to commonly cited metrics such as ImageNet accuracy, ImageNet distribution shift accuracy - a proxy for natural robustness, and retrieval performance. ImageNet shifts include ImageNet-V2 , ImageNet Sketch , ImageNet-A , ImageNet-O , ImageNet-R  and ObjectNet . Retrieval score is the average of the performance on Flickr30K , MSCOCO  and WinoGAViL . Throughout the paper we also highlight GeoDE worst-region performance --a task that involves geographically diverse images--to demonstrate the added benefits of geographical inclusivity that training on more (translated) multilingual captions offers.

## 4 Impacts of using (translated) multilingual captions on standard vision tasks

We explore training on each caption distribution separately, as well as combining them. Below we describe the baselines from Table 1 in more detail:

* _Filtered raw captions:_ As mentioned in Section 3, we use the _public_ DFN from  by default to filter the starting pool (128M samples). Given the images and the corresponding web-crawled captions, we experiment with varying the filtering threshold to keep top x% of the pool based on DFN score. In Table 1, we only report the best average performance obtainable after the filtering threshold has been tuned, and the resulting dataset size. Refer to Appendix G for the full results.
* _Filtered translated captions:_ Similar to the approach above, we tune the filtering threshold, but using DFN score between an image and the English translation of the original web-crawled caption.
* _Filtered English-only captions:_ Similar to "Filtered raw captions" baseline, here we also tune the filtering threshold to keep only a subset of the pool with the highest DFN scores, but with an additional constraint of only filtering from samples with web-crawled captions already in English.

  
**Baseline name** &  **Dataset** \\ **size** \\  &  **ImageNet** \\ **shifts** \\  &  **Retrieval** \\ **GeoDE** \\ **over 38 tasks** \\  & 
 **Average** \\ **over 38 tasks** \\  \\    \\  Filtered raw captions & 25.6M & 0.316 & 0.260 & 0.282 & 0.688 & 0.350 \\ Filtered raw captions, replaced with translated captions & 25.6M & 0.304 & 0.252 & 0.268 & 0.668 & 0.331 \\ Filtered translated captions & 25.6M & 0.329 & 0.275 & 0.296 & 0.709 & 0.359 \\ Filtered English-only captions & 16.0M & 0.283 & 0.236 & 0.278 & 0.666 & 0.327 \\ Filtered raw captions \(\) Filtered translated captions & 34.2M & 0.329 & 0.271 & 0.298 & 0.720 & **0.364** \\ Filtered raw captions \& Filtered translated captions & 51.2M & **0.336** & **0.280** & **0.301** & **0.725** & 0.361 \\   \\  Filtered raw captions & 38.4M & 0.414 & 0.340 & 0.344 & 0.742 & 0.414 \\ Filtered translated captions & 38.4M & 0.427 & 0.347 & 0.352 & 0.771 & 0.414 \\ Filtered raw captions \& Filtered translated captions & 34.2M & 0.441 & 0.359 & 0.353 & 0.775 & 0.427 \\ Filtered raw captions \& Filtered translated captions & 51.2M & **0.456** & **0.369** & **0.371** & **0.776** & **0.435** \\   

Table 1: **On the DataComp benchmark, training on translated captions outperforms training on raw captions across a range of metrics; using both types of captions yields even more performance gains.** We report the performance of select baselines on the DataComp benchmark ; all baselines are trained for the same number of steps as specified. Here the filtering threshold (and thus the resulting dataset size) has been tuned for each baseline and we only show the filtered subset that yields the highest average accuracy. We find that with the same filtering method (i.e., using DFN score), training on translated captions (“Filtered translated captions”) is more effective than training on raw captions (“Filtered raw captions”) as seen from higher performance on ImageNet, ImageNet distribution shifts, retrieval, GeoDE (worst-region accuracy) and on average across 38 tasks. Combining both sources of captions leads to the best performance. Appendix G contains the full results.

* _Filtered raw captions, replaced with translated captions:_ Given samples from "Filtered raw captions" (i.e. again, based on the cosine similarity score between image and original web text), we keep the images selected and replace the raw captions with the corresponding English translations.
* _Filtered raw captions \(\) Filtered translated captions:_ We combine image-text pairs from "Filtered raw captions" and "Filtered translated captions" subsets uncovered above. However, these subsets have about two-thirds of their images in common (see Appendix D). For such images, we only include one copy in the final training set and use English-translated caption by default. For the rest of the images in "Filtered raw captions" that do not appear in "Filtered translated captions", we include them in the training set with the corresponding original captions (which could be non-English).
* one copy with the original web caption and one copy with the English-translated caption.

### Overall performance trends

**Combining high-quality raw and translated captions offers the best performance** We find that using _both_ sources of captions, and image data--since top (image, raw text) pairs and top (image, translated text) pairs only have two-thirds of the images in common--leads to the best performance (bolded entries of Table 1). This approach surpasses training on only high-quality raw data by 2% on ImageNet, ImageNet shifts and retrieval, and improves GeoDE worst-region performance by 3.7%. We note that this is not simply due to having more unique image-text samples, as filtered subsets of similar sizes but using a single source of captions (e.g., top 40% raw captions totalling 51.2M samples) yields significantly lower performance (Appendix G).

**Using only translated multilingual captions is still better than using only raw captions** Zooming in on "Filtered translated captions" and "Filtered raw captions" baselines, we find that the former outperforms the latter on many standard metrics (ImageNet, ImageNet distribution shifts, image-text retrieval). This is unexpected in light of prior work showing that ImageNet exhibits strong amerocentric and euocentric bias , with images from America and Great Britain taking up 53% of the dataset.

### Ablations

We perform more ablation studies to disentangle the reasons for the performance gains from using high-quality translated captions, as well as to verify that the gains are robust.

**The performance gain from using translated captions is not simply due to converting all text data to a common language medium** Given image-text pairs from the "Filtered raw captions" subset, we replace the web-crawled captions with the corresponding English translations ("Filtered raw captions, replaced with translated captions"). This intervention on only the captions leads to performance drop across the board. We hypothesize that this is due to noise in the translation process, as (i) many web captions are formed by stringing together short, ungrammatical phrases and thus are "out-of-distribution" for the NLLB translation model, (ii) web captions may contain multiple languages in the same sentence, thereby leading to noisy language detection and translation.

**Re-filtering data after translation is also necessary due to significant changes in the data ranking** Besides noisy artifacts introduced by translation, the process also changes the image-text cosine

Figure 2: **Filtering with translated captions allows substantially more (translated) non-English samples to be included in the final training set.** While English data only makes up about one-third of the raw web crawl, it dominates the top 20% of the pool, selected based on DFN score between image and _raw_ caption. With translation, English-translated non-English captions now make up the majority of the “high-quality” data and thus are more likely to be selected for training.

similarity score and thus the quality ranking of the data samples in the pool. More specifically, we find that while "Filtered raw captions" is dominated by English samples, (translated) non-English samples make up the majority of "Filtered translated captions" (Figure 2). These two filtered subsets only share about two-thirds of the images in common, see Appendix D for more details. Therefore, by changing the caption distribution, we are also inducing changes to the image distribution that the best-performing model would see.

**The benefits of training with translated multilingual captions are consistent across data filtering networks** As alluded to in Section 3, we also explore using cosine similarities output by OpenAI CLIP-ViT-L/14  for data filtering. The full results for this ablation can be found in Appendix E. Similar to the previous observations, we find that using filtered translated captions yields better performance than using filtered raw captions.

**The performance benefits of using translated data persists with much longer training duration** We also experiment with training for 10\(\) more steps (i.e., 1.28B samples seen) as this is more in line with the number of epochs typical vision-language models are often trained on (e.g., OpenAI CLIP models were trained on 400M datapoints for 32 epochs ). When using either the raw caption or the translated caption distribution, setting the filtering threshold to top 30% of the pool works best. Even though the two filtered datasets now yield the same average accuracy, training on high-quality translated captions still offers significant advantages when it comes to ImageNet, ImageNet shifts, retrieval and GeoDE. Combining high-quality data from both sources of captions continues to be the best performing approach, giving 4.2% improvement on ImageNet and 2.1 percentage points improvement on average, compared to just training on filtered raw captions. Results for more baselines can be found in Appendix H.

### Individual task analysis

After observing improvement across different metrics from using more (translated) multilingual captions, in this section we break down the performance changes for each of the 38 tasks in the DataComp benchmark. The base model for comparison is CLIP trained on top 30% image-text pairs filtered from the raw data pool, and the new improved model is the one trained on top 30% image-text pairs after the same pool has been all translated to English. Both models are trained for 128M steps. Averaged across 38 evaluation tasks, the latter yields a 1.5 percentage points improvement. The biggest gains come from Flickr retrieval, fairness (GeoDE, Dollar Street) and remote sensing (EuroSAT, RESISC45) tasks (Figure 3).

Figure 3: **With the same degree of filtering, training with (image, translated caption) pairs improves performance on 28 out of 38 tasks compared to training with (image, raw caption) pairs, including on ImageNet distribution shifts, retrieval, and tasks with geographically diverse inputs. We compare performance on each task of the DataComp benchmark between training with raw captions and training with translated captions. Both datasets have been filtered with image-text cosine similarities output by the public DFN  to select the top 30% examples. We find that using translated captions leads to 1.5 percentage points improvement on average across 38 tasks. We highlight the performance changes on ImageNet distribution shifts (red), retrieval (blue) and fairness-related tasks (dark yellow).**

## 5 Understanding the differences between English and (translated) non-English data

Given that using more image-text pairs of non-English origins in the training set offers significant benefits on most vision tasks, including those that are English-centric, we seek to further understand the various ways that non-English data complements and improves the diversity of English data, in both image and text space.

### Image distribution

As a proxy for capturing image distribution differences, we train simple classifiers--a Support Vector Machine (SVM) on CLIP embeddings and a ResNet-50--to distinguish images with English captions from those with non-English captions. We randomly sample 100K images from each distribution for training and 10K for testing. We only use images from the top 20% of the candidate pool (based on DFN cosine similarity score), to ensure that (i) these are the samples that the best-performing CLIP models are eventually trained on, (ii) images are of sufficient quality, to the extent that they have fitting captions accompanying them.

We note that this classification task is non-trivial for a number of reasons:

Figure 4: **On GeoDE, using filtered translated captions leads to improvements across _all_ regions compared to using filtered raw captions, with Africa observing the biggest gain. We break down the GeoDE performance by region and compare training on top 30% translated captions to training on top 30% raw captions. On average, classification accuracy improves by 4.2%, and the improvement applies to all regions in the dataset, especially Africa where the accuracy gain is the biggest at 5.5%.**

Figure 5: **Visualizations of what an SVM deems typical of images with English captions and those with non-English captions. We show examples of easy-to-classify images in our English versus non-English data classification task. Besides the product logo and text in some images that are suggestive of the language distribution, the image content mostly depicts common scenes and objects.*** Many images are duplicated across the web, i.e., after DataComp performs image deduplication it is possible for these images to appear with either English captions or non-English captions in our data pool.
* The language detection model is not perfect and web-crawled captions may contain more than one language in the same sentence.
* Images with non-English captions contain many sub-distributions of images, some of which may overlap with the distribution of images with English captions (e.g., eurocentric data).

Despite these challenges, our simple classifiers achieve 67% accuracy on the binary classification task, significantly better than random chance performance. In Figure 5, we show some examples of what the SVM deems easy to classify. Overall this experiment suggests that the distribution of images with non-English captions is sufficiently distinct from that of images with English captions. Therefore, not training on more of the former means we are missing out on a considerable amount of visual information that can only be found in a separate part of the web.

### Text distribution

In the text space, we leverage MAUVE score  to quantify the differences between English captions and non-English captions that have been translated to English. MAUVE was originally designed to measure the gap between machine- and human-generated texts. The metric computes KL divergences in a quantized, low-dimensional space after embedding text samples from each distribution with a language model (by default, GPT-2). The output score ranges between 0 and 1 and the higher it is, the more similar the text distributions are. Similar to our analysis in the image space, we only use caption samples from the top 20% of the candidate pool (based on DFN score).

In Table 2, as a sanity check, we randomly sample two disjoint sets of 10K captions from the same text distribution (e.g., non-English captions having been translated to English with the NLLB translation model, _or_ English captions having been passed through the same model). We find that the two sets indeed exhibit high MAUVE scores (above 0.95). When comparing raw English captions to English captions that have been passed through the NLLB model (i.e., "translated English"), we find that the MAUVE score decreases slightly (0.890), indicating that the translation process introduces some artifacts making English-translated English text look somewhat different from raw English text.

When comparing raw English texts and non-English texts, both having been passed through the translation model and thus undergone the same "preprocessing" (i.e., English translation), the resulting MAUVE score is relatively low (0.616). This signals that independent of differences in language, what is discussed in English captions and non-English captions differs in many ways. We should therefore leverage both sources of text information as much as possible for training.

## 6 Discussion

LimitationsWe fix the data filtering method to be based on image-text cosine similarity output by a trained model, and study the impact of selecting training data based on different caption distributions. We show that the advantages of using translated multilingual data are robust to the choice of the filtering network. However, our best-performing baseline is currently not state-of-the-art on the DataComp benchmark . It remains an open question whether the performance benefits of our method persist with other score metrics, e.g. hyperbolic entailment  - currently the best method for DataComp's medium scale, or other filtering methods that are used jointly with CLIP score, e.g. T-MARS  which also removes text-spotting images with limited visual information.

 
**Text distributions** & **MAUVE score** \\  Translated non-English vs. & 0.964 \(\) 0.005 \\ Translated non-English & \\  Translated English vs. & 0.957 \(\) 0.005 \\ Translated English & \\  English vs. & 0.890 \(\) 0.004 \\ Translated English & \\  Translated English vs. & 0.616 \(\) 0.010 \\ Translated non-English & \\  English vs. & 0.449 \(\) 0.008 \\ Translated non-English & \\  

Table 2: **There exists a substantial gap between the distribution of English captions and that of non-English captions, even when we apply translation to both, suggesting that they capture different contents.** We use MAUVE score  to measure the difference between English captions and (translated) non-English captions in the training set. We find that (i) translation indeed introduces some artifacts and changes what “English” texts may look like, (ii) the English text distribution is remarkably different from the non-English one, even after they are converted to the same medium with translation. All scores are averaged over 3 randomly sampled sets of 10K captions.

Besides, we acknowledge that translation can introduce artifacts and reduce the richness of expressions in the original languages. Prior work has shown that translated sentences are less effective compared to manually-written sentences as sources of training data for a vision task, e.g. image captioning . Our work mainly leverages translation as a way to convert all image-text pairs to the same medium, and remove confounding impacts of language in data selection and model training.

ConclusionIn this work, we bring all web-crawled image-text pairs into a common language medium via English translation, and systematically study the empirical benefits of using non-English data with respect to standard computer vision tasks (that are in English). By including significantly more (translated) multilingual data in the filtered training set, the improved cultural and linguistic diversity in turn leads to substantial gains across all major metrics--ImageNet, distribution shift robustness, retrieval capabilities and average performance across 38 tasks--even if some of these metrics have been shown to overfit to English. We also find that despite being translated to the same language, English and non-English data distributions are still distinct from each other.

Future workThis work motivates future studies into data curation techniques that directly improves the diversity of data origins. Another interesting direction of exploration is adapting trained CLIP models from this paper for multilingual benchmarks, such as by re-training the text encoder (that has only been trained on English and English-translated captions) with the technique proposed in . We hypothesize that text adaptation alone is sufficient for our models to perform competitively on non-English tasks, owing to the presence of significantly more multilingual and multicultural images in our pre-training dataset.

Broader impactWhile most studies have looked into non-English data with the goal of increasing societal representation and subsequently improving performance on under-served populations or tasks, we observe that non-English data can actually help enhance model capabilities _as a whole_ (including on standard English benchmarks). This suggests that diverse representation in training data, e.g. as measured by cultural and linguistic backgrounds, should be a deliberate design decision in the data curation process, instead of existing only as a byproduct of the preprocessing pipeline or out of societal considerations.