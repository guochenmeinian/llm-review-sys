# From One to Zero: Causal Zero-Shot Neural Architecture Search by Intrinsic One-Shot Interventional Information

From One to Zero: Causal Zero-Shot Neural Architecture Search by Intrinsic One-Shot Interventional Information

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

"Zero-shot" neural architecture search (ZNAS) is key to achieving real-time neural architecture search. ZNAS comes from "one-shot" neural architecture search but searches in a weight-agnostic supernet and consequently largely reduce the search cost. However, the weight parameters are agnostic in the zero-shot NAS and none of the previous methods try to explain it. We question whether there exists a way to unify the one-shot and zero-shot experiences for interpreting the agnostic weight messages. To answer this question, we propose a causal definition for "zero-shot NAS" and facilitate it with interventional data from "one-shot" knowledge. The experiments on the standard NAS-bench-201 and CIFAR-10 benchmarks demonstrate a breakthrough of search cost which requires merely **8 GPU seconds on CIFAR-10** while maintaining competitive precision.

## 1 Introduction

Neural architecture search has been an interesting topic in the AutoML community . Traditional methods search by training the distinct neural architecture iteratively  whose training cost is huge. One-shot model cleverly use a supernet to merge all the singular neural architectures into one and consequently, the waste of search time is largely saved . Further, the gradient-based one-shot method  is proposed which acquires robust results on NASNet . Though the one-shot model largely reduces the search cost, it still suffers from a weight-sharing problem, and especially, gradient-based approaches cause degenerate architectures . The work  gives theoretical proof for this and subtly uses a progressive tuning metric to discretize the one-shot supernet iteratively which gets awesome neural architectures. However, it still gets degenerate architectures with different training settings.

The brilliant work  from Google Brain gives a hint for searching neural networks without tuning the parameters. "To produce architectures that themselves encode solutions, the importance of weights must be minimized". In this manner, a zero-shot neural architecture search (ZNAS) is born. The work  firsts propose the idea of ZNAS to be "it does not optimize network parameters during search ". From a one-shot perspective, the "zero-shot" is given credit by "one-shot" where single neural architectures are supposed to be selected from the weight-agnostic supernet . Considering causal weight messages, the "zero-shot" select neural architecture with the minimum impact of any weight parameter . Thus a causal definition is supposed to be that the weight messages are multi-environmentally distributed. Compared to one-shot NAS, zero-shot NAS gets imperfect weight messages due to random initialization and searching without training [10; 2].

A training-free approach is first proposed by the work . Different from the previous zero-shot model , the work  samples well-trained architectures and get validation accuracy to train the statistical proxy before it searches. The work  follows the way of the previous work  and uses the DARTS search space to conduct zero-shot NAS on CIFAR-10 and ImagNet in a training-free manner. However, the number of samples directly decides the belief of the final precision. The "well-trained" architectures might not be "perfectly-trained" in different training settings.

Zero-shot NAS learns the representation of neural architectures to get the best one. Consistently compared to one-shot NAS methods, zeros-shot NAS methods ignore the weight information. By merely measuring the architectural expressivity, they overlooked the impact of weights as a necessary assessment element. From a one-shot NAS perspective, architectural information can be represented by a list of neuron representations . The message of training weights \(\) supports the neuron's representation [15; 12; 25]. Because the structural dependencies of shared (mutual) messages across neurons are all agnostic , in the zero-shot neural architecture search, the neuron's representation is harder to interpret due to the random messages. What is worse, the uninterpretability might result in large bias and variances because the imprecise observational data might be misleading. Finally, it will lead the search to get degenerate architectures through the process of accumulating errors.

We first propose to interpret the zero-shot NAS in a causal-representation-learning setting. According to the weight-agnostic setting, we formulate the zero-shot NAS as a novel framework for imperfect-information NAS. The structural information of zero-shot NAS is interpreted by impact with the latent factors. As a consequence, intrinsic high-level interventional data acquired by one-shot NAS is properly adopted to refine the imperfectness. Moreover, we reformulate the causality by game theory and interpret the imperfect-information NAS as imperfect information game \(\). Extensive experiments on various benchmark datasets including CIFAR-10, NAS-Bench-201, and ImageNet have shown the super search efficiency (\(10000\) faster than DARTS) of our methods. In this work, our main contributions are as follows:

* We propose that the causal zero-shot NAS is to learn the neuron's representation with latent factors in observationally imperfect messages.
* We theoretically demonstrate the validation information of either a neuron or a neuron ensemble obeys a Gaussian distribution given a Gaussian input.
* The proposed method uses high-level interventional data from one-shot NAS to facilitating zero-shot NAS to solve the imperfectness.
* Our method sets the new state-of-the-art in zero-shot NAS of search cost (8 GPU seconds) while maintaining comparable test accuracies.

## 2 Preliminaries and Related Work

In this section, we talk about the preliminaries and the previous works on one-shot NAS and zero-shot NAS. We talk about the motivation to replace statistical proxy by introducing the basic knowledge on causal interventaional representation learning in causality [20; 1].

### One-shot NAS

One-shot NAS methods [12; 16], that unify all the single-path neural architectures into one super-network \(\) (supernet), select the single-path neural architecture as the best one by training the weights \(\) in a weight-sharing manner and maximizing the validation accuracy (\(\)) of architecture \(\) as follows:

\[Max_{}((,)) s.t. =+_{}_{}\] (1)

The iterative updating of \(\) and selection of \(\) makes the one-shot NAS a bi-level optimization problem that is NP-hard. Differentiable one-shot model also relies on the observational data from unitelly trained validation accuracies of differentiable subnets . Wang et al.  propose a selection-based approach to modify the output of differentiable one-shot NAS  to discretize a single-path neural architecture that consists of operations (neurons) with strength. As a consequence, the perturbation-based inductive bias is demonstrated to be helpful to solve the degeneration.

### Statistical proxies in zero-shot NAS

We compare the various training-free and zero-shot NAS methods according to the usage of statistical representation. Some training-free approaches use the statistic of validation accuracy to predict the final architecture. NASWOT  samples a number (\(N\)) of well-trained neural architectures from the NAS-Bench-201 dataset to learn the kernel. However, to get these representations, the training costs tremendously. The zero-shot methods directly use zero-cost statistical proxies to represent the expressivity without weights and validation accuracy. Zen-NAS  uses a Gaussian complexity to measure the network expressivity and evolve the architectures to maximize the expressivity. Other training-free approaches such as TE-NAS  and NASI  imitate the train of NAS by neural tangent kernel (NTK) which largely reduces the waste of train cost. TE-NAS  propose to maximize the number of linear region of activation patterns . On the opposite, NASI  subtly optimize the trace of NTK by sampling.

Here raise the question that to what extent the validation accuracy outperforms the statistical proxy. Vice versa, we question if the statistical proxy is in substitute of the validation accuracy. Compared to the proxy-based methods with approximations, the validation-based method is more reproducible. The validation accuracy is an intrinsic robust and upper-bounded proxy to measure the neural architectures. Besides, previous arts of one-shot manner usually use the validation accuracy to be the objective to maximize. Despite these benefits, the zero-shot representation is imperfect due to the weight-agnostic messages.

### Causal representation learning

The study  demonstrates that causality is a "subtle concept" which can not be fully described by Boolean or Probabilistic. It is more about reasoning. Reichenbach demonstrates a common cause principle to explain the causality by dependencies among variables . Causal representation learning mainly deals with learning causally for representations. By observational data, we can hardly learn the real circumstances (environments), especially in complex scenes and high-dimensional data scenarios. Causal representation learning seeks to extract high-level information (dependencies) from low-level data. Interventions have taken a prominent role in representation learning literature on causation. The work  uses interventional data to facilitate the causal representations to get precise outcomes. Neural architecture search aims at learning the architectural representations automatically. The automatism of the previous arts of neural architecture search might not be causal especially in zero-shot setting.

## 3 Method

### Imperfect information

Neural architecture search is a task aiming at interpreting the mechanism of architectural knowledge of neural networks given methods of evaluations. Activation patterns, statistical proxies, and naive validation accuracy are adopted to evaluate the score of a neural network. However, we can hardly understand any neural network and even hardly explain the weight distribution of any neural network without assumptions. Observational data are always imperfect due to the infinite environments (search spaces/training schemes/hardware/etc.) of all possible networks with finite observations and limited tools. Architecture information is not stand-alone.

In one-shot NAS, demonstrated in Equation 4, given a neural network, we first train the weights \(\) and the \(\) combined with architecture \(\) can give a validation accuracy \(\). After \(\) is given, we then update the \(\) to get \(\) and a novel architecture \(\) until the validation accuracy \(\) is maximum. In the train, the architecture of a neural network is the key factor that impacts the other two factors \(\) and validation accuracy \(\). The search is actually a reverse way of train to the aspect of the intrinsic dependency of accuracy \(\) on the weight \(\) and architecture \(\). However, we have overlooked a lot of factors like data distributions, batch sizes, rates of weight decay, and so on and on which we can not optimize as "one shot". If the observational data alone can not interpret the phenomenon, it is a must to model the latent factors \(\) that cause this uninterpretability. Figure 1 illustrates the dependencies of architecture \(\), validation accuracy \(\), and weights \(\). The dashed line reveals that \(\) changes the dependencies of selected neurons (or searched architectures) on observational data of \(\) and \(\), which indeed implies strong causality . In logical condition, the structural relationship between \(\) and \(\) can be almost broken1.

We assume the validation accuracy \(\) of a set of neural architectures \(\{\}\) obeys a Gaussian distribution.

\[(,^{2})\] (2)

Due to the random weight information, artificial neural networks (ANN) themselves have architectural information to deliver the neural networks' expressivity with large variances . It is demonstrated that the weight-agnostic neural network still preserves the 92% accuracy-level information for digit classification by the work . However, the weights are agnostic and consequently the validation accuracies are imperfect. We assume the true validation accuracy is the difference of the observational \(^{obser}\) and latent impact of factor \(\) demonstrated in Equation 3.

\[(_{obser}-_{},_{obser}^{2 }-_{}^{2})\] (3)

### Problem formulation

In Zen-NAS, the adoption of statistical proxy on the feature map is impressive while it is constrained to structural dependencies . We question to what extent, when we search a neural network, the statistical proxies can be replaced with the more robust functions such as validation accuracy causally . In some one-shot [16; 12] and training-free methods , the evaluation metrics are usually the validation accuracy of the associated neural architectures.

Inspired by the previous work , we evaluate each neuron to select respectively in substitute. Intuitively, we measure the importance of each neuron by a simple validation accuracy of a singular associate neuron while resting other neurons on the same edge. DARTS+PT  the perturbation-based approach mutes the irrelevant neurons to conduct an inference while saving the other paralleled edges. For each paralleled edge (layer) \(\) that contains \(M\) neurons \(\)s, we mute the other neurons while only saving the \(i^{th}\) neuron \(_{(i)}\). The \(k^{th}\) paralleled edge \(_{i}^{(k)}\) consequently only contains one neuron (operation): \(_{i}^{(k)}=\{0_{(1)},0_{(2)}, ,_{(i)},,0_{(M)}\}\). When saving the other paralleled edges \(\{_{(j)}\}_{j k}\), \(_{(i)}\) denotes any single sub-architecture (a neuron) in the supernet \(\) with tuned weights \(_{}\) of the supernet. Formally, the one-shot neuron selection for \(k_{th}\) paralleled edge is defined as:

\[^{*}=argmax((\{(_{(i)},_{ }\})))_{(i)}^{(k)}\] (4)

where validation accuracy \(\) is measured by an intrinsic inductive bias function \(\) such as a reinforcement learning policy \(\)[31; 32]. \((_{(i)})=(\{^{(1)},^{ (2)},,_{i}^{(k)},,^{(N)}\})\) in practise.

In zero-shot NAS, the weight information is agnostic, which is impacted by a latent factor \(\) as shown in Figure 1. . The latent variable obeys a distribution \(\) in dimension \(\):

\[^{}\] (5)

When we sample larger enough numbers of impacts, the sample of factor \(\) obeys a Gaussian distribution by the central limit theorem (CLT). The causal zero-shot neural architecture search (Causal-Znas) that searches in imperfect messages is defined as:

\[^{*}=argmax((\{(_{(i)},)\} |))_{(i)}^{(k)}\] (6)

Figure 1: Illustrations of the dependencies of architecture \(\), validation accuracy \(\), and weights \(\) with latent factor \(\) on the train (left), one-shot neural architecture search (middle), and causal zero-shot neural architecture search (right).

for \(i=1,2,,M\). In this Equation 6, \(Z\) means the latent information to impact agnostic-weights (such as a random initialization [5; 10]) and consequently validation accuracies \(\). Therefore, we get a causal information set of singular neuron representation \(\{(_{(i)})|\}\) for \(i=1,2,,M\). For each paralleled edge (layer) \(\) that contains \(M\) neurons \(\): \(=\{_{(1)},_{(2)},,_{(M)}\}\). We calculate the information of singular neuron \(_{i}\) on edge \(^{(j)}\) by freezing the other layers (ensembles/edges) \(\{^{(k)}\}_{k j}\) so that the causal information is only impacted by the current neurons due to the same condition (in the same paralleled edge). Then the causal information set of a paralleled edge \(\) is as:

\[\{()|\}=\{_{(1)}(| ),_{(2)}(|),,_{( M)}(|)\}\] (7)

In a Causal-Znas, a prediction function \(\) is able to measure the selected architectures from the un-trained supernet. To avoid the improper introduction of inductive biases, we use an identity function to measure the importance of neurons.

### Gaussian intervention

Most existing NAS approaches use observational data and make assumptions on the architectural dependencies to achieve provable representation identification. However, in our causal zero-shot neural architecture search, there is a wealth of interventional data available. To perfect the observational validation accuracies \(^{observ}\) in \(\), we sample \(^{ven}\) from an interventional distribution \(()\) to be in substitute for the ones derived by the observation \(^{observ}\). Formally, we have:\(^{ven}()\). Though pure architectural information is imperfectly observed, we can use an interventional function \(\)**(do intervn**) to replenish data from a one-shot perspective:

\[=_{p}^{()}^{ven} _{1-p}^{}^{observ}\] (8)

Ming et al.  assume the inputs obey Gaussian distribution and get comparable results with one-shot methods [12; 16]. What we use as the input for each neuron is a Gaussian image which also obeys the assumption of Gaussian inputs of Zen-NAS .

**Lemma 1**.: _Given a Gaussian input \((,^{2})\), the output of a neuron \(\) in the first layer is Gaussian._

Proof.: Assuming each neuron is a distinct convolution denoted as \(Conv_{i}\) for \(i=1,2,,M\), then the output of this edge is:

\[=_{i=1}^{M}(\{Conv_{(1)}(,_{(1)}),Conv_{ (2)}(,_{(2)}),,Conv_{(M)}(, _{(M)})\})\] (9)

where \((,^{2})\) and \(_{(i)}(_{w},_{w}^{2})\) for \(i=1,2,,M\). Given the i.i.d. inputs and weights, the output score (validation accuracy) of the neural network layer is Gaussian since the Convolution of a Gaussian (random variable) is still a Gaussian (random variable). We have Gaussian weights \(_{(i)}\) and \(Conv_{(i)}(,_{(i)})(_{(i)}, _{(i)}^{2})\). Then \(_{i}Conv_{(i)}(,_{(i)})(_ {(i)},_{(i)}^{2})\). 

**Lemma 2**.: _Given a Gaussian input \((,^{2})\), the output of a neuron \(\) in any layer is Gaussian._

Proof.: Apparently, any weighted summation of random variables that obey two distinct Gaussian is still a Gaussian. In neural networks, the layers are stacked. Based on Lemma 1, in the latter layer, the outputs also obey the Gaussian, whose inputs are the former layer's outputs. The convolution (neuron) \(Conv_{(i)}^{}\) of the next layer with output of latter layer \(\) (in Equation 9) has \(Conv_{(i)}^{}()(_{(i)}^{},{ _{(i)}^{}}^{2})\). 

**Corollary 2.1**.: _Given a Gaussian input \((,^{2})\), the output of any neuron ensemble \(\{_{(i)}\}_{i}\) is Gaussian._

Formally, we have \(^{(i)}^{(i)}(^{},^{ 2})\). \(}=\{^{(1)},^{(2)},,^{(K)}\}\) where \(}\) denotes all the outputs across edges \(_{(1)},_{(2)},,_{(K)}}\). Based on Lemma 1 and Lemma 2, we get the Corollary 2.1 to select edges (topology preferences).

Proof.: By Lemma 1, we have any neuron \(_{(i)}\) has a Gaussian output \(^{(i)}(_{(i)},_{(i)}^{2})\). Any ensemble of neurons has an output \(_{i}^{(i)}\). Then we have \(_{i}^{(i)}(_{(i)},_{(i)}^{ 2})\). 

As demonstrated in Equation 8, we propose an intervention function \(^{}\) to facilitate the imperfect causal representation of the validation information. We propose that the ideal information is distributed in the information set by a probability \(p\). The distribution \(\) is \((,)\) in the context.

Herein, we question to what extent, the imperfectness can be interventionally refined . We use the parameter \(p\) to asymmetrically flipping the random Gaussian \(^{N(,^{2})}_{p}\) to understand the imperfect information in dimension \(\) which is mapped to a vanilla Gaussian (in Equation 5). As shown in Figure 2, it compares the information difference between the observational information set and interventional information set impacted by the parameter \(p\). In different environments, the data of interventional data combined with observation obeys a distinct Gaussian, which implies strong coherence and robustness. When \(p=1\), the causality is perfectly achieved due to breaking the dependency of validation accuracy \(\) on weights \(\); otherwise, it is imperfect. The mean and variance coefficients of the additional notion of intervention are derived by sampling validation accuracy of one-shot prior. We propose that setting of \(p\) is conditional on the fraction of the mean of latent factor to the difference of the mean of observational data and the mean of interventional data.

**Proposition 1**.: _When \(p}{_{object}-_{ren}}\), the mean of the intervened data \(_{true}\)._

As demonstrated in Proposition 1, a sufficient condition of the mean of intervened data is getting closer to the true mean of the validation accuracy is that the \(p\) is closer to 1 and interventional data is closer to the true data.

### Causal zero-shot neural architecture search

We formulate the zero-shot NAS into ensemble selection and neuron selection. There are \(K\) neuron ensembles \(_{(i)}\}_{i}^{(1)},\{_{(i)} \}_{i}^{(2)},,\{_{(i)}\}_{i}^{(K )}}\). For each ensemble, there are \(M\) neurons (operations). The ensemble selection is the selection of an ensemble \(\{_{(i)}\}_{i}^{(j)}\) of neurons among the \(K\) ensembles (\(j\)), while neuron selection follows the same formula and selects a neuron \(_{(i)}\) from a neuron ensemble \(\{_{(i)}\}_{i}^{(j)}\).

Figure 3: The distribution plate of three neurons and a big distribution plate of ensemble of them.

Figure 2: Illustration of intervention of observational data. The blue denotes interventional data while the white denotes observational data.

As is shown in Figure 3, the validation accuracy of both a neuron and a neuron ensemble obey Gaussian distributions respectively. From a macro perspective it is an ensemble selection while from a minor perspective, it is a neuron selection. Thus we talk about both types in the same formula.

As demonstrated in Equation 6, the final outcome neurons are derived by maximizing their validation accuracies according to the latent factor. Given the Gaussian intervention in Equation 8, we further modify the formula of the causal neuron selection by doing intervention (without the additional inductive bias ):

\[^{*}}=argmax(\{}(_{(i) })\}_{i})\] (10)

, where \(}\) is the validation accuracy with intervention.

The methodology of neuron selection is given in Algorithm 1. The search process of neuron ensemble follows the same formulation as mentioned in this Section. **do interven** represents to do intervention. At first, the weight \(\) of the supernet is randomly initialized . Second, validation scores \(\) on the validation set are prepared for the calculation of the neurons \(\) which adopts probability \(p\) to do the intervention. At last, the maximum of values is compared to select the best neuron (operation). In practice, when the probability \(p\) is close to 1, the validation accuracy of observation has less need to compute.

``` Initialize supernet weights \(\); For \(i=1,2,,M\): Calculate validate accuracy \(^{observe}(_{(i)}())\); do interven by \(p\); Maximize the \(\) and select the \(^{*}\). ```

**Algorithm 1** Causal zero-shot neuron selection.

Equation 6 reveals a universal formula for causal neural architecture search in the zero-shot settings. The measure function \(\) measures the importance  ("responsibility") of a neuron and Shapley value is proposed to be ideal for the selection of a neuron  or ensemble .

\[^{*}=argmax(\{_{(i)}(\{}\})\}_{i })\] (11)

We use the game-theoretic inductive bias to extract the valuable information [20; 7]. \(\) represent the Shapely value . Given Corollary 2.1, we know that any the neuron ensemble obeys a Gaussian distribution. The information set of Shapley value is thus build on top of an ensemble of Gaussian variables. However, we could not guarantee a Gaussian distribution of the Shapley value . As a consequence, we use a Gaussian distribution to do intervention on validation accuracy and then calculate the Shapely value of the intervened validation accuracy. At last, the Shapley value is maximized whose associated neuron is supposed to be more expressive .

### Weight-agnostic weights

In the assumptions of various methods, weights are initialized as Gaussian. However, in our framework, we demonstrate that this strong assumption is not a must. Supernet can be initialized in different ways: i) with Gaussian , ii) Uniform , and iii) Constant number .

**Corollary 2.2**.: _Given a Gaussian input \((,^{2})\), if the initial weights are Uniform or Constant number \(C\), the output of any neuron ensemble \(\{_{(i)}\}_{i}\) is not Gaussian._

Proof.: Apparently, the convolution of a Gaussian input with constant or uniform weights obeys a difference of CDF \(\) of the Gaussian in the range of constant or uniform. 

In the previous work , it is proposed that weights are supposed to be initialized by a distribution but not a constant (\(C\)). To be more precise, we propose that the constant value could not represent the agnostic weights and thus could not reflect the latent information while a uniform distribution can guarantee the randomness. By training on a "wide range" of uniform weight samples, Gaier et al. propose that "the best performing values were outside of this training set" . We propose that this phenomenon is essentially resulted from a distribution shift of the Gaussian validation accuracy which causes the change of search procedure. To solve the distribution shift, we could use the difference of CDF of Gaussian (\(\)) to conduct intervention. Even in a broader view, if the weights distributions are totally unknown, we can use Bayesian method to approximate a distribution \(()\) in Equation 8.

Experiments

We present the results and all experiment details of our method in this section. A robustness analysis is included to examine the stability of our method, which also explains the time efficiency. Results are given on the benchmark datasets, NAS-Bench-201 and CIFAR-10.

### Experimental details

We use the search space of DARTS  for fair comparisons with the state-of-the-art NAS approaches. During the searching process, we follow adopting the **same** and hyper-parameters as DARTS  to initialize the supernet on the CIFAR-10 and NAS-Bench-201 datasets for a fair comparison with DARTS-variants (one-shot methods). All the training is conducted on a single 2080Ti GPU.

### Results on CIFAR-10

As shown in Table 1, we compare the proposed Causal-Znas and game-version Causal-Znas-G with the state-of-the-art methods. The comparisons are made with respect to the informatics of the model, including test accuracy on the test set (Test Error), the number of parameters (Params), the search costs, and the search strategies. As shown, our results set the new state-of-the-art search speed with a competitive test error rate. Compared to DARTS , our method is 10000\(\) faster with comparable accuracy (2.75% v.s. 2.76%). Compared to DARTS+PT , our model is much simpler without introducing the perturbation-based inductive bias  and achieves a similar test error rate (2.61% v.s. 2.61%). DrNAS  and ISTA-NAS  are not only precise (2.54%) but also theoretically sound approaches. ISTA-NAS  is extremely fast in one-shot NAS while ours are more competitive (500\(\) faster) in search efficiency.

We compare our method with other zero-shot NAS approaches in Table 1. It demonstrates that the TE-NAS  which is the first algorithm that reaches 4 GPU hours search cost is experimentally awesome. TE-NAS uses the neural tangent kernel to approximate the train so it largely reduces the cost of training the neural networks. Compared to TE-NAS, our proposed approach is 500\(\) faster and our game-based result (-G) gets a comparable test error rate (2.61% v.s. 2.63 %) with a smaller number of parameters (3.1M v.s. 3.8M). We also surpass the current state-of-the-art zero-shot (training-free) method (NASI)  by more than 100\(\) in search efficiency and get fewer errors in both settings (2.75% v.s.2.79%; 2.89% v.s. 2.90%).

### Results on NAS-Bench-201

NAS-Bench-201 is a pure-architecture-aware dataset where the neural architectures are trained in the same settings, and the info such as performance, parameters, architecture topologies, and operations

  
**Algorithm** &  **Test Error** \\ **(\%)** \\  &  **Params** \\ **(M)** \\  &  **Search Cost** \\ **(GPU seconds)** \\  & 
 **Search Strategy** \\  \\  DenseNet-BC  & 3.46 & 25.6 & - & manual \\  NASNet-A + cutout  & 2.65 & 3.3 & 1.73\(\)10\({}^{8}\) & RL. \\ AmoebaNet-A  & 3.34 \(\) 0.06 & 3.2 & 2.72\(\)10\({}^{8}\) & GA \\ AmoebaNet-B  & 2.55 \(\) 0.05 & 2.8 & 2.72\(\)10\({}^{8}\) & GA \\ PNAS  & 3.41 \(\) 0.09 & 3.2 & 1.94\(\)10\({}^{7}\) & SMBO \\  ENAS  & 2.89 & 4.6 & 43200 & RL \\ DARTS(180)  & 3.00 \(\) 0.14 & 3.3 & 34560 & gradient \\ DARTS(2nd)  & 2.76 \(\) 0.09 & 3.3 & 86400 & gradient \\ BayesNAS  & 2.81 \(\) 0.04 & 3.4 & 17280 & gradient \\ DrNAS  & **2.54 \(\) 0.03** & 4.0 & 34560 & gradient \\ ISTA-NAS  & 2.54 \(\) 0.05 & 3.3 & 4320 & gradient \\ DARTS+PT  & 2.61 \(\) 0.10 & 3.0 & 69120 & gradient \\  TE-NAS  & 2.63 \(\) 0.06 & 3.8 & 4320 & NTK \\ NASSI-FFk  & 2.79 \(\) 0.01 & 3.9 & 864 & NTK \\ NASI-NASIA  & 2.90 \(\) 0.01 & 3.7 & 864 & NTK \\  Causal-Znas(\(p=0.5\)) & 2.89 \(\) 0.08 & **2.6** & 142 & causal \\ Causal-Znas(\(p=1\)) & 2.75 \(\) 0.10 & 3.2 & **8** & causal \\ Causal-Znas-G(\(p=1\)) & 2.61 \(\) 0.04 & 3.1 & 30 & causal \\   

Table 1: Comparison with state-of-the-art NAS methods on CIFAR-10.

are available. Compared to NAS-Bench-101 , NAS-Bench-201 adopts a different search space and gets results on various datasets such as CIFAR-10, CIFAR-100, and ImageNet16-120.

As shown in Table 2, it compares our proposed method with the state-of-the-art methods on NAS-Bench-201. Compared to NASWOT(N=10) , NASWOT(N=100) and NASWOT(N=1000) are much more accurate due to enlarged sample amounts. However, it also cause 10\(\) and 100\(\) waste of search costs. NASI  also enlarges its search cost to get much more precise results with extension of 90s. Our approach gets the same search cost with NASWOT (3s) while being much more precise on CIFAR-10 (90.03% v.s. 89.14%, 93.49% v.s. 92.44), CIFAR-100 (70.18% v.s. 68.50%, 71.18% v.s. 68.62%) and ImageNet 16-120 (43.83% v.s. 41.09%, 44.43% v.s. 41.31). A 9s extension of search cost (**Ours-G**) by neuron games gets even better results than NASWOT and NASI for their extreme results.

### Results on ImageNet with the DARTS search space

As shown in Table 3, we report the searched results on ImageNet. The validation size of the observation data batch is 1024. On ImageNet, the number of classes is 1000 so a large data batch is necessary. Compared to NASI , and TE-NAS , our search costs are faster when \(p=1\). The larger batches for evaluation enlarge the search cost for observational data resulting in a slightly larger search cost when \(p=0.5\). **Ours(p=1)** gets a competitive test error rate (25.0%) in the table and NASI-ADA  gets similar result (24.8%) but NASI-ADA has a larger search cost (864s v.s. 8s).

## 5 Conclusion

In this work, we interpret the zero-shot NAS as a causal representation learning and solve it by interventional data from one-shot NAS. Besides, our work is dedicated to displaying the inheriting relationship among the latent variables. We demonstrate that the neural architectures can be evaluated and selected by a Gaussian distribution given Gaussian inputs. Experiments on benchmark datasets reveal awesome efficiency and competitive accuracy.

    & **Search Cost** &  &  \\   & **GPU seconds** & **Val(\(\%\))** & **Test(\(\%\))** & **Val(\(\%\))** & **Test(\(\%\))** & **Val(\(\%\))** & **Test(\(\%\))** \\  ResNet  & - & 90.83 & 93.97 & 70.42 & 70.86 & 44.53 & 43.63 \\
**Optimal** & - & **91.61** & **94.37** & **73.49** & **73.51** & **46.77** & **47.31** \\  RSPS  & 7587 & 84.16 \(\) 1.69 & 87.66 \(\) 1.69 & 45.78 \(\) 6.33 & 46.60 \(\) 6.57 & 31.09 \(\) 5.65 & 30.78 \(\) 6.12 \\ DARTS(1st)  & 10890 & 39.77 \(\) 0.00 & 54.30 \(\) 0.00 & 15.03 \(\) 0.00 & 15.61 \(\) 0.00 & 16.43 \(\) 0.00 & 16.32 \(\) 0.00 \\ DARTS(2nd)  & 29902 & 39.77 \(\) 0.00 & 54.30 \(\) 0.00 & 15.63 \(\) 0.00 & 15.61 \(\) 0.00 & 16.43 \(\) 0.00 & 16.32 \(\) 0.00 \\  NASWOT(N=10)  & **3** & 89.14 \(\) 1.14 & 92.44 \(\) 1.13 & 68.50 \(\) 2.03 & 68.62 \(\) 2.04 & 41.09 \(\) 3.97 & 41.31 \(\) 4.11 \\ NASWOT(N=100)  & 30 & 89.55 \(\) 0.89 & 92.81 \(\) 0.99 & 69.83 \(\) 1.70 & 69.48 \(\) 1.70 & 42.81 \(\) 3.05 & 43.10 \(\) 3.16 \\ NASWOT(N=100)  & 300 & 89.69 \(\) 0.73 & 92.96 \(\) 0.81 & 69.86 \(\) 1.21 & 69.98 \(\) 1.22 & 43.95 \(\) 2.05 & 44.44 \(\) 2.10 \\ NASI(T)  & 30 & 30 & 0.88 \(\) 0.24 & & 69.51 \(\) 0.59 & & 40.87 \(\) 0.85 \\ NASI(4T)  & 120 & - & 93.55 \(\) 0.10 & - & 71.20 \(\) 0.14 & - & 44.84 \(\) 1.41 \\ 
**Ours** & **3** & 90.03 \(\) 0.61 & 93.49 \(\) 0.71 & 70.18 \(\) 1.38 & 71.18 \(\) 1.41 & 43.83 \(\) 2.10 & 44.43 \(\) 2.11 \\
**Ours-G** & 12 & 90.12 \(\) 0.52 & 93.59 \(\) 0.67 & 70.54 \(\) 1.29 & 71.50 \(\) 1.31 & 45.77 \(\) 1.20 & 45.73 \(\) 1.21 \\   

Table 2: Comparison with the state-of-the-art methods on NAS-Bench-201.

    & **Search Cost** & **Test Error** & **Params** \\  & **(GPU seconds)** & **(\%)** & **(M)** \\  DARTS  & 8.64\( 10^{5}\) & 26.7 & 4.7 \\ DARTS+PT  & 2.94\( 10^{5}\) & 25.5 & **4.6** \\ DNNSA  & 3.37\( 10^{5}\) & **24.2** & 5.2 \\  TE-NAS  & 4320 & 26.2 & 5.0 \\ TE-NAS  & 14688 & 24.5 & 5.4 \\ NASI-ADA  & 864 & 24.8 & 5.2 \\ NASI-PIX  & 864 & 24.3 & 5.5 \\ 
**Ours(p=0.5)** & 1020 & 25.5 & 4.9 \\
**Ours(p=1)** & **8** & 25.0 & 5.2 \\
**Ours-G** & 31 & 24.8 & 5.4 \\   

Table 3: Comparisons with the state-of-the-art on ImageNet.