# WISE: Rethinking the Knowledge Memory for

Lifelong Model Editing of Large Language Models

 Peng Wang\({}^{1}\)1  Zexi Li\({}^{1}\)1  Ningyu Zhang\({}^{1}\)2  Ziwen Xu\({}^{1}\)  Yunzhi Yao\({}^{1}\)

**Yong Jiang\({}^{2}\)  Pengjun Xie\({}^{2}\)  Fei Huang\({}^{2}\)  Huajun Chen\({}^{1,3}\)2 \({}^{1}\) Zhejiang University \({}^{2}\) Alibaba Group \({}^{3}\) Zhejiang Key Laboratory of Big Data Intelligent Computing {peng2001,zexi.li,zhangingyu}@zju.edu.cn**

[MISSING_PAGE_POST]

{peng2001,zexi.li,zhangingyu}@zju.edu.cn

###### Abstract

Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle--reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral\({}^{}\).

## 1 Introduction

Large language models (LLMs) show emergent intelligence when scaling the number of parameters and data , which reveals the sparks of artificial general intelligence . However, when deployed, LLMs still make mistakes , generating responses with hallucinations , bias , and factual decays . On the other hand, the world's knowledge is ever-growing, so the up-to-date knowledge is usually different from the one during LLMs' pretraining . Many such errors and emerging facts will arise sequentially in deployment, some of which have to be addressed timely and efficiently without waiting for retraining or finetuning . Also, retraining or finetuning is often too computationally expensive , which is not sustainable for lifelong growing knowledge. Therefore, _lifelong model editing_ was proposed to remedy the continual knowledge updates and injections for LLMs in a cheap and timely manner.

An effective lifelong model editing approach should satisfy the following properties : **i) reliability**, the model can remember both current and previous edits after sequential editing; **ii) locality**, model editing will not influence inherent pretrained knowledge which is irrelevant to the edited knowledge; **iii) generalization**, the model is not just merely memorizing the query-target pairs; instead, it should understand and generalize when given other forms of queries with the same knowledge. We compare existing model editing and continual learning methods on the three metrics in Figure 1 and find that _it seems to be an impossible triangle--reliability, generalization, and locality_ can not be realized at the same time in the continual editing settings. We find that where the updated knowledge resides in memories affects editing performances, and previous methods can be generally divided into editing either long-term memory, e.g., ROME , MEMIT , and FT-EWC (Finetuning with Elastic Weight Consolidation , a continual learning method), or working memory, e.g., GRACE . Note that the categorization of long-term and working memories is derived from human recognition  and neuroscience  which has recently been adopted in the study of LLMs . Model editing of long-term memory refers to directly editing the model parameters, which contain generalizable parametric knowledge . However, editing long-term memory will cause conflicts with previous pretrained knowledge, resulting in poor locality (e.g., ROME and FT-EWC in Figure 1). Working memory refers to the non-parametric knowledge of neural network activations/representations by retrieval, and it does not change the network parameters ; instead, it replaces the representations by retrieval at working (inference) time, like GRACE. GRACE's working memory shows promising results in reliability and locality, but in our experiments, it shows poor generalization since retrieval-based representations can hardly make the model understand the edits and generalize to different queries. It reveals that long-term memory and working memory both have drawbacks for lifelong model editing, though there were some special memory designs for LLM architectures, like MemorryLLM , SPALM , and Memoria , they change the architectures and cannot be directly applied for different LLMs. Intuitively, there is a gap between editing working and long-term memories, thus, in this paper, we study:

_What is the better memory mechanism for lifelong model editing to break the impossible triangle?_

Human brains contain the left and right hemispheres, which have different divisions as studied in recognition science , e.g., the left brain is typically associated with logical tasks while the right brain is more involved in intuitive processes. This inspires us to design **WISE**, which makes model editor _WISER_ in _memories_. WISE contains a dual parametric memory mechanism for LLMs' editing: the main memory for the pretrained knowledge and a side memory for the edited knowledge, realizing both long-term memory's generalization and retrieval-based working memory's reliability and locality. The side memory is a form of mid-term memory. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we design a knowledge-sharding mechanism where different sets of edits reside in distinct and orthogonal subspaces of parameters. These are then merged into a common side memory without conflicts. Our contributions are as follows:

* We identify the pitfalls of current model editing methods in lifelong settings, that is, the impossible triangle among--reliability, generalization, and locality. Behind the impossible triangle, we find there is a gap between editing long-term memory and working memory.
* We propose WISE, with a side parametric memory as the mid-term memory, realizing the advantages of both parametric long-term memory and retrieval-based working memory. We design memory routing, sharding, and merging modules in WISE, making WISE lead in continual knowledge editing, reaching the three metrics better simultaneously.
* Extensive experiments on GPT, LLaMA, and Mistral across QA, Hallucination, and out-of-distribution datasets validate the effectiveness of WISE for lifelong model editing.

Figure 1: **Metric triangle among reliability, generalization, and locality.** ZsRE dataset, number of continual edits \(T=100\), LLaMA-2-7B. Editing methods based on long-term memory (ROME and FT-EWC) and working memory (DEFER and GRACE) show the impossible triangle in metrics, while our WISE is leading in all three metrics.

Methodology

### Preliminaries: Lifelong Model Editing

We focus on lifelong model editing problem [10; 11], which can ensure hundreds or even thousands of sequential edits on LLMs to make the outputs of target queries align with human expectations while maintaining LLMs' previous knowledge and capability. Let \(f_{}:\), parameterized by \(\), denote a model function mapping an input \(\) to the prediction \(f_{}()\). The initial model before editing is \(_{0}\), which is trained on a large corpus \(_{}\). When the LLM makes mistakes or requires injections of new knowledge, it needs model editing with a time-evolving editing dataset as \(_{}=\{(_{e},_{e})|(_{1},_{1}),...,(_{T},_{T})\}\). At the time step \(T\), a model editor (ME) takes the \(T\)-th edit and the LLM of the \(T-1\) time step \(f_{_{T-1}}\) as inputs and produce the revised LLM model \(f_{_{T}}\) following the equation below:

\[f_{_{T}}=(f_{_{T-1}},_{T},_{T}), f_{_{T}}()=_{e}& _{e},\\ f_{_{0}}()&_{e}.\] (1)

Equation 1 describes that after model editing, the LLM should make the correct prediction on the current edit as \(f_{_{T}}(_{T})=_{T}\), while also preserving knowledge from past editing instances \((_{<T},_{<T})_{}\) as well as maintaining capability of \(f_{_{0}}\) on the irrelevant data when \(x_{e}\), especially for general training corpus \(_{}\).

### Rethinking the Memory Design of Lifelong Model Editing

In Table 1, we compare current model editing methods in terms of memory types and lifelong editing abilities. FT-EWC , ROME , MEMIT , and MEND  edit the long-term memory stored in the LLMs' model parameters, but they either do not support continual editing or have negative effects on irrelevant knowledge (poor locality). GRACE  is designed for lifelong editing via retrieval-based working memory. The retrieval codebook can avoid the conflicts of irrelevant knowledge, but GRACE fails to generalize due to its codebook being a non-parametric knowledge representation that solely memorizes queries without comprehension. It is worth noting that SERAC /DEFER  uses working memory that is stored in additional small models: a scope classifier and a counterfactual model, whose knowledge is parametric. However, the small counterfactual model cannot match the expressiveness and generalization capabilities of LLM itself, making it challenging for the edited knowledge to generalize effectively.

To enable effective lifelong model editing, the method should take advantage of both LLM parameters' long-term memory and retrieval-based working memory. Therefore, we propose WISE as follows.

### WISE: Side Memory with Knowledge Sharding, Merging, and Routing

As illustrated in Figure 2, WISE comprises two key components: 1) **Side Memory Design**: i) _side memory_: side memory is a memory container that is initialized as a copy of LLM's certain FFN layer, storing the stream of edits; ii) _memory routing mechanism_: similar to retrieval, a routing activation component is adopted to identify the scope of edits, routing the main (original) or side memories during inference; 2) **Knowledge Sharding and Merging**: i) _knowledge in random memory subspaces_: to make the edits in appropriate knowledge density and avoid forgetting, we shard the side memory into several random subspaces for editing; ii) _knowledge merging_: we leverage model merging techniques to merge different memory shards into one side memory without loss of knowledge.

  Methods & Long-term Memory & Working Memory & Parametric Knowledge & Retrieval Knowledge & Whether Lifelong & Reliability & Generalization & Locality \\  FT-EWC & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ \\ ROMEMEMIT & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ \\ MEND & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ \\ SERAC/DEFER & ✗ & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ GRACE & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ & ✓ \\ 
**WISE** & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\  

Table 1: **Comparison of current model editing methods. “✓” refers to “yes” and “well-supported”, ✗ refers to “no” or “badly-supported”, and “\(\)” refers to “less-supported”. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.**

#### 2.3.1 Side Memory Design

Side memory in FFN's value matrix.Each layer in a Transformer contains a multi-head self-attention (MHA) mechanism and a feed-forward network (FFN), where the FFN constitutes two-thirds of the model parameters . The question of how Transformers retrieve and utilize stored knowledge remains unresolved [18; 34], yet past works [31; 33] have demonstrated that editing the weights of the FFN is consistently more effective for LLMs. The FFN typically consists of key-value linear matrices: \(_{k},_{v}\), i.e., two multi-layer perceptron (MLP) layers. For the output of attention feature \(\), the computation of the feed-forward network, omitting the bias terms, can be represented as:

\[()=_{v}=(^{} _{k})_{v},\] (2)

where \(\) is a nonlinear activation function (e.g. SwiGLU, GeLU), and \(\) represents the activation values of the first MLP layer. Following previous works [18; 33], we edit the value matrix \(_{v}\) of the chosen FFN layer.

However, directly editing the value matrix may cause forgetting and side effects in a lifelong setting. Thus, we **copy a value matrix as side memory and edit the side memory instead of the original matrix (main memory)**. Specifically, the side memory is initialized with the copy of main memory as \(_{v^{}}_{v}\). Given the side memory, the new output is expressed as \(_{s}()=_{v^{}}\). We will introduce how to update the side memory in Section 2.3.2.

Locating side memory's FFN layer.Transformer LLMs have been widely demonstrated to encode "lower-level" information (e.g., parts of speech) in earlier layers while processing more advanced linguistic phenomena like anaphora and coreference in later layers [35; 36; 37]. Representations in later hidden layers propagate through residual connections without drastic changes [38; 18], enabling effective early exit in LLMs [39; 40]. Therefore, to minimize the side effects of editing and adjust advanced linguistic phenomena, we target mid-to-late layers (e.g. 27) for side memory. Further analysis of layer selection is provided in Section 3.3.

Routing between side memories and main memory.Similar to the retrieval-based methods [10; 32], during inference, it is needed to decide whether the main memory or the side memory is used. If a given query is within the scope of previous edits, the side memory is used; otherwise, the main memory. Inspired by , we introduce a routing activation indicator, given an input \(\), it is formulated:

\[_{}()=\|()(_{v ^{}}-_{v})\|_{2},\] (3)

where \(()=\) is the activation of the side memory's corresponding FFN layer in Equation 2. We want the activation indicators of editing queries to be larger than the ones of irrelevant queries by a large margin, which is:

\[\{_{}(_{e})|_{e}_{ {edit}}\}\{_{}(_{i})|_{i} _{}\},\] (4)

where \(_{}\) is the irrelevant dataset which includes \(_{}\).

Figure 2: **Overview of WISE.** Side memory (in blue) and main memory (in green) store edited and pretrained knowledge, respectively. Note: during inference, if WISE-Retrieve, the activation routing will retrieve and select one side memory with maximal activation score.

To achieve the above objective, we design a margin-based loss function during editing training, similar to contrastive  or triplet loss . The margin-based loss function for routing activation is:

\[L_{a}=_{_{v^{}}}\{(0,_{}( _{i})-)+(0,- _{}(_{e}))+(0,-(_{}( _{e})-_{}(_{i})))\},\] (5) \[_{e}_{},_{i} _{}.\]

Equation 5 aims that for all queries of irrelevant examples \(_{i}\), the activation indicators should be less than threshold \(\), and for the edit samples \(_{e}\), the activations should be larger than threshold \(\), with a certain distance \(\) between \(_{}(_{e})\) and \(_{}(_{i})\).

In the continual stream of incoming edits, the smallest activation indicator within the edits is updated and saved: \(=\{_{}(_{e})|_{e}_{}\}\). We aim to recognize the local scope of edits in this form. During inference, if the activation indicator of a new input is greater than \(\), WISE will use the side memory \(_{v^{}}\); otherwise, using the main memory \(_{v}\). Thus, given the query \(\), the output of the targeted FFN in Equation 2 is replaced by:

\[_{}()=() _{v^{}}&\|()(_{v^{}}- _{v})\|_{2}>,\\ ()_{v}&\] (6)

#### 2.3.2 Knowledge Sharding and Merging

How to effectively and efficiently store continual knowledge in model parameters is important for lifelong editing. We introduce the notion of "_knowledge density_" (similar to knowledge capacity ) that describes how many pieces of knowledge are stored per parameter on average. There is an editing dilemma w.r.t. knowledge density: i) If only a few edits are made for full fine-tuning or editing the entire memory, the knowledge density is low, which may lead to overfitting. ii) If numerous edits are made within a common and limited parameter space, the knowledge density is high, resulting in conflicts within the edited knowledge and potentially causing catastrophic forgetting. To remedy this dilemma, we propose a knowledge sharding and merging mechanism to divide the edits into several shards, store them in different parameter subspaces, and merge them into a common side memory.

Knowledge in random memory subspaces.We edit the side memory \(_{v^{}}\). We divide \(n\) edits into \(k\) shards, copy the side memory for \(k\) times, and generate \(k\) random gradient mask with mask ratio \(\) for each copy of side memory. A random gradient mask \(_{i}\{0,1\}^{|_{v^{}}|},i[k]\) is a binary mask whose proportion of \(1\) is \(\). For edit shard \(i,i[k]\), we edit the knowledge into the subspace \(_{i}\) as follows:

\[_{v^{}}^{i}_{v^{}}^{i}-(_{i}_{i}(_{v^{}}^{i})),\] (7)

where \(_{v^{}}^{i}\) is the \(i\)-th copy of the side memory, \(\) is the learning rate, \(_{i}()\) is the gradient of the \(i\)-th shard of edits, and the gradient is the autoregressive loss plus the routing activation loss \(L_{a}\)(Equation 5): \(L_{}=- P_{W_{v^{}}}(_{e}|_{e})+L_{a}\).

The random mask of gradients freezes the parameters intact when the elements are 0 and updates the weights when the elements are 1. It is superior to pruning because it does not harm the network performance while regularizing optimization in a subspace . In addition, the \(\) subspace will have higher knowledge density when \(k<1\), resulting in higher generalization (e.g., Figure 5). Also, different shards of edits have different random masks, and due to the (sub)orthogonality of random masks, different shards will not conflict with each other. Therefore, we can non-destructively merge the \(k\) copies of side memory into one.

Knowledge merging.We merge the \(k\) subspace pieces of side memory into one. Because we randomly generate the subspace masks, different random masks will have some overlapping elements and some disjoint elements, following the theorem below:

**Theorem 2.1**: _Subspace Overlap. Generate \(k\) memory subspaces \(_{v^{}}^{i},i[k]\) by random mask with 1's ratio \(\), so each memory has \(|_{v^{}}|\) active trained parameters. For any two subspaces \(_{v^{}}^{i}\) and \(_{v^{}}^{j}\)\(i j;i,j[k]\), there are \(^{2}|_{v^{}}|\) active parameters that are overlapped. For all \(k\) subspaces, there are \(^{k}|_{v^{}}|\) overlapped active parameters._

The theorem shows that larger \(\) will cause more overlap of subspace parameters, and the proof is in Appendix C. We find that this overlap is helpful in playing the role of "anchors" for knowledge merging (See Figure 5 and Appendix B.5). However, knowledge conflicts also exist in the overlapped parameters, so we leverage the recent task arithmetic model merging technique Ties-Merge  to relieve the conflicts. First, we compute the edit weight shift vectors \(_{e}=\{_{e}^{i}=_{v^{}}^{i}-_{v}|i[k]\}\). Then, we use Ties-Merge to merge the edit vectors into one:

\[_{v^{}}_{v}+(_{e}; _{v}).\] (8)

Ties-Merge consists of three steps: i) trim: trim the redundant parameters for each task vector; ii) elect the sign: elect the signs of each parameter; ii) disjoint merge: compute the disjoint mean for each parameter which has the same and correct signs . By Ties-Merge, different subspaces of knowledge are integrated into one with fewer conflicts. We study the effects of different merging techniques in Table 11 of Appendix B.2.

Routing and retrieving among several side memories.One single side memory has its limited knowledge capacity . For the lifelong editing stream, we can produce several side memories and retrieve them via activation score routing. We compute different activation indicator scores of side memories and retrieve the top-1 during inference. This design is named WISE-Retrieve, which enables a more challenging lifelong editing scenario. For WISE with only one side memory, it is notated as WISE-Merge. For most of the experiments, we use WISE-Merge by default, and we compare WISE-Retrieve in Table 6 and Figure 6.

The pseudo-code of our method can be found in Algorithms 1 and 2.

## 3 Experiments

### Experimental Settings and Evaluation Metrics

In the experiments, we compare the performance of different baselines and WISE in sequentially editing LLM models hundreds to thousands of times. In practice, we augment \(_{e}\) by generating 10 random token sequences of length 10 using \(f_{}\), enhancing editing generalization/adaptation to diverse contexts. We ensure that this augmentation with random tokens is applied across all baselines (See Appendix B.6, we ablate the contribution of Random Token).

Datasets and Models.We choose trending autoregressive LLM models **LLaMA-2-7B**, **Mistral-7B**, and **GPT-J-6B** for evaluation. The dataset details are in Table 3. Following , we evaluate WISE on the closed-book question-answering (QA) dataset **ZsRE**, and also evaluate its ability to correct **Hallucination** in SelfCheckGPT . The **Temporal** dataset  is employed to test the out-of-distribution (OOD) generalization of editing. Since Temporal comprises emerging entities post-2019, we avoid using the latest LLMs in OOD experiments. Instead, we follow the original literature of the Temporal dataset  and adopt **GPT-J-6B** as the base model, which is pretrained on the Pile  with a cutoff in 2020. Implementation details and editing examples for each dataset and can be found in Appendix A.

    &  \\   &  &  &  &  \\   & Rel. & Gen. & Loc. & Avg. & Rel. & Gen. & Loc. & Avg. & Rel. & Gen. & Loc. & Avg. & Rel. & Gen. & Loc. & Avg. \\   \\  FFL & 0.57 & 0.52 & 0.96 & 0.68 & 0.48 & 0.48 & 0.76 & 0.57 & 0.30 & 0.27 & 0.23 & 0.27 & 0.19 & 0.16 & 0.03 & 0.13 \\ FT-EWC & 0.96 & **0.95** & 0.02 & 0.64 & 0.82 & 0.76 & 0.01 & 0.53 & 0.83 & 0.74 & 0.08 & 0.55 & 0.76 & 0.69 & 0.08 & 0.51 \\ MEND & 0.95 & 0.93 & 0.98 & 0.95 & 0.26 & 0.28 & 0.28 & 0.27 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ ROME & 0.85 & 0.80 & 0.90 & 0.88 & 0.64 & 0.62 & 0.75 & 0.67 & 0.23 & 0.22 & 0.04 & 0.16 & 0.01 & 0.01 & 0.00 & 0.01 \\ MEMIT & 0.84 & 0.81 & 0.99 & 0.88 & 0.58 & 0.58 & 0.85 & 0.67 & 0.02 & 0.02 & 0.02 & 0.04 & 0.04 & 0.02 & 0.03 \\ MEMIT-MASS & 0.84 & 0.81 & 0.99 & 0.88 & 0.75 & 0.72 & 0.97 & 0.81 & 0.76 & 0.68 & 0.85 & 0.76 & 0.69 & 0.65 & 0.62 & 0.65 \\ DEFER & 0.68 & 0.58 & 0.56 & 0.61 & 0.65 & 0.47 & 0.36 & 0.49 & 0.20 & 0.12 & 0.27 & 0.20 & 0.03 & 0.03 & 0.74 & 0.27 \\ GRACE & **0.99** & 0.36 & **1.00** & 0.78 & **0.96** & 0.16 & **1.00** & 0.71 & **0.96** & 0.15 & **1.00** & 0.70 & **0.93** & 0.08 & **1.00** & 0.67 \\ 
**WISE** & 0.98 & 0.92 & **1.00** & **0.97** & 0.94 & **0.88** & **1.00** & **0.94** & 0.90 & **0.81** & **1.00** & **0.90** & 0.77 & **0.72** & **1.00** & **0.83** \\   \\  FT-L & 0.58 & 0.54 & 0.91 & 0.68 & 0.39 & 0.39 & 0.50 & 0.43 & 0.11 & 0.10 & 0.02 & 0.08 & 0.16 & 0.13 & 0.01 & 0.10 \\ FT-EWC & **1.00** & **0.99** & 0.01 & 0.67 & 0.84 & 0.78 & 0.02 & 0.55 & 0.82 & 0.72 & 0.09 & 0.54 & 0.76 & 0.69 & 0.09 & 0.51 \\ MEND & 0.94 & 0.93 & 0.98 & 0.95 & 0.01 & 0.01 & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ ROME & 0.79 & 0.77 & 0.98 & 0.85 & 0.58 & 0.57 & 0.75 & 0.63 & 0.05 & 0.05 & 0.02 & 0.04 & 0.04 & 0.04 & 0.02 & 0.03 \\ MEMIT & 0.81 & 0.79 & 0.99 & 0.86 & 0.46 & 0.45 & 0.61 & 0.51 & 0.00 & 0.00 & 0.01 & 0.00 & 0.04 & 0.02 & 0.03 \\ MEMIT-MASS & 0.81 & 0.79 & 0.99 & 0.86 & 0.74 & 0.71 & 0.97 & 0.81 & 0.73 & 0.71 & 0.88 & 0.77 & 0.73 & **0.70** & 0.62 & 0.68 \\ DEFER & 0.64 & 0.54 & 0.79 & 0.66 & 0.53 & 0.43 & 0.29 & 0.42 & 0.28 & 0.17 & 0.26 & 0.24 & 0.02 & 0.02 & 0.67 & 0.24 \\ GRACE & **1.00** & 0.36 & **1.00** & 0.79 & **1.00** & 0.15 & **1.00** & 0.72 & **1.00** & 0.15 & **1.00** & 0.72 & **1.00** & 0.02 & **1.00** & 0.67 \\ 
**WISE** & 0.98 & 0.97 & **1.00** & **0.98** & 0.92 & **0.89** & **1.00** & **0.94** & 0.87 & **0.80** & **1.00** & **0.89** & 0.70 & 0.67 & **1.00** & **0.79** \\   

Table 2: **Main editing results for QA setting (ZsRE dataset). \(T\)**: Num Edits.

   Setting & Eototnet Data & \(T\) & Pre-edit (LLaMA-MMMMMMMM) & Locality Data \\  QA & 2,841  & 1,000 & 0.3619.93.94C & 30,147  \\ Halle. & SelfCheckGPT  & 600 & 274.19.47B & **1.00**M**S**S** \\ OOD Gen. & Temporal  & 100 & 0.36 \(\)-ACC (GPT

**Baselines.** The baselines include methods of continual learning and model editing. We compare WISE against direct fine-tuning **FT-L** with an additional KL divergence loss , and continual learning fine-tuning based on Elastic Weight Consolidation (**FT-EWC**) . We also compare WISE to other model editors, including 1) GPT-style editors based on causal tracing: **ROME**, **MEMIT**, and **MEMIT-MASS** (a batch-editing version of MEMIT); 2) hypernetwork-based editors: **MEND**; and 3) the latest memory-based editors: **DEFER** (inspired by SERAC  for inference routing) and **GRACE**. Details on all comparisons are found in Appendix A.2.

Metrics.Each edit example includes an edit descriptor (i.e., query) \(_{e}\), its paraphrase prompts \(_{e^{}}\) (if available) for testing generalization, and an unrelated statement \(_{}\) for testing locality. For the editing dataset \(_{}=\{(_{e},_{e})\}\) with \(T\) edits, we evaluate the final post-edit model \(f_{_{T}}\) after the \(T\)-th edit example \((_{T},_{T})\). We evaluate the model editor's reliability and generalization using the metrics **Rel.** (a.k.a Edit Success Rate ) and **Gen.** (Generalization Success Rate ), while **Loc.** (Localization Success Rate ), defined as the post-edit model should not change the output of the irrelevant examples \(_{}\), assesses specificity. We report these metrics and their mean scores, which are formally defined as:

\[=_{i=1}^{T}(f_{_{T}}(_{e}^{})=_{e}^{}),=_{i=1}^{T}(f_{_{T}}(_{}^{})=_{e}^{}),=_{i=1}^{T}(f_{_{T}}(_{}^{})=f_{_{0}}(_{}^{})),\] (9)

where \(()\) is the indicator function. Notably, for the Hallucination dataset, following , we use the perplexity (PPL) to verify the locality, and there is no proper metric for generalization.

### Main Results

**Competitive Performance of WISE.** The competitive performance of WISE is evident in Table 2 and 4, which compare its results with eight baselines on the QA (ZsRE) and Hallucination (SelfCheckGPT) settings. In general, we observe the followings: \(\) WISE outperforms existing methods on multiple tasks after long editing sequences; \(\) direct editing of long-term memory (ROME, MEMIT, etc.) creates conflicts with prior pretraining knowledge, resulting in poor locality; and \(\) retrieving working memory and modifying activations (GRACE, DEFER, etc) struggle to generalize to diverse queries.

In the **QA** setting, with \(T=1000\), WISE achieves average scores of 0.83 and 0.79 on LLaMA and Mistral, respectively, reflecting improvements of 18% and 11% over the nearest competitor. This demonstrates WISE's outstanding stability and effective management of long-sequential edits. While methods like MEND and ROME are competitive early in editing, they show clear shortcomings as the edit sequence extends. Directly editing long-term memory (e.g., MEMIT, FT-EWC, MEND) results in a significant decline in Loc. When \(T\{100,1000\}\), this indicates that these methods cannot preserve LLMs' knowledge structure and significantly impair the model's generalization ability. GRACE excels in Loc. and Rel. (close to 1.00), however, it sacrifices generalization in continual editing. A possible reason is that token representation may not be suitable for measuring semantic similarity in autoregressive LMs, leading to paraphrase \(_{e^{}}\) failing to achieve similarity matching with any CodeBook _Key_ in GRACE (detailed in Appendix B.1). Overexpression on preserving and precisely adapting training data (working memory) hampers adaptability to new contexts. In a nutshell, most previous methods struggle to balance Rel., Gen., and Loc., particularly in long-form editing tasks. In addition, the results of GPT-J-6B can be found in Figure 9 in the Appendix.

WISE also surpasses the baselines on the **Hallucination** dataset, maintaining the lowest perplexity scores of 3.12 and 5.21 at \(T=600\), with Loc. remaining above 0.93. We similarly observe

    &  &  \\   &  &  &  &  &  &  &  \\ 
**Mistral** & Edit (PPL) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) \\  FFL & 4.41 & 0.96 & 12.57 & 0.71 & 30.06 & 0.81 & 0.92 & 0.28 & 2.05 & 2.03 & 0.83 & 100.06 & 0.05 & 100.05 & 100.05 & 0.00 \\ F-EWC & 2.56 & 0.34 & 3.68 & 0.09 & 2.10 & 0.16 & 4.56 & 0.45 & 0.15 & 0.04 & 2.43 & 0.17 & 5.46 & 0.25 \\ MEND & 5.65 & 0.57 & 1.01 & 0.66 & 0.60 & 0.68 & 10.87 & 0.90 & 0.74 & 9.66 & 8.73 & 0.06 & 2114.04 & 0.01 \\ ROME & 1.68 & 0.99 & 2.06 & 0.94 & 0.95 & 0.06 & 0.09 & 0.22 & 0.99 & 5.48 & 0.02 & 1027.55 & 0.05 \\ MISTART-MASS & 1.64 & **1.06** & **1.06** & 2.06 & 0.97 & 7.06 & 0.06 & 10.74 & 0.02 & 1.64 & **1.06** & 1.09 & 0.59 & 0.99 & 0.92 & 0.64 & 123.02 & 0.02 \\ MISTART-MASS & 1.66 & **1.06** & **1.06** & 1.61 & 0.99 & 7.13 & 0.59 & 10.47 & 0.44 & **1.06** & 1.09 & 2.78 & 0.59 & 2.92 & 0.77 & 2.88 & 0.95 \\ DEPER & **1.25** & 0.23 & 5.84 & 0.28 & 0.81 & 0.91 & 0.19 & 0.16 & 0.12 & 0.14 & 0.68 & 0.73 & 0.25 & 9.54 & 0.43 & 2.46 & 0.13 \\ GACE & 2.21 & **1.09** & 8.67 & **1.09** & **5.57** & **1.09** & 9.54 & **1.09** & **1.09** & **1.09** & **1.09** & 5.97 & **1.06** & 9.53 & 1.08 & 0.57 \\ 
**WISE** & 1.91 & **1.20** & **1.04** & **1.06** & **1.04** & **1.06** & **3.42** & **1.09** & **1.04** & **1.06** & **2.06** & **1.04** & **1.03** & 0.09 & **1.02** & 0.09 \\   

Table 4: **Main editing results for Hallucination setting (SelfCheckGPT dataset).**_T_**: Num Edits.

     &  &  \\   & Edit (PPL) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) & Dec. (T) & Edit (L) \\  FFL & 4.41 & 0.96 & 12.57 & 0.71 & 30.06 & 0.81 & 0.92 & 0.28 & 1.08 & 0.40 \\ F-EWC & 2.56 & 0.34 & 3.68 & 0.09 & 2.10 & 0.16 & 4.56 & 0.45 & 0.15 & 0.04 & 2.43 & 0.17 & 5.46 & 0.25 \\ MEND & 5.65 & 0.57 & 1.01 & 0.66 & 0.60 & 0.08 & 10.87 & 0.90 & 0.74 & 9.66 & 8.73 & 0.06 & 2114.44 & 0.01 \\ ROME & 1.68 & 0.99 & 2.06 & 0.94 & 0.95 & 0.06significant _PPL_ increases for FT-L, MEND, and ROME in long-context editing tasks, while GRACE's performance is lackluster in LLM long texts (possibly due to the limited fitting capacity of the very small active trained parameters \(|h^{l}|\) of GRACE).

Out-of-Distribution Evaluation.Ideally, model editing needs to generalize distributionally from formulaic editing examples to natural texts , where the distributional shift involves complexity rather than conventional domain shift . Following , we evaluate the OOD generalization of editing methods on emerging entities using the temporal updating dataset, **Temporal**. Editing examples and evaluation metrics are provided in Appendix A.1. As shown in Table 5, WISE effectively handles out-of-distribution generalization tasks (achieving the best OOD Gen. and overall performance). DEFER delivers mediocre performance on OOD Gen. due to the limited capacity of the auxiliary model. During the fine-tuning phase, GRACE and MEMIT focus on the representation \(v*\) of a **single** input token after \(_{v}\) (GRACE: last token, MEMIT: last subject token). However, regarding \(v*\) the editing carrier encounters two problems: 1) the training objective is not aligned with the pretraining phase, and 2) the single representation limits the search scope of gradient descent, making it difficult to handle OOD generalization. WISE, on the other hand, avoids these challenges.

### Further Analysis

Visualization of WISE's Routing Activation.To demonstrate the effectiveness of memory routing, we record the activation values \(_{}()\) of 1000 (QA, ZsRE)/600 (Halluc.) queries during the inference stage via knowledge merging into a single side memory. As shown in Figure 3, the purple horizontal line represents the activation threshold \(\) recorded during the editing phase. Almost all unrelated queries show low activations with values less than 10 in ZsRE and less than 20 in Halluc.; meanwhile, WISE accurately routes the editing prompt and unseen paraphrases into the side memory. This ensures editing locality and prevents excessive shifts from the pre-training distribution during lifelong editing.

Localization Analysis of WISE's Side Memory.To validate the benefits of editing mid-to-late layers, we select decoder layers from early, intermediate, mid-to-late, and late stages. As shown in Figure 4, the ablation results reveal that editing critical layers like the early and final layers (0, 1, 31) is ineffective, even resulting in a very low Loc. value of 0.096, which indicates a failure to recognize the editing scope. This may occur because the early layers represent fundamental grammatical information, and the final layer directly controls the decoding procedure, leading to poor editing of advanced language functions. Editing in the intermediate layers is suboptimal but still shows a markable improvement compared to early layers, possibly because intermediate layers start to integrate basic grammatical information with more complex semantic data. Notably, the mid-to-late layers demonstrate exceptional editing performance; for instance, selecting layer 26 results in an 80% success rate and generalization while maintaining 100% locality. This empirically supports our claim in Section 2.3.1 that the redundant mid-to-late layers  are ideal side memory layers and confirms the hierarchical nature of information processing in Transformer LLMs [57; 58].

Analysis of \(\) and \(k\) for WISE.We analyze the important hyperparameters of WISE: the mask ratio \(\) and the number of subspaces \(k\) in Figure 5. On the left figure, for \(k=2\), the best \(\) is 0.2,satisfying \(k*=0.4<1\), which implies the effectiveness of our subspace design that higher knowledge density will cause better generalization. When scaling \(k\), we observe an increasing demand of \(\). From Theorem 2.1, the probability of subspace overlap is \(^{k}\), and we hypothesize that this overlap is important as an anchor for model merging. Interestingly, from the right figure, it can be observed that the optimal cases always have the \(^{k}\) closest to 0.03. This shows an inherent tradeoff between merge anchor and merge conflicts, and the subspace overlaps around 0.03 are optimal for the best performances. Such experiments indicate that 20% FFN parameters can accommodate at least 500 edited samples. When "mask memory exhaustion" occurs, we can allocate new mask parameters to store new knowledge. Using retrieve when knowledge isn't full and merging as needed to save memory, achieves true lifelong model editing.

Scale Up to 3K of Edits.We scale the number of continual edits to 3K in Table 6. We compare WISE-Merge, keeping one side memory by multi-time merging, and WISE-Retrieve, keeping several side memories by routing and retrieving among different side memories. For WISE-Retrieve, we show an upper bound "_oracle_", which always identifies the correct routing path. We observe that the WISE series maintains high scalability, consistently outperforming the strongest baselines including MEMIT-MASS and GRACE. WISE-Retrieve based on top-1 activation retrieval demonstrates the best results in 3K edits, showing the effectiveness of well-organized memory subspaces and routing strategies during editing. We note that the "_oracle_" exhibits marginal performance decline when scaling the edits from 2K to 3K, yet it demonstrates remarkable performance across all metrics. This underscores the potential of WISE to handle extremely long continual edits, contingent upon substantial improvement in the retrieval of side memories. Additionally, an appropriate replay of edits can further improve retrieval accuracy, as detailed in Appendix B.3.

Contribution of Router designs in WISE.Without the router strategy, all inputs either pass solely through the main or side memory. To further validate its effectiveness, we conduct additional ablations with \(L_{a}\). WISE's performance on ZsRE is shown in Table 7. We observe the expected decrease in Loc. w.o. \(L_{a}\), such as dropping from 1.00 to 0.72 at T=1000, reveals the router's effectiveness in identifying editing scopes, minimizing side effects, and retaining a substantial amount of pre-training knowledge.

Inference Time Analysis of WISE.Figure 6 shows the inference time of a single instance for LLaMA after \(t\) editing steps, measured across 10 trials of each setting. Consistent with our expectations, we find that WISE-Merge incurs a constant inference delay (about 3%) as the editing stream expands. WISE-Retrieve, due to the introduction of retrieval routing, shows an increase in inference time as the number of edits increases, with a time cost increment of about 7% after 3K edits. Knowledge merging ensures that WISE-Merge only brings constant additional costs (0.64% extra parameters and 4% extra GPU VRAM, as detailed in Appendix B.7), contrasting with past memory-based works that continuously demand more available memory [10; 32].

## 4 Related Works

Memory and Knowledge Injection of LLMs.LLMs have long-term (episodic) and working memory [24; 25; 27]. Long-term memory is stored in model parameters, updatable via (re)pretraining , finetuning , and model editing . Working memory resides in neuron activations, utilized during inference . In-context learning and retrieval-based editing methods like GRACE contribute to working memory [60; 10]. However, whether finetuning or retrieval is debated [61; 62]. Also, current knowledge injection methods often suffer from computational overhead [13; 10], catastrophic forgetting , and overfitting . Methods like MemoryLLM , SPALM , NKB , and Memoria  are proposed to improve the memories from the architecture design perspective.

    &  &  \\   & Rel. & Gen. & Loc. & Avg. & Rel. & Gen. & Loc. & Avg. \\  GRACE & **0.96** & 0.03 & 0.00 & 0.66 & **0.96** & 0.03 & 0.00 & 0.66 \\ MEMIT-MASS & 0.64 & 0.58 & 0.55 & 0.59 & **0.58** & 0.53 & 0.47 & 0.53 \\  WISE-Merge & 0.66 & 0.63 & 1.00 & 0.76 & 0.58 & 0.56 & 1.00 & 0.71 \\ WISE-Retrieve & 0.68 & **0.64** & **1.00** & **0.77** & 0.61 & **0.58** & **1.00** & **0.73** \\ WISE-Retrieve\({}_{}\) & 0.77 & 0.72 & 1.00 & 0.83 & 0.75 & 0.70 & 1.00 & 0.82 \\   

Table 6: **Scaling to 3K edits of ZsRE.** LLaMA-2-7B.

   WISE\({}_{L}\), & Rel. & Gen. & Loc. & Avg. \\  \(T=1\) & 1.00 & 0.96 & 0.93 & 0.07 & 0.96 & -0.01 \\ \(T=10\) & 0.93 & 0.90 & 0.88 & -0.12 & 0.90 & -0.04 \\ \(T=100\) & 0.92 & 0.85 & 0.81 & -0.19 & 0.86 & -0.04 \\ \(T=1000\) & 0.84 & 0.79 & 0.72 & -0.28 & **0.78** & -0.05 \\   

Table 7: **Ablation study of Router (compared with Table 2).** LLaMA.

Figure 6: **Inference time of WISE when varying \(T\). ZsRE, LLaMA-2-7B.**

Model Editing of LLMs.Model editing encompasses constrained finetuning, locating-and-editing, meta-learning, and retrieval-based methods. ROME identifies factual associations and edits efficiently using MLP-based memories , extended by MEMIT for mass-editing . T-Patcher adds neurons for edits in LLMs' feed-forward layers . Meta-learning methods like MEND decouple finetuning gradients to generalize edits , complemented by MALMEN addressing cancellation effects . Retrieval-based methods like SERAC and GRACE improve working memory for editing [32; 10]. From single to mass editing and static to lifelong editing, model editing evolves to meet realistic demands. The latest efforts in lifelong editing such as LTE , MALMEN , and RECIPE  require extensive training with domain-specific edits before specific editing, yet we cannot predict the domain of upcoming edits in the editing flow and accessing these data is often impractical or unrealistic. It potentially increases the risks associated with retraining.

Model MergingModel merging , also known as model fusion [69; 70], studies how to aggregate different models' knowledge into one by parameter merging. However, in the research of linear mode connectivity, it is found that different minima of neural networks can hardly be merged into a generalized one even if trained on the same datasets from the same initialization (but with different random seeds) [71; 72]. The main reason is considered to be the permutation invariance property of deep neural networks, which means that the positions of neurons can be permuted without affecting the network function ; as a result, different minima reside in different loss basins . To improve linear mode connectivity and model merging, methods like optimal transport [70; 73], re-basin , and training-time alignment  are developed. For the applications, model merging techniques can help to improve the generalization of federated learning [74; 75] and enable knowledge aggregation of different-task models in a task arithmetic way [76; 77]. Recently, methods like task arithmetic in tangent space , TIES-Merging , ZipIt! , and ColD fusion  have been proposed for deep model fusion of pretrained foundation models, such as CLIP, ViT, and large language models. Specifically, TIES-Merging  consists of trim, elect sign & merge pipeline, which inspires the merge process of side memories in our paper.

For detailed related works, please refer to Appendix D.

## 5 Limitations and Broader Impacts

Although WISE shows promising results in lifelong editing, it also has some limitations. One limitation is addressed in Table 6 that the side memory retrieval has room for improvement to reach the oracle. Also, in Figure 6, the inference time of WISE-Retrieve increases with ever-growing editing streams. However, the current limitations cannot outweigh the merits of WISE in that it currently reaches better performance in general for lifelong model editing. We bridge the gap between long-term and working memory, it may inspire further work on memory design for model editing or even LLM architecture. However, the application of such technologies should be guided by ethical considerations. Malicious users may attempt to edit LLMs to propagate hate, highlighting the need for safeguards to prevent abuse and mitigate harmful outcomes. Some current model editors update the model's weights directly, making edits hard to trace and withdraw. WISE uses a modular and non-destructive side memory, allowing users to discard it if edits are unnecessary or harmful, without modifications to the main LLMs.

## 6 Conclusion

In this paper, we point out the impossible triangle of current lifelong modeling editing approaches that reliability, generalization, and locality can hardly be achieved simultaneously. We find the reason behind this is the gap between working and long-term memory. Therefore, we propose WISE, consisting of side memory and model merging, to remedy the gap.