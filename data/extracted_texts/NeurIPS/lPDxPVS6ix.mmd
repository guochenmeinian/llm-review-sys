# SPEAR: Exact Gradient Inversion of Batches in Federated Learning

Dimitar I. Dimitro\({}^{1}\), Maximilian Baader\({}^{2}\), Mark Niklas Muller\({}^{2,3}\), Martin Vechev\({}^{2}\)

\({}^{1}\) INSAIT, Sofia University "St. Kliment Ohridski" \({}^{2}\) ETH Zurich \({}^{3}\) LogicStar.ai

{dimitar.iliev.dimitrov}@insait.ai \({}^{1}\)

{mbaader, mark.mueller, martin.vechev}@inf.ethz.ch \({}^{2}\)

###### Abstract

Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of \(b=1\), with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, _the first algorithm reconstructing whole batches with \(b>1\) exactly_. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to \(b 25\) exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.

## 1 Introduction

Federated Learninghas emerged as the dominant paradigm for training machine learning models collaboratively without sharing sensitive data . Instead, a central server sends the current model to all clients which then send back gradients computed on their private data. The server aggregates the gradients and uses them to update the model. Using this approach sensitive data never leaves the clients' machines, aligning it better with data privacy regulations such as the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA).

Gradient Inversion AttacksRecent work has shown that an honest-but-curious server can use the shared gradient updates to recover the sensitive client data . However, while _exact_ reconstruction was shown to be possible for batch sizes of \(b=1\), it was assumed to be infeasible for larger batches. This led to a line of research on approximate methods that sacrificed reconstruction quality in order to recover batches of \(b>1\) inputs . In this paper we challenge this fundamental assumption and, for the first time, show that exact reconstruction is possible for batch sizes \(b>1\).

This Work: Exact Reconstruction of BatchesWe propose the _first gradient inversion attack reconstructing inputs exactly for batch sizes \(b>1\)_ in the honest-but-curious setting. In Fig. 1, we show the resulting reconstructions versus approximate methods  for a batch of \(b=20\) images.

Figure 1: A sample of four images from a batch of \(b=20\), reconstructed using our SPEAR (top) or the prior state-of-the-art Geiping et al.  (mid), compared to the ground truth (bottom).

Our approach leverages two key properties of gradient updates in fully connected ReLU networks: First, these gradients have a specific _low-rank structure_ due to small batch sizes \(b n,m\) compared to the input dimensionality \(n\) and the hidden dimension \(m\). Second, the (unknown) gradients with respect to the inputs of the first ReLU layer are sparse due to the ReLU function itself. We combine these properties with ideas from sparsely-used dictionary learning  to propose a sampling-based algorithm, called SPEAR (**S**parsity **E**pploiting **A**ctivation **R**ecovery) and show that it succeeds with high probability for \(b<m\). While SPEAR scales exponentially with batch size \(b\), we provide a highly parallelized GPU implementation, which empirically allows us to reconstruct batches of size up to \(b 25\) exactly even for large inputs (ImageNet) and networks (widths up to \(2000\) neurons and depths up to \(9\) layers) in around one minute per batch.

Main Contributions:
* The first gradient inversion attack showing theoretically that _exact reconstruction_ of complete batches with size \(b\!>\!1\) in the honest-but-curious setting is possible.
* SPEAR: a sampling-based algorithm leveraging _low rankness_ and ReLU-induced _sparsity of gradients_ for exact gradient inversion that succeeds with high probability.
* A highly parallelized GPU implementation of SPEAR, which we empirically demonstrate to be effective across a wide range of settings and make publicly available on GitHub.

## 2 Method Overview

We first introduce our setting before giving a high-level overview of our attack SPEAR, whose sketch is shown in Fig. 2.

SettingWe consider a neural network \(\) containing a linear layer \(=+\) followed by ReLU activations \(=()\) trained with a loss function \(\). Let now \(^{n b}\) be a batch of \(b\) inputs to the linear layer \(=+(||)\), with weights \(^{m n}\), bias \(^{m}\) and output \(^{m b}\). Further, let \(^{m b}\) be the result of applying the ReLU activation to \(\), i.e., \(=()\) and assume \(b m,n\). The goal of SPEAR is to _recover the inputs_\(\) (up to permutation) given the gradients \(}{}\) and \(}{}\) (see Fig. 2, i).

Low-Rank DecompositionWe first show that the weight gradient \(}{}=}{ }^{}\) naturally has a low rank \(b m,n\) (Theorem 3.1) and can therefore be decomposed as \(}{}=\) with \(^{m b}\) and \(^{b n}\) using SVD (Fig. 2, ii). We then prove the existence disagreement matrix \(=(_{1}||_{b})_{b}()\), allowing us to express the inputs as \(^{}=^{-1}\) and activation gradients as \(}{}=\) (Theorem 3.2). Next, we leverages the sparsity of \(}{}\) to recover \(\) exactly.

ReLU Induced SparsityWe show that ReLU layers induce sparse activation gradients \(}{}\) (Sec. 3.2). We then leverage this sparsity to show that, with high probability, there exist submatrices \(}^{b-1 b}\) of \(\), such that their kernel is an unscaled column \(}_{i}\) of our disaggregation matrix \(\), i.e., \((})=(_{i})\), for all \(i\{1,,b\}\) (Theorem 3.3). Given these unscaled columns \(}_{i}\), we recover their scale by leveraging the bias gradient \(}{}\) (Theorem 3.5).

Sampling and Filtering DirectionsTo identify the submatrices \(}\) of \(\) which induce the directions \(}_{i}\), we propose a sampling approach (Sec. 4.1): We randomly sample \(b-1\) rows of \(\) to obtain an \(}\) and thus proposal direction \(}_{i}^{}=(})\) (Fig. 2 iii). Crucially, the product \(}_{i}^{}=}{ _{i}}\) recovers a column of the sparse activation gradient \(}{}\) for correct directions \(}_{i}^{}\) and a dense linear combination of such columns for incorrect ones. This sparsity gap allows the large number \(N\) of proposal directions obtained from submatrices \(}\) to be filtered to \(c b\) unique candidates (Fig. 2 iv).

Figure 2: Overview of SPEAR. The gradient \(}{}\) is decomposed to \(\) and \(\). Sampling gives \(N\) proposal directions, which we filter down to \(c\) candidates via a sparsity criterion with threshold \(*m\). A greedy selection method selects batchsize \(b\) directions. Scale recovery via \(}{}\) returns the disaggregation matrix \(\) and thus the inputs \(\).

Greedy Direction SelectionWe now have to select the correct \(b\) directions from our set of \(c\) candidates (Fig. 2, y). To this end, we build an initial solution \(^{}\) from the \(b\) directions inducing the highest sparsity in \(}{}^{}=^{}\). To assess the quality of this solution \(^{}\), we introduce the sparsity matching score \(\) which measures how well the sparsity of the activation gradients \(}{}^{}\) matches the ReLU activation pattern induced by the reconstructed input \(^{}=^{-1}\). Finally, we greedily optimize \(^{}\) to maximize the sparsity matching score, by iteratively replacing an element \(^{}_{i}\) of \(^{}\) with the candidate direction \(^{}_{j}\) yielding the greatest improvement in \(\) until convergence. We can then validate the resulting input \(^{}=^{-1}\) by checking whether it induces the correct gradients. We formalize this as Alg. 1 in Sec. 5 and show that it succeeds with high probability for \(b<m\).

## 3 Gradient Inversion via Sparsity and Low-Rankness

In this section, we will demonstrate that both low rankness and sparsity arise naturally for gradients of fully connected ReLU networks and explain theoretically how we recover \(\). Specifically, in Sec. 3.1, we first argue that \(}{}=}{ }^{T}\) follows directly from the chain rule. We then show that for every decomposition \(}{}=\), there exists an unknown disaggregation matrix \(\) allowing us to reconstruct \(^{}=^{-1}\) and \(}{}=\). The remainder of the section then focuses on recovering \(\). To this end, we show in Sec. 3.2 that ReLU layers induce sparsity in \(}{}\), which we then leveraged in Sec. 3.3 to reconstruct the columns of \(\) up to scale. Finally, in Sec. 3.4, we show how the scale of \(\)'s columns can be recovered from \(}{}\). Unless otherwise noted, we defer all proofs to App. B.

### Explicit Low-Rank Representation of \(}{}\)

We first show that the weight gradients \(}{}\) can be written as follows:

**Theorem 3.1**.: _The network's gradient w.r.t. the weights \(\) can be represented as the matrix product:_

\[}{}=}{ }^{T}.\] (1)

For batch sizes \(b n,m\), the dimensionalities of \(}{}^{m b}\) and \(^{n b}\) in Eq. 1 directly yield that the rank of \(}{}\) is at most \(b\). This confirms the observations of Kariyappa et al.  and shows that \(\) and \(}{}\) correspond to a specific low-rank decomposition of \(}{}\).

To actually find this decomposition and thus recover \(\), we first consider an arbitrary decomposition of the form \(}{}=\), where \(^{m b}\) and \(^{b n}\) are of maximal rank. We chose the decomposition obtained via the reduced SVD decomposition of \(}{}=\) by setting \(=^{}\) and \(=^{}\), where \(^{n b}\), \(^{b b}\) and \(^{b n}\). We now show that there exists an unique disaggregation matrix \(\) recovering \(\) and \(}{}\) from \(\) and \(\):

**Theorem 3.2**.: _If the gradient \(}{}\) and the input matrix \(\) are of full-rank and \(b n,m\), then there exists an unique matrix \(^{b b}\) of full-rank s.t. \(}{}=\) and \(^{T}=^{-1}\)._

Theorem 3.2 is a direct application of Lemma B.1 shown in App. B, a general linear algebra result stating that under most circumstances different low-rank matrix decompositions can be transformed into each other via an unique invertible matrix. Crucially, this implies that recovering the input \(\) and the gradient \(}{}\) matrices is equivalent to obtaining the unique disaggregation matrix \(\). Next, we show how the ReLU-induced sparsity patterns in \(}{}\) or \(\) can be leveraged to recover \(\) exactly.

### ReLU-Induced Sparsity

ReLU activation layers can induce sparsity both in the gradient \(}{}\) (if the ReLU activation succeeds the considered linear layer) or in the input (if the ReLU activation precedes the linear layer).

Gradient SparsityIf a ReLU activation succeeds the linear layer, i.e., \(=()\), we have \(}{}=}{ }_{[>0]}\), where \(\) is the elementwise multiplication and \(_{[>0]}\) is a matrix of \(0\)s and \(1\)s with each entry indicating if the corresponding entry in \(\) is positive. At initialization, roughly half of the entries in \(\) are positive, making \(}{}\) sparse with \( 0.5\) of the entries \(=0\).

Input SparsityReLUs also introduce sparsity if the linear layer in question is preceded by a ReLU activation. Here, \(=(})\) will again be sparse with \( 0.5\) of the entries \(=0\) at initialization.

Note that for all but the first and the last layer of a fully connected network, we have sparsity in both, \(\) and \(}{}\). Due to the symmetry of their formulas in Theorem 3.2, our method can be applied in all three arising sparsity settings. In the remainder of this work, we assume w.l.o.g. that only \(}{}\) is sparse, corresponding to the first layer of a fully connected network. We now describe how to leverage this sparsity to compute the disaggregation matrix \(\) and thus recover the input batch \(\).

### Breaking Aggregation through Sparsity

Our exact recovery algorithm for the disaggregation matrix \(\) is based on the following insight:

If we can construct two submatrices \(^{b-1 b}\) and \(_{A}^{b-1 b}\) by choosing \(b-1\) rows with the same indices from \(}{}\) and \(\), respectively, such that \(\) has full rank and an all-zero \(i^{}\) column, then the kernel \((_{})\) of \(_{}\) contains a column \(_{i}\) of \(\) up to scale. We formalize this as follows:

**Theorem 3.3**.: _Let \(^{b-1 b}\) be a submatrix of \(}{}\) s.t. its \(i^{}\) column is \(\) for some \(i\{1,,b\}\). Further, let \(}{}\), \(\), and \(\) be of full rank and \(\) be as in Theorem 3.2. Then, there exists a full-rank submatrix \(_{A}^{b-1 b}\) of \(\) s.t. \((_{i})=(_{A})\) for the \(i^{}\) column \(_{i}\) of \(=(_{1}||_{b})\)._

Proof.: Pick an \(i\{1,,b\}\). By assumption, there exists a submatrix \(^{b-1 b}\) of \(}{}\) of rank \(b-1\) whose \(i^{}\) column is \(\). To construct \(_{}\), we take rows from \(\) with indices corresponding to \(\)'s row indices in \(}{}\). As \(}{}\) and \(\) have full rank, by Theorem 3.2, we know that \(}{}=\), and hence \(=_{}\). Multiplying from the right with \(_{i}\) yields \(0=_{i}=_{}_{i}=_{}_{i}\), and hence \((_{})(_{i})\). Further, as \(()=b-1\) and \(()=b\), we have that \((_{})=b-1\). By the rank-nullity theorem \(((_{}))=1\) and hence \((_{})=(_{i})\). 

As \(}{}\) is not known a priori, we can not simply search for such a set of rows. Instead, we have to sample submatrices \(_{}\) of \(\) at random and then filter them using the approach discussed in Sec. 4. However, we will show in Sec. 5.2 that we will find suitable submatrices with high probability for \(b<m\) due to the sparsity of \(}{}\) and the large number \(\) of possible submatrices. We will now discuss how to recover the scale of the columns \(_{i}\) given their unscaled directions \(}_{i}\) forming \(}\).

### Obtaining \(\): Recovering the Scale of columns in \(}\)

Given a set of \(b\) correct directions \(}=(_{1}}||_{b}})\), we can recover their scale, enabling us to reconstruct \(\), as follows. We first represent the correctly scaled columns as \(_{i}=s_{i}_{i}}\) with the unknown scale parameters \(s_{i}\). Now, recovering the scale is equivalent to computing all \(s_{i}\). To this end, we leverage the gradient w.r.t. the bias \(}{}\):

**Theorem 3.4**.: _The gradient w.r.t. the bias \(\) can be written in the form \(}{}=}{ }1\\ 1\)._

Thus, the coefficients \(s_{i}\) can be calculated as:

**Theorem 3.5**.: _For any left inverse \(^{-L}\) of \(\), we have \(s_{1}\\ \\ s_{b}=}^{-1}^{-L}}{ }\)_

Theorem 3.5 allows us to directly obtain the true matrix \(=}(s_{1},,s_{b})\) from the unscaled matrix \(}\). We now discuss how to recover \(}\) via sampling and filtering candidate directions \(}_{i}\).

## 4 Efficient Filtering and Validation of Candidates

In the previous section, we saw that given the correct selection of submatrices \(_{}\), we can recover \(\) directly. However, we do not know how to pick \(_{}\) a priori. To solve this, we rely on a sampling approach: We first randomly sample submatrices \(_{}\) of \(\) and corresponding direction candidates \(}^{}\) spanning \((_{})\). However, checking whether \(}^{}\) is a valid direction is not straightforward as we do not know \(}{}\) and hence can not observe \(\) directly as reconstructing \(}{}=\) requires the full \(\).

To address this, we filter the majority of wrong proposals \(}^{}\) using deduplication and a sparsity-based criterion (Sec. 4.1), leaving us with a set of candidate directions \(=\{}^{}_{j}\}_{j\{1,,c\}}\). We then select the correct directions in \(\) greedily based on a novel sparsity matching score (Sec. 4.2).

### Efficient Filtering of Directions \(}^{}\)

Filtering Mixtures via SparsityIt is highly likely (\(p=(1--1})^{b}\)) that a random submatrix of \(\) will not correspond to an \(\) with any \(\) column. We filter these directions by leveraging the following insight. The kernel of such submatrices is spanned by a linear combination \(}^{}=_{i}_{i}}_{i}\). Thus \(}^{}\) will be a linear combination of sparse columns of \(}{}\). As this sparsity structure is random, linear combinations will have much lower sparsity with high probability. We thus discard all candidates \(}^{}\) with sparsity of \(}^{}\) below a threshold \(\), chosen to make the probability of falsely rejecting a correct direction \(p_{fr}(,m)=}_{i=0}^{ m}\), obtained from the cumulative distribution function of the binomial distribution, small. For example for \(m=400\) and \(p_{fr}(,m)<10^{-5}\), we have \(=0.395\). We obtain the candidate pool \(=\{}^{}_{j}\}_{j\{1,,c\}}\) from all samples that were not flered this way.

Filtering DuplicatesAs it is highly likely to have multiple full-rank submatrices \(\), whose \(i^{}\) column is \(\), we expect to sample the same proposal \(}^{}_{i}\) multiple times. We remove these duplicates to substantially reduce our search space.

### Greedy Optimization

While filtering duplicates and linear combinations significantly reduces the number \(c\) of candidates, we usually still have to select a subset of \(b<c\). Thus, we have \(\) possible \(b\) sized subsets, each inducing a candidate \(^{}\) and thus \(^{}\). A naive approach is to compute the gradients for all \(^{}\) and compare them to the ground truth. However, this is computationally infeasible even for moderate \(c\).

To address this, we propose a greedy two-stage procedure optimizing a novel sparsity matching score \(\), which resolves the computational complexity issue above while also accurately selecting the correct batch elements and relying solely on \(}{}^{}\) and \(^{}\). As both can be computed directly via \(^{}\), the procedure is local and does not need to backpropagate gradients. Next, we explain the first stage.

Dictionary Learning As a first stage, we leverage a component of the algorithm proposed by Spielman et al.  for sparsely-used dictionary learning. This approach is based on the insight that the subset of column vectors \(=\{}^{}_{i}\}_{i=1}^{b}\), yielding the sparsest full-rank gradient matrix \(}\) is often correct. As the scaling of \(}^{}_{i}\) does not change the sparsity of the resulting \(}\), we can construct the subset \(\) by greedily collecting the \(b\) directions \(}^{}_{i}\) with the highest corresponding sparsity that still increase the rank of \(\). While this method typically recovers most directions \(}_{i}\), it often misses directions whose gradients \(_{i}}\) are less sparse by chance.

Sparsity MatchingWe alleviate this issue by introducing a second stage to the algorithm where we greedily optimize a novel correctness measure based solely on the gradients of the linear layer, which we call the sparsity matching coefficient \(\).

**Definition 4.1**.: Let \(_{-}\) be the number of non-positive entries in \(\) whose corresponding entries in \(}{}\) are \(0\). Similarly, let \(_{+}\) be the number of positive entries in \(\) whose corresponding entries in \(}\) are not \(0\). We call their normalized sum the _sparsity matching coefficient_\(\):

\[=+_{+}}{m b}.\]

Intuitively, this describes how well the pre-activation values \(\) match the sparsity pattern of the gradients \(}{}\) induced by the ReLU layer (See Sec. 3.2). While this sparsity matching coefficient \(\) can take values between \(0\) and \(1\), it is exactly \(=1\) for the correct \(\), if the gradient \(}{}\) w.r.t. the ReLU output is dense, which is usually the case. We note that \(\) can be computed efficiently for arbitrary full rank matrix \(}^{}\) by computing \(}{}^{}=^{}\) and \(^{}=^{}+(||)\) for \(^{}=^{-1}\).

To optimize \(\), we initialize \(}^{}\) with the result of the greedy algorithm in Spielman et al. , and then greedily swap the pair of vectors \(}^{}_{i}\) improving \(\) the most, while keeping the rank, until convergence.

Final Algorithm and Complexity Analysis

In this section, we first present our final algorithm SPEAR (Sec. 5.1) and then analyse its expected complexity and failure probability (Sec. 5.2).

### Final Algorithm

We formalize our gradient inversion attack SPEAR in Alg. 1 and outline it below. First, we compute the low-rank decomposition \(}{}=\) of the weight gradient \(}{}\) via reduced SVD, allowing us to recover the batch size \(b\) as the rank of \(}{}\) (Line 2). We now sample (at most \(N\)) submatrices \(}\) of \(\) and compute proposal directions \(}^{}_{i}\) as their kernel \((})\) via SVD (Lines \(4\)-\(5\)). We note that our implementation parallelizes both sampling and SVD computation (Lines \(4\)-\(5\)) on a GPU. We then filter the proposal directions \(}^{}_{i}\) based on their sparsity (Line \(6\)), adding them to our candidate pool \(\) if they haven't been recovered already and are sufficiently sparse (Line \(7\)). Once our candidate pool contains at least \(b\) directions, we begin constructing candidate input reconstructions \(^{}\) using our two-stage greedy algorithm GreedyFilter (Line \(8\)), discussed in Sec. 4.2. If this reconstruction leads to a solution with sparsity matching coefficient \(=1\), we terminate early and return the corresponding solution (Line \(9\)). Otherwise, we continue sampling until we have reached \(N\) samples and return the best reconstruction we can obtain from the resulting candidate pool (Line \(14\)). The pseudocode for ComputeSigma (Alg. 2) and GreedyFilter (Alg. 3) are shown in App. C.

### Analysis

In this section, we will analyze SPEAR w.r.t. the number of submatrices we _expect_ to sample until we have recovered all \(b\) correct directions \(}_{i}\) (Lemma 5.2), and the probability of failing to recover all \(b\) correct directions despite checking all possible submatrices of \(\) (Lemma 5.3). For an analysis of the number of submatrices we have to sample until we have recovered all \(b\) correct directions \(}_{i}\)_with high probability_, we point to Lemma B.2. Further, as before, we defer all proofs also to App. B.

**Expected Number of Required Samples**

To determine the expected number of required samples until we have recovered the correct \(b\) direction vectors \(}_{i}\), we first compute a lower bound on the probability \(q\) of sampling a submatrix which satisfies the conditions of Theorem 3.3 for an arbitrary column \(i\) in \(}\) and then use the coupon collector problem to compute the expected number of required samples.

We can lower bound the probability of a submatrix \(^{b-1 b}\), randomly sampled as \(b-1\) rows of \(}{}\), having exactly one all-zero column and being full rank as follows:

**Lemma 5.1**.: _Let \(^{b-1 b}\) be submatrix of the gradient \(}{}\) obtained by sampling \(b-1\) rows uniformly at random without replacement, where each element of \(}{}\) is distributed i.i.d. as \(}{_{j,k}}=||\) with \((=0,^{2}>0)\) and \((p=)\). We then have the probability

Figure 3: Visualizations of the upper bound (\(p_{}^{}\), dashed) on and approximation of (\(p_{}^{}\), solid) the failure probability of SPEAR for different batch sizes \(b\) and network widths \(m\) for \(p_{fr}=10^{-9}\).

\(q\) of \(\) having exactly one all-zero column and being full rank lower bounded by:_

\[q}(1-(+o_{b-1}(1))^{b-1}) {2^{b-1}}(1-0.939^{b-1}).\]

We can now compute the expected number of submatrices \(n^{*}_{}\) we have to draw until we have recovered all \(b\) correct direction vectors using the Coupon Collector Problem:

**Lemma 5.2**.: _Assuming i.i.d. submatrices \(\) following the distribution outlined in Lemma 5.1 and using Alg. 1, we have the expected number of submatrices \(n^{*}_{}\) required to recover all \(b\) correct direction vectors as:_

\[n^{*}_{}=_{k=0}^{b-1}=}{q} (b(b)+ b+),\]

_where \(H_{b}\) is the \(b^{}\) harmonic number and \( 0.57722\) the Euler-Mascheroni constant._

We validate this result experimentally in Fig. 4 where we observe excellent agreement for wide networks (\(m b\)) and obtain, e.g., \(n^{*}_{} 1.8 10^{5}\) for a batch size of \(b=16\).

Failure ProbabilityWe now analyze the probability of SPEAR failing despite considering all possible submatrices of \(\) and obtain:

**Lemma 5.3**.: _Under the same assumptions as in Lemma 5.1, we have an upper bound on the failure probability \(p^{}_{}\) of Alg. 1 even when sampling exhaustively as:_

\[p^{}_{} b(1-_{k=b-1}^{m} {2^{m}}(1-0.939^{(b-1)}))+1-(1-p_{fr})^{b},\]

_where \(p_{fr}\) is the probability of falsely rejecting a correct direction \(}^{}\) via our sparsity filter (Sec. 4.1)._

If we assume the full-rankness of submatrices \(\) to i) occur with probability \(1-(-o_{b-1}(1))^{b-1}\) for \(o_{b-1}(1) 0\) (true for large \(b\)) and ii) be independent between submatrices, we instead obtain:

\[p^{}_{} 1-(_{k=b-1}^{m} }(1-0.5^{(b-1)}))^{b}+1-(1-p_{fr })^{b}.\]

We illustrate this bound in Fig. 3 and empirically validate this bound in Fig. 8 and observe the true failure probability to lie between \(p^{}_{}\) and \(p^{}_{}\).

## 6 Empirical Evaluation

In this section, we empirically evaluate the effectiveness of SPEAR on MNIST , CIFAR-10 , TinyImageNet, and ImageNet across a wide range of settings. In addition to the reconstruction quality metrics PSNR and LPIPS, commonly used to evaluate gradient inversion attacks, we report accuracy as the portion of batches for which we recovered the batch up to numerical errors and the number of sampled submatrices (number of iterations).

Experimental SetupFor all experiments, we use our highly parallelized PyTorch  GPU implementation of SPEAR. Unless stated otherwise, we run all experiments on CIFAR-10 batches of size \(b=20\) using a \(6\) layer ReLU-activated FCNN with width \(m=200\) and set \(\) to achieve a false rejection rate of \(p_{fr} 10^{-5}\). We supply ground truth labels to all methods _except_ SPEAR.

### Comparison to Prior Work

In Table 1, we compare SPEAR against prior gradient inversion attacks from the image domain on the ImageNet dataset rescaled to \(256 256\) resolution. In particular, we compare to Geiping et al. 1, as well as, the recent CI-Net . As CI-Net only considers networks with the less common Sigmoid activations, we report its performance on both ReLU and Sigmoid versions of our network.

   Method & PSNR \(\) & Time/Batch \\  CI-Net  Sigmoid & \(38.0\) & \(1.6\) hrs \\ CI-Net  ReLU & \(15.6\) & \(1.6\) hrs \\ Geiping et al.  & \(19.6\) & \(18.0\) min \\ SPEAR (Ours) & **124.2** & **2.0 min** \\   

Table 1: Comparison to prior work in the image domain.

We observe that while CI-Net obtains very good reconstructions with the Sigmoid network (PSNR of 38), SPEAR still achieves a much higher PSNR (124) as it is exact. Further, for the more common ReLU activations, the performance of CI-Net drops significantly to a PSNR \(<16\) compared to \(19.6\) for Geiping et al. . Additionally, SPEAR is much faster compared to both Geiping et al.  and CI-Net, taking \(10\) and \(~{}100\) less time, respectively. Finally, we want to emphasize that both prior works rely on strong prior knowledge, including label information and knowledge of the structure of images, whereas we assume _no information at all_ about the data distribution and still achieve much better results in only a fraction of the time taken.

To confirm the versatility of SPEAR, we compare it to the SoTA attack in the tabular domain, Tableak , in Table 2. We see that due to the exact nature of our attack, we recover both continuos and discrete features better on the ADULT dataset  with \(b=16\), while still being \(6\) faster.

### Main Results

We evaluate SPEAR on MNIST, CIFAR-10, TinyImageNet and ImageNet at two different resolutions, reporting results in Table 3. Across datasets, SPEAR can reconstruct almost all batches perplexing of \(b=20\) for images as large as \(720 720\) in \(<3\) minutes. We provide additional results on heterogeneous data and trained networks in App. E, as well as, on the FedAvg protocol in App. F.

Effect of Batch Size \(b\)We evaluate the effect of batch size \(b\) on accuracy and the required number of iterations \(n^{*}_{}\) for a wide (\(m=2000\)) and narrow (\(m=200\)) network. While \(n^{*}_{}\) increases exponentially with \(b\), for both networks, the narrower network requires about \(20\) times more iterations than the wider network (see Fig. 4). While trends for the wider network (\(m b\)) are perfectly described by our theoretical results in Sec. 5.2, some independence assumptions are violated for the narrower network, explaining the larger number of required iterations. While we can recover all batches perfectly for the wider network, we see a sharp drop in accuracy from \(99\%\) at \(b=20\) to \(63\%\) at \(b=24\) (see Fig. 6) for the narrower network. This is due to increasingly more batches requiring more than the \(N=2 10^{9}\) submatrices we sample at most.

Effect of Network ArchitectureWe visualize the performance of SPEAR across different network widths and depths in Fig. 5. We observe that while accuracy is independent of both (given sufficient width \(m b\)), the number of required iterations reduces with increasing width \(m\). We provide further ablations on the effect of our two-stage filtering in App. E.3 and DPSGD noise in App. E.6.

Effect of Layer DepthOur experiments so far focused on recovering inputs to the first layer of FCNNs. However, SPEAR's capabilities extend beyond this, as highlighted in Sec. 3.2. To

   Method & Discr Acc (\%) \(\) & Cont. MAE \(\) & Time/Batch \\  Tableak  & \(97\) & 4922.7 & 2.6 min \\ SPEAR (Ours) & **100** & **20.4** & **0.4 min** \\   

Table 2: Results vs prior work in the tabular domain.

Figure 4: Effect of batch size \(b\) on the number of required submatrices. Expectation from Lemma 5.2 dashed and median (10th to 90th percentile shaded) depending on network width \(m\) solid. We always evaluate \(10^{4}\) submatrices in parallel, explaining the plateau.

   Dataset & PSNR \(\) & LPIPS \(\) & Acc (\%) \(\) & Time/Batch \\  MNIST & \(99.1\) & NaN & **99** & 2.6 min \\ CIFAR-10 & \(106.6\) & \(1.16 10^{-5}\) & **99** & 1.7 min \\ TinyImageNet & \(110.7\) & \(1.62 10^{-4}\) & **99** & **1.4 min** \\ ImageNet \(224 224\) & \(125.4\) & \(1.05 10^{-5}\) & **99** & 2.1 min \\ ImageNet \(720 720\) & **125.6** & **8.08\(\)10\({}^{-11}\)** & **99** & 2.6 min \\   

Table 3: Reconstruction quality across 100 batches.

Figure 5: Accuracy (green) and number of median iterations (blue) for different network widths \(m\) at \(L=6\) (left) and depths \(L\) at \(m=200\) (right).

demonstrate this, we use SPEAR to reconstruct the inputs to all FC layers followed by a ReLU activation in a 6-layer FCNN with a width of \(m=400\) at initialization.

The results, presented in Table 4, show that SPEAR successfully recovers the inputs to all layers almost perfectly. However, attacking later layers is more computationally expensive. Specifically, the runtime for \(l=5\) increases to 70 minutes/batch resulting in \(17\) batches that timed-out. This increased computational cost is due to the initialization of the network, which causes the outputs of later layers to be dominated by their bias terms with their inputs being almost irrelevant. This issue is mitigated after a few training steps, as weights and biases adjust to better reflect the relationships between inputs and outputs. We find that after \(5000\) gradient steps the time per batch reduces to \(<1\) min at an accuracy of \(>95\%\) for layer \(l=5\).

### Scaling SPEAR via Optimization-based Attacks

As we prove theoretically in Sec. 5.2 and verify practically in App. E.5, in the common regime where the batch size \(b\) is much smaller than dimensions of the attacked linear layer w.h.p. the input information is losslessly represented in the client gradient. However, in practice for \(b>25\) the exponential sampling complexity of SPEAR becomes a bottleneck that prevents the recovery of the input (see Fig. 4).

In this section, we propose a method for alleviating the exponential sampling complexity by combining SPEAR with an approximate reconstruction method to get a prior on which submatrices \(_{A}\) satisfy the conditions of Theorem 3.3, i.e., have corresponding matrices \(\) containing a 0-column. To this end, we first obtain an estimate of the client pre-activation values \(}\) based on the approximate input reconstructions from Geiping et al. . As large negative pre-activation values in \(}\) are much more likely to correspond to negative pre-activation values in the true \(\), and, thus, to \(0\)s in gradients \(}{}\), we record the locations of the \(3b\) largest negative values for each column of \(}\). Importantly, by choosing the locations this way, we ensure that each group of \(3b\) locations correspond to locations of likely 0s in _same column_ of \(}{}\). Restricting the sampling of the row indices of \(_{A}\) and \(\) only within each group of locations, ensures that \(_{A}\) is very likely to satisfying the conditions of Theorem 3.3.

We confirm the effectiveness of this approach in a preliminary study, shown in Table 5, that demonstrates the combined approach allows a substantial increase in the batch size SPEAR can scale to (up to 100), thus effectively eliminating its exponential complexity. The results show that the combined approach drastically improves the reconstruction quality of Geiping et al.  as well, as unlike Geiping et al. , it achieves exact reconstruction. Importantly, we observe that even for the 4 batches SPEAR failed to recover in Table 5, SPEAR still reconstructs \(>97\) of the \(100\) directions \(_{i}}\) correctly, suggesting that future work can further improve upon our results.

### Feature Inversion in Convolutional Neural Networks

Following the Cocktail Party Attack (CPA) , we experiment with using SPEAR to recover the input features to the first linear layer of a pre-trained VGG16 convolutional network with size \(25088 4096\) for ImageNet batches of \(b=16\) and use them in a feature inversion (FI) attack to approximately recover the client images. We show the results of our experiments, based on the CPA's code and parameters, in Table 6. We see the inverted features drastically improve quality of the final reconstructions, and that SPEAR achieves almost perfect feature cosine similarity, resulting in better overall reconstruction versus CPA.

   Method & \(\)PIPS \(\) & Feature Sim \(\) \\  Geiping et al.  & \(0.562\) & - \\ CPA + FI + Geiping et al.  & \(0.388\) & \(0.939\) \\ SPEAR + FI + Geiping et al.  & **0.362** & **0.984** \\   

Table 6: Comparison between the reconstructions on VGG16 for Geiping et al. , CPA , and SPEAR for 10 ImageNet batches (\(b=16\)).

   \(l\) & MAE \(\) & Acc(\%) \(\) & Time/Batch \\ 
1 & \(}\) & **100** & 2.3 min \\
2 & \(1.33 10^{-6}\) & **100** & **2.2 min** \\
3 & \(1.67 10^{-6}\) & **100** & 5.6 min \\
4 & \(2.80 10^{-6}\) & \(99\) & 19 min \\
5 & \(3.04 10^{-6}\) & \(83\) & 70 min \\   

Table 4: Effect of the attacked layerâ€™s depth \(l\) (\(1 l 6\)) on reconstruction time and quality for 100 TinyImageNet batches of size \(b=20\).

   Method & \(b\) & \(m\) & Acc(\%) \(\) & PSNR \(\) \\  Geiping et al.  & 50 & 400 & - & \(26.5\) \\ SPEAR + Geiping et al.  & 50 & 400 & 100 & **124.5** \\  Geiping et al.  & 100 & 2000 & - & \(32.8\) \\ SPEAR + Geiping et al.  & 100 & 2000 & 60 & **81.5** \\   

Table 5: Comparison between the reconstruction quality of Geiping et al.  and a version of SPEAR that uses Geiping et al.  to speed up its search procedure evaluated on 10 TinyImageNet batches.

Related Work

In this section, we discuss how we relate to prior work.

Gradient Inversion AttacksSince gradient inversion attacks have been introduced , two settings have emerged: In the _malicious setting_, the server does not adhere to the training protocol and can adversarially engineer network weights that maximize leaked information [19; 20; 21; 22]. In the strictly harder _honest-but-curious setting_, the server follows the training protocol but still aims to reconstruct client data. We target the honest-but-curious setting, where prior work has either recovered the input exactly for batch sizes of \(b=1\)[5; 6], or approximately for \(b>1\)[1; 23; 7; 8; 9]. In this setting, we are the _first to reconstruct inputs exactly for batch sizes \(b>1\)_.

Most closely related to our work is Kariyappa et al.  which leverage the low-rank structure of the gradients to frame gradient inversion as a blind source separation problem, improving their approximate reconstructions. In contrast, we derive an _explicit_ low-rank representation and additionally leverage gradient sparsity reconstruct inputs exactly.

Unlike a long line of prior work, we rely neither on any priors on the data distribution [8; 24; 25; 26] nor on a reconstructed classification label [1; 27; 7; 23; 8]. This allows our approach to be employed in a much wider range of settings where neither is available.

Defenses Against Gradient InversionDefenses based on Differential Privacy  add noise to the computed gradients on the client side, providing provable privacy guarantees at the cost of significantly reduced utility. Another line of work increases the empirical difficulty of inversion by increasing the effective batch size, by securely aggregating gradients from multiple clients  or doing multiple gradient update steps locally before sharing an aggregated weight update . Finally, different heuristic defenses such as gradient pruning  have been proposed, although their effectiveness has been questioned .

Sparsely-used Dictionary learningRecovering the disaggregation matrix \(\) is related to the well-studied problem of sparsely-used dictionary learning. However, there the aim is to find the sparsest coefficient matrix (corresponding to our \(}{}\)) and dense dictionary (\(^{-1}\)) approximately encoding a signal (\(\)). In contrast, we do not search for the sparsest solution yielding an approximate reconstruction but a solution that exactly induces consistent \(\) and \(}{}\), which happens to be sparse. Sparsely-used dictionary learning is known to be NP-hard  and typically solved approximately [32; 10; 33]. However, under sufficient sparsity, it can be solved exactly in polynomial time . While our \(}{}\) are not sparse enough, we still draw inspiration from Spielman et al.  in Sec. 4.

## 8 Limitations

We focus on recovering the inputs to fully connected layers with ReLU activations such as they occur at the beginning of fully connected networks or as aggregation layers of many other architectures. Extending our approach to other layers is an interesting direction for future work.

Further, our approach scales exponentially with batch size \(b\). While SPEAR's massive parallelizability and its ability to be combined with optimization-based attacks, as shown in Sec. 6.3, can partially mitigate the computational complexity, future research is still required to make reconstruction of batches of size \(b>100\) practical.

## 9 Conclusion

We propose SPEAR, the first algorithm permitting batches of \(b>1\) elements to be recovered exactly in the honest-but-curious setting. We demonstrate theoretically and empirically that SPEAR succeeds with high probability and that our highly parallelized GPU implementation is effective across a wide range of settings, including batches of up to \(25\) elements and large networks and inputs.

We thereby demonstrate that contrary to prior belief, an exact reconstruction of batches is possible in the honest-but-curious setting, suggesting that federated learning on ReLU networks might be inherently more susceptible than previously thought. To still protect client privacy, large effective batch sizes, obtained, e.g., via secure aggregation across a large number of clients, might prove instrumental by making reconstruction computationally intractable.

#### Acknowledgments

This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure).

This work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them.

The work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI).