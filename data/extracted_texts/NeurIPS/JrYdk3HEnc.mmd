# Online Control with Adversarial Disturbance for Continuous-time Linear Systems

Jingwei Li

IIIS, Tsinghua University

Shanghai Qizhi Institute

ljw22@mails.tsinghua.edu.cn

&Jing Dong

The Chinese University of Hong Kong, Shenzhen

jingdong@link.cuhk.edu.cn

Can Chang

IIIS, Tsinghua University

cc22@mails.tsinghua.edu.cn

&Baoxiang Wang

The Chinese University of Hong Kong, Shenzhen

bxiangwang@cuhk.edu.cn

Jingzhao Zhang

IIIS, Tsinghua University

Shanghai Qi zhi Institute

jingzhaoz@mail.tsinghua.edu.cn

###### Abstract

We study online control for continuous-time linear systems with finite sampling rates, where the objective is to design an online procedure that learns under non-stochastic noise and performs comparably to a fixed optimal linear controller. We present a novel two-level online algorithm, by integrating a higher-level learning strategy and a lower-level feedback control strategy. This method offers a practical and robust solution for online control, which achieves sublinear regret. Our work provides the first nonasymptotic results for controlling continuous-time linear systems with finite number of interactions with the system. Moreover, we examine how to train an agent in domain randomization environments from a non-stochastic control perspective. By applying our method to the SAC (Soft Actor-Critic) algorithm, we achieved improved results in multiple reinforcement learning tasks within domain randomization environments. Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems. Furthermore, our work brings practical intuition into controller learning under non-stochastic environments.

## 1 Introduction

A major challenge in robotics is to deploy simulated controllers into real-world. This process, known as sim-to-real transfer, can be difficult due to misspecified dynamics, unanticipated real-world perturbations, and non-stationary environments. Various strategies have been proposed to address these issues, including domain randomization, meta-learning, and domain adaptation . Although they have shown great effectiveness in experimental results, training agents within these setups poses a significant challenge. To accommodate different environments, the strategies developed by agents tend to be overly conservative  or lead to suboptimal outcomes .

In this work, we provide an analysis of the sim-to-real transfer problem from an online control perspective. Online control focuses on iteratively updating the controller after deployment (i.e.,online) based on collected trajectories. Significant progress has been made in this field by applying insights from online learning to linear control problems [2; 1; 12; 19; 11; 8; 6; 16].

Following this line of work, we approach the sim-to-real transfer issue for continuous-time linear systems as a non-stochastic control problem, as explored in previous works [19; 11; 8]. These studies provide regret bounds for an online controller that lacks prior knowledge of system perturbations. However, a gap remains as no previous analysis has specifically investigated continuous-time systems, but real world systems often evolve continuously in time.

Existing literature on online continuous control is limited [42; 22; 13; 32]. Most continuous control research emphasizes the development of model-free algorithms, such as policy iteration, under the assumption of noise absence. Recently,  examined online continuous-time linear quadratic control problem and achieves sublinear regret. However, it relies on the assumption of standard Brownian noise instead of non-stochastic noise that may not always hold true in real-world applications. This leads us to the crucial question:

_Is it possible to design an online non-stochastic control algorithm_

_in a continuous-time setting that achieves sublinear regret?_

Our work addresses this question by proposing a two-level online controller. The higher-level controller symbolizes the policy learning process and updates the policy at a low frequency to minimize regret. Conversely, the lower-level controller delivers high-frequency feedback control input to reduce discretization error. Our proposed algorithm results in regret bounds for continuous-time linear control in the face of non-stochastic disturbances.

Furthermore, we implement the ideas from our theoretical analysis and test them in several experiments. Note that the key difference between our algorithm and traditional online policy optimization is that we utilize information from past states with some skips to enable faster adaptation to environmental changes. Although the aforementioned concepts are often adopted experimentally as frame stacking and frame skipping, there is relatively little known about the appropriate scenarios for applying these techniques. Our analysis and experiments demonstrate that these techniques are particularly effective in developing adaptive policies for uncertain environments. We choose the task of training agents in a domain randomization environment to evaluate our method, and the results confirm that these techniques substantially improve the agents' performance.

## 2 Related Works

The control theory of linear dynamical systems under disturbances has been thoroughly examined in various contexts, such as linear quadratic stochastic control , robust control [37; 23], system identification [17; 24; 9; 25]. However, most of these problems are investigated in non-robust settings, with robust control being the sole exception where adversarial perturbations in the dynamic are permitted. In this scenario, the controller solves for the optimal linear controller in the presence of worst-case noise. Nonetheless, the algorithms designed in this context can be overly conservative as they optimize over the worst-case noise, a scenario that is rare in real-world applications. We will elaborate on the difference between robust control and online non-stochastic control in Section 3.

Online ControlThere has been a recent surge of interest in online control, as demonstrate by studies such as [2; 1; 12]. In online control, the player interacts with the environment and updates the policy in each round aiming to achieve sublinear regret. In scenarios with stochastic Gaussian noise,  provides the first efficient algorithm with an \(O()\) regret bound. However, in real-world applications, the assumption of Gaussian distribution is often unfulfilled.

 pioneers research on non-stochastic online control, where the noises can be adversarial. Under general convex costs, they introduce the Disturbance-Action Policy Class. Using an online convex optimization (OCO) algorithm with memory, they achieve an \(O()\) regret bound. Subsequent studies extend this approach to other scenarios, such as quadratic costs , partial observations [36; 35] or unknown dynamical systems [19; 11]. Other works yield varying theoretical guarantees like online competitive ratio [15; 33].

Online Continuous ControlCompared to online control, there has been relatively little research on model-based continuous-time control. Most continuous control works focus on developing model-freealgorithms such as policy iteration (e.g. [42; 22; 32]), typically assuming zero-noise. This is because analyzing the system when transition dynamics are represented by differential equations, rather than recurrence formulas, poses a significant challenge.

Recently,  studies online continuous-time linear quadratic control with standard Brownian noise and unknown system dynamics. They propose an algorithm based on the least-square method, which estimates the system's coefficients and solves the corresponding Riccati equation. The papers [34; 14] also focus on online control setups with continuous-time stochastic linear systems and unknown dynamics. They achieve \(O( T)\) regret by different approaches.  uses the Thompson sampling algorithm to learn optimal actions.  takes a randomized-estimates policy to balance exploration and exploitation. The main difference between [8; 34; 14] and our paper is that they consider stochastic noise of Brownian motion which can be quite stringent and may fail in real-world applications, while the noise in our setup is non-stochastic. This makes our analysis completely different from theirs.

Domain RandomizationDomain randomization, which is proposed by , is a commonly used technique for training agents to adapt to different (real) environments by training in randomized simulated environments. From the empirical perspective, many previous works focus on designing efficient algorithms for learning in a randomized simulated environment (by randomizing environmental settings, such as friction coefficient) such that the algorithm can adapt well in a new environment, [29; 44; 26; 28; 30]. Other works study how to effectively randomize the simulated environment so that the trained algorithm would generalize well in other environments [43; 27; 38]. However, prior research has not explored how to apply certain theoretical analysis ideas to train agents in domain-randomized environments. Limited previous works, such as  and , concentrate on theoretically analyzing the sim-to-real gap within specific domain randomization models but they do not test their algorithms in real domain randomization environments.

## 3 Problem Setting

In this paper, we consider the online non-stochastic control for continuous-time linear systems. Therefore, we provide a brief overview below and define our notations.

### Continuous-time Linear Systems

The Linear Dynamical System can be considered a specific case of a continuous Markov decision process with linear transition dynamics. The state transitions are governed by the following equation:

\[_{t}=Ax_{t}+Bu_{t}+w_{t}\,,\]

where \(x_{t}\) is the state at time \(t\), \(u_{t}\) is the action taken by the controller at time \(t\), and \(w_{t}\) represents the disturbance at time \(t\). Follow the setup of , we assume \(x_{0}=0\). We do not make any strong assumptions about the distribution of \(w_{t}\), and we also assume that the distribution of \(w_{t}\) is unknown to the learner beforehand. This implies that the disturbance sequence \(w_{t}\) can be selected adversarially.

When the action \(u_{t}\) is applied to the state \(x_{t}\), a cost \(c_{t}(x_{t},u_{t})\) is incurred. Here, we assume that the cost function \(c_{t}\) is convex. However, this cost is not known in advance and is only revealed after the action \(u_{t}\) is implemented at time \(t\). In the system described above, an online policy \(\) is defined as a function that maps known states to actions, i.e., \(u_{t}=(\{x_{}|[0,t]\})\). Our goal, then, is to design an algorithm that determines such an online policy to minimize the cumulative cost incurred. Specifically, for any algorithm \(\), the cost incurred over a time horizon \(T\) is:

\[J_{T}()=_{0}^{T}c_{t}(x_{t},u_{t})dt\,.\]

In scenarios where the policy is linear (i.e., a linear controller), such that \(u_{t}=-Kx_{t}\), we use \(J(K)\) to denote the cost of a policy \(K\) from a certain class \(\).

### Difference between Robust and Online Non-stochastic Control

While both robust and online non-stochastic control models incorporate adversarial noise, it's crucial to understand that their objectives differ significantly.

The objective function for robust control, as seen in [37; 23], is defined as:

\[_{u_{1}}_{w_{1:T}}_{u_{2}}_{u_{t}}_{u_{T}}J_{T}()\,,\]

Meanwhile, the objective function for online non-stochastic control, as discussed in , is:

\[_{}_{w_{1:T}}(J_{T}()-_{K}J_{T }(K))\,.\]

Note that the robust control approach seeks to directly minimize the cost function, while online non-stochastic control targets the minimization of regret, which is the discrepancy between the actual cost and the cost associated with a baseline policy. Additionally, in robust control, the noise at each step can depend on the preceding policy, whereas in online non-stochastic control, all the noise is predetermined (though unknown to the player).

### Assumptions

We operate under the following assumptions throughout this paper. To be concise, we denote \(\|\|\) as the \(L_{2}\) operator norm of the vector and matrix. Firstly, we make assumptions concerning the system dynamics and noise:

**Assumption 1**.: The matrices that govern the dynamics are bounded, meaning \(\|A\|_{A}\) and \(\|B\|_{B}\), where \(_{A}\) and \(_{B}\) are constants. Moreover, the perturbation and its derivative are both continuous and bounded: \(\|w_{t}\|,\|_{t}\| W\), with \(W\) being a constant.

These assumptions ensure that we can bound the states and actions, as well as their first and second-order derivatives. Next, we make assumptions regarding the cost function:

**Assumption 2**.: The costs \(c_{t}(x,u)\) are convex in \(x\) and \(u\). Additionally, if there exists a constant \(D\) such that \(\|x\|,\|u\| D\), then we have the following inequalities of the costs: \(|c_{t}(x,u)| D^{2},\|_{x}c_{t}(x,u)\|, \|_{u}c_{t}(x,u)\| GD\), \(|c_{t_{1}}(x,u)-c_{t_{2}}(x,u)| L|t_{1}-t_{2}|D^{2}\),

where \(\),\(G\) and \(L\) are constants corresponding to the cost function. This assumption implies that if the differences between states and actions are small, then the error in their cost will also be relatively small.

### Strongly Stable Policy

We next describe our baseline policy class introduced in . Note that the continuous system and the discrete system are different. If we consider the approximation over a relatively small interval \(h\), we get

\[x_{t+h}=x_{t}+_{s=t}^{t+h}_{s}ds= x_{t}+_{s=t}^{t+h}Ax_{s}+Bu_{s}+w_{s}ds\] \[ x_{t}+h(Ax_{t}+Bu_{t}+w_{t})=(I+hA)x_{t}+hBu_{t}+hw_{t}\,.\]

Therefore, if we consider the transition of a discrete system \(x_{i+1}=x_{i}+u_{i}+_{i}\), we get the approximation \( I+hA\), \( hB\). Hence, we extend the definition of a strongly stable policy [12; 3] in the discrete system to the continuous system as follows:

**Definition 1**.: A linear policy \(K\) is \((,)\)-strongly stable if, for any \(h>0\) that is sufficiently small, there exist matrices \(L_{h},P\) such that \(I+h(A-BK)=PL_{h}P^{-1}\), with the following two conditions:

1. The norm of \(L_{h}\) is strictly smaller than unity and dependent on \(h\), i.e., \(\|L_{h}\| 1-h\).
2. The controller and transforming matrices are bounded, i.e., \(\|K\|\) and \(\|P\|\), \(\|P^{-1}\|\).

The above definition ensures the system can be stabilized by a linear controller \(K\).

### Regret Formulation

To evaluate the designed algorithm, we follow the setup in [12; 3] and use regret, which is defined as the cumulative difference between the cost incurred by the policy of our algorithm and the cost incurred by the best policy in hindsight. Let \(\) denotes the class of strongly stable linear policies, i.e. \(=\{K:K(,)\}\). Then we try to minimize the regret of algorithm:

\[_{}_{w_{1:T}}()=_{ }_{w_{1:T}}(J_{T}()-_{K}J_{T}(K))\,.\]Algorithm Design

In this section, we outline the design of our algorithm and formally define the concepts involved in deriving our main theorem. We summarize our algorithm design as follows:

First, we discretize the total time period \(T\) into smaller intervals of length \(h\). We use the information at each point \(x_{h},x_{2h},\) and \(u_{h},u_{2h},\) to approximate the actual cost of each time interval, leveraging the continuity assumption. This process does introduce some discretization errors.

Next, we employ the Disturbance-Action policy (DAC) . This policy selects the action based on the current time step and the estimations of disturbances from several past steps. This policy can approximate the optimal linear policy in hindsight when we choose suitable parameters. However, the optimal policy \(K^{*}\) is unknown, so we cannot directly acquire the optimal choice. To overcome this, we employ the OCO with memory framework  to iteratively adjust the DAC policy parameter \(M_{t}\) to approximate the optimal solution \(M^{*}\).

After that, we introduce the concept of the ideal state \(y_{t}\) and ideal action \(v_{t}\) that approximate the actual state \(x_{t}\) and action \(u_{t}\). Note that both the state and policy depend on all DAC policy parameters \(M_{1},M_{2},,M_{t}\). Yet, the OCO with memory framework only considers the previous \(H\) steps. Therefore, we need to consider ideal state and action. \(y_{t}\) and \(v_{t}\) represent the state the system would reached if it had followed the DAC policy \(\{M_{t-H},,M_{t}\}\) at all time steps from \(t-H\) to \(t\), under the assumption that the state \(x_{t-H}\) was \(0\).

From all the analysis above, we can decompose the regret as three parts: the discretization error \(R_{1}\), the regret of the OCO with memory \(R_{2}\), and the approximation error between the ideal cost and the actual cost \(R_{3}\).

Then we will formally introduce out method and define all the concepts. In the subsequent discussion, we use shorthand notation to denote the cost, state, control, and disturbance variables \(c_{ih}\), \(x_{ih}\), \(u_{ih}\), and \(w_{ih}\) as \(c_{i}\), \(x_{i}\), \(u_{i}\), and \(w_{i}\), respectively.

First, we need to define the Disturbance-Action Policy Class(DAC) for continuous systems:

**Definition 2**.: The Disturbance-Action Policy Class(DAC) is defined as:

\[u_{t}=-Kx_{t}+_{i=1}^{l}M_{t}^{i}_{t-i}\,,\]

where \(K\) is a fixed strongly stable policy, \(l\) is a parameter that signifies the dimension of the policy class, \(M_{t}=\{M_{t}^{1},,_{t}^{l}\}\) is the weighting parameter of the disturbance at step \(t\), and \(_{t}\) is the estimated disturbance:

\[_{t}=-x_{t}-h(Ax_{t}+Bu_{t})}{h}\,.\] (1)

We note that this definition differs from the DAC policy in discrete systems  as we utilize the estimation of disturbance over an interval \([t,t+h]\) instead of only the noise in time \(t\). It counteracts the second-order residue term of the Taylor expansion of \(x_{t}\) and is also an online policy as it only requires information from the previous state.

Our higher-level controller adopts the OCO with memory framework. A technical challenge lies in balancing the approximation error and OCO regret. To achieve a low approximation error, we desire the policy update interval \(H\) to be inversely proportional to the sampling distance \(h\). However, this relationship may lead to large OCO regret. To mitigate this issue, we introduce a new parameter \(m=()\), representing the lookahead window. We update the parameter \(M_{t}\) only once every \(m\) iterations, further reducing the OCO regret without negatively impacting the approximation error:

\[M_{t+1}=_{}(M_{t}- g_{t}(M))& \,,\\ M_{t}&\,.\]

Where \(g_{t}\) is a function corresponding to the loss function \(c_{t}\) and we will introduce later in Algorithm 1. For notational convenience and to avoid redundancy, we denote \(_{[t/m]}=M_{t}\). We can then define the ideal state and action. Due to the properties of the OCO with memory structure, we need to consider only the previous \(Hm\) states and actions, rather than all states. As a result, we introduce the definition of the ideal state and action. During the interval \(t[im,(i+1)m-1]\), the learning policy remains unchanged, so we could define the ideal state and action follow the definition in :

**Definition 3**.: The ideal state \(y_{t}\) and action \(v_{t}\) at time \(t[im,(i+1)m-1]\) are defined as

\[y_{t}=x_{t}(_{i-H},...,_{i}),v_{t}=-Ky_{t}+_{j=1}^{l}M_{i}^ {j}w_{t-i}\,.\]

where the notation indicates that we assume the state \(x_{t-H}\) is \(0\) and that we apply the DAC policy \((_{i-H},,_{i})\) at all time steps from \(t-Hm\) to \(t\).

We can also define the ideal cost in this interval follow the definition in :

**Definition 4**.: The ideal cost function during the interval \(t[im,(i+1)m-1]\) is defined as follows:

\[f_{i}(_{i-H},,_{i})=_{t=im}^{(i+1)m-1}c _{t}(y_{t}(_{i-H},,_{i}),v_{t}( _{i-H},,_{i}))\,.\]

With all the concepts presented above, we are now prepared to introduce our algorithm:

``` Input: step size \(\), sample distance \(h\), policy update parameters \(H,m\), parameters \(,,T\).  Define sample numbers \(n= T/h\), OCO policy update times \(p= n/m\).  Define DAC policy update class \(=\{=\{^{1}^{Hm}\} :\|^{i}\| 2h^{3}(1-)^{i-1}\}\).  Initialize \(M_{0}\) arbitrarily. for\(k=0,,p-1\)do for\(s=0,,m-1\)do  Denote the discretization time \(r=km+s\).  Use the action \(u_{t}=-Kx_{r}+h_{i=1}^{Hm}_{i}^{i}_{r-i}\) during the time period \(t[rh,(r+1)h]\).  Observe the new state \(x_{r+1}\) at time \((r+1)h\) and record \(_{r}\) according to Equation (1). endfor  Define the function \(g_{k}(M)=f_{k}(M,,M)\).  Update OCO policy \(_{k+1}=_{}(_{k}- g_{k}(_{k}))\). endfor ```

**Algorithm 1** Continuous two-level online control algorithm

## 5 Main Result

In this section, we present the primary theorem of online continuous control regret analysis:

**Theorem 1**.: _Under Assumption 1, 2, a step size of \(=(})\), and a DAC policy update frequency \(m=()\), Algorithm 1 attains a regret bound of_

\[J_{T}()-_{K}J_{T}(K) O(nh(1-h)^{})+O()+O(Th)\,.\]

_With the sampling distance \(h=(})\), and the OCO policy update parameter \(H=((T))\), Algorithm 1 achieves a regret bound of_

\[J_{T}()-_{K}J_{T}(K) O( (T))\,.\]

Theorem 1 demonstrates a regret that matches the regret of a discrete system . Despite the analysis of a continuous system differing from that of a discrete system, we can balance discretization error, approximation error, and OCO with memory regret by selecting an appropriate update frequency for the policy. Here, \(O()\) and \(()\) are abbreviations for the polynomial factors of universal constants in the assumption.

While we defer the detailed proof to the appendix, we outline the key ideas and highlight them below.

Challenge and Proof SketchWe first explain why we cannot directly apply the methods for discrete nonstochastic control from  to our work. To utilize Assumption 2, it is necessary first to establish a union bound over the states. In a discrete-time system, it can be easily proved by applying the dynamics inequality \(\|x_{t+1}\| a\|x_{t}\|+b\) (where \(a<1\)) and the induction method presented in . However, for a continuous-time system, a different approach is necessary because we only have the differential equation instead of the state recurrence formula.

To overcome this challenge, we employ Gronwall's inequality to bound the first and second-order derivatives in the neighborhood of the current state. We then use these bounded properties, in conjunction with an estimation of previous noise, to bound the distance to the next state. Through an iterative application of this method, we can argue that all states and actions are bounded.

Another challenge is that we need to discretize the system but we must overcome the curse of dimensionality caused by discretization. In continuous-time systems, the number of states is inversely proportional to the discretization parameter \(h\), which also determines the size of the OCO memory buffer. Our regret is primarily composed of three components: the error caused by discretization \(R_{1}\), the regret of OCO with memory \(R_{2}\) and the difference between the actual cost and the approximate cost \(R_{3}\). The discretization error \(R_{1}\) is \(O(hT)\), therefore if we achieve \(O()\) regret, we must choose \(h\) no more than \(O(})\).

If we update the OCO with memory parameter at each timestep follow the method in , we will incur the regret of OCO with memory \(R_{2}=O(H^{2.5})\). The difference between the actual cost and the approximate cost \(R_{3}=O(T(1-h)^{H})\). To achieve sublinear regret for the third term, we must choose \(H=O()\), but since \(h\) is no more than \(O(})\), \(H\) will be larger than \(()\), therefore the second term \(R_{2}\) will definitely exceed \(O()\).

Therefore, we adjust the frequency of updating the OCO parameters by introducing a new parameter \(m\), using a two-level approach and update the OCO parameters once in every \(m\) steps. This will incur the third term \(R_{3}=O(T(1-h)^{Hm})\) but keep the OCO with memory regret \(R_{2}=O(H^{2.5})\), so we can choose \(H=O()\) and \(m=O()\). Then the term of \(R_{2}\) is \(O( T)\) and we achieve the same regret compare with the discrete system.

## 6 Experiments

In this section, we apply our theoretical analysis to the practical training of agents. First we highlight the key difference between our algorithm and traditional online policy optimization.

1. _Stack:_ While standard online policy optimization learns the optimal policy from the current state \(u_{t}=(x_{t})\), an optimal non-stochastic controller employs the DAC policy as outlined in Definition 2. Leveraging information from past states aids the agent in adapting to dynamic environments.
2. _Skip:_ Different from the analysis in , in a continuous-time system we update the state information every few steps, rather than updating it at every step. This solves the curse of dimensionality caused by discretization in continuous-time system.

The above inspires us with an intuitive strategy for training agents by stacking past observations with some observations to skip. We denote this as _Stack & skip_ for convenience. _Stack & skip_ is frequently used as a heuristic in reinforcement learning, yet little was known about when and why such a technique could boost agent performance.

How should we evaluate our algorithm in a non-stochastic environment? We opt for learning an optimal policy within a domain randomization environment. In this context, each model's parameters are randomly sampled from a predetermined task distribution. We train policies to optimize performance across various simulated models .

Figure 1: Bounding the states and their derivatives separately. We employ Gronwall’s inequality with the induction method to bound the states.

We observe that learning in Domain Randomization (DR) significantly differs from stochastic or robust learning problems. In DR, sampling from environmental variables occurs at the beginning of each episode, rather than at every step, distinguishing it from stochastic learning where randomness is step-wise independent and identically distributed. This episodic sampling approach allows agents in DR to exploit environmental conditions and adapt to episodic changes within an episode. On the other hand, robust learning focuses on worst-case scenarios depending on an agent's policy. DR, in contrast, is concerned with the distribution of conditions aimed at broad applicability rather than worst-case perturbations.

In the context of non-stochastic control, the disturbance, while not disclosed to the learner beforehand, remains fixed throughout the episode and does not adaptively respond to the control policy. This setup in non-stochastic control shows a clear parallel to domain randomization: fixed yet unknown disturbances in non-stochastic control mirror the unknown training environments in DR. As the agent continually interacts with these environments, it progressively adapts, mirroring the adaptive process observed in domain randomization. Therefore, we propose evaluating our algorithm within a domain randomization training task. Subsequently, we introduce the details of our experimental setup:

Environment SettingWe conduct experiments on the hopper, half-cheetah, and walker2d benchmarks using the MuJoCo simulator . The randomized parameters include environmental physical parameters such as damping and friction, as well as the agent properties such as torso size. We set the range of our domain randomization to follow a distribution with default parameters as the mean value, shown in Table 1. When training in the domain randomization environment, the parameter is uniformly sampled from this distribution. To analyze the result of generalization, we only change one of the parameters and keep the other parameters as the mean of its distribution in each test environment. We conducted experiments using NVIDIA A40 graphics card.

Algorithm Design and BaselineWe design a practical meta-algorithm that converts any standard deep RL algorithm into a domain-adaptive algorithm, shown in Figure 2. In this algorithm, we augment the original state observation \(o_{t}^{}\) at time \(t\) with past observations, resulting in \(o_{t}^{}=[o_{t}^{},o_{t-m}^{},,o_{t- (h-1)m}^{}]\). Here \(h\) is the number of past states we leverage and \(m\) is the number of states we skip when we get each of the past states. For clarity in our results, we selected the SAC algorithm for evaluation. We use a variant of Soft Actor-Critic (SAC)  and leverage past states with some skip as our algorithm. We compare our algorithm with the standard SAC algorithm training on domain randomization environments as our baseline.

Impact of Frame Stack and Frame SkipTo understand the effects of the frame stack number \(h\) and frame skip number \(m\), we carried out experiments in the hopper environment with different \(h\) and \(m\). For each parameter we train with 3 random seeds and take the average. Figure 3 shows that the performance increases significantly when the frame stack number is increased from \(1\) to \(3\), and

 
**Environment** & **Parameters** & **DR distribution** \\   & Joint damping & [0.5, 1.5] \\  & Foot friction &  \\  & Height of head & [1.2, 1.7] \\  & Torso size & [0.025, 0.075] \\   & Joint damping & [0.005, 0.015] \\  & Foot friction &  \\  & Torso size & [0.04, 0.06] \\   & Joint damping & [0.05, 0.15] \\  & Density &  \\   & Torso size & [0.025, 0.075] \\  

Table 1: The DR distributions of environment.

Figure 2: Leverage past observation of states with some skip.

remains roughly unchanged when the frame stack number continues to climb up. Figure 4 shows that the optimal frame skip number is \(3\), while both too large or too small frame skip numbers result in sub-optimal results. Therefore, in the following experiments we fix the parameter \(h=3\), \(m=3\). We train our algorithm with this parameter and standard SAC on hopper and test the performance on more environments. Figure 5 shows that our algorithm outperforms the baseline in all environments.

Results on Other EnvironmentsEach algorithm was trained using three distinct random seeds in the half-cheetah and walker2d domain randomization (DR) environments. Consistent with previous experiments, we employed a frame stack number of \(h=3\) and frame skip number of \(m=3\). The comparative performance of our algorithm and the baseline algorithm, across various domain parameters, is presented in Figure 6. The result clearly demonstrates that our algorithm consistently outperforms the baseline in all evaluated test environments.

Figure 5: Agents’ reward in various test environments of hopper.

Conclusion, Limitations and Future Directions

In this paper, we propose a two-level online controller for continuous-time linear systems with adversarial disturbances, aiming to achieve sublinear regret. This approach is grounded in our examination of agent training in domain randomization environments from an online control perspective. At the higher level, our controller employs the Online Convex Optimization (OCO) with memory framework to update policies at a low frequency, thus reducing regret. The lower level uses the DAC policy to align the system's actual state more closely with the idealized setting.

In our empirical evaluation, applying our algorithm's core principles to the SAC (Soft Actor-Critic) algorithm led to significantly improved results in multiple reinforcement learning tasks within domain randomization environments. This highlights the adaptability and effectiveness of our approach in practical scenarios.

It is important to note that our theoretical analysis depends on the known dynamics of the system and the assumption of convex costs. This reliance could represent a limitation to our method, as it may not adequately address scenarios where these conditions do not hold or where system dynamics are incompletely understood. For future research, there are several promising directions in online non-stochastic control of continuous-time systems. These include extending our methods to systems with unknown dynamics, exploring the impact of assuming strong convexity in cost functions, and shifting the focus from regret to the competitive ratio. Further research can also explore how to utilize historical information more effectively to enhance agent training in domain randomization environments. This might involve employing time series analysis instead of simply incorporating parameters into neural network training.