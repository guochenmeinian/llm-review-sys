# Faster Margin Maximization Rates for Generic Optimization Methods

Guanghui Wang\({}^{1}\), Zihao Hu\({}^{1}\), Vidya Muthukumar\({}^{2,3}\), Jacob Abernethy\({}^{1,4}\)

\({}^{1}\)College of Computing, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

\({}^{3}\)School of Industrial and Systems Engineering, Georgia Institute of Technology

\({}^{4}\)Google Research, Atlanta

{gwang369,zihaohu,vmuthukumar8}@gatech.edu, abernethyj@google.com

###### Abstract

First-order optimization methods tend to inherently favor certain solutions over others when minimizing a given training objective with multiple local optima. This phenomenon, known as _implicit bias_, plays a critical role in understanding the generalization capabilities of optimization algorithms. Recent research has revealed that gradient-descent-based methods exhibit an implicit bias for the \(_{2}\)-maximal margin classifier in the context of separable binary classification. In contrast, generic optimization methods, such as mirror descent and steepest descent, have been shown to converge to maximal margin classifiers defined by alternative geometries. However, while gradient-descent-based algorithms demonstrate fast implicit bias rates, the implicit bias rates of generic optimization methods have been relatively slow. To address this limitation, in this paper, we present a series of state-of-the-art implicit bias rates for mirror descent and steepest descent algorithms. Our primary technique involves transforming a generic optimization algorithm into an online learning dynamic that solves a regularized bilinear game, providing a unified framework for analyzing the implicit bias of various optimization methods. The accelerated rates are derived leveraging the regret bounds of online learning algorithms within this game framework.

## 1 Introduction

The training objective in the optimization of modern over-parametrized ML models typically presents various local optima with low training error. Despite this, empirical studies have demonstrated that first-order optimization methods generally converge to the solution with strong generalization, even without any explicit regularization (Neyshabur et al., 2014; Zhang et al., 2021). This observation has spurred interest in the study of the _implicit bias_ of the algorithm. In other words, _among all potential parameter choices with low training error, which ones are inherently favored by optimization methods?_

For the classical linear classification problem with separable data, the pioneering works (Soudry et al., 2018; Ji and Telgarsky, 2018) reveal that minimizing the (unregularized) empirical risk with exponential loss by the classical gradient descent (GD) automatically _maximizes the \(\|\|_{2}\)-margin_, meaning its margin converges to that of the best classifier within an \(_{2}\)-norm ball (referred to as the \(\|.\|_{2}\)-maximal margin classifier). This finding implies that GD exhibits an _implicit bias_ towards the \(\|.\|_{2}\)-maximal margin classifier, which helps account for its favorable generalization performance. However, these works show that GD only maximizes the \(\|\|_{2}\)-margin at a slow \(O()\) rate, where \(T\) is the time horizon, and \(n\) is the cardinality of the data set. Since then, several _faster_ marginmaximization rates have been reported. Nacson et al. (2019) revealed that for the exponential loss, GD with an aggressive step size attains an \(O(})\|\|_{2}\)-margin maximization rate. This result was later improved to \(O()\) by Ji & Telgarsky (2021), via an elegant primal-dual analysis. Recent work (Ji et al., 2021; Wang et al., 2022b) further proved that momentum-based GD and Nesterov-accelerated GD can maximize the \(\|\|_{2}\)-margin at an \(O(})\) rate.

As the implicit bias of gradient-descent-based methods becomes better understood, it is natural to explore similar characterizations for other optimization methods. Note that, since gradient-descent-based methods are biased towards the \(\|\|_{2}\)-maximal margin classifier, they might generalize poorly when the data does not adhere to the \(_{2}\)-geometry, as shown in Gentile (2000); Chen et al. (2001). This limitation suggests that it is essential to study the implicit bias of alternative optimization methods that could potentially be biased in different directions. Two such methods include steepest descent with respect to different norms and mirror descent with different potentials. For instance, Gunasekar et al. (2018) demonstrate that for the exponential loss, the steepest descent algorithm with respect to a general norm \(\|\|\) asymptotically converges to the corresponding \(\|\|\)-maximal margin classifier. This result implies that the steepest descent algorithm can adapt to different data geometries by changing the norm used in the algorithm. On the other hand, Sun et al. (2022) show that the mirror descent algorithm with the potential \(\|\|_{q}^{q}\) for \(q>1\) can maximize the \(\|\|_{q}\)-margin at a rate of \(O(})\).

While the asymptotic directional convergence of these generic optimization methods is well-understood, a natural question remains: _can generic optimization methods (e.g., mirror descent and steepest descent) achieve faster margin maximization rates?_ Several papers have contributed partial answers to this question. Li et al. (2021) show that mirror descent with an aggressive step size maximizes the margin in an \(O(})\) rate. However, their analysis assumes the potential function is _both strongly-convex and smooth_ with respect to some norm, and thus is mainly limited to the \(_{2}\)-geometry. For steepest descent with respect to a general norm \(\|\|\), Nacson et al. (2019) prove that an \(O(})\|\|\)-margin maximization rate can be achieved with an appropriately chosen step size, but it is unclear whether it can be further improved. In this paper, we provide the fastest known rates for margin maximization for generic optimization methods, through the following contributions:

* First, we study a _weighted-average_ version of mirror descent with the squared \(_{q}\)-norm \(\|\|_{q}^{2}\) as the potential for \(q(1,2]\). This potential function is strongly convex but not smooth. We show that, with an appropriately chosen step size, the algorithm achieves a faster \(\|\|_{q}\)-margin maximization rate on the order of \(O()\). We also further improve the rate to \(O(+})\) with a more aggressive step size. When \(q=2\), the algorithm reduces to average GD, and our rate \(O(+})\) is a \( n\)-factor tighter than the \(O()\) rate of the last-iterate GD (Ji & Telgarsky, 2021).
* Next, for the steepest descent algorithm with respect to the \(_{q}\)-norm for \(q(1,2]\), we show the margin maximization rate can be improved from \(O(})\) to \(O()\).
* Finally, we demonstrate that a even faster \(O((q-1)})\|\|_{q}\)-margin maximization rate can be achieved in two ways: a) mirror descent with Nesterov acceleration, or b) steepest descent with extra gradient and momentum.

The essential premise for our approach is that _minimizing empirical risk (ERM) with generic optimization methods can be equivalently viewed as solving a regularized bilinear game with online learning dynamics_. Within this framework, we design new pairs of online learning methods whose outputs (and, by extension, the outputs of the corresponding generic optimization methods) automatically maximize the margin. The convergence rates are determined by the time-averaged regret bounds of these online learning algorithms when played against each other, which turn out to be much faster than the worst-case \(O(1/)\) rate.

Wang et al. (2022b) were the first to draw parallels between _Nesterov-accelerated GD_ for ERM and solving the bilinear game through an online dynamic. However, it was still open that whether this kind of analysis suits other GD-based methods. Moreover, the non-linearity of the mirror map in generic optimization methods makes analysis particularly challenging. In this paper, we reveal that the game framework can in fact encompass implicit bias analysis for a range of generic optimization methods, and offer a more streamlined and unified analysis. Within this game framework, we derive several other results beyond those mentioned above:

* By selecting suitable online learning algorithms, we obtain a momentum-based data-dependent MD algorithm with an \(O(_{T}}{T^{2}(q-1)}+}) \|_{q}\)-margin maximization rate, where \(_{T}=_{t=2}^{T}\|_{t}-_{t-1}\|_{1}^{2}\) is the path-length of a series of distributions on the training data \(_{t}\). In the worst case, this reduces to the margin maximization rate of MD, but this can be much tighter if \(_{T}\) is sublinear in \(T\).
* Apart from margin maximization rates, we also demonstrate the corresponding directional error, i.e., the bound on the \(_{q}\)-distance between the maximal margin classifier and the normalized output of the generic methods, which are also controlled by the regret bounds of two-players against each other. This kind of convergence rates are new for most of the generic methods. In general, we prove that the directional errors are typically a square-root factor worse than the margin maximization rates.
* For our steepest descent, by setting the norm to the general norm \(\|\|\) and the \(_{2}\)-norm respectively, we can recover the algorithms and theoretical guarantees in Nacson et al. (2019); Ji and Telgarsky (2021) under the game framework. This implies that these algorithms can also be viewed as solving a _regularized_ bilinear game using online learning algorithms, offering a deeper understanding of the role of implicit bias in optimization methods.

Additional related workThe strategy of solving a zero-sum game using online learning algorithms playing against each other has been extensively studied, primarily through the lens of _independent learning agents_(e.g., Rakhlin and Sridharan, 2013; Daskalakis et al., 2018; Wang and Abernethy, 2018; Daskalakis and Panageas, 2019; Zhang et al., 2022). In contrast, our central motivation and challenge lies in identifying the exact equivalent forms of generic optimization algorithms under the regularized bilinear game dynamic. Our framework is also motivated by the line of research that employs the _Fenchel-game_ to elucidate commonly used convex optimization methods (Abernethy et al., 2018; Wang and Abernethy, 2018; Wang et al., 2021b). However, our framework diverges significantly from these approaches. These works focus on the convergence of the optimization problem itself, while our framework emphasizes that the choice of optimization algorithm, which solely targets the minimization of empirical risk, has a significant impact on maximizing the margin, which we might view as an "algorithmic externality." Max-margin guarantees can not arise from convergence of the ERM objective alone, as there are typically multiple global minima in ERM minimization. Our analysis also considers an entirely different min-max problem than that of the Fenchel game (Wang et al., 2021b). Consequently, the correspondences we establish between optimization algorithms and online dynamics also differ. Finally, we note that previous work has also analyzed the implicit bias through direct primal optimization analysis (e.g., Nacson et al., 2019; Sun et al., 2022) or using a dual perspective (e.g., Ji and Telgarsky, 2021; Ji et al., 2021). For the former analysis, it is unclear whether and how faster rates can be obtained. For the latter, it remains an open question how to extend the framework beyond \(_{2}\)-geometry, which in some sense was the motivation for the present work. For more related work, we refer the reader to Appendix A.

## 2 Preliminaries

In this section, we present our basic setting along with some standard assumptions and definitions.

NotationWe use lower case bold face letters \(\), \(\) to denote vectors, lower case letters \(a,b\) to denote scalars, and upper case bold face letters \(,\) to denote matrices. For a vector \(^{d}\), we use \(x_{i}\) to denote the \(i\)-th component of \(\). For a matrix \(^{n d}\), let \(_{(i,:)}\) be its \(i\)-th row, \(_{(:,j)}\) the \(j\)-th column, and \(_{(i,j)}\) the \(i\)-th element of the \(j\)-th column. \(^{d}\), we use \(\|\|\) to denote a general norm in \(^{d}\), \(\|\|_{*}\) its dual norm, \(\|\|_{q}\) the \(q\)-norm of \(\), defined as \(\|\|_{q}=(_{i=1}^{d}|x_{i}|^{q})^{1/q}\), and \(q(1,2]\). We use \(\|\|_{p}\) to denote the dual norm of \(q\)-norm, where \(p[2,)\), \(+=1\)We denote \(_{\|\|}\) the \(\|\)\(\|\)-ball, defined as \(_{\|\|}=\{^{d}|\|\| 1\}\). \(,^{}^{d}\), we define the Bregman divergence between \(\) and \(^{}\) with respect to a strictly convex potential function \(()\) as \(D_{}(,^{})=()-(^{ })-(^{})^{}(-^{})\). For a positive integer \(n\), we denote \(\{1,,n\}\) as \([n]\), and the \((n-1)\)-dimensional simplex as \(^{n}\). Let \(E:^{n}\) be the negative entropy function, defined as \(E()=_{i=1}^{n}p_{i} p_{i},^{n}\).

Basic settingConsider a set of \(n\) data points \(=\{(^{(i)},y^{(i)})\}\), where \(^{(i)}^{d}\) is the feature vector for the \(i\)-th example, and \(y^{(i)}\{-1,+1\}\) the corresponding binary label. We are interested the optimization trajectory of first-order methods for minimizing the following unbounded and unregularized empirical risk:

\[_{^{d}}L()=_{i=1}^{n}r( ^{}^{(i)};y^{(i)}),\] (1)

where \(^{d}\) is a linear classifier, \(r:\{ 1\}\) is the loss function. Following previous work, we focus on the exponential loss, given by \(r(^{};y)=(-y^{})\). We introduce the following standard assumption and definitions.

**Definition 1** (\(\|\|\)-margin).: _For a linear classifier \(^{d}\) and a norm \(\|\|\), we define its \(\|\|\)-margin as_

\[()=y^{(i)}^{} ^{(i)}}{\|\|}=^{n}} ^{}}{\|\|},\]

_where \(=[;y^{(i)}^{(i)};]^{n d}\) is the matrix that contains all data._

**Assumption 1**.: _Assume \(\) is linearly separable and bounded with respect to some norm \(\|\|\). More specifically, we assume \(_{\|\|}^{*}_{\|\|}\), s.t., \(_{\|\|}^{*}=*{argmax}_{\|\| 1}_{i [n]}y^{(i)}^{(i)},\) whose margin \((_{\|\|}^{*})=>0\). We refer to \(_{\|\|}^{*}\) as the \(\|\|\)-maximal margin classifier. Note that, for any \(^{d}\), if \(()=\), \(\) and \(_{\|\|}^{*}\) are at the same direction. Finally, we also assume \(\) is bounded wrt some dual norm \(\|\|_{*}\), i.e., \( i[n]\), \(\|^{(i)}\|_{*} 1\)._

**Definition 2** (\(\|\|\)-Margin maximization rate and \(\|\|\)-directional error).: _Suppose Assumption 1 is satisfied. We consider a sequence of solutions \(_{1},,_{\|\|}\), and state that \(_{t}\) converges to \(_{\|\|}^{*}\) if either \(_{t}(_{t})\), or \(_{t}\|_{t}}{\|_{t}\|}-_{\| \|}^{*}\| 0\). We define the upper bound on \(|-(_{t})|\) the \(\|\|\)-margin maximization rate, and \(\|_{t}}{\|_{t}\|}-_{\|\|}^{*}\|\) the \(\|\|\)-directional error._

## 3 A Game Framework for Maximizing the Margin

In this section, we present a general game framework and demonstrate that solving this game with online learning algorithms can directly maximize the margin and minimize the directional error.

Figure 1: Illustration of the game framework for implicit bias analysis. In Section 3, we show that solving a regularized bilinear game with online learning algorithms (top box) can directly maximize the margin, and the convergence rate is on the same order of the averaged regret \(C_{T}\) (right box); In Sections 4, we prove that minimizing the empirical risk with a series of generic optimization methods (left box) is equivalent to using online learning algorithms to solve the regularized bilinear game. Thus, the implicit bias rates can be directly obtained by plugging in the regret bounds.

Then, in Section 4, we show that many generic optimization methods can be considered as solving this game with different online dynamics. As a result, the margin maximization rate (and also the directional error) of these optimization methods are exactly characterized by the regret bounds of the corresponding online learning algorithms. We illustrate this procedure in Figure 1. The game objective is defined as follows:

\[_{^{d}}_{^{n}}g(, )=^{}-(),\] (2)

where \(()=\|\|^{2}\) is a regularizer and \(\|\|\) denotes some _general norm_ in \(^{d}\). Following previous work (Wang et al., 2021, 2022b), we apply a weighted no-regret dynamic protocol (summarized in Protocol 1) to solve the game. We first give a brief introduction of Protocol 1, and then present the theorem about the margin of its output.

Description of Protocol 1In Protocol 1, the players of the zero-sum game try to find the equilibrium by applying online learning algorithms. In each round \(t\), the \(\)-player first picks a decision \(_{t}\), and passes a weighted loss function to the \(\)-player, defined as

\[_{t}h_{t}()=-_{t}(_{t}^{} -())=-_{t}g(_{t},).\]

Then, the \(\)-player observes the loss, picks a decision \(_{t}\), and passes a weighted loss function

\[_{t}_{t}()=_{t}(^{} _{t}-(_{t}))=_{t}g(,_{t}),\]

to the \(\)-player. Note that the order of the two players can also be reversed. After \(T\) iterations, the algorithm outputs the weighted sum of the \(\)-player's decisions: \(}_{T}=_{t=1}^{T}_{t}_{t}\). Under this framework, we define the weighted regret upper bound of both players respectively as

\[_{t=1}^{T}_{t}_{t}(_{t})-_{^{n }}_{t=1}^{T}_{t}_{t}()_{T}^{},\ \ \ \ _{t=1}^{T}_{t}h_{t}()-_{^{d}} _{t=1}^{T}_{t}h_{t}()_{T}^{}.\]

Denote the upper bound on the _average_ weighted regret by \(C_{T}=(_{T}^{}+_{T}^{})/_{t=1}^{T }_{t}\). We have the following conclusion on the margin and directional error of \(}_{T}\). The proof of this theorem can be found in Appendix B.

**Theorem 1**.: _Suppose Assumption 1 holds with respect to some general norm \(\|\|\). Consider solving the two-player zero-sum game defined in (2) by applying Protocol 1. Then \(}_{T}\) will have a positive margin on round \(T\) if \(C_{T}}{4}\). Moreover, as long as \(C_{T}}{4}\), we have_

\[^{n}}^{} }_{T}}{\|}_{T}\|}-}{^{2}}.\] (3)

_If \(()\) is \(\)-strongly convex with wrt \(\|\|\), we have_

\[\|}_{T}}{\|}_{T}\|}- _{\|\|}^{*}\|}{^{2}}}.\]

Theorem 1 shows that the output of Protocol 1, denoted as \(}_{T}\), achieves a positive margin when the average regret \(C_{T}}{4}\). In the following sections, we demonstrate that with appropriately chosen online learning algorithms \(C_{T}\) always decreases with respect to \(T\); in fact \(C_{T} 0\) as \(T\). Therefore, once the condition \(C_{T}}{4}\) is met for a particular value \(T_{0}\), it will also be met for all \(T T_{0}\). Thereafter, \(}_{T}\) continues to increase the \(\|\|\)-margin and converges to the maximum \(\|\|\)-margin classifier, and the rate is directly characterized by \(C_{T}\). Since \(C_{T}\) is the average regret of the online learning algorithms, better bounds on \(C_{T}\) lead to a less stringent condition on large enough \(T\). Finally, we note that the condition on sufficiently large \(T\) is also (explicitly or implicitly) required in all previous work on the non-asymptotic margin maximization rates of generic methods (Nacson et al., 2019; Li et al., 2021; Sun et al., 2022). We refer to Appendix A for more details.

## 4 Implicit Bias of Generic Methods

In this section, we show that average mirror descent and steepest descent can find their equivalent online learning forms under Protocol 1. Thus, their margin maximization rates can be directly characterized by the corresponding average regret \(C_{T}\). For clarity, we use \(_{t}\) to denote the classifier updates in the original methods, and \(_{t}\) the update under the game framework. Note that Theorem 1 clearly implies that the convergence rate of the directional error is always a square-root worse than that of the margin maximization rate. Due to space limitations, we only present the margin maximization rates, while the corresponding rates on directional error are presented in the appendix.

### Mirror-Descent-Type of Methods

First, we consider minimizing (1) by applying the following mirror descent algorithm:

\[_{t}=*{argmin}_{^{d}}_{t}  L(_{t-1}),+D_{}( ,_{t-1}),\] (4)

where \(D_{}(,_{t-1})\) is the Bregman divergence between \(\) and \(_{t-1}\), and \(()\) is a strongly convex potential function that defines the mirror map. Note that since the feasible domain in (4) is unbounded, we can rewrite the algorithm in the following form:

\[(_{t})=(_{t-1})-_{t} L( _{t-1}).\]

In this paper, we consider weighted-average mirror descent with the squared \(q\)-norm, i.e., \(()=\|\|_{q}^{2}\), where \(q(1,2]\), and demonstrate that this optimization algorithm can enable faster \(\|\|\)-margin maximization rates. The detailed update rule is summarized in the left box of Algorithm 1. It is worth noting that this type of regularizer is \((q-1)\)-strongly convex with respect to \(\|\|_{q}\), and can be updated efficiently in closed form as below1: for each coordinate \(i[d]\), we have

\[_{t,i} =(v_{t-1,i})|v_{t-1,i}|^{q-1}\|_{t-1}\|_{q}^ {2-q}-_{t}[ L(_{t-1})]_{i},\] (5) \[v_{t,i} =(_{t,i})|_{t,i}|^{p-1}\| }_{t}\|_{p}^{2-p}.\]

We make a few final observations about this algorithm: 1) Instead of using the weighted sum \(}_{T}\), we could output the weighted average \(}_{t}}{_{s=1}^{q}_{s}}\) without altering the margin or directional convergence rate. This is attributed to the scale-invariance of the margin, i.e., \( c>0,^{d}\), \(()=(c)\). The same argument applies to the directional error. 2) The use of the weighted average is standard in the analysis of mirror descent (e.g., Section 4.2 of Bubeck et al. (2015)). This paper shows that using non-uniform weights is advantageous for achieving rapid margin maximization rates; 3) The per-round computational complexity of (5) is \(O(d)\), which is similar to that of \(p\)-mirror-descent of Sun et al. (2022). However, we note that the \(p\)-MD algorithm of Sun et al. (2022) does not need to compute the norm of the decision at each round, which could be more efficient in real-world applications where parallel or distributed computation is desired.

For Algorithm 1, we have the following theorem. We present its proof in Appendix C, along with a more general theorem that allows a general configuration of the parameters \(_{t}\), \(_{t}\) and \(_{t}\).

**Theorem 2**.: _Suppose Assumption 1 holds wrt \(\|\|_{q}\)-norm for \(q(1,2]\). For the left box of Algorithm 1, let \(_{t}=_{t-1})}\). For the right box, let \(_{t}=1\), and \(_{1}=1\), \(_{t}=\). Then the methods in the two boxes of Algorithm 1 are identical, in the sense that \(}_{T}=}_{T}\). Moreover, we have the average regret upper bound \(C_{T}=+2 n)( T+2)}{T}\). Therefore, the algorithm achieves a positive margin when \(T\) is sufficiently large such that \(T+2 n)( T+2)}{^{2}}\). We have the convergence rate_

\[^{n}}^{}}_{T}}{\|}_{T}\|_{q}}- +2 n)( T+2)}{^{2}T}=-O (T}),\] (6)

_and_

\[\|}_{T}}{\|}_{T} \|_{q}}-_{\|\|_{q}}^{*}\|_{q}}{ ^{2}}+2 n)( T +2)}{T}}=O(}{^{2}(q-1)}).\]

The first part of Theorem 2 indicates that the mirror descent algorithm can be described as two players using certain cleverly designed online learning algorithms to solve the _regularized_ bilinear game in (2). More specifically, for the \(\)-player, we propose a new and unusual online learning algorithm, which we call _regularized greedy_.

\[_{t}=*{argmin}_{^{n}}_{t} _{t-1}()+_{t}D_{}(,}{n }).\]

Essentially, in round \(t\), the \(\)-player minimizes the previous round's loss function, \(_{t-1}\), plus a regularizer at round \(t\), and the two terms are balanced by the parameters \(_{t}\) and \(_{t}\). On the other hand, we select the _follow-the-leader_\({}^{+}\) algorithm for the \(\)-player:

\[_{t}=*{argmin}_{^{d}}_{j=1} ^{t}_{j}h_{j}(),\]

which returns the solution that minimize the cumulative loss so far. The \(+\) sign in the name is because the algorithm can pick the decision \(_{t}\)_after_ seeing its loss function. This is an interesting and unusual design because the regularized greedy algorithm will clearly suffer a worst-case _linear_ regret for the \(\)-player. Fortunately, for our specific case we are able to prove a sharper _data-dependent_ regret bound for the \(\)-player as below:

\[_{T}^{}=O(_{t=2}^{T}\| _{t}-_{t-1}\|_{q}^{2}+ n T),\]

Therefore, the dominating term (i.e., the first term above) of the \(\)-player's regret bound can be canceled by the \(\)-player's regret bound, given by:

\[_{T}^{}=O(-_{t=2}^{T}\| _{t}-_{t-1}\|_{q}^{2}).\]

Note that the \(\)-player's regret bound is negative as the corresponding algorithm used is _clairvoyant_, i.e. can see the current loss \(_{t}\) before making a decision at round \(t\). This ensures that sublinear (and more generally fast) rates are possible. Note that \(_{t}\) and \(_{t}\) will influence both the regret bound and the algorithm equivalence analysis, so finding the right parameter configuration that works for both is a non-trivial task. We make the choice \(_{t}=}{_{i=1}^{t-1}_{i}}\), which ensures both algorithmic equivalence and sublinear regret bounds.

The second part of Theorem 2 shows that the average regret \(C_{T}\) of Algorithm 1 is on the order of \(O(T})\). Therefore, by plugging in Theorem 1, we observe that the margin shrinks on the order of \(-O((q-1)T})\), and the implicit bias convergence rate is \(O((q-1)}).\) Next, we show an improved rate with a more aggressive step size on the order of \(O(_{t})})\) instead of \(O(_{t})})\). The proof of this result is given in Appendix C.

**Theorem 3**.: _Suppose Assumption 1 holds wrt \(\|\|_{q}\)-norm for \(q(1,2]\). For the left box of Algorithm 1, let \(_{t}=_{t-1})}\), and let the final output be \(}_{T}=_{t=1}^{T}_{t}\). For the right box, let \(_{t}=t\), and \(_{1}=1\), \(_{t}=\). Then the two algorithms are identical, in the sense that \(}_{T}=}_{T}\). Moreover, when \(T+4 n T+1+2 n]}{^ {2}}}\), we have_

\[^{n}}^{}}_{T}}{\|}_{T}\|_{q}}- T(q-1)}-T^{2}},\] (7)

_and_

\[\|}_{T}}{\|}_{T} \|_{q}}-_{\|\|_{q}}^{*}\|_{q}}{ ^{2}}+}}.\]

Observe that the margin maximization rate in Theorem 3 is \(O(T})+O( T^{2}})\). Compared to (6), it has a better dependence on \( n\) and \( T\).

Finally, we focus on a momentum-based mirror descent algorithm, which is given in Algorithm 2. For this algorithm, we have the following guarantee.

**Theorem 4**.: _Suppose Assumption 1 holds wrt \(\|\|_{q}\)-norm for \(q(1,2]\). For the left box of Algorithm 2, let \(_{t}=_{t-1})}\), and \(_{t}=_{t-1})}\). For the second box, let \(_{t}=t\), and \(_{t}=\). Then the methods in the two boxes of Algorithm 2 are identical, in the sense that \(}_{T}=}_{T}\). Moreover, when \(T)}}{}\), we have_

\[^{n}}^{}}_{T}}{\|}_{T}\|_{q}}- {_{t=1}^{T}\|_{t}-_{t-1}\|_{1}^{2}}{^{2}(q-1)T^ {2}}-T^{2}}\] (8)

_and_

\[\|}_{T}}{\|}_{T} \|_{q}}-_{\|\|_{q}}^{*}\|_{q}}{ ^{2}}^{T}\|_{t}-_{t -1}\|_{1}^{2}}{(q-1)T^{2}}+}}.\]

The above theorem shows that, for sufficiently large \(T\), the margin maximization rate can be data-dependent. Note that \(_{t=1}^{T}\|_{t}-_{t-1}\|_{1}^{2} 2T\), so in the worst case, the bound reduces to the results in Theorem 3, but it can become significantly better when \(_{t=1}^{T}\|_{t}-_{t-1}\|_{1}^{2}\) is small. We expect that when \(T\) is very large, \(_{T}\) will change very slowly as we already know that the direction of \(}_{T}\) will converge -- however, turning this into a precise faster rate dependent on the original training data geometry, i.e. \(\), is an intriguing open question.

[MISSING_PAGE_FAIL:9]

the method maintains a momentum term \(_{t}\) with an additional gradient (Step 2), then identifies the steepest direction with respect to \(_{t}\) (Step 3), and applies this direction to update the decision (Step 4). At first glance, these two algorithms appear markedly different. However, we show that with appropriately chosen parameters, they are actually equivalent, in the sense that they both correspond to the online dynamic in the bottom box of Algorithm 4. More specifically, we provide the following theoretical guarantee. The proof is given in Appendix E.

**Theorem 6**.: _Suppose Assumption 1 holds wrt a general norm \(\|\|\), and \(\|\|^{2}\) is \(\)-strongly convex wrt \(\|\|\). For the left box, let \(_{t,1}=\), \(^{}_{t,1}=\), \(_{t,2}=1\), \(^{}_{t,2}=\), and \(_{t}=_{t})}\). For the right box, let \(_{t,3}=\), \(_{t,4}=\), \(^{}_{t,4}=_{t-1}\|_{*}}{4}\), \(^{}_{t,3}=_{t-1}+^{ }_{t,4}_{t-1})}\), and \(_{t}=t\|_{t}\|_{*}\). For the bottom box, let \(c=\), \(_{t}=t\). Then all three methods in Algorithm 4 are identical, in the sense that \(}_{T}=_{T}=}_{T}\). Moreover, when \(T}{}\), we have_

\[^{n}}^{}A}_{ T}}{\|}_{T}\|}-T^{2} },\]

_and_

\[\|_{T}}{\|_{T}\|}-^{*}_{\|\|} \|}{^{2} T}.\]

RemarkTheorem 6 reveals that the two strategies implemented in Algorithm 4 yield an optimal \(O( n/[^{2}T^{2}])\) rate. It is worth noting that a similar online dynamic to the one detailed in the bottom box of Algorithm 4 was also considered by (Algorithm 5, Wang et al., 2022b). Nonetheless, there are some crucial distinctions: 1) Their work only demonstrated that this dynamic could achieve a positive margin, leaving open questions regarding whether the margin can be maximized (i.e., converge to \(\)), and if so, what the margin maximization rate would be; 2) They only presented the online dynamic, without its equivalent optimization form. Finally, we note that, Theorem 6 requires the norm to be strongly convex, which is satisfied for, e.g. the \(q\)-norm when \(q(1,2]\).

## 5 Conclusion and Future Work

This paper examines the implicit bias of generic optimization methods, delivering accelerated margin maximization and directional errors for average mirror descent and steepest descent. Our approach translates _generic optimization methods for ERM_ into _online learning dynamics for a regularized bilinear game_, offering a simpler analysis and a fresh perspective on implicit bias. Despite the effectiveness of the game framework in handling generic methods and accelerated techniques, it presently holds some limitations: 1) the framework is currently operational only for exponential loss, making its extension to handle more general losses a vital area for future research; 2) identifying algorithmic equivalence is nuanced and non-trivial, and it is as yet unresolved whether this framework can elucidate other methods, such as the last-iterate of MD; 3) Finally, it remains to be seen whether more advanced online learning algorithms are beneficial under our framework, such as parameter-free online learning (Orabona & Pal, 2016; Cutkosky & Orabona, 2018) or adaptive online learning methods (van Erven & Koolen, 2016; Wang et al., 2019; Zhang et al., 2019; Wang et al., 2021a).

**Acknowledgments.** GW was supported by a ARC-ACO fellowship provided by Georgia Tech. VM was supported by the NSF (through CAREER award CCF-2239151 and award IIS-2212182), an Adobe Data Science Research Award, an Amazon Research Award and a Google Research Colabs Award. JA was supported by the AI4OPT Institute, as part of NSF Award 2112533, and NSF through Award IIS-1910077.