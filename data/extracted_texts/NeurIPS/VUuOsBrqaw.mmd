# FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features

Jitao Zhao1, Di Jin1, Meng Ge2, Lianze Shan1, Xin Wang1, Dongxiao He1, Zhiyong Feng1

1College of Intelligence and Computing, Tianjin University, China

2Department of Electrical and Computer Engineering, National University of Singapore, Singapore

1{zjtao, jindi, shanlz2119, wangx, hedongxiao, zyfeng}@tju.edu.cn, 2gemeng@nus.edu.sg

Corresponding author

###### Abstract

Graph Neural Networks (GNNs), known for their effective graph encoding, are extensively used across various fields. Graph self-supervised pre-training, which trains GNN encoders without manual labels to generate high-quality graph representations, has garnered widespread attention. However, due to the inherent complex characteristics in graphs, GNNs encoders pre-trained on one dataset struggle to directly adapt to others that have different node feature shapes. This typically necessitates either model rebuilding or data alignment. The former results in non-transferability as each dataset need to rebuild a new model, while the latter brings serious knowledge loss since it forces features into a uniform shape by preprocessing such as Principal Component Analysis (PCA). To address this challenge, we propose a new Feature-Universal Graph contrastive pre-training strategy (FUG) that naturally avoids the need for model rebuilding and data reshaping. Specifically, inspired by discussions in existing work on the relationship between contrastive Learning and PCA, we conducted a theoretical analysis and discovered that PCA's optimization objective is a special case of that in contrastive Learning. We designed an encoder with contrastive constraints to emulate PCA's generation of basis transformation matrix, which is utilized to losslessly adapt features in different datasets. Furthermore, we introduced a global uniformity constraint to replace negative sampling, reducing the time complexity from \(O(n^{2})\) to \(O(n)\), and by explicitly defining positive samples, FUG avoids the substantial memory requirements of data augmentation. In cross domain experiments, FUG has a performance close to the re-trained new models. The source code is available at: https://github.com/hedongxiao-tju/FUG.

## 1 Introduction

Graph Neural Networks (GNNs) , renowned for their effectiveness in encoding both graph topology and feature information, have emerged as the mainstream methods in graph representation learning. They are extensively applied in various scenarios such as protein molecule prediction , e-commerce recommendation systems , and community detection . Training high-quality GNNs typically relies on extensive manually labeled data , which is costly and limits their applications. To address this challenge, graph self-supervised pre-trainings have gained significant attentions [6; 7; 5]. These approaches leverage self-supervised signals mined from the graph data themselves and design proxy tasks to train models without manually labeled data.

Unfortunately, due to the inherent complexity of graph data, models trained on one graph dataset often fail to perform well on another. This complexity arises from two main aspects: 1). topologicaldiversity, where a node in a graph can connect to any number of other nodes, and 2). feature diversity, where nodes represent a variety of information such as textual or float data. In domains like vision , text , and speech , data shares common latent semantics (such as RGB channels in pixels, word vectors, or audio samples), and have relative positional information like spatial relations in vision or sequence relations in text and audio. Unlike these domains, features in graphs are unordered and chaotic. Even within the same citation dataset category using identical feature extraction methods, such as Cora, CiteSeer, and PubMed [11; 12], the node features have different shapes.

Existing GNNs can address the issue of topological diversity through well-designed message-passing and aggregation methods [13; 14]. However, due to the neural networks' requirement for fixed input-output dimensions, GNNs struggle with feature diversity. This necessitates retraining a new graph neural network for datasets with different node feature shapes, significantly restricting the applicability of existing graph self-supervised pre-training models. This leads to some graph pre-training works [6; 15] that have to ignore node attributes and encode only the structural information. Recent "All-In-One" work in graph attempts to reshape node features to the same dimensions through preprocessing methods like Principal Component Analysis (PCA) [16; 17], feature similarity embedding  and language models . However, these methods inevitably lose crucial semantic information related to downstream tasks during data preprocessing. Classical machine learning methods such as PCA cannot participate in training, and they primarily encode the information with the highest variance as principal components, overly focusing on differences between node features and losing substantial homophily information in node features. Encoding informative features into single-dimensional similarity also brings obvious information loss. Language models suffer assumption biases, presuming that all input data is textual, thus ineffectively handling extensive float features. Furthermore, using large language model interfaces or involving language models in training incurs additional significant costs. Additionally, sliding window-based neural networks, such as 1D-CNNs  and LSTMs , also fail to address the issue of feature diversity. While they can handle inputs of varying dimensions, they operate under the prior assumption that the data is sequential, which is not satisfied by graph node features.

Among the feature-unifying approaches discussed earlier, only PCA-like methods exhibit no assumption bias and are adaptable to various feature forms. Their primary limitation lies in the misalignment between their model objectives and the proxy tasks of graph pre-training, coupled with their inability to train based on specific loss functions. Noting discussions in existing works that link contrastive learning with PCA-like methods, which regard contrastive pre-training as a generalized form of non-linear PCA , we draw inspiration to propose a question: _Could a graph contrastive pre-training principle based on PCA theory be developed to address inherent issues of PCA and broadly applied to node features of different shapes?_

In this paper, we theoretically demonstrate that classical PCA and contrastive learning fundamentally follow the same theoretical framework. PCA's optimization objective is a special case of that in contrastive learning. The adaptability of PCA-like methods to various feature shapes stems from their intrinsic component that acts as a columnar encoder for values under specific feature dimension, creating a basis transformation matrix, which we term dimensional encoding. The loss and parameter update mechanisms of contrastive learning could offer traditional PCA methods new goals and training mechanisms that align better with the graph data and downstream tasks. Based on these analyses, we propose a new strategy for **F**eature-**U**niversal **G**raph pre-training (FUG strategy), which includes three steps: dimensional encoding, graph encoding, and relative relationship optimization task.

Guided by this theory, we instantiate a simple FUG model. Simulating PCA, this model utilizes sampling to capture the distribution among different nodes within specific feature dimension for dimensional encoding, and generates basis transformation vectors for feature dimensions which are used to losslessly unify the node features. It employs GNN encoder for graph encoding. We also design an improved and more efficient graph contrastive loss as the relative relationship optimization task. According to this task, FUG effectively learns the relative relations within graph data, and transfers this encoding capability effectively to other datasets with different node feature shapes.

Figure 1: Motivation.

We conducted extensive experiments to validate FUG's efficacy. In in-domain self-supervised learning experiments, FUG demonstrated competitive performance with existing advanced graph self-supervised models. In the cross-domain learning experiments, the FUG trained and tested on different datasets shows similar performance to the model trained and tested on the same dataset. These results prove the strong adaptability and transferability of FUG.

## 2 Notations and Preliminary

In this section, we introduce some preliminary concepts and notations to facilitate analysis in later. We consider a graph data \(=(,)\), where \(=\{v_{1},v_{2},...,v_{n}\}\) represents the set of nodes and \(E V V\) represents the set of edges. \(X^{n d}\) denotes the feature matrix of the nodes. \(A\) is the adjacency matrix representing the topological relationships between nodes, with \(a_{ij}=1\) if \(e_{ij}\) and \(a_{ij}=0\) otherwise.

**Graph Neural Networks (GNNs)**. GNNs can be uniformly described as \(H^{(t+1)}=WH^{(t)}\), where \(H\) represents the embeddings in hidden layer, with \(t=0\) denoting the node features \(X\). \(\) is the message passing matrix, which can be derived through neighbor sampling , random walks, or deep learning  techniques to mine graph data. \(W\) is a learnable parameter matrix. Under the successful design of existing works, the message passing matrix \(\) can generically encode graph data with varying edges. However, the presence of the parameter matrix \(W\) means that a trained GNN model can only be directly applied to graphs with the same node feature dimensionality as the training dataset, significantly limiting the application scenarios of GNNs.

**Graph Contrastive Pre-training**. It is based on Contrastive Learning (CL). Given graph \(\), graph contrastive pre-training aims to train a GNN encoder \(f()\) in a self-supervised manner, which generates low-dimensional dense graph representations \(Z=f()\) for downstream tasks. Typically, InfoNCE loss functions are used for training, formulated as:

\[_{}=-(,v_{j}) PS}e^{ (g(v_{i}),g(v_{j}))/}}{_{(v_{i},v_{j}) PS}e^{( g(v_{i}),g(v_{j}))/}+_{(v_{i},v_{j}) NS}e^{(g(v_{i}),g(v_{j}))/ }}),\] (1)

where \(PS\) denotes the data sampled as positive pairs, and \(NS\) denotes the data sampled as negative pairs. \(()\) represents the cosine similarity calculation, \(g()\) is a non-linear projector, \(\) is a temperature parameter. The training objective is to increase the similarity between positive pairs and decrease the similarity between negative pairs.

**Principal Component Analysis (PCA)**. Given data \(X\) with any number of dimensions \(d\), PCA aims to reduce the dimensionality to \(k\) dimensions, where \(k d\), while maximizing the retention of variance between the data. PCA based on eigendecomposition is shown as:

\[(X)=P,P=[\{c_{i}|c_{i} C, _{c}^{(k)}\}],(,C)=[(},},...,})],\] (2)

where \(P\) represents the basis transformation matrix, \(=X-(X)\). \(c_{i}\) denotes the \(i\)-th eigenvector, \(_{i}\) denotes the corresponding eigenvalue, \(^{(k)}\) denotes the \(k\)-th largest eigenvalue. \(()\) represents concatenating the eigenvectors within the set, and \(()\) represents normalization. \(C\) and \(\) represent the matrix composed of eigenvectors and the corresponding eigenvalues, respectively. \(()\) represents covariance computation, and \(\) denotes matrix decomposition. Note that since the covariance matrix is symmetric, the extracted eigenvectors are orthogonal to each other. This formulation mathematically ensures that the dimensionality-reduced embeddings maximize the retention of variance between the original data and minimize homogeneity among the data.

## 3 Graph Contrastive Pretrained Models from the PCA perspective

In this section, we first theoretically summarize the framework that PCA essentially follows. We then analyze contrastive learning, demonstrating that it also adheres to this framework, and the PCA optimization task is a special case of that of CL. Furthermore, we answer the question why the PCA embeddings generally underperform graph contrastive pre-training in downstream tasks within the same framework, and why graph contrastive pre-training models lose the ability to adapt datasets with varying node feature shapes. Based on these findings, we propose a new strategy for Feature-Universal Graph pre-training (FUG strategy). First we define a class of objectives:

**Definition 3.1** (Data relative relationship optimization task \(_{RT}\)): _. Given a dataset \(X\) and a model \(()\), \(_{RT}\) involves maximizing or minimizing the \(l2-\)distance between certain pairs of data points, formulated as:_

\[[_{RT}][_{(x_{i},x_{j})  X^{-}}[(z_{i},z_{j})]-_{(x_{i},x_{j}) X^{+}}[(z_{i},z_{j})]],\] (3)

_where \(X^{+}\) represents the set of data pairs that need to be encoded as similar, whereas \(X^{-}\) denotes the set of pairs that should be encoded as dissimilar. And \(z_{i}\) denotes the embedding of \(x_{i}\)._

Notably, unlike the contrastive loss, \(X^{+}\) represents positive pairs within the raw data, rather than positive pairs between the augmented views. It means \(_{RT}\) focuses solely on the relative semantic relationships among the data points, disregarding their absolute semantic.

**Definition 3.2** (Encoder Dim\(()\)): _Dim\(()\) refers to a series of dimension-specific encoders that directly take the data of all samples under a specific dimension \(i\), \(X^{T}_{:,i}\), and output the embedding of that dimension, \(DimEmb_{i}\), formalized as \(DimEmb_{i}=Encode(X^{T}_{:,i})\), \(DimEmb^{D N^{}}\)._

**Definition 3.3** (Encoder Fea\(()\)): _Fea\(()\) refers to a series of sample-specific encoders that take the data of all dimensions of a specific sample \(j\), \(X_{j}\), and output the embedding of that sample, \(FeaEmb_{j}\), formalized as \(FeaEmb_{j}=Encode(X_{j},)\), \(FeaEmb^{N D^{}}\), where \(\) represents additional input._

**Theorem 3.1** (PCA framework): _. Consider Dim\(()\) as an encoder modeling the distribution of data across a specific dimension (as in Definition 3.2), \(Fea()\) as an encoder modeling the relationships between different dimensions of data (as in Definition 3.3). PCA is an instantiation model for these two encoders. Assuming that data representations have equal length \(m\), when \(X^{+}=\) in Definition 3.1 of \(_{RT}\), PCA adheres to the following form:_

\[[_{PCA}((X))][_{RT}(((X)))].\] (4)

Rigorous proof can be found in Appendix A.1. Here, \(_{PCA}=-[||(x_{i})-((X))|| _{2}^{2}]\) is interpreted as an instantiation of \(_{RT}\), based on the principle of maximizing the retention of data variance. We consider the covariance function in Eq. 2, \(()\), as an implementation of Dim\(()\), since it essentially encodes the distribution information of a single dimension through the computation of covariance. The matrix decomposition and normalization concatenation in Eq. 2 are viewed as an implementation of \(()\), as they fundamentally encode the relationships among different data dimensions. This interpretation also applies to PCA methods based on Singular Value Decomposition. Thus, we summarize the framework that PCA adheres to as shown in Eq. 4.

**PCA framework has powerful transfer ability.** From Eq. 4, it can be inferred that PCA essentially functions as a neural network model trained to reach a global optimum. The parameters of PCA's two encoders learn to encode the covariance of dimensional data and to extract and concatenate eigenvectors to form the basis transformation matrix, respectively. Extensive researchers have validated PCA's success across various fields with the same parameters , demonstrating that under the guidance of the \(_{RT}\) loss, the model's capacity to encode difference in data demonstrates formidable transferability. This transform capability originates from the model's focus on encoding the differential relationships among data rather than their absolute semantics.

PCA is capable of adapting to arbitrary Euclidean data and demonstrates strong transfer performance. However, its effectiveness generally falls short of advanced NN-based models, and it lacks the ability to encode structure information. To address the issue of node feature generality in graph pre-training models, inspired by existing work , we attempt to further theoretically analyze the relationship between existing graph contrast pre-training models and the PCA framework. We start by analyzing the InfoNCE loss which is commonly used in contrastive learning.

**Theorem 3.2** (Equivalence of Contrastive Loss and \(_{RT}\)): _. Assuming that data augmentation in contrastive learning does not alter the essential semantics of node pairs for downstream tasks, it follows:_

\[[_{}][_{RT}].\] (5)

Detailed proof can be found in Appendix A.2. Here we offer an intuitive explanation: the negative sampling component in InfoNCE loss is widely acknowledged for encoding dissimilarities withinthe data. We contend that the augmentation-based positive sampling part, inherently encodes similar relationship between data points rather than their absolute semantics. Consider a pair of data points, \(x_{i}\) and \(x_{j}\), with the same semantics. Ideally, when data augmentation does not alter the essential semantics of the nodes, the augmented data, \(Aug(x_{i})\) and \(Aug(x_{j})\), should be identical. Consequently, Maximize \([(x_{i},Aug(x_{i}))+(x_{j},Aug(x_{j}))]\), essentially equates to Maximize \([(x_{i},x_{j})\). Thus, data augmentation is essentially looking for positive samples in the dataset, aligning with the views expressed in work . Furthermore, this view explains why augmentation is less critical in graph contrastive learning compared to other domains , leading to the proposal of many augmentation-free graph contrastive studies [29; 30; 31], as the edges in graph data (especially in homophilic graphs) inherently convey similarities between data points.

**Corollary 3.2.1**: _Consider a specific instantiation of a dimension encoder defined as \((X)=X\), and the contrastive model viewed as an instantiation of an \(()=()\). Under this the contrastive model also adheres to the framework in Theorem 3.1, which states:_

\[\ [_{}((X))] \ [_{RT}(((X)))].\] (6)

From this, we can conclude that the PCA optimization objective is, in fact, a special case of the CL loss, detailed proofs are in Appendix A.2. As demonstrated by Theorem 3.1 and Corollary 3.2.1, PCA and contrastive pre-training fundamentally follow the same framework. Our analysis extends the existing discussions  on the relationship between the two. Within this framework, we can more clearly analyze why PCA is versatile across various data, while advanced contrastive pre-training models generally outperform PCA when directly used as embeddings in downstream tasks.

**Answer 1. The robust versatility of PCA across various types of Euclidean data is attributed to its inclusion of the \(()\) component.** As illustrated in Table 1, PCA, in contrast to contrastive learning models, incorporates the \(()\) component. Most existing neural network layers encode features of individual data points rather than encoding across the same dimension for different data. Due to these layers' fixed input-output dimensionality requirements, they cannot universally adapt to data with varied feature forms except using a sliding window strategy. In contrast, PCA encodes different data within the same dimension by computing covariance between dimensions, inherently encoding the data distribution of that dimension. This enables PCA to generate transform matrix and integrate data into embeddings within a unified shape.

**Answer 2. The superior performance of contrastive pre-training models over PCA primarily stems from their additional encoding of data homogeneity and a more flexible \(()\) component.** As noted in Table 1, a key advantage of contrastive learning (CL) compared to PCA is its consideration of homogeneity in the optimization objective, rather than merely encoding differences between data points. Furthermore, thanks to a variety of neural network layers, these models are capable of extracting richer data information and are adaptable to non-Euclidean graph data.

**Feature-Universal Graph Contrastive Pre-training.** Based on the above analysis, we propose that a Feature-Universal Graph Contrastive Pre-training model should include a dimension-specific data distribution encoder \(()\), a cross-dimensional data information encoder \(()\), and an optimization objective \(_{RT}\) that encodes both similarities and differences between data. Models designed under this theory can universally adapt to graph data with any node feature shapes. Moreover, guided by \(_{RT}\), the model captures the relative semantic relationships among data, rather than absolute semantics. This type of knowledge, which is more easily transferable, ensures that the model can generalize across different datasets.

## 4 Feature-Universal Graph Pre-training Model

Guided by the theories presented in Section 3, we instantiated a Feature-Universal Graph pre-training model (FUG model), as depicted in Fig. 2. This model incorporates a Dimension Encoding component \(()\), a Graph Encoder \(()\), and a relative relationship optimization task \(_{}\). The \(()\)

    & \(()\) & \(()\) & \(X^{+}\) & \(X^{-}\) \\  PCA & ✓ & ✓ & ✗ & ✓ \\ CL & ✗ & ✓ & ✓ & ✓ \\  FUG (Ours) & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparative Analysis of Model Components in PCA, Contrastive Pre-training Models (CL), and the proposed FUGlearns the data distribution within a specific dimension to generate a basis transformation matrix \(T\), which converts any form of node features to a unified shape. The \(()\) encodes both the structure and the node features to obtain graph representations. \(_{}\) is an improved contrastive loss based on the \(_{RT}\) concept. Compared to the classical InfoNCE loss, \(_{}\) more effectively guides the model in encoding the relative semantic information in the data. Furthermore, in line with the earlier discussion on augmentation essentially serving as positive sampling, we explicitly define positive sampling within \(_{}\) to replace data augmentation.

**Dimension Encoder \(()\).** We designed a non-linear dimension encoder, \(()\), to encode the distribution information of a specific dimension. If it encodes the features of all nodes under one dimension, with the input dimension fixed at the number of nodes in the training set \(n_{t}\), this encoder would be constrained by the number of input nodes. Therefore, we randomly sample \(n_{s}\) nodes from the training set as the input for this encoder, approximating the feature distribution of the entire data through the sampled nodes. The dimension distribution information, after being processed by \(()\), generates a basis transformation vector for each dimension as follows:

\[t_{i}=(X_{:,i})=[(}^{})],\] (7)

where \(=(X)\) and MLP represents a Multi-Layer Perceptron. \(X_{:,i}\) represents the i-th column of the X matrix, that is, all the data under the i-th feature dimension. Eq. 7 can be seen as \(()\) assigning a point to each feature dimension in a hyperspherical embedding space which the center at the origin and radius one. The basis transformation vectors should maximally reflect the differences among the feature dimensions while retaining inter-dimension correlations. In terms of the embedding space, this means the base vectors should be uniformly distributed over the hypersphere globally. This implies that the mean of the base vectors should approximate the origin. Therefore, we designed a separate loss for \(()\) as follows:

\[_{}(T)=||_{i=1}^{d}t_{i}-||_{2}^{ 2},\] (8)

where \(d\) represents the number of dimensions, \(T\) denotes the basis transformation matrix. Eq. 8 ensures the global uniformity of the basis transformation vectors, while the local similarity or dissimilarity is controlled by the dimension's inherent information and the overall model's loss.

**Graph Encoder \(()\).** Following numerous existing works [5; 32; 33], we implement our graph encoder using GNNs, which effectively encode the feature and structural information of graphs. We use \(H=XT\) as the node feature embedding input to \(()\). This approach standardizes any shape of node features to the same dimension, making \(()\) generally applicable across various datasets.

**Relative Relationship Optimization Task \(_{}\).** Based on the analysis of \(_{RT}\) in Section 3, the \(_{}\) loss should be divided into positive and negative parts, \(_{^{+}}\) and \(_{^{-}}\). For negative part, although the InfoNCE loss has demonstrated strong performance in prior studies [8; 5], it faces challenges with false negative sampling [34; 35] and high computational complexity (which is \(O(n^{2})\), see Appendix B.1). Moreover, existing studies have shown that InfoNCE-based loss functions face

Figure 2: The overview of the proposed FUG model.Given the node features \(X\) and structure \(A\) of a graph, \(X\) is first processed through a learnable dimensional encoding component, \(()\), to generate a basis transformation matrix \(T\), which is used to embed features into a unified shape \(H=XT^{n k}\). \(H\) and \(A\) are then input into the graph encoder \(()\) to produce representations \(Z\). We set three losses, \(L_{}\), \(L_{^{+}}\), and \(L_{^{-}}\), which collaboratively train \(()\) and \(()\).

conflicts in graph data, making the model gradient unable to descend . Thus, we propose a new type of loss based on the analysis in \(_{RT}\). Ideally, representations should be able to be divided into multiple clusters based on their intrinsic semantics, with nodes within the same cluster close together (positive pairs) and nodes between clusters far apart (negative pairs). Without prior knowledge, accurately identifying negative pairs is difficult and prone to sampling bias. Therefore, we opt not to explicitly determine negative pairs but instead constrain the overall representations, as:

\[_{^{-}}=||_{i=1}^{n}(z_{i})- ||_{2}^{2},\] (9)

where \(z_{i}\) represents the embedding of node \(v_{i}\). Similar to Eq. 8, we first constrain the representations to a unit sphere and then force the mean of the representations to be close to the origin. From Eq. 9, we reduce the complexity of the negative sampling in InfoNCE loss which is \(O(n^{2})\) to \(O(n)\).

For positive sampling, inspired by existing work [37; 31; 30] and based on the natural homophily assumption in graphs, which means connected nodes have similar semantics, we consider node neighbors as positive samples. As Eq. 8 and 9 both use the \(l2\) distance, for loss balance, we also adopt \(l2\) loss here:

\[_{^{+}}=|}_{(v_{i},v_{j}) }||z_{i}-z_{j}||_{2}^{2},\] (10)

where \(\) represents the set of edges. Note that Eq 9 focuses on encoding global differences, while Eq. 10 focuses on encoding local homogeneity, complementing each other without conflict. The relationship between of \(_{RT}\) and \(_{}\) is in Appendix A.3. The final loss is defined as:

\[_{}=_{1}_{^{-}}+_{ 2}_{^{+}}+_{3}_{},\] (11)

where, \(_{1},_{2},_{3}\) are hyper-parameters which adjust the weight of the three losses. Compared to the classical InfoNCE contrastive loss, \(_{}\) better encodes the relative relationships between nodes and offers a lower computational complexity of \(O(n+e+d)\) (Please see Appendix B.1 for details). Notably, although the final loss involves three hyper-parameters, in fact, FUG has only one additional hyper-parameter compared to classical GCLs (or even fewer, as FUG does not require tuning multiple augmentation-related parameters). Details can be found in Appendix E.3.

## 5 Experiments

### Experiment Settings

To comprehensively evaluate the effectiveness of FUG, we selected commonly used graph datasets with diverse node feature shapes, including Cora, CiteSeer, PubMed [11; 12], Photo, Computers , CS, and Physics . Detailed information about these datasets can be found in Appendix E.1. We primarily compared four types of methods: supervised GNN model (GCN ), classic machine learning algorithms (PCA, DeepWalk ), advanced self-supervised contrastive models (GRACE , BGRL , GBT  DGI , GCA , ProGCL ), and recent graph universal pre-training models (OFA , GraphControl ). In both self-supervised model-rebuilding tests and cross-domain transfer tests, FUG is initially trained in a self-supervised manner. The trained

  
**Method** & **Cora** & **CiteSeer** & **PubMed** & **Photo** & **Computers** & **CS** & **Physics** \\  OFA  & \(75.90 1.26\) & - & \(78.25 0.71\) & - & - & - & - \\ GraphControl  & - & - & - & \(89.66 0.56\) & - & - & \(94.31 0.12\) \\  FUG-C (Cora) & \(82.35 2.60\) & \(67.13 2.80\) & \(84.58 0.57\) & \(92.78 1.23\) & \(86.27 1.60\) & \(\) & \(95.41 0.42\) \\ FUG-C (CiteSeer) & \(82.82 2.65\) & \(68.53 2.43\) & \(84.57 0.35\) & \(92.89 1.06\) & \(85.76 1.59\) & \(92.62 0.80\) & \(95.41 0.41\) \\ FUG-C (PubMed) & \(83.31 2.72\) & \(\) & \(\) & \(92.82 1.04\) & \(87.70 1.18\) & \(92.65 0.72\) & \(95.20 0.39\) \\ FUG-C (Photo) & \(\) & \(68.92 2.05\) & \(84.34 0.41\) & \(92.93 1.14\) & \(88.01 1.38\) & \(92.44 0.69\) & \(95.25 0.41\) \\ FUG-C (Computers) & \(83.31 2.25\) & \(69.01 1.79\) & \(84.37 0.47\) & \(\) & \(\) & \(92.50 0.65\) & \(\) \\ FUG-C (CS) & \(81.73 2.34\) & \(66.32 2.64\) & \(84.44 0.50\) & \(91.96 1.11\) & \(83.28 1.55\) & \(92.54 0.78\) & \(95.34 0.37\) \\ FUG-C (Physics) & \(80.85 2.15\) & \(63.32 2.28\) & \(84.26 0.61\) & \(84.59 1.80\) & \(73.43 2.08\) & \(92.13 0.66\) & \(95.35 0.42\) \\  FUG (Rebuild) & \(84.45 2.45\) & \(72.43 2.92\) & \(85.47 1.13\) & \(93.07 0.82\) & \(88.42 0.98\) & \(92.89 0.45\) & \(95.45 0.27\) \\   

Table 2: Cross-domain Node Classification. The performances of OFA and GraphControl are reported from [17; 18]. FUG-C (Cora) means FUG-C model pre-trained on Cora dataset.

model is then frozen, and node representations are generated on the test datasets. These representations are subsequently used as input for downstream predictors. For the node classification experiments, to simulate a few-shot scenario, we split the dataset into train, valid, and test sets by 10%/10%/80%. We train a simple downstream \(l2\) classifier  using only 10% labels. Detailed experimental settings can be found in Appendix E.3.

### Evaluating on Cross-domain Learning

We conduct experiments to test FUG's performance in cross-domain learning, as shown in Table 2. FUG-C represents a fixed FUG model where all hyperparameters remain constant during pre-training. This presents a more challenging scenario, as it means we cannot adjust parameters based on changes in the training datasets. In Table 2, we report the optimal accuracy of the state-of-the-art graph universal pre-training models, OFA and GraphControl, which are capable of training across datasets with different node features. These models have access to richer data during pre-training than FUG, which is not fair to FUG. However, as Table 2 shows, FUG's overall performance in cross-domain learning significantly surpasses both models. This is because OFA and GraphControl employ data preprocessing methods that significantly degrade the information in the node features (OFA uses LLM to reshape node features to the same dimension, and GraphControl replaces original node features with feature similarity). This validates our views made in Section 1 and Appendix D. It also demonstrates that FUG's dimension encoder can almost losslessly adapt to graph data with various node feature shapes.

Additionally, we derived some interesting findings from Table 2: 1). Counterintuitively, FUG-C's performances are not necessarily the best when pre-trained and tested on the same dataset. This is mainly because our loss functions focus on the relative semantic relationships between nodes, allowing the encoder to learn the knowledge of relative relationships. This leads to: **a)** This knowledge is generalizable across different datasets, so even though the datasets are from different domains, this knowledge remains the same. In other words, relative semantic relationship knowledge has weak cross-domain limitations, as stated in Section 3. **b)** The learning of relative semantic knowledge primarily relies on the distributional differences between nodes and between different feature dimensions, rather than on the absolute semantics contained in the data. Therefore, it is common in Table 2 to see that FUG-C pre-trained on other datasets performs better than when pre-trained and tested on the same dataset. 2). The performances of cross-domain trained FUG-C are close to that of FUG which is trained and tested on the same datasets with tuned hyper-parameters. In

    & **Cora** & **CiteSeer** & **PubMed** & **Photo** & **Computers** & **CS** & **Physics** \\  FUG w/o \(_{}\) & 81.32 ± 2.56 & 71.74 ± 2.88 & 84.68 ± 1.19 & 57.37 ± 3.38 & 50.55 ± 1.69 & 91.25 ± 0.75 & 93.77 ± 0.40 \\ FUG w/o \(_{}\) & 83.31 ± 2.85 & 68.44 ± 2.63 & 85.18 ± 1.01 & 92.80 ± 0.71 & 88.29 ± 1.09 & 92.71 ± 0.53 & 95.18 ± 0.29 \\ FUG w/o \(_{}\) & 84.30 ± 2.88 & 70.48 ± 2.47 & 85.24 ± 1.05 & 92.89 ± 0.82 & 88.27 ± 0.86 & 92.74 ± 0.47 & 95.29 ± 0.19 \\  FUG & **84.45 ± 2.45** & **72.43 ± 2.92** & **85.47 ± 1.13** & **93.07 ± 0.82** & **88.42 ± 0.98** & **92.89 ± 0.45** & **95.45 ± 0.27** \\   

Table 4: Ablation study of the loss functions of FUG.

  
**Method** & **Cora** & **CiteSeer** & **PubMed** & **Photo** & **Computers** & **CS** & **Physics** \\  Raw features & \(57.90 3.90\) & \(57.60 2.85\) & \(79.55 0.98\) & \(80.99 1.65\) & \(75.59 1.69\) & \(89.92 0.95\) & \(93.18 0.45\) \\ PCA & \(56.36 4.14\) & \(48.29 3.18\) & \(82.80 0.91\) & \(74.92 1.82\) & \(71.26 1.34\) & \(87.12 0.73\) & \(92.48 0.47\) \\ DeepWalk  & \(75.70 0.00\) & \(50.50 0.00\) & \(80.50 0.00\) & \(89.44 0.00\) & \(85.68 0.00\) & \(84.61 0.00\) & \(91.77 0.00\) \\ DeepWalk+Features & \(73.10 0.00\) & \(47.60 0.00\) & \(83.70 0.00\) & \(90.05 0.00\) & \(86.28 0.00\) & \(87.70 0.00\) & \(94.90 0.00\) \\  GRACE  & \(83.20 1.87\) & \(70.99 2.29\) & \(85.46 0.54\) & \(91.93 0.83\) & \(85.36 0.82\) & \(91.84 0.37\) & OOM \\ BGRL  & \(81.57 2.07\) & \(70.10 2.04\) & \(83.67 0.84\) & \(92.34 0.73\) & \(86.51 1.53\) & \(92.12 0.63\) & \(95.42 0.41\) \\ GBT  & \(82.45 1.97\) & \(70.16 2.32\) & \(85.69 1.22\) & \(92.44 0.48\) & \(87.46 0.82\) & \(92.39 0.78\) & \(95.07 0.23\) \\ DGI  & \(83.24 2.12\) & \(71.23 2.37\) & \(84.62 0.83\) & \(92.32 0.49\) & \(86.12 0.73\) & \(92.47 0.60\) & \(94.47 0.50\) \\ GCA  & \(82.83 2.29\) & \(72.06 1.91\) & **85.69 ± 0.68** & \(92.63 1.12\) & \(87.78 0.78\) & \(92.69 0.49\) & OOM \\ ProCoT  & \(82.65 1.74\) & **7.285 ± 2.17** & OOM & \(92.87 0.44\) & OOM & OOM & OOM \\  FUG (Rebuild) & \(\) & \(72.43 2.92\) & \(85.47 1.13\) & \(\) & \(\) & \(\) & \(\) \\  GCN (Supervised) & \(82.80 0.00\) & \(72.00 0.00\) & \(84.80 0.00\) & \(92.42 0.00\) & \(86.51 0.00\) & \(93.03 0.00\) & \(95.65 0.00\) \\   

Table 3: Intra-domain Model Re-building Node Classification. Data without standard deviation are reported from previous works . Aside from GCN, the best results are highlighted in bold, and the second-best results are underlined. OOM indicates Out-Of-Memory on an RTX 3090 GPU.

fact, on the Photo dataset, FUG-C (Computers) outperforms FUG. This demonstrates FUG's strong cross-domain learning capability and the success of our proposed dimension encoding and relative relationship loss. It proves FUG's robust applicability to various graph data.

### Evaluating on In-domain Learning

In Table 3, we designed node classification experiments to evaluate FUG in intra-domain self-supervised scenarios. We also report the computational overhead of FUG when the hidden layer is fixed to 256 as shown in Table 5, and the complete results are in Appendix B.2. From Table 3 and 5, we can observe that FUG demonstrates competitive performance. In Table 3, we observed that: 1). FUG outperforms GRACE across all datasets, reaffirming that the learnable DE\(()\) encoder in FUG can almost losslessly standardize node features to the same shape. This also supports our views in Section 3 that both positive and negative sampling in the InfoNCE loss inherently constrain the relative relationships between nodes. Furthermore, it demonstrates that our loss function maintains excellent performance while reducing computational overhead. 2). FUG significantly surpasses PCA on all datasets. Although PCA, FUG, and GRACE follow the same framework, PCA is limited by its inability to encode graph structure and the relative semantics between data points, leading to poor performance. This validates our previous views. FUG simulates PCA's encoding process through graph contrastive learning, thereby addressing PCA's shortcomings and significantly improving performance. 3). FUG generally outperforms the negative-sample-free GBT and BGRL, while maintaining comparable time costs as shown in Table 5. This indicates that considering negative relative relationships between data points is effective, and FUG successfully reduces the time and space overhead associated with these negative considerations to match that of methods that do not consider negative samples. Additionally, we compare FUG with latest advanced in-domain self-supervised methods [43; 44; 45] in Appendix E.4.

### Model Analysis

To validate the effectiveness of each component in FUG, we conducted ablation studies and parameter analysis, as shown in Table 4 and Fig. 3. From Table 4, it is evident that FUG outperforms other models without part of loss functions across all datasets, demonstrating the effectiveness of the three proposed loss functions. Furthermore, we observed that FUG w/o

\(_{^{+}}\) generally achieves the second-best performance. This is primarily due to \(_{^{-}}\) constraining global uniformity without focusing on the distance between local node pairs, preventing the model from over-encoding differences between nodes. Additionally, FUG w/o \(_{}\) generally exhibits the worst results. The main reason is that, \(L_{RT-FUG^{+}}\) and \(L_{RT-FUG^{-}}\) are effective when based on \(L_{DE}\), especially for datasets with high quality features. As described in Section 5.4, when \(L_{DE}\) is absent, DE is not sufficiently trained, which leads to DE embedding \(X\) almost randomly to generate \(H\), which is an important input for GE. Furthermore, without the guidance of \(L_{DE}\), GE only needs to extract relationships in \(H\) to reduce the loss, ignoring whether \(H\) contains sufficient information from \(X\). Therefore, in FUG w/o \(L_{DE}\), the \(H\) input to GE is of poor quality, resulting in worst performances on some datasets. From Fig. 3, we can observe that as the number of sampled nodes and the dimensionality of the basis transformation vectors increase, the overall model performance

    & GRACE & BGRL & GBT & FUG \\  \(X^{-}\) & \(\) & \(\) & \(\) & \(\) \\  CS Time & 0.2169 & 0.0462 & 0.0593 & 0.0526 \\ CS VRAM & 11.960 & 4.482 & 3.324 & 2.482 \\  Physics Time & OOM & 0.1084 & 0.1320 & 0.1047 \\ Physics VRAM & OOM & 10.278 & 8.002 & 5.190 \\   

Table 5: Comparison of computational cost

Figure 3: Hyper-parameter analysis of FUG.

improves until it stabilizes at around 1024. This indicates that a limited number of sampled nodes can effectively represent the overall data distribution. Moreover, once the dimensionality of the basis transformation vectors reaches a certain threshold, it can embed dimensional information nearly losslessly, thereby unifying data features to the same shape. This further validates the design of the dimensional encoder.

## 6 Discussions and Future Works

In this paper, we explore the connection between contrastive learning and PCA, and propose the FUG strategy, which includes specific dimension encoding, inter-dimensional encoding, and relative semantic relationship loss. The models instantiated under this framework achieve near-lossless unification of node attributes with different shapes without data preprocessing, making them applicable to graph data and demonstrating strong cross-domain learning capabilities. We instantiated a FUG model and validated its effectiveness through extensive experiments. Additionally, our theoretical analysis shows that the complexity of the GCL loss involving negative samples is reduced from \(O(n^{2})\) to \(O(n)\), further lowering FUG's training costs.

In future works, the FUG strategy can support research on graph prompt tuning, universal graph models, and general large models based on graph networks, enabling graph models to train directly on various data. Moreover, upon acceptance of this paper, we will release a pre-trained FUG model on existing seven datasets, allowing for direct use of FUG-generated graph representations as priors for further extensions. While the FUG strategy is general, the instantiated model has some limitations, such as only considering homophilic graphs in positive sampling, which may perform poorly on heterophilic graphs, and not constraining absolute semantic information in the loss function. Future work can further refine the universal graph contrastive pre-training model based on the FUG strategy.