# Causal Component Analysis

Liang Wendong \({}^{1,2}\)  Armin Kekic \({}^{1}\)  Julius von Kugelgen \({}^{1,3}\)  Simon Buchholz \({}^{1}\)

Michel Besserve \({}^{1}\)  Luigi Gresele\({}^{*}\)  Bernhard Scholkopf\({}^{*}\)\({}^{1}\)

\({}^{1}\) Max Planck Institute for Intelligent Systems, Tubingen, Germany

\({}^{2}\) ENS Paris-Saclay, Gif-sur-Yvette, France \({}^{3}\) University of Cambridge, United Kingdom

{wendong.liang,armin.kekic,jvk,simon.buchholz}@tue.mpg.de

{besserve,luigi.gresele,bs}@tue.mpg.de

Shared last author. Code available at https://github.com/akekic/causal-component-analysis.

###### Abstract

Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically _dependent_) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed _Causal Component Analysis (CauCA)_. CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a corollary, this interventional perspective also leads to new identifiability results for nonlinear ICA--a special case of CauCA with an empty graph--requiring strictly fewer datasets than previous results. We introduce a likelihood-based approach using normalizing flows to estimate both the unmixing function and the causal mechanisms, and demonstrate its effectiveness through extensive synthetic experiments in the CauCA and ICA setting.

## 1 Introduction

Independent Component Analysis (ICA)  is a principled approach to representation learning, which aims to recover independent latent variables, or sources, from observed mixtures thereof. Whether this is possible depends on the _identifiability_ of the model : this characterizes assumptions under which a learned representation provably recovers (or _disentangles_) the latent variables, up to some well-specified ambiguities . A key result shows that, when nonlinear mixtures of the latent components are observed, the model is non-identifiable based on independent and identically distributed (i.i.d.) samples from the generative process . Consequently, a learned model may explain the data equally well as the ground truth, even if the corresponding representation is strongly entangled, rendering the recovery of the original latent variables fundamentally impossible.

Identifiability can be recovered under deviations from the i.i.d. assumption, e.g., in the form of temporal autocorrelation  or spatial dependence  among the latent components; _auxiliary variables_ which render the sources _conditionally_ independent ; or additional, noisy _views_. An alternative path is to restrict the class of mixing functions .

Despite appealing identifiability guarantees for ICA, the independence assumption can be limiting, since interesting factors of variation in real-world data are often statistically, or causally, dependent . This motivates Causal Representation Learning (CRL) , which aims instead to infer causally related latent variables, together with a causal graph encoding their causal relationships. This is challenging if both the graph and the unmixing are unknown. Identifiability results in CRL therefore require strong assumptions such as counterfactual data , temporal structure , a parametric family of latent distributions , graph sparsity , pairs of interventions and _genericity_ or restrictions on the mixing function class . It has been argued that knowing either the graph or the unmixing might help better recover the other, giving rise to a _chicken-and-egg_ problem in CRL .

We introduce an intermediate problem between ICA and CRL which we call _Causal Component Analysis (CauCA)_, see Fig. 1 for an overview. CauCA can be viewed as a generalization of ICA that models causal connections (and thus statistical dependence) among the latent components through a causal Bayesian network . It can also be viewed as a special case of CRL that presupposes knowledge of the causal graph, and focuses on learning the unmixing function and causal mechanisms.

Since CauCA is solving the CRL problem with partial ground truth information, it is strictly easier than CRL. This implies that impossibility results for CauCA also apply for CRL. _Possibility_ results for CauCA, on the other hand, while not automatically generalizing to CRL, can nevertheless serve as stepping stones, highlighting potential avenues for achieving corresponding results in CRL. Note also that there are only finitely many possible directed acyclic graphs for a fixed number of nodes, but the space of spurious solutions in representation learning (e.g., in nonlinear ICA) is typically infinite. By solving CauCA problems, we can therefore gain insights into the minimal assumptions required for addressing CRL problems. CauCA may be applicable to scenarios in which domain knowledge can be used to specify a causal graph for the latent components. For instance, in computer vision applications, the image generation process can often be modelled based on a fixed graph .

**Structure and Contributions.** We start by recapitulating preliminaries on causal Bayesian networks and interventions in SS 2. Next, we introduce Causal Component Analysis (CauCA) in SS 3. Our primary focus lies in characterizing the identifiability of CauCA from multiple datasets generated through various types of interventions on the latent causal variables (SS 4). Importantly, all our results are applicable to the _nonlinear_ and _nonparametric_ case. The interventional perspective we take exploits the _modularity_ of the causal relationships (i.e., the possibility to change one of them without affecting the others)--a concept that was not previously leveraged in works on nonlinear ICA. This leads to extensions of existing results that require strictly fewer datasets to achieve the same level of identifiability. We introduce and investigate an estimation procedure for CauCA in SS 5, and conclude with a summary of related work (SS 6) and a discussion (SS 7). We highlight the following _main contributions_:

* We derive sufficient and necessary conditions for identifiability of CauCA from different types of interventions (Thm. 4.2, Prop. 4.3, Thm. 4.5).
* We prove additional results for the special case with an empty graph, which corresponds to a novel ICA model with interventions on the latent variables (Prop. 4.6, Prop. 4.7, Corollary 4.8, Prop. 4.9).
* We show in synthetic experiments in both the CauCA and ICA settings that our normalizing flow-based estimation procedure effectively recovers the latent causal components (SS 5).

Figure 1: **Causal Component Analysis (CauCA). We posit that observed variables \(\) are generated through a nonlinear mapping \(\), applied to unobserved latent variables \(\) which are causally related. The causal structure \(G\) of the latent variables is assumed to be known, while the causal mechanisms \(_{i}(Z_{i}_{(i)})\) and the nonlinear mixing function are unknown and to be estimated. (Known or observed quantities are highlighted in red.) CauCA assumes access to multiple datasets \(_{k}\) that result from stochastic interventions on the latent variables.**

## 2 Preliminaries

**Notation.** We use \(\) to denote a probability distribution, with density function \(p\). Uppercase letters \(X,Y,Z\) denote unidimensional and bold uppercase \(,,\) denote multidimensional random variables. Lowercase letters \(x,y,z\) denote scalars in \(\) and \(,,\) denote vectors in \(^{d}\). We use \([\![i,j]\!]\) to denote the integers from \(i\) to \(j\), and \([d]\) denotes the natural numbers from \(1\) to \(d\). We use common graphical notation, see App. A for details. The _ancestors_ of \(i\) in a graph are the nodes \(j\) in \(G\) such that there is a directed path from \(j\) to \(i\), and they are denoted by \((i)\). The _closure_ of the parents (resp. ancestors) of \(i\) is defined as \(}(i):=(i)\{i\}\) (resp. \(}(i):=(i)\{i\}\)).

A key definition connecting directed acyclic graphs (DAGs) and probabilistic models is the following.

**Definition 2.1** (Distribution Markov relative to a DAG ).: _A joint probability distribution \(\) is Markov relative to a DAG \(G\) if it admits the factorization \((Z_{1},,Z_{d})=_{i=1}^{d}_{i}(Z_{i}| _{(i)})\)._

Defn. 2.1 is a key assumption in directed graphical models, where a distribution being Markovian relative to a graph implies that the graph encodes specific independences within the distribution, which can be exploited for efficient computation or data storage [43, SS6.5].

**Causal Bayesian networks and interventions.** Causal systems induce multiple distributions corresponding to different interventions. Causal Bayesian networks [CBNs; 41] can be used to represent how these interventional distributions are related. In a CBN with associated graph \(G\), arrows signify causal links among variables, and the conditional probabilities \(_{i}(Z_{i}_{(i)})\) in the corresponding Markov factorization are called _causal mechanisms.2_

Interventions are modelled in CBNs by replacing a subset \(_{k} V(G)\) of the causal mechanisms by new, intervened mechanisms \(\{}_{j}(Z_{j}_{^{k}( j)})\}_{j_{k}}\), while all other causal mechanisms are left unchanged. Here, \(^{k}(j)\) denotes the parents of \(Z_{j}\) in the post-intervention graph in the interventional regime \(k\) and \(_{k}\) the intervention targets. We will omit the superscript \(k\) when the parent set is unchanged and assume that interventions do not add new parents, \(^{k}(j)(j)\). Unless \(}_{j}\) is a point mass, we call the intervention _stochastic_ or soft. Further, we say that \(}_{j}\) is a _perfect_ intervention if the dependence of the \(j\)-th variable from its parents is removed (\(^{k}(j)=\)), corresponding to deleting all arrows pointing to \(i\), sometimes also referred to as _graph surgery_.3 An _imperfect_ intervention is one for which \(^{k}(j)\). We summarise this in the following definition.

**Definition 2.2** (CBN ).: _A causal Bayesian network (CBN) consists of a graph \(G\), a collection of causal mechanisms \(\{_{i}(Z_{i}_{(i)})\}_{i[d]}\), and a collection of interventions \(\{\{}_{j}^{k}(Z_{j}_{^ {k}(j)})\}_{j_{k}}\}_{k[K]}\) across \(K\) interventional regimes. The joint probability for interventional regime \(k\) is given by:_

\[^{k}():=_{i=1}^{d}_{i}(Z_{i} _{(i)})&k=0\\ _{j_{k}}}_{j}^{k}(Z_{j}_{ ^{k}(j)})_{i_{k}}_{i}(Z_{i }_{(i)})& k[K]\] (1)

_where \(^{0}\) is the unintervened, or observational, distribution, and \(^{k}\) are interventional distributions._

_Remark 2.3_.: The joint probabilities \(^{k}\) in (1) are uniquely factorized into causal mechanisms according to \(G\). We therefore use the equivalent notation \((G,(^{k},_{k})_{k[0,K]})\), where \(^{k}\) is defined as in (1).

## 3 Problem Setting

The main object of our study is a latent variable model termed _latent causal Bayesian network (CBN)_.

**Definition 3.1** (Latent CBN).: _A latent CBN is a tuple \((G,,(^{k},_{k})_{k[0,K]})\), where \(:^{d}^{d}\) is a diffeomorphism (i.e. invertible with both \(\) and \(^{-1}\) differentiable)._

**Data-generating process for Causal Component Analysis (CauCA).** In CauCA, we assume that we are given multiple datasets \(\{_{k}\}_{k[0,K]}\) generated by a latent CBN \((G,,(^{k},_{k})_{k[\![0,K]\!]})\):

\[_{k}:=(_{k},\{^{(n,k)}\}_{n=1}^{N_{k} })\,,^{(n,k)}=(^{(n,k)}) ^{(n,k)}}}{{ }}^{k},\] (2)

where \(N_{k}\) denotes the sample size for interventional regime \(k\), see Fig. 1 for an illustration. The graph \(G\) is assumed to be known. Further, we assume that the intervention targets \(_{k}\) are observed, see SS 7 for further discussion. Both the mixing function \(\) and the latent distributions \(^{k}\) in (2) are unknown.

The problem we aim to address is the following: given only the graph \(G\) and the datasets \(_{k}\) in (2), can we learn to _invert_ the mixing function \(\) and thus recover the latent variables \(\)? Whether this is possible, and up to what ambiguities, depends on the identifiability of CauCA.

**Definition 3.2** (Identifiability of CauCA).: _A model class for CauCA is a tuple \((G,,_{G})\), where \(\) is a class of functions and \(_{G}\) is a class of joint distributions Markov relative to \(G\). A latent CBN \((G,,(^{k},_{k})_{k[\![0,K]\!]})\) is said to be in \((G,,_{G})\) if \(\) and \(^{k}_{G}\) for all \(k[\![0,K]\!]\). We say \((G,,_{G})\) has known intervention targets if all its elements share the same \(G\) and \((_{k})_{k[\![0,K]\!]}\)._

_We say that CauCA in \((G,,_{G})\) is identifiable up to \(\) (a set of functions called "indetermacy set") if for any two latent CBNs \((G,,(^{k},_{k})_{k[\![0,K]\!]})\) and \((G^{},^{},(^{k},_{k})_{k[\![0,K]\!]})\), the equality of pushforward \(,^{k}=^{}_{*}^{k}\  k[\![0,K]\!]\) implies that \(\) s.t. \(=^{-1}\) on the support of \(\)._

We justify the definition of known intervention targets and generalize them to a more flexible scenario in App. E. Defn. 3.2 is inspired by the identifiability definition of ICA in [4, Def. 1]. Intuitively, it states that, if two models in \((G,,_{G})\) give rise to the same distribution, then they are equal up to ambiguities specified by \(\). Consequently, when attempting to invert \(\) based on the data in (2), the inversion can only be achieved up to those ambiguities.

In the following, we choose \(\) to be the class of all \(^{1}\)-diffeomorphisms \(^{d}^{d}\), denoted \(^{1}(^{d})\), and suppose the distributions in \(_{G}\) are absolutely continuous with full support in \(^{d}\), with the density \(p^{k}\) differentiable.

A first question is what ambiguities are unavoidable by construction in CauCA, similar to scaling and permutation in ICA [22, SS 3.1]. The following Lemma characterizes this.

**Lemma 3.3**.: _For any \((G,,(^{k},_{k})_{k[\![0,K]\!]})\) in \((G,,_{G})\), and for any \(_{}\) with_

\[_{}:=\{:^{d}^{d }()=(h_{1}(z_{1}),,h_{d}(z_{d})),\,h_{i}\},\] (3)

_there exists a \((G,,(^{k},_{k})_{k[\![0,K]\!]})\) in \((G,,_{G})\) s.t. \(_{*}^{k}=()_{*}^{k}\) for all \(k[\![0,K]\!]\)._

Lemma 3.3 states that, as in nonlinear ICA, the ambiguity up to element-wise nonlinear scaling is also unresolvable in CauCA. However, unlike in nonlinear ICA, there is no permutation ambiguity in CauCA: this is a consequence of the assumption of known intervention targets. The next question is under which conditions we can achieve identifiability up to (3), and when the ambiguity set is larger.

## 4 Theory

In this section, we investigate the identifiability of CauCA. We first study the general case (SS 4.1), and then consider the special case of ICA in which the graph is empty (SS 4.2).

### Identifiability of CauCA

**Single-node interventions.** We start by characterizing the identifiability of CauCA based on _single-node_ interventions. For datasets \(_{k}\) defined as in (2), every \(k>0\) corresponds to interventions on a single variable: i.e., \( k>0,|_{k}|=1\). This is the setting depicted in Fig. 1, where each interventional dataset is generated by intervening on a single latent variable. The following assumption will play a key role in our proofs.

**Assumption 4.1** (Interventional discrepancy).: _Given \(k[K]\), let \(p_{_{k}}\) denote the causal mechanism of \(z_{_{k}}\). We say that a stochastic intervention \(_{_{k}}\) satisfies interventional discrepancy if_

\[})}{ z_{_{k}}}(z_{_{k}} _{pa(_{k})})_{_{k}})}{  z_{_{k}}}(z_{_{k}}_{pa^{k}(_{k})} ).\] (4)Asm. 4.1 can be applied to imperfect and perfect interventions alike (in the latter case the conditioning on the RHS disappears). Intuitively, Asm. 4.1 requires that the stochastic intervention is sufficiently different from the causal mechanism, formally expressed as the requirement that the partial derivative over \(z_{i}\) of the ratio between \(p_{i}\) and \(_{i}\) is nonzero a.e. One case in which Asm. 4.1 is violated is when \(}}{{ z_{i}}}\) and \(_{i}}}{{ z_{i}}}\) are both zero on the same open subset of their support. In Fig. 2_(Left)_, we provide an example of such a violation (see App. C for its construction), and apply a measure-preserving automorphism within the area where the two distributions agree (see Fig. 2_(Right)_).

We can now state our main result for CauCA with single-node interventions.

**Theorem 4.2**.: _For CauCA in \((G,,_{G})\),_

1. _Suppose for each node in_ \([d]\)_, there is one (perfect or imperfect) stochastic intervention that satisfies Asm._ 4.1_. Then CauCA in_ \((G,,_{G})\) _is identifiable up to_ \[_{}=\{:^{d}^{d}| ()=(h_{i}(_{}(i)}) )_{i[d]},^{1}$-diffeomorphism}\}.\] (5)
2. _Suppose for each node_ \(i\) _in_ \([d]\)_, there is one perfect stochastic intervention that satisfies Asm._ 4.1_. Then CauCA in_ \((G,,_{G})\) _is identifiable up to_ \(_{}\)_._

Thm. 4.2_(i)_ states that for single-node stochastic interventions, perfect or imperfect, we can achieve identifiability up to an indeterminacy set where each reconstructed variable can at most be a mixture of ground truth variables corresponding to nodes in the closure of the ancestors of \(i\). While this ambiguity set is larger than the one in eq. (3), it is still a non-trivial reduction in ambiguity with respect to the spurious solutions which could be generated without Asm. 4.1. A related result in [49, Thm. 1] shows that for _linear mixing, linear latent SCM and unknown graph_, \(d\) interventions are sufficient and necessary for recovering \(\) (the transitive closure of the ground truth graph \(G\)) and the latent variables up to elementwise reparametrizations. Thm. 4.2_(i)_ instead proves that \(d\) interventions are sufficient for identifiability up to mixing of variables corresponding to the coordinates in \(\) for _arbitrary \(^{1}\)-diffeomorphisms \(\), non-parametric \(_{G}\) and known graph_.

Thm. 4.2_(ii)_ shows that if we further constrain the set of interventions to _perfect_ single-node, stochastic interventions only, then we can achieve a much stronger identifiability--i.e., identifiability up to scaling, which as discussed in SS 3 is the best one we can hope to achieve in our problem setting without further assumptions. In short, the unintervened distribution together with one single-node, stochastic perfect intervention per node is sufficient to give us the strongest achievable identifiability in our considered setting. In App. D, we also discuss identifiability when only imperfect stochastic interventions are available. In short, with a higher number of imperfect interventions, the ambiguity in Thm. 4.2_(i)_ can be further constrained to the closure of parents, instead of the closure of ancestors.

Figure 2: **Violation of the Interventional Discrepancy Assumption. The shown distributions constitute a counterexample to identifiability that violates Asm. 4.1 and thus allows for spurious solutions, see App. C for technical details. _(Left)_ Visualisation of the joint distributions of two independent latent components \(z_{1}\) and \(z_{2}\) after no intervention (red), and interventions on \(z_{1}\) (green) and \(z_{2}\) (blue). As can be seen, each distribution reaches the same plateau on some rectangular interval of the domain, coinciding within the red square. _(Center/Right)_ Within the red square where all distributions agree, it is possible to apply a measure preserving automorphism which leaves all distributions unchanged, but non-trivially mixes the latents. The right plot shows a distance-dependent rotation around the centre of the black circle, whereas the middle plot show a reference identity transformation.**

Thm. 4.2_(ii)_ shows that \(d\) datasets generated by single-node interventions on the latent causal variables are sufficient for identifiability up to \(_{}\). We additionally prove below that \(d\) interventional datasets are _necessary_--i.e., for CauCA, and for any nonlinear causal representation learning problem, \(d-1\) single-node interventions are not sufficient for identifiability up to \(_{}\).

**Proposition 4.3**.: _Given a DAG \(G\), with \(d-1\) perfect stochastic single node interventions on distinct targets, if the remaining unintervened node has any parent in \(G\), \((G,,P_{G})\) is not identifiable up to_

\[_{}:=:^{d} ^{d}=,\,_{}}.\] (6)

A similar result in  shows that one intervention per node is necessary when the underlying graph is unknown. Prop. 4.3 shows that this is still the case, _even when the graph is known_.

**Fat-hand interventions.** A generalization of single-node interventions are _fat-hand interventions_--i.e., interventions where \(|_{k}|>1\). In this section, we study this more general setting and focus on a weaker form of identification than for single-node intervention.

**Assumption 4.4** (Block-interventional discrepancy).: _We denote \(_{_{k}}^{0}\) as the causal mechanism of \(_{_{k}}\) in the unintervened regime. For each \(k[K]\), we denote \(_{_{k}}^{s}\) as the intervention mechanism in the \(s\)-th interventional regime that has \(_{k}[d]\) as targets of intervention, i.e., \(_{_{k}}^{s}\) is a (conditional) joint distribution over \(_{_{k}}\). Then the Block-interventional discrepancy for \(_{k}\) is defined as follows:_

* _if there is no arrow into_ \(_{k}\) _(i.e.,_ \(_{k}\) _has no parents in_ \([d]_{k}\)_), suppose that there are_ \(n_{k}\) _interventions with target_ \(_{k}\) _such that the following_ \(n_{k} n_{k}\) _matrix_ \[_{_{k}}:=}( q_{ _{k}}^{1}- q_{_{k}}^{0})(_{_{k}})&&}}( q_{_{k}}^{1}- q_{_{k}}^{0})( _{_{k}})\\ &&\\ }( q_{_{k}}^{n_{k}}- q_{_{k}}^{0} )(_{_{k}})&&}}( q_{ _{k}}^{n_{k}}- q_{_{k}}^{0})(_{_{k}})\] (7) _is invertible for_ \(_{_{k}}^{n_{k}}\) _almost everywhere, where_ \(q_{_{k},j}^{s}\) _denotes the_ \(j\)_-th marginal of_ \(q_{_{k}}^{s}\)_,_ \(z_{_{k},j}\) _denotes the_ \(j\)_-th dimension of_ \(_{_{k}}\)_, and_ \(s=0\) _denotes the unintervened (observational) regime;_
* _otherwise, suppose that there are_ \(n_{k}+1\) _interventions with target_ \(_{k}\) _such that the matrix (_7_) is invertible for_ \(_{_{k}}^{n_{k}}\) _almost everywhere, where_ \(q_{_{k},j}^{s}\) _and_ \(z_{_{k},j}\) _are defined as before, but_ \(s=0,,n_{k}\) _now indexes the_ \(n_{k}+1\) _interventions_--i.e.,_ without an unintervened regime.

Asm. 4.4 is tightly connected to Asm. 4.1: if \(G\) has no arrow, if \( k:n_{k}=1\), then Asm. 4.4 is reduced to Asm. 4.1. However, for any \(G\) that has arrows, the number of interventional regimes required by Asm. 4.4 is strictly larger than Asm. 4.1.

**Theorem 4.5**.: _Given any DAG \(G\). Suppose that our datasets encompass interventions over all variables in the latent graph, i.e., \(_{k[K]}_{k}=[d]\). For all \(k[K]\), suppose the targets of interventions are strict subsets of all variables, i.e., \(|_{k}|=n_{k}\), \(n_{k}[d-1]\). Suppose the interventions over \(_{k}\) are perfect, i.e. the intervention mechanisms \(_{_{k}}^{s}\) are joint distributions over \(_{_{k}}\) without conditioning on other variables. Suppose Asm. 4.4 is satisfied for \(_{k}\)._

_Then CauCA in \((G,,_{G})\) is block-identifiable (following ): namely, if for all \(k[K]\), \(_{}\,^{k}=_{}^{}^{k}\), then for \(:=^{-1}\), for all \(k[K]\),_

\[[()]_{_{k}}=_{_{k}} (_{_{k}}).\] (8)

We illustrate the above identifiability results through an example in Tab. 1.

### Special Case: ICA with stochastic interventions on the latent components

An important special case of CauCA occurs when the graph \(G\) is empty, corresponding to independent latent components. This defines a nonlinear ICA generative model where, in addition to the mixtures, we observe a variable \(_{k}\) which indicates _which latent distributions_ change in the interventional regime \(k\), while _every other distribution is unchanged_.4 This nonlinear ICA generative model is closely related to similar models with observed _auxiliary variables_: it is natural to interpret \(_{k}\) itself as an auxiliary variable. As we will see, our interventional interpretation allows us to derive novel results and re-interpret existing ones. Below, we characterize identifiability for this setting.

**Single-node interventions.** We first focus on _single-node_ stochastic interventions, where the following result proves that we can achieve the same level of identifiability as in Thm. 4.2_(ii)_, with one less intervention than in the case where the graph is non-trivial.

**Proposition 4.6**.: _Suppose that \(G\) is the empty graph, and that there are \(d-1\) variables intervened on, with one single target per dataset, such that Asm. 4.1 is satisfied. Then CauCA (in this case, ICA) in \((G,,_{G})\) is identifiable up to \(_{}\) defined as in eq. (3)._

The result above shows that identifiability can be achieved through single-node interventions on the latent variables using strictly fewer datasets (i.e., auxiliary variables) than previous results in the auxiliary variables setting (\(d\) in our case, \(2d+1\) in [21, Thm. 1]). One potentially confusing aspect of Prop. 4.6 is that the ambiguity set does not contain permutations--which is usually an unavoidable ambiguity in ICA. This is due to our considered setting with known targets, where a total ordering of the variables is assumed to be known. The result above can also be extended to the case of _unknown intervention targets_, where we only know that, in each dataset, a distinct variable is intervened on, but we do not know _which one_ (see App. E). For that case, we prove (Prop. E.6) that ICA in \((G,,_{G})\) is in fact identifiable up to scaling and _permutation_. Note that Prop. 4.6 is not a special case of Thm. 4.5 in which \(n_{k}=1\)\( k\), since it only requires \(d-1\) interventions instead of \(d\).

We can additionally show that for nonlinear ICA, \(d-1\) interventions are _necessary_ for identifiability.

**Proposition 4.7**.: _Given an empty graph \(G\), with \(d-2\) single-node interventions on distinct targets, with one single target per dataset, such that Asm. 4.1 is satisfied. Then CauCA (in this case, ICA) in \((G,,_{G})\) is not identifiable up to \(_{}\)._

**Fat-hand interventions.** For the special case with independent components, the following corollary characterises identifiability under fat-hand interventions.

**Corollary 4.8**.: _[Corollary of Thm. 4.5] Suppose \(G\) is the empty graph. Suppose that our datasets encompass interventions over all variables in the latent graph, i.e., \(_{k[K]}_{k}=[d]\). Suppose for every \(k\), the targets of interventions are a strict subset of all variables, i.e., \(|_{k}|=n_{k}\), \(n_{k}[d-1]\)._

  
**Requirement of interventions** & **Learned representation \(}=}^{-1}()\)** & **Reference** \\  \(1\) intervention per node & \([h_{1}(z_{1}),h_{2}(z_{1},z_{2}),h_{3}(z_{1},z_{2},z_{3})]\) & Thm. 4.2_(i)_ \\ \(1\) perfect intervention per node & \([h_{1}(z_{1}),h_{2}(z_{2}),h_{3}(z_{3})]\) & Thm. 4.2_(ii)_ \\ \(1\) intervention per node for \(z_{1}\) and \(z_{2}\), plus \([}(3)](|}(3)|+1)=2 3\) imperfect interventions on \(z_{3}\) with “variability” assumption & \([h_{1}(z_{1}),h_{2}(z_{2}),h_{3}(z_{2},z_{3})]\) & Prop. D.1 \\ \(1\) perfect intervention on \(z_{1}\) and \(2+1{=}3\) perfect fat-hand interventions on \((z_{2},z_{3})\) & \([h_{1}(z_{1}),h_{2}(z_{2},z_{3}),h_{3}(z_{2},z_{3})]\) & Thm. 4.5 \\   

Table 1: **Overview of identifiability results.** For the DAG \(Z_{1} Z_{2} Z_{3}\) from Fig. 1, we summarise the guarantees provided by our theoretical analysis in § 4.1 for representations learnt by maximizing the likelihoods \(_{}^{k}(X)\) for different sets of interventional regimes.

Figure 3: We use the “ symbol together with a “times” symbol to represent how many interventions are required by the two assumptions. _(Left)_ (Thm. 4.5) For Asm. 4.4, we need \(n_{k}\) interventions to get block-identification of \(_{_{k}}\). _(Right)_ (Prop. 4.9) For the _block-variability_ assumption, we need \(2n_{k}\) to get to elementwise identification up to scaling and permutation.

_Suppose Assm. 4.4 is verified, which has a simpler form in this case: there are \(n_{k}\) interventions with target \(_{k}\) such that \(_{k}(_{_{k}},1)-_{k}(_{_{k}},0), ,_{k}(_{_{k}},n_{k})-_{k}(_{ _{k}},0)\) are linearly independent, where_

\[_{k}(_{_{k}},s):=(( q_{_{k},1}^{s} )^{}(z_{_{k},1}),,( q_{_{k},n_{k} }^{s})^{}(z_{_{k},n_{k}})),\] (9)

_where \(q_{_{k}}^{s}\) is the intervention of the \(s\)-th interventional regime that has the target \(_{k}\), and \(q_{_{k},j}^{s}\) is the \(j\)-th marginal of it. \(z_{_{k},j}\) is the \(j\)-th dimension of \(_{_{k}}\). \(s=0\) denotes the unintervened regime._

_Then CauCA in \((G,,_{G})\) is block-identifiable, in the same sense as Thm. 4.5._

Our interventional perspective also allows us to re-interpret and extend a key result in the theory of nonlinear ICA with auxiliary variables, [21, Thm.1]. In particular, the following Proposition holds.

**Proposition 4.9**.: _Under the assumptions of Thm. 4.5, suppose furthermore that all density functions in \(_{G}\) and all mixing functions in \(\) are \(^{2}\), and suppose there exist \(k[K]\) and there are \(2n_{k}\) interventions with targets \(_{k}\) such that for any \(_{_{k}}^{n_{k}}\), \(_{k}(_{_{k}},1)-_{k}( _{_{k}},0),,_{k}(_{_{k} },2n_{k})-_{k}(_{_{k}},0)\) are linearly independent, where_

\[_{k}(_{_{k}},s):=((,1}^{s}}{q_{_{k},1}^{s}})^{}(z_{_{k},1 }),,(,n_{k}}^{s}}{q_{_{k},n_{k}}^ {s}})^{}(z_{_{k},n_{k}}),,1}^{s }}{q_{_{k},1}^{s}}(z_{_{k},1}),,,n_{k}}^{s}}{q_{_{k},n_{k}}^{s}}(z_{_{k},n_{k}} )),\]

_where \(q_{_{k}}^{s}\) is the intervention of the \(s\)-th interventional regime that has the target \(_{k}\), and \(q_{_{k},j}^{s}\) is the \(j\)-th marginal of it. \(z_{_{k},j}\) is the \(j\)-th dimension of \(_{_{k}}\). \(s=0\) denotes the unintervened regime. Then_

\[_{_{k}}_{}:=\{ :^{n_{k}}^{n_{k}}|= _{}\}.\]

_Remark 4.10_.: The assumption of linear independence \(_{k}(_{_{k}},s),s[2n_{k}]\) precisely corresponds to the assumption of _variability_ in [21, Thm. 1]; however, we only assume it within a \(n_{k}\)-dimensional block (not over all \(d\) variables). We refer to it as _block-variability_.

Note that the block-variability assumption _implies_ block-interventional discrepancy (Asm. 4.4): i.e., it is a strictly stronger assumption, which, correspondingly, leads to a stronger identification. In fact, block-interventional discrepancy only allows _block-wise identifiability_ within the \(n_{k}\)-dimensional intervened blocks based on \(n_{k}\) interventions. In contrast, the variability assumption can be interpreted as a sufficient assumption to achieve identification _up to permutation and scaling_ within a \(n_{k}\)-dimensional block, based on \(2n_{k}\) fat-hand interventions (in both cases one unintervened dataset is required), see Fig. 3 for a visualization. We summarise our results for nonlinear ICA in Tab. 2, App. F.

In , the variability assumption is assumed to hold over _all_ variables, which in our setting can be interpreted as a requirement over \(2d\) fat-hand interventions over all latent variables simultaneously (plus one unintervened distribution). In this sense, Prop. 4.9 and _block-variability_ extend the result of [21, Thm. 1], which only considers the case where _all_ variables are intervened, by exploiting variability to achieve a strong identification only _within_ a subset of the variables.

## 5 Experiments

Our experiments aim to estimate a CauCA model based on a known graph and a collection of interventional datasets with known targets. We focus on the scenarios with single-node, perfect interventions described in SS 4. For additional technical details, see App. H.

**Synthetic data-generating process.** We first sample DAGs \(G\) with an edge density of \(0.5\). To model the causal dependence among the latent variables, we use the family of CBNs induced by linear Gaussian structural causal model (SCM) consistent with \(G\).5 For the ground-truth mixing function, we use \(M\)-layer multilayer perceptrons \(=_{M}_{1}\), where \(_{m}^{d d}\) for \(m 1,M\) denote invertible linear maps (sampled from a multivariate uniform distribution), and \(\) is an element-wise invertible nonlinear function. We then sample observed mixtures from these latent CBNs, as described by eq. (2).

**Likelihood-based estimation procedure.** Our objective is to learn an encoder \(_{}:^{d}^{d}\) that approximates the inverse function \(^{-1}\) up to tolerable ambiguities, together with latent densities \((p^{})_{k[0,d]}\) reproducing the ground truth up to corresponding ambiguities (cf. Lemma 3.3). Weestimate the encoder parameters by maximizing the likelihood, which can be derived through a change of variables from eq. (1): for an observation in dataset \(k>0\) taking a value \(\), it is given by

\[ p_{}^{k}()=| _{}()|+_{_{k}}(( _{})_{_{k}}())+_{i_{k} } p_{i}((_{})_{i}()( _{})_{(i)}()),\] (10)

where \(_{}()\) denotes the Jacobian of \(_{}\) evaluated at \(\). The learning objective can be expressed as \(^{*}=_{}_{k=0}^{K}(} _{n=1}^{N_{k}} p_{}^{k}(^{(n,k)}))\), with \(N_{k}\) representing the size of dataset \(k\).

**Model architecture.** We employ normalizing flows  to parameterize the encoder. Instead of the typically deployed base distribution with independent components, we use the collection of densities (one per interventional regime) induced by the CBN over the latents. Following the CauCA setting, the parameters of the causal mechanisms are learned while the causal graph is assumed known. For details on the model and training parameters, see App. H.

**Settings.** We investigate two learning problems: _(i)_ CauCA, corresponding to SS 4.1, and _(ii)_ ICA, where the sampled graphs in the true latent CBN contain no arrows, as discussed in SS 4.2.

**Results.**_(i)_ For a data-generating process with non-empty graph, experimental outcomes are depicted in Fig. 4 (a, e). We compare a well-specified CauCA model (_blue_) to misspecified baselines, including a model with correctly specified latent model but employing a linear encoder (_red_), and a model with a nonlinear encoder but assuming a causal graph with no arrows (_orange_). See caption of Fig. 4 for details on the metrics. The results demonstrate that the CauCA model accurately identifies the latent variables, benefiting from both the nonlinear encoder and the explicit modelling of causal dependence. We additionally test the effect of increasing a parameter influencing the magnitude of the sampled linear parameters in the SCM (we refer to this as _signal-to-noise ratio_, see App. H.1 for details)--which increases the statistical _dependence_ among the true latent components. The gap between the CauCA model and the baseline assuming a trivial graph widens (Fig. 4 (g)), indicating that _correctly_

Figure 4: **Experimental results.** Figures (a) and (e) present the mean correlation coefficients (MCC) between true and learned latents and log-probability differences between the model and ground truth (\(\) log prob.) for CauCA experiments. Misspecified models assuming a trivial graph (\(E(G){=}\)) and a linear encoder function class are compared. All violin plots show the distribution of outcomes for 10 pairs of CBNs and mixing functions. Figures (c) and (d) display CauCA results with varying numbers of nonlinearities in the mixing function and latent dimension. For the ICA setting, MCC values and log probability differences are illustrated in (b) and (f). Baselines include a misspecified model (linear mixing) and a naive (single-environment) unidentifiable normalizing flow with an independent Gaussian base distribution (labelled _i.i.d._). The naive baseline is trained on pooled data without using information about interventions and their targets. Figure (g) shows the median MCC for CauCA and the misspecified baseline (\(E(G){=}\)) as the strength of the linear parameters relative to the exogenous noise in the structural causal model generating the CBN increases. The shaded areas show the range between minimum and maximum values.

modelling the causal relationships becomes increasingly important the more (statistically) dependent the true latent variables are._ Finally, we verify that the model performs well for different number of layers \(M\) in the ground-truth nonlinear mixing (c) (performance degrades slightly for higher \(M\)), and across various latent dimensionalities for the latent variable (d).

_(ii)_ For data-generating processes where the graph contains no arrows (ICA), results are presented in Fig. 4 (b, f). Well-specified, nonlinear models _(blue)_ are compared to misspecified linear baselines _(red)_ and a naive normalizing flow baseline trained on pooled data _(purple)_. The findings confirm that _interventional information provides useful learning signal even in the context of nonlinear ICA_.

## 6 Related Work

**Causal representation learning.** In the present work, we focus on identifiability of _latent CBNs_ with a _known graph_, based on _interventional data_, and investigate the _nonlinear and nonparametric_ case. In CRL (_unknown graph_), many studies focus on identifiability of latent _SCMs_ instead, which requires strong assumptions such as weak supervision (i.e., _counterfactual data_) [1; 3; 36; 54]. Alternatively, the setting where _temporal information_ is available, i.e., dynamic Bayesian networks, has been studied extensively [29; 30; 33; 34; 62]. In a non-temporal setting, other works assume interventional data and _linear mixing functions_[49; 52]; or that the _latent distributions are linear Gaussian_. Ahuja et al.  identify latent representations by _deterministic hard_ interventions, together with _parametric_ assumptions on the mixing, and an _independent support_ assumption . Concurrent work studies the cases with non-parametric mixing and linear Gaussian latent causal mode  or non-parametric latent causal model under faithfulness, _genericity_ and Asm. 4.1 .

**Prior knowledge on the latent SCM.** Other prior works also leverage prior knowledge on the causal structure for representation learning. Yang et al.  introduce the CausalVAE model, which aims to disentangle the endogenous and exogenous variables of an SCM, and prove identifiability up to affine transformations based on known intervention targets. Shen et al.  also consider the setting in which the graph is (partially) known, but their approach requires additional supervision in the form of annotations of the ground truth latent. Leeb et al.  embed an SCM into the latent space of an autoencoder, provided with a topological ordering allowing it to learn latent DAGs.

**Statistically dependent components.** Models with causal dependences among the latent variables are a special case of models where the latent variables are statistically dependent . Various extensions of the ICA setting allow for dependent variables: e.g., independent subspace analysis ; topographic ICA  (see also ); independently modulated component analysis . Morioka and Hyvarinen  introduce a multi-modal model where _within-modality dependence_ is described by a Bayesian network, with _joint independence across the modalities_, and a mixing function for same-index variables across these networks. Unlike our work, it encodes no explicit notions of interventions.

## 7 Discussion

**Limitations. (i) Known intervention targets:** We proved that with _fully unknown targets_, there are fundamental and strong limits to identifiability (see Corollary E.8). We also studied some relaxations of this assumption (App. E), and generalized our results to _known targets up to graph automorphisms_ and _matched intervention targets_ (see Prop. E.6 and Prop. E.6). Other relaxations are left for future work; e.g., the case with a non-trivial graph and matched intervention targets is studied in , under _faithfulness_ and _genericity_ assumptions. **(ii) Estimation:** More scalable estimation procedures than our likelihood-based approach (SS 5) may be developed, e.g., based on variational inference.

**CauCA as a causal generalization of ICA.** As pointed out in SS 4.2, the special case of CauCA with a trivial graph corresponds to a novel ICA model. Beyond the fact that CauCA allows statistical dependence described by general DAGs among the components, we argue that it can be viewed as a _causal_ generalization of ICA. Firstly, we exploit the assumption of _localized and sparse_ changes in the latent mechanisms [42; 45], in contrast to previous ICA works which exploit _non-stationarity_ at the level of the entire joint distribution of the latent components [17; 21; 38], leading to strong identifiability results (e.g., in Thm. 4.2_(ii)_). Secondly, we exploit the modularity of causal mechanisms: i.e., it is possible to intervene on some of the mechanisms while leaving the others _invariant_[41; 43]. To the best of our knowledge, our work is the first ICA extension where latent dependence can actually be interpreted in a causal sense.

## Funding Transparency Statement

This work was supported by the Tubingen AI Center. L.G. was supported by the VideoPredict project, FKZ: 01IS21088.