# PointMamba: A Simple State Space Model for Point Cloud Analysis

Dingkang Liang\({}^{1}\)*, Xin Zhou\({}^{1}\)*, Wei Xu\({}^{1}\), Xingkui Zhu\({}^{1}\), Zhikang Zou\({}^{2}\),

Xiaoqing Ye\({}^{2}\), Xiao Tan\({}^{2}\), Xiang Bai\({}^{1}\)+

Footnote *: Equal contribution. \(\) Corresponding author.

\({}^{1}\)Huazhong University of Science & Technology, \({}^{2}\)Baidu Inc.

{dkliang, xzhou03, xbai}@hust.edu.cn

**Abstract**

Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose **PointMamba**, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our

Figure 1: Comprehensive comparisons between our PointMamba and its Transformer-based counterparts . (a) Without bells and whistles, our PointMamba achieve better performance than the representative Transformer-based methods on the various point cloud analysis datasets. (b)-(d) The Transformer presents quadratic complexity, while our PointMamba has linear complexity. For example, with the length of point tokens increasing, we significantly reduce GPU memory usage and FLOPs and have a faster inference speed compared to the most convincing Transformer-based method, i.e., PointMAE .

method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code is available at https://github.com/LMD0311/PointMamba.

## 1 Introduction

Point cloud analysis is one of the fundamental tasks in computer vision and has a wide range of real-world applications [47; 55; 8], including robotics, autonomous driving, and augmented reality. It is a challenging task due to the intrinsic irregularity and sparsity of point clouds. To address the issues, there has been rapid progress in deep learning-based methods [38; 33; 41; 44], consistently pushing the performance to the new record.

Recently, Transformers have achieved remarkable progress in point cloud analysis. The key to the Transformer is the attention mechanism, which can effectively capture the relationship of a set of points. By integrating self-supervised learning paradigms with fine-tuning for downstream tasks, these Transformer-based methods have achieved superior performance [60; 33; 42]. However, the complexity of attention mechanisms is quadratic, bringing significant computational cost, which is not friendly to low-resource devices. Thus, this naturally raises a question: _how to design a simple, elegant method that operates with linear complexity, thereby retaining the benefits of global modeling for point cloud analysis?_

We note the recent advance of the State Space Models (SSMs). As a pioneer, the Structured State Space Sequence Model  (S4) has emerged as a promising class of architectures for sequence modeling thanks to its strong representation ability and linear-time complexity (achieved by eliminating the need to store the complete context). Another pioneer, Mamba , adopts time-varying parameters to the SSM based on S4, proposing an efficient hardware-aware algorithm to enable highly efficient training and inference with dynamic modeling. Recently, a few concurrent methods [71; 35] successfully transfer the 1D-sequence Mamba from NLP to 2D vision tasks (e.g., image classification and segmentation), achieving similar or surpass the Transformer counterpart  while significantly reducing memory usage. However, regarding the more complex, unstructured data, e.g., 3D point cloud, the effectiveness of Mamba remains unclear. The lack of early exploration of Mamba's potential for point cloud-related tasks hinders further development of its capabilities across the diverse range of applications in this domain.

Inspired by this, this paper aims to unlock the potential of SSM in point cloud analysis tasks, discussing whether it can be a viable alternative to Transformers in this domain. Through a series of pilot experiments, we find that directly using the pioneering SSM, Mamba , can not achieve ideal performance. We argue that the main inherent limitation comes from the unidirectional modeling employed by the default Mamba, as the context is obtained by compressing the historical hidden state instead of through the interaction between each element. In contrast, the self-attention of the Transformer is invariant to the permutation of the input elements. Given the three-dimensional nature (e.g., unstructured and disordered) of point clouds, using a single scanning process often struggles to concurrently capture dependency information across various directions, which makes it difficult to construct global modeling for the RNN-like modes (e.g., Mamba).

Therefore, we introduce a simple yet effective Point Cloud State Space Model (denoted as **PointMamba**) with global modeling and linear complexity. Specifically, to enable Mamba to capture the point cloud structure causally, we first use a point tokenizer to generate two types of point tokens via a point scanning strategy, employing two space-filling curves to scan key points from different directions. As a result, the unstructured 3D point clouds can be transformed into a regular sequence. The first type of token has local modeling capabilities through sequential encoding, with the latest token holding global sequence information. Consequently, the second type of token can achieve global modeling by containing global information that comes from the first type. Besides, we propose an extremely simple order indicator to maintain the distinct spatial characteristics of different scanning when training, preserving the integrity of the spatial information.

To make the model as simple as possible, PointMamba only employs plain and non-hierarchical Mamba as the backbone to extract features for given serialized point tokens without bells and whistles. We demonstrate that PointMamba is very flexible in the pre-training paradigm, where we customize an MAE-like pertaining strategy to provide a good prior, which chooses a random serialization strategy from a pre-defined serialization bank to perform mask modeling. It facilitates the model to exact the general local relationships from different scanning perspectives, better matching the requirement of indirection modeling of Mamba.

Despite no elaborate or complex designs in the model, our PointMamba achieves superior performance on various point cloud analysis datasets (Fig. 1(a)). Besides the superior performance, thanks to the linear complexity of Mamba, we show the surprisingly low computational cost2, as shown in Fig. 1(b)-(c). These notable results underscore the potential of SSM in 3D vision-related tasks.

In conclusion, the contributions of this paper are twofold. **1)** We introduce the first state space model for point cloud analysis, named **PointMamba**, which features global modeling with linear complexity. Despite the absence of elaborate or complex structural designs, PointMamba demonstrates its potential as an optional model for 3D vision applications. **2)** Our PointMamba exhibits impressive capabilities, including structural simplicity (e.g., vanilla Mamba), low computational cost, and knowledge transferability (e.g., support for self-supervised learning).

## 2 Related work

### Point Cloud Transformers

Vision Transformer (ViT)  has become one of the mainstream architectures in point cloud analysis tasks due to its excellent global modeling ability. Specifically, Point-BERT  and Point-MAE  introduce a standard Transformer architecture for self-supervised learning and is applicable to different downstream tasks. Several works further introduce GPT scheme , multi-scale [64; 61], and multi-modal [11; 42; 43] to guide 3D representation learning. On the other hand, some researchers [20; 68; 51] focus on modifying the Transformers for point clouds. The PCT  conducts global attention directly on the point cloud. Point Transformer  applies vector attention  to perform local attention between each point and its adjacent points. The later Point Transformer series [56; 55] further extends the performance and efficiency of the Transformer for different tasks. OctFormer  leverages sorted shuffled keys of octrees to partition point clouds and significantly improve efficiency and effectiveness.

Standard Transformers can be smoothly integrated into autoencoders using an encoder-decoder design , which makes this structure ideal for pre-training and leads to significant performance improvements in downstream point cloud analysis tasks [60; 38; 6; 62; 63]. However, the attention mechanism has a time complexity of \(O(n^{2}d)\), where \(n\) represents the length of the input token sequence and \(d\) represents the dimension of the Transformer. This implies that as the input sequence grows, the operational efficiency of the Transformer is significantly constrained.

In this work, we focus on designing a simple State Space Model (SSM) for point cloud analysis without attention while maintaining the global modeling advantages of the Transformer.

### State Space Models

Linear state space equations [15; 18], combined with deep learning, offer a compelling approach for modeling sequential data, presenting an alternative to CNNs or Transformers. The Structured State Space Sequence Model  (S4) leverages a linear state space for contextualization and shows strong performance on various sequence modeling tasks, especially with lengthy sequences. To alleviate computational burden, HTTPH , DSS , and S4D  propose employing a diagonal matrix within S4, maintaining performance without excessive computational costs. The S5  proposes a parallel scan and the MIMO SSM, enabling the state space model to be efficiently utilized and widely implemented. Recently, Mamba  introduced the selective SSM mechanism, a breakthrough achieving linear-time inference and effective training using a hardware-aware algorithm, garnering considerable attention. In the vision domain, Vision Mamba  compresses the visual representation through bidirectional state space models. VMamba  introduces the Cross-Scan Module, enabling 1D selective scanning in 2D images with global receptive fields. Besides, the great potential of Mamba motivates a series of work across diverse domains, including graph [50; 3], medical segmentation [36; 34], video understanding [30; 7] and generative models [29; 31].

To the best of our knowledge, there are limited works that study SSMs for point cloud analysis. In this work, we delve into the potential of Mamba in point cloud analysis and propose PointMamba, which achieves superior performance and significantly reduces computational costs.

## 3 Preliminaries

**State Space Model.** Drawing inspiration from control theory, the State Space Model (SSM) represents a continuous system that maps a state \(x_{t}\) to \(y_{t}\) through an implicit latent state \(h_{t}^{N}\). To integrate SSMs into deep models, S4  defines the system with four parameters (\(,,\), and sampling step size \(\)). The sequence-to-sequence transformation is defined as:

\[h_{t}=}h_{t-1}+}x_{t}, y_{t}=h_{t }+x_{t},\] (1)

where \(^{1 N}\) is a project parameter, and \(^{1 N}\) represents a residual connection. The parameters \(},}\) are defined using the zero-order hold (ZOH) discretization rule:

\[}^{N N}=(), }^{N 1}=()^{-1} (()-).\] (2)

However, the parameter (\(},},,\)) are fixed across all time steps due to the Linear Time-Invariant (LTI) property of SSMs, which limits their capacity to handle varied input sequences.

Recently, Selective SSM (S6) considers parameters \(,,\) as functions of the input, effectively transforming the SSM into a time-variant model. Our PointMamba adopts a hardware-aware implementation  of S6, showing linear complexity and strong sequence modeling capability.

**Space-filling curve**. Space-filling curves are paths that traverse every point within a higher-dimensional discrete space while maintaining spatial proximity to a certain degree. Mathematically, they can be defined as a bijective function \(:^{3}\) for point clouds. Our PointMamba focuses on the Hilbert space-filling curve  and its transposed variant (called Trans-Hilbert), both of which are recognized for effectively preserving locality, ensuring that data points close in \(\) space remain close after transformation to \(^{3}\). We note that some methods [55; 51] utilize space-filling curves to partition the point cloud for capturing spatial contexts, whereas our work mainly focuses on transferring the point clouds to serialization-based sequences and combine with Mamba to implement global modeling. The motivation and objective are different.

## 4 PointMamba

This paper aims to design a simple yet solid Mamba-based Point cloud analysis method. The pipeline of our method is shown in Fig. 2. Starting with an input point cloud, we first sample the key points via Farthest Point Sampling (FPS). Then, a simple space-scanning strategy is applied to reorganize these points, resulting in serialized key points. Under a KNN and lightweight PointNet , we obtain the serialized point tokens. Finally, the entire sequence is subsequently processed by a plain, non-hierarchical encoder structure composed of several stacked Mamba blocks. Besides, to provide a good prior for PointMamba, we propose a serialization-based mask modeling paradigm, which randomly chooses a space-filling curve for serialization and mask, as shown in Fig. 4.

### The structure of PointMamba

In this section, we introduce the structure of our PointMamba. The goal of this paper is to provide a simple yet solid Mamba baseline for point cloud analysis tasks and explore the potential of plain and non-hierarchical Mamba. Thus, in the spirit of _Occam's razor_, we make the structure as simple as possible without any complex or elaborate design.

**Point scanning strategy.** Building on the pioneer works [60; 38], we first utilize the Farthest Point Sampling (FPS) to select the key points. Specifically, given an input point cloud \(^{M 3}\), where \(M\) is the number of points, the FPS is applied to sample \(n\) key points from the original point cloud \(\), resulting in \(^{n 3}\). In general, the order of the sampling key points \(\) is random, without specific order. This is not a significant problem for the previous Transformer-based methods, as the Transformer is order-invariant when processing sequence data: in the self-attention mechanism, each element at a given position can interact with all other elements in the sequence through attention weights. However, for the selective state space model, i.e., Mamba, we argue that it is hard to model the unstructured point clouds due to the unidirectional modeling. Thus, we propose to leverage the space-filling curves to transform the unstructured point clouds into a regular sequence. Specifically, we choose two representative space-filling curves to scan the key points: the Hilbert curve  and its transposed variant, denoted as Trans-Hilbert. Compared with the random sequence, space-filling curves like the Hilbert curve can preserve spatial locality well, i.e., along the scanned 1D serialized point sequence, adjacent key points often have geometrically close positions in 3D space. We argue that this property ensures that the spatial relationships between points are largely maintained, which is crucial for accurate feature representation and analysis in point cloud data. As a complementary, the Trans-Hilbert performs similarly but scans from different clues, which can provide diverse perspectives on spatial locality. By applying Hilbert and Trans-Hilbert to the key points, we obtain two different point serializations, \(_{h}\) and \(_{h^{}}\), which will be used to construct point tokens.

**Point tokenizer**. After obtaining the two serialized key points \(_{h}\) and \(_{h^{}}\), we then utilize the KNN algorithm to select \(k\) nearest neighbors for each key point, forming \(n\) token patches \(_{h}^{n k 3}\) and \(_{h^{}}^{n k 3}\) with patch size \(k\). To aggregate local information, points within each patch are normalized by subtracting the key point to obtain relative coordinates. We map the unbiased local patches to feature space using a lightweight PointNet  (point embedding layer), obtaining serialized point tokens \(_{0}^{h}^{n C}\) and \(_{0}^{h^{}}^{n C}\), where the former is the Hilbert-based and the latter is Trans-Hilbert-based.

**Order indicator.** Directly fed the two type serialized point tokens \(_{0}^{h}^{n C}\) and \(_{0}^{h^{}}^{n C}\) into Mamba encoder might cause confusion as \(_{0}^{h}\) and \(_{0}^{h^{}}\) actually share the same center but with different order. Maintaining the distinct characteristics of these different scanning strategies is important for preserving the integrity of the spatial information. Thus, we propose an extremely simple order indicator to indicate the scanning strategy used. Specifically, the proposed order indicator performs the linear transformation to transfer features into different latent spaces. The formulation can be written as follows:

\[_{0}^{h}=_{0}^{h}_{h}+_{h},_ {0}^{h^{}}=_{0}^{h^{}}_{h^{}}+_{h^{}},\] (3)

where \(_{h}/_{h^{}}^{C}\) and \(_{h}/_{h^{}}^{C}\) refer to the scale and shift factors, respectively. \(\) is the Hadamard product and is implemented by the broadcast mechanism. We then concat \(_{0}^{h}\) and \(_{0}^{h^{}}\), resulting in \(_{0}^{2n C}\).

Figure 2: The pipeline of our PointMamba. It is simple and elegant, without bells and whistles. We first utilize Farthest Point Sampling (FPS) to select the key points. Then, we propose to utilize two types of space-filling curves, including Hilbert and Trans-Hilbert, to generate the serialized key points. Based on these, the KNN is used to form point patches, which will be fed to the token embedding layer to generate the serialized point tokens. To indicate the tokens generated from which space-filling curve, the order indicator is proposed. The encoder is extremely simple, consisting of \(N\) plain and non-hierarchical Mamba blocks.

**Mamba encoder.** After obtaining the token \(_{0}\), we will feed it into the encoder, containing \(N\) Mamba block, to extract the feature. Specifically, for each Mamba block, layer normalization (LN), Selective SSM, depth-wise convolution  (DW), and residual connections are employed. A standard Mamba layer is shown in Fig. 2, and the output can be summarized as follows:

\[_{l-1}^{}=(_{l-1}), _{l}^{}=((( _{l-1}^{})))\] (4) \[_{l}^{}=(( {Z}_{l-1}^{})),_{l}=( (_{l}^{})_{l}^{ })+_{l-1}\]

\(_{l}^{2n C}\) is the output of the \(l\)-th block, and \(\) indicates SiLU activation . The \(\) is the key to the Mamba block, with a detailed description in Sec. 3. To better understand why the proposed PointMamba has global modeling capacity, we provide an intuitive visualization. As shown in Fig. 3, after modeling the first group of point tokens (i.e., Hilbert-based), the accumulated global information can improve the serialization process for the next set of tokens (i.e., Trans-Hilbert-based). This mechanism ensures that each serialized point in the Trans-Hilbert sequence is informed by the entire history of the previously processed Hilbert sequence, thereby enabling a more contextually rich and globally aware modeling process. More discussions can be found in Appendix A.2.

In our study, we show that even a very simple Mamba block without specific designs, our PointMamba can surpass the various Transformer-based point cloud analysis methods.

### The serialization-based mask modeling

One intriguing characteristic of Transformers-based methods [33; 60; 6] is their improved performance using the pre-training scheme, especially mask modeling . In this paper, considering the unidirectional modeling of Mamba, we customize a simple yet effective serialization-based mask modeling paradigm, as shown in Fig. 4.

Specifically, after obtaining the key points, we randomly choose Hilbert or Trans-Hilbert curve to implement serialization in each iteration, resulting in serialization-based key points, i.e., \(_{h}\) and \(_{h^{}}\), are obtained from Hilbert and Trans-Hilbert, respectively. Such a scheme allows the model to exact the local relationships from different scanning clues. Then, the KNN and the token embedding layer are used to generate the point tokens. To discriminate the point tokens serialized from which space-filling curves, we apply the order indicator to the point tokens, where different serialized point tokens have different order indicators, which are similar to the mentioned Eq. 3. Next, we randomly mask the serialization-based point tokens with a high ratio of 60%. Then, an asymmetric autoencoder, consisting of several vanilla Mamba blocks, is employed to extract the point feature, and the final layer of the autoencoder utilizes a simple prediction head for reconstruction. To reconstruct masked point patches in coordinate space, we employ a linear head to project the masked token to the shape of the masked input points. The Chamfer Distance  is then used as the reconstruction loss to recover the coordinates of the points in each masked point patch.

We demonstrate that with such a simple serialization-based mask modeling paradigm, PointMamba can easily achieve superior performance.

## 5 Experiments

### Implementation details

SSM is new to 3D point cloud analysis, with no existing works detailing the specific implementation. To handle different resolutions of the input point cloud, we divide them into different numbers of patches with a linear scaling (e.g., \(M=1024\) input points are divided into \(n=64\) point patches), with each patch containing \(k=32\) points determined by the KNN algorithm. The PointMamba encoder has \(N=12\) vanilla Mamba blocks, each Mamba block featuring \(C=384\) hidden dimensions. For the pre-training, we utilize ShapeNetCore  as the dataset, following previous methods [60; 38; 6]. In addition, we utilize 4 \(\) Mamba blocks as the decoder to reconstruct the masked point clouds.

Figure 3: An intuitive illustration of global modeling from PointMamba.

### Compared with Transformer-based counterparts

This paper aims to unlock the potential of Mamba in point cloud tasks, discussing whether it can be a viable alternative to Transformers. Thus, in the following experiments, **we mainly compare with the state-of-the-art vanilla Transformer-based point cloud analysis methods.**

**Real-world object classification on ScanObjectNN.** ScanObjectNN  is a challenging 3D dataset comprising about 15,000 objects across 15 categories, scanned from real-world indoor scenes with cluttered complexity backgrounds. As shown in Tab. 1, we conduct experiments on three versions of ScanObjectNN (i.e., OBJ-BG, OBJ-ONLY, and PB-T50-RS), each with increasing complexity. When compared with the most convincing Transformer-based method, i.e., PointMAAE , PointMamba surpasses it by 1.55%, 1.38% and 0.27% on OBJ-BG, OBJ-ONLY, and PB-T50-RS respectively while using less computational costs. Besides, we also outperform the SOTA PointGPT-S  by 0.93%, 0.17%, 0.14% across three variants on a comparable scale setting. Note that our method follows _Occam's Razor_, without auxiliary tasks like generation during fine-tuning

   Methods & Reference & Backbone & Param. (M) \(\) & FLOPs (G) \(\) & OBJ-BG \(\) & OBJ-ONLY \(\) & PB-T50-RS \(\) \\    \\  PointNet  & CVPR 17 & - & 3.5 & 0.5 & 73.3 & 79.2 & 68.0 \\ PointNet++  & NeurIPS 17 & - & 1.5 & 1.7 & 82.3 & 84.3 & 77.9 \\ PointCNN  & NeurIPS 18 & - & 0.6 & 0.9 & 86.1 & 85.5 & 78.5 \\ DGCNN  & TOG 19 & - & 1.8 & 2.4 & 82.8 & 86.2 & 78.1 \\ PRANet  & TIP 21 & - & - & - & - & - & 81.0 \\ MVTN  & ICCV 21 & - & 11.2 & 43.7 & - & - & 82.8 \\ PointNeXt  & NeurIPS 22 & - & 1.4 & 1.6 & - & - & 87.7 \\ PointMLP  & ICLR 22 & - & 13.2 & 31.4 & - & - & 85.4 \\ RepSur-U  & CVPR 22 & - & 1.5 & 0.8 & - & - & 84.3 \\ AIDS  & ICCV 23 & - & - & - & - & - & 87.5 \\    \\  Point-BERT  & CVPR 22 & Transformer & 22.1 & 4.8 & 87.43 & 88.12 & 83.07 \\ MaskPoint  & CVPR 22 & Transformer & 22.1 & 4.8 & 89.30 & 88.10 & 84.30 \\ Point-MAAE  & ECCV 22 & Transformer & 22.1 & 4.8 & 90.02 & 88.29 & 85.18 \\ Point-MAAE  & NeurIPS 22 & Transformer & 15.3 & 3.6 & 91.22 & 88.81 & 86.43 \\ PointM  & CVPR 24 & Transformer & - & - & 93.29 & 91.91 & 87.61 \\ Point-MAAE+IDPT  & ICCV 23 & Transformer & 1.7 & 7.2 & 91.22 & 90.02 & 89.44 \\ Point-MAAE+DAPT  & CVPR 24 & Transformer & 1.1 & 5.0 & 90.88 & 90.19 & 85.08 \\ Point-MAE\({}^{}\) & ECCV 22 & Transformer & 22.1 & 4.8 & 92.77 & 91.22 & 89.04 \\ PointGPT-S\({}^{}\) & NeurIPS 23 & Transformer & 29.2 & 5.7 & 93.39 & 92.43 & 89.17 \\ PointMamba (ours) & - & **Mamba** & **12.3** & **3.1** & **94.32** & **92.60** & **89.31** \\    \\  ACT\({}^{}\) & ICLR 23 & Transformer & 22.1 & 4.8 & 93.29 & 91.91 & 88.21 \\ Joint-MAAE  & IJCAI 23 & Transformer & - & - & 90.94 & 88.86 & 86.07 \\ I2P-MAE\({}^{}\) & CVPR 23 & Transformer & 15.3 & - & 94.15 & 91.57 & 90.11 \\ ReCons\({}^{}\) & ICML 23 & Transformer & 43.6 & 5.3 & **95.18** & **93.29** & **90.63** \\   

Table 1: Object classification on the ScanObjectNN dataset . We evaluate PointMamba on three variants, with PB-T50-RS being the most challenging. Overall accuracy (%) is reported. Param. denotes the number of tunable parameters during training. \({}^{}\) indicates that using simple rotational augmentation  for training.

Figure 4: The details of our proposed serialization-based mask modeling. During the pre-training, we randomly choose one space-filling curve to generate the serialized point tokens for mask modeling, and different serialized point tokens have different order indicators.

used in PointGPT . Furthermore, compared to cross-modal learning methods  that use additional training data (cross-modal information) or teacher models, which is not a fair comparison, our PointMamba still maintains highly competitive. We mainly want to introduce a new Mambab-based point cloud analysis methods paradigm. Although using some complex designs can bring improvement, they might be heuristics. More importantly, these heuristic designs will decrease the objectivity of the evaluation of our method.

**Synthetic object classification on ModelNet40.** ModelNet40  is a pristine 3D CAD dataset consisting of 12,311 clean samples across 40 categories. As shown in Tab. 2, we report the overall accuracy without adopting the voting strategy. The proposed PointMamba achieves the best results compared with various self-supervised Transformer-based methods . In particular, PointMamba surpasses Point-MAE  and PointGPT-S  by 0.4% and 0.3% respectively. It is worth noting that the single-modal-learned PointMamba achieves comparable results with the cross-modal-based ACT  while significantly reducing parameters and FLOPs about 44% and 38%, respectively. Additionally, PointMamba demonstrates competitive performance against elaborately designed Transformer models like OctFormer .

**Few-shot learning.** We further conduct few-shot experiments on ModelNet40  to demonstrate our few-shot transfer ability. Consistent with prior studies , we utilize the "\(n\)-way, \(m\)-shot" setup, where \(n\{5,10\}\) denotes the category count and \(m\{10,20\}\) represents the samples per category. Following standard procedure, we carry out 10 separate experiments for each setting and reported mean accuracy along with the standard deviation. As indicated in Tab. 3, our PointMamba shows competitive results with limited data, e.g., +1.0% mean accuracy compared to the cross-modal method ACT  on the 5-way 20-shot split.

**Part segmentation on ShapeNetPart.** Part segmentation on ShapeNetPart  is a challenging task that aims to predict a more detailed label for each point within a sample. As shown in Tab. 4, we report mean IoU (mIoU) for all classes (Cls.) and all instances (Inst.). Our PointMamba model demonstrates highly competitive performance compared to the Transformer-based counterparts . These impressive results further prove the potential of SSM in the point cloud analysis tasks.

   Methods & Cls. mIoU (\%) \(\) Inst. mIoU (\%) \(\) \\   \\  PointNet  & 80.39 & 83.7 \\ PointNet++  & 81.85 & 85.1 \\ DGCNN  & 82.33 & 85.2 \\ APES  & 83.67 & 85.8 \\   \\  Transformer  & 83.4 & 85.1 \\ OcCo  & 83.4 & 85.1 \\ MaskPoint  & 84.6 & 86.0 \\ Point-BERT  & 84.1 & 85.6 \\ Point-MAE  & 84.2 & 86.1 \\ PointGPT-S  & 84.1 & **86.2** \\ ACT  & **84.7** & 86.1 \\ PointMamba (**ours**) & 84.4 & **86.2** \\   

Table 4: Part segmentation on the ShapeNetPart . The mIoU for all classes (Cls.) and for all instances (Inst.) are reported.

   Methods & Param. (M) \(\) & FLOPs (G) \(\) & OA (\%) \(\) \\   \\  PointNet  & 3.5 & 0.5 & 89.2 \\ PointNet++  & 1.5 & 1.7 & 90.7 \\ PointCNN  & 0.6 &. & 92.2 \\ DGCNN  & 1.8 & 2.4 & 92.9 \\ PointNeX  & 1.4 & 1.6 & 92.9 \\ PCT  & 2.9 & 2.3 & 93.2 \\ OctFormer  & 3.98 & 31.3 & 92.7 \\   \\  Point-BERT  & 22.1 & 2.3 & 92.7 \\ MaskPoint  & 22.1 & 2.3 & 92.6 \\ PointMamba  & 12.8 & 4.6 & 93.4 \\ Point-MAE  & 22.1 & 2.4 & 93.2 \\ PointGPT-S  & 29.2 & 2.9 & 93.3 \\ ACT  & 22.1 & 2.4 & **93.6** \\   \\    
   Methods & Cls. mIoU (\%) \(\) Inst. mIoU (\%) \(\) \\   \\  PointNet  & 80.39 & 83.7 \\ PointNet++  & 81.85 & 85.1 \\ DGCNN  & 82.33 & 85.2 \\ APES  & 83.67 & 85.8 \\   \\  Transformer  & 83.4 & 85.1 \\ OcCo  & 83.4 & 85.1 \\ MaskPoint  & 84.6 & 86.0 \\ Point-BERT  & 84.1 & 85.6 \\ Point-MAE  & 84.2 & 86.1 \\ PointGPT-S  & 84.1 & **86.2** \\ ACT  & **84.7** & 86.1 \\ PointMamba (**ours**) & 84.4 & **86.2** \\   

Table 3: Few-shot learning on ModelNet40 . Overall accuracy (%)\(\)the standard deviation (%) without voting is reported.

### Analysis and ablation study

To investigate the architecture design, we conduct ablation studies on ScanObjectNN  with both pre-training and fine-tuning. Default settings are marked in \(\).

**The structural efficiency.** We first discuss the efficiency of our method. To fully explore the potential of processing the long point tokens (sequence), we gradually increase the sequence length until the GPU (NVIDIA A800 80GB) memory explodes. The comprehensive efficiency comparisons are present in Fig. 1(b)-(d), where Compared with the most convincing Transformer-based method , our PointMamba demonstrates significantly improved inference speed and reduce the GPU usage and FLOPs, especially when facing the long sequence. For example, when the length increases to more than 32,768, we outperform PointMAE by 30.2\(\), 24.9\(\), and 5.2\(\) in terms of inference speed, GPU memory, and FLOPs, respectively. More importantly, even presenting impressive efficiency, we still achieve impressive performance on various point cloud analysis datasets.

**The effect of each component.** We then study the effectiveness of the proposed components of PointMamba as shown in Tab. 5. We can make the following observations: 1) Directly utilizing random serialization, PointMamba only achieves 92.26% and 90.18% overall accuracy on OBJ-BG and OBJ-ONLY, respectively. It is reasonable as Mamba is hard to model the unstructured point clouds due to its unidirectional modeling. 2) By introducing the locality-preserved Hilbert or Trans-Hilbert scanning, PointMamba's ability to capture sequence information is enhanced, leading to performance improvements compared to random serialization. Further applying both Hilbert and Trans-Hilbert scanning curves, PointMamba surpasses the random serialization by 1.20% and 1.73% on two datasets, respectively. 3) By using the order indicator to maintain the distinct characteristics

   Scanning curve & OBJ-BG & OBJ-ONLY \\  Random & 92.60 & 90.18 \\ Huber and Trans-Hilbert & **94.32** & **92.60** \\  Z-order and Trans-Z-order & 93.29 & 90.36 \\ Hilbert and Z-order & 93.29 & 90.88 \\ Trans-Hilbert and Trans-Z-order & 93.29 & 91.91 \\   

Table 6: The effect of different scanning curves.

   Setting & Param. & OBJ-BG & OBJ-ONLY \\  w/ Identity & 11.4 & 93.80 & 91.57 \\ w/ Attention & 39.8 & 92.77 & 91.22 \\ w/ MLP & 18.5 & 93.29 & 91.22 \\ w/ Selective SSM & 12.3 & **94.32** & **92.60** \\   

Table 7: The effect of Selective SSM.

Figure 5: Different variant of PointMamba. (a) Directly removing the SSM part. (b) Replacing SSM with attention. (c) Replacing SSM with MLP. (d) Ours PointMamba with Selective SSM.

   Number & Trans-Hilbert & Order indicator & OBJ-BG & OBJ-ONLY \\  Random & 92.60 & 90.18 \\ Huber and Trans-Hilbert & **94.32** & **92.60** \\  Z-order and Trans-Z-order & 93.29 & 90.36 \\ Hilbert and Z-order & 93.29 & 90.88 \\ Trans-Hilbert and Trans-Z-order & 93.29 & 91.91 \\   

Table 8: The effect of Order indicator.

Figure 5: Different variant of PointMamba. (a) Directly removing the SSM part. (b) Replacing SSM with attention. (c) Replacing SSM with MLP. (d) Ours PointMamba with Selective SSM.

of the two different scanning strategies, we achieve notable improvement, resulting in 94.32% and 92.60% on OBJ-BG and OBJ-ONLY, respectively. Note that the order indicator is extremely light (only 1.5k parameters), which will not introduce additional computational costs.

**The effect of different scanning curves.** We further explore the effect of using different scanning curves to construct serialized point tokens. Specifically, we select two widely used space-filling curves, including Hilbert and Z-order, along with their transposed variants, i.e., Trans-Hilbert and Trans-Z-order. As listed in Tab. 6, we empirically find that serializing point clouds with space-filling curves scanning can achieve better performance compared to random sequences. We argue that scanning sequences along a specific pattern of spatial locations offers a more logical sequence modeling order for SSM. We choose the combination of Hilbert and Trans-Hilbert for PointMamba due to their superior locality-preserving properties.

**The effect of Selective SSM.** The key of S6 models or Mamba  is the SSM with the selective mechanism. We prove that as a unidirectional modeling method, SSM can be analogous to masked self-attention, ensuring each position can only attend to previous positions (the detailed proof can be found in the Appendix A.1). Thus, as shown in Tab. 7, we analyze the effect of selective SSM by removing it (i.e., identity setting) or replace with masked self-attention or MLP (an illustration is shown in Fig. 5). Compared with the identity setting, the selective SSM brings notable improvement, indicating the effectiveness of introducing global modeling from SSM. Note that while a very recent method, MambaOut , thinks the SSM of Mamba might negatively impact image classification tasks, our findings demonstrate that this is not the case for point cloud analysis tasks. Another interesting thing is that when Selective SSM is replaced with masked self-attention, the performance is even lower than that of the identity setting. We argue the main reason is that masked self-attention is hard to combine with the Gated MLP  used in default Mamba, leading to optimized difficulty, which might need to be explored in the future.

**Analysis on order indicator.** This part analyzes the effect of the order indicator. PointMamba applies Hilbert and Trans-Hilbert to recognize the key points, obtaining two types of serialized point tokens \(_{0}^{h}\) and \(_{0}^{h^{}}\). The order indicator is used to indicate the scanning strategy. As shown in Tab. 8, using two different order indicators can improve 1.20% and 1.03% compared to no indicator on OBJ-BG and OBJ-ONLY, respectively. However, using the same order indicator for both types of sequences without distinguishing between different scanning strategies does not yield positive results.

### Limitation

Although PointMamba achieves promising results, there are some limitations: 1) We only focus on the point cloud analysis task in this paper while designing a unified Mamba-based foundation model for various 3D vision tasks (e.g., 3D object classification/detection/segmentation) is a more appealing direction. 2) We only use the point clouds as training data while combining them with 2D images or language knowledge to improve the performance, which is also worthy of exploration. We left these in our future work.

## 6 Conclusion

In this paper, we present an elegant, simple Mamba-based method named PointMamba for point cloud analysis. PointMamba utilizes a space-filling curve-based point tokenizer and a plain, non-hierarchical Mamba architecture to achieve global modeling with linear complexity. Despite its structural simplicity, PointMamba delivers state-of-the-art performance across various datasets, significantly reducing computational costs in terms of GPU memory and FLOPs. PointMamba success highlights the potential of SSMs, particularly Mamba, in handling the complexities of point cloud data. As a newcomer to point cloud analysis, PointMamba is a promising option for constructing 3D vision foundation models, and we hope it can offer a new perspective for the field.