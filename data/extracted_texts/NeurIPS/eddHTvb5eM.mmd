# Nuclear Norm Regularization for Deep Learning

Christopher Scarvelis

MIT CSAIL

scarv@mit.edu &Justin Solomon

MIT CSAIL

jsolomon@mit.edu

###### Abstract

Penalizing the nuclear norm of a function's Jacobian encourages it to locally behave like a low-rank linear map. Such functions vary locally along only a handful of directions, making the Jacobian nuclear norm a natural regularizer for machine learning problems. However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its SVD. We show how to efficiently penalize the Jacobian nuclear norm using techniques tailor-made for deep learning. We prove that for functions parametrized as compositions \(f=g h\), one may equivalently penalize the average squared Frobenius norms of \(Jg\) and \(Jh\). We then propose a denoising-style approximation that avoids Jacobian computations altogether. Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems. We complement our theory with an empirical study of our regularizer's performance and investigate applications to denoising and representation learning.

## 1 Introduction

Building models that adapt to the structure of their data is a key challenge in machine learning. As real-world data typically concentrates on low-dimensional manifolds, a good model \(f\) should only be sensitive to changes to its inputs along the data manifold. One may encourage this behavior by regularizing \(f\) so that its _Jacobian_\(Jf[x]\) has low rank. This causes \(f\) to locally behave like a low-rank linear map and therefore be locally constant in directions that are in the kernel of \(Jf[x]\).

How should we regularize \(f\) so that its Jacobians have low rank? Directly penalizing \((Jf[x])\) during training is challenging, as the rank function is not differentiable. In light of this, the _nuclear norm_\(\|Jf[x]\|_{*}\) is an appealing alternative: Being the \(L^{1}\) norm of a matrix's singular values, it is the tightest convex relaxation of the rank function, and as it is differentiable almost everywhere, it can be included in standard deep learning pipelines.

One critical flaw in this strategy is its computational cost. The nuclear norm of a matrix is the sum of its _singular values_, so to penalize \(\|Jf[x]\|_{*}\) in training, one must (1) compute the Jacobian of a typically high-dimensional map \(f:^{n}^{m}\), (2) take the singular value decomposition (SVD) of this \(m n\) matrix, (3) sum its singular values, and (4) differentiate through each of these operations. The combined cost of these operations is prohibitive for high-dimensional data. Consequently, nuclear norm regularization has yet to be widely adopted by the deep learning community.

This work shows how to efficiently penalize the Jacobian nuclear norm \(\|Jf[x]\|_{*}\) using techniques tailor-made for deep learning. We first show that parametrizing \(f\) as a composition \(f=g h\) - a feature common to _all deep learning pipelines_ - allows one to replace the expensive nuclear norm \(\|Jf[x]\|_{*}\) with two squared _Frobenius_ norms \(\|Jh[x]\|_{F}^{2}\) and \(\|Jg[h(x)]\|_{F}^{2}\), which admit an elementwise computation that avoids a costly SVD. We prove that the resulting problem is _exactly_ equivalent to the original problem with the nuclear norm penalty. We in turn approximate this by a denoising-style objective that avoids the Jacobian computation altogether. Our method issimple, efficient, and accurate - both in theory and in practice - and enables Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems.

We complement our theoretical results with an empirical study of our regularizer's performance on synthetic data. As the Jacobian nuclear norm has seldom been used as a regularizer in deep learning, we propose applications of our method to unsupervised denoising, where one trains a denoiser given a dataset of noisy images without access to their clean counterparts, and to representation learning.

**Our work makes the Jacobian nuclear norm a feasible component of deep learning pipelines**, enabling users to learn locally low-rank functions unencumbered by the heavy cost of naive Jacobian nuclear norm regularization.

## 2 Related work

Nuclear norm regularization.As penalizing the nuclear norm in matrix learning problems encourages low-rank solutions, nuclear norm regularization (NNR) has been widely used throughout machine learning. Rennie and Srebro (2005) propose NNR for collaborative filtering, where one attempts to predict user interests by aggregating incomplete information from a large pool of users. Candes et al. (2011) introduce robust PCA, which decomposes a noisy matrix into low-rank and sparse parts and uses nuclear norm regularization to learn the low-rank part. Cabral et al. (2013); Dai et al. (2014) use NNR to regularize the ill-posed structure-from-motion problem, which recovers a 3D scene from a set of 2D images.

A line of work beginning with Candes and Recht (2012) takes inspiration from compressed sensing and studies the conditions under which a low-rank matrix can be perfectly recovered from a small sample of its entries via nuclear norm minimization. This work was followed by Candes and Tao (2010); Keshavan et al. (2009); Recht (2011), which progressively sharpen the bounds on the number of samples required for exact recovery. In parallel, Cai et al. (2010) propose singular value thresholding (SVT), a simple algorithm for approximate nuclear norm minimization that avoids solving a costly semidefinite program as with earlier algorithms. However, SVT still requires computing a singular value decomposition in each iteration, which is an onerous requirement for large matrices. Motivated by this challenge, Rennie and Srebro (2005) show how to convert a nuclear norm-regularized matrix learning problem into an equivalent non-convex problem involving only squared Frobenius norms. Our work generalizes their method to non-linear learning problems.

Jacobian regularization in deep learning.Sokolic et al. (2017); Varga et al. (2017); Hoffman et al. (2020) penalize the spectral and Frobenius norms of a neural net's Jacobian with respect to its inputs to improve classifier generalization, particularly in the low-data regime. Similarly, Jakubovitz and Giryes (2018) fine-tune neural classifiers with Jacobian Frobenius norm regularization to improve adversarial robustness. Unlike our work, these papers do not consider nuclear norm regularization.

Neural ODEs (Chen et al., 2018) parametrize functions as solutions to initial value problems with neural velocity fields. Ensuring that the learned dynamics are well-conditioned minimizes the number of steps required to accurately solve these ODEs. To this end, Finlay et al. (2020) penalize the squared Frobenius norm of the velocity field's Jacobian and observe a tight relationship between the value of this norm and the step size of an adaptive ODE solver. Kelly et al. (2020) extend this approach by regularizing higher-order derivatives as well.

Finally, it has been observed as early as Webb (1994); Bishop (1995) that training a neural net on noisy data approximately penalizes the squared Frobenius norm of the network Jacobian at training data. Inspired by this observation, Vincent et al. (2008, 2010) propose denoising autoencoders for representation learning, and Rifai et al. (2011) propose directly penalizing the squared Frobenius norm of the encoder Jacobian to encourage robust latent representations. Alain and Bengio (2014) show that an autoencoder trained with a penalty on the squared Frobenius norm of its Jacobian learns the score of the data distribution for small regularization values. Recently, Kadkhodaie et al. (2024) employ a similar analysis of the denoising objective to study the generalization of diffusion models.

Denoising via singular value shrinkage.While a full survey of the denoising literature is out of scope (see e.g. Elad et al. (2023)), we highlight a handful of works that employ _singular value shrinkage_ (SVS) to denoise low-rank data corrupted by isotropic noise given a noisy data matrix. SVS denoises a noisy data matrix \(Y\) by applying a shrinkage function \(\) to its singular values \(_{d}\)

[MISSING_PAGE_FAIL:3]

### Our key result

Equation (2) enables the use of simple and efficient gradient-based methods for learning a low-rank linear map by parametrizing the matrix \(A\) as a _composition_\(A=UV^{}\) of linear maps \(U\) and \(V^{}\). In deep learning, one is interested in learning _non-linear_ functions that are parametrized by compositions of simpler functions. Such functions \(f\) are differentiable almost everywhere, so they are _locally_ well-approximated by linear maps specified by their _Jacobians_\(Jf[x]\).

Encouraging the learned function to have a low-rank Jacobian is a natural prior: It corresponds to learning a function that locally behaves like a low-rank linear map. Such functions vary locally along only a handful of directions and are constant in the remaining directions. When the training data is supported on a low-dimensional manifold, these directions correspond to the tangents and normals, respectively, to the data manifold. One may implement this low-rank prior on \(Jf\) by solving the following optimization problem:

\[_{f:^{n}^{m}x()}[(f(x ),x)+\|Jf[x]\|_{*}],\] (3)

where \(\) is a generic differentiable loss function and \(()\) is a data distribution supported on \(^{n}\). If \(f\) is parametrized as a neural network and \(n,m\) are large, this problem is costly to optimize via stochastic gradient descent, as \(Jf[x]^{m n}\) and computing the subgradient of \(\|Jf[x]\|_{*}\) requires a cubic-time SVD. In fact, simply _storing_\(Jf[x]\) in memory is often intractable for large \(n,m\), which is typical when \(f\) is an image-to-image map. For example, if \(f\) is a denoiser operating on \(1024 1024\) RGB images, its inputs are \(3 1024 1024=3,\!145,\!728\)-dimensional, and \(Jf[x]\) occupies nearly 40 TB of memory.

To address these challenges, we first prove a theorem generalizing (2) to non-linear functions. We then show how to avoid computing \(Jf[x]\) altogether using a first-order Taylor expansion and Hutchinson's estimator. **Our primary contribution is the following result:**

**Theorem 3.1**: _Let \(D()\) be a data distribution supported on a compact set \(^{n}\) with measure \(\) that is absolutely continuous with respect to the Lebesgue measure on \(\). Let \( C^{1}(^{m}^{n})\) be a continuously differentiable loss function. Then,_

\[_{f C^{}()}}_{x()}[(f(x),x)+\|Jf[x]\|_{*}]\\ =_{h C^{}()\\ g C^{}(h())}}_{x( )}[(g(h(x)),x)+(\|Jg[h(x)]\|_{F}^{2}+\|Jh[x ]\|_{F}^{2})].\] (4)

On the left-hand side, we learn a function \(f:^{n}^{m}\) given fixed input and output dimensions \(n,m\). On the right-hand side, we learn functions \(h:^{n}^{d}\) and \(g:^{d}^{m}\) with \(n,m\) fixed but optimize over the inner dimension \(d\). We prove this theorem in Appendix A.2 and sketch the proof below. Theorem 3.1 shows that by parametrizing \(f\) as a composition of \(g\) and \(h\) - a feature common to all deep learning pipelines - one may learn a _locally_ low-rank function without computing expensive SVDs during training.

Proof sketch.We denote the left-hand side objective by \(E_{L}(f)\) and its inf by \((L)\); we denote the right-hand side objective by \(E_{R}(g,h)\) and its inf by \((R)\). We prove that \((L)(R)\) and \((R)(L)\). \((L)(R)\) is the easy direction. The basic observation is that if \(f=g h\), then \(Jf[x]=Jg[h(x)]Jh[x]\) by the chain rule. Equation (1) then implies that

\[\|Jf[x]\|_{*}=_{U,V:UV^{}=Jf[x]}(\|U\|_{F}^{2}+\|V\|_{F }^{2})(\|Jg[h(x)]\|_{F}^{2}+\|Jh[x]\|_{F}^{2}).\]

\((R)(L)\) is the hard direction. The proof strategy is as follows:

1. We begin with a function \(f_{m} C^{}()\) such that \(E_{L}(f_{m})\) is arbitrarily close to its inf over \(C^{}()\). We use \(f_{m}\) to construct parametric families of affine functions \(g_{m}^{z},h_{m}^{z}\) whose composition is a good local approximation to \(f_{m}\) in a neighborhood of \(z\), both pointwise and in terms of the contributions to \(E_{R}(g_{m}^{z},h_{m}^{z})\) and \(E_{L}(f_{m})\), resp., due to \(x\) near \(z\).

2. We then stitch together these local approximations to form a sequence of global approximations \(g_{m}^{k},h_{m}^{k}\) to \(f_{m}\). These functions are piecewise affine and hence not regular enough to lie in \(C^{}()\) as required by the right-hand side of Equation (4).
3. Finally, mollifying the piecewise affine functions \(g_{m}^{k},h_{m}^{k}\) yields a minimizing sequence of \(C^{}()\) functions \(g_{m,}^{k},h_{m,}^{k}\) such that \(E_{R}(g_{m,}^{k},h_{m,}^{k})\) approaches the inf of \(E_{L}\).

While Theorem 3.1 shows how to regularize a learning problem with the Jacobian nuclear norm without a cubic-time SVD, the Jacobian computation incurs a quadratic time and memory cost, which remains heavy for high-dimensional learning problems. To mitigate this issue, the following section shows how to approximate the \(\|Jg[h(x)]\|_{F}^{2}\) and \(\|Jh[x]\|_{F}^{2}\) terms in (4).

### Estimating the Jacobian Frobenius norm

When \(f:^{n}^{m}\) is a function between high-dimensional spaces, \(Jf[x]\) is an \(m n\) matrix that is costly to compute and to store in memory. Previous works employing Jacobian regularization for neural networks have noted this issue and proposed stochastic approximations based on Jacobian-vector products (JVP) against random vectors (Varga et al., 2017; Hoffman et al., 2020). As JVPs may be costly to compute for large neural nets, we propose an alternative stochastic estimator that requires only evaluations of \(f\) and analyze its error:

**Theorem 3.2**: _Let \(f:^{n}^{m}\) be continuously differentiable. Then,_

\[^{2}\|Jf[x]\|_{F}^{2}=*{}_{(0,^{2}I)}[\|f(x+)-f(x)\|_{2}^{2}]+O(^{2}).\] (5)

Similar results appear in the ML literature as early as Webb (1994). Our proof in Appendix A.3 relies on a first-order Taylor expansion and Hutchinson's trace estimator; a similar proof is given by Alain and Bengio (2014). In practice, we obtain accurate approximations to \(\|Jf[x]\|_{F}^{2}\) by using a small noise variance \(^{2}\) and rescaling the expectation in (5) by \(}\) to compensate. In Section 4, we also show that a single noise sample \((0,^{2}I)\) suffices in practice.

Using this efficient approximation, we obtain the following regularizer:

\[(x;f)=}*{}_{ (0,^{2}I)}[\|g(h(x)+)-g(h(x))\|_{2}^{2}+\|h (x+)-h(x)\|_{2}^{2}],\] (6)

where \(f=g h\). In practice, one may use a single draw of \((0,^{2}I)\) per training iteration while maintaining good performance on downstream tasks; see e.g. the results in Section 5. In this case, our regularizer \((x;f)\) costs merely two additional function evaluations, enabling it to scale to large neural networks acting on high-dimensional data. In Section 4, we show that parametrizing \(f_{}\) as a neural net and solving

\[_{f_{}:^{n}^{m}x()} [(f_{}(x),x)+(x;f_{})]\] (7)

yields good approximations to the solution to (3) for problems where exact solutions are known. We then propose two applications of Jacobian nuclear norm regularization in Section 5.

## 4 Validation

In this section, we empirically validate our method on a special case of (3) for which closed-form solutions are known. We consider the following problem:

\[_{f:^{n}}_{^{n}}[\|f (x)-(x)\|_{2}^{2}+\|Jf[x]\|_{*}]x,\] (8)

where \(:^{n}\) is the indicator function of the unit ball in \(^{n}\). As \(f\) is a scalar-valued function in this problem, \(Jf[x]\) is a _vector_, and \(\|Jf[x]\|_{*}=\| f(x)\|_{2}\). This is an instance of the celebrated _Rudin-Osher-Fatemi_ (ROF) model for image denoising [Rudin et al., 1992]. Meyer [2001, p. 36] shows that the exact solution to (8) given this target function \((x)\) is \(f(x):=(1-n)(x)\). This is a rescaled indicator function of the unit ball.

We parametrize \(f\) as a multilayer perceptron (MLP) \(f_{}\) and solve (8) along with the problem using our regularizer:

\[_{f_{}:^{n}_{x}( )}[\|f_{}(x)-(x)\|_{2}^{2}+(x;f _{})],\] (9)

where \(f_{}=g_{} h_{}\). We approximate the integral over \(^{n}\) by Monte Carlo integration over a box \(\) centered at the origin. We experiment with \(n=2\) and \(n=5\) and in each case depict results for a small and a large regularization value \(\). We track the objective values of problems (8) and (9) and show that they converge to the same value, as predicted by Theorem 3.1. We also track the absolute error of both problem's solutions across training iterations and plot solutions to each problem at convergence for the 2D case. We give full implementation details in Appendix B.1.

Figure 1 depicts the exact solution to (8) for \(n=2\) and two values of \(\), along with neural solutions to the same problem (8) and the problem with our regularizer (9). Solving our Jacobian-free problem (9) with 10 draws of \(\) per training iteration yields accurate solutions to the ROF problem for both values of \(\), with our problem yielding slightly diffuse transitions across the boundary of the unit disc.

Figure 2 confirms the accuracy our our method's solutions, which attain absolute error comparable to the neural solutions to (8).

Figure 3 depicts the objective values for Problems (8) and (9) on log scale across training iterations. As predicted by Theorem 3.1, both converge toward nearly identical objective values; the larger gap in Figure 3(c) is an artifact of the loss magnitudes being smaller and the plot being on log scale.

## 5 Applications

Unsupervised denoising.In this section, we apply our regularizer \((x;f_{})\) to _unsupervised denoising_. We learn a denoiser \(f_{}\) that maps noisy images \(x+\) to clean images \(x\) given a training set of noisy images _without_ their corresponding clean images. We motivate the use of our regularizer via a connection with denoising by singular value shrinkage, and demonstrate that our unsupervised denoiser nearly matches the performance of a denoiser trained on clean-noisy image pairs.

Singular value shrinkage.A line of work beginning with Shabalin and Nobel (2010) studies denoising by _singular value shrinkage_ (SVS). These works seek to recover a low-rank data matrix \(X^{D N}\) of _unknown_ rank \(r\) given only a single matrix \(Y=X+_{}Z\) of clean data \(X\) corrupted by iid white noise \(Z\). Since the clean data is low-rank, the components of \(Y\) corresponding to its small singular values contain mostly noise, so SVS denoises \(Y\) by applying a shrinkage function \(\) to its singular values \(_{d}\). This function _shrinks_ small singular values of \(Y\) while leaving larger singular values untouched. For convenience, we denote the denoised matrix by \((Y)\).

Gavish and Donoho (2017) show that under certain assumptions on the noise \(Z\) and data \(X\), one can derive the optimal shrinker \(\) that asymptotically minimizes \(\|X-(Y)\|_{F}^{2}\). In Appendix A.4, we show that this optimal shrinker is also the solution to the following problem:

\[_{A^{D D}}\|AY-Y\|_{F}^{2}+\|A\|_{*},\] (10)

where we set \(=_{}^{2}\) to be equal to the noise variance. This problem is a special case of the following instance of Problem (3)

\[_{f_{}:^{D}^{D}}()}{}[\|f_{}(y)-y\|_{2}^{2}+ \|Jf_{}[y]\|_{*}]\] (11)

when \(()\) is an empirical distribution over \(N\)_noisy_ training samples and \(f_{}\) is restricted to be a linear map. Just as (10) yields an optimal shrinker for denoising low-rank data which _globally_ lies in a low-dimensional subspace, we conjecture that solving (11) yields an effective denoiser for manifold-supported data such as images, which _locally_ lie near low-dimensional subspaces - even when trained on noisy images. We test this conjecture by solving (11) using a neural denoiser \(f_{}\). To make this problem tractable, we replace \(\|Jf_{}[y]\|_{*}\) with our regularizer \((y;f_{})\), which we compute using a single draw of \(\) per training iteration.

Experiments.We train our denoiser by solving (11) with \(()\) being the empirical distribution over 288k _noisy_ images from the Imagenet training set Russakovsky et al. (2015). Consequently,

  \\   &  &  \\ 
**Method** & \(=1\) & \(=2\) & \(=1\) & \(=2\) \\  BM3D & \(21.26 2.81\) & \(18.71 2.33\) & \(19.42 1.88\) & \(16.77 1.30\) \\ Ours & \(23.10 3.12\) & \(21.05 2.85\) & \(21.08 2.04\) & \(19.10 1.80\) \\ N2N & \(23.12 3.05\) & \(21.21 3.02\) & \(20.37 1.71\) & \(19.37 1.89\) \\ Supervised & \(23.37 3.25\) & \(21.39 2.97\) & \(21.62 2.28\) & \(19.54 1.95\) \\  

Table 1: Denoiser performance via average PSNR on held-out images. Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data without access to clean images.

our denoiser does not see any clean images during training. The clean images' channel intensities lie in \([-1,1]\), and we corrupt them with Gaussian noise with standard deviations \(\{1,2\}\). We set \(=^{2}\) when solving (11). We parametrize the denoiser \(f_{}=g_{} h_{}\) as a Unet , letting \(h_{}\) and \(g_{}\) be its downsampling and upsampling blocks, resp.

We benchmark our method against a supervised denoiser trained with the usual MSE loss \(\|f_{}(x+)-x\|_{2}^{2}\) on clean-noisy pairs, Lehtinen et al. 's Noise2Noise (N2N) method, which requires access to independent noisy copies of each ground truth image during training, and BM3D, a classical unsupervised denoiser . We implement the supervised and N2N denoisers using the same Unet architecture as our denoiser, and train them on the same dataset with the same hyperparameters. We evaluate the denoisers via their average peak signal-to-noise ratio (PSNR) across the CBSD68 dataset  and across 100 randomly-drawn noisy images from the Imagenet validation set, randomly cropped to \(256 256\). We provide full details for this experiment in Appendix B.2.

We report each denoiser's performance in Table 1 and include 1-sigma error bars computed across the test images. Despite being trained _exclusively on highly corrupted images_, our denoiser nearly matches the performance of an ordinary supervised denoiser at both noise levels and performs comparably to Noise2Noise, which requires independent noisy copies of each ground truth image during training.

We further illustrate the comparison in Figure 4. All neural methods recover substantially more fine detail than the classical BM3D denoiser, particularly at the larger noise level \(=2\). Notably, our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data.

We also demonstrate the sparsity-inducing effect of our regularizer on the singular values of \(Jf_{}\) in Figure 5, where we plot the Jacobian singular values of our denoiser and a supervised denoiser at a randomly-drawn validation image corrupted with \(=2\) Gaussian noise. We normalize the singular values so that each Jacobian's largest singular value is 1 and depict the singular values on log scale. As expected, our denoiser's Jacobian singular values decay more rapidly than those of the supervised denoiser at the same point.

These results show that our regularizer (6) can be used to construct a tractable non-linear generalization of Gavish and Donoho 's optimal shrinker that performs nearly as well as a supervised denoiser on image denoising tasks, despite being trained exclusively on highly corrupted images.

Figure 4: Denoiser performance comparison on held-out image corrupted by Gaussian noise with \(=1\) (first row) and \(=2\) (second row). Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data.

Figure 5: Jacobian singular values of supervised denoiser (blue) and our denoiser (orange) evaluated at a noisy held-out image with \(=2\).

Representation learning.We now apply our method to unsupervised representation learning. We train a deterministic autoencoder consisting of an encoder \(f_{}\) and a decoder \(g_{}\) on the CelebA dataset (Liu et al., 2015) and approximately penalize the Jacobian nuclear norm \(\|J_{f_{}}[x]\|_{*}\) of the encoder at training data \(x()\) using our regularizer \((x;f_{})\). This encourages the encoder to locally behave like a low-rank linear map whose _image_ is low-dimensional. One may interpret this as a deterministic autoencoder with locally low-dimensional latent spaces. We demonstrate that the left-singular vectors of the encoder Jacobian \(J_{f_{}}[x]\) are semantically meaningful directions of variation about training data in latent space. We provide full experimental details in Appendix B.3.

To demonstrate our autoencoder's ability to learn meaningful representations, we select an arbitrary training point \(x\) and traverse the latent space of our regularized autoencoder and an unregularized baseline along rays of the form \(z=f_{}(x)+ u_{}^{d}(x)\), where \(u_{}^{d}(x)\) is the \(d\)-th left-singular vector of the encoder Jacobian \(Jf_{}[x]\). These left-singular vectors form a basis for the image of \(Jf_{}[x]\) and approximate a basis for the tangent space of the latent manifold at \(f_{}(x)\). We depict decoded images along this traversal for our regularized autoencoder in Figure 7 and for the baseline in Figure 6.

The traversals in Figure 7 are semantically meaningful. For instance, a traversal along the first singular vector edits the tint of the decoded image, and a traversal along the second singular vector edits the facial expression of the image's subject. The traversals of the unregularized autoencoder's latent space in Figure 6 edit the colors of the decoded image but are unable to control other attributes.

We also follow Higgins et al. (2017) and visualize traversals along latent variables of a \(\)-VAE at the same training point \(x\) in Figure 8. While the \(\)-VAE is able to discover meaningful directions of variation, the decoded images are highly diffuse, as is typical of VAEs. In contrast, our autoencoder's reconstructions retain finer details. We conjecture that our model's improved capacity results from our autoencoder's ability to learn a locally low-dimensional latent space without constraining its global structure.

## 6 Conclusion

The Jacobian nuclear norm \(\|Jf[x]\|_{*}\) is a natural regularizer for learning problems, where it steers solutions towards having low-rank Jacobians. Such functions are locally sensitive to changes in their inputs in only a few directions, which is an especially desirable prior for data that is supported on a low-dimensional manifold. However, computing \(\|Jf[x]\|_{*}\) naively requires both evaluating a Jacobian and taking its SVD; the combined cost of these operations is prohibitive for the high-dimensional maps \(f\) that often arise in deep learning.

Our work resolves this computational challenge by generalizing a surprising result (2) from matrix learning to non-linear learning problems. As they rely on parametrizing the learned function \(f=g h\) as a composition of functions \(g\) and \(h\), our methods are tailor-made for deep learning, where such parametrizations are ubiquitous. We anticipate that the deep learning community will discover additional applications of Jacobian nuclear norm regularization to make use of our efficient methods.

As an efficient implementation of our regularizer relies on estimating the squared Jacobian Frobenius norm using Hutchinson's trace estimator, some error is inevitable. This error manifests itself in slightly diffuse boundaries in the solutions to the Rudin-Osher-Fatemi problem in Figure 1. However, we do not find this error problematic for high-dimensional applications such as unsupervised denoising as in Section 5. Future implementations of our method may employ more accurate estimators of the squared Jacobian Frobenius norm for applications where accuracy is of paramount concern.