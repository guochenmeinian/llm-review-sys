# NRGBoost: Energy-Based Generative Boosted Trees

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an effective energy-based generative boosting algorithm that is analogous to the second order boosting algorithm implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT algorithms on a number of real world tabular datasets and outperform competing approaches for sampling.

## 1 Introduction

Generative models have achieved tremendous success in computer vision and natural language processing, where the ability to generate synthetic data guided by user prompts opens up many exciting possibilities. While generating synthetic table records does not necessarily enjoy the same wide appeal, this problem has still received considerable attention as a potential avenue for bypassing privacy concerns when sharing data. Estimating the data density, \(p()\), is another typical application of generative models which enables a host of different use cases that can be particularly interesting for tabular data. Unlike discriminative models which are trained to perform inference over a single target variable, density models can be used more flexibly for inference over different variables or for out of distribution detection. They can also handle inference with missing data in a principled way by marginalizing over unobserved variables.

The development of generative models for tabular data has mirrored its progression in computer vision with many of its Deep Learning (DL) approaches being adapted to the tabular domain (Jordon et al., 2018; Xu et al., 2019; Engelmann and Lessmann, 2020; Fan et al., 2020; Zhao et al., 2021; Kotelnikov et al., 2022). Unfortunately, these methods are only useful for sampling as they either don't model the density explicitly or can't evaluate it due to untractable marginalization over high dimensional latent variable spaces. Furthermore, despite growing in popularity, DL has still failed to displace tree-based ensemble methods as the tool of choice for handling tabular discriminative tasks with gradient boosting still being found to outperform neural-network-based methods in many real world datasets (Grinsztajn et al., 2022; Borisov et al., 2022).

While there have been recent efforts to extend the success of tree-based models to generative modeling (Correia et al., 2020; Wen and Hang, 2022; Nock and Guillame-Bert, 2022; Watson et al., 2023; Nock and Guillame-Bert, 2023; Jolicoeur-Martineau et al., 2023), we find that direct extensions of Random Forests (RF) and Gradient Boosted Decision Tree (GBDT) are still missing. It is this gap that we try to address, seeking to keep the general algorithmic structure of these popular algorithmsbut replacing the optimization of their discriminative objective with a generative counterpart. Our main contributions in this regard are:

* Proposing NRGBoost, a novel energy-based generative boosting model that, analogously to the boosting algorithms implemented in popular GBDT packages, is trained to maximize a local second order approximation to the likelihood at each boosting round.
* Proposing an approximate sampling algorithm to speed up the training of any tree-based multiplicative generative boosting model.
* Exploring the use of bagged ensembles of Density Estimation Trees (DET) [Ram and Gray, 2011] with feature subsampling as the generative counterpart to RF.

The longstanding popularity of GBDT models in machine learning practice can, in part, be attributed to the strength of its empirical results and the efficiency of its existing implementations. We therefore focus on an experimental evaluation in real world datasets spanning a range of use cases, number of samples and features. We find that, on smaller datasets, our implementation of NRGBoost can be trained in a few minutes on a mid-range consumer CPU and achieve similar discriminative performance to a standard GBDT model while also being able to generate samples that are generally harder to distinguish from real data than state of the art neural-network-based models.

## 2 Energy Based Models

An Energy-Based Model (EBM) parametrizes the logarithm of a probability density function directly (up to an unspecified normalizing constant):

\[q_{f}()=))}{Z[f]}.\] (1)

Here \(f():\) is a real function over the input domain.1 We will avoid introducing any parametrization, instead treating the function \(f()\) lying in an appropriate function space over the input space as our model parameter directly. \(Z[f]=_{}(f())\), known as the partition function, is then a functional of \(f\) giving us the necessary normalizing constant.

This is the most flexible way one could represent a probability density function making essentially no compromises on its structure. The downside to this is that for most interesting choices of \(\), computing or estimating this normalizing constant is untractable which makes training these models difficult. Their unnormalized nature however does not prevent EBMs from being useful in a number of applications besides sampling. Performing inference over a small enough subset of variables requires only normalizing over the set of their possible values and for anomaly or out of distribution detection, knowledge of the normalizing constant is not necessary.

One common way to train an energy-based model to approximate a data generating distribution, \(p()\), is to minimize the Kullback-Leibler divergence between \(p\) and \(q_{f}\), or equivalently, maximize the expected log likelihood functional:

\[L[f]=_{ p} q_{f}()=_{ p}f()- Z[f]\] (2)

Figure 1: Downsampled MNIST samples generated by NRGBoost and two tabular DL methods.

This optimization is typically carried out by gradient descent over the parameters of \(f\), but due to the untractability of the partition function, one must rely on Markov Chain Monte Carlo (MCMC) sampling to estimate the gradients (Song and Kingma, 2021).

## 3 NRGBoost

Expanding the increase in log-likelihood in equation 2 due to a variation \( f\) around an energy function \(f\) up to second order we have

\[L[f+ f]-L[f]_{ p} f()- _{ q_{f}} f()-_{  q_{f}} f() L_{f}[ f]\,.\] (3)

The \( f\) that maximizes this quadratic approximation should thus have a large positive difference between the expected value under the data and under \(q_{f}\) while having low variance under \(q_{f}\). We note that just like the original log-likelihood, this Taylor expansion is invariant to adding an overall constant to \( f\). This means that, in maximizing equation 3 we can consider only functions that have zero expectation under \(q_{f}\) in which case we can simplify \( L_{f}[ f]\) as

\[ L_{f}[ f]=_{ p} f()- _{ q_{f}} f^{2}()\,.\] (4)

We thus formulate our boosting algorithm as modelling the data density with an additive energy function. At each boosting iteration we improve upon the current energy function \(f_{t}\) by finding an optimal step \( f_{t}^{*}\) that maximizes \( L_{f_{t}}[ f]\)

\[ f_{t}^{*}=_{ f_{t}} L_{f_{t}}[ f ]\,,\] (5)

where \(_{t}\) is an appropriate space of functions (satisfying \(_{ q_{f_{t}}} f()=0\) if equation 4 is used). The solution to this problem can be interpreted as a Newton step in the space of energy functions. Because for an energy-based model, the Fisher Information matrix with respect to the energy function and the hessian of the expected log-likelihood are the same, we can also interpret the solution to equation 5 as a natural gradient step (see the Appendix A). This approach is essentially analogous to the second order step implemented in modern discriminative gradient boosting libraries such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017) and which can be traced back to Friedman et al. (2000).

In updating the current iterate, \(f_{t+1}=f_{t}+_{t} f_{t}^{*}\), we scale \( f_{t}^{*}\) by an additional scalar step-size \(_{t}\). This can be interpreted as a globalization strategy to account for the fact that the quadratic approximation in equation 3 is not necessarily valid over large steps in function space. A common strategy in nonlinear optimization would be to select \(_{t}\) via a line search based on the original log-likelihood. Common practice in discriminative boosting however is to interpret this step size as a regularization parameter and to select a fixed value in \(]0,1]\) with (more) smaller steps typically outperforming fewer larger ones when it comes to generalization. We choose to adopt a hybrid strategy, first selecting an optimal step size by line search and then shrinking it by a fixed factor. We find that this typically accelerates convergence allowing the algorithm to take comparatively larger steps that increase the likelihood in the initial phase of boosting. For a starting point, \(f_{0}\), we can choose the logarithm of any probability distribution over \(\) as long as it is easy to evaluate. Sensible choices are a uniform distribution (i.e., \(f 0\)), the product of marginals for the training set, or any mixture distribution between these two.

### Weak Learners

As a weak learner we will consider functions defined by trees over the input space. I.e., letting \(_{j=1}^{J}X_{j}=\) be the partitioning of the input space induced by the leaves of a binary tree whose internal nodes represent a split along one dimension into two disjoint partitions, we take as \(\) the set of functions such as

\[ f()=_{j=1}^{J}w_{j}_{X_{j}}()\,,\] (6)

where \(_{X}\) denotes the indicator function of a subset \(X\) and \(w_{j}\) are values associated with each leaf \(j[1..J]\). In a standard decision tree these values would typically encode an estimate of \(p(y| X_{j})\), with \(y\) being a special _target_ variable that is never considered for splitting. In our generative approach they encode unconditional densities (or more precisely energies) over each leaf's support and every variable can be used for splitting. Note that our functions \( f\) are thus parametrized by the values \(w_{j}\) as well the structure of the tree and the variables and values for the split at each node which ultimately determine the \(X_{j}\). We omit these dependencies for brevity.

Replacing the definition in equation 6 in our objective (equation 4) we get the following optimization problem to find the optimal decision tree:

\[_{w_{1},,w_{J},X_{1},,X_{J}} _{j=1}^{J}(w_{j}P(X_{j})-w_{j}^{2}Q_{f}(X_{j}))\] (7) s.t. \[_{j=1}^{J}w_{j}Q_{f}(X_{j})=0\,,\]

where \(P(X_{j})\) and \(Q_{f}(X_{j})\) denote the probability of the event \( X_{j}\) under the respective distribution and the constraint ensures that \( f\) has zero expectation under \(q_{f}\). With respect to the leaf weights this is a quadratic program whose optimal solution and objective values are respectively given by

\[w_{j}^{*}=)}{Q_{f}(X_{j})}-1\,, L_{f}^{*}( X_{1},,X_{J})=(_{j=1}^{J}(X_{j})}{Q_{f}( X_{j})}-1)\,.\] (8)

Because carrying out the maximization of this optimal value over the tree structure that determines the \(X_{j}\) is hard, we approximate its solution by greedily growing a tree that maximizes it when considering how to split each node individually. A parent leaf with support \(X_{P}\) is thus split into 2 child leaves, with disjoint support, \(X_{L} X_{R}=X_{P}\), so as to maximize over all possible partitionings along a single dimension, \((X_{P})\), the following objective:

\[_{X_{L},X_{R}(X_{P})}(X_{L})}{Q_{f}(X_{L})}+ {P^{2}(X_{R})}{Q_{f}(X_{R})}-(X_{P})}{Q_{f}(X_{P})}\,.\] (9)

Note that when using parametric weak learners, computing a second order step would typically involve solving a linear system with a full Hessian. As we can see, this is not the case when the weak learners are decision trees where the optimal value to assign to a leaf \(j\) does not depend on any information from other leaves and, likewise, the optimal objective value is a sum of terms, each depending only on information from a single leaf. This would have not been the case had we tried to optimize the likelihood functional in Equation 2 directly instead of its quadratic approximation.

### Sampling

To compute the leaf values in equation 8 and the splitting criterion in equation 9 we would have to know \(P(X)\) and be able to compute \(Q_{f}(X)\) which is infeasible due to the untractable normalization constant. We therefore estimate these quantities, with recourse to empirical data for \(P(X)\), and to samples approximately drawn from the model with MCMC. Because even if the input space is not partially discrete, \(f\) is still discontinuous and constant almost everywhere we can't use gradient based samplers and therefore rely on Gibbs sampling instead. This only requires evaluating each \(f_{t}\) along one dimension at a time, while keeping all others fixed which can be computed efficiently for a tree by traversing it only once. However, since at boosting iteration \(t\) our energy function is a sum of \(t\) trees, this computation scales linearly with the iteration number. This makes the overall time spent sampling quadratic in the number of iterations and thus precludes us from training models with a large number of trees.

In order to reduce the burden associated with this sampling, which can dominate the runtime of training the model, we propose a new sampling approach that leverages the cumulative nature of boosting. The intuition behind this approach is that the set of samples used in the previous boosting round are (approximately) drawn from a distribution that is already close to the new model distribution. It could therefore be helpful to keep some of those samples, especially those that conform the best to the new model. Rejection sampling allows us to do just that. The boosting update in terms of the densities takes the following multiplicative form:

\[q_{t}()=k_{t}\,q_{t-1}()(_{t} f_{t}( ))\,.\] (10)Here, \(k\) is an unknown multiplicative constant and since \( f_{t}\) is given by a tree, we can easily bound the exponential factor by finding the leaf with the largest value. We can therefore use the previous model, \(q_{t-1}()\), as a proposal distribution for which we already have a set of samples and keep each sample, \(\), with an acceptance probability of:

\[p_{accept}()=[_{t}( f_{t}()- _{} f_{t}())]\,.\] (11)

We note that knowledge of the constant \(k_{t}\) is not necessary to compute this acceptance probability. After removing samples from the pool, we can use Gibbs sampling to draw a new set of samples in order to keep a fixed total number of samples per round of boosting. Note also that \(q_{0}\) is typically a simple model for which we can both directly evaluate the desired quantities (i.e., \(Q_{0}(X)\) for a given partition \(X\)) and cheaply draw exact samples from. As such, no sampling is required for the first iteration of boosting and for the second we can draw exact samples from \(q_{1}\) with rejection sampling using \(q_{0}\) as a proposal distribution.

This approach works better when either the range of \(f_{t}\) is small or when the step sizes \(_{t}\) are small as this leads to larger acceptance probabilities. Note that in practice it can be helpful to independently refresh a fixed fraction samples, \(p_{refresh}\), at each round of boosting in order to encourage more diverse samples between rounds. This can be accomplished by keeping each sample with a probability \(p_{accept}()(1-p_{refresh})\) instead.

### Regularization

The simplest way to regularize a boosting model is to stop training when overfitting is detected by monitoring a suitable performance metric on a validation set. For NRGBoost this could be the increase in log-likelihood at each boosting round. However, estimating this quantity would require drawing additional validation samples from the model (see Appendix A). An alternative viable validation strategy which needs no additional samples is to simply monitor a discriminative performance metric (over one or more variables). This essentially amounts to monitoring the quality of \(q_{f}(x_{i}|_{-i})\) instead of the full \(q_{f}()\).

Besides early stopping, the decision trees themselves can be regularized by limiting the depth or total number of leaves of each tree. Additionally we can rely on other strategies such as disregarding splits that would result in a leaf with too little training data, \(P(X)\), model data, \(Q_{f}(X)\), volume \(V(X)\) or too high of a ratio between training and model data \(}{{Q_{f}(X)}}\). We found the latter to be the most effective of these, not only yielding better generalization performance than other approaches, but also having the added benefit of allowing us to lower bound the acceptance probability of our rejection sampling scheme.

## 4 Density Estimation Trees and Density Estimation Forests

Density Estimation Trees (DET) were proposed by Ram and Gray (2011) as an alternative to histograms and kernel density estimation but have received little attention as generative models for sampling or other applications. They model the density function as a constant value over the support of each leaf in a binary tree, \(q=_{j=1}^{J}(X_{j})}{V(X_{j})}_{X_{j}}\), with \((X)\) being an empirical estimate of probability of the event \( X\) and \(V(X)\) denoting the volume of \(X\). Note that it is possible to draw an exact sample from this type of model by randomly selecting a leaf, \(j[1..J]\), given probabilities \((X_{j})\), and then drawing a sample from a uniform distribution over \(X_{j}\).

To fit a DET, Ram and Gray (2011) propose optimizing the Integrated Squared Error (ISE) between the data and model distributions which, following a similar approach to Section 3.1, leads the following optimization problem when considering how to split a leaf node:

\[_{X_{L},X_{R}(X_{P})}D(P(X_{L}),V(X_{L}))+D(P(X_{R}),V(X_{R}) )-D(P(X_{P}),V(X_{P}))\,.\] (12)

For the ISE, \(D\) should be taken as the function \(D_{ISE}(P,V)=}}{{V}}\) which leads to a similar splitting criterion to Equation 12 but replacing the previous model's distribution with the volume measure \(V\) which can be interpreted as the uniform distribution on \(\) (up to a multiplicative constant).

Maximum LikelihoodOften generative models are trained to maximize the likelihood of the observed data. This was left for future work in Ram and Gray (2011) but, as we show in Appendix B, can be accomplished by replacing the \(D\) in Equation 12 with \(D_{KL}(P,V)=P\,(}{{V}})\).This choice of minimization criterion can be seen as analogous to the choice between Gini impurity and Shannon entropy in the computation of the information gain in decision trees.

Bagging and Feature SubsamplingFollowing the common approach in decision trees, Ram and Gray (2011) suggest the use of pruning for regularization of DET models. Practice has however evolved to prefer bagging as a form of regularization rather than relying on single decision trees. We employ same principle to DETs by fitting many trees on bootstrap samples of the data. We also adopt the common practice from Random Forests of randomly sampling a subset of features to consider when splitting any leaf node in order to encourage independence between the different trees in the ensemble. The ensemble model, which we call _Density Estimation Forests_ (DEF) in the sequence, is thus an additive mixture of DETs with uniform weights, therefore still allowing for normalized density computation and exact sampling.

## 5 Related Work

Generative BoostingMost prior work on generative boosting focuses on unstructured data and the use of parametric weak learners and is split between two approaches: (i) Additive methods that model the density function as an additive mixture of weak learners such as Rosset and Segal (2002), Tolstikhin et al. (2017). (ii) Those that take a multiplicative approach modeling the density function as an unnormalized product of weak learners. The latter is equivalent to the energy based approach that writes the energy function (log density) as an additive sum of weak learners. Welling et al. (2002) in particular also approach boosting from the point of view of functional optimization of the likelihood or the logistic loss of an energy-based model. However, they rely on a first order local approximation of the objective since they focus on parametric weak learners such as restricted boltzman machines for which a second order step would be impractical.

Greedy Multiplicative BoostingAnother more direct multiplicative boosting framework was first proposed by Tu (2007). At each boosting round a discriminative classifier is trained to distinguish between empirical data and data generated by the current model by estimating the likelihood ratio \()}}{{q_{t}()}}\). This estimated ratio is used as a direct multiplicative factor to update the current model \(q_{t}\) (after being raised to an appropriate step size). In ideal conditions this greedy procedure would converge in a single iteration if a step size of 1 would be used. While Tu (2007) does not prescribe a particular choice of classifier to use, Grover and Ermon (2017) proposes a similar concept where the ratio is estimated based on an adversarial bound for an \(f\)-divergence and Cranko and Nock (2019) provides additional analysis on this method. In Appendix C we dive deeper into the differences between NRGBoost and this approach when it is adapted to use trees as weak learners. We note, however, that the main difference is that NRGBoost attempts to update the current density proportionally to an exponential of the ratio, \(({_{t}}{{q_{t}(x)}}})\), instead of the ratio directly.

Tree-Based Density ModellingOther authors have proposed tree-based density models similar to DET (Nock and Guillame-Bert, 2022) or additive mixtures of tree-based models (Correia et al., 2020; Wen and Hang, 2022; Watson et al., 2023) but perhaps surprisingly, the natural idea of creating an ensemble of DET models through bagging has not been explored before as far as we are aware. Two distinguishing features of some of these alternative approaches are: (i) Unlike DETs, the partitioning of each tree is not driven directly by a density estimation goal. Correia et al. (2020) leverages a standard discriminative Random Forest, therefore giving special treatment to a particular input variable whose conditional estimation drives the choice of partitions and Wen and Hang (2022) proposes using a mid-point random tree partitioning. (ii) Besides modelling the density function as uniform at the leaf of each tree, other authors propose leveraging more complex models (Correia et al., 2020; Watson et al., 2023) which can allow for the use of trees that are more representative with a smaller number of leaves. (iii) Nock and Guillame-Bert (2022) and Watson et al. (2023) both propose generative adversarial frameworks where the generator and discriminator are both a tree or an ensemble of trees respectively. Note that, unlike with boosting, in these approaches the new model doesn't add to the previous one but replaces it instead.

Other Recent Tree-Based approachesNock and Guillame-Bert (2023) proposes a different ensemble approach where each tree does not have their own leaf values that get added or multiplied to produce the final density, but instead serve to collectively define the partitioning of the input space. To train such models the authors propose a boosting framework where, rather than adding a new tree to the ensemble at every iteration, the model is initialized with a fixed number of tree root nodes and each iteration adds a split to an existing leaf node. Finally Jolicoeur-Martineau et al. (2023) propose a diffusion model where a tree-based model (e.g., GBDT) is used to regress the score function. Being a diffusion model, however, means that computing densities is untractable.

## 6 Experiments

For our experiments we use 5 tabular datasets from the UCI Machine Learning Repository (Dheeru and Karra Taniskidou, 2017): Abalone (AB), Physicochemical Properties of Protein Tertiary Structure (PR), Adult (AD), MiniBooNE (MBNE) and Covertype (CT) as well as the California Housing (CH) available through the Scikit-Learn package (Pedregosa et al., 2011). We also include a downsampled version of MNIST (by 2x along each dimension) which allows us to visually assess the quality of individual samples, something that is generally not possible with structured tabular data, and provides an example of the performance that can be achieved in an unstructured dataset with many features that are correlated among themselves. More details about these datasets are given in Appendix E.

We split our experiments into two sections, the first to evaluate the quality of density models directly on a single variable inference task and the second to investigate the performance of our proposed models when used for sampling.

### Single Variable Inference

In this section we test the ability of a generative model, trained to learn the density over all input variables, \(q()\), to infer the value of a single one. I.e., we wish to test how good is its estimate of \(q(x_{i}|_{-i})\). For this purpose we pick \(x_{i}=y\) as the original target of the dataset, noting that the models that we train do not treat this variable in any special way, except for the selection of the best model in validation. As such, we would expect that the model's performance in inference over this particular variable is indicative of its strength on any other single variable inference task and also indicative of the quality of the full \(q()\) from which the conditional probability estimate is derived.

We use XGBoost (Chen and Guestrin, 2016) as a baseline for what should be achievable by a very strong discriminative model. Note that this model is trained to maximize the discriminative likelihood, \(_{ p} q(x_{i}|_{-i})\), directly, not wasting model capacity in learning other aspects of the full data distribution. As another generative baseline we use our own implementation of RFDE (Wen and Hang, 2022) which allows us to gauge the impact of the guided partitioning used in the DEF models over a random partitioning of the input space.

We use random search to tune the hyperparameters of the XGBoost model and a grid search to tune the most important hyperparameters of the generative density models. We employ 5-fold cross-validation, repeating the hyperparameter tuning on each fold for all datasets except for the largest one (CT) for which we report results on a single fold. For the full details of the experimental protocol please refer to Appendix F.

    & \)} &  &  \\   & AB & CH & PR & AD & MBNE & MNIST & CT \\  XGBoost & 0.552 & 0.849 & 0.678 & 0.927 & 0.987 & 0.976 & 0.972 \\  RFDE & 0.071 & 0.340 & 0.059 & 0.862 & 0.668 & 0.302 & 0.681 \\ DEF (ISE) & 0.467 & 0.737 & 0.566 & 0.854 & 0.653 & 0.206 & 0.790 \\ DEF (KL) & 0.482 & 0.801 & 0.639 & 0.892 & 0.939 & 0.487 & 0.852 \\  NRGBoost & **0.547** & **0.850** & **0.676** & **0.920** & **0.974** & **0.966** & **0.949** \\   

Table 1: Single variable inference results. The reported values are the averages over 5 cross-validation folds. The corresponding sample standard deviations are reported in Appendix G.

We find that NRGBoost performs better than the additive ensemble models (see Table 1) despite producing more compact ensembles. It often achieves comparable performance to XGBoost on the smaller datasets and with a small gap on the three larger ones. We note also that for the regression datasets the generative models provide an estimate of the full conditional distribution over the target variable rather than a point estimate like XGBoost. While there are other variants of discriminative boosting that also provide an estimate of the aleatoric uncertainty (Duan et al., 2020), they rely on a parametric assumption about \(p(y|)\) that needs to hold for any \(\).

### Sampling

In this section, we compare the sampling performance of our proposed methods to neural-network-based methods TVAE (Xu et al., 2019) and TabDDPM (Kotelnikov et al., 2022) on two metrics.

Machine Learning EfficiencyThe Machine Learning (ML) efficiency has been a popular way to measure the quality of generative models for sampling (Xu et al., 2019; Kotelnikov et al., 2022; Borisov et al., 2022). It relies on using samples from the model to train a discriminative model which is then evaluated on the real data. Note that this is similar to the single variable inference performance from Section 6.1. In fact, if the density model's support covers that of the full data, one would expect the discriminative model to recover the generator's \(q(y|)\), and therefore its performance, in the limit where infinite generated data is used to train it.

We use an XGBoost model (with the hyperparameters tuned in real data) as the discriminative model and train it using a similar number of training and validation samples as in the original data. For the density models, we generate samples from the best model found in the previous section and for non-density models we select their hyperparameters by evaluating the ML Efficiency in the real validation set. Note that this leaves the sampling models at a potential advantage since the hyperparameter selection is based on the metric that is being evaluated rather than the direct inference performance of the previous section.

Discriminator MeasureSimilar to Borisov et al. (2022) we test the capacity of a discriminative model to distinguish between real and generated data. We use the original validation set as the real part of the training data in order to avoid benefiting generative methods that overfit their original training set. A new validation set is carved out of the original test set (20%) and used to tune the hyperparameters of an XGBoost model which we use as our choice of discriminator, evaluating its AUC on the remainder of the real test data.

We repeat all experiments 5 times, with 5 different generated datatsets from each model. Results are reported in Tables 2 and 3 showing that (i) NRGBoost outperforms all other methods by substantial margins in the discriminator measure except for the PR and the MBNE datasets. (ii) On the ML Efficiency metric, TabDDPM outperforms NRGBoost by small margins on the small datasets which could in part be explained by the denser hyperparameter tuning favouring models that perform particularly well at inferring the target variable at the expense of the others. Nevertheless, NRGBoost still significantly outperforms all other models on MNIST and CT. Its samples also look visually similar to the real data in both the MNIST and California datasets (see Figures 1 and 2).

    & \)} &  &  \\   & AB & CH & PR & AD & MBNE & MNIST & CT \\  XGBoost & 0.554 & 0.838 & 0.682 & 0.927 & 0.987 & 0.976 & 0.972 \\  TVAE & 0.483 & 0.758 & 0.365 & 0.898 & 0.975 & 0.688 & 0.724 \\ TabDDPM & **0.539** & **0.807** & **0.596** & 0.910 & **0.984** & 0.579 & 0.818 \\  DEF (KL) & 0.450 & 0.762 & 0.498 & 0.892 & 0.943 & 0.230 & 0.753 \\ NRGBoost & 0.528 & 0.801 & 0.573 & **0.914** & 0.977 & **0.959** & **0.895** \\   

Table 2: ML Efficiency results. The reported values are the averages over 5 different datasets generated by the same model. The best methods for each dataset are in **bold** and methods whose difference is \(<2\) away from zero are underlined. The performance of XGBoost trained on the real data is also reported for reference.

## 7 Discussion

While the additive tree models like DEF require no sampling to train and are easy to sample from, we find that in practice they require very deep trees to model the data well which, in turn, also requires using a large number of trees in the ensemble to regularize. In our experiments we found that their performance was often capped by the maximum number of leaves we allowed them to grow to (\(2^{14}\)).

In contrast, we find that NRGBoost is able to model the data better while using shallower trees and in fewer number. Its main downside is that it can only be sampled from approximately using more expensive MCMC and also requires sampling during the training process. While our fast Gibbs sampling implementation coupled with our proposed sampling approach were able to mitigate the slow training, making these models much more usable in practice they are still cumbersome to use for sampling due to autocorrelation between samples from the same Markov Chain. We argue however that unlike in image or text generation where fast sampling is necessary for an interactive user experience, this can be less of a concern for the task of generating synthetic datasets where the one time cost of sampling is not as important as faithfully capturing the data generating distribution.

We also find that tuning the hyperparameters of tree-based models is easier and less crucial than DL models for which many trials fail to produce a reasonable model. In particular we found NRGBoost to be rather robust, with different hyperparameters leading to small differences in performance.

Finally, we note that like any other machine learning models, generative models are susceptible to overfitting and are thus liable to leak information about their training data when generating synthetic samples. In this respect, we believe that NRGBoost offers better tools to monitor and control overfitting than other alternatives (see Section 3.3) but, still, due consideration for this risk must be taken into account when sharing synthetic data.

## 8 Conclusion

In this work, we extend the two most popular tree-based discriminative methods for use in generative modeling. We find that our boosting approach, in particular, offers generally good discriminative performance and better overall sampling performance than alternatives. We hope that these results encourage further research into generative boosting approaches for tabular data, in particular exploring other applications besides sampling that are enabled by density models.

    & AB & CH & PR & AD & MBNE & MNIST & CT \\  TVAE & 0.971 & 0.834 & 0.940 & 0.898 & 1.000 & 1.000 & 0.999 \\ TabDDPM & 0.818 & 0.667 & **0.628** & 0.604 & **0.789** & 1.000 & 0.915 \\  DEF (KL) & 0.823 & 0.751 & 0.877 & 0.956 & 1.000 & 1.000 & 0.999 \\ NRGBoost & **0.625** & **0.574** & 0.631 & **0.559** & 0.993 & **0.943** & **0.724** \\  

Table 3: Discriminator measure results. All results are the AUC of an XGBoost model trained to distinguish real from generated data an therefore lower means better. The reported values are the averages over 5 different datasets generated by the same model.

Figure 2: Joint histogram for the latitude and longitude for the California Housing dataset.