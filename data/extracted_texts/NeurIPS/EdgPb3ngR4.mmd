# Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning

In Huh\({}^{1,*}\), Changwook Jeong\({}^{2,}\), Jae Myung Choe\({}^{1}\), Young-Gu Kim\({}^{1}\), Dae Sin Kim\({}^{1}\)

\({}^{1}\)CSE Team, Innovation Center, Samsung Electronics

\({}^{2}\)Graduate School of Semiconductor Materials and Devices Engineering, UNIST

\({}^{*}\)in.huh@samsung.com \({}^{}\)changwook.jeong@unist.ac.kr

###### Abstract

We study structure-preserving low-dimensional representation of a data manifold embedded in a high-dimensional observation space based on variational auto-encoders (VAEs). We approach this by decomposing the data manifold \(\) as \(=/G G\), where \(G\) and \(/G\) are a group of symmetry transformations and a quotient space of \(\) up to \(G\), respectively. From this perspective, we define the structure-preserving representation of such a manifold as a latent space \(\) which is isometrically isomorphic (i.e., distance-preserving) to the quotient space \(/G\) rather \(\) (i.e., symmetry-preserving). To this end, we propose a novel auto-encoding framework, named _isometric quotient VAEs (IQVAEs)_, that can extract the quotient space from observations and learn the Riemannian isometry of the extracted quotient in an unsupervised manner. Empirical proof-of-concept experiments reveal that the proposed method can find a meaningful representation of the learned data and outperform other competitors for downstream tasks.

## 1 Introduction

There has been a common consensus that natural image datasets form a low-dimensional manifold \(\) embedded in a high-dimensional observation space \(^{d}\), i.e., \(() d\). From this perspective, a good neural network is a mapping function that can recognize the underlying structure of the data manifold well . A question that arises is what structures should be represented from the data.

For unsupervised learning task, especially in generative modeling, if we suppose the data manifold to be a Riemannian one, then preserving geometric structures in the representation space is a key consideration . To make things more explicit, if \(\) is a latent representation of \(\), then \(\) should be isometrically isomorphic to \(\) in the sense of the Riemannian isometry. This means that an infinitesimal distance on the data manifold should be preserved in the latent space as well, promoting geodesic distances in the latent space that accurately reflect those in the data space. Recently, several papers  have suggested using the concept of Riemannian isometry for deep generative models, such as variational auto-encoders (VAEs)  and generative adversarial networks (GANs) , to obtain more meaningful and relevant representations. These concepts have been applied to improve latent space interpolations  or clustering  of data manifolds.

However, the Riemannian isometry does not tell the whole story of the data structure, particularly in the case of vision datasets. One common property of natural images is that there are symmetry transformations that preserve the semantic meaning of the given image. For example, if one applies a rotational transformation to a certain image, then its semantic meaning, e.g., its label, does not change. In this situation, what one really wants to do is represent the inherent geometry of the manifold, i.e., the geometry up to such symmetry transformations .

The concept of the inherent geometry can be formalized by using the notion of the principal bundle \(=/G G\), a fiber bundle that consists of the group \(G\) of symmetry transformations and a quotient space \(/G\) as the fiber and base spaces, respectively [16; 26]. In this case, the inherent geometry of \(\) up to \(G\) is indeed determined by its quotient \(/G\) solely; the quotient formulates an invariant structure of the data, thus a measure on \(/G\) gives a measure on \(\) that is invariant to the actions of \(G\). Therefore, one should find the Riemannian isometry of \(/G\) rather than \(\) for a geometric-meaningful representation of the dataset (see Figure 1). Nevertheless, to the best of our knowledge, there has been a lack of studies, at least in the field of generative models, that integrate both the concepts of the quotient space and its Riemannian geometry.

In this paper, we propose a novel auto-encoding framework, named _isometric quotient VAEs (IQVAEs)_, that can extract the quotient space from observations and learn its Riemannian isometry in an unsupervised manner, allowing for a correct representation of the data manifold and accurate measurement of distances between samples from the learned representation. The proposed IQVAE model can be applied to a variety of practical tasks such as downstream clustering and out-of-distribution (OoD) detection. We evaluate the model's performance using three datasets: rotated MNIST , mixed-type wafer defect maps (MixedWM38)  and cervical cancer cell images (SIPaKMeD) . In summary, our contribution is threefold:

* We propose a novel approach for structure-preserving representation learning by viewing a data manifold as a principal bundle, and formulate it as an unsupervised learning task that finds a Riemannian isometry for the quotient of the data manifold.
* We introduce a practical method, called the IQVAE, which can learn such a structure-preserving representation in an unsupervised manner by using the auto-encoding framework.
* We demonstrate through experimental evaluations on various datasets that our proposed method can find useful representations and outperform other competing methods for downstream tasks such as classification, clustering, and OoD detection.

## 2 Related Works

### _G_-Invariance and Quotient Space Learning

Quotient space learning is highly related to the notion of the \(G\)-invariance. Data augmentation is the most common approach to deal with the \(G\)-invariance of label distributions . To achieve such a symmetry more explicitly, various \(G\)-invariant and \(G\)-equivariant architectures have been proposed;  has proposed \(G\)-convolution beyond the conventional translational equivariance of convolutional neural networks.  has generalized it further for Lie groups. \(G\)-orbit pooling, proposed by , can be applied to both arbitrary discrete groups and continuous groups by discretizing them. Some research has attempted to discover the underlying invariances within data [4; 46; 31; 35]. These approaches can be integrated as a major component of auto-encoders to represent the quotient symmetry of the data manifold, as we will discuss later in Section 3.3.

More relevant to our work,  has proposed using the consistency regularization for VAEs to encourage consistency in the latent variables of two identical images with different poses. It can be considered as the unsupervised learning version of the data augmentation and does not guarantee the explicit \(G\)-invariance of the regularized latent space.  has disentangled image content from pose variables, e.g., rotations or translations under the VAE framework. However, it has some limitations,such as the requirement of using a specific decoder, the spatial transformer network . It also has a problem that it cannot handle arbitrary groups beyond the rotation and translation.  has dealt with more complicated SO(3) group under the VAE framework.  has proposed the quotient auto-encoder (QAE) that can learn the quotient space of observations with respect to a given arbitrary group, extending early works in quotient images [34; 41]. However, it is a deterministic model and therefore cannot generate new samples easily. In addition, all the above-mentioned studies have not considered the Riemannian geometry of the learned latent representation.

### Rimeannian Geometry of Generative Models

Recent studies have proposed to view the latent space of generative models as a Riemannian manifold [1; 9; 7]. In this perspective, the metric tensor of the latent space is given by a pullback metric induced from the observation space through the generator function. In line with this theoretical perspective, the flat manifold VAE (FMVAE)  and its variant [25; 15] have been proposed to regularize the generator function to be a Riemannian isometry by matching the pullback metric with the Euclidean one. The regularized latent space preserves geodesic distances of the observation space, making it a more geometrically meaningful representation that can be useful. However, the FMVAE and its variants do not take into account the underlying group symmetry, which can lead to inaccurate estimation of distances between data points. For example, they could estimate a non-zero distance between two identical images with different rotational angles, which is unsuitable for clustering tasks.

## 3 Method

Suppose \(^{d}\) is an observation where \(\) is an extrinsic view of \(\), i.e., the data manifold is realized via a mapping \(:^{d}\) such that \(=()\), and \(^{n}\) is its latent representation where \(n d\). In addition, suppose \(G\) to be a group (of symmetry transformations) that its group elements \(g G\) naturally act on \(\) by a left group action \(:G\). We will denote \((g,)\) as \(g*\). Typical examples of \(G\) include the special orthogonal group SO(2), a group of 2-dimensional rotations that can also naturally act on \(d\)-dimensional images by rotating them.

### Auto-Encoders

The auto-encoder framework consists of two parameterized neural networks, an encoder \(_{}:^{d}^{n}\) and a decoder \(_{}:^{n}^{d}\). Vanilla auto-encoders find a low-dimensional compression of observations by minimizing the following reconstruction loss:

\[_{ p_{}()}_{ }(,;)=_{ p_{ }()}\|-_{}_{}( )\|_{2}^{2},\] (1)

where \(p_{}()\) is the data distribution. The expectation over \(p_{}()\) can be computed via Monte Carlo (MC) estimation with finite samples \(=\{_{i}\}_{i=1}^{N}\) as \(_{p_{}()}_{}(,;)\ (1/N)_{i=1}^{N}\|_{i}-_{}_{ }(_{i})\|_{2}^{2}\). Unless otherwise mentioned, we will omit the expectation over \(p_{}()\) for brevity.

Although sufficiently deep auto-encoders can provide effective latent compression \(_{}()=^{n}\) of high-dimensional data, they tend to overfit and the learned latent manifolds are often inaccurate , i.e., they cannot consider the underlying geometry of data. This can lead to problems such as incorrect latent interpolation or poor performance in downstream tasks, in addition to the absence of the ability to generate new samples.

### Variational Auto-Encoders

The VAE  is a stochastic auto-encoding architecture belonging to the families of deep generative models. Contrary to vanilla auto-encoders, the VAE framework consists of two parameterized distributions \(q_{}(|)\) and \(p_{}(|)\) where the former and the latter are variational posterior and likelihood, respectively. VAEs try to maximize the marginal log-likelihood of observations by optimizing the following evidence lower bound:

\[ p_{}()_{ q_{}(|)} p_{}(|)-D_{ }q_{}(|)\|p_{}() -_{}(,;),\] (2)

where \(p_{}()\) is the latent prior and \(D_{}(,)\) is the Kullback-Leibler (KL) divergence. \(p_{}()\) is often given by a normal distribution \((|,_{n})\) where \(\) is a zero vector and \(_{n}\) is \(n n\) identity matrix.

For vanilla VAEs, \(q_{}(|)\) is chosen as a multivariate Gaussian distribution with a diagonal covariance matrix \((|_{}(),[_{}^{2 }()])\) and represented by a neural network encoder \((_{},_{}):^{d}^{n} ^{n}_{+}\). A latent variable is sampled by using the reparameterization trick \(=_{}()+_{}()\) where \((|,_{n})\). Although \(p_{}(|)\) is chosen depending on the modeling of the data, it is often taken as a simple distribution such as a Gaussian with fixed variance, \((|_{}(),_{d})\), represented by a neural network decoder \(_{}:^{n}^{d}\). In this case, the VAE objective (2) can be seen as a regularized version of (1) given by a sum of the stochastic auto-encoding reconstruction term \(_{}(,;,)=\|-_{ }(_{}()+_{}())\|_{2 }^{2}\) and \(\)-weighted KL divergence \(_{}(;,)= D_{}( (|_{}(),[_{ }^{2}()])\|(|,_{n}))\) as follows:

\[_{}(,;,)=_{ ()}_{}(,;,)+_{}(;,).\] (3)

Practically, the expectation over \(()\) term is estimated via a single-point MC when the mini-batch size is sufficiently large . When \( 0\) and \(_{}^{2} 0\), VAEs reduce to vanilla auto-encoders.

By minimizing (3), VAEs can learn the probabilistic process on the smooth latent space  that can easily generate plausible new samples by sampling a latent variable and decoding it, i.e., \(_{}()\). However, VAEs, like vanilla auto-encoders, do not ensure that the learned latent representation preserves the crucial information about the symmetry and geometry of the data manifold.

### Quotient Auto-Encoders

QAE  is a modification of the deterministic auto-encoding framework that can find the quotient of the observation set \(/G\) by replacing (1) as the following quotient reconstruction loss:

\[_{}(,;,G)=_{g G}\|g*-_{}_{}()\|_{2}^{2}.\] (4)

As shown in (4), QAEs aim to minimize the set distance between the auto-encoded sample \(_{}_{}()\) and \(G\)-orbit \(G*=\{g*|g G\}\) of the given \(\) rather than \(\) itself. It is noteworthy that \(G\)-orbit loses the information of \(G\) for a given \(\), which is the case of the natural quotient for \(\) up to \(G\). Thus, QAE can learn the quotient space of observations directly: for the ideal case of QAEs, i.e., when \(_{}(,;,G) 0\), the following property holds:

\[_{}_{}()=_{}_{}( =g*)}, g G,\]

which means that the image \(_{}_{}()}\) does not contain any information on \(g\). In addition, QAEs typically adopt \(G\)-invariant architectures such as \(G\)-orbit pooling  (see Section B of Supplementary Material (SM) for details) for the encoder part to explicitly guarantee the following \(G\)-invariant latent representation:

\[_{}^{G}()=_{}^{G}(g*)}, g G,\]

where \(_{}^{G}:^{d}^{n}\) is the \(G\)-invariant encoder. It is worth mentioning that these encoder architectures are unable to learn a meaningful representation of the data when using a vanilla auto-encoding framework, as the reconstruction loss (1) cannot be reduced enough; on the other hand, QAEs can use the explicitly \(G\)-invariant encoding architectures easily by taking advantage of the infimum in (4). This allows for both \(}\) and \(_{}^{G}()}\) to possess the invariant property up to \(G\).

### Isometric Quotient Variational Auto-Encoders

Although QAEs with \(G\)-invariant encoders can extract the underlying symmetry of observations, they still have the following drawbacks. First, QAEs are deterministic and may overfit to limited observations, similar to vanilla auto-encoders. Second, the Riemannian geometry of the extracted quotient manifold may not be preserved in the latent space. To address these issues, we present a stochastic version of QAEs called QVAEs, and further improve them with a novel framework for isometric learning called IQVAEs.

QVAEs.Inspired by the connection among (1), (3), and (4), we propose the stochastic version of QAEs (QVAEs) which minimizes the following stochastic quotient objective:

\[_{}(,;,,G)_{ ()}_{}(,; ,,G)+_{}(;, ,G),\] (5)

[MISSING_PAGE_FAIL:5]

the following regularization should be added to (5) to make the decoder be a Riemannian isometry between the latent representation \(}\) and quotient \(}\):

\[_{}(,;,_{}},)=_{} q_{}( }|)}\|_{_{}}(}) _{_{}}(})-_{}}(})\|_{F},\] (6)

where \(_{_{}}(})\) is Jacobian of \(_{}\) at \(}\), \(\) is a hyper-parameter, and \(_{}}(})\) is a metric tensor of \(}\).

There are two things that should be clarified for practical computation of (6). The first aspect to consider is the selection of the Riemannian metric \(_{}}(})\). A commonly used and particularly useful choice of \(_{}}(})\) is the scaled Euclidean metric in \(^{n}\), i.e., \(_{}}(})=_{n}=c^{2}_{n}\) where \(c\) is a constant. This is because it is the case that using the Euclidean distance1 on the latent representation \(d_{}}(}_{0},}_{1})=}_{1}-}_{0})^{}_{n}(}_{1}-}_{0})}\) can preserve the geodesic distance on the quotient manifold \(d_{}}(}_{0},}_{1})\) up to \(c\). The constant \(c\) can be regarded as either a pre-defined hyper-parameter or a learnable one. More generally, one can use any parameterized symmetric positive-definite matrix \(_{n}\) for \(_{}}(})\). Note that such matrices can be achieved from an arbitrary learnable parameterized \(n n\) matrix \(_{n}\) by using the Cholesky decomposition as follows:

\[_{n}=_{n}(_{n})^{}, _{n}=[_{n}]-[_{n}]+ [_{n}]^{2}+^{2}_{n},\] (7)

where \(_{n}\) is a real lower triangular matrix with all positive diagonal entries and \(\) is a small non-zero constant. In this case, using the Mahalanobis distance on the latent space \(d_{}}(}_{0},}_{1})=}_{1}-}_{0})^{}_{n}(}_{1}-}_{0})}\) preserves the geodesic distance of the quotient space of the observation manifold \(d_{}}(}_{0},}_{1})\).

The second aspect to consider is the sampling from \(q_{}(}|)\). It can be considered as Gaussian vicinal distribution for finite observation samples that smoothly fills the latent space where data is missing. In addition to that, following , we use the mix-up vicinal distribution  to effectively regularize the entire space of interest to be isometric. As a result, the tractable form of (6) is equal to2:

\[_{_{i,j} p_{X},_{i,j} ,}\|_{_{}}^{}(f_{}^{ }(_{i,j},_{i,j}))_{_{}}(f_{}^{ }(_{i,j},_{i,j}))-_{n}\|_{F},\] (8)

where \(f_{}^{}(_{i,j},_{i,j})=(1-) }_{i}+}_{j}\) is the latent mix-up for \(}_{i,j}=_{}^{G}(_{i,j})+_{}^{G} (_{i,j})_{i,j}\). Practically, it is computed by sampling a mini-batch of latent variables, i.e., \(\{}_{i}\}_{i=1}^{N}=\{_{}^{G}(_{i})+_{ }^{G}(_{i})_{i}\}_{i=1}^{N}\), shuffling it for \(\{}_{j}\}_{j=1}^{N}=(\{}_{j}\}_ {i=1}^{N})\), and mixing-up them.

Iqvae.We define the IQVAE as a class of QVAEs whose objective function is given by the sum of the variational quotient objective \(_{}\) (5) and Riemannian isometry \(_{}\) (8) as follows:

\[_{}(,;,,,G) _{}(,;,,G)+_{}(,,_{n};,).\] (9)

The proposed IQVAE optimization procedure is summarized in Algorithm 1 (see Section A of SM for additional tips). In the IQVAE, we typically set \(_{n}=c^{2}_{n}\). For clarity, we will refer to the IQVAE with (7) as IQVAE-M, to distinguish it from the basic version.

``` Input: data \(\{_{i}\}_{i=1}^{N}\), hyper-parameters (\(,\)), group \(G\), \(G\)-invariant encoders (\(_{}^{G},_{}^{G}\)), decoder \(_{}\) Initialize \(\), \(\), \(_{n}\) while training do  Sample \(\{_{i}\}^{N}\), \(\{_{i}(,)\}_{i=1}^{N}\) Compute \(\{_{}^{G},_{}^{G}\}_{i=1}^{N}=\{_{}^{G}(_{i })\,_{}^{G}(_{i})\}_{i=1}^{N}\) Sample \(\{_{i}\}_{i=1}^{N}=\{_{}^{G}+_{}^{G} _{i}\}_{i=1}^{N}\) Shuffle \(\{_{i}\}_{i=1}^{N}=(\{_{i}\}_{i=1}^{N})\)  Augment \(\{}_{i}\}_{i=1}^{N}=\{(1-_{i})_{i}+_{i} _{j}\}_{i=j=1}^{i=j=N}\) Compute \(_{}_{i=1}^{N}_{j G}\|g_{i }-_{}(_{i})\|_{2}^{2}\) Compute \(_{}=_{i=1}^{N}D_{}((_{}^{G},[_{}^{G}]^{2})\|(,_{n}))\) Compute \(\{_{_{}}^{}\}_{i=1}^{N}=\{_{_{}}( }_{i})\}_{i=1}^{N}\) Compute \(\{_{_{}}^{}\}_{i=1}^{N}=\{_{_{}}( }_{i})\}_{i=1}^{N}\) Compute \(_{}=_{i=1}^{N}\|(_{_{}}^{}) ^{}_{_{}}^{}-_{n}\|_{F}\) Optimize \((_{}+_{}+_{ })/N\) w.r.t \(,\) endwhile ```

**Algorithm 1** IQVAE

## 4 Experiments

We compared our proposed QVAE and IQVAE with six different competitors: the auto-encoder (AE), VAE, \(\)-VAE [19

[MISSING_PAGE_FAIL:7]

While the QVAE shows a more convincing interpolation compared to the VAE, it also shows abrupt changes at \(t=7\) and \(t=8\). The IQVAE shows the smoothest interpolation between digits 1 and 7 compared to the other models owing to the isometry regularization. The smooth interpolation of the IQVAE is also quantitatively confirmed in Figure 5 (b) which compares the geometric volume measures along each linear path of the QVAE and IQVAE. The volume measure is computed via \(_{_{}}^{}((t))_{_{ }}((t))}\) and can be viewed as a local infinitesimal volume ratio between the latent and observation spaces. The volume measure of the QVAE shows a clear peak near \(t=8\), while that of the IQVAE is nearly constant.

Evidence for isometry.To demonstrate that the learned decoding function of IQVAEs up-holds the desired Riemannian isometry, we evaluated the condition numbers of the pull-back metric tensors and the Riemannian volume measures for all test samples across models. The condition number is defined as the ratio \(_{}/_{}\), where \(_{}\) and \(_{}\) respectively represent the maximum and minimum eigenvalues of the pull-back metric, denoted as \(_{_{}}^{}()_{_{}}( )\). All computed volume measures are normalized by their average values. Therefore, condition numbers and normalized volume measures approaching 1.0, accompanied by minimal variances, indicate that the learned latent representation is isometrically isomorphic to Euclidean space. In other words, the learned pull-back metric aligns with the Euclidean metric. As shown in Figure 6, the IQVAE presents condition numbers and volume measures close to 1.0, exhibiting trivial variances compared to other models.

Sample efficiency comparison.Table 2 compares the sample efficiency of IQVAE and the most comparable baseline, QAE, in terms of classification accuracy. The results demonstrate the effectiveness of the proposed method as IQVAE shows more robust performance with respect to variations in the training sample size.

Test-time orbit augmentation.The current QVAE and IQVAE require more computational cost than vanilla VAE due to the \(G\)-orbit pooling encoders, which augment the number of observations \(|G|\) times larger by expanding a data point \(\) as a \(G\)-orbit \(*\). It might be a bottleneck, especially when training the proposed model with larger-scale datasets. To resolve this issue, we suggest the test-time orbit augmentation strategy, which involves using a coarse step size when discretizing the group parameters (e.g., rotation angle) during training, and then switching to a finer step size during the inference phase. Table 3 compares wall-clock training

   Sample size & QAE + SVM & IQVAE + SVM \\ 
5,000 & 83.9\(\)0.7 & 87.6\(\)0.6 \\
10,000 & 85.5\(\)0.8 & 89.8\(\)1.0 \\
60,000 & 91.8\(\)0.6 & 92.9\(\)0.3 \\   

Table 2: Classification test accuracies for \(2\)-rotated MNIST with varying training sample sizes.

   Method & Time [s] & \(k\)-means & GMM \\  VAE & 1.12 & 11.4\(\)1.2 & 14.7\(\)1.6 \\ QVAE (36-36) & 10.17 & 53.9\(\)7.2 & 66.3\(\)6.5 \\ QVAE (12-12) & 4.06 & 50.8\(\)2.6 & 59.2\(\)1.5 \\ QVAE (12-36) & 4.06 & 54.6\(\)2.8 & 63.8\(\)3.9 \\ IQVAE (36-36) & 13.22 & 70.9\(\)2.9 & 72.9\(\)1.5 \\ IQVAE (12-12) & 6.11 & 59.6\(\)2.7 & 60.9\(\)1.8 \\ IQVAE (12-36) & 6.11 & 64.3\(\)2.1 & 65.0\(\)2.1 \\   

Table 3: Training wall-clock time per epoch and clustering ARIs of VAEs, QVAEs, and IQVAEs. We used a single V100 32GB GPU.

Figure 5: (a) Linear interpolations between digits 1 and 7 for the VAE, QVAE, and IQVAE. (b) The geometric volume measures along each linear path of the QVAE and IQVAE.

Figure 6: (left) Condition numbers and (right) volume measures for the \(2\)-rotated MNIST for the VAE, QVAE, and IQVAE.

[MISSING_PAGE_FAIL:9]

class-conditional mean latent vector of learned in-distribution sets, in terms of the Mahalanobis distance  (see Section G of SM for details). Table 5 shows the comparison of competing models using standard metrics (AUCROC and AUPRC) for threshold-based OoD detection, demonstrating that the proposed IQVAE excels in the OoD tasks as it accurately learns the data structure.

### SIPaKMeD

The SIPaKMeD dataset includes 4,049 single-cell Pap smear images for cervical cancer diagnosis and is split into 5 classes  (see Section C of SM for examples). We assume O(2) symmetry for the SIPaKMeD dataset. We resized the original SIPaKMeD images to size 32 \(\) 32 for better usability. In accordance with the original data splitting, 3,549 samples were used for training and 500 samples for testing.

IQVAE-M.As we did with rotated MNIST, we conducted the same experiment on the SIPaKMeD dataset. Furthermore, we also assessed the proposed IQVAE-M that incorporates a learnable flexible Riemannian metric as described in (7) and utilizes the latent Mahalanobis distance when computing the radial basis function (RBF) kernel \(K:^{n}^{n}\) of SVMs as \(K(_{0},_{1})=(-(_{1}-_{0})^ {T}_{n}(_{1}-_{0}))\). To quantitatively analyze the learned representations, we present the downstream task performance of the competing models on the SIPaKMeD dataset in Table 6 (see Section E of SM for UMAP visualizations). As shown in Table 6, the proposed methods exhibit superior performance compared to other models4.

## 5 Conclusion and Limitation

We have proposed and demonstrated the effectiveness of IQVAE, a simple yet effective approach that maintains symmetry and geometry in data. However, our work has two main limitations as follows.

Predefined groups of symmetry transformations.We assume the group structure of a given dataset is known in advance. This group invariance is represented using the quotient auto-encoding framework with a Riemannian isometry. However, several recent papers have delved into learning an unknown group directly from data. For example,  presents a novel neural network that identifies bispectral invariants.  employs Lie algebra to find underlying Lie group invariances.  tackles the disentanglement challenge with VAEs, aiming to learn the latent space in terms of one-parameter subgroups of Lie groups. Integrating these methods with our IQVAEs learning approach could be a promising direction for future research.

Euclidean metric tensors.We focus on cases where the metric tensor of the observation space is Euclidean. If the intrinsic dimensionality of the image manifold is significantly lower than that of the ambient observation space, the Euclidean metric of the observation space can well serve as a Riemannian metric for the intrinsic geometry of the image manifold. This reasoning supports our use of the pullback metric as a metric tensor for the latent space. However, when constructing a latent representation that effectively addresses a specific task, a specialized metric tensor might be more appropriate than the standard Euclidean metric of the ambient space. For instance, when tasks involve capturing subtle variations in a local image patch, the pullback metric derived from the entire observation dimensions may not provide the most efficient geometry. In such cases, a specialized metric structure better suited for capturing these local variations should be considered. In this context, a task-specific metric can be determined using prior knowledge about the tasks . Alternatively, IQVAEs can semi-supervisedly learn this specialized metric with minimal labeled data, leveraging metric learning concepts . This approach holds significant potential for future research, as it allows the model to tailor its representation to the specific requirements of the task, thereby improving overall performance.

   Method & \(k\)-means & GMM & SVM & RF \\  AE & 33.4\(\)0.4 & 35.6\(\)1.0 & 78.6\(\)0.5 & 76.9\(\)1.1 \\ VAE & 33.9\(\)1.5 & 34.0\(\)1.7 & 79.4\(\)1.0 & 76.5\(\)2.1 \\ \(\)-VAE & 34.4\(\)1.5 & 33.7\(\)1.1 & 79.1\(\)1.0 & 76.6\(\)1.3 \\ CRVAE & 24.9\(\)3.6 & 31.9\(\)1.3 & 77.1\(\)0.7 & 80.6\(\)1.1 \\ QAE & 40.9\(\)1.9 & 41.1\(\)1.9 & 80.5\(\)9.9 & 81.6\(\)0.7 \\ FMVAE & 39.6\(\)0.3 & 37.9\(\)1.1 & 80.0\(\)0.8 & 75.7\(\)1.2 \\ QVAE & 40.6\(\)3.2 & 40.6\(\)2.5 & 82.2\(\)0.8 & **81.7\(\)0.9** \\ IQVAE & 39.3\(\)0.4 & 44.7\(\)2.0 & 82.7\(\)0.3 & 80.9\(\)0.7 \\ IQVAE-M & **43.0\(\)1.1** & **46.5\(\)2.9** & **84.3\(\)0.7** & **81.6\(\)1.8** \\   

Table 6: Clustering ARIs and classification test accuracies for SIPaKMeD (repeated five times).