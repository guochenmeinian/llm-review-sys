# pfl-research: simulation framework for accelerating research in Private Federated Learning

Filip Granqvist

Apple

fgranqvist@apple.com

Corresponding author.

Congzheng Song

Apple

Aine Cahill

Apple

Aine Cahill

Apple

Rogier van Dalen

Work done while at Apple

Martin Pelikan

Apple

Yi Sheng Chan

Apple

Xiaojun Feng

Apple

Natarajan Krishnaswami

Apple

Vojta Jina

Work done while at Apple

###### Abstract

Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on large and realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72\(\) faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.

## 1 Introduction

Federated learning (FL) is an approach to collaboratively train a machine learning (ML) model between clients, with coordination by a central server . Participating clients can be either user devices connected to the internet (cross-device FL) or data centers of distinct institutions, which each store data of multiple users (cross-silo FL). The common constraint is that it is infeasible to upload the data to a central server for training because of privacy, bandwidth, or other compliance concerns . However, even though only statistics derived from the original data are uploaded to a central server, FL algorithms alone are not formally or practically privacy preserving . Therefore, it is essential to combine FL with techniques to preserve the privacy of users. Deployed systems often use secure aggregation and give differential privacy (DP) guarantees to achieve this . We call such systems _private federated learning_ (PFL).

(P)FL is a rapidly growing field , and it is unrealistic for most research to be evaluated with a real-world (P)FL deployment as most researchers do not have access to a large-scale deployment.

Even for researchers with such access, user experience constraints  would typically significantly limit such evaluation. Additionally, training with real edge devices is slow and typically cannot support the extensive hyperparameter tuning that may be needed .

Therefore, testing hypotheses in simulation is essential for continued growth in (P)FL research. FL simulation is typically much more resource intensive than training a regular (non-federated) ML model due to the extra steps involved (see Algorithm 1), thus it is of particular importance that FL simulators be efficient and scale well.

The community has identified a comprehensive list of open problems in FL , and effective evaluation of proposed solutions requires high quality large-scale datasets representative of these challenges. Several realistic benchmarks have been proposed to facilitate reliable evaluation of FL algorithms (see Section 2). Unfortunately, the largest and most realistic open-source benchmark datasets for FL have not been widely adopted because of the resources required to use them. As we discuss in Section 4.1, the resource demand is exacerbated by slow simulators.

We introduce pfl-research, a modular and easy-to-use framework for researchers to simulate FL and PFL training that is 7-72\(\) faster than previous FL simulators. Our framework has well defined APIs that allow an FL researcher to implement their algorithms and bundle them into components that can be shared and combined with other algorithms. pfl-research supports both PyTorch  and TensorFlow , and it is straightforward to integrate with other ML frameworks. Additionally, scaling up with distributed simulations is seamless with Horovod , including multi-host, multi-GPU scenarios. Cluster workload management and scheduling systems such as slurm  can be used to enable running multiple simulation runs with different configurations in parallel, e.g. for hyperparameter tuning.

Our main contributions are:

**Speed** pfl-research is much faster than other popular FL simulators (Section 4.1) since it simulates only the computation, not the topology, of federated learning. This can significantly increase both the productivity of (P)FL researchers and the quality of publications because it makes it feasible for researchers with limited resources to evaluate algorithms on larger datasets.

**User-friendly distributed simulations** pfl-research makes it easy to transition from single process to distributed simulations with zero code changes. Scale-up spans multiple dimensions: number of processes, GPUs, and machines. Multiple processes can share a GPU to increase GPU utilization. Debugging, testing, and profiling are simpler compared to alternative FL frameworks.

**Privacy integration** pfl-research is tightly integrated with state-of-the-art privacy accountants and mechanisms, enabling a convenient workflow for experimenting with PFL and combining it with orthogonal FL features and algorithms.

**Non-gradient-descent training** pfl-research is also a suitable framework for researching federated learning with models that require training algorithms beyond gradient descent, such as some classical ML algorithms. We provide implementations of federated gradient boosted decision trees (GBDTs) and federated Gaussian mixture models (GMMs).

**Diverse and unified benchmarks** To get a complete picture of where an algorithm performs well, we provide setups for benchmarking algorithms in a variety of scenarios: datasets of diverse domains, IID / non-IID, no DP / Central DP. We aim to provide the same benchmarks for both PyTorch and TensorFlow so that results are comparable across frameworks.

pfl-research began as an inner-source project and has been used both in research and for modelling practical use cases .

## 2 Related Work

There are multiple open-source efforts for realizing FL. Some focus on providing infrastructure for a production environment with real edge devices , some focus on simulations for research , and some provide both .

There are pros and cons to each framework for simulation, but there is one validly comparable metric that is key to research velocity: speed. If an FL framework runs too slowly, researchers with constrained resources will not be able to test their hypotheses on realistic benchmarks, which can outweigh other framework benefits. For example,  and  used FLAIR in their experiments with the original TensorFlow Federated code3 and they both mentioned that the computational resource requirements were substantial enough for them to have to reduce the scope of their experiments. Therefore, we mainly compare previous work with pfl-research by implementing the same cross-device benchmarks and measuring wall-clock time to finish one simulation run; see Section 4.1 for the results.

The existing FL simulation frameworks and additional works from [16; 38; 53; 56] provide benchmarks for evaluating general FL algorithms. Additionally,  focuses on evaluation of cross-silo FL, and [19; 59; 86] on personalized FL. A subset of each suite of benchmarks are datasets that resemble realistic FL partitions. Unfortunately, publications have largely been focused on datasets such as CIFAR10 , MNIST , Shakespeare , and LibSVM  that are not, in our view, generated by processes that adequately represent realistic FL settings A lot of progress has been made on creating large-scale, realistic datasets for PFL, including for example [15; 18; 34; 36; 50; 54; 79]. However, the large datasets realistic for FL cannot be used unless simulations become much faster. We did a survey on most recent publications at NeurIPS 2023 , ICLR 2024 , ICML 2024 . There are 162 publications that provide simulation results in federated learning. 142 out of 162 publications provide results on 1-100 clients, while only 6 out of the 162 publications consider more than 1000 clients for the empirical analysis. Appendix A.1 of  confirmed the same trend for previous years.

Few FL frameworks integrate security and privacy features, despite FL alone not preserving the privacy of individual clients [12; 29]. Client's privacy can be violated by: (1) a server actor inspecting gradients from individual clients before aggregation; and (2) information about clients' data being extracted from the model after deployment . The second issue is commonly addressed using central differential privacy (DP) [26; 65; 89]. The first issue requires, in addition to DP, secure aggregation [10; 14; 81].

Most secure aggregation schemes merely compute a sum, and therefore do not need to be simulated, but DP mechanisms need to be included in PFL simulations to accurately reflect the effects of noisy statistics. There are several popular libraries that provide implementations of DP mechanisms to apply DP noise on generic vectors [31; 91]. However, most existing FL frameworks do not provide a simple way to plug these DP mechanisms into existing FL algorithms whilst ensuring consistency of DP parameters with FL hyperparameters in experiments.

## 3 System design

pfl-research is built entirely in Python and bundled into one package, making installation easy on any platform. The framework is designed to be modular such that researchers are able to select a model and an algorithm, specify additional processing such as DP, and then quickly try these out for any use case.

In this section, we highlight key design elements that differentiate pfl-research from alternative FL frameworks. We defer the introduction to FL and DP to Appendix A, and of the detailed system design to Appendix B, with the specific classes to which these extension points map in pfl-research to Appendix B.1.

In pfl-research simulations, the algorithm drives the training procedure by: constructing queries to execute on sampled cohorts of one or multiple federated datasets for each central iteration, defining the local optimization procedure with the assistance of a local model to execute the query for a user, defining how to use aggregated statistics to update the central state, and determining when to stop training. The output of the local optimization procedure and output of the aggregation can be postprocessed by various composable features like DP mechanisms, weighting, sparsification and compression. Algorithm 1 describe this generalized PFL simulation algorithm in detail.

We highlight these key elements that contribute to the high efficiency of pfl-research:

1. Only one model per worker process is initialized and preserved on the GPU at all times.
2. Model parameters are updated in-place on the single model the worker process has access to. In practice, the same instantiation is used for both local training, local evaluation, central updates and central evaluation. It is only the model's state that is sometimes cloned, e.g. before training first user in new sampled cohort, but it is always cloned to already allocated tensors on the GPU. There is no memory in the order of the model size that is released and re-allocated during the course of the FL simulation.
3. pfl-research does not simulate the topology of FL, i.e. there is no main coordinating process to gather and broadcast model updates. Further details in Section 3.1.
4. ML framework tensors are used on the GPU end-to-end in the outer training loop. This includes DP mechanisms. Alternative frameworks, e.g. Flower, use NumPy for parts of the outer loop.
5. pfl-research performs load balancing of training users. This is particularly important in the case of a high dispersion in user dataset lengths, e.g. FLAIR dataset. A detailed analysis on the effect of this feature is presented in Appendix B.6.
6. Using torch.utils.data and tf.data we provide support for asynchronously loading and preprocessing entire user datasets.

pfl-research specifically emphasizes the flexibility for experimenting with DP. The framework is superior to existing (P)FL frameworks for simulating FL with privacy guarantees in several ways: (1) Central and local DP mechanisms and accounting methods are abstracted as pluggable components, allowing researchers to easily experiment with different combinations of DP methods; (2) Tight integration between the DP mechanisms and FL hyperparameters to prevent errors in using DP with FL, e.g. DP noise is correctly scaled according to the actual clipping bound used in each iteration; (3) DP mechanisms are implemented with GPU acceleration without data transferring between CPU and GPU; (4) pfl-research offers a broad range of state-of-the-art privacy mechanisms and accountants out-of-the-box for different use-cases, listed in Appendix B.5.

pfl-research provides interfaces that allow straightforward integration with Foundation Model (FM) frameworks such as HuggingFace . The open-sourced models and datasets for FM can be easily adapted and trained in pfl-research, which we believe will enable and accelerate the frontier research on FM with PFL.

### Distributed simulations

Most existing FL frameworks have explicit communication steps as they aim to simulate the topology of a real-world FL system where the central server is a bottleneck. In many deployments, the central server can be adequately scaled, and it is valuable to have a faster simulation that ignores the topology. pfl-research simulates only the computation of FL. Communication is necessary only if the simulation runs with multiple workers (using Aggregator.worker_reduce, see Appendix B.2). There is no dedicated coordinator or aggregator process needed with which all other workers communicate, unlike real-world FL. All worker processes are replicas, making debugging and transition from single-process to multi-worker training straightforward.

As depicted in Figure 1, multiple worker processes can both live on a single machine and be instantiated across multiple machines. If a machine has \(g\) GPUs and \(gp\) worker processes, then \(p\) processes will share the same GPU. (P)FL generally has more non-GPU overhead than conventional

Figure 1: (a) The (simplified) architecture of distributed simulations. Each process is a replica with a different distributed context. One synchronous communication step is done to aggregate gradients and metrics. (b) Each process has a balanced queue of users to train.

ML training, resulting in a lower GPU utilization. Section 4.2 empirically shows that spawning more worker processes than GPUs available will increase the GPU utilization and speed up simulations.

In each central iteration, a sampled cohort of user IDs is scheduled among the worker processes. Given an associated weight for each user, pfl-research will do a greedy optimization based on these weights to schedule users across the worker processes to minimize stragglers. See Appendix B.6 for further details on the scheduling algorithm. Each worker process locally aggregates client model updates and metrics (while there are more clients to train for the current central iteration). Then, inter-worker aggregation is done with all-reduce.

## 4 Experiments

### Comparing performance to alternative FL simulators

In this section, we present comprehensive speed benchmarking of FL simulation frameworks. We compare frameworks on two datasets: CIFAR10 IID and FLAIR. These are two of the setups also included in pfl-research benchmarks, described in Section 4.3. CIFAR10 simulation was done on one NVIDIA A100 GPU (40GB) and FLAIR simulation was done on four NVIDIA A100 GPUs. We refer the reader to Appendix D for details on how the frameworks were selected, the specific versions used, and additional information about the setups. For a fair comparison, the hyperparameter values are the same for different frameworks on both CIFAR10 IID and FLAIR.

In Table 1, we compare the accuracy (mainly as a consistency check) and the wall-clock time of simulation runs on CIFAR10 for the most popular open-source FL simulators. The hyper-parameters are the same for all framework setups and described in C.5. The pfl-research setup in PyTorch with four processes sharing the single GPU is the fastest, with a wall-clock time of only 4 minutes and 20 seconds. This means that our framework executes this task 7\(\) faster than the runner-up, Flower, and 72\(\) faster than the slowest framework, FedScale. The difference in wall-clock time is so vast because smaller scale experiments like the CIFAR10 benchmark specifically emphasizes the necessary overhead that can exist in FL simulation, and frameworks that do not minimize this overhead suffer proportionally.

As a second comparison, we use FLAIR, which instead emphasizes the efficiency of training both a larger model and dataset in distributed simulations. Due to development time constraints, we implemented the FLAIR setup for the two frameworks that are most frequently used for publications: TFF and Flower.

Table 2 shows that pfl-research outperforms the alternatives prominently in terms of speed on the larger scale FLAIR benchmark. Relative to pfl-research, TFF is more efficient on FLAIR than

    &  &  & Wall-clock time &  & pfl-research \\  & & & & (minutes) & & is faster \\  pfl-research (PyTorch) & 1 & 1 & \(10.13_{ 0.06}\) & \(70.45\%_{ 0.30}\) & - \\ pfl-research (PyTorch) & 1 & 5 & \(4.20_{ 0.10}\) & \(70.25\%_{ 0.20}\) & - \\ pfl-research (TF) & 1 & 1 & \(15.34_{ 0.11}\) & \(70.43\%_{ 0.11}\) & - \\ pfl-research (TF) & 1 & 5 & \(7.89_{ 0.26}\) & \(70.52\%_{ 0.17}\) & - \\  FedML (PyTorch)  & 1 & 1 & \(90.95_{ 0.47}\) & \(71.23\%_{ 0.00}\) & 21.6\(\) \\ TensorFlow Federated  & 1 & 1 & \(113.52_{ 1.26}\) & \(70.02\%_{ 0.36}\) & 27\(\) \\ TensorFlow Federated  & 1 & 10 & \(82.23_{ 0.49}\) & \(70.12\%_{ 0.35}\) & 19.6\(\) \\ Flower (PyTorch)  & 1 & 1 & \(86.88_{ 1.42}\) & \(68.30\%_{ 0.12}\) & 16.4\(\) \\ Flower (PyTorch)  & 1 & 10 & \(29.26_{ 0.90}\) & \(67.66\%_{ 0.28}\) & 7\(\) \\ FedScale  & 1 & 1 & \(425.2_{ 17.66}\) & \(70.41\%_{ 0.00}\) & 101\(\) \\ FedScale  & 1 & 10 & \(301.7_{ 19.22}\) & \(70.90\%_{ 0.43}\) & 71.8\(\) \\ FLUTE  & 1 & 1 & \(67.86_{ 3.36}\) & \(62.267\%_{ 0.07}\) & 16.1\(\) \\   

Table 1: Comparison of simulating CIFAR10 IID dataset with different FL frameworks. \(g\) is the number of GPUs and \(p\) is the number of processes concurrently training users and sharing one GPU. All experiments are run for 5 times with different random seed, and both mean and standard deviation of metrics are reported.

on CIFAR10, and vice versa for Flower. The third row show that enabling DP only adds \(9\%\) extra simulation wall-clock time.

The number of processes per GPU, \(p\), was tuned to minimize wall-clock time, and the results show that pfl-research needs many fewer training processes per GPU to saturate GPU utilization. Tables 1 and 2 demonstrate that before the release of pfl-research, the comparison of framework speed greatly depended on the choice of benchmarks.

The superior performance of pfl-research can be attributed to several design choices, some of which are unique to it or not commonly employed by other FL simulators, described previously in Section 3. Appendix D.4.2 show system metrics from training the CIFAR10 benchmark, giving more insight into how pfl-research is faster.

### Scaling distributed training

Figure 2 shows the wall-clock time when scaling up number of processes in distributed simulations. In pfl-research, the number of worker processes is decoupled from the number of GPUs available, allowing processes to share a GPU. Using more than one process per GPU, \(p>1\), always has a positive effect, but the optimal number depends on the use-case (model size, cohort size). For CIFAR10 and StackOverflow , we can set \(p=5\) and the wall-clock time is less than \(0.43\) of that with \(p=1\) with same resources, while for FLAIR we can only fit a maximum of \(p=3\) on the GPU and the relative speedup is smaller.

   Framework & \(p\) &  Wall-clock time \\ (hours) \\  &  mAP \\  & 
 pfl-research \\ is faster \\  \\  pfl-research 0.1.0 (PyTorch) & 2 & 2.16 & \(61.8_{ 0.001}\) & - \\ pfl-research 0.2.0 (PyTorch) & 2 & 1.77 & \(61.1_{ 0.3}\) & - \\ pfl-research 0.2.0 (PyTorch) with central DP & 2 & 1.93 & - & - \\ TensorFlow Federated  & 5 & 18 & \(62.0_{ 0.3}\) & 10.2\(\) \\ Flower  & 5 & 28.8 & 31.5 4  & 16.3\(\) \\   

Table 2: Comparison of simulating FLAIR dataset with different FL frameworks. The metric is the same as FL-F-17 C-AP in . Note that version 0.2.0 was released after the initial open-source date of pfl-research, which implements improved scheduling for training users, see Appendix B.6.

Figure 2: Speedup from scaling up number of processes per GPU in distributed simulations, while keeping the hardware resources fixed. As long as number of GPUs \(\) cohort size (unlike the blue line, where we use an unnecessarily large amount of GPUs given the size of model, users and cohort), the wall-clock time is monotonically decreasing when increasing number of models to train in parallel on the same GPU.

The left panel of Figure 3 shows the wall-clock time when scaling up the number of GPUs on the StackOverflow benchmark with a cohort size of 400. The wall-clock time is reduced to 3 hours with just one GPU when scaling up number of worker processes to five, and the wall-clock time is reduced to 15 minutes when scaling up to 32 GPUs. Thus, pfl-research scales well with number of GPUs. The _rate_ of improvement is expected to be reduced as we increase the total number of GPUs because there are fewer users per process to load balance. This is revealed by the total number of GPU hours to finish (green lines). To demonstrate a more compute intensive scenario that can be scaled up further, we increase cohort size to 50,000 and show results from scaling up in the right panel of Figure 3. We see that distributed simulations scale even better in this case, for example the increase in GPU hours when moving from 16GPUs to 32GPUs is only 3.6%.

### Benchmarks for research

Along with the framework, we unify benchmarking of algorithms implemented in either PyTorch or TensorFlow (TF) under a collection of dataset-privacy setups, creating the matrix of benchmarks \(\{,\}\{,\}\{,\}\). The modularity of our framework and benchmark setups enables us to grow in any dimension: Add a dataset to benchmark all algorithms on, benchmark another algorithm, or add support for another framework or privacy mechanism. We consider the adaptive federated optimization such as FedAdam  as a tunable component of these algorithms, i.e. the choice of the central optimizer. The detailed hyperparameter choices for each dataset can be found in Appendix C. In this section we present the benchmark results for the PyTorch setups; we discuss the TensorFlow setups in Appendix C.1.

The initial set of algorithms we benchmark is shown in Table 3 without DP and in Table 4 with central DP. All experiments are run for five times with different random seed and metrics are averaged.

   Algorithm & C10-IID \(\) & C10 \(\) & SO \(\) & FLR-IID \(\) & FLR \(\) & SA \(\) & Aya \(\) & OA \(\) \\  FedAvg  & \(70.37_{.21}\) & \(69.80_{.35}\) & \(60.87_{.28}\) & \(65.08_{.60}\) & \(61.83_{.10}\) & \(3.61\) & \(4.02\) & \(6.39\) \\ FedProx  & \(70.69_{.33}\) & \(69.79_{.35}\) & \(60.88_{.28}\) & \(64.45_{ 1.13}\) & \(62.02_{.11}\) & \(3.61\) & \(4.02\) & \(6.39\) \\ AdaFedProx  & \(70.39_{.33}\) & \(69.61_{.45}\) & \(61.01_{.34}\) & \(64.53_{ 1.01}\) & \(61.99_{.11}\) & \(3.61\) & \(4.02\) & \(6.40\) \\ SCAFFOLD  & - & \(60.17_{ 1.29}\) & \(62.43_{.09}\) & - & \(60.82_{.12}\) & \(3.73\) & \(4.77\) & \(6.90\) \\   

Table 3: Performance of FL algorithms on the pfl-research benchmark suite without DP. C10 stands for accuracy on CIFAR10, SO stands for perplexity on StackOverflow, FLR stands for mean average precision (mAP) on FLAIR. The last three columns are the perplexity scores for three LLM benchmark datasets: Stanford Alpaca (SA), Aya, and OpenAssistant (OA). If not mentioned, the natural or artificial non-IID partitioning is used for the dataset, and -IID means partitioning with fixed number of samples per client drawn IID. AdaFedProx is FedProx with adaptive \(\) from Appendix C.3.3 in . Errors for LLM experiments are detailed in Table 12

Figure 3: Speedup from scaling up distributed simulations. **Left panel:** Sweep number of processes per GPU on CIFAR10, StackOverflow and FLAIR benchmarks and keep the hardware resources pinned. **Right panel:** Sweep the number of GPUs to train the StackOverflow benchmark, repeat for 1, 3 and 5 processes per GPU. Blue lines (tracked on left y-axis) show wall-clock time. Green lines (tracked in right y-axis) show total GPU hours for the same experiments. Note that the runs with >8 GPUs use multiple hosts and thus become slightly less efficient.

Evaluation is done on the validation data partitions of the original datasets without any federated splits. The common hyperparameters, such as cohort size and number of local epochs, were optimized using the FedAvg experiments. Meanwhile, specific hyperparameters for each algorithm were individually tuned. As a result, direct comparisons between the performance of different algorithms and FedAvg may not be conclusive. However, some observations based on the results include:

* Banded matrix factorization mechanism, also known as DP-FTRL when applied to FL, significantly outperforms Gaussian mechanism with PLD moments accountant on StackOverflow, with 10% relative improvement.
* SCAFFOLD, a popular algorithm according to citations, does not perform better than FedAvg on any of our benchmarks.  also report that SCAFFOLD consistently perform worse than FedAvg.
* FedProx is known for performing well on heterogeneous data partitions, but this is only marginally reflected in the FLAIR benchmark results. It is worse than FedAvg baseline on FLAIR IID, but better on FLAIR with natural heterogeneous client partitions.

The breadth of algorithms is limited at this moment, but we believe we have provided robust FedAvg baselines and a flexible foundation for future work to expand the results in terms of algorithms, complementary datasets, better hyper-parameter tuning of algorithms for more fair comparison, and other evaluation metrics, e.g. amount of communicated bits.

LLM benchmarksWe consider three datasets for benchmarking LLM fine-tuning with pfl-research to demonstrate the scalability to larger models: Stanford Alpaca (SA), Aya and OpenAssistant (OA). Compared to existing FL LLM benchmarks  where centralized datasets are artificially partitioned into users, Aya and OA have a natural user partition to simulate heterogeneity that reflects a more realistic production setting. SA contains 52,002 instruction-following data which is then artificially partitioned into users in an IID fashion. Aya contains a total of 204,112 human-annotated prompt-completion pairs, where each label has the identifier of the annotator. OA contains more than 120,000 conversational messages between human and AI assistant pairs. We collect 85,318 pairs of user inputs and assistant response with associated user identifier. We partition Aya and OA into users in a non-IID fashion using the provided user identifiers. Details of dataset partition and model fine-tuning are described in Appendix C.8.

## 5 Future work

pfl-research currently provides a small collection of federated algorithms. Our primary goal has been to release a fast, modular and easy-to-use simulator which is useful for advancing research in this area. We will continuously benchmark new algorithms and methods, and will rely heavily on the community for contributing implementations of new research.

We plan to carefully curate more benchmarks while maintaining a diverse set of domains for the benchmarks. All benchmark results reported in Section 4.3 used PyTorch, and releasing similar benchmarks in TensorFlow is work in progress. We recognize that all our current benchmarks

   Algorithm & DP & C10-IID \(\) & C10 \(\) & SO \(\) & FLR-IID \(\) & FLR \(\) & SA \(\) & Aya \(\) & OA \(\) \\  FedAvg & G & \(68.77_{.38}\) & \(66.70_{.09}\) & \(69.11_{.35}\) & \(65.24_{.22}\) & \(61.49_{.11}\) & \(3.62\) & \(4.19\) & \(6.52\) \\ FedAvg & BMF & \(69.21_{.51}\) & \(67.07_{.14}\) & \(62.47_{.22}\) & \(65.55_{.05}\) & \(61.64_{.10}\) & \(3.62\) & \(4.20\) & \(6.53\) \\ FedProx & G & \(68.72_{.34}\) & \(66.75_{.21}\) & \(69.12_{.34}\) & \(65.24_{.17}\) & \(61.55_{.11}\) & \(3.62\) & \(4.20\) & \(6.52\) \\ AdaFedProx & G & \(68.78_{.48}\) & \(66.80_{.09}\) & \(69.14_{.34}\) & \(65.18_{.16}\) & \(61.58_{.08}\) & \(3.62\) & \(4.20\) & \(6.52\) \\ SCAFFOLD & G & - & \(61.31_{.24}\) & \(77.17_{.10}\) & - & \(51.64_{.44}\) & \(3.79\) & \(4.76\) & \(7.00\) \\   

Table 4: Performance of FL algorithms on the pfl-research benchmark suite with central DP. The same rules and notation as in Table 3 are used. G stands for Gaussian mechanism with accounting using privacy loss distribution  and BMF stands for banded matrix factorization mechanism . Errors for LLM experiments are detailed in Table 13.

use cross-device FL. Cross-silo FL can be simulated with pfl-research, but we do not provide benchmarks for this scenario yet. 5

The aggregator interface can be extended to make a client-server architecture with a dedicated aggregator and coordinator process, which might be required for testing certain research ideas not limited to synchronous FL. We also do not provide interfaces for implementing PFL with real edge devices. This is outside the scope of pfl-research at this time.

In some scenarios, such as large-scale cross-silo PFL simulations or training foundation models, it may be useful to train a local model for a single user (silo) on multiple GPUs. Implementation of model parallelism in pfl-research remains for future work.

Some benchmarking related to system performance left for future work are on the concept of weak scaling--to investigate the efficiency of the distributed setup as the workload and resources scale proportionally. Figure 3 had a fixed cohort size and didn't increase the cohort sizes proportionally.

## 6 Conclusion

We have introduced pfl-research, a fast, modular, and easy-to-use simulation framework for accelerating research in federated learning (FL) and private federated learning (PFL). It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. It requires no code changes for scaling up distributed simulations across multiple processes, GPUs, and machines.

We compare the performance to other open-source FL simulators and our benchmarks show that pfl-research is 7-72\(\) faster. The immense speedup, flexible APIs, benchmark setups with realistic datasets, integration with visualization tools, and simple debugging experience will significantly increase the productivity of (P)FL researchers. We believe that researchers will view the speedup that that pfl-research offers as an opportunity to validate research on more challenging and realistic datasets than what is currently common practice in the field.