# MomentumSMoE: Integrating Momentum into

Sparse Mixture of Experts

 Rachel S.Y. Teo

Department of Mathematics

National University of Singapore

rachel.tsy@u.nus.edu

&Tan M. Nguyen

Department of Mathematics

National University of Singapore

tanmn@nus.edu.sg

###### Abstract

Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase in parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs, named MomentumSMoE. We theoretically prove and numerically demonstrate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations. The code is publicly available at https://github.com/rachtsy/MomentumSMoE.

## 1 Introduction

Scaling up deep models has demonstrated significant potential for enhancing the model's performance on a wide range of cognitive and machine learning tasks, ranging from large language model pre-training  and vision understanding  to reinforcement learning  and scientific applications . However, increasing the model's size requires a higher computational budget, which can be often challenging to meet. As a result, Sparse Mixture of Experts (SMoE) has been recently studied as an efficient approach to effectively scale up deep models. By modularizing the network and activating only subsets of experts for each input, SMoE maintains constant computational costs while increasing model complexity. This approach enables the development of billion-parameter models and achieves significant success in various applications, including machine translation , image classification , and speech recognition .

### Sparse Mixture of Experts

A MoE replaces a component in the layer of the model, for example, a feed-forward or convolutional layer, by a set of networks termed experts. This approach largely scales up the model but increasesthe computational cost. A SMoE inherits the extended model capacity from MoE but preserves the computational overhead by taking advantage of conditional computation. In particular, a SMoE consists of a router and \(E\) expert networks, \(u_{i}\), \(i=1,2,,E\). For each input token \(_{t}^{D}\) at layer \(t\), the SMoE's router computes the affinity scores between \(_{t}\) and each expert as \(g_{i}(_{t})\), \(i=1,2,,E\). In practice, we often choose the router \(g(_{t})=[g_{1}(_{t}),g_{2}(_{t}),,g_{E}(_{t})]^{ }=+\), where \(^{E D}\) and \(^{E}\). Then, a sparse gating function \(\) is applied to select only \(K\) experts with the greatest affinity scores. Here, we define the \(\) function as:

\[(g_{i}):=g_{i},&$ is in the $K$ largest elements of $g$}\\ -,&\] (1)

The outputs from \(K\) expert networks chosen by the router are then linearly combined as

\[_{t+1}=_{t}+_{i=1}^{E}((g_{i}(_{t}))u_{i}(_{t})=_{t}+u(_{t}),\] (2)

where \((g_{i}):=(g_{i})/_{j=1}^{E}(g_{j})\). We often set \(K=2\), i.e., top-2 routing, as this configuration has been shown to provide the best trade-off between training efficiency and testing performance [35; 16; 76].

**Limitations of SMoE.** Despite their remarkable success, SMoE suffers from unstable training [11; 78] and difficulty in adapting to new distributions, leading to the model's lack of robustness to data contamination [55; 75]. These limitations impede the application of SMoE to many important large-scale tasks.

### Contribution

In this paper, we explore the role of the residual connection in SMoE and show that simple modifications of this residual connection can help enhance the stability and robustness of SMoE. In particular, we develop a gradient descent (GD) analogy of the SMoE, showing that the dynamics of the expert representations in SMoE is associated with a gradient descent step toward the optimal solution of a multi-objective optimization problem. We then propose to integrate heavy-ball momentum into the dynamics of SMoE, which results in the Momentum Sparse Mixture-of-Experts (MomentumSMoE). At the core of MomentumSMoE is the use of momentum to stabilize and robustify the model. The architecture of MomentumSMoE is depicted in Fig. 1. MomentumSMoE can be extended beyond heavy-ball momentum to integrate well with other advanced momentum-accelerated methods such as AdamW [33; 42] and Robust Momentum . Our contribution is three-fold:

1. We incorporate heavy-ball momentum in SMoE to improve the model's stability and robustness.
2. We theoretically prove that the spectrum of MomentumSMoE is better-structured than SMoE, leading to the model's stability enhancement.
3. We show that the design principle of MomentumSMoE can be generalized to other advanced momentum-based optimization methods, proposing AdamSMoE and Robust MomentumSMoE.

Our experimental results validate that our momentum-based SMoE's improve over the baseline SMoE in terms of accuracy and robustness on a variety of practical benchmarks, including WikiText-103 language modeling and ImageNet-1K object recognition. We also empirically demonstrate that our momentum-based design framework is universally applicable to many existing SMoE models, including the Sparse MoE model for vision (V-MoE)  and the Generalist Language Model (GLaM) , just by changing a few lines of the baseline SMoE code.

**Organization.** We structure this paper as follows: In Section 2, we establish the connection between SMoE and gradient descent and derive our MomentumSMoE. In Section 3, we theoretically prove the stability advantage of MomentumSMoE over SMoE. In Section 4, we introduce AdamSMoE and Robust MomentumSMoE. In Section 5, we present our experimental results to justify the advantages of our momentum-based SMoE models over the traditional SMoE and other SMoE baselines. In Section 6, we empirically analyze our MomentumSMoE. We discuss related works in Section 7. The paper ends with concluding remarks. More experimental details are provided in the Appendix.

## 2 Momentum Sparse Mixture of Experts

### Background: Multiple-Gradient Descent Algorithm for Multi-objective Optimization

A multi-objective optimization problem comprises of the concurrent optimization of \(E\) objective functions, \(F_{i}(x)\), \(i=1,2,,E\), which might be formulated as the following minimization problem

\[_{x D}F(x):=_{i=1}^{E}c_{i}F_{i}(x)\] (3)

where \(D\) is the feasible region and \(c_{i}\) are weights representing the importance of each objective function. The optimal solution to the multi-objective optimization problem above is a Pareto-optimal point such that there is no other solution that can decrease at least one of the objective functions without increasing any other objective functions.  shows that a necessary condition for a solution to be Pareto-optimal is for it to be Pareto-stationary, which is defined as:

**Definition 1** (Pareto-stationary): _Let \(x\) be in the interior of the feasible region, \(D\), in which the \(E\) objective functions, \(F_{i}\), are smooth, and \(f_{i}(x)=_{x}F_{i}(x)\) be the local gradients for \(i=1,,E\). \(x\) is said to be Pareto-stationary if there exists a vector \(=[_{1},,_{E}]^{}^{E}\) such that \(_{i} 0\), \(_{i=1}^{E}_{i}=1\) and \(_{i=1}^{E}_{i}f_{i}(x)=0\). That is, there exists a convex combination of the gradient-vectors \(f_{i}(x)\) that is equal to \(0\)._

Therefore, it would be intuitive to extend the steepest descent algorithm to a multi-objective setting by finding a descent direction, that is common to all objectives, in the convex hull of the normalized local gradients \(_{i}(x)=f_{i}(x)/\|f_{i}(x)\|\). We denote such a set as \(=\{v^{N}|v=_{i=1}^{E}_{i}_{i}(x); _{i} 0, i;_{i=1}^{E}_{i}=1\}\). Indeed,  developed the Multiple-Gradient Descent Algorithm (MGDA) from such an understanding, proving that there does exist such a descent direction in \(\), which is the direction with the smallest norm in the set. Then, the update rule of MGDA is

\[x_{t+1}=x_{t}-_{i=1}^{E}_{i}^{*}_{i}(x_{t})\] (4)

where \(^{*}=(_{1}^{*},,_{E}^{*})\) minimizes \(\{\|v\||v\}\).

### Background: Momentum Acceleration for Gradient-Based Optimization

Among the simplest learning algorithms is gradient descent, also termed the steepest descent method. It typically starts with an objective function \(F(x)\) whose minima we aim to find by modifying our iterate \(x_{t}\) at each time step \(t\) through its gradient \(f(x_{t})=_{x}F(x_{t})\), scaled by a step size \(>0\):

\[x_{t+1}=x_{t}-_{x}F(x_{t})=x_{t}- f(x_{t}).\] (5)

However, following this update rule might result in slow convergence. A classical acceleration method to speed up the steepest descent, known as the heavy-ball method [53; 67], includes a momentum term in the algorithm. This takes the form of

\[p_{t}=-f(x_{t})+ p_{t-1}; x_{t+1}=x_{t}+ p_{t},\] (6)

Figure 1: Illustration of SMoE (Left) and MomentumSMoE layer (Right). We establish a connection between Multiple-Gradient Descent and SMoE to introduce momentum into the model, leading to better accuracy, enhanced robustness, and faster convergence.

where \(>0\) is the step size, and \( 0\) is the momentum constant. Eqn. 6 can then be rewritten as:

\[x_{t+1}=x_{t}+[-f(x_{t})+ p_{t-1}]=x_{t}- f(x_{t})+(x_{t}-x_{t- 1}).\] (7)

By incorporating the past gradients in each update, the descent path can become smoother with fewer oscillations, resulting in a faster convergence .

### (S)MoE as (Stochastic) Gradient Descent on Multi-objective Optimization and MomentumSMoE

We will now consider the SMoE model from the multi-objective optimization perspective.

**MoE as Gradient Descent.** In viewing each expert in an SMoE as specializing in optimizing an objective function, we are going to establish a connection between MoE and GD, and further leverage momentum to enhance MoE and SMoE. We rewrite Eqn. 2 of MoE as follows:

\[_{t+1}=_{t}-_{i=1}^{E}(g_{i}(_{t})) [-u_{i}(_{t})].\] (8)

If we regard \(-u_{i}(_{t})\) as the local "gradient" \(_{}F_{i}(_{t})\) at the \(t\)-th iteration and \((g_{i}(_{t}))\) as to be learned to approximate \(_{i}^{*}\), then we can consider the MoE in Eqn. 2 and 8 as the dynamical system which updates \(_{t}\) using the MGDA to minimize the multi-objective optimization in Eqn. 3.

**SMoE as Stochastic Gradient Descent.** Given the analogy between MoE and GD, SMoE can be interpreted as a stochastic version of MoE, which corresponds to an SGD algorithm applied to the multi-objective optimization problem in Eqn. 3. SMoE is then reformulated as:

\[_{t+1}=_{t}-_{i=1}^{K}((g_{i}( _{t})))[-u_{i}(_{t})]=_{t}- f(_{t}).\] (9)

Here, \(-f(_{t})=_{i=1}^{K}((g_{i}(_{ t})))u_{i}(_{t})\) is the SMoE output.

**Empirical Evidences for the Gradient Descent Analogy of (S)MoE.** We provide empirical justification for the connection between (S)MoE and (S)GD in Fig. 2 and 3.

Figure 3: Average output norm at each layer across 1K train/validation samples of the (S)MoE trained on WikiText-103.

Figure 2: Average output norms at layers 1 and 6 of the MoE/SMoE during 80 training epochs on WikiText-103.

_Gradient norm \(\|f(_{t})\|\) decreases when \(t\) increases:_ As shown in Eqn. 8 and 9 above, the norm of the MoE and SMoE output corresponds to the gradient norm \(\|f(_{t})\|=_{}F(_{t})\), respectively. It is expected that this gradient norm decreases when \(t\) increases or equivalently when the number of layers in an (S)MoE model increases. Fig. 3 confirms this expectation by showing that the norm of the (S)MoE output decreases over layers in a 6-layer (S)MoE model trained on the WikiText-103 language modeling task. At the last layer, the norm increases might be due to overshooting, a common phenomenon that can occur when using gradient descent.

_Gradient norm \(\|f(_{t})\|\) at each layer \(t\) decreases during training:_ According to the update rule of MGDA in Eqn. 7, the coefficient \(_{i}^{*}\) minimizes the norm \(\|_{i=1}^{E}_{i}_{i}\|\). In Eqn. 8 and 9, as discussed above, \((g_{i}(_{t}))\) and \(((g_{i}(_{t})))\) try to learn \(_{i}^{*}\), respectively. Thus, it is expected that these two terms learn to reduce the corresponding \(\|_{i=1}^{E}_{i}_{i}\|\) in Eqn. 8 and 9, which is the norm of the SMoE output at layer \(t\). Fig. 2 verifies this expectation by showing that each MoE and SMoE layer learns to reduce its output norm during training, suggesting that the routers \((g_{i}(_{t}))\) and \(((g_{i}(_{t})))\) learn to approximate \(_{i}^{*}\). We provide the full plots for all layers in Appendix C, Fig. 6 and 7.

### MomentumSMoE

We propose the new _MomentumSMoE_ layer, depicted in Fig. 1, to accelerate the dynamics of 8, which is principled by the accelerated gradient descent theory (see Section 2.2):

\[_{t}=-f(_{t})+_{t-1};_{t+1}=_{t}+ _{t},\] (10)

where \( 0\) and \(>0\) are hyperparameters corresponding to the momentum coefficient and step size in the momentum-accelerated GD, respectively. The formulation of MomentumSMoE can be applied to MoE to derive the MomentumMoE.

## 3 Stability Analysis: MomentumSMoE vs. SMoE

In this section, we demonstrate the theoretical advantages of MomentumSMoE over SMoE. In particular, we show that the spectrum of MomentumSMoE is better-structured than that of SMoE, thus MomentumSMoE is more stable than SMoE. We rewrite MomentumSMoE using the equivalent form of momentum acceleration given in Eqn. 7 as follows:

\[_{t+1}=_{t}- f(_{t})+(_{t}-_{t-1}).\] (11)

Taking inspiration from , we then expand \(f(_{t})\) around the Pareto-stationary solution \(^{*}\) at which \(f(^{*})=0\) (see Definition 1) using Taylor expansion to obtain an approximation of \(f(_{t})\):

\[f(_{t}) f(^{*})+_{}f(^{*})(_{t}- ^{*})=_{}f(^{*})(_{t}-^{*}).\] (12)

Substituting the Taylor expansion of \(f(_{t})\) in Eqn. 12 into Eqn. 11, we attain

\[_{t+1}=_{t}-_{}f(^{*})(_{t}-^{ *})+(_{t}-_{t-1}).\]

Without loss of generality, we further let \(^{*}=0\) as we can always replace \(_{t}\) with \(_{t}+^{*}\). The formula of MomentumSMoE can then be simplified as

\[_{t+1}=_{t}-_{}f(^{*})_{t}+(_{t}-_{t-1}).\] (13)

Suppose that \(_{}f(^{*})\) does not have any defective eigenvalues and hence is diagonalizable. Then, \(_{}f(^{*})=^{-1}\), for some invertible matrix \(\) and the diagonal matrix \(\) with diagonal entries being the eigenvalues \((n),\,n=1,2,,N\), of \(_{}f(^{*})\). We can then rewrite Eqn. 13 as

\[_{t+1}=_{t}-_{t}+(_{t}-_{t-1}).\] (14)

Since we have decoupled the \(N\) features in \(_{t}\), we can consider each feature \(_{t}(n)\), separately. Introducing a dummy equation \(_{t}=_{t}\), we rewrite Eqn. 14 as follows:

\[_{t}(n)\\ _{t+1}(n)=0&1\\ -&(1+)-(n)_{t-1}(n)\\ _{t}(n)=_{t-1}(n)\\ _{t}(n)\] (15)

The convergence of \(_{t}(n)\) then depends on the eigenvalues \(_{1}()\) and \(_{2}()\) of \(\). In particular, we require \(\{|_{1}()|,|_{2}()|\}<1\). It should be noted that omitting the momentum parameter, i.e., \(=0\), recovers the standard, unaccelerated SMoE layer.

**Lemma 1**: _Given the matrix \(=0&1\\ -&(1+)-(n)\) and \(_{1}()\), \(_{2}()\) are eigenvalues of \(A\), \(\{|_{1}()|,|_{2}()|\}<1\) if and only if \((-1,1)\) and \((n)(0,2+2)\)._

**Proposition 1** (Convergence of MomentumSMoE): _The MomentumSMoE defined in Eqn. 10 converges if and only if \((-1,1)\) and \((n)(0,2+2)\)._

The proofs of the results above are provided in Appendix A. It is worth noting that in both the MomentumSMoE and standard SMoE, the convergence of \(_{t}\) depends on the eigenvalues of the Jacobian \(_{}f\) of the SMoE layer. Since the step size \(>0\), we require that \(_{}f\) to be positive definite for its eigenvalues \((n)\) to be positive. Furthermore, even though among the convergence conditions of MomentumSMoE is that \((-1,1)\), in practice, \(\) is chosen to be positive.

Proposition 1 implies that the spectrum of MomentumSMoE is better-structured than that of SMoE. Thus, MomentumSMoE is more stable than SMoE. We summarize this finding in Corollary 1 below.

**Corollary 1** (MomentumSMoE is more stable than SMoE): _Without momentum, \(=0\), the range of values that \((n)\) can take for the system to be stable is limited to \(0<(n)<2\). The addition of momentum expands this margin, almost doubling it, providing a larger parameter range for the network to converge stably to a good output in the forward pass._

## 4 Beyond Heavy-ball Momentum: AdamSMoE and Robust MomentumSMoE

In addition to heavy-ball momentum, there are several advanced momentum-based algorithms in optimization that can be utilized for SMoE design. In this subsection, we propose two additional variants of MomentumSMoE, AdamSMoE and Robust MomentumSMoE, which are derived from the AdamW [33; 42] and Robust Momentum , respectively.

### Adam Sparse Mixture of Experts (AdamSMoE)

Adam  accelerates the gradient dynamics by utilizing the moving average of historical gradients and element-wise squared gradients. Adam with a decoupled weight decay regularization (AdamW) is more commonly used in practice thanks to its better generalization over Adam. We employ AdamW to derive the _AdamSMoE_ as follows:

\[_{t}=_{t-1}+(1-)[-f(_{t})];_{t} =_{t-1}+(1-)f(_{t}) f(_{t})\] \[_{t+1}=_{t}+_{t}}+ }_{t}-_{t}\]

where \(\) is a small constant to prevent numerical instability, \(\) the weight decay parameter, \(\) the step size, and \(\) and \(\) are the decay parameters for the moment estimates.

### Robust Momentum Sparse Mixture of Experts (Robust MomentumSMoE)

Deep learning models, including SMoE, are known to not be robust to distribution shifts and data distortions [60; 19; 14]. Utilizing the connection between (S)MoE and (S)GD in Section 2.3, we develop the new _Robust MomentumSMoE_ from the Robust Momentum Method .

The Robust Momentum Method proposed by  has the following update rule

\[y_{t}=x_{t}+(x_{t}-x_{t-1}); x_{t+1}=x_{t}- f(y_{t})+(x_{ t}-x_{t-1}),\] (16)

where \(\), \(\) and \(\) are parameterized by an additional hyperparameter \(p\) as follows:

\[=(1+p)}{L};=}{k-1};= {p^{3}}{(k-1)(1-p)^{2}(1+p)}.\] (17)

Here, \(k=L/m\) is a condition ratio of the objective function assuming that it is \(m\)-strongly convex and \(L\)-smooth. Compared with the heavy-ball momentum in Eqn. 7, there is an additional variable \(y_{t}\) that can be interpreted as a feedback signal to steer the \(x_{t}\) toward a robust solution. The parameters \(\), \(\), and \(\) are designed such that the new system is robust.

We incorporate the Robust Momentum Method above in our SMoE optimization framework developed in Section 2.3 and formulate the novel Robust MomentumSMoE as follows:

\[_{t}=_{t}+(_{t}-_{t-1});_{t+1}=_{t }- f(_{t})+(_{t}-_{t-1}),\] (18)

where \(\), \(\) and \(\) are as defined in Eqn. 17, and \(-f(_{t})=_{i=1}^{K}((g_{i}(_{t}) ))u_{i}(_{t})\) is the SMoE output given the input \(_{t}\). Equivalently, at each layer \(t\), we update the input \(_{t}\) and momentum vector \(_{t}\) as

\[_{t}=_{t}+_{t-1};_{t}=-f(_{t})+ _{t-1};_{t+1}=_{t}+_{t}.\]

**Remark 1**: _We provide an interpretation of robust momentum in Appendix B._

## 5 Experimental Results

In this section, we numerically justify the advantages of our momentum-based SMoE over the baseline SMoE on both WikiText-103 language modeling and ImageNet-1k object recognition tasks. We aim to show that: (i) MomentumSMoE improves model performance across both language and vision tasks; (ii) AdamSMoE significantly outperforms the baseline and accelerates convergence in language models, even surpassing MomentumSMoE; (iii) Robust MomentumSMoE is highly effective at improving robustness of vision models to data corruption; (iv) MomentumSMoE is universal and can be easily integrated into many state-of-the-art SMoE and MoE models.

Throughout the experiments, we compare our momentum-based SMoE with the baseline SMoE of the same configuration, replacing SMoE layers with our momentum-based SMoE. For Adam-based SMoE models, we use AdamSMoE in the first layer of the model and MomentumSMoE for the subsequent layers. We provide an explanation for this implementation in Appendix D.1. We find that implementing AdamSMoE in the first layer is enough to significantly improve the model's overall performance and accelerate its convergence. Our results are averaged over 5 runs. Details on datasets, models, and training are provided in Appendix D, along with Table 4 summarizing the momentum methods implemented on SMoE/MoE models for different tasks in our experiments. More results can be found in Appendix E. All experiments are conducted on a server with 8 A100 GPUs.

### WikiText-103 Language Modeling

We use the Switch Transformer , referred to as SMoE in our tables and figures below, and GLaM  baselines. We consider 2 configurations: medium (6 layers) and large (12 layers). We report the perplexity (PPL) of MomentumSMoE and AdamSMoE in comparison with the baseline SMoE on word-level WikiText-103 validation and test datasets for both model sizes in Table 1. We also include experiments on the medium-sized GLaM. A lower PPL indicates a better performance of the model. To further demonstrate the robustness of our method, we test the models on word swap attacked WikiText-103 data and present their results. Across all metrics, AdamSMoE and AdamGLaM outperform the baseline by a significant margin, verifying the strength of our method. Additionally, in Figure 4(Left), we provide the training and validation PPL during the first 5 training epochs of MomentumSMoE and AdamSMoE compared to the baseline SMoE to illustrate the accelerated convergence of our momentum-based models.

   Model/Metric & Parameters &  Clean WikiText-103 \\ Valid PPL \(\) \\  & 
 Attacked WikiText-103 \\ Test PPL \(\) \\  \\  _SMoE-medium (baseline)_ & 216M & 33.76 & 35.55 & 42.24 & 44.19 \\ MomentumSMoE-medium & 216M & 32.29 & 33.46 & 40.94 & 42.33 \\ AdamSMoE-medium & 216M & **31.59** & **33.25** & **39.27** & **41.11** \\  _SMoE-large (baseline)_ & 388M & 29.31 & 30.33 & 36.77 & 37.83 \\ MomentumSMoE-large & 388M & **27.58** & **29.03** & **35.21** & **36.78** \\   _GLaM-medium (baseline)_ & 220M & 36.37 & 37.71 & 45.83 & 47.61 \\ MomentumGLaM-medium & 220M & 33.87 & 35.29 & 42.15 & 43.64 \\ AdamGLaM-medium & 220M & **32.99** & **34.32** & **41.09** & **42.81** \\   

Table 1: Perplexity (PPL) of momentum-based SMoE vs. SMoE baseline on clean/attacked WikiText-103.

An important advantage of MomentumSMoE is its simplicity, which allows easy implementation with negligible computational overhead. We provide a comparison of the run time/sample, memory, number of parameters, and computation time between models in Table 11 and 12 in Appendix E.5.

### ImageNet-1K Object Recognition Task

In this section, we investigate our momentum-based models on two popular vision models, Vision MoE (V-MoE)  and Soft MoE  on the ImageNet-1K (IN-1K) object recognition task. We focus on \(i)\) improving the robustness of V-MoE using Robust MomentumSMoE (18) and \(ii)\) demonstrating that our momentum method is not limited to sparse models but can be generalized to MoE models such as Soft MoE. To benchmark robustness to data corruptions, we use the standard datasets, ImageNet-R (IN-R) , ImageNet-A (IN-A) , and ImageNet-C (IN-C) .

**Vision Mixture of Experts (V-MoE).** We use a V-MoE (small) model as the baseline. This V-MoE consists of 8 Vision Transformer (ViT) blocks  with every odd block's MLP being replaced by a SMoE layer. In Table 2, we provide the top-1 accuracy (%) on the training and validation set of IN-1K, IN-R, IN-A, and IN-C, as well as the mean Corruption Error (mCE) for IN-C. While MomentumV-MoE and Robust MomentumV-MoE have marginal gains on clean IN-1K data, we see significant improvement on IN-R, IN-A, and IN-C with at least a 1% increase in accuracy across these metrics. Specifically, Robust MomentumV-MoE has an almost 2% increase and 2 mCE decrease on IN-C, justifying the advantage of our method. Furthermore, we visualize the top-1 accuracy and mCE across increasing severity of two corruption types in Fig. 14 in Appendix E.3 to illustrate the increasing effectiveness of our method with escalating data corruption. The results of Robust MomentumSMoE on WikiText-103 can also be found in Appendix E.4, Table 9.

**Soft Mixture of Experts (Soft MoE).** We use the Soft MoE-tiny with 12 layers. The first 6 layers consist of standard ViT blocks, and the last 6 layers replace the MLP in those blocks with a Soft MoE layer. We train a Momentum-Soft MoE and a baseline Soft MoE model on ImageNet-1K and present their results in Table 3. In addition, we plot the training loss and top-1 accuracy of both models for 120 training epochs in Fig. 4(Right). Notably, there is a considerable increase in the accuracy of Momentum-Soft MoE over the baseline, as well as a clear acceleration to a good solution during

   Model & Params &  Train IN-1K \\ Top-1 \(\) \\  &  Valid IN-1K \\ Top-5 \(\) \\  &  IN-R \\ Top-1 \(\) \\  &  IN-A \\ Top-5 \(\) \\  &  IN-A \\ Top-1 \(\) \\  &  IN-C \\ Top-1 \(\) \\  &  IN-C \\ Top-1 \(\) \\  & 
 IN-C \\ mCE \(\) \\  \\  _V-MoE (baseline)_ & 297M & 76.49 & **92.27** & 73.16 & **90.42** & 36.10 & 5.25 & 46.98 & 67.14 \\  MomentumV-MoE & 297M & **76.92** & 92.19 & **73.26** & 90.30 & 37.45 & **6.48** & 48.11 & 65.77 \\ Robust MomentumV-MoE & 297M & 76.66 & **92.27** & 73.20 & 90.36 & **37.57** & 6.37 & **48.82** & **64.92** \\   

Table 2: Top-1 accuracy (%) and mean corruption error (mCE) of MomentumV-MoE and Robust MomentumV-MoE vs. the V-MoE baseline on ImageNet-1K and popular robustness benchmarks for image classification.

Figure 4: **Left: WikiText-103 train/validation perplexity (PPL) curves during the first 5 training epochs for MomentumSMoE, AdamSMoE, and SMoE. AdamSMoE has significantly faster convergence compared to SMoE. Right: Training loss/top-1 accuracy (%) of Momentum-Soft MoE vs. Soft MoE baseline on ImageNet-1K across 120 epochs of training. Momentum-Soft MoE has faster convergence and improved accuracy.**

training. These findings justify the benefits and universality of our momentum-based method, that extends beyond SMoE to MoE.

## 6 Empirical Analysis

We conduct empirical analysis based on the SMoE-medium trained on WikiText-103 in Section 5.1.

**Norm-based Load Imbalance.** Load imbalance in SMoE occurs when only a small subset of experts are consistently selected [63; 77]. Numerous methods have been developed to counter this common phenomenon, such as introducing a buffer capacity for each expert and a load balancing loss [65; 18]. Orthogonal to these, in line with our GD framework for SMoEs, we examine the choice of experts determined by the size of the norm of their outputs, \(\|f_{i}(x)\|\).

From a multi-objective optimization perspective, the optimal descent direction is one that minimizes the norm in the convex hull of the normalized gradients \(\) (see Section 2.1). If the gradients are not normalized, the minimum norm direction is then expected to be mainly influenced by the gradients with the smallest norms . From our GD analogy of SMoE in Section 2.3, the gradients correspond to the experts, whose outputs are not normalized. We then visualize the proportion of times the experts in a SMoE are chosen according to their norms during inference in Fig. 5(Left, A). We exactly observe the corresponding phenomenon in the SMoE, further empirically justifying our connection between SMoE and GD. The full plots for all layers and all models are provided in Appendix C.

The direction with the smallest norms are frequently related to the objectives that have already had a substantial degree of convergence and is insufficient for a balanced minimization of the multi-objective criteria. In this light, an ideal SMoE output would have a norm-based balanced choice of experts and should translate to improved model performance. Indeed, in Section 5.1, we established the superior performance of MomentumSMoE and AdamSMoE on large-scale WikiText-103 language modeling task, and in Figure 5(Left, B, C), this directly correlates with a significantly more balanced selection of experts with respect to their norms.

**Ablation Study on Momentum \(\) and Step Size \(\).** To better understand the effects of the momentum parameter and step size on the performance of the trained MomentumSMoE models, we do an ablation

Figure 5: **Left: Proportion of each expert chosen, ordered from the largest norm of each expert output to the smallest norm, in layers 3 and 5 of SMoE, MomentumSMoE, and Adam SMoE, averaged over the WikiText-103 validation set. Right: Log validation perplexity (PPL) during the finetuning of hyperparameters, \(\) and \(\), for 40 training epochs in MomentumSMoE. When tuning \(\), we keep \(=0.7\) and vice versa with \(=1.0\).**

    &  &  \\  & & Top-1 \(\) & Top-5 \(\) \\  _Soft MoE (baseline)_ & 231M & 73.52 & 90.94 \\  Momentum-Soft MoE & 231M & **75.47** & **92.34** \\   

Table 3: Top-1/top-5 accuracy (%) of Momentum-Soft MoE vs. Soft MoE baseline on ImageNet-1K (IN-1K).

study on these two hyperparameters and include results in Fig. 5(Right), which contains a plot of log validation PPL during 40 training epochs. We notice that MomentumSMoE is robust to the choice of \(\), and we select \(=0.7\) for the final comparison with the baseline SMoE. On the other hand, when the value of \(\) is too small, there is an adverse effect on the model. Hence, we select \(=1.0\).

**Making Momentum \(\) and Step Size \(\) Learnable.** We note that additional time and effort are required to tune the momentum parameter and step size. Thus, we explore different methods to circumvent this limitation. We discuss the results of one such method, making \(\) and \(\) learnable parameters, in this section and include results in Table 5 in Appendix E.1. We leave the discussion on other methods to Appendix E.1. As shown in Table 5 in Appendix E.1, MomentumSMoE \((ii)\), with learnable \(\) and fixed \(\), significantly outperforms the baseline for both clean validation and test data by at least 1.5 PPL, even surpassing the tuned model in Table 1. On the attacked data, the benefits of MomentumSMoE are further enhanced with more than 2 PPL improvements. These results confirm that the benefits of our model can be leveraged with minimal effort. Since the MomentumSMoE is robust to the choice of \(\), we do not consider the setting of fixing \(\) and making \(\) learnable.

**Other Optimizers.** We study the integration of other advanced momentum and optimization methods, such as the Nesterov accelerated gradient , RMSProp , and sharpness-aware minimization , into our MomentumSMoE framework in Appendix E.4.

## 7 Related Work

**Sparse Mixture of Experts.** SMoE has been extensively studied to enhance the training efficiency of large language models (LLMs), with various stable routing strategies proposed, including (i) allowing tokens to select the top-k experts [35; 18; 79; 11], (ii) allowing experts to select the top-k tokens , and (iii) globally determining expert assignment [36; 9]. Recent works have also tried to enhance the robustness of SMoE.  study the robustness of SMoE for ViTs  while  investigates the robustness of SMoE for CNNs. Furthermore,  explores the potential of SMoE for domain generalization, and  employs SMoE for robust multi-task learning. Various works have also focused on addressing load imbalance in SMoE, including [63; 77; 7]. Our momentum-based SMoE can be easily incorporated into these methods above to further improve their performance.

**Deep Learning Models with Momentum.** Momentum has been utilized in the design of deep neural network (DNN) architectures [72; 38].  applies momentum to create large and consistent dictionaries for unsupervised learning using a contrastive loss, with a momentum-based moving average of the queue encoder at the core of this approach. Many DNN-based methods for sparse coding have been designed by unfolding classical optimization algorithms, such as FISTA , where momentum is used in the underlying optimizer . In addition,  introduces momentum into ResNet and DenseNet, [73; 49] integrate momentum into neural differential equations,  incorporates momentum into transformers, and  designs RNNs using momentum-accelerated first-order optimization algorithms.

## 8 Concluding Remarks

In this paper, we propose MomentumSMoE, a new class of SMoE that utilizes heavy-ball momentum to stabilize and robustify SMoE. We theoretically justify the stability of our MomentumSMoE models compared to the SMoE baseline. Furthermore, we demonstrate that our momentum-based design framework for SMoE is universal and can incorporate advanced momentum-based optimization methods, including AdamW [33; 42] and Robust Momentum , into many existing SMoE models. We empirically validate the advantage of our momentum-based SMoE over the standard SMoE baseline on WikiText-103 and ImageNet-1K. As shown in Table 7 in Appendix E.2, momentum has no positive effect on the small SMoE of only 3 layers but attains an increasing improvement with the medium and large models of 6 and 12 layers, respectively. This is expected as each layer represents an iteration of GD. A limitation of MomentumSMoE is that while beneficial for larger models, for models that have few layers, MomentumSMoE has little effect. From a theoretical perspective, it would be intriguing to develop a theory to explain the enhanced robustness of MomentumSMoE. Furthermore, MomentumSMoE can be analyzed as a fixed-point iteration. We leave these theoretical developments as future work.