# Interpretable Mesomorphic Neural Networks

For Tabular Data

Arlind Kadra

Department of Representation Learning

University of Freiburg

kadraa@cs.uni-freiburg.de &Sebastian Pineda Arango

Department of Representation Learning

University of Freiburg

pineda@cs.uni-freiburg.de &Josif Grabocka

Department of Machine Learning

University of Technology Nuremberg

josif.grabocka@utn.de

###### Abstract

Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this work, we propose a new class of interpretable neural networks for tabular data that are both deep and linear at the same time (i.e. mesomorphic). We optimize deep hypernetworks to generate explainable linear models on a per-instance basis. As a result, our models retain the accuracy of black-box deep networks while offering _free lunch_ explainability for tabular data by design. Through extensive experiments, we demonstrate that our explainable deep networks have comparable performance to state-of-the-art classifiers on tabular data and outperform current existing methods that are explainable by design.

## 1 Introduction

Tabular data are arguably the most widely spread traditional data modality arising in a plethora of real-world application domains (Bischl et al., 2021; Borisov et al., 2022). There exists a recent trend to deploy neural networks for predictive tasks on tabular data (Kadra et al., 2021; Gorishniy et al., 2021; Somepalli et al., 2022; Hollmann et al., 2023). In a series of such application realms, it is important to be able to explain the predictions of deep learning models to humans (Ras et al., 2022), especially when interacting with human decision-makers, such as in healthcare (Gulum et al., 2021; Tjoa and Guan, 2021), or the financial sector (Sadhwani et al., 2020). Heavily parametrized models such as deep neural networks can fit complex interactions in tabular datasets and achieve high predictive accuracy, however, they are not explainable. In that context, achieving both high predictive accuracy and explainability remains an open research question for the Machine Learning community.

In this work, we introduce _mesomorphic_ neural architectures1, a new class of deep models that are **both deep and locally linear** at the same time, therefore, offering interpretability by design. In a nutshell, we propose a new architecture that is simultaneously (i) _deep and accurate_, as well as (ii) _linear and explainable_ on a per-instance basis. Technically speaking, we learn _deep_ hypernetworks that generate _linear_ models that are accurate concerning the data point we are interested in explaining.

Our interpretable mesomorphic networks for tabular data (dubbed IMN) are classification or regression models that identify the relevant tabular features by design. It is important to highlight that this worktackles explaining predictions for a single data point (Lundberg and Lee, 2017), instead of explaining a model globally for the whole dataset (Ras et al., 2022). Similarly to existing prior works (Alvarez-Melis and Jaakkola, 2018; Chen et al., 2018), we train deep models that generate explainable local models for a data sample of interest. In contrast, we train hypernetworks that generate linear models in the original feature space through a purely supervised end-to-end optimization.

We empirically show that the proposed explainable deep models are both as accurate as existing black-box classifiers for tabular datasets and achieve better performance compared to explainable end-to-end prior methods. At the same time, IMN is as interpretable as explainer techniques. Throughout this work, explainers can be categorized into two groups: _i)_ interpretable surrogate models that are trained to approximate black-box models (Lundberg and Lee, 2017), and _ii)_ end-to-end explainable methods by design. Concretely, we show that our method achieves comparable accuracy to competitive black-box classifiers and manages to outperform current state-of-the-art end-to-end explainable methods on the tabular datasets of the popular AutoML benchmark (Gijsbers et al., 2019). In addition, we compare our technique against state-of-the-art predictive explainers on the recent XAI explainability benchmark for tabular data (Liu et al., 2021) and empirically demonstrate that our method offers competitive interpretability. As a result, our method represents a significant step forward in making deep learning explainable by design for tabular datasets. Overall, this work offers the following contributions:

* We present a technique that makes deep learning explainable by design via training hypernetworks to generate instance-specific linear models.
* We offer ample empirical evidence that our method is as accurate as black-box classifiers, with the benefit of being as interpretable as state-of-the-art prediction explainers.

## 2 Proposed Method

### Shallow Interpretability through Deep Hypernetworks

Let us denote a tabular dataset consisting of \(N\) instances of \(M\)-dimensional features as \(X^{N M}\) and the \(C\)-dimensional categorical target variable as \(Y\{1,,C\}^{N}\). A model with parameters \(w\) estimates the target variable as \(f:^{M}\ \ \ \ ^{C}\) and is optimized by minimizing the empirical risk \(_{w}_{n=1}^{N}(y_{n},f(x_{n};w))\), where \(:\{1,,C\}^{C} _{+}\) is a loss function. An explainable model \(f\) is one whose predictions \(_{n}=f(x_{n};w)\) for a data point \(x_{n}\) are interpretable by humans. For instance, linear models and decision trees are commonly accepted to be interpretable by Machine Learning practitioners (Ribeiro et al., 2016; Lundberg and Lee, 2017).

In this work, we rethink shallow interpretable models \(f(x_{n};w)\) by defining their parameters \(w\) to be the output of deep non-interpretable hypernetworks \(w(x_{n};):^{M}\), where the parameters of the hypernetwork are \(\). We remind the reader that a hypernetwork (a.k.a. meta-network, or "network of networks") is a neural network that generates the parameters of another network (Ha et al., 2017). In this mechanism, we train deep non-interpretable hypernetworks to generate interpretable models \(f\) in an end-to-end manner as \(_{}_{n=1}^{N}(y_{n},f(x_{n};w(x_{n };)))\).

### Interpretable Mesomorphic Networks (IMN)

Our method trains deep Multi-Layer Perceptron (MLP) hypernetworks that generate the parameters of linear models. For the case of multi-class classification, we consider linear models with parameters \(w^{C(M+1)}\), denoting one set of weights and bias terms per class, as \(f(x_{n};w)_{c}=e^{z(x_{n};w)_{c}}/_{k=1}^{C}e^{z (x_{n};w)_{k}}\), with \(z(x_{n};w)_{c}=_{m=1}^{M}w_{c,m}x_{n,m}+w_{c,0}\) representing the logit predictions for the \(c\)-th class. For the case of regression the linear model is simply \(f(x_{n};w)=_{m=1}^{M}w_{m}x_{n,m}+w_{0}\) with \(w^{M+1}\).

Let us present our method IMN by starting with the case of multi-class classification following the hypernetwork mechanism explained in Section 2.1. The hypernetwork \(w(x_{n};):^{M}^{C(M+1)}\) with parameters \(\) is a function that given a data point \(x_{n}^{M}\) generates the predictions as:\[f(x_{n};w(x_{n};))_{c}=;w(x_{n};) )_{c}}}{_{k=1}^{C}e^{z(x_{n};w(x_{n};))_{k}}},\;\;z (x_{n};w(x_{n};))_{c}=_{m=1}^{M}w(x_{n};) _{c,m}x_{n,m}+w(x_{n};)_{c,0}\] (1)

Instead of training weights \(w\) as in a standard linear classification, we use the output of an MLP network as the linear weights \(w(x_{n};)\). We illustrate the architecture of our mesomorphic network in Figure 1. In the case of regression, our linear model with hypernetworks is \(f(x_{n};w(x_{n};))=_{m=1}^{M}w(x_{n};)_{m}x_{n,m}+w (x_{n};)_{0}\). We highlight that our experimental protocol (Section 4) includes both classification and regression datasets.

Ultimately, we train the optimal parameters of the hypernetwork to minimize the following loss in an end-to-end manner: \(_{n=1}^{N}(y_{n},f (x_{n};w(x_{n};)))+||w(x_{n};)|| _{1}\).

Our hypernetworks generate interpretable models that are accurate concerning a data point of interest (e.g. "Explain why patient \(x_{n}\) is estimated to have cancer \(f(x_{n};w(x_{n};))>0.5\) by analyzing the impact of features using the generated linear weights."). We stress that our novel method IMN does not simply train one linear model per data point, contrary to prior work (Ribeiro et al., 2016). Instead, the hypernetwork learns to generate accurate linear models by a shared network across all data points. As a result, generating the linear weights demands a single forward pass through the hypernetwork, rather than a separate optimization procedure. Furthermore, our method intrinsically learns to generate similar linear hyperplanes for neighboring data instances. The outputted linear models are accurate both in correctly classifying the data point \(x_{n}\), but also for the other majority of training instances in the neighborhood (see proof-of-concept experiment below). The outcome is a linear model with parameters \(w(x_{n};)\) that both interprets the prediction, but also serves as an accurate local model for the neighborhood of points.

### Explainability Through Feature Attribution

The generated linear models \(w(x_{n};)\) can be used to explain predictions through feature attribution (i.e. feature importance) (Liu et al., 2021). It is important to re-emphasize that our method offers interpretable predictions for the estimated target \(f(x_{n};w(x_{n};))\) of a particular data point \(x_{n}\). Concretely, we can analyse the linear coefficients \(\{w(x_{n};)_{1},,w(x_{n};)_{M}\}\) to distill the importances of \(\{x_{n,1},,x_{n,M}\}\) by measuring the residual impact on the target. The impact of the \(m\)-th feature \(x_{n,m}\) in estimating the target variable, is proportional to the change in the estimated target if we remove the feature (Hooker et al., 2019). Considering our linear models, the impact of the \(m\)-th feature is proportional to the change of the predicted target if we set the \(m\)-th feature to zero. In terms of notation, we multiply the feature vector element-wise with a Kronecker delta vector \(_{i}^{m}=_{m i}\).

\[f(x_{n};w(x_{n};))-f(x_{n}^{m};w(x_{n}; )) w(x_{n};)_{m}\,x_{n,m}\] (2)

As a result, our feature attribution strategy is that the \(m\)-th feature impacts the prediction of the target variable by a signed magnitude of \(w(x_{n};)_{m}\,x_{n,m}\). In our experiments, all the features are normalized to the same mean and variance, therefore, the magnitude \(w(x_{n};)_{m}\,x_{n,m}\) can be directly used to explain the impact of the \(m\)-th feature. In cases where the unsigned importance is required, a practitioner can use the absolute impact \(|w(x_{n};)_{m}\,x_{n,m}|\) as the attribution. Furthermore, to measure the global importance of the \(m\)-th feature for the whole dataset, we can compute \(_{n=1}^{N}|w(x_{n};)_{m}\,x_{n,m}|\).

Figure 1: The IMN architecture.

### Proof-of-concept: Globally Accurate and Locally Interpretable Classifiers

As a proof of concept, we run our method on the half-moon toy task that consists of a 2-dimensional tabular dataset in the form of two half-moons that are not linearly separable.

Initially, we investigate the global accuracy of our method. As shown in Figure 2 (left), our method correctly classifies all the examples. Furthermore, our method learns an optimal non-linear decision boundary that separates the classes (plotted in green). To determine the decision boundary, we perform a fine-grid prediction on all possible combinations of \(x_{1}\) and \(x_{2}\). Subsequently, we identify the points that exhibit the minimal prediction distance to a probability prediction of 0.5. Lastly, in Figure 2 (right) we investigate the local interpretability of our method, by taking a point \(x^{}\) and calculating the corresponding weights \((w(x^{}),w(x^{})_{0})\) generated by our hypernetwork, where we omited the dependence on \(\) for simplicity. The black line shows all the points that reside on the hyperplane \(w(x^{})\) as \(\{x\,|\,w(x^{})^{T}\,x+w_{0}(x^{})=0\}\). It is important to highlight that the local hyperplane does not only correctly classify the point \(x^{}\), but also the neighboring points, retaining an accurate linear classifier for the neighborhood of points.

To validate our claim that the per-example (local) hyperplane correctly classifies neighboring points, we conduct the following analysis: For every datapoint \(x_{n}\) we take a specific number of nearest neighbor examples from every class, and we evaluate the classification accuracy of the hyperplane generated for the datapoint \(x_{n}\) on the set of all neighbors. We repeat the above procedure with varying neighborhood sizes and we present the results in Table 1. The results indicate that the mesomorphic neural network generates hyperplanes that are accurate in the neighborhood of the point whose prediction we are interested in explaining.

## 3 Related Work

Interpretable Models by Design:There exist Machine Learning models that offer interpretability by default. A standard approach is to use linear models (Tibshirani, 1996; Efron et al., 2004; Berkson, 1953) that assign interpretable weights to each of the input features. On the other hand, decision trees (Loh, 2011; Craven & Shavlik, 1995) use splitting rules that build up leaves and intermediate nodes. Every leaf node is associated with a predicted label, making it possible to follow the rules that led to a specific prediction. Bayesian methods such as Naive Bayes (Murphy et al., 2006) or Bayesian Neural Networks (Friedman et al., 1997) provide a framework for reasoning on the interactions of prior beliefs with evidence, thus simplifying the interpretation of probabilistic outputs. Instance-based models allow experts to reason about predictions based on the similarity to the train samples. The prediction model aggregates the labels of the neighbors in the training set, using the average of the top-k most similar samples (Freitas, 2014; Kim et al., 2015), or decision functions extracted from prototypes Martens et al. (2007). Attention-based models like TabNet (Arik & Pfister, 2021) make

  
**Number of Neighbors** & **Accuracy** \\ 
10 & 0.84 \\
25 & 0.82 \\
50 & 0.78 \\
100 & 0.77 \\
200 & 0.77 \\   

Table 1: Accuracy of local hyperplanes for neighboring points.

Figure 2: Investigating the accuracy and interpretability of IMN. **Left:** The global decision boundary of our method that separates the classes correctly. **Right:** The local hyperplane pertaining to an example \(x^{}\) which correctly classifies the local example and retains a good global classification for the neighboring points.

use of sequential attention to generate feature weights on a per-instance basis, while, DANet (Chen et al., 2022) generates global importance weights for both the raw input features and higher order concepts. Neural additive models (NAMs) (Agarwal et al., 2021) use a neural network per feature to model the additive function of individual features to the output. However, these models trade-off the performance for the sake of interpretability, therefore challenging their usage on applications that need high performance. A prior similar work also trains hyper-networks to generate local models by learning prototype instances through an encoder model Alvarez-Melis and Jaakkola (2018). In contrast, we directly generate interpretable linear models in the original feature space.

Interpretable Model Distillation:Given the common understanding that complex models are not interpretable, prior works propose to learn simple surrogates for mimicking the input-output behavior of the complex models (Burkart and Huber, 2021). Such surrogate models are interpretable, such as linear regression or decision trees (Ribeiro et al., 2016). The local surrogates generate interpretations only valid in the neighborhood of the selected samples. Some approaches explain the output by computing the contribution of each attribute (Lundberg and Lee, 2017) to the prediction of the particular sample. An alternative strategy is to fit globally interpretable models, by relying on decision trees (Frosst and Hinton, 2017; Yang et al., 2018), or linear models (Ribeiro et al., 2016). Moreover, global explainers sometimes provide feature importances (Goldstein et al., 2015; Cortez and Embrechts, 2011), which can be used for auxiliary purposes such as feature engineering. Most of the surrogate models tackle the explainability task disjointly, by first training a black box model, then learning a surrogate in a second step.

Interpretable Deep Learning via Visualization:Given the success of neural networks in real-world applications in computer vision, a series of prior works (Ras et al., 2022) introduce techniques aiming at explaining their predictions. A direct way to measure the feature importance is by evaluating the partial derivative of the network given the input (Simonyan et al., 2013). CAM upscales the output of the last convolutional layers after applying Global Average Pooling (GAP), obtaining a map of the class activations used for interpretability (Zhou et al., 2016). DeepLift calculates pixel-wise relevance scores by computing differences with respect to a reference image (Shrikumar et al., 2017). Integrated Gradients use a baseline image to compute the cumulative sensibility of a black-box model \(f\) to pixel-wise changes (Sundararajan et al., 2017). Other methods directly compute the pixel-wise relevance scores such that the network's output equals the sum of scores computed via Taylor Approximations (Montavon et al., 2017).

## 4 Experimental Protocol

### Predictive Accuracy Experiments

Baselines:In terms of interpretable white-box classifiers, we compare against **Logistic Regression** and **Decision Trees**, based on their scikit-learn library implementations (Pedregosa et al., 2011). On the other hand, we compare against two strong classifiers on tabular datasets, **Random Forest** and **CatBoost**. We use the scikit-learn interface for Random Forest, while for CatBoost we use the official implementation provided by the authors (Prokhorenkova et al., 2018). Lastly, in terms of interpretable deep learning architectures, we compare against **TabNet**(Arik and Pfister, 2021), a transformer architecture that makes use of attention for instance-wise feature-selection and **NAM**(Agarwal et al., 2021), a neural additive model which learns an additive function for every feature. For TabNet we use a well-maintained public implementation 2, while, for NAM we use the official public implementation from the authors 3.

Protocol:We run our predictive accuracy experiments on the AutoML benchmark that includes 35 diverse classification problems, containing between 690 and 539 383 data points, and between 5 and 7 201 features. For more details about the datasets included in our experiments, we point the reader to Appendix C. In our experiments, numerical features are standardized, while we transform categorical features through one-hot encoding. For binary classification datasets we use target encoding, where a category is encoded based on a shrunk estimate of the average target values for the data instances belonging to that category. In the case of missing values, we impute numerical features with zero and categorical features with a new category representing the missing value. For CatBoost and TabNet we do not encode categorical features since the algorithms natively handle them. For all the methods considered we tune the hyperparameters with Optuna Akiba et al. (2019), a well-known hyperparameter optimization (HPO) library. We use the default HPO algorithm (TPE) from the library and we tune every method for 100 HPO trials or a wall-time limit of 1 day, whichever condition gets fulfilled first. The HPO search spaces of the different baselines were taken from prior work Gorishniy et al. (2021); Hollmann et al. (2023). For a more detailed description, we kindly refer the reader to Appendix C. Additionally, we use the area under the ROC curve (AUROC) as the evaluation metric. Lastly, the methods that offer GPU support are run on a single NVIDIA RTX2080Ti, while, the rest of the methods are run on an AMD EPYC 7502 32-core processor.

### Explainability Experiments

Baselines:First, we compare against **Random**, a baseline that generates random importance weights. Furthermore, **BreakDown** decomposes predictions into parts that can be attributed to certain features Staniak and Biecek (2018). **TabNet** offers instance-wise feature importances by making use of attention. **LIME** is a local interpretability method Ribeiro et al. (2016) that fits an explainable surrogate (local model) to single instance predictions of black-box models. On the other hand, **L2X** is a method that applies instance-wise feature selection via variational approximations of mutual information Chen et al. (2018) by making use of a neural network to generate the weights of the explainer. **MAPLE** is a method that uses local linear modeling by exploring random forests as a feature selection method Plumb et al. (2018). **SHAP** is an additive feature attribution method Lundberg and Lee (2017) that allows local interpretation of the data instances. Last but not least, **Kernel SHAP** offers a reformulation of the LIME constrains Lundberg and Lee (2017).

Metrics and Benchmark:As explainability evaluation metrics we use faithfulness Lundberg and Lee (2017), monotonicity Luss et al. (2021) (including the ROAR variants Hooker et al. (2019)), infidelity Yeh et al. (2019) and Shapley correlation Lundberg and Lee (2017). For a detailed description of the metrics, we refer the reader to XAI-Bench, a recent explainability benchmark Liu et al. (2021).

For our explainability-related experiments, we use all three datasets (Gaussian Linear, Gaussian Non-Linear, and Gaussian Piecewise) available in the XAI-Bench Liu et al. (2021). For the state-of-the-art explainability baselines, we use the Tabular ResNet (TabResNet) backbone as the model for which the predictions are to be interpreted (same as for IMN). We experiment with different versions of the datasets that feature diverse \(\) values, where \(\) corresponds to the amount of correlation among features. All datasets have a train/validation set ratio of 10 to 1.

Implementation Details:We use PyTorch as the main library for our implementation. As a backbone, we use a TabResNet where the convolutional layers are replaced with fully-connected layers as suggested by recent work Kadra et al. (2021). For the default hyperparameters of our method, we use 2 residual blocks and 128 units per layer combined with the GELU activation Hendrycks and Gimpel (2016). When training our network, we use snapshot ensembling Huang et al. (2017) combined with cosine annealing with restarts Loshchilov and Hutter (2019). We use a learning rate and weight decay value of 0.01, where, the learning rate is warmed up to 0.01 for the first 5 epochs, a dropout value of 0.25, and an L1 penalty of 0.1 on the weights. Our network is trained for 500 epochs with a batch size of 64. We make our implementation publicly available4.

## 5 Experiments and Results

Hypothesis 1:IMN outperforms interpretable white-box models in terms of predictive accuracy.

We compare our method against decision trees and logistic regression, two white-box interpretable models. We run all aforementioned methods on the AutoML benchmark and we measure the predictive performance in terms of AUROC. Lastly, we measure the statistical significance of the results using the autorank package Herbold (2020) that runs a Friedman test with a Nemenyi post-hoc test, and a \(0.05\) significance level. Figure 3 presents the average rank across datasets based on the AUROC performance. As observed IMN achieves the best rank across the AutoML benchmarkdatasets. Furthermore, the difference is statistically significant against both decision trees and logistic regression. The detailed per-dataset results are presented in Appendix C.

Hypothesis 2:The explainability of IMN does not have a statistically significant negative impact on predictive accuracy. Additionally, it achieves a comparable performance against state-of-the-art methods.

This experiment addresses a simple question: _Is our explainable neural network as accurate as a black-box neural network counterpart, that has the same architecture and same capacity?._ Since our hypernetwork is a slight modification of the TabResNet Kadra et al. (2021), we compare it against TabResNet as a classifier. For completeness, we also compare against four other strong baselines, Gradient-Boosted Decision Trees (CatBoost), Random Forest, TabNet, and NAMs. Since the official implementation of NAMs only supports binary classification and regression, we separate the results into: _i_) results over 18 binary classification datasets (Figure 4**Top**), and _ii_) results over all datasets (Figure 4**Bottom**).

The results of Figure 4 demonstrate that IMN achieves a comparable performance to state-of-the-art tabular classification models, while significantly outperforming explainable methods by design. IMN achieves a comparable performance to TabResNet, while outperforming TabNet and NAMs, indicating that its explainability does not harm accuracy in a significant way. There is no statistical significance of the differences between IMN, TabResNet and CatBoost. However, the difference in performance between IMNs, Random Forest, TabNet and NAMs is statistically significant.

Additionally, we investigate the runtime performance of the different baselines (NAM is excluded since it cannot be run on the full benchmark). We present the results in Table 2. As expected, deep learning methods take a longer time to train, however, both IMN and TabResNet are the most efficient during inference. We observe that TabResNet takes longer to converge compared to IMN5, however, both methods demand approximately the same inference time. As a result, the explainability of our method comes as a _free-lunch_ benefit. Lastly, IMN is **64x faster in inference** compared to TabNet, an end-to-end deep-learning interpretable method. **Hypothesis 1 and 2 are valid even when default hyperparameters are used**, for more details we kindly refer the reader to Appendix B.

Hypothesis 3:IMNs offer competitive levels of interpretability compared to state-of-the-art explainer techniques.

We compare against 8 explainer baselines in terms of 5 explainability metrics in the 3 datasets of the XAI benchmark Liu et al. (2021), following the protocol we detailed in Section 4.2. The results of Table 3 demonstrate that IMN is competitive against all explainers across the indicated interpretability metrics. We tie in per

  
**Method Name** & **Median Training Time (sec)** & **Median Inference Time (sec)** \\  TabResNet (GPU) & 192 & 0.025 \\ TabResNet (GPU) & 252 & 0.020 \\ TabNet (GPU) & 237 & 1.60 \\ CalBoost (GPU) & 63.2 & 0.20 \\ Random Forest & 42.55 & 2.20 \\ Logistic Regression & 0.23 & 0.07 \\ Decision Tree & 0.4 & 0.06 \\   

Table 2: Aggregated training and inference times for all methods.

Figure 4: Black-box methods comparison with critical difference diagrams. **Top**: The average rank for the binary datasets present in the benchmark. **Bottom:** The average rank for all datasets present in the benchmark. A lower rank indicates a better performance. Connected ranks via a bold bar indicate that performances are not significantly different (\(p>0.05\)).

Figure 3: The critical difference diagram for the white-box interpretable methods. A lower rank indicates a better performance over datasets.

formance with the second-best method Kernel-SHAP (Lundberg and Lee, 2017) and perform strongly against the other explainers. It is worth highlighting that in comparison to all the explainer techniques, the interpretability of our method comes as _a free-lunch_. In contrast, all the rival methods except TabNet are surrogate interpretable models to black-box models. Moreover, IMN strongly outperforms TabNet, the other baseline that offers explainability by design, achieving both better interpretability (Table 3) and better accuracy (Figure 4).

As a result, for all surrogate interpretable baselines we first need to train a black-box model. Then, for the prediction of **every** data point, we additionally train a local explainer around that point by predicting with the black-box model multiple times. In stark contrast, our method combines prediction models and explainers as an all-in-one neural network. To generate an explainable model for a data point \(x_{n}\), IMN does not need to train a per-point explainer. Instead, IMN requires only a forward pass through the trained hypernetwork to generate a linear explainer. To quantify the difference in runtime between our method and other interpretable methods we compare the runtimes on a few datasets from the benchmark with a varying number of instances/features such as Credit-g (1000/21), Adult (48842/15), and Christine (5418/1637). Table 4 presents the results, where, as observed IMN has the fastest inference times, being **11-65x faster** compared to TabNet which employs attention, **1710-11400x faster** compared to SHAP that uses the same (TabResNet) backbone, and **455-215850x faster** compared to SHAP that uses CatBoost as a backbone.

  
**Method Name** & **Credit-g** & **Adult** & **Christine** \\  IMN & **0.01** & **0.02** & **0.02** \\ TaNet & 0.11 & 1.30 & 0.43 \\ SHAP (TabResNet) & 17.69 & 565.11 & 228.31 \\ SHAP (CatBoost) & 4.55 & 66.89 & 4317.61 \\   

Table 4: Interpretable method inference times. All the methods are run on the GPU and the time is reported in seconds.

Figure 5: Performance analysis of different interpretability methods over a varying degree of feature correlation \(\). We present the performance of all methods on faithfulness (ROAR), monotonicity (ROAR), faithfulness, and infidelity (ordered from **left** to **right**) on the Gaussian Linear dataset for \(\) values ranging from .

  
**Metric** & **Dataset** & **Random** & **Breakd.** & **Maple** & **LIME** & **L2X** & **SHAP** & **K. SHAP** & **TabNet** & **IMN** \\   & Gaussian Linear & 0.004 & 0.645 & 0.980 & 0.882 & 0.010 & 0.974 & 0.981 & 0.138 & **0.987** \\  & Gaussian Non-Linear & -0.079 & -0.001 & 0.487 & 0.796 & 0.155 & 0.926 & **0.970** & 0.161 & 0.621 \\  & Gaussian Piecewise & 0.091 & 0.634 & 0.967 & 0.929 & 0.016 & 0.981 & **0.990** & 0.058 & 0.841 \\   & Gaussian Linear & -0.039 & 0.494 & 0.548 & 0.544 & 0.049 & 0.549 & 0.550 & 0.041 & **0.639** \\  & Gaussian Non-Linear & 0.050 & 0.006 & 0.040 & -0.040 & -0.060 & -0.010 & -0.036 & -0.001 & 0.027 \\  & Gaussian Piecewise & -0.055 & 0.372 & 0.347 & 0.450 & 0.015 & 0.409 & 0.426 & 0.072 & 0.404 \\   & Gaussian Linear & 0.219 & 0.041 & **0.007** & **0.007** & 0.034 & **0.007** & **0.007** & 0.049 & **0.007** \\  & Gaussian Non-Linear & 0.075 & 0.086 & 0.021 & 0.071 & 0.089 & 0.030 & 0.022 & 0.047 & **0.018** \\  & Gaussian Piecewise & 0.132 & 0.047 & 0.014 & 0.019 & 0.070 & 0.016 & 0.019 & 0.046 & **0.008** \\   & Gaussian Linear & 0.487 & 0.605 & 0.700 & 0.652 & 0.437 & 0.680 & 0.667 & 0.585 & **0.785** \\  & Gaussian Non-Linear & 0.497 & 0.542 & 0.645 & 0.587 & 0.457 & **0.670** & 0.632 & 0.493 & 0.637 \\   & Gaussian Piecewise & 0.485 & 0.665 & 0.787 & 0.427 & 0.442 & 0.717 & **0.797** & 0.542 & 0.682 \\   & Gaussian Linear & -0.016 & 0.246 & **0.999** & 0.942 & -0.214 & 0.993 & **0.999** & 0.095 & **0.999** \\   & Gaussian Non-Linear & -0.069 & -0.179 & 0.686 & 0.872 & -0.095 & 0.974 & **0.999** & 0.125 & 0.741 \\   & Gaussian Piecewise & -0.078 & 0.099 & 0.983 & 0.959 & 0.157 & 0.991 & **0.999** & 0.070 & 0.875 \\   &  &  &  &  &  &  &  \\   

Table 3: Investigating the interpretability of IMNs against state-of-the-art interpretability methods. The results are generated from the XAI Benchmark (Liu et al., 2021) datasets (with \(=0\)).

[MISSING_PAGE_FAIL:9]

Conclusion

In this work, we propose explainable deep networks that are comparable in performance to their black-box counterparts but also as interpretable as state-of-the-art explanation techniques. With extensive experiments, we show that the explainable deep learning networks outperform traditional white-box models in terms of performance. Moreover, the experiments confirm that the explainable deep-learning architecture does not include a significant degradation in performance or an overhead on time compared to the plain black-box counterpart, achieving competitive results against state-of-the-art classifiers in tabular data. Our method matches competitive state-of-the-art explainability methods on a recent explainability benchmark in tabular data, offering explanations of predictions as a free lunch.

## 7 Limitations and Future Work

One potential limitation of our method is that although interpretable, the per-instance models are linear. A potential future work can focus on generating other types of non-linear interpretable models, such as decision trees. More concretely, the hypernetwork can generate the parameters of the decision splits and the decision value at each node, as well as the leaf weights. Another potential strategy is to generate local Support Vector Machines, by expressing the prediction for a data point as a function of the similarity of the informative neighbors.