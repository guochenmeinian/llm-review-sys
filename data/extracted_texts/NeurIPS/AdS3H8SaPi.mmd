# What does guidance do?

A fine-grained analysis in a simple setting

 Muthu Chidambaram

Duke University

muthu@cs.duke.edu

Khashayar Gatmiry

MIT

gatmiry@mit.edu

&Sitan Chen

Harvard University

sitan@seas.harvard.edu

Holden Lee

Johns Hopkins University

hlee283@jhu.edu

Lead authors, equal contributionEqual contributionEqual contribution

Jianfeng Lu

Duke University

jianfeng@math.duke.edu

###### Abstract

The use of guidance in diffusion models was originally motivated by the premise that the guidance-modified score is that of the data distribution tilted by a conditional likelihood raised to some power. In this work we clarify this misconception by rigorously proving that guidance fails to sample from the intended tilted distribution. Our main result is to give a fine-grained characterization of the dynamics of guidance in two cases, (1) mixtures of compactly supported distributions and (2) mixtures of Gaussians, which reflect salient properties of guidance that manifest on real-world data. In both cases, we prove that as the guidance parameter increases, the guided model samples more heavily from the boundary of the support of the conditional distribution. We also prove that for any nonzero level of score estimation error, sufficiently large guidance will result in sampling away from the support, theoretically justifying the empirical finding that large guidance results in distorted generations. In addition to verifying these results empirically in synthetic settings, we also show how our theoretical insights can offer useful prescriptions for practical deployment.

## 1 Introduction

With diffusion models having emerged as the leading approach to generative modeling in domains like image, video, and audio , there is a pressing need to develop principled methods for modulating their output. For example, how do we impose certain constraints on generated samples, or control their "temperature"? To formalize this, suppose one has access to an unconditional diffusion model approximating the data distribution \(p\), as well as a conditional diffusion model approximating the conditional distribution \(p^{(} z)\) for various choices of class labels or prompts \(z\).3 Given \(z\) and a parameter \(w\), one might wish to sample from the distribution \(p\)_tilted_ by the conditional likelihood, i.e. the distribution \(p^{z,w}\) with density

\[p^{z,w}(x) p(x) p(z x)^{1+w}\,.\]

(Throughout, we use lower-case letters to denote densities, and capital letters to denote distributions.)

By varying \(w\), we can naturally trade off between diversity and quality: If \(w=-1\) then \(p^{z,w}\) is the unconditional distribution, if \(w=0\) then \(p^{z,w}\) is the conditional distribution \(p( z)\) by Bayes' rule, and as \(w\), \(p^{z,w}\) converges to being supported on the maximizers of the conditional likelihood.

In practice, the standard approach to try to sample from the tilted distribution is to use _diffusion guidance_. The idea for this is roughly as follows. To sample from a given distribution using diffusion generative modeling, one numerically solves a certain ODE or SDE whose drift depends on the _score function_, i.e. gradient of the log-density, of \(p\) convolved with various levels of Gaussian noise. By definition, the score function of \(p^{z,w}\) satisfies

\[ p^{z,w}= p+(1+w) p(z)=-w p+(1 +w) p( z)\,. \]

Assuming we have access to approximations of both terms on the right, we can run the corresponding ODE or SDE whose drift can be computed using the above to approximately sample from \(p^{z,w}\).

There is however one fundamental snag in the reasoning above. The aforementioned ODE or SDE involves the score function at _different noise levels_, i.e. we would need access to \( p_{t}^{z,w}\), where we use the notation \(q_{t}\) to denote the distribution given by running a certain noising process (see Section 2.1) for time \(t\) starting from a distribution \(q\). Here \(p_{t}^{z,w}\) means we tilt before adding noise, that is, we take \(q=p^{z,w}\) and then apply noise to \(q\). Unfortunately, as soon as \(t>0\), the analogue of Eq. (1) no longer holds, i.e.

\[ p_{t}^{z,w}-w p_{t}+(1+w) p_{t}( z )\,. \]

In other words, the operation of applying noise to \(p\) and the operation of tilting it in the direction of the conditional likelihood _do not commute_. Nevertheless, in practice it is standard to use the right-hand side of Eq. (2) as an approximation . Sampling using this approximation is called diffusion guidance.

Intriguingly, for appropriate choices of \(w\), this heuristic results in generations with high perceptual quality. Yet despite the popularity and empirical success of guidance, our theoretical understanding of this approach is lacking. In this work, we ask:

_What distribution is diffusion guidance actually sampling from?_

A motivating example.To see clearly that diffusion guidance is not simply sampling from the tilted distribution, consider the following simple setting. Suppose that there are only two classes \(z=-1\) and \(z=+1\), and that the corresponding conditional distributions \(p( z=-1)\) and \(p( z=+1)\) have disjoint supports \(_{-},_{+}\). In this case, the conditional likelihood is simply given by \(p(z=i x)[x_{i}]\). In particular, the conditional likelihood is binary-valued, which implies that for any \(w>0\), the tilted distribution \(p^{z,w}\) is exactly the same! On the other hand, as Figure 1 shows, increasing \(w\) changes the distribution of generated samples to concentrate towards the edge of the support of the guided class.

This simple example already reflects two properties of diffusion guidance that manifest on real-world data as the guidance parameter \(w\) increases:

* **Drop in diversity**: The entropy of the distribution over generated samples resulting from diffusion guidance tends towards zero.
* **Divergence towards "archetypes"**: The generated outputs drift more and more to the extreme points in the support of the conditional distribution \(p( z=i)\) to which the diffusion model is

Figure 1: We consider sampling from the positive class of a 2D mixture of uniforms (a) using the probability flow ODE with the conditional score (b) and the guided score (c). As can be seen, increasing the guidance weight \(w\) clearly biases the distribution of samples to concentrate towards points far away from the other class support.

being guided. Note that these extreme points do not necessarily coincide with the set of maximizers of the conditional likelihood, as our example shows.

These phenomena are surprising because we have shown that they can occur even when the conditional likelihood contains _no geometric information_ about the data distribution: in our example, all points \(x_{+}\) have the same conditional likelihood under \(p(z=+1 x)\), so it is not at all clear why the sampling process should end up preferring points in \(_{+}\) which are furthest from points in \(_{-}\).

In this work, we hone in on simple settings in which we can precisely characterize the dynamics of diffusion guidance and explain this counterintuitive behavior. Our main result is to execute such an analysis for mixtures of compactly supported distributions:

**Theorem 1** (Compactly supported setting, informal - see Theorem 4).: _Consider a data distribution \(p=p^{(1)}+p^{(-1)}\) where \(p^{(1)},p^{(-1)}\) are \(\)-bounded and supported on disjoint intervals \([_{1},_{2}]\) and \([-_{2},-_{1}]\) respectively (see Assumption 1). Suppose that one runs the probability flow ODE with guidance parameter \(w\) which is larger than some absolute constant. Then with probability \(1-e^{-(w)}\), the resulting sample lies in the interval_

\[(_{2}1-O(1/),_{2}), \]

_where the \(O()\) notation hides constants depending on \(_{1},_{2},\)._

We also conduct this analysis in a setting where the conditional distributions \(p^{(1)}\) and \(p^{(-1)}\) do not have compact support by proving an analogous result for mixtures of Gaussians. The proofs in this setting turn out to be somewhat simpler:

**Theorem 2** (Gaussian setting).: _Consider the data distribution \(p=(1,1)+(-1,1)\). Suppose that one runs the probability flow ODE with guidance parameter \(w\) which is larger than some absolute constant. Then if the resulting sample is denoted by \((1)\), we have_

\[((1) 0) 1-e^{-(w^{2})}((1)) 1-e^{-(w)}\,.\]

Theorems 1 and 2 illustrate that diffusion guidance results in a strong bias towards points in the support of one conditional distribution which are far from points in the support of the other.

These results apply even when the unconditional and conditional diffusion models in question incur zero score estimation error. One shortcoming however is that they fail to corroborate a third commonly observed behavior of guidance in practice:

* **Degradation when guidance is too large**: In practice, even ignoring issues of diversity, there is typically a "sweet spot" for the choice of \(w\) such that past that point, the quality of the generated output begins to degrade.

Next, we show how to leverage ideas in the proof of Theorem 1 to explain this degradation. Concretely, we give a simple example where a small perturbation to the score estimate at the tails of the data distribution is enough to take the sampling trajectory given by diffusion guidance far away from the trajectory predicted by Theorem 1:

**Theorem 3**.: _Given \(0<<1\), assume \(w()\), where the hidden constant factor is sufficiently large. There exist densities \(p^{(1)},p^{(-1)}\) satisfying the assumptions of Theorem 1, as well as functions \(s_{t}^{(1)},s_{t}\) satisfying_

\[\| p_{t}^{(1)}+s_{t}^{(1)}\|_{L_{2}(p_{t}^{(1)})}^{2} \| p_{t}+s_{t}\|_{L_{2}(p_{t})}^{2}\]

_such that, if one runs the probability flow ODE with guidance parameter \(w\) but with \( p_{t}^{(1)}\) and \( p_{t}\) replaced by \(s_{t}^{(1)}\) and \(s_{t}\) respectively, then with probability at least \(1-e^{-(w)}\), the resulting sample lies outside of the domain of \(p\)._

In other words, for any level of score estimation error, if one takes the guidance parameter \(w\) to be too large, the sampler will end up going off the support of the data distribution \(p\). Roughly speaking, the idea derives from the proof of Theorem 1. As we will see, one key feature of the guided ODE in the setting of Theorem 1 is that the trajectory first _swings past the edges of the support of \(p\) and into the tails of the noised data distribution \(p_{t}\)_ before returning. As a result, errors in score estimation at these tails can move the sampling process away from the intended trajectory and thus prevent the trajectory from ever returning to the support of \(p\), leading to corrupted outputs. We stress that this phenomenon is not an issue of numerical precision: Theorem 3 applies even if one runs diffusion guidance with infinite precision.

Taking inspiration from our theory, we posit that the optimal choice of guidance (from the perspective of sample quality) for compactly supported distributions that approximately satisfy the assumptions of our theory is the largest possible \(w\) for which the resulting trajectory does not exhibit this behavior of swinging away from the support of the data distribution and returning. Specifically, we propose a rule of thumb for selecting the guidance strength based on looking at a certain _monotonicity_ property of the trajectory and experimentally validate this rule of thumb in both synthetic settings and on image classification datasets. Additionally, for compactly supported distributions that fall outside the scope of Theorem 1, we propose an alternative heuristic based on the ideas of Theorem 3: we should choose the guidance strength as large as possible while still ensuring that final samples are contained within the distribution support. See Section 3 for details.

### Related work

It has been observed previously [15; 20] that the score of the tilted distribution convolved with noise is different from what is used in diffusion guidance, i.e. Eq. (2). These works conclude informally that as a result, diffusion guidance should not be sampling from the tilted distribution. In contrast, our work gives rigorous justification for this and provides a fine-grained analysis of the behavior of diffusion guidance on simple toy examples, shedding new light on several key features of the dynamics of guidance.

To our knowledge, only two prior works have sought to theoretically characterize the behavior of guidance, one by Wu et al.  and one by Bradley and Nakkiran . Here we discuss the connection to these works in detail and briefly summarize some other relevant results.

Comparison to Wu et al. .This previous work studied the effect of the guidance parameter \(w\) when sampling Gaussian mixture models. They considered two summary statistics: the "classification confidence" and the "diversity" of the generated output.

The former refers to the conditional likelihood \(p(z x)\), where \(z\) is the index of the component of the Gaussian mixture model to which the sampler is being guided, and \(x\) is the generated output. They prove a comparison result showing that the classification confidence of the output of the guided sampler is at least as high as that of the unguided sampler, and they give some quantitative bounds on how much the former exceeds the latter. In particular, they prove that as \(w\), the classification confidence tends to \(1\).

As for diversity, they show that the differential entropy of the output distribution of the guided sampler is at most that of the unguided sampler, though they do not provide quantitative bounds on the extent to which the entropy decreases with \(w\).

Instead of studying summary statistics of the generated output, we instead give a fine-grained analysis of where exactly the trajectory ends up at different times in the reverse process. While we do not directly study classification confidence, note that in the setting of our main result, Theorem 1, for mixtures of compactly supported product distributions, the statement that classification confidence increases is uninformative because, as mentioned previously, the conditional likelihood for _any point_ in the support of the target class is \(1\). The dynamics that we elucidate in our results can be thought of as a more geometric notion of classification confidence. As for diversity, implicit in our Theorems 1 and 2 are quantitative bounds on how the diversity decreases as \(w\) increases.

Additionally, the analysis of how score estimation error impacts diffusion guidance, as well as our empirical findings on real data, are unique to our work.

Comparison to Bradley and Nakkiran .During the preparation of this manuscript, a very recent theoretical work  also studied the extent to which diffusion guidance fails to sample from the tilted distribution. They provided a simple example where the conditional likelihood \(p(z x)\) is Gaussian (so that the tilted distribution is also Gaussian) and the probability flow ODE with guidance provably does not sample from the correct tilted distribution. Interestingly, they also study the reverse _SDE_ with guidance and show that it behaves differently under this example than under the probability flow ODE with guidance.

They also observed that diffusion guidance is equivalent to a predictor-corrector scheme where the predictor makes a step according to the conditional distribution \(p(x z)\), and the corrector makes a step using Langevin dynamics with respect to the _noised-then-tilted_ distribution.

In addition, they considered the mixture of two Gaussians example that we study here. They provide numerical, but non-rigorous evidence that diffusion guidance results in a very different distribution than the tilted one. In contrast, we provide a rigorous analysis of the dynamics proving that this is the case. On the other hand, to our knowledge, the example of a mixture of distributions with compact support has not been considered previously, and the qualitative difference in the behavior of guidance in this setting versus under the mixture of Gaussians setting has not been reported in the literature.

Sampling guarantees for diffusion models.Most of the theoretical literature on diffusion models has focused on _unconditional_ sampling, e.g., proving that SDE diffusion models can efficiently sample from essentially arbitrary data distributions assuming \(L^{2}\)-accurate score estimation [21; 7; 5; 4; 1; 9]. Note the notion of \(L^{2}\) error matches the objective function used in practice. Similar results hold for the ODE under additional smoothness constraints or by using a corrector step [23; 22; 6; 24].

We also mention various recent works on understanding other aspects of diffusion models using mixture models, including provable score estimation [30; 10; 8; 17] and feature emergence .

Finally, an unrelated work that touches upon guidance and conditional generation is that of . They give representational bounds on how well conditional score functions can be approximated by ReLU networks in nonparametric settings, which translate to sample complexity bounds for conditional score estimation. In their work, "classifier-free guidance" does not refer to the sampling process that we focus on (indeed, they take guidance parameter \(w=0\) so that the tilted distribution is simply the conditional distribution \(p(x z)\)). Instead, it refers to the _training_ of a neural network that simultaneously parametrizes the unconditional and conditional scores.

## 2 Preliminaries and proof overview

### Technical preliminaries

Mixture models.We focus on data distributions \(p\) which are uniform mixtures of two constituent distributions, taking the form

\[pp^{(1)}+p^{(-1)}\,. \]

Throughout, we freely conflate probability measures with their densities. Here \(p^{(1)}\) and \(p^{(-1)}\) are meant to represent class-conditional distributions, and \(p\) is meant to represent the unconditional data distribution.

We will denote a sample from \(p\) by the pair \((x,z)\) where \(z\{ 1\}\) specifies the class (\( 1\) with probability \(\)). Given class \(z\), the conditional distribution on \(x\) is given by \(p^{(z)}\).

Probability flow ODE.Here we briefly review some basics on diffusion models, specifically the _probability flow ODE_, tailored to the mixture model setting outlined above. Throughout, let \(t\) be a time variable which varies from \(0\) to some terminal time \(T\), such that the output of the sampling algorithm is the iterate at time \(T\).

To formally introduce the probability flow ODE, we define the parameters \(a_{t}=e^{-T+t},b_{t}=^{2}}\). Let \(p_{t}()\) be the distribution of \((a_{t}x+_{t},z)\) where \(x p\) is sampled from the mixture and \(z\) denotes its class, and \(_{t} N(0,b_{t}^{2})\) is Gaussian noise corresponding to time \(t\) of the backward process. Hence, the marginal \(p_{t}(x)\) is the convolution of the target \(p\) scaled by \(a_{t}\), and \(N(0,b_{t}^{2})\). Denote by \(a_{*}p\) the distribution of \(aX\) where \(X p\). Then the distribution at time \(t\) given \(z\) is given by \(p_{t}(|z=1)=a_{t*}p^{(1)} N(0,b_{t}^{2})\) and \(p_{t}(|z=-1)=a_{t*}p^{(-1)} N(0,b_{t}^{2})\), respectively.

The probability flow ODE with respect to the component \(p^{(1)}\) is given by

\[x^{}(t)=x(t)+ p_{t}(x(t)|z=1)\,, \]and analogously for the other component. This ODE has the property that if \(x(0)\) is distributed as a sample from \(a_{T*}p^{(1)} N(0,b_{T}^{2})\), then \(x(t)\) is a sample from \(a_{(T-t)*}p^{(1)} N(0,b_{T-t}^{2})\).

Guidance.Our goal is to understand the effect of introducing _guidance_ into the probability flow ODE (5). Given guidance parameter \(w\), the resulting guided ODE is given by

\[x^{}(t) =x(t)+ p_{t}(x(t))+(w+1) p_{t}(z|x(t)) \] \[=x(t)+(w+1) p_{t}(x(t)|z)-w p_{t}(x(t))\,, \]

where in the second step we used Bayes' rule. We will sometimes refer informally to the position of \(x(t)\) (or time-reparametrizations thereof) as a _particle_.

Note that when \(w=0\), this is identical to the vanilla probability flow ODE in Eq. (5) for \(z=1\). When \(w=-1\), then this is identical to the probability flow ODE for the _unconditional distribution_\(p\). Our goal in this work is to understand the behavior of the guided ODE for general \(w\), especially large \(w\). In particular, as noted at the outset, the "guided score" term \((w+1) p_{t}(x(t)|z)-w p_{t}(x(t))\) in Eq. (7) does not correspond to the score function of the tilted distribution convolved with noise, so it is not _a priori_ clear what the distribution over the final iterate \(x(T)\) actually is.

### Intuition for our characterization of the dynamics of guidance

Having formalized the probability flow ODE with guidance, we now provide a high-level overview of our proofs by presenting general intuition for the effect of guidance. The behavior of the guided ODE in the setting of mixtures of compactly supported distributions is the richest, so we focus on illustrating the proof of Theorem 1. In that setting, roughly speaking, we will show that there are three distinct regimes for the evolution of the guided ODE, depending on how the posterior probabilites \(p_{t}(z=-1|x(t))\) and \(p_{t}(z=1|x(t))\) relate to each other.

First, when the posterior probability \(p_{t}(z=-1|x(t))\) is much larger than \(p_{t}(z=1|x(t))\), then the score function of the convolved mixture model is dominated by the score of \(p^{(-1)}\) convolved with the appropriate Gaussian; in particular, the guided score term in Eq. (7) is almost

\[(w+1) p_{t}(x(t)|z)-w p_{t}(x(t))(2w+1) p_{t}(x (t)|z),\]

i.e. \(x(t)\) gets pushed toward the \(p^{(1)}\) component with maximum velocity.

The second case is when the posterior probabilities \(p_{t}(z=1|x(t))\) and \(p_{t}(z=-1|x(t))\) are approximately equal. In this case, the score of the convolved mixture is almost zero since the influences from \(p^{(1)}\) and \(p^{(-1)}\) cancel each other out. Hence, the guided score term in (7) roughly becomes

\[(w+1) p_{t}(x(t)|z)-w p_{t}(x(t))(w+1) p_ {t}(x(t)|z).\]

We can see that here \(x(t)\) will still converge to the right component with high velocity proportional to \(w+1\) in this regime.

Finally the third regime is when \(p_{t}(z=1|x(t))>p_{t}(z=-1|x(t))\), i.e. \(x(t)\) is "closer" to \(p^{(1)}\). Then the RHS roughly becomes

\[(w+1) p_{t}(x(t)|z)-w p_{t}(x(t)) p_{t}(x (t)|z).\]

In this case \(x(t)\) converges to the right component with minimum speed, i.e. proportional to a constant independent of \(w\).

Overall, we observe the behavior that when the particle is close to the wrong component, guidance adds more biasing on it to repel it from that component toward the correct one, whereas when the particle is closer to the correct component, it decreases in velocity. This intuitively means that guidance somehow biases the distribution of the correct component to points that are "farther" from the other component, an intuition that we rigorize in Appendices A and B where we prove formal versions of Theorems 1 and 2.

## 3 Experiments

Here we empirically verify the guidance dynamics predicted by Theorems 1 and 2. All experiments in this section were conducted on a single A5000 GPU. We use Jax  for the experiments in Section 3.1 and PyTorch  for all other experiments.

### Synthetic experiments

We first revisit the distribution used in Figure 1 (mixture of uniforms), and then consider the case of mixture of Gaussians in Appendix C.1. The distribution in Figure 1 is constructed by taking \(p^{(z)}\) to be \(([-1/2,1/2][-1/2,1/2])\) shifted by \(2z\) in the \(x\)-coordinate. Note that although this is a 2-D distribution, the distributions \(p^{(1)}\) and \(p^{(-1)}\) can be written as \(p_{1}^{(z)} q\) with \(q\) shared, which does not affect the dynamics of Theorem 1. We demonstrated in Figure 1 how sampling with a larger guidance parameter yields a distribution of samples that is more concentrated than the true conditional distribution of the data. We now examine the ODE dynamics that produced these samples and compare them to the dynamics predicted by our theory.

We generate samples using guidance by numerically solving the guided probability flow ODE (7) using the Dormand-Prince method  as implemented in JAX . For solving, we use \(1000\) evaluation steps and take \(T=10\), which we is sufficiently large based on the stipulations of Theorem 1. For obtaining the unconditional and conditional scores necessary for the ODE, it is straightforward to write down exact expressions for this case (which consist of integrals that we can numerically approximate). However, we estimate the scores using a more general approach that can be effectively applied to any mixture distribution for which we can sample both conditionally and unconditionally from.

For brevity, let \(A_{t}\) follow the distribution of \(a_{t}X\), where \(a_{t}\) is defined as before and \(X p\) (the mixture distribution). Similarly, let \(A_{t,z}\) follow the conditional distribution \(a_{t}X z\). Lastly, letting \(p_{b_{t}}\) denote the density of \(_{t}\), we have the following expressions for the scores:

\[ p_{t}(x) =_{A_{t}}[ p_{b_{t}}(x-A_{t})]}{_{A_{t}}[p_{b_{t}}(x-A_{t})]}, \] \[=-_{A_{t}}[^{2}}p_{b_{t}}(x -A_{t})(x-A_{t})]}{_{A_{t}}[p_{b_{t}}(x-A_{t})]},\] (9) \[ p_{t}(x z) =_{A_{t,z}}[ p_{b_{t}}(x-A_{t,z})]}{ _{A_{t,z}}[p_{b_{t}}(x-A_{t,z})]}. \]

Both (8) and (10) follow from rewriting the convolutions as expectations and then using dominated convergence to pass the gradient into the expectations. Using the above, we can compute the scores by standard Monte-Carlo.

We use this ODE solving procedure to generate 500 samples from the conditional distribution \(p(x z=+1)\) with varying levels of guidance. For each generated sample, we project the computed ODE trajectory on to the \(x\)-coordinate (as this is the coordinate handled by our theory in this case).

Theorem 1 suggests that as we increase the guidance parameter \(w\), the ODE dynamics will push samples farther and farther in the direction of the guided class support before ultimately pulling them back to the support if necessary (i.e. \(w\) is large). As we show in Theorem 3, this behavior can be undesirable if the sampling trajectory moves too far away from the desired class support, as it can amplify score estimation errors and lead to issues in the fidelity of the final produced samples.

We can thus intuitively think of increasing the guidance parameter as not only trading off diversity and sample quality, but also trading off stability with sample concentration. This observation indicates that there should be some range of guidance values that allow for the sampling concentration effect while not entering the unstable regime; i.e. those guidance values that do not lead to sampling trajectories that move far away from the guided class support.

To verify this, we plot the mean of the projected ODE trajectories for increasing guidance parameter values alongside the final produced samples from each trajectory in Figure 2. Since large choices of the guidance parameter lead to some trajectories diverging due to numerical instability/score approximation errors, we visualize only the samples and trajectories that were "good" in that they produced final samples constrained within the guided class support (and we indicate this proportion on the plots). The results show that the projected trajectories do indeed follow the predicted dynamics, with larger choices of \(w\) leading to a pronounced pullback towards the end of the trajectories.

Furthermore, as suggested earlier, the qualitatively best choices of \(w\) appear to correspond to trajectories that do not (significantly) exhibit this pullback effect. In our case, \(w=3\) exhibits thesharpest sample concentration while having significantly better sample fidelity when compared to higher guidance values.

### Approximately separable image data

While the synthetic experiments serve to verify our theory, they obviously do not constitute a practical setting in which guidance is used. The most popular use case for guidance in the literature is sampling from image data, and indeed this is what motivated our investigation of guidance for distributions with compact support in the first place.

However, typical image datasets used in the diffusion literature such as ImageNet  are known to not be linearly separable, and therefore cannot fall under the exact conditions of Theorem 1. That being said, simpler image datasets are known to be close to linearly separable - in particular, MNIST.

We thus consider using the classifier-free guidance formulation (which corresponds to the second equality in (1)) of  to conditionally sample from MNIST with guidance. We use the open-source classifier-free guidance implementation of  designed for MNIST.

Although MNIST is perhaps the simplest generative image modeling testbed, it still presents a significant increase in complexity from the synthetic setting. Firstly, compared to the experiments of Section 3.1 and the setting of our theory, we are no longer considering only two classes. Furthermore, there is no guarantee that the class supports are well-separated, or even disjoint. Even more worrying, we do not have access to approximations of the true score functions of the conditional distributions that are guaranteed to be close as in (8) and (10); we have to instead learn a model-based score.

We address the multi-class issue by using the standard one-vs-all reduction. In particular, we fix a single class as the positive class \(y=+1\), and then let the union of all other classes represent the negative class \(y=-1\). We note that after this reduction, the distribution is close to linearly separable, and we are thus at least close in spirit to maintaining the separation from Theorem 1 under an appropriate basis.

To obtain a projection direction for the sampling dynamics analogous to what was done in Section 3.1, we generate 100 samples from the positive and negative classes using a guidance of \(w=0\), to approximate sampling from the conditional distributions. We then let the projection direction be the difference between the two sample means. For sampling, we use DDPM  with 400 time steps and a linear noise schedule, and we found that training the guidance model of  for 40 epochs was sufficient to generate high quality samples.

Figure 3 shows the mean projected sampling trajectories alongside the final produced samples for the same choices of guidance parameters used in Figure 2 and the positive class fixed to be the digit 0. We observe the same phenomenon as before: after the guidance parameter \(w\) is taken to be sufficiently large, there is a pullback effect in the projected sampling dynamics. Furthermore, once again as before we note that the qualitatively best choice of \(w\) (again \(w=3\)) is the largest choice for which

Figure 2: Final samples and mean sampling trajectories produced from solving the probability flow ODE guided towards \(y=+1\) in the distribution of Figure 1. The proportion of good samples (i.e. those that were correctly in the class support) is shown with each sample plot, and a 1 standard deviation band is shown around each mean trajectory.

we can preserve monotonicity of the projected sampling dynamics. These results are not sensitive to the choice of positive class; we show similar plots for every other choice of positive class (i.e. all the non-zero digits) in Appendix C.2. Interestingly, for almost any choice of positive class used in the reduction, the qualitatively optimal choice of guidance amongst the values we consider remains roughly the same.

### ImageNet experiments

Although we previously mentioned that experiments on more complicated datasets such as ImageNet are outside the scope of Theorem 1, we show in this section that it is still possible to make qualitative guidance recommendations in such settings based on the ideas of Theorem 3. The idea is that as we scale the guidance parameter \(w\) to be large, we start to obtain samples that are no longer within the original data distribution support due to amplification of score/precision errors.

To conduct experiments on ImageNet, we use the _classifier-guided_ ImageNet models available from . This is due to the fact that there are no classifier-free guidance models available from . The classifier-guidance formulation corresponds to the first equality in (1). To be consistent with the notation in  and to also clearly distinguish the classifier-guided setting from the classifier-free setting of Section 3.2, we will use \(s=1+w\) throughout the experiments in this section.

First, we illustrate that the behavior exhibited in Figure 3 no longer holds when running diffusion with guidance on ImageNet, at least using the same experimental setup as before. To parallel the experiments of Section 3.2, we use the \(256 256\)_conditional_ diffusion model released by  to generate samples from a fixed ImageNet class (corresponding to \(y=+1\) as before), and then use the same model to generate samples from all other classes (corresponding to \(y=-1\)). We generate 50 samples from the positive and negative classes (due to the cost of sampling at this resolution and the overhead of storing the entire sampling trajectories), and then compute the normalized direction between the two sample means as before.4 For sampling, we use DDIM  with 25 steps, once again because storing the entire sampling trajectories using DDPM with a large number of steps is prohibitive.

For sampling with guidance, we use the _unconditional_ diffusion model of  with the \(256 256\) ImageNet classifier also released by . Note here that  combined diffusion guidance with their conditional model for their best results, but this does not fall in to the formulation of (1) and so we use the unconditional model. We use DDIM with 25 steps for the guidance samples as well.

Figure 4 shows the final produced samples alongside the mean projected trajectories for an arbitrarily fixed positive class as in the experiments of Section 3.2. We see that even for extreme guidance scales \(s=25\) the previously observed non-monotonicity phenomenon in the projected trajectories no longer

Figure 3: Final samples and mean projected trajectories produced from sampling using the classifier-free guidance model of . For the positive class, we fix the digit to be 0, and the negative class corresponds to all other digits. Each row of samples from top to bottom corresponds to increasing guidance values.

occurs. We suspect this can largely be attributed to the fact that the class supports are no longer close to separated, and as a result the direction corresponding to the difference in sample means is no longer a direction for which we can expect the dynamics of Theorem 1 (in fact, we can expect that there is no such direction along which these dynamics occur since the data is not linearly separable even after reducing to two classes). However, we note that as we increase the guidance strength, the final sample correlation along this mean difference direction continues to increase, more akin to the result of Theorem 2.

In tandem with this increasing correlation, we also observe an increase in the mean "support error" of the final samples, which is overlain on to the trajectory plots in Figure 4. This error is computed by taking the mean absolute deviation of every dimension of the final produced samples from the range of valid RGB values \(\); dimensions that are outside of this range are truncated so as to form valid images. We find that, at least qualitatively, the largest guidance value (\(s=5\)) for which we have no support error seems to perform the best, as taking guidance values larger seems to introduce various visual idiosyncrasies and taking guidance small leads to insufficient concentration on the correct class (as we are guiding an unconditional diffusion model).

We verify that these observations hold for a number of different choices of the positive class; these experiments, along with further discussion of limitations of our experimental setup, are available in Appendix C. We emphasize again that this is merely a minimal demonstration of a possibly useful heuristic, and once again point out that this setting is outside the scope of our theory. Still, an interesting direction for future work could be to run more comprehensive experiments regarding this heuristic (and other heuristics in this section) - such experiments were outside the scope of our available compute resources.

## 4 Conclusion

In this work we gave the first fine-grained analysis of the dynamics of the probability flow ODE with guidance, focusing on two toy settings involving mixture models in one dimension. Our key finding was that not only does the guided ODE fail to sample from the tilted distribution that originally motivated the formulation of guidance, but in fact the guided ODE implicitly leverages geometric information about the data distribution even if such information is absent in the classifier being used for guidance.

Our results open up a number of interesting follow-up directions. For example, our guarantees are restricted to one-dimensional settings, and it would be useful to obtain analogous guarantees for non-trivial high-dimensional settings such as mixtures of bounded densities over disjoint convex bodies. Additionally, we have made no effort to optimize the choice of \(w\) in our theoretical guarantees, and it would be interesting to see how small one can take \(w\) in theory while still obtain the behavior in our results.

Figure 4: Final samples and mean projected trajectories produced from sampling using the classifier-guided ImageNet diffusion model of . The positive class here is taken to be 292 (tiger). As before, each row of samples from top to bottom corresponds to increasing guidance values.