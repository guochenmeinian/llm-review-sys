# Thought of Search: Planning with Language Models Through The Lens of Efficiency

Michael Katz

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

michael.katz1@ibm.com &Harsha Kokel

IBM Almaden Research Center

San Jose, CA 95120

harsha.kokel@ibm.com &Kavitha Srinivas

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

kavitha.srinivas@ibm.com &Shirin Sohrabi

IBM T. J. Watson Research Center

Yorktown Heights, NY 10598

sohrab@us.ibm.com

###### Abstract

Among the most important properties of algorithms investigated in computer science are soundness, completeness, and complexity. These properties, however, are rarely analyzed for the vast collection of recently proposed methods for planning with large language models. In this work, we alleviate this gap. We analyse these properties of using LLMs for planning and highlight that recent trends abandon both soundness and completeness for the sake of inefficiency. We propose a significantly more efficient approach that can, at the same time, maintain both soundness and completeness. We exemplify on four representative search problems, comparing to the LLM-based solutions from the literature that attempt to solve these problems. We show that by using LLMs to produce the code for the search components we can solve the entire datasets with 100% accuracy with only a few calls to the LLM. In contrast, the compared approaches require hundreds of thousands of calls and achieve significantly lower accuracy. We argue for a responsible use of compute resources; urging research community to investigate sound and complete LLM-based approaches that uphold efficiency.

## 1 Introduction

Recent work has addressed the issue of planning in Large Language Models (LLMs), spurred by their abilities in natural language tasks. The approaches vary widely from giving a planning problem to an LLM and asking it to output an entire plan to asking an LLM to plan step by step, including backtracking. Roughly, these approaches can be partitioned into two sets. The first exemplifies yet another capability of language models, while the second aims at presenting practical tools for solving planning problems. In the latter case, however, since planning problems are often computationally hard, it is crucial to understand the properties and the complexity of the algorithms proposed.

The purpose of our work is precisely that. Going over a collection of recent methods for planning with large language models, we analyse the most important properties of the proposed methods such as soundness and completeness. We find all these methods to be neither sound nor complete. We further investigate the computational efficiency of these methods in terms of the number of invocations of their most expensive routine - LLM evaluation. From a pragmatic perspective, the cost of LLM evaluations is significant, either in terms of GPU resources, or expensive API calls to hosted LLMs. We find that abandoning the soundness and completeness does not provide any benefitin computational efficiency, as the proposed methods are prohibitively inefficient, expensive, and most importantly harming the environment.

We propose an alternative named Thought of Search: thinking **before** searching, an approach that exploits the strengths of LLM, while mitigates the weaknesses of the existing approaches, doing so in an efficient manner. We propose using the language models for deriving the symbolically represented search components that allow for performing the search itself without calling LLMs. That way, the search space correctness can be checked before the search is performed, allowing for soundness and completeness (and sometimes optimality) of the search algorithms imply these properties of the overall solution. Specifically, we focus on the two essential components of any search, successor generator and goal test, using the large language models to obtain their implementation in Python. We exemplify our proposed approach on four representative search problems, comparing to the LLM-based solutions from the literature that attempt to solve these problems and show these approaches to be prohibitively expensive. We show that by using LLMs, possibly with human feedback, to produce the code for the search components we can solve the entire datasets with 100% accuracy with only a few calls to the LLM. We argue for a responsible use of compute resources; urging research community to investigate sound and complete LLM-based approaches that uphold efficiency.

## 2 Properties and Complexity of Planning using LLMs

In this section, we review the existing methods for planning with language models from the point of view of the soundness and completeness properties of the proposed algorithms, as well as their _LM Evaluation Complexity_. We quantify the number of LLM requests required by each algorithm. An algorithm is sound if it produces only valid solutions and it is complete if it is guaranteed to produce a solution before terminating successfully. For consistency, we unify the notation here. In what follows, we denote the bound on the number of successors generated per state by \(b\), the number of rollouts by \(T\), and the length of a rollout/path by \(L\).

IoThe Input-Output (IO) prompting is the straightforward use of LLMs to generate an output for a given input. Some examples that employ IO for planning include querying a pre-trained model  or fine-tune a model [17; 5]. Here, the model is usually evaluated once, generating an output that may include a sequence of steps, making the complexity of this approach \(O(1)\). The method is neither sound nor complete for planning, as it can generate incorrect solutions and not guaranteed to provide a solution if one exists. Further, the methods that fine-tune a model can have a computationally intensive step of data generation.

CoTThe Chain-of-Thought approach  prompts the model with a predefined example of a chain of steps (thoughts) to resolve the question, in an attempt to make the model generate similar chains in its output. Here, still the model is evaluated once, so the complexity is \(O(1)\) and the approach is still neither sound nor complete for planning, for the same reasons as before.

ReActThe ReAct approach  aims at interleaving CoT with acting steps. Each acting step may result in a separate evaluation of the language model. Therefore, the number of evaluations is worst case linear in the number of steps \(L\) in the expected output, \(O(L)\). This approach is akin to re-planning at each step; or treating LLM as a policy, referenced for each state in the trajectory. Still, no guarantees of soundness or completeness can be obtained for this approach.

ReWOoReWOO aims at tackling the inefficiency in the number of LLM evaluations, requesting the first model evaluation to plan all future steps, then each step is executed without model evaluation if possible (and with, if not), finalizing by a model evaluation with the concatenated output of previous states as the input. The best case complexity is therefore \(O(1)\), with only two evaluations performed and when external tools are not LLM. The worst case complexity is however still \(O(L)\), where \(L\) is the plan's length, since each of the \(L\) external tool calls can be to an LLM. Same as before, no guarantees of soundness or completeness can be obtained.

RapReasoning via Planning (RAP) approach performs a search (e.g., MCTS) using the LLM for expansion (generate successors) and for heuristic or reward prediction of a state . Here, the complexity is the worst among the approaches explored so far, being linear in the size of the search space. MCTS is an incomplete algorithm, where the search tree size can be controlled by bounding the number of successors generated per state \(b\), number of rollouts \(T\), and their depth \(L\). The overall complexity is \(O(T b L)\). Since the LLM is used for generating successors, it can generate incorrect successors, making the approach also not sound.

ToTTree of Thoughts  approach is similar to that of RAP, where the "thought generator" is expanding a state by calling an LLM and the state evaluator is calling an LLM to evaluate the generated states. The evaluation can be done per state (one call per state), or across states (one call across all the current generated states). Then, a search is performed on the search tree. The worse case overall complexity is \(O(N)\), where \(N\) is the number of states generated by the search algorithm. The authors use bounded incomplete versions of the well-known Breadth-First Search (BFS) and Depth-First Search (DFS) algorithms, with the depth bound \(L\), branching bound \(b\), and beam size \(m\), restricting the complexity to \(O(b m L)\). Here as well, the successor generation by LLM eliminates soundness of the otherwise sound algorithms.

GoTGraph of Thoughts (GoT)  is similar to the tree of thought approach, except that it can connect the thought nodes in the tree above into a graph for more flexible representations through (a) aggregation of thoughts \(a\), (b) improvement of a thought to refine it further \(i\), (c) generation of a thought based on existing ones \(g\). In GoT, in addition, repeat operation \(k\) defines how often a particular operation is repeated. The complexity is similar to that of the ToT, \(O(N)\), where \(N\) is the number of nodes in the graph generated, in practice bounded by the branching bound \(b\) and the depth bound \(L\), resulting in the overall complexity of \(O(b L)\). Here as well, there are no guarantees of soundness or completeness, as the graph is constructed with the language model.

ReflexionReflexion  is a feedback incorporated approach where in addition to a so-called actor model, which can be either a CoT or a ReAct approach, it has an internal and external feedback component, named an evaluator model and a self-reflection model. The feedback is given within a bounded number of trials and a bounded memory that provides additional context to the agent. The number of evaluations is dependent on which actor model is used plus a constant (two evaluations, one for the evaluator model and one for the self-reflection model) times the number of trials, \(T\). So its either \(O(T)\) if CoT is used, or \(O(L T)\) if ReAct is used. While the authors report \(100\%\) accuracy given a large enough \(T\) in some domains, the approach is not a complete approach. Similar to the other cases, the approach is also not sound.

LatsSimilarly to , Language Agent Tree Search (LATS)  performs a Monte-Carlo Tree Search (MCTS), using language models as algorithm components. Using our notation of \(b\) for the number of successors generated per state, \(T\) for the number of rollouts, and \(L\) for their depth, the algorithm complexity here as well is \(O(T b L)\). The authors note that in practice, however, their method produce more calls to the LLM than RAP, exploring larger portions of the search space and producing somewhat higher accuracy solutions. As mentioned before, MCTS is already an incomplete algorithm by itself, and LATS limits it further. Since the successors here as well are generated by a language model, the approach is not sound.

AoTAlgorithm of Thoughts (AoT)  combines the CoT and the ToT approach in a following sense: rather than a standard CoT prompting, it incorporates the search strategy (DFS or BFS) within the prompt through the in-context examples. That is they use a standard DFS or BFS to produce the in-context examples. The approach is neither sound nor complete, as it relies completely on the language model. Further, it assumes an existence of a search component that can solve the in-context examples. If such component already exists, the only possible reason to use this approach would be if the search component is not efficient, which would imply that the problems are too large to be handled by BFS/DFS (e.g., above \(10^{12}\) states). But then, these search spaces would certainly be beyond any imaginable future capabilities of LLMs.

While technically AoT makes one call to the LLM per problem instance, the input and output number of tokens grows exponentially with the instance size. Therefore it would be misleading to say that the complexity of this approach is \(O(1)\). From the cost perspective, when charged based on the number of input and output tokens, it might well be more expensive than the other approaches. Since we do not have an adequate uniform way of transforming this single call to multiple calls of similar size to other approaches, we skip this approach in our analysis.

LLM-Modulo FrameworkThe framework suggests to perform planning by a so-called Generate-Test-Critique loop . The LLM is generating candidate plans, which are validated/critiqued by external means, providing feedback to the LLM. These means are assumed to be sound, thus the overall framework is claimed to inherit soundness guarantee. It is not mentioned, however, how to obtain such means of sound validation or critique for cases where no symbolic model already exists. Once critiques deemed a plan valid, the algorithm stops. Otherwise, next candidate is generated. There is no guarantee of convergence towards a valid plan and therefore the algorithm is not guaranteed to terminate. This can be fixed by employing a bound on the number of iterations. This, however, does not provide completeness guarantees.

Policy generation with LLMsFocusing on problems where no search is required (e.g., polynomial cases) and a general policy (or a generalized plan) exists, the authors propose querying LLMs to provide a policy implementation in python directly . The policy is then checked on a few held out problem instances from the domain, providing a feedback on problems that are not solved by the policy. The number of calls to the LLM is therefore \(O(1)\)**per domain**, and the result can be amortised among all the problem instances of that domain. This approach, while tackling the simpler case of problems where no search is needed, has served as an inspiration for our proposed approach.

## 3 Proposed Approach

In this work, we argue for an efficient and responsible use of compute resources. First and foremost, this means using the computationally expensive large language models efficiently and appropriately. In the case of solving search problems, as in the focus of this work, we argue against using an LLM at each expansion and evaluation. Such an implementation of search algorithms is inefficient, but equally importantly, it also sacrifices important properties that search algorithms possess, such as soundness and completeness. While completeness is often sacrificed knowingly, in order to limit the number of possible calls to the language model, soundness can be lost unintentionally. If the state successors are generated by a large language model, there is no guarantee that the produced successors are _valid_. An additional mechanism that validates the produced output would be required to render the algorithm sound. Such a mechanism would be symbolic by nature, since it must guarantee correctness. However, if such mechanism exists, it may be possible to use that mechanism to produce successors, without the need for performing the calls to a large language model at each evaluation. The large language models would be a prime candidate for producing such a mechanism. The mechanism can be a symbolic planning model, e.g., Planning Domain Definition Language (PDDL), from which all the search components can be computed, as is done by classical planners that perform heuristic search . Alternatively, large language models can also directly produce the search components code: a successor function, a goal test, and even a heuristic function  or reward. While the former approach has been explored in the literature [6; 16], here we focus on the latter. We propose to use large language models for obtaining a Python implementation of two critical search components, _successor functions_ and _goal test_. We query the language model for each component separately, using a textual description of the problem at hand. We assume the process to be iterative and user-guided, by a person capable of validating the code obtained, leaving automation of the process for future work. It is worth noting that if the expensive evaluation of large language models is not performed during search, there is no need to artificially restrict the algorithms to their incomplete variants. Still, the code must be validated for soundness, ensuring that all produced successors are correct, as well as completeness, ensuring that all possible immediate successors are produced.

## 4 Experiments

We exemplify the proposed approach with a variety of popular search problems, demonstrating the use of large language models to obtain the search components code for a representative set of the same search problems that the vast recent work on planning using LLMs used. Our intention is to exemplify the efficiency and accuracy gap created by the unsupervised use of large language models at every search step. For each search problem, we count the number of interactions with the language model to obtain valid1 successor function and goal test implementations. We then run a standard implementation of either a BFS or a DFS algorithm with these two functions on a set of instances_User:_ The 24 Game is a mathematical card game in which the objective is to find a way to manipulate four integers so that the end result is 24. The game is played with a list of four numbers, and the player must use all four numbers exactly one, using any combination of addition, subtraction, multiplication, or division, to arrive at the number 24. If a state is defined by a list of numbers (4 or less), how are successor states defined? Provide only the python code that returns a list of successor states for a state.

_CutGPT:_ Below is the Python code that defines how to generate the successor states for a given state in the 24 Game. A state in the 24 Game is a list of numbers. The successor states are generated by taking two numbers from the state, applying an operation (addition, subtraction, multiplication, or division if the divisor isn'zero), and then forming a new list with the result and the remaining unused numbers.

``` importiretools defgenerate_successors_states(state): n=len(state) for(i,j)initertools.combinations(range(n),2): a,b=state[i],state[j] remaining=[state[k]forkinrange(n)ifk!=iandk!=j] #Generateallpossiblenewstates successors.append(remaining+[a+b]) successors.append(remaining+[a+b]) ifa>b: successors.append(remaining+[a-b]) else: successors.append(remaining+[b-a]) ifb!=0: successors.append(remaining+[a/b]) ifa!=0: successors.append(remaining+[b/a]) returnsuccessors ```

This function takes a state (a list of numbers), iterates over all pairs of numbers, applies the operations to these pairs, and then forms new lists with the result of the operation and the other numbers that were not used in the operation. This ensures that each successor state is a valid step towards finding a solution to reach 24, using all four numbers exactly once along with any of the allowed operations.

_User:_ Provide only the python code that tests whether a state is a goal state.

_CutGPT:_ Here's the Python code that tests whether a given state is a goal state in the 24 Game. A goal state is achieved if the list contains exactly one number, and that number is 24.

``` defis_goal_state(state): returnlen(state)==1andstate==24 ```

This function checks if the length of the state list is 1 and whether the sole element in the list is the number 24.

Listing 1: **24 game** example correspondence to obtain the successor function and the goal test.

The 24 game example correspondence to obtain the successor function and the goal test.

and report the accuracy and the total time to solve the instances. We repeat the experiment 5 times, obtaining 5 implementation variants and compare the efficiency of these variants in terms of total time. Note that if the implementation of successor function and goal test obtained from the large language model is correct, the accuracy is guaranteed to be 100% (if sufficient resources are given to the search). The opposite is not necessarily true, as the set of tested instances might not include some corner cases that make the code perform incorrectly. However, we see that as one of the strengths of our approach, as such cases can be used for testing and debugging the obtained code. Under the assumption of correctness of the successor function and goal test implementations, our approach inherits the soundness and completeness guarantees of the search algorithms used . In all our experiments we use the GPT-4 model , in a chat mode. The code obtained from GPT-4 in all 5 correspondences is provided in the appendix. The search with the obtained code was run locally, on a machine with 2.3 GHz 8-Core Intel Core i9 CPU, no GPUs were used.

24 gameThe 24 game  takes 4 integers as an input that can be manipulated through the four most common arithmetic operations: addition, subtraction, multiplication, and division. The goal of the game is to produce a formula (also representable by a sequence of operations) that evaluates to 24, if one exists. As mentioned before,  achieve that with an incomplete algorithm that performs \( 100\) calls to the LLM, according to their report. The authors report a success rate of \(75\%\) on a subset of 100 medium difficulty problems out of a collection of 1362 problems .

As mentioned before, we propose instead to use the LLM to produce the code for two functions: _successor function_ and _goal test_. We leave the heuristic function to future work, noting that even without it, the search space of the 24 game is rather small, up to \(1+36+648+3888=4573\) states. Listing 1 describes an example correspondence with the LLM. We observe that the model required 1.2 interactions on average for the successor function and 1 interaction for the goal test. We see that, with a minor help from the user, the model is able to produce correct successor and goal test functions for this problem. We further tested all 5 successor functions and goal test within a simple BFS implementation on the entire suite of 1362 instances from ToT . For each of the 5 tested successor functions, BFS found a solution in the 1361 cases where a solution exists and report that no solution exists in the only one unsolvable case, a 100% success rate. The total time to solve all 1362 problems varies over these 5 cases from 1.92s to 6.83s in our naive BFS implementation, hinting that some successor functions can be more efficient than other. This is comparable to a **single** LLM evaluation time; which is \( 7s\) for GPT-4 Chat . Note that the generated successor functions are generic enough to be able to solve the generalized version of the 24game, Countdown , with only minimal adaptation to the goal test.

Mini crosswordsThe mini crosswords  is a 5x5 crosswords dataset that includes 20 games, where the input describes the 5 horizontal and 5 vertical clues and the output is the full 25 letters board. We used GPT-4 to produce the two functions: _successor function_ and the _goal test_ and repeated the experiment 5 times. The correspondence with the model can be found in the appendix. We observe that the model required 2.4 interactions on average to produce a valid successor function, and 1.4 interactions on average to produce the goal test, with errors primarily related to not handling the corner cases of incorrect input. In all cases, after providing the exact error to the model, it added safeguards which fixed the issues. We tested the obtained functions within a standard implementation of a DFS with a closed list on the entire suite of 20 games . As our focus in this work is on the search aspects, we assume that each clue in each crossword has 10 variants of possible answers, including the correct one. All 20 games were solved by all five generated pairs of implementation of successor function and goal test (100% accuracy), with a total time for all 20 games varying from 5.5s to 346s, yet again signifying the importance of efficient implementation of the successor function.

To compare to the ToT approach, with the bound of \(100\) on state expansions, it calls the LLM \( 200\) times in the worst case for each of the 20 games. To be fair to the ToT approach, it does not assume the availability of possible answers, deriving the candidates, at each step, with the help of the language model. This, however is a mixed blessing, since the language model does not provide a guarantee of including the correct answer among produced variants.

BlocksWorldProbably the most famous planning domain is BlocksWorld, where the blocks can be picked up from the table, put down on the table, unstacked from other blocks or stacked on other blocks to transform the initial configuration to a given goal configuration. The domain has a known planning model, described in PDDL and it is one of the tasks considered by the reasoning-as-planning approach . As in the previous cases, we use the GPT-4 model to obtain the successor function and the goal test implementation in Python. We use the same textual description of the domain as . A correct successor function and goal test are obtained after 2.8 and 1 iterations on average, respectively.

The mistakes GPT-4 makes when producing the code repeat from one experiment to another, and are often easy to fix inline, without additional iterations (e.g., using shallow copy when deep copy is needed). In our experiments, however, we did count these as additional iterations. In order to evaluate the obtained functions, we used them within a standard implementation of a BFS and experimented with the collection of 502 instances from . All 502 tasks were solved by all five generated pairs of successor function and goal test (100% accuracy), with the total time for all 502 tasks varying from 0.56s to 9.7s. The more time efficient approaches represented a state as a set of strings representing boolean variables (or logical predicates), while the less efficient representation used dictionaries of lists of objects or pairs of objects with predicates as keys. The simpler state representation also resulted in a more efficient duplicate detection in our rather naive implementation.

Note that the accumulated number of expanded2 and generated states in the most efficient case (with duplicate detection) was 50143 and 129408, respectively. If we needed to call the GPT-4 model on each expansion and generation, by the most conservative estimation, it would take approximately 14 days and cost over \(\$1000\), while not guaranteeing correctness of the outcome. Note that the instances considered are quite small - they have only 4 to 5 blocks. Larger BlocksWorld instances would require significantly more resources. This is true for the proposed approach as well, and larger instances mean larger state spaces, becoming too large for an uninformed search such as BFS. In such cases, a heuristic function may be of help, either, similarly, implemented in Python or automatically derived 

[MISSING_PAGE_FAIL:7]

100% accuracy for both, with the total time for all 4000 questions varying between 2.16s and 2.53s. Listing 2 shows an example correspondence with the model, with non-code text removed for brevity.

## 5 Discussion

It is hard to overstate the importance of the ability to solve search problems and it is natural to solve these problems by exploring some portion of their state space. All the methods we discuss here do that in one way or another. The difference between those approaches is in how big is the portion of the explored state space and what is the cost of exploring that portion. As all the discussed approaches are greedy, unsound, and incomplete, the accuracy of the solutions they provide can only be evaluated experimentally for particular problem and a dataset, and the results do not reflect on other problems or even datasets within the same problem.

In this section we perform a thought experiment. We ignore the accuracy of the approaches and only compare their estimated cost and the portion of the search space they have the potential to explore. For each approach we estimate two values. First, we estimate the number of calls to a language model performed if the approach was run on the four datasets from the previous section, as a proxy for the cost of using the approach. Second, we estimate the number of states explored during search, as a proxy for a portion of the state space size. The actual state space size for these datasets can be calculated precisely, and we provide these values. Note that the number of states explored is an over-estimation for the portion of the state space explored, since the same state can be explored several times. This is true for sound and complete search algorithms as well. These algorithms often employ a duplicate detection mechanism to prevent re-evaluating and re-expanding such states, when such re-expansions are not needed to guarantee algorithm properties (e.g., optimality when admissible but inconsistent heuristics are used). We also provide the actual number of calls to the language model and the number of states explored using our proposed approach. The actual number of calls for our proposed approach is the number of calls that was sufficient for obtaining soundness and completeness (and sometimes optimality). The number of states explored indicated an upper bound on the portion of the states space necessary for exploration.

Datasets sizeWe use the same four representative search problem datasets from previous section. The size of each dataset is denoted by \(D\). For the 24 game, \(D\) is \(1362\), for Crossword it is \(20\), for BlocksWorld it is \(502\), while for PrOntoQA it is \(4000\). We used the datasets provided by the approaches we compare to, but these numbers could also have been significantly larger. For these datasets, we have also computed the size of the search problems in terms of the summed number of states over the instances in the dataset. For the 24 game, the number of states per instance is \(4,573\) and therefore the summed number of states is \(6,228,426\). For Crossword, the number of states is \(_{i=0}^{10}{n i}10^{i}=(10+1)^{10}\) per puzzle, and therefore the summed number of states is \(518,748,492,020\). For BlocksWorld, our dataset includes \(447\) instances with \(4\) blocks and \(55\) instances with \(5\) blocks. The instances with \(4\) blocks have \(125\) states each and the instances with \(5\) blocks have \(866\) states each, giving us the total of \(103,505\) states. Finally, for PrOntoQA, the number of states per task varies between \(12\) and \(54\), with the total summed number being \(97,608\).

Uniform restriction of the search spaceEach approach sets limitations to restrict the number calls to the language model, which makes it difficult to compare. In this thought experiment, we choose to use only the bounds on the breadth/depth of the search and the number of trials, ignoring the absolute constant bounds on the number of explored states used by some approaches. For a fair comparison across the approaches, we will use the same parameter value for all, even if the original work used a different value. We aimed at smaller values among used across the different approaches for each parameter. This results in under-approximation of the number of calls in most cases, and possibly would have resulted in an accuracy that is lower than the one reported by the approaches. In this experiment, however, we ignore the accuracy of the approaches. We use the branching bound \(b=5\), the number of trials \(T=10\), and the beam size \(m=5\). The only parameter we set according to the dataset is \(L\), the bound on the sequence length. This is due to the fact that if \(L\) is not sufficiently large, the algorithms will not be able to find a solution. In the 24 game \(L=3\) is sufficient, while in Mini crosswords it must be at least \(10\). In both BlocksWorld and PrOntoQA the length of a plan/proof varies, with the maximal length over the instances in the datasets in BlocksWorld being \(16\) and in PrOntoQA being \(6\).

AnalysisThe complexity analysis performed in the previous section does not tell the whole story. It hides the constant multiplier, which in some cases could be significant. We also need an additional information about the number of states traversed. Let us take another look at the explored approaches.

Both IO and CoT only call a language model once per instance. Assuming that the output is a plan, it traverses \(L\) states. ReAct and ReWOO (worst case) make exactly \(L\) calls to the LLM, exploring \(L\) states. Both RAP and LATS perform MCTS, but the actual number of calls to the LLM varies. RAP performs \(LT+bLT\) calls (see Algorithm 1 in ), while LATS performs \(2LT+bLT\) calls (see Algorithm 1 in ). The number of states explored in both cases is \(bLT\). Reflection with ReAct performs \((2+L)T\) calls, exploring \(LT\) states. ToT introduced their own variants of well-known BFS and DFS algorithms with bounds on the branching factor, search depth, but also a limit on the open list size (beam size). While their performance can vary greatly on individual tasks, with DFS typically being greedier and therefore faster, in the worst case they both call the LLM \(bmL\) times and explore \(bmL\) states. The GoT approach calls the LLM \(bL\) times, exploring \(bL\) states.

Table 1 shows the estimated numbers according to the parameters values and the analysis above. For comparison, the last row of the table depicts our proposed approach. %States columns denotes the search effort in terms of the number of states explored, possibly with repetitions, described as a percentage of the overall number of states. For our approach, this is the actual value from our experiments. Recall, in the case of PrOntoQA the BFS search is performed once or twice, until the hypothesis or its opposite is proven. For the other approaches, this is an estimate, under the assumptions that the search does not explore the same states multiple times. Clearly, this assumption does not hold in practice, with the methods exploring the same state many times. This is also true for a sound and complete search, albeit to a lower extent, it can generate the same state multiple times. An important property of a sound and complete search is that it generates states _systematically_, and expands them at most once. The number of times the same state is generated tends to be higher in MCTS than in BFS/DFS. Non-systematic successor generation is also a major contributor to that inefficiency. Having in mind that these numbers are very crude over-approximations, we observe that the investigated approaches explore only a very small portion of the search space. This is one of the major sources of their low accuracy - if the solution is not completely included in that explored portion, then there is no way for that solution to be found. In 24 game and crossword, the portion explored tends to be very low, one or two orders of magnitude smaller compared to our method. In BlocksWorld it is comparable to our method, and in PrOntoQA it is often much higher, order of magnitude larger than the size of the state space, indicating that the same states are explored over and over again.

Looking at the overall number of calls, some methods are more expensive than other. Assuming an average of 500 tokens per input3 and 50 tokens per output, according to the current pricing of

  &  &  &  &  \\ Approach & Complexity & States & Calls & States & Calls & States & Calls & States & Calls \\  IO & O(D) & 0.02\% & 1362 & 4e-9\% & 20 & 0.5\% & 502 & 4\% & 4000 \\ CoT & O(D) & 0.02\% & 1362 & 4e-9\% & 20 & 0.5\% & 502 & 4\% & 4000 \\ ReAct & O(LD) & 0.07\% & 4086 & 4e-8\% & 200 & 7.8\% & 8032 & 24.6\% & 24K \\ ReWOO & O(LD) & 0.07\% & 4086 & 4e-8\% & 200 & 7.8\% & 8032 & 24.6\% & 24K \\ RAP & O(TbLD) & 3.3\% & 245K & 2e-6\% & 12K & 388\% & 482K & 1229\% & 1.44M \\ ToT & O(bmLD) & 1.6\% & 102K & 1e-6\% & 5K & 194\% & 201K & 615\% & 600K \\ GoT & O(bLD) & 0.3\% & 20K & 2e-7\% & 1K & 39\% & 40K & 122\% & 120K \\ Reflection & O(LTD) & 0.7\% & 68K & 4e-7\% & 2.4K & 77.6\% & 90K & 245\% & 320K \\ LATS & O(TbLD) & 3.3\% & 286K & 2e-6\% & 14K & 388\% & 562K & 1229\% & 1.68M \\  ToS (ours) & O(1) & 27.0\% & **2.2** & 3e-4\% & **3.8** & 125\% & **3.8** & 175\% & **2.6** \\ 

Table 1: Comparison of various approaches on evaluated tasks. The number of tasks is \(D\), the length bound of the search/rollout/plan is \(L\), number of rollouts is \(T\), beam size is \(m\), and branching bound is \(b\). The summed number of states over the \(D\) tasks and the projected number of LLM evaluations are given per approach and problem.

GPT-4-turbo (the cheaper of GPT-4 models), the overall cost varies from $40 for IO/CoT and $200 for ReAct/ReWOO, to $14, 000 for RAP and $16, 000 for LATS.

## 6 Related work

As the related work on the problem of planning with language models is detailed in Section 2, here we review the work related to our proposed method.

Generation of planning representationOur work proposes to use LLM to generate python code for successor and goal functions. By using these two functions, along with a search algorithm, one can thoroughly explore the entire state space of a problem and compute a plan. Recently, LLMs have also been used to generate PDDL [6; 16; 11; 30], which can be used with existing planners to search for a plan. Of these work, Guan et al.  is quite closely connected to our work as they also use corrective human feedback to generate the PDDL descriptions. While related, our work differs from theirs in three major ways. First, they assume the goal state is provided in the PDDL form. Our work does not make this assumption; rather asks LLM to generate a goal function. Second, in their approach action descriptions are obtained one by one (separate calls to LLMs) along with predicates to use. We provide entire description of the problem and asks for a single successor function. Third, their feedback on the generated PDDL is explicit and requires nuance understanding of modeling in PDDL (e.g., 'unnecessary precondition "no other object stacked on object?x"). While ours is mostly generic python code feedback (e.g., issues with shallow copy of a dictionary with list values, error trace) and generic logic (e.g., 'two of the operations are not symmetric, division and subtraction').

Code generationOur work proposes to use LLMs to generate code that helps solve the search problem, instead of using LLMs to solve the search problem directly. Similar idea has been explored by Zhou et al  for mathematical reasoning problems. In that, they illustrate that LLMs can be used to generate, execute (verify), and self-refine the python code that helps solve the math problem. Their study corroborate our findings that LLMs can be used for generating verifiable code with some feedback. Additional evidence supporting the feasibility of our approach of using LLMs to generate code comes from the recent advancements in LLM-based program synthesis literature [13; 27; 3; 26; 12].

## 7 Conclusions and Future Work

The current trends in planning with large language models focus on performing a search when the search components are realized through the large language models. We analyze the existing approaches and show them to be unsound, incomplete, and quite expensive. We propose to use the large language models to instead generate a code for these search components, to be reused throughout the entire dataset. The significant differences in the use of computational resources and the performance measures clearly demonstrate that LLMs can be effectively employed for planning purposes without compromising on soundness, completeness, efficiency or accuracy.

For future work, we would like to explore the use of large language models to obtain the code for search guidance or search pruning techniques. But more importantly, we would like to relax the need for human feedback in coming up with valid implementations of the search components. This could be done in ways similar to the way a policy is obtained in generalized planning . The first attempt at automating the process was recently made with AutoToS , which replaced the human experts in the loop of feedbacking the language model with an automated feedback, based instead on human-generated unit tests. These unit tests attempt to find evidence of non-soundness and incompleteness and may in turn be automated in the future.