# Taming Local Effects in

Graph-based Spatiotemporal Forecasting

 Andrea Cini 1, Ivan Marisca 1, Daniele Zambon 1, Cesare Alippi 12

1 The Swiss AI Lab IDSIA USI-SUPSI, Universita della Svizzera italiana, 2 Politecnico di Milano

{andrea.cini, ivan.marisca, daniele.zambon, cesare.alippi}@usi.ch

Equal contribution.

###### Abstract

Spatiotemporal graph neural networks have shown to be effective in time series forecasting applications, achieving better performance than standard univariate predictors in several settings. These architectures take advantage of a graph structure and relational inductive biases to learn a single (_global_) inductive model to predict any number of the input time series, each associated with a graph node. Despite the gain achieved in computational and data efficiency w.r.t. fitting a set of _local_ models, relying on a single global model can be a limitation whenever some of the time series are generated by a different spatiotemporal stochastic process. The main objective of this paper is to understand the interplay between _globality_ and _locality_ in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures. We ascribe to trainable node embeddings the role of amortizing the learning of specialized components. Moreover, embeddings allow for 1) effectively combining the advantages of shared message-passing layers with node-specific parameters and 2) efficiently transferring the learned model to new node sets. Supported by strong empirical evidence, we provide insights and guidelines for specializing graph-based models to the dynamics of each time series and show how this aspect plays a crucial role in obtaining accurate predictions.

## 1 Introduction

Neural forecasting methods  take advantage of large databases of related time series to learn models for each individual process. If the time series in the database are not independent, functional dependencies among them can be exploited to obtain more accurate predictions, e.g., when the considered time series are observations collected from a network of sensors. In this setting, we use the term _spatiotemporal time series_ to indicate the existence of relationships among subsets of time series (sensors) that span additional axes other than the temporal one, denoted here as _spatial_ in a broad sense. Graph representations effectively model such dependencies and graph neural networks (GNNs)  can be included as modules in the forecasting architecture to propagate information along the spatial dimension. The resulting neural architectures are known in the literature as spatiotemporal graph neural networks (STGNNs)  and have found widespread adoption in relevant applications ranging from traffic forecasting  to energy analytics . These models embed inductive biases typical of graph deep learning and graph signal processing  and, thus, have several advantages over standard multivariate models; in fact, a shared set of learnable weights is used to obtain predictions for each time series by conditioning on observations at the neighboring nodes. Nonetheless, it has become more and more common to see node-specific trainable parameters being introduced as means to extract node(sensor)-level features then used as spatial identifiers within the processing . By doing so, the designer accepts a compromise in transferability that oftenempirically leads to higher forecasting accuracy on the task at hand. We argue that the community has yet to find a proper explanatory framework for this phenomenon and, notably, has yet to design proper methodologies to deal with the root causes of observed empirical results, in practical applications.

In the broader context of time series forecasting, single models learned from a set of time series are categorized as _global_ and are opposed to _local_ models, which instead are specialized for any particular time series in the set . Global models are usually more robust than local ones, as they require a smaller total number of parameters and are fitted on more samples. Standard STGNNs, then, fall within the class of global models and, as a result, often have an advantage over local multivariate approaches; however, explicitly accounting for the behavior of individual time series might be problematic and require a large memory and model capacity. As an example, consider the problem of electric load forecasting: consumption patterns of single customers are influenced by shared factors, e.g., weather conditions and holidays, but are also determined by the daily routine of the individual users related by varying degrees of affinity. We refer to the dynamics proper to individual nodes as _local effects_.

Local effects can be accounted for by combining a global model with local components, e.g., by using encoding and/or decoding layers specialized for each input time series paired with a core global processing block. While such an approach to building hybrid global-local models can be effective, the added model complexity and specialization can negate the benefits of using a global component. In this paper, we propose to consider and interpret learned node embeddings as a mechanism to amortize the learning of local components; in fact, instead of learning a separate processing layer for each time series, node embeddings allow for learning a single global module conditioned on the learned (local) node features. Furthermore, we show that - within a proper methodological framework - node embeddings can be fitted to different node sets, thus enabling an effective and efficient transfer of the core processing modules.

ContributionsIn this paper, we analyze the effect of node-specific components in spatiotemporal time series models and assess how to incorporate them in the forecasting architecture, while providing an understanding of each design choice within a proper context. The major findings can be summarized in the following statements.

* Local components can be crucial to obtain accurate predictions in spatiotemporal forecasting.
* Node embeddings can amortize the learning of local components.
* Hybrid global-local STGNNs can capture local effects with contained model complexity and smaller input windows w.r.t. fully global approaches.
* Node embeddings for time series outside the training dataset can be obtained by fitting a relatively small number of observations and yield more effective transferability than fine-tuning global models.
* Giving structure to the embedding space provides an effective regularization, allowing for similarities among time series to emerge and shedding light on the role of local embeddings within a global architecture.

Throughout the paper, we reference the findings with their pointers. Against this backdrop, our main novel contributions reside in:

* A sound conceptual and methodological framework for dealing with local effects and designing node-specific components in STGNN architectures.
* An assessment of the role of learnable node embeddings in STGNNs and methods to obtain them.
* A comprehensive empirical analysis of the aforementioned phenomena in representative architectures across synthetic and real-world datasets.
* Methods to structure the embedding space, thus allowing for effective and efficient reuse of the global component in a transfer learning scenario.

We believe that our study constitutes an essential advancement toward the understanding of the interplay of different inductive biases in graph-based predictors, and argue that the methodologies and conceptual developments proposed in this work will constitute a foundational piece of know-how for the practitioner.

Preliminaries and problem statement

Consider a set of \(N\) time series with graph-side information where the \(i\)-th time series is composed by a sequence of \(d_{x}\) dimensional vectors \(_{t}^{i}^{d_{x}}\) observed at time step \(t\); each time series \(\{_{t}^{i}\}_{t}\) might be generated by a different stochastic process. Matrix \(_{t}^{N d_{x}}\) encompasses the \(N\) observations at time \(t\) and, similarly, \(_{t:t+T}\) indicates the sequence of observations within the time interval \([t,t+T)\). Relational information is encoded by a weighted adjacency matrix \(^{N N}\) that accounts for (soft) functional dependencies existing among the different time series. We use interchangeably the terms _node_ and _sensor_ to indicate the entities generating the time series and refer to the node set together with the relational information (graph) as _sensor network_. Eventual exogenous variables associated with each node are indicated by \(_{t}^{N d_{u}}\); the tuple \(_{t}=_{t},_{t},\) indicates all the available information associated with time step \(t\).

We address the multistep-ahead time-series forecasting problem, i.e., we are interested in predicting, for every time step \(t\) and some \(H,W 1\), the expected value of the next \(H\) observations \(_{t:t+H}\) given a window \(_{t-W:t}\) of \(W\) past measurements. In case data from multiple sensor networks are available, the problem can be formalized as learning from \(M\) disjoint collections of spatiotemporal time series \(=\{_{t_{1}:t+T_{1}}^{(1)},_{t_{2}:t_{2 }:t_{2}}^{(2)},,_{t_{m}:t_{m}+T_{m}}^{(M)}\}\), potentially without overlapping time frames. In the latter case, we assume sensors to be homogeneous both within a single network and among different sets. Furthermore, we assume edges to indicate the same type of relational dependencies, e.g., physical proximity.

## 3 Forecasting with Spatiotemporal Graph Neural Networks

This section provides a taxonomy of the different components that constitute an STGNN. Based on the resulting archetypes, reference operators for this study are identified. The last paragraph of the section broadens the analysis to fit STGNNs within more general time series forecasting frameworks.

Spatiotemporal message-passingWe consider STGNNs obtained by stacking spatiotemporal message-passing (STMP) layers s.t.

\[_{t}^{l+1}=^{l}(_{ t}^{l},),\] (1)

where \(_{t}^{l}^{N d_{h}}\) indicates the stack of node representations \(_{t}^{i,l}\) at time step \(t\) at the \(l\)-th layer. The shorthand \( t\) indicates the sequence of all representations corresponding to the time steps up to \(t\) (included). Each \(^{l}(\ \ )\) layer is structured as follows

\[_{t}^{i,l+1}=^{l}_{ t}^{i,l}, {}\{^{l}_{ t}^{i,l},_{ t}^{j,l},a_{ji}\},\] (2)

where \(^{l}\) and \(^{l}\) are respectively the update and message functions, e.g., implemented by multilayer perceptrons (MLPs) or recurrent neural networks (RNNs). \(\{\ \ \}\) indicates a generic permutation invariant aggregation function, while \((i)\) refers to the set of neighbors of node \(i\), each associated with an edge with weight \(a_{ji}\). Models of this type are fully _inductive_, in the sense that they can be used to make predictions for networks and time windows different from those they have been trained on, provided a certain level of similarity (e.g., homogenous sensors) between source and target node sets .

Among the different implementations of this general framework, we can distinguish between time-then-space (TTS) and time-and-space (T&S) models by following the terminology of previous works [16; 17]. Specifically, in TTS models the sequence of representations \(_{ t}^{i,0}\) is encoded by a sequence model, e.g., an RNN, before propagating information along the spatial dimension through message passing (MP) . Conversely, in T&S models time and space are processed in a more integrated fashion, e.g., by a recurrent GNN  or by spatiotemporal convolutional operators . In the remainder of the paper, we take for TTS model an STGNN composed by a Gated Recurrent Unit (GRU)  followed by standard MP layers :

\[_{t}^{l}=(_{ t}^{0}), _{t}^{l+1}=^{l}(_{t}^{l},),\] (3)

where \(l=1,,L-1\) and \((\ \ )\) processes sequences node-wise. Similarly, we consider as reference T&S model a GRU with an MP network at its gates [5; 20], that process input data as

\[_{t}^{l+1}=^{l}(\{^{l}(_{t}^{l}, )\}_{ t}).\] (4)Moreover, STGNN models can be further categorized w.r.t. the implementation of message function; in particular, by loosely following Dwivedi et al. , we call _isotropic_ those GNNs where the message function \(^{l}\) only depends on the features of the sender node \(_{ t}^{j,l}\); conversely, we call _anisotropic_ GNNs where \(^{l}\) takes both \(_{ t}^{i,l}\) and \(_{ t}^{j,l}\) as input. In the following, the case-study isotropic operator is

\[_{t}^{i,l+1}=_{1}^{l}_{t}^{i,l}+(i)}{}_{2}^{l}_{t}^{j,l} },\] (5)

where \(_{1}^{l}\) and \(_{2}^{l}\) are matrices of learnable parameters and \((\ \ )\) a generic activation function. Conversely, the operator of choice for the anisotropic case corresponds to

\[_{t}^{j i,l} =_{2}^{l}_{1}^{l}_{t}^{i,l}|| _{t}^{j,l}||a_{ji},_{t}^{j i,l}= _{0}^{l}_{t}^{j i,l},\] (6) \[_{t}^{i,l+1} =_{3}^{l}_{t}^{i,l}+(i)}{}_{t}^{j i,l}_{t}^{j  i,l}},\] (7)

where matrices \(_{0}^{l}^{1 d_{m}}\), \(_{1}^{l}\), \(_{2}^{l}\) and \(_{3}^{l}\) are learnable parameters, \((\ \ )\) is the sigmoid activation function and \(||\) the concatenation operator applied along the feature dimension (see Appendix A.2 for a detailed description).

Global and local forecasting modelsFormally, a time series forecasting model is called _global_ if its parameters are fitted to a group of time series (either univariate or multivariate), while _local_ models are specific to a single (possibly multivariate) time series. The advantages of global models have been discussed at length in the time series forecasting literature [22; 14; 13; 1] and are mainly ascribable to the availability of large amounts of data that enable generalization and the use of models with higher capacity w.r.t. the single local models. As presented, and further detailed in the following section, STGNNs are global, yet have a peculiar position in this context, as they exploit spatial dependencies localizing predictions w.r.t. each node's neighborhood. Furthermore, the transferability of GNNs makes these models distinctively different from local multivariate approaches enabling their use in cold-start scenarios  and making them inductive both temporally and spatially.

## 4 Locality and globality in Spatiotemporal Graph Neural Networks

We now focus on the impact of local effects in forecasting architectures based on STGNNs. The section starts by introducing a template to combine different processing layers within a global model, then continues by discussing how these can be turned into local. Contextually, we start to empirically probe the reference architectures.

### A global processing template

STGNNs localize predictions in space, i.e., with respect to a single node, by exploiting an MP operator that contextualizes predictions by constraining the information flow within each node's neighborhood. STGNNs are global forecasting models s.t.

\[}_{t:t+H}=F_{}(_{t-W:t};)\] (8)

where \(\) are the learnable parameters shared among the time series and \(}_{t:t+H}\) indicate the \(H\)-step ahead predictions of the input time series given the window of (structured) observations \(_{t-W:t}\). In particular, we consider forecasting architectures consisting of an encoding step followed by STMP layers and a final readout mapping representations to predictions; the corresponding sequence of operations composing \(F_{}\) can be summarized as

\[_{t}^{i,0} =(_{t-1}^{i},_{t-1}^{i} ),\] (9) \[_{t}^{l+1} =^{l}_{ t}^{l},,  l=0,,L-1\] (10) \[}_{t:t+H}^{i} =(_{t}^{i,L}).\] (11)

\((\ \ )\) and \((\ \ )\) indicate generic encoder and readout layers that could be implemented in several ways. In the following, the encoder is assumed to be a standard fully connected linear layer, while the decoder is implemented by an MLP with a single hidden layer followed by an output (linear) layer for each forecasting step.

### Local effects

Differently from the global approach, local models are fitted to a single time series (e.g., see the standard Box-Jenkins approach) and, in our problem settings, can be indicated as

\[}_{t:t+H}^{i}=f_{i}(_{t-W:t}^{i};^{i}),\] (12)

where \(f_{i}(\ \ ;^{i})\) is a sensor-specific model, e.g., an RNN with its dedicated parameters \(^{i}\), fitted on the \(i\)-th time series. While the advantages of global models have already been discussed, local effects, i.e., the dynamics observed at the level of the single sensor, are potentially more easily captured by a local model. In fact, if local effects are present, global models might require an impractically large model capacity to account for all node-specific dynamics , thus losing some of the advantages of using a global approach (**S3**). In the STGNN case, then, increasing the input window for each node would result in a large computational overhead. Conversely, purely local approaches fail to exploit relational information among the time series and cannot reuse available knowledge efficiently in an inductive setting.

Combining global graph-based components with local node-level components has the potential for achieving a two-fold objective: 1) exploiting relational dependencies together with side information to learn flexible and efficient graph deep learning models and 2) making at the same time specialized and accurate predictions for each time series. In particular, we indicate global-local STGNNs as

\[}_{t:t+H}^{i}=F(_{t-W:t};,^{i})\] (13)

where function \(F\) and parameter vector \(\) are shared across all nodes, whereas parameter vector \(^{i}\) is time-series dependent. Such a function \(F(\ \ )\) could be implemented, for example, as a sum between a global model (Eq. 8) and a local one (Eq. 12):

\[}_{t:t+H}^{(1)}=F_{}(_{t-W:t};),}_{t:t+H}^{i,(2)}=f_{i}(_{t-W:t}^{i};^{i} ),\] (14)

\[}_{t:t+H}^{i}=}_{t:t+H}^{i,(1)}+}_{t:t+H}^{i,(2)},\] (15)

or - with a more integrated approach - by using different weights for each time series at the encoding and/or decoding steps. The latter approach results in using a different encoder and/or decoder for each \(i\)-th node in the template STGNN (Eq. 9-11) to extract representations and, eventually, project them back into input space:

\[_{t}^{i,0}=_{i}(_{t-1}^{i},_{t-1}^{i}; _{enc}^{i}),\] (16)

MP layers could in principle be specialized as well, e.g., by using a different local update function \(_{i}(\ \ )\) for each node. However, this would be impractical unless subsets of nodes are allowed to share parameters to some extent (e.g., by clustering them).

To support our arguments, Tab. 1 shows empirical results for the reference TTS models with isotropic message passing (TTS-IMP) on \(2\) popular traffic forecasting benchmarks (METRA and PEMS-BAY ). In particular, we compare the global approach with \(3\) hybrid global-local variants where local weights are used in the encoder, in the decoder, or in both of them (see Eq. 16-17 and the light brown block in Tab. 1). Notably, while fitting a separate RNN to each individual time series fails (LocalRNNs), exploiting a local encoder and/or decoder significantly improves performance w.r.t. the fully global model (**S1**). Note that the price of specialization is paid in terms of the number of learnable parameters which is an order of magnitude higher in global-local variants. The table reports as a reference also results for FC-RNN, a multivariate RNN taking as input the concatenation of all time series. Indeed, having both encoder and decoder implemented as local layers leads to a large number of parameters and has a marginal impact on forecasting accuracy. The

   &  &  &  \\  }}}} }}}}}}}}}}}}}}}}} &&& ** ** **M** & **1.7\(\)**0.00} & **4.71\(\)**10\({}^{4}\)** \\   & }}}}}}}}}}}}}}} &}\)**Encoder & 3.15 \(\)**0.01} & 1.66 \(\)**0.01} & 2.75\(\)**10\({}^{5}\)** \\  & & Decoder & 3.09 \(\)**0.01} & 1.58 \(\)**0.00} & 3.00\(\)**10\({}^{5}\)** \\  & & Enc. + Dec. & 3.16 \(\)**0.01} & 1.70 \(\)**0.01} & 5.28\(\)**10\({}^{5}\)** \\   & }}}}}}}}}}}}}}\) & **2.08 \(\)**0.01} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.13 \(\)**0.00} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Encoder & 3.07 \(\)**0.01} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.01} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.01} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{5}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.09 \(\)**0.01} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.07 \(\)**0.01} & 1.58 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\    & 3.56 \(\)**0.03} & 2.32 \(\)**0.01} & 3.04\(\)**10\({}^{5}\)** \\   & & Decoder & 3.69 \(\)**0.00} & 1.91 \(\)**0.00} & 1.10\(\)**10\({}^{7}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.15 \(\)**0.00} & 1.60 \(\)**0.00} & 5.96\(\)**10\({}^{4}\)** \\   & & Decoder & 3.07 \(\)**0.01} & 1.58 \(\)**0.00} & 5.61\(\)**10\({}^{4}\)** \\    & 3.56 \(\)**0.03} & 2.32 \(\)**0.01} & 3.04\(\)**10\({}^{5}\)** \\   & & Decoder & 3.69 \(\)**0.00} & 1.91

[MISSING_PAGE_FAIL:6]

where \(P=(0,)\) is the prior, \(D_{}\) the Kulback-Leibler divergence, and \(\) controls the regularization strength. This regularization scheme results in a smooth latent space where it is easier to interpolate between representations, thus providing a principled way for accommodating different node embeddings.

Clustering regularizationA different (and potentially complementary) approach to structuring the latent space is to incentivize node embeddings to form clusters and, consequently, to self-organize into different groups. We do so by introducing a regularization loss inspired by deep \(K\)-means algorithms . In particular, besides the embedding table \(^{N d_{v}}\), we equip the embedding module with a matrix \(^{K d_{v}}\) of \(K N\) learnable centroids and a cluster assignment matrix \(^{N K}\) encoding scores associated to each node-cluster pair. We consider scores as logits of a categorical (Boltzmann) distribution and learn them by minimizing the regularization term

\[_{reg}_{}[\|- \|_{2}], p(_{ij}=1)=_{ij}/}}{ e^ {_{ik}/}},\]

where \(\) is a hyperparameter. We minimize \(_{reg}\) - which corresponds to the embedding-to-centroid distance - jointly with the forecasting loss by relying on the Gumbel softmax trick . Similarly to the variational inference approach, the clustering regularization gives structure to embedding space and allows for inspecting patterns in the learned local components (see Sec. 7).

Transferability of graph-based predictorsGlobal models based on GNNs can make predictions for never-seen-before node sets, and handle graphs of different sizes and variable topology. In practice, this means that the graph-based predictors can easily handle new sensors being added to the network over time and be used for zero-shot transfer. Clearly, including in the forecasting architecture node-specific local components compromises these properties. Luckily, if local components are replaced by node embedding, adapting the specialized components is relatively cheap since the number of parameters to fit w.r.t. the new context is usually contained, and - eventually - both the graph topology and the structure of the embedding latent space can be exploited (**S4**). Experiments in Sec. 7 provide an in-depth empirical analysis of transferability within our framework and show that the discussed regularizations can be useful in this regard.

## 6 Related works

GNNs have been remarkably successful in modeling structured dynamical systems [31; 32; 33], temporal networks [34; 35; 36] and sequences of graphs [37; 38]. For what concerns time series processing, recurrent GNNs [5; 6] were among the first STGNNs being developed, followed by fully convolutional models [7; 39] and attention-based solutions [40; 41; 42]. Among the methods that focus on modeling node-specific dynamics, Bai et al.  use a factorization of the weight matrices in a recurrent STGNN to adapt the extracted representation to each node. Conversely, Chen et al.  use a model inspired by Wang et al.  consisting of a global GNN paired with a local model conditioned on the neighborhood of each node. Node embeddings have been mainly used in structure-learning modules to amortize the cost of learning the full adjacency matrix [39; 45; 12] and in attention-based approaches as positional encodings [46; 47; 41]. Shao et al.  observed how adding spatiotemporal identification mechanisms to the forecasting architecture can outperform several state-of-the-art STGNNs. Conversely, Yin et al.  used a cluster-based regularization to fine-tune an AGCRN-like model on different datasets. However, none of the previous works systematically addressed directly the problem of globality and locality in STGNNs, nor provided a comprehensive framework accounting for learnable node embeddings within different settings and architectures. Finally, besides STGNNs, there are several examples of hybrid global and local time series forecasting models. Wang et al.  propose an architecture where \(K\) global models extract dynamic global factors that are then weighted and integrated with probabilistic local models. Sen et al.  instead use a matrix factorization scheme paired with a temporal convolutional network  to learn a multivariate model then used to condition a second local predictor.

## 7 Experiments

This section reports salient results of an extensive empirical analysis of global and local models and combinations thereof in spatiotemporal forecasting benchmarks and different problem settings;complete results of this systematic analysis can be found in Appendix B. Besides the reference architectures, we consider the following baselines and popular state-of-the-art architectures.

**RNN:**: a global univariate RNN sharing the same parameters across the time series.
**FC-RNN:**: a multivariate RNN taking as input the time series as if they were a multivariate one.
**LocalRNNs:**: local univariate RNNs with different sets of parameters for each time series.
**DCRNN :**: a recurrent T&S model with the Diffusion Convolutional operator.
**AGCRN :**: the T&S global-local Adaptive Graph Convolutional Recurrent Network.
**GraphWaveNet:**: the deep T&S spatiotemporal convolutional network by Wu et al. .

We also consider a global-local RNN, in which we specialize the model by using node embeddings in the encoder. Note that among the methods selected from the literature only DCRNN can be considered fully global (see Sec. 6). Performance is measured in terms of _mean absolute error_ (MAE).

Synthetic dataWe start by assessing the performance of hybrid global-local spatiotemporal models in a controlled environment, considering a variation of GP-VAR , a synthetic dataset based on a polynomial graph filter , that we modify to include local effects. In particular, data are generated from the spatiotemporal process

\[_{t} =_{l=1}^{L}_{q=1}^{Q}_{q,l}^{l-1}_{t-q},\] \[_{t+1} =(_{t})+( _{t-1})+_{t},\] (20)

where \(^{Q L}\), \(^{N}\), \(^{N}\) and \(_{t}(,^{2})\). We refer to this dataset as GPVAR-L: note that \(\) and \(\) are node-specific parameters that inject local effects into the spatiotemporal process. We indicate simply as GPVAR the process obtained by fixing \(==\), i.e., by removing local effects. We use as the adjacency matrix the community graph used in prior works, increasing the number of nodes to \(120\) (see Appendix A.1).

Tab. 2 shows forecasting accuracy for reference architectures with a \(6\)-steps window on data generated from the processes. In the setting with no local effects, all STGNNs achieve performance close to the theoretical optimum, outperforming global and local univariate models but also the multivariate FC-RNN that - without any inductive bias - struggles to properly fit the data. In GPVAR-L, global and univariate models fail to match the performance of STGNNs that include local components (**S1**); interestingly, the global model with anisotropic MP outperforms the isotropic alternative, suggesting that the more advanced MP schemes can lead to more effective state identification.

BenchmarksWe then compare the performance of reference architectures and baselines with and without node embeddings at the encoding and decoding steps. Note that, while reference architectures and DCRNN are purely global models, GraphWaveNet and AGCRN use node embeddings to obtain an adjacency matrix for MP. AGCRN, furthermore, uses embeddings to make the convolutional filters

   Models & GPVAR & GPVAR-L \\  FC-RNN & 4393\(\)0024 & 5978\(\)0109 \\ LocalRNNs & -4047\(\)0001 &.4610\(\)0003 \\    } & RNN & 3999\(\)0000 & 5440\(\)0003 \\  & TTS-IMP & 3232\(\)0002 &.4059\(\)0002 \\  & TTS-AMP & 3193\(\)0000 & 3587\(\)0009 \\    } & RNN & 3991\(\)0001 &.4612\(\)0003 \\  & TTS-IMP & 3195\(\)0000 &.3200\(\)0002 \\  & TTS-AMP & 3194\(\)0001 &.3199\(\)0001 \\  Optimal model &.3192 &.3192 \\   

Table 2: One-step-ahead forecasting error (MAE) of the different models in GPVAR datasets (5 runs).

    &  &  \\  \(W\) & \(d_{h}=16\) & \(d_{h}=32\) & \(d_{h}=64\) & \(d_{h}=16\) & \(d_{h}=32\) & \(d_{h}=64\) \\ 
2 & 5371\(\)0014 & 4679\(\)0016 & 4124\(\)0021 & 3198\(\)0001 &.3199\(\)0001 &.3203\(\)0001 \\
6 & 4059\(\)0023 & 3578\(\)0031 &.3365\(\)0006 & 3200\(\)0002 &.3201\(\)0001 &.3209\(\)0002 \\
12 & 3672\(\)0035 & 3362\(\)0012 &.3280\(\)0003 &.3200\(\)001 &.3200\(\)0006 &.3211\(\)001 \\
24 &.3485\(\)0002 &.3286\(\)0005 &.3250\(\)001 &.3200\(\)0002 &.3200\(\)0006 &.3211\(\)001 \\   

Table 3: TTS-IMP one-step-ahead MAE on GPVAR-L with varying window length \(W\) and capacity \(d_{h}\) (5 runs).

adaptive w.r.t. the node being processed. We evaluate all models on real-world datasets from three different domains: traffic networks, energy analytics, and air quality monitoring. Besides the already introduced traffic forecasting benchmarks (**METR-LA** and **PEMS-BAY**), we run experiments on smart metering data from the **CER-E** dataset  and air quality measurements from **AQI** by using the same pre-processing steps and data splits of previous works . The full experimental setup is reported in appendix A. Tab. 4 reports forecasting _mean absolute error_ (MAE) averaged over the forecasting horizon. Global-local reference models outperform the fully global variants in every considered scenario (**S1**). A similar observation can be made for the state-of-art architectures, where the impact of node embeddings (at encoding and decoding) is large for the fully global DCRNN and more contained in models already equipped with local components. Note that hyperparameters were not tuned to account for the change in architecture. Surprisingly, the simple TTS-IMP model equipped with node embeddings achieves results comparable to that of state-of-the-art STGNNs with a significantly lower number of parameters and a streamlined architecture. Interestingly, while both global and local RNNs models fail, the hybrid global-local RNN ranks among the best-performing models, outperforming graph-based models without node embeddings in most settings.

Structured embeddingsTo test the hypothesis that structure in embedding space provides insights on the local effects at play (**S5**), we consider the clustering regularization method (Sec. 5.1) and the reference TTS-IMP model trained on the CER-E dataset. We set the number of learned centroids to \(K=5\) and train the cluster assignment mechanism end-to-end with the forecasting architecture. Then, we inspect the clustering assignment by looking at intra-cluster statistics. In particular, for each load profile, we compute the weekly average load curve, and, for each hour, we look at quantiles of the energy consumption within each cluster. Fig. 0(a) shows the results of the analysis by reporting the _median_ load profile for each cluster; shaded areas correspond to quantiles with \(10\%\) increments. Results show that users in the different clusters have distinctly different consumption patterns. Fig. 0(b)

   Models & **METR-LA** & **PEMS-BAY** & **CER-E** & **AQI** & **METR-LA** & **PEMS-BAY** & **CER-E** & **AQI** \\   Reference arch. &  &  \\  RNN & 3.54\({}_{ 0.0}\) & 1.77\({}_{ 0.0}\) & 456.98\({}_{ 0.61}\) & 14.02\({}_{ 0.4}\) & **3.15\({}_{ 0.3}\)** & **1.59\({}_{ 0.0}\)** & **421.50\({}_{ 1.78}\)** & **13.73\({}_{ 0.4}\)** \\  T\&S-IMP & 3.35\({}_{ 0.01}\) & 1.70\({}_{ 0.01}\) & 443.85\({}_{ 0.99}\) & 12.87\({}_{ 0.2}\) & **3.10\({}_{ 0.01}\)** & **1.59\({}_{ 0.00}\)** & **417.71\({}_{ 1.28}\)** & **12.48\({}_{ 0.33}\)** \\ TTS-IMP & 3.34\({}_{ 0.01}\) & 1.72\({}_{ 0.00}\) & 439.13\({}_{ 0.51}\) & 12.74\({}_{ 0.2}\) & **3.08\({}_{ 0.01}\)** & **1.58\({}_{ 0.00}\)** & **412.44\({}_{ 1.20}\)** & **12.33\({}_{ 0.22}\)** \\ T\&S-AMP & 3.22\({}_{ 0.02}\) & 1.65\({}_{ 0.00}\) & N/A & N/A & **3.07\({}_{ 0.02}\)** & **1.59\({}_{ 0.00}\)** & N/A & N/A \\ TTS-AMP & 3.24\({}_{ 0.01}\) & 1.66\({}_{ 0.00}\) & 431.33\({}_{ 0.68}\) & 12.30\({}_{ 0.02}\) & **3.06\({}_{ 0.01}\)** & **1.58\({}_{ 0.01}\)** & **412.95\({}_{ 1.28}\)** & **12.15\({}_{ 0.22}\)** \\   Baseline arch. &  &  \\  DCRNN & 3.22\({}_{ 0.01}\) & 1.64\({}_{ 0.00}\) & 428.36\({}_{ 1.23}\) & 12.96\({}_{ 0.03}\) & **3.07\({}_{ 0.02}\)** & **1.60\({}_{ 0.00}\)** & **412.87\({}_{ 1.51}\)** & **12.53\({}_{ 0.02}\)** \\ GraphWaveNet & 3.05\({}_{ 0.03}\) & **1.56\({}_{ 0.01}\)** & **397.17\({}_{ 0.67}\)** & 12.08\({}_{ 1.1}\) & **2.99\({}_{ 0.02}\)** & 1.58\({}_{ 0.00}\) & 401.15\({}_{ 1.49}\)** & **11.81\({}_{ 0.04}\)** \\ AGCRN & 3.16\({}_{ 0.01}\) & **1.61\({}_{ 0.00}\)** & 444.80\({}_{ 1.25}\) & 13.33\({}_{ 0.02}\) & **3.14\({}_{ 0.00}\)** & 1.62\({}_{ 0.00}\)** & **436.84\({}_{ 2.06}\)** & **13.28\({}_{ 0.03}\)** \\   

Table 4: Forecasting error (MAE) on \(4\) benchmark datasets (5 runs). The best result between each model and its variant with embeddings is in **bold**. N/A indicates runs exceeding resource capacity.

Figure 1: Time series clusters in CER-E, obtained by regularizing the embedding space. **(a)** Average load for each clusters. **(b)** t-SNE plot of the corresponding node embeddings.

shows a 2D t-SNE visualization of the learned node embeddings, providing a view of the latent space and the effects of the cluster-based regularization.

TransferIn this experiment, we consider the scenario in which an STGNN for traffic forecasting is trained by using data from multiple traffic networks and then used to make predictions for a disjoint set of sensors sampled from the same region. We use the **PEMS03**, **PEMS04**, **PEMS07**, and **PEMS08** datasets , which contain measurements from \(4\) different districts in California. We train models on \(3\) of the datasets, fine-tune on \(1\) week of data from the target left-out dataset, validate on the following week, and test on the week thereafter. We compare variants of TTS-IMP with and without embeddings fed into encoder and decoder. Together with the unconstrained embeddings, we also consider the variational and clustering regularization approaches introduced in Sec. 5.1. At the fine-tuning stage, the global model updates all of its parameters, while in the hybrid global-local approaches only the embeddings are fitted to the new data. Tab. 5 reports results for the described scenario. The fully global approach is outperformed by the hybrid architectures in all target datasets (**S4**). Besides the significant improvement in performance, adjusting only node embeddings retains performance on the source datasets. Furthermore, results show the positive effects of regularizing the embedding space in the transfer setting (**S5**). This is further confirmed by results in Tab. 6, which report, for PEMS04, how forecasting error changes in relation to the length of the fine-tuning window. We refer to Appendix B for an in-depth analysis of several additional transfer learning scenarios.

## 8 Conclusions

We investigate the impact of locality and globality in graph-based spatiotemporal forecasting architectures. We propose a framework to explain empirical results associated with the use of trainable node embeddings and discuss different architectures and regularization techniques to account for local effects. The proposed methodologies are thoroughly empirically validated and, although not inductive, prove to be effective in a transfer learning context. We argue that our work provides necessary and key methodologies for the understanding and design of effective graph-based spatiotemporal forecasting architectures. Future works can build on the results presented here and study alternative, and even more transferable, methods to account for local effects.