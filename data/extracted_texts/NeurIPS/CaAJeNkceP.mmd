# Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation

Claude Formanek\({}^{1,2}\)\({}^{*}\) &Callum Rhys Tilbury\({}^{1}\) &Louise Beyers\({}^{1}\)

&Jonathan Shock\({}^{2,3,4}\) &Arnu Pretorius\({}^{1}\)

\({}^{1}\)InstaDeep \({}^{2}\)University of Cape Town \({}^{3}\)INRS, Montreal \({}^{4}\)NITheCS, Stellenbosch

Corresponding author: c.formanek@instadeep.com

###### Abstract

Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on \(35\) out of \(47\) datasets used in prior work (almost \(75\)% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.

## 1 Introduction

Offline reinforcement learning (RL) attempts to derive optimal sequential control policies from static data alone, without access to online interactions (e.g. a simulator). Though the single-agent variant has received fairly widespread research attention (Prudencio et al., 2023), progress in the multi-agent context has been slower, due to a variety reasons. For one, multi-agent problems are fundamentally more difficult, and bring a host of new challenges--including difficulties in coordination (Barde et al., 2024), large joint-action spaces (Yang et al., 2021), heterogeneous agents (Zhong et al., 2024) and non-stationarity (Papoudakis et al., 2019), which are absent in single-agent situations.

Nonetheless, some progress has been made in offline multi-agent reinforcement learning (MARL) in recent years. In particular, in better understanding the aforementioned difficulties of offline learning in the multi-agent setting and proposing certain remedies for them (Jiang and Lu, 2021; Yang et al., 2021; Pan et al., 2022; Wang et al., 2023; Shao et al., 2023; Tian et al., 2023; Meng et al., 2023).

However, we argue that this progress might be a mirage and that offline MARL research is ultimately being held back by a lack of clarity and consistency in baseline implementations and evaluation protocols. To support this claim, we demonstrate in Figure 1 a surprising but telling result--we show that good implementations of straightforward baseline algorithms can achieve state-of-the-art performance across a wide-range of tasks, beating several published works claiming such a title. We view our analysis as robust, using datasets and environments with experimental settings that exactly match prior work (see Section 3).

This paper proceeds as follows. We first assess the state of the field, diagnosing the key points of friction for progress in offline MARL. Thereafter, we describe in more detail how well-implemented baseline methods perform surprisingly well compared to leading methods from the literature, suggesting that algorithmic progress has not advanced at the rate perhaps previously perceived by the community. In response, we introduce a standardised baseline and evaluation methodology, in an effort to support the field towards being more scientifically rigorous. We hope that researchers will build upon this work, advocating for a cleaner, more reproducible and robust empirical science for offline MARL.

## 2 Methodological Problems in Offline MARL

In this section, we briefly assess the state of offline MARL research by focusing on the baselines and evaluation protocols commonly employed. We consider the following five papers, all published at top-tier venues, for our case study: MAICQ (Yang et al., 2021), OMAR (Pan et al., 2022), MADT (Meng et al., 2023), CFCQL (Shao et al., 2023), and OMIGA (Wang et al., 2023). Given the nascency of the field, we consider these papers to serve as a good representative sample of the current trends in offline MARL research. By looking at this cross-section, we can assess the current methodologies for measuring progress. We present our findings below.

Figure 1: We compare our baseline implementations to the reported performance of various algorithms from the literature across a wide range of datasets. We normalise results from each dataset (i.e. scenario-quality-source combination) by the SOTA performance from the literature for that dataset. Standard deviation bars are given and when our baseline is significantly better or equal to the best method, using a two-side t-test, we indicate so using a gold star. **We find that on 35 out of the 47 datasets tested (almost 75% of cases), we match or surpass the performance of the current SOTA.**

Ambiguity in the Naming of Baseline AlgorithmsIn single-agent RL, the naming of a given algorithm is fairly unambiguous: it is widely understood what core algorithmic steps constitute, e.g., DDPG (Lillicrap et al., 2016). Yet, in MARL, algorithms become more complex, since one must specify how multiple agents should learn and interact. Unlike in the single agent case, there is ambiguity when referring to _multi-agent_ DDPG--for this might be referring to MADDPG (Lowe et al., 2017), or independent DDPG agents, or perhaps some other way of interleaving training and/or execution with DDPG forming the base of the algorithm's design. The corresponding impact on the performance of such choices can be significant (Lyu et al., 2021), and thus it is important to be as explicit as possible.

Clarity in this naming has been lacking in offline MARL literature. For instance, we consider Conservative Q-Learning (CQL) (Kumar et al., 2020) as a prime example of this problem. As an influential single-agent offline RL algorithm, CQL is critical to consider as a baseline when proposing new work in the field. Whereas the core CQL algorithmic steps are well-established, though, there does not exist a common understanding in the literature of what _multi-agent_ CQL is, despite widespread appearance of the abbreviation, MACQL, and its permutations. Consider Table 1, which shows how the same baseline method is purportedly included in each of the papers in our case study, yet the details provided for this algorithm are sparse, often lacking publicly available code, and can vary dramatically across papers.

To highlight these discrepancies and their effects more clearly, we note that the authors of each paper compare their proposed method with "MACQL" and claim superior performance. But _which_ MACQL is being compared, has a significant bearing on the degree to which these conclusions are likely to hold. For example, consider the following simple experiment. We compare two viable candidates for MACQL across three SMACv1 (Samvelyan et al., 2019) maps (8m, 2s3z, 5m v 6m). Specifically, we compare MADDPG (Lowe et al., 2017) with the Gumbel-Softmax (Jang et al., 2016), against QMIX (Rashid et al., 2018), each with the addition of CQL. Importantly, note that in both cases, we could present such an algorithm as "MACQL" as used in prior work. However, the results in Figure 2 clearly reveal their relative differences in performance. Here we report the median, mean and interquartile mean (IQM) as recommended by Agarwal et al. (2022). We notice, too, that the outcome changes depending on the scenario used--MADDPG with CQL outperforms QMIX on 8m, whereas the ordering is reversed on 5m_vs_6m.

Mismatches in Scenarios Used for ComparisonIn addition to the ambiguity in baselines used for comparison, there is also a confusing mismatch in the selected scenarios used across prior work. Quite

**Paper** & **Name for MACQL** & **Implementation Details Provided in the Paper** & **Is the Full Code Available?** \\  Yang et al. (2021) & MA-CQL & Loss function, value-decomposition structure & No, only single-agent CQL \\  Pan et al. (2022) & MA-CQL & MADDPG (Lowe et al., 2017) & Only for continuous settings \\  & & (Discrete: + Gumbel Softmax (Jang et al., 2016)) & No \\  Meng et al. (2023) & CQL-MA & CQL + “mixing network” & No \\  Shao et al. (2023) & MACQL & “Naive extension of CQL to multi-agent settings” + Loss function & Yes \\  Wang et al. (2023) & CQL-MA & Value decomposition structure + policy constraint & No \\  

Table 1: Demonstration of how papers in our case study are essentially using the same name for Multi-Agent CQL, for markedly different algorithms, often providing only sparse information about their implementations.

Figure 2: Comparing the performance of QMIX+CQL and MADDPG+CQL, two algorithms that could reasonably be called MACQL in the literature (see Table 1), using the Medium dataset from three different SMACv1 scenarios. We see that the difference in performance of these algorithms is significant, and depends on the scenario considered.

[MISSING_PAGE_FAIL:4]

is specified and training budget is considered an important hyperparameter, researchers can avoid unintentionally cherry-picking results.

The lack of consistency, transparency, and completeness of evaluation procedures in offline MARL slows progress by forcing researchers to perform expensive re-evaluations of baselines for their comparisons. But perhaps most damaging is the inability to compare and build upon prior work. This allows the field to maintain a mirage of steady progress, while in reality, algorithms are not becoming materially better.

## 3 Reevaluating the Empirical Evidence from Prior Work

Given the problems of baseline and evaluation methodology in offline MARL, we now revisit the empirical evidence from prior work through an independent standardised study.

Arguably, the gold standard for such a study would use the exact datasets from the authors and their original code (note, even this approach may have drawbacks such as potentially perpetuating poor experimental design and algorithm implementations). Alternatively, we could obtain a selection of similar yet distinct datasets (e.g. those from Formanek et al. (2023)), and use existing code implementations from the respective authors for each algorithm considered. Unfortunately, this approach often proves to be infeasible. In some works, only parts of the proposed algorithm code are shared (e.g. Pan et al. (2022) only share code for their method in continuous action spaces), and in many works, code for the baseline algorithms is omitted completely. As another alternative, we could decide to use our _own_ algorithm implementations and datasets, however, this would put us in similar territory as prior work with regards to drawing concrete conclusions. As the most sensible middle ground, we do the following: we use the exact same datasets as provided in prior work, but train our own _baseline_ implementations on these datasets; for the results of the other algorithms, we extract the values exactly as they are given in their respective papers. As an illustrative example, suppose we are comparing our implementation of MADDPG with CQL against the results from the OMIGA paper (Wang et al., 2023), in one of the scenarios from MAMuJoCo (Peng et al., 2021). Here, we take the _datasets_ provided by Wang et al. (2023), train our MADDPG+CQL algorithm, and compare these results to the tabular values reported by Wang et al. (2023) themselves. We feel this methodology is the fairest to the original authors, especially in the situation where the publicly available code is lacking and/or broken. We also view the task of implementing our own baselines, instead of attempting to re-implement the author's proposed algorithm, as a more faithful exercise.

Nonetheless, this approach still has challenges, for it requires access to the datasets used by other authors. Regrettably, in some cases we could not access this data, either because it was never made publicly available, or because the provided download links were broken and multiple attempts to reach the original authors were unsuccessful. In Table 4, we summarise our dataset access record, across the papers in the case study.

The next challenge is that several works use modified versions of environments to generate their datasets, and to evaluate their algorithms. For example, Wang et al. (2023) modify the MAMuJoCo environment such that agents all receive a global observation of the environment, rather than decentralised, partial observations as is standard. They also use a different version of SMACv1, seemingly

Figure 3: A comparison of the performance of behaviour cloning (BC) and QMIX+CQL on the SMACv1 8m scenario with the Medium dataset, across 10 seeds. Although QMIX+CQL outperforms BC during the first half of training, its performance deteriorates in the second half, making BC the preferred algorithm over the maximum training time.

first modified by Yu et al. (2022), that has important differences from the original. Similarly, Pan et al. (2022) include their own code for the Multi Particle Environments (MPE), which differs from the standardised and maintained version in PettingZoo (Terry et al., 2021). As a consequence, several datasets from different papers _seem_ to share a common environment, but in reality do not (e.g., the 5m_vs_6m datasets generated by Wang et al. (2023) are not compatible with the 5m_vs_6m datasets from Shao et al. (2023)). To facilitate fair comparisons to each of the original works, we re-use the respective unique environment configuration, even when this is different from the standard. _We do not advocate this approach for future work_ and note that standardisation is crucial going forward (which we discuss in more detail in the next section).

We use four baselines for our experiments--two for discrete action spaces, and two for continuous. In the discrete case, we implement BC, and independent Q-learners (IQL) (Tampuu et al., 2017) with CQL regularisation to stabilise offline training. Notably, these are very straightforward baselines that do not rely on any value factorisation or global state information. In continuous action spaces, we use independent DDPG agents with behaviour cloning regularisation--a naive multi-agent extension of the algorithm by Fujimoto and Gu (2021), which notably only requires a single line to change in our vanilla implementation of independent DDPG. Finally, we also test MADDPG (Lowe et al., 2017) with CQL to stabilise offline critic learning. Interestingly, this baseline is used in multiple works (Pan et al., 2022; Wang et al., 2023), but its reported performance is poor.

We train our baselines on MPE, SMACv1, and MAMuJoCo scenarios for \(50k\), \(100k\), and \(200k\) training updates, respectively. At the end of training, we compute the mean episode return over 32 episodes and repeat each run across 10 independent random seeds. We avoid fine-tuning our algorithms on each scenario independently in an attempt to control for the online tuning budget (Kurenkov and Kolesnikov, 2022), and instead keep the hyperparameters fixed across each respective scenario.

ResultWe provide all the experimental results in tabular form in the appendix. From our own training results, we provide the mean episode return with the standard deviation across 10 independent seeds. As stated above, we extract values verbatim from prior work for other algorithms that are being compared. We perform a simple heteroscedastic, two-sided t-test with a 95% confidence interval for testing statistical significance, following Papoudakis et al. (2021). We summarise the tabulated results in an illustrative plot in Figure 1 (plotting our best baseline per dataset). To standardise comparisons, we normalise results from each dataset (i.e. scenario-quality-source combination) by the SOTA performance from the literature for that dataset. When our method is significantly better or equal to the best method in the literature, we indicate so using a star. We find that on \(35\) out of the \(47\) datasets tested, we match or surpass the performance of the current SOTA from the literature.2

## 4 Standardising Baselines and Evaluation

The outcome from our benchmarking exercise paints a worrying picture of the state of offline MARL. We maintain that most of the contributions made by the research community to date are valuable.

**Paper** & **Environment** & **Number of Datasets** & **Accessibility** & **Benchmarked** \\  Yang et al. (2021) & SMACv1 & 4 & Link broken & \(\) \\  Pan et al. (2022) & SMACv1 & 4 & Not available & \(\) \\  & MAMuJoCo & 4 & Yes & \(\) \\  & MPE & 12 & Yes & \(\) \\  Meng et al. (2023) & SMACv1 & 62 & Download fails & \(\) \\  Shao et al. (2023) & SMACv1 & 16 & Yes & \(\) \\  & MAMuJoCo & 4 & Yes, from Pan et al. (2022) & \(\) \\  & MPE & 12 & Yes, from Pan et al. (2022) & \(\) \\  Wang et al. (2023) & SMACv1 & 12 & Yes & \(\) \\  & MAMuJoCo & 12 & Yes & \(\) \\  

Table 4: Summary of dataset accessibility and whether we benchmarked our baselines on them.

However, because several works seem to be building upon unreliable baselines and using inconsistent evaluation protocols, the overall value to the community is diminished. We believe the community will be better served if we adopt a common set of datasets, baselines, and evaluation methodologies.

DatasetsWith regards to common datasets, OG-MARL (Formanek et al., 2023) includes a wide range of offline MARL datasets which the community has begun to adopt (Zhu et al., 2023; Yuan et al., 2023). We find that a notable advantage of OG-MARL datasets, aside from their ease of accessibility, is that they are generated on the standard environment configurations rather than customised ones. This significantly eases the challenge of matching datasets to environments, as highlighted in Section 3. Having said that, we also believe there is value in improving access to the datasets from prior works, which we used here for benchmarking. Thus, we convert all of the datasets available to us from the literature (see Table 4) to the OG-MARL dataset API to make them more easily available to the community, in one place. We include statistical profiles of the datasets in the appendix and give credit to the original creators.

BaselinesWhile notable progress has been made standardising offline MARL datasets, we maintain that inconsistent use of baselines remains an overlooked issue in the field. Indeed, to highlight this, we conducted extensive benchmarking in Section 3. Now, to address the issue, we release our implementations of BC, IQL+CQL, IDDPG+BC, and MADDPG+CQL, as high-quality baselines for future research. Our baselines come with three main advantages. First, their demonstrated correctness, where we have shown they perform as well as, or better than, most algorithms in the literature on a wide range of datasets. Second, our baselines are easy to parse while also being highly performant on hardware accelerators. The core algorithm logic of our baselines is contained within a single file, which makes it easy for researchers to read, understand, and modify. In addition, all of the training code can be jit (just-in-time) compiled to XLA, making it very fast on hardware accelerators. Furthermore, we use the hardware accelerated replay buffers from Flashbax (Toledo et al., 2023), delivering performance gains by speeding up the time to sample from datasets. The third advantage is their compatibility with OG-MARL datasets, which comes "out of the box", offering the widest compatibility with offline MARL datasets in the literature 3. As a foundation for future work, we provide tabular performance values across multiple scenarios for these algorithms, in Table 5. Furthermore, all raw training results can be viewed and downloaded, and are linked to in the appendix.

EvaluationRecent efforts have been made to address the issue in the online setting (Gorsane et al., 2022; Agarwal et al., 2022). However, similar efforts are still absent in the offline setting, as discussed in Section 2. Following in the spirit of this work, we propose a set of evaluation guidelines for offline MARL, given in the blue box on Page \(8\), which we believe will significantly improve research outcomes, if adopted.

Figure 4: Performance profiles (Agarwal et al., 2022) aggregated across all results from Table 5 on SMACv1 and MAMuJoCo. Scores are normalised as per Fu et al. (2021).

**Choosing the settings to evaluate on:**

* Select at least 2-3 different environments on which to test. For example, evaluating on both SMAC and MAMuJoCo is the most common combination. We encourage additionally evaluating on environments beyond these two, to avoid overfitting to them. Do not use non-standard environment configurations without explicitly stating so.
* For each environment choose at least 3-4 different scenarios. If the environment creators provide a minimal set of recommended scenarios, use those. Alternatively, focus on scenarios that are common in prior literature.
* Choose a range of dataset quality types. We recommend at least including a "good" dataset where the majority of samples are from good policies and a "mixed" dataset where samples come from a wide range of policies including good, medium and poor ones.
* Try to use common existing datasets from the literature (Formanek et al., 2023). If you include your own dataset, provide a clear reason for why, and make it easily accessible to the community.

**Choosing the baselines to compare to:**

* Choose at least 3-4 relevant baselines to compare to.
* Usually include behaviour cloning, especially on good datasets where it can be challenging to beat.
* Try to use common and existing implementations of baselines from the literature.
* If you include your own novel baseline, make the code available and easy to run for future comparisons. It is not sufficient to only share the code for the novel algorithm being proposed.

**Choosing the training and evaluation parameters:**

* For each environment, set a training budget and keep it constant across algorithms. For example, we used 100k training updates on SMAC and 200k on MAMuJoCo.
* If possible, do regular evaluations during training so that you can plot a training curve for analysis at the end of training. Do not use information from these evaluations to influence a training run online, as this would violate the assumptions around the training being offline.
* We recommend at every evaluation step unrolling for 32 episodes and reporting the average episode return.
* As per Agarwal et al. (2022), you should repeat each run across 10 random seeds.

**Reporting your results:**

* Report the final evaluation result, averaged across all 10 seeds, along with the standard deviation.
* If doing regular evaluations during training, also report the average and maximum episode return during training as per Papoudakis et al. (2021) and plot sample efficiency curves as per Gorsane et al. (2022).
* Use appropriate statistical significance calculations to report on whether your algorithms significantly outperform the baselines (Papoudakis et al., 2021; Agarwal et al., 2022).
* In addition to reporting results on a per-dataset basis, also report aggregated results across scenarios and dataset types. Results can be aggregated by first normalising them as per Fu et al. (2021) and then applying the utilities from _MARL-eval_(Gorsane et al., 2022) (e.g. performance profile plots, see Figure 4).

## 5 Conclusion

We conducted a thorough analysis of prior work in offline MARL and identified several significant methodological failures which we demonstrated are inhibiting progress in the field. Furthermore, we benchmarked simple baselines against several proposed SOTA algorithms and showed that our baselines outperform them in most cases. We used these insights to propose improving standards in evaluation with a simple protocol.

LimitationsOur work highlights some important challenges faced by the field of offline MARL, but does not capture _all_ such challenges. We hope our contributions will make it easier for authors to align and compare their future work, but we ultimately realise that our efforts require vetting by the community, to be tested and validated over time. We welcome such engagements, to collectively chart a path forward.

Table 5: Three return metrics—the Final, \(\)Maximum\(\), (Average)—from the two baseline algorithms in two respective environments, shown across various scenario and dataset quality combinations. Each result is presented as the mean and standard deviation, over 10 seeds. For the Final return, boldface indicates the best performing algorithm, and an asterisk (*) indicates a metric is not significantly different from the best performing metric in that situation, based on a heteroscedastic, two-sided t-test with 5% significance.