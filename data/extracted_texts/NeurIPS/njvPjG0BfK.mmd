# HardCore Generation: Generating Hard Unsat

Problems for Data Augmentation

 Joseph Cotnareanu\({}^{1,2,3,}\)1  Zhanguang Zhang \({}^{4}\)  Hui-Ling Zhen\({}^{4}\)

Yingxue Zhang\({}^{4}\)  Mark Coates\({}^{1,2,3,}\)2

McGill University\({}^{1}\), International Laboratory on Learning Systems (ILLS)\({}^{2}\),

Mila Quebec Institute\({}^{3}\), Huawei Noah's Ark Lab\({}^{4}\)

joseph.cotnareanu@mail.mcgill.ca,mark.coates@mcgill.ca,

{zhanguang.zhang,zhenhuiling2,yingxue.zhang}@huawei.com

###### Abstract

Efficiently determining the satisfiability of a boolean equation -- known as the SAT problem for brevity -- is crucial in various industrial problems. Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets. The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods. In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles. In this paper we address both by identifying and manipulating the key contributors to a problem's "hardness", known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes2.

## 1 Introduction

The boolean satisfiability problem (the SAT problem) emerges in multiple industrial settings such as circuit design (Goldberg et al., 2001), cryptoanalysis (Ramamoorthy and Jayagowri, 2023), and scheduling (Habiby et al., 2021). While machine learning is not well suited for solving SAT problems -- solvers are typically required to have perfect accuracy and return correct proofs -- it does have applications in predicting wall-clock solving time for a given solver, which is important for algorithm selection (Kadigoglu et al., 2010; KhudaBukhsh et al., 2009) and benchmarking (Fuchs et al., 2023). SAT has also been gaining attention in Large-Language-Model reasoning, as it is a natural tool for interacting with the propositional-logical structure of many reasoning problems (Ye et al., 2023).

A major challenge for SAT-related learning is the scarcity of high quality, reasonably homogeneous, real-structured data. The most commonly-used datasets have been compiled via a series of annualInternational SAT Competitions. The industrial origins of the compiled instances differ substantially, so the dataset is highly heterogeneous. The data is a good test for heuristic SAT solvers but for data-driven learning methods, this heterogeneous, sparse data is unsuitable. More complex models are thus forced to use randomly generated data (Selsam and Bjorner, 2019). This is problematic because the hardness-inducing dynamics in industrial data are very different from those in randomly generated problems. Training or testing on most existing randomly generated data provides little insight into how a model will perform on real industrial problems (Balyo et al., 2022).

Recently, deep-learning methods have been introduced to generate more realistic SAT instances. Early models (Wu and Ramanujan, 2019; You et al., 2019; Garzon et al., 2022) can generate instances that are structurally similar to original instances, but the problems are considerably easier to solve, a phenomenon called hardness collapse. Preserving hardness is essential, as generating only very easy problems renders the resultant dataset ineffective for distinguishing the best-performing solver from the worst. Additionally, such datasets fail to help the model learn to predict real runtimes. A recent study has succeeded in preserving hardness (Li et al., 2023). Unfortunately, the resultant method is prohibitively computationally expensive for synthetic data generation and augmentation for deep-learning. It can take over a week to generate a limited number of new problem instances. We summarize the cost/hardness trade-offs in Figure 1.

In this work, we take advantage of the connection between a problem's _core_ and its hardness. The core is comprised of the identifiable minimal subsets of a boolean SAT problem that are unsatisfiable (UNSAT). Our strategy is to preserve the core of an original instance while iteratively adding random clauses to construct similar, but sufficiently diverse, problem instances that can enhance learning. To do this, we need to detect the core after each iteration. Unfortunately, traditional core detection algorithms are slow and can take hundreds of seconds, as they often require to solve the SAT problem (Wetzler et al., 2014). Clearly, such an algorithm is impractical for building a fast generator, as core detection needs to be performed hundreds of times for every instance we generate.

To address this, we rephrase core detection as a binary node classification algorithm (core/not-core). We train a graph neural network to perform the task. Importantly, we can circumvent the data starvation issue, because our random data generation procedure generates hundreds of example instances that can be used for training the core detection algorithm. We can also take advantage of the fact that while it is important to identify the vast majority of clauses that belong to the core, we can tolerate a relatively high number of false-alarms by post-processing with a fast pruning algorithm.

We make the following novel research contributions:

* We propose a novel method for SAT generation that is the first that can both (i) _preserve hardness_ and (ii) _generate instances in a reasonable time frame_. We can thus generate thousands of hard instances to augment a dataset in minutes or hours.
* We demonstrate experimentally that our proposed procedure preserves the key aspects of the original instances that impact solver runtimes. This hardness preservation is crucial when augmented dataset is used to learn to predict solver times, a vital task for solver benchmarking and selection.
* We illustrate the value of our augmentation process for solver runtime prediction. On an example dataset, our augmentation process reduces mean absolute error by 20-50 percent. In contrast, all other generation algorithms achieve no statistically significant improvement.

## 2 Background: Boolean Satisfiability

Definitions and NotationThe Boolean Satisfiability Problem (SAT) is the problem of determining whether there exists an assignment of variable values that satisfies the given Boolean formula, rendering it true. Typically, a SAT instance is represented in Conjunctive Normal Form (CNF), which is written as a conjunction (logical AND) of disjunctions (logical OR), for example

Figure 1: Our method (HardCore) achieves the best trade-off of inference cost and SAT-problem hardness.

(\( A B C(A C)( B C)\). The signed version of each variable that appears in the formula is known as a literal. For example, \(A\) and \( A\) are both literals of the variable \(A\)(Biere et al., 2009, Chapter 2).

Another useful representation of a CNF is as a set of sets, where each set (referred to as a clause) represents a disjunction in the CNF and contains the literals included in that disjunction. Denote the \(i\)-th clause in the formula \(f\) by \(c_{i}\) and the \(j\)-th literal in clause \(c_{i}\) as \(l_{j}\). If there are \(n_{c}\) clauses in \(f\) and \(n_{l_{i}}\) literals in clause \(c_{i}\), we can express the formula as \(c_{i}=_{j=1}^{n_{i_{l}}}l_{j}\), \(f=_{i=1}^{n_{c}}c_{i}\).

Core DefinitionGiven an unsatisfiable (UNSAT) instance \(U\), there is a subset of clauses called a Minimally Unsatisfiable Subset (MUS) or a Core. This subset is the smallest possible subset of clauses from \(U\) that is UNSAT (Biere et al., 2009, Chapter 11).

Graph Representation of CNFsThere are several common CNF graph representations. In this work, we use the Literal-Clause Graph (LCG), an undirected and bipartite graph. Each node in the first set of nodes represents a clause and each node in the second represents a literal. We construct an edge for each occurrence of a literal in a clause; the set of undirected edges \(e\) is defined as \(e=_{i=1}^{n_{c}}_{j=0}^{n_{l_{i}}}(l_{j_{c_{i}}},c_{i})\).

## 3 Related Work

### Deep-learned SAT generation

The problem of learned generation for SAT problems was first established in 2019 with SATGEN (Wu and Ramanujan, 2019), motivated by a lack of access to industrial SAT problems. SATGEN used a graph generative adversarial network (GAN) to generate graph representations of SAT problems.

G2SAT (You et al., 2019) represents problems as graphs. The graphs are progressively split into small trees, and a graph neural network (GNN) is trained to discern which trees should be merged to restore the original graph. While innovative, the method is slow due to its need to sample many tree pairs to form a SAT problem of sufficient size. The most recent improvement on the G2SAT framework, HardSATGEN (Li et al., 2023), includes some domain-inspired considerations in its design, such as communities and cores. HardSATGEN is the first deep-learned SAT generation method that can generate problems which are not trivial to solve for solvers: often the generated problems take nearly as long or even longer for a solver to solve than the corresponding seed problem. Unfortunately, however, the core awareness aspects of the design cause HardSATGEN to be extremely slow, making it challenging to use in any setting that needs many new instances.

W2SAT (Wen and Yu, 2023) follows an approach more similar to the original SATGEN. It employs a low-cost general graph generation model, and obtains new SAT problems via graph decoding. W2SAT is extremely efficient, but like G2SAT, it is incapable of generating hard problems. G2MILP (Geng et al., 2023). is designed to generate Mixed Integer Linear Programs (MILPs), which are the general case of SAT. A naive modification allows us to use G2MILP to generate SAT problems. The method is nearly as efficient as W2SAT, but also struggles to generate hard instances.

### Core Prediction

Core Detection can be a helpful tool for understanding UNSAT problems. Cores are often seen as a strong indicator of the hardness of an UNSAT problem (Ansotegui et al., 2008). There are multiple classical, verifiable methods for Core Detection, with the current standard being Drat-Trim (Wetzler et al., 2014). Drat-trim requires that the problem be solved once by a SAT solver, which is very slow. In response to this, Neurocore (Selsam and Bjorner, 2019) was designed to predict the core of a SAT problem. Neurocore converts the input problem to a graph and uses a GNN to predict cores. Strangely, however, Neurocore does this on variables rather than clauses. Cores are defined to be subsets of clauses, rather than variables, and so this choice seems unnatural. Neurocore strives to be a machine-learning based variable-selection heuristic for SAT solvers, which motivates the focus on variables.

## 4 Problem Statement

Given a training set of UNSAT CNFs \(S=\{f_{1},f_{2},...,f_{m_{S}}\}\), and a corresponding set of label vectors \(R=\{_{1},_{2},...,_{m_{S}}\}\), we wish to train a generative model \(G\) that can construct new examples. The label vector \(^{d}\) represents the hardness of the SAT problem and we model it as a deterministic mapping, i.e., \(_{1}=g(f_{1})\). In our experiments, the vector is derived by recording the SAT solving time for a pre-specified set of SAT solvers.

We assume that the \(m_{S}\) CNFs in the training set are i.i.d. examples from an underlying distribution \(\). We denote the generative model distribution by \(_{G}(S)\), highlighting that it is dependent on the random training set \(S\). We can obtain a new dataset of \(m_{G}\) i.i.d. samples \(S_{G}\) using the generative model. The total number of samples in the augmented set \(\) is then \(m_{S}+m_{G}\).

Our primary goal is to derive a generative procedure that produces sufficiently representative but also diverse samples such that the error obtained by training a model on the augmented dataset \(\) is less than that obtained by training on the original dataset \(\). As an example task, we consider the prediction of runtime for a candidate solver. In this case, the appropriate loss function is the absolute error between the predicted time and the true time.

Beyond this, we are also interested in the distance between the distributions \(\) and \(_{G}\). We examine this through the lens of hardness label vectors. The application of \(g\) to the CNF descriptors generated according to \(\) or \(_{G}\) induces distributions in \(^{d}\). To evaluate the similarity of the original and generated instances, we calculate the empirical maximum mean discrepancy (MMD) distance between these induced distributions.

## 5 Methodology

Our generation strategy can be broken into three steps: (1) extraction of the core from a seed instance; (2) addition of random new clauses, generated with low cost; and (3) iterative core refinement. Figure 2 provides an overview of the key core refinement procedure. It consists of a two-step cycle of (a) high-speed core extraction using our novel GNN-based method; and (b) unconflicted literal addition to break any undesirably easy core.

### Generating Hard Instances

Trivial CoresCores are the primary underlying hardness providers in UNSAT instances, because a solver must only determine that a subset of a CNF is UNSAT for the whole CNF to be UNSAT, and

Figure 2: **Core Refinement**. The core refinement process comes in two steps: (1) Core Prediction, in which we use a GNN-based architecture to identify the core of the generated instance; and (2) De-Coring, in which we add a non-conflicted literal to a clause in the core, rendering the core satisfiable and giving rise to a new, harder minimal unsatisfiable subset (core). As steps (1) and (2) are repeated, the easiest core of the problem is gradually refined, raising the hardness of the generated instances.

a core is the smallest subset of clauses of a CNF that is UNSAT. Small cores with few clauses are generally easier to solve due to less variable assignment combinations. An example of a trivial core is \((A B)\)\(( A B)\)\((A B)\)\(( A B)\).

Whenever we add a new random clause to an UNSAT instance, there is the danger of creating a trivial core. For example, consider an UNSAT instance which includes three of the clauses from the example above: \((A B)\)\(( A B)\)\((A B)\). If during generation we unknowingly add the clause \(( A B)\), the UNSAT instance's large (hard) core will be replaced by a trivial one, leading to hardness collapse. Maintaining awareness of cores and potential cores in a CNF as we perform modifications is challenging. We take a different approach, which we refer to as _Core Refinement_.

Core RefinementThe Core Refinement process is made up of two steps that are repeated \(n\) times, where \(n\) is the number of generated clauses. The procedure is depicted in Figure 2. The first step of the process is to identify the core of the generated instance. The addition of random clauses during generation is very likely to create a core that is trivially easy to solve and it may not be the same as the core of the original instance. Once we have detected this easy core, we make it satisfiable by adding a new literal to a clause in the core. The addition of a single, flexible literal eliminates the constraints of the core and makes it possible to satisfy.

Returning to the previous example, the UNSAT CNF \((A B)\)\(( A B)\)\((A B)\)\(( A B)\) can be made satisfiable by modifying any of the clauses in this fashion: \((A B C)\)\(( A B)\)\((A B)\)\(( A B)\). The introduction of literal \(C\) in the first clause means that \((A=0,B=0,C=1)\) is now a satisfying solution.

As these two steps are repeated, the core of the instance gradually becomes larger and is likely to be more difficult. The process ends after a fixed number of iterations. In our experiments, we choose this to be the number of generated clauses. Since the hardness of the core is the hardness of the instance (Ansotegui et al., 2008), the refinement process can be seen as progressively raising the hardness of the problem.

Underlying Hard Core GuaranteeThe Core Refinement process is designed to repeatedly eliminate easy cores, so after each iteration, the core becomes harder. Finally, after many iterations, we hope that the remaining core is as hard as the original instance. This process can only be guaranteed to lead to a hard core if an underlying hard core exists in the instance at the start of the refinement process. Refinement then whittles away easy cores until only the hard one remains.

There is a possibility of creating a hard core through the random generation of clauses, but we cannot rely on this. We must introduce an element to our design to ensure there is a hard core. To achieve this we identify cores from the original instances and include them in the generated instances.

### Core Prediction

We have two critical objectives for our method: low cost and hard outputs. While the Core Refinement process serves us well in generating hard instances, a naive implementation using existing core detection algorithms is unacceptably expensive in terms of computation requirements. Current core

Figure 3: **Core Prediction GNN Architecture**. We construct our GNN using three parallel message passing neural networks (MPNN) whose calculated node embeddings are aggregated at each layer to form the layer’s node embeddings. Readout is done by taking the sigmoid of a fully-connected layer on clause node embeddings and thresholding. Training is supervised by taking a binary classification loss between the true core labels and the clause nodes’ core prediction probabilities.

detection algorithms first solve the SAT problem, making Core Detection NP-Complete (Wetzler et al., 2014).

We adopt the strategy of approximating the Core Detection algorithm. Since an instance can be naturally represented using a bipartite graph, and the goal of core detection is binary classification of each clause, we expect that a graph neural network is a promising approach.

Graph ConstructionWe represent each instance as a graph as outlined in Section 2. We make two changes: (a) we add message-passing edges to connect matching positive and negative literals (e.g, \( A\) and \(A\)); (b) we replace each undirected edge with two directed edges. These changes are designed to facilitate the diffusion of information in the GNN. We denote the set of literal-literal message passing edges by \(_{ll}=_{i=1}^{n_{v}}(l_{i+},l_{i-})\), where \(n_{v}\) is the number of variables in the instance. We denote the set of literal-to-clause directed edges by \(_{lc}=_{i=1}^{n_{c}}_{j=0}^{n_{l_{c_{i}}}}(l_{j_{c_{i }}},c_{i})\). We denote the set of clause-to-literal directed edges by \(_{cl}=_{i=1}^{n_{c}}_{j=0}^{n_{c_{i}}}(c_{i},l_{ j_{c_{i}}})\).

GNN ArchitectureGiven the heterogeneous nature of our graph, arising from different node and edge types, we use three Graph Message Passing models (one for each edge type), as described in Figure 3. We couple these models by averaging their embeddings after each layer. We define a single layer where \(\) is a non-linear activation function. Finally, we obtain a core membership probability for each clause node by passing the embeddings through a fully connected linear readout layer followed by a sigmoid function to the clause node embeddings. We threshold the values to obtain positive and negative classifications of core membership:

\[h^{l+1} =((GNN(,_{cl},h^{l})+GNN( ,_{lc},h^{l})+GNN(,_{ll},h^{l})))\,,\] (1) \[out =_{>0.5}((xh_{c}^{L}+b))\,.\] (2)

TrainingOur augmentation process is motivated by a scarcity of data. We must therefore address this when training the core detection GNN. We achieve augmentation of the available data by executing the generation pipeline described above for a small number of instances, using a slow, traditional but proof-providing tool for Core Detection in the Core Refinement process. By saving the instance-core pair after each iteration of the core refinement process, we can construct sufficient supervision data for training the Core Prediction GNN model. Although the instance-core pairs we construct this way are correlated, there is sufficient variability for the GNN model to generalize well to other instances. We train the model using the standard binary cross-entropy loss function. For experimental results showing the performance of our Core Prediction model, see row titled "LEC" in Table 4 in the Appendix B.

## 6 Experiments and Results

### Experimental Setting

Proprietary Circuit Data (LEC Internal)This LEC Internal data is a set of UNSAT instances which are created and solved during the Logic Equivalence Checking (LEC) step of circuit design. LEC needs to be performed after certain circuit optimization steps to ensure that the optimization process has not corrupted the logic of the circuit. If the logic is uncorrupted, the created SAT problem will be UNSAT. Since it is extremely rare that these optimizations in fact corrupt the circuit, more than 99% of LEC instances are UNSAT. For each generative method, we will generate 5 problems for each problem in the data.

Synthetic Data (K-SAT Random)Acknowledging the importance of reproducibility, we also provide results on synthetic data. This data is generated by randomly sampling a CNF with \(m\) clauses of \(k\) literals over \(n\) variables. Clauses are sampled without replacement. We have previously argued that random data differs from real data in important ways that make it unsuitable for machine learning applied to real problems. Holding to this view, we use this data primarily to provide a surrogate to the internal data for experimental reproduction purposes, rather than to present results on a second dataset. For each generative method, we will generate 5 problems for each problem in the data. For details concerning both the LEC Internal and K-SAT data, see Table 3 in the Appendix.

Note on Public Competition DataA large and commonly-used public dataset for SAT is the SAT Competition data (Heule et al., 2019). Unfortunately, as argued previously, this data is generally ill-positioned for machine learning as it is highly heterogeneous. Despite this, we recognize its value and importance as a widely accepted dataset. Thus, we include in Tables 5 and 6 of Appendix B final results on the Data Augmentation experiment done below, done on the "Tseitin" and "FDMUS" families found in the public competition data. Our method shows improvement on the un-augmented data consistent with our findings on the synthetic and proprietary data presented in this section.

Data splits for training the HardCore GNN and the runtime-prediction modelThere are three separate groupings of the dataset: (i) Core Prediction training data, (ii) generation seeding data, and (iii) the remaining data. This split is chosen randomly. Core Prediction training data can be small (we used 15 problems), because we use each problem as a seed instance 5 times for generation followed by core-refinement with a traditional core detector. Saving problem-core pairs at each step, we obtain 15,000 training pairs for the core-predictor model. The seeding data are used to seed HardCore once the core predictor is trained in order to obtain generations to evaluate. These generations are then compared against the seed data for runtime similarity. Finally, these generations (and their seeds) are used to train a runtime-predictor model, which is evaluated on the remaining un-used data.

SAT SolversWe select 7 solvers for hardness analysis: Kissat3 (Biere et al., 2020), Bulky (Fleury and Biere, 2022), UCB (Cherif et al., 2022), ESA (Cherif et al., 2022), MABGB (Cherif et al., 2022), moss (Cherif et al., 2022) and hywalk (Chowdhury, 2023). These solvers exhibit complementary performance characteristics: when some of these solvers perform well on certain instances, some perform very poorly. This results in a diverse runtime distributions in our analysis. We run our experiments on a Intel(R) Xeon(R) Platinum 8276 CPU @ 2.20GHz cpu and 3 Nvidia Tesla V100 GPUs.

We compare to the following baselines:

* **HardSATGEN Li et al., 2023**): A high-cost split-merge generator with community structure and core detection that is capable of generating hard instances.
* **W2SAT Wen and Yu, 2023**): A low-cost generative method that utilizes a less common SAT graph representation which was reported to generate very easy problems.
* **G2MILP Geng et al., 2023**): A low-cost VAE-based generative model designed for the general case of SAT: MILPs.

### Research Questions

Our work is motivated by the goal of **fast** generation of **hard** and **realistic** UNSAT datasets for **data augmentation**. Given these goals, we now establish our strategy for evaluating our model, identifying the key research questions that our experiments explore.

#### 6.2.1 Question 1: Is the method able to generate hard instances?

In order to quantify 'hardness', we choose the wall-clock solving time for each solver as a metric. We deem a set of generated instances 'hard' if the average solver runtime is at minimum 80% of the original dataset's average hardness. If average solver time for the set of generated instances is below 5%, we consider that _hardness collapse_ has occurred.

In Table 1 we compare generated with original hardness. W2SAT and G2MILP both suffer hardness collapse, whereas HardSATGEN and HardCore generate hard instances.

#### 6.2.2 Question 2: Is the method fast?

We measure generation speed by the time required to generate an instance (in seconds). We evaluate this by measuring the wall-clock time of each model during inference and dividing by the number of generated instances. Generally, a method should be able to generate hundreds of instances per hour so that we can augment a dataset in a reasonable time frame.

In Table 1, the division between fast and slow procedures is very clear: W2SAT, G2MILP, and HardCore all exhibit similar instance generation times, with W2SAT being the fastest. In contrast,HardSATGEN takes close to 2 hours to generate a single instance. To generate 1000 LEC instances at this speed we would need 75 days.

2.3 Question 3: Is the method able to generate datasets that are similar to the original datasets in terms of hardness distribution?

Although past work such as Li et al. (2023); Wen and Yu (2023); You et al. (2019) has examined graph statistics such as modularity and clustering coefficients, we find little evidence that these are indicative of the hardness of generated instances. Instead, we focus on the similarity of the distributions of the hardness vectors because hardness is of primary importance when working with SAT problems.

    & W2SAT & HardSATGEN & G2MILP & HardCore \\  Hardness (\%) & \(\)0 & 267 & \(\)0 & **176** \\ Time per instance (s) & **1.2** & 6441 & 3.3 & 4.3 \\ Similarity (MMD) & — & 0.492 & — & **0.004** \\   

Table 1: Evaluation of generated datasets on LEC data. Hardness level (%): percentage of runtime of generated dataset relative to original dataset, closer to 100% is better. Speed (s): average time cost to generate one instance, lower is better. Maximum Mean Discrepancy (MMD): distance between distributions of generated and original datasets, lower is better.

Figure 4: HardCore (Left) and HardSATGEN (Right). Boxplots of runtimes per solver for Original (Green) and Generated (Blue) instances on LEC data. HardCore appears to produce per-solver distributions which are much closer to the original than HardSATGEN, which tends to produce high-variance and on-average much harder problems than the original.

Figure 5: LEC Internal Rank 1 Solvers. We compare original and synthetic best-solver observations for HardCore (left) and HardSATGEN (right).

As G2MILP and W2SAT exhibit hardness collapse, we only compare HardSATGEN and HardCore for runtime distribution analysis. Note that due to HardSATGEN's high cost, we can only generate 50 LEC instances and 50 K-SAT instances within 3 days. In the following experiments, we compare "original" and "generated" data. Here, "original" refers to only those instances used as seeds during inference for each model; "generated" refers to the outputs. Hence, the "original" sets for HardSATGEN and HardCore are different because the number of seed instances is different (due to time constraints we are limited in how many HardSATGEN instances we can generate). We evaluate the similarity between original and generated data through the Maximum Mean Discrepancy (MMD) metric, the runtime distribution, and the best solver distribution.

As shown in Table 1, HardCore achieves runtime distributions far closer to the original distributions compared to HardSATGEN with respect to the MMD metric. We calculate these values by taking the MMD between the set of instances used as seeds during generation (a subset of the training set) and the corresponding set of generated instances. We note that while HardCore achieves low MMD, the solving time of individual instances is considerably different from that of their associated seeds. This implies that low MMD of HardCore is not achieved by replicating or barely modifying seed instances. Our later experiments investigating augmentation suggest that there is sufficient diversity being injected in the generated instances.

In Figure 4, we visually compare the per-solver runtime distribution of HardCore's generated datasets to the corresponding original datasets. HardCore produces per-solver distributions which are visibly much closer to the original distributions than HardSATGEN. In Figure 5, we see a striking similarity between the HardCore distribution of best-performing solvers and the original distribution, indicating that the HardCore synthetic instances are solved most efficiently by the same solvers as the original instances, in a distributional sense. Meanwhile, a greater discrepancy can be seen between original and HardSATGEN-generated data, particularly for solvers 5 and 6. A full histogram of LEC solver ranks is shown in Figure 6 of Appendix B.

2.4 Question 4: Can we successfully augment training data with the method's generated data for machine learning?

We address the task of runtime prediction and compare the performance of two models: one trained on only original data and the other trained on a dataset augmented with generated instances. We train the SATzilla model to predict solver runtime of one specific solver on a given instance. We repeat this for each of the 7 solvers. We calculate the MAE of the predicted total runtime for each solver and average over the solvers. We compare HardCore, W2SAT and two versions of HardSATGEN: (i) HardSATGEN-Strict and (ii) HardSATGEN-\(N\). For HardSATGEN-Strict, we only generate as many instances as possible in the time it takes HardCore to generate the desired number of instances. For HardSATGEN-\(N\), we generated \(N\) instances, where \(N\) was selected as the number that could be generated in approximately 3 days of computation. We also compare to the un-augmented training sets and refer to it as Original.

In order to observe performance over varying sizes of training data, we conduct this experiment for several quantities of original training instances, which is denoted Data Size. Three augmentation instances are generated per original instance, and augmentation is only allowed by using the original instances in the training set. Validation sets are selected from the original data only, with an 80/20 split train/validation split. For LEC the test-set is made up of 10000 randomly selected problems which were not selected for training or validation. For K-SAT the test-set is made up of the problems which were not picked for train/validation from the 1351 original instances.

Table 2 shows that for both K-SAT random data and LEC Internal dataset, training on data augmented using HardCore leads to a 20-50 percent reduction in MAE. The gain of data augmentation increases with larger data size. In contrast, no other data generation method leads to a comparable improvement.

### GNN generalization to other datasets: do we have to re-train for new unseen circuits?

In order to measure GNN generalization to new data without re-training, we create a new split of the LEC data. Each problem in the LEC data can be traced back to one of 29 circuits. By randomly splitting circuits into training circuits and test circuits (and then building training and evaluation sets with their respective problems), we can measure generalizability. Note that we would not expect the model to generalize to problems derived from a completely different application domain (although fine-tuning a previously model in a domain adaptation strategy might be interesting to explore).

In row titled "Circuit-Split LEC" in Table 4 of Appendix B we report the GNN performance on this experiment. In the paper we discussed that Core recovery is the priority, because if we falsely classify true-positives then we may be unable to de-core the current core (since the necessary clause may be undetected, whereas if we mis-classify true-negatives then we will simply de-core a non-core clause. Given enough iterations of core-refinement, a true-positive clause will eventually be selected for de-coring (since the clause for de-coring is randomly selected from among the detected clauses). With this in mind, the threshold hyper-parameter which is used on the sigmoid outputs at model readout becomes a useful parameter in cases where classification performance is weakened: we can boost Core Recovery (recall) by lowering the threshold. Tuning this threshold is very low-cost: testing a thousand problems takes 500 seconds on a GPU. We find that by testing values \([0.1,0.3,0.5,0.7,0.9]\) -- which takes 25 minutes -- we can tune the threshold to provide similar recall to the in-distribution model.

## 7 Limitations

The primary limitation of our work is that it is restricted to UNSAT problems. While some SAT applications are almost entirely UNSAT (e.g., circuit design), many are not. With our proposed approach, this limitation is unavoidable because cores are only present in UNSAT problems. However, there is a concept for SAT problems analogous to the core, known as a _backbone_. Focusing on preserving the backbone is a potential avenue for an analogous method for satisfiable SAT problems.

Another limitation is that our work relies solely upon empirical results to demonstrate its efficacy, and these results are only presented on two datasets, one of which is synthetic. To partially address this concern, we conducted several trials and statistical significance testing to ensure the reliability of our empirical analysis.

Another limitation is that our method struggles to scale to extremely large SAT problems. As the size of the SAT problem increases, memory and computation costs scale in polynomial complexity, meaning that SAT problems which have millions of clauses are currently out of reach for this method. For a more in-depth discussion of scaling, please see the Scaling subsection A.3 in Appendix A.

## 8 Conclusion

We present a fast method for generating UNSAT problems that preserves hardness. Existing deep-learned SAT generation algorithms either (1) are incapable of generating problems that are even 5% as hard as the example input problems; or (2) can generate hard problems but take many hours for each instance. Our proposed method targets the core of a SAT problem and iteratively performs refinement using a GNN-based core detection procedure. Our experiments demonstrate that the method generates instances with a similar solver runtime distribution as the original instances. For a more challenging industrial dataset, we show that data augmentation using our proposed technique leads to a significant reduction in runtime prediction error.

    &  &  \\  Data Size & 10 & 20 & 30 & 40 & 100 & 200 & 300 & 400 & 500 \\  HardSATGEN-\(N\) & 2416 & 2306 & 2172 & 2182 & 666 & 797 & 605 & 617 & 463 \\ HardSATGEN-Strict & 2179 & 2578 & 2488 & 2456 & 627 & 742 & 565 & 638 & 513 \\ W2SAT & 2606 & 2046 & 1807 & 1377 & 724 & 704 & 634 & 611 & 535 \\ Original & 2750 & 2743 & 2109 & 1449 & 707 & 795 & 557 & 606 & 526 \\ HardCore & **2156** & **1796*** & **1615** & **930*** & **514** & **481*** & **369*** & **282*** & **338*** \\   

Table 2: MAE of Runtime Prediction averaged across 7 solvers and 15 trials. Asterisks are placed at the best result which passes the Wilcoxon pairwise ranking test against the second-best for \(p<0.05\). For a boxplot visualization showing each trials result, see Appendix Figure 7