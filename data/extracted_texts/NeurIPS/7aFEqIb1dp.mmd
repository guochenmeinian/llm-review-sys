# Untrained Neural Nets for Snapshot Compressive Imaging: Theory and Algorithms

Mengyu Zhao\({}^{1}\), Xi Chen\({}^{1}\), Xin Yuan\({}^{2}\), Shirin Jalali\({}^{1}\)

\({}^{1}\) ECE Department, Rutgers University, New Brunswick

\({}^{2}\) School of Engineering, Westlake University

Shirin Jalali is the corresponding authors <shirin.jalali@rutgers.edu>.

###### Abstract

Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency. In this paper, we focus on SCI recovery algorithms that employ untrained neural networks (UNNs), such as deep image prior (DIP), to model source structure. Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios. We first develop a theoretical framework for characterizing the performance of such UNN-based methods. The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN. We also employ the recently proposed bagged-deep-image-prior (bagged-DIP) idea to develop SCI Bagged Deep Video Prior (SCI-BDVP) algorithms that address the common challenges faced by standard UNN solutions. Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions. Code is publicly available at https://github.com/Computational-Imaging-RU/SCI-BDVP.

## 1 Introduction

Snapshot Compressive Imaging (SCI) refers to imaging systems that optically encode a three-dimensional (3D) data cube into a two-dimensional (2D) image and computationally recover the 3D data cube from the 2D projection. As a novel approach in computational imaging, SCI has attracted significant attention in recent years. Initially proposed for spectral imaging , its application has since expanded to various fields, including video recording , depth imaging , and coherence tomography  (Refer to  for a comprehensive review).

The key advantage of SCI systems lies in significantly accelerating the data acquisition process. Traditional hyperspectral imaging methods, for example, often encounter bottlenecks due to their reliance on spatial or wavelength scanning, leading to time-consuming operations. In contrast, hyperspectral SCI systems capture measurements across multiple pixels and wavelengths in a single snapshot, effectively bypassing this limitation .

The optical encoding process in SCI systems can be mathematically modeled as a linear measurement system, characterized by a sparse and structured sensing matrix, commonly referred to as a'mask'. Consequently, SCI recovery algorithms aim to reconstruct high-dimensional (HD) 3D data from ahighly underdetermined system of linear equations. A wide range of SCI recovery methods has been proposed in the literature, which can broadly be categorized into:

**Classic approaches:** These methods model source structure using convex regularization functions and employ convex optimization techniques (e.g., [7; 8; 9; 10]). While robust to measurement and source distribution non-idealities, they are typically limited to simpler structures and challenging to extend to 3D HD data cubes central to SCI applications. **DNN-based methods:** These approaches use deep neural networks (DNNs) to capture complex source structures, learning from training data. They can be further categorized as: i) End-to-end solutions (e.g., [11; 12; 13; 14; 15; 16]); ii) Iterative plug-and-play solutions (e.g., [17; 18]); iii) Unrolled methods (e.g., [19; 20; 21; 22; 23; 24]). While these methods extend beyond simple structures to model intricate source patterns, they require extensive training data, often struggle with generalization, and are computationally intensive.

An alternative approach to SCI recovery involves using UNNs, such as deep image prior (DIP)  or deep decoder , to model the source structure. These methods capture complex source structures without requiring any training data. Existing UNN-based SCI solutions either recover the image end-to-end in one shot  or employ iterative methods akin to projected gradient descent (PGD) . Despite their advantages, these approaches often exhibit lower performance compared to pre-trained methods and may require additional data processing steps for enhancement.

In this work, we focus on leveraging UNNs to address the SCI problem. We begin by establishing a theoretical framework for analyzing UNN-based methods, providing insights into optimizing the adjustable SCI masks, under both noise-free and noisy measurements. We then explore DIP-based algorithms and introduce SCI-BDVP solutions. Our results demonstrate the robustness of these solutions to measurement noise and their competitive performance across diverse datasets, using a consistent set of parameters.

### Contributions of this Work

Theoretical:We theoretically characterize the performance of DIP-based SCI recovery optimization for both noise-free and noisy measurements. Using our theoretical results, we establish an upper bound on the number of frames that can be recovered from a single 2D measurement, as a function of the dimensions of the DIP. Furthermore, we show how the developed theoretical results enable us to optimize the parameters of the masks for both noisy and noise-free cases, enhancing the performance of the recovery process.

Algorithmic:Inspired by the newly proposed bagged-DIP algorithm for the problem of coherent imaging , developed to address common shortcomings of DIP-based solutions for inverse problems, we explore the application of bagged-DIP for SCI recovery. We conduct extensive experimental evaluations, demonstrating the following: **i)** Confirmation of our theoretical results on the optimized masks for both noise-free and noisy measurements. **ii)** The proposed SCI-BDVP solution robustly achieves state-of-the-art performance among UNN-based solutions in the case of noise-free measurements. **iii)** In scenarios with noisy measurements, our proposed method achieves state-of-the-art performance among both end-to-end supervised and untrained methods.

### Notations

Vectors are represented by bold characters like \(\) and \(\). \(\|\|_{2}\) denotes the \(_{2}\) norm of \(\). For \(^{n_{1} n_{2}}\), \(()^{n}\) denotes the vectorized version of \(\), where \(n=n_{1}n_{2}\). This vector is created by concatenating the columns of \(\). Given \(,^{n_{1} n_{2}}\), \(=\) denotes the Hadamard product of \(\) and \(\), such that \(Y_{ij}=A_{ij}B_{ij}\), for all \(i,j\). Sets are represented by Calligraphic letters, like \(,\). For a finite set \(\), \(||\) denotes the number of elements in \(\). Throughout the paper, \(\) refers to the logarithm in base 2, while \(\) denotes the natural logarithm.

## 2 Related Work

**UNNs for SCI.** While the majority of SCI recovery algorithms developed for various applications fall under classic optimization-based methods (e.g., [8; 9; 10]) or supervised DNN-based methods , in recent years, there has been increasing interest in leveraging UNNs in solving inverse problems. In SCI recovery, this trend has been motivated by the diversity of applications and datasets encounteredin various SCI applications, necessitating the availability of pre-trained denoising networks tailored to different resolutions and noise levels for various datasets. Another challenge with these traditional solutions is their robustness to various problem settings, such as measurement noise. These challenges have spurred a notable interest in developing solutions that harness the ability of DNNs to capture complex source models while not relying on training data.

While deep image priors (DIPs) have been applied to various inverse problems [29; 30; 31], their application to SCI recovery has been limited. The authors in  developed an iterative DIP-based solution for hyperspectral SCI. To enhance the performance and address the challenges faced by DIP-based methods in terms of falling into local mimins, they initialize the algorithm by the solutions obtained by GAP-TV . In , the authors propose Factorized Deep Video Prior (DVP), which is a DIP-based SCI recovery algorithm for videos, which is based on separating the video into foreground and background and treating them separately.  develops a DIP-based solution for compressed ultrafast photography (CUP), where in addition to the normal SCI 2D measurement and additional side information consisting of the integral of all the frames (referred to as the time-unsheared view in ) is also collected. The video is reconstructed using an end-to-end approach using the DIP to enforce the source model. In , the authors leverage the concept of video snapshot compressive imaging (SCI) reconstruction to develop an algorithm for snapshot temporal compressive microscopy. They propose an iterative algorithm that utilizes UNNs to incorporate the source structure.

In the context of image recovery from underdetermined measurements corrupted by speckle noise, the authors in  recently proposed the idea of bagged-DIP, which is based on independently training multiple DIPs operating at different frame sizes and averaging the results. In this paper, we extend the idea to videos and construct a bagged-DVP, which as we show in our experimental results robustly achieves state-of-the-art performance among all UNN-based SCI video recovery methods.

Mask optimization.In various SCI applications, one can design the masks, which are typically binary-valued, and used for modulating the input 3D data cube. This naturally raises the question of optimizing the masks to improve the performance. To address this problem, several empirical works have designed solutions that simultaneously solve the SCI recovery problem and optimize the masks. In , the authors design an end-to-end autoencoder network to train the reconstruction and mask simultaneously for video data and find the trained mask has some distribution such as non-zero probability around 0.4 and varies smooth spatially and temporally. Similarly, in , deep unfolding style networks are trained to simultaneously reconstruct 3D images and also optimize the binary masks. They show that for the _empirically_ jointly optimized masks have a non-zero probability of around \(0.4\). The authors in  design an end-to-end VIT-based SCI video recovery solution that simultaneously learns the reconstruction signal and the mask. They consider a special type of mask that constrained by their hardware design.

Due to the highly non-convex nature of the described joint optimization problem, empirically-jointly-optimized solutions are likely to converge to suboptimal results. Furthermore, the optimized solution, inherently dependent on the training data, lacks theoretical guarantees. To address these limitations,  employed a compression-based framework to theoretically optimize the binary-valued masks in the case of noiseless measurements and showed that in that case the optimized probability of non-zero entries is always smaller than \(0.5\). Here, we theoretically characterize the performance of UNN-based SCI recovery methods and show a consistent result in the case of noise-free measurements. Interestingly, as shown in our experiments, for noisy measurements, the optimized probability can be larger than 0.5. We derive novel theoretical results explaining this phenomenon.

## 3 DIP for SCI inverse problem

### SCI inverse problem

The objective of a SCI system is to reconstruct a three-dimensional (3D) data cube from its two-dimensional (2D) compressed measurement. Specifically, let \(^{n_{1} n_{2} B}\) represent the target 3D data cube. In an SCI system, \(\) is mapped to a singular measurement frame \(^{n_{1} n_{2}}\). This mapping, particularly as implemented in hyperspectral SCI and video SCI , can be modeled as a linear system as follows [2; 37]: \(=_{i=1}^{B}_{i}_{i}+\). Here, \(^{n_{1} n_{2} B}\) represents the sensing kernel (or mask), and \(^{n_{1} n_{2}}\) denotes the additive noise. The terms \(_{i}=(:,:,i)\) and \(_{i}=(:,:,i)^{n_{1} n_{2}}\) correspond to the \(b\)-th sensing kernel (mask) and the associated signal frame, respectively.

To simplify the mathematical representation of the system, we vectorize each frame as \(_{i}=(_{i})^{n}\) with \(n=n_{1}n_{2}\). Then, we vectorize the data cube \(\) by concatenating the \(B\) vectorized frames into a column vector \(^{nB}\) as \(=[_{1}^{},,_{B}^{}]^{}\). Similarly, we define \(=()^{n}\) and \(=()^{n}\). Using these definitions, the measurement process can also be expressed as

\[=+.\] (1)

The sensing matrix \(^{n nB}\), is a highly sparse matrix that is formed by the concatenation of \(B\) diagonal matrices as

\[=[_{1},...,_{B}],\] (2)

where, for \(i=1, B\), \(_{i}=((_{i}))^{n n}\). Using this notation, the measurement vector can be written as \(=_{i=1}^{B}_{i}_{i}\) The goal of a SCI recovery algorithm is to recover the data cube \(\) from undersampled measurements \(\), while having access to the sensing matrix (or mask) \(\).

### Theoretical analysis of DIP-based SCI recovery

The Deep Image Prior (DIP)  hypothesis provides a framework for understanding the potential of UNNs in capturing the essence of complex source structures without requiring training data. Define \(^{n}\) as the class of signals of interest (e.g., class of video signals consisting of \(B\) frames.). Also, let \(g_{}:\ ^{p}^{n}\) represent a UNN parameterized by \(^{k}\). Informally, DIP hypothesis states that any signal in \(\) can be presented as the output of the DIP parameterized by parameters \(^{k}\). This can be represented more formally as follows.

**DIP hypothesis:** Assume that \(^{p}\) is sampled i.i.d. from a uniform distribution \((0,1)\). For any \(\), the DIP hypothesis states that for any \(^{k}\), there exists \(^{k}\), such that \(\|g_{}()-\|_{2}\), almost surely.

This hypothesis underscores the capability of UNNs to function as powerful priors, capturing intricate data structures inherent in natural images and other complex datasets, thereby bridging the gap between classical analytic methods and modern machine learning techniques.

Given SCI measurements \(=+\), as described in (1) with \(\) defined in (2), a DIP represented by \(g_{}:\ ^{p}^{n}\) can be used to recover \(\) from measurements \(\) as follows: Step 1) Randomly sample \(\) (independent of \(\) and \(\)), as required by the DIP. Step 2) Solve the DIP-SCI optimization:

\[}\ =\ \|-\|_{2},=g_{}(),\ ^{k}.\] (3)

Before describing our proposed approach to solving DIP-SCI optimization in Section 4, we theoretically characterize the performance of (3), under noise-free and noisy measurements and use our theoretical results to i) bound the number of frames that can be recovered from a single 2D measurement, and ii) optimize the parameters of the mask \(\) that is used for modulating the data.

#### 3.2.1 Noise-free measurements

The following theorem characterizes the performance of (3) in case where the measurements are noise-free and connects its performance (\(\|-}\|_{2}\)) to the ambient dimension \(n\), number of frames \(B\), number of parameters of the DIP \(k\), the distortion \(\) and the Lipschitz coefficient \(L\).

**Theorem 3.1**.: _Let \(\), where \(\) denotes a compact subset of \(^{n}\), such that \(\|\|_{}\), for all \(\). Assume that \(g_{}():^{N}^{nB}\) is \(L\)-Lipschitz as a function of \(\). Let \(=\), where \(=[_{1},,_{B}]\), where \(_{i}=(D_{i,1},,D_{i,n})\), \(i=1,,B\), are independently generated with \(D_{i,1},,D_{i,n}\) i.i.d. \((p)\). Given randomly generated \(\), let \(}\) denote the solution of (3). Then, if \(_{:\ =g_{}(),}\| -\|_{2}\), we have_

\[}\|-}\|_{2} }+}  n}{n}^{}+}(}+1),\] (4)

_with a probability larger than \(1-2^{-0.5k n+1}\)._The bound in (4) consists of multiple terms. The first term, i.e., \(}\), accounts for the effect of the DIP representation error. For instance if \(\) is directly selected from the output space of DIP, then \(=0\). The goal of the following two corollaries to shed light on the interplay of the three terms in (4) and highlight their implications on the performance of DIP-SCI optimization. First, Corollary 3.2 characterizes an upper bound on the number of frames \(B\) that are to be recovered from a single 2D measurement.

**Corollary 3.2**.: _Consider the same setup as in Theorem 3.1. If_

\[B},\] (5)

_then \(}\|-}\|_{2}}+}{}\), where \(c_{n}=O(1/( n)^{})\) does not depend on \(p\)._

Next, Corollary 3.3 states that in the case where the measurements are not corrupted by noise, the value of \(p\), the probability of a mask entry being non-zero, that minimizes the upper bound in (4) is always less than \(0.5\). This is consistent with the results established i) empirically in the literature  and ii) theoretically in  using a compression-based framework.

**Corollary 3.3**.: _Consider the same setup as in Theorem 3.1. The upper bound in (4) is minimized at \(p^{*}(0,0.5)\)._

#### 3.2.2 Noisy measurements

In many practical SCI applications, the measurements are corrupted by additive noise. This raises the following natural question: How does the inclusion of noise in the model affects the optimized mask parameters? To address this question, we develop two theoretical results: Theorem 3.4 characterizing the reconstruction error \(\|-}\|_{2}\) and Theorem 3.5 bounding the error in estimating the mean of the input frames \(}=_{i=1}^{B}_{i}\). As we explain later, the combination of these two results provide a theoretical understanding on the performance of SCI recovery methods in the presence of noise and the corresponding optimized masks.

**Theorem 3.4**.: _Consider the same setup as in Theorem 3.1. For \(\), let \(=_{i=1}^{B}_{i}_{i}+\), where \(^{n}\) denotes the additive noise and \((,_{z}^{2}I_{n})\), for some \(_{z} 0\). Let \(}\) denote the solution of DIP-SCI optimization (3). If \(B\) satisfies the bound in (5), then_

\[}\|-}\|_ {2}&}+}{p(1-p)} }\\ &+()^{}}{ p(1-p)}}(1+_{n})+}(1+ _{n})+_{n},\] (6)

_with a probability larger than \(1-(2^{-0.5 n+3}+^{-0.3n})\). Here, \(_{n}=O(})\), \(_{n}=o(}})\) and \(_{n}=o()\) do not depend on \(_{z}\) and \(p\)._

**Theorem 3.5**.: _Consider the same setup as in Theorem 3.4. Assuming that \(B\) satisfies the bound in (5), then with probability larger than \(1-(2^{-0.5k n+3}+^{-0.3n})\),_

\[}\|_{i=1}^{B}(_{i}-} _{i})\|_{2}}+}{B}}h(p)^{}+}_{n}+},\]

_where \(_{n}=O(( n)^{-})\) and does not depend on \(p\)._

To shed light on the implications of these two theorems, the following corollary characterizes the value of \(p\) optimizing each bound.

**Corollary 3.6**.: _Consider the same setting as Theorem 3.4. The upper bound in Theorem 3.4 is always optimized at \(p^{*}<0.5\). On the other hand, the upper bound in Theorem 3.5 is a decreasing function of \(p\) and is minimized at \(p^{*}=1\)._

Let \(}_{B}=[}^{},,}^{}]^{ }^{nB}\), i.e., the reconstruction signal derived by repeating the average frame \(}=_{i=1}^{B}_{i}\). Then, using the triangle inequality, we have

\[\|-}\|_{2}\|-}_{B}\|_{2} +\|}_{B}-}\|_{2}.\]Figure 1 shows \(\|-}\|_{2}\), \(\|-}_{B}\|_{2}\), and \(\|}_{B}-}\|_{2}\), for different video test samples. Here are our key observations: 1) \(\|}_{B}-}\|_{2}\) is an increasing function of \(p\), which is consistent with Corollary 3.6. 2) The optimal value of \(p^{*}\) that minimizes \(\|-}\|_{2}\), is an increasing function of \(_{z}\), for all test videos. 2) In cases where the difference between \(}_{B}\) and \(\) is relatively large, e.g. Traffic, the optimized \(p^{*}\) stays smaller than \(0.5\), even for large values of \(_{z}\), as predicted by Theorem 3.4. 3) On the other hand, in cases where \(}_{B}\) provides a high-fidelity representation of \(\) and \(\|}_{B}-\|_{2}\) is relatively small (e.g., Drop), for large values of noise power, the optimal value of \(p^{*}\) can move beyond \(0.5\), as predicted by Theorem 3.5. In other words, in such cases, the algorithm moves toward estimating the mean of the frames, which is a good representation of the actual data frame.

## 4 SCI-BDVP: Bagged-DVP for video SCI

Recall the DIP-SCI optimization described in (3), i.e., \(}=*{arg\,min}_{}\|-\|_{2}\), where \(=g_{}()\), \(^{k}\) and \(\) generated independently and randomly according to a pre-specified distribution. To solve this optimization, one straightforward approach is to solve \(_{}f()\), with \(f()=\|-_{g}()\|_{2}^{2}\), by directly applying gradient descent to the differentiable function \(f()\). However, given the highly non-linearity and non-convexity of \(f()\), this approach is prone to readily getting trapped into local a minima and achieving considerably sub-optimal performance. Generally, a better approach to is to write the DIP-SCI optimization as \(}=*{arg\,min}_{()}\|-\|_{2}^{2}\), where \(()\{=g_{}(): ^{k}\}\). This alternative presentation of the problems leads to minimizing a convex cost function over a non-convex set. A classic approach to solve this optimization is projected gradient descent (PGD), which while in general is not guaranteed to converge to the global minima is more apt to recover a solution in the vicinity of the desired signal.

**Remark 4.1**.: _Theoretical feasibility of SCI recovery was first established in  using a compression-based framework for modeling source structure. There, the authors considered \(}=*{arg\,min}_{}\|-\|_{2}^{2}\), where \(\) denotes a discrete set of the codewords of a compression code. They theoretically proved that in that case, despite the non-convexity of the problem, PGD is able to converge to the vicinity of the desired signal._

The PGD applied to \(}=*{arg\,min}_{()}\|-\|_{2}^{2}\) proceeds as follows: Start form an initialization point \(_{0}\). For \(t=1,2,,T\), perform the following two steps i) Gradient descent: \(_{t+1}^{G}=_{t}+^{}(-_{t})\), and ii) Projection: \(_{t+1}=*{arg\,min}_{()}\|-_{t+1}\|_{2}\), or

\[_{t+1}\ =\ *{arg\,min}_{}\|g_{}()- _{t+1}^{G}\|_{2},_{t+1}=g_{_{t+1}}( )\] (7)

To solve the non-convex optimization required at the projection step, one can again employ gradient descent. However, in addition to the non-convexity of the cost function, another common known issue with projection into the domain of a DIP is overfitting [25; 6; 39; 40]. Moreover, in PGD, ideally one needs to set the resolution of the projection step adaptively, such that during the initial steps the DIP has a coarser resolution and as it proceeds it becomes finer and finer. This poses the following question: Which DIP structure should one use to optimize the final performance?

To address this question, the authors in  have proposed, bagged-DIP, which consists of employing multiple DIP with different structures in parallel, for the DIP projection step and averaging the outputs. They show that this approach provides a robust projection module which consistently outperforms the performance achievable by each individual DIP network, and also provides, at least partially, the flexibility and adaptability required by PGD.

Bagged-DIP, essentially employs bagging idea to mitigate overfitting. As the DIP projection iterations proceeds (within each step of PGD), overfitting tends to occur after a certain threshold. However, due to the variance reduction facilitated by bagging, the bagged estimate can demonstrate less overfitting. In other words, the bagged estimate is less sensitive to the stopping time of the DIP training. In essence, each DIP is not required to produce the best estimate at every iteration of PGD.

Inspired by the bagged-DIP solution, here we propose the bagged-DVP for SCI (SCI-BDVP), as shown in Figure 3. SCI-BDVP, in addition to the standard gradient descent (GD) step, defined as \(_{t+1}^{G}=_{t}+^{T}(- _{t})\), consists of two main additional components: i) The bagged-DVP module that simultaneously projects the output of the GD step onto the domain of multiple DVP networks operating at varying patch sizes (refer to Figure 2 and then averages their outputs, and ii) a skip connection that computes a weighted average of the output of the GD step and the bagged-DVP step. Next, we briefly explain the detailed construction of each component.

Sci-Bdvp.Figure 2 schematically shows the structure of a bagged-DVP consisting of \(K\) individual DVPs, each operating at a different scale and trained separately. More specifically, for each \(k\), \(k=1,,K\), the \(3D\) video is partitioned into non-overlapping video cubes of dimensions \((h_{k},w_{k})\). For each video cube of dimension \((h_{k},w_{k},B)\), we train a separate DVP. In other words, at scale \(k\), we need to train \(N_{k}=H/h_{k} W/w_{k}\) separate DVPs. (The total aggregate number of DVPs that are trained is going to be \(_{k=1}^{K}N_{k}\).) At each scale, the separately projected video cubes are concatenated to form \(_{t+1,k}^{P}\), a video frame of the same dimensions as the desired video. At scale \(k\), let \(g_{}^{k}()\) denote a DVP that generates an output video frame of dimensions \(h_{k} w_{k} B\). To cover the whole video frame at scale \(k\), we need to train \(N_{k}\) separate DVPs \(g_{}^{k,i}()\), each having an independently drawn input, \(_{k,i}\). \(i\) denotes the index of partitioned video cube. To train each of these \(N_{k}\) DVPs, we first extract the corresponding parts from \(_{t+1}^{G}\), \(\) and \(\) and denote them as \(_{t+1,i}^{G}\), \(_{i}\) and \(_{i}\), respectively.2 Then, to train the corresponding DVP to form reconstruction \(_{t+1,k,i}^{P}\), we minimize

Figure 2: The structure of SCI-BDVP. There are \(K\) estimates generated, each using a different patch size. The blue dot denotes averaging the \(K\) estimates, the orange dot denotes averaging \(_{t}^{P}\) and \(_{t}^{G}\) with weight \(\), the red dot denotes the loss function used for training the DIP parameters, requiring \(_{t}^{G}\), \(\) and \(\). The red lines denote using 3D gradient descent result \(_{t}^{G}\), which is used for training the parameters of the \(k\)-th DIP (red dot), and averaging with projection output \(_{t}^{P}\) (orange dot). The green lines denote using 2D measurement \(\) and 3D binary mask H for training parameters of DIPs in different estimate \(k\).

\(\|_{t+1,i}^{G}-g_{}^{k,i}(_{k,i})\|_{2}^{2}+\| _{i}-_{i}g_{}^{k,i}(_{k,i})\|_{2}^{2}\), where \(>0\) denotes the regularization parameter. Unlike classic DIP cost function, here we use the measurements \(\) as an additional regularizer. After recovering \(_{t+1,k,i}^{P}\), \(i=1,,N_{k}\), we concatenate them based on their locations to form \(_{t+1,k}^{P}\). We repeat the same process, for each \(k=1,,K\) to find \(_{t+1,1}^{P},,_{t+1,K}^{P}\). Finally, we use the idea of bagging and define \(_{t+1}^{P}=_{k=1}^{K}_{t+1,k}^{P}\).

**Skip connection.** After obtaining \(_{t+1}^{P}\) and \(_{t+1}^{G}\), we define \(_{t+1}\) as their weighted average: \(_{t+1}=_{t+1}^{G}+(1-)_{t+1}^{P}\), where \((0,1)\). (See Figure 3.) In the experiments in Appendix C.3, we show how the addition of this skip connection consistently improves the achievable performance.

## 5 Experiments

We evaluate the performance of SCI-BDVP and compare it with existing SCI methods, for \(_{z}=0\) and \(_{z}>0\). Our experimental results are consistent with our theoretical results on mask optimization. To evaluate the performance we use peak-signal-to-noise-ratio (PSNR) and structured similarity index metrics (SSIM) . All the tests are performed on a single NVIDIA RTX 4090 GPU.

**Datasets and baselines.** We compare our method against the baselines on 6 gray-scaled benchmark videos including Kobe, Runner, Drop, Traffic, Aerial, Vehicle, where the spatial resolution is \(256 256\), and \(B=8\). We choose \(5\) representative baseline methods i) GAP-TV  : the Plug-and-play (PnP) method that employs a total-variation denoiser; ii) PnP-FFDnet  and PnP-FastDVDnet  : PnP methods that employ pre-trained deep denoisers, iii) PnP-DIP : DIP-based iterative method; iv) Factorized-DVP : Untrained End-to-End (E2E) network. Baseline setups follows that exactly stated in the respective papers. The details of proposed SCI-BDVP can be found in Appendix B.

**Masks for noiseless and noisy measurements.** For the case of SCI without noise, we obtain the measurements from equation (1), where we randomly sample mask values from \((p)\) with \(p=0.2,0.3,,0.8\). For the noisy setup, zero-mean Gaussian noise with variance (\(^{2}\)), \(=10\), \(=25\) and \(=50\), is added to the measurements. For the results reported in Tables 1 and 2, the masks are randomly and independently generated as as \((0.5)\).

### Reconstruction results for video SCI

**Noiseless measurement.** In Table 1 we compare the performance of SCI-BDVP against baselines. To highlight the effectiveness of the bagged DVP idea, we also implemented two versions of our proposed method: i) SCI-BDVP (E2E), an end-to-end BDVP-based solution and 2) SCI-BDVP (GAP): an iterative algorithm that employs generalized alternative projection (GAP) update rule and BDVP projection (Refer to Appendix B.1 for a description of GAP and GD and our rationale for the choice of each method.). It can be observed that both SCI-BDVP (E2E) and SCI-BDVP (GAP) outperform existing untrained methods. Specifically, SCI-BDVP (GAP) achieves state-of-the-art performance and on average improves about \(1\) dB in PSNR compared to other methods.

**Noisy measurement.** Table 2 compares the performance of SCI-BDVP (E2E) and SCI-BDVP (GD) with baseline methods. As explained in Appendix B.1, unlike noise-free measurements, in the case of noisy measurements, especially when noise variance grows, GAP update rule is no longer a reasonable

Figure 3: SCI-BDVP (GD): Iterative PGD-type algorithm. Each step consists of GD and BDVP projection, with an additional skip-connection.

[MISSING_PAGE_FAIL:9]

consistent with empirical observations reported in [35; 34]. For the noisy measurements, we see that \(p^{*}\) is an increasing function of \(_{z}\), consistent with our theoretical results discussed earlier in Section 3. (Refer to Appendix C.1 for further results.)

## 6 Conclusion

We have studied application of UNNs SCI recovery. We propose an iterative solution with bagged DVP (multiple, separately trained DVPs with averaged outputs), achieving state-of-the-art performance among unsupervised solutions for noise-free measurements and robustly outperforming both supervised and UNN methods for noisy measurements. Additionally, we provide a theoretical framework analyzing the performance of UNN-based methods, characterizing achievable performance and guiding hardware parameter optimization. Simulations validate our theoretical findings.

An important application of SCI is hyperspectral snapshot imaging (HSI). Our results in this paper provide a theoretical foundation to understand HSI systems and optimize their hardware. Additionally, the developed theoretical framework can be used to explore aspects specific to HSI, such as masks being shifted versions of each other. We also expect our algorithm to effectively address overfitting in HSI tasks, enhancing reconstruction performance. We plan to explore these aspects further in our future research.

Several other aspects remain for future work. Theoretically, we only considered i.i.d. Bernoulli masks, while practical SCI systems typically are more constrained. Additionally, deriving information-theoretic lower bounds on SCI recovery is an open problem. Experimentally, we focused on classic baseline videos; exploring a richer set of samples and studying noise models beyond additive Gaussian noise are interesting directions for future research. We also plan to enhance the algorithm's efficiency by parallelizing the projections required by the bagged solution.

Figure 4: Reconstruction PSNR (\(\|-}\|_{2}\)) and SSIM as a function of \(p\), using SCI-BDVP (GAP) (two leftmost figures) and PnP-FastDVDnet (GAP) (two rightmost figures). For each value of \(p\), the masks are independently generated i.i.d.\((p)\).

Figure 5: Reconstruction PSNR (\(\|-}\|_{2}\)) of SCI-BDVP (GD), y-axis, as a function of \(p\), x-axis. For each value of \(p\), the masks are independently generated i.i.d.\((p)\).

  
**Patch size** & \(\#\) of patches & Time (min.) \\ 
64 & 16 & 1.5 \\
128 & 4 & 0.28 \\
256 & 1 & 0.12 \\   

Table 3: Time complexity of our proposed SCI-BDVP was evaluated on various patch sizes (64, 128, 256) of video blocks, using a standard 1000 DVP iterations for training.