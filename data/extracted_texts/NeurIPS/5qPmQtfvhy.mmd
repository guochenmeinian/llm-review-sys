# Algorithmic progress in language models

Anson Ho\({}^{1}\)

Joint first authors. \({}^{1}\)Epoch. \({}^{2}\)MIT FutureTech, CSAIL, \({}^{3}\)Northeastern University. Email correspondence to tamay@epochai.org. You can find our code and data here: https://github.com/epoch-research/lm-algorithmic-progress.

Tamay Besiroglu\({}^{1,2}\)1

Ege Erdil\({}^{1}\)

David Owen\({}^{1}\)

Robi Rahman\({}^{1}\)

Zifan Carl Guo\({}^{2}\)

David Atkinson\({}^{1,3}\)

Neil Thompson\({}^{2}\)

Jaime Sevilla\({}^{1}\)

Joint first authors. \({}^{1}\)Epoch. \({}^{2}\)MIT FutureTech, CSAIL, \({}^{3}\)Northeastern University. Email correspondence to tamay@epochai.org. You can find our code and data here: https://github.com/epoch-research/lm-algorithmic-progress.

###### Abstract

We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 90% confidence interval of around 2 to 22 months, substantially faster than hardware gains per Moore's Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.

## 1 Introduction

The field of language modeling has seen rapid advances, with recent large language models (LLMs) demonstrating strong performance in domains such as programming , mathematics , and standardized tests . But how has this progress been possible?

One salient factor is the scaling of training compute based on neural scaling laws , with state-of-the-art systems being trained for months on tens of thousands of GPUs. But this is only part of the story: another key factor has been algorithmic improvements, which result in more efficient use of resources such as compute and training data. These include changes in model architectures, optimization algorithms, and software frameworks.

This picture raises some important questions: How much of recent progress in language models has come from algorithmic improvements during pre-training, and how much has been from scaling up models and datasets? The answers to these questions have crucial implications for the future of AI progress, and have an important role in informing AI policy.

In this paper, we aim to answer these questions by following the approach first presented by Erdil and Besiroglu (2022) in computer vision.2 To this end, we produce a dataset of over 200 language models evaluated on popular language modeling datasets, and use this to fit a statistical model that helps estimate the rate of algorithmic progress. Using our model, we further quantify the relative importance of algorithms and scaling training compute, shedding light on one of the most important drivers of AI progress.

### Previous work

Thus far, there have been few works investigating algorithmic progress in machine learning. Notably, Hernandez and Brown (2020) re-implement popular open-source ImageNet models and find a 44\(\) decrease in the compute required to achieve the same performance as AlexNet. Karpathy (2022) reproduced the convolutional neural network of LeCun et al. (1989) using modern algorithmic innovations, achieving a 60% reduction in error rate. Dorner (2021) measures progress in the sample efficiency of deep reinforcement learning algorithms over time with doubling times ranging from 5 to 18 months. More recently, Erdil and Besiroglu (2022) estimate algorithmic progress based on fitting a statistical model inspired by neural scaling laws. They find that algorithms and hardware contribute roughly equally to performance, and the training compute needed to reach a certain level of performance halves every 9 months.

## 2 Methodology

### Model definitions

We want to estimate the rate at which newer language models are able to achieve a certain level of performance more efficiently than older models. We do this by fitting a model that meets two key desiderata: (1) the model must be broadly consistent with previous work on neural scaling laws (e.g. Hoffmann et al. (2022)), and (2) the model should allow for a decomposition of the main contributors to increased performance, such as improvements in how efficiently data or free parameters in the model are used. In this sense, our core approach is similar to that in Erdil and Besiroglu (2022).

The starting point is the scaling law from Hoffmann et al. (2022), which relates the training loss \(L\) of a dense transformer to its number of parameters \(N\) and the training dataset size \(D\):

\[L=E+}+},\] (1)

where \(L\) is per-token cross entropy loss on the dataset, and \(E\), \(A\), \(B\), \(\) and \(\) are constants. \(E\) represents the 'irreducible loss' of the dataset, while the second and third terms, \(}\) and \(}\), capture the errors that are due to the finiteness of the model or dataset, respectively.

Following Erdil and Besiroglu (2022) and Hernandez and Brown (2020), we quantify algorithmic progress in terms of reductions of the resources (\(N\) and \(D\)) required to achieve the same level of performance over time. To measure this, we introduce the concepts of "effective data" \(D_{}\) and "effective model size" \(N_{}\) into the model:3

\[N_{} N(^{}(Y-Y_{0})),~{}and~{}~{}}D_ {} D(^{}(Y-Y_{0})),\] (2)

where \(Y\) is the current year, \(Y_{0}\) is some reference year4, and \(^{}\) and \(^{}\) characterize the rate of algorithmic progress for model size and dataset size, respectively. In other words, we assume that continued algorithmic progress results in an exponential increase in \(D_{}\) and \(N_{}\) over some time interval \(Y-Y_{0}\), even with fixed \(D\) and \(N\). Plugging these into the original scaling law gives:

\[L=E+}^{_{}}}+}^{ _{}}}=E+}}}e^{-_{ {perm}}(Y-Y_{0})}+}}}e^{-_{}(Y- Y_{0})},\] (3)where \(A\), \(B\), \(_{}\), \(_{}\), \(_{}\) and \(_{}\) are constants. In relation to equation 2, we have that \(^{}=_{}/_{}\) and \(^{}=_{}/_{}\). Algorithmic progress is thus captured as a constant exponential trend that multiplies with each of the two terms in the scaling law. In doing so, we are able to estimate the rate at which fewer'resources' (\(N\) and \(D\)) are required to achieve the same level of performance over time. Furthermore, given that that the physical compute is approximately given by \(C 6ND\)(Hoffmann et al., 2022; Kaplan et al., 2020), we can similarly define an "effective compute" which is determined from the effective parameters and effective data.

### Estimation approach

#### 2.2.1 Model selection

We estimate variants of the augmented scaling law presented in equation (3) on our dataset of language model evaluations. We perform extensive cross-validation exercises to identify the variant of the model that fits the data best. The goal of this exercise is to consider different models that capture different effects (e.g. different scaling behavior across different model architectures, different forms of algorithmic progress, etc.).

Concretely, we consider dataset-specific coefficients (\(A,B\)), rates of algorithmic progress (e.g. \(_{},_{}\)), different scaling coefficients for different architectures, regularization (\(_{},_{}\)), and more. The model variants we consider generally do not contain an irreducible loss term (i.e. \(E=0\)) since this is poorly estimated on our data, and because it does not change our estimated doubling times in practice--we check the robustness of this change in appendix H. In total, we evaluate around 90 different model specifications through leave-one-out-cross validation and pick the models that perform best on relevant out-of-sample metrics, see Appendix J for more details. In the end, the model we select is model 7, where the coefficients \(A\) and \(B\) are benchmark specific, but estimates of algorithmic progress and scaling exponents (e.g. \(\) and \(\)) are not. This model achieves an \(R^{2}\) of around 0.91 between predictions and held-out test data.

A further important consideration is the possibility of alternative forms of algorithmic progress. In particular, in section 2.1 we model algorithmic progress as causing exponential increases in an "effective" budget, e.g. of parameters. But one could also observe progress through changes in scaling exponents (i.e. \(_{}\) and \(_{}\)). There are _a priori_ reasons to suspect that this might be the case--for instance, one notable innovation is due to a change in scaling laws such as those introduced in Kaplan et al. (2020) and Hoffmann et al. (2022). Different model architectures, such as recurrent neural networks and transformers, are also known to have different scaling behaviors (see for instance Tay et al. (2022) and Droppo and Elibol (2021)).

We attempt to account for this possibility in the cross validation analysis. In particular, we introduce three models (models 13 to 15) which account for different kinds of scaling exponents, including the possibility of changing exponents over time. Our chosen main model (model 7) outperforms these models in cross validation, but these alternatives also perform similarly well, typically with an \(R^{2}\) of between 0.88 and 0.91. This analysis is described in more detail in appendix J.

We also consider other factors that could potentially impact measured perplexity, and thereby measured rates of algorithmic progress. For example, different tokenization schemes during preprocessing have been found to improve WT103 perplexity in some instances (Radford et al., 2019), and training models for multiple epochs has been a common way of improving performance (Muennighoff et al., 2023). We find that our core results are broadly the same while varying these degrees of freedom--we provide more details on these experiments in the appendices.5

#### 2.2.2 Data

Our dataset contains over 400 language models evaluated on WikiText-103 (WT103), WikiText-2 (WT2), and Penn Treebank (PTB), about 60% of which we are able to use in our analysis. In particular, relevant information was retrieved from around 200 different papers, as well as evaluations of 25 models that we performed ourselves using the framework from Gao et al. (2021). We then consider the subset of the data that contains the information necessary to fit our proposed model structure in equation 3: token-level test perplexity (which determines the cross-entropy loss), publication date,number of model parameters, and training dataset size. This leaves us with around 231 models for analysis.

In some instances, multiple models are retrieved from the same paper, even if they constitute similar algorithmic innovations. This could pose problems around autocorrelation, which could result in underestimating the uncertainty in our individual parameter estimates. In the following main analysis, we therefore only include up to three models per paper, which results in approximately 50 more models being excluded. To verify the robustness of this approach, we also consider an alternative technique that directly accounts for autocorrelation in the analysis, which yields doubling time and confidence interval estimates that are consistent with our main results (see Appendix I).

## 3 Empirical results

### Models require 2\(\) less compute roughly every eight months

How quickly are the algorithms underpinning language models improving? Our core approach is to back out doubling times based on fitting the augmented scaling law introduced in equation (8), and using the definitions of effective data, effective parameters, and effective compute we introduced in section 2.1. Here the effective data is given by \(D_{}=D[}}{_{}}(Y-Y _{0})]\), so the doubling time for \(D_{}\) is determined by the time \(Y-Y_{0}\) where \(D_{}=2D\). Thus we have:

\[T_{D}=Y-Y_{0}=}}{_{}} 2.\] (4)

The doubling times for parameter efficiency can be determined similarly, giving

\[T_{N}=}}{_{}} 2,\] (5)

which we can use to work out the doubling times for effective compute. In particular, since the total compute in FLOP, \(C\), required during training is approximately \(6ND\), the growth rates are related via \(g_{C}=g_{N}+g_{D}\). Here \(g_{C}\) is the growth rate in effective compute, \(g_{N}\) is the growth rate in effective parameters, and \(g_{D}\) is the growth rate in effective data. Since doubling times are inversely related to growth rates, we therefore have that

\[T_{C}=(}+})^{-1},\] (6)

where \(T_{C}\), \(T_{N}\), and \(T_{D}\) are the doubling times (due only to algorithmic progress in pre-training) for effective compute, effective parameters, and effective data respectively. Based on this approach, using our preferred model, we find that the median doubling time for effective compute is 6.1 months, with a 90% confidence interval of 3.3 to 11.3 months.

We further check the robustness of this result by looking at the predictions from different models. In particular, because we perform model selection using leave-one-out cross-validation, we can compare the predictions of our preferred model with the predictions from other models we considered.6 Concatenating the doubling time estimates from all the models in Figure 0(b), we find a median doubling time of 7.5 months [95% CI: 1.7 to 22.5 months], which is consistent with our preferred model.7 This estimate falls within the range of confidence intervals of the estimated rates of algorithmic progress in computer vision (Erdil and Besiroglu, 2022), and sample efficiency improvements in reinforcement learning (Dorner, 2021).

While the structure of our model is not amenable to analyzing fine-grained speedups or slowdowns in the rate of algorithmic improvements, we can nevertheless test the possibility of a one-time increase or decrease in growth rates over the full time period. To this end, we consider a variant of our preferred model (model 7) where a dummy variable is introduced--this is equal to 0 for any model that is published before the start of a certain year, and 1 otherwise. This allows us to consider doubling times before and after a certain year cutoff (e.g. 2017), and we perform this analysis for several such cutoffs.

The result is shown in Figure 2. Here we see that the difference in estimated doubling time before and after the start of 2017 is very pronounced, however this is not the case for other choices of the cutoff year. In each year the median doubling time is faster after the start of the cutoff year, but usually only marginally so. Overall, this does not provide strong evidence of a drastic speedup in algorithmic progress. This does not rule out the possibility of weaker effect sizes, since our approach is statistically under-powered.

Figure 1: Estimates of algorithmic progress of models selected by cross validation. Figure 2(a) shows aggregated estimates over doubling times, and Figure 2(b) illustrates via swarm plots sorted from left to right in order of decreasing cross validation performance (increasing MSE test loss). Note that model 14 is omitted from Figure 2(b) —we elaborate on our reasoning in appendix J.2.

Figure 2: **Left**: Comparison of estimated doubling times for effective compute from algorithmic progress, before and after set cutoff years from 2016-2020. Shorter doubling times in the ”post” period relative to ”pre” indicate an acceleration in the rate of algorithmic progress after that cutoff year. Longer doubling times indicate a deceleration. **Right**: A stylized illustration of the relative contribution of compute scaling and algorithmic progress to effective compute. The physical compute contribution is estimated from the doubling times in Sevilla et al. (2022), and the algorithmic progress contribution is based on the aggregated doubling time estimate across model specifications (see section 3.1). We further plot the physical training compute values for several notable models (e.g. GPT-2) in their publication years.

### Most recent performance gains in next-token prediction have been from compute-scaling

Naively extrapolating our estimated doubling times suggests that, between 2014 and 2023, pre-training algorithmic progress has enabled performance to improve as much as it would have with around 22,000\(\) more compute.8 At the same time, Sevilla et al. (2022) find that physical compute budgets have doubled roughly every 6 months since the start of deep learning, including in language models. This suggests that physical compute has instead grown by a factor of around one-million-fold. This paints a stylized picture where "effective compute" expanded by about 22-billion-fold since 2014, with slightly under two-thirds of the scaling being due to increased use of actual, physical computing resources.

There are reasons to be cautious about this naive extrapolation. For one, we do not directly observe gains of \(22,000\) (or even \(10,000\)) anywhere in our dataset. However, given that it is unlikely that early researchers trained language models on very large quantities of compute, it is therefore improbable that we observe such large declines over the analyzed time period. Nevertheless, the lack of such observations still raises questions about the reliability of extrapolating these trends between long multi-year periods.

One specific reason for caution is that the extrapolation neglects the scale-dependence of algorithmic innovations. It is likely that some algorithmic innovations will become obsolete over time as models are trained at larger scales of compute--e.g. the effectiveness of specific tokenizers or hyperparameter settings may diminish, making them less useful for future, larger models. Conversely, recent innovations might fail to produce large or any benefits when implemented at much smaller scales than models today. For example, the gains from scaling laws are related to the scale of compute used (see Appendix B), and older architectures, such as the LSTM and convolutional network, can exhibit higher efficiency at small scales relative to the transformer (Droppo and Elibol, 2021; Karpathy, 2022).

While a naive extrapolation of doubling times predicts substantial reductions in compute requirements, our work does not provide compelling evidence that we can currently or in the future train extremely small models to achieve the performance of much larger ones by applying the full suite of modern innovations. The scale-dependence of algorithmic improvements and the lack of direct observations of such large efficiency gains in our dataset suggest that further research and more comprehensive data are needed to validate these extrapolations.

Besides doubling times, we can also decompose the relative contributions from algorithms and compute scaling by evaluating our estimated models directly. We approach this using a Shapley value analysis, and the results weakly support the stylized picture above that compute scaling has been more important for explaining performance improvements than algorithmic progress since 2014.

The findings indicate that the relative contribution of algorithmic progress to performance improvements has diminished over time, at least within the dataset of models that have historically been close to the state-of-the-art. This observation aligns with the stylized representation in Figure 2 and the findings of Erdil and Besiroglu (2022) for computer vision, where compute scaling has shown increasing importance over time.

One explanation for the diminishing relative contribution of algorithmic progress is that investments in expanding physical compute have increased substantially, outpacing the rate of algorithmic improvements. This framing aligns with the increased emphasis on scaling large language models over the last few years, particularly since the introduction of GPT-2 in 2019 (Radford et al., 2019), relative to fundamental algorithmic or architectural changes.9Figure 2 illustrates a stylized version of this perspective, depicting a sharp increase in physical compute scaling around 2018-2019, followed by a return to previous compute scaling growth rates.

There are other potential explanations - for example, it is possible that the transformer architecture was a pivotal innovation (see section 3.3), and subsequent algorithmic advances have been lesssignificant in comparison. Alternatively, this observation could also be explained by a secular decline in the rate of algorithmic innovation. However, we find these two explanations less compelling than the results of Figure 2, where the rate of algorithmic progress does not clearly decrease after the release of the transformer (e.g. with a 2018 cutoff). If anything, the rate _increases_ slightly, contrary to what both of these explanations predict.

### The significance of the transformer architecture

Since its introduction in 2017 (Vaswani et al., 2017), the transformer architecture has become the dominant algorithmic architecture in language modeling, forming the base of multiple notable systems. We attempt to quantify the its contribution in terms of the "compute-equivalent gain" over other architectures in our dataset (LSTMs, RNNs, state space models, among others). This is akin to the approach outlined in Davidson et al. (2023)--in this context, the compute-equivalent gain is the amount by which training compute must be scaled to improve benchmark performance as the same amount as the introduction of the transformer. For example, Hernandez and Brown (2020) find that a transformer (2017) achieves the same performance as a Seq2Seq (2014) model on the WMT-14-EN-FR benchmark, with 61\(\) less compute.

To capture the improvement represented by the transformer, we modify our core model as follows:

\[L=(_{T})(}^{} }+}^{}}),&,\\ }^{}}+}^{ }},&.\] (7)

where \(:(0,1)\) is the sigmoid function, given by \((x)=1/(1+e^{-x})\). \(_{T}\) is a constant and all other terms have the same meaning as in the original model.10 The key intuition is that the transformer could enable us to use compute (or perhaps data) more efficiently than the architectures that precede it.

After preprocessing, our dataset contains 103 transformer models, and 127 non-transformer models, largely consisting of recurrent networks such as the LSTM. Fitting the model on this data reveals that the transformer architecture typically lowers reducible loss proportionally by 5.4% [90% CI: 3.8%, 6.9%].

We can calculate its contribution in terms of "compute-equivalent gains" numerically: we first calculate the predicted loss for a transformer with some \(N\) and \(D\), and the predicted loss for a

    & Parameter & Data & Data \\  & scaling & scaling & efficiency \\  RNN (2012) \(\) LSTM (2016) & 16.9\% & 40.4\% & 43.8\% \\ RNN (2012) \(\) Transformer (2018) & 48.1\% & 20.6\% & 32.1\% \\ RNN (2012) \(\) GPT-2 (2019) & 47.7\% & 20.1\% & 32.9\% \\ RNN (2012) \(\) GPT-3 (2021) & 50.2\% & 26.0\% & 24.4\% \\ RNN (2012) \(\) Gopher (2021) & 54.8\% & 24.0\% & 21.7\% \\ LSTM (2016) \(\) Transformer (2018) & 82.6\% & 0.0\% & 17.9\% \\ LSTM (2016) \(\) GPT-2 (2019) & 72.1\% & 16.1\% & 12.1\% \\ LSTM (2016) \(\) GPT-3 (2021) & 69.9\% & 20.0\% & 10.3\% \\ LSTM (2016) \(\) Gopher (2021) & 68.8\% & 17.6\% & 13.9\% \\ Transformer (2018) \(\) GPT-2 (2019) & 56.8\% & 38.2\% & 5.0\% \\ Transformer (2018) \(\) GPT-3 (2021) & 63.1\% & 29.7\% & 7.4\% \\ Transformer (2018) \(\) Gopher (2021) & 61.9\% & 25.4\% & 12.9\% \\   

Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli (2018) (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. (2017) to be decoder-only.

non-transformer with the same inputs. We then determine reduction in \(N\) and \(D\) to match this difference in loss. Compute is then approximated as usual, as \(C 6ND\). In short, if an innovation halves the compute needed to achieve a specific loss, then that innovation has a compute-equivalent gain of 2.

Based on 100 bootstraps, we obtain a median estimate of 9.6\(\) [90% CI: 4.3\(\), 34.5\(\)] for the transformer's compute-equivalent gain.11 This substantial gain indicates that the efficiency offered by the transformer architecture is equivalent to around \((9.6)/(24) 23\%\) of the total gains from algorithms in the past nine years, or nearly two years of algorithmic progress in the field.12 Moreover, this could understate the gains if the transformer architecture also provides a convenient vehicle through which to productively channel compute, thereby facilitating some of the gains through the scaling of compute that have likely dominated the overall gains we have seen recently.

One caveat here is that the measured significance of the transformer may depend on how it is evaluated. For example, transformers may be better adapted to long contexts than recurrent networks, and evaluations using longer contexts (e.g. \(>\)1000 tokens) may suggest a larger improvement from transformers than evaluations using shorter contexts (Kaplan et al., 2020). We have not explicitly controlled for context length here, and we discuss the potential impact of this assumption in more detail in appendix E.2.1.

## 4 Discussion and conclusion

### Summary of our findings

This paper presents a comprehensive empirical analysis of algorithmic progress in language model pre-training from 2012 to 2023. By curating a dataset of over 200 language model evaluations on WikiText and Penn Treebank benchmarks, we quantify the relative contributions of compute scaling and algorithmic efficiency improvements to the overall performance gains. Our key findings are as follows:

First, we estimate that the compute required to reach a set language modeling performance level has halved every 7-8 months on average since 2012. This supports the common intuition that language modeling is an unusually rapidly-advancing field of computer science.

Second, our work reveals that the majority of recent advancements in language modeling stem more from scaling models and datasets than from pre-training algorithmic innovations. A Shapley

Figure 3: Pareto frontiers for GPT-2 (Radford et al., 2019) and Chinchilla (Hoffmann et al., 2022) level performance on WT103. We truncate the frontiers to a factor of \(13\) greater or smaller than the existing training dataset size and parameter size of the actual model since extrapolating further out would not be reliable.

value-based analysis suggests that 60-95% of the performance gains stem from compute scaling, while algorithms contribute only 5-40%.

Third, the introduction of the transformer architecture in 2017 was a major algorithmic advance, representing between 3x and 46x in compute-equivalent gain, which accounts for more than 10% of the algorithmic innovation in pre-trained language models in the past decade. This highlights the significance of the transformer as a key architectural breakthrough in the field.

### Limitations

While our analysis is an advance in quantifying algorithmic progress, several limitations reduce the precision of and temper our confidence in our estimates:

* **Lack of estimates of gains from specific innovations**. Our model is specified to quantify algorithmic progress over relatively large time periods (e.g. over several years). However, it is unable to give reliable fine-grained information, such as progress over shorter time scales, or the significance of specific innovations. Experimental work is better suited to estimating efficiency gains for specific algorithmic innovations.
* **Limited availability of quality data, resulting in noisy or unrealistic estimates of progress**. The approach we use in our analysis relies heavily on having many data samples across many years. This proved to be very challenging for a number of reasons--e.g. models are not always evaluated on the same benchmark, data is relatively sparse prior to 2017, and papers may not report relevant information such as parameter counts. Among other reasons this can result in our estimates being very noisy, yielding wide confidence intervals over doubling times. In addition, algorithmic improvements and scaling have historically been introduced concurrently, and this correlation between the two in our dataset can make it hard to disentangle their relative contributions to overall effective compute growth.
* **Inconsistencies in model training and evaluations**. Inconsistencies in evaluations are well-known. While we have excluded non-standard evaluations from our dataset, our dataset spans models with different tokenization schemes, text preprocessing, stride lengths, and other details. This introduces noise and potential bias in our estimates of algorithmic progress, as researchers might have adopted more favorable evaluation schemes over time. However, our estimated reductions in perplexity from algorithmic improvements are large; likely larger than can be accounted for by changes in evaluation procedures. We expand on these points in Appendix E.2.3.
* **Inability to distinguish between data quality and efficiency in data use**. Reduction in data requirements could be due to both improved data quality and improved algorithms, but our model is not equipped to distinguish between these effects. Understanding the relative contributions of each could be a subject of future research.
* **Reliance on the Chinchilla scaling law**. The scaling law from which our model is derived applies to dense transformers following a GPT-3 architecture (Hoffmann et al., 2022; Rae et al., 2021). However, we use this scaling law to model algorithmic improvements in different transformer architectures, recurrent neural networks, etc. Future algorithms might also follow different scaling laws (e.g. GPT-4 is rumored to be a mixture of experts). However, we believe it is likely that our core results should still hold: for one, neural scaling is not a phenomenon restricted to transformers (e.g. it is known to happen in RNNs as well, see Kaplan et al. (2020)). We find that a wide range of statistical model structures provide consistent estimates, and that alternative methods of estimating pre-training algorithmic progress also give similar results (see e.g. appendix A), so it is probable that our core results are robust to the use of the scaling law from Hoffmann et al. (2022).
* **Limited insight about future progress**. While the results from this paper could be used to inform one about future progress in language modeling, our paper focuses on historical improvements. Future rates of progress could be slower (e.g. if one thinks that historical progress consisted of picking "low hanging-fruit"), but they could potentially also be faster (e.g. due to increased research interest and investment). Expectations about future progress need to account for factors such as these, which we do not discuss in depth for the most part.

### Conclusion

Using a dataset of over 200 language model evaluations spanning 2012-2023 evaluated on Wikitext and Penn Treebank, we find that the compute required to reach a fixed performance threshold has halved approximately every 8 months. This is much faster than the rate associated with Moore's law and many other domains of computing. While algorithmic innovations have occurred rapidly, compute scaling has expanded by over a million-fold in this same period, exceeding the gains from algorithms and constituting the predominant source of performance improvements in recent years.

Overall, our work provides a quantitative estimate of the rapid pace of progress in language modeling. It also reveals the dominant role of scale rather than algorithms for recent gains. Future work could benefit from extending this analysis to additional, specific benchmarks and more closely examining the impact of data quality improvements and the gains from additional specific innovations. Despite its limitations, this research demonstrates the valuable insights that can be gained from a detailed statistical analysis of extensive datasets of machine learning results. By identifying the main drivers of performance improvements, this work lays the groundwork for further exploration and understanding of these trends in the field.