# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Let \(\) be a probability measure on \(\). Given noisy training data

\[\{(X_{i},F(X_{i})+E_{i})\}_{i=1}^{m},\] (1.2)

where \(X_{1},,X_{m}_{}\) and \(E_{i}\) is noise, a typical operator learning methodology consists of three objects: an approximate encoder \(_{}:^{d_{X}}\), an approximate decoder \(_{}:^{d_{}}\) and a DNN \(:^{d_{X}}^{d_{}}\). It then approximates \(F\) as

\[F:=_{} _{}.\] (1.3)

The encoder and decoder are either specified by the problem, learned separately from data, or learned concurrently with \(\). The goal, as in all supervised learning problems, is to ensure good generalization via the learned operator \(\) from as little training data \(m\) as possible.

### Contributions

As noted in, e.g., [16; 66], the theory of deep operator learning is still in its infancy. We contribute to this growth in the following ways. We consider learning classes of _holomorphic_ operators (Assumption 2.2), with arbitrary approximate encoders \(_{}\) and decoders \(_{}\). As we explain in SS2.3 (see also [52; SS5.2], [53; SS3.4] and ) these operators are relevant in many applications, notably those involving _parametric_ PDEs. The main contributions of this work are as follows.

1. We consider operators taking values in general Banach spaces. As noted, the vast majority of existing work (with the notable exception of ) considers Hilbert spaces.
2. We consider standard feedforward DNN architectures (constant width, width exceeds depth) and training procedures (\(^{2}\)-loss minimization).
3. (Theorem 3.1) We construct a family of DNNs such that any approximate minimizer of the corresponding training problem satisfies a generalization bound that is explicit in the various error sources: namely, an _approximation error_, which decays algebraically in the amount of training data \(m\); _encoding-decoding errors_, which depend on the accuracy of the learned encoders and decoders; an _optimization error_, and; a _sampling error_, which depends on the noise \(E_{i}\) in (1.2).
4. These DNN architectures are _problem agnostic_; they depend on \(m\) only. In particular, the architectures are completely independent on the regularity assumptions of target operator.
5. (Theorem 3.2) We show that training problems based on _any_ family of fully-connected DNNs possess uncountably many minimizers that achieve the same generalization bounds.
6. (Theorems 3.1-3.2) We provide bounds in both the \(L^{2}_{}\)- and \(L^{}_{}\)-norms that hold in high probability, rather than just expectation.
7. (Theorems 4.1-4.2) We show that the generalization bound is optimal with respect to \(m\): no learning procedure (not necessarily DL-based) can achieve better rates in \(m\) up to log terms.
8. Finally, we present a series of experiments demonstrating the efficacy of DL on challenging problems such as the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs, the latter two of which involve operators whose codomains are Banach, as opposed to Hilbert, spaces.

### Relation to previous work

Approximating an operator between function spaces with training data obtained through numerical PDEs solves presents a formidable challenge. Nevertheless, in recent years, significant advances have been made through the development of DL techniques, leading to the field of _operator learning_[51; 40; 53; 56; 58; 60; 62; 69; 63; 103; 15]. These approaches often leverage intricate DNN architectures to approximate the complex mappings inherent in physical modelling scenarios. Many works have also focused on the practical aspects of operator learning in real-world applications [13; 24; 35; 36; 37; 42; 45; 48; 49; 59; 61; 64; 65; 70; 73; 77; 80; 81; 84; 96; 98; 99; 100; 101; 105].

On the theoretical side, universal approximation theorems for operator learning have been developed in [50; 55; 68; 69] and elsewhere. Such bounds are typically not quantitative in the size of the DNN needed to achieve a certain error. For this, one typically either restricts to specific operators (e.g., certain PDEs) or imposes regularity conditions. One such assumption is Lipschitz regularity - see [10; 16; 55; 66; 87] and references therein. However, learning Lipschitz operators suffers from a _curse of parametric complexity_, meaning that algebraic rates may not be achievable. Anothercommon assumption is holomorphy. While stronger, it is, as noted, very relevant to operator learning problems involving parametric PDEs. Quantitative approximation results for holomorphic operators have been shown in [26; 31; 41; 55; 71] and elsewhere.

However, these works do not consider the generalization error, i.e., the error incurred when learning the approximation (1.3) from the finite training data (1.2). This is particularly important in applications of operator learning where data is obtained through expensive numerical PDE solves, since such problems are highly _data-starved_. Several works have tackled this question from the perspective of statistical learning theory and nonparametric estimation [16; 55; 66], but only for Lipschitz operators. As observed in [6; SS9.5], this approach generally leads to a best \((m^{-1/2})\) decay of the \(L^{2}_{}\)-norm error with respect to \(m\). Theorem 4.1 shows that such a rate is strictly suboptimal for learning the classes of holomorphic operators we consider. Our generalization bounds in Theorems 3.1-3.2 do not use such techniques, and yield near-optimal rates in both the \(L^{2}_{}\)_- and \(L^{}_{}\)-norms. See also  for some related work in this direction for reduced-order modelling with convolutional autoencoders.

Our work is inspired by recent research on learning holomorphic, Banach-valued functions [2; 5; 6]. We extend both these works, in particular, , to learning holomorphic operators. We also significantly improve the error decay rates in  with respect to \(m\) and show they can be achieved using substantially smaller DNNs with standard training (i.e., \(^{2}\)-loss minimization). See Remarks C.1-C.2. Our theoretical guarantees fall into the category of _encoder-decoder-nets_, which includes the well-known _PCA-Net_ and _DeepONet_ frameworks. As in other recent works [16; 30; 55; 66], in Theorems 3.1-3.2 we assume the encoder-decoder pair \((_{},_{})\) in (1.3) have been learned, and focus on the generalization error when training the DNN \(\).

## 2 Notation, assumptions, setup and examples

### Notation

Let \((,\|\|_{})\) and \((,\|\|_{})\) be Banach spaces and \(\) be a probability measure on \(\). Let \((^{*},\|\|_{^{*}})\) be the dual of \(\) and \(B(^{*})\) be its unit ball. The _Bochner_ and _Pettis_\(L^{p}\)-norms of a (strongly and weakly, respectively) measurable operator \(F:\) are defined as

\[\|F\|_{L^{p}_{}(;)} =(_{}\|F(X)\|^{p}_{}\, (X))^{1/p}\] \[\|F\|_{L^{p}_{}(;)} =_{y^{*} B(^{*})}(_{}|y^{* }(F)|^{p}\,(X))^{1/p},\]

respectively, for \(1 p<\), and analogously for \(p=\) (see, e.g., [8; 44]). Notice that \(\|\!\|F\|_{L^{p}_{}(;)}\|F\|_{L^{p}_{}( ;)}\) for \(1 p<\), while \(\|\!\|F\|_{L^{}_{}(;)}=\|F\|_{L^{}_{} (;)}\).

Throughout this work, \(^{p}()\), \(0<p\) denotes the standard \(^{p}\) space with (quasi-)norm \(\|\|_{p}\). We also define the _monotone \(^{p}\) space_\(^{p}_{}()\) as the space of all sequences \(=(z_{i})_{i=1}^{}^{}\) whose minimal monotone majorant \(}^{p}()\). Here \(=(_{i})_{i=1}^{}\) is defined as \(_{i}=_{j i}|z_{j}|\).

Given a (componentwise) activation function \(\), we consider feedforward DNNs of the form

\[N:^{n}^{k},\  N()=_{L+1}( (_{L}(((_{0}()))))),\] (2.1)

where \(_{l}:^{N_{l}}^{N_{l+1}}\) are affine maps, and \(N_{0}=n\) and \(N_{L+2}=k\). We define \((N)=\{N_{1},,N_{L+1}\}\) and \((N)=L\). We denote a class of DNNs of the form (2.1) with a fixed architecture (i.e., fixed activation function, depth and widths) as \(\), and write \(()=\{N_{1},,N_{L+1}\}\) and \(()=L\).

### Assumptions and setup

Let \(F:\) be the unknown operator we seek to learn and

\[}_{}:^{d_{X}},\ }_{}:^{d_{X}}, _{}:^{d_{}},\ _{}:^{d_{}}\]

be approximate encoders and decoders for \(\) and \(\), respectively. As mentioned, we assume that these maps have already been learned, and focus on the training of the DNN \(\) in (1.3). Our main results allow for arbitrary encoders and decoders (subject to the assumptions detailed below), and provide generalization bounds that are explicit in these terms: specifically, they depend on how well each encoder-decoder pair approximates the respective identity map on \(\) or \(\).

In order to formulate the precise notion holomorphy for the operator \(F\), we require the following. Let \(D=[-1,1]^{}\) and \(\) be the uniform probability measure on \(D\). Given \(>1\), we define the _Bernstein ellipse_\(()=(x+x^{-1})/2:x,\;1|x| }\), and, for convenience, we let \((1)=[-1,1]\). Next, for \(=(_{i})_{i}\), we define the _Bernstein polyellipse_ as the product \(()=(_{1})(_{2}) ^{}\).

**Definition 2.1** (Holomorphic map).: Let \(>0\), \(^{1}()\) with \(\). A Banach-valued function \(f:D\) is \((,)\)_-holomorphic_ if it is holomorphic in the region

\[(,)=():,\;_{j=1}^{}(_{j}+_{j}^{-1})/2-1 \,b_{j}}^{},=(b_{ j})_{j}.\] (2.2)

See, e.g., [17; 85]. As noted in  we can, by rescaling \(\), assume that \(=1\). For convenience, we define the following unit ball, consisting of all such functions of norm at most one over \((,1)\):

\[()=\{f:D\;(,1):\|f()\|_{} 1,\;(,1)\}.\] (2.3)

**Assumption 2.2**.: _Let \(D=[-1,1]^{}\) and \(\) be the uniform probability measure on \(D\). (A.1) There is a measurable mapping \(:^{}\) such that pushforward \(:=\) is a quasi-uniform measure supported on \(D\) and \(|_{()}:^{}()\) is Lipschitz with constant \(L_{} 0\). (A.2) The operator \(F\) has the form \(F=f\), where \(f()\) for some \(^{p}_{}()\) and \(0<p<1\). (A.3) The map \(_{}:=_{d_{}}} _{}}_{}\) is measurable (here \(_{d_{}}:^{d_{}}\) is the restriction of \(\), i.e., \(_{d_{}}(X)=((X)_{i=1}^{d_{}}\)) and the pushforward \(:=_{}\) is absolutely continuous with respect to \(\). (A.4) The maps \(_{}\) and \(_{}\) are linear and bounded._

Now let \(X_{1},,X_{m}_{}\) and consider the training data

\[\{(X_{i},Y_{i})\}_{i=1}^{m}()^{m}, Y_{i}=F(X_{i})+E_{i}\] (2.4)

and \(E_{i}\) represents noise. Let \(\) be a class of DNNs \(N:^{d_{}}^{d_{}}\), and define

\[F:=_{}_{},*{argmin}_{N}_{i=1}^{m }\|Y_{i}-_{} N_{}(X_{i}) \|_{}^{2}.\] (2.5)

### Discussion of assumptions

We now discuss (A.1)-(A.4). In SS6 we describe future work on relaxing these assumptions.

(A.1) is a weak assumption. It asserts that there is a Lipschitz map \(\) under which the pushforward of \(\) is a quasi-uniform measure supported in \(D\). As we discuss in Example 2.3, this is notably the case when \(\) is the law of some random field with an affine parametrization involving bounded random variables - a situation that occurs frequently in parametric and stochastic PDE problems. (A.2) describes the specific holomorphy of the operator \(F\) - see Remark 2.4 for details. Note that we require \(^{p}_{}()\), not just \(^{p}()\). It is known  that one cannot learn holomorphic functions (and hence operators) from finite data if \(^{p}()\) only. (A.3) is a relatively weak assumption. In view of (A.1), we expect it to hold as long as the \(}_{}}_{ }_{}\) sufficiently well. Finally, (A.4) is a standard assumption, which holds for instance in the case of PCA-Net and DeepONet. The former also enforces the learned encoder \(}_{}\) to be linear, which is not needed in our setup. Moreover, both approaches usually only deal with the case where both \(\) and \(\) are Hilbert spaces.

**Example 2.3** (Parametric PDEs).: A common operator learning problem involves learning the map

\[F:a u(a),u(a) _{a}u=0\] (2.6)

and \(_{a}\) specifies a certain PDE depending on a parameter or function \(a\). A standard example is the elliptic diffusion equation over a domain \(^{n}\). Here \(a=a()^{}()=:\) is the diffusion coefficient and \(u=u(;a)\) is the solution of the PDE

\[-(a u(;a))=g,\;, u(;a)=0,\; {z}.\] (2.7)Problems such as (2.6) are ubiquitous in scientific computing, with many applications in engineering, biology, physics, finance and beyond. In many such applications, it is common to assume that the measure \(\) on \(\) is the law of a random field

\[a()=a(;)=a_{0}()+_{i=1}^{}c_{i}x_{i}_{i}( ),\] (2.8)

for functions \(a_{0},_{i}\), where the \(x_{i}\) are random variables and \(c_{i} 0\) are scalars that ensure that \(a^{}()\). Under some mild assumptions, (2.8) is then the Karhunen-Loeve (KL) expansion of the measure \(\). See, e.g.,  (see also [55, SS3.5.1]). The \(x_{i}\) are typically independent. While in some settings, they may have infinite support, it is also common in practice to assume they range between finite maxima and minima. After rescaling, one may therefore assume that \( D=[-1,1]^{}\).

Problems of this type fits into our framework. Suppose that \(=(x_{1},x_{2},)\). The measure \(\) is then given as the pushforward \(=a\) and \(f:D\) is the parametric solution map \(f: D u(;a())\). If needed, the map \(\) can be defined in a number of different ways. Suppose, for instance, that \(\) is a Hilbert space, e.g., \(=^{2}()\), and \(\{_{i}\}_{i=1}^{}\) is a Riesz system (this holds, for instance, in the case of a KL expansion, in which case \(\{_{i}\}_{i=1}^{}\) is an orthonormal basis). Then \(\{_{i}\}_{i=1}^{}\) has a unique biorthogonal dual Riesz system \(\{_{i}\}_{i=1}^{}\). We may therefore define \(:a((a-a_{0},_{i})_{1^{2}()}/c_{i})_{i=1}^{}\). Notice that \(\) is a bounded linear map and \(F(X)=f(X)=f()\) for \(X=a()\). However, evaluating \(\) is often not required for computations (see SSA.1).

This example considers an affine parametrization (2.8) inducing the measure \(\). Note that other parametrizations can be considered. Common examples include the _quadratic_\(a(z;)=a_{0}(z)+(_{i=1}^{}c_{i}x_{i}_{i}(z))^{2}\) and _log-transformed_\(a(z,)=(_{i=1}^{}c_{i}x_{i}_{i}(z))\) parametrizations .

**Remark 2.4** (Holomorphy assumption): In the previous example, the operator \(F\) stems from the solution map \(f:D\) of a parametric PDE. The regularity of solution maps of parametric PDEs has been intensively studied, and it is known that many such maps are \((,)\)-holomorphic (hence the resulting operator satisfies (A.II)). Consider, for instance, the affine diffusion problem (2.7)-(2.8). Under a mild _uniform ellipticity_ condition, the solution map of the standard weak form of the PDE \(f: D u(a(;))_{0}^{1}()\) is \((,)\)-holomorphic with \(=(b_{i})_{i=1}^{}\) and \(b_{i}=c_{i}\|_{i}\|_{^{}()}\). See, e.g., [3, Prop. 4.9], as well as SSB.3. Similar results are known for other parametric PDEs. This includes parabolic PDEs, various types of nonlinear, elliptic PDEs, PDEs over parametrized domains, parametric hyperbolic problems and parametric control problems. See  or [3, Chpt. 4] for reviews.

## 3 Main results I: upper bounds

We now present our first two main results. In these results, given an optimization problem \(_{t}f(t)\), we say that \(\) is a \(\)_-approximate minimizer_ for some \( 0\) if \(f()_{t}f(t)+^{2}\).

**Theorem 3.1** (Existence of good DNN architectures).: _Let \(m 3\), \(>0\), \(0<<1\) and \(L=L(m,)=^{4}(m)+(1/)\). Then there exists a class \(\) of hyperbolic tangent (tanh) DNNs \(N:^{d_{}}^{d_{}}\) depending on \(m\) and \(\) only with_

\[()(m/L)^{1+},( )(m/L),\] (3.1)

_such that following holds. Suppose that Assumption 2.2 holds and_

\[d_{} m/L, L_{}\|_{ }-}_{}} _{}\|_{L_{}^{2}(;)} c(m/L)^{ -1/2},\] (3.2)

_where \(_{}:\) is the identity map and \(c>0\) is a universal constant. Let \(X_{1},,X_{m}_{}\) and consider the noisy training data (2.4) with arbitrary noise \(E_{i}\). Then, with probability at least \(1-\), every \(\)-minimizer \(\) of (2.5), where \( 0\) is arbitrary, yields an approximation \(\) that satisfies_

\[|\!|F-|\!|\!|_{L_{}^{2}( ;)}  E_{,2}+E_{,2}+E_{,2}+E_{ ,2}+E_{,2},\] (3.3) \[|\!|F-|\!|_{L_{}^{}( ;)}  E_{,}+E_{,}+E_{ ,}+E_{,}+E_{,},\] (3.4)

_and, if \(\) is a Hilbert space,_

\[|\!|F-|\!|_{L_{}^{2}(; )}  E_{,2}+E_{,2}+E_{,2}+E_{ ,2}+E_{,2}.\] (3.5)_Here, the approximation error terms \(E_{,q}\), \(q=2,\), are given by_

\[E_{,q}=a_{} C(,p,)(m/L)^{+1-1/q-1 /p},\] (3.6)

_where \(a_{}=\|_{}_{}\|_{ }\), \(C(,p,)>0\) depends on \(\), \(p\) and \(\) only and \(=0\) if \(\) is a Hilbert space (as in (3.5)) or \(=1/2\) otherwise (as in (3.3)-(3.4)). The other terms are given by_

\[& E_{,2}=a_{} L_{} \|_{}-}_{}}_{}\|_{L^{2} _{}(;)}\\ & E_{,}=a_{} L_{} {m/L}(\|_{}-}_{}}_{}\|_{L^{2} _{}(;)}+\|_{}-}_{}}_{}\|_{L^{ }_{}(;)})\\ & E_{,2}=\|_{}-_{ }_{}\|_{L^{2}_{}( ;)}/\\ & E_{,}=\|_{}-_{ }_{}\|_{L^{}_{}( ;)}+\|_{}-_{}_{}\|_{L^{2}_{}( ;)},\] (3.7)

_where \(_{}:\) is the identity map and, if \(\|\|_{2;}^{2}=_{i=1}^{m}\|E_{i}\|_{ }^{2}\),_

\[E_{}=+2^{-m}&q=2\\ +2^{-m}&q=, E_{,q}= \|\|_{2;}/&q=2\\ \|\|_{2;}/&q=.\] (3.8)

(Proofs of this and all other theorems are in SSC-G of the supplemental material.) This theorem shows that there is a family of tanh DNNs that yield provable bounds for learning holomorphic operators. The error (3.3)-(3.5) decomposes into an _approximation error_ (3.6), which decays algebraically in the amount of training data \(m\). Later, in Theorems 4.1-4.2, we show that these rates are optimal when \(\) is a Hilbert space, up to log factors. Next, are the _encoding-decoding errors_ (3.7), which depend on how well the approximate encoder-decoder pairs \((}_{},}_{})\) and \((_{},_{})\) approximate the identity maps on \(\) and \(\), respectively. Observe that these terms are increasing in \(m\) for fixed encoders and decoders. Therefore, as one expects, the accuracy of the encoder-decoder approximations \(}_{}}_{}_{}\) and \(_{}_{}_{ }\) should increase with increasing \(m\) to ensure decay to zero of the generalization error as \(m\). The specific terms in (3.7) (for \(q=2\)) are quite standard in operator learning. See, e.g., . When the encoders and decoders are computed via PCA, as in PCA-Net, standard bounds can be derived for these terms . For similar analysis in the case of DeepONNets, see . Finally, the error (3.3)-(3.5) involves an _optimization error_\(E_{}\), which primarily depends on how accurately the optimization problem (2.5) is solved (i.e., the term \(\)), and a _sampling error_\(E_{}\), which depends on the error in the training data (2.4).

Theorem 3.1 allows \(\) to be a Banach or a Hilbert space. Overall, when \(\) is only a Banach space, we obtain a weaker \(L^{2}_{}\)-norm bound involving the Pettis norm (3.3) and, moreover, the approximation error \(E_{,q}\) is worse by a factor of \(1/2\) than when \(\) is a Hilbert space. (Note that one can establish a bound for the Bochner \(L^{2}_{}\)-norm error when \(\) is a Banach space via (3.4) and the inequality \(\|\|_{L^{2}_{}(;)}\|\|_{L^{}_{ }(;)}\). However, we do not believe the resulting bound is sharp). As we discuss in Remark D.18, the discrepancies between the two cases stem from the lack of an inner product structure and, in particular, the absence of Parseval's identity when \(\) is a Banach space.

Observe that the DNN architecture in Theorem 3.1 is independent of the smoothness of the operator being learned. We term such an architecture _problem agnostic_. This theorem considers tanh activations only. However, as we discuss in Remark D.11, other activations can be readily used instead. Other key facets of Theorem 3.1 are the width and depth bounds (3.1). Qualitatively, these agree with empirical practice: namely, better performing DNNs tend to be wider than they are deep, and relatively shallow DNNs perform well in practice (see  and references therein). We also see this later in SS5.

On the other hand, the family \(\) is not fully connected. As we describe in SSC.2.1, while the weights on the final layer can be arbitrary real numbers, the weights and biases in the hidden layers come from a finite (but large) set: they are _handcrafted_ to approximately emulate certain multivariate orthogonal polynomials. Since fully-connected DNNs are typically used in practice, Theorem 3.1 is essentially a theoretical contribution. In our next result, we consider the more practical scenario of fully-connected DNNs.

**Theorem 3.2** (Fully-connected DNN architectures are good).: _There are universal constants \(c_{1},c_{2},c_{3},c_{4} 1\) such that the following holds. Let \(m\), \(\), \(\) and \(L\) be as in Theorem 3.1,_

\[d_{} c_{1}(m+(1/)), L_{}\|_{ }-}_{}}_{ }\|_{L^{2}_{}(;)} c()(m+(1/ ))^{-1/2},\] (3.9)_where \(c()>0\) depends on \(\) only, consider any class \(\) of fully-connected DNNs satisfying_

\[(n_{0},n_{L+2})=(d_{},d_{}), N_{1},,N_{L+1} c _{2}(m+(1/))(m/L)^{}, L c_{3}(m/L).\] (3.10)

_Suppose that Assumption 2.2 holds and that the pushforward \(\) in (A.1) is the tensor-product of a univariate probability distribution with mean zero and variance \( 1\). Let \(X_{1},,X_{m}_{}\) and consider (2.4) with arbitrary \(E_{i}\). Then the following hold with probability at least \(1-\)._

1. _Uncountably many 'good' minimizers._ _The problem (_2.5_) has uncountably many minimizers that satisfy (_3.3_) with_ \(=0\) _or (_3.5_) with_ \(=0\) _if_ \(\) _is a Hilbert space. They also satisfy (_3.4_) with_ \(=0\) _and the modified right-hand side_ \(_{,}+LE_{,}+_{, }+E_{,}+_{,}\)_._
2. _Good minimizers are stable._ _Suppose that_ \(_{} L^{}_{}(;^{d_{X}})\) _and let_ \(_{o}>0\) _be arbitrary. Then there is a neighbourhood of DNN parameters around the parameters of each minimizer in (A) for which the approximation corresponding to any parameters in this neigbourhood also satisfies the same bounds as in (A) with_ \(=_{o}\)_._
3. _Good minimizers can be far apart in parameter space._ _For sufficiently large_ \(m\)_, there are at least_ \((m/(c_{4}L))^{2 m}\) _minimizers satisfying the bounds in (A) such that, for any two such minimizers, their parameters satisfy_ \(\|^{}\|=\|\|\) _and_ \(\|^{}-\| 1\)_._

This theorem states that DL with fully-connected DNN architectures of sufficient width and depth (3.10) can succeed, since there are minimizers that yield the optimal bounds of Theorem 3.1. Such minimizers are uncountably many in number (A), stable to perturbations (B) and many of them (exponentially in \(m\)) have sufficiently distinct and nonvanishing/nonexploding parameters (C). This theorem does not imply that _all_ minimizers are 'good' - an issue we discuss further in SS6 - but our numerical results in SS5 suggest that (approximate) minimizers obtained through training do, at least for the experiments considered, achieve the rates specified in Theorem 3.1.

## 4 Main results II: lower bounds

We now show that the various approximation errors are nearly optimal. For this, we ignore the encoding-decoding, optimization and sampling errors and proceed as follows. Let \(C(;)\) be the Banach space of continuous operators. We term an _(adaptive) sampling map_ as any map

\[:C(,)^{m}, F (F)=(F(X_{i}))_{i=1}^{m}\,,\] (4.1)

where \(X_{1}\), \(X_{2}=X_{2}(F(X_{1}))\) potentially depends on the previous evaluation \(F(X_{1})\), \(X_{3}=X_{3}(F(X_{1}),F(X_{2}))\), and so forth. Next, we term a _reconstruction map_ as any map \(:^{m} L^{2}_{}(;)\). Given this, we let \((,)=\{F=f:f()\}\) and define

\[_{m}()=_{,}_{F(,)}\|\!\|F-(F)\|\!\|_{ L^{2}_{}(;)},\] (4.2)

where the infimum is taken over all such \(\) and \(\). In other words, \(_{m}()\) measures how well one can learn holomorphic operators using _arbitrary_ training data and an _arbitrary_ reconstruction procedure.

**Theorem 4.1** (Optimal \(L^{2}\) error rates).: _Suppose that (A.1) holds. Then, for any \(0<p<1\) there is a constant \(c(p)>0\) such that the following hold._

1. _For each_ \(m\)_, there is a_ \(^{p}_{}()\)_,_ \(\)_,_ \(\|\|_{p,}=1\) _such that_ \(_{m}() c(p) m^{1/2-1/p}\)_._
2. _There is a_ \(^{p}_{}()\)_,_ \(\)_,_ \(\|\|_{p,}=1\) _such that_ \(_{m}() c(p)}{^{2/p}(2m)}\)_,_ \( m\)_._

This theorem shows that the error \(E_{,2}\) in Theorems 3.1-3.2 is optimal, up to log terms, whenever \(\) is a Hilbert space: there does not exist a reconstruction map surpasses the rate \(m^{1/2-1/p}\) for learning holomorphic operators. Note that this result applies not only to DL-based procedures, but _any_ procedure that learns such operators from \(m\) samples. Another consequence of this result is that adaptive sampling, i.e., _active learning_, is of no benefit. As shown by Theorems 3.1-3.2, the optimal rate \(m^{1/2-1/p}\) can, up to log terms, be achieved through inactive learning, i.e., i.i.d. sampling from \(\).

Theorem 4.1 considers \(L^{2}\)-norm. For the \(L^{}\)-norm, we present a somewhat weaker result. Let

\[_{m}()=_{}\{_{X_{1},,X_{m} }_{F(,)}\|F-(\{X_{i},F(X_{ i})\})\|_{L^{}_{}(;)}\},\] (4.3)where the infimum is taken over all reconstruction maps \(:()^{m} L^{}_{}(; )\) only.

**Theorem 4.2** (Optimal \(L^{}\) error rates).: _Suppose that (A.1) holds and that the pushforward \(\) is the tensor-product of a univariate probability distribution with mean zero and variance \( 1\). Then, for any \(0<p<1\) there is a constant \(c(p)>0\) such that the following hold._

1. _For each_ \(m\)_, there is a_ \(^{p}_{}()\)_,_ \(\)_,_ \(_{p,}=1\) _such that_ \(_{m}() c(p)}{(m)}\)_._
2. _There is a_ \(^{p}_{}()\)_,_ \(\)_,_ \(_{p,}=1\) _such that_ \(_{m}() c(p)}{^{2/p+1}( 2m)}\)_,_ \( m\)_._

As with the previous theorem, this result asserts that the rate \(m^{1-1/p}\) is optimal in the \(L^{}\)-norm when \(\) is a Hilbert space. However, it is strictly weaker than Theorem 4.1 as it only considers i.i.d. random sampling from \(\), as opposed to arbitrary (adaptive) samples. Note that Theorem 4.1 is an extension of [7, Thm. 4.4]. Theorem 4.2 is new, and is of independent interest since it partially addresses an open problem of  about deriving lower bounds in the \(L^{}_{}\)-norm, as opposed to just the \(L^{2}_{}\)-norm. See SSC.2.3-C.2.4 for more discussion.

## 5 Numerical experiments

We now present numerical results for DL applied to various different parametric PDE problems, as in Example 2.3. For a full description of our experimental setup, see SSA-B.

Since the main objective of this work is to examine the approximation error, we follow a standard setup and fix the encoder and decoders for each experiment, so that \(_{}\) and \(_{}\) in (1.3) do not change for different choices of \(\). We also set up our experiments so that encoding-decoding (3.7) and sampling (3.8) errors are zero. We do this in a standard way. To ensure that \(E_{,q}=0\), we truncate the parametric expansions (2.8) after \(d\) terms (henceforth termed the _parametric dimension_) and define the encoder \(_{}\) accordingly. This means we effectively consider a parametric PDE depending on finitely-many parameters. We use Finite Element Methods (FEMs) to both solve the PDE (for generating training and testing data) and define the decoder \(_{}\) (see (A.2)). To ensure that \(E_{,q}=0\), we compute errors with respect to the Bochner \(L^{2}_{}(;})\)-norm, where \(}=_{}(^{d_{}})\) is the FEM discretization of \(\). In other words, we use the same FEM code to generate test data and compute the errors as we do to construct the operator approximation \(\). See SSA.1 for further details.

The DNNs in our experiments are fully-connected and of the form (2.1). We denote by \(\)\(L N\) DNN a DNN \(\) with activation function \(\), width \(N\) and depth \(L\). To solve (2.5) we use Adam  with early stopping and an exponentially decaying learning rate. We train our DNN architectures for 60,000 epochs and results are averaged over a number of trials. See SSA.2 for further details.

**Parametric elliptic diffusion equation.** Our first example is the parametric elliptic diffusion equation (2.7). This PDE arises in many scientific computing applications, such as groundwater flow modelling, see, e.g., . We describe the full PDE and its FE discretization in SSB.3. In our experiments, we consider both affine (B.1) and log-transformed (B.2) diffusion coefficients. The latter is particularly useful in the groundwater flow problem as the permeability of various layers of sediment can vary on logarithmic scales. Differing from most prior work, we consider a novel _mixed variational formulation_ of (2.7), which has a number of key practical benefits (see SSB.3.1). In this case, \(=^{2}()\) is a Hilbert space. Fig. 1 compares the error versus the amount of training data \(m\) for various DNN architectures for learning the solution map of this PDE in \(d=4\) and \(d=8\) parametric dimensions with these two diffusion coefficients. We observe that architectures with the Exponential Linear Unit (ELU) or hyperbolic tangent (tanh) activation generally outperform similar architectures with the Rectified Linear Unit (ReLU) activation (as we discuss in Remark D.11, this difference is in agreement with our theoretical analysis). Overall, the best performing DNNs appear to roughly match the plotted rate \(m^{-1}\). As we explain further in SSB.3.2, this rate is precisely that predicted by our theory. In particular, the parametric solution map (recall Remark 2.4) is \((,)\)-holomorphic with \(^{p}_{}()\) for any \(p<2/3\), giving an effective convergent rate \(m^{1/2-1/p}\) that is arbitrarily close to \(m^{-1}\). Another important fact that we observe is that despite the parametric dimension doubling from 4 to 8, there is little change in the error behaviour.

**Parametric Navier-Stokes-Brinkman equations.** We next consider the parametric Navier-Stokes-Brinkman (NSB) equations. See SSB.4 and (B.14) for the full definition. Here the solution is a pair \((,p)\), where \(\) is the velocity field and \(p\) is the pressure. These equations describe the dynamics of a viscous fluid flowing through porous media with random viscosity. See, e.g., [28; 43; 46; 94]. We use a mixed variational formulation  to discretize the PDE. This formulation is more sophisticated that standard variational formulations, but conveys various practical advantages. Unlike the previous example, it leads to \(\) being either \(=^{4}()\) for \(\) or \(=^{2}()\) for \(p\). See SSB.4.1 for details. Fig. 2 compares a variety of DNN architectures for approximating the velocity field component in \(d=4\) and \(d=8\) parametric dimensions. Here again we observe the ELU and tanh DNN architectures outperform similar sized ReLU architectures. We also observe a rate close to \(m^{-1}\). Note that it is currently unknown whether this or the next example possess the same \((,)\)-holomorphy guarantee as that of the previous example. Yet we observe the same rate, and therefore conjecture that such a property does indeed hold in these cases. Similar to the previous example, there is also no deterioration of the rate when moving from \(d=4\) to \(d=8\).

**Parametric stationary Boussinesq equation.** Our final example is a parametric stationary Boussinesq PDE. See SSB.5 and (B.16) for the full definition. Here the solution is a triplet \((,,p)\), where \(\) is the velocity field, \(\) is the temperature and \(p\) is the pressure of the solution. The Boussinesq model arises in a variety of engineering, fluid dynamics and natural convection problems where changes in temperature affect the velocity of a fluid [14; 22; 39]. Similar to the previous example, we consider a fully mixed variational formulation (see SSB.5.1), which leads to \(=^{4}()\) (for \(\)), \(=^{4}()\) (for \(\)) or \(=^{2}_{0}()\) (for \(p\)). Fig. 3 provides numerical results. Our observations are in line with the previous two examples, with the ELU and the smaller tanh networks being most often the best performers in this problem. Once more, the errors roughly correspond to the rate \(m^{-1}\) and there is no deterioration with increasing \(d\).

## 6 Conclusions and limitations

The purpose of this work was to derive near-optimal generalization bounds for learning certain classes of holomorphic operators that arise frequently in operator learning tasks involving PDEs. Complementing and extending previous works [26; 31; 41; 55; 71] on the approximation of such operators via DNNs, we showing sharp algebraic rates of convergence in \(m\), thus confirming that

Figure 1: **Elliptic diffusion equation.** Average relative \(L^{2}_{}(;})\)-norm error versus \(m\) for different DNNs approximating the solution operator for the elliptic diffusion equation (B.9). The first two plots use the affine coefficient \(a_{1,d}\) (B.1) with \(d=4,8\), respectively. The rest use the log-transformed coefficient \(a_{2,d}\) (B.2).

Figure 2: **NSB equations.** Average relative \(L^{2}_{}(;})\)-norm error versus \(m\) for different DNNs approximating the velocity field \(\) of the NSB problem in (B.14). See Fig. 7 for results for the pressure component \(p\). The diffusion coefficients \(a_{1,d},a_{2,d}\) and \(d=4,8\) are as in Fig. 1.

such operators can be learned efficiently and without the _curse of dimensionality_. It is notable that the sizes of the various DNNs in Theorems 3.1-3.2 also do not succumb to the so-called _curse of parametric complexity_, since the width and depth bounds are at most algebraic in \(m\).

We end by discussing a number of limitations. First, assumption (A.I) may not hold in some applications. The domain \(D\) can easily be replaced by bounded hyperrectangle through rescaling and the condition that \(\) be quasi-uniform relaxed to quasi-ultraspherical (by considering ultraspherical polynomials). However, it is currently an open problem whether our results can be extended to the case where \(\) is Gaussian, in which case \(\) would typically be a tensor-product Gaussian measure on \(^{}\) and the relevant polynomials would be the Hermite polynomials. Second, the reader may have noticed that the encoder \(_{}\) defined in (A.III) and used to construct the approximation (2.5) involves the pair \((}_{},}_{})\) and the map \(_{d_{}}\). This is a technical requirement - also found in other theoretical works on operator learning - needed to obtain encoding-decoding errors of the form \(E_{,q}\), \(q=2,\). It is unknown whether it can be relaxed. It is also unknown whether the assumption on \(\) in (A.III) can be relaxed. We believe this can be done, at least if the \(L^{2}_{}\)-norm in (3.2) is replaced by the \(L^{}_{}\)-norm. Whether this is possible without modifying (3.2) is currently unknown.

Third, a limitation of Theorem 3.2 is that it only asserts that some minimizers are 'good', not all. Techniques from statistical learning theory can provide stronger bounds that hold for all minimizers. Yet, as noted in SS1.2, these tools typically produce slower rates of decay in \(m\). Overcoming this limitation - e.g., by refining these tools for the holomorphic setting or showing that the 'good' minimizers can indeed be obtained via standard training - is a topic of future work.

Finally, as noted, our theorems provided worse generalization bounds when \(\) is a Banach space than when \(\) is a Hilbert space. Our numerical results in Figs. 2-3 suggest that this factor is an artefact of the proofs. Whether it can be removed is an interesting open problem.

Figure 3: **Boussinesq equation.** Average relative \(L^{2}_{}(;})\)-norm error versus \(m\) for different DNNs approximating the temperature \(\) of the Boussinesq problem in (B.16) (see Fig. 9 for \(\) and \(p\)). The diffusion coefficients \(a_{1,d},a_{2,d}\) and \(d=4,8\) are as in Fig. 1. In this example, we also consider an additional parametric dependence in the tensor \(=_{d}\) describing the thermal conductivity of the fluid. See §B.5 and (B.17).