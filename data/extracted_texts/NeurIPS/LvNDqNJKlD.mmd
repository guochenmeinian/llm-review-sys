# A Framework for Bilevel Optimization

on Riemannian Manifolds

 Andi Han\({}^{1}\)  Bamdev Mishra\({}^{2}\)  Pratik Jawanpuria\({}^{2}\)  Akiko Takeda\({}^{1}\)

\({}^{1}\)RIKEN AIP \({}^{2}\)Microsoft, India \({}^{3}\)University of Tokyo

andi.han@riken.jp

{bamdevm, pratik.jawanpuria}@microsoft.com.

takeda@mist.i.u-tokyo.ac.jp

###### Abstract

Bilevel optimization has gained prominence in various applications. In this study, we introduce a framework for solving bilevel optimization problems, where the variables in both the lower and upper levels are constrained on Riemannian manifolds. We present several hypergradient estimation strategies on manifolds and analyze their estimation errors. Furthermore, we provide comprehensive convergence and complexity analyses for the proposed hypergradient descent algorithm on manifolds. We also extend our framework to encompass stochastic bilevel optimization and incorporate the use of general retraction. The efficacy of the proposed framework is demonstrated through several applications.

## 1 Introduction

Bilevel optimization is a hierarchical optimization problem where the upper-level problem depends on the solution of the lower-level, i.e.,

\[_{x^{d_{x}}}F(x)=f(x,y^{*}(x)),y^{*}(x)=*{arg \,min}_{y^{d_{y}}}g(x,y).\]

Applications involving bilevel optimization include meta learning , hyperparameter optimization , and neural architecture search (NAS) , to name a few. The lower-level problem is usually assumed to be strongly convex.

Common strategies for solving such problem can be classified into two categories: single-level reformulation  and approximate hypergradient descent . The former aims to reformulate the bilevel optimization problem into a single-level one using the optimality conditions of the lower-level problem as constraints. However, this may impose a large number of constraints for machine learning applications. The latter scheme directly solves the bilevel problem through iteratively updating the lower and upper-level parameters and, hence, is usually more efficient. Nevertheless, existing works have mostly focused on unconstrained bilevel optimization .

In this work, we study bilevel optimization problems where \(x\) and \(y\) are on Riemannian manifolds \(_{x}\) and \(_{y}\), respectively. We focus on the setup where the lower-level function \(g(x,y)\) is geodesic strongly convex (a generalized notion of convexity on manifolds, defined in Section 2) in \(y\). This ensures the lower-level problem has a unique solution \(y^{*}(x)\) given \(x\). The upper-level function \(f\) can be nonconvex on \(_{x}_{y}\). Because the unconstrained bilevel optimization is a special case of our formulation on manifolds, such a formulation includes a wider class of applications. Examples of Riemannian bilevel optimization include Riemannian meta learning  and NAS over SPD networks . Moreover, there has been a surge of interest of min-max optimization over Riemannian manifolds , which also gets subsumed in the framework of bilevel optimization with \(g=-f\).

**Contributions.****(i)** We derive intrinsic Riemannian hypergradient via the implicit function theorem and propose four strategies for estimating the hypergradient, i.e., through Hessian inverse, conjugate gradient, truncated Neumann series, and automatic differentiation. We then provide hypergradient estimation error bounds for all the proposed strategies. **(ii)** We introduce the Riemannian hypergradient descent algorithm to solve bilevel optimization problems on manifolds and provide convergence guarantees. We also generalize the framework to the stochastic setting and to allow the use of retraction. **(iii)** The efficacy of the proposed modeling is shown on several problem instances including hyper-representation over SPD matrices, Riemannian meta learning, and unsupervised domain adaptation. The proofs, extensions, and experimental details are deferred to the appendix sections.

**Related works in unconstrained setting.** Unconstrained bilevel optimization where the lower-level problem is strongly convex has been widely studied [19; 32; 40; 11; 52; 45; 14]. A crucial ingredient is the notion of hypergradient in bilevel optimization problems and its computation. There exist strategies for approximating the hypergradient, e.g., using conjugate gradient , Neumann series , iterative differentiation , and Nystrom method . While bilevel optimization with constraints is relatively unexplored, a few works exists that impose constraints only for the upper level problem [32; 10]. Recently, linearly lower-level constrained bilevel optimization has been explored in [65; 68], where a projected gradient method is employed for the lower-level problem.

**Related works on manifolds.** There has been limited work on bilevel optimization problems on manifolds.  studies semivectorial bilevel optimization on Riemannian manifolds where the upper-level is a scalar optimization problem while the lower-level is a multiobjective problem under greatest coalition. [50; 49] reformulate bilevel problems on manifolds into a single-level problem based on the KKT conditions on manifolds. However, for all those works, it is unclear whether there exists an algorithm that efficiently solves the problem in large-scale settings. In contrast, we aim to provide a general framework for solving bilevel optimization on Riemannian manifolds.  is a contemporary work that also proposes gradient-based algorithms for bilevel optimization on Riemannian manifolds. The main differences of our work with respect to  are as follows: (1) We provide an analysis for various hypergradient estimators while  focuses on conjugate gradient for deterministic setting and Neumann series for stochastic setting; (2) We provide an analysis for retraction which is more computationally efficient than exponential map and parallel transport employed in ; and (3) We explore the utility of Riemannian bilevel optimization in various machine learning applications, which is not the case with .

## 2 Preliminaries and notations

A Riemannian manifold \(\) is a smooth manifold equipped with a smooth inner product structure (a Riemannian metric) \(,_{p}:T_{z} T_{z} \) for any \(z\) and its tangent space \(T_{z}\). The induced norm is thus \(\|u\|_{z}=}\) for any \(u T_{z}\). A geodesic \(c:\) generalizes the line segment in the Euclidean space as the locally shortest path on manifolds. The exponential map on a manifold is defined as \(_{z}(u)=c(1)\) for a geodesic \(c\) that satisfies \(c(0)=z,c^{}(0)=u\). In a totally normal neighbourhood \(\) where exponential map has a smooth inverse, the Riemannian distance \(d(x,y)=\|_{x}^{-1}(y)\|_{x}=\|_{y}^{-1}(x)\|_{y}\). The parallel transport operation \(_{z_{1}}^{z_{2}}:T_{z_{1}} T_{z_{2}}\) is a linear map which preserves the inner product, i.e., \( u,v_{z_{1}}=_{z_{1}}^{z_{2}}u,_{z_{1}}^{z_{ 1}}v_{z_{2}}\), \( u,v T_{z_{1}}\). The (Cartesian) product of Riemannian manifolds \(_{x}_{y}\) is also a Riemannian manifold.

For a differentiable function \(f:\), the Riemannian gradient \(f(z) T_{z}\) is the tangent vector that satisfies \(f(z),u_{z}=f(z)[u]\) for all \(u T_{z}\). Here \(\) is the differential operator and \(f(z)[u]\) represents the directional derivative of \(f\) at \(z\) along \(u\). For a twice differentiable function \(f\), Riemannian Hessian \(f(z)\) is defined as the covariant derivative of Riemannian gradient.

Geodesic convexity extends the convexity notion in the Euclidean space to Riemannian manifolds. A geodesic convex set \(\) is where any two points can be joined by a geodesic. A function \(f:\) is said to be geodesic (strongly) convex if for all geodesics \(c:\), \(f(c(t))\) is (strongly) convex in \(t\). If the function is smooth, then \(f\) is called \(\)-geodesic strongly convex if and only if \(f(_{z}(tu)) f(z)+tf(z),u_{z}+t^{2} \|u\|_{z}^{2}\),\( t\). An equivalent second-order characterization is \((z)\), where we denote \(\) as the identity operator.

For a bifunction \(:_{x}_{y}\), we denote \(_{x}(x,y),_{y}(x,y)\) as the Riemannian (partial) gradient and \(_{x}(x,y),_{y}(x,y)\) as the Riemannian Hessian. The Riemannian cross-derivativesare linear operators \(^{2}_{xy}(x,y):T_{y}_{y} T_{x}_{x}, ^{2}_{yx}(x,y):T_{x}_{x} T_{y}_{y}\) defined as \(^{2}_{xy}(x,y)[v]=_{y}_{x}(x,y)[v]\) for any \(v T_{y}_{y}\) (with \(\) representing the differential operator) and similarly for \(^{2}_{yx}(x,y)\). For a linear operator \(T:T_{x}_{x} T_{y}_{y}\), the adjoint operator, denoted as \(T^{}\) is defined with respect to the Riemannian metric, i.e., \( T[u],v_{y}= T^{}[v],u_{x}\) for any \(u T_{x}_{x},v T_{y}_{y}\). The operator norm of \(T\) is defined as \(\|T\|_{y}_{u T_{x}_{x}:\|u\|_{x}=1}\|T[u]\|_{y}\).

## 3 Proposed Riemannian hypergradient algorithm

In this work, we consider the constrained bilevel optimization problem

\[_{x_{x}}F(x) f(x,y^{*}(x)),y^{*}(x)=*{arg\,min}_{y_{y}}g(x,y),\] (1)

where \(_{x},_{y}\) are two Riemannian manifolds and \(f,g:_{x}_{y}\) are real-valued jointly smooth functions. We focus on the setting where the lower-level function \(g(x,y)\) is geodesic strongly convex. This ensures the lower-level problem has a unique solution \(y^{*}(x)\) for a given \(x\). The upper-level function \(f\) can be nonconvex on \(_{x}_{y}\).

We propose to minimize \(F(x)\) directly within the Riemannian optimization framework. To this end, we need the notion of the Riemannian gradient of \(F(x) f(x,y^{*}(x))\), which we call the Riemannian hypergradient.

**Proposition 1**.: _The differential of \(y^{*}(x)\) and the Riemannian hypergradient of \(F(x)\) are given by_

\[y^{*}(x)&=-_{y}^{-1}g(x,y^{*}(x)) ^{2}_{yx}g(x,y^{*}(x))\\ F(x)&=_{x}f(x,y^{*}(x))-^{2}_{xy}g(x,y^{*}(x ))[_{y}^{-1}g(x,y^{*}(x))[_{y}f(x,y^{*}(x))]].\] (2)

The above proposition crucially relies on the implicit function theorem on manifolds  and requires the invertibility of the Hessian of the lower level function \(f\) with respect to \(y\). This is guaranteed in our setup as \(f\) is geodesic strongly convex in \(y\). Hence, there exists a unique differentiable function \(y^{*}(x)\) that maps \(x\) to the lower-level solution. We show the Riemannian hypergradient descent (RHGD) algorithm for (1) in Algorithm 1.

```
1:Initialize \(x_{0}_{x},y_{0}_{y}\).
2:for\(k=0,...,K-1\)do
3:\(y_{k}^{0}=y_{k}\).
4:for\(s=0,...,S-1\)do
5:\(y_{k}^{s+1}=_{y_{k}^{s}}(-_{y}\,_{y}g(x_{k},y_{k}^ {s}))\).
6:endfor
7:Set \(y_{k+1}=y_{k}^{S}\).
8:Compute approximated hypergradient \(}F(x_{k})\).
9:Update \(x_{k+1}=_{x_{k}}(-_{z}}F(x_{k}))\).
10:endfor ```

**Algorithm 1** Riemannian hypergradient descent (RHGD)

We highlight that Step 8 of Algorithm 1 approximates the Riemannian hypergradient. In the rest of the section, we discuss various computationally efficient ways to estimate the Riemannian hypergradient and discuss the corresponding theoretical guarantees for RHGD. The error of hypergradient approximation comes from the inaccuracies of \(y_{k+1}\) to \(y^{*}(x_{k})\) and also from the Hessian inverse.

### Hypergradient estimation

When the inverse Hessian of the lower-level problem can be computed efficiently, we can estimate the hypergradient directly by evaluating the Hessian inverse (**HINV**) at \(y_{k+1}\), i.e., \(}_{}F(x_{k})=_{x}f(x_{k},y_{k+1}) --^{2}_{xy}g(x_{k},y_{k+1})_{y}^{-1}g(x_{k},y_{k+1 })[_{y}f(x_{k},y_{k+1})]\). However, computing the inverse Hessian is computationally expensive in many scenarios. We now discuss three practical strategies for estimating the Riemannian hypergradient when \(y_{k+1}\) is given.

**Conjugate gradient approach (CG).** When evaluating the Hessian inverse is difficult, we can solve the linear system \(_{y}g(x_{k},y_{k+1})[u]=_{y}f(x_{k},y_{k+1})\) for some \(u T_{y_{k+1}}_{y}\). To this end, we employ the tangent space conjugate gradient algorithm (Appendix F, Algorithm 3) that solves the linear system on the tangent space \(T_{y_{k+1}}_{y}\) with only access to Hessian-vector products, i.e., \(}_{}F(x_{k})=_{x}f(x_{k},y_{k+1})- _{xy}^{2}g(x_{k},y_{k+1})[_{k}^{T}]\), where \(_{k}^{T}\) is computed as a solution to \(_{y}g(x_{k},y_{k+1})[_{k}^{T}]=_{y}f(x_{k},y_{k+1})\), where \(T\) is the number of iterations of the tangent space conjugate gradient algorithm.

**Truncated Neumann series approach (NC).** The Neumann series states for an invertible operator \(H\) such that \(\|H\| 1\), its inverse \(H^{-1}=_{i=0}^{}(-H)^{i}\), where \(\) is the identity operator. An alternative approach to estimate the Hessian inverse is to use a truncated Neumann series, which leads to the following approximated hypergradient, \(}_{}F(x_{k})=_{x}f(x_{k},y_{k+1})- _{xy}^{2}g(x_{k},y_{k+1})[_{i=0}^{T-1}(- _{y}g(x_{k},y_{k+1}))^{i}[_{y}f(x_{k},y_{k+1})]]\), where \(\) is chosen such that \((-_{y}g(x_{k},y_{k+1})) 0\). \(\) can be set as \(=}\), where the gradient operator is \(L\)-Lipschitz (discussed later in Definition 1). Empirically, we observe that this approach is faster than the conjugate gradient approach. However, it requires estimating \(T\) and \(L\) beforehand.

**Automatic differentiation approach (AD).** Another hypergradient estimation strategy follows the idea of iterative differentiation by backpropagation. After running several iterations of gradient update to obtain \(y_{k+1}\) (which is a function of \(x_{k}\)), we can use automatic differentiation to compute directly the Riemannian gradient of \(f(x_{k},y_{k+1}(x_{k}))\) with respect to \(x_{k}\). We can compute the Riemannian hypergradient from the differential in the direction of arbitrary \(u T_{x_{k}}_{x}\) using basic chain rules.

### Theoretical analysis

This section provides theoretical analysis for the proposed hypergradient estimators as well as the Riemannian hypergradient descent. First, we require the notion of Lipschitzness of functions and operators defined on Riemannian manifolds. Below, we introduce the definition in terms of bi-functions and bi-operators and state the assumptions that are required for the analysis.

**Definition 1** (Lipschitzness).: (1) For a bifunction \(f:_{x}_{y}\), we say \(f\) has \(L\) Lipschitz Riemannian gradient in \(_{x}_{y}_{x}_ {y}\) if it satisfies for any \(x,x_{1},x_{2}_{x},y,y_{1},y_{2}_{y}\), \(\|_{y_{1}}^{y_{2}}_{y}f(x,y_{1})-_{y}f(x,y_{2})\| _{y_{2}} Ld(y_{1},y_{2})\), \(\|_{x}f(x,y_{1})-_{x}f(x,y_{2})\|_{x} Ld(y_{1},y_{2})\), \(\|_{x_{1}}^{x_{2}}_{x}f(x_{1},y)-_{x}f(x,y)\|_{x _{2}} Ld(x_{1},x_{2})\) and \(\|_{y}f(x_{1},y)-_{y}f(x,y)\|_{y} Ld(x_{1},x_{2})\).

(2) For an operator \((x,y):T_{y}_{y} T_{x}_{x}\), we say \((x,y)\) is \(\)-Lipschitz if it satisfies, \(\|_{x_{1}}^{x_{2}}(x_{1},y)-(x_{2},y)\|_{x_{2}} \,d(x_{1},x_{2})\) and \(\|(x,y_{1})-(x,y_{2})_{y_{1}}^{y_{2}}\|_{x} \,d(y_{1},y_{2})\).

(3) For an operator \((x,y):T_{y}_{y} T_{y}_{y}\), we say \((x,y)\) is \(\)-Lipschitz if it satisfies, \(\|_{y_{1}}^{y_{2}}(x,y_{1})_{y_{2}}^{y_{1}}-(x,y_{2})\|_{y_{2}}\,d(y_{1},y_{2})\) and \(\|(x_{1},y)-(x_{2},y)\|_{y}\,d(x_{1},x_{2})\).

It is worth mentioning that Definition 1 implies the joint Lipschitzness over the product manifold \(_{x}_{y}\), which is verified in Appendix C.2. Due to the possible nonconvexity for the upper level problem, the optimality is measured in terms of the Riemannian gradient norm of \(F(x)\).

**Definition 2** (\(\)-stationary point).: We call \(x_{x}\) an \(\)-stationary point of bilevel optimization (1) if it satisfies \(\|F(x)\|_{x}^{2}\).

**Assumption 1**.: All the iterates in the lower level problem are bounded in a compact subset that contains the optimal solution, i.e., there exists a constants \(D_{k}>0\), for all \(k\) such that \(d(y_{k}^{*},y^{*}(x_{k})) D_{k}\) for all \(s\). Such a neighbourhood has unique geodesic. We take \(_{k}\{D_{1},...,D_{k}\}\).

**Assumption 2**.: Function \(f(x,y)\) has bounded Riemannian gradients, i.e., \(\|_{y}f(x,y)\|_{y} M\), \(\|_{x}f(x,y)\|_{x} M\) for all \((x,y)\) and the Riemannian gradients are \(L\)-Lipschitz in \(\).

**Assumption 3**.: Function \(g(x,y)\) is \(\)-geodesic strongly convex in \(y_{y}\) for any \(x_{x}\) and has \(L\) Lipschitz Riemannian gradient \(_{x}g(x,y),_{y}g(x,y)\) in \(\). Further, the Riemannian Hessian \(_{y}g(x,y)\), cross derivatives \(_{xy}^{2}g(x,y)\), \(_{yx}^{2}g(x,y)\) are \(\)-Lipschitz in \(\).

Assumption 1 is standard in Riemannian optimization literature by properly bounding the domain of variables, which allows to express Riemannian distance in terms of (inverse) Exponential map. Also, the boundedness of the domain implies the bound on curvature, as is required for analyzing convergence for geodesic strongly convex lower-level problems [41; 71]. Assumptions 2 and 3 are common regularity conditions imposed on \(f\) and \(g\) in the bilevel optimization literature. This translates into the smoothness of the function \(F\) and \(y^{*}(x)\) (discussed in Appendix C.3).

We first bound the estimation error of the proposed schemes of approximated hypergradient as follows. For the hypergradient computed by automatic differentiation, we highlight that due to the presence of exponential map in the chain of differentiation, it is non-trivial to explicitly express \(_{x_{k}}y_{k}^{S}\). Here, we adopt the property of exponential map (which is locally linear) in the ambient space , i.e., \(_{x}(u)=x+u+O(\|u\|_{x}^{2})\). This requires the use of tangent space projection of \(\) in the ambient space as \(_{x}()\), which is solved for the \(v\) such that \( v,_{x}= u,\) for any \( T_{x}\).

For notation simplicity, we denote \(_{l}\) and \(_{}\). For analysis, we consider \(_{}=(_{l})\).

**Lemma 1** (Hypergradient approximation error bound).: _Under Assumptions 1, 2, 3, we can bound the error for approximated hypergradient as_

1. _Hinv:_ \(\|}_{}F(x_{k})-F(x_{k})\|_{x_{k}} (L+_{}M+_{l}L+_{l}_{}M)dy^{*}(x_{k}),y_ {k+1}\)_._
2. _CG:_ \(\|}_{}F(x_{k})-F(x_{k})\|_{x_{k}} L+_{}M+L1+2}_ {l}+}{}d(y^{*}(x_{k}),y_{k+1})+2L}}-1}{}+1}^{T} \|_{0}^{0}-_{y^{*}(x_{k})}^{y_{k+1}}v_{k}^{*}\|_{y_{k+1}}\)_, where_ \(v_{k}^{*}=_{y}^{-1}g(x_{k},y^{*}(x_{k}))[_{y}f(x_{k},y^{* }(x_{k}))]\)_._
3. _NS:_ \(\|}_{}F(x_{k})-F(x_{k})\|_{x_{k}} (L+_{l}L+_{}M+_{l}_{}M)d(y^{*}(x_{k}),y_{k +1})+_{l}M(1-)^{T}\)_._
4. _AD: Suppose further there exist_ \(C_{1},C_{2},C_{3}>0\) _such that_ \(\|_{x_{k}}y_{k}^{s}\|_{y_{k}^{s}}^{} C_{1}\)_,_ \(\|_{x}^{}_{x}v-v\|_{y} C_{2}d(x,y)\|v\|_{y}\) _and_ \(_{x}_{x}(u)=_{_{x}(u)} +_{x}u+\) _where_ \(\|\|_{_{x}(u)} C_{3}\|_{x}u\|_{x}\|u\|_{x}\) _for any_ \(x,y\) _and_ \(v T_{y}M_{y}\)_,_ \(u T_{x}_{x}\)_. Then,_ \[\|}_{}F(x_{k})-F(x_{k})\|_{x_{k}} }{-_{y} L^{2}}+L(1+_{l}) (1+_{y}^{2} L^{2}-_{y})^{}d(y_{k},y^{*}(x_ {k}))+M_{l}(1-_{y})^{S}\)_, where_ \((_{l}+1)+(C_{2}+_{y}C_{3})L(1-_ {y})C_{1}+_{y}L\)_._

From Lemma 1, it is evident that the exact Hessian inverse exhibits the tightest bound, which is followed by conjugate gradient (CG) and truncated Neumann series (NS). Automatic differentiation (AD) presents the worst upper bound on the error due to the introduction of curvature constant \(\), resulting in \((1-(}{L^{2}}))^{S}=(1-(^{2} }))^{S}\) for the trailing term, which could be much larger than \((1-)^{T}=(1-(}))^{T}\) for NS and \(}-1}{}+1}^{T}=(1-( }}))^{T}\) for CG. Further, the error critically relies on the number of inner iterations \(S\) compared with \(T\) for CG and NS, and the constants \(C_{1},C_{2},C_{3}\) can be large for manifolds with high curvature. We now present the main convergence result with the four proposed hypergradient estimation strategies.

**Theorem 1**.: _Denote \(_{0} F(x_{0})+d^{2}(y_{0},y^{*}(x_{0}))\) and \(L_{F}+1L++}+}{}=O(_{l}^{3})\). Under Assumptions 1, 2, 3, we have the following bounds on the hypergradient norm obtained by Algorithm 1._

* _Hinv:_ _Let_ \(_{x}=}\) _and_ \(S(_{l}^{2})\)_. We have_ \(_{k=0,,K-1}\|F(x_{k})\|_{x_{k}}^{2} 80L_{F}_{0}/K\)_._

   Methods & \(G_{f}\) & \(G_{g}\) & \(JV_{g}\) & \(HV_{g}\) \\  HGD-CG  & \(O(_{l}^{3}^{-1})\) & \((_{l}^{4}^{-1})\) & \(O(_{l}^{3}^{-1})\) & \((_{l}^{3.5}^{-1})\) \\ -AD  & \(O(_{l}^{3}^{-1})\) & \((_{l}^{4}^{-1})\) & \((_{l}^{4}^{-1})\) & \((_{l}^{4}^{-1})\) \\ SHGD-NS  & \(O(_{l}^{5}^{-2})\) & \((_{l}^{3}^{-2})\) & \(O(_{l}^{5}^{-2})\) & \((_{l}^{6}^{-2})\) \\  RHGD-HINV & \(O(_{l}^{3}^{-1})\) & \((_{l}^{5}^{-1})\) & \(O(_{l}^{3}^{-1})\) & NA \\ -CG & \(O(_{l}^{4}^{-1})\) & \((_{l}^{6}^{-1})\) & \(O(_{l}^{4}^{-1})\) & \((_{l}^{4}^{-1})\) \\ -NS & \(O(_{l}^{3}^{-1})\) & \((_{l}^{5}^{-1})\) & \(O(_{l}^{3}^{-1})\) & \((_{l}^{4}^{-1})\) \\ -AD & \(O(_{l}^{3}^{-1}

* _CG: Let \( C_{v}^{2}+_{l}^{2}(C_{v}^{2}D^{2}}{}+1)\), where \(C_{v}}{}+_{l}}{}+ _{l}^{2}+_{l}\). Choosing \(_{x}=\), \(S(_{l}^{2})\), and \(T_{ cg}(})\), we have \(_{k=0,...,K-1}\|F(x_{k})\|_{x_{k}}^{2} _{0}+\|v_{0}^{*}\|_{p^{*}(x_{0})}^{2}\)._
* _NS: Choosing \(_{x}=},S(_{l}^{2})\), and \(T_{ ns}(())\) for an arbitrary \(>0\), we have \(_{k=0,...,K-1}\|F(x_{k})\|_{x_{k}}^{2}}{K} _{0}+\)._
* _AD: Choosing \(_{x}=}\) and \(S(_{l}^{2}())\) for an arbitrary \(>0\), we have \(_{k=0,...,K-1}\|F(x_{k})\|_{x_{k}}^{2}}{K} _{0}+\)._

**Complexity analysis.** Based on the convergence guarantees in Theorem 1, we have analyzed (in Corollary 1), the computational complexity of the proposed algorithm with four different hypergradient estimation strategies in reaching the \(\)-stationary point. The results are summarized in Table 1. For reference, we also provide the computational cost of Euclidean algorithms which solve bilevel Euclidean optimization problem . We notice that except for CG, the gradient complexity for \(f\) (i.e., \(G_{f}\)) matches the Euclidean version. For conjugate gradient, the complexity is higher by \(O(_{l})\), which is due to the additional distortion from the use of vector transport when tracking the error of conjugate gradient at each epoch. In terms of gradient complexity for \(g\) (i.e., \(G_{g}\)), all deterministic methods require a higher complexity by at least \((_{l})\) compared to the Euclidean baselines. This is because of the curvature distortion when analyzing the convergence for geodesic strongly convex functions. Similar comparisons can be also made with respect to the computations of cross-derivatives and Hessian vector products.

### Extension to stochastic bilevel optimization

```
1:Initialize \(x_{0}_{x},y_{0}_{y}\).
2:for\(k=0,...,K-1\)do
3:\(y_{k}^{0}=y_{k}\).
4:for\(s=0,...,S-1\)do
5: Sample a batch \(_{1}\).
6:\(y_{k}^{s+1}=_{y_{k}^{s}}(-_{y}\,_{y_{1 }}(x_{k},y_{k}^{s}))\).
7:endfor
8: Set \(y_{k+1}=y_{k}^{S}\).
9: Sample batches \(_{2},_{3},_{4}\).
10: Compute \(}F(x_{k})\).
11: Update \(x_{k+1}=_{x_{k}}(-_{x}}F(x_{k}))\).
12:endfor ```

**Algorithm 2** Riemannian stochastic bilevel optimization with Hessian inverse.

In this section, we consider the bilevel optimization problem (1) in the stochastic setting, where \(f(x,y^{*}(x))_{i=1}^{n}f_{i}(x,y^{*}(x))\) and \(g(x,y)_{i=1}^{m}g_{i}(x,y)\). The algorithm for solving the stochastic bilevel optimization problem is in Algorithm 2, where we sample \(_{1},_{2},_{3},_{4}\) afresh every iteration. The batch index is omitted for clarity. The batches are sampled uniformly at random with replacement such that the mini-batch gradient is an unbiased estimate of the full gradient. Here, we denote \(f_{}(x,y)|}_{i}f_{i} (x,y)\) and similarly for \(g\). We let \([n]\{1,,n\}\).

In Step 10 of Algorithm 2, we can employ any hypergradient estimator proposed in Section 3.1. In this work, we only show convergence under the Hessian inverse approximation of hypergradient, i.e., \(}F(x_{k})=_{x}f_{_{2}}(x_{k},y_{k+1 })-_{xy}^{2}g_{_{3}}(x_{k},y_{k+1})[_{y}^{-1}g_ {_{4}}(x_{k},y_{k+1})][_{y}f_{_{2}}(x_{k},y_{ k+1})]]\). Similar analysis can be followed for other approximation strategies. The theoretical guarantees are in Theorem 2, where we require Assumption 4, which is common in existing works for analyzing stochastic algorithms on Riemannian manifolds .

**Assumption 4**.: Under stochastic setting, Assumption 1 holds and Assumptions 2, 3 are satisfied for component functions \(f_{i}(x,y),g_{j}(x,y)\), for all \(i[n],j[m]\). Further, stochastic gradient, Hessian, and cross derivatives are unbiased estimates.

**Theorem 2**.: _Under Assumption 4, consider Algorithm 2. Suppose we choose \(_{x}=},S(_{l}^{2})\), and \(|_{1}|,|_{2}|,|_{3}|,|_{4}| (_{l}^{2}^{-1})\) for an arbitrary \(>0\). Then we have \(_{k=0,...,K-1}\|F(x_{k})\|_{x_{k}}^{2} _{0}}{K}+\) and the gradient complexity to reach \(\)-stationary solution is \(G_{f}=O(_{l}^{5}^{-2}),G_{g}=(_{l}^{3} ^{-2}),JV_{g}=O(_{l}^{5}^{-2})\)._

In Table 1, we compare our attained complexities with that of stocBiO , which makes use of a truncated Neumann series. With exact Hessian inverse, we can match the \(G_{f}\) and \(JV_{g}\) complexitieswith stocBio. For the \(G_{g}\) complexity, the additional curvature constant is inevitable from the convergence analysis for geodesic strongly convex functions. Nevertheless we observe the same order dependency on \(_{l}\). This is mainly due to the analysis where we choose a smaller stepsize \(_{y}=(})\) compared to \(()\) in . The larger stepsize, despite increasing the convergence rate, also increases the variance under stochastic setting. We believe an order of \((_{l})\) lower can be established for stocBio, following our analysis.

### Extension to retraction

Our analysis till now has been limited to the use of the exponential map. However, the retraction mapping is often preferred over the exponential map due to its lower computational cost. Here, we show that use of retraction in our algorithms also leads to similar convergence guarantees.

**Assumption 5**.: There exist constants \( 1,c_{R} 0\) such that \(d^{2}(x,y)\|u\|_{x}^{2}\) and \(\|_{x}^{-1}(y)-u\|_{x} c_{R}\|u\|^{2}\), for any \(x,y=_{x}(u)\).

Assumption 5 is standard (e.g. in [42; 23]) in bounding the error between exponential map and retraction given that retraction is a first-order approximation to the exponential map.

**Theorem 3**.: _Suppose Assumptions 1, 2, 3 and 5 hold and let \(_{F}=4_{l}c_{R}M+5L_{F}\). Then consider Algorithm 1 with exponential map replaced with general retraction. We can obtain the following bounds._

* _HINV: Let_ \(_{x}=(1/_{F}),S(_{l}^{2} )\)_. Then_ \(_{k=0,,K-1}\|F(x_{k})\|_{x_{k}}^{2} 16_{F} _{0}/K\)_._
* _CG: Let_ \(_{x}=(1/),S(_{l}^{2} ),T_{}(})\)_, where_ \(=C_{v}^{2}+_{l}^{2}(C_{a}^{2} ^{2}}{}+)\)_. Then_ \(_{k=0,,K-1}\|F(x_{k})\|_{x_{k}}^{2} }{K}(_{0}+\|v_{0}^{*}\|_{y^{*}(x_{0})}^{2})\)_._
* _NS: Let_ \(_{x}=(1/_{F}),S(_{l}^{2} )\)_. Then for an arbitrary_ \(>0\)_,_ \(T_{}(_{l}(1/))\)_, we have_ \(_{k=0,,K-1}\|F(x_{k})\|_{x_{k}}^{2}_{F}}{K}_{0}+\)_._
* _AD: Let_ \(_{x}=(1/_{F})\)_,_ \(S(_{l}^{2}(1/))\)_. Then for an arbitrary_ \(>0\)_, we have_ \(_{k=0,,K-1}\|F(x_{k})\|_{x_{k}}^{2}_{F}}{K}_{0}+\)_._

Theorem 3 demonstrates that employing a general retraction preserves the same order of convergence and complexity as the exponential map in Theorem 1. This is due to the fact that \(_{F}=(L_{F})\) and \(=()\), where \(L_{F}\) and \(\) are as defined in Theorem 1. In addition, when exponential map is used, Theorem 3 recovers the results in Theorem 1 as \(c_{R}=0\) and \(=1\).

## 4 Experiments

This section explores various applications of bilevel optimization problems over manifolds. All the experiments are implemented based on Geoopt  and the codes are available at https://github.com/andyjm3/rhgd.

### Synthetic problem

We consider the following bilevel optimization problem on the Stiefel manifold \((d,r)=\{^{d r}:^{} =_{r}\}\) and SPD manifold \(_{++}^{d}=\{^{d d}: 0\}\) (in Appendix A):

\[_{(d,r)}(^{*}^{ }^{}),^{*}=*{arg\,min}_{_{++}^{d}} \ ,^{}+^{-1}, ^{}^{}+,\]

where \(^{n d},^{n r}\), with \(n d r\), are given matrices and \(>0\) is the regularization parameter. The above is a synthetically constructed problem that aims to maximize the similarity between \(\) and \(\) in different feature dimensions. We align \(\) and \(\) to the same dimension via \((d,r)\) and also learn an appropriate geometric metric \(_{++}^{d}\) in the lower-level problem . The geodesic convexity of the lower-level problem and the Hessian inverse expression are discussed in Appendix H.1.

**Results.** We generate random data matrices \(,\) with \(n=100,d=50\), and \(r=20\). We set \(=0.01\) and fix \(_{x}=_{y}=0.5\). We compare the three proposed strategies for approximating the hypergradient where we select \(=1.0\) and \(T_{}=50\) for Neumann series (NS) and set maximum iterations \(T_{}\) for conjugate gradient (CG) to be \(50\) and break once the residual reaches a tolerance of \(10^{-10}\). We set the number of outer iterations (epochs) \(K\) to be \(200\). Figure 1 compares RHGD with different approximation strategies implemented with \(S=20\) or \(50\) number of inner iterations.

### Hyper-representation over SPD manifolds

Hyper-representation [54; 61] aims to solve a regression/classification task while searching for the best representation of the data. It can be formulated as a bilevel optimization problem, where the lower-level optimizes the regression/classification parameters while the upper-level searches for the optimal embedding of the inputs. Suppose we are given a set of SPD matrices, \(=\{_{i}\}_{i=1}^{n}\) where \(_{i}_{++}^{d}\) and the task is to learn a low-dimensional embedding of \(_{i}\) while remaining close to their semantics labels. In particular, we partition the set into a training set \(_{}\) and validation set \(_{}\).

**Shallow hyper-representation for regression.** We consider a shallow learning paradigm over \(\) through the regression task. The representation is parameterized with \(^{}_{i}\) for \((d,r)\). The requirement of orthogonality on \(\) follows [38; 30; 33] that ensures the learned representations are SPD. The learned representation is then transformed to a Euclidean space for performing regression, namely through a matrix logarithm (that acts as a bijective map between the space of SPD matrices and symmetric matrices) and a vectorization operation \(()\) that extract the upper-triangular part of the symmetric matrix. The bilevel optimization problem is

\[_{(d,r)}_{i _{}}((^{ }_{i}))^{*}-_{i})^{2}}{2| _{}|},\\ ^{*}=^{r(r+1 )/2}}{}_{i_{}}((^{}_{i})) -_{i})^{2}}{2|_{}|}+\| \|^{2}.\]

The regularization \(>0\) ensures the lower-level problem is strongly convex. The upper-level problem is on the validation set while the lower-level problem is on the training set. We generate random \(,_{i}\) and \(\) and construct \(\) with \(y_{i}=((^{}_{i})) +_{i}\), where \(_{i}(0,1)\). We generate \(200\ _{i}\) with \(|_{}|=100\) and \(|_{}|=100\). In Figure 1(a), we show the loss on validation set (the upper loss) in terms of number of outer iterations. We compare both the deterministic (RHGD) and stochastic (RSHGD) versions of Riemannian hypergradient descent. We again observe that the best performance is attained by either the ground-truth Hessian inverse or the conjugate gradient. NS requires carefully selecting the hyperparameters \(,T\), which pose difficulties in real applications. For the stochastic versions, all the methods perform similarly.

**Deep hyper-representation for classification.** We now explore a 2-layer SPD network  for classifying ETH-80 image set . The dataset consists of 8 classes, each with 10 objects. Each object is represented by an image set consisting of images taken from different viewing angles. Here, we represent each image set by taking the covariance matrix of the images in the same set after resizing them into \(10 10\). This results in 80 SPD matrices \(_{i}\) of size \(100 100\) for classification. Let \((_{i})=((_{2}^{}(_{1}^{}_{i}_{1})_{2}))\) be the output of the 2 layer network where \(()=\{,\}^{}\) is the eigenvalue rectifying activation with the eigenvectors \(\) and

Figure 1: Figures (a) & (b) show the plot of objective of the upper-level problem (Upper Objective) for different strategies. HINV and CG strategies have fastest convergence, followed by NS and AD. The corresponding estimation errors are shown in (c). Figure (d) specifically shows the robustness of approximation error obtained by NS across different \(\) and \(T\) values.

eigenvalues \(\) of \(\). We consider the same bilevel optimization as above except the least-squares loss function becomes the cross-entropy loss. Here we sample \(5\) samples from each class to form the training set and the rest as the validation set. We set \(d_{1}=20,d_{2}=5\), and fix learning rate to be \(0.1\) for both lower and upper problems. Figures 1(b) and 1(c) show the good performance on the validation accuracy (upper-level loss).

### Riemannian meta learning

Meta learning [16; 34] allows adaptation of models to new tasks with minimal amount of additional data and training, by distilling past learning experiences. A recent work  considers meta learning with orthogonality constraint. In particular, the upper-level optimization searches for the base parameters shared by all tasks while the lower level optimizes over the task-specific parameters to ensure generalization ability. Let \(P_{}\) denote the distribution of meta tasks and for each training epoch, we sample \(m\) tasks \(^{} P_{},=1,...,m\). Each task is composed of a support and query set denoted by \(^{}_{},^{}_{}\), and the task is to learn a set of base parameters \(\) such that the model can quickly adapt to the query set from the support set by adjusting only a few parameters \(w\). For each task, the task-specific parameter \(w^{*}_{}\) is learned from the support set, which is used to update the base parameters by minimizing the loss over the query set. In standard settings, \(w_{}\) corresponds to the final linear layer of a neural network [39; 40]. Here, we adopt the setup with \(w_{}\) to be the last layer parameters in the Euclidean space while enforcing \(\) on the Stiefel manifold. The problem of Riemannian meta-learning is \(_{}_{=1}^{m}(,w^ {*}_{};^{}_{})\) s.t. \(w^{*}_{}=_{w_{}}_{=1}^{m}( ,w_{};^{}_{})+(w_{})\), where \(^{}_{}\), \(^{}_{}\) are the support and query sets for task \(\) and \(()\) is a regularizer that ensures strong convexity of the lower-level problem.

**Results.** We consider 5-ways 5-shots meta learning over the MiniImageNet dataset  where the backbone network is a 4-block CNN with the kernel of the first 2 layers constrained to be orthogonal in terms of the output channel (following ). The kernel size is \(3 3\) and we consider \(16\) output channels with a padding of \(1\). Each convolutional block consists of a convolutional layer, followed by a ReLU activation, a max-pooling and a batch normalization layer. \(\), thus, has the dimension \((16*3*3) 16=144 16\), which is constrained to the Stiefel manifold.

In Figure 1(d), we plot the test accuracy averaged for over 200 tasks. We compare RHGD with an extrinsic update baseline PHGD, which projects the update from the Euclidean space to the Stiefel manifold at every iteration. We observe the RHGD converges faster compared to the extrinsic update PHGD, thereby showing the benefit of the Riemannian modeling.

### Unsupervised domain adaptation

Given two marginals \(^{n},^{m}\) with equal total mass, i.e., \(^{}_{n}=^{}_{m}=1\) where we assume unit mass without loss of generality. Let

[MISSING_PAGE_EMPTY:10]