# Designs for Enabling Collaboration in

Human-Machine Teaming via Interactive and

Explainable Systems

 Rohan Paleja

MIT Lincoln Laboratory

Lexington, MA 02142

rohan.paleja@ll.mit.edu

&Michael Munje

The University of Texas at Austin

Austin, TX 78712

michaelmunje@utexas.edu

Kimberlee Chestnut Chang, Reed Jensen

MIT Lincoln Laboratory

Lexington, MA 02142

{chestnut, rjensen}@ll.mit.edu

&Mathew Gombolay

Georgia Institute of Technology

Atlanta, GA 30332

matthew.gombolay@cc.gatech.edu

###### Abstract

Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human's actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and that black-box approaches are easier to train and result in better HMT performance, highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.

+
Footnote †: DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering. Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.

## 1 Introduction

Successful human-machine teaming (HMT) has long been sought after for its wide utility across potential applications, ranging from virtual agents such as "clippy" that provide on-demand supportfor improving documents to embodied robotic healthcare aides that can provide doctors with a helping hand . While promising, achieving fluent HMT is challenging because interactions with humans can be incredibly complex due to the diversity across users , human teammates benefit from explainable systems to support the development of mental models , and the lack of bidirectional communication (i.e., unclear how humans can "tell" a machine online to perform a desired behavior) . In this paper, we transition from the conventional approach of crafting an HMT solution that aims for flawless out-of-the-box performance to a paradigm where end-users can actively interact with and program AI teammates, fostering a more dynamic and developmental interaction between humans and AI. Specifically, we explore enabling humans to perform user-specific modifications to a collaborative AI's interpretable policy representation across repeated iterations of teaming episodes and provide a set of design guidelines to support team development in HMT drawn from a large-scale user study.

Recently, data-driven techniques (e.g., imitation and reinforcement learning) have become popular in HMT, allowing for the generation of collaborative agent behavior without cumbersome manual programming [40; 5]. However, these prior works utilize opaque, black-box models, limiting human's ability to develop a shared mental model and maintain situational awareness , crucial for high-performance teaming . We posit that successful, real-world HMT is not feasible without the use of white-box methods, especially in safety-critical domains such as healthcare and manufacturing. Furthermore, collaborative interactions with machines have often lacked the ability to effectively learn with and adapt to human teammates in real-time . In ad hoc human-human teams, effective teaming is often developed through an iterative process . Bi-directional communication is often a key component of this process, enabling the development of successful coordination strategies . In our work, we build towards such a team development paradigm in HMT by 1) creating a pathway of bi-directional communication, utilizing interpretable policy representations as a mechanism to allow users to understand their machine teammates and allowing for explicit teammate policy modification through an interface (users can modify the machine's tree-based policy via a GUI), and 2) allowing for the process of iterative mixed-initiative team development through repeated teaming episodes. We believe this paradigm is necessary because human-partnered systems need explainable components and adaptable systems. We provide the following contributions:

* We provide a case study regarding prior work in HMT [5; 40], finding that the generated machine behavior is unable to adapt to human-preferred strategies, and that high performance is typically driven by independent machine actions rather than collaboration, which can ultimately result in a higher team score.
* We create a novel InterpretableML architecture to support the creation of tree-based cooperative agent policies via reinforcement learning and a GUI to allow users to modify the AI's behavior to their specifications. This capability is promising, enabling end-users to "go under-the-hood" of machine learning models and tune affordances or interactively and iteratively reprogram behavior.
* We conduct a 50-participant between-subjects user study assessing the effects of interpretability and interactive policy modification across repeated interactions with an AI. We summarize our study findings into a set of design guidelines to support future HMT research.

## 2 Preliminaries

Here, we introduce prior work in HMT and Explainable AI, our experimental domain, Overcooked-AI, a model of team development used to understand our findings, Tuckman's Model, and the mathematical framework under which we generate agents, Markov Games.

**Human-Machine Teaming -** The field of HMT is concerned with understanding, designing, and evaluating machines for use by or with humans [6; 44; 30]. A popular technique that has been used to produce collaborative AI agents is Reinforcement Learning (RL) , where researchers have concentrated efforts on reducing the dissimilarity between synthetic human training partners and testing with human end-users. Approaches that have achieved some success include utilizing human gameplay data to finetune simulated training partners to behave more human-like , which can be expensive, and training with a diverse-skilled population of synthetic partners to create an agent that can better generalize to non-expert end-users , which may bias the AI teammate to exhibit individualized strategies, as we display in Section 3. _We note our work focuses on an interaction different from AI-assisted decision-making or decision support. Here, a human and an agent must collaborate across a series of timesteps, aiming to maximize a multifaceted joint objective function._

**Explainable AI -** xAI is concerned with understanding and interpreting the behavior of AI systems . In our work, we follow recent trends that show black-box methods paired with local explanations can be harmful  and utilize interpretable, _white-box tree-based models_ in a multi-agent sequential decision-making problem. These models have been shown to be beneficial in improving the user's ability to simulate a decision-making model  and providing users with increased situational awareness over a teammate's behavior in an HMT setting . While tree-based models can provide users insight into the model, the complexity of the tree-based model limits its utility . While we note this as a potential weakness of utilizing tree-based models, effective state representations can provide a tradeoff between granular control and tree depth. Accordingly, we design our trees to reason over a state-space with high-level binary features and multi-step macro-actions, expanded on below. Furthermore, in our work, we explore a paradigm where a user can directly modify and visualize a tree-based AI teammate the user is interacting with after a teaming episode. Prior work in explainable debugging  and robotics  has explored similar paradigms, creating interactive systems that allow end-users to modify agent behavior to increase performance, but has not explored deploying tree-based models trained via RL in a collaborative HMT setting. We provide a working definition of what we mean by "interpretable" within the Appendix Section G.

**Overcooked-AI -** Overcooked-AI  is a testbed to evaluate human-AI interaction and has been used across HMT research concerned with collaboration , teammate identification , intention prediction , and behavior influence . Here, two agents are tasked with creating and delivering as many soups as possible within a given time. Achieving a high score requires agents to navigate a kitchen and repeatedly complete a set of sequential high-level actions, including collecting ingredients, placing ingredients in pots, cooking ingredients into a soup, collecting a dish, getting the soup, and delivering it. Both players receive the same score increase upon delivering the soup. _We modify the original Overcooked-AI game to be a simultaneous-move game as opposed to the original formulation of allowing agents to perform actions asynchronously._ This modification prevents the collaborative score metric from being dominated by super-human AI speed, causing the overall score to be more reliant upon effective collaboration and strategy. We provide details about the state and action space below and complete details in the appendix.

_State-Space:_ Policies reason over a semantically meaningful feature space as opposed to pixel space, detailing the objects each agent is holding, pot statuses, and counter objects. This state space allows for learning an interpretable tree-based policy that can be understood and manipulated by end-users.

_Action-Space:_ Instead of using cardinal actions, we allow the AI to utilize macro-actions that can accomplish high-level objectives such as ingredient collection, ingredient placement, and soup serving. Macro-actions are planned using an A* planner, and we perform dynamic replanning at each timestep. Constructing trees on a higher level of abstraction results in smaller trees that are easier to interpret.

**Tuckman's Model -** Tuckman  describes the different stages that a team goes through before reaching high performance, including "Forming", "Storming", "Norming" and "Performing," often seeing a drop in performance as team members acclimate, followed by a rise as team members understand how to collaborate. Assuming that human-machine teams will follow similar stages to human-human teams, this paper looks into how we can support human-machine teams in reaching the Performing stage, where the team is achieving its full potential and exhibiting the highest level of cooperation. We provide a depiction of these stages as part of Figure 2.

**Markov Game -** We formulate our setting as a Markov Game , defined by a set of global states, \(S_{1},S_{2} S\), a set of actions, \(A_{1},A_{2} A\), transition function, \(T:S A_{1} A_{2} S\). and reward function \(r_{i}:S A_{i}\). Agent \(i\) aims to maximize its discounted reward \(R_{i}=_{t=0}^{T}^{t}r_{i}^{t}\), where \(\) is a discount factor. For training, we utilize agent-agent collaborative training, which trains two separate agents jointly via single-agent PPO. We utilize PantheonRL  for training our agents, incorporating our novel tree-based architecture (Section 4.1) into the codebase.

## 3 A Gap in Teaming Performance

In this section, we present two examples to display a gap in the quality of AIs in HMT. Specifically, we look at two recent approaches to produce collaborative AI agents . We argue and display that the AIs trained via these approaches are rigid and exhibit individualized behaviors, missing out on collaborative teaming strategies that can ultimately result in higher team scores. _We require AI agents that can effectively reach a consensus with humans on a teaming strategy that ultimately results in high performance. In cases where the human has a preferred strategy, the AI teammate should be able to support said strategy._In Figure 1, we display the _Coordination Ring_ scenario. A simple collaboration strategy (which we term "human-preferred") in this domain is to utilize the counter to continuously pass objects, minimizing agent movement through efficient handoffs. To test a set of collaboration strategies, we utilize agents publicly available from Carroll et al. . In Figure 1, we display a frame-by-frame of the human-preferred coordination strategy (Figure 0(a)) and AI-preferred coordination strategy (Figure 0(b)), which was a strategy where agents act individually to collect ingredients and place them in pots. The latter behavior was inferred through repeated play with the publicly-available AI. With the human-preferred strategy, the AI agent freezes for the majority of the game, creating an extremely frustrating and low-performing AI teammate. In this scenario, the human (green) picks up an ingredient and places it on the counter at the start of the game. The AI agent (blue), unfamiliar with this teaming strategy, freezes for approximately \(80\%\) of the remaining episode before finally placing an onion in the pot. With the AI-preferred strategy, the human is able to successfully team with the AI, with each agent retrieving and placing ingredients while moving in a clockwise motion, but the strategy is not optimal or what the human prefers. As the AI produced by Carroll et al.  is created via RL teaming human-like AI teammates, the generated behavior may not be ideal for the current teammate, especially if the current teammate's preferred strategy was not present in the original training dataset used to create human-like AI training partners. This highlights a need for systems that can _explain strategies_ exhibited by trained agent policies and allow humans to adapt these pre-trained policies toward human-preferred behavior.

In a second example, we utilize the _Optional Collaboration_ domain, displayed on the right-hand side of Figure 3(b), which is also utilized in our human-subjects experiment. This domain was designed to incentivize collaboration, where creating mixed-ingredient dishes facilitated by agents passing ingredients across the central counter will result in a higher score per dish. Here, we program two intelligent deterministic heuristics: In the first, each agent acts completely individually, cooking single-ingredient dishes and serving. In the second, agents share ingredients, which costs additional timesteps, but are able to successfully cook mixed ingredient dishes. We find that the collaboration strategy achieves a \(408\) cumulative team score, approximately \(30\%\) more score compared to the individualized strategy of \(306\). However, we find that trained policies under Ficticious Co-Play  exhibit similar team score to that of the individual coordination strategy and further, find that real human end-users collaborating with these agents are unable to far surpass the individual strategy score. As Strouse et al.  trains an agent to work well with a population of agents, where approximately a third of the diverse-skilled population of agents used in training are completely random agents, we posit that the teammate agent must compensate and exhibit individualized behavior, limiting the algorithm's ability to effectively learn effective team coordination strategies. In line with the first case study, the trained collaborative agent policies miss out on high-performance teaming behaviors, and thus, we need systems where humans can iteratively improve agent behavior online.

_Thus, in the rest of the paper, we look to explore xAI techniques as a mechanism for closing this gap and allowing agents within a human-machine team to facilitate collaborative strategies that outperform the individualized and rigid behaviors trained agents assume._

## 4 Methodology

In this section, we first present our architecture for training interpretable AI teammates. We then present a contextual pruning algorithm, allowing for ease-of-training and enhanced interpretability for neural tree-based models. We display an overview of our training procedure as part of Figure 2.

Figure 1: Case Study in Human-Machine Teaming with Different Teaming Strategies. It is clear that the models are not robust to multiple strategies of play and can result in agents performing nonsensical behavior (e.g., stuck in place).

### Interpretable Discrete Control Trees

We create an interpretable machine learning architecture, Interpretable Discrete Control Trees (IDCTs), that can be used directly with RL to produce interpretable teammate policies. Below, we briefly detail our architecture, as well as advancements to enhance ease-of-training and interpretability.

**Architecture** Our IDCTs are based on differentiable decision trees (DDTs)  - a neural network architecture that takes the topology of a decision tree (DT). DDTs contain decision nodes and leaf nodes; however, each decision node within the DDT utilizes a sigmoid activation function (i.e., a "soft" decision) instead of a Boolean decision (i.e., a "hard" decision). Each decision node, \(i\), is represented by a sigmoid function, displayed as \(y_{i}=(1+(-(_{i}^{T}-b_{i})))^{-1}\). As this representation is difficult to interpret, Paleja et al.  presented differentiable crispification, which recasts each decision node to split upon a single dimension of the input feature and translates the outcome of a decision node so that the outcome is a Boolean decision rather than a set of probabilities. This, in turn, allows for an interpretable forward propagation through the model that traces down a single branch of a tree as well as gradient flow afforded by the straight-through trick to update parameters of the neural tree model. We utilize this approach to learn interpretable tree-based teammate policies via reinforcement learning.

We initialize our IDCTs to be symmetric DTs with \(N_{l}\) decision leaves and \(N_{l}-1\) decision nodes. Each decision leaf is represented by a sparse categorical probability distribution over actions. At each timestep, a state variable is propagated through each decision node, split on a single decision rule, with the output being a Boolean causing the decision to proceed via the left or right branch until arrival at a leaf node. At each leaf node, we sample from the respective distribution to produce a macro-action (e.g., in Overcooked-AI, "get an onion" or "place ingredient on counter"). Further, we improve model predictability by applying an L1 norm loss over leaf node distributions to ensure sparsity, penalizing high entropy action distributions at a leaf1. _Importantly, the resultant representation after training is that of a simple decision tree with categorical probability distributions at each leaf node._

**Contextual Pruning** As we focus on creating agents that must cooperate with and be interpreted by humans, we must limit the size of our tree-based models to a certain depth to promote user understanding. Analogous to the "lottery ticket hypothesis" in network training that supports the practicality of employing large models , a small tree with a limited number of sub-trees (lottery tickets) may not have the representational power to learn a high-performing policy. Thus, the ability to effectively train IDCTs is at odds with maintaining user readability and simulatability. Following work in neural network pruning , we design a post-hoc _contextual pruning_ algorithm that allows us to simplify large IDCT models while precisely adhering to model behavior by accounting for:

1. **Boundaries of a variable's state distribution**: We utilize the minimum and maximum of each variable's range to parse impossible subspaces of a tree.
2. **Node hierarchy**: Ancestor nodes for a specific decision node may have already captured a specific splitting criterion and, thus, may lead to redundancy. By detecting redundancies, we can prune subspaces of the tree.

Figure 2: Here, we provide an overview of the steps to produce a collaborative AI teammate with an interpretable policy and the proposed policy modification scheme evaluated in our user study.

We provide further details and an algorithm for contextual pruning in the supplementary material. _This, in turn, allows us the benefit of training large tree-based models, greatly improving ease-of-training, while still being able to simplify the resultant model to a smaller, equivalent representation._

### Modifying an Interpretable Policy

While the above architecture can be used alongside RL to produce a collaborative AI policy, the result may not actually be helpful or what the human wants. _Humans, when teaming with machines, should be able to intuitively update what the robot has learned or change it based upon preferences that may evolve over time._ Such is critical in the positive development of coordination strategies and is associated with the calibration of trust, assignment of roles, and development of a shared mental model. As such, we propose a _policy modification scheme_ that allows the user to repeatedly team with an AI maintaining an IDCT policy, visualize the current behavior in tree form, and modify its AI's behavior.

The iterative process generated through this scheme can facilitate a feedback loop, allowing for the possibility of team development and improved HMT performance over teaming episodes.

We term our modification scheme _human-led policy modification_. We provide humans with an explicit pathway to "communicate" with an AI after each teaming interaction through a GUI, with capabilities displayed in Figure 3. Within this interface, users start with the pre-trained collaborative AI IDCT policy and can modify the AI's behavior by creating a new tree structure that may vary in what state features appear in the decision nodes, actions taken in leaf nodes, and the respective probabilities of actions within the leaf node. It is important to note that users are limited to expanding the tree to a depth of four (i.e., a max of 16 leaves), and the modification is not timed.

### Trained Collaborative Teammate Policies

Across our experiment, we study collaboration in two domains, Forced Coordination and Optional Collaboration, displayed on the left-hand side of Figure 4. In each domain, we train an IDCT policy via agent-agent collaborative training and a neural network (NN) policy following the population-based training scheme in Strouse et al. . In the first domain of Forced Coordination, the IDCT policy converged to a policy with an average reward of \(315.22 14.59\), and the neural network policy converged to an average reward of \(403.16 16.08\) evaluated over 50 teaming simulations with the synthetic human teammate the policy was trained with. In the second domain, Optional Collaboration, the IDCT policy converged to a policy with an average reward of \(171.46 18.89\), and the neural network policy converged to an average reward of \(295.02 1.86\). Thus, a consequent confound due to the current difference in performance capabilities between interpretable vs. black-box models is that the NN policy outperforms the IDCT policy in both domains. This displays a need for improving optimization algorithms for interpretable models representing collaborative agent policies. _However importantly, while the initial simulated performance of interpretable models may underperform black-box models, the ability for humans to understand machine behavior and improve upon behavior may allow these approaches to compete or even outperform black-box NN models_. We can also compare to the heuristic policies presented in Section 3, observing that the training performance of the IDCT and NN policies in the Optional Collaboration domain underperform the collaborative heuristic (408 vs. 295.02 and 171.46). We provide visualizations of the trained IDCT policies for

Figure 3: Users have several capabilities in creating an effective teammate, including modifying the tree structure by adding or removing decision nodes, changing state features the tree is conditioned on, and modifying actions and/or their respective probabilities at leaf nodes.

each domain in the appendix, finding that after contextual pruning, the AI IDCT policy has two and three leaves, respectively.

## 5 Human-Subjects Study

Here, we discuss our between-subjects user study that seeks to understand how users interact with an AI across repeated play under different factors. Below, we introduce our research questions, provide a description of the independent variables and procedure, and discuss our findings.

**Research Questions** The presented research questions below seek to understand changes in overall human-machine teaming performance and performance changes across repeated gameplay. The latter question pivots from an episodic attitude of teaming to a longer-term gauge, allowing us to study the process of adaptation in HMT.

1. **RQ1**: How does human-machine teaming performance vary across factors?
2. **RQ2**: How does team development vary across factors?

**Independent Variables** We have two independent variables, **IV1**: the teaming method, and **IV2:** the domain. For **IV1**, we consider the following conditions (abbreviated by **IV1-C**):

1. **IV1-C1: Human-Led Policy Modification:** After interacting with the agent (one teaming episode), the user can modify the policy via the GUI, allowing the user to update decision nodes and action nodes in the tree as well as tune affordances. Upon completion, the user can visualize the updated policy in its tree form prior to the next interaction.
2. **IV1-C2: AI-Led Policy Modification:** After interacting with the agent, the AI utilizes recent gameplay to fine-tune a human gameplay model via Behavioral Cloning and performs reinforcement learning for five minutes2 to optimize its own policy to better support the human teammate. Upon completion of policy optimization, the user can visualize the updated AI policy in its interpretable tree form prior to the next interaction. This is similar to HA-PPO , adapted to an online setting. 3. **IV1-C3: Static Policy - Interpretability:** After interacting with the agent, the user can visualize the AI's policy in its interpretable tree form prior to the next interaction. _Throughout this condition, the AI's policy is static._
4. **IV1-C4: Static Policy - Black-Box:** After interacting with the agent, the user does _not_ see the AI's policy. _Here, the AI policy is the same as **IV1-C3**, but the human has lost access to direct insight into the model._
5. **IV1-C5: Static Policy - Fictitious Co-Play:**: User teams with an AI maintaining a static black-box, neural network (NN) policy trained across a diverse partner set. As this is a baseline, we utilize an NN rather than the legible IDCT policy used in other conditions (**IV1:C1-4**).

For **IV2**, we consider the following domains displayed on the left-hand side of Figure 4:

1. **IV2-D1: Forced Coordination:** Users team with an AI that is separated by a barrier and must pass over items in a timely manner. Here, agents are forced to collaborate.
2. **IV2-D2: Optional Collaboration:** In this domain, the team can operate individually or collaboratively. This domain has increased complexity, both with respect to the size of the domain and the types of soups that can be cooked. _Collaboration is incentivized through a higher reward for mixed-ingredient dishes (combining onions and tomatoes) over single-ingredient dishes.

**Procedure:** A participant is first randomly placed into one of the five conditions in **IV1**. The participant starts with a pre-experiment survey collecting demographic information, experience with video games and decision trees, and the Big Five Personality Questionnaire . Afterward, a participant conducts a brief tutorial in Overcooked with a random AI agent, improving

   &  Exploent \\ Interaction \\  &  Policy Changes \\ Action Reactions \\  &  \\ White-Box \\  & 
 Base \\ Policy \\  \\ 
**IV1-C2** & ✗ & ✓ & ✓ & IDCT \\
**IV1-C3** & ✗ & ✓ & IDCT \\
**IV1-C4** & ✗ & ✗ & IDCT \\
**IV1-C5** & ✗ & ✗ & ✗ & IDCT \\
**IV1-C5** & ✗ & ✗ & ✗ & NN \\  

Table 1: A comparison across different **IV1** factors.

the user's understanding of game controls and the assigned task. Once completed, the primary experimentation begins. Users will team with an AI four times in each domain (randomly ordered), starting with the unique domain-specific pre-trained agent, and are told that their goal is to maximize their score in the last teaming interaction, the "performance round." After each teaming interaction, in the first three factors, the user will modify and visualize the AI's policy (**IV1-C1**), the AI will optimize its own policy proceeded by user visualization (**IV1-C2**), or the user will solely view the policy (**IV1-C3**). In **IV1-C4** and **IV1-C5**, as the AI is black-box (perceived to be black-box in **IV1-C4** and truly black-box in **IV1-C5**), transitionary pages are shown to the participant, providing them a pause before they team with the agent again. Upon completion of the condition-specific (or lack of) actions, users complete a NASA-TLX Workload Survey. After users have completed a domain, providing us with four episodes of teaming data and workload assessments, we administer several post-study scales, including the Human-Robot Collaborative Fluency Assessment , Inclusion of Other in the Self scale , and Godspeed Questionnaire . Upon completion of the two domains, the experiment concludes.

### Results

Our experiment is a 5 (teaming method; between-subjects) \(\) 2 (no. of domains; within-subjects) \(\) 4 (no. of repeated evaluations; within-subjects) mixed-factorial experiment. We recruited 50 participants under an IRB-approved protocol, whose ages range from 18 to 32 (Mean age: 24.14; Std. Dev.: 4.10; 46% Female, 52% Male, 2% Non-Binary), with participants randomly assigned to each of the factor levels, with ten total subjects per level. The duration of the experiment was \(70.98 19.71\) minutes 3. Our data was modeled as a full-factorial, between-subjects ANOVA. We test for normality and homoschedasticity (see appendix) and employed a corresponding non-parametric test if the data failed to meet these assumptions. We display our objective findings in the right-hand side of Figure 4.

**RQ1: Team Performance:** In analyzing reward, we find trends with respect to the maximum reward participants obtained within a domain across iterations (Figure 5). Using Friedman's test, we find a significant difference across domains (\(^{2}\)(1)=46.08, \(p<0.001\)) and analyze the domains separately.

Figure 4: User gameplay scores across teaming iterations with per-iteration means connected by the red dotted line and the per-iteration standard deviation shaded in red.

In **IV2-D1**, a Kruskal-Wallis Test was conducted to analyze differences in maximum performance obtained across teaming paradigms, finding a significant effect (\(^{2}(4)=20.146,p<0.001\)). We conduct post-hoc pairwise comparisons, utilizing Dunn's test, and find that **IV1-C5** (Fictitious Co-Play) is significantly better than **IV1-C1** (\(p<0.001\)), **IV1-C2** (\(p<0.01\)), **IV1-C3** (\(p<0.01\)), and **IV1-C4** (\(p<0.05\)). Even though fictitious Co-Play (**IV1-C5**) outperformed the tree-based models, likely due to its ability to converge to a higher-performance teaming policy, it is interesting that Human-Led Policy Modification (**IV1-C1**) has several participants that outperform the maximum performance of **IV1-C5** in teaming iterations three and four (Figure 3(a)).

In **IV2-D2**, a Kruskal-Wallis Test was conducted to analyze differences in participant teaming performance across conditions, finding a significant effect (\(^{2}(4)=29.922,p<0.001\)). We conduct post-hoc pairwise comparisons, utilizing Dunn's test, and find that **IV1-C5** (Ficticious Co-Play) is significantly better than **IV1-C2** (\(p<0.001\)), **IV1-C3** (\(p<0.001\)), and **IV1-C4** (\(p<0.001\)), and **IV1-C1** (Human-Led Policy Medication) is significantly better than **IV1-C2** (\(p<0.05\)), **IV1-C3** (\(p<0.05\)), and **IV1-C4** (\(p<0.05\)). For white-box AI teammates (**IV1:C1-3**), the latter finding displays the benefit of Human-Led Policy Modification in improving HMT performance for interpretable models. These findings display that 1) white-box approaches supported with policy modification can outperform white-box approaches alone, 2) black-box models can outperform white-box approaches in HMT, and 3) by comparing **IV1-C3** to **IV1-C4**, interpretability alone afforded via tree visualizations did not provide any direct objective benefits. Finally, in Optional Collaboration, across all conditions we see that HMT scores are not near that of the collaborative heuristic, displaying a gap that must be addressed to achieve effective HMT.

**RQ2: Team Development:** In analyzing RQ2, we look at the change in reward across iterations one to four and relate our findings to Tuckman's model. Utilizing a Friedman's test, we find a difference across domains (\(^{2}(1)\)=20.48, p\(<\)0.001) and analyze the domains separately. In **IV2-D1**, we see that none of the conditions results in a significant improvement in teaming performance over repeated iterations. In **IV2-D2**, we see **IV1-C1** (\(p<0.01\)) and **IV1-C2** (\(p<0.01\)) significantly improve over repeated teaming interactions. The improving interactions can be connected to the Norming stage in team development, where teams begin to develop a strategy and team mental models. _We see conditions that facilitate Norming have the attribute of policy adaptation and are white-box._

Next, we analyze whether different person-specific factors allow HMT to improve more quickly than others. In **IV2-D1**, we find that conscientiousness is trending in its correlation with improvement (\(0.05<p<0.1\)). In **IV2-D2**, we find that participants with high familiarity with Trees improve more across iterations (\(F(1)=7.448,p<0.01\)). These findings signify that positive interaction with interpretable models may be more beneficial to those with an engineering background and specific personality traits.

Finally, we detect an interesting trend in **IV2-D1** under the **IV1-C1** condition. We see a drop in performance between the first teaming iteration and later iterations, followed by a rise. We believe this relates to the Forming and Storming stages, where team members are still developing effective strategies to coordinate. As the last iteration shows an improvement in performance, we hypothesize that the team was shifting into the Norming stage. In future, it would be interesting to evaluate a larger number of iterations to see if the behavior would continue to uptrend. This requires further research due to the additional resources and time needed for more teaming iterations.

Figure 5: Maximum Reward and Subjective Ratings Across **IV1** Factors.

**Subjective Findings:** In **IV2-D1**, we find that users did not find any subjective differences toward the teaming interaction across conditions. In **IV2-D2** (Figure 5), we find that users find collaboration with AIs under condition **IV1-C2** and **IV1-C4**, on average as less fluent than **IV1-C1** (p\(<\)0.01, p\(<\)0.01), and **IV1-C4** as less fluent than **IV1-C5** (\(p<0.05\)). Users also trusted the AI and perceived the AI contributed more in **IV1-C5** than in **IV1-C2** (p\(<\)0.05, p\(<\)0.05) and **IV1-C4** (\(p<0.05,p<0.05\)). Furthermore, the users viewed the AI more positively in **IV1-C1** and **IV1-C5** than in both **IV1-C2** (p\(<\)0.05, p\(<\)0.05) and **IV1-C4** (p\(<\)0.05, p\(<\)0.01). Overall, participants generally assessed higher-performing agents more positively in their subjective ratings. In considering conditions that utilized a tree-based model (**IV1-C1**, **IV1-C2**, **IV1-C3**, and **IV1-C4**), we see the addition of interaction with the tree policy provides significant subjective benefits in positive teaming traits and collaborative fluency (defined within the Human-Robot Collaborative Fluency Assessment ). In including the remaining condition, which utilizes a black-box model, **IV1-C5:****Fictitious Co-Play**, and comparing it to **IV1-C1:****Human-Led Policy Modification**, we see that even though Fictitious Co-Play outperformed Human-Led Policy Modification in terms of team reward (though not significantly in the domain of Optional Collaboration), no significant subjective differences were observed between these two conditions. This presents an interesting relationship between transparency, interaction, and performance in relation to subjective perception that warrants future research.

**Design Guidelines:** To achieve fluent HMT, we specify the following forward-facing guidelines.

1. _The creation of white-box learning approaches that can produce interpretable collaborative agents that achieve competitive initial performance to that of black-box agents._ This guideline is critical to providing humans with the subjective benefits obtained from interactivity with white-box models, objective benefits of black-box models, and the ability to interact with policies to facilitate team development.
2. _The design of learning schemes to support the generation of collaborative AI behaviors rather than individual coordination._ We need techniques that avoid converging to the local maxima of individual coordination and scenarios that allow for properly evaluating cooperation.
3. _The creation of mixed-initiative interfaces that enable users, who may vary in ability and experience, to improve team collaboration across and within interactions._ As we found a large diversity in perceived usability of our interface (finding an average score of \(58.25 27.61\), with some users finding the interface good (\(>\)75) and others poor (\(<\)35)), effective interfaces are vital in shifting from only a subset of users benefiting to all users being able to create effective teammates.
4. _The evaluation of teaming in a larger number of interactions._ As agents are deployed, team performance will change over time, going through a transient period before reaching peak performance. Understanding this process of team development is essential in creating high-performance HMT.

## 6 Conclusion

This work investigates repeated interactions with machine learning models within a sequential decision-making HMT paradigm. We present a key gap in HMT, displaying that current methods do not facilitate human-machine collaboration to the fullest. We find that human-led policy modification allows for a team to achieve higher performance than white-box models without this capability. However, as interpretable models are more difficult to generate, Fictitious Co-Play is able to better support high performance. Given these mixed findings, future work must focus on developing better white-box teammates, study the modality of communication in HMT, and explore mechanisms to allow HMT to scale beyond individual coordination and toward effective collaboration.