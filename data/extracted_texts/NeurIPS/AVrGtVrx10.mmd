# Probabilistic Conformal Distillation for Enhancing Missing Modality Robustness

Mengxi Chen\({}^{1,3}\)  Fei Zhang\({}^{1}\)  Zihua Zhao\({}^{1}\)  Jiangchao Yao\({}^{1,3}\)  Ya Zhang\({}^{2,3}\), Yanfeng Wang\({}^{2,3}\)

\({}^{1}\) Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{2}\) School of Artificial Intelligence, Shanghai Jiao Tong University

\({}^{3}\) Shanghai Artificial Intelligence Laboratory

{mxchen_mc, ferenas, sjtuszzh, Sunarker, ya_zhang, wangyanfeng}@sjtu.edu.cn

###### Abstract

Multimodal models trained on modality-complete data are plagued with severe performance degradation when encountering modality-missing data. Prevalent cross-modal knowledge distillation-based methods precisely align the representation of modality-missing data and that of its modality-complete counterpart to enhance robustness. However, due to the irreparable information asymmetry, this determinate alignment is too stringent, easily inducing modality-missing features to capture spurious factors erroneously. In this paper, a novel multimodal Probabilistic Conformal Distillation (PCD) method is proposed, which considers the inherent indeterminacy in this alignment. Given a modality-missing input, our goal is to learn the unknown Probability Density Function (PDF) of the mapped variables in the modality-complete space, rather than relying on the brute-force point alignment. Specifically, PCD models the modality-missing feature as a probabilistic distribution, enabling it to satisfy two characteristics of the PDF. One is the extremes of probabilities of modality-complete feature points on the PDF, and the other is the geometric consistency between the modeled distributions and the peak points of different PDFs. Extensive experiments on a range of benchmark datasets demonstrate the superiority of PCD over state-of-the-art methods. Code is available at: https://github.com/mxchen-mc/PCD.

## 1 Introduction

Classical multimodal learning [29; 20; 36; 3] typically pre-supposes that the modalities of all data are complete throughout both the training and testing. However, due to collection constraints such as device limitations, budget constraints, and restrained working conditions, it is challenging to guarantee such a perfect condition . When modalities are partially available, the performance of models trained on modality-complete data will deteriorate remarkably. This thereby attracts a range of explorations contributed recently, given that multimodal learning is playing an increasing role.

The existing approaches to address this problem generally fall into two paradigms, i.e., independent modeling [11; 39; 7] and unified modeling [9; 13; 46] for different modality-missing combinations, of which the latter is preferred due to the merits of low-storage cost and flexibility. As one prevalent line of unified modeling, cross-modal knowledge distillation (KD) has achieved persistent advancements in recent years [40; 51; 47; 46]. It attempts to guide the modality-missing representation to align with its modality-complete counterpart, facilitating the training under the guidance of privileged modality-complete information. However, these methods fail to consider that once a modality is missing, it is impossible to recover its personalized information via a brute-force alignment, whichhas been revealed theoretically by . Roughly ignoring this inherent information asymmetry in the alignment can instead lead multimodal models to fit spurious factors erroneously.

We conjecture that when partial modalities are missing, the retaining information is merely correlated to that of modality-complete input in a probabilistic sense. Specifically, given a modality-missing input, the unknown Probability Density Function (PDF) of its mapped variables in the modality-complete space peaks at the corresponding modality-complete feature and diminishes when diverging away from this point, as illustrated in Figure 1 (b). Compared to previous deterministic methods, learning the PDF is a more reasonable and tolerant way to transfer privileged information. Although the closed form of the oracle PDF is unknown, we can approximate it by modeling a probabilistic distribution with two key characteristics: (1) In a modeled distribution, the positive points closer to the modality-complete representation should demonstrate high probabilities and the negative points farther away should exhibit low probabilities. (2) For different distributions from distinct samples, the relation of their peak points should be conformal with that of their modality-complete representations. Here, the former focuses on extreme probability points, while the latter ensures geometric consistency.

With the above intuition, we propose a novel multimodal Probabilistic Conformal Distillation (PCD) method, which aims to align the modality-missing feature with its modality-complete counterpart probabilistically. Specifically, PCD parameterizes each modality-missing representation as an independent probabilistic distribution and optimizes it to satisfy the two characteristics. To achieve (1), the log probabilities of the distribution are maximized at positive points and minimized at negative points. To achieve (2), PCD introduces a contrastive-learning-based approach to align the geometric structure of the peak points of distributions with that of the modality-complete features. In this way, the modeled modality-missing distributions can approximate their corresponding PDFs, thereby facilitating the privileged modality-complete information transfer more efficiently.

In a nutshell, our contributions can be summarized as follows:

* We propose a multimodal Probabilistic Conformal Distillation method to handle the missing modality problem, which transfers privileged information of modality-complete representation by considering the indeterminacy in the mapping from incompleteness to completeness.
* We parameterize different modality-missing representations as distinct distributions to fit their unknown PDFs in the modality-complete space. This is specially realized by considering the probabilities of extreme points and ensuring the geometric consistency between peak points of different PDFs and modeled distributions.
* We conduct comprehensive experiments to demonstrate the effectiveness of PCD across a range of modality-missing scenarios. Extensive comparison on multimodal classification and segmentation tasks consistently validate the superior performance of our method compared to the state-of-the-art approaches. Particularly, PCD achieves an average improvement of about 5% for the seven modality-missing scenarios on the classification dataset CeFA.

## 2 Related Work

We roughly categorize recent explorations to improve the missing modality robustness into two paradigms: independent modeling methods and unified modeling methods.

### Independent Modeling for Missing Modality

Many works address the modality-missing problem by training specific models for different modality-missing combinations [41; 10; 31; 26]. In a certain modality-missing case, some approaches recon

Figure 1: In a two-modality scenario, when both modalities are present, the modality-complete representation is derived through fusion. When one modality is absent, the mapped representation inferred from the remaining modality is subject to a certain probability distribution in the modality-complete space.

struct the original data of the missing modalities from the available ones [2; 22; 28; 28]. However, the complexity of the data reconstruction usually leads to instability and may introduce noise to affect the main task [30; 52]. To alleviate this problem, many works try to reconstruct missing modalities at the representation level [11; 39; 7; 5]. Nevertheless, training specific models for each missing case tend to be inflexible and storage-consuming for real-world scenarios.

### Unified Modeling for Missing Modality

Recently, there has been a growing interest in improving the robustness of unified multimodal models against a range of modality-missing combinations [56; 33; 21; 19]. To achieve this goal, some methods attempt to extract redundant information across modalities by designing different _fusion_ networks [15; 53; 9; 50]. However, these methods ignore the complementary information, resulting in suboptimal performance to the specific models. Other methods capture the comprehensive information through dynamical fusion strategies [13; 14; 12; 6]. To be specific, these methods utilize uncertainty estimation techniques to learn the dynamical strength relationships among modalities within different samples, allowing for the adaptive assignment of weights to each available modality. To harness both redundant and complementary information of available modalities more effectively, some methods [32; 51; 47; 46] introduce a _distillation_ loss to guide the unified model to imitate representations or inter-sample relations of the modality-complete model. This distillation process help the unified model acquire additional privileged information from complete modalities, so as to improve multimodal robustness [44; 43; 45; 42]. However, previous KD-based methods often emphasize precisely aligning the modality-missing representation with its complete counterpart, which probably causes the overfitting on spurious features due to the inherent information asymmetry.

## 3 Method

### Preliminary

**Notations.** Suppose that we have a modality-complete training set of \(\{(_{i}^{},y_{i})\}_{i=1}^{N}\), where each input \(_{i}^{}\) comprises \(M\) modalities, denoted as \(_{i}^{}=\{x_{i}^{m}\}_{m=1}^{M}\), and \(y_{i}\) represents the corresponding ground-truth label. \(N\) is the dataset size. Our goal is to train a unified model capable of accurately predicting the label \(y_{i}\) for any modality-missing case \(_{i}_{i}^{}_{i}\). Here, we use an auxiliary indicator vector \(_{i}\) for \(_{i}\), where \( m,\ _{i}^{m}\{0,\ 1\}\) indicates the modality in \(_{i}\) missing or not. During testing, we construct different modality-missing cases to comprehensively evaluate the robustness.

**Motivation.** Owing to the inherent information asymmetry, modality-complete and modality-missing representations cannot be perfectly aligned, even with redundant information. This claim is experientially supported by the results in Appendix D. Therefore, we try to align the representation of modality-missing input \(_{i}\) with that of modality-complete input \(_{i}^{}\) in a probabilistic sense. As shown in the right panel of Figure 1, we conjecture that the representation \(z_{i}\) of modality-missing input \(_{i}\) has a probabilistic peak expectation towards the representation \(z_{i}^{}\) of the modality-complete input \(_{i}^{}\). In other words, the corresponding PDF \(p(z_{i}|_{i})\) satisfies the following requirement

\[z_{i}^{}=_{z_{i} Z}\ p(z_{i}|_{i}),\] (1)

where \(Z\) denotes the representation space. Generally, approximating the unknown PDF \(p(z_{i}|_{i})\) is a more relaxed condition compared with the stringent point alignment in previous KD-based methods.

### Probabilistic Conformal Distillation

#### 3.2.1 Objective

Although \(p(z_{i}|_{i})\) is unknown, even about the function family of the distribution, we can define an easier distribution \(q(z_{i}|_{i})\) to approximate its characteristics. Specifically, we can force \(q(z_{i}|_{i})\) to follow the two-fold characteristics: 1) _extremum property._ In a modeled distribution \(q(z_{i}|_{i})\), positive points near the modality-complete representation \(z_{i}^{}\) should exhibit higher probabilities, and negative points distant from \(z_{i}^{}\) approach far smaller probabilities. (2) _conformal property._ Given different samples, the relationship of the peak points of \(q(z|)\) should be conformal with that of their corresponding modality-complete points \(z^{}\).

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_EMPTY:6]

RGB-D dataset designed for urban scene comprehension. There are 5,000 annotated samples, where 2,975 samples are for training, 500 for validation, and 1,525 for testing.

Experimental Details.For classification CASIA-SURF and CeFA, the SGD optimizer  is used and the batch size is 64. The dimension of the Gaussian distribution is 512. We report the results using the metric of Average Classification Error Rate (ACER). Each modality leverages a separate ResNet-18  as the unimodal encoder. We employ an exponential decay learning rate strategy in which the learning rate is fixed at 1e-3 during the warm-up stage and then decays exponentially. Weight decay and momentum are set to 0.0005 and 0.9, respectively. For segmentation experiments on NYUv2 and Cityscapes, we use the Adam optimizer  and set the batch size to 16. The results are evaluated by the metric of mean IOU (mIOU). The learning rate is initialized with 1e-2 and 1e-4 respectively for two datasets and adapted by the one-cycle scheduler. Following , we use ESANet  as the backbone. On all datasets, the variances are obtained through a two-layer MLP, where the hidden size is 1024. During training, we augment each modality-complete data by simulating all potential modality-missing scenarios and randomly sample one of the augmented data as the training sample for the current epoch. For bimodal datasets, three cases are included, that is, missing RGB, missing depth, and complete. For trimodal datasets, there are seven missing cases.

    &  \\ Method & \{R\} & \{D\} & \{I\} & \{R,D\} & \{R,I\} & \{D,I\} & \{R,D,I\} & Average \\  Traditional  & 23.03 & 17.10 & 49.53 & 10.40 & 41.02 & 11.26 & 1.40 & 22.11 \\  Separate Model  & 10.01 & 4.45 & 11.65 & 3.41 & 6.32 & 3.54 & 1.23 & 5.80 \\  Augmentation  & 11.75 & 5.87 & 16.62 & 4.61 & 6.68 & 4.95 & 2.21 & 7.52 \\ HeMIS  & 14.36 & 4.70 & 16.21 & 3.23 & 6.27 & 3.68 & 1.97 & 7.18 \\ MMFormer  & 11.15 & 4.67 & 13.99 & 1.93 & 4.77 & 3.10 & 1.94 & 5.93 \\ MMANET  & 8.57 & 2.27 & 10.04 & 1.61 & 3.01 & 1.18 & 0.87 & 3.94 \\ MD  & 10.84 & 6.65 & 19.43 & 12.64 & 7.84 & 3.99 & 0.96 & 7.30 \\ ETMC  & 7.91 & 4.73 & 7.54 & 1.39 & 4.56 & 1.46 & 0.76 & 4.05 \\ RAML  & 11.26 & 3.10 & 11.65 & 1.92 & 5.35 & 1.76 & 1.09 & 5.16 \\  PCD & **7.23** & **2.20** & **5.66** & **0.99** & **2.86** & **0.89** & **0.74** & **2.93** \\ \(\) & 0.74\% \(\) & 0.07\% \(\) & 1.88\% \(\) & 0.40\% \(\) & 0.15\% \(\) & 0.29\% \(\) & 0.02\% \(\) & 1.01\% \(\) \\    &  \\ Method & \{R\} & \{D\} & \{I\} & \{R,D\} & \{R,I\} & \{D,I\} & \{R,D,I\} & Average \\  Traditional  & 50.00 & 50.00 & 49.96 & 49.25 & 47.28 & 48.95 & 39.62 & 47.86 \\  Separate Model  & 27.44 & 33.75 & 36.17 & 35.62 & 31.62 & 36.62 & 24.15 & 32.20 \\  Augmentation  & 27.93 & 36.90 & 36.14 & 32.10 & 28.47 & 35.12 & 31.87 & 32.65 \\ HeMIS  & 34.14 & 37.97 & 36.94 & 36.02 & 33.94 & 31.92 & 40.66 & 35.94 \\ MMFormer  & 28.51 & 33.58 & 39.56 & 29.47 & 27.66 & 32.17 & 30.72 & 31.52 \\ MMANET  & 27.15 & 32.50 & 35.62 & 22.87 & 23.27 & 30.45 & 23.68 & 27.94 \\ MD  & 27.13 & 35.81 & 37.99 & 26.25 & 31.29 & 34.69 & 30.49 & 31.95 \\ ETMC  & 24.74 & 34.28 & 37.62 & 22.52 & 24.25 & 30.63 & 21.59 & 27.95 \\ RAML  & 28.54 & 33.88 & 40.01 & 23.82 & 28.81 & 28.85 & 22.11 & 29.43 \\  PCD & **21.38** & **28.01** & **34.79** & **17.19** & **20.92** & **21.68** & **14.39** & **22.63** \\ \(\) & 3.36\% \(\) & 4.49\% \(\) & 0.83\% \(\) & 5.33\% \(\) & 2.35\% \(\) & 5.75\% \(\) & 7.20\% \(\) & 5.31\% \(\) \\    &  &  \\ Method & \{R\} & \{D\} & \{R,D\} & Average & \{R\} & \{D\} & \{R,D\} & Average \\  Traditional  & 11.15 & 4.18 & 48.78 & 21.41 & 3.17 & 4.87 & 78.73 & 28.89 \\  Separate Model  & 44.22 & 40.55 & 48.89 & 44.55 & 77.60 & 59.11 & 78.62 & 7

### Performance Comparison

To evaluate the robustness of PCD, we choose the following methods in the comparison: 1) Baselines. Traditional [49; 36]: a benchmark method trained solely on modality-complete data. Separate Model [49; 36]: separate intermediate-fusion models for each modality combination. 2) Redundancy-based methods: Augmentation , MMFormer . 3) Cross-modal KD-based methods: MMIN , MMANET . 4) Dynamical fusion-based methods: MD , ETMC , RAML .

**Classification Task.** The results in Table 1 show the performance of PCD and other state-of-the-art (SOTA) methods across various testing conditions with missing modalities on two classification datasets CASIA-SURF and CeFA. We can see that the 'Traditional' method, which is exclusively trained on modality-complete samples, exhibits a high sensitivity to the missing modality problem. Specifically, the error rate surges by 21.63% on CASIA-SURF when only the RGB modality is available. Comparing the results of various missing modality methods, PCD achieves the best results in almost all the settings on the two multimodal classification datasets. In comparison to the second-best method, PCD demonstrates the error rate reductions of 1.01% and 5.31% on CASIA-SURF and CeFA. These results illustrate the effectiveness of our proposed method in privileged information transfer. Besides, the performance of some methods declines with an increasing number of modalities. For example, on CeFA, the error rate of MMANET with complete modalities is 0.81% higher than when IR is absent. This deterioration may potentially caused by overfitting resulting from deterministic alignment. In contrast, our method employs a probabilistic distillation, which introduces a more relaxed framework for aligning representations, mitigating this issue effectively.

**Segmentation Task.** We conducte experiments on NYUv2 and Cityscapes to verify the effectiveness of PCD on segmentation tasks. Compared to the second-best method, PCD achieves average accuracy improvements of 0.91% and 0.83% on NYUv2 and Cityscapes, respectively. Furthermore, in the Depth-missing scenarios on the NYUv2 and Cityscapes datasets, PCD demonstrates relatively small improvements. This may be because that the performance of the input RGB is already very close to that of the modality-complete input. Consequently, it is challenging to obtain additional privileged information through distillation, limiting the potential enhancement.

### Further Analysis

**Ablation on Loss Components.** In this part, we investigate the impact of each loss component in Eq. (10) on CeFA. In Table 2, we conduct the ablation study and summarize the corresponding performance with or without different loss components. According to the results in Table 2, we can observe that the classification model with the probability extremum loss \(_{u}\) performs 3.36% better than the simple model with only \(_{c}\), which suggests that constraining probabilities of extreme points indeed helps to the privileged information transfer from the modality-complete teacher to the modality-missing student. Additionally, PCD with all loss components outperforms the model with \(_{c}\) and \(_{u}\) on average, which validates the effectiveness of the geometric consistency loss.

**Ablation on Probabilistic Distillation.** To study the effect of probabilistic distillation, we conduct experiments to compare the performance of PCD with its determinate distillation variant. Here, the variant is the degradation method of PCD that transfers knowledge by directly minimizing the Euclidean distance of the complete-incomplete pairs in teacher and student networks. The results are shown in Table 3. It can be seen that PCD consistently

   \(_{c}\) & \(_{u}\) & \(_{g}\) & \{R\} & \{D\} & \{I\} & \{R,D\} & \{R,I\} & \{D,I\} & \{R,D,I\} & Average \\  ✓ & \(\) & \(\) & 26.95 & 38.06 & 37.06 & 24.18 & 24.75 & 32.82 & 25.38 & 29.89 \\ ✓ & \(\) & \(\) & 21.14 & 33.76 & 37.22 & 21.28 & 23.61 & 27.56 & 21.19 & 26.53 \\ ✓ & \(\) & ✓ & **20.62** & 34.43 & 35.23 & 18.18 & 21.86 & 32.63 & 21.72 & 26.38 \\  ✓ & ✓ & ✓ & 21.38 & **28.01** & **34.79** & **17.19** & **20.92** & **21.68** & **14.39** & **22.63** \\   

Table 2: Ablation study on CeFA. \(\) and ✓ in the table indicate without and with the corresponding loss term respectively.

   Configurations & \{R\} & \{D\} & \{I\} & \{R,D\} & \{R,I\} & \{D,I\} & \{R,D,I\} & Average \\  Determinate & 23.52 & 38.96 & 38.95 & 25.75 & 24.52 & 36.1 & 28.21 & 30.99 \\ Pretrained & 23.52 & 31.64 & 39.86 & 22.57 & 24.89 & 29.43 & 26.50 & 28.34 \\  PCD & **21.38** & **28.01** & **34.79** & **17.19** & **20.92** & **21.68** & **14.39** & **22.63** \\   

Table 3: The comparison between PCD and its variants on CeFA, where ”Determinate” means the degradation of PCD with determinate distillation, while ”Pretrained” is the distillation with a pretrained teacher.

outperforms its "Determinate" variant in all missing modality combinations and decreases the error rate by 8.36% on average. This demonstrates the effectiveness of transferring privileged information via probabilistic distillation, which is more tolerant.

**Analysis about KD Strategy.** To explore the effectiveness of self-KD, we compare PCD with its pretrained teacher variant. This variant refers to training a modality-complete teacher individually to guide students in optimizing from scratch. The results are shown in Table 3. As can be seen, the error rate of PCD is 5.71% lower on average than its pretrained variant. In addition to training a fixed teacher to offer modality-complete supervision, our self-KD strategy also provides a good initialization for the student. With the help of the shared predictor, the semantic coherence of modality-complete and modality-missing representations is indirectly ensured, which narrows information gap between them at the beginning of KD, thereby facilitating privileged information transfer.

**Classification Boundary of the Teacher and Student.** In order to further validate the effectiveness of probabilistic distillation for the transfer of privileged complete-modality information, we analyze the predictions of both the fixed teacher obtained from the warm-up stage and the distilled student under all multimodal conditions. The results are shown in Figure 3. It can be observed that, apart from the reduced error rate, the logits of the student exhibit a higher concentration around 0 or 1, demonstrating a more separable inter-class boundary. The probabilistic distillation process transfers privileged information to hard samples around the classification boundary in a more tolerant way, mitigating the erroneous fit to spurious factors, so as to further refine modality-missing features.

**Hyperparameter \(\).** The hyperparameter \(\) controls the balance between distillation and classification. To validate the stability of PCD against \(\), we conducted several experiments with different values of \(\) on CeFA. The results are shown in the left half of Figure 4, where values of \(\) range from 1.4 to 2.4. From the curve, we can see that setting a relatively large value for \(\) enhances the distillation of privileged information, thereby enhancing the multimodal robustness. Specifically, in [1.4, 2.2], \(\) appears to be insensitive within a certain range, In our experiments, we set \(=1.8\).

**Hyperparameter \(\).** In the right panel of Figure 4, we conducted several experiments with different values of \(\) to assess its impact on our results. The hyperparameters \(\) is the temperature in Equation (8), which scales the similarity measures. The results reveal \(\) is insensitive within a certain range. In our experiments, we set \(=0.5\).

**Computational Overhead.** Compared to the multimodal models with the same backbone, PCD only introduces _a few additional head modules_ in the encoder to estimate the variance. To demonstrate the minor change PCD brings, we estimated the number of parameters and FLOPs of PCD and the other three late fusion methods in Table 4. It can be seen that PCD does not significantly increase the number of parameters or FLOPs, where the FLOPs are almost equal to

   Method & Backbone & Paramters & FLOPs \\  MD  & ResNet-18 & 35.88 & 1.392 \\ ETMC  & ResNet-18 & 34.30 & 1.391 \\ RAML  & ResNet-18 & 35.09 & 1.393 \\ PCD & ResNet-18 & 38.50 & 1.395 \\   

Table 4: The numbers of parameters (M) and FLOPs (G) of several methods on CeFA.

Figure 4: The average performance of PCD under different \(\) and \(\) values on CeFA. The hyperparameter \(\) is used to balance the loss terms, \(\) is the temperature.

Figure 3: The prediction distributions of both the teacher and the distilled student of PCD under all multimodal combinations on CeFA. The X-axis represents the normalized logit output and the Y-axis is the number of samples after taking the square root.

the second-best method MMANET, while the number of parameters only increased by 4.20M. This lightweight change of MPCD makes it easily be applied to many existing multimodal fusion methods.

Modality-Missing Training Data.All the experiments above are conducted with the modality-complete training data. In this part, we extend PCD by considering the scenario where the modality-complete data of some training samples is also unavailable. PCD is only applied to the data that has modality-complete counterpart, and for the remaining data, only \(_{t}\) is optimized. Here, we introduce a case where 30% of the data is consistently missing from each modality during training. As shown in Table 5, while some modality-missing cases may underperform compared to the SOTA, PCD still outperforms the second-best method by 5.20% on average. Although PCD is not specifically designed for modality-missing training data, these results demonstrate its scalability for such scenarios.

## 5 Conclusion

In this paper, we propose a multimodal Probabilistic Distillation (PCD) method to mitigate the missing modality problem, which considers the indeterminacy in the alignment between the modality-complete and modality-missing representations. Specifically, PCD aims to parameterize the modality-missing representations as different Gaussian distributions and fit PDFs of their mapped variables in the modality-complete space. This is achieved by ensuring the characteristics of probabilities at extreme points and maintaining geometric consistency with that of the modality-complete features. Extensive experiments validate the superiority of PCD in increasing multimodal robustness.