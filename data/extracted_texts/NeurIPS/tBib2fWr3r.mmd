# Understanding Deep Gradient Leakage via Inversion Influence Functions

Haobo Zhang

Michigan State University

zhan2060@msu.edu

&Junyuan Hong

Michigan State University

University of Texas at Austin

jyhong@utexas.edu

&Yuyang Deng

Pennsylvania State University

yzd82@psu.edu

&Mehrdad Mahdavi

Pennsylvania State University

mahdavi@cse.psu.edu

&Jiayu Zhou

Michigan State University

jiayuz@msu.edu

Equal contribution.

###### Abstract

Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of _when and how privacy leakage happens_, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I\({}^{2}\)F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I\({}^{2}\)F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I\({}^{2}\)F effectively approximated the DGL generally on different model architectures, datasets, modalities, attack implementations, and perturbation-based defenses. With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization. Our codes are provided in https://github.com/illidanlab/inversion-influence-function.

## 1 Introduction

With the growing demands for more data and distributed computation resources, distributed learning gains its popularity in training large models from massive and distributed data sources (McMahan et al., 2017; Gemulla et al., 2011). While most distributed learning requires participating clients to share only gradients or equivalents and provides data confidentiality, adversaries can exploit such information to infer training data. Deep Gradient Leakage (DGL) showed such a real-world risk in distributed learning of deep networks (Zhu et al., 2019; Jin et al., 2021). Only given a private gradient and the corresponding model parameters, DGL reverse-engineers the gradient and recovers the private images with high quality. The astonishing success of the DGL started an intensive race between privacy defenders (Sun et al., 2020; Wei et al., 2021; Gao et al., 2021; Scheliga et al., 2022) and attackers (Geiping et al., 2020; Zhao et al., 2020; Jin et al., 2021; Jeon et al., 2021; Zhu and Blaschko, 2021; Fowl et al., 2022).

A crucial question to be answered behind the adversarial game is _when and how DGL happens_, understanding of which is fundamental for designing attacks and defenses. Recent advances intheoretical analysis shed light on the question: Balunovic et al. (2022) unified different attacks into a Bayesian framework, leading to Bayesian optimal adversaries, showing that finding the optimal solution for the DGL problem might be non-trivial and heavily relies on prior knowledge. Pan et al. (2022); Wang et al. (2023) advanced the understanding of the leakage in a specific class of network architectures, for example, full-connected networks. However, it remains unclear how the privacy leakage happens for a broader range of models, for example, the deep convolutional networks, and there is a pressing need for a _model-agnostic_ analysis.

In this paper, we answer this question by tracing the recovered sample by DGL algorithms back to the private gradient, where the privacy leakage ultimately derives from. To formalize the optimal DGL adversary without practical difficulties in optimization, we ask the counterpart question: _what would happen if we slightly change the private gradient?_

Answering this question by perturbing the gradient and re-evaluating the attack can be prohibitive due to the difficulty of converging to the optimal attack in a highly non-convex space. Accordingly, we propose a novel Inversion Influence Function (I\({}^{2}\)F) that provides an analytical description of the connection between gradient inversion and perturbation. Compared to directly solving DGL, the analytical solution in I\({}^{2}\)F efficiently scales up for deep networks and requires only oracle access to gradients and Jacobian-vector products. We note that I\({}^{2}\)F shares the same spirit of the influence function (Koh and Liang, 2017; Hampel, 1974) that describes how optimal model parameters would change upon the perturbation of a sample and I\({}^{2}\)F can be considered as an extension of the influence function from model training to gradient inversion.

The proposed I\({}^{2}\)F characterizes leakage through the lens of private gradients and provides a powerful tool and new perspectives to inspect the privacy leakage: (1) First, we find that gradient perturbation is not homogeneous in protecting the privacy and is more effective if it aligns with the Jacobian singular vector with smaller singular values. (2) As the Jacobian hinges on the samples, the variety of their Jacobian structures may result in unfair privacy protection under homogeneous Gaussian noise. (3) We also examine how the initialization of model parameters reshapes the Jocabian and therefore leads to quite distinct privacy risks.

These new insights provide useful tips on how to defend DGL, such as perturbing gradient in specific directions rather than homogeneously, watching the unfairness of protection, and carefully initializing models. We envision such insights could lead to the development of fine-grained privacy protection mechanisms. Overall, our contributions can be summarized as follows. (1) For the first time, we introduce the influence function for analyzing DGL; (2) We show both theoretical and empirical evidence of the effectiveness of the proposed I\({}^{2}\)F which efficiently approximates the DGL in different settings; (3) The tool brings in multiple new insights into when and how privacy leakage happens and provides a tool to gauge improved design of attack and defense in the future.

## 2 Related Work

_Attack._ Deep Gradient Leakage (DGL) is the first practical privacy attack on deep networks (Zhu et al., 2019) by only matching gradients of private and synthetic samples. As accurately optimizing the objective could be non-trivial for nonlinear networks, a line of work has been developed to strengthen the attack (Zhao et al., 2020). Geiping et al. (2020) introduce scaling invariance via cosine similarity. Balunovic et al. (2022) summarizes these attacks as a variety of prior choices. The theorem is echoed by empirical studies that show better attacks using advanced image priors (Jeon et al., 2021). It was shown that allowing architecture modification could weigh in higher risks in specific cases (Zhu and Blaschko, 2021; Fowl et al., 2022). Besides the study of gradient leakage in initialized models, recent evidence showed that the well-trained deep network can also leak the private samples (Haim et al., 2022). Yet, such an attack still requires strict conditions to succeed, e.g., small dataset and gradient flow assumptions. Though these attacks were shown to be promising, exactly solving the attack problem for all types of models or data is still time-consuming and can be intractable in reasonable time (Huang et al., 2021).

_Defense._ Most defense mechanisms introduce a perturbation in the pipeline of gradient computation. _1) Gradient Noise._ Since the DGL is centered on reverse engineering gradients, the straightforward defense is to modify the gradient itself. Zhu et al. (2019) investigated a simple defense by Gaussian noising gradients or pruning low-magnitude parameters-based defense. The Gaussian noise was motivated by Differential Privacy Gradient Descent (Dwork, 2006; Abadi et al., 2016), which guarantees privacy in an aggregated gradient. Except for random perturbation, the perturbation could also be guided by representation sensitivity (Sun et al., 2020) or information bottleneck Scheliga et al. (2022). _2) Sample Noise._ The intuition of sample noise is to change the source of the private gradient. For example, Gao et al. (2021) proposed to seek transformations such that the transformed images are hard to be recovered. Without extensive searching, simply using strong random data augmentation, e.g., MixUp (Zhang et al., 2017), can also help defense (Huang et al., 2021). If public data are available, hiding private information inside public samples is also an alternative solution (Huang et al., 2020). Though the defenses showed considerate protection against attacks in many empirical studies, the security still conceptually relied on the expected risks of a non-trivial number of evaluations. Explanation of how well a single-shot protection, for example, noising the gradient, is essential yet remains unclear.

_Understanding DGL._ There are many recent efforts in understanding when noise-based mechanisms help privacy protection. Though Differential Privacy (DP) provides a theoretical guarantee for general privacy, it describes the worst-case protection and does not utilize the behavior of DGL, motivating a line of studies to understand when and how the DGL happens. Sun et al. (2020) connected the privacy leakage to the representation, which aligns with later work on the importance of representation statistics in batch-normalization layers (Hatamizadeh et al., 2021; Huang et al., 2021). Balunovic et al. (2022) unified different attacks in a Bayesian framework where existing attacks differ mainly on the prior probability. Chen and Campbell (2021) investigated the model structure to identify the key reasons for gradient leakage. Hatamizadeh et al. (2021); Huang et al. (2021) evaluated the DGL in practical settings and showed that BN statistics is critical for inversion. There are efforts advancing theoretical understandings of DGL. Pan et al. (2022) studied the security boundary of ReLU activation functions. In a specific class of fully-connected networks, Wang et al. (2023b) showed that a single gradient query at randomized model parameters like DP protection can be used to reconstruct the private sample. Hayes et al. (2023); Fan et al. (2020) proposed an upper bound of the success probability of the data reconstruction attack and the relative recovery error, respectively, which is considered as the least privacy risk. Instead, we consider the worst-case privacy risk.

Though many useful insights are provided, existing approaches typically focus on specific model architectures. This work provides a new tool for analyzing the DGL without assumptions about the model architectures or attacking optimization. Independent of the model architectures, our main observation is that the Jacobian, the joint derivative of loss w.r.t. input and parameters, is the crux.

## 3 Inversion Influence Function

In this section, we propose a new tool to unravel the black box of DGL. We consider a general loss function \(L(x,)\) defined on the joint space of data \(\) and parameter \(\). For simplicity, we assume \(x\) is an image with oracle supervision if considering supervised learning. Let \(x_{0}\) be the private data and \(g_{0}_{}L(x_{0},)\) be its corresponding gradient which is treated as a constant. An inversion attack aims at reverse engineering \(g_{0}\) to obtain \(x_{0}\). Deep Gradient Leakage (DGL) attacks by directly searching for a synthetic sample \(x\) to reproduce the private gradient \(g\) by \(_{}L(x,)\)(Zhu et al., 2019). Formally, the DGL is defined as a mapping from a gradient \(g\) to a synthetic sample \(x_{g}^{*}\):

\[x_{g}^{*}=G_{r}(g)*{arg\,min}_{x}\{ L_{I}(x;g)\|_{}L(x,)-g\|^{2}\},\] (1)

where \(L_{I}\) is the _inversion loss_ matching two gradients, \(\|\|\) denotes the \(_{2}\)-norm either for a vector or a matrix. The \(L_{2}\)-norm of a matrix \(A\) is defined by the induced norm \(\|A\|=_{x 0}(\|Ax\|\ /\ \|x\|)\). The Euclidean distance induced by \(_{2}\)-norm can be replaced by other distances, e.g., cosine similarity (Geiping et al., 2020), which is out of our scope, though.

Our goal is to understand _how_ the gradient of a deep network at input \(x_{0}\) encloses the information of \(x_{0}\). For a linear model, e.g., \(L(x,)=x^{}\), the gradient is \(x\) and directly exposes the direction of the private sample. However, the task gets complicated when a deep neural network is used and \(L(x,)\) is a nonlinear loss function. The complicated functional makes exactly solving the minimization in Eq. (1) highly non-trivial and requires intensive engineering in hyperparameters and computation (Huang et al., 2021). Therefore, analyses tied to a specific DGL algorithm may not be generalizable. We introduce an assumption of a _perfect attacker_ to achieve a generalizable analysis:

**Assumption 3.1**.: _Given a gradient vector \(g\), there is only a unique minimizer for \(L_{I}(x;g)\)._Since \(_{}L(x,)\) is a minimizer for \(G_{r}(_{}L(x,))\), Assumption 3.1 implies that \(G_{r}(_{}L(x,))\) is a perfect inversion attack that recovers \(x\) exactly. The assumption of a perfect attacker considers the worst-case. It allows us to develop a concrete analysis, even though such an attack may not always be feasible in practice, for example, in non-linear deep networks. To start, we outline the desired properties for our privacy analysis. **(1) Efficiency:** Privacy evaluation should be efficient in terms of computation and memory; **(2) Proximity**: The alternative should provide a good approximation or a lower bound of the risk, at least in the high-risk region; **(3) Generality**: The evaluation should be general for different models, datasets, and attacks.

### Perturbing the Private Gradient

To figure out the association between the leakage and the gradient \(g\), we formalize a counterfactual: what kind of defense can diminish the leakage? A general noise-based defense can be written as \(g=_{}L(x_{0},)+\) where \(\) is a small perturbation. According to Assumption 3.1, a zero \(\) is not private. Instead, we are interested in non-zero \(\) and the recovery error (RE) of the recovered image:

\[(x_{0},G_{r}(g_{0}+))\|x_{0}-G_{r}(g_{0}+)\|\] (2)

For further analysis, we make two common and essential assumptions as follows.

**Assumption 3.2**.: \(L(x,)\) _is twice-differentiable w.r.t. \(x\) and \(\), i.e., \(J_{x}_{}L(x,)^{d_{x} d _{}}\) exist, where \(d_{x}\) and \(d_{}\) are the dimensions of \(x\) and \(\), respectively._

**Assumption 3.3**.: \(JJ^{}\) _is invertible._

We then approximate \(G_{r}(g_{0}+)\) by the first-order Taylor expansion:

\[G_{r}(g_{0}+) G_{r}(g_{0})+(g_{0})}{ g _{0}}=x_{0}+(g_{0})}{ g_{0}}.\] (3)

By the implicit function theorem, we can show that \((g_{0})}{ g_{0}}=(JJ^{})^{-1}J\) (proof in Appendix B.1). Thus, for a small perturbation \(\), we can approximate the privacy leakage through DGL by

\[^{2}$F):}\|G_{r}(g_{0}+)-x_{0} \|(;x_{0})\|(JJ^{})^{-1}J \|.\] (4)

The I\({}^{2}\)F includes a matrix inversion, computing which may be expensive and unstable for singular matrixes. Thus, we use a tractable lower bound of I\({}^{2}\)F as:

\[\|(JJ^{})^{-1}J\|( JJ^{})}_{}(;x_{0}),\] (5)

where \(_{}(A)\) denotes the maximal eigenvalues of a matrix \(A\). Computing the maximal eigenvalue is usually much cheaper than the matrix inversion. We note that the approximation uses Assumption 3.1 and provides a general risk measurement of different attacks.

The lower bound describes how risky the gradient is in the worst case. This follows the similar intuition of Differential Privacy (DP) (Dwork, 2006) to bound the max chance (worst case) of identifying a private sample from statistic aggregation. The proposed lower bound differentiates the DP worst case in that it leverages the problem structure of gradient inversion and therefore characterizes the DGL risk more precisely.

**Efficient Evaluation.** The evaluation of \(\) or its lower bound \(_{}\) is non-trivial due to three parts: computation of the Jacobian, the matrix inversion, and eigenvalue. The former two computations can be efficiently evaluated using established techniques, for example, (Koh and Liang, 2017). _1) Efficient evaluation of \(J\)_. Instead of computing the dense Jacobian, an efficient alternative is to leverage the Jacobian-vector product, which is similar to the Hessian-vector product. We can efficiently evaluate the product of Jacobian and \(\) by rewriting the product as \(J=_{x}(_{}^{}L(x_{0},))\). Since \(_{}^{}L(x_{0},)\) is a scalar, the second derivative w.r.t. \(x\) can be efficiently evaluated by autograd tools, e.g., PyTorch. The computation complexity is equivalent to two times of gradient evaluation in addition to one vector production. _2) Efficient matrix inversion._ We can compute \(b(JJ^{})^{-1}J\) by solving \(_{b}\|-J^{}b\|^{2}\), whose gradient computation only contains two Jacobian-vector products. There are other alternative techniques, for example, Neumann series or stochastic approximation, as discussed in (Koh and Liang, 2017). For brevity, we discuss these alternatives in Appendix A.

In this paper, we use the _least square trick_ and solve the minimization by gradient descent directly.

_3) Efficient evaluation of privacy with the norm of Jacobian._ Because that \( J=(JJ^{})}\), we need to compute the maximum eigenvalue of \(JJ^{}\), which can be done efficiently by power iteration and the trick of Jacobian-vector products.

**Complexity.** Suppose the computation complexity for evaluating gradient is a constant in the order \((d)\) where \(d\) is the maximum between the dimension of \(\) and \(x\). Then evaluating the Jacobian-vector product is of complexity \((d)\). By the least square trick with \(T\) iterations, we can compute \(\) in \((dT)\) time. The lower bound \(_{}\) replaces the computation of matrix inversion with computation on the max eigenvalue. By \(T^{}\)-step power iteration, the complexity is \((d(T+T^{}))\).

**Case study on Gaussian perturbation.** Gaussian noise is induced from DP-SGD (Abadi et al., 2016) where gradients are noised to avoid privacy leakage. Assuming the perturbation follows a standard Gaussian distribution, we can compute the expectation as follows:

\[[^{2}(x_{0},G_{r}(g_{0}+))][ ^{2}(;x_{0})]=_{i=1}^{d}_{i}^{-1}(J^{ }J),\] (6)

where \(_{i}(J^{}J)\) is the \(i\)-th eigenvalue of \(J^{}J\). The above illustrates the critical role of the Jacobian eigenvalues. For a perturbation to offer a good defense and yield a large recovery error, the Jacobian should have at least one small absolute eigenvalue. In other words, the protection is great when the Jacobian is nearly singular or rank deficient. The intuition for the insight is that a large eigenvalue indicates the high sensitivity of gradients on the sample, and therefore an inaccurate synthesized sample will be quickly gauged by the increased inversion loss.

### Theoretic Validation

The proposed I\({}^{2}\)F assumes that the perturbation is small enough for an accurate approximation, and yet it is interesting to know how good the approximation is for larger \(\). To study the scenario, we provide a lower bound of the inversion error on a non-infinitesimal \(\), using weak Lipschitz assumptions for any two samples \(x\) and \(x^{}\).

**Assumption 3.4**.: _There exists \(_{J}>0\) such that \(_{x}_{}L(x,)-_{x}_{}L(x ^{},)_{J} x-x^{}\)._

**Assumption 3.5**.: _There exists \(_{L}>0\) such that \(_{}L(x,)-_{}L(x^{},) _{L} x-x^{}\)._

**Theorem 3.1**.: _If Assumption 3.4 and 3.5 hold, then the recovery error satisfies:_

\[ x_{0}-G_{r}(g_{0}+) J+2_{J} g _{0}+},\] (7)

_where \(J=_{x}_{}L(x_{0},)\)._

The proof is available in Appendix B.2. Similar to I\({}^{2}\)F, the inversion error could be lower bound using the Jacobian matrix. Eq.7 matches our lower bound \(_{}\) in Eq.5 with an extra term of gradients. The bound implies that our approximation could be effective as a lower bound even for larger \(\). Besides, the lower bound provide additional information when \(\) is large: when \(\) gets infinitely large, the bound converges to \(( J/(_{J}))\), which does not scale up to infinity. The intuition is that the \(x\) is not necessarily unbounded for matching an unbounded 'gradient'.

## 4 Empirical Validation and Extensions

We empirically show that our metric is general for different attacks with different models on different datasets from different modalities.

**Setup.** We evaluate our metric on two image-classification datasets: MNIST (LeCun, 1998) and CIFAR10 (Krizhevsky et al., 2009). We use a linear model and a non-convex deep model ResNet18 (He et al., 2015) (RN18) trained with cross-entropy loss. We evaluate our metric on two popular attacks: the DGL attack (Zhu et al., 2019) and the GS attack (Geiping et al., 2020). DGL attack minimizes the \(L_{2}\) distance between the synthesized and ground-truth gradient, while the GS attack maximizes the cosine similarity between the synthesized and ground-truth gradient. We use the Adam optimizer with a learning rate of 0.1 to optimize the dummy data. Per attacking iteration, the dummy data is projected into the range of the ground-truth data \(\) with the GS attack. When the DGL attack cannot effectively recover the data, e.g., on noised linear gradients, we only conduct the GS attack to show the highest risk. We use the root of MSE (RMSE) to measure the difference between the ground-truth and recovered images. All the experiments are conducted on one NVIDIA RTX A5000 GPU with the PyTorch framework. More details and extended results are attached in Appendix C.

**Extension to singular Jacobians.** We consider the \(JJ^{}\) be singular, which often happens with deep neural networks where many eigenvalues are much smaller than the maximum. In this case, the inversion of \(JJ^{}\) does not exist or could result in unbounded outputs. To avoid the numerical instability, we use \((JJ^{}+ I)^{-1}J\) with a constant \(\). In our experiments, the \(\) is treated as a hyperparameter and will be tuned on samples to best fit the linear relation between RMSE and \(\).

**Batch data.** In practice, an attacker may only be able to carry out batch gradient inverting, \(_{i}^{n}_{}L(x_{i},)- _{i}^{n}g_{i}\), which can be upper bounded by \(_{i}^{n}\|_{}L(x_{i},)-g_{i}\|\). Such decomposition can be implemented following (Wen et al., 2022), which modifies the parameters of the final layer to reduce the averaged gradient to an update of a single sample. Thus, we only consider the more severe privacy leakage by individual samples instead of a batch of data.

**How well are the influence approximations?** In Fig. 1(a), we compare the two approximations of \(^{2}\). \(_{}\) relaxes the matrix inversion in \(\) to the reciprocal of \( JJ^{}\). \(_{}\) further drops the denominator and only keep the nominator \( J\). The two figures show that \(_{}\) serves as a tight approximation to the real influence while \(_{}\) is much smaller than the desired value. In Fig. 1(b), we see that the relaxation \(_{}\) is also a good approximation or lower bound for the RMSE.

**Validation on different models, datasets, and attacks.** In Fig. 2, we show that \(^{2}\) can work approximately well in different cases though the approximation accuracy varies by different cases. _(1) Attack:_ In Figs. 2(a) and 2(b), we compare DGL to the Gradient Similarly (GS) (Geiping et al., 2020) that matches gradient using cosine similarity instead of the \(L_{2}\) distance. We see that different attacks do not differ significantly. _(2) Dataset:_ As shown in Figs. 2(b) and 2(c), different datasets could significantly differ regarding attack effectiveness. The relatively high RMSE implies that CIFAR10 tends to be harder to attack due to more complicated features than MNIST. Our metric suggests that there may exist a stronger inversion attack causing even lower RMSE. _(3) Model:_

Figure 1: Value comparisons attacking ResNet18 on MNIST by DGL, where the grey line indicates the equal values and darker dots imply smaller Gaussian perturbation \(\). In (a), the y-axis is calculated as defined in Eq. (4) and \(_{lb}\) is calculated as defined in Eq. (5). \(^{2}\) lower bound (\(_{}\)) provides a good approximation to the exact value with matrix inversion and to the root of mean square error (RMSE) of recovered images. Instead, removing the denominator in \(\) results in overestimated risks.

Figure 2: \(^{2}\) works under different settings: datasets, attacks, and models. The grey line indicates the equal values, and darker dots imply smaller Gaussian perturbation \(\).

Comparing Figs. 2 and 2, the linear model presents a better correlation than the ResNet18 (RN18). Because the linear model is more likely to be convex, the first-order Taylor expansion in Eq. (3) can approximate its RMSE than one of the deep networks.

**Results on Large Models and Datasets.** We also evaluate our I\({}^{2}\)F metric on ResNet152 with ImageNet. For larger models, the RMSE is no longer a good metric for the recovery evaluation. Even if state-of-the-art attacks are used and the recovered image is visually similar to the original image in Fig. 3, the two images are measured to be different by RMSE, due to the visual shift: The dog head is shifted toward the left side. To capture such shifted similarity, we use LPIPS (Zhang et al., 2018) instead, which measures the semantic distance between two images instead of the pixel-to-pixel distance like RMSE. Fig. 3 shows that I2F is correlated to LPIPS using large models and image scales. This implies that I2F is a good estimator of recovery similarity. In Fig. 3, original images with a lower I2F also have a smaller LPIPS, which means a better reconstruction. Recovered images on the right (the German shepherd and the panda) cannot be recognized while those on the left (the owl and the beagle) still have enough semantic information for humans to recognize.

**Results on Language Models and Datasets.** Since the input space of language models is not continuous like images, current attacks on images cannot be directly transferred to text. Besides, continuously optimized tokens or embeddings should be projected back to the discrete text, which induces a gap between the optimal and the original text. Thus, the investigation of privacy risks in text is an interesting yet non-trivial problem. We evaluate the proposed I\({}^{2}\)F metric on BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) with TAG attack (Deng et al., 2021), which minimizes \(L_{2}\) and \(L_{1}\) distance between gradients of the original and recovered images. We use _ROUGE-L_(Lin, 2004) and _Google BLEU_(Wu et al., 2016) to measure the semantic similarity between the original text and the recovered text. More experimental details and results with _ROUGE-1_ and _feature MSE_ are provided in C.2. The results are presented in Fig. 4. A darker color indicates a larger noise variance. Since the _ROUGE-L_ and _Google BLEU_ measure the semantic similarity of the text pair while our metric estimates the difference, two semantic metrics are negatively correlated to \(_{lb}\). The results demonstrate the effectiveness of our metric in evaluating the privacy risks of discrete data.

## 5 When Does Privacy Leakage Happen?

Guided by the formulation of I\({}^{2}\)F, we provide insights on when privacy leakage happens based on the proposed metric. Our insights are rooted in the properties of Jacobian. Since we want to consider the whole property of the Jacobian matrix, we choose a shallow convolutional neural network LeNet (LeCun et al., 1998) to trade off the utility and efficiency of experiments by default.

### Perturbations Directions Are Not Equivalent

**Implication on choosing \(\).** Eq. (4) clearly implies that the perturbation is not equal in different directions. Decomposing \(J=U V^{}\) using Singular Value Decomposition (SVD), we obtain \((;x_{0})=\|U^{-1}V^{}\|\). Thus, \(\) tends to yield a larger I\({}^{2}\)F value if it aligns with the directions of small eigenvalues of \(JJ^{}\).

Figure 3: Evaluation of I\({}^{2}\)F on ResNet152 and ImageNet. (a): Darker color means larger noise variance. LPIPS is used to evaluate the semantic information of the recovered and original images. \(I_{lb}\) is a good estimator of the semantic distance between the recovered images and original images. (b): Original (top) and recovered (bottom) images with their corresponding I\({}^{2}\)F and LPIPS. Images with a lower I\({}^{2}\)F also have a smaller LPIPS, which implies a better reconstruction.

**Singular values of Jacobian of deep networks.** In Fig. 5, we examine the eigenvalues of LeNet on the MNIST and CIFAR10 datasets. On both datasets, there are always a few eigenvalues of \(JJ^{}\) (around \(10^{-2}\)) that are much smaller than the largest one (\( 1\)). Observing such a large gap between \(JJ^{}\) eigenvalues, it is natural to ask: how do the maximal/minimal eigenvalues affect the DGL attacks?

**Comparing eigenvectors in defending DGL.** We consider a special case of perturbation by letting \(\) be an eigenvector of \(JJ^{}\). Then the \(^{2}\) will be \(1/\) where \(\) is the corresponding eigenvalue. We conjecture \(1/\) could predict the MSE of DGL attacks. To verify the conjecture, we choose 4 eigenvectors with distinct eigenvalues per sample. The results for the LeNet model are present in Fig. 6. We see that the MSE decreases by \(\). For the MNIST dataset, the MSE-\(\) relation is very close to the predicted \(1/\). Though the curve is biased from the ground truth for CIFAR10, we still can use \(1/\) to lower bound the recovery error. The bias in CIFAR10 is probably due to the hardness of recovering the more complicated patterns than the digit images. The recovered images in Fig. 6 suggest that even with the same perturbation scale, there exist many bad directions for defense. In the worst case, the image can be fully covered. The observation is an alerting message to the community: _protection using random noise may leak private information_.

### Privacy Protection Could Be Unfair

Our analysis of the Gaussian perturbation in Eq. (6) indicates that the privacy risk hinges on the Jacobian and therefore the sample. We conjecture that the resulting privacy risk will vary significantly due to the sample dependence. In Fig. 7, we conduct fine-grained experiments to study the variance of privacy protection, when Gaussian perturbation on gradients is used for protection. We use a well-trained model which is thought to be less risky than initial models, for example, in (Balunovic et al., 2022). Though the average of MSE implies a reasonable privacy degree as reported in previous literature, the large variance delivers the opposite message that some samples or classes are not that

Figure 4: Evaluation of \(_{lb}\) on BERT (a-b) and GPT-2 (c-d). A darker color means a larger noise variance. ROUGE-L and Google BLEU are used to evaluate the semantic similarity between the original text and the recovered text. \(_{lb}\) is linearly correlated to the two semantic metrics, which means \(_{lb}\) can be used to estimate the privacy risk of the private text.

Figure 5: The distribution of eigenvalues of \(JJ^{}\) of two datasets on the LeNet model.

Figure 6: Same perturbation sizes but different protection effects by different eigenvectors. In (a) and (b), MSEs of DGL attacks are reversely proportional to eigenvalues on the LeNet model. Blue curves are scaled \(1/\). Darker dots indicate smaller MSE (higher risks). Recovered MNIST images associated with different eigenvalues are present on the right.

safe. In the sense of samples, many samples are more vulnerable than the average. For the classes, some classes are obviously more secure than others. Thus, when the traditional metric focusing on average is used, it may deliver a fake sense of protection unfairly for specific classes or samples.

To understand which kind of samples could be more vulnerable to attack, we look into the Jacobian and ask when \(JJ^{}\) will have larger eigenvalues and higher risks in Fig. 7. Consider a poisoning attack, where an attacker aims to maximize the training loss by adding noise \(_{x}\) to the sample \(x\). Suppose the parameter is updated via \(^{}(x+_{x})=-_{}L(x+_{x},)\). Let \(J_{}=_{x}_{}L(x_{1}+_{x},)\) and we can derive the maximal loss amplification on a test sample \(x_{1}\) when perturbing \(x\) as:

\[_{\|_{x}\| 1}_{x_{1}}[L(x_{1},^{ }(x+_{x}))] _{\|_{x}\| 1}_{x_{1}}[L(x_{1}, ^{}(x))]+_{x_{1}}[_{x}^{}_{x}L(x_{1}, ^{}(x))]\] \[=_{\|_{x}\| 1}_{x}^{}J_{} _{x_{1}}[_{^{}}L(x_{1},^{})]\] \[=\|J_{}_{x_{1}}[_{^{}}L(x_{1}, ^{})]\|\] \[\|J_{}\|\|_{x_{1}}[_{^{ }}L(x_{1},^{})]\|.\]

As shown by the inequality, the sample with large \(\|J_{}\|\) may significantly bias the training after mild perturbation \(_{x}\). If Assumption 3.4 holds, then \(\|J\|\) can be associated with \(\|J_{}\|\) by \(|\|J\|-\|J_{}\|_{J}\). Now we can connect the privacy vulnerability to the data poisoning. With Eq. (6), samples that suffer high privacy risks due to large \(\|J\|\) could also be influential in training and can be easily used to poison training.

### Model Initialization Matters

Since Jocabian leans on the parameters, we hypothesize that the way we initialize a model can also impact privacy. Like previous work (Sun et al., 2020; Balunovic et al., 2022; Zhu et al., 2019), we are interested in the privacy risks when a model is randomly initialized without training. Unlike previous work, we further ask which initialization strategy could favor privacy protection under the same Gaussian perturbation on gradients.

Case Study: One-layer network.A one-layer network with nonlinear activation \(()\) is a simple yet sufficient model for analyzing initialization. Let \(L(x,)=\|(^{}x)-b\|^{2}\), where \(b\) is a constant. Denote the product \(^{}x\) as \(a\). The Jacobian is given by:

\[J=L}{ x}=\{((^{}x)-b)I+x^{}\}.\]

(1) Apparently, a well-trained \(\) with \(((^{}x)-b)=0\) will cause \(J\) to be a rank-one matrix. Note that the gradient will be zero in such a case and no longer provide any information about the private sample. Consistent with our theoretical results, where the inverse Jacobian makes \(\) infinite, the gradient will be perfectly private at any perturbation. (2) If \((^{}x)-b\) is non-zero, the singular values of \(J\) will depends on how close \([(^{}x)]\) is to \(b\). If \([(^{}x)]\) approaches \(b\) at initialization, then \(J\) will be approximately singular, and therefore the initialization enhances privacy preservation.

Figure 8: Different initialization strategies could result in distinct MSEs.

Figure 7: The sample-wise and class-wise statistics of the DGL MSE on the MNIST dataset, when gradients are perturbed with Gaussian noise of variance \(10^{-3}\). The purple lines indicate the average values. Large variances are observed among samples and classes. The original (top) and recovered (bottom) images for the well- and poorly-protected classes are depicted on the right side.

**Comparing initialization.** To evaluate the effect of initialization on the inversion, we conduct experiments of the LeNet model on MNIST and CIFAR10 datasets with DGL and GS attacks. We add Gaussian noise to the gradients, which implies an expected MSE should be proportional to \([^{2}]=_{i}_{i}^{-1}\) as proved in Eq. (6). Here, we evaluate how the initialization will change eigenvalues of \(JJ^{}\) and thus change the MSE of DGL. Four commonly used initialization techniques are considered here: uniform distribution in range \([-0.5,0.5]\), normal distribution with zero mean and variance \(0.5\), kaiming (He et al., 2015) and xavier (Glorot and Bengio, 2010) initialization. For each initialization, the same 10 samples are used to conduct the attack three times. The results are shown in Fig. 8. We observe a significant gap between initialization mechanisms. Using uniform initialization cast serious risks of leaking privacy under the same Gaussian defense. Though not as significant as uniform initialization, the normal initialization is riskier than rest two techniques. kaiming and xavier methods can favor convergence in deep learning and here we show that they are also preferred for privacy. A potential reason is that the two methods can better normalize the activations to promote the Jacobian singularity.

In parallel with our work, Wang et al. (2023) also finds that the initialization is impactful on inversion attacks. Despite their analysis being based on a different metric, the layer-wise variance of the model weights, their work and ours have a consistent conclusion that models initialized with a uniform distribution face higher privacy risks than that with a kaiming distribution

## 6 Conclusion and Discussion

In this paper, we introduce a novel way to use the influence functions for analyzing Deep Gradient Leakage (DGL). We propose a new and efficient approximation of DGL called the Inversion Influence Function (I\({}^{2}\)F). By utilizing this tool, we gain valuable insights into the occurrence and mechanisms of DGL, which can greatly help the future development of effective defense methods.

**Limitations.** Our work may be limited by some assumptions and approximations. First, we worked on the worst-case scenario where a strong attack conducts perfect inversion attacks. In practice, such an assumption can be strong, especially for highly complicated deep networks. The gap between existing attacks and perfect attacks sometimes leads to a notable bias. However, we note that recent years witnessed many techniques that significantly improved attacking capability (Geiping et al., 2020; Jeon et al., 2021; Zhao et al., 2020), and our work is valuable to bound the risks when the attacks get even stronger over time. Second, similar to the traditional influence function, I\({}^{2}\)F can be less accurate and suffers from large variance in extremely non-convex loss functions. Advanced linearization techniques (Bae et al., 2022) can be helpful in improving the accuracy of influence. Then extending our analysis to bigger foundation models may bring intriguing insights into the scaling law of privacy.

**Future Directions.** As the first attempt at influence function in DGL, our method can serve multiple purposes to benefit future research. For example, our metric can be used to efficiently examine the privacy breach before sending gradients to third parties. Since I\({}^{2}\)F provides an efficient evaluation of the MSE, it may be directly optimized in conjunction with the loss of main tasks. Such joint optimization could bring in the explicit trade-off between utility and privacy in time. In comparison, traditional arts like differential privacy are complicated by tuning the privacy parameter for the trade-off. Furthermore, we envision that many techniques can be adopted to further enhance the analysis. For example, unrolling-based analysis leverages the iterative derivatives in the DGL to uncover the effectiveness of gradient perturbations (Pruthi et al., 2020).

**Broader Impacts.** Data privacy has been a long-term challenge in machine learning. Our work provides a fundamental tool to diagnose privacy breaches in the gradients of deep networks. Understanding when and how privacy leakage happens can essentially help the development of defenses. For example, it can be used for designing stronger attacks, which leads to improved defense mechanisms and ultimately benefit the privacy and security of machine learning.

## 7 Acknowledgments

This research was supported by the National Science Foundation (IIS-2212174, IIS-1749940), National Institute of Aging (IRF1AG072449), and the Office of Naval Research (N00014- 20-1-2382).