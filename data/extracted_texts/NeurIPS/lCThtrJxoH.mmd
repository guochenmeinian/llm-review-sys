# Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning

Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning

 Stephen McAleer

Carnegie Mellon University

&Gabriele Farina

MIT

&Gaoyue Zhou

Carnegie Mellon University

Mingzhi Wang

Peking University

&Yaodong Yang

Peking University

&Tuomas Sandholm

Carnegie Mellon University

###### Abstract

Recent algorithms have achieved superhuman performance at a number of two-player zero-sum games such as poker and go. However, many real-world situations are multi-player games. Zero-sum two-_team_ games, such as bridge and football, involve two teams where each member of the team shares the same reward with every other member of that team, and each team has the negative of the reward of the other team. A popular solution concept in this setting, called TMECor, assumes that teams can jointly correlate their strategies before play, but are not able to communicate during play. This setting is harder than two-player zero-sum games because each player on a team has different information and must use their public actions to signal to other members of the team. Prior works either have game-theoretic guarantees but only work in very small games, or are able to scale to large games but do not have game-theoretic guarantees. In this paper we introduce two algorithms: Team-PSRO, an extension of PSRO from two-player games to team games, and Team-PSRO Mix-and-Match which improves upon Team PSRO by better using population policies. In Team-PSRO, in every iteration both teams learn a joint best response to the opponent's meta-strategy via reinforcement learning. As the reinforcement learning joint best response approaches the optimal best response, Team-PSRO is guaranteed to converge to a TMECor. In experiments on Kuhn poker and Liar's Dice, we show that a tabular version of Team-PSRO converges to TMECor, and a version of Team PSRO using deep cooperative reinforcement learning beats self-play reinforcement learning in the large game of Google Research Football.

## 1 Introduction

Two-player zero-sum games have served as testbeds for artificial intelligence research. Algorithms that have achieved superhuman performance in games such as go (Silver et al., 2017) and poker (Bowling et al., 2015; Brown and Sandholm, 2017) are widely seen as milestones. These algorithms use game-theoretic techniques to find an approximate Nash equilibrium, in other words a strategy that cannot be exploited by humans. Despite these successes in two-player games, real-world scenarios often involve more than two players.

In this paper we study a class of games with multiple players where each player shares reward with the other players in one of two teams, which we call two-team games. A canonical example of a two-team game is bridge, a game in which current algorithms fail to compete with expert humans. There are multiple solution concepts for team games, but the one we consider is known as TMECor. In this setting, we allow each team to jointly sample a value from a distribution to correlate their strategies before play. This setting has many nice properties and is the natural solution concept for many team games. Two joint team strategies are in a TMECor if the value that a joint best response for an opponent team gets against each team is the same as the value when both teams are playing their current strategy.

There are two strands of research surrounding two-team games. The first has developed tabular methods that have game-theoretic guarantees. These algorithms work well on very small games but do not scale to large team games. The second has developed deep reinforcement learning algorithms that can scale to large two-team games, but end up being exploitable because they are not based on game-theoretic algorithms.

In this paper, we introduce the first scalable game-theoretic techniques for two-team games. We show that a straightforward extension of PSRO to team games, called Team PSRO, and is guaranteed to converge to an approximate TMECorr. We also introduce a novel algorithm called Team PSRO Mix-and-Match (Team PSRO-MM) that mixes and matches best responses to create a larger population and show that it outperforms Team PSRO. In our experiments, we show that Team PSRO-MM outperforms Team PSRO and that both methods outperform self-play on the large benchmark game of Google Research Football.

Team PSRO is based on _policy space response oracles (PSRO)_(Lanctot et al., 2017). PSRO is already one of the most promising methods for finding approximate Nash equilibrium in large two-player zero-sum games because it is simple to use with existing RL methods, it naturally provides a measure of approximate exploitability, and doesn't require full game-tree traversals. Methods based on PSRO such as AlphaStar (Vinyals et al., 2019) and Pipeline PSRO (McAleer et al., 2020) have achieved state-of-the-art performance on Starcraft and Barrage Stratego, respectively. Our method, called Team PSRO is the first PSRO-based algorithm for team games and we show that it converges to TMECor when the RL best response is strong enough.

Despite being the first scalable method for computing TMECor, Team PSRO does not efficiently use the population of best responses because each policy must be deployed with the corresponding policy in its joint best response. Building on Team PSRO we fix this issue by allowing policies to be mixed and matched with other policies in the population that are not the corresponding policy in its joint best response. We show that the resulting algorithm, called _Team PSRO Mix-and-Match (Team PSRO-MM)_ is able to empirically improve upon Team PSRO. We hypothesize that this is because computing best responses is more costly that evaluating the expected value of policies when the number of policies is low enough.

Our contributions include the following:

1. We show that a straightforward extension of PSRO to team games converges to TMECor.
2. We introduce a novel method based on Team PSRO, called Team PSRO Mix-and-Match, that emperically converges faster by better using the best responses in the population.

## 2 Background

Extensive-form games (EFGs) model games that are played on a game tree, and can capture both sequential and simultaneous moves, as well as private information. In this section, for simplicity we focus on four-player zero-sum games where two players--\(\) and \(\)--play as a team against two opponent players, denoted by \(\) and \(\). However, these methods generalize to games with more than two players, as shown in our experiments on Google Research Football.

Each node \(v\) in the game tree belongs to exactly one player \(i\{,,,\}\{\}\) whose turn is to move. Player \(\) is a special player, called the _chance player_. It models exogenous stochasticity in the environment, such as drawing a card from a deck or tossing a coin. The edges leaving \(v\) represent the actions available at that node. Any node without outgoing edges is called a _leaf_ and represents an end state of the game. We denote the set of such nodes by \(Z\). Each \(z Z\) is associated with a tuple of payoffs specifying the payoff \(u_{i}(z)\) of each player \(i\{,,,\}\) at \(z\). The product of the probabilities of all actions of \(\) on the path from the root of the game to leaf \(z\) is denoted by \(p_{}(z)\).

Private information is represented via _information set_ (infoset). In particular, the set of nodes belonging to \(i\{,,, \}\) is partitioned into a collection \(_{i}\) of non-empty sets: each \(I_{i}\) groups together nodes that Player \(i\) cannot distinguish among, given what they have observed. Necessarily, for any \(I_{i}\) and \(v,w I\), nodes \(v\) and \(w\) must have the same set of available actions. Consequently, we denote the set of actions available at all nodes of \(I\) by \(A_{I}\). As it is customary in the related literature, we assume _perfect recall_, that is, no player forgets what he/she knew earlier in the game. Finally, given players \(i\) and \(j\), two infostes \(I_{i}_{i}\), \(I_{j}_{j}\) are _connected_, denoted by \(I_{i} I_{j}\), if there exist \(v I_{i}\) and \(w I_{j}\) such that the path from the root to \(v\) passes through \(w\) or vice versa.

Sequences.The set of _sequences_ of Player \(i\), denoted by \(_{i}\), is defined as \(_{i}\{(I,a):I_{i},a A_{I}\}\{\}\), where the special element \(\) is called the _empty sequence_ of Player \(i\). The _parent sequence_ of a node \(v\) of Player \(i\), denoted \((v)\), is the last sequence (information set-action pair) for Player \(i\) encountered on the path from the root of the game to that node. Since the game has perfect recall, for each \(I_{i}\), nodes belonging to \(I\) share the same _parent sequence_. So, given \(I_{i}\), we denote by \((I)_{i}\) the unique parent sequence of nodes in \(I\). Additionally, we let \((I)=\) if Player \(i\) never acts before infoset \(I\).

**Reduced-normal-form plans.** A _reduced-normal-form_ plan \(_{i}\) for Player \(i\) defines a choice of action for every information set \(I_{i}\) that is still reachable as a result of the other choices in \(\) itself. The set of reduced-normal-form plans of Player \(i\) is denoted \(_{i}\). We denote by \(_{i}(I)\) the subset of reduced-normal-form plans that prescribe all actions for Player \(i\) on the path from the root to information set \(I_{i}\). Similarly, given \(=(I,a)_{i}\), let \(_{i}()_{i}(I)\) be the set of reduced-normal-form plans belonging to \(_{i}(I)\) where Player \(i\) plays action \(a\) at \(I\), and let \(_{i}()_{i}\). Finally, given a leaf \(z Z\), we denote with \(_{i}(z)_{i}\) the set of reduced-normal-form plans where Player \(i\) plays so as to reach \(z\).

**Sequence-form strategies.** A _sequence-form strategy_ is a compact strategy representation for perfect-recall players in EFGs (Romanovskii, 1962; Koller et al., 1996). Given a player \(i\{,,,\}\) and a normal-form strategy \((_{i})\),2 the sequence-form strategy induced by \(\) is the real vector \(\), indexed over \(_{i}\), defined as \(y[]_{_{i}()}()\). The set of sequence-form strategies that can be induced as \(\) varies over \((_{i})\) is denoted by \(_{i}\) and is known to be a convex polytope (called the _sequence-form polytope_) defined by a number of constraints equal to \(|_{i}|\)(Koller et al., 1996).

**TMECor as a Bilinear Saddle-Point Problem.** A TMECor strategy is a probability distribution \(_{}\) over the set of randomized strategy profiles \(_{}_{}\) that guarantees maximum expected utility for the team against the best-responding opponent team \(\{,\}\). Since each player has perfect recall, any randomized strategy for a player is equivalent to a distribution over reduced-normal-form pure strategies (Kuhn, 1953). Hence, any distribution over profiles of randomized strategies of the team members can be expressed in an equivalent way as a distribution over _deterministic_ strategy profiles \(_{}_{}\). Denote by \((_{})\) all possible combinations of pure strategies of a population \(_{}\): \(\{(_{},_{})|_{},_{}_{}\}\). The benefit of this transformation is that \(_{}_{}\) is a finite set, unlike \(_{}_{}\). For this reason, TMECor is usually defined in the literature as a distribution over \(_{}_{}\) without loss of generality. We will follow the same approach in our characterization.

For each leaf \(z\), let \(_{}(z)(u_{}(z)+u_{ }(z))p_{}(z)\). The expected utility of the team can be written as the following function of the distributions of play \(_{}(_{}_{ }),_{}(_{}_{ })\):

\[u_{}(_{},_{})\!:=\!_{z Z}_{ }(z)\!\!(\!_{_{} _{}(z)\\ _{}_{}(z)}\!\!coordinated strategy \(_{}(_{}_{2})\) and the opponent team plays according to the coordinated strategy \(_{}(_{}_{2})\). In the zero-sum setting, this amounts to finding a solution of the optimization problem

\[_{_{}(_{}_{ })}_{_{}(_{} _{2})}u_{}(_{},_{}).\] (2)

Note that if a pair of joint strategies \((_{},_{})\) are in a TMECor, then

\[u_{}(_{},_{})=u_{}( _{}(_{}),_{})=u_{}( _{},_{}(_{}))\] (3)

### Approximate TMECor

Define an \(\)-best response (\(\)-BR) \(_{}^{}(_{})\) as any coordinated strategy that achieves expected utility against the opponent coordinated strategy within \(\) of optimal: \(u_{}(_{}(_{}),_{ })- u_{}(_{}^{ }(_{}),_{})\). The _exploitability_\(e(_{},_{})\) of a pair of correlated strategies \((_{},_{})\) is defined as \(e(_{},_{})=u_{}(_{ }(_{}),_{})+u_{}(_{},_{}(_{}))\). A pair of joint strategies \((_{},_{})\) is in an \(\)_-approximate TMECor_ if \(e(_{},_{})\)

## 3 Related Work

### Double Oracle (DO) and Policy Space Response Oracles (PSRO)

Double Oracle (McMahan et al., 2003) is an algorithm for finding a NE in two-player zero-sum normal-form games. The algorithm works by maintaining a population of strategies for each player. Each iteration a NE is computed for the game restricted to strategies in each player's population. Then, a best response to this NE for each player is computed and added to the population. Although in the worst case DO must expand all pure strategies, in many games DO empirically terminates early and outperforms existing methods. Policy-Space Response Oracles (PSRO) Lanctot et al. (2017) scales DO to large games by using reinforcement learning to approximate a best response. The restricted-game NE is computed on the empirical game matrix generated by having each policy in the population play each opponent policy and tracking average utility in a payoff matrix (Wellman, 2006).

There are a number of methods related to PSRO. NXDO (McAleer et al., 2021; Tang et al., 2023) iteratively adds reinforcement learning policies to a population but solve an extensive-form restricted game and has shown to be more efficient than PSRO in certain games. AlphaStar (Vinyals et al., 2019) trains a population of policies through a procedure that is somewhat similar to PSRO. AlphaStar also uses some elements of self-play when constructing its population, and outputs a meta-Nash equilibrium of the population at test time. P2SRO (McAleer et al., 2020) parallelizes PSRO with convergence guarantees. Other work has generalized PSRO to more players (Muller et al., 2020; Marris et al., 2021), incorporated diversity objectives to cover more of the strategy space (Liu et al., 2021; Perez-Nieves et al., 2021; McAleer et al., 2022; Slumbers et al., 2023; Yao et al., 2023), and meta-learned the meta-distribution (Feng et al., 2021). PSRO has also been applied to core problems in game theory (Zhang et al., 2023) and reinforcement learning (Liang et al., 2023).

### Team Games

In this paper we study the TMECor Celli & Gatti (2018); Farina et al. (2018); Zhang & Sandholm (2021); Zhang et al. (2022) setting where players on the same team are allowed to coordinate before playing. A related solution concept is an equilibrium where team members are not able to coordinate, which is known as a _team-maxmin equilibrium_ (TME) strategy (von Stengel & Koller, 1997; Basilico et al., 2017; Zhang & An, 2020, 2020, 2021; Kalogiannis et al., 2021; Anagnostides et al., 2023). A solution of this type yields the maximum expected utility for the team players against a best-responding opponent. The TMECor solution concept has several advantages over TME. First, the team is guaranteed at least as much expected utility under TMECor than under TME (Celli & Gatti, 2018). Second, the TMECor objective is convex while the TME objective is not convex.

### Cooperative Reinforcement Learning

In this section we focus on the regime of _centralized training and decentralized execution (CTDE)_ in cooperative reinforcement learning. In the CTDE regime, we assume to have access to all agent'sobservations during training time, which is a reasonable assumption for our setting of computational game solving. CTDE methods such as MADDPG Lowe et al. (2017) and COMA Foerster et al. (2018) make use of this centralized training learning a centralized critic that can condition on private information. Although this critic is taken away at test time and only the actor is used in decentralized execution, by conditioning on all relevant information, the centralized critic can reduce much more variance than an individual agent's critic.

Multi-Agent PPO (MAPPO) (Yu et al., 2021; Kuba et al., 2022) trains one PPO agent for the whole team with a centralized critic, among other tricks. Although off-policy CTDE methods such as MADDPG and QMix(Rashid et al., 2018; Schafer et al., 2023; Mguni et al., 2023) tend to outperform on-policy methods such as COMA (Papoudakis et al., 2020; Wang et al., 2021), in practice MAPPO can be a surprisingly effective and stable baseline, and we use it as our joint best response in Team PSRO.

### Deep RL for Team Games

A number of previous works have studied deep reinforcement learning methods in team games. Jaderberg et al. (2019) and Liu et al. (2021, 2019) train deep RL agents via self play and population based training to achieve high performance on a capture the flag video game and a simulated soccer environment, respectively. Berner et al. (2019) use self-play reinforcement learning to achieve expert level performance on Dota. None of these methods are grounded in game theoretic techniques, however, and may not be expected to converge to a TMECor. Concurrent to our work, Xu et al. (2023) propose an algorithm that is similar to Team PSRO, but they do not make a connection to TMECor and do not propose the idea of mixing and matching.

## 4 Team PSRO

As described in the previous section, existing methods for learning approximate TMECorrs are tabular, and as a result will not scale to large games. In this paper we propose Team-PSRO. Team-PSRO makes the simple observation that approximate joint best responses can be learned via cooperative reinforcement learning. The resulting algorithm is very similar to PSRO in that every iteration, a best response is computed against the opponent's restricted distribution and the resulting policy is added to a population. However, instead of computing single-agent best responses for a single player, the best responses for Team-PSRO are joint best responses for a single team. These joint best responses are a set of policies for each member of the team and are learned through cooperative reinforcement learning.

### Tabular Team Double Oracle

In this section we introduce _team double oracle_, a double oracle method for computing TMECor in zero-sum games with two teams. Team DO is conceptually the same as existing single oracle algorithms (Celli and Gatti, 2018) for team games but extends these algorithms to two team games. Team DO is very similar to the original double oracle algorithm for two-player zero-sum normal form games. Team DO maintains a population of pure joint strategies for each team \(_{},_{}\). Each pure strategy in a team's population is a pure joint strategy for each player on that team \(_{}=(_{_{1}},_{_{2}})\). Similar to DO, we construct a restricted game in the space of pure strategies of the populations for each team. Our restricted game \(G(_{},_{})\) is a two-player zero-sum normal-form game that takes as actions the pure strategies \(_{}_{}\) for the team and \(_{}_{}\) for the opponent team. Every iteration, the NE \((_{}^{},_{}^{c})\) of the restricted game \(G(_{}^{t},_{}^{t})\) is computed. Next, a best response to this NE in the original game \(_{}(_{}^{})\) is computed and added to the population. Team DO terminates when the best response for both players does not improve over the Nash value of the restricted game. Like DO, since there are a finite number of pure strategies, team DO must terminate (potentially in a number of iterations exponential in the size of the game). When it does terminate, the NE of the restricted game is a TMECor of the original game because no best response can improve on the Nash value. Team DO is described in Algorithm 1.

Compared to other methods for finding TMECor, team DO has a number of downsides. First, there are no known convergence rates for double oracle algorithms in general and team DO in particular. In fact, there exist games where Team DO must expand all joint pure strategies of the game before 

[MISSING_PAGE_FAIL:6]

### Tabular Team Double Oracle Mix-and-Match

In this section we introduce _team double oracle mix-and-match (Team DO-MM)_, which builds on Team DO by adding additional joint policies to the population for free. Similarly to Team DO, Team DO-MM maintains a population of pure joint strategies for each team \(_{},_{}\). Each pure strategy in a team's population is a pure joint strategy for each player on that team \(_{}=(_{},_{})\). In Team DO-MM, however, we construct a restricted game in an expanded space of pure strategies that considers all possible mixtures of policies from each joint best response. Contrasted with Team DO, Team DO-MM considers pure strategies that mix and match policies from different best responses of the players within each team. Our restricted game \(G((_{}),(_{}))\) is a two-player zero-sum normal-form game that takes as actions the pure strategies \(_{}(_{})\) for the team and \(_{}(_{})\) for the opponent team. Similar to Team DO, every iteration, the NE \((_{}^{},_{}^{})\) of the restricted game \(G((_{}^{}),(_{}^{}))\) is computed. Next, a best response to this NE in the original game \(_{}(_{}^{})\) is computed and added to the population. Team DO-MM also terminates when the best response for both players does not improve over the Nash value of the restricted game. Because upon termination no best response can improve on the Nash value, Team DO-MM inherits the convergence guarantee of Team DO to TMECor. Team DO-MM is shown as a one-line change in Algorithm 1.

**Proposition 1**.: _Tabular Team DO and Team DO-MM with exact best responses converge to a TMECor._

Proof.: Proof is contained in the Appendix. 

### Team PSRO

We now describe our main contribution, _team PSRO_ and _team PSRO Mix-and-Match_. All existing tabular algorithms cannot scale to a game the size of Google Research Football. And existing algorithms such as self play do not minimize exploitability, even though they might do well sometimes in practice. Team PSRO scales up Team DO to large games by swapping out an exact joint best response \(_{}(_{}^{})\) for an approximate joint best response \(_{}\) that is trained through reinforcement learning. Instead of learning a policy for each player individually, as in PSRO, Team PSRO trains a joint best response every iteration. This joint best response is a pair of policies that are trained using cooperative RL. In particular, we use MAPPO to train the joint best response. As in PSRO, once the joint best response for a team is trained, it is added to to that team's population \(_{}^{t}\) and a two-player zero-sum normal form restricted game \(G(_{}^{t},_{}^{t})\) is created. The actions of this normal form game correspond to pure joint strategies in the population and the payoffs correspond to the estimated expected value when one team chooses one joint policy and the other team chooses another joint policy \(u_{}(_{}(_{}),_{}^{r})\). Payoff values in the restricted game are estimated by sampling rollouts from both teams' joint policies. Team PSRO is described in Algorithm 2.

Note that when the joint best responses achieve expected value within \(\) of the optimal best response, then upon convergence Team PSRO will output an approximate TMECor. Even when the cooperative reinforcement learning is not within \(\) of the optimal best response, we find that in practice the reward that joint best responses achieve against the restricted NE tends to go down over time. We call this evaluation metric of the sum of the reward that both joint best responses achieve against the restricted NE _approximate exploitability_ because the this is a lower bound on the exact exploitability. The better the cooperative RL algorithm is in team PSRO, the more accurately the approximate exploitability will match true exploitability. As mentioned previously, a key benefit of Team PSRO is that it outputs approximate exploitability as a byproduct of the algorithm. If the RL algorithm is very strong and Team PSRO converges, then one can be reasonably sure that the strategy produced is strong.

The MAPPO algorithm we use contains a centralized critic value function that has access to the history of the game. Since we train the joint best response centrally but execute decentrally, MAPPO allows the critic to see the partner's cards at training time to reduce variance in the update. Another MAPPO trick we use is to share the parameters of the neural network between partners in a joint best response. This has been found to help stabilize training. Although we use MAPPO as our joint RL best response, in principle any cooperative RL algorithm could be used to learn an approximate joint best response.

### Team PSRO Mix-and-Match

Similarly to how Team DO-MM builds on Team DO by adding mixed-and-matched pure strategies, Team PSRO Mix-and-Match (Team PSRO-MM) builds on Team PSRO by adding mixed-and-matched deep RL policies. As in Team PSRO, once the joint best response for a team is trained, it is added to to that team's population \(_{}^{t}\). Similarly to Team DO-MM, an expanded two-player zero-sum normal form restricted game \(G((_{}^{t}),(_{}^{t}))\) is created. The actions of this normal form game correspond to mixed-and-matched joint strategies in the population and the payoffs correspond to the estimated expected value when one team chooses one joint policy and the other team chooses another joint policy \(u_{}(_{}(_{}),_{}^{r})\). Payoff values in the restricted game are estimated by sampling rollouts from both teams' joint policies. Team PSRO-MM is described in Algorithm 2.

``` Result: Approximate TMECorr Input: Initial population \(_{}^{0},_{}^{0}\) repeat{for\(t=0,1,\)} ifTeam-PSRO-MMthen \((_{}^{r},_{}^{r})\) NE in restricted game \(G((_{}^{t}),(_{}^{t}))\) else \((_{}^{r},_{}^{r})\) NE in restricted game \(G(_{}^{t},_{}^{t})\) for\(m\) iterations do  Update joint best response \(_{}\) toward \(_{}(_{}^{r})\) via cooperative RL  Update joint best response \(_{}\) toward \(_{}(_{}^{r})\) via cooperative RL \(_{}^{t+1}_{}^{t}\{_{}\}\) \(_{}^{t+1}_{}^{t}\{_{}\}\) until\(u_{}(_{}(_{}),_{}^{r})+u_{}(_{}^{r}, _{}(_{})) u_{}(_{}^{r},_{}^{r})+ u_{}(_{}^{r},_{}^{r})+\) Return:\((_{}^{r},_{}^{r})\) ```

**Algorithm 2** Team PSRO (Mix-and-Match)

## 5 Experiments

### Team DO Experiments

We run Team DO and Team DO-MM on two small games, Kuhn poker and Liar's dice, and present the results in Figure 1. Although we do not know of any existing regret bounds for double oracle techniques, these results demonstrate that in practice double oracle methods work well and can find approximate TMECor in a reasonable number of iterations. Notably, Team DO-MM is able to achieve faster convergence compared to Team DO in the number of iterations. However, we found that since calculating best responses in this game does not take much time, computing the extra payoff values caused Team DO-MM to converge much slower in wall clock time compared to Team DO.

Figure 1: Experimental results in tabular gamesAdditionally, both Team-DO methods outperform Fictitious Team Play (Farina et al., 2018), which adds team best responses to the opponent average strategy every iteration. In large games, as shown below, finding a best response requires much more time than evaluating the expected value of two team policies, so Team DO-MM converges faster in wall clock time as well. We include a detailed analysis of wall clock time on these tabular experiments in the appendix.

### Team PSRO Experiments

We demonstrate that Team PSRO can scale to large games by training it on the benchmark game of Google Research Football (GRF) (Kurach et al., 2020). Because there do not exist any other game-theoretic techniques for approaching TMECor in large games, we only compare to self play. Although we do not know of any theoretical guarantees that self play will converge to TMECor, methods based on self play (Berner et al., 2019; Liu et al., 2019; Jaderberg et al., 2019) have achieved impressive performance on large team games. As a result, we believe that self-play reinforcement learning is a strong benchmark to beat. In these experiments we use the four-player version of GRF and modify it so that each player only observes the information of his own team, the ball, and three closest opponents' information (which means that there are always two opponents that are not observable for a player). We also include results on the perfect-information version of the game in the Appendix and find that the same pattern holds where Team PSRO-MM is better than Team PSRO which is better than self-play. We also include further training details in the Appendix.

We use the same reinforcement learning algorithm for both Team PSRO and self play, namely MAPPO with a centralized critic and shared parameters for a team. We allow the centralized critic to see all players' information. Self play reinforcement learning continually trains one joint policy for one team against the other joint policy for the other team and does not make use of a population like Team PSRO. In Figure 2, we see that Team PSRO and Team PSRO-MM are both able to outperform self-play across all three measures of Elo, average goal difference against the built-in AI, and relative population performance (Balduzzi et al., 2019). Furthermore, Team PSRO-MM outperforms Team PSRO with a small additional cost of extra rollouts to evaluate the matrix payoffs. Since this is such a large game, the time needed to compute the BRs is much more than the time needed to evaluate two team policies, so the extra time is negligible.

## 6 Discussion

### Limitations

As with any double oracle algorithm, the main limitation of our algorithm is that it suffers from a lack of practical guarantees. It is possible that all pure strategies of the game must be expanded before termination, and indeed we find that in Kuhn poker, even using an exact best response plateaus indefinitely. With that being said, sometimes exploitability can be too strict of a measure if the exact best response is hard to find. In the case of team games, where finding an exact best response is NP-Hard, perhaps being approximately unexploitable against a reinforcement learning best response is a reasonable goal, and one that we achieve with Team-PSRO.

Figure 2: Team PSRO Results on Google Research Football. Both Team PSRO and Team PSRO-MM outperform self play, with Team PSRO-MM performing the best.

### Future Work

There are many future directions to build on this work. First, we are interested in continuing to scale up to large games, with the goal of becoming superhuman at bridge. A second direction of future work involves studying the problem of cooperative reinforcement learning directly, where improvements should directly transfer to our setting. A third research direction is transferring over new PSRO algorithms such as Anytime PSRO (McAleer et al., 2022b), and NXDO (McAleer et al., 2021). Related efforts in solving team games using regret minimization and subgame solving are possibly complimentary to our approach.

## 7 Acknowledgements

This material is based on work supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award A240108S001.