# Category-Extensible Out-of-Distribution Detection via Hierarchical Context Descriptions

Kai Liu\({}^{1,2}\), Zhihang Fu\({}^{2}\), Chao Chen\({}^{2}\), Sheng Jin\({}^{2}\), Ze Chen\({}^{2}\),

**Mingyuan Tao\({}^{2}\), Rongxin Jiang\({}^{1}\), Jieping Ye\({}^{2}\)**

\({}^{1}\)Zhejiang University, \({}^{2}\)Alibaba Cloud

Work done during Kai Liu's research internship at Alibaba Cloud. Email: kail@zju.edu.cn.Corresponding authors. Email: rongxinj@zju.edu.cn, zhihang.fzh@alibaba-inc.com.

###### Abstract

The key to OOD detection has two aspects: generalized feature representation and precise category description. Recently, vision-language models such as CLIP provide significant advances in both two issues, but constructing precise category descriptions is still in its infancy due to the absence of unseen categories. This work introduces two hierarchical contexts, namely perceptual context and spurious context, to carefully describe the precise category boundary through automatic prompt tuning. Specifically, perceptual contexts perceive the inter-category difference (e.g., cats vs apples) for current classification tasks, while spurious contexts further identify spurious (similar but exactly not) OOD samples for every single category (e.g., cats vs panthers, apples vs peaches). The two contexts hierarchically construct the precise description for a certain category, which is, first roughly classifying a sample to the predicted category and then delicately identifying whether it is truly an ID sample or actually OOD. Moreover, the precise descriptions for those categories within the vision-language framework present a novel application: CATegory-EXtensible OOD detection (CATEX). One can efficiently extend the set of recognizable categories by simply merging the hierarchical contexts learned under different sub-task settings. And extensive experiments are conducted to demonstrate CATEX's effectiveness, robustness, and category-extensibility. For instance, CATEX consistently surpasses the rivals by a large margin with several protocols on the challenging ImageNet-1K dataset. In addition, we offer new insights on how to efficiently scale up the prompt engineering in vision-language models to recognize thousands of object categories, as well as how to incorporate large language models (like GPT-3) to boost zero-shot applications.

## 1 Introduction

Out-of-distribution (OOD) detection focuses on determining whether an input image is in-distribution (ID) or OOD, while classifying the ID data into respective categories . The key to OOD detection has two aspects: (1) constructing a sufficiently generalized feature representation capability, so that images of arbitrarily different categories can be roughly separated from each other regardless of semantic shifts ; and (2) acquiring precise descriptions (namely decision boundary) for each ID category in the image feature space (in CNNs the last fully-connected layer plays this role), so as to determine whether the input image belongs to the corresponding category or just OOD .

Specifically, generalized feature representation comes from large-scale and diverse training data for learning-based data-driven models. Therefore, OOD detection works have made great progress on the basis of large-scale pre-trained models such as ResNet  and ViT . To achievethe precise description for a certain ID category, on the other hand, we need to let the model acquire as much "prior knowledge" as possible. For instance, if the description comes from a binary classifier that has only seen _cat_ and _apple_ during training, the description of category _cat_ is obviously not precise enough to distinguish a _panther_ image as an OOD: it lacks prior knowledge to distinguish different quadruped mammals.

Recently, vision-language models such as CLIP  provided significant advances in the two key issues above for OOD detection. Huge corpora of paired image-text data training brings both powerful feature representation capabilities and more comprehensive prior knowledge than single-vision-modal training [71; 37]. Therefore, CLIP-based OOD detection methods have been proposed successively, such as the zero-shot method MCM  and the encoder fine-tuning method NPOS . However, there still exist the issues of generalization of feature representations and precision of category descriptions under the multi-modal framework.

Our observations can be summarized into two aspects: 1. The performance of the zero-shot CLIP-based OOD methods  is limited. As CLIP is trained by contrastive learning with informative image-text caption pairs, simply using the category names as all text information constrains the potential image-text discriminability. 2. Even though fine-tuning CLIP's encoder may boost the performance [18; 50], the generalization of multi-modal feature representation is sacrificed. In other words, such methods impair the model's ability to resist data shifts. Although the existing works are promising and inspiring to academia, constructing precise category descriptions via CLIP-like multi-modal features for OOD detection is still in its infancy .

Based on the observations above, we take a step towards generalized OOD detection by seeking the precise description prompt for each category and meanwhile maintaining the representation capacity. To this end, we develop a pair of perceptual context and spurious context for each in-distribution category to hierarchically construct the precise category description. CLIP encoders are kept frozen, and the contexts are learned through automatic prompt tuning.

As illustrated in Fig. 1, perceptual contexts first roughly classify a sample to the predicted category (colored regions), and spurious contexts then delicately identify whether it is truly an ID sample or actually spurious OOD (gray regions). Specifically, perceptual contexts perceive the descriptions of inter-category differences across all ID categories, while spurious contexts are relatively independent of the specific ID data task. The latter models a more strictly defined description of the current category itself by training on both real ID data and well-designed adversarial samples. We also introduce a robust sample synthesis strategy that leverages the prior knowledge of the VLM to select the qualified syntheses inspired by [50; 60].

We conduct extensive experiments to show the proposed hierarchical context descriptions are crucial to precisely and universally define each category. As a result, our method consistently outperforms the competitors on the large-scale OOD datasets, while showing comparable or even better generalization than the remarkable zero-shot methods. With the vision-language prompting framework, the precise and universal category descriptions via hierarchical contexts present a novel application: CATegory-EXtensible OOD detection (CATEX). We then merge the context descriptions learned from different

Figure 1: **Method comparison. Compared to previous approaches, our method utilizes the perceptual context to classify different categories under the current ID task (solid lines), and leverages the spurious context to strictly define the category boundaries independent of the current setting (dashed lines). The hierarchical perceptual and spurious contexts jointly describe the precise and universal boundaries for each category (combination of solid and dashed lines).**

task data, and directly test on the union ID setting. The competitive results demonstrate that the learned descriptions can be used across tasks. Consequently, to illustrate the category-extensibility, we incrementally extend the context descriptions to the whole ImageNet-21K categories at an acceptable GPU memory cost, and achieve superior performance to rivals. In addition, CATEX offers new insights on how to incorporate large language models (like ChatGPT) to boost zero-shot classifications (_e.g._, implicitly constructing spurious contexts to perform one-class OOD detection).

In summary, we make the following contributions:

1. We construct the perceptual and spurious contexts to hierarchically describe each category to perform ID/OOD detection. Our method consistently surpasses the SOTA method on the challenging ImageNet-1K benchmark, leading to an 8.27% decrease in FPR95.
2. We empirically demonstrate the proposed hierarchical contexts are prone to learning precise category descriptions while keeping the generalized feature representation. When a data shift occurs, our method shows stable robustness on both ID classification and OOD tasks.
3. We present a novel category-extensible (CATEX) OOD detection framework. By simply merging the successively learned hierarchical contexts, we show that precise descriptions enable cross-task ID classification and OOD detection. We hope to offer new insights on how to scale up the prompt engineering in VLMs to recognize thousands of categories, as well as how to incorporate LLMs to boost zero-shot applications.

## 2 Preliminaries

This paper considers the multi-class classification as the in-distribution (ID) situation, and the OOD detection task is formalized as follows.

**Notations.** Let \(^{H W 3}\) denote the input image space and \(^{in}=\{1,2,,C\}\) denote the ID label space of \(C\) categories in total. The labeled training set \(^{in}_{tr}=\{(I_{i},y_{i})\}_{i=1}^{n}\) is drawn _i.i.d._ from the joint data distribution \(_{^{in}}\). Let \(_{}\) denote the marginal distribution on \(\), which is referred to as the in-distribution (ID). For ID classification task, let \(f:^{C}\) denote a function predicting the label of an input sample by \(_{i}=argmax_{k}\,f[k](I_{i})\).

**In-distribution classification.** In vision-language models, let \(^{d}\) denote the \(l_{2}\)-normalized \(d\)-dimension visual feature of input images via the image encoder: \(=(I)\), the mapping function \(f\) can be expressed as \(f(I)=W^{T}(I)=W^{T}\), where \(W=[_{1};_{2};;_{C}]^{T}^{d \,C}\) is the collection of \(l_{2}\)-normalized text features for \(C\) categories. Specifically, each text feature is extracted from the category description via text encoder: \(_{k}=([v_{k,1};v_{k,2};;v_{k,m};_{k}]) ([_{k};_{k}])\), where \(_{k}\) presents the \(m\) word embedding vectors for \(k\)-th ID category and \(_{k}\) is the given category name. The ID-label prediction becomes \(=argmax_{k}\,_{k}^{T}\,=argmax_{k}\, _{k},\).

**Out-of-distribution detection.** When an OOD sample \(\) with unknown category \(y^{in}\) emerges at test time, it should not be predicted to any known category. Hence, the OOD detection task can be viewed as a binary classification problem. Let \(g:^{1}\) denote a function distinguishing between the in- _v.s._ out-of-distribution samples. OOD detection estimates the lower level set \(:=\{:g()\}\), where samples with lower scores \(g()\) are detected as OOD and vice versa. The threshold \(\) is typically chosen by ensuring a high fraction of ID data (_e.g._, 95%) is correctly preserved.

## 3 Method

From the data-distribution perspective, the key to OOD detection is to acquire precise descriptions for each ID category to determine the distribution boundary. Inputs beyond the specific boundary should belong to either OOD samples or other ID categories. However, it has always been a challenge [22; 17; 38]. In conventional methods using vision or multi-modal models, one may fine-tune the image encoder to reshape the feature representation space. Then, the classification layer (for vision models) or text features (for VLMs) play the role in category descriptions, where calculated logits measure the relative distances between inputs and corresponding ID categories. However, due to the absence of unseen categories, both the over-fitted feature representations  and the ID-label-biased category descriptions  are prone to overconfident predictions on unseen OOD data .

To alleviate these problems, we take the vision-language prompting framework  to learn precise and universal descriptions for each ID category. First, both the image and text encoders are frozen to preserve the generalized representation capacity. Second, we propose to hierarchically construct the perceptual and spurious context for each category to mitigate label bias.

In the following sections, we present the proposed CATEX by elaborating on the following questions: 1. How do perceptual and spurious contexts model the precise and universal category descriptions (Sec. 3.1)? 2. How to learn such hierarchical contexts (Sec. 3.2)? 3. How to leverage the precise category descriptions for ID classification and OOD detection (Sec. 3.3)? The overview framework of our method is illustrated in Fig. 2.

### Hierarchical Contexts

Recall that empirical risk minimization (ERM)  operates under the closed-world assumption. When an OOD sample \(_{}\) emerges, the ID classifier might produce overconfident predictions as \(=argmax_{k}_{k}^{p},\), where \(_{k}^{p}=([_{k}^{p};])\) is the encoded text feature for \(k\)-th ID category. Here \(_{k}^{p}\) is denoted as **perceptual context** that perceives the original multi-category classifications. However, with \(_{k}^{p}\) only, whether \(\) actually belongs to the \(k\)-th ID category or is just an unseen OOD semantically close to this category is still unclear. Hence, we propose to add a hierarchical context to model such OOD samples as another spurious category. The image-text similarity is measured by \(_{k}^{s},\), where \(_{k}^{s}=([_{k}^{s};])\). Here \(_{k}^{s}\) is denoted as **spurious context** that identifies such semantically similar but spurious OOD samples. We propose to utilize such two types of hierarchical contexts to jointly model the precise and universal boundaries for each category.

First, perceptual context serves to carefully adapt to the current ID classification task. As we propose to model a potentially spurious category for \(C\) ID categories respectively, the total category number is equivalent to grows to \(C+C=2C\) in the scope. When an ID sample \((_{i},y_{i})\) emerges, to learn a more strict classification boundary, we put it apart from both other ID categories and their corresponding spurious categories by:

\[^{p},^{s}}{argmin}_{i} [-_{y_{i}}^{p},_{i})}}{e^{(_ {y_{i}}^{p},_{i})}+_{k y_{i}}^{C}e^{(_{k}^{p}, _{i})}+_{k y_{i}}^{C}e^{(_{k}^{s},_{i})}}]\] (1)

Then, spurious context is hierarchically combined to describe the specific category boundary to handle the OOD detection. To formulate the open-world situation, when an ID sample \(\) or OOD sample \(\) with the predicted category \(\) emerges, the OOD detection risk can be expressed as:

\[R=_{_{}}[\{ _{}^{p}<_{}^ {s},\}]+_{_{ }}[\{_{}^{p}, >_{}^{s},\}]\] (2)

Figure 2: **Illustration of our method.** Perceptual context perceives a certain ID category, and spurious context explicitly describes a spurious category around this ID category. Random perturbation is applied to the perceptual context for synthesizing outliers to train the non-trivial spurious context. The hierarchical perceptual and spurious contexts jointly describe the precise category boundary.

For each ID category, if we can proactively draw the spurious OOD samples and their category-predictions as \(^{s}_{tr}=\{(}_{i},_{i})\}_{i=1}^{}\), the risk in Eq. (2) can be empirically minimized by:

\[^{p},^{s}}{argmin}& _{i}[-^{p}_{y_{i}}, }_{i}}}{e^{^{p}_{y_{i}},}_{i}}+e^{^{s}_{y_{i}},}_{i }}}]+}_{i}[-^{s}_{y_{i}},}_{i}}}{e^{^{ p}_{y_{i}},}_{i}}+e^{^{s}_{y_{i}}, }_{i}}}]\] (3)

In short, based on Eq. (1), perceptual context first learns to model a more strict inter-class decision boundary. Hierarchically, combining with Eq. (3), perceptual context and spurious context jointly model the precise category boundary, which is the key to OOD detection. More details on the whole training procedure are provided in Appendix A.1.

Intuitively, how to draw the spurious training set \(^{s}_{tr}\) is crucial. This paper presents a well-designed sampling strategy to learn the hierarchical contexts.

### Perturbation Guidance

Generating adversarial data samples has been widely studied in recent years [39; 43; 72], among which feature-space sampling is proven more tractable [15; 50]. In general, previous approaches randomly synthesize spurious samples away from ID features' clustering centers (with low likelihood or large distance), and treat them as visual outliers. To improve the quality of spurious syntheses, we provide a perturbation-guided sampling strategy that leverages the prior knowledge of the pre-trained large-scale vision-language models.

In practice, an unseen spurious sample generally shares part of visual characteristics with the ID images, and the rest remains different . Under the vision-language prompting framework, in the learned perceptual context of a certain ID category \(^{p}_{k}=[v^{p}_{k,1};v^{p}_{k,2};;v^{p}_{k,m}])\), each word embedding \(v^{p}_{k,j}\) actually describes a visual characteristic for this category . According to the contrastive learning objective during the pre-training process on image-text pairs, if the text is perturbed, it should not be paired with the original image. Correspondingly, if the learned word embeddings are perturbed, the encoded text feature should also not describe images from the original ID category. Thus, as shown in Fig. 3, we can proactively leverage the perturbation to guide the spurious syntheses to boost context learning. Specifically, after randomly applying a perturbation \(u\) (_e.g._, masking ) onto the word \(v^{p}_{k,j}\) of perceptual context, the perturbed text feature becomes \(}^{p}_{k}=([v^{p}_{k,1};;u;;v^{p}_{k,m}; _{k}])\). Rather than the original ID category, \(}^{p}_{k}\) now describes a kind of unknown spurious category, of which the images should share higher similarities with perturbed text feature \(}^{p}_{k}\) than the original \(^{p}_{k}\). Hence, we propose to re-sample the randomly synthesized spurious candidates by:

\[^{s}_{tr}=\{(}_{i},_{i}):}^{p}_{_{i}},}_{i}>^ {p}_{_{i}},}_{i}\}\] (4)

Combining with Eq. (3) and Eq. (1), the spurious context \(^{s}_{k}\) is able to learn to describe the synthesized spurious samples with such perturbation. After numerous iterations, random perturbations on perceptual contexts bring diverse spurious syntheses, which lead to learning non-trivial spurious contexts. More details on perturbation guidance (_e.g._, perturbed ratio) are discussed in Appendix A.2.

After learning the hierarchical perceptual and spurious contexts for each category, we are able to probe the precise and universal ID category boundaries.

### Integrated Inference

After learning the hierarchical perceptual and spurious contexts, to further alleviate the overconfidence predictions on unseen OOD samples, we propose to regularize the vanilla image-text similarities by:

\[r_{k}=^{p}_{k},^{p}_{k},}}{e^{^{p}_{k}, }+e^{^{s}_{k},}} s_{k} _{k}\] (5)

Figure 3: Guiding process.

Eq. (5) is a unified measurement. For ID classification, the category is predicted by \(=argmax_{k}r_{k}\). For OOD detection, the commonly-used scoring function [38; 50] is adopted: \(g()=-\ max}}{_{j}^{n}e^{}}\).

The motivation is intuitive. When perceptual context \(^{p}\) produces a relatively higher image-text similarity \(s\) on unseen OOD samples or misclassified ID inputs, the hierarchical spurious context \(^{s}\) could reduce the overconfident similarity by the regularization item \(\). More empirical experiments are presented in the following sections.

## 4 Experiment

In this section, we empirically validate the effectiveness of our CATEX on real-word large-scale classification and OOD detection tasks. The setup is described below, based on which extensive experiments and analysis are displayed in Sec. 4.1-Sec. 4.2.

**Datasets.** Following the common benchmarks in the literature [59; 50; 60; 38], we mainly consider the large-scale ImageNet  as the in-distribution data. Subsets of iNaturalist , SUN, Places, and Texture are adopted as the OOD datasets. The categories in each OOD dataset are disjoint with the ID dataset [25; 38; 50]. This paper investigates four practical scenarios in real-world application of ID classification and OOD detection tasks: (1) _standard OOD detection_, (2) _ID-shifted OOD detection_, (3) _category-extended ID classification and OOD detection_, and (4) _zero-shot ID classification_. Details are presented in Sec. 4.1.

**Evaluation metrics.** For OOD detection, two metrics are used: (1) FPR95, the false positive rate of OOD samples when the true positive rate of ID samples is 95%, and (2) AUROC, the area under the receiver operating characteristic curve. For ID classification, we report the mean accuracy (ACC).

**Implementation details.** Without loss of generality, our method is implemented based on the vision-language prompting framework  with the CLIP model , one of the most popular and publicly available pre-trained models. CLIP aligns an image and its corresponding textual description in the feature space through a contrastive objective. We mainly take CLIP-B/16 in our experiments, which comprises of a ViT-B/16 Transformer  as the image encoder and a masked self-attention Transformer  as the text encoder. During training, all parameters of the image and text encoders are frozen. We only learn a pair of perceptual and spurious contexts for each ID category. Following the default setting , each context consists of 16 learnable 512-D prompt embeddings, which are trained for 50 epochs using the SGD optimizer with a momentum of 0.9. The initial learning rate is 0.002, which is decayed by the cosine annealing rule. The contexts are optimized with synthesized samples, and no surrogate OOD datasets are involved during training.

### Main Results

**CATEX significantly improves _standard OOD detection_.** We first compare the proposed CATEX with competitive OOD detection methods that also delve into describing the ID category boundary. In particular, MCM  is the latest one to adopt the pre-trained CLIP to perform the ID boundary description in a zero-shot way, and others turn to fine-tune the encoders to optimize the ID boundary. The results are presented in Tab. 1, where the best performance is marked **bold**, and the

    &  &  \\   &  &  &  &  &  \\   & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\  MCM  & 32.08 & 94.41 & 39.21 & 92.28 & 44.88 & 89.83 & 58.05 & 85.96 & 43.55 & 90.62 \\  MSP  & 54.05 & 87.43 & 73.37 & 77.80 & 72.98 & 70.83 & 68.85 & 79.06 & 67.31 & 80.64 \\ Energy  & 29.75 & 94.68 & 53.18 & 87.33 & 56.40 & 58.60 & 51.35 & 88.00 & 47.67 & 88.90 \\ ViM  & 32.19 & 93.16 & 54.01 & 87.19 & 60.67 & 83.75 & 53.94 & 87.18 & 50.20 & 87.82 \\ KNN  & 29.17 & 94.52 & 35.62 & 92.67 & **39.61** & **91.02** & 64.35 & 85.67 & 42.19 & 90.97 \\ SSD+ & 59.60 & 85.54 & 75.62 & 73.80 & 83.60 & 68.11 & 39.40 & 82.40 & 64.55 & 77.46 \\ DOE  & 55.87 & 85.98 & 80.94 & 76.26 & 67.84 & 83.05 & 34.67 & 88.90 & 59.83 & 83.54 \\ VOS  & 31.65 & 94.53 & 43.03 & 91.92 & 41.62 & 90.23 & 56.67 & 86.74 & 43.24 & 90.86 \\ NPOS  & 16.58 & 96.19 & 43.77 & 90.44 & 45.27 & 89.44 & 46.12 & 88.80 & 37.93 & 91.22 \\
**Ours** & **10.18** & **97.88** & **33.87** & **92.83** & 41.43 & 90.48 & **33.17** & **92.73** & **29.66** & **93.48** \\   

Table 1: OOD detection performance for ImageNet-1k  as ID dataset.

second best is underlined. Accordingly, our CATEX achieves superior OOD detection performance, outperforming competitive rivals by a large margin. Specifically, CATEX consistently surpass the SOTA method NPOS  on all of the four OOD datasets, leading to an 8.27% decrease in FPR95 and 2.24% increase in AUROC on average. It implies that compared to zero-shot probing and encoder-fine-tuning, learning the hierarchical perceptual and spurious contexts for each category is more effective in acquiring precise ID boundaries. In addition, our method dose not conflict with other post-hoc approaches (such as ReAct  and ASH ), and appropriate combinations may bring further improvements (_e.g._, **27.56%** of FPR95), as discussed in Appendix B.2.

**CATEX properly generalizes to _ID-shifted OOD detection_.** If the description of each category is precise enough, it will not only perform well in OOD tasks, but also be discriminative even when the distribution of ID data is shifted. Hence, we evaluate both the ID classification and OOD detection ability of models trained on ImageNet-1K  while tested on shifted ID datasets including ImageNet-A , ImageNet-R , and ImageNet-Sketch . As shown in Tab. 2, CATEX reaches the best ID and OOD performance on both ImageNet-A and ImageNet-R and competitive results on ImageNet-Sketch. It indicates that the generalization ability from the large pre-trained models is well-maintained and even straightened. The fine-tuned models like NPOS are unstable across different datasets, which may be a sign of generalization degradation caused by parameter fine-tuning. Because the data shift is inevitable in real-world applications, we suggest that the proposed hierarchical contexts are instructive for model generalization.

**CATEX carefully adapts to _Category-extended ID classification and OOD detection_.** If the hierarchical contexts learn a precise category description, it should prevent overconfident predictions on any unseen samples in open-world but beyond the current ID task scope. We thus conduct a cross-task experiment to further evaluate the open-set discriminability. Specifically, two models are independently trained on ImageNet-100 (I)  and another disjoint ImageNet-100 (II) randomly selected from ImageNet-1K. Then, models are directly tested on the union ImageNet-200 (I \(\) II). OOD detection is simultaneously evaluated, and the results are displayed in Tab. 3.

Accordingly, our CATEX achieves the highest performance on the union ImageNet-200 and significantly surpasses the competitors (_e.g._, 3.38% increase on accuracy and 12.2% decrease on FPR95). In the category-extended scenario, our classification accuracy only drops by 4%, which is much lower than the fine-tuning methods (_e.g._, 8% decrease from VOS ). It implies that fine-tuning the encoder is prone to overfitting on seen training samples, and sacrificing the ability (obtained from

    &  \\   &  &  &  \\   & ACC\(\) & FPR95\(\) & AUROC\(\) & ACC\(\) & FPR95\(\) & AUROC\(\) & ACC\(\) & FPR95\(\) & AUROC\(\) \\  MCM  & 50.61 & 71.52 & 80.65 & 75.95 & 52.67 & 89.49 & 47.42 & **73.73** & **80.74** \\  MSP  & 43.96 & 86.25 & 63.25 & 66.50 & 82.21 & 75.07 & 45.18 & 91.67 & 58.45 \\ Energy  & 43.96 & 89.78 & 63.96 & 66.50 & 89.55 & 68.43 & 45.18 & 95.56 & 54.32 \\ VOS  & 43.79 & 80.03 & 73.45 & 66.83 & 78.42 & 80.09 & 46.06 & 86.68 & 68.58 \\ NPOS  & 50.16 & 74.57 & 75.37 & 73.38 & 75.64 & 82.71 & **50.29** & 82.87 & 71.56 \\
**Ours** & **50.87** & **71.13** & **81.04** & **76.67** & **51.75** & **89.75** & 48.59 & 74.68 & 80.69 \\   

Table 2: Generalization across ID domains. Models are trained on ImageNet-1K  and directly tested on shifted ID datasets.

    &  \\   &  &  &  \\   & ACC\(\) & FPR95\(\) & AUROC\(\) & ACC\(\) & FPR95\(\) & AUROC\(\) & ACC\(\) & FPR95\(\) & AUROC\(\) \\  MCM  & 89.00 & 24.38 & 95.59 & 90.48 & 19.84 & 96.44 & 83.35 & 27.18 & 94.88 \\  MSP  & **94.70** & 41.34 & 93.39 & **94.82** & 38.22 & 93.75 & 87.69 & 60.93 & 87.26 \\ Energy  & **94.70** & 32.11 & 94.36 & **94.82** & 32.95 & 93.86 & 87.69 & 37.78 & 92.62 \\ VOS  & 94.68 & 25.19 & 95.60 & 94.72 & 20.97 & 96.01 & 86.14 & 34.16 & 93.42 \\ NPOS  & 94.24 (0.00) & 16.54 (0.00) & 96.62 & 94.32 (0.00) & 16.84 (0.00) & 96.35 & 86.23 (0.00) & 25.54 (0.00) & 94.35 \\
**Ours** & 94.12 (0.12) & **10.31** (6.23) & **97.82** & 94.42 (0.10) & **7.91** (8.93) & **98.31** & **89.61** (4.38) & **13.34** (1.220) & **97.19** \\   

Table 3: Generalization across ID tasks. Models are independently trained on disjoint ImageNet-100 (I) and ImageNet-100 (II), and then tested on the union ImageNet-200 (I \(\) II) without fine-tuning.

large-scale pre-training) to distinguish unseen categories. We randomly select five categories from each ImageNet-100 subset respectively, whose feature distribution shown in Fig. 4 is consistent with Tab. 3. Moreover, compared to the SOTA method NPOS, our CATEX outperforms more on the union ImageNet-200 than separated ImageNet-100 subsets (_e.g._, 12.2% v.s. 6.23% decrease on FPR95). It further demonstrates that in each ImageNet-100 subset, besides our learned perceptual contexts that classify the current 100 categories, the spurious contexts are able to identify the unseen categories (both of the new-coming ID samples and OOD samples), as illustrated in Fig. 4. In this way, the precise boundaries for each category are acquired. On the other hand, to deal with such category-extensible tasks in real-world practice , this paper provides a new perspective: separately learning the precise descriptions for the new-coming categories besides the existing ones, and jointly testing on the full ID category scope.

To further evaluate the efficacy of the precise descriptions, we conduct a more challenging larger-scale category-extensible experiments: scaling up ImageNet-1K to ImageNet-21K . In particular, we randomly split the huge ImageNet-21K dataset into several subsets, individually train the hierarchical perceptual and spurious contexts for each category on each subset, and concurrently test on the full ImageNet-21K. Such paradigm is similar to category-incremental learning, and the results are shown in Fig. 5, where we mainly compare with zero-shot CLIP  and baseline CoOp . According to Fig. 5, our CATEX consistently outperforms CLIP and CoOp. Specifically, when the total number of categories extends, CoOp has a higher accuracy drop, since its learned contexts on each separated subset share limited discriminability to novel categories across subsets. And training on twenty-thousand categories together is challenging due to the generally prohibitive computational costs, which require over _300 GB_ GPU memory for the text-encoder on 21K categories. To alleviate this problem, this paper proposes to learn the hierarchical contexts to describe the precise and universal category boundaries, and achieves 38% accuracy on the full ImageNet-21K with a single V100-32G GPU card. We hope to offer new insights to the community on how to adopt large-scale VLMs to classify numerous categories with limited GPU resources.

**CATEX encouragingly boosts _zero-shot ID classification_.** Under the category-extended scenario, the hierarchical contexts help establish the category boundary for each given category, and prevent overconfidence on other unseen categories by adjusting the image-text similarity \(s\) with the spurious-context-regularized item \(\), as introduced in Eq. (5). Intuitively, in zero-shot classification scenarios, the regularized score \(r=s\) may also rectify the category predictions and lead to a higher classification accuracy, with proper perceptual and spurious contexts. To verify it, as CLIP's default prompt template "A photo of a [CLS]" contains no visual information, we adopt rich visual descriptions from large language models  (such as GPT-3 ) as our perceptual context \(_{p}\) to perform classification (denoted as _VisDesc_). Then, we randomly perturb the visual description to simulate spurious contexts \(_{s}\), and regularize text-image similarities via Eq. (5). The results shown in Tab. 4 indicate the regularized score \(r\) brings a higher classification accuracy, without any training

Figure 4: **Feature visualization by t-SNE. (a) Previous approaches that fine-tune the encoders may distort the generalized feature space and make unseen OOD samples inseparable; (b) instead, our method freezes the encoders to maintain the discriminability. (c) Compared with traditional prompting methods using a single perceptual context only, (d) our spurious context provides a better metric for unseen OOD detection.**

Figure 5: Scaling up ImageNet-1K  to ImageNet-21K  with category-incremental learning.

cost. We hope to provide new insights that in the category-extensible setting or a true zero-shot classification scenario, explicitly constructing spurious contexts can perform the one-class OOD detection task, as other categories can also be viewed as OOD for a certain ID category.

### Ablation Studies

In this section, we take ImageNet-100  as the ID dataset, and other settings follow Sec. 4.1. We mainly verify the key contributions of this work in Tab. 5, and the influence on model capacity in Tab. 6. In particular, we evaluate the number of spurious contexts used to help probe the category boundary in Tab. 7. More experiments and discussions are displayed in Appendix A and Appendix B.

**The proposed components are effective**. In Tab. 5, a baseline model is first constructed following the vision-language prompt-learning framework CoOp , which only learns the perceptual contexts with the vanilla softmax cross-entropy loss. Then, we learn the hierarchical perceptual and spurious contexts using initially synthesized OOD samples with Eq. (1) and Eq. (3). According to Tab. 5, the OOD detection performance is improved immediately, whereas the ID classification accuracy slightly drops by 0.3%. It indicates that the quality of initial OOD syntheses is limited, which puts the learned descriptions at risk of identifying ID inputs as OOD or even other ID categories. On the contrary, the proposed perturbation guidance mechanism in Eq. (4) provides extra constraints on OOD syntheses to improve the quality, which brings consistent gains in both ID classification and OOD detection protocol. Finally, the integrated inference strategy in Eq. (5) brings free improvement on FPR95 without loss of ID accuracy. The efficacy of our proposed method is further demonstrated.

**Our method is scalable to model capacity**. In Tab. 6, we investigate the scalability of our method with different model capacities, _i.e._, a lighter model with modified ResNet50 (RN50) and a heavier model with ViT-L/14 as the image encoder. The results imply larger models indeed lead to superior performance, suggesting larger models are endowed with better representation qualities [38; 50]. Interestingly, though the zero-shot method MCM  with ViT-L/14 surpasses our method with RN50 by 1.1% of ID classification accuracy, the OOD performance is inferior to ours (_e.g._, over 1.2% increase on FPR95). It further demonstrates that learning precise category descriptions via our hierarchical contexts is still a key to deriving a better OOD detector in the open-world.

**Multiple spurious contexts bring more precise descriptions**. For each ID category (_e.g._, cat), the spurious OOD samples can be diverse (_e.g._, panthers, lions, _etc_). Thus, we aim to only describe the spurious sample surrounding the ID category, rather than the whole OOD space (which is too complicated). According to common sense, it is intuitive to use more spurious contexts to describe better category boundaries. To verify it, we have tested the number of spurious contexts for each ID category (denoted as _W/o Constraints_), and the results shown in Tab. 7 implies using more spurious contexts only leads to 0.1% gain on performance. The reason may be that the learned 2 or more spurious contexts are too redundant without any constraints. To alleviate this problem, we simply add an orthogonal constraint (making the similarities between each two spurious contexts close to zero) (denoted as _W/ Constraints_), and the OOD detection performance is significantly boosted, as displayed in Tab. 7. Therefore, how to effectively and efficiently leverage more spurious contexts to better describe the category boundary deserves further exploration, and we view it as our future work.

   Model & Method & ID-ACC \(\) & FPR95\(\) & AUROC\(\) \\   & MCM & 84.26 & 32.31 & 94.61 \\  & Ours & **90.58** & **19.58** & **96.25** \\   & MCM & 91.66 & 20.79 & 96.35 \\  & Ours & **96.18** & **6.97** & **98.43** \\   

Table 6: Ablation study on model capacity.

   Method & Prompt Example & ACC-Top1\(\) & ACC-Top5\(\) \\  CLIP & A photo of a goldfish. & 63.50 & 88.99 \\ VisDesc & A yellow fish with a long flowing tail is goldfish. & 65.47 & 90.14 \\ Ours & + A [MASK] fish with a long [MASK] [MASK] is goldfish. & **65.84** & **90.36** \\   

Table 4: Zero-shot classification via text-image similarity regularized by simulated spurious contexts.

   Model & Method & ID-ACC \(\) & FPR95\(\) & AUROC\(\) \\   & MCM & 84.26 & 32.31 & 94.61 \\  & Ours & **90.58** & **19.58** & **96.25** \\   & MCM & 91.66 & 20.79 & 96.35 \\  & Ours & **96.18** & **6.97** & **98.43** \\   

Table 5: Ablation study on proposed framework.

## 5 Related Works

**OOD detection in computer vision.** To keep the safe deployment of multi-category classification models in open-world scenarios, the goal of OOD detection is to derive a binary ID-_v.s._-OOD detector for the visual inputs. To reduce the overconfidence on unseen OOD samples , a surge of post-hoc scoring functions has been devised based on various information, including output confidence [21; 32; 25], free energy [33; 58; 15], Bayesian inference [30; 34; 5], gradient information , model/data sparsity [46; 73; 12], and visual distance [31; 49; 50]. Another promising approach is adding open-set regularization in the training time [36; 22; 26; 52], making models produce lower confidence or higher energy on OOD data. Manually-collected [22; 60] or synthesized outliers [15; 50] are required for auxiliary constraints. Works insofar have mostly focused on regularizing the task-specific classification models trained on downstream scenarios using only visual information. In contrast, this paper explores the OOD detection on category-extensible scenarios, which incorporates rich textual information to probe the task-agnostic category boundaries.

**OOD detection with vision-language models (LVMs).** Exploring textual information for visual OOD detection with large-scale pre-trained models has recently attracted a surge of interest. One may simply use the vision-language model CLIP  to explicitly collect potential OOD labels [18; 16] or conduct zero-shot OOD detection  with manually pre-defined text prompts. However, as such manual text prompts lack the description of unseen categories, the prior knowledge learned by VLMs during pre-training is not fully exploited, leading to inferior OOD detection performance. On the contrary, we propose to learn hierarchical contexts to describe the category boundaries against unseen spurious categories via vision-language prompt tuning . RPL  and ARPL  explored a similar idea of "Reciprocal Points" to constrain a more compact in-distribution feature space, which may distort the generalized representation like NPOS . Our method largely boosts the OOD detection performance, and simultaneously maintains the generalization capacity of pre-trained VLMs.

## 6 Conclusion

This paper proposes a novel framework to learn the precise category boundaries via the hierarchical perceptual and spurious contexts. Specifically, the perceptual context first perceives a relatively more strict classification boundary on the current ID task, and then integrates the spurious context to determine a more precise category boundary that is independent of the current task setting. The extensive empirical experiments demonstrate the proposed hierarchical contexts are prone to learning precise category descriptions, which shows robustness to various data shifts. By simply merging the task-independent precise category descriptions, we provide a new perspective to efficiently extend the ID classification scope to twenty-thousand categories at an acceptable resource cost. We also offers new insights to boost zero-shot classification via text-image similarities regularized from an one-class OOD detection perspective.

**Limitations and societal impact.** To keep the representation capacity of visual features and the generalization ability to identify unseen samples, we freeze the image encoder of the pre-trained CLIP. It limits the upper-bound performance on ID classification and OOD detection of our method. Larger models (_e.g._, ViT-L/14@336px) might bring higher performance but require more costs. There is a trade-off between efficacy and efficiency. Besides, as we do not fine-tune the encoder, the social biases accumulated during pre-training on uncurated web-scale datasets are inherited. Directly applying our method may cause biased predictions containing stereotypes or racist content. Leveraging debiasing techniques could be the key to alleviating this problem.

   \)} &  &  \\   & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\ 
1 & 10.31 & 97.82 & 10.31 & 97.82 \\
2 & **10.21** & 97.86 & 10.17 & 97.86 \\
4 & 10.27 & **97.88** & 9.89 & **97.89** \\
8 & 10.25 & 97.86 & **9.76** & 97.84 \\   

Table 7: Ablation study on multiple spurious contexts.