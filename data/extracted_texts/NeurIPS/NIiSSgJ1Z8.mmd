# SCIURus: Shared Circuits for Interpretable Uncertainty Representations in Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We investigate the mechanistic sources of uncertainty in large language models (LLMs), an area with important implications for their reliability and trustworthiness. To do so, we conduct a series of experiments designed to identify whether the factuality of generated responses and a model's uncertainty originate in separate or shared circuits  in the model architecture. We approach this question by adapting the well-established mechanistic interpretability techniques of path patching and zero-ablation that allows identifying the effect of different circuits on LLM generations. Our extensive experiments on eight different models and five datasets, representing tasks predominantly requiring factual recall, clearly demonstrate that uncertainty is produced in the same parts of a model that are responsible for the factuality of generated responses. We release code for our implementation.

## 1 Introduction

Uncertainty quantification (UQ) in large language models (LLMs) for knowledge-intensive tasks  remains a critical yet understudied area. Despite achieving human-level performance on various benchmarks, LLMs often struggle with reliable uncertainty estimation, leading to issues such as overconfidence and hallucination . This limitation has strong implications for their trustworthiness and safety in high-stakes applications. While recent research has explored verbalized uncertainty in LLMs [1; 10; 11], significant gaps remain in our understanding and ability to improve UQ. In particular, existing UQ techniques typically provide little insight into the factors responsible for an uncertainty estimate, limiting their usefulness both as tools for trustworthiness. We propose leveraging mechanistic interpretability, an approach focused on characterizing models' internal mechanisms of reasoning, to advance our comprehension and enhancement of uncertainty quantification in LLMs.

Following Kadavath et al. , to better understand how LMs generate uncertainty estimates, we used parametric \(()\) ("probability that I know") probes--one-layer binary classifiers that are trained to predict the probability that a given LM knows the answer to a given question. As in , we trained \(()\) probes on several datasets and with several models. We then used these probes' predicted confidences as target metrics for path patching and zero-ablation, two mechanistic interpretability techniques which identify the components of a model relevant for a task by testing the effect of an interventions made on activations in the model during evaluation. We compared the mechanistic signatures of changes in the model's accuracy and the probe's output to evaluate whether the same circuits were responsible for the answer and the predicted confidence.

In our empirical evaluation, in which we performed zero-ablation for a large range of model-dataset combinations and path patching for one combination, we found that model accuracy and probe behavior largely responded to the same interventions, indicating that circuits responsible for the factuality of responses and for uncertainty quantification are located in the same parts of the model.

For a group of knowledge-intensive question answering  tasks, model accuracy and probe confidence are (highly) positively related to one another. We conclude that, at least on recall tasks, alanguage model's representation of confidence may derive mainly from introspection on its question-answering process, rather than from separate reasoning specific to the UQ task.

To summarize, the key contributions of this paper are as follows:

1. We use mechanistic interpretability and uncertainty quantification tools to investigate the mechanistic sources of uncertainty in large language models. To do so, we use a logistic \(()\) probe with path patching and zero-ablation to perform a hypothesis test to examine whether LLM uncertainty and the factuality of answers generated by an LLM reside in shared or separate circuits within the model.
2. We perform an extensive empirical analysis on eight different models and five recall-intensive datasets, and find evidence that uncertainty quantification and the factuality of answers generated by an LLM are handled by the same parts of the model.

## 2 Background

Path Patching.Path patching is a causal intervention method that aims to trace and identify important components in neural models for a given task [14; 18], which is a generalization of causal mediation analysis . In this work, we use path patching  to examine the importance and role of individual circuits and components in LLMs. Specifically, given a specific input \(q\), path patching involves three runs: (1) a clean run, in which the original input \(q\) is given to the model, which is used to obtain the hidden states of each layer; (2) a corrupted run, in which the input embeddings of certain tokens are corrupted by adding noise or (in this paper) replaced with zeros; and (3) a corrupted-with-restoration run, in which the computation is similar to the corrupted run except that the hidden states at specific locations \(\) in the model are restored using the hidden states obtained from the clean run. By comparing the differences between the output (predicted probabilities) of the clean, corrupted, and restored runs, path patching allows the identification of important components in LLMs. That is, if the restored run achieves a similar effect as the clean run, it is likely that the corresponding restored component plays an important role in the model's processing.

Zero-Ablation.Zero-ablation is a mechanistic intervention technique that takes advantage of a transformer's residual structure by treating attention or MLP layers as separable modules which read from and write to the residual stream [6; 15]. A component \(\) (in this paper, an attention or MLP layer) is "ablated" by replacing its output with zero. The drop in model performance on a given task after an intervention removing a component \(\) provides a measure of the importance of \(\) for the task.

Figure 1: **Left:**\(()\) probing. The LLM takes a question as input and returns an answer and last-layer activations. Answers are checked for correctness. The probe learns to predict whether the model’s answer is correct, based on the last-layer activations. Our analysis uses the probe as a proxy for an LLM’s \(()\). We conduct path patching and zero ablation studies on the probe and the corresponding LLM. **Right:** Locations used in interventions. Path-patching restorations are at mlp.resid, mlp.out, layer.out, and embed.out. Zero-ablations are at attn.out and mlp.out.

## 3 Uncertainty Introspection: Investigating the Shared Circuits Hypothesis

The aim of this paper is to make progress toward characterizing the mechanistic structures used for UQ in language models. To this end, we propose a theoretical hypothesis ("shared circuits") about the locations of these structures, along with operationalizations which we test experimentally.

Shared Circuits Hypothesis.Uncertainty quantification in question-answering (QA) systems may be carried out in a variety of ways. We hypothesize that language models are capable of expressing uncertainty using **shared circuits** that both solve the underlying question-answering task and output uncertainty information. This contrasts with the possibility that uncertainty quantification emerges in **separate circuits**, either to post-process messy uncertainty signals from question-answering circuits or to do uncertainty calculations of their own.

We study eight Llama[12; 13] and Gemma  models and five datasets, described in Appendix 3.2.

### Experiment Design: Path Patching

On a given question \(q_{i}\) in a dataset \(\), for each path patching run (clean, corrupted, and restored) we compute the model's sample probability \(m(q_{i})\) for the correct first token of the answer, and the probe's confidence \(p(q_{i})\). (We omit the question for the rest of this section for legibility; we consider questions individually.) Locations \(\) where \(m_{()} m_{}\) correspond to parts of the model which are important for solving the QA task; likewise, locations \(\) where \(p_{()} p_{}\) correspond to parts of the model which are important for the UQ task.12 In the context of path patching, we operationalize the shared circuits hypothesis in the claim that \(m_{}\) can be predicted from \(p_{}\) by interpolating between the clean and corrupted values: for example, if the model's correct-token probability on a restored run is halfway between the values from the clean and corrupted runs, then the probe's confidence should also be halfway between the clean and corrupted runs.

Specifically, for each question \(q_{i}\), we claim that the linear predictor \(_{}\) defined by

\[_{()}-m_{}}{m_{}-m_{}}=()}-p_{}}{p_{}-p_{}}\]

Figure 2: **Left:** Results of path patching for Llama 3 8B Instruct on a question in CounterFact. Only layer.out locations are shown (plus embed.out in the first row). The input embeddings for the starred tokens are replaced with zeros in the corrupted and restored runs. **Center:** Predicting \(m\) given \(p\). The black and red X (top-right and bottom-left) show the clean and corrupted runs; all others show restored runs. Yellow points are later in the sequence. The grey line shows the predictor \(\). **Right:** Results of zero ablation for Llama 3 8B Instruct on four different datasets. Circle, triangle, and X markers represent MLP ablations, attention ablations, and clean runs respectively. Warmer colors represent earlier layers.

explains most of the variance in \(m_{}\) (i.e., has a high coefficient of determination \(R^{2}\)). As a (somewhat weak) formalization of this, we attempt to reject the null hypothesis

\[H_{0}:R^{2}.\] (1)

### Experiment Design: Zero-Ablation

We also test the shared circuits hypothesis via zero-ablation on layers. Here, because we are interested in multi-token answers, we define \(m(q_{i})\) as the probability of a correct answer sampled by the model when prompted on the question \(q_{i}\), and \(p(q_{i})\) as the probe output on that question. Taking means over \(\), we can compare changes in the model accuracy \(\) and the average probe output \(\). Under the shared circuits hypothesis, the change in the probe output from ablation \(|_{()}-p_{}|\) is large when the change in model accuracy \(|_{()}-m_{}|\) is large. Concretely, we claim that the predictor \(}\) defined by

\[m_{}-}_{()}=|_{ ()}-p_{}|\]

explains most of the variance in \(_{}\) (has a high \(R^{2}\)), and attempt to reject the null hypothesis

\[H_{0}:R^{2}.\] (2)

### Testing the Hypothesis

Path Patching.We performed path patching with Llama 3 8B Instruct  on a random sample of 16 questions from the CounterFact dataset , considering only questions which the model could answer (\(m_{}>0.5\)). We used the probe and few-shot prompt for TriviaQA. Across this sample, the predictors \(_{}\) generally estimated \(m_{}\) well, with \(R^{2}>0.6\) in all but three cases.3 For each question \(q_{i}\), we tested the null hypothesis (1) by sampling 10,000 permutations.4 In all cases, we reject \(H_{0}\) with \(p<0.0001\).

Zero-Ablation.We performed zero ablation with eight models across five question-answering datasets (see Appendix B). Across this sample, the predictors \(}_{}\) generally estimated \(_{}\) better than chance, with a median of \(R^{2}=0.33\). For each model-dataset combination, we tested the null hypothesis (2) by sampling 10,000 permutations. We reject the null hypothesis with \(p<0.05\) in 36 out of 38 cases, and \(p<0.0001\) in 31 out of 38 cases.

In many cases, the model's uncertainty representation plays particularly nicely with zero-ablation, remaining calibrated on average even after an intervention: using the same statistical framework as above, the very simple predictor \(}_{}=_{}\) does better than expected under random permutations in 27 out of 38 cases (at \(p<0.05\)).6 While other explanations may be possible, one interpretation of these results is that a given component makes a nonzero contribution to the model's uncertainty representation if and only if it can also contribute information about the answer.

## 4 Discussion and Conclusion

The results of our path patching and zero-ablation analyses broadly support the shared circuits hypothesis, implying that across the setups we considered the sets of model components used for question-answering and uncertainty quantification were largely, albeit not entirely, the same. This suggests that \(()\) probing may be a viable way of eliciting introspective, interpretable uncertainty estimates. Based on these findings, further research could analyze the mechanisms responsible for \(()\) estimates in greater detail or apply \(()\) probing as an interpretability tool to study phenomena such as hallucination in LLMs and meaningfully contribute to technical AI governance.