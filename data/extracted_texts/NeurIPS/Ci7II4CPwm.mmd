# Fast Proxy Experiment Design

for Causal Effect Identification

Sepehr Elahi

EPFL, Switzerland

sepehr.elahi@epfl.ch

&Sina Akbari

EPFL, Switzerland

sina.akbari@epfl.ch

&Jalal Etesami

TUM, Germany

j.etesami@tum.de

&Negar Kiyavash

EPFL, Switzerland

negar.kiyavash@epfl.ch

&Patrick Thiran

EPFL, Switzerland

patrick.thiran@epfl.ch

Authors contributed equally.

###### Abstract

Identifying causal effects is a key problem of interest across many disciplines. The two long-standing approaches to estimate causal effects are _observational_ and _experimental (randomized)_ studies. Observational studies can suffer from unmeasured confounding, which may render the causal effects unidentifiable. On the other hand, direct experiments on the target variable may be too costly or even infeasible to conduct. A middle ground between these two approaches is to estimate the causal effect of interest through _proxy experiments_, which are conducted on variables with a lower cost to intervene on compared to the main target. In an earlier work, we studied this setting and demonstrated that the problem of designing the optimal (minimum-cost) experiment for causal effect identification is NP-complete and provided a naive algorithm that may require solving exponentially many NP-hard problems as a sub-routine in the worst case. In this work, we provide a few reformulations of the problem that allow for designing significantly more efficient algorithms to solve it as witnessed by our extensive simulations. Additionally, we study the closely-related problem of designing experiments that enable us to identify a given effect through valid adjustments sets.

## 1 Introduction

Identifying _causal effects_ is a central problem of interest across many fields, ranging from epidemiology all the way to economics and social sciences. While conducting randomized (controlled) trials provides a framework to analyze and estimate the causal effects of interest, such experiments are often impractical due to various limitations, including financial, logistical, and ethical constraints. Even when they are practical, gathering sufficient data to draw statistically significant conclusions is often challenging due to the high costs.

Costs can arise in multiple forms: financial costs (e.g., implementing costly interventions), time resources (e.g., upgrading infrastructure), and other

Figure 1: The average runtime of our approach compared with the state-of-the-art (S.O.T.A) from Akbari et al. (2022).

logical constraints such as human resources. Moreover, certain experiments might be unethical or outright unfeasible, such as exposing patients to harmful treatments, and hence have infinite cost.

Observational data, which is usually more abundant and accessible, offers an alternative avenue. However, observational studies bring a new challenge: the causal effect may not be _identifiable_ due to _unmeasured confounding_, making it impossible to draw inferences from the observed data (Pearl, 2009; Hernan and Robins, 2006). A middle ground between observational and experimental approaches is to fuse data from both types of studies (Barenboim and Pearl, 2016; Athey et al., 2020). For example, cities might introduce low-emission zones as a proxy experiment before banning diesel vehicles (Dey et al., 2018), or governments might impose regional sugary drink taxes to estimate the effects of nationwide policies Redondo et al. (2018). Several works have studied the achievability of identification from ensembles of observational and experimental data (Bareinboim and Pearl, 2012; Lee et al., 2020; Kivva et al., 2022; Jamshidi et al., 2024). Our previous work (Akbari et al., 2022) was the first to investigate the problem of designing _proxy experiments_ to identify a causal effect that is not identifiable from observational data alone.

To illustrate the need for proxy experiments, consider the following drug-drug interaction example, based on the example in Lee et al. (2020).

**Example 1**.: _(Complex Drug Interactions and Cardiovascular Risk) Consider a simplified example involving the interaction between a new antihypertensive therapy (\(v_{1}\)), anti-diabetic medications (\(v_{2}\)), renal function modulators (\(v_{3}\)), and their effects on blood pressure (\(w\)) and cardiovascular disease (\(y\)). Blood pressure and cardiovascular health are closely linked. The antihypertensive therapy \(v_{1}\) directly influences the need for renal function modulators \(v_{3}\), and \(v_{3}\) in turn directly affects blood pressure \(w\). Additionally, anti-diabetic medications \(v_{2}\) reduce the risk of cardiovascular disease \(y\) by controlling blood sugar levels, while blood pressure \(w\) directly impacts \(y\). Unmeasured factors confound these relationships: shared health conditions can influence the prescription of both \(v_{1}\) and \(v_{3}\); lifestyle factors affect both \(v_{1}\) and \(w\); and common conditions like metabolic syndrome can impact both \(v_{1}\) and \(v_{2}\). Fig. 2(a) illustrates the resulting causal graph, whose directed edges represent direct causal effects, and bidirected edges indicate unmeasured confounders. Suppose we are interested in estimating the intervention effects of \(v_{2}\) and \(v_{3}\) on \(y\), which are not identifiable from observational data alone. Moreover, we cannot directly intervene on these variables because \(v_{2}\) and \(v_{3}\) are essential for managing immediate, life-threatening conditions. Instead, we can intervene on \(v_{1}\), which is a feasible and safer approach due to the broader range of treatment options and more manageable risks associated with adjusting antihypertensive therapy. As we shall see, intervention on \(v_{1}\) suffices for identifying the effects of \(v_{2}\) and \(v_{3}\) on \(y\)._

Selecting the _optimal_ set of proxy experiments is not straightforward in general. In particular, in (Akbari et al., 2022) we proved that the problem of finding the _minimum-cost intervention set_ to identify a given causal effect, hereon called the MCID problem, is NP-complete and provided a naive algorithm that requires solving exponentially many instances of the minimum hitting set problem in the worst case. As the minimum hitting set problem is NP-complete itself, our earlier algorithm in (Akbari et al., 2022) can become computationally intractable even for graphs with a modest number of vertices. Moreover, this algorithm was tailored to a specific class of causal effects in which the effect of interest is a functional of an interventional distribution where the intervention is made on

Figure 2: (a): Causal graph of Example 1. (b): Transformed graph when considering identifiability of \(_{v_{2},v_{3}}(y)\), with \(S=\{w,y\}\). (c): A causal graph with \(2^{m}\) minimal hedges. (d): Running example.

every variable except one _district_ of the causal graph2. For a general causal effect, the complexity of this algorithm includes an additional (super-)exponential multiplicative factor, where the exponent is the number of districts.

In this work, we revisit the MCID problem and develop tractable algorithms by reformulating the problem as instances of well-known problems, such as the weighted maximum satisfiability and integer linear programming problems. Furthermore, we analyze the problem of designing minimum cost interventions to obtain a valid adjustment set for a query. This problem not only merits attention in its own right, but also serves as a proxy for MCID. Our contributions are as follows:

* We formulate the MCID problem in terms of a partially weighted maximum satisfiability, integer linear programming, submodular function maximization, and reinforcement learning problem. While our main focus is on the former two reformulations, we state the others in Appendix D.
* see Remark 2.
* We formulate and study the problem of designing minimum-cost experiments for identifying a given effect through finding a valid adjustments set. Besides the practical advantages of valid adjustment, including ease of interpretability and tractable sample complexity, this approach enables us to design a polynomial-time heuristic algorithm for the MCID problem that outperforms the heuristic algorithms provided in Akbari et al. (2022).
* We present new numerical experiments that demonstrate the exceptional speed of our exact algorithms when compared to the current state-of-the-art, along with our heuristic algorithm showcasing superior performance over previous heuristic approaches.

## 2 Problem formulation

We begin by reviewing relevant graphical definitions. An _acyclic directed mixed graph_ (ADMG) is a graph with directed (\(\)) and bidirected (\(\)) edges such that the directed edges form no cycles (Richardson, 2003). We denote an ADMG \(\) by a tuple \(= V,,\), where \(V\), \(\), and \(\) represent the set of vertices, directed edges, and bidirected edges, respectively. Note that \(\) is a set of ordered pairs of vertices in \(V\), whereas \(\) is a set of unordered pairs of vertices.

Vertices of \(\) represent variables of the system under consideration, while the edges represent causal relations between them. We use the terms 'variable' and'vertex' interchangeably. When \((y,x)\), we say \(y\) is a parent of \(x\) and \(x\) is a child of \(y\). The set of parents of \(X V\) denoted by \((X)=\{y:(y,x)\) for some \(x X\} X\). We denote by \([W]\), the induced subgraph of \(\) over vertices \(W V\). A subset \(S\) of \(V\) is said to form a _district_ in \(\) if \(S\) is a maximal set such that any pair of vertices \(x,y S\) are connected through a bidirected path \(x y\) in \([S]\). In other words, \([S]\) is a connected component through its bidirected edges. We say \(x V\) is an ancestor of \(S V\) if there is a directed path \(x s\) for some \(s S\). We denote the set of ancestors of \(S\) in the subgraph \([W]\) by \(_{W}(S)\). Note that \(S_{W}(S)\). When \(W=V\), we drop the subscript for ease of notation.

Let \(X,Y V\) be two disjoint sets of variables. The probability distribution of \(Y\) under a (possibly hypothetical) intervention on \(X\) setting its value to x is often represented as either \((Y^{()})\), using Rubin's potential outcomes model (Rubin, 1974), or \((Y(X=))\) using Pearl's do operator (Pearl, 2009). We will adopt the shorthand \(_{}(Y)\) to denote this interventional distribution3.

**Definition 1** (Identifiability).: _An interventional distribution \(_{}(Y)\) is identifiable given an ADMG \(\) and the intervention set family \(}=\{_{1},,_{t}\}\), with \(_{i} V\), over the variables corresponding to \(\), if \(_{}(Y)\) is uniquely computable as a functional of the members of \(\{_{}():}\}\)._

**Remark 1**.: _It is common in the literature to define identifiability with respect to observational data only (i.e., when \(}=\{_{1}=\}\)). Our definition above follows what is known as the 'general identifiability' from Lee et al. (2020)._

We will now formally define a _hedge_, which, as we will see shortly after, is central to deciding the identifiability of an interventional distribution given the data at hand.

**Definition 2** (Hedge).: _Let \(S V\) be a district in \([S]\). We say \(W S\) forms a hedge for \(S\) if (i) \(W\) is a district in \([W]\), and (ii) every vertex \(w W\) is an ancestor of \(S\) in \([W]\) (i.e., \(W=_{W}(S)\)). We denote by \(H_{}(S)\) the set of hedges formed for \(S\) in \(\)._

For example, in Fig. 2(b), \(S\) has two hedges given by \(H_{}(S)=\{S\{v_{3},v_{1}\},S\{v_{3},v_{1},v_{2}\}\}\).

**Definition 3** (Hedge hull Akbari et al., 2022).: _Let \(S\) be a district in ADMG \(\). Also let \(H_{}(S)\) be the set of all hedges formed for \(S\) in \(\). The union of all hedges in \(H_{}(S)\), denoted by \(_{}(S)=_{W H_{}(S)}W,\) is said to be the hedge hull of \(S\) in \(\)._

For instance, in Fig. 2(d), the hedge hull of \(S_{1}=\{s_{1}\}\) is \(_{}(S_{1})=\{s_{1},s_{2},v_{1},v_{2},v_{3},v_{4},v_{5}\}\) and the hedge hull of \(S_{2}=\{s_{2}\}\) is \(_{}(S_{2})=\{s_{2},v_{3}\}\). When a set \(S\) consists of more than one district, we simply define the hedge hull of \(S\) as the union of the hedge hulls of each district of \(S\). The hedge hull of a set can be found through a series of at most \(|V|\) depth-first-searches. In the latter example, the hedge hull of \(S=\{s_{1},s_{2}\}\) is \(_{}(S)=\{s_{1},s_{2},v_{1},v_{2},v_{3},v_{4},v_{5}\}\). For the sake of completeness, we have included the algorithm for finding a hedge hull in Appendix B.1.

The following proposition from Lee et al. (2020) and Kivva et al. (2022) establishes the graphical criterion for deciding the identifiability of a causal effect given a set family of interventions.

**Proposition 1**.: _Let \(\) be an ADMG over the vertices \(V\). Also let \(X,Y V\) be disjoint sets of variables. Define \(S=_{V X}(Y)\), and let \(}=\{S_{1},,S_{r}\}\) be the (unique) set of districts in \([S]\). The interventional distribution \(_{}(Y)\) is identifiable given \(\) and the intervention set family \(}=\{_{1},,_{t}\}\), if and only if for every \(S_{}}\), there exists an intervention set \(_{k}}\) such that (i) \(_{k} S_{}=\), and (ii) there is no hedge formed for \(S_{}\) in \([V_{k}]\)._

Note that there is no hedge formed for \(S_{}\) in \([V_{k}]\) if and only if \(_{k}\)_hits_ every hedge of \(S_{}\) (i.e., for any hedge \(W_{}(S_{})\), \(_{k} W\)). For ease of presentation, we will use \(_{k}}}{{}}S_{}\) to denote that \(_{k} S_{}=\) and \(_{k}\) hits every hedge formed for \(S_{}\). For example, given the graph in Fig. 2(d) and with \(}=\{S_{1},S_{2}\},\) an intervention set family that hits every hedge is \(}=\{\{s_{2}\},\{v_{3}\}\}\).

**Minimum-cost intervention for causal effect identification (MCID) problem.** Let \(C:V\!\!^{ 0}\{+\}\) be a known function4 indicating the cost of intervening on each vertex \(v V\). An infinite cost is assigned to variables where an intervention is not feasible. Given \(\) and disjoint sets \(X,Y V\), our objective is to find a set family \(}^{*}\) with minimum cost such that \(_{}(Y)\) is identifiable given \(}^{*}\); that is, for every district \(S_{}\) of \(S\), there exists \(_{k}\) such that \(_{k}}}{{}}S_{}\). Since every \(}\) is a subset of \(V\), the space of such set families is the power set of the power set of \(V\).

To formalize the MCID problem, we first write the cost of a set family \(}\) as \(C(}):=_{}}_{v}C(v),\) where with a slight abuse of notation, we denoted the cost of \(}\) by \(C(})\). The MCID problem then can be formalized as follows.

\[}^{*}*{argmin}_{} 2^{V} S}C(})\;S_{} }:(\;_{k}}:_{k} }}{{}}S_{}),\] (1)

where \(}=\{S_{1},,S_{r}\}\) is the set of districts of \(S=_{V X}(Y)\), and \(2^{2^{V}}\) represents the power set of the power set of \(V\). In the special case where \(S\) comprises a single district, the MCID problem can be presented in a simpler way.

**Proposition 2** (Akbari et al., 2022).: _If \(S=_{V X}(Y)\) comprises a single district \(=\{S_{1}=S\}\), then the optimization in (1) is equivalent to the following optimization:_

\[^{*}*{argmin}_{} 2^{V S}}C(})\;W H_{}(S):\;  W.\] (2)

_That is, the problem reduces to finding the minimum-cost set that 'hits' every hedge formed for \(S\)._Recall example Example 1, we were interested in finding the least costly proxy experiment to identify the effect of \(v_{2}\) and \(v_{3}\) on \(y\). By Proposition 2, this problem is equivalent to finding an intervention set with the least cost (i.e., a set of proxy experiments) that hits every hedge of \(S=\{y,w\}\) in the transformed graph (Fig. 2(b)). If \((v_{1})<(v_{3})\), then the optimal solution would be \(^{*}=\{v_{1}\}\).

In the remainder of the paper, we consider the problem of identification of \(_{X}(Y)\) for a given pair \((X,Y)\), and with \(S\) defined as \(S=_{V X}(Y)\), unless otherwise stated. We will first consider the case where \(S\) comprises a single district, and then generalize our findings to multiple districts.

## 3 Reformulations of the min-cost intervention problem

In the previous section, we delineated the MCID problem as a discrete optimization problem. This problem, cast as Eq. (1), necessitates search within a doubly exponential space, which is computationally intractable. Algorithm 2 of (Akbari et al., 2022) is an algorithm that conducts this search and eventually finds the optimal solution. However, even when \(S\) comprises a single district, this algorithm requires, in the worst case, exponentially many calls to a subroutine which solves the NP-complete minimum hitting set problem on exponentially many input sets, hence resulting in a doubly exponential complexity. More specifically, our previous algorithm described in Akbari et al. (2022) attempts to find a set of _minimal_ hedges, where minimal indicates a hedge that contains no other hedges, and solves the minimum hitting set problem on them. However, there can be exponentially many minimal hedges, as shown for example in Fig. 2(c). Letting \(m=n/2\), then any set that contains one vertex from each level (i.e., directed distance from \(S\)) is a minimal hedge, of which there are \((2^{n/2})\).

Furthermore, the computational complexity of Algorithm 2 of Akbari et al. (2022) grows super-exponentially in the number of districts of \(S\). This is due to the necessity of exhaustively enumerating every possible partitioning of these districts and executing their algorithm once for each partitioning.

In this section, we reformulate the MCID problem as a weighted partially maximum satisfiability (WPMAX-SAT) problem (Fu and Malik, 2006), as well as an integer linear programming (ILP) problem. We focus on the WPMAX-SAT and ILP reformulations due to their computational efficiency and practical applicability, but we provide alternative reformulations as a submodular maximization problem and a reinforcement learning problem in Appendix D for completeness. The advantage of the WPMAX-SAT and ILP formulations is two-fold: (i) compared to Algorithm 2 of (Akbari et al., 2022), we state the problem as a _single_ instance of another problem for which a range of well-studied solvers exist, and (ii) these formulations allow us to propose algorithms with computational complexity that is quadratic in the number of districts of \(S\). We will see how these advantages translate to drastic performance gains in Section 5.

### Min-cost intervention as a WPMAX-SAT problem

We begin with constructing a 3-SAT formula \(F\) that is satisfiable if and only if the given query \(_{X}(Y)\) is identifiable. To this end, we define \(m+2\) variables \(\{x_{i,j}\}_{j=0}^{m+1}\) for each vertex \(v_{i} V\), where \(m=|_{}(S) S|\) is the cardinality of the hedge hull of \(S\), excluding \(S\). Intuitively, \(x_{i,j}\) is going to indicate whether or not vertex \(v_{i}\) is reachable from \(S\) after \(j\) iterations of alternating depth-first-searches on directed and bidirected edges. This is in line with the workings of Algorithm 2 for finding the hedge hull of \(S\). In particular, if a vertex \(v_{i}\) is reachable after \(m+1\) iterations, that is, \(x_{i,m+1}=1\), then \(v_{i}\) is a member of the hedge hull of \(S\). The query of interest is identifiable if and only if \(_{}(S)=S\), that is, the hedge hull of \(S\) contains no other vertices. Therefore, we ensure that the formula \(F\) is satisfiable if and only if \(x_{i,m+1}=0\) for every \(v_{i} S\). The formal procedure for constructing this formula is as follows.

SAT Construction Procedure.Suppose a causal ADMG \(= V,,\) and a set \(S V\) are given, where \(S\) is a district in \(\). Suppose \(_{}(S)=\{v_{1},,v_{n}\}\) is the hedge hull of \(S\) in \(\), where without loss of generality, \(S=\{v_{m+1}, v_{n}\}\), and \(\{v_{1}, v_{m}\} S=\). We will construct a corresponding boolean expression in conjunctive normal form (CNF) using variables \(\{x_{i,j}\}\) for \(i\{1,,m\}\) and \(j\{0,,m+1\}\). For ease of presentation, we also define \(x_{i,j}=1\) for all \(i\{m+1,,n\}\), \(j\{0,,m+1\}\). The construction is carried out in \(m+2\) steps, where in each step, we conjoin new clauses to the previous formula using 'and'. The procedure is as follows:* For odd \(j\!\!\{1,,m\!+\!1\}\), for each directed edge \((v_{i},v_{})\!\!\), add \(( x_{i,j-1} x_{i,j}\! x_{,j})\) to \(F\).
* For even \(j\{1,,m+1\}\), for each bidirected edge \(\{v_{i},v_{}\}\!\), add both clauses \(( x_{i,j-1} x_{i,j} x_{,j})\) and \(( x_{,j-1} x_{,j} x_{i,j})\) to \(F\).
* Finally, at step \(m+2\), add clauses \( x_{i,m+1}\) to the expression \(F\) for every \(i\{1,,m\}\).

As an example, consider the graph of Figure 3 with \(n=3\) vertices. The hedge hull of \(S=\{v_{3}\}\) is \(\{v_{1},v_{2},v_{3}\}\), and \(m=|\{v_{1},v_{2}\}|=2\). Following the SAT construction procedure outlined above, the construction is carried out in \(m+2=4\) steps. Our SAT expression will consist of 8 variables, \(x_{i,j}\) for \(i\{1,2\}\) - corresponding to \(v_{1}\) and \(v_{2}\) - and \(j\{0,1,2,3\}\) - corresponding to the four steps of construction. Below, we explain each step.

At step 1, we add the clauses \(( x_{1,0} x_{1,1} x_{2,1})\) and \(( x_{2,0} x_{2,1} x_{3,1})\), corresponding to the edges \(v_{1} v_{2}\) and \(v_{2} v_{3}\), respectively. Note that by convention, \(x_{3,1}=1\), hence the second clause reduces to \(( x_{2,0} x_{2,1})\). At step 2, for the edge \(v_{1} v_{2}\), we add the clauses \(( x_{1,1} x_{1,2} x_{2,2})\) and \(( x_{2,1} x_{2,2} x_{1,2})\). Similarly for the edge \(v_{1} v_{3}\), we add the clauses \(( x_{1,1} x_{1,2} x_{3,2})\) and \(( x_{3,1} x_{3,2} x_{1,2})\). Since by convention, \(x_{3,1}=x_{3,2}=1\), the latter two clauses reduce to \(( x_{1,1} x_{1,2})\) and \(1\), respectively. At step 3, we add the clauses \(( x_{1,2} x_{1,3} x_{2,3})\) and \(( x_{2,2} x_{2,3} x_{3,3})\), corresponding to the edges \(v_{1} v_{2}\) and \(v_{2} v_{3}\), respectively. Again \(x_{3,3}=1\), and the latter clause reduces to \(( x_{2,2} x_{2,3})\). At step 4, the clauses \( x_{1,3}\) and \( x_{2,3}\) are added.

Finally, combining all the clauses together, the SAT expression is given by

\[( x_{1,0} x_{1,1} x_{2,1})( x_{2,0}  x_{2,1})( x_{1,1} x_{1,2} x_{2,2})( x_{2, 1} x_{2,2} x_{1,2})\] \[( x_{1,1} x_{1,2})( x_{1,2} x_{1,3}  x_{2,3})( x_{2,2} x_{2,3}) x_{1,3} x _{2,3}.\]

**Theorem 1**.: _The 3-SAT formula \(F\) constructed by the procedure above given \(\) and \(S\) has a satisfying solution \(\{x^{*}_{i,j}\}\) where \(x^{*}_{i,0}\!=\!0\) for \(i\!\!\!\!\{1,,m\}\) and \(x^{*}_{i,0}\!=\!1\) for \(i\!\!\{1,,m\}\!\) if and only if \(\) intersects every hedge formed for \(S\) in \(\); i.e., \(\) is a feasible solution to the optimization in Eq. (2)._

The proofs of all our results appear in Appendix C. The first corollary of Theorem 1 is that the SAT formula is always satisfiable, for instance by setting \(x^{*}_{i,0}=0\) for every \(i\{1,,m\}\). The second (and more important) corollary is that the optimal solution to Eq. (2) corresponds to the satisfying assignment for the SAT formula \(F\) that minimizes

\[_{i=1}^{m}(1-x^{*}_{i,0})C(v_{i}).\] (3)

This suggests that the problem in Eq. (2) can be reformulated as a weighted partial MAX-SAT (WPMAX-SAT) problem. **WPMAX-SAT** is a generalization of the MAX-SAT problem, where the clauses are partitioned into _hard_ and _soft_ clauses, and each soft clause is assigned a weight. The goal is to maximize the aggregate weight of the satisfied soft clauses while satisfying all of the hard ones.

To construct the WPMAX-SAT instance, we simply define all clauses in \(F\) as hard constraints, and add a soft clause \(x_{i,0}\) with weight \(C(v_{i})\) for every \(i\{1,,m\}\). The former ensures that the assignment corresponds to a feasible solution of Eq. (2), while the latter ensures that the objective in Eq. (3) is minimized - which, consequently, minimizes the cost of the corresponding intervention.

Multiple districts.The formulation above was presented for the case where \(S\) is a single district. In the more general case where \(S\) has multiple districts, we can extend our formulation to solve the general problem of Eq. (1) instead. To this end, we will use the following lemma.

**Lemma 1**.: _Let \(}=\{S_{1},,S_{r}\}\) be the set of districts of \(S\), where \(S=_{V X}(Y)\). There exists an intervention set family \(}^{*}\) of size \(|}|=r\) that is optimal for identifying \(_{X}(Y)\)._

Based on Lemma 1, we can assume w.l.o.g. that the optimizer of Eq. (1) contains exactly \(r\) intervention sets \(_{1},,_{r}\). We will modify the SAT construction procedure described in the previous section

Figure 3: Example graph for 3SAT construction, where \(S=\{v_{3}\}\).

to allow for multiple districts as follows. For any district \(S_{}\), we will construct \(r\) copies of the SAT expression, one corresponding to each intervention set \(_{k}\), \(k\{1,,r\}\). Each copy is built on new sets of variables indexed by \((k,)\), except the variables with index \(j=0\), which are common across districts. We introduce variables \(\{z_{k,}\}_{k,=1}^{r}\), which will serve as indicators for whether \(_{k}\) hits all the hedges formed for \(S_{}\). We relax every clause corresponding to the \(k\)-th copy by conjoining a \( z_{k,}\) literal with an 'or.' Intuitively, this is because it suffices to hit the hedges formed for \(S_{}\) with some \(_{k}\). Additionally, we add the clauses \((z_{1,} z_{r,})\) for any \(\{1,,r\}\) to ensure that for every district, there is at least one intervention set that hits every hedge. This modified procedure, detailed in Algorithm 3, appears in Appendix B.2. The following result generalizes Theorem 1.

**Theorem 2**.: _Suppose \(\), a set of its vertices \(S\) with districts \(}=\{S_{1},,S_{r}\}\), and an intervention set family 2 \(}=\{_{1},,_{r}\}\) are given. Define \(m_{}=|_{}(S_{}) S_{}|\), i.e., the cardinality of the hedge hull of \(S_{}\) excluding \(S_{}\) itself. The SAT formula \(F\) constructed by Algorithm 3 has a satisfying solution \(\{x_{i,0,k}^{*}\}\{x_{i,j,k,}^{*}\}\{z_{k,}^{*}\}\) where for every \(\{1,,r\}\), there exists \(k\{1,,r\}\) such that (i) \(z_{k,}^{*}=1\), (ii) \(x_{i,0,k}^{*}=0\) for every \(i_{k}\), and (iii) \(x_{i,0,k}^{*}=1\) for every \(i\{1,,m_{}\}_{k}\), if and only if \(}\) is a feasible solution to optimization of Eq. (1)._

Constructing the corresponding WPMAX-SAT instance follows the same steps as the case for a single district, except that the soft clauses are of the form \((x_{i,0,k} z_{k,})\) with weight \(C(v_{i})\) for every \(i\{1,,m_{}\}\) and \(k\{1,,r\}\).

**Remark 2**.: _The SAT construction of Algorithm 3 is advantageous because its complexity grows quadratically with the number of districts of \(S\) in the worst case. This is because the inner-loop of the SAT construction algorithm (line 8 of Algorithm 3) is executed \(r^{2}\) many times. In contrast, the runtime of the algorithm of Akbari et al. (2022), when \(S\) consists of multiple districts, is super-exponential in the number of districts, because they need to execute their single-district algorithm at least as many times as the number of partitions of the set \(\{1,,r\}\)._

Min-cost intervention as an ILP problem.The WPMAX-SAT formulation of Section 3.1 paves the way for a straightforward formulation of an integer linear program (ILP) for the MCID problem. ILP allows for straightforward integration of various constraints and objectives, enabling flexible modeling of potential extra constraints. Moreover, there exist efficient and scalable solvers for ILP (Gearhart et al., 2013; Gurobi Optimization, LLC, 2023). To construct the ILP instance for the MCID problem, it suffices to represent every clause in the boolean expression \(F\) of Algorithm 3 as a linear inequality. For example, clauses of the form \(( a b c)\) is rewritten as \((1-a)+b+(1-c) 1\). The soft constraints may be rewritten as a sum to maximize over, given by Eq. (3).

## 4 Minimum-cost intervention design for adjustment criterion

A special case of identifying interventional distributions is identification through _adjusting_ for confounders. A set \(Z V\) is a valid adjustment set for \(_{X}(Y)\) if \(_{X}(Y)\) is identified as

\[_{X}(Y)=_{}[(Y X,Z)],\] (4)

where the expectation w.r.t. \((Z)\). Adjustment sets have received extensive attention in the literature because of the straightforward form of the identification formula (Eq. 4) and the intuitive interpretation: \(Z\) is the set of confounders that we need to _adjust for_ to identify the effect of interest. The simple form of Eq. (4) has the added desirable property that its sample efficiency and asymptotic behavior are easy to analyze (Witte et al., 2020; Rotnitzky and Smucler, 2020; Henckel et al., 2022). A complete graphical criterion for adjustment sets was given by Shpitser et al. (2010). As an example, when all parents of \(X\) (i.e., \((X)\)) are observable, they form a valid adjustment set. However, in the presence of unmeasured confounding, no valid adjustment sets may exist. Below, we generalize the notion of adjustment sets to the interventional setting.

**Definition 4** (Generalized adjustment).: _We say \(Z V\) is a generalized adjustment set for \(_{X}(Y)\) under intervention \(\) if \(_{X}(Y)\) is identified as \(_{X}(Y)=_{_{Z}}[_{Z}(Y X,Z)],\) where \(_{}()\) represents the distribution after intervening on \( V\) and the expectation is w.r.t. \(_{}(Z)\)._

Note that unlike the classic adjustment, the generalized adjustment is always feasible - a trivial generalized adjustment can be formed by choosing \(=X\) and \(Z=\).

Equipped with Definition 4, we can define a problem closely linked to Eq. (2), but with a (possibly) narrower set of solutions, which can be defined as follows: find the minimum-cost intervention \(\) such that a generalized adjustment exists for \(_{X}(Y)\) under \(\):

\[^{*}=*{argmin}_{ 2^{V}}C()\;Z V:_{X}(Y)= _{_{}}[_{}(Y X,Z)].\] (5)

**Observation.** The existence of a valid (generalized) adjustment set ensures the identifiability of \(_{X}(Y)\). As such, any feasible solution to the optimization above is also a feasible solution to Eq. (2). Eq. (5) is not only a problem that deserves attention in its own right, but also serves as a proxy for our initial problem (Eq. 2).

To proceed, we need the following definitions. Given an ADMG \(= V,,\), let \(^{d}= V^{d},^{d},\) be the ADMG resulting from replacing every bidirected edge \(e=\{x,y\}\) by a vertex \(e\) and two directed edges \((e,x),(e,y)\). In particular, \(V^{d}=V\), and \(^{d}=\{(e,x):e,x e\}\). Note that \(^{d}\) is a directed acyclic graph (DAG). The _moralized graph_ of \(\), denoted by \(^{m}\), is the undirected graph constructed by moralizing \(^{d}\) as follows: The set of vertices of \(^{m}\) is \(V^{d}\). Each pair of vertices \(x,y V^{d}\) are connected by an (undirected) edge if either (i) \((x,y)^{d}\), or (ii) \( z V^{d}\) such that \(\{(x,z),(y,z)\}^{d}\).

Throughout this section, we assume without loss of generality that \(X\) is minimal in the following sense: there exists no proper subset \(X_{1} X\) such that \(_{X}(Y)=_{X_{1}}(Y)\) everywhere5. Otherwise, we apply the third rule of do calculus  as many times as possible to make \(X\) minimal. We also assume w.l.o.g. that \(V=(X Y)\) as other vertices are irrelevant for our purposes . We will utilize the following graphical criterion for generalized adjustment.

**Lemma 2**.: _Let \(X,Y\) be two disjoint sets of vertices in \(\) such that \(X\) is minimal as defined above. Set \(Z V\) is a generalized adjustment set for \(_{X}(Y)\) under intervention \(\) if (i) \(Z(S)\), and (ii) \(Z\) is a vertex cut6 between \(S\) and \((S)\) in \((_{(S)}})^{m}\), where \(S=_{V X}(Y)\), and \(_{(S)}}\) is the ADMG resulting from omitting all edges incoming to \(\) and all edges outgoing of \((S)\)._

Based on the graphical criterion of Lemma 2, we present the following _polynomial-time7_ algorithm for finding an intervention set that allows for identification of the query of interest in the form of a (generalized) adjustment. This algorithm will find the intervention set \(\) and the corresponding generalized adjustment set \(Z\) simultaneously. We begin by making \(X\) minimal in the sense of applicability of rule 3 of do calculus. Then we omit all edges going out of \((S)\), and construct the graph \((_{(S)})^{d}= V^{d},^{d},\) as defined above - by replacing bidirected edges with vertices representing unobserved confounding. Finally, we construct an (undirected) vertex cut network \(^{vc}= V^{vc},E^{vc}\) as follows. Each vertex \(v V^{d}\) is represented by two connected vertices \(v_{1},v_{2}\) in \(^{vc}\). If \(v V\), then \(v_{1}\) has a cost of zero, and \(v_{2}\) has cost \(C(v)\). Otherwise, both \(v_{1}\) and \(v_{2}\) have infinite costs. Intuitively, choosing \(v_{1}\) will correspond to including \(v\) in the adjustment set, whereas choosing \(v_{2}\) in the cut would imply intervention on \(v\). We connect \(v_{2}\) to all vertices corresponding to \((v)\) with index \(1\), i.e., \(\{w_{1}:(w,v)^{d}\}\). This serves two purposes: (i) if \(v_{2}\) is included in the cut (corresponding to an intervention on \(v\)), all connections between \(v\) and its parents are broken, and (ii) when \(v_{2}\) is not included in the cut (corresponding to no intervention on \(v\)), \(v_{2}\) connects the parents of \(v\) to each other, completing the necessary moralization process. We solve for the minimum vertex cut between vertices with index \(1\) corresponding to \(S\) and \((S)\). Algorithm 1 summarizes this approach. In the solution set \(J\), the vertices with index 2 represent the vertices where an intervention is required, while those with index \(1\) represent the generalized adjustment set under this intervention.

**Theorem 3**.: _Let \((,Z)\) be the output returned by Algorithm 1 for the query \(_{X}(Y)\). Then, - \(Z\) is a generalized adjustment set for \(_{X}(Y)\) under intervention \(\). - \(\) is the minimum-cost intervention for which there exists a generalized adjustment set based on the graphical criterion of Lemma 2._

**Remark 3**.: _Algorithm 1 enforces identification based on (generalized) adjustment for \(_{X}(Y)\). As discussed above, this algorithm can be utilized as a heuristic approach to solve the MCID problem in (2). In this case, one can run the algorithm on the hedge hull of \(S\) rather than the whole graph. We prove in Appendix C that the cost of this approach is always at most as high as heuristic algorithm 1 of Akbari et al. (2022), and is often in practice lower, as verified by our experiments._

The worst-case time complexity of Algorithm 1 is cubic in the number of variables. The two main computational bottlenecks are (i) the preprocessing in lines 2-3, which involves up to \(|V|\) rounds of m-separation tests (performed via depth-first search), resulting in a complexity of \(O(|V|[|V|+||+||])\), and (ii) the minimum-cut instance in line 9, solved using a max-flow algorithm with \(|V^{vc}| O(|V|+||)\) and \(|E^{vc}| O(|V|+||+||)\) many vertices and edges, respectively.

## 5 Experiments

In this section, we present numerical experiments that showcase the empirical performance and time efficiency of our proposed exact and heuristic algorithms. A comprehensive set of synthetic and real-world experiments analyzing the impact of various problem parameters on the performance of these algorithms, along with the complete implementation details, is provided in Appendix A. We first compare the time efficiency of our exact algorithms: WPMAX-SAT and ILP, with the exact algorithm of Akbari et al. (2022). Then, we present results pertaining to performance of our heuristic algorithm. All experiments, coded in Python, were conducted on a machine equipped two Intel Xeon E5-2680 v3 CPUs, 256GB of RAM, and running Ubuntu 20.04.3 LTS.

Figure 4: Average time taken by Algorithm 2 of Akbari et al. (2022) (MHS), ILP, and WPMAX-SAT to solve one graph versus (a) the number of vertices in the graph and (b) the number of districts of \(S\).

**Results on exact algorithms.** We compare the performance of the WPMAX-SAT formulation, the ILP formulation, and Algorithm 2 of Akbari et al. (2022), called Minimal Hedge Solver (MHS) from hereon. We used the RC2 algorithm (Ignatiev et al., 2019), and the Gurobi solver (Gurobi Optimization, LLC, 2023), to solve the WPMAX-SAT problem, and the ILP, respectively. We ran each algorithm for solving the MCID problem on \(100\) randomly generated Erdos-Renyi (Erdos and Renyi, 1960) ADMG graphs with directed and bidirected edge probabilities ranging from \(0.01\) to \(1.00\), in increments of \(0.01\). We performed two sets of simulations: for single-district and multiple-district settings, respectively. In the single-district case, we varied \(n\), the number of vertices, from \(20\) to \(100\), while in the multiple-district case, we fixed \(n=20\) and varied the number of districts from \(1\) to \(9\).

We plot the average time taken to solve each graph versus the number of vertices (single-district) in Fig. 4(a) and versus the number of districts (\(n=20\)) in Fig. 4(b). The error bands in our figures represent 99% confidence intervals. Focusing on the single-district plot, we observe that both of our algorithms are faster than MHS of Akbari et al. (2022) for all graph sizes. More specifically, ILP is on average one to two orders of magnitude faster than MHS, while SAT is on average four to five orders of magnitude faster. All three algorithms exhibit exponential growth in time complexity with the number of vertices, which is expected as the problem is NP-hard, but SAT grows at a much slower rate than the other two algorithms. This is likely due to RC2's ability of exploiting the structure of the SAT problem to reduce the search space efficiently. In the multiple-district case, we observe that the time complexity of both SAT and ILP grows polynomially with the number of districts, while the time complexity of MHS grows exponentially. This is consistent with theory, as MHS iterates over all partitions of the set of districts, which grows exponentially with the number of districts.

**Results on inexact algorithms.** We compared Algorithm 1, our proposed heuristic, with the two best performing heuristic algorithms in Akbari et al. (2022), \(H_{1}\) and \(H_{2}\). We ran each algorithm on \(500\) randomly generated Erdos-Renyi ADMG graphs with directed and bidirected edge probabilities in \(\{0.1,0.5\}\), with \(n\) ranging from \(10\) to \(200\). We randomly sampled the cost of each vertex from a discrete uniform distribution on \([1,n]\). In Fig. 5, we plot the normalized cost of each algorithm, computed by dividing the cost of the algorithm by the cost of the optimal solution, provided by WPMAX-SAT. Observe that Algorithm 1 consistently outperforms \(H_{1}\) and \(H_{2}\) for all graph sizes.

## 6 Conclusion

We introduced novel formulations and efficient algorithms for the MCID problem, demonstrating significant improvements over existing methods. Our extensive experiments showed that the WPMAX-SAT reformulation, particularly when using a high-performance solver like RC2, excels in both speed and effectiveness. In contrast, the ILP reformulation offers a more interpretable approach, especially valuable for incorporating additional constraints such as domain expert knowledge.

Moreover, our work on designing minimum-cost experiments for obtaining valid adjustment sets demonstrates both practical and theoretical advancements. We highlighted the superior performance of our proposed methods through extensive numerical experiments. We envision designing efficient approximation algorithms for MCID as future work.

Figure 5: Average normalized cost of the heuristic algorithms \(H_{1}\) and \(H_{2}\) of Akbari et al. (2022) and Algorithm 1 versus the number of vertices in the graph.