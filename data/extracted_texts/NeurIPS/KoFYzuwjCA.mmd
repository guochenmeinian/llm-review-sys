# Disentangling Voice and Content with Self-Supervision

for Speaker Recognition

 Tianchi Liu\({}^{1,2}\), Kong Aik Lee\({}^{*}\)\({}^{3,\,1}\), Qionggiong Wang\({}^{1}\), Haizhou Li\({}^{4,\,2}\)

\({}^{1}\) Institute for Infocomm Research (I\({}^{2}\)R), Agency for Science, Technology and Research (A\({}^{*}\)STAR), Singapore

\({}^{2}\) Dept. of Electrical and Computer Engineering, National University of Singapore, Singapore

\({}^{3}\) Dept. of Electrical and Electronic Engineering, Hong Kong Polytechnic University, Hong Kong

\({}^{4}\) School of Data Science, The Chinese University of Hong Kong, Shenzhen, China

{liu_tianchi, wang_qiongiong}@i2r.a-star.edu.sg.kongaik.lee@ieee.org,haizhouli@cuhk.edu.cn

###### Abstract

For speaker recognition, it is difficult to extract an accurate speaker representation from speech because of its mixture of speaker traits and content. This paper proposes a disentanglement framework that simultaneously models speaker traits and content variability in speech. It is realized with the use of three Gaussian inference layers, each consisting of a learnable transition model that extracts distinct speech components. Notably, a strengthened transition model is specifically designed to model complex speech dynamics. We also propose a self-supervision method to dynamically disentangle content without the use of labels other than speaker identities. The efficacy of the proposed framework is validated via experiments conducted on the VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and minDCF, respectively. Since neither additional model training nor data is specifically needed, it is easily applicable in practical use.

## 1 Introduction

Automatic speaker recognition aims to identify a person from his/her voice  based on speech recordings . Typically, two fixed-dimensional representations are extracted from the enrollment and test speech utterances, respectively . Then the recognition procedure is done by measuring their similarity . These representations are referred to as the speaker embeddings . The concept of speaker embedding is similar to that of the face embedding  in the face recognition task and the token embedding  in the language model, while the main difference lies in the carriers of the information source and the nature of downstream tasks. Different from the discrete sequential inputs in Natural Language Processing (NLP) and continuous inputs in Computer Vision (CV), speech signals are continuous-valued variable-length sequences . Speech signals contain both speaker traits and content . For speaker recognition, to form a refined speaker embedding of a speech signal, conventional methods aggregate the frame-level features by pooling across the time-axis to extract speaker characteristics while factoring out content information. Simple temporal aggregation fails to disentangle the content information and therefore affects the quality of the resulting embeddings. The content information is regarded as unwanted variations hindering the accurate representation of voice characteristics.

To reduce the effects of the content variation, prior works model the phonetic content representation and use it as a reference for speaker embedding extraction. In , the phonetic bottleneck features from the last hidden layer of a pre-trained automatic speech recognition (ASR) network are combined with raw acoustic features to normalize the phonetic variations. A coupled stem is designed in  to jointly learn acoustic features and frame-level ASR bottleneck features. In , a phonetic attention mask dynamically generated from a sub-branch is used to benefit speaker recognition. The existing methods can be summarized into two types in terms of content representation modeling: (1) by apre-trained ASR model [57; 34; 45; 89; 46], and (2) by a jointly trained multi-task model with extra components for content representations [43; 88; 17; 46; 80; 44; 42]. These methods prove that the utilization of content representation benefits speaker recognition. However, both types lead to an obvious limit in practical applications. The employment of a pre-trained ASR model greatly increases the parameters and computation complexity in the inference, as the ASR models are typically one or two orders of magnitude larger than the speaker recognition model [87; 71]. The joint training of the speaker recognition model and extra components for content representations requires either an extra dataset with text labels in addition to the dataset with speaker identities, or one dataset with speaker identities and text labels simultaneously which is expensive and not easy to come by. For the former type of joint training, additional components are still needed with extra effort and may encounter optimization difficulties.

For the purpose of speaker recognition, one aims to derive an embedding vector representing the vocal characteristics of the speaker. The benefits of leveraging content representation are significant, while the drawbacks of text labels and extra model requirements are obvious. Motivated by this, we seek to design a novel framework to solve these problems.

Speech signals consist of many components, of which the two major parts are speaker traits and speech content [32; 14]. In this paper, we focus on decomposing speech signals into static and dynamic components. The former is static, i.e., fixed, with respect to temporal evolution and dominated by speaker characteristics, while the latter mainly consists of speech content with some other components, such as the prosody. Based on this assumption, we design a framework with three Gaussian inference layers which aims to decompose the static and dynamic components of the speech. The static part is modeled by static Gaussian inference with the criterion of speaker classification loss. During inference, the static latent factor is associated with the vocal characteristics of the speaker, and we refer to it as speaker representation. The remaining dynamic components related to verbal content are modeled by dynamic Gaussian inference. The motivation of the three-layer design is simple and intuitive - the speaker representation from _layer 1_ is not accurate and may include content information as the content-related reference is absent in training. However, it helps _layer 2_ extract the content representation which can be used as the reference for _layer 3_. Thus, a more accurate speaker representation is extracted in _layer 3_. It is worth noting that the framework is trained without text labels and no extra model or branch is employed thus not much increase in model size. This is achieved with the guide of self-supervision and considered content representations. We named this framework as **Rec**urrent **Xi**-vector (RecXi). The major contributions are summarized as follows:

* **RecXi: A novel disentangling framework** with the following features: (1) We enhance the xi-vector  with the ability to capture temporal information and name it the recurrent xi-vector layer. (2) A frame-wise content-aware transition model \(_{t}\) is proposed for modeling dynamic components of the speech. (3) A novel design of three layers of Gaussian inference is proposed to disentangle speaker and content representations by static and dynamic modeling with the corresponding counterpart removal, respectively.
* **A self-supervision method** is proposed to guide content disentanglement by preserving speaker traits. It reduces the impact of the absence of text labels.

## 2 Related Work

**Conventional Speaker Recognition Methods.** Speaker embeddings are fixed-length continuous-value vectors extracted from variable-length utterances to represent speaker characteristics. They live in a simpler Euclidean space in which distance can be measured for the comparison between speakers . A well-known example of the extractor is the x-vector framework , which mainly consisted of the following three components: **1). An encoder** is implemented by stacking multiple layers of time-delay neural network (TDNN) and used to extract the frame-level features from utterances . Recent works strengthen the encoder by replacing the simple TDNNs with powerful ECAPA-TDNN  and its variants [41; 74; 23; 53; 47], or ResNet series models [90; 36; 84; 83; 51]. **2). A temporal aggregation layer** aggregates frame-level features from the encoder into fixed-length utterance-level representations. **3). A decoder** classifies the utterance-level representations to speaker classes for supervised learning by a classification loss. The decoder stacks several fully-connected layers including one bottleneck layer used to extract speaker embedding.

**Speech Disentanglement.** Various informational factors are carried by the speech signal, and the speech disentanglement ultimately depends on which informational factors are desired and how they will be used . For speaker recognition, in addition to the use of content information we introduced above, some works attempt to disentangle speaker representation with the removal of irrelevant information, like devices, noise, and channel information with corresponding labels [50; 52; 28].  minimizes the mutual information between speaker and device embeddings, with the goal of reducing their interdependence. In , nuisance variables like gender and accent are removed from speaker embeddings by learning two separate orthogonal representations. Many other speech-relevant tasks also show great improvements by disentangling these two components properly. For speaker diarization, the ASR model is employed to obtain content representations leveraged by speaker embedding extractions [70; 29]. In voice conversion or personalized voice generation tasks, a popular method is to disentangle the speech into linguistic and speaker representations before performing the generation [26; 86; 92; 11; 18; 91; 38]. In ASR, speaker information is extracted for speaker variants removal for performance improvements [49; 35; 25] or privacy preserving . Similarly, auxiliary network-based speaker adaptation  or residual adapter  are explored to handle large variations in the speech signals produced by different individuals. These works have led us to the design of RecXi, which is the first speaker and content disentanglement framework for speaker recognition in the absence of extra labels for practical use to the best of our knowledge.

**Self-Supervision in Speech.** Self-supervised learning has been the dominant approach for utilizing unlabeled data with impressive success [1; 16; 7; 20]. In speaker recognition, _contrastive learning_ is used to force the encoder to produce similar activation between a positive pair. The positive pair can be two disjoint segments from the same utterance [6; 85] or from cross-referencing between speech and face data . The trained speaker encoder also can produce pseudo speaker labels for supervised learning [72; 5]. In target speaker extraction, self-supervision is helpful to find the speech-lip synchronization . In voice conversion, much research focuses on applying _variational auto-encoder_ (VAE) to disentangle the speaker [10; 27; 59]. Similar to reconstruction-based methods, mutual information (MI) and identity change loss are used in  for robust speaker disentanglement. ContentVec  disentangles and removes speaker variations from the speech representation model HuBERT  for various downstream tasks, such as language modeling and zero-shot content probe.

Considering the main objective of this work is to benefit speaker recognition by disentangling the speaker with the speaker classification supervision, we want to clarify that the content disentanglement and corresponding self-supervision serve this final target. Therefore, the self-supervision method is designed to be simple yet effective while avoiding extra signal re-constructors or training efforts.

## 3 Approach

### Xi-vector

Many works have been proposed to estimate the uncertainty for the speaker embedding [65; 33; 78]. Among them, the xi-vector  is proposed to enhance x-vector embedding in handling the uncertainty caused by the random intrinsic and extrinsic variations of human voices. Specifically, an input frame \(_{t}\) is encoded to a point-wise estimate \(_{t}\) in a latent space by an encoder. And an auxiliary network is employed to characterize the frame-level uncertainty with a posterior covariance matrix \(_{t}^{-1}\) associated with \(_{t}\) as shown in the encoder part of Figure 1. These two operations are formulated as

\[_{t}=f_{}(_{t}|_{t}^{t n}),  42.679134pt(1) 42.679134pt_{t}=g_{ }(_{t}|_{t}^{t n}),\] (2)

where the neighbour frames \(_{t}^{t n}\) of time \(t\) are taken into consideration for the frame-wise estimation. The precision matrices \(_{t}\) are assumed to be diagonal and are estimated as log-precision for the convenience of following steps. It is assumed that a _linear Gaussian model_ is responsible to generate the representations \(_{t}\) as follows:

\[_{t}=+_{t},latent\ variable:(_{},_{}^{-1}), uncertainty:_{t}(,_{t}^{-1}).}\] (3)

Here, \(\) is the latent variable for the entire utterance, and \(_{t}\) is a frame-wise random variable for the uncertainty covariance measure, \(_{}\) and \(_{}^{-1}\) are the Gaussian prior mean and covariance matrix of the variable \(\). The posterior mean vector \(_{}\) and precision matrix \(_{}\) for the whole sequence are formulated as

\[_{}=_{}^{-1}[_{t=1}^{T}_{ t}_{t}+_{}_{}], 28.452756pt(4)  42.679134pt_{}=_{t=1}^{T}_{t}+ _{},\] (5)

where the posterior mean \(_{}\) is enriched by frame-wise uncertainty and is further used to be decoded into speaker embedding.

### Disentangling Speaker and Content Representations

**Basic Recurrent Xi-vector Layer.** Xi-vector has the advantage of modeling static components of the speech by the utterance-level speaker representation aggregation with the use of uncertainty estimation, while the drawback of modeling dynamic signals is obvious. For approximating non-linear functions in high-dimensional feature spaces and restricting the inference through the adjacent Gaussian hidden state efficiently, similar to , a learnable linear transition model \(\) is applied. We further extend the xi-vector into a recursive form with frame-level estimation. It is worth noting that xi-vector is a special case of the basic recurrent xi-vector when \(\) is an identity matrix.

The Gaussian inference in the latent space can be implemented in two stages: _predict stage_ and _update stage_. In the _predict stage_, the predictive mean vector \(_{t}^{+}\) and precision matrix \(_{t}^{+}\) are formulated as

\[_{t}^{+}=_{t},\] (6) \[_{t}^{+}=[_{t}^{-1}^{}]^{-1},\] (7)

where \(\) represents a linear transition model, and \({}^{+}\) indicates the results of the given frame \(t\) after the _predict stage_. T indicates a transpose operation.

For sake of clarity, we assign the time (i.e., frame) index \(t\) = 0 to the priors, such that \(_{0}\) = \(_{}\), \(_{0}\) = \(_{}\). In the _update stage_, the posterior mean vector \(_{t}\) of frame \(t\) is derived by incorporating the previously predicted posterior mean and precision \(\{_{t-1}^{+},_{t-1}^{+}\}\) of the hidden states of the former frame \(t-1\) with encoded features \(_{t}\) and estimated uncertainty \(_{t}\) of current frame \(t\). The uncertainty measures \(_{t}\) of frame \(t\) is added into the posterior precision matrix. These two operations are derived as

\[_{t}=_{t}^{-1}[_{t}_{t}+ _{t-1}^{+}_{t-1}^{+}],\] (8) \[_{t}=_{t}+_{t-1}^{+}.\] (9)

**Frame-wise Content-aware Transition Model \(_{t}\).** Speech signals are a complex mixture of dynamic information sources. Even though the transition model \(\) is learnable during training, it remains the same across all the hidden states of Gaussian inference for each sample. To enhance the ability to model dynamic speech components for content disentanglement, we propose a transition

Figure 1: The network architecture of the proposed RecXi system with self-supervision. The structure at the bottom-middle of the figure is a simplified speaker recognition system. The figures in three dotted boxes are the specific explanations of its three parts. At the encoder, \(\{_{1},_{2}..._{T}\}\) is the input sequence of length \(T\). The three colors of blue, orange, and green indicate three recursive layers. For each layer, the inference flow for frames \(_{1}\), \(_{2}\) and \(_{T}\) is drawn, while the frames in between are replaced by ‘...’. The dashed lines indicate recurrent operation and the solid lines represent the operations within the same frame. The block with \(_{t}\) gen indicates the filter generator. For the decoder, \(\) is a subtraction operation. The dotted line indicates the operation is optional.

model \(_{t}\) which is dynamically adjusted by a filter generator for each frame according to the content during the Gaussian inference and hereby named frame-wise content-aware transition model.

Specifically, a set of \(N\) learnable transition matrices \(\{^{},,^{}{}_{2}...^{}{ }_{N}\}\) is employed to model \(N\) dynamic components in speech signals. For each frame \(t\), a vector \(_{t}^{N}\) is generated representing the importance of different dynamic components to content representation modeling, by observing content information \(_{t}\) from _update stage_. The content-aware transition model for frame \(t\) is finally obtained as a weighted sum over the \(N\) component-dependent transition models with weight \(w_{t,n}\).

\[_{t}=_{n=1}^{N}w_{t,n}^{}{}_{n},\] (10) \[w_{t,n}=[_{t}]_{n},\] (11) \[_{t}=(f(_{t})),\] (12)

where \(_{t}^{N}\). \(\) and \(f\) indicate the \(Softmax\) function and a non-linear operation, respectively. In this work, the non-linear operation is designed as two fully connected layers with a ReLU  activation function in between. It is to be noted that this procedure is frame-wise as the dynamic components vary along the sequence of \(T\). As an extension to Equations (6) and (7), the _predict stage_ of recurrent xi-vector is reformulated as

\[_{t}^{+}=_{t}_{t},\] (13) \[_{t}^{+}=[_{t}_{t}^{-1} _{t}^{}]^{-1}.\] (14)

**Three Layers of Gaussian Inference.** We propose a structure based on three layers of Gaussian inference for disentangling speaker and content representations in the absence of text labels. In this work, this structure is utilized in the temporal aggregation layer and named RecXi. The three layers of Gaussian inferences each aim at precursor speaker representation, disentangled content representation, and disentangled speaker representation, as shown in the RecXi part of Figure 1. The last frame's representations from each layer are used for deriving embeddings within the decoder. The details of each layer are discussed as follows.

_Layer 1:_** Precursor Speaker Representation.** This is a basic recurrent xi-vector layer that aims to represent the speaker characteristics. Therefore, the transition model of the Gaussian inference is set as an identity matrix to model static components of the speech. The _predict stage_ can be derived from Equations (6) and (7) as

\[_{t}^{+}=_{t},\] (15) \[_{t}^{+}=_{t}.\] (16)

The speaker representation from this layer is similar to that in the original xi-vector, where the content information from the high-dimensional frame-level features remains and affects the speaker embedding quality. We refer to the representation of this layer as the precursor speaker representation. Specifically, the frame-wise representations of posterior mean \(_{t}\) and precision \(_{t}\) of the hidden state \(\) of frame \(t\) are estimated. They are derived partly according to frame-level features \(_{}\) and uncertainty \(_{}\) extracted from the corresponding frame \(t\), and partly from previous frames \(\{1,2,...t-1\}\) by recursively passing in the posteriors from the previous frame. The _update stage_ is formulated as

\[_{t}=_{t}^{-1}[_{t}_{t}+_{t-1}^{+} _{t-1}^{+}],\] (17) \[_{t}=_{t}+_{t-1}^{+}.\] (18)

_Layer 2:_**Disentangled Content Representation.** This layer aims to disentangle content representation from the sequence. To model dynamic subtle content changes, the frame-wise content-aware transition model \(_{t}\) introduced above is applied. As shown in the orange part of Figure 1, the transition model \(_{t}\) is generated by a filter generator according to Equations (10), (11) and (12). Equations (13) and (14) are applied to the _predict stage_ of _layer 2_.

To disentangle the content representation more effectively, in addition to equipping the layer with dynamic modeling ability, we further attempt to remove speaker information from the frame-level features for each frame during Gaussian inference. This ensures that the remaining information is more likely to be dynamic and associated with the content.

The posterior mean \(^{+}\) and posterior precision \(^{+}\) of the hidden state of _layer 1_ are rich with speaker information and utilized by the _update stage_ of _layer 2_. Benefiting from the linear operations in this three layers design, the speaker removal operation can simply be done by subtracting the posterior mean \(^{+}\) from features \(\) while adding the posterior covariance matrix \((^{+})^{-1}\) into the uncertainty estimation matrix \(^{-1}\). The procedure is dynamically processed for each frame, and the _update stage_ is formulated as \[_{t}=_{t}^{-1}[(_{t}^{-1}+(_{t}^{+})^{-1})^{- 1}(_{t}-_{t}^{+})+_{t-1}^{+}_{t-1}^{+}],\] (19)

\[_{t}=(_{t}^{-1}+(_{t}^{+})^{-1})^{-1}+_{t-1}^{+}.\] (20)

_Layer 3:_**Disentangled Speaker Representation.** This layer uses the Gaussian inference with an identity matrix for modeling static components of the speech. Furthermore, the speaker classification loss is applied to the output of this layer and provides supervision restrictions. Different from _layer 1_, _layer 3_ is designed to model the desired disentangled speaker representation by removing the content information provided by _layer 2_ from the frame-level features. The procedure is similar to that in _layer 2_, and the _predict stage_ is formulated as

\[_{t}^{+}=_{t},\] (21)

\[}_{t}^{+}=}_{t},\] (22)

where \(_{t}^{+}\) and \(}_{t}^{+}\) are the posterior mean and posterior precision, respectively. The symbol of \(\) is used to differentiate them from the posteriors in _layer 1_. The _update stage_ is formulated as

\[_{t}=}_{t}^{-1}[(_{t}^{-1}+( _{t}^{+})^{-1})^{-1}(_{t}-_{t}^{+})+}_{t-1}^{+}_{t-1}^{+}],\] (23)

\[}_{t}=(_{t}^{-1}+(_{t}^{+})^{-1})^{-1 }+}_{t-1}^{+}.\] (24)

It is worth noting that to avoid expensive matrix multiplication operations and numerically problematic matrix inversions, a simplified implementation is adopted (see Appendix A).

### Speech Disentanglement with Self-supervision

A well-trained speaker embedding neural network requires a huge number of speakers and utterances to achieve discriminative ability. For such a large dataset, text labels are very difficult to come by. In the absence of text labels, disentangling content information from the speech is a very difficult task.

In Section 3.2, the _layer 3_ of RecXi is designed to be the desired speaker representation. This is achieved by static Gaussian inference with the assumption that disentangled content representation provided by _layer 2_ is reliable. We note that the disentangled speaker representation from _layer 3_ is directly optimized through the classification criterion, while the optimization for disentangling content information is indirect through content removal operations in Gaussian inference. In order to ameliorate the shortcoming due to the absence of text label and to provide an extra supervisor for _layer 2_, we propose a self-supervision method to preserve speaker information via knowledge distillation in a similar fashion to those proposed in [62; 76; 8] for the teacher-student pair.

Generally speaking, large models usually outperform small models, while the small model is computationally cheaper . Knowledge distillation aims to benefit a small model with the guidance of a large model. Different from the general knowledge distillation methods, our 'teacher' and'student' are two different layers within the RecXi. Since this guidance comes from the RecXi itself, and all the training data is considered as unlabelled data for the content disentanglement task, the proposed method is considered a self-supervision method .

The output \(\) from _layer 1_ of RecXi is precursor speaker representation, and the content information remains, while _layer 2_ is designed to disentangle content representation \(\). Benefiting from the linear operations used in RecXi, we can remove content information and preserve speaker representation by subtracting the content representation \(\) of _layer 2_ from the precursor speaker representation \(\) of _layer 1_. This speaker representation is marked as \(_{}\) and derived as

\[_{}=-\] (25)

where the '\(\)' indicates that it is obtained by a **line**er operation, differing from \(\) in _layer 3_ which is disentangled by Gaussian inference. For the same input sequence, the speaker representation \(_{}\) derived by this linear operation should be consistent with \(\) obtained from disentanglement. Therefore, by restricting their difference, the gradient from supervision speaker classification loss will form a guide for _layer 2_ through the linear operation in Equation (25). This serves as an extra supervisory signal different from the constraints imposed by speaker removal operation during the Gaussian inference. As \(\) is optimized by classification loss directly, it is considered as a 'teacher',while \(_{ lin}\) is a'student'. Since \(_{ lin}\) is derived by preserving speaker representation in Equation (25), we name the loss as self-supervision speaker preserving loss \(_{ ssp}\).

The \(_{ ssp}\) loss can be generated by different knowledge distillation methods, a simple comparison is available in Appendix D. In this work, the idea of similarity-preserving loss  is in line with our layer-wise design and is inherited to guide the student towards the activation correlations induced in the teacher, instead of mimicking the teacher's representation space directly. It is derived as

\[_{ ssp}=}_{(s,s^{})}\| (\|^{(s)}^{(s)}\|_{2}-\| ^{(s^{})}_{ lin}^{(s^{})}_{  lin}\|_{2})\|_{F}^{2},\] (26)

where \(\) collects all the \((s,s^{})\) pairs from the same mini-batch. \(\|\|_{2}\) and \(\|\|_{F}\) indicate the row-wise L2 normalization and Frobenius norm, respectively. T indicates a transpose operation and \(b\) indicates the batch size. We define the total loss for training the framework as

\[_{ total}=_{ cls}+_{ ssp},\] (27)

where \(_{cls}\) is the speaker classification loss, \(\) and \(\) are hyperparameters for balancing the total loss.

## 4 Experiments Setup

### Dataset, Training Strategy, and Evaluation Protocol

The experiments are conducted on VoxCeleb1 , VoxCeleb2 , and the Speaker in the Wild (SITW)  datasets. It is worth noting that the text labels are not available for these datasets. For a fair comparison, the baselines are all re-implemented and trained with the same strategy as RecXi which follows that in ECAPA-TDNN . All the models are evaluated by the performance in terms of equal error rate (EER) and the minimum detection cost function (minDCF). Detailed descriptions of datasets, training strategy, and evaluation protocol are available in Appendix C.

### Systems Description

We evaluate the systems that are combinations of different backbone models in the encoder and different aggregation layers. To verify the compatibility of the proposed RecXi, both 2D convolution (Conv2D)-based and time delay neural network (TDNN)-based backbones are adopted:

* **1) ECAPA-TDNN** is a state-of-the-art speaker recognition model based on TDNNs. The layers before the temporal aggregation layer are considered the backbone network.
* **2) ResNet  and tResNet** represent the models based on 2D convolution. The modified ResNet34  is adopted as a baseline. In order to model more local regions with larger frequency bandwidths at different scales, we further modify the ResNet34 in  by simply changing the stride strategy and name itResNet. The details are provided in Appendix B.

The following are temporal aggregation layers. The first three are considered baselines:

* **1) Temporal Statistics Pooling** (term as TSP from here onwards)  is a well-known aggregation method. It is the default option for x-vector and adopted in ResNet for speaker recognition [12; 90].
* **2) Channel- and Context-dependent Statistics Pooling** (term as Chan.&Con. from here onwards)  is the default aggregation layer in ECAPA-TDNN, modified from the attentive statistics pooling  by adding channel-dependent frame attention and allowing the self-attention produced across global properties.
* **3) Xi-vector Posterior Inference** (term as Xi from here onwards)  inserts uncertainty estimation into speaker embeddings as introduced in Section 3.1.
* **4) RecXi (\(\)) and RecXi (\(\), \(_{ lin}\))** are the proposed methods. The former uses only \(\) as the input to the decoder to derive the speaker embedding, and the latter uses a concatenation of both \(\) and \(_{ lin}\) as the input.

[MISSING_PAGE_FAIL:8]

are reported in Table 2. Compared to the original aggregation layer of ECAPA-TDNN (system #1), the use of xi-vector posterior inference (system #10) performs slightly better. The comparison between system #10 and #11 presents that for the ECAPA-TDNN backbone, the proposed RecXi achieves average EER/minDCF reductions of 12.15%/10.66% over the Xi baseline.

**Brief summary.** Since the Xi baseline performs the best among all baselines and SOTA systems discussed above with different training conditions and backbone networks, we refer to it as a SOTA baseline for the following discussion. We observe that compared to the SOTA baseline, the proposed RecXi consistently improves the performance for both backbone networks on all four test sets with overall average EER/minDCF reductions of 9.56%/8.24% (system #4 vs. #5 and #10 vs. #11). The experiment results remain consistent regardless of the backbone types and whether augmented data is used. This may be due to the fact that the proposed disentanglement framework disentangles the static speaker components effectively and benefits speaker recognition. As the speaker is disentangled under the assistance of disentangled content representation, the significant improvement also proves the quality of the dynamic counterpart modeling. In addition, we found that the performance of tResNet-based systems is generally better than that of the systems with ECAPA-TDNN backbone.

**Ablation Study.** We perform ablation studies on the proposed RecXi. As the results shown in Table 3 are detailed but not intuitive, we show Figure 2 for a better view. From the charts, the following conclusions are summarized and are consistent for both ECAPA-TDNN and tResNet backbones:

    & Aggregation & _{}\)} & }{}\)} &  &  &  &  \\  & Layer & & & & EER & minDCF & EER & minDCF & EER & minDCF & EER & minDCF \\ 
11 & ECAPA & RecXi(\(,_{}\)) & ✓ & 6.43 & 2.467 & **0.227** & 1.292 & 0.141 & **2.105** & **0.184** & -7.58\% & -12.18\% \\
12 & ECAPA & RecXi(\(\)) & ✓ & 6.13 & **2.445** & 0.233 & **1.286** & **0.139** & 2.160 & 0.191 & -7.30\% & -11.04\% \\
13 & ECAPA & RecXi(\(,_{}\)) & ✗ & 6.43 & 2.477 & 0.228 & 1.326 & 0.142 & 2.351 & 0.196 & -3.41\% & -10.40\% \\
14 & ECAPA & RecXi(\(\)) & ✗ & 6.13 & 2.498 & 0.229 & 1.325 & 0.146 & 2.597 & 0.273 & Benchmark \\ 
5 & tResNet & RecXi(\(\)) & ✓ & 7.06 & **2.097** & 0.196 & **1.197** & **0.124** & 1.832 & **0.172** & -5.78\% & -5.83\% \\
15 & tResNet & RecXi(\(\)) & ✓ & 6.73 & 2.117 & **0.192** & 1.215 & 0.128 & **1.750** & 0.177 & -6.35\% & -4.82\% \\
16 & tResNet & RecXi(\(\)) & ✗ & 7.06 & 2.130 & 0.198 & 1.222 & 0.128 & 1.886 & 0.184 & -3.71\% & -2.38\% \\
17 & tResNet & RecXi(\(\)) & ✗ & 6.73 & 2.185 & 0.204 & 1.230 & 0.128 & 2.050 & 0.193 & Benchmark \\   

Table 3: Performance in EER(%) and minDCF of various RecXi systems on VoxCeleb1 and SITW test sets for ablation study. # is the index number for the system. BN and Para. indicate the type of backbone network and number of parameters in million, respectively.

Figure 2: Bar charts for ablation studies. Performance in EER(%) and minDCF of proposed RecXi under different conditions are drawn. The blue and orange each indicate RecXi(\(\)) and RecXi(\(,_{}\)). Patterned color bars at the left front and solid color bars at the right behind represent the performance with and without \(_{}\) for the same test trial, respectively. For all bars, the shorter the better.

1) As illustrated in Figure 2, by comparing the solid color bars with the patterned color bars while ignoring the difference of the colors, we find that most patterned color bars are shorter. It indicates that the systems with proposed self-supervision \(_{ ssp}\) perform better than those without. It also proves the effectiveness of \(_{ ssp}\) for RecXi frameworks. The results in Table 3 shows that the proposed self-supervision \(_{ ssp}\) leads to an overall 5.07%/5.44% average reductions (system #11 vs. #13, #12 vs. #14, #5 vs. #16, #15 vs. #17) in EER/minDCF for all RecXi systems.

2) By comparing RecXi(\(\)) shown as the solid blue color bars and RecXi (\(\), \(_{ lin}\)) shown as solid orange color bars, we conclude that when \(_{ ssp}\) is not applied, the systems derived only from \(\) perform worse than those using both \(\) and \(_{ lin}\) in most of the trials (system #13 vs. #14 and #16 vs. #17). When we consider the patterned color bars, however, we find that these gaps are overcome, and very similar performance is achieved when \(_{ ssp}\) is applied. We believe that the gaps between RecXi(\(\))-based systems and RecXi(\(\), \(_{ lin}\))-based systems are caused by the constraints on the content disentanglement layer provided by \(_{ lin}\). This brings RecXi (\(\), \(_{ lin}\)) systems an advantage in content disentanglement and further improves the efficiency of disentangling speaker representation. When both kinds of systems are enhanced by the self-supervision loss \(_{ ssp}\), a sufficient guide for disentangling content representations is provided, and it leads to a similar and improved performance. It shows the effectiveness and superiority of the proposed \(_{ ssp}\) and also proves that the speaker embedding benefits from the quality of disentangled content.

Additional experiments are provided in the Appendix, covering diverse aspects including visualization, the ablation study on three RecXi layers, comparisons between RecXi and SOTA systems using ASR models or contrastive learning, as well as the evaluation of the effectiveness of \(_{t}\). For details, please refer to Appendix F, G, H, and I, respectively.

## 6 Limitations

This work has some limitations: 1). The novel \(_{ ssp}\) is intuitive and effective but needs further investigation and improvement. 2). As mentioned in Appendix C.2, the number of mini-transition models for deriving \(_{t}\) is set as \(N=16\). In Appendix I, we verify the effectiveness and necessity of utilizing the proposed \(_{t}\). However, a comprehensive investigation of this hyperparameter is not conducted. This hyperparameter can be further exploited as it is related to the acoustic features and dynamic components we wish to disentangle.

The discussion about broader impacts is available in Appendix E.

## 7 Conclusion and Future Work

We propose the RecXi - a Gaussian inference-based disentanglement learning neural network. It models the dynamic and static components in speech signals with the aim to disentangle vocal and verbal information in the absence of text labels and benefit the speaker recognition task. In addition, a novel self-supervised speaker-preserving method is proposed to relieve the effect of text labels absent for fine content representation disentanglement. The experiments conducted on both VoxCeleb and SITW datasets prove the consistent superiority of the proposed RecXi and the effectiveness of the proposed self-supervision method. We expect that the proposed model is applicable to automatic speech recognition (ASR), where speaker-independent representation is desirable. In addition, the disentangled content and speaker embeddings are useful in the voice conversion and speech synthesis tasks. For future work, we plan to reconstruct speech signals and utilize the interaction between these two tasks to benefit each other.

## 8 Acknowledgements

This work is supported by the Agency for Science, Technology and Research (A\({}^{*}\)STAR), Singapore, through its Council Research Fund (Project No. CR-2021-005). Haizhou Li is in part supported by the National Natural Science Foundation of China (Grant No. 62271432) and the Agency for Science, Technology and Research (A\({}^{*}\)STAR) under its AME Programmatic Funding Scheme (Project No. A18A2b0046). The authors would like to thank the reviewers and the meta-reviewer for their comments, which greatly improved the article.