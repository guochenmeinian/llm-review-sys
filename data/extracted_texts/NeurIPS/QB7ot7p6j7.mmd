# DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification

Mintong Kang

UIUC

mintong2@illinois.edu

&Dawn Song

UC Berkeley

dawnsong@berkeley.edu

&Bo Li

UIUC

lbo@illinois.edu

###### Abstract

Diffusion-based purification defenses leverage diffusion models to remove crafted perturbations of adversarial examples and achieve state-of-the-art robustness. Recent studies show that even advanced attacks cannot break such defenses effectively, since the purification process induces an extremely deep computational graph which poses the potential problem of vanishing/exploding gradient, high memory cost, and unbounded randomness. In this paper, we propose an attack technique DiffAttack to perform effective and efficient attacks against diffusion-based purification defenses, including both DDPM and score-based approaches. In particular, we propose a deviated-reconstruction loss at intermediate diffusion steps to induce inaccurate density gradient estimation to tackle the problem of vanishing/exploding gradients. We also provide a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient backpropagation. We validate the attack effectiveness of DiffAttack compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that DiffAttack decreases the robust accuracy of models compared with SOTA attacks by over 20% on CIFAR-10 under \(_{}\) attack \((=8/255)\), and over 10% on ImageNet under \(_{}\) attack \((=4/255)\). We conduct a series of ablations studies, and we find 1) DiffAttack with the deviated-reconstruction loss added over uniformly sampled time steps is more effective than that added over only initial/final steps, and 2) diffusion-based purification with a moderate diffusion length is more robust under DiffAttack.

## 1 Introduction

Since deep neural networks (DNNs) are found vulnerable to adversarial perturbations [52; 20], improving the robustness of neural networks against such crafted perturbations has become important, especially in safety-critical applications [18; 5; 54]. In recent years, many defenses have been proposed, but they are attacked again by more advanced adaptive attacks [7; 30; 11; 12]. One recent line of defense (_diffusion-based purification_) leverages diffusion models to purify the input images and achieves the state-of-the-art robustness. Based on the type of diffusion models the defense utilizes, diffusion-based purification can be categorized into _score-based purification_ which uses the score-based diffusion model  and _DDPM-based purification_[4; 62; 57; 51; 55; 56] which uses the denoising diffusion probabilistic model (DDPM) . Recent studies show that even the most advanced attacks [12; 34] cannot break these defenses due to the challenges of vanishing/exploding gradient, high memory cost, and large randomness. In this paper, we aim to explore the vulnerabilities of such diffusion-based purification defenses, and _design a more effective and efficient adaptive attack against diffusion-based purification_, which will help to better understand the properties of diffusion process and motivate future defenses.

In particular, the diffusion-based purification defenses utilize diffusion models to first diffuse the adversarial examples with Gaussian noises and then perform sampling to remove the noises. In this way, the hope is that the crafted adversarial perturbations can also be removed since the trainingdistribution of diffusion models is clean [49; 25]. The diffusion length (i.e., the total diffusion time steps) is usually large, and at each time step, the deep neural network is used to estimate the gradient of the data distribution. This results in an extremely deep computational graph that poses great challenges of attacking it: _vanishing/exploding gradients_, _unavailable memory cost_, and _large randomness_. To tackle these challenges, we propose a deviated-reconstruction loss and a segment-wise forwarding-backwarding algorithm and integrate them as an effective and efficient attack technique _DiffAttack_.

Essentially, our **deviated-reconstruction loss** pushes the reconstructed samples away from the diffused samples at corresponding time steps. It is added at multiple intermediate time steps to relieve the problem of vanishing/exploding gradients. We also theoretically analyze the connection between it and the score-matching loss , and we prove that maximizing the deviated-reconstruction loss induces inaccurate estimation of the density gradient of the data distribution, leading to a higher chance of attacks. To overcome the problem of large memory cost, we propose a **segment-wise forwarding-backwarding** algorithm to backpropagate the gradients through a long path. Concretely, we first do a forward pass and store intermediate samples, and then iteratively simulate the forward pass of a segment and backward the gradient following the chain rule. Ignoring the memory cost induced by storing samples (small compared with the computational graph), our approach achieves \((1)\) memory cost.

Finally, we integrate the deviated-reconstruction loss and segment-wise forwarding-backwarding algorithm into DiffAttack, and empirically validate its effectiveness on CIFAR-10 and ImageNet. We find that (1) DiffAttack outperforms existing attack methods [34; 60; 53; 1; 2] by a large margin for both the score-based purification and DDPM-based purification defenses, especially under large perturbation radii; (2) the memory cost of our efficient segment-wise forwarding-backwarding algorithm does not scale up with the diffusion length and saves more than 10x memory cost compared with the baseline ; (3) a moderate diffusion length benefits the robustness of the diffusion-based purification since longer length will hurt the benign accuracy while shorter length makes it easier to be attacked; (4) attacks with the deviated-reconstruction loss added over uniformly sampled time steps outperform that added over only initial/final time steps. The effectiveness of DiffAttack and interesting findings will motivate us to better understand and rethink the robustness of diffusion-based purification defenses.

We summarize the main _technical contributions_ as follows:

* We propose DiffAttack, a strong evasion attack against the diffusion-based adversarial purification defenses, including score-based and DDPM-based purification.
* We propose a deviated-reconstruction loss to tackle the problem of vanishing/exploding gradient, and theoretically analyze its connection with data density estimation.
* We propose a segment-wise forwarding-backwarding algorithm to tackle the high memory cost challenge, and we are the _first_ to adaptively attack the DDPM-based purification defense, which is hard to attack due to the high memory cost.
* We empirically demonstrate that DiffAttack outperforms existing attacks by a large margin on CIFAR-10 and ImageNet. Particularly, DiffAttack decreases the model robust accuracy by over \(20\%\) for \(_{}\) attack \((=8/255)\) on CIFAR-10, and over 10% on ImageNet under \(_{}\) attack \((=4/255)\).
* We conduct a series of ablation studies and show that (1) a moderate diffusion length benefits the model robustness, and (2) attacks with the deviated-reconstruction loss added over uniformly sampled time steps outperform that added over only initial/final time steps.

## 2 Preliminary

There are two types of diffusion-based purification defenses, **DDPM-based purification**, and **score-based purification**, which leverage _DDPM_[46; 25] and _score-based diffusion model_ to purify the adversarial examples, respectively. Next, we will introduce the basic concepts of DDPM and score-based diffusion models.

Denote the diffusion process indexed by time step \(t\) with the _diffusion length_\(T\) by \(\{_{t}\}_{t=0}^{T}\). DDPM constructs a discrete Markov chain \(\{_{t}\}_{t=0}^{T}\) with discrete time variables \(t\) following \(p(_{t}|_{t-1})=(_{t};}_{t-1},_{t})\) where \(_{t}\) is a sequence of positive noise scales (e.g., linear scheduling, cosine scheduling ). Considering \(_{t}:=1-_{t}\), \(_{t}:=_{s=1}^{t}_{s}\), and \(_{t}=(1-_{t-1})/(1-_{t})}\), the reverse process (i.e., sampling process) can be formulated as:

\[_{t-1}=}}(_{t}-}{_{t}}}_{}(_{t},t)) +_{t}\] (1)

where \(\) is drawn from \((,)\). \(_{}\) parameterized with \(\) is the model to approximate the perturbation \(\) in the diffusion process and is trained via the _density gradient loss_\(_{d}\):

\[_{d}=_{t,}[^{2}}{2 _{t}^{2}_{t}(1-_{t})}\|-_{} (_{t}}_{0}+_{t}},t)\|_{2}^{2}]\] (2)

where \(\) is drawn from \((,)\) and \(t\) is uniformly sampled from \([T]:=\{1,2,...,T\}\).

Score-based diffusion model formulates diffusion models with stochastic differential equations (SDE). The diffusion process \(\{_{t}\}_{t=0}^{T}\) is indexed by a continuous time variable \(t\). The diffusion process can be formulated as:

\[d=f(,t)dt+g(t)d\] (3)

where \(f(,t):^{n}^{n}\) is the drift coefficient characterizing the shift of the distribution, \(g(t)\) is the diffusion coefficient controlling the noise scales, and \(\) is the standard Wiener process. The reverse process is characterized via the reverse time SDE of Equation (3):

\[d=[f(,t)-g(t)^{2}_{} p_{t}()]dt+g(t)d\] (4)

where \(_{} p_{t}()\) is the time-dependent score function that can be approximated with neural networks \(_{}\) parameterized with \(\), which is trained via the score matching loss \(_{s}\)[26; 47]:

\[_{s}=_{t}[(t)_{_{t}| _{0}}\|_{}(_{t},t)-_{_{t }}(p(_{t}|_{0}))\|_{2}^{2}]\] (5)

where \(:\) is a weighting function and \(t\) is uniformly sampled over \(\).

## 3 DiffAttack

### Evasion attacks against diffusion-based purification

A class of defenses leverages generative models for adversarial purification [43; 48; 45; 60]. The adversarial images are transformed into latent representations, and then the purified images are sampled starting from the latent space using the generative models. The process is expected to remove the crafted perturbations since the training distribution of generative models is assumed to be clean. With diffusion models showing the power of image generation recently [15; 39], diffusion-based adversarial purification has achieved SOTA defense performance [34; 4].

We first formulate the problem of evasion attacks against diffusion-based purification defenses. Suppose that the process of diffusion-based purification, including the diffusion and reverse process, is denoted by \(P:^{n}^{n}\) where \(n\) is the dimension of the input \(_{0}\), and the classifier is denoted by \(F:^{n}[K]\) where \(K\) is the number of classes. Given an input pair \((_{0},y)\), the adversarial example \(}_{0}\) satisfies:

\[*{arg\,max}_{i[K]}F_{i}(P(}_{0})) y s.t.\;d(_{0},}_{0})_{max}\] (6)

where \(F_{i}()\) is the \(i\)-th element of the output, \(d:^{n}^{n}\) is the distance function in the input space, and \(_{max}\) is the perturbation budget.

Since directly searching for the adversarial instance \(}_{0}\) based on Equation (6) is challenging, we often use a surrogate loss \(\) to solve an optimization problem:

\[_{}_{0}}(F(P(}_{0})),y) s.t.\;d(_{0},}_{0})_{max}\] (7)

where \(P()\) is the purification process with DDPM (Equation (1)) or score-based diffusion (Equations (3) and (4)), and the surrogate loss \(\) is often selected as the classification-guided loss, such as CW loss , Cross-Entropy loss and difference of logits ratio (DLR) loss . Existing adaptive attack methods such as PGD  and APGD attack  approximately solve the optimization problem in Equation (7) via computing the gradients of loss \(\) with respect to the decision variable \(}_{0}\) and iteratively updating \(}_{0}\) with the gradients.

However, we observe that the gradient computation for the diffusion-based purification process is challenging for _three_ reasons: 1) the long sampling process of the diffusion model induces an extremely deep computational graph which poses the problem of vanishing/exploding gradient , 2) the deep computational graph impedes gradient backpropagation, which requires high memory cost , and 3) the diffusion and sampling process introduces large randomness which makes the calculated gradients unstable and noisy.

To address these challenges, we propose a deviated-reconstruction loss (in Section 3.2) and a segment-wise forwarding-backwarding algorithm (in Section 3.3) and design an effective algorithm DiffAttack by integrating them into the attack technique (in Section 3.4).

### Deviated-reconstruction loss

In general, the surrogate loss \(\) in Equation (7) is selected as the classification-guided loss, such as CW loss, Cross-Entropy loss, or DLR loss. However, these losses can only be imposed at the classification layer, and induce the problem of vanishing/exploding gradients  due to the long diffusion length. Specifically, the diffusion purification process induces an extremely deep graph. For example, DiffPure applies hundreds of iterations of sampling and uses deep UNet with tens of layers as score estimators. Thus, the computational graph consists of thousands of layers, which could cause the problem of gradient vanishing/exploding. Similar gradient problems are also mentioned with generic score-based generative purification (Section 4, 5.1 in ). Backward path differentiable approximation (BPDA) attack  is usually adopted to overcome such problems, but the surrogate model of the complicated sampling process is hard to find, and a simple identity mapping function is demonstrated to be ineffective in the case .

To overcome the problem of exploding/vanishing gradients, we attempt to impose intermediate guidance during the attack. It is possible to build a set of classifiers on the intermediate samples in the reverse process and use the weighted average of the classification-guided loss at multiple layers as the surrogate loss \(\). However, we observe that the intermediate samples are noisy, and thus using classifier \(F\) that is trained on clean data cannot provide effective gradients. One solution is to train a set of classifiers with different noise scales and apply them to intermediate samples to impose classification-guided loss, but the training is too expensive considering the large diffusion length and variant noise scales at different time steps. Thus, we propose a deviated-reconstruction loss to address the challenge via imposing discrepancy for samples between the diffusion and reverse processes adversarially to provide effective loss at intermediate time steps.

Concretely, since a sequence of samples is generated in the diffusion and reverse processes, effective loss imposed on them would relieve the problem of vanishing/exploding gradient and benefit the optimization. More formally, let \(_{t}\), \(^{}_{t}\) be the samples at time step \(t\) in the diffusion process and the reverse process, respectively. Formally, we maximize the deviated-reconstruction loss

Figure 1: DiffAttack against diffusion-based adversarial purification defenses. DiffAttack features the _deviated-reconstruction loss_ that addresses vanishing/exploding gradients and the _segment-wise forwarding-backwarding algorithm_ that leads to memory-efficient gradient backpropagation.

formulated as follows:

\[_{dev}=_{t}[(t)_{_{t},^{}_{t}|_{0}}d(_{t},^{}_{t})]\] (8)

where \(()\) is time-dependent weight coefficients and \(d(_{t},^{}_{t})\) is the distance between noisy image \(_{t}\) in the diffusion process and corresponding sampled image \(^{}_{t}\) in the reverse process. The expectation over \(t\) is approximated by taking the average of results at uniformly sampled time steps in \([0,T]\), and the loss at shallow layers in the computational graph (i.e., large time step \(t\)) helps relieve the problem of vanishing/exploding gradient. The conditional expectation over \(_{t},^{}_{t}\) given \(_{0}\) is approximated by purifying \(_{0}\) multiple times and taking the average of the loss.

Intuitively, the deviated-reconstruction loss in Equation (8) pushes the reconstructed sample \(^{}_{t}\) in the reverse process away from the sample \(_{t}\) at the corresponding time step in the diffusion process, and finally induces an inaccurate reconstruction of the clean image. Letting \(q_{t}()\) and \(q^{}_{t}()\) be the distribution of \(_{t}\) and \(^{}_{t}\), we can theoretically prove that the distribution distance between \(q_{t}()\) and \(q^{}_{t}()\) positively correlates with the score-matching loss of the score-based diffusion or the density gradient loss of the DDPM. In other words, maximizing the deviated-reconstruction loss in Equation (8) induces inaccurate data density estimation, which results in the discrepancy between the sampled distribution and the clean training distribution.

**Theorem 1**.: _Consider adversarial sample \(}_{0}:=_{0}+\), where \(_{0}\) is the clean example and \(\) is the perturbation. \(p_{t}(),p^{}_{t}(),q_{t}(),q^{}_{t}( )\) are the distribution of \(_{t},^{}_{t},}_{t},}^{}_{t}\) where \(^{}_{t}\) represents the reconstruction of \(_{t}\) in the reverse process. \(D_{TV}(,)\) measures the total variation distance. Given a VP-SDE parameterized by \(()\) and the score-based model \(_{}\) with mild assumptions that \(\|_{} p_{t}()-_{}(,t)\|_{2}^{2} L_{u}\), \(D_{TV}(p_{t},p^{}_{t})_{re}\), and a bounded score function by \(M\) (specified in Appendix C.1), we have:_

\[D_{TV}(q_{t},q^{}_{t})_{t,| _{0}}\|_{}(,t)-_{}  q^{}_{t}()\|_{2}^{2}+C_{1}}+\| \|_{2}^{2}\}}+_{re}\] (9)

\(C_{1}=(L_{u}+8M^{2})_{t}^{T}(t)dt\)_, \(C_{2}=(8(1-_{s=1}^{t}(1-_{s})))^{-1}\)._

Proof sketch.: We first use the triangular inequality to upper bound \(D_{TV}(q_{t},q^{}_{t})\) with \(D_{TV}(q_{t},p_{t})+D_{TV}(p_{t},p^{}_{t})+D_{TV}(p^{}_{t},q^{ }_{t})\). \(D_{TV}(q_{t},p_{t})\) can be upper bounded by a function of the Hellinger distance \(H(q_{t},p_{t})\), which can be calculated explicitly. \(D_{TV}(p_{t},p^{}_{t})\) can be upper bounded by the reconstruction error \(_{re}\) by assumption. To upper bound \(D_{TV}(p^{}_{t},q^{}_{t})\), we can leverage Pinker's inequality to alternatively upper bound the KL-divergence between \(p^{}_{t}\) and \(q^{}_{t}\) which can be derived by using the Fokker-Planck equation  in the reverse SDE.

_Remark_.: A large deviated-reconstruction loss can indicate a large total variation distance \(D_{TV}(q_{t},q^{}_{t})\), which is the lower bound of a function with respect to the score-matching loss \(_{t,}\|_{}(,t)-_{ } q^{}_{t}()\|_{2}^{2}\) (in RHS of Equation (9)). Therefore, we show that maximizing the deviated-reconstruction loss implicitly maximizes the score-matching loss, and thus induces inaccurate data density estimation to perform an effective attack. The connection of deviated-reconstruction loss and the density gradient loss for DDPM is provided in Thm. 3 in Appendix C.2.

### Segment-wise forwarding-backwarding algorithm

Adaptive attacks against diffusion-based purification require gradient backpropagation through the forwarding path. For diffusion-based purification, the memory cost scales linearly with the diffusion length \(T\) and is not feasible in a realistic application. Therefore, existing defenses either use a surrogate model for gradient approximation [55; 56; 60; 45] or consider adaptive attacks only for a small diffusion length , but the approximation can induce error and downgrade the attack performance a lot. Recently, DiffPure  leverages the adjoint method  to backpropagate the gradient of SDE within reasonable memory cost and enables adaptive attacks against score-based purification. However, it cannot be applied to a discrete process, and the memory-efficient gradient backpropagation algorithm is unexplored for DDPM. Another line of research [9; 8; 19] proposes the technique of gradient checkpointing to perform gradient backpropagation with memory efficiency. Fewer activations are stored during forwarding passes, and the local computation graph is constructed via recomputation. However, we are the first to apply the memory-efficient backpropagation technique to attack diffusion purification defenses and resolve the problem of memory cost during attacks, which is realized as a challenging problem by prior attacks against purification defenses [34; 60]. Concretely, we propose a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient computation of the attack loss with respect to the adversarial examples.

We first feed the input \(_{0}\) to the diffusion-based purification process and store the intermediate samples \(_{1},_{2},...,_{T}\) in the diffusion process and \(_{T}^{},_{T-1}^{},...,_{0}^{}\) in the reverse process sequentially. For ease of notation, we have \(_{t+1}=f_{d}(_{t})\) and \(_{t}^{}=f_{r}(_{t+1}^{})\) for \(t[0,T-1]\). Then we can backpropagate the gradient iteratively following:

\[}{_{t+1}^{}}=}{_{t}^{}}_{t}^{ }}{_{t+1}^{}}=}{ _{t}^{}}(_{t+1}^{ })}{_{t+1}^{}}\] (10)

At each time step \(t\) in the reverse process, we only need to store the gradient \(/_{t}^{}\), the intermediate sample \(_{t+1}^{}\) and the model \(f_{r}\) to construct the computational graph. When we backpropagate the gradients at the next time step \(t+1\), the computational graph at time step \(t\) will no longer be reused, and thus, we can release the memory of the graph at time step \(t\). Therefore, we only have _one segment of the computational graph_ used for gradient backpropagation in the memory at each time step. We can similarly backpropagate the gradients in the diffusion process. Ignoring the memory cost of storing intermediate samples (usually small compared to the memory cost of computational graphs), the memory cost of our segment-wise forwarding-backwarding algorithm is \((1)\) (validated in Figure 3).

We summarize the detailed procedures in Algorithm 1 in Appendix B. It can be applied to gradient backpropagation through any discrete Markov process with a long path. Basically, we _1) perform the forward pass and store the intermediate samples, 2) allocate the memory of one segment of the computational graph in the memory and simulate the forwarding pass of the segment with intermediate samples, 3) backpropagate the gradients through the segment and release the memory of the segment, and 4) go to step 2 and consider the next segment until termination_.

### DiffAttack Technique

Currently, AutoAttack  holds the state-of-the-art attack algorithm, but it fails to attack the diffusion-based purification defenses due to the challenge of _vanishing/exploding gradient, memory cost_ and _large randomness_. To specifically tackle the challenges, we integrate the deviated-reconstruction loss (in Section 3.2) and the segment-wise forwarding-backwarding algorithm (in Section 3.3) as an attack technique _DiffAttack_ against diffusion-based purification, including the score-based and DDPM-based purification defenses. The pictorial illustration of DiffAttack is provided in Figure 1.

Concretely, we maximize the surrogate loss \(\) as the optimization objective in Equation (7):

\[=_{cls}+_{dev}\] (11)

where \(_{cls}\) is the CE loss or DLR loss, \(_{dev}\) is the deviated-reconstruction loss formulated in Equation (8), and \(\) is the weight coefficient. During the optimization, we use the segment-wise forwarding-backwarding algorithm for memory-efficient gradient backpropagation. Note that \(_{dev}\) suffers less from the gradient problem compared with \(_{cls}\), and thus the objective of \(_{dev}\) can be optimized more precisely and stably, but it does not resolve the gradient problem of \(_{cls}\). On the other hand, the optimization of \(_{dev}\) benefits the optimization of \(_{cls}\) in the sense that \(_{dev}\) can induce a deviated reconstruction of the image with a larger probability of misclassification. \(\) controls the balance of the two objectives. A small \(\) can weaken the deviated-reconstruction object and make the attack suffer more from the vanishing/exploded gradient problem, while a large \(\) can downplay the guidance of the classification loss and confuse the direction towards the decision boundary of the classifier.

_Attack against randomized diffusion-based purification._ DiffAttack tackles the randomness problem from two perspectives: 1) sampling the diffused and reconstructed samples across different time steps multiple times as in Equation (8) (similar to EOT ), and 2) optimizing perturbations for all samples including misclassified ones in all steps. Perspective 1) provides a more accurate estimation of gradients against sample variance of the diffusion process. Perspective 2) ensures a more effective and stable attack optimization since the correctness of classification is of high variance over different steps in the diffusion purification setting. Formally, the classification result of a sample can be viewed as a Bernoulli distribution (i.e., correct or false). We should reduce the success rate of the Bernoulli distribution of sample classification by optimizing them with a larger attack loss, which would lead to lower robust accuracy. In other words, one observation of failure in classification does not indicate that the sample has a low success rate statistically, and thus, perspective 2) helpsto continue optimizing the perturbations towards a lower success rate (i.e., away from the decision boundary). We provide the pseudo-codes of DiffAttack in Algorithm 2 in Appendix D.1.

## 4 Experimental Results

In this section, we evaluate DiffAttack from various perspectives empirically. As a summary, we find that 1) DiffAttack significantly outperforms other SOTA attack methods against diffusion-based defenses on both the score-based purification and DDPM-based purification models, especially under large perturbation radii (Section 4.2 and Section 4.3); 2) DiffAttack outperforms other strong attack methods such as the black-box attack and adaptive attacks against other adversarial purification defenses (Section 4.4); 3) a moderate diffusion length \(T\) benefits the model robustness, since too long/short diffusion length would hurt the robustness (Section 4.5); 4) our proposed segment-wise forwarding-backwarding algorithm achieves \((1)\)-memory cost and outperforms other baselines by a large margin (Section 4.6); and 5) attacks with the deviated-reconstruction loss added over uniformly sampled time steps outperform that added over only initial/final time steps (Section 4.7).

### Experiment Setting

**Dataset & model.** We validate DiffAttack on CIFAR-10  and ImageNet . We consider different network architectures for classification. Particularly, WideResNet-28-10 and WideResNet-70-16  are used on CIFAR-10, and ResNet-50 , WideResNet-50-2 (WRN-50-2), and ViT (DeiT-S)  are used on ImageNet. We use a pretrained score-based diffusion model  and DDPM  to purify images following [34; 4].

**Evaluation metric.** The performance of attacks is evaluated using the _robust accuracy_ (Rob-Acc), which measures the ratio of correctly classified instances over the total number of test data under certain perturbation constraints. Following the literature , we consider both \(_{}\) and \(_{2}\) attacks under multiple perturbation constraints \(\). We also report the clean accuracy (Cl-Acc) for different approaches.

**Baselines.** To demonstrate the effectiveness of DiffAttack, we compare it with 1) SOTA attacks against score-based diffusion _adjoint attack_ (AdjAttack) , 2) SOTA attack against DDPM-based diffusion _Diff-BPDA attack_, 3) SOTA black-box attack _SPSA_ and _square attack_, and 4) specific attack against EBM-based purification _joint attack_. We defer more explanations of baselines and experiment details to Appendix D.2. The codes are publicly available at https://github.com/kangmintong/DiffAttack.

   Models & T & Cl-Acc & \(_{p}\) Attack & \(\) & Method & Rob-Acc & Diff. \\   &  &  & \)} & \(8/255\) & AdjAttack & 70.64 &  \\  & & & & DiffAttack & **46.88** & & \\   & & & & \(4/255\) & AdjAttack & 82.81 &  \\  & & & & DiffAttack & **71.88** & & \\   & 0.075 & 91.03 & \(_{2}\) & \(0.5\) & AdjAttack & 78.58 &  \\  & & & & DiffAttack & **64.06** & & \\   &  &  & \)} & \(8/255\) & AdjAttack & 71.29 &  \\  & & & & DiffAttack & **45.31** & & \\    & & & & \(4/255\) & AdjAttack & 81.25 &  \\  & & & & DiffAttack & **75.00** & & \\    & 0.075 & 92.68 & \(_{2}\) & \(0.5\) & AdjAttack & 80.60 &  \\    & & & & DiffAttack & **70.31** & & \\   

Table 1: Attack performance (Rob-Acc (%)) of DiffAttack and AdjAttack  against score-based purification on CIFAR-10.

[MISSING_PAGE_FAIL:8]

### Comparison with other adaptive attack methods

Besides the AdjAttack and Diff-BPDA attacks against existing diffusion-based purification defenses, we also compare DiffAttack with other general types of adaptive attacks: 1) **black-box attack** SPSA  and 2) square attack , as well as 3) adaptive attack against score-based generative models **joint attack** (Score / Full) . SPSA attack approximates the gradients by randomly sampling from a pre-defined distribution and using the finite-difference method. Square attack heuristically searches for adversarial examples in a low-dimensional space with the constraints of perturbation patterns. Joint attack (score) updates the input by the average of the classifier gradient and the output of the score estimation network, while joint attack (full) leverages the classifier gradients and the difference between the input and the purified samples. The results in Table 4 show that _DiffAttack outperforms SPSA, square attack, and joint attack by a large margin on score-based and DDPM-based purification defenses_. Note that joint attack (score) cannot be applied to the DDPM-based pipeline due to the lack of a score estimator. AdjAttack fails on the DDPM-based pipeline since it can only calculate gradients through SDE.

### Robustness with different diffusion lengths

We observe that the diffusion length plays an extremely important role in the effectiveness of adversarial purification. Existing DDPM-based purification works  prefer a small diffusion length, but we find it vulnerable under our DiffAttack. The influence of the diffusion length \(T\) on the performance (clean/robust accuracy) of the purification defense methods is illustrated in Figure 2. We observe that _1) the clean accuracy of the purification defenses negatively correlates with the diffusion lengths_ since the longer diffusion process adds more noise to the input and induces inaccurate reconstruction of the input sample; and _2) a moderate diffusion length benefits the robust accuracy_ since diffusion-based purification with a small length makes it easier to compute the gradients for attacks, while models with a large diffusion length have poor clean accuracy that deteriorates the robust accuracy. We also validate the conclusion on ImageNet in Appendix D.3.

### Comparison of memory cost

Recent work  computes the gradients of the diffusion and sampling process to perform the gradient-based attack, but it only considers a small diffusion length (e.g., 14 on CIFAR-10). They construct the computational graph once and for all, which is extremely expensive for memory cost with a large diffusion length. We use a segment-wise forwarding-backwarding algorithm in Section 3.3 to avoid allocating the memory for the whole computational graph. In this part, we validate the memory efficiency of our approach compared to . The results

   Method & Score-based & DDPM-based \\  SPSA & 83.37 & 81.29 \\ Square Attack & 82.81 & 81.68 \\ Joint Attack (Score) & 72.74 & – \\ Joint Attack (Full) & 77.83 & 76.26 \\ Diff-BPDA & 78.13 & 75.00 \\ AdjAttack & 70.64 & – \\  DiffAttack & **46.88** & **54.69** \\   

Table 4: Robust accuracy (%) of DiffAttack compared with other attack methods on CIFAR-10 with WideResNet-28-10 under \(_{}\) attack (\(=8/255\)).

Figure 3: Comparison of memory cost of gradient backpropagation between  and DiffAttack with batch size \(16\) on CIFAR-10 with WideResNet-28-10 under \(_{}\) attack.

Figure 2: The clean/robust accuracy (%) of diffusion-based purification with different diffusion length \(T\) under DiffAttack on CIFAR-10 with WideResNet-28-10 under \(_{}\) attack (\(=8/255\)).

in Figure 3 demonstrate that 1) the gradient backpropagation of  has the memory cost linearly correlated to the diffusion length and does not scale up to the diffusion length of \(30\), while 2) DiffAttack has almost constant memory cost and is able to scale up to extremely large diffusion length (\(T=1000\)). The evaluation is done on an RTX A6000 GPU. In Appendix D.3, we provide comparisons of _runtime_ between DiffAttack and  and demonstrate that DiffAttack reduces the memory cost with comparable runtime.

### Influence of applying the deviated-reconstruction loss at different time steps

We also show that the time steps at which we apply the deviated-reconstruction loss also influence the effectiveness of DiffAttack. Intuitively, the loss added at small time steps does not suffer from vanishing/exploding gradients but lacks supervision at consequent time steps, while the loss added at large time steps gains strong supervision but suffers from the gradient problem. The results in Figure 4 show that adding deviated-reconstruction loss to uniformly sampled time steps (Uni(0,T)) achieves the best attack performance and tradeoff compared with that of adding loss to the same number of partial time steps only at the initial stage (\((0,T/3)\)) or the final stage (\((2T/3,T)\)). For fair comparisons, we uniformly sample \(T/3\) time steps (identical to partial stage guidance \((0,T/3)\), \((2T/3,T)\)) to impose \(_{}\).

## 5 Related Work

**Adversarial purification** methods purify the adversarial input before classification with generative models. Defense-gan  trains a GAN to restore the clean samples. Pixeldefend  utilizes an autoregressive model to purify adversarial examples. Another line of research [50; 22; 17; 24; 60] leverages energy-based model (EBM) and Markov chain Monte Carlo (MCMC) to perform the purification. More recently, diffusion models have seen wide success in image generation [15; 40; 41; 42; 31; 39]. They are also used to adversarial purification [34; 4; 62; 57; 51; 55; 56] and demonstrated to achieve the state-of-the-art robustness. In this work, we propose DiffAttack specifically against diffusion-based purification and show the effectiveness in different settings, which motivates future work to improve the robustness of the pipeline.

**Adversarial attacks** search for visually imperceptible signals which can significantly perturb the prediction of models [52; 20]. Different kinds of defense methods are progressively broken by advanced attack techniques, including white-box attack [6; 2; 32] and black-box attack [1; 53; 35; 12; 37]. [11; 37; 59] propose a systematic and automatic framework to attack existing defense methods. Despite attacking most defense methods, these approaches are shown to be ineffective against the diffusion-based purification pipeline due to the problem of vanishing/exploding gradient, memory cost, and randomness. Therefore, we propose DiffAttack to specifically tackle the challenges and successfully attack the diffusion-based purification defenses.

## 6 Conclusion

In this paper, we propose DiffAttack, including the deviated-reconstruction loss added on intermediate samples and a segment-wise forwarding-backwarding algorithm. We empirically demonstrate that DiffAttack outperforms existing adaptive attacks against diffusion-based purification by a large margin. We conduct a series of ablation studies and show that a moderate diffusion length benefits the model robustness, and attacks with the deviated-reconstruction loss added over uniformly sampled time steps outperform that added over only initial/final time steps, which will help to better understand the properties of diffusion process and motivate future defenses.

**Acknolwdgement.** This work is partially supported by the National Science Foundation under grant No. 1910100, No. 2046726, No. 2229876, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant no. 80NSSC20M0229, the Alfred P. Sloan Fellowship, and the Amazon research award.

Figure 4: The impact of applying \(_{dev}\) at different time steps on decreased robust accuracy (%). \(T\) is the diffusion length and \((0,T)\) represents uniform sampling.