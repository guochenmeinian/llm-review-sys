# Recurrent Hypernetworks are Surprisingly Strong in Meta-RL

Jacob Beck

Department of Computer Science

University of Oxford, United Kingdom

jacob_beck@alumni.brown.edu

&Risto Vuorio

Department of Computer Science

University of Oxford, United Kingdom

risto.vuorio@keble.ox.ac.uk

Zheng Xiong

Department of Computer Science

University of Oxford, United Kingdom

zheng.xiong@cs.ox.ac.uk

Shimon Whiteson

Department of Computer Science

University of Oxford, United Kingdom

shimon.whiteson@cs.ox.ac.uk

###### Abstract

Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually achieve the strongest performance of all methods evaluated. We provide code at https://github.com/jacoba/hyper.

## 1 Introduction

Meta-reinforcement learning (Beck et al., 2023) uses sample-inefficient reinforcement learning (RL) to learn a sample-efficient reinforcement learning algorithm. The sample-efficient algorithm maps the data an agent has gathered so far to a policy based on that experience. To this end, any sequential model such as a recurrent neural network (RNN), can be deployed to learn this mapping end-to-end (Duan et al., 2016; Wang et al., 2016). Such methods are also called _black-box_(Beck et al., 2023).

Alternatively, much prior work has focused on a category of _task-inference_ methods that are specialized for meta-RL. A meta-RL algorithm learns to reinforcement learn over a distribution of MDPs, or _tasks_. By explicitly learning to infer the task, many methods have shown improved performance relative to the recurrent baseline Humplik et al. (2019); Zintgraf et al. (2020); Kamienny et al. (2020); Liu et al. (2021); Beck et al. (2022).

Recent work has shown the simpler recurrent methods to be a competitive baseline relative to task-inference methods (Ni et al., 2022). However, such claims are contentious, as the supporting experiments compare only to one task-inference method designed for meta-RL, the experiments provide additional compute to the recurrent baseline, and the results still show similar or inferior performance to more complicated methods on the majority of difficult domains. In particular, they consider two toy domains and four challenging domains, with RNNs significantly outperformed on two of the four challenging domains, and superior to the single task-inference baseline on only one.

In this paper, we conduct a far more extensive empirical investigation with stronger and carefully designed baselines in meta-RL specifically. In addition, we afford equal computation in terms of number of samples for hyper-parameter tuning to all existing baselines. We present the key insight that the use of a hypernetwork architecture (Ha et al., 2017) is crucial to maximizing the potential of recurrent networks. For an illustration of the potential magnitude of improvement, see Figure 1. While the use of a hypernetwork with RNNs is not a novel idea, they have never been evaluated in meta-RL beyond a single environment, let alone shown to outperform contemporary task-inference methods (Beck et al., 2022). We additionally provide preliminary evidence that the robust performance hyper networks achieve such is in part due to how they condition on the current state and history. Finally, our results establish recurrent hypernetworks as an exceedingly strong method on meta-RL benchmarks that is also far simpler than alternatives, providing significant ease of use for practitioners in meta-RL.

## 2 Related Work

Recurrent Meta-RL.Many meta-RL methods structure the learned RL algorithm as a black box using a neural network as a general purpose sequence model (Duan et al., 2016; Wang et al., 2016; Mishra et al., 2018; Fortunato et al., 2019; Ritter et al., 2021; Wang et al., 2021; Ni et al., 2022). While any sequence model can be used, often the model is structured as an RNN (Duan et al., 2016; Wang et al., 2016; Ni et al., 2022). Such models (Duan et al., 2016; Wang et al., 2016) are commonly used as simple meta-RL baselines.

One study has shown RNNs to be a competitive baseline in meta-RL (Ni et al., 2022); however, the scope of the study was broader than meta-RL and the evidence specific to meta-RL is inconclusive. First, the study evaluates only a single specialized meta-RL method (Zintgraf et al., 2020), which was, but is not currently, state-of-the-art (Beck et al., 2022). Second, the experiments use results or hyperparameters from the original papers, while affording extra computation to tune the RNNs on each benchmark, including dimensions that were not tuned for the other baselines. This computation includes tuning architecture choices, the context length, the RL algorithm used, and the inputs (Ni et al., 2022). And third, the study does not show particularly strong performance of recurrent methods relative to the chosen specialized baseline. On the MuJoCo domains, the recurrent baseline outperforms the specialized method on only one of these four domains, performs similarly on another, and is significantly outperformed on the other remaining two (Ni et al., 2022). In contrast, our work compares against four specialized baselines; affords equal computation to all methods, defaulting to hyper-parameters that favor existing task-inference methods for parameters that are not tuned; and still establishes recurrent hypernetworks as the strongest method evaluated.

Task Inference Meta-RL.In addition to recurrent meta-RL methods, task-inference methods (Humplik et al., 2019; Zintgraf et al., 2020; Kamienny et al., 2020; Liu et al., 2021; Beck et al., 2022) and policy-gradient methods (Yoon et al., 2018; Finn et al., 2017; Vuorio et al., 2019; Zintgraf et al., 2019) constitute a significant bulk of existing work. We exclude the latter methods from

Figure 1: In some environments, recurrent neural networks fail to learn meta-RL tasks, whereas recurrent hypernetworks achieve strong performance.

comparison since the estimation of a policy gradient in policy-gradient approaches requires more data than in our benchmarks (Zintgraf et al., 2019; Beck et al., 2023). Task inference methods are a strong baseline for our benchmark, but are generally more complicated than recurrent meta-RL methods. For example, such methods typically add a task inference objective (Humplik et al., 2019), and may also add a variational inference component (Zintgraf et al., 2021), or pre-training of embeddings with privileged information (Liu et al., 2021). In this paper, we ablate each of these components to create the strongest task-inference baselines possible. In the end, we find the more complicated task inference methods are still inferior to the recurrent baseline with hypernetworks.

Hypernetworks.A hypernetwork (Ha et al., 2017) is a neural networks that produces the parameters (weights and biases) for another neural network, called the base network. Hypernetworks have been used in supervised learning (SL) (Ha et al., 2017; Chang et al., 2020), Meta-SL (Rusu et al., 2019; Munkhdalai and Yu, 2017; Przewiezlikowski et al., 2022), and meta-RL (Beck et al., 2022; Xian et al., 2021; Peng et al., 2021; Sarafian et al., 2021). While these networks are complicated, and can fail to work out-of-the-box, simple initialization methods can be sufficient to enable stable learning (Beck et al., 2022; Chang et al., 2020). In meta-RL, only Beck et al. (2022) have investigated training a hypernetwork and-to-end to arbitrarily modify the weights of a policy. This study suggests that hypernetworks are particularly useful in preventing interference between different tasks and enable greater returns as the the number of parameters increases. However, their study shows a task-inference method to be superior, and the recurrent hypernetwork is evaluated only on a single task with results that are statistically insignificant. Recurrent hypernetworks have never been widely evaluated in meta-RL, let alone shown to outperform contemporary task-inference methods.

## 3 Methods

### Problem Setting

A task in RL is formalized as a Markov Decision Processes (MDP), defined as a tuple of \((,,,,)\). At each time-step \(t\), the agent inhabits a state, \(s_{t}\), which it can observe. The agent then performs an action \(a_{t}\). The MDP subsequently transitions to the next state \(s_{t+1}(s_{t+1}|s_{t},a_{t}) _{+}\), and the agent receives reward \(r_{t}=(s_{t},a_{t}) \) upon entering \(s_{t+1}\). The agent acts to maximize the expected future discounted reward, \(R()=_{r_{t}}^{t}r_{t}\), where \(\) denotes the agent's trajectory throughout an episode in the MDP, and \([0,1)\) is a discount factor. The agent takes actions sampled from a learned policy, \((a|s):_{+}\).

Meta-RL algorithms learn an RL algorithm, \(f()\), that maps from the data, \(\), sampled from a single MDP, \( p()\), to policy parameters \(\). As in a single RL task, \(\) is a sequence up to time-step \(t\) forming a trajectory \(_{t}()^{t}\). Here, however, \(\) may span multiple episodes within a single MDP, since multiple episodes of interaction may be necessary to produce a reasonable policy. We use the same symbol, \(\), but refer to it as a _meta-episode_. The policy, \(_{}(a|=f_{}())\), is parameterized by \(\). \(f\) is itself parameterized by \(\), which are referred to as the _meta-parameters_.

Figure 2: The standard RNN policy (a) and the RNN policy with a hypernetwork (b).

The objective in meta-RL is to find meta-parameters \(\) that maximize the sum of the returns in the meta-episode across a distribution of tasks (MDPs):

\[*{arg\,max}_{}_{ p()} _{}R()_{}(|f_{}( )),.\] (1)

\(f_{}\) is referred to as the _inner-loop_, which produces \(\), in contrast to the _outer-loop_, which produces \(\).

### Recurrent Methods

Recurrent methods are perhaps the simplest and most common meta-RL baseline. Recurrent methods use an RNN to encode history and train all meta-parameters end-to-end on Equation 1. These methods are depicted in Figure 2. While neither recurrent networks (RNN below), nor the the combination of a hypernetwork with recurrent networks (RNN+HN below) is a novel, the combination has never been widely evaluated in meta-RL (Beck et al., 2022), but we will show that the combination actually achieves the strongest results.

Rnn.Our first recurrent baseline is the simplest and is equivalent to RL2 (Duan et al., 2016) and L2RL (Wang et al., 2016). In this case \(_{}(a|=f_{}())\), where \(f\) is a recurrent network, \(\) is a feed-forward network, and \(f\) and \(\) each use distinct subsets of the meta-parameters, \(\).

Rnn+HN.Our second recurrent model is the recurrent hypernet and achieves the strongest results. Here, the recurrent network produces the weights and biases for the policy directly: \(_{}(a|s)\). The state must be passed as input again to this policy for the feed-forward policy to condition on an input, and we follow the initialization method for hypernetworks, Bias-HyperInit, suggested by Beck et al. (2022). In this initialization, the hypernetwork's final linear layer is initialized with a zero weight matrix and a non-zero bias, so that the hypernetwork produces the same base-network parameters for any trajectory at the start of training.

### Task-Inference Methods

Task-inference methods (Beck et al., 2023) constitute the main category of meta-RL methods capable of adaptation as quickly as recurrent (i.e., black-box) methods (Humplik et al., 2019; Zintgraf et al., 2020; Kamienny et al., 2020; Liu et al., 2021; Beck et al., 2022). These methods train the inner-loop not end-to-end but rather to identify the task, within the given task distribution. One perspective on these methods is that they attempt to shift the problem from the more difficult meta-RL setting to the easier multi-task setting by learning to explicitly infer the task (Beck et al., 2023). Here we define relevant task-inference methods used as baselines (Figures 3 and 4). For additional ablations, summary, and details on the method selection process, see the appendix.

TI Naive.The inner-loop of a meta-RL method must take in the trajectory, \(\), and produce the policy parameters, \(\). In task inference methods, the inner-loop additionally produces an estimate

Figure 3: Task-inference baselines. Naive task (a), additional multi-task pre-training (b).

of the current task, \(_{}\), given a known task representation, \(c_{}\). While it is possible to represent \(\) as \(_{}\) directly, i.e. pass \(_{}\) to the policy, it is common to compute \(\) from an intermediate layer of the network that predicts \(_{}\), which contains more information about the belief state (Humplik et al., 2019; Zintgraf et al., 2020). It is also common to use an information bottleneck to remove information not useful for task inference from the trajectory (Humplik et al., 2019; Zintgraf et al., 2020). Following Zintgraf et al. (2020), we condition \(\) on the mean and variance of the bottleneck layer in order to explicitly condition the policy on task uncertainty for more efficient exploration (Figure 3). Putting these together, we can write a task inference method as follows:

\[ =P^{}(RNN())\] \[ =P^{}(RNN())\] \[IB =(z;,)\] \[_{} =P^{c}(z IB)\] \[ =ReLU(P^{}(,))\] \[J_{infer}() =_{}[_{|}[-||c_{}-_{}||_{2}^{2}]]\] \[J_{prior}() =_{}[_{|}[D(IB||(z;=0,=I))]],\]

where \(P^{c}\), \(P^{}\), \(P_{}\), and \(P_{}\) are all linear projections, \(\) represents a stop-gradient, \(P^{}(,)\) is the matrix multiplication of \(P^{}\) with stop-gradient of a concatenation of \((,)\), and \(D\) is the KL-divergence. Here, \(J_{infer}+J_{prior}\) constitutes the evidence lower bound from Zintgraf et al. (2020) and is used to train \(IB\), whereas all other parameters (\(P^{}\) and \((|)\)) are trained via Equation 1.

TI.As presented, the TI Naive baseline may suffer from a known issue where the given task representation contains too little or too much information (Beck et al., 2023). When too little information is present, the policy may miss information crucial to the task. When too much information is present, the policy may be presented with the difficult problem of separating the useful information from irrelevant task features. Toward this end, it is possible to pre-train the task representation end-to-end using an additional policy (Humplik et al., 2019; Kamimany et al., 2020; Liu et al., 2021). Our TI baseline (Figure 3) is the same as TI Naive, except that an additional multi-task policy, \(^{}\), is pre-trained to learn a representation of the task, \(g_{}(c_{})\). Given a linear projection, \(P^{g}\):

\[ =P^{g}(z IB)\] \[J_{multi}() =_{ p()}[_{} [R()|^{}_{}(|g_{}(c_{})),]\] \[J_{infer}() =_{}[_{|(|)}[-||g_ {}(c_{})-||_{2}^{2}]].\]

For a fair comparison, training of the multi-task policy, \(^{}\), occurs at the expense of training the meta-learned policy \(\), with the total number of samples remaining constant. Instead of fully training the multi-task policy, we experiment with different amounts of pre-training in the appendix, finding significant benefits already from less than 5% of total training allocation for the pre-training.

TI++HN.The TI++HN baseline is the same as TI, with three additions that we found to strengthen task inference (Figure 4). The first two additions (++) are novel and are 1) initializing the parameters of the meta-policy, \(\), to that of the pre-trained multi-task policy, \(^{}\), to encourage transfer and 2)

Figure 4: Task-inference baselines. Task inference with additional parameter reuse and hypernetwork (a), an existing contemporary task-inference algorithm (Beck et al., 2022) (b).

training of the task-inference (\(J_{infer}\)) over trajectories from the initial multi-task training phase in addition to the meta-learning phase, since the former tend to be more informative and simply provide extra data. The third addition (HN) uses a hypernetwork to condition the policy on \(\)(Beck et al., 2022). We write this as \(_{}(|s)\) to show that \(\) represents the weights and biases of \(\), just as in the recurrent baseline. The output of the hypernetwork is \(\), and the input to the hypernetwork is a the projection of \(\) and \(\), \(=h(ReLU(P^{}(,)))\). When using a hypernetwork with the first two additions (++), the parameters of the hypernetwork for the meta-learned policy are initialized to the parameters of hypernetwork for the multi-task policy, instead of sharing policy parameters directly.

VI+HN.While task-inference methods rely on known task representations, it is also possible to design methods that can infer the MDP more directly. This can be done by inferring transitions and rewards in full trajectories, since the transition function and reward function collectively define the MDP. In particular, such a method, called VariBAD, is proposed by Zintgraf et al. (2020), and extended with the use of hypernetworks by (Beck et al., 2022). Here, we call this method VI+HN, and it is a contemporary task-inference method (Figure 4). Precisely, this model reconstructs full trajectories including future transitions for meta-episodes, \(_{0:T}\), instead of task embeddings, from \(\), the current trajectory:

\[J_{infer}()=_{}[_{|(|)}[ -||_{0:T}-_{0:T}||_{2}^{2}]].\]

See the appendix for ablations with VI alone.

## 4 Experiments

In this section we compare recurrent hypernetworks (RNN+HN) to task inference baselines. We evaluate over three simple navigation domains (Zintgraf et al., 2020; Humplik et al., 2019; Rakelly et al., 2019), designed to test learning of exploration and memory, in addition to four more difficult tasks using MuJoCo (Todorov et al., 2012), and one task testing long-term memory from visual observations in MineCraft (Beck et al., 2020). Results show meta-episode return, optimized over five learning rates and averaged over three seeds (four in MineCraft), with a 68% confidence interval using bootstrapping. Additional details and results on Meta-World (Yu et al., 2020) are available in the appendix. Our experiments demonstrate that while our task inference methods are strong baselines, RNN+HN is able to outperform them and achieve the highest returns.

### Grid-Worlds

Navigation tasks are a common benchmark in meta-RL (Zintgraf et al., 2020; Humplik et al., 2019; Rakelly et al., 2019). Here we evaluate on the grid-world variant from Zintgraf et al. (2020), in addition to two of our own variants. The first environment, Grid-World, consists of a five by five grid with a goal location in one of the grid cells. The agent starts in the bottom left corner of the grid, and then must navigate to a goal location, which is held constant throughout the meta-episode. (Details are in the appendix.) This environment is useful for testing how well a meta-learning algorithm learns to efficiently explore in the gridworld as it searches for the goal. Additionally, our Grid-World Show environment was designed to be relatively harder for end-to-end methods, in order to provide a challenge for our proposed method. In this environment, the goal position is visible to the agent

Figure 5: Evaluation on grid-world benchmarks. RNN+HN and TI++HN improve return. RNN+HN achieves the greatest asymptotic return and sample efficiency.

at the first timestep of each episode. Task inference methods will directly encourage the storage of this information in memory, whereas the end-to-end recurrent methods must learn to store this information through its effect on the policy. In contrast, our Grid-World Dense environment provides dense rewards and may be easier for end-to-end methods. In this environment, the agent receives and observes a reward equal to the Manhattan distance to the goal location. Instead of inferring the task explicitly, the agent can simply move up or to the right until the reward stops increasing.

Surprisingly, on all three grid-worlds, RNN+HN achieves both the greatest asymptotic return and greatest sample efficiency (Figure 5). The recurrent hypernetwork achieves the fastest learning on Gridworld Show, despite the environment being specifically designed to be harder for end-to-end methods. TI++HN dominates all other task-inference baselines on these grid-worlds, suggesting that it is a relatively strong task-inference method. Collectively, these grid-worlds demonstrate that end-to-end learning with hypernetworks can learn to store the task in memory and to explore optimally with this information directly from return, just as well as task-inference methods.

### MuJoCo

Here we evaluate baselines on more challenging domains. We evaluate on all four MuJoCo variants proposed by Zintgraf et al. (2020), which is known to be a common and more challenging meta-RL benchmark involving distributions of tasks requiring legged locomotion (Zintgraf et al., 2020; Humplik et al., 2019; Rakelly et al., 2019; Beck et al., 2023). Ant-Dir and Cheetah-Dir both involve non-parametric task variation, whereas Cheetah-Vel and Walker include parametric variation of the target velocity and physics coefficients respectively. For environment details, see the appendix. We expect Walker in particular to be difficult for end-to-end methods since it has the largest space of tasks with the dynamics defined by 65 different parameters. Assuming the values of these parameters are important for the optimal policy, task inference methods may learn the optimal policy faster.

On Cheetah-Dir and Ant-Dir, RNN+HN achieves greater returns than all other baselines by a wide margin (Figure 6). On Cheetah-Vel, all methods achieve fairly similar results, with RNN+HN

Figure 6: Models evaluated on MuJoCo benchmark. RNN+HN matches returns on Walker and Cheetah-Vel, and exceeds returns on Cheetah-Dir and Ant-Dir.

still achieving the greatest asymptotic return by a small margin. As expected, RNN+HN does not outperform task inference on Walker. For Walker, only TI outperforms RNN+HN in terms of efficiency, and only TI Naive outperforms RNN+HN in terms of asymptotic return; however, the effect size is small and both TI and TI Naive have among the worst performance on Cheetah-Dir and the grid-worlds. Still, RNN+HN achieves similar performance on Walker, which is notable in a high dimensional task space. And, RNN+HN achieves greater returns overall.

### MineCraft

We additionally evaluate on the MC-LS environment from Beck et al. (2020), designed to test long-term memory from visual observations in MineCraft. Here, the agent navigates through a series of 16 rooms. In each room, the agent navigates left or right around a column, depending on whether the column is made of diamond or iron. Discrete actions allow for a finite set of observations. Correct behavior can be computed from the observation and receives a reward of 0.1. At the end, the agent moves right or left depending on a signal (red or green) that defines the task and is shown before the first room. Correct and incorrect behavior receives a reward of 4 and -3, respectively. We allow the agent to adapt over two consecutive episodes, forming a single meta-episode.

On MC-LS we compare RNN+HN to VI+HN alone, given a limited compute budget and since VI+HN is an established contemporary task-inference baseline Beck et al. (2022). Additionally, we add an extra seed (four in total) and a linear learning rate decay due to high variance in the environment. In Figure 7, we see that RNN+HN significantly outperforms VI+HN. While VI+HN learns to navigate through all rooms, it does not reliably learn the correct long-term memory behavior. In contrast, RNN+HN is able to adapt reliably within two episodes, and one seed even learns to adapt reliably within a single episode. While further work is needed to learn the optimal policy, these experiments demonstrate that RNN+HN outperforms VI+HN, even on more challenging domains.

## 5 Discussion

Here we investigate why the recurrent hypernetworks have such robust performance on meta-RL benchmarks. First, we observe that the state processing differs between the RNN and RNN+HN

Figure 8: The RNN+S policy (a) and the RNN policy with a hypernetwork (b).

Figure 7: RNN+HN outperforms VI+HN on MC-LS (MineCraft) environment.

baselines. In particular, RNN conditions on the current state only through its dependence on \(\), whereas hypernetworks pass in the state again to the policy. Thus, we investigate whether the difference in inputs alone could be the cause of the improvement in performance. To this end, we introduce a new ablation to test the effect of just passing in the state again. Details are below. Second, we inspect how sensitivity to latent variables encoding the trajectory affects performance.

RNN+S.Hypernetworks condition on the current state both through \(\), which contains information about trajectory, including the current state, and by directly conditioning on the state. Since the hypernetwork conditions on state twice, we test to see the effect of conditioning on the state twice without hypernetworks. We call this ablation RNN+S, which we write as \(_{}(a|s,)\) (Figure 8). In an empirical evaluation, we see that while RNN+S does perform favorably relative to RNN alone, RNN+HN still outperforms RNN+S (Figures 9 and 10). In particular, RNN+HN achieves similar returns to RNN+S on Ant-Dir and Cheetah-Vel, and outperforms RNN+S on all other environments, in terms of asymptotic return and sample efficiency. Taken together, we see that the advantage of RNN+HN comes both from the ability to re-condition on state directly and from the hypernetwork architecture. These results confirm that to achieve the strongest performance, re-conditioning on state directly is not sufficient, and that the hypernetwork architecture itself is still critical.

Figure 10: RNN+HN matches or exceeds RNN+S on MuJoCo tasks. RNN alone is a weak baseline.

Figure 9: RNN+HN matches or exceeds RNN+S and RNN, but RNN+S is also strong on grid-worlds.

Latent Gradients.We also investigate how the hypernetworks condition on the trajectory. In particular, we investigate the sensitivity of the output of the network to an intermediate latent representation of the trajectory. For this purpose, we chose to measure the gradient norm of the first hidden layer of the hypernetwork on the Walker environment. We perform this investigation both with the initialization method designed for hypernetworks that we used throughout our experiments, Bias-HyperInit [Beck et al., 2022], and with Kaiming initialization [He et al., 2015], not designed for hypernetworks. We add Kaiming initialization, since Bias-HyperInit ignores trajectories at the start of training [Beck et al., 2022]. First, we confirm the finding of Beck et al.  that Bias-HyperInit is crucial for performance (Figure 11). Second, we see the two models that performs worst, RNN and RNN+HN Kaiming, also have the greatest norm. Moreover, we find that both RNN+HN and RNN+S start with a low gradient norm and then further decrease this norm throughout training, whereas the RNN model increases this norm. We hypothesize that a low norm, i.e., low sensitivity to the latent variable, is crucial for stable training and that the RNN model increases this norm to remain sensitive to the state, since the state is only encoded in the latent for this model.

## 6 Limitations

As an empirical study of meta-RL, we cannot guarantee that recurrent hypernetworks will improve over every baseline nor on every environment. However, we mitigate this issue by comparing to many baselines and performing many ablations. In particular, we compare to a contemporary task-inference method (VI+HN), design our own baseline which we show to be stronger than others on all grid-worlds (TI+HN), and also include standard methods (TI and TI Naive), in addition to further ablations in the appendix. In as much as an empirical study can, we believe our study demonstrates a significant improvement of the RNN+HN method over existing baselines.

## 7 Conclusion

In this paper, we establish recurrent hypernetworks as a surprisingly strong method in meta-RL. While much effort has gone into designing specialized task-inference methods for meta-RL, we present the surprising result that the simpler recurrent methods can be easily adapted to outperform the task-inference methods. By combining recurrent methods with the hypernetwork architecture, we achieve a new strong baseline in meta-RL that is both robust and easy to implement. In comparison to existing evidence, we provide much stronger empirical results, afford equivalent computation for tuning to all baselines, and establish recurrent hypernetworks as a strong method. We additionally show that passing the state variable to the policy is a crucial component of this method. Finally, we presented gradient analysis suggesting lower latent gradient norms to play an important role in the performance of meta-RL methods. Since the gradient analysis is preliminary and investigates state and latent variables in isolation, future work could investigate the interaction between these variables. Future work could also analyze the interaction between hypernetworks and other sequence models, such as transformers. We hope these insights, along with a simple and robust method, open the way for the broader use of sample-efficient learning in meta-RL and beyond.

Figure 11: Returns (a) and Gradient norms on Walker (b). The RNN method must increase this norm to condition on state, whereas others do not. Lower gradient norms seem important to performance.