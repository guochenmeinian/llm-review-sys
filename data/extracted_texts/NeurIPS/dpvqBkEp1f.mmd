# Noether's Razor: Learning Conserved Quantities

Tycho F. A. van der Ouderaa

Imperial College London

London, UK &Mark van der Wilk

University of Oxford

Oxford, UK &Pim de Haan

CuspAI

Amsterdam, NL

###### Abstract

Symmetries have proven useful in machine learning models, improving generalisation and overall performance. At the same time, recent advancements in learning dynamical systems rely on modelling the underlying Hamiltonian to guarantee the conservation of energy. These approaches can be connected via a seminal result in mathematical physics: Noether's theorem, which states that symmetries in a dynamical system correspond to conserved quantities. This work uses Noether's theorem to parameterise symmetries as learnable conserved quantities. We then allow conserved quantities and associated symmetries to be learned directly from train data through approximate Bayesian model selection, jointly with the regular training procedure. As training objective, we derive a variational lower bound to the marginal likelihood. The objective automatically embodies an Occam's Razor effect that avoids collapse of conservation laws to the trivial constant, without the need to manually add and tune additional regularisers. We demonstrate a proof-of-principle on \(n\)-harmonic oscillators and \(n\)-body systems. We find that our method correctly identifies the correct conserved quantities and U(\(n\)) and SE(\(n\)) symmetry groups, improving overall performance and predictive accuracy on test data.

## 1 Introduction

Symmetries provide strong inductive biases, effectively reducing the volume of the hypothesis space. A celebrated example of this is the convolutional layer embedding translation equivariance in neural networks, which can be generalised to other symmetry groups (Cohen and Welling, 2016).

Meanwhile, physics-informed machine learning models (Greydanus et al., 2019; Cranmer et al., 2020), typically relying on neural differential equations (Chen et al., 2018), embed constraints known from classical mechanics into model architectures to improve accuracy on physical dynamical systems.

Rather than strictly constraining a model to certain symmetries, recent works have explored whether invariance and equivariance symmetries in machine learning models can also be automatically learned from data. This often relies on separate validation data (Maile et al., 2022), explicit regularisers (Finzi et al., 2021) or additional outer loops (Cubuk et al., 2018). Alternatively, we can take a Bayesian approach where we embed symmetries into the prior and empirically learn them through Bayesian model selection (van der Wilk et al., 2018; Immer et al., 2022; van der Ouderaa et al., 2022).

We propose to use _Noether's theorem_(Noether, 1918) to parameterise symmetries in Hamiltonian machine learning models in terms of their conserved quantities. To do so, we propose to symmetrise a learnable Hamiltonian using a set of learnable quadratic conserved quantitites. By choosing the conserved quantities to be quadratic, we can find closed-form transformations that can be used to obtain an unbiased estimate of the symmetrised Hamiltonian.

Secondly, we phase symmetries implied by conserved quantities in the prior over Hamiltonians an leverage the _Occam's razor_ effect of Bayesian model selection (Rasmussen and Ghahramani, 2000; van der Wilk et al., 2018) to learn conserved quantities and their implied symmetries directly from train data. We derive a practical lower bound using variational inference (Hoffman et al., 2013)resulting in a single end-to-end training procedure capable of learning the Hamiltonian of a system jointly with its conserved quantities. As far as we know, this is the first case in which Bayesian model selection with variational inference is successfully scaled to deep neural networks, an achievement in its own right, whereas most works so far have relied on Laplace approximations (Immer et al., 2022).

Experimentally, we evaluate our _Noether's razor_ method on various dynamical systems, including \(n\)-simple harmonic oscillators and \(n\)-body systems. Our results suggest that our method is indeed capable of learning the conserved quantities that give rise to correct symmetry groups for the problem at hand. Quantitatively, we find that our method that learns symmetries from data matches the performance of models with the correct symmetries built-in as oracle. We outperform vanilla training, resulting in improved test generalisation and predictions that remain accurate over longer time periods.

## 2 Background

### Hamiltonian mechanics

Hamiltonian mechanics is a framework that describes dynamical systems in _phase space_, denoted \(=^{M}\), with \(M\) even. Phase space elements \((,)\) follow _Hamiltonian equations of motion_:

\[_{i}=},_{i}=-} \]

where the Hamiltonian \(H:\) is an _observable_1, which are smooth functions on the phase space, that corresponds to the energy of the system. It is often simpler to write \(=(,)\), so that we have: \(}=J H\) and \(J=[1&J\\ -I&0]\), where \(I\) is the identity matrix, \(J\) is called the _symplectic form_ and \( H=_{}H()\) is the gradient of phase space coordinates.

_Example: \(n\)-body problem in 3d._ If we consider a \(d{=}3\) dimensional Euclidean space containing \(n\) bodies, our position and velocity spaces are each \(^{3n}\) making up phase space \(=^{2 3n}\). Our Hamiltonian \(H:^{3n}^{3n}\), which in this case is a separable function \(H(q,p)=K(q)+P(p)\) of kinetic energy \(K(q)=_{i}m_{i}||p||^{2}/2\) and the potential energy \(P(p)=_{i j}Gm_{i}m_{j}/||q_{i}-q_{j}||\) where \(m_{i}\) is the mass of a body \(i\) and \(G\) is the gravitational constant.

### Learning Hamiltonian mechanics from data

We can model the Hamiltonian from data (Greydanus et al., 2019; Ross and Heinonen, 2023; Tanaka et al., 2022; Zhong et al., 2019). Concretely, we are interested in a posterior over functions that the Hamiltonian can take \(p(H_{})\), conditioned on trajectory data \(=\{(_{t}^{n},_{t}^{n})\}_{n=1}^{N}\) sampled from phase space at different time points \((t,t^{})\), or time difference \( t{=}t^{}{-}t\). Given a new data point \(_{t}^{*}\), we would like to make predictions \(p(_{t^{}}^{*}|_{t}^{*},H_{},)\) over phase space trajectories into the future \(t^{}\).

Hamiltonian neural networksHamiltonian neural networks (Greydanus et al., 2019; Toth et al., 2019; Rezende et al., 2019) model the Hamiltonian \(H\) using a learnable Hamiltonian \(H_{}:\) parameterised by \(^{P}\). With a straightforward Gaussian likelihood \(p(_{t^{}}|_{t},)=(_{t^{}}| _{t}+J H_{}(_{t}) t,_{}^{2} )\) with a small observation noise \(_{}^{2}\), a maximum likelihood fit can be found by minimising the negative log-likelihood \(_{*}{=}_{}_{i}_{t}- p(_{t+  t}^{i}|_{t}^{i},)\) on minibatches of data using stochastic gradient descent. The mean of this likelihood represents a single Euler integration step (Sec. 2.1 of David and Mehats (2023)), which bounds the possible accuracy of the fit to the true Hamiltonian \(H\). In practice, we may replace this by more accurate differentiable numerical integrators (Kidger, 2022).

### Noether's theorem

The theorem of (Noether, 1918), here presented in the Hamiltonian formalism (Baez, 2020; Arnold, 1989), links the concepts of an observable being conserved, to the Hamiltonian being invariant to the symmetries generated by an observable.

Conserved quantityLet \(\) be the set of observables, which are smooth real-valued functions \(\) on the phase space. Given a trajectory \((t)\) generated by the Hamiltonian \(H\), we can compute the variation of an observable \(O\) in time via the chain rule and Hamilton's equations of motion (Equation (1))

\[O}{t}=_{i}}_{i}+}_{i}=_{i}}}-}}=\{O,H\}, \]

where the last equality defines the _Poisson bracket_\(\{,\}:\). The Poisson bracket relates to the symplectic form via \(\{O,H\}()= O() J H()\). An observable that does not change along any trajectory is called a _conserved quantity_. As we can see from Equation (2), an observable \(O\) is conserved if and only if \(\{O,H\}=0\).

From two conserved quantities \(O,O^{}\), we can create a new conserved quantity by linear combination \( O+ O^{}\) with coefficients for \(\), \(\), which is conserved because the Poisson bracket is linear in both arguments. Also, we can take the product \(OO^{}\), with \((OO^{})()=O()O^{}()\), which is conserved because the Poisson bracket satisfies Leibniz's law of differentiation \(\{OO^{},H\}=\{O,H\}O^{}+O\{O^{},H\}\). Finally, the Poisson bracket of the conserved quantities \(\{O,O^{}\}\) is also conserved, because of the Jacobi identity.

Symmetries generated by observablesReferring back as to the Hamiltonian equations of motion in Equation (1), note that these equations work not just for the Hamiltonian \(H\) of the system, but for _any_ observable \(O\). So given any starting point \(x_{0}\), we can generate a trajectory \(()\) satisfying

\[(0)=_{0} 56.905512pt}()=J O(( )). \]

We have used a different symbol to not conflate the ODE time \(\) with regular time \(t\) of the trajectory generated by the Hamiltonian. Denote the flow associated to this ODE generated by observable \(O\) by \(^{}_{O}:\), mapping \(_{0}\) to \(^{}_{O}(_{0})=()\). Note that any ODE flow satisfies \(^{0}_{O}=_{}\) and \(^{+}_{O}=^{}_{O}^{}_{O}\). Hence, the observable \(O\) generates a one-dimensional group \(_{O}\), parametrized by \(\), that is a subgroup of the group \(()\) of diffeomorphisms \(\).

**Theorem 1** (Noether).: _The observable \(O\) is a conserved quantity on the trajectories generated by Hamiltonian \(H\) if and only if \(H\) is invariant to \(_{O}\), meaning that for all \(\), \(H^{}_{O}=H\)._

Proof.: By reasoning analogous to that in Equation (2), the value of the Hamiltonian changes under the flow generated by observable \(O\) as \(H}{}=\{H,O\}\). Noting that the Poisson bracket is anti-symmetric, we have that: \(O\) is a conserved quantity \(\{O,H\}=0\{H,O\}=0 H\) is invariant to the flow generated by \(O\). 

### Automatic symmetry discovery

Symmetries play an important role in machine learning models, most notably group invariance and equivariance constraints (Cohen and Welling, 2016). Instead of having to define symmetries explicitly in advance, recent attempts have been made to learn symmetries automatically from data. Even if learnable symmetries can be differentiably parameterised, learning them can remain difficult as symmetries act as constraints on the functions a model can represent and are, therefore, not encouraged by objectives that solely optimise train data fit. As a result, even if a symmetry would lead to better test generalisation, the training collapses into selecting no symmetry. Common ways to overcome this are designing explicit regularisers that encourage symmetry (Benton et al., 2020; van der Ouderaa et al., 2022), which often require tuning, or use of validation data (Alet et al., 2021; Maile et al., 2022; Zhou et al., 2020). Learning symmetries for integrable systems was proposed in (Bondesan and Lamacraft, 2019), whereas our framework works more generally also for non-integrable systems, such as the 3-body problem. Recent works have demonstrated effectivity of Bayesian model selection to learn symmetries directly from training data. This works by optimising the marginal likelihood, which embodies an Occam's razor effect that trades off data fit and model complexity. For Gaussian processes, the quantity can often be computed in closed-form (van der Wilk et al., 2018), and can be scaled to neural networks through variational inference (van der Ouderaa and van der Wilk, 2021) and linearised Laplace approximations (Immer et al., 2022).

Symmetries Hamiltonians with Conserved Quantities

Our method introduced in the next section will learn the Hamiltonian of a system together with a set of conserved quantities. First, in this section we discuss how the learned conserved quantities will be parametrised, and how we can make the Hamiltonian invariant to the symmetry generated by conserved quantities.

### Parameterising conserved quantities

In this work, we limit ourselves to modelling up to a fixed maximum number of \(K\) conserved quantities \(C^{1}_{},C^{2}_{},,C^{K}_{}:\) are observables parameterised by _symmetrisation parameters_\(\), to distinguish them from the _model parameters_\(\) parameterising the Hamiltonian scalar field.

In this paper, we consider quadratic conserved quantities of the form \(C_{}()=^{T}/2+^{T}+c\). As we use the conserved quantities only through their gradients, the constant is arbitrary and can be ignored. The learnable symmetrisation parameters are thus \(=\{,\}\), for a symmetric matrix \(\). A quadratic conserved quantity \(C\) generates a symmetry transformation whose scalar field \(}=J C()=J+J\) is affine, or linear on the homogeneous coordinates \((,1)\). Its flow can be analytically solved

\[^{}_{C}()\\ 1=(J&J\\ ^{T}&0)\\ 1 \]

using the matrix exponential \(()\) for which efficient numerical algorithms exist (Moler and Van Loan, 2003). This equation can be verified to have the correct scalar field and boundary condition, and thus forms the unique solution to the ODE in Equation (3).

### Symmetrising observables

Given an observable \(C\), we want to transform an observable \(f\) into \(\) that is invariant to the transformations generated by \(C\). This means that \(^{}_{C}=\) for all symmetry time \(\). Via Noether's theorem, we know that this is equivalent to \(C\) being conserved in the trajectories generated by \(f\), and also equivalent to \(\{C,\}=0\). However, this equation does not prescribe how to obtain such \(\). Instead, we'll create \(\) by symmetrizing over the symmetry group generated by \(C\). This is done by averaging over the orbit of the transformation

\[()=_{}f(^{}_{C}())().\]

with a measure \(\) over symmetry time \(\). This measure \(\) induces a measure on the 1-dimensional subgroup \(_{C}\) of the group of diffeomorphisms \(\). If this measure on \(_{C}\) is uniform (specifically, a right-invariant measure (Halmos, 1950)), then \(\) is indeed invariant.

Instead of a single symmetry generator, we can also have a set \(=\{C_{1},...,C_{K}\}\) of observables and we want to make \(f\) invariant to all of these. Assume that this set spans a vector space of observables that is closed under the Poisson bracket (i.e. they form a Lie subalgebra). In that case, the groups of transformations of the observables combined generate a group \(_{}\)(Hall, 2015, Thm. 5.20). This group is parameterized by a vector of symmetry times \(^{K}\). The corresponding flow is \(^{}_{}=^{1}_{_{i}_{i}C_{i}}\). To make an observable \(f\) invariant to the symmetries of all conserverved quantities \(\), equivalently to the group \(_{}\), we symmetrize

\[()=_{^{K}}f(^{}_{}())( ). \]

with some measure \(\) over \(^{K}\). As before, if this induces a uniform measure over \(_{}\), then this symmetrization indeed makes \(\) invariant to \(_{}\).

However, a probability measure \(()\) that gives a uniform distribution over \(_{}\) might not exist, for example when the group contains a non-compact group of translations. Even when such a measure does exist, it may be hard to construct, and the symmetrisation integral in Equation (5) may be intractable to compute. So instead, in practice, we approximate this by choosing \(()\) to be a unit normal distribution \((0,_{K})\) or uniform distribution. This results in a relaxed notion of symmetry in \(\) which can be interpreted as a form of robustness to actions of the symmetry group implied by the conserved quantity, by smoothing the function in this direction around data, in contrast to strict invariance by definition closed under group actions along the full orbit. Finally, we approximate the integral by an unbiased Monte Carlo estimate with \(S\) samples.

## 4 Automatic Symmetry Discovery using Noether's Razor

Now that we have a way of parameterising symmetry differentiably as conservation laws through Noether's theorem, we need an objective function that is capable of selecting the right symmetry. Unfortunately, regular training objectives that only rely on data fit can not necessarily distinguish the correct inductive bias, as noted in prior work (van der Wilk et al., 2018; Immer et al., 2022; van der Ouderaa and van der Wilk, 2021). This is because, even if train data originates from a symmetric distribution, there can be both non-symmetric and symmetric solutions that fit the train data equally well, given a sufficiently flexible model. Consequently, the regular maximum likelihood objective that only measures train data fit will not necessarily favour a symmetric model, even if we expect this to generalise best on test data. Instead of having to resort to cross-validation to select the right symmetry inductive bias, we propose to use an approximate marginal likelihood on the train data. This has the additional benefit of being differentiable, allowing symmetrisation to be learned with back-propagation along with regular parameters in a single training procedure. In our case, we use _Noether's theorem_ to parameterise symmetries in our prior through conserved quantities, which we can optimise with back-propagation using a differentiable lower bound on the marginal likelihood. This quantity, also known as the 'evidence', differs distinctly from maximum likelihood in that it balances both train fit as well as model complexity. The _Occam's razor_ effect encourages symmetry and leverages the symmetrisation process to 'cut away' prior density over Hamiltonian functions that are not symmetric, if this does not result in a worse data fit. The resulting posterior predictions automatically becomes symmetric if observed data obeys a symmetry (high evidence for symmetry), but can become non-symmetric if this does not match the data (low evidence for symmetry). Hence, the name of our proposed method for automatic inductive bias selection is _Noether's razor_.

### Probabilistic model with symmetries embedded in the prior.

To be more explicit about our probabilistic model, we can introduce four variables, namely a non-symmetrised observable \(F_{}\), a set of conserved quantities \(C_{}\), which induce a symmetrised Hamiltonian \(H\) generating the observed trajectory data \(X\). We treat trajectory data an an observed variable, consider the conserved quantities as part of an empirical prior as we optimise over them, and integrate out the Hamiltonian as latent. The construction can be interpreted a placing a sophisticated prior over the functions that the symmetrised Hamiltonian \(H\) can represent, which is the variable of primary interest. The underlying non-symmetrised \(F_{}\) does not have a direct physical meaning as \(H\) does, but defines a prior over neural networks to flexibly define a density over a rich class of possible functions. The conserved quantities \(C_{}\) control the amount of symmetry in the effective prior over symmetrised Hamiltonians \(H\). Empirically optimising \(C_{}\) through Bayesian model selection allows us to 'cut away' density in the prior over \(H\) that correspond to functions that are not symmetric - as the symmetrisation averages functions in \(F_{}\) that lie in the same orbit and thereby increases the relative density of symmetric functions in \(H\). We hypothesise that we will not over-fit conserved quantities as \(\) is relatively low-dimensional, only representing quadratic functions, while we integrate out the high-dimensional neural network model parameters \(\) that parameterises the observable \(F_{}\). In future work, it would be interesting to explore a richer function classes for conserved quantities, such as neural networks, although we do expect this to be more difficult and to require additional priors or regularisation techniques to avoid over-fitting.

Figure 1: Graphical probabilistic model. Trajectory data \(X\) depends on a symmetrised Hamiltonian \(H\) induced by non-symmetrised observable \(F\) and conservation laws \(C\).

### Bayesian model selection for symmetry discovery

To learn the right symmetry from data, we propose to use Bayesian model selection through optimisation of the marginal likelihood. In the previous sections, we have phrased symmetries parameterised by \(\) as part of the prior over Hamiltonians. The symmetry parameters \(\) parameterise the space of possible'models' that we consider, whereas the model parameters \(\) parameterise the weights of a single model. To perform Bayesian model selection on the symmetries, we are interested in computing the marginal likelihood:

\[p(|)=_{}p(|,)p() \]

which requires integrating (marginalising) the likelihood over model parameters \(\) weighted by the prior, and is sometimes referred to as the 'evidence' for a particular model. Unlike maximum likelihood, the marginal likelihood has an Occam's razor effect (Smith and Spiegelhalter, 1980; Rasmussen and Ghahramani, 2000) that balancing both data fit and model complexity, allowing optimisation of symmetry parameters \(\). Although the marginal likelihood is typically intractable, certain approximate Bayesian inference techniques can provide differentiable estimates. In the next sections, we will use variational inference to derive a tractable and differentiable lower bound to the marginal likelihood that can be used to find a posterior over \(\) and optimise symmetries \(\).

Why the marginal likelihood can learn symmetryTo understand why the marginal likelihood objective is capable of learning the right symmetry (to learn \(\)), Sec. 3.2(van der Wilk et al., 2018) proposed to decompose it through the product rule:

\[p()=p(_{1})p(_{2}|_{1},)p(_{3}_{1:2},)_{c=4}^{C}p(_{c} {x}_{1:c-1},) \]

which shows that the marginal likelihood measures how much parts of the dataset predict other parts of the data - a measure of generalisation that does not require cross-validation. Given a perfect data fit, the marginal likelihood will be higher when the right symmetry is selected, as parts of the dataset will result in better and more certain predictions on other part of the data. This is unlike the maximum likelihood, which is always maximised with perfect data fit, with or without the right symmetry. For some posterior approximations, such as linearised Laplace approximations, it can be analytically shown that symmetry maximises the approximate marginal likelihood (App. G.2 of Immer et al. (2022)). Our method is very similar, but uses more expressive variational inference which can optimise the posterior globally, rather than relying on a local Taylor expansion.

### Lower bounding the marginal likelihood

The marginal likelihood of an Hamiltonian neural network is typically not tractable in closed-form. However, we can derive a lower bound to the marginal likelihood using variational inference (VI):

\[ p() _{}[ p(, )]-(q_{,}() p()) \] \[_{}[_{}[ _{i=1}^{N}(_{t^{}}^{i} _{,}^{}(_{t}^{i}),_{}^{2} )]]-(q_{,}() p(,_{}^{2}))\]

where \(_{,}^{}(_{t}^{i})= _{s=1}^{S}H_{,}(_{}^{^{(s) }}(_{t}^{i}))\) and \(\) is an unbiased \(S\)-sample Monte Carlo estimator of the symmetrised Hamiltonian. We write \(_{}:=_{}q_{,}\) and \(_{}=_{_{s=1}^{S}()}\) for which we can obtain an unbiased estimate by taking Monte Carlo samples. The first inequality is the standard VI lower bound. The second inequality follows from applying Jensen's inequality (again) which uses the fact that the log likelihood is a convex function. Similar lower bounds to invariant models that average over a symmetry group have recently appeared in prior work (van der Ouderaa and van der Wilk, 2021; Schwobel et al., 2022; Nabarro et al., 2022). Full derivation in Appendix A.1.

### Improved variational inference for scalable Bayesian model selection

Variational inference is a common tool to perform Bayesian inference on models with intractable marginal likelihoods, including neural networks. In deep learning literature, however, its use is typically limited to better predictive uncertainty estimation and rarely for Bayesian model selection. Meanwhile, linearised Laplace approximations have recently been successfully applied to Bayesianmodel selection (Immer et al., 2021) and symmetry learning in specific (Immer et al., 2022; van der Ouderaa et al., 2024), with a few reported cases of model selection using VI only in single neural network layers (van der Ouderaa and van der Wilk, 2021; Schwobel et al., 2021). Optimising Bayesian neural networks with variational inference is much less established than training regular neural networks, for which many useful heuristics are available. This work, however, provides evidence that it is also possible to perform approximate Bayesian model selection using VI in deep neural networks, which we deem an interesting observation in its own right. To make sure the lower bound on the marginal likelihood is sufficiently tight, we employ a series of techniques, including a richer non-mean field family of matrix normal posteriors (Louizos and Welling, 2016), and closed-form updates of the prior precision and output variances derived with expectation maximisation. Details on how we train a Bayesian neural network using variational inference can be found in Appendix D.

## 5 Results

In this section, we will discuss how the learned symmetries are analysed and then list our experiments and results.

### Analyzing learned symmetries

In our experiments, we will find a set of \(K\) conserved quantities \(C_{k}:\). As we consider quadratic conserved quantities in particular, we can equivalently analyze the resulting generators of the associated symmetries \(_{k}()=J C_{k}\) which are affine and thus representable with a matrix \(_{k}^{(M+1)(M+1)}\) on homogeneous coordinates \((,1)\). In Appendix B, we list for each system the \(L\) ground truth conserved quantities generators \(G_{l}^{*}\). The learned and ground truth generators can be stacked in to the matrices \(}^{K(M+1)^{2}},^{*}^{L( M+1)^{2}}\) respectively. As we can identify the symmetries only up to linear combinations, we have learned the correct symmetries if the learned generators span a linear subspace of \(^{(M+1)^{2}}\) that coincides with the space spanned by the ground truth generators. To verify this, we test two properties. First, we show that the matrix \(}\) has \(L\) non-zero singular values. Secondly, for the first \(L\) right singular vectors \(v_{i}^{(M+1)^{2}}\), we decompose \(v_{i}=v_{i}^{}+v_{i}^{}\) in a vector in ground truth subspace, and one orthogonal to it. The learned \(v_{i}\) is a correct conserved quantity if \(v_{i}^{}=0\), or equivalently, because the singular vectors are normalized, if \(\|v_{i}^{}\|=1\). We call this measure the "parallelness".

### Simple Harmonic Oscillator

We start with a demonstration on the simple harmonic oscillator. This text book example has a 2-dimensional phase space, making learned Hamiltonians amenable to visualisation. Further, it has a clear rotational symmetry SO(2), relating to the conserved phase. On a finite set of generated train data, we model the Hamiltonian using a vanilla HNN, our symmetry learning method, and a model with true symmetry built-in as reference oracle (experimental details in Appendix B.1). In Figure 2, we find that our symmetry learning method results in a rotationally invariant Hamiltonian that matches the fixed rotational SO(2) symmetry. Further away from the origin, the learned Hamiltonian differs from the ground truth Hamiltonian, as there is no data in that region. In Table 1, we find that the learned symmetry has a better ELBO on the train set and matches the improved predictive performance of the model with the correct symmetry built-in. The symmetry learning method outperforms the vanilla model in terms of predictive performance on the test set.

Figure 2: Learned Hamiltonians on phase space of simple harmonic oscillator by HNN models.

### \(n-\)Harmonic Oscillators

Now, we consider \(n-\)harmonic oscillators. This system has as symmetry group the unitary Lie group U(\(n\)) of dimensionality of \(n^{2}\) (see Appendix B.2). We sample random trajectories from phase space and train a HNN neural network without and with symmetry learning using variational inference. Again, we find improved ELBO and test performance for learned symmetries Table 2. Following the protocol from Section 5.1, we analyze the learned symmetries. In Figure 3 (right), we see that for varying \(n\), we indeed find that the matrix of learned symmetries has \(n^{2}\) nonzero singular values. Furthermore, the first \(n^{2}\) singular vectors lie in the ground truth subspace of generators with measured parallelness \(\|v_{i}^{}\|>0.99\), as seen in Figure 3 (left). This shows that the \(U(n)\) symmetry is correctly learned.

### \(n\)-Body System

To investigate performance of our method on more interesting systems, we consider learning the Hamiltonian of an \(n\)-body system with gravitational interaction. We use 3 bodies in 2 dimensions so that trajectories and generators remain easy to visualise. As the Hamiltonian depends only on the norm of the momenta and on positions via the relative distances of the bodies, the three dimensional group SE(2) of rototranslations is an invariance of the ground truth Hamiltonian. However, as explained in Appendix B.3, the Hamiltonian has four more quadratic conserved quantities. They generate a 7-dimensional Lie group \(\) of symmetries. This group has the same orbits on the phase space as SE(2). Therefore, a function being invariant to SE(2) is equivalent to it being invariant to \(\). We'll find that Noether's razor discovers not just SE(2), but all seven generators of \(\).

   Learned dynamics: & &  &  \\
**simple harmonic oscillator** & & Train MSE & NLL/N & KL/N & -ELBO/N (\(\)) & Test MSE (\(\)) \\  HNN & & 0.00106 & -12.04 & 5.27 & -6.77 & 0.00002141 \\ HNN + learned symmetry & (**ours**) & 0.00102 & -12.16 & 2.53 & **-9.63** & **0.0000994** \\ HNN + fixed symmetry U(\(n\)) & (reference oracle) & 0.00102 & -12.15 & 2.21 & **-9.94** & **0.00000898** \\   

Table 2: Learning Hamiltonian dynamics of \(3-\)fold harmonic oscillators. We compare HNN with symmetry learning to a vanilla HNN without symmetry learning and to the correct U(3) symmetry built-in as fixed reference oracle. We find that our method can discover the correct symmetry, achieves reference oracle performance, and outperforms vanilla training in both ELBO and test performance.

Figure 4: Singular value and parallelness of the singular vectors of the learned generators for three body system in two dimensions. The 7-dimensional Lie group \(\) of quadratic conserved quantities is correctly learned.

   Learned dynamics: & &  &  \\
**simple harmonic oscillator** & & & & & & & \\  & & & NLL/N & KL/N & -ELBO/N (\(\)) & Test MSE (\(\)) \\  HNN & & 0.005 & 0.3667 & 3314.374 & 3314.741 & 0.005 \\ HNN + learned symmetry & (**ours**) & 0.002 & -2.618 & 3304.754 & **3302.136** & **0.002** \\ HNN + fixed SO(2) & (reference oracle) & 0.002 & -3.213 & 3298.357 & **3295.144** & **0.002** \\   

Table 1: Learning Hamiltonian dynamics of the simple harmonic oscillator. We compare a vanilla HNN, our symmetry learning method, and a model with the correct SO(2) symmetry built-in as reference oracle. Our method achieves reference oracle performance, indicating correct symmetry learning, and outperforms the vanilla model by improving predictive performance on the test set.

Figure 3: Singular value and parallelness of the singular vectors of the learned generators, for \(n\) oscillators. U(\(n\)) is correctly learned.

In Table 3, we compare performance of a vanilla variational HNN with our symmetry learning approach and a model that has the appropriate SE(2) symmetry of rototranslations built-in as an reference oracle. We find that our method is able to _automatically discover_ the conserved quantities and associated generators that span the symmetry group. The model achieves the same performance as the model with the symmetry built-in as reference oracle, but without having required the prior knowledge. Compared to the vanilla baseline, our approach improves test accuracy on both in-distribution as out-of-distribution test sets.

After training, we can analyse the learned conserved quantities and implied symmetries by inspecting their associated generators. In Figure 5, we plot these generators as well as their singular value decomposition. We see that our method correctly learns 7 singular values with \(_{i}>0.05\) and the associated singular vectors lie in the ground truth subspace with \(\|v_{i}^{}\|>0.95\). This indicates that our method is in fact capable of inferring the right symmetries from train data, beyond merely improving generalisation by improving predictive performance on the test set.

## 6 Conclusion

In this work, we propose to use _Noether's theorem_ to parameterise symmetries in machine learning models of dynamical systems in terms of conserved quantities. Secondly, we propose to leverage the _Occam's razor_ effect of Bayesian model selection by phrasing symmetries implied by conserved quantities in the prior and learning them by optimising an approximate marginal likelihood directly on train data, which does not require validation data or explicit regularisation of the conserved quantities. Our approach, dubbed _Noether's razor_, encourages symmetries by balancing both data fit and model complexity. We derive a variational lower bound on the marginal likelihood providing a concrete objective capable of jointly learning the neural network as well as the conserved quantities that symmetrise the Hamiltonian. As far as we know, this is also the first time differentiable Bayesian model selection using variational inference has been demonstrated on deep neural networks. We demonstrate our approach on \(n\)-harmonic oscillators and \(n\)-body systems. We find that our method learns the correct conserved quantities by analysing the singular values and correctness of the subspace spanned by the generators implied by learned conserved quantitites. Further, we find that our method performs on-par with models with the true symmetries built-in explicitly and we outperform vanilla model, improving generalisation and predictive accuracies on test data.

   Learned dynamics: & &  &  & Test data (msor) & Test data (msor) \\
**2d -body system** & & Train MSE & NLL/N & KL/N & -ELBON (\(\)) & Test MSE (\(\)) & Test MSE (\(\)) & Test MSE (\(\)) \\  HNN & & 0.0028 & -13.87 & 33.4 & -9.52 & 0.0016 & 0.0035 & 0.0016 \\ HNN + learned symmetry & (**ours**) & 0.0017 & -20.09 & 7.28 & **-12.81** & **0.0006** & **0.0004** & **0.0006** \\ HNN + fixed SE(2) & (reference oracle) & 0.0019 & -19.27 & 7.96 & **-1.32** & **0.0006** & **0.0006** & **0.0006** \\   

Table 3: Learning Hamiltonian dynamics of 2d 3-body system with variational Hamiltonian neural networks (HNN). We compare our symmetry learning method to a vanilla model without symmetry learning and a model with the correct SE(2) symmetry built-in as a reference oracle. Our method capable of discovering symmetry achieves the oracle performance, outperforming the vanilla method.

Figure 5: Learned generators associated by conserved quantities and their singular value decomposition. We find a subspace spanned by the 7 linear generators that correspond to the correct symmetries (see Appendix B.3): (1) rotation of the center of mass \(R^{ COM}\), (2) rotation around the origin \(R^{ ABS}\), (4+5) translation, (5+6+7) momentum-dependent translations \(P,Q\), (8+9+10) inactive (\(<0.05\)). The first 7 singular vectors lie in the ground truth subspace of generators with measured parallelness \(||v_{i}^{||}||>0.95\).