# Shallow Diffuse: Robust and Invisible Water-Marking through Low-Dimensional Subspaces in Diffusion Models

Shallow Diffuse: Robust and Invisible Water-Marking through Low-Dimensional Subspaces in Diffusion Models

Wenda Li\({}^{1*}\) Huijie Zhang\({}^{1*}\) Qing Qu\({}^{1}\)

\({}^{1}\)Department of Electrical Engineering & Computer Science, University of Michigan

{wdli,huijiezh,qingqu}@umich.edu

###### Abstract

The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce _Shallow Diffuse_, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, _Shallow Diffuse_ decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our _Shallow Diffuse_ outperforms existing watermarking methods in terms of robustness and consistency.

## 1 Introduction

Diffusion models (Ho et al., 2020; Song et al., 2021) have recently become a new dominant family of generative models, powering various commercial applications such as Stable Diffusion (Rombach et al., 2022; Esser et al., 2024), DALL-E (Ramesh et al., 2022; Betker et al., 2023), Imagen (Sahara et al., 2022) Stable Audio (Evans et al., 2024) and Sora (Brooks et al., 2024). These models have significantly advanced the capabilities of text-to-image, text-to-audio, text-to-video, and multi-modal generative tasks. However, the widespread usage of AI-generated content from commercial diffusion models on the Internet has raised several serious concerns: (a) AI-generated misinformation presents serious risks to societal stability by spreading unauthorized or harmful narratives on a large scale (Zellers et al., 2019; Goldstein et al., 2023; Brundage et al., 2018); (b) the memorization of training data by those models (Gu et al., 2023; Sompeplalli et al., 2023; Wen et al., 2023; Zhang et al., 2024) challenges the originality of the generated content and raises potential copyright infringement issues; (c) Iterative training on AI-generated content, known as model collapse (Fu et al., 2024; Alemohammad et al., 2024; Dohmatob et al., 2024; Shumailov et al., 2024; Gibney, 2024) can degrade the quality and diversity of outputs over time, resulting in repetitive, biased, or low-quality generations that may reinforce misinformation and distortions in the wild Internet.

To deal with these challenges, watermarking is a crucial technique for identifying AI-generated content and mitigating its misuse. Typically, it can be applied in two main scenarios: (a) _the server scenario_: where given an initial random seed, the watermark is embedded to the image during the generation process; and (b) _the user scenario_: where given a generated image, the watermark is injected in a post-process manner; (as shown in the left two blocks in Figure 3). Traditional watermarking methods (Cox et al., 2007; Solachidis and Pitas, 2001; Chang et al., 2005; Liu et al., 2019) are mainly designed for the user scenario, embedding detectable watermarks directly into images with minimal modification. However, these methods are vulnerable to attacks. For example, the watermarks can become undetectable with simple corruptions such as blurring on watermarked images. More recent methods considered the server scenario (Zhang et al., 2024; Fernandez et al., 2023; Wen et al., 2023; Yang et al., 2024; Ci et al., 2024), where they improve robustness by integratingwatermarking into the sampling process of diffusion models. For example, the work (Ci et al., 2024; Wen et al., 2023a) embeds the watermark into the initial random seed in the Fourier domain and then samples an image from the watermarked seed. As illustrated in Figure 1, these approaches often lead to inconsistent watermarked images because they significantly alter the noise distribution away from Gaussian. Moreover, they require access to the initial random seed, limiting their use in the user scenario. To the best of our knowledge, there is currently no robust and consistent watermarking method suitable for both the server and user scenarios (more detailed discussion about related works could be found in Appendix A).

To address these limitations, we proposed _Shallow Diffuse_, a robust and consistent watermarking approach that can be employed for both the server and user scenarios. Unlike prior works (Ci et al., 2024; Wen et al., 2023a) that embed watermarks into the initial random seed and entangle the watermarking process with sampling, Shallow Diffuse decouples these two steps by leveraging the low-dimensional subspace in the generation process of diffusion models (Wang et al., 2024; Chen et al., 2024). The key insight is that, due to the low dimensionality of the subspace, a significant portion of the watermark will lie in the null space of this subspace, effectively separating the watermarking from the sampling process (see Figure 3 for an illustration). Our theoretical and empirical analyses demonstrate that this decoupling strategy significantly improves the consistency of the watermark. With better consistency as well as independence from the initial random seed, Shallow Diffuse is flexible for both server and user scenarios.

**Our contributions.** The proposed Shallow Diffuse offers several key advantages over existing watermarking techniques (Cox et al., 2007; Solachidis and Pitas, 2001; Chang et al., 2005; Liu et al., 2019; Zhang et al., 2024; Fernandez et al., 2023; Wen et al., 2023a; Yang et al., 2024; Ci et al., 2024) that we highlight below:

* **Flexibility.** Watermarking via Shallow Diffuse works seamlessly under both server-side and user-side scenarios. In contrast, most of the previous methods only focus on one scenario without a straightforward extension to the other; see Table 1 and Table 2 for demonstrations.
* **Consistency and Robustness.** By decoupling the watermarking from the sampling process, Shallow Diffuse achieves higher robustness and better consistency. Extensive experiments (Table 1 and Table 2 ) support our claims, with extra ablation studies in Figure 3(a) and Figure 3(b).
* **Provable Guarantees.** Unlike previous methods, the consistency and detectability of our approach are theoretically justified. Assuming a proper low-dimensional image data distribution (see Assumption 1), we rigorously establish bounds for consistency (Theorem 1) and detectability (Theorem 2).

Figure 1: **Sampling variance of Tree-Ring Watermarks, RingID and Shallow Diffuse. On the left are the original images, and on the right are the corresponding watermarked images generated using three different techniques: Tree-Ring (Wen et al., 2023a), RingID (Ci et al., 2024), and Shallow Diffuse. For each technique, we generated watermarks using two distinct random seeds, resulting in the respective watermarked images.**

## 2 Preliminaries

We start by reviewing the basics of diffusion models (Ho et al., 2020; Song et al., 2021b; Karras et al., 2022), followed by several key empirical properties that will be used in our approach: the low-rankness and local linearity of the diffusion model (Wang et al., 2024; Chen et al., 2024).

### Preliminaries on diffusion models

Basics of diffusion models.In general, diffusion models consist of two processes:

* _The forward diffusion process._ The forward process progressively perturbs the original data \(_{0}\) to a noisy sample \(_{t}\) for some integer \(t[0,T]\) with \(T\). As in Ho et al. (2020), this can be characterized by a conditional Gaussian distribution \(p_{t}(_{t}|_{0})=(_{t};}_{ 0},(1-_{t})_{d})\). Particularly, parameters \(\{_{t}\}_{t=0}^{T}\) sastify: (_i_) \(_{0}=1\), and thus \(p_{0}=p_{}\), and (_ii_) \(_{T}=0\), and thus \(p_{T}=(,_{d})\).
* _The reverse sampling process._ To generate a new sample, previous works Ho et al. (2020); Song et al. (2021a); Lu et al. (2022); Karras et al. (2022) have proposed various methods to approximate the reverse process of diffusion models. Typically, these methods involve estimating the noise \(_{t}\) and removing the estimated noise from \(_{t}\) recursively to obtain an estimate of \(_{0}\). Specifically, One sampling step of Denoising Implicit Models (DDIM) Song et al. (2021a) from \(_{t}\) to \(_{t-1}\) can be described as: \[_{t-1}=}_{t}-}_{}(_{t},t)}{}}) }_{:=_{,t}(_{t})}+}_{ }(_{t},t),\] (1) where \(_{}(_{t},t)\) is parameterized by a neural network and trained to predict the noise \(_{t}\) at time \(t\). From previous works Zhang et al. (2024b); Luo (2022), the first term in Equation (1), defined as \(_{,t}(_{t})\), is the _posterior mean predictor_ (PMP) that predict the posterior mean \([_{0}|_{t}]\). DDIM could also be applied to a clean sample \(_{0}\) and generate the corresponding noisy \(_{t}\) at time \(t\), named DDIM Inversion. One sampling step of DDIM inversion is similar to Equation (1), by mapping from \(_{t-1}\) to \(_{t}\). For any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote multi-time steps DDIM operator and its inversion as \(_{t_{1}}=(_{t_{2}},t_{1})\) and \(_{t_{2}}=(_{t_{1}},t_{2})\).

Text-to-image (T2I) diffusion models & classifier-free guidance (CFG).The diffusion model can be generalized from unconditional to T2I (Rombach et al., 2022; Esser et al., 2024), where the latter enables controllable image generation \(_{0}\) guided by a text prompt \(\). In more detail, when training T2I diffusion models, we optimize a conditional denoising function \(_{}(_{t},t,)\). For sampling, we employ a technique called _classifier-free guidance_ (CFG) (Ho and Salimans, 2022), which substitutes the unconditional denoiser \(_{}(_{t},t,)\) in Equation (1) with the conditional counterpart \(}}(_{t},t,)\) that can be described as \(}}(_{t},t,)=(1-)_{}(_{t},t,)+_{}( _{t},t,)\). Here, \(}\) denotes the empty prompt and \(>0\) denotes the strength for the classifier-free guidance. For simplification, for any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote multi-time steps CFG operator as \(_{t_{1}}=(_{t_{2}},t_{1},)\). DDIM and DDIM inversion could also be generalized to T2I version, denotes as \(_{t_{1}}=(_{t_{2}},t_{1},)\) and \(_{t_{2}}=(_{t_{1}},t_{2},)\).

### Local Linearity and Intrinsic Low-Dimensionality in PMP

In this work, we will leverage two key properties of the PMP \(_{,t}(_{t})\) introduced in Equation (1) for watermarking diffusion models. Parts of these properties have been previously identified in recent papers (Wang et al., 2024; Manor and Michaeli, 2024b;a), and they have been extensively studied in (Chen et al., 2024). At one given timestep \(t[0,T]\), let us consider the first-order Taylor expansion of the PMP \(_{,t}(_{t}+)\) at the point \(_{t}\):

\[_{}(_{t};)\ :=\ _{,t}(_{t})+_{,t}(_{t}) ,}\] (2)

where \(^{d-1}\) is a perturbation direction with unit length, \(\) is the perturbation strength, and \(_{,t}(_{t})=_{_{t}}_{,t}( _{t})\) is the Jacobian of \(_{,t}(_{t})\). As shown in (Chen et al., 2024), it hasbeen found that within a certain range of noise levels, the learned PMP \(_{,t}\) exhibits local linearity, and its Jacobian \(_{,t}^{d d}\) is low rank:

* **Low-rankness of the Jacobian \(_{,t}(_{t})\).** As shown in Figure 2(a) of (Chen et al., 2024), the _rank ratio_ for \(t[0,T]\)_consistently_ displays a U-shaped pattern across various network architectures and datasets: (_i_) it is close to \(1\) near either the pure noise \(t=T\) or the clean image \(t=0\), (_ii_) \(_{,t}(_{t})\) is low-rank (i.e., the numerical rank ratio less than \(10^{-2}\)) for all diffusion models within the range \(t[0.2T,0.7T]\), (_iii_) it achieves the lowest value around mid-to-late timestep, slightly differs on different architectures and datasets.
* **Local linearity of the PMP \(_{,t}(_{t})\).** As shown in Figure 2(b) of (Chen et al., 2024), the mapping \(_{,t}(_{t})\) exhibits strong linearity across a large portion of the timesteps, which is consistently true among different architectures trained on different datasets. In particular, the work (Chen et al., 2024) evaluated the linearity of \(_{,t}(_{t})\) at \(t=0.7T\) where the rank ratio is close to the lowest value, showing that \(_{,t}(_{t}+)_{}(_{t};)\) even when \(=40\),

```
1:Inject watermark:
2:Input: original image \(_{0}\) for the user scenario (initial random seed \(_{T}\) for the server scenario), watermark \(\), embedding timestep \(t\),
3:Output: watermarked image \(_{0}^{}\),
4:if user scenario then
5:\(_{t}=(_{0},t)\)
6:else server scenario
7:\(_{t}=(_{T},t)\)
8:endif
9:\(_{t}^{}_{t}+\), \(_{0}^{}(_{t}^{ },0)\)\(\) Embed watermark
10:\(_{0}^{}(_{t},0)\), \(_{0}^{}(_{0}^ {},_{0}^{})\)\(\) Channel Average
11:Return:\(_{0}^{}\)
12:
13:Detect watermark:
14:Input: Attacked image \(}_{0}^{}\), watermark \(\), embedding timestep \(t\),
15:Output: Distance score \(\),
16:\(}_{t}^{}(}_{0} ^{},t)\)
17:\(=(}_{t}^{},)\)
18:Return:\(\) ```

**Algorithm 1** Unconditional Shallow Diffuse

## 3 Watermarking by Shallow-Diffuse

In this section, we introduce Shallow Diffuse for watermarking diffusion models. Building on the benign properties of PMP discussed in Section 2.2, we explain how to inject and detect invisible watermarks in _unconditional_ diffusion models in Section 3.1 and Section 3.2, respectively. Algorithm 1 outlines the overall watermarking method for unconditional diffusion models. In Section 3.3, we extend this approach to _text-to-image_ diffusion models, illustrated in Figure 3.

### Injecting invisible watermarks

Consider an unconditional diffusion model \(_{}(_{t},t)\) as we introduced in Section 2.1. Instead of injecting the watermark \(\) in the initial noise, we inject it in a particular timestep \(t[0,T]\) with

\[_{t}^{}=_{t}+,\] (3)

where \(\) is the watermarking strength, \(_{t}=(_{0},t)\) under the user scenario and \(_{t}=(_{T},t)\) under the server scenario. Based upon Section 2.2, we choose the timestep \(t\) so that the Jacobian of the PMP \(_{,t}(_{t})=_{_{t}}_{,t}( _{t})\) is _low-rank_. Moreover, based upon the linearity of PMP discussed in Section 2.2, we approximately have

\[_{,t}(_{t}^{})\ =\ _{,t}(_{t})+ _{,t}(_{t})\ \ _{,t}(_{t})\ =\ }_{0,t},\] (4)where we select the watermark \(\) to span the entire space \(^{d}\)_uniformly_; a more detailed discussion on the pattern design of \(\) is provided in Section 3.2. The key intuition for Equation (4) to hold is that, when \(r_{t}=(_{,t}(_{t})) d\) is low, a significant proportion of \(\) lies in the _null space_ of \(_{,t}(_{t})\) so that \(_{,t}(_{t})\).

Therefore, the selection of \(t\) is based on ensuring that \(_{,t}(_{t})\) is locally linear and that the dimensionality of its Jacobian \(r_{t} d\). In practice, we choose \(t=0.3T\) based on results from the ablation study in Section 4.3. As a results, the injection in Equation (4) maintains better consistency without changing the predicted \(_{0}\). In the meanwhile, it is very robust because any attack on \(_{0}\) would remain disentangled from the watermark, so that \(\) remains detectable.

Although in practice we employ the DDIM method instead of PMP for sampling high-quality images, the above intuition still carries over to DDIM. From Equation (1), one step sampling of DDIM in terms of \(_{,t}(_{t})\) becomes:

\[_{t-1}=}_{,t}(_{t} )}_{_{0}}+}}{}} _{t}-}_{,t}(_{t}))}_ {_{t}}.\] (5)

As explained in Song et al. (2021), the first term predicts \(_{0}\) while the second term points towards \(_{t}\). When we inject the watermark \(\) into \(_{t}\) as given in Equation (3), we know that

\[_{t-1}^{} =}_{,t}(_{t}^{})+}}{}}(_{t}^{ }-}_{,t}(_{t}^{} ))\] \[}_{,t}(_{t})+ }}{}}(_{t}+ -}_{,t}(_{t})),\] (6)

where the second approximation follows from Equation (4). This implies that the watermark \(\) is embedded into the DDIM sampling process entirely through the second term of Equation (6) and it decouples from the first which predicts \(_{0}\). Therefore, similar to our analysis for PMP, the first term in equation 6 maintains the consistency of data generation, while the difference in second term highlighted by blue would be useful for detecting the watermark which we will discuss next. In Appendix D, we provide more rigorous proofs validating the consistency and detectability of our approach.

### Watermark Design and Detection

Second, building on the watermark injection method described in Section 3.1, we discuss the design of the watermark pattern and the techniques for effective detection.

Watermark pattern design.Building on the method proposed by Wen et al. (2023), we inject the watermark in the frequency domain to enhance robustness against adversarial attacks. Specifically, we adapt this approach by defining a watermark \(\) for the input \(_{t}\) at timestep \(t\) as follows:

\[\ :=\ ((_{t} )(1-)+)-_{t},\] (7)

where the Hadamard product \(\) denotes the element-wise multiplication. Additionally, we have the following for Equation (7):

* **Transformation into the frequency domain.** Let \(()\) and \(()\) represent the forward and inverse Discrete Fourier Transform (DFT) operators, respectively. As shown in Equation (7), we first apply \(()\) to transform \(_{t}\) into the frequency domain, where we then introduce the watermark via a mask. Finally, the modified input is transformed back into the pixel domain using \(()\).
* **The mask and key of watermarks.**\(\) is the mask used to apply the watermark in the frequency domain as shown in the top-left of Figure 2, and \(\) denotes the key of the watermark. Typically, the mask \(\) is circular, with the white area representing \(1\) and the black area representing \(0\) in Figure 2, where we use it to modify specific frequency bands of the image. In the following, we discuss the design of \(\) and \(\) in detail.

Previous methods (Wen et al., 2023; Ci et al., 2024) design the mask \(\) to modify the low-frequency components of the initial noise input. While this approach works, as most of the energy in natural images is concentrated in the low-frequency range, it tends to distort the image when such watermarks are injected (see Figure 1 for an illustration). In contrast, as shown in Figure 2, we design the mask \(\) to target the high-frequency components of the image. Since high-frequency components capture fine details where the energy is less concentrated on these bands, modifying them results in less distortion of the original image. This is especially true in our case because we are modifying \(_{t}\), which is closer to \(_{0}\), compared to the initial noise used in (Wen et al., 2023; Ci et al., 2024).To modify the high-frequency components, we apply the DFT without shifting and centering the zero frequency, as illustrated in the bottom-left of Figure 2.

In terms of designing the key \(\), we follow Wen et al. (2023). The key \(\) is composed of multi-rings and each ring has the same value that is drawn from Gaussian distribution; see the top-right of Figure 2 for an illustration. Further ablation studies on the choice of \(\), \(\), and the effects of selecting low-frequency or high-frequency regions for watermarking can be found in Table 3.

**Watermark detection.** During watermark detection, suppose we are given a watermarked image \(}_{0}^{}\) with certain corruptions, we apply the DDIM Inversion to recover the watermarked image at timestep \(t\), denoted as \(}_{t}^{}=(}_{0}^{ },t)\). To detect the watermark, following Wen et al. (2023); Zhang et al. (2024), the \(()\) in Algorithm 1 calculates the following p-value:

\[=()\|-(}_{t}^{})\|_{F}^{2}}{\| (}_{t}^{})\|_{F}^{2}},\] (8)

where \(()\) is the summation of all elements of the matrix. Ideally, if \(}_{t}^{}\) is a watermarked image, \(=(}_{t}^{})\) and \(=0\). When \(}_{t}^{}\) is a non-watermarked image, \((}_{t}^{})\) and \(>0\). By choosing a threshold \(_{0}\), non-watermarked images will have \(>_{0}\) and watermarked images will have \(<_{0}\). Theoretically, the derivation of the p-value \(\) could be found in Zhang et al. (2024).

### Extension to Text-to-Image (T2I) Diffusion Models

Up to this point, our discussion has focused exclusively on unconditional diffusion models. Next, we demonstrate how our approach can be readily extended to text-to-image (T2I) diffusion models, which are predominantly used in practice.

Figure 3 provides an overview of our method for T2I diffusion models, which can be flexibly applied to both server and user scenarios. Specifically,

Figure 3: **Overview of Shallow Diffuse for T2I diffusion models.**

* **Watermark injection.** Shallow Diffuse embeds watermarks into the noise corrupted image \(_{t}\) at a specific timestep \(t=0.3T\). In the **server scenario**, given \(_{T}(,_{d})\) and prompt \(\), we calculate \(_{t}=(_{T},t,)\). In the **user scenario**, given the generated image \(_{0}\), we compute \(_{t}=(_{0},t,)\), using an empty prompt \(\). Next, similar to Section 3.1, we apply DDIM to obtain the watermarked image \(_{0}^{}=(_{t}^{},0,)\) and channel averaging \(_{0}^{*}(_{0}^{ },(_{t},0))\). The detailed discussion about channel averaging is in Appendix B.
* **Watermark detection.** During watermark detection, suppose we are given a watermarked image \(}_{0}^{}\) with certain corruptions, we apply the DDIM Inversion to recover the watermarked image at timestep \(t\), denoted as \(}_{t}^{}=(}_{0}^{ },t,)\). We detect the watermark \(\) in \(}_{t}^{}\) by calculating \(\) in Equation (8), with detail explained in Section 3.2.

## 4 Experiments

In this section, we present a comprehensive set of experiments to demonstrate the robustness and consistency of _Shallow-Diffuse_ across various datasets. Detailed experiment settings could be found in Appendix C.1. We begin by highlighting its performance in terms of robustness and consistency in both the server scenario (Section 4.1) and the user scenario (Section 4.2). Additionally, we compare Shallow Diffuse with other related works in the trade-off between robustness and consistency, as detailed in Appendix C.3. Moreover, we investigate the effect of timestep \(t\) on both robustness and consistency, with results presented in Section 4.3. Lastly, we provide an ablation study on watermark pattern design, and channel averaging in Appendix C.

### Consistency and robustness under the server scenario

Table 1 compares the performance of Shallow Diffuse with other methods in the user scenario. For reference, we also apply stable diffusion to generate images from the same random seeds, without adding watermarks (referred to as "Stable Diffusion w/o WM" in Table 1). In terms of generation quality, Shallow Diffuse achieves the best FID score among the diffusion-based methods. Additionally, the FID and CLIP scores of Shallow Diffuse are very close to those of Stable Diffusion w/o WM. This similarity arises because the watermarked distribution produced by Shallow Diffuse remains highly consistent with the original generation distribution. Regarding robustness, Shallow Diffuse outperforms all other methods. Although both Gaussian Shading and RingID exhibit comparable generation quality and robustness in the server scenario, they are less suitable for the user scenario. Specifically, Gaussian Shading embeds the watermark into \(_{T}\), which is not accessible to the user, while RingID suffers from poor consistency, as demonstrated in Figure 1 and Table 2.

### Consistency and robustness under the user scenario

Table 2 presents a comparison of Shallow Diffuse's performance against other methods in the user scenario. In terms of consistency, Shallow Diffuse outperforms all other diffusion-based approaches. To measure the upper bound of diffusion-based methods, we apply stable diffusion with \(}_{0}=((_{0},t,),0,)\), and measure the data consistency between \(_{0}\) and \(_{0}\) (denotes in Stable Diffusion w/o WM in Table 2). The upper bound is constrained by errors introduced through DDIM inversion, and Shallow Diffuse comes the closest to reaching this limit. For

   Method & CLIP-Score \(\) & FID \(\) & Clean & JPEG & G-Blur & G-Noise & Color Inter & Average \\   DarkNet & **0.3298** & 23.73 & 0.970.35 & 0.640.03 & 0.780.00 & 0.440.02 & 0.330.09 & 0.630.03 \\ DarkNet & 0.3291 & 26.00 & **1.001.00** & 0.800.00 & 0.990.80 & 0.970.84 & 0.500.09 & 0.320.45 \\ RandomAN & 0.3252 & **24.60** & 1.000.99 & **0.980.76** & **0.970.72** & **1.000.99** & **0.960.77** & **0.980.81** \\   \\  Stable Diffusion w/o WM & 0.3256 & 25.56 & & & & & & \\ Stable Signature & 0.3622 & 30.86 & **1.001.00** & 0.990.76 & 0.570.00 & 0.710.14 & 0.960.87 & 0.810.46 \\ Tree-Bing Watermarks & 0.310 & 52.82 & **1.001.00** & 0.990.73 & 0.800.93 & 0.800.94 & 0.960.67 & 0.970.80 \\ RingID & 0.3285 & 27.13 & **1.001.00** & **1.001.00** & **1.001.00** & 1.000.99 & 0.990.98 & 1.000.99 \\ Gaussian Shading & **0.3631** & 26.17 & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** \\   \\   

Table 1: **Comparison under the server scenario.**non-diffusion-based methods, which are not affected by DDIM inversion errors, better image consistency is achievable. As for the robustness, Shallow Diffuse outperforms all other methods in all three datasets. While RivaGAN achieves the best image consistency and comparable watermarking robustness to Shallow Diffuse in the user scenario, Shallow Diffuse is much more efficient. Unlike RivaGAN, which requires training for each individual image, Shallow Diffuse only involves the computational overhead of DDIM and DDIM inversion.

### Relation between injecting timestep, consistency and robustness

Figure 4 shows the relationship between the watermark injection timestep \(t\) and both consistency and robustness 1. Shallow Diffuse achieves optimal consistency at \(t=0.2T\) and optimal robustness at \(t=0.3T\). In practice, we select \(t=0.3T\). This result aligns with the intuitive idea proposed in Section 3.1 and the theoretical analysis in Appendix D: low-dimensionality enhances both data generation consistency and watermark detection robustness. However, according to Chen et al. (2024), the optimal timestep \(r_{t}\) for minimizing \(r_{t}\) satisfies \(t^{*}[0.5T,0.7T]\). We believe the best consistency and robustness are not achieved at \(t^{*}\) due to the error introduced by DDIM-Inv. As \(t\) increases, this error grows, leading to a decline in both consistency and robustness. Therefore, the best tradeoff is reached at \(t[0.2T,0.3T]\), where \(}(_{t})\) remains low-rank but \(t\) is still below \(t^{*}\).

    &  &  &  &  \\   & & & & & & & & & \\  Clean & JPEG & G.Blur & G.Noise & Color filter & Average \\
**COCO** & & & & & & & & \\  DuCluster & 37.88 & 0.97 & **0.62** & 0.900.83 & 0.480.02 & 0.500.00 & 0.300.00 & 0.350.16 & 0.460.06 \\ DuCluster & 38.06 & **0.96** & **0.62** & **1.000.00** & 0.700.26 & 0.900.83 & 0.910.35 & 0.541.4 & 0.790.43 \\ Re-GAN & **0.85** & **0.98** & **0.04** & **1.000.10** & **1.000.100** & **0.990.86** & **1.000.39** & **9.700.83** & **9.990.22** \\ Style Diffusion \& w/ WM & 32.28 & 0.78 & 0.06 & & & & & \\ Tree-Image Watermarks & 28.22 & 0.51 & 0.41 & **1.000.100** & 0.990.87 & 0.900.86 & 1.000.03 & 0.850.49 & 0.970.81 \\ RivaGAN & **28.22** & 0.38 & 0.61 & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.00** & 9.990.96 \\
**Shadow Diffuse (ours)** & **32.11** & **0.77** & **0.66** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** \\
**DiffusionDB** & & & & & & & & & \\  DuCluster & 37.77 & 0.96 & **0.2** & 0.900.76 & 0.710.23 & 0.900.70 & 0.350.01 & 0.520.12 & 0.640.27 \\ DuCluster & 37.84 & 0.97 & **0.82** & **1.000.100** & 0.710.23 & 0.500.00 & 0.320.59 & 0.500.00 & 0.720.23 \\ Re-GAN & **0.85** & **0.98** & 0.94 & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** \\  Single Diffusion with WM & 33.42 & 0.85 & 0.10 & & & & & & \\ Tree-Image Watermarks & 25.03 & 0.62 & 0.29 & **1.000.100** & 0.990.68 & 0.940.762 & **1.000.00** & 0.850.13 & 0.940.61 \\ RivaID & 27.9 & 0.21 & 0.77 & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.00** & **1.000.00** & 9.990.86 \\
**Shadow Diffuse (ours)** & **33.07** & **0.84** & **0.84** & **1.000.100** & 1.000.000 & 1.000.99 & 1.000.100** & **1.000.100** & **1.000.00** \\ 
**Wuixart** & & & & & & & & & \\  DuCluster & 38.84 & 0.97 & 0.02 & 0.900.73 & 0.440.000 & 0.510.00 & 0.260.00 & 0.496.12 & 0.240.03 \\ DuCluster & 39.44 & 0.98 & 0.02 & **1.000.00** & 0.691.3 & 0.970.00 & 0.370.00 & 0.500.13 & 0.700.04 \\ Re-GAN & **0.44** & **0.98** & **0.05** & **1.000.000** & 0.970.001 & 1.000.52 & **1.000.00** & 0.900.00 & 0.70.05 \\ Simple Diffusion with WM & 31.6 & 0.7 & 0.96 & **1.000.000** & 1.000.97 & 1.000.38 & **1.000.000** & 0.710.000 & 0.920.78 \\ Tree-Image Watermarks & 25.20 & 0.39 & 0.34 & **1.000.000** & 1.000.97 & 1.000.38 & 1.000.000 & 0.920.78 \\
**Shadow Diffuse (ours)** & **34.05** & **0.10** & **1.000.000** & 1.000.99 & 1.000.59 & 1.000.100 & **1.000.00** & **1.000.99** \\   

Table 2: **Comparison under the user scenario.**

Figure 4: **Ablation study of the watermark at different timestep \(t\).**

Conclusion

We proposed Shallow Diffuse, a novel and flexible watermarking technique that operates seamlessly in both server-side and user-side scenarios. By decoupling the watermark from the sampling process, Shallow Diffuse achieves enhanced robustness and greater consistency. Our theoretical analysis demonstrates both the consistency and detectability of the watermarks. Extensive experiments further validate the superiority of Shallow Diffuse over existing approaches.