# On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence

Achraf Azize

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

achraf.azize@inria.fr &Marc Jourdan

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

marc.jourdan@inria.fr &Aymen Al Marjani

UMPA, ENS Lyon

Lyon, France

aymen.al_marjani@ens-lyon.fr &Debabrota Basu

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

debabrota.basu@inria.fr

###### Abstract

Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of _BAI with fixed confidence under \(\)-global Differential Privacy (DP)_. First, to quantify the cost of privacy, we derive a _lower bound on the sample complexity_ of any \(\)-correct BAI algorithm satisfying \(\)-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget \(\). In the high-privacy regime (small \(\)), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the _Total Variation Characteristic Time_. In the low-privacy regime (large \(\)), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an \(\)-global DP variant of the Top Two algorithm. AdaP-TT runs in _arm-dependent adaptive episodes_ and adds _Laplace noise_ to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.

## 1 Introduction

We study the stochastic multi-armed _bandit_ problem , which allows us to reflect on fundamental information-utility trade-offs involved in interactive sequential learning. Specifically, in a bandit problem, a learning _agent_ is exposed to interact with \(K\) unknown probability distributions \(\{_{1},,_{K}\}\) with bounded expectations, referred as the _reward distributions_ (or _arms_). \(\{_{1},,_{K}\}\) is called _a bandit instance_. At every step \(t>0\), the agent chooses to interact with one of the reward distributions \(_{A_{t}}\) for an \(A_{t}[K]\), and obtains a sample (or _reward_) \(r_{t}\) from it. The goal of the agent can be of two types: (a) maximise the reward accumulated over time, or equivalently to minimise the regret, and (b) to find the reward distribution (or arm) with highest expected reward. The first problem is called the regret-minimisation problem , while the second one is called the _Best Arm

[MISSING_PAGE_EMPTY:2]

1. _What is the fundamental hardness of ensuring Differential Privacy in the Best-Arm Identification with fixed confidence problem (\(\)-DP-FC-BAI) in terms of a lower bound on the sample complexity?_
2. _How to design an efficient \(\)-DP-FC-BAI algorithm that achieves the lower bound order-optimally?_

**Our contributions.** These questions have led to the following contributions:

1. _Hardness as lower bounds:_ We commence our study by deriving the lower bound on sample complexity (or expected stopping time) of any FC-BAI algorithm ensuring \(\)-global DP, in brief \(\)**-DP-FC-BAI** (Section 3). In Theorem 2, we prove that the sample complexity of \(\)-DP-FC-BAI depends on the minimum of two information-theoretic quantities one that depends on privacy, and the other that originates from the classical sample complexity of non-private FC-BAI . The term dependent on privacy depends on the privacy budget \(\), and a novel information-theoretic quantity, referred to as the _Total Variation Characteristic Time_\(T_{}^{*}\). \(T_{}^{*}\) depends on the Total Variation (TV) distance between the reward distributions in the bandit problem and corresponding most confusing instance. The lower bound also indicates that, in a similar spirit as the regret minimisation with \(\)-global DP , there are two regimes of hardness for \(\)-DP-FC-BAI. For lower level of privacy (i.e. higher \(\)), the sample complexity of \(\)-DP-FC-BAI is identical to that of the non-private FC-BAI. But for higher level of privacy (i.e. lower \(\)), the sample complexity depends on \(\) and \(T_{}^{*}\).
2. _Algorithm design:_ Following the lower bounds, we aim to design an efficient \(\)-DP-FC-BAI algorithm that simultaneously achieves the lower bound order-optimally and is computationally efficient (Section 4). Due to the superior empirical performance and computational efficiency of Top Two algorithms, we design an \(\)-DP variant of Top Two algorithms, named AdaP-TT. Specifically, we show two simple design techniques, i.e. adaptive episodes for each arm and Laplacian mechanism, if properly used, lead to AdaP-TT from the non-private TTUCB . We further derive an asymptotic (as \( 0\)) upper bound on the sample complexity of AdaP-TT (Theorem 5). In the high-privacy regime, the sample complexity upper bound of AdaP-TT coincides with the lower bound up to multiplicative constants. Thus, it is an order-optimal \(\)-DP-FC-BAI algorithm in this regime, whereas the looseness in the low-privacy regime is as same as the looseness of the non-private Top Two algorithm. In Section 5, we experimentally show that AdaP-TT is more sample efficient than the existing \(\)-DP-FC-BAI algorithm, i.e. DP-SE . We also show that its sample complexity is independent of the privacy budget in the low-privacy regime, as already indicated by the lower bound.
3. _Technical tools:_ (a) To derive the lower bound, we provide an \(\)-global DP version of the transportation lemma  (Lemma 1), which we prove using a sequential coupling argument. We also define and study the TV characteristic time (\(T_{}^{*}\)) quantifying the hardness of FC-BAI in high privacy regimes (Proposition 1). (b) To design the algorithm, we propose a _generic wrapper_, _which adapts the existing FC-BAI algorithm to tackle DP-FC-BAI_. It builds on two components: (i) Adaptive episodes with per-arm doubling and forgetting, (ii) A private GLR stopping rule obtained by plugging in private empirical means in the non-private GLR stopping rule used of FC-BAI with Gaussian distributions. To use this proposed wrapper, one can choose among the numerous existing sampling rules to tackle FC-BAI. In this work, we consider the Top Two algorithms since they have good theoretical guarantees and empirical performance. To provide the sample complexity upper bound, we study step-by-step the effects of doubling, forgetting, and adding noise on the performance of the algorithm. Building on , we provide a generic analysis of the class of Top Two algorithms when combined with our wrapper.

### Related works

**Lower bound.** Efficient algorithm design in BAI literature is propelled by the derivation of lower bounds on sample complexity.  derive the first lower bounds for classical fixed-confidence BAI setting without privacy, which is further improved in  by introducing KL characteristic time \(T_{}^{*}\) (Corollary 1). Motivated by this, _we prove the first-known lower bound on sample complexity for FC-BAI with \(\)-global DP_ (Theorem 2). The proof employs a similar sequential coupling argument as in the regret lower bound for bandits with \(\)-global DP . This similarity is also reflected in the existence of two privacy regimes depending on the privacy budget \(\). And for both lower bounds, the Total Variation (TV) appears to be the information-theoretic measure that captures the hardness in the high privacy regime. Specifically, the TV characteristic time (\(T_{}^{*}\), Corollary 1) serves as the BAI counterpart to the TV-distinguishability gap (\(t_{}\)) in the problem-dependent regret lower bound for bandits with \(\)-global DP as in [1, Theorem 3].

**Algorithms for BAI with fixed confidence (FC-BAI).** The optimal sample complexity for the non-private FC-BAI problem (i.e., \(T_{}^{*}\)) is well-understood , and algorithms are proposed with the aim to achieve this lower bound. Early approaches involved Successive Elimination (SE) based algorithms  with uniform sampling to find the optimal arm. Inspired by the success of Upper Confidence Bound (UCB) algorithms in the regret setting, the Lower Upper Confidence Bound (LUCB) algorithm was proposed . However, neither SE nor LUCB algorithms achieve asymptotic optimality. The Track-and-Stop (TnS) algorithm introduced in  was the first to asymptotically achieve the exact optimal sample complexity \(T_{}^{*}\). TnS attains asymptotic optimality by solving a plug-in estimate of the lower bound optimization problem at each step. The game-based approach presented in  relaxes this requirement by casting the optimization problem as an unknown game and proposing sampling rules based on iterative strategies to estimate and converge to its saddle point. Finally, the Top Two algorithms arose as an identification strategy based on the praised Thompson Sampling algorithm for regret minimization . In recent years, numerous variants have been analyzed and shown to be asymptotically near optimal . At every step, a Top Two sampling rule selects the next arm to sample from among two candidate arms, a leader and a challenger. In addition to their great empirical performance, and easy implementation compared to TnS and Game-based algorithms, the Top Two algorithms achieve near asymptotic optimality. In Sec. 4, _we derive an \(\)-global DP version of a Top Two algorithm:_ AdaP-TT_.

\(\)**-DP BAI algorithms.** DP-SE  is an \(\)-global DP version of the Successive Elimination algorithm. Although the algorithm was proposed and analysed for the regret minimisation setting in , it is possible to derive a sample complexity from the analysis in . We compare in-depth DP-SE and AdaP-TT, both theoretically (Section 4) and experimentally (Section 5). _In both aspects, our proposed algorithm_ AdaP-TT _outperforms DP-SE._ Another adaptation of DP-SE, namely DP-SEQ, is proposed in  for the problem of privately finding the arm with the highest quantile at a fixed level. But this is a different setting of interest than the present paper.  also studies privacy for BAI under fixed confidence but with multiple agents. They propose and analyse the sample complexity of DP-MASE, a multi-agent version of DP-SE. They show that multi-agent collaboration leads to better sample complexity than independent agents, even under privacy constraints. While the multi-agent setting with federated learning allows tackling large-scale clinical trials taking place at several locations simultaneously, we study the single-agent setting, which is relevant for many small-scale clinical trials (see Example 1).

## 2 Differential privacy and best arm identification

**Background: Differential Privacy (DP).** DP ensures protection of an individual's sensitive information when her data is used for analysis. A randomised algorithm satisfies DP if the output of the algorithm stays almost the same, regardless of whether any single individual's data is included in or excluded from the input. This is achieved by adding controlled noise to the algorithm's output.

**Definition 1** (\((,)\)-Dp ).: _A randomised algorithm \(\) satisfies \((,)\)-Differential Privacy (DP) if for any two neighbouring datasets \(\) and \(^{}\) that differ only in one entry, i.e. \(d_{}(,^{})=1\), and for all sets of output \(()\),_

\[[()] e^{} [(^{})]+,\] (1)

_where the probability space is over the coin flips of the mechanism \(\), and for some \((,)^{ 0}^{ 0}\). If \(=0\), we say that \(\) satisfies \(\)-DP. A lower privacy budget \(\) implies higher privacy._

The Laplace mechanism  ensures \(\)-DP by injecting controlled random noise into the output of the algorithm, which is sampled from a calibrated Laplace distribution (as specified in Theorem 1). We use \(Lap(b)\) to denote the Laplace distribution with mean 0 and variance \(2b^{2}\).

**Theorem 1** (\(\)-DP of Laplace mechanism (Theorem 3.6, )).: _Let us consider an algorithm \(f:^{d}\) with sensitivity \(s(f)_{,^{}\,s_{I}|\,-^{}|_{}=1} f()-f(^{ }) 1\). Here, \(_{1}\) is the \(L_{1}\) norm on \(^{d}\). If \(d\) noise samples \(\{N_{i}\}_{i=1}^{d}\) are generated independently from \(Lap()\), then the output injected with the noise, i.e. \(f()+[N_{1},,N_{d}]\), satisfies \(\)-DP._

**Background: BAI with fixed confidence.** Now, we describe the canonical best-arm identification problem with fixed confidence (_FC-BAI_). BAI is a variant of pure exploration, where the goal is toidentify the optimal arm. In FC-BAI, the learner is provided with a confidence level \(1-(0,1)\)1. Learner aims to recommend an arm that is optimal with probability at least \(1-\), while using as few samples as possible. To achieve this, the learner defines a FC-BAI strategy to interact with the bandit instance \(=\{_{a}:a[K]\}\). We denote the action played at step \(t\) by \(a_{t}\), and the corresponding observed reward by \(r_{t}_{a_{t}}\). \(_{t}=(a_{1},r_{1},,a_{t},r_{t})\) is the history of actions played and rewards collected until time \(t\). We augment the action set by a _stopping action_\(\), and write \(a_{t}=\) to denote that the algorithm has stopped before step \(t\). A FC-BAI strategy \(\) is composed of

**i. A pair of sampling and stopping rules \((_{t}:_{t-1}([|1,K|]| \{\}))_{t 1}\)**. For an action \(a[K]\), \(_{t}(a_{t-1})\) denotes the probability of playing action \(a\) given history \(_{t-1}\). On the other hand, \(_{t}(_{t-1})\) is the probability of the algorithm halting given \(_{t-1}\). For any history \(_{t-1}\), a consistent sampling and stopping rule \(_{t}\) satisfies \(_{t}(_{t-1})=1\) if \(\) has been played before \(t\).

**ii. A recommendation rule \((_{t}:_{t-1}([|1,K|]) )_{t>1}\)**. A recommendation rule dictates \(_{t}(a_{t-1})\), i.e. the probability of returning action \(a\) as a guess for the best action given \(_{t-1}\).

We denote by \(\) the **stopping time** of the algorithm, i.e. the first step \(t\) demonstrating \(a_{t}=\). A BAI strategy \(\) is called \(\)**-correct** for a class of bandit instances \(\), if for every instance \(\), \(\) recommends the optimal action \(a^{}()=_{a[K]}_{a}\) with probability at least \(1-\), i.e. \(_{,}(<,=a^{}( )) 1-\).

**FC-BAI with \(\)-global DP (\(\)-DP-FC-BAI).** Now, we formally define \(\)-global DP for FC-BAI, where the BAI strategy (a.k.a. the centralised decision maker) is trusted with all the intermediate rewards. We represent each user \(u_{t}\) by the vector \(_{t}(x_{t,1},,x_{t,K})^{K}\), where \(x_{t,a}\) represents the **potential** reward observed, if action \(a\) was recommended to user \(u_{t}\). Due to the bandit feedback, only \(r_{t}=x_{t,a_{t}}_{a_{t}}\) is observed at step \(t\). We use an underline to denote any sequence. Thus, we denote the sequence of sampled actions until \(T\) as \(^{T}=(a_{1},,a_{T})\). We further represent a set of users \(\{u_{t}\}_{t=1}^{T}\) until \(T\) by **the table of potential rewards \(^{T}\{_{1},,_{T}\}(^{K})^{T}\)**. First, we observe that \(^{T}\) is the sensitive input generated through interaction with the users, and \((^{T},,T)\) is the output of the BAI strategy. Hence, we define the probability that the BAI strategy \(\) samples the action sequence \(^{T}\), recommends the action \(\), and halts at time \(T\), as

\[(^{T},,T^{T})_{ T+1}(_{T})_{T+1}( _{T})_{t=1}^{T}\ _{t}(a_{t}_{t-1})\] (2)

where \(T\) users under interaction are represented by the table of potential rewards \(^{T}\)

Thus, a BAI strategy satisfies \(\)-global DP if the probability defined in Eq. (2) is similar when the BAI strategy interacts with two neighbouring tables of rewards differing by a user (i.e. a row in \(^{T}\)).

**Definition 2** (\(\)-global DP for BAI).: _A BAI strategy satisfies \(\)**-global DP**, if for all \(T 1\), all neighbouring table of rewards \(^{T}\) and \(^{T}\), i.e. \(d_{}(^{T},^{T})=1\), all sequences of sampled actions \(^{T}[K]^{T}\) and recommended actions \([K]\) we have that_

\[(^{T},,T^{T}) e^{}( ^{T},,T^{T}).\]Definition 2 can be seen as a BAI counterpart of the \(\)-global DP definition proposed in  for regret minimization. We demonstrate the BAI strategy-Users interaction in Algorithm 1.

**Remark 1**.: _It is possible to consider that the output of a BAI strategy is only the final recommended action \(\), i.e. not publishing the intermediate actions \(^{T}\). This gives a weaker definition of privacy compared to Definition 2, since the latter defends against adversaries that may look inside the execution of the BAI strategy, i.e. pan-privacy . In addition, Definition 2 is needed in practice. For example, in the case of dose-finding (Example 1), the experimental protocol, i.e. the intermediate actions, needs to be published too._

**The goal** in \(\)-DP-FC-BAI is to design a \(\)-correct \(\)-global DP algorithm, with \(\) as small as possible.

## 3 Lower bound on sample complexity for FC-BAI with \(\)-global DP

The central question that we address in this section is

_How many additional samples a BAI strategy must select for ensuring \(\)-global DP?_

In response, we prove a lower bound on the sample complexity of any \(\)-correct \(\)-DP BAI strategy. Our lower bound features problem-dependent characteristic times reminiscent of the FC-BAI setting.

Let \(\{_{a}:a[K]\}\) be a bandit instance, consisting of \(K\) arms with finite means \(\{_{a}\}_{a[K]}\). Now, we define the set of alternative instances as \(()\{:a^{}() a ^{}()\}\), i.e. the bandit instances with a different optimal arm than \(\). For two probability distributions \(,\) on the same measurable space \((,)\), the Total Variation (TV) distance is defined as \((\ \|\ )_{A}\{ (A)-(A)\}\), while the KL divergence (or relative entropy) is \((\ \|\ )(}{}())\ ()\), when \(\), and \(\) otherwise. We denote the probability simplex by \(_{K}\{^{K}:_{a=1}^{K}_{a}=1\}\).

First, we derive an \(\)-global DP variant of the 'transportation' lemma, i.e. Lemma 1 in .

**Lemma 1** (Transportation lemma under \(\)-global DP).: _Let \((0,1)\) and \(>0\). Let \(\) be a bandit instance and \(()\). For any \(\)-correct \(\)-global DP BAI strategy, we have that_

\[6_{a=1}^{K}_{,}[N_{a}()] (_{a}_{a})(1-, ),\]

_where \((1-,) x+(1-x)\) for \(x,y(0,1)\)._

_Proof sketch._ We use Sequential Karwa-Vadhan Lemma [1, Lemma 2] with a data-processing inequality in the BAI canonical model. Extra care is needed _to deal with the stopping times in the coupling, compared to a fixed horizon \(T\) in regret minimization_. The proof is deferred to Appendix B.

Leveraging Lemma 1, we derive a sample complexity lower bound for any \(\)-DP-FC-BAI strategy.

**Theorem 2** (Sample complexity lower bound for \(\)-DP-FC-BAI).: _Let \((0,1)\) and \(>0\). For any \(\)-correct \(\)-global DP BAI strategy, we have that_

\[_{}[] T^{}(;)( 1/3),\] (3)

_where \((T^{}(;))^{-1}_{ _{K}}_{()}(_{a=1}^{ K}_{a}(_{a}_{a}),6_{a=1}^{ K}_{a}(_{a}_{a}))\)._

**Comments on the lower bound.** Similar to the lower bound for the non-private BAI , the lower bound of Theorem 2 is the value of a two-player zero-sum game between a MIN player and MAX player. MIN plays an alternative instance \(\) close to \(\) in order to confuse MAX. The latter plays an allocation \(_{K}\) to explore the different arms, with the purpose of maximising the divergence between \(\) and the confusing instance \(\) that MIN played. On top of the KL divergence present in the non-private lower bound, our bound features the TV distance that appears naturally when incorporating the \(\)-global DP constraint. The proof is deferred to Appendix B. In order to compare the lower bound of an \(\)-global BAI strategy with the non-private lower bound of , we relax Theorem 2 to further derive a simpler bound, as in Corollary 1.

**Corollary 1**.: _For any \(\)-correct \(\)-global DP BAI strategy, we have that_

\[_{}[](T^{}_{}(), T^{}_{}())(1/3),\]

_where \((T^{}_{}())^{-1}_{_{K}}_{ ()}_{a=1}^{K}_{a}(_{a}, _{a})\), and \(\) is either \(\) or \(\)._Proof.: The proof is direct by observing that \(T^{}(;) T^{}_{}()\) and \(T^{}(;)T^{}_{}()\). 

**Comparison with the non-private lower bound.**\(T^{}_{}\) is the characteristic time in the non-private lower bound , and we refer to Section 2.2 of  for a detailed discussion on its properties. The sample complexity lower bound suggests the existence of _two hardness regimes depending on \(\)_, \(T^{}_{}\) _and_\(T^{}_{}\). (1) _Low-privacy regime_: When \(>T^{}_{()}/(6T^{}_{}())\), the lower bound retrieves the non-private lower bound, i.e. \(T^{}_{}()\), and thus, **privacy can be achieved for free**. (2) _High-privacy regime:_ When \(<T^{}_{}()/(6T^{}_{}())\), the lower bound becomes \(T^{}_{}/(6)\) and \(\)-global DP \(\)-BAI requires more samples than non-private ones.

In the following proposition, we characterise \(T^{}_{}\) for Bernoulli instances.

**Proposition 1** (TV characteristic time for Bernoulli instances).: _Let \(\) be a bandit instance, i.e. such that \(_{a}=(_{a})\) and \(_{1}>_{2}_{K}\). Let \(_{a}_{1}-_{a}\) and \(_{}_{a 1}_{a}\). We have that_

\[T^{}_{}()=}}+_{a=2}^{K} },}} T^{}_{}()}}.\]

Proof sketch.: The proof is direct by solving the optimisation problem defining \(T^{}_{}\) and using that \(((p)(q))=|p-q|\). We refer to Appendix B for details.

_Comment._ The aforementioned bound on TV characteristic time for Bernoulli instances is \(\)-global DP parallel of the KL-characteristic time bound \(T^{}_{}()_{a=1}^{K}_{a}^{-2}\). Using Pinsker's inequality, one can connect the TV and KL characteristic times by \(T^{}_{}()_{}()}\).

## 4 Algorithm design: Private Top Two with adaptive episodes (AdaP-TT)

In this section, we propose AdaP-TT, an \(\)-global DP version of the TTUCB algorithm . We show that AdaP-TT satisfies \(\)-global DP, is \(\)-correct, and has an asymptotic sample complexity that matches the high privacy lower bounds up to multiplicative constants.

**TTUCB** belongs to the family of Top Two algorithms , JDB+22], which selects at each time two arms called leader and challenger, and sample among them. After initialisation, TTUCB uses a UCB-based leader and a Transportation Cost (TC) challenger, expressed by

\[B_{n}=*{arg\,max}_{a[K]}\{_{n,a}+}\}, C_{n}=*{arg\,min}_{a B_{n}} _{n,B_{n}}-_{n,a}}{}+1/N_{n,a}}}\;.\]

Here, \((_{n},N_{n})\) are the empirical means and counts on the whole history. The theoretical motivation behind the TC challenger comes from the theoretical lower bound in FC-BAI, which involves the KL-characteristic time \(T^{}_{}()=_{(0,1)}T^{}_{, }()\). For Gaussian distributions, we have

\[2T^{}_{,}()^{-1}=_{_{K}, _{a}*=}}-_{a})^{2}}{1/+1/_{a}}  T^{}_{,1/2}() 2T^{}_{}()\;,\]

The maximiser of the above equation is denoted by \(^{}_{,}()\), and is further referred to as the \(\)-optimal allocation as it is unique. Let \(N^{a}_{n,b}\) denote the number of times arm \(b\) was pulled when \(a\) was the leader, and \(L_{n,a}\) denotes the number of times arm \(a\) was the leader. In order to select the next arm to sample \(I_{n}\), TTUCB relies on \(K\) tracking procedures, i.e. set \(I_{n}=B_{n}\) if \(N^{B_{n}}_{n,B_{n}} L_{n+1,B_{n}}\), else \(I_{n}=C_{n}\). This ensures that \(_{a[K],n>K}|N^{a}_{n,a}- L_{n,a}| 1\). Standing on this premise, we now describe how we design an \(\)-global DP extension of TTUCB.

**Private algorithm design.** As illustrated in Algorithm 2, AdaP-TT relies on three ingredients: _adaptive episodes with doubling_, _forgetting_, and _adding calibrated Laplacian noise_. (1) AdaP-TT maintains \(K\) episodes, i.e. one per arm. The private empirical estimate of the mean of an arm is only updated at the end of an episode, that means when the number of times that a particular arm was played doubles (Line 5). (2) For each arm \(a\), AdaP-TT forgets rewards from previous phases of arm \(a\), i.e. the private empirical estimate of arm \(a\) is only computed using the rewards collected in the last phase of arm \(a\) (Line 8). This assures that the means of each arm are estimated using a non-overlapping sequence of rewards. (3) Thanks to this _doubling_ and _forgetting_, AdaP-TT is \(\)-global DP as soon as each empirical mean (Line 9) is made \(\)-DP, and thus, avoiding any use of privacy composition. This is achieved by adding Laplace noise. We formalise this intuition in Lemma 2 of Appendix C.

**Remark 2**.: _The aforementioned generic wrapper can be used to construct a near-optimal differentially private version of any existing FC-BAI algorithm that deploys a sampling rule with the empirical means of rewards. In this work, we consider and rigorously analyse the Top Two algorithms since they demonstrate both good theoretical guarantees and empirical performance._

**Theorem 3** (Privacy analysis).: _For rewards in \(\), AdaP-TT satisfies \(\)-global DP._

Proof sketch.: A change in one user _only affects_ the empirical mean calculated at one episode of an arm, which is made private using the Laplace Mechanism and Lemma 2. Since the sampled actions, recommended action, and stopping time are computed only using the private empirical means, AdaP-TT satisfies \(\)-global DP thanks to post-processing lemma. We refer to Appendix C for details.

**Private GLR stopping rule.** We consider the private GLR stopping rule based on the private means and on the pulling counts from the last phase (Line 12), and recommend the arm with the highest private mean (Line 11). Lemma 4 yields a threshold function ensuring that any sampling rule is \(\)-correct, when using the private GLR stopping rule.

**Theorem 4** (\(\)-correctness).: _Let \((0,1)\), \(>0\). Let \(s>1\) and \(\) be the Riemann \(\) function. Let \(c_{k}(n,m,)=2_{G}(((K-1)(s)k^{s}/)/2)+2(4 + n)+2(4+ m)\) be the threshold without privacy. Given any sampling rule, the following threshold_

\[c_{,k_{1},k_{2}}(n,m,)=2c_{k_{1}k_{2}}(n,m,/2)+}(^{s}(s)}{})^{2}+ {m^{2}}(^{s}(s)}{})^{2}\] (4)

_with the GLR stopping rule yields a \(\)-correct algorithm for sub-Gaussian distributions. The function \(_{G}\) is defined in (15). It satisfies \(_{G}(x) x+(x)\)._

_Remark_.: We observe that approximately \(c_{,k_{1},k_{2}}(n,m,) 2(1/)+(1/n+1/m)(1/ )^{2}/^{2}\).

Proof sketch.: Proving \(\)-correctness of a GLR stopping rule is done by leveraging concentration results. Specifically, we start by decomposing the failure probability \(_{}(_{}<+, a^{*})\) into a non-private and a private part using the basic property of \((X+Y a+b)(X a)+(Y b)\). The two-factor in front of \(c_{k_{1}k_{2}}\) originates from the looseness of this decomposition. To remove it, we would need a tighter stopping threshold that jointly controls both the non-private and the private parts. We conclude using concentration results from sub-Gaussian random variables for the non-private part, and Laplace random variables for the private part.

**Theorem 5** (Asymptotic upper bound on expected sample complexity).: _Let \((,)(0,1)^{2}\) and \(>0\). Combined with the private GLR stopping rule using threshold as in (4), AdaP-TT is \(\)-correct and satisfies that, for all \(^{K}\) such that \(_{a b}|_{a}-_{b}|>0\),_

\[_{ 0}_{}[_{}]}{(1/)}  4T_{,}^{*}()(1+^{2}}{2^{2}}})\;.\]

Proof sketch.: We adapt the asymptotic proof of the TTUCB algorithm, which is based on the unified analysis of Top Two algorithms from . Below, we present high-level ideas of the proof and specify the effect of different elements of AdaP-TT on the expected sample complexity.

_Consequences of Theorem 5_.: (1) The **non-private TTUCB algorithm** achieves a sample complexity of \(T_{,}^{*}()\) for sub-Gaussian random variables. The proof relies on showing that the empirical pulling counts are converging towards the \(\)-optimal allocation \(_{,}^{*}()\). (2) The **effect of doubling and forgetting** is a multiplicative four-factor, i.e. \(4T_{,}^{*}()\). The first two-factor is due to forgetting since we throw away half of the samples. The second two-factor is due to doubling since we have to wait for the end of an episode to evaluate the stopping condition. (3) The **Laplace noise** only affects the empirical estimate of the mean. Since the Laplace noise has no bias and a sub-exponential tail, the private means will still converge towards their true values. Therefore, the empirical counts of AdaP-TT will also converge to \(_{,}^{*}()\) asymptotically. (4) While the **Laplace noise has little effect on the sampling rule** itself, it **changes drastically the dependency in \((1/)\) of the threshold** used in the GLR stopping rule. The private threshold \(c_{,k_{1},k_{2}}\) has an extra factor \((^{2}(1/))\) compared to the non-private one \(c_{k}\). Using the convergence towards \(_{,}^{*}()\), the stopping condition is met as soon as \(,}^{*}()} 2(1/)+^{2}}{2^{2}},}^{*}()}{n} ^{2}(1/)\). Solving the inequality for \(n\) concludes the proof while adding a multiplicative four-factor.

_Discussion._ For \(=1/2\), it is well known that \(T^{}_{,1/2}() 2T^{}_{}() 8 _{a a^{}}_{a}^{-2}\). We consider Bernoulli instances (\(0<_{}_{}<1\)), where the gaps have the same order of magnitude, i.e. _Condition 1_: there exists a constant \(C 1\) such that \(_{}/_{} C\). For such instances, there exists a universal constant \(c\), such that

\[_{ 0}_{}[_{}]}{(1/)} \ c\ \{T^{}_{,1/2}(),C^{-1} _{a a},_{a}^{-1}\}\,.\]

Without privacy, i.e. \(+\), \(\) yields a multiplicative eight-factor. On top of the four-factor due to doubling and forgetting, another multiplicative two comes from \(2c_{k_{1}k_{2}}\) in Equation (4).

**Comparison to the lower bound.** For Bernoulli bandits verifying _Condition 1_, the upper bound of Theorem 5 matches the \(T^{}_{}()/\) lower bound of Corollary 1 up to constants in the high-privacy regime, i.e. when \( T^{}_{}()/T^{}_{}()\). In the low-privacy regime, the upper bound reduces to \(T^{}_{,1/2}()\). In Appendix E.7, we discuss in-depth why this difference is necessary for private BAI algorithms based on the GLR stopping rule, which poses an interesting open problem.

**Comparison to DP-SE.** DP-SE is a private version of the successive-elimination algorithm studied in  for the regret minimisation setting. The algorithm samples active arms uniformly during phases of geometrically increasing length. Based on the private confidence bounds, DP-SE eliminates provably sub-optimal arms at the end of each phase. Due to its phased-elimination structure, DP-SE can be easily converted into an \(\)-DP-FC-BAI algorithm, where we stop once there is only one active arm left. In particular, the proof of Theorem 4.3 of  shows that with high probability any sub-optimal arm \(a a^{}\) is sampled no more than \((_{a}^{2}+(_{a})^{-1})\). From this result, it is straightforward to extract a sample complexity upper bound for DP-SE, i.e. \((_{a a^{}}_{a}^{-2}+_{a a^{}}( _{a})^{-1})\). This shows that DP-SE too achieves (ignoring constants) the high-privacy lower bound \(T^{}_{}()/\) for Bernoulli instances. However, due to its uniform sampling within the phases, DP-SE is less adaptive than \(\). Inside a phase, DP-SE continues to sample arms that might already be known to be bad, while \(\) adapts its sampling rule based on the transportation costs that reflect the amount of evidence collected in favour of the hypothesis that the leader is the best arm. Finally, \(\) has the advantage of being anytime, i.e. its sampling strategy does not depend on the risk \(\).

## 5 Experimental analysis

We perform experiments to show that: (i) AdaP-TT has better empirical performance compared to DP-SE, and (ii) the transition between high and low-privacy regimes is reflected empirically.

**Experimental setup.** We compare the performances of AdaP-UCB and DP-SE for FC-BAI in different Bernoulli instances as in . The first instance has means \(_{1}=(0.95,0.9,0.9,0.9,0.5)\) and the second instance has means \(_{2}=(0.75,0.7,0.7,0.7)\). As a benchmark, we also compare to the non-private TTUCB. We set the risk \(=10^{-2}\). We implement all the algorithms in Python (version \(3.8\)) and on an 8-core 64-bits Intel i5@1.6 GHz CPU. We run each algorithm \(100\) times, and plot corresponding average and standard deviations of stopping times in Figure 1. We also test the algorithms on other Bernoulli instances and report the results in Appendix F.

**Result analysis.**_a. Efficiency in performance._ AdaP-TT requires less samples than DP-SE to provide a \(\)-correct answer. In the high privacy regime, i.e. small \(\), AdaP-TT outperforms DP-SE in all the instances tested. In the low privacy regimes, i.e. large \(\), both algorithms have similar performance that in the worst case is four times the samples required of TTUCB, as shown theoretically.

_b. Impact of privacy regimes._ As indicated by the theoretical sample complexity lower bounds and upper bounds, the experimental performance of AdaP-TT demonstrates two regimes: a high-privacy regime (for \(<0.2\)), where the stopping time of AdaP-TT depends on the privacy budget \(\), and a low privacy regime (for \(>0.2\)), where the performance of AdaP-TT does not depend on \(\).

## 6 Conclusion and future works

We study FC-BAI with \(\)-global DP. We derive a sample complexity lower bound that quantifies the additional samples needed by a \(\)-correct BAI strategy in order to ensure \(\)-global DP. The lower bound further suggests the existence of two privacy regimes. In the _low-privacy regime_, no additional samples are needed, and _privacy can be achieved for free_. For the _high-privacy regime_, the lower bound reduces to \((^{-1}T_{}^{*})\), and _more samples are required_. We also propose AdaP-TT, an \(\)-global DP variant of the Top Two algorithms, that runs in adaptive phases and adds Laplace noise. AdaP-TT achieves the high privacy regime lower bound up to multiplicative constants.

The upper bound matches the lower bound by a multiplicative constant in the high privacy regime, and is also loose in some instances in the low privacy regime, due to the mismatch between the KL divergence of Bernoulli distributions and that of Gaussian. It would be an interesting technical challenge to merge this gap. One possible direction to solve this issue is to use transportation costs tailored to Bernoulli for both the Top Two Sampling and the stopping. Another interesting direction would be to extend the proposed technique to other variants of pure DP, namely \((,)\)-DP and Renyi-DP , or other trust models, namely local DP  and shuffle DP .

Figure 1: Evolution of the stopping time \(\) (mean \(\) std. over 100 runs) of AdaP-TT, DP-SE, and TTUCB with respect to the privacy budget \(\) for \(=10^{-2}\) on Bernoulli instance \(_{1}\) (left) and \(_{2}\) (right). The shaded vertical line separates the two privacy regimes. AdaP-TT outperforms DP-SE.