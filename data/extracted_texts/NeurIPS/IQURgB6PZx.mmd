# RetroBridge: Modeling Retrosynthesis with Markov Bridges

Ilia Igashov

Ecole Polytechnique Federale de Lausanne

ilia.igashov@epfl.ch

&Arne Schneuing

Ecole Polytechnique Federale de Lausanne

arne.schneuing@epfl.ch

&Marwin Segler

Microsoft Research

marwinsegler@microsoft.com

&Michael Bronstein

University of Oxford

michael.bronstein@cs.ox.ac.uk

These authors contributed equally

Bruno Correia

Ecole Polytechnique Federale de Lausanne

bruno.correia@epfl.ch

###### Abstract

Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing multi-step reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior distribution. We then address the retrosynthesis planning problem with our novel framework and introduce RetroBridge, a template-free retrosynthesis modeling approach that achieves state-of-the-art results on standard evaluation benchmarks.

## 1 Introduction

Computational and machine learning methods for _de novo_ drug design show great promise as more cost-effective alternatives to experimental high-throughput screening approaches  to propose molecules with desirable properties. While _in silico_ results suggest high predicted target binding affinities and other favorable properties of the generated molecules, limited emphasis has so far been placed on their synthesizability . For laboratory testing, synthetic pathways need to be developed for the newly designed molecules, which is an extremely challenging and time-consuming task.

Retrosynthesis planning  tools address this challenge by proposing reaction steps or entire pathways that can be validated and optimized in thelab. Single-step retrosynthesis models predict precursor molecules for a given target molecule (Segler and Waller, 2017; Coley et al., 2017; Liu et al., 2017; Strieth-Kalthoff et al., 2020; Tu et al., 2023). Applying these methods recursively allows to decompose the initial molecule in progressively simpler intermediates and eventually reach available starting molecules (Segler et al., 2018).

While most works have used a discriminative formulation for retrosynthesis modeling (Strieth-Kalthoff et al., 2020; Tu et al., 2023; Jiang et al., 2022), we propose to view the task as a conditional distribution learning problem, as shown in Figure 1. This approach has several advantages, including the ability to model uncertainty and to generate new and diverse retrosynthetic pathways. Furthermore, and most importantly, the probabilistic formulation reflects the fact that the same product molecule can often be synthesized with different sets of reactants and reagents.

Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and other modern score-based and flow-based generative methods (Rezende and Mohamed, 2015; Song et al., 2020; Lipman et al., 2022; Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023) may seem like good candidates for retrosynthesis modeling. However, as we show in this work, such models do not fit naturally to the formulation of the problem, as they are designed to approximate a single intractable data distribution. To do so, one typically samples initial noise from a simple prior distribution and then maps it to a data point that follows a complex target distribution. In contrast, we aim to learn the dependency between two intractable distributions rather than one intractable distribution itself. While this can be achieved by conditioning the sampling process on the relevant context and keep sampling from the prior noise, we show here that such use of the original unconditional generative idea is suboptimal for approximating the dependency between two discrete distributions.

In this work, we propose RetroBridge, a template-free probabilistic method for single-step retrosynthesis modeling. As shown in Figure 1, we model the dependency between the spaces of products and reactants as a stochastic process that is constrained to start and to end at specific data points. To this end, we introduce the Markov Bridge Model, a generative model that learns the dependency between two intractable discrete distributions through the finite sample of coupled data points. Taking a product molecule as input, our method models the trajectories of Markov bridges starting at the given product and ending at data points following the distribution of reactants. To score reactant graphs sampled in this way, we leverage the probabilistic nature of RetroBridge and measure its uncertainty at each sample. We demonstrate that RetroBridge achieves competitive results on standard retrosynthesis modeling benchmarks. Besides, we compare RetroBridge with the state-of-the-art graph diffusion model DiGress (Vignac et al., 2022), and demonstrate quantitatively and qualitatively that the proposed Markov Bridge Model is better suited to tasks where two intractable discrete distributions need to be mapped.

To summarise, the main contributions of this work are the following:

Figure 1: Markov bridges between the distribution of products and distribution of reactants.

* We introduce the Markov Bridge Model to approximate the probabilistic dependency between two intractable discrete distributions accessible via a finite sample of coupled data points.
* We demonstrate the superiority of the proposed formulation over diffusion models in the context of learning the dependency between two intractable discrete distributions.
* We propose RetroBridge, the first Markov Bridge Model for retrosynthesis modeling. RetroBridge is a template-free single-step retrosynthesis prediction method that achieves state-of-the-art results on standard benchmarks.

## 2 Related Work

Diffusion ModelsDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) form a class of powerful and effective score-based generative methods that have recently achieved promising results in many different domains including protein design (Watson et al., 2023), small molecule generation (Hoogeboom et al., 2022; Igashov et al., 2022; Schneuing et al., 2022), molecular docking (Corso et al., 2022), and sampling of transition state molecular structures (Duan et al., 2023; Kim et al., 2023). While most models are designed for the continuous data domain, a few methods were proposed to operate on discrete data (Hoogeboom et al., 2021; Johnson et al., 2021; Austin et al., 2021; Yang et al., 2023) and, in particular, on discrete graphs (Vignac et al., 2022). To the best of our knowledge, however, no diffusion models have been applied to modeling chemical reactions and recovering retrosynthetic pathways.

Schrodinger Bridge ProblemGiven two distributions and a reference stochastic process between them, solving the Schrodinger bridge (SB) problem (Schrodinger, 1932; Leonard, 2013) amounts to finding a process closest to the reference in terms of Kullback-Leibler divergence on path spaces. While most recent methods employ the SB formalism in the context of unconditional generative modeling (Vargas et al., 2021; Wang et al., 2021; De Bortoli et al., 2021; Chen et al., 2021; Bunne et al., 2023; Liu et al., 2022), a few works aimed to approximate the reference stochastic process through training on coupled samples from two continuous distributions (Holdijk et al., 2022; Somnath et al., 2023). To the best of our knowledge, there are no methods operating on categorical distributions, which is the subject of the present work.

Retrosynthesis ModelingRecent retrosynthesis prediction methods can be divided into two main groups: template-based and template-free methods (Jiang et al., 2022). While template-based methods depend on predefined sets of specific reaction templates or leaving groups, template-free methods are less restricted and therefore are able to explore new reaction pathways. Two common data representations used for retrosynthesis prediction are symbolic representations SMILES (Weininger, 1988) and molecular graphs. A variety of language models have been recently proposed (Liu et al., 2017; Zheng et al., 2019; Tetko et al., 2020) to operate on SMILES. Due to the nature of the sequence-to-sequence translation problem, all these methods are template-free. Among the existing graph-based methods (Segler and Waller, 2017), the most recent template-based ones are GLN (Dai et al., 2019), GraphRetro (Somnath et al., 2021) and LocalRetro (Chen and Jung, 2021), and template-free approaches are G2G (Shi et al., 2020) and MEGAN (Sacha et al., 2021). In this work, we propose a novel template-free graph-based method.

## 3 RetroBridge

We frame the retrosynthesis prediction task as a generative problem of modeling a stochastic process between two discrete-valued distributions of products \(p_{}\) and reactants \(p_{}\). These distributions are intractable and are represented by a finite collection of \(D\) coupled samples \(\{(_{i},_{i})\}_{i=1}^{D}\), where \(_{i} p_{}(_{i})\) is a product molecule and \(_{i} p_{}(_{i})\) is a corresponding set of reactant molecules. While products and reactants follow distributions \(p_{}\) and \(p_{}\) respectively, there is a dependency between these variables that can be expressed in the form of the joint distribution \(p_{,}\) such that \( p_{,}(,)d=p_{}()\) and \( p_{,}(,)d=p_{}()\). The joint distribution \(p_{,}\) is also intractable and accessible only through the discrete sample of coupled data points \(\{(_{i},_{i})\}_{i=1}^{D}\).

First, we introduce the Markov Bridge Model, a general framework for learning the dependency between two intractable discrete-valued distributions. Next, we discuss a special case where random variables are molecular graphs. Upon this formulation, we introduce RetroBride, a Markov Bridge Model for single-step retrosynthesis modeling. Finally, we explain a simple but rather effective way of scoring RetroBridge samples based on the statistical uncertainty of the model.

### Markov Bridge Model

We model the dependency between two discrete spaces \(\) and \(\) by a Markov bridge (Fitzsimmons et al., 1992; Cetin and Danilova, 2016), which is a Markov process pinned to specific data points in the beginning and in the end. For a pair of samples \((,) p_{,}(,)\) and a sequence of time steps \(t=0,1,,T\), we define the corresponding Markov bridge as a sequence of random variables \((_{t})_{t=0}^{T}\), that starts at \(\), i.e., \(_{0}=\), and satisfies the Markov property,

\[p(_{t}|_{0},_{1},,_{t-1},)=p(_{t}| {z}_{t-1},).\] (1)

To pin the process at the data point \(\), we introduce an additional requirement,

\[p(_{T}=|_{T-1},)=1.\] (2)

Assuming that both distributions \(p_{}\) and \(p_{}\) are categorical with a finite sample space \(\{1,,K\}\), we can represent data points as \(K\)-dimensional one-hot vectors: \(,,_{t}^{K}\). To model a Markov bridge defined by equations (1-2), similar to Austin et al. (2021), we introduce a sequence of transition matrices \(_{0},_{1},,_{T-1}^{K K}\), defined as

\[_{t}_{t}()=_{t}_{K}+(1-_{t})_{K}^{},\] (3)

where \(_{K}\) is a \(K K\) identity matrix, \(_{K}\) is a \(K\)-dimensional all-one vector, and \(_{t}\) is a schedule parameter transitioning from \(_{0}=1\) to \(_{T-1}=0\). Transition probabilities (1) can be written as follows,

\[p(_{t+1}|_{t},)=(_{t+1};_{t}_{t}),\] (4)

where \((\ ;)\) is a categorical distribution with probabilities given by \(\). We note that setting \(_{T-1}=0\) ensures the requirement (2).

Using the finite set of coupled samples \(\{(_{i},_{i})\}_{i=1}^{D} p_{,}\), our goal is to learn a Markov bridge (1-2) to be able to sample \(\) when only \(\) is available. To do this, we replace \(\) with an approximation \(}\) computed with a neural network \(_{}\):

\[}=_{}(_{t},t),\] (5)

and define an approximated transition kernel,

\[q_{}(_{t+1}|_{t})=(_{t+1};_{t}( })_{t}).\] (6)

Figure 2: The process of changing atom types along the trajectory of the Markov bridge. The trajectory starts at time step \(t=0\) with the product molecule and several disconnected “dummy” atoms that will be included in the final reactant molecule. The probability of sampling the target atom type increases as \(t\) grows. Five circles filled with different colors represent these probabilities. To make the illustration less bulky, we omitted a part of the product molecule and one of two reactant molecules.

We train \(_{}\) by maximizing a lower bound of log-likelihood \( q_{}(|)\). As shown in Appendix A.1, it has the following closed-form expression,

\[ q_{}(|)-T_{t(0,...,T-1 )}_{_{t} p(_{t}|,)}D_{}(p( _{t+1}|_{t},)\|q_{}(_{t+1}|_{t})).\] (7)

For any \(,\), and \(t=1,,T\), sampling of \(_{t}\) can be effectively performed using a cumulative product matrix \(}_{t}=_{i}_{t-1}..._{0}\). As shown in Appendix A.2, the cumulative matrix \(}_{t}\) can be written in closed form,

\[}_{t}=_{t}_{K}+(1-_{t} )_{K}^{},\] (8)

where \(_{t}=_{s=0}^{t}_{s}\). Therefore, \(p(_{t+1}|_{0},_{T})\) can be written as follows,

\[p(_{t+1}|_{0},_{T})=(_{t+1};}_{t}_{0}).\] (9)

To sample a data point \(_{T}\) starting from a given \(_{0} p_{}()\), one iteratively predicts \(}=_{}(_{t},t)\) and then derives \(_{t+1} q_{}(_{t+1}|_{t})=(_{t+ 1};_{t}(})_{t})\) for \(t=0,,T-1\). Training and sampling procedures of the Markov Bridge Model are provided in Algorithms 1 and 2 respectively.

``` Input: coupled sample \((,) p_{,}\), neural network \(_{}\) \(t(0,,T-1),~{}_{t}(_{t}; }_{t-1})\)\(\) Sample time step and intermediate state \(}_{}(_{t},t)\)\(\) Output of \(_{}\) is a vector of probabilities \(p(_{t+1}|_{t},)(_{t+1};_{ t}()_{t})\)\(\) Reference transition distribution \(q_{}(_{t+1}|_{t})(_{t+1};_{t}(}) _{t})\)\(\) Approximated transition distribution \(_{t+1} q_{}(_{t+1}|_{t})\) Return \(_{T}\) ```

**Algorithm 1** Training of the Markov Bridge Model

### RetroBridge: Markov Bridge Model for Retrosynthesis Planning

In our setup, each data point is a molecular graph with nodes representing atoms and edges corresponding to covalent bonds. We represent the molecular graph with a matrix of node features \(^{N K_{i}}\) which are, for instance, one-hot encoded atom types, and a tensor of edge features \(^{N N K_{e}}\), which can be one-hot encoded bond types.

In the scope of our probabilistic framework, we consider such a molecular graph representation as a collection of independent categorical random variables. More formally, we denote product and reactants data points \(\) and \(\) as tuples of the corresponding node and edge feature tensors: \(=[_{x},_{x}]\) and \(=[_{y},_{y}]\). For such complex data points, we modify the definitions of transition matrices and probabilities accordingly:

\[[_{t}^{H}]_{j}=_{t}_{K_{t}}+(1-_{t}) _{K_{t}}[_{y}]_{j},\ p(_{t+1}|_{t},_{y})=(_{t+1};_{t}_{t}^{H}),\] (10) \[[_{t}^{E}]_{j,k}=_{t}_{K_{t}}+(1-_{t}) _{K_{e}}[_{y}]_{j,k},\ p(_{t+1}|_{t},_{y})= (_{t+1};_{t}_{t}^{E}),\] (11)

where \([]_{j}^{1 K_{i}}\) is the \(j\)-th row of the feature matrix \(\) (i.e. transposed feature vector of the \(j\)-th node), and \([]_{j,k}^{1 K_{e}}\) is the transposed feature vector of the edge between \(j\)-th and \(k\)-th nodes.

Because some atoms present in the reactant molecules can be absent in the corresponding product molecule, we add "dummy" nodes to the initial graph of the product. As shown in Figure 2, some "dummy" nodes are transformed into atoms of reactant molecules. In our experiments, we always add 10 "dummy" nodes to the initial product graphs.

### Confidence and Scoring

It is important to have a reliable scoring method that selects the most relevant sets of reactants out of all generated samples. In order to rank RetroBridge samples, we benefit from the probabilistic nature of the model and utilize its confidence in the generated samples as a scoring function. We estimate the confidence of the model by computing the likelihood \(q_{}(|)\) of a set of reactants \(\) for an input product molecule \(\). For a set of \(M\) samples \(\{_{T}^{(i)}\}_{i=1}^{M}\) generated by RetroBridge for an input product \(\), we compute the likelihood-based confidence score for the set of reactants \(\) as follows,

\[q_{}(|)=_{^{} q_{}(| {x})}\{^{}=\}_{i=1}^{M} \{_{T}^{(i)}=\}.\] (12)

## 4 Results

### Experimental Setup

DatasetFor all the experiments we use the USPTO-50k dataset (Schneider et al., 2016) which includes 50k reactions found in the US patent literature. We use standard train/validation/test splits provided by Dai et al. (2019). Somnath et al. (2021) report that the dataset contains a shortcut in that the product atom with atom-mapping 1 is part of the edit in almost 75% of the cases. Even though our model does not depend on the order of graph nodes, we utilize the dataset version with canonical SMILES provided by Somnath et al. (2021). Besides, we randomly permute graph nodes once SMILES are read and converted to graphs.

BaselinesWe compare RetroBridge with template-based methods GLN (Dai et al., 2019), LocalRetro (Chen and Jung, 2021), and GraphRetro (Somnath et al., 2021), and template-free methods MEGAN (Sacha et al., 2021), G2G (Shi et al., 2020), Augmented Transformer Tetko et al. (2020) and SCROP (Zheng et al., 2019). We note that SCROP, G2G and Augmented Transformer were trained and evaluated using different data splits provided by Liu et al. (2017). We obtained GLN predictions using the publicly available code and model weights2 and used the latest LocalRetro predictions provided by its authors. Additionally, MEGAN was originally trained and evaluated on random data splits, so we retrained and evaluated it ourselves. Finally, as described in Appendix A.4, we compare RetroBridge with the state-of-the-art discrete graph diffusion model DiGress Vignac et al. (2022) and a template-free baseline based on a graph transformer architecture (Dwivedi and Bresson, 2020; Vignac et al., 2022).

EvaluationFor each input product, we sample 100 reactant sets and report top-\(k\) exact match accuracy (\(k=1,3,5,10\)) which is measured as the proportion of input products for which the method managed to produce the correct set of reactants in its top-\(k\) samples. Subsequently, for top-\(k\) samples produced for every input product, we run the forward reaction prediction model Molecular Transformer (Schwaller et al., 2019) and report round-trip accuracy and coverage (Schwaller et al., 2020). Round-trip accuracy is the percentage of correctly predicted reactants among all predictions, where reactants are considered correct either if they match the ground truth or if they lead back to the input product. Round-trip coverage, on the other hand, measures if there is at least one correct prediction in the top-\(k\) according to the definition above. These metrics reflect the fact that one product can be mapped to multiple different valid sets of reactants, as shown in Figure 1.

### Neural Network

We use a graph transformer network (Dwivedi and Bresson, 2020; Vignac et al., 2022) to approximate the final state of the Markov bridge process. We represent molecules as fully-connected graphs where node features are one-hot encoded atom types (sixteen atom types and additional "dummy" type) and edge features are covalent bond types (three bond types and additional "none" type). Similarly to Vignac et al. (2022) we compute several graph-related node and global features that include number of cycles and spectral graph features. Details on the network architecture, hyperparameters and training process are provided in Appendix A.3.

[MISSING_PAGE_FAIL:7]

(which are not correct) have scores 0.18 and 0.38 respectively (cf. 0.17 and 0.2 for the correct ones). More examples are provided in Figure 5.

## 5 Conclusion

In this work, we introduce the Markov Bridge Model, a new generative framework for tasks that involve learning dependencies between two intractable discrete-valued distributions. We furthermore apply the new methodology to the retrosynthesis prediction problem, which is an important challenge in medicinal chemistry and drug discovery. Our template-free method, RetroBridge, achieves state-of-the-art results on common evaluation benchmarks. Importantly, our experiments show that choosing a suitable probabilistic modeling framework positively affects the performance on this task compared to the straightforward adaptation of diffusion models.

While this work is focused on the retrosynthesis modeling task, we note that application of Markov Bridge Models is not limited to this problem. The proposed framework can be used in many other settings where two discrete distributions accessible via a finite sample of coupled data points need to be mapped. Such applications include but are not limited to image-to-image translation, inpainting, text translation and design of protein binders. We leave exploration of Markov Bridge Models in the scope of these and other possible challenges for future work.

#### Acknowledgments

We thank Max Welling, Philippe Schwaller, Rebecca Neeser, Clement Vignac and Anar Rzayev for helpful feedback and insightful discussions. Ilia Igashov has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 945363. Arne Schneuing is supported by Microsoft Research AI4Science.