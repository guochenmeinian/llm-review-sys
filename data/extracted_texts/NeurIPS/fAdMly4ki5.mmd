# Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning

Haoran He\({}^{1,2}\)1 Chenjia Bai\({}^{2}\)2 Kang Xu\({}^{2,3}\) Zhuoran Yang\({}^{4}\) Weinan Zhang\({}^{1}\)

&Dong Wang\({}^{2}\) Bin Zhao\({}^{2,5}\) Xuelong Li\({}^{2,5}\)2

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Shanghai Artificial Intelligence Laboratory

\({}^{3}\)Fudan University \({}^{4}\)Yale University \({}^{5}\)Northwestern Polytechnical University

###### Abstract

Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (MTDiff), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. MTDiff leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find MTDiff outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, MTDiff generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.

## 1 Introduction

The high-capacity generative models trained on large, diverse datasets have demonstrated remarkable success across vision and language tasks. An impressive and even preternatural ability of these models, e.g. large language models (LLMs), is that the learned model can generalize among different tasks by simply conditioning the model on instructions or prompts . The success of LLMs and vision models inspires us to utilize the recent generative model to learn from large-scale offline datasets that include multiple tasks for generalized decision-making in reinforcement learning (RL). Thus far, recent attempts in offline decision-making take advantage of the generative capability of diffusion models  to improve long-term planning  or enhance the expressiveness of policies . However, these works are limited to small-scale datasets and single-task settings where broad generalization and general-purpose policies are not expected. In multi-task offline RL which considers learning a single model to solve multi-task problems, the dataset often contains noisy, multimodal, and long-horizon trajectories collected by various policies across tasks and with various qualities, which makes it more challenging to learn policies with broad generalization and transferable capabilities. Gato  and other generalized agents  take transformer-based architecture via sequence modeling to solve multi-task problems, while they are highly dependent on the optimality of the datasets and are expensive to train due to the huge number of parameters.

To address the above challenges, we propose a novel diffusion model to further explore its generalizability in a multi-task setting. We formulate the learning process from multi-task data as a denoising problem, which benefits the modeling of multimodal data. Meanwhile, we develop a relatively lightweight architecture by using a GPT backbone  to model sequential trajectories, which has less computation burden and improved sequential modeling capability than previous U-Net-based  diffusion models . To disambiguate tasks during training and inference, instead of providing e.g. one-hot task identifiers, we leverage demonstrations as _prompt_ conditioning, which exploits the few-shot abilities of agents .

We name our method the Multi-Task Diffusion Model (**MTDiff**). As shown in Figure 1, we investigate two variants of MTDiff for planning and data synthesis to further exploit the utility of diffusion models, denoted as **MTDiff-p** and **MTDiff-s**, respectively. (a) For planning, MTDiff-p learns a prompt embedding to extract the task-relevant representation, and then concatenates the embedding with the trajectory's normalized return and historical states as the _conditions_ of the model. During training, MTDiff-p learns to predict the corresponding future action sequence given the conditions, and we call this process generative planning . During inference, given few-shot prompts and the desired return, MTDiff-p tries to denoise out the optimal action sequence starting from the current state. Surprisingly, MTDiff-p can adapt to unseen tasks given well-constructed prompts that contain task information. (b) By slightly changing the inputs and training strategy, we can unlock the abilities of our diffusion model for data synthesis. Our insight is that the diffusion model, which compresses informative multi-task knowledge well, is more effective and generalist than previous methods that only utilize single-task data for augmentation . Specifically, MTDiff-s learns to estimate the joint conditional distribution of the full transitions that contain states, actions, and rewards based on the task-oriented prompt. Different from MTDiff-p, MTDiff-s learns to synthesize data from the underlying dynamic environments for each task. Thus MTDiff-s only needs prompt conditioning to identify tasks. We empirically find that MTDiff-s synthesizes high-fidelity data for multiple tasks, including both seen and unseen ones, which can be further utilized for data augmentation to expand the offline dataset and enhance policy performance.

To summarize, MTDiff is a diffusion-based method that leverages the multimodal generative ability of diffusion models, the sequential modeling capability of GPT architecture, and the few-shot generalizability of prompt learning for multi-task RL. To the best of our knowledge, we are the first to achieve both effective planning and data synthesis for multi-task RL via diffusion models. Our contributions include: (i) we propose MTDiff, a novel GPT-based diffusion model that illustrates the supreme effectiveness in multi-task trajectory modeling for both planning and data synthesis; (ii) we incorporate prompt learning into the diffusion framework to learn to generalize across different tasks and even adapt to unseen tasks; (iii) our experiments on Meta-World and Maze2D benchmarks demonstrate that MTDiff is an effective planner to solve the multi-task problem, and also a powerful data synthesizer to augment offline datasets in the seen or unseen tasks.

Figure 1: Overall architecture of MTDiff. Different colors represent different tasks. \(S\), \(A\) and \(R\) denote the state sequence, action sequence, and reward sequence from multi-task data, respectively. \(S_{}\) and \(R_{}\) represent historical states and normalized return.

Preliminaries

### Reinforcement Learning

MDP and Multi-task MDP.A Markov Decision Process (MDP) is defined by a tuple \((,,,,,)\), where \(\) is the state space, \(\) is the action space, \(:\) is the transition function, \(:\) is the reward function for any transition, \((0,1]\) is a discount factor, and \(\) is the initial state distribution. At each timestep \(t\), the agent chooses an action \(a_{t}\) by following the policy \(:_{}\). Then the agent obtains the next state \(s_{t+1}\) and receives a scalar reward \(r_{t}\). In single-task RL, the goal is to learn a policy \(^{*}=*{arg\,max}_{}_{a_{t}}[\,_{t =0}^{}^{t}r_{t}]\) by maximizing the expected cumulative reward of the corresponding task.

In a multi-task setting, different tasks can have different reward functions, state spaces and transition functions. We consider all tasks to share the same action space with the same embodied agent. Given a specific task \( p()\), a task-specified MDP can be defined as \((^{},,^{}, ^{},^{T},)\). Instead of solving a single MDP, the goal of multi-task RL is to find an optimal policy that maximizes expected return over all the tasks: \(^{*}=*{arg\,max}_{}_{ p()}_{a_{t}^{}}\,_{t=0}^{} ^{t}r_{t}^{}\).

Multi-Task Offline Decision-Making.In offline decision-making, the policy is learned from a static dataset of transitions \(\{(s_{j},a_{j},s^{}_{j},r_{j})\}_{j=1}^{N}\) collected by an unknown behavior policy \(_{}\). In the multi-task offline RL setting, the dataset \(\) is partitioned into per-task subsets as \(=_{i=1}^{N}_{i}\), where \(_{i}\) consists of experiences from task \(_{i}\). The key issue of RL in the offline setting is the distribution shift problem caused by temporal-difference (TD) learning. In our work, we extend the idea of Decision Diffuser  by considering multi-task policy learning as a conditional generative process without fitting a value function. The insight is to take advantage of the powerful distribution modeling ability of diffusion models for multi-task data, avoiding facing the risk of distribution shift.

Offline RL learns policies from a static dataset, which makes the quality and diversity of the dataset significant . One can perform data perturbation  to up-sample the offline dataset. Alternatively, we synthesize new transitions \((s,a,s^{},r)\) by capturing the underlying MDP of a given task via diffusion models, which expands the original dataset and leads to significant policy improvement.

### Diffusion Models

We employ diffusion models to learn from multi-task data \(=_{i=1}^{N}_{i}\) in this paper. With \(\) the sampled trajectory from \(\), we denote \(_{k}()\) as the \(k\)-step denoised output of the diffusion model, and \(()\) is the condition which represents the task attributes and the trajectory's optimality (e.g., returns). A forward diffusion chain gradually adds noise to the data \(_{0}() q(())\) in \(K\) steps with a pre-defined variance schedule \(_{k}\), which can be expressed as

\[q(_{k}()|_{k-1}()):=(_{k}();\,}_{k-1}(),_{k}).\] (1)

In this paper, we adopt Variance Preserving (VP) beta schedule  and define \(_{k}=1\,-\,-_{min}()-0.5(_{}-_{ })},\) where \(_{}=10\) and \(_{}=0.1\) are constants. A trainable reverse diffusion chain, constructed as \(p_{}(_{k-1}()|_{k}(),()):=(_{k-1}()|_{}(_{k}(),(),k),_{k})\), can be optimized by a simplified surrogate loss :

\[_{}:=_{k(1,K),_{0 }() q,(,)} -_{}(_{k}(),(),k)^{2},\] (2)

where \(_{}\) parameterized by a deep neural network is trained to predict the noise \((,)\) added to the dataset sample \(_{0}()\) to produce \(_{k}()\). By setting \(_{k}:=1-_{k}\) and \(_{k}:=_{s=1}^{k}_{s}\), we obtain

\[_{k-1}()\!\!}}(_{k}( )\!-\!}{_{k}}}_{}(_ {k}(),(),k))\!+\!},\,\!\! (,),k=\{K,...,1\}.\]

Classifier-free guidance  aims to learn the conditional distribution \(q(()|())\) without separately training a classifier. In the training stage, this method needs to learn both a conditional \(_{}(_{k}(),(),k)\) and an unconditional \(_{}(_{k}(),,k)\) model, where \(()\) is dropped out. Then the perturbed noise \(_{}(_{k}(),,k)+(_{}(_{k}(),(),k)-_{}(_{k}(),,k))\) is used to generate samples latter, where \(\) can be recognized as the guidance scale.

## 3 Methodology

### Diffusion Formulation

To capture the multimodal distribution of the trajectories sampled from multiple MDPs, we formulate the multi-task trajectory modeling as a conditional generative problem via diffusion models:

\[_{}_{_{i}_{i}} p_{}( _{0}()\ (),\] (3)

where \(_{0}()\) is the generated desired sequence and \(()\) is the condition. \(_{0}()\) will then be used for generative planning or data synthesis through conditional reverse denoising process \(p_{}\) for specific tasks. Maximizing Eq. (3) can be approximated by maximizing a variational lower bound .

In terms of different inputs and outputs in generative planning and data synthesis, \(()\) can be represented in different formats. We consider two choices to formulate \(()\) in MTDiff-p and MTDiff-s, respectively. (i) For **MTDiff-p**, \(()\) represents the action sequence for planning. We model the action sequence defined as:

\[_{k}^{p}():=(a_{t},a_{t+1},...,a_{t+H-1})_{k},\] (4)

with the context condition as

\[^{p}():=^{}(),R(),\ \ \ \ \ ^{}():=(Z,s_{t-L+1},...,s_{t}),\] (5)

where \(t\), \(H\), \(R()\) and \(L\) denote the time visited in trajectory \(\), the length of the input sequence \(\), the normalized cumulative return under \(\) and the length of the observed state history, respectively. \(Z\) is the task-relevant information as _prompt_. We use \(^{}()\) as an ordinary condition that is injected into the model during both training and testing, while considering \(R()\) as the classifier-free guidance to obtain the optimal action sequence for a given task. (ii) For data synthesis in **MTDiff-s**, the inputs and outputs become the transition sequence that contains states, actions, and rewards, and then the outputs are utilized for data augmentation. We define the transition sequence as:

\[_{k}^{s}():=s_{t}&s_{t+1}&&s_{t+H-1}\\ a_{t}&a_{t+1}&&a_{t+H-1}\\ r_{t}&r_{t+1}&&r_{t+H-1},\] (6)

with the condition:

\[^{s}():=[Z],\] (7)

where \(^{s}()\) takes the same conditional approach as \(y^{}()\). Figure 2 illustrates the reverse denoising process of MTDiff-p learned on multi-task datasets collected in Meta-World . The result demonstrates that our diffusion model successfully distinguishes different tasks and finally generates the desired \(_{0}()\). We illustrate the data distribution of \(_{0}()\) in a 2D space with dimensional reduction via T-SNE , as well as the rendered states after executing the action sequence. The result shows that, with different task-specific prompts as conditions, the generated planning sequence for a specific task will be separate from sequences of other tasks, which verifies that MTDiff can learn the distribution of multimodal trajectories based on \(()\).

Figure 2: An example of the denoising process of MTDiff. We choose 4 tasks for visualization and \(_{K}()\) is sampled from the Gaussian noise for each task. Since different tasks require different manipulation skills, the corresponding action sequences are dispersed in the embedding space. Our model learns such properties and generates task-specific sequences based on task-relevant prompts.

### Prompt, Training and Sampling

In multi-task RL and LLM-driven decision-making, existing works use one-hot task identifiers [56; 74] or language descriptions [1; 6] as conditions in multi-task training. Nevertheless, we argue that the one-hot encoding for each task [22; 14] suffices for learning a repertoire of training tasks while cannot generalize to novel tasks since it does not leverage semantic similarity between tasks. In addition, the language descriptions [51; 1; 6; 41; 40] of tasks require large amounts of human labor to annotate and encounter challenges related to ambiguity . In MTDiff, we use expert demonstrations consisting of a few trajectory segments to construct more expressive prompts in multi-task settings. The incorporation of prompt learning improves the model's ability for generalization and extracting task-relevant information to facilitate both generative planning and data synthesis. We remark that a similar method has also been used in PromptDT . Nonetheless, how such a prompt contributes within a diffusion-based framework remains to be investigated.

Specifically, we formulate the task-specific label \(Z\) as trajectory prompts that contain states and actions:

\[Z:=s_{i}^{*}&s_{i+1}^{*}&&s_{i+J-1}^{*}\\ a_{i}^{*}&a_{i+1}^{*}&&a_{i+J-1}^{*},\] (8)

where each element with star-script is associated with a trajectory prompt, and \(J\) is the number of environment steps for identifying tasks. With the prompts as conditions, MTDiff can specify the task by implicitly capturing the transition model and the reward function stored in the prompts for better generalization to unseen tasks without additional parameter-tuning.

In terms of decision-making in MTDiff-p, we aim to devise the optimal behaviors that maximize return. Our approach is to utilize the diffusion model for action planning via classifier-free guidance . Formally, an optimal action sequence \(_{0}^{p}()\) is sampled by starting with Gaussian noise \(_{K}()\) and refining \(_{k}^{p}()\) into \(_{k-1}^{p}()\) at each intermediate timestep with the perturbed noise:

\[_{}_{k}^{p}(),^{}(),,k+_{}(_{k}^{p}(),^{} (),R(),k)-_{}_{k}^{p}(),^{ }(),,k,\] (9)

where \(^{}()\) is defined in Eq. (5). \(R()\) is the normalized return of \(\), and \(\) is a hyper-parameter that seeks to augment and extract the best portions of trajectories in the dataset with high return. During training, we follow DDPM  as well as classifier-free guidance  to train the reverse diffusion process \(p_{}\), parameterized through the noise model \(_{}\), with the following loss:

\[^{p}():=_{k(1,K),_{0}()  q,(,),(p)} -_{}_{k}^{p}(),^{} (),(1-)R()+,k^{2}.\] (10)

Note that with probability \(p\) sampled from a Bernoulli distribution, we ignore the conditioning return \(R()\). During inference, we adopt the _low-temperature sampling_ technique  to produce high-likelihood sequences. We sample \(_{k-1}^{p}()(_{}(_{k-1}^{p},^{ }(),R_{}(),k-1),_{k-1})\), where the variance is reduced by \([0,1)\) for generating action sequences with higher optimality.

For MTDiff-s, since the model aims to synthesize diverse trajectories for data augmentation, which does not need to take any guidance like \(R()\), we have the following loss:

\[^{s}():=_{k(1,K),_{0}()  q,(,)}- _{}(_{k}^{s}(),^{s}(),k)^{2}.\] (11)

We sample \(_{k-1}^{*}()(_{}(_{k-1}^{*},^{s }(),k-1),_{k-1})\). The evaluation process is given in Fig. 3.

### Architecture Design

Notably, the emergence of Transformer  and its applications on generative modeling [44; 5; 47; 6] provides a promising solution to capture interactions between modalities of different tasks. Naturally, instead of U-Net  which is commonly used in previous single-task diffusion RL works [21; 2; 33], we parameterize \(_{}\) with a novel transformer architecture. We adopt GPT2  architecture for implementation, which excels in sequential modeling and offers a favorable balance between performance and computational efficiency. Our key insight is to train the diffusion model in a unified manner to model multi-task data, treating different inputs as tokens in a unified architecture, which is expected to enhance the efficiency of diverse information exploitation.

As shown in Figure 3, first, different raw inputs \(x\) are embedded into embeddings \(h\) of the same size \(\) via separate MLPs \(f\), which can be expressed as:

\[h_{P}=f_{P}(x^{}),h_{Ti}=f_{Ti}(x^{}), \] \[h_{T}^{s}=f_{Tr}(x^{}),\] \[h_{A}^{}=f_{A}(x^{}),h_{H}^{p}=f_{H}(x ^{}),h_{R}^{p}=f_{R}(x^{}).\]

Then, the embeddings \(h_{P}\) and \(h_{Ti}\) are prepended as follows to formulate input tokens for MTDiff-p and MTDiff-s, respectively:

\[h_{}^{p}=(h_{Ti}[h_{P},h_{Ti},h_{R}^{p},h_{H} ^{p},h_{A}^{p}]+h_{R}^{p}+E^{}),h_{}^{s}=(h_{Ti}[h_{P},h_{Ti},h_{T}^{s}]+E^{}),\]

where \(E^{}\) is the positional embedding, and \(\) denotes layer normalization  for stabilizing training. In our implementation, we strengthen the condition of the stacked inputs through multiplication with the diffusion timestep \(h_{Ti}\) and addition with the return \(h_{R}^{p}\). GPT2 is a decoder-only transformer that incorporates a self-attention mechanism to capture dependencies between different positions in the input sequence. We employ the GPT2 architecture as a trainable backbone in MTDiff to handle sequential inputs. It outputs an updated representation as:

\[h_{}^{p}=(h_{}^{p}), h_{ }^{s}=(h_{}^{s}).\]

Finally, given the output representation, we use a prediction head consisting of fully connected layers to predict the corresponding noise at diffusion timestep \(k\). Notice that the predicted noise shares the same dimensional space as the original inputs, which differs from the representation size \(\). This noise is used in the reverse denoising process \(p_{}\) during inference. We summarize the details of the training process, architecture and hyperparameters used in MTDiff in Appendix A.

## 4 Related Work

**Diffusion Models in RL.** Diffusion models have emerged as a powerful family of deep generative models with a record-breaking performance in many applications across vision, language and combinatorial optimization [49; 46; 17; 31; 32]. Recent works in RL have demonstrated the capability of diffusion models to learn the multimodal distribution of offline policies [63; 43; 11] or human behaviors . Other works formulate the sequential decision-making problem as a conditional generative process  and learn to generate the trajectories satisfying conditioned constraints. However, these works are limited to the single-task settings, while we further study the trajectory modeling and generalization problems of diffusion models in multi-task settings.

**Multi-Task RL and Few-Shot RL.** Multi-task RL aims to learn a shared policy for a diverse set of tasks. The main challenge of multi-task RL is the conflicting gradients among different tasks, and

Figure 3: Model architecture of MTDiff, which treats different inputs as tokens in a unified architecture. The two key designs are (i) the trainable GPT2 Transformer which enhances sequential modeling, and (ii) the MLPs and prediction head which enable efficient training.

previous online RL works address this problem via gradient surgery , conflict-averse learning , and parameter composition [70; 56]. Instead, MTDiff addresses such a problem in an offline setting through a conditional generative process via a novel transformer architecture. Previous Decision-Transformer (DT)-based methods [47; 29; 72] which consider handling multi-task problems, mainly rely on expert trajectories and entail substantial training expenses. Scaled-QL  adopts separate networks for different tasks and is hard to generalize to new tasks. Instead of focusing on the performance of training tasks in multi-task RL, few-shot RL aims to improve the generalizability in novel tasks based on the learned multi-task knowledge. Nevertheless, these methods need additional context encoders [77; 79] or gradient descents in the finetuning stage [57; 58; 29]. In contrast, we use prompts for few-shot generalization without additional parameter-tuning.

**Data Augmentation for RL.** Data augmentation [13; 39] has been verified to be effective in RL. Previous methods incorporate various data augmentations (e.g. adding noise, random translation) on observations for visual-based RL [71; 28; 52; 27], which ensure the agents learn on multiple views of the same observation. Differently, we focus on data augmentation via synthesizing new experiences rather than perturbing the origin one. Recent works [76; 10] consider augmenting the observations of robotic control using a text-guided diffusion model whilst maintaining the same action, which differs from our approach that can synthesize novel action and reward labels. The recently proposed SynthER  is closely related to our method by generating transitions of trained tasks via a diffusion model. However, SynthER is studied in single-task settings, while we investigate whether a diffusion model can accommodate all knowledge of multi-task datasets and augment the data for novel tasks.

## 5 Experiments

In this section, we conduct extensive experiments to answer the following questions: (1) How does MTDiff-p compare to other offline and online baselines in the multi-task regime? (2) Does MTDiff-s synthesize high-fidelity data and bring policy improvement? (3) How is MTDiff-s compared with other augmentation methods for single-task RL? (4) Does the synthetic data of MTDiff-s match the original data distribution? (5) Can both MTDiff-p and MTDiff-s generalize to unseen tasks?

### Environments and Baselines

Meta-World TasksThe Meta-World benchmark  contains 50 qualitatively-distinct manipulation tasks. The tasks share similar dynamics and require a Sawyer robot to interact with various objects with different shapes, joints, and connectivity. In this setup, the state space and reward functions of different tasks are different since the robot is manipulating different objects with different objectives. At each timestep, the Sawyer robot receives a 4-dimensional fine-grained action, representing the 3D position movements of the end effector and the variation of gripper openness. The original Meta-World environment is configured with a fixed goal, which is more restrictive and less realistic in robotic learning. Following recent works [70; 56], we extend all the tasks to a random-goal setting and refer to it as MT50-rand. We use the average success rate of all tasks as the evaluation metric.

By training a SAC  agent for each task in isolation, we utilize the experience collected in the replay buffer as our offline dataset. Similar to , we consider two different dataset compositions: (i) **Near-optimal** dataset consisting of the experience (100M transitions) from random to expert (convergence) in SAC-Replay, and (ii) **Sub-optimal** dataset consisting of the initial 50% of the trajectories (50M transitions) from the replay buffer for each task, where the proportion of expert data decreases a lot. We summarize more details about the dataset in Appendix G.

Maze2D TasksMaze2D  is a navigation task that requires an agent to traverse from a randomly designated location to a fixed goal in the 2D map. The reward is 1 if succeed and 0 otherwise. Maze2D can evaluate the ability of RL algorithms to stitch together previously collected sub-trajectories, which helps the agent find the shortest path to evaluation goals. We use the agent's scores as the evaluation metric. The offline dataset is collected by selecting random goal locations and using a planner to generate sequences of waypoints by following a PD controller.

BaselinesWe compare our proposed MTDiff (MTDiff-p and MTDiff-s) with the following baselines. Each baseline has the same batch size and training steps as MTDiff. For **MTDiff-p**, we have following baselines: (i) **PromptDT.** PromptDT  built on Decision-Transformer (DT)  aims to learn from multi-task data and generalize the policy to unseen tasks. PromptDT generates actions based on the trajectory prompts and reward-to-go. We use the same GPT2-network as in MTDiff-p. The main difference between our method and PromptDT is that we employ diffusion models for generative planning. (ii) **MTDT.** We extend the DT architecture  to learn from multi-task data. Specifically, MTDT concatenates an embedding \(z\) and a state \(s\) as the input tokens, where \(z\) is the encoding of task ID. In evaluation, the reward-to-go and task ID are fed into the Transformer to provide task-specific information. MTDT also uses the same GPT2-network as in MTDiff-p. Compared to MTDT, our model incorporates prompt and diffusion framework to learn from the multi-task data. (iii) **MTCQL.** Following scaled-QL , we extend CQL  with multi-head critic networks and a task-ID conditioned actor for multi-task policy learning. (iv) **MTIQL.** We extend IQL  for multi-task learning using a similar revision of MTCQL. The TD-based baselines (i.e., MTCQL and MTIQL) are used to demonstrate the effectiveness of conditional generative modeling for multi-task planning. (v) **MTBC.** We extend Behavior cloning (BC) to multi-task offline policy learning via network scaling and a task-ID conditioned actor that is similar to MTCQL and MTIQL.

As for **MTDiff-s**, we compare it with two baselines that perform direct data augmentation in offline RL. (i) **RAD.** We adopt the random amplitude scaling  that multiplies a random variable to states, i.e., \(s^{}=s z\), where \(z[,]\). This augmentation technique has been verified to be effective for state-based RL. (ii) **S4RL.** We adopt the adversarial state training  by taking gradients with respect to the value function to obtain a new state, i.e. \(s^{} s+_{s}_{Q}((s))\), where \(_{Q}\) is the policy evaluation update performed via a \(Q\) function, and \(\) is the size of gradient steps. We summarize the details of all the baselines in Appendix B.

### Result Comparison for Planning

**How does MTDiff-p compare to baselines in the multi-task regime?** For a fair comparison, we add MTDiff-p-onehot as a variant of MTDiff-p by replacing the prompt with a one-hot task-ID, which is used in the baselines except for PromptDT. The first action generated by MTDiff-p is used to interact with the environment. According to Tab. 1, we have the following key observations. (i) Our method achieves better performance than baselines in both near-optimal and sub-optimal settings. For near-optimal datasets, MTDiff-p and MTDiff-p-onehot achieve about 60% success rate, significantly outperforming other methods and performing comparably with MTBC. However, the performance of MTBC decreases a lot in sub-optimal datasets. BC is hard to handle the conflict behaviors in experiences sampled by a mixture of policies with different returns, which has also been verified in previous offline imitation methods [35; 68]. In contrast, both MTDiff-p and MTDiff-onehot perform the best in sub-optimal datasets. (ii) We compare MTDiff-p with two SOTA multi-task online RL methods, CARE  and PaCo , which are trained for 100M steps in MT50-rand. MTDiff-p outperforms both of them given the near-optimal dataset, demonstrating the potential of solving the multi-task RL problems in an offline setting. (iii) MTDT is limited in distinguishing different tasks with task ID while PromptDT performs better, which demonstrates the effect of prompts in multi-task settings. (iv) As for TD-based baselines, we find MTCQL almost fails while MTIQL performs well. We hypothesize that since MTCQL penalizes the OOD actions for each task, it will hinder other tasks' learning since different tasks can choose remarkably different actions when facing similar states. In contrast, IQL learns a value function without querying the values of OOD actions.

We remark that MTDiff-p based on GPT network outperforms that with U-Net in similar model size, and detailed results are given in Appendix C. Overall, MTDiff-p is an effective planner in multi-task settings including sub-optimal datasets where we must stitch useful segments of suboptimal trajectories, and near-optimal datasets where we mimic the best behaviors. Meanwhile, we argue that although MTDiff-p-onehot performs well, it cannot generalize to unseen tasks without prompts.

  
**Methods** & **Near-optimal** & **Sub-optimal** \\ 
**CARE** (Online) & \(50.8 1.0\) & \(-\) \\
**PaCo** (Online) & \(57.3 1.3\) & \(-\) \\ 
**MTDT** & \(20.99 2.66\) & \(20.63 2.21\) \\
**PromptDT** & \(45.68 1.84\) & \(39.76 2.79\) \\
**MTBC** & \(60.39 0.86\) & \(34.53 1.25\) \\
**MTCQL** & \(-\) & \(-\) \\
**MTIQL** & \(56.21 1.39\) & \(43.28 0.90\) \\ 
**MTDiff-p (ours)** & \(59.53 1.12\) & \(\) \\
**MTDiff-p-onehot (ours)** & \(\) & \(\) \\   

Table 1: Average success rate across 3 seeds on MetaWorld-V2 MT50 with random goals (MT50-rand). Each task is evaluated for 50 episodes.

Does MTDiff-p generalize to unseen tasks?We further carry out experiments on Maze2D to evaluate the generalizability of MTDiff-p. We select PromptDT as our baseline, as it has demonstrated both competitive performances on training tasks and adaptation ability for unseen tasks . We use 8 different maps for training and one new map for adaptation evaluation. The setup details are given in Appendix D. We evaluate these two methods on both seen maps and an unseen map. The average scores obtained in 8 training maps are referred to Figure 5. To further illustrate the advantages of our method compared to PromptDT, we select one difficult training map and the unseen map for visualization, as shown in Figure 5. According to the visualized path, we find that (1) for seen maps in training, MTDiff-p generates a shorter and smoother path, and (2) for unseen maps, PromptDT fails to obtain a reasonable path while MTDiff-p succeed, which verifies that MTDiff-p can perform few-shot adaptation based on trajectory prompts and the designed architecture.

### Results for Augmentation via Data Synthesis

Does MTDiff-s synthesize high-fidelity data and bring policy improvement?We train MTDiff-s on near-optimal datasets from 45 tasks to evaluate its generalizability. We select 3 training tasks and 3 unseen tasks, and measure the policy improvement of offline RL training (i.e., TD3-BC ) with data augmentation. For each evaluated task, MTDiff-s synthesizes 2M transitions to expand the original 1M dataset. From the results summarized in Table 2, MTDiff-s can boost the offline performance for all tasks and significantly increases performance by about 180%, 131%, and 161% for _box-close_, _hand-insert_, and _coffee-push_, respectively.

How does MTDiff-s perform and benefit from multi-task training?From Table 2, we find MTDiff-s achieves superior policy improvement in seen tasks compared with previous SOTA augmentation methods (i.e., S4RL and RAD) that are developed in single-task RL. We hypothesize that, by absorbing vast knowledge of multi-task data in training, MTDiff-s can perform implicit data sharing  by integrating other tasks' knowledge into data synthesis of the current task. To verify this hypothesis, we select three tasks (i.e., _coffee-push_, _disassemble_ and _dial-turn_) to re-train MTDiff-s on the corresponding single-task dataset. We denote this variant as MTDiff-s-single. We implement SynthER  to further confirm the benefits of multi-task training. SynthER also utilizes diffusion models to enhance offline datasets for policy improvement, but it doesn't take into account the aspect of learning from multi-task data. We observe that MTDiff-s outperforms both two methods, as shown in Figure 6. What's more, we re-train MTDiff-s on 10, 20, and 30 tasks respectively, in order to obtain the relationship between the performance gains and the number of training tasks. Our findings, as outlined in Fig. 7, provide compelling evidence in support of our hypothesis: MTDiff-s exhibits progressively superior data synthesis performance with increasing task diversity.

Does MTDiff-s generalize to unseen tasks?We answer this question by conducting offline RL training on the augmented datasets of 3 unseen tasks. According to Table 2, MTDiff-s is well-generalized and obtains significant improvement compared to the success rate of original datasets. MTDiff-s boost the policy performance by 131%, 180% and 32% for _hand-insert_, _box-close_ and _bin-picking_, respectively. We remark that S4RL performs the best on the two unseen tasks, i.e., _box-close_ and _bin-picking_, since it utilizes the entire datasets to train \(Q\)-functions and obtains the augmented states. Nevertheless, we use much less information (i.e., a single trajectory as prompts) for augmentation.

Does the synthetic data of MTDiff-s match the original data distribution?We select 4 tasks and use T-SNE  to visualize the distribution of original data and synthetic data. We find the synthetic data overlap and expand the original data distribution while also keeping consistency with the underlying MDP. The visualization results and further analyses are given in Appendix E.

## 6 Conclusion

We propose MTDiff, a diffusion-based effective planner and data synthesizer for multi-task RL. With the trajectory prompt and unified GPT-based architecture, MTDiff can model multi-task data and generalize to unseen tasks. We show that in the MT50-rand benchmark containing fine-grained manipulation tasks, MTDiff-p generates desirable behavior for each task via few-shot prompts. By compressing multi-task knowledge in a single model, we demonstrate that MTDiff-s greatly boosts policy performance by augmenting original offline datasets. In future research, we aim to develop a practical multi-task algorithm for real robots to trade off the sample speed and generative quality. We further discuss the limitations and broader impacts of MTDiff in Appendix F.