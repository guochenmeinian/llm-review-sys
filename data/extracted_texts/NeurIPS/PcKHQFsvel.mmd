# On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making

Prateek Jaiswal

Department of Statistics

Texas A&M University

College Station, TX 77843

jaiswalp@stat.tamu.edu

&Harsha Honnappa

School of Industrial Engineering

Purdue University

West Lafayette, IN, 47906

honnappa@purdue.edu

&Vinayak A. Rao

Department of Statistics

Purdue University

West Lafayette, IN, 47907

varao@purdue.edu

###### Abstract

We study data-driven decision-making problems in the Bayesian framework, where the expectation in the Bayes risk is replaced by a risk-sensitive entropic risk measure with respect to the posterior distribution. We focus on problems where calculating the posterior distribution is intractable, a typical situation in modern applications with large datasets and complex data generating models. We leverage a dual representation of the entropic risk measure to introduce a novel risk-sensitive variational Bayesian (RSVB) framework for jointly computing a risk-sensitive posterior approximation and the corresponding decision rule. Our general framework includes _loss-calibrated_ VB  as a special case. We also study the impact of these computational approximations on the predictive performance of the inferred decision rules. We compute the convergence rates of the RSVB approximate posterior and the corresponding optimal value. We illustrate our theoretical findings in parametric and nonparametric settings with the help of three examples.

## 1 Introduction

This paper focuses on _risk-sensitive_ Bayesian decision-making, considering objective functions of the form

\[_{a}_{_{n}}^{}(R(a,)):=^{-1} _{_{n}}[( R(a,))].\] (SO)

Here \(\) is the decision/action space, \(\) is a random model parameter lying in an arbitrary measurable space \((,)\), and \(R(a,):\) is a problem-specific model risk function. For any \(B\), \(_{n}(B):=(B|_{n})\) is the Bayesian posterior distribution over the parameters given observations \(_{n}\) from the true model \(P_{_{0}}^{n}( P_{0}^{n})\) with parameter \(_{0}\). The scalar \(\) is user-specified and characterizes the sensitivity of the decision-maker (DM) to the distribution \(_{n}\). Recall that the posterior distribution is obtained by updating a prior probability distribution \(()\), capturing subjective beliefs of the decision maker over \(\), according to the Bayes rule

\[d(|_{n}) d()dP_{}^{n}(_{n}),\] (1)

where \(dP_{}^{n}(_{n})\) is the likelihood of observing \(_{n}\). We denote the corresponding densities (if they exist) for the model, prior, and posterior as \(p_{}^{n}()\), \(()\), and \((|_{n})\) respectively.

The functional \(^{}\) in (SO) is also known as the _entropic risk measure_, and models a range of risk-averse or risk-seeking behaviors in a succinct manner through the parameter \(\). Consider only strictly positive \(\), and observe that \(_{ 0}_{_{n}}[(  R(a,))]=_{_{n}}(R(a,))\); that is, there is no sensitivity to potential risks due to large tail effects and the decision-maker is risk neutral.

On the other hand, \(_{+}_{_{n}}^{}(R(a,))= *{ess\,sup}_{_{n}}(R(a,)),\) the essential supremum of the model risk \(R(a,)\). In other words, a decision maker is completely risk averse and anticipates the worst possible realization (\(_{n}\)-almost surely). Observe that (SO) strictly generalizes the standard Bayesian decision-theoretic formulation of a decision-making problem, where the goal is to solve \(_{a}_{_{n}}[R(a,)].\)

The risk-sensitive formulation (SO) is very general and can be used to model a wide variety of decision-making problems in machine learning , operations research/management science , simulation optimization , and finance . However, solving (SO) to compute an optimal decision over \(\) is challenging. The difficulty mainly stems from the fact that, with the exception of conjugate models, the posterior distribution in (1) is an intractable quantity. Canonically, posterior intractability is addressed using either a sampling- or optimization-based approach. Sampling-based approaches, such as Markov chain Monte Carlo (MCMC), offer a tractable way to compute the integrals and theoretical guarantees of exact inference in the large computational budget limit.

Optimization-based methods such as variational Bayes (VB) or variational inference (VI) have emerged as a popular alternative . The VB approximation of the true posterior is a tractable distribution chosen from a'simpler' family of distributions known as a variational family by minimizing the discrepancy between the true posterior and members of that family. Kullback-Liebler (KL) divergence is the most often used measure of the approximation discrepancy, although other divergences (such as the \(\)-Renyi divergence ) have been used. The minimizing member, termed the VB approximate posterior, can be used as a proxy for the true posterior. Empirical studies have shown that VB methods are computationally faster and far more scalable to higher-dimensional problems and large datasets. Theoretical guarantees, such as large sample statistical inference, have been a topic of recent interest in the theoretical statistics community, with asymptotic properties such as convergence rate and asymptotic normality of the VB approximate posterior recently established in  and  respectively.

Our ultimate goal is not to merely approximate the posterior distribution but also to make decisions when that posterior is intractable. A _naive_ approach would be to plug in the VB approximation in place of the true posterior in (SO) and compute the optimal decision. However, it has been noted in  that such a naive loss-unaware approach can be suboptimal. In particular,  demonstrated, through an example, that a naive posterior approximation only captures the most dominant mode of the true posterior, which may not be relevant from a decision-making perspective. Consequently, they proposed a loss-calibrated variational Bayesian (LCVB) algorithm for solving Bayesian decision-making problems where the underlying risk function is discrete.  extended their approach to continuous risk functions. Despite these algorithmic advances in developing decision-centric VB methods, their statistical properties, such as asymptotic consistency and convergence rates of the loss-aware posterior approximation and the associated decision rule are not well understood. With an aim to address these gaps, we summarize our contribution in this paper below:

1. In Section 2, we introduce a minimax optimization framework titled 'risk-sensitive variational Bayes' (RSVB), extracted from the dual representation of (SO) using the so-called Donsker-Varadhan variational free-energy principle . The decision-maker computes a risk-sensitive approximation to the true posterior (termed as RSVB posterior) and the decision rule simultaneously by solving a minimax optimization problem. Moreover, we recover the LCVB approach  as a special case of RSVB (with \(=1\)).
2. In Section 3, we identify verifiable regularity conditions on the prior, likelihood model and the risk function under which the RSVB posterior enjoys the same rate of convergence as the true posterior to a Dirac delta distribution concentrated at the true model parameter \(_{0},\) as the sample size increases. Using this result, we also prove the rate of convergence of the RSVB decision rule when the decision space \(\) is compact. Our theoretical results also imply the asymptotic properties of the LCVB posterior and the associated decision rule.
3. In Section 4, we demonstrate our theoretical results with the help of three widely studied decision-making problems, including Gaussian process classification and a newsvendor problem. For each example, we show that the rate of convergence of the respective RSVB approximate posterior matches that of the corresponding true posterior.
4. In Section 4, we also present some simulation results with the single product (1-d) newsvendor problem, which is summarized here in Figure 1. The figures demonstrate the effect of changing \(\) on the optimality gap in values (see Definition 2.1) and the variance of the RSVB posterior for a given \(n\). In particular, we observe that for smaller \(n\), increasing \(\) (after a certain value) results in a significantly more risk-averse decision; however, the effect of increasing \(\) on risk-averse decision-making reduces as \(n\) increases. This observation demonstrates the fact that when there is enough certainty in the parameter estimation, the need to be risk-sensitive to parametric uncertainty diminishes. The Figure 1 also compares the optimality gap in values evaluated at RSVB decision rules and variance of the RSVB posterior for various values of \(\) against the naive VB approach and against the true posterior when the conjugate prior is used.

## 2 Risk-Sensitive Variational Bayes

Our approach exploits the dual representation of the log-exponential risk measure in (SO), which is convex (or extended coherent) risk measure [23; 10]. From the Donsker-Varadhan variational free energy principle  observe that,

\[_{_{n}}^{}(a)=_{Q}\{_{Q}[R(a, )]-^{-1}(Q||_{n})\} 56.905512pt>0,\] (DV)

where \(\) is the set of all distribution functions that are absolutely continuous with respect to the posterior distribution \(_{n}\) and 'KL' represents the Kullback-Leibler divergence. Formally, for any two distributions \(P\) and \(Q\) defined on measurable space \((,)\), the KL divergence is defined as \((Q||P)=_{}dQ()\), when measure \(Q\) is absolutely continuous with respect to \(P\) and \(\) otherwise. Notice that this dual formulation exposes the reason we choose to use the log-exponential risk - the 'free-energy' objective on the RHS provides a combined assessment of the risk associated with model estimation (the KL divergence \((Q||_{n})\)) and the decision risk under the estimated posterior \(Q\) (\(_{Q}[R(a,)]\)).

As stated above, the reformulation presented in (DV) offers no computational gains. However, restricting ourselves to an appropriately chosen subset \(\), that consists of distributions where the integral \(_{q}[R(a,)]\) and \((Q||_{n})\) can be tractably computed and optimized, we immediately obtain a _risk-sensitive variational Bayesian_ (RSVB) formulation from (DV):

\[^{-1}_{_{n}}[e^{ R(a,)}] _{Q}\{_{Q}[R(a,)]-^{-1} (Q||_{n})\}=:(a;Q(),_{n},).\] (RSVB)

RSVB is our framework for data-driven risk-sensitive decision-making. The family of distributions \(\) is popularly known as the _variational family_. Our analysis in Section 3.1 reveals general guidelines on how to choose \(\) that ensures a small optimality gap (defined below) with high probability.

With an appropriate choice of \(\), the optimization on the RHS can yield a good approximation to the log-exponential risk measurement on the left hand side (LHS). For brevity, for a given \(a\) we define the RSVB approximation to the true posterior \((|_{n})\) as

\[Q_{a,}^{*}(|_{n}):=\{Q: (a;Q(),_{n},)\}\]

Figure 1: (a) Optimality gap in values and (b) variance of the RSVB posterior (mean over 100 sample paths) against the number of samples (\(n\)) for various values of \(\).

and the RSVB optimal decision as

\[^{*}_{}:=_{a}\;(a;Q^{*} _{a,}(|_{n}),_{n},)=_{a }_{Q}(a;Q(),_{n},).\]

Observe that \(Q^{*}_{a,}(|_{n})\) and \(^{*}_{}\) are random quantities, conditional on the data \(_{n}\).

Examples of \(\) include the family of Gaussian distributions, delta functions, or the family of factorized'mean-field' distributions that discard correlations between components of \(\). The choice of \(\) is decisive in determining the performance of the algorithm. Part of the analysis in this paper is to articulate sufficient conditions on \(\) that ensure small optimality gap (defined below) for the optimal decision, \(^{*}_{}\). This establishes the statistical "goodness" of the procedure as number of samples increase. In this paper, we analyze the efficacy of the decision rules obtained using the RSVB approximation, by providing high-probability bounds on the optimality gap. We define the _optimality gap_ for any \(\) with value \(V=R(,_{0})\) as,

**Definition 2.1** (Optimality Gap).: _Let \(V^{*}_{0}:=_{a}R(a,_{0})\) be the optimal value for the true model parameter \(_{0}\). Then, the optimality gap in the value is the difference \(V-V^{*}_{0}\)._

A similar performance measure was used in , to measure the effectiveness of loss-calibrated VB (LCVB) approach, which can be obtained by setting \(=1\). We provide definitions of some standard terminologies that we use in the subsequent sections, such as covering number, test functions, \(\)-convergence, and primal feasibility in the Appendix A for ready reference. In the following section, we lay down important assumptions used throughout the paper to establish our theoretical results.

### Assumptions

In order to bound the optimality gap, we require some control over how quickly the posterior distribution concentrates at the true parameter \(_{0}\). Our next assumption in terms of a verifiable test condition on the model (sub-)space is one of the conditions required to quantify this rate. Let \(L_{n}:[0,)\) be a distance metric on the model space.

**Assumption 2.1** (Model indentifiability).: _Fix \(n 1\). Then, for any \(>_{n}\) such that \(_{n} 0\) as \(n\) and \(n_{n}^{2} 1\), there exists a measurable sequence of test functions \(_{n,}:_{n}\) and sieve set \(_{n}()\) such that (i) \(_{P^{n}_{0}}[_{n,}] C_{0}(-Cn^{2}),\) and (ii) \(_{\{_{n}():L_{n}(,_{0}) C_{1}ne^{ 2}\}}_{P^{n}_{}}[1-_{n,}](-Cn^{2})\)._

Observe that Assumption 2.1\((i)\) quantifies the rate at which a Type I error diminishes with the sample size, while the condition in Assumption 2.1\((ii)\) quantifies that of a Type II error. Notice that both of these are stated through test functions; indeed, what is required are consistent test functions. [11, Theorem 7.1] (stated in Appendix as Lemma C.7 for completeness) roughly implies that an appropriately bounded model subspace \(\{P^{n}_{},\}\) guarantees the existence of _consistent_ test functions, to test the null hypothesis that the true parameter is \(_{0}\) against an alternate hypothesis - the alternate being defined using the distance function \(L_{n}\). Subsequently, we will use a specific distance function to obtain finite sample bounds for the optimality gap in decisions and values. Note that in some cases, it is also possible to construct consistent test functions directly without recourse to Lemma C.7. We demonstrate this in Section 4.1 below. Next, we assume a condition on the prior distribution that ensures that it provides sufficient mass to the sieve set \(_{n}()\), as defined above in Assumption 2.1.

**Assumption 2.2**.: _Fix \(n 1\). Then, for any \(>_{n}\) such that \(_{n} 0\) as \(n\) and \(n_{n}^{2} 1\), the prior distribution satisfies \((^{}_{n}())(-Cn^{2})\)._

Notice that Assumption 2.2 is trivially satisfied if \(_{n}()=\). The next assumption ensures that the prior distribution places sufficient mass around a neighborhood of the distribution \(P^{n}_{0}\).

**Assumption 2.3** (Prior thickness).: _Fix \(n 1\) and a constant \(>0\). Let \(A_{n}:=\{:D_{1+}.\)\(.(P^{n}_{0}\|P^{n}_{}) C_{3}n_{n}^{2}\},\) where \(D_{1+}(P^{n}_{0}\|P^{n}_{}):= (_{0}}{dP^{n}_{}})^{}dP^{n}_{0}\) is the Renyi divergence between \(P^{n}_{0}\) and \(P^{n}_{}\), assuming \(P^{n}_{0}\) is absolutely continuous with respect to \(P^{n}_{}\). The prior distribution satisfies \((A_{n})(-nC_{2}_{n}^{2})\)._

This assumption guarantees that the prior distribution covers the neighborhood with positive mass. The above three assumptions are adopted from  and has also been used in  to prove convergence rates of variational posteriors. Interested readers may refer to  and  to read more about the above assumptions.

It is apparent by the first term in (RSVB) that in addition to Assumption 2.1, 2.2, and 2.3, we also require regularity conditions on the risk function \(R(a,)\). The next assumption restricts the prior distribution with respect to \(R(a,)\).

**Assumption 2.4**.: _Fix \(n 1\) and \(>0\). For any \(>_{n}\), \(a\), \(_{}[_{\{ R(a,)>C_{4}()ne^{2}\}}e^{  R(a,)}](-C_{5}()n^{2})\), where \(C_{4}()\) and \(C_{5}()\) are scalar positive functions of \(\)._

Note that the set \(\{ R(a,)>C_{4}()n^{2}\}\) represents the subset of the model space where the risk \(R(a,)\) (for a fixed decision \(a\)) is large, and the prior is assumed to place sufficiently small mass over such sets. Moreover, using the Cauchy-Schwarz inequality observe that \(_{}[_{\{ R(a,)>C_{4}()n^{2} \}}e^{ R(a,)}](_{}[_{\{ R( a,)>C_{4}()ne^{2}\}}])^{1/2}(_{}[e^{2  R(a,)}])^{1/2} e^{-C_{4}()n^{2}}_{}[e^{2 R(a,)}]\), which implies that if the risk function is bounded in \((a,)\), then the assumption is trivially satisfied. Finally, we also require the following condition lower bounding the risk function \(R\).

**Assumption 2.5**.: \(R(a,)\) _is assumed to satisfy \(W:=_{}_{a}e^{R(a,)}>0\)._

Note that any risk function which is bounded from below in both the arguments satisfies this condition. In the next section, we establish high-probability bounds on the optimality gap in values and decision rules computed using RSVB approach for sufficiently large \(n\).

## 3 Asymptotic Analysis of the Optimality Gaps

Our first result, establishes an upper bound on the expected deviation from the true model \(P_{0}^{n}\), measured using distance function \(L_{n}(,_{0})\), under the RSVB approximate posterior. We also note that the following result generalizes Theorem 2.1 of , which is exclusively for the case when \( 0^{+}\). Our proof techniques follows that of Theorem 2.1 in .

**Theorem 3.1**.: _Fix \(a^{}\) and \(>0\). For any \(L_{n}(,_{0}) 0\), under Assumptions 2.1, 2.2, 2.3, 2.4, and 2.5, and for \((C,C_{4}()+C_{5}())>C_{2}+C_{3}+C_{4}()+2\) and \(_{n}^{R}():=_{Q}_{P_{0}^{n}} [(Q()||(|_{n}))-_{a }_{Q}[R(a,)]],\) for sufficiently large \(n\) the RSVB approximator of the true posterior \(Q_{a^{},}^{*}(|_{n})\) satisfies,_

\[_{P_{0}^{n}}[_{}L_{n}(,_{0})dQ_{a^{ },}^{*}(|_{n})] n(M()_{n}^ {2}+M_{n}^{R}()),\] (2)

_for a positive number \(M()=2(C_{1}+MC_{4}())\), where \(M=}{(C,,1)}\)._

Proof sketch.: For brevity we denote the likelihood ratio as \(_{n}(,_{0})=_{n}|)}{p(_{n}|_{0})}\). We prove this result using a series of lemma. The first Lemma C.1 separates the term \(_{n}^{R}()\), which is later analyzed using Assumption 3.1 in Section 3.1. Lemma C.1 establish that

\[_{P_{0}^{n}}[_{}L_{n}(, _{0})\;dQ_{a^{},}^{*}(|_{n})]_{P_{0}^{n}}[_{}e^{ L_{n}(,_{0})},)}\;_{n}(,_{0})d() }{_{}e^{ R(a^{},)}\;_{n}(, _{0})d()}]\\ +_{P_{0}^{n}}[_{}e^{ R(a^{ },)}\;_{n}(,_{0})d()}{_ {}_{n}(,_{0})d()}]+n_{n}^{R}( ).\] (3)

The proof of Lemma C.1 uses simple arguments that follow easily from Jensen's inequality and the definition of the posterior distribution.

To analyze the first term in the above display, we define the set \(K_{n}:=\{:L_{n}(,_{0})>C_{1}n^{2}\}\), with an aim to control the exponential moment of \(L_{n}(,_{0})\) by characterizing its tails. Also, notice that set \(K_{n}\) is the set of alternate hypotheses as defined in Assumption 2.1 through Lemma C.2. For brevity, define \(_{R}(K_{n}|_{n}):=}\,e^{ R(a^{},)} \;_{n}(,_{0})d()}{_{}e^{ R(a^{ },)}\;_{n}(,_{0})d()}\). Next, we divide the expected calibrated posterior probability of the set \(K_{n}\) as follows:

\[_{P_{0}^{n}}[_{R}(K_{n}|_{n})]_ {P_{0}^{n}}_{n,}+_{P_{0}^{n}}[_{B_{n}^{C}} ]+_{P_{0}^{n}}[(1-_{n,})_{B_{n}} _{R}(K_{n}|_{n})],\] (4)where recall that \(\{_{n,}\}\) is the sequence of test function from Assumption 2.1 and set \(B_{n}=\{_{n}:_{}_{n}(, _{0})d() e^{-(1+C_{3})n^{2}}(A_{n})\}\) with set \(A_{n}\) is defined in Assumption 2.3. Set \(B_{n}\) is introduced to separately control (by lower bounding) the denominator in the definition of the posterior distribution in the last term of (4). In addition, the Assumption 2.3 is used to show that \(P_{0}^{n}(B_{C}^{C}) e^{- n^{2}}\). The first term can be controlled by Assumption 2.1 (i). Now, it remains to analyze the last term in (4). Using Assumption 2.3 and 2.5 observe that on set \(B_{n}\), \(_{}e^{ R(a^{},)}_{n}(,_{0} )d() W^{}e^{-(1+C_{2}+C_{3})n^{2}}.\) This observation, with the application of Fubini's theorem, enables us to bifurcate further the last term in (4) on the set \(S_{}():=\{e^{ R(a^{},)}>e^{C_{4}()n ^{2}}\}\) as

\[_{P_{0}^{n}}[(1-_{n,})_{B_ {n}}_{R}(K_{n}|_{n})] W^{-}e^{n^{ 2}} _{K_{n} S_{}()}_{P_{0}^{n}}[(1- _{n,})]d()\] \[+e^{-C_{4}()n^{2}}\!_{K_{n} S_{}( )}e^{ R(a^{},)}d(),\] (5)

\(=1+C_{2}+C_{3}+C_{4}().\) Observe that the last term above can be controlled using Assumption 2.4. The first integral in (5) can be further bounded by \(_{K_{n}_{n}()}_{P_{0}^{n}}[(1-_{n, })]d()+(_{n}()^{c}),\) where \(_{n}()\) is defined in Assumption 2.1 and both the terms can be exponentially bounded using Assumption 2.1 (ii) and 2.2 respectively. The last term in (3) can be bounded using a similar set of arguments and techniques. 

_Remark:_ We note that while deriving the bound in Lemma C.1, we can interchange the order of expectation and infimum and compute a tighter bound. However, this bring us back to the RSVB optimizer, the very objective of the whole analysis. Taking expectation before infimum enables us to derive a more interpretable and meaningful bound. This is mainly because it is easier to control the expectation of the RSVB objective than its infimum through Assumption 3.1, as presented in Section 3.1.

The detailed proof of Theorem 3.1 is provided in the Appendix C.2. Now recall that \(_{n}\) is the convergence rate of the true posterior [11, Theorem 7.3]. Notice that the additional term \(_{n}^{R}()\) emerges from the posterior approximation and depends on the choice of the variational family \(\), risk function \(R(,)\), and the parameter \(\). The appearance of \(_{n}^{R}()\) in the bound also signifies that to minimize the expected gap (under the RSVB posterior) between the true model and any other model (defined using \(n^{-1}L_{n}(,_{0})\)) the expected RSVB objective has to be maximized. Later in this section, we specify the conditions that ensure \(_{n}^{R}() 0\) as \(n\). Moreover, we also identify mild regularity conditions on \(\) to show that \(_{n}^{R}()\) is \(O(_{n}^{2})\) and show that as \(\) increases \(_{n}^{R}()\) decreases. We discuss this result and the bound therein later in the next subsection. Before that, we establish our main result (the bounds on the optimality gap) using the theorem above. We now fix

\[L_{n}(,_{0})=n(_{a}|R(a,)-R(a, _{0})|)^{2}.\] (6)

Notice that for a given \(\), \(n^{-1/2}(,_{0})}\) is the uniform distance between the \(R(a,)\) and \(R(a,_{0})\). Intuitively, Theorem 3.1 implies that the expected uniform difference \(L_{n}(,_{0})\) with respect to the RSVB approximate posterior is \(O(M()_{n}^{2}+M_{n}^{R}())\), and if \(M()_{n}^{2}+M_{n}^{R}() 0\) as \(n\) then it converges to zero at that rate.

Also, note that in order to use (6) we must demonstrate that it satisfies Assumption 2.1. This can be achieved by constructing bespoke test functions for a given \(R(a,)\). We demonstrate this approach by an example in Section B.2. We also provide sufficient conditions for the existence of the test functions in the appendix. These conditions are typically easy to verify when the loss function \(R(,)\) is bounded, for instance. Next, we bound the optimality gap between \(R(_{}^{*},_{0})\) and \(V_{0}^{*}\).

**Theorem 3.2**.: _Fix \(>0\). Suppose the set \(\) is compact. Then, under Assumptions 2.1, 2.2, 2.3, 2.4, and 2.5, for \((C,C_{4}()+C_{5}())>C_{2}+C_{3}+C_{4}()+2\) and any \(>0\), the \(P_{0}^{n}-\) probability of \(\{_{n}:R(_{}^{*},_{0})-_{a }R(a,_{0}) 2[M()_{n}^{2}+M_{n}^{R}( )]^{}\}\) is at least \(1-^{-1}\), for a positive mapping \(M()=2(C_{1}+MC_{4}())\), where \(M=}{(C,,1)}\) for sufficiently large \(n\)._

### Properties of \(_{n}^{R}()\)

Evidently, the bounds obtained in both the results that we have proved so far depend on \(_{n}^{R}()\). Consequently, we establish some important properties of \(_{n}^{R}()\) with respect to \(n\) and \(\), under additional regularity conditions. In order to characterize \(_{n}^{R}()\), we specify conditions on variational family \(\) such that \(_{n}^{R}()=O(_{n}^{ 2})\), for some \(_{n}^{}}\) and \(_{n}^{} 0\). We impose the following condition on the variational family \(\) that lets us obtain a bound on \(_{n}^{R}()\) in terms of \(n\) and \(\).

**Assumption 3.1**.: _There exists a sequence of distributions \(\{q_{n}()\}\) in the variational family \(\) such that for a positive constant \(C_{9}\),_

\[[(Q_{n}()||())+_ {Q_{n}()}[(dP_{0}^{n}(_{n})||dP_{}^{n} (_{n}))]] C_{9}_{n}^{ 2}.\] (7)

If the observations in \(_{n}\) are i.i.d, then observe that \(_{Q_{n}()}[(dP_{0}^{n}(_{n})\|dP_{}^{n}(_{n}))]=_{Q_{n}( )}[(dP_{0}||dP_{})].\) Intuitively, this assumption implies that the variational family must contain a sequence of distributions weakly convergent to a Dirac delta distribution concentrated at the true parameter \(_{0}\) otherwise the second term in the LHS of (7) will be non-zero. Also, note that the above assumption does not imply that the minimizing sequence \(Q_{,}^{*}(|_{n})\) (automatically) converges weakly to a Dirac-delta distribution at the true parameter \(_{0}\). Furthermore, unlike Theorem 2.3 of , our condition on \(\) in Assumption 3.1, to obtain a bound on \(_{n}^{R}(\,)\), does not require the support of the distributions in \(\) to shrink to the true parameter \(_{0}\) at some appropriate rate, as the numbers of samples increases.

The condition that the variational family contains Dirac delta distributions at each point in the parameter space is a mild and reasonable requirement for consistency. Further, Assumption 3.1 requires that \(\) contains sequences of distributions that weakly converge to each Dirac delta distribution at a certain rate. This is easily satisfied if \(\) has no "holes", e.g. if it is the family of Gaussians with all means and variances, then we can always construct sequences converging to any Dirac delta at any rate. A similar assumption has also been made in , and is true for most exponential family distributions. For instance, in the newsvendor application, we fix the variational family to a class of shifted-Gamma distributions and choose a sequence of distributions with parameter sequence \(=n\) and \(=n/_{0}\). This implies that the sequence of distributions has mean \(_{0}\) and variance \(_{0}^{2}/n\), and, therefore, it converges to a Dirac delta distribution at \(_{0}\). Next, we show that

**Proposition 3.1**.: _Under Assumption 3.1 and for a constant \(C_{8}=-_{Q}_{a}_{Q}[R(a,)]\) and \(C_{9}>0\), \(_{n}^{R}() n^{-1}C_{8}+C_{9}_{n}^{ 2}\)._

In Section 4, we present an example where the likelihood is exponentially distributed, the prior is inverse-gamma (non-conjugate), and the variational family is the class of gamma distributions, where we construct a sequence of distributions in the variational family that satisfies Assumption 3.1. We also provide another example where the likelihood is multivariate Gaussian with unknown mean and variational family is uncorrelated Gaussian restricted to a compact subset of \(^{d}\) with a uniform prior on the same compact set satisfy Assumption 3.1.

By definition \(_{n}^{2} 0\) and \(_{n}^{} 0\) as \(n\), and therefore it follows from Proposition 3.1 that \(M()_{n}^{2}+M_{n}^{R}() 0\). However, the bound obtained in the last proposition might be loose with respect to \(\), when \(C_{8}<0\). To see this, we prove the following result.

**Proposition 3.2**.: _If the solution to the optimization problem in \(_{n}^{R}()\) is primal feasible then \(_{n}^{R}()\) decreases as \(\) increases._

## 4 Applications

In the examples below, we use three examples to study the interplay between sample size \(n\) and the risk parameter \(\), and their effect on the optimality gap in values. Additionally, we consider a multi-product newsvendor example in the Appendix B.2.

### Single-product Newsvendor Model

We start with a canonical data-driven decision-making problem with a 'well-behaved' risk function \(R(a,)\), the data-driven newsvendor model. This problem has received extensive study in the literature and remains a cornerstone of inventory management [25; 2; 18]. Recall that the newsvendorloss function is defined as \((a,):=h(a-)^{+}+b(-a)^{+}\) where \(h\) (underage cost) and \(b\) (overage cost) are given positive constants, \([0,)\) the random demand, and \(a\) the inventory or decision variable, typically assumed to take values in a compact decision space \(\) with \(:=\{a:a\}\) and \(:=\{a:a\}\), and \(>0\). The distribution over the random demand, \(P_{}\) is assumed to be exponential with unknown rate parameter \((0,)\). The model risk \(R(a,):=_{P_{}}[(a,)]=ha-+(b+h) }{}\), which is convex in \(a\). We assume that \(_{n}:=\{_{1},_{2}_{n}\}\) be \(n\) observations of the random demand, assumed to be i.i.d random samples drawn from \(P_{0}\).

We fix the model space \(=[T,)\) for some \(T>0\) and assume that \(_{0}\) lies in the interior of \(\). We now assume a non-conjugate truncated inverse-gamma (\(-\)) prior distribution restricted to \(\), with shape and rate parameter \(\) and \(\) respectively, that is for a set \(A\), we define \((A)=-_{}(A;,)=-(A ;,)/-(;,)\,.\) We verify Assumptions 2.2, 2.1, 2.3, 2.5 and 2.4 (in that order) in this newsvendor setting and provide the proofs in the Appendix B.1. Next, we bound the optimality gap in values for the single product newsvendor model risk.

**Theorem 4.1**.: _Fix \(>0\). Suppose that the set \(\) is compact. Then, for the newsvendor model with exponentially distributed demand with rate \(=[T,)\), prior distribution \(()=-_{}(;,)=-(A ;,)/-(;,)\), and the variational family fixed to shifted (by \(T>0\)) gamma distributions, and for any \(>0\), the \(P_{0}^{n}-\) probability of the following event \(\{_{n}:R(_{}}^{*},_{0})-_{z }R(z,_{0}) 2 M^{}()()^{1/2}\}\) is at least \(1-^{-1}\) for sufficiently large \(n\) and for some known mapping \(M^{}:^{+}^{+}\), where \(R(,)\) is the newsvendor model risk._

The proof of the theorem above is a direct consequence of Theorem 3.2 and Proposition 3.2, and Lemmas B.1, B.2, B.3, B.4, B.5 stated in the Appendix. We also extend the analysis above to a multi-product newsvendor problem. The details are provided in Appendix B.2.

Next, we demonstrate the effect of varying the risk-sensitivity parameter \(\). We fix \(_{0}=0.1\), \(b=1\), \(h=5\), \(=1\), and \(=4.1\). We run RSVB algorithm with \(\{0(),1,2,4.5,5,6\}\) and repeat the experiment over 100 sample paths. We plot the results in Figure 1. In Figure 1(a) we plot the optimality gap in values that is \(R(_{}}^{*}(),_{0})-R(a_{0}^{*},_{0})\) for various values of \(\). We observe that the gap decreases when \(n\) increases. This observation supports our results in Propositions 3.1 and 3.2 that establishes the properties of \(_{n}^{R}()\) as \(n\) increases. Lastly, in Figure 1(b), we plot the variance of the RSVB posterior as \(n\) increases for various values of \(\); as anticipated, the variance reduces as \(n\) increases. To observe the effect of \(\), first recall that as \(\) increases the decision maker becomes more risk averse, and so is our algorithmic framework RSVB. Indeed, from the rightmost variance plot in Figure 1, it is evident that for a larger value of \(\) (\(>4\)), the RSVB posterior is more concentrated on the subset of \(\), where risk is more and consequently we observe large optimality gaps in values. Moreover, as \(n\) increases, the effect of larger \(\) reduces, since as \(n\) increases, the incentive to deviate from the posterior reduces (due to increased KL divergence dominance for larger \(n\) in RSVB). We also observe that the decision rule learned using the conjugate prior (Gamma distribution for exponential models) has a similar performance as the naive approach. However, the variance of the true conjugate posterior is higher than those computed through the RSVB and VI approach, corresponding to the well known fact that the variational approximations underestimate the true posterior variance ([28; 19]).

### Gaussian process classification

Consider a problem of classifying an input pattern or features \(Y\) lying in measure space \((^{d},,)\) (with sigma algebra \(\) and probability measure \(\)) into one of two classes \(\{-1,1\}\). Let \(Y(Y)\{-1,1\}\) denote the class of \(Y\). For a given \(Y\), we model the classifier using a Bernoulli distribution \(p(|Y,)=_{}((Y))\), where \(:^{d}\) is a non-parametric model parameter in a separable Banach space \((,\|\|)\) and measurable functions \(_{1}(x)=(1+e^{-x})^{-1}\) and \(_{-1}(x)=1-_{1}(x)\). Note that \(_{1}()\) is a logistic function. We denote \(()\) as the derivative of \(_{1}()\). Assume that the features \(Y\) are generated independently of \(\). Thus the sequence of independent observations \(\{_{n},_{n}\}=\{(Y_{1},_{1}),(Y_{2},_{2}),,(Y_{n},_{n})\}\) are assumed to be generated from the model \(dP_{}(a,y)=P_{}(=a,Y dy)=p(=a|Y=y,)(dy)\). In the above binary classification problem, the objective is to estimate \(()\) using the observation vector \(\{_{n},_{n}\}\).

We posit a Gaussian process (GP) prior \(()\) on \(()\) defined as \(W()=_{j=1}^{J_{}}_{k=1}^{2^{jd}}_{j}Z_{j,k}_{j,k}( )\), where \(\{_{j}\}\) is a sequence that decreases with \(j\), \(\{Z_{i,j}\}\) are i.i.d. standard Gaussian random variables and \(\{_{j,k}\}\) form a double-indexed orthonormal basis (with respect to measure \(\)), that is \(_{}[_{j,k}_{l,m}]=_{\{j=l,k=m\}}\)). \(_{}\) is the smallest integer satisfying \(2^{_{}d}=n^{d/(2+d)}\) for a given \(>0\). This prior construction is motivated from the work in . We also assume that \(()\) is known, and we do not place any prior on it. The posterior distribution over \(()\) given observations \(\{_{n},_{n}\}\) can be defined as \(d(|\{_{n},_{n}\})=^{n }_{_{i}}((Y_{i}))(Y_{i})}{_{i=1}^{n}_{_{i}}( (Y_{i}))()}=^{n}_{_{i}}( (Y_{i}))}{_{i=1}^{n}_{_{i}}((Y_{i}))d()}\). Consider the loss function \((a,)=\{0,a=; c_{+},a=+1,=-1; c_{-}, a=-1,=+1\}\), where \(c_{+}\) and \(c_{-}\) are known positive constants. For a given feature \(Y\), the model risk is given by

\[R(a,)=_{P_{q}}[(a,)]=c_{+}_{ }[_{-1}((Y))], a=+1,\\ c_{-}_{}[_{1}((Y))], a=-1.\] (8)

We fix the variational family \(_{GP}\) is a class of Gaussian distributions on \(\), defined as \((m_{q},_{q})\), \(m_{q}\) belongs to \(\) and \(_{q}\) is the covariance operator defined as \(_{q}=^{1/2}(I-S)^{1/2}\), for any \(S\) which is a symmetric and Hilbert-Schmidt (HS) operator on \(\) (eigenvalues of HS operator are square summable). Note that \(S\) and \(m_{q}\) span the distributions in \(_{GP}\). We can show, using the technical lemmas derived in Section B.3 in Appendix, that the optimality gap in values of the binary GP classification problem converges to zero at a minimax optimal rate (upto logarithmic factors).

**Theorem 4.2**.: _Fix \(>0\) and for a given \(J\). For the binary GP classification problem with GP prior induced by \(W=_{j=1}^{J}_{k=1}^{2^{jd}}_{j}Z_{j,k}_{j,k}\), where \(_{j}=2^{-jd/2-j}\) for some \(>0\), \(\{Z_{i,j}\}\) are i.i.d. standard Gaussian random variables and \(\{_{j,k}\}\) form a double-indexed orthonormal basis (with respect to measure \(\)), and \(\|_{0}\|_{;,}<\), and \(_{0}^{J}(y)\) lie in the Cameron-Martin space \((^{1/2})\), the variational family \(_{GP}\), and for any \(>0\), the \(P_{0}^{n}-\) probability of \(\{_{n}:R(_{}^{*},_{0})-_{z }R(z,_{0}) 2 M^{}()_{n}\}\) is at least \(1-^{-1}\) for sufficiently large \(n\) and for some mapping \(M^{}:^{+}^{+}\), where \(R(,)\) is defined in (8) and_

\[_{n}=n^{-} n& [,]; n^{-} n& [,];\\ n^{-}{(2+d)}}( n)^{}, [,]; n^{-}( n )^{}&[,].\] (9)

A similar Gaussian process classification problem was studied empirically in .

### Eight-schools model

We consider the eight-schools problem [33; 15], where the objective is to study the effectiveness of the special coaching program for SAT exams, offered in \(8\) schools. Each school reported the treatment effect \(y_{i}\) and its standard deviation \(_{i}\), where \(i\{1,2,3,,8\}\). The observations \(\{y_{i},_{i}\}_{i=1}^{8}\) are modeled using the following hierarchical Bayesian model : 1) Prior distributions: \((|0,5)\,,\,(|0,5)\), 2) \(_{i}(|,^{2})\) and \(y_{i}(|_{i},_{i}^{2})\) for each \(i\{1,2,3,,8\}\), where \(\{_{i}\}_{i=1}^{8}\) are assumed to be a known sequence of covariates. The posterior distribution is defined as \(_{8}(,,\{_{i}\}_{i=1}^{8}|\{y_{i},_{i}\}_{i=1}^{8}) (|0,5)(|0,5)_{i=1}^{8}( _{i}|,^{2})(y_{i}|_{i},_{i}^{2}).\) For a given treatment effect for eight schools \(y^{8}\), the loss function is defined as:

\[(y,a)=_{i=1}^{8}l(y_{i},a_{i}),l(y_{i},a_{i})= 0.2\,|a_{i}-y_{i}|,\,\,\,y_{i} a_{i}\\ (1-0.2)\,|a_{i}-y_{i}|,\,\,\,y_{i}<a_{i},\] (10)

where the decision variable \(a=\{a_{1},a_{2},,a_{8}\}\) denotes the effectiveness level. We define \(R(a,)= p(y|,\{_{i}\}_{i=1}^{8})(y,a)dy\), \(^{8}\) considering \(\{_{i}\}_{i=1}^{8}\) is a given sequence of covariates. Now for any \(>0\), RSVB joint-optimization problem is defined as \(_{a}_{Q}\{_{q}[R(a,)] -(q||_{8})\}.\) We fix the variational family \(\) to be the mean-field variational family, that is \(q_{}(,,_{1}^{8}=)=_{}(|_{9}) _{}(|_{10})_{i=1}^{8}_{_{i}}(| _{i}),\) where \(=\{_{1},_{2},,_{10}\}\), \(_{i}\) is the mean and variance parameter for each Gaussian distribution. Following , we measure the performance of the RSVB method using the metric called _empirical risk reduction_ (ERR), \(=_{}-_{( )},,\,_{}=}_{j=1}^{N_{Y}}l(y_{ j}^{},a_{}),\) where \(_{}\) denote the empirical risk evaluated at the decision rule \(a^{}\) obtained using method alg \(\) {VB,RSVB(\(\))}

for various values of \(\), and \(N_{Y}\) is the size of the test data \(\{y^{}_{j}\}\). Note that ERR is the empirical approximation of the difference of optimality gap between the two-step naive VB approach and RSVB\(()\) approach. Recall, in the naive VB method, we first compute the KL minimizer of the true posterior and then compute the optimal decision using this approximate posterior. Following , due to small size of the dataset in this example, the ERR is evaluated on the training data itself.

We modified the experiments in  by introducing \(=\{5,2.5,1,0.5\}=\{0.2^{-1},0.4^{-1},1^{-1},2^{-1}\}\) and obtain the results as summarized in Figure 2(a) and (b). In Figure 2(a), we observe that as \(\) increases, the ERR also increases, which implies that the decisions are more optimistic as empirical loss of the RSVB decision rule decreases as \(\) increases. Also, observe from Figure 2(b) that, as \(\) increases, the RSVB posterior (joint marginal posterior distribution for \((_{3},)\)) approaches the true posterior, and the variance of the approximate posterior also reduces.

Notice that it is not obvious that the variance of the RSVB posterior will reduce as \(\) increases. We believe that it depends on the landscape of the expected risk function and the choice of the variational family. Intuitively, a possible explanation of this phenomenon can be provided using the equation (RSVB). Consider the RSVB formulation and note that \(>0\), therefore as \(\) increases, there is more incentive to deviate from the true posterior and choose \(Q\) that maximizes expected risk for a given \(a\). Note that a \(Q\) that places more mass near the \(\) that maximizes the risk will be preferred over the one with more spread.

## 5 Discussion and Future Work

The RSVB formulation as stated in this paper requires us to solve a stochastic minimax optimization problem to compute a decision rule. This is often difficult to solve, particularly in the nonconvex-nonconcave setting of RSVB, and indeed, we are not aware of any computationally efficient methods for solving such problems in all generality . To circumvent this, the authors in the LCVB literature maximize utility instead of minimizing risk, and thus convert the whole problem to a much simpler max-max optimization problem. If we replace risk with utility in the RSVB derivation, we will also get a max-max problem. In this formulation, as you increase \(\), the incentive to deviate from the posterior and maximize the maximum utility increases. Therefore, increasing \(\) corresponds to being more optimistic in making decisions than being more risk averse in the risk setting, as observed in the empirical results for the _eight schools model_ in Section 4.3.

We note that we assumed the risk-function is lower-bounded, this is a natural assumption to place on risk functions. Converting our problem to a max-max problem would further require the risk to be upper bounded, which is also assumed for the methodologies presented (without theory) in [16; 15]. We emphasize that our theoretical results will continue to hold in this setting, but since our emphasis was on the statistical aspects of this problem, we chose to present our results with minimum assumptions. Developing an algorithm for solving general minimax optimization in RSVB without transforming it into a max-max problem is open and a part of our future work. Also an interesting topic for future work is identifying minimum additional assumptions to theoretically characterize computational aspects of this problem.

Figure 2: a) Empirical risk reduction plot for different level of risk sensitivity, b) Joint RSVB posterior distribution of \(\{_{3},\}\), plotted for different level of risk sensitivity.

Acknowledgement

We thank the anonymous reviewers for their helpful comments and discussions. This work is supported by the National Science Foundation under grant DMS-1812197.