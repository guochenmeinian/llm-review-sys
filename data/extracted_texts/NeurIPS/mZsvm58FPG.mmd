# ECMamba: Consolidating Selective State Space Model with Retinex Guidance for Efficient Multiple Exposure Correction

ECMamba: Consolidating Selective State Space Model with Retinex Guidance for Efficient Multiple Exposure Correction

 Wei Dong\({}^{1,*}\), Han Zhou\({}^{1,*}\), Yulun Zhang\({}^{2}\), Xiaohong Liu\({}^{2,}\), Jun Chen\({}^{1}\)

\({}^{1}\)McMaster University \({}^{2}\)Shanghai Jiao Tong University

{dongw22, zhouh115, chenjun}@mcmaster.ca yulun100@gmail.com xiaohongliu@sjtu.edu.cn

\({}^{*}\)Equal Contribution \({}^{}\)Corresponding Author

###### Abstract

Exposure Correction (EC) aims to recover proper exposure conditions for images captured under over-exposure or under-exposure scenarios. While existing deep learning models have shown promising results, few have fully embedded Retinex theory into their architecture, highlighting a gap in current methodologies. Additionally, the balance between high performance and efficiency remains an under-explored problem for exposure correction task. Inspired by Mambo which demonstrates powerful and highly efficient sequence modeling, we introduce a novel framework based on **Mamba** for **E**xposure **C**orrection (**ECMamba**) with dual pathways, each dedicated to the restoration of reflectance and illumination map, respectively. Specifically, we firstly derive the Retinex theory and we train a Retinex estimator capable of mapping inputs into two intermediary spaces, each approximating the target reflectance and illumination map, respectively. This setup facilitates the refined restoration process of the subsequent **E**xposure **C**orrection **M**amba **M**odule (**ECMM**). Moreover, we develop a novel **2D** **S**elective **S**tate-space layer guided by **Retinex** information (**Retinex-SS2D**) as the core operator of **ECMM**. This architecture incorporates an innovative 2D scanning strategy based on deformable feature aggregation, thereby enhancing both efficiency and effectiveness. Extensive experiment results and comprehensive ablation studies demonstrate the outstanding performance and the importance of each component of our proposed ECMamba. Code is available at https://github.com/LowlevelAI/ECMamba.

## 1 Introduction

Images captured under over-exposure and under-exposure conditions suffer from various degradations, including reduced contrast, color distortion, and information loss in extremely dark or bright regions. The objective of exposure correction is to enhance the visibility, contrast, and structural details for images with various illumination conditions, which is pivotal for improving the performance of a plethora of downstream applications such as object detection, tracking, and segmentation systems  in scenarios with improper exposure.

Similar to other image restoration tasks , many deep learning models  have been proposed for under-exposed image enhancement and demonstrate commendable results. However, our preliminary experiments indicate that these methods generally perform poorly in multi-exposure correction. This inadequacy stems from the distinct mapping flow between Over-Exposed (OE) and Normal-Exposed (NE) images compared to that between Under-Exposed (UE) and NE images. Recently, some promising works  have introduced several interesting deep learning networks to learn consistent exposure representations for multi-exposure correction. However, despite its widespread adoption and outstanding performance in under-exposure correction,Retinex theory  has not yet been deeply integrated into deep learning models for multi-exposure correction. As deep learning models often struggle to distinguish between illumination information and the intrinsic reflectance properties of objects in images, simply adopting deep learning models to address such a difficult problem usually obtains sub-optimal results and the incorporation of Retinex theory offers a physically justified way to decompose the illumination and reflectance within deep learning models. Moreover, current state-of-the-art (SOTA) performance is achieved through introducing specific designs (exposure normalization  or exposure regularization term ) to existing networks. However, these methods present limited generalization, it is essential to develop stronger foundational model with good generalizable ability. Additionally, many methods face a trade-off between performance and efficiency, particularly those based on transformers.

To address these issues, we introduce a novel two-branch **E**xposure **C**orrection network (**ECMamba**) based on standard **Mamba** architecture, which has demonstrated impressive sequence modeling ability with high efficiency . Specifically, we derive the Retinex theory and develop a Retinex estimator to transform the input into two intermediary spaces, each approximating the target reflectance and illumination map, respectively. As shown in Fig. 1a, compared to the input distribution, the generated intermediary space (\(^{}\)) shows closer approximation of target distribution, thus enabling subsequent network to execute the fine-grained restoration. Moreover, the visually compelling results in Fig. 1b column 4 demonstrate that our proposed two-branch framework offers more precise estimation and improved performance than simply optimizing the reflectance. Furthermore, we develop a novel **2D** Selective **S**tate-space layer guided by **Retinex** information (**Retinex-SS2D**) as the core operator of our ECMamba. Different from other scan strategies (_i.e.,_ cross-scan mechanism ) which considers the scanning of 2D data to 1D sequence a "direction-sensitive" problem, we regard this issue as a "feature-sensitive" problem. Therefore, we first perform feature fusion and then introduce a **D**eformable **F**eature **A**ggregation (**DFA**) guided by Retinex information. Then based on the activation response map derived from DFA, we develop a **F**eature-**A**ware **2D** Selective **S**canning (**FA-SS2D**) mechanism to flatten the aggregated feature into 1D sequence, which is subsequently fed into the standard Selective State Space process (S6) to capture long-range dependencies.

The contributions of this work are summarized as follows:

\(\) We present a novel **dual-branch framework** that fully embeds **Retinex theory** for exposure correction and we provide detailed explanation for its significance.

Figure 1: (a) T-SNE  visualization of distributions of modulated reflectance (\(^{}\)), restored reflectance (\(_{out}\)) and the final output (\(_{out}\)) for Under-Exposed (UE) and Over-Exposed (OE) images. Compared to the input data, modulated reflectance (\(^{}\)) demonstrates closer approximation of Normal-Exposed (NE) images. Besides, compared to the restored reflectance (\(_{out}\)), our final output (\(_{out}\)) are better aligned with NE data. (b) Visual result of \(^{}\), \(_{out}\) and \(_{out}\) produced by our method. From column \(2\)-\(4\), we observe a noticeable improvement on color preservation and structure recovery, which demonstrates the importance of our introduced two-branch Retinex-based pipeline and the effectiveness of our proposed **ECMamba** network.

\(\) By analyzing the operating mechanism of Selective State Space Model, we regard the scanning of vision data is a **"feature-sensitive"** issue and we propose an **efficient Retinex-SS2D layer** with Retinex-guided Feature-Aware 2D Selective Scanning Mechanism.

\(\) Extensive experiments and ablations demonstrate the **impressive performance** of our proposed method.

## 2 Related Works

Learning based Multi-Exposure CorrectionMulti-exposure correction is a challenging task due to the opposite optimization flows of under-exposure and over-exposure correction. MSEC  introduces a Laplacian pyramid architecture to restore lightness and structures. Later, several normalization and regularization methods [4; 21; 19; 20] are proposed for exposure correction. For example, the exposure normalization  is proposed for exposure compensation, ECLNet  introduces exposure-consistency representations with bilateral activation mechanism, and FECNet  opts to correct illumination in the frequency domain. Different from previous methods, we aim to develop a Retinex-based network, where Retinex guidance is utilized to modulating the optimization flows of under-exposure and over-exposure correction.

State Space Model (SSMs)Due to its impressive modeling capability for long-range dependencies and its promising efficiency, State Space Models (SSMs) and recent proposed Structured State-Space Sequence model (S4)  has attracted great interests among researchers. Based on S4, several models and strategies are introduced to improve the efficiency and boost the capability, among which Mamba  introduces an input-dependent SSM with selective mechanism and achieves superior performance than Transformers for natural language processing. Moreover, some pioneering works have applied Mamba on vision task such as image segmentation , classification  and even restoration [17; 29]. We are the first to address exposure correction problem based on Mamba, and we innovatively introduce an efficient feature-aware scanning strategy in this work.

## 3 Preliminaries

State Space Model (S4)State Space Model (S4) is introduced by combining recurrent neural networks (RNNs), convolutional neural networks (CNNs), and classical state space models. Specifically, for a sequence of \(L\) length \(^{L}\), the input at any time step \((t)\) can be mapped to an output \((t)\) through the following state space modeling:

\[h^{}(t) =h(t)+x(t),\] (1) \[y(t) =h(t),\]

where \((t)^{N 1}\) represents latent state and \(N\) denotes the dimension scaling ratio in latent state. \(^{N N}\), \(^{N 1}\), and \(^{1 N}\) are state transition matrix, control matrix, and output matrix, respectively. Mathematically, the differential equation in Eq. 1 has an equivalent integral equation and it can be solved using numerical computation. In order to integrating state space modeling into deep learning models, the discretization is required to convert continues-time model to discrete-time system by introducing the timescale \(\). Specifically, the zero-order hold (ZOH) rule, which is commonly used in SSM-based deep learning algorithms, is applied to transform continuous parameters \(,\) in Eq. 1 to discrete matrix \(},}\) as follows:

\[} =(),}=() ^{-1}(()-),\] (2) \[h(t) =}h(t-1)+}x(t), y(t)=h(t),\]

where \(}^{N N}\), \(}^{N 1}\). Moreover, given a sequence with dimension \(D\) and length \(L\), the SSM is applied independently to each dimension and \(B\), \(C\), \(\) are extended with an extra dimension \(D\). The overall computation complexity is \(O(LDN)\), which is linear to the sequence length.

Selective State-Space Model (S6)Selective State Space Model is introduced in Mamba with a selective mechanism so that the parameters in SSM can dynamically select necessary information from the context. Specifically, \(}\), \(\), \(\) are designed as input-dependent parameters by utilizing linear functions and broadcast operation. This selective mechanism can help Mamba effectively filtering out irrelevant noise and focusing on important tokens, thereby achieving outstanding performances in multiple language and vision tasks.

## 4 Methodology

The overall framework of our method is shown as Fig. 2, which demonstrates that our proposed exposure correction network is designed based on Mambo and Retinex theory. In this section, we first introduce our formulated Retinex-based Exposure Correction Framework (Sec. 4.1), then we propose to utilize **E**xposure **C**orrection **M**amba **M**odule (**ECMM**) to achieve precise restoration for the reflectance and illumination map (Sec. 4.2). More importantly, to enhance the efficiency and effectiveness, we introduce a new **2D** Selective **S**tate-space layer with an innovative scanning mechanism guided by **Retinex** information (**Retinex-SS2D**) in Sec. 4.3 and Sec. 4.4.

### Retinex-Guided Exposure Correction Framework

The Retinex theory can be expressed as \(_{GT}=_{GT}_{GT}\), where \(\) denotes Hadamard product, \(_{GT}\) is an ideal image without degradation, \(_{GT}\) and \(_{GT}\) represents the reflectance image and illumination map, respectively. However, a low-quality image \(_{LQ}\) captured under non-ideal illumination conditions (under-exposure or over-exposure scenes) inevitably suffers from severe noise, color distortion, and constrained contrast. Therefore, we introduce a perturbation to \(_{GT}\) and \(_{GT}\) respectively (\(}\) and \(}\)) to model these degraded images as:

\[_{LQ}=(_{GT}+})(_{GT}+ })=_{GT}_{GT}+_{GT} }+}_{GT}+} }.\] (3)

Some existing Retinex-based methods [6; 13; 18; 33] regard the reflectance component \(_{GT}\) as the final enhanced result, thus they ignore the last three terms in Eq. 3 and focus on modeling the mapping: \(_{GT}=(_{LQ})(_{LQ})\) using network \(\). However, these models can only achieve sub-optimal performance due to the difficulty of acquiring accurate mapping, especially for multiple exposure correction task, where multiple Under-Exposed (UE) and Over-Exposed (OE) inputs correspond to one Normal-Exposed (NE) image. Therefore, we choose to restore the reflectance and illumination component simultaneously in order to obtain satisfactory outputs. Specifically, we element-wisely multiply the both sides of Eq. 3 by \(}\) and \(}\) respectively as:

\[_{LQ}}=^{}=_{GT}+_{GT}}}+}+}}},\] (4) \[_{LQ}}=^{}=_{GT}+}_{GT}}+}+}}},\]

Figure 2: The overall architecture of our proposed Retinex-based framework for exposure correction, which includes a Retinex estimator \(\) and primary restoration network \(_{R}\) and \(_{L}\).

where \(}\) and \(}\) are the matrix such that \(}_{GT}=\) and \(}_{GT}=\), and we assume we can approximate \(}\) and \(}\) via Retinex estimator \(\). \(_{GT}}}+}+ }}}\) and \(}_{GT}}+}+ }}}\) indicate the remaining degradation in \(^{}\) and \(^{}\). Therefore, the well-exposed result can be retrieved using deep-learning networks by:

\[(},},_{c})=( _{LQ}),^{}=_{LQ}},^{}=_{LQ}},\] (5) \[_{out}=^{}+_{R}(^{ };_{c}),_{out}=^{}+ _{L}(^{};_{c}),_{out}=_{out }_{out},\]

where \(_{R}\) and \(_{L}\) are networks utilized to predict the minus degradation in \(^{}\) and \(^{}\), and \(_{c}\) serves as a Retinex guidance information derived from the \(_{LQ}\).

As shown in Fig. 2, the Retinex estimator \(\) takes \(_{LQ}\) and its mean matrix along the channel dimension (which is omitted for clarity in Fig. 2) as inputs. \(\) firstly utilizes a \(1 1\) convolution and a depth-wise convolution with \(5 5\) kernel to extract features. Then, \(}\), \(}\) and \(_{c}\) are generated by one \(1 1\) convolution, respectively. More importantly, \(^{}\) and \(^{}\) are fed into \(_{R}\) and \(_{L}\) for further restoration. In addition to optimizing \(_{out}\) to approximate \(_{GT}\), our training objective incorporates a constraint on \(}\) and \(}\), as discussed in Sec 4.5.

Discussion(i) Many Retinex-based methods  aim to learn the mapping from the input to the reflectance image and illumination map, then obtain the final result using Hadamard product operation. However, this strategy is not suitable for multi-exposure correction task. Fig. 0(a) illustrates the complicated and distant distribution patterns of OE and UE images relative to their normally exposed equivalents. Such complex distributions challenge the establishment of accurate mappings from inputs. However, by carefully analyzing Retinex theory, we construct an intermediary space that significantly reduces the distance to our optimization objectives and facilitates the subsequent fine-tuning restoration process, as shown in Fig.0(b). **(ii)** Some methods [6; 13; 33; 18] treat \(_{GT}\) as the final enhanced result, which deviates from the original explanation of Retinex theory and leads to limited performance. Therefore, we adopt a two-branch framework to reconstruct the reflectance and illumination map using distinct deep learning networks. The significance of our framework is discussed in the ablation study (Sec. 5.3).

### Exposure Correction Mamba Module (ECMM)

Together with our proposed Retinex-guided exposure correction framework, we also develop innovative networks that serves as \(_{R}\) and \(_{L}\) in Eq. 5 to estimate the remaining corruption in \(^{}\) and \(^{}\). In order to develop an effective and efficient module that is capable to achieve high performance and is friendly to resource-limited devices, we propose an novel Retinex-guided **E**xposure **C**orrection

Figure 3: The details of our proposed Retinex-SS2D layer. We firstly fuse the input feature \(_{in}\) and the Retinex guidance \(_{in}\). Then we propose an innovative Feature-Aware 2D Selective State-spce Mechanism, which utilizes Deformable Convolution (DCN) for feature aggregation. Then we propose the feature-aware scanning strategy based on the activation response map derived from DCN. Compared to other 2D scanning methods, our approach generates a sequence ordered by feature importance, thereby maximizing the robust sequence modeling capabilities of Mamba.

**Mamba Module (ECMM)** which succeeds the powerful modeling capability of Mamba. Notably, \(_{R}\) and \(_{L}\) share similar structure and we discuss the details of \(_{R}\) in this section.

As illustrated in Fig. 2, our ECMM adopts a two-scale U-Net architecture. For the encoding process, the input \(^{}\) is firstly processed by a \(conv\)\(3 3\) and one **R**etinex**M**amba **B**lock (**RMB**) to generate the initial feature \(_{0}\). Then the downsampling operation is achieved by one \(4 4\) convolution with stride 2, and the down-sampled feature is fed into another RMB to obtain the middle feature \(_{1}\). For the decoding stage, \(_{1}\) is firstly up-scaled to \(^{}_{0}\) by a \(2 2\)\(deconv\) with stride 2. To alleviate the information loss caused by the down-sampling process, we introduce an adaptive mix-up feature fusion  to transfer the encoding information to the decoder stage as:

\[_{a}=()_{0}+(1-())^{ }_{0},\] (6)

where \(\) represents a learnable coefficient, and \(\) denotes the sigmoid function. Then the fused feature \(_{a}\) is fed into the RMB and the convolution layer sequentially and the restored reflectance \(_{out}\) is obtained by a residual addition accorrding to Eq. 5.

### RetinexMamba Block (RMB)

As the core operator to extract and aggregate features in ECMM, our RMB block adopts a similar architecture with Transformer block. However, the significant computational demands of self-attention and cross-attention mechanisms obviously compromise the efficiency of Transformer-based methods, precluding their application in real-time or resource-constrained environments. To this end, we remove the attention process and introduce an innovative **R**etinex-guided **2D** Selective **S**tate-space (**R**etinex-SS2D) layer to capture long-range dependencies and facilitate dynamic feature aggregation. Therefore, the feature flow of our RMB can be described as:

\[^{}_{out}=_{in}+(( _{in}),_{c}),_{out}=^{}_{ out}+((^{}_{out})),\] (7)

where \(\) denotes the LayerNorm, \(_{in}\) and \(_{in}\) represent the input and output feature of RMB. \(_{c}\) is the Retinex guidance information extract by the Retinex estimator \(\). Moreover, inspired by ConvNext [28; 8], we remove the gating mechanism and the depth-wise convolution to develop an **E**fficient **F**eed **F**orward (**EFF**) layer that follows the \(conv\)\(1 1\)\(\)\(GELU\)\(\)\(conv\)\(1 1\) flow, which operates similarly to MLPs in Transformers, while requiring fewer parameters.

### Retinex-SS2D Layer

The detailed illustration of Retinex-SS2D layer is shown as Fig. 3. We first conduct feature fusion for input feature \(_{in}\) and Retinex guidance feature \(_{c}\) by linear operation, depth-wise convolution, element-wisely multiplication, and SiLU operation. Subsequently, the fused feature \(_{f}\) is fed into our proposed **F**eature-**A**ware **2D** Selective **S**tate-space (**FA-SS2D**) mechanism to capture dynamic long-range dependencies and achieve adaptive spatial aggregation. Besides, a gating signal \(_{s}\) and a linear operation is utilized to obtain the aggregated feature \(_{g}\).

Feature-Aware 2D Selective State-space MechanismThe standard Selective State-space Model (S6) achieves outstanding performance on sequence modeling, especially for NLP task that involves temporal sequence. However, significant challenges arise when applying S6 to 2D image. To better modeling the spatial information in 2D images, several interesting works propose multiple scan strategies to unfold image patches into 1D sequences. For example,  introduces cross-scan strategy that generate four sequences along four distinct traversal paths, and each sequence is processed by a separate S6 operation. However, such strategy incredibly increase the computational demands, which contradicts the inherently high efficiency and low computational requirements of S6. Furthermore, these techniques only involve simple scanning of images across different directions, which results in a substantial separation of local textures and global structures in some sequences. This separation, to some extent, impairs the S6 framework's modeling ability for images.

The deficiencies in the existing scanning approach drive us to reassess how S6 can be more effectively utilized for 2D images. As described in Eq. 2, for each token in a sequence, the output \(y(t)\) depends on its input \(x(t)\) and previous inputs \(\{x(1),x(2),,x(t-1)\}\). This mechanism requires the 1D sequences transformed from 2D image to meet the following two criteria to ensure excellent performance: **(1)** The sequence should prioritize the most critical feature regions at the beginning, while relegating less significant information to the end. **(2)** Spatially adjacent features should besequenced closely to avoid significant gaps in the sequence. However, existing 2D scanning strategies fail to meet these two requirements, motivating us to propose new solutions to address this gap.

Based on these observations, we introduce an efficient Feature-Aware 2D Selective State-space (FA-SS2D) mechanism, as shown in Fig. 3. Firstly, we develop a deformable feature aggregation operation modulated by Retinex information. Specifically, Deformable Convolution (DCN) [48; 9] is adopted to capture dynamic long range dependencies of the fused feature \(_{f}\). For example, when DCN is applied to the token delineated by the red frame in \(_{f}\) of Fig. 3, its receptive field is an irregular kernel and the activated tokens are outlined in blue. More importantly, when the red frame is sliding across the feature map, the irregular kernel varies and we record which tokens are activated. After this process, we obtain the total activation number and calculate the average activation frequency for each token. Therefore, we can obtain an activation response map shown in \(_{d}\) of Fig. 3, where tokens with higher activation frequency represent important features. Specifically, relatively brighter areas in under-exposed images or relatively normally exposed areas in over-exposed images contain important features and exhibit a large activation response. Based on the obtained activation response map, we propose a feature-aware scanning strategy. Different from "direction-sensitive" scanning method , our feature-aware strategy ranks tokens by their activation frequency and place tokens with higher frequencies at the start of the sequence. Therefore, our generated sequence effectively meets Mampa's requirements, thereby maximizing its modeling capabilities for vision data.

### Loss Functions and Constraints

In this work, we adopt the one-stage strategy to train our proposed ECMamba network, which means that the \(\), \(_{R}\), and \(_{L}\) are optimized simultaneously. Our ultimate training objective is to approximate \(_{out}\) to \(_{GT}\), and we also integrate several constraints on \(}\), \(}\), and \(_{out}\) to achieve stable training. Therefore, our complete optimize strategy is shown as below:

\[_{,_{R},_{L}}(_{out },_{GT})+_{L}_{1}(} _{out},)+_{R}_{1}(} _{out},)+_{1}(_{ out},_{GT}),\] (8)

where \(_{1}(}_{out},)\) and \(_{1}(}_{out},)\) are constraints applied on \(}\) and \(}\), and they essentially employ a self-supervised strategy to learn \(}\) and \(}\). In addition, considering this optimization is inherently an ill-posed problem, we adopt \(_{1}(_{out},_{GT})\) to guide the optimization towards the appropriate direction. Moreover, \(_{L}(_{out},_{GT})\) is the primary loss function in our training process and it can be calculated by:

\[(_{out},_{GT})=_{1}(_{ out},_{GT})+_{ssim}_{ssim}(_{out}, _{GT})+_{per}_{per}(_{out},_ {GT}),\] (9)

    &  &  \\   & Under-exposed & Over-exposed &  &  &  &  &  \\  & PSNR \(\) & SSIM & PSNR \(\) & SSIM & PSNR \(\) & SSIM & PSNR \(\) & SSIM & PSNR \(\) & SSIM\(\) & PSNR \(\) & SSIM\(\) \\  ZeroDCE  CVPR’20 & 14.55 & 0.589 & 10.40 & 0.512 & 12.06 & 0.544 & 16.92 & 0.633 & 7.11 & 0.429 & 12.02 & 0.531 \\ RUAS  CVPR’21 & 13.43 & 0.681 & 6.39 & 0.466 & 9.20 & 0.552 & 16.63 & 0.559 & 4.54 & 0.320 & 10.59 & 0.439 \\ URetinexNet  CVPR’22 & 13.85 & 0.737 & 9.81 & 0.673 & 11.42 & 0.699 & 17.39 & 0.645 & 7.40 & 0.454 & 12.40 & 0.550 \\ KinD  MM’19 & 15.51 & 0.761 & 11.66 & 0.730 & 13.20 & 0.742 & 13.43 & 0.484 & 7.85 & 0.478 & 10.64 & 0.481 \\ LLFlow\({}^{*}\) AAAT’22 & 22.35 & 0.858 & 22.46 & 0.863 & 22.42 & 0.861 & 21.45 & 0.679 & 20.29 & 0.671 & 20.87 & 0.675 \\ LLFlow-SSF\({}^{*}\) CVPR’23 & 22.58 & 0.859 & 22.72 & 0.865 & 22.66 & 0.863 & 21.61 & 0.671 & 20.55 & 0.695 & 21.08 & 0.683 \\ DRBN  CVPR’20 & 19.74 & 0.829 & 19.37 & 0.832 & 19.52 & 0.831 & 17.96 & 0.677 & 17.33 & 0.683 & 17.65 & 0.680 \\ DRBN+ERL’12  CVPR’23 & 19.91 & 0.831 & 19.60 & 0.838 & 19.73 & 0.836 & 18.09 & 0.674 & 17.93 & 0.687 & 18.01 & 0.680 \\ FECNet+ ECCV’22 & 22.96 & 0.860 & 23.22 & 0.875 & 23.12 & 0.869 & 22.01 & 0.674 & 19.91 & 0.696 & 20.96 & 0.685 \\ FECNet+ERL  CVPR’23 & 23.10 & 0.864 & 23.18 & 0.876 & 23.15 & 0.871 & 22.35 & 0.667 & 20.10 & 0.689 & 21.22 & 0.678 \\ Retiferm\({}^{*}\) ECCV’23 & 22.77 & 0.862 & 22.24 & 0.860 & 22.45 & 0.861 & 22.15 & 0.665 & 20.21 & 0.669 & 21.18 & 0.667 \\ LACT  ICCV’23 & 23.49 & 0.862 & 23.68 & 0.872 & 23.57 & 0.869 & - & - & - & - & - & - \\  Ours & **23.64** & **0.875** & **23.84** & **0.882** & **23.76** & **0.879** & **22.87** & **0.745** & **21.23** & **0.727** & **22.05** & **0.736** \\   

Table 1: Quantitative comparisons of different methods on multi-exposure correction datasets. The best and second-best results are highlighted in **bold** and underlined, respectively:“\(\)” means the larger, the better. Note that we obtain these results either from the original papers, or by running the officially released pre-trained models. “\(\)” means that original papers don’t report corresponding performance and we train their models using their officially released code.

where \(_{ssim}\) denotes the structure similarity loss and \(_{per}\) represents the difference between features extracted by VGG19 . The coefficients for corresponding loss functions are set as: \(_{ssim}=0.2\), \(_{per}=0.01\), \(=0.1\), \(_{R}=0.1\), and \(_{L}=0.1\) in this work.

## 5 Experiments

### Experiment Settings

DatasetsTo evaluate the performance of our method, we conduct experiments on five prevailing datasets for multi exposure correction and under-exposure correction: ME , SICE , LOLv1 , LOLv2-real , and LOLv2-synthetic  datasets. Specifically, each scene in ME dataset has five exposure levels, and we regard the images with the first two exposure level as under-exposed images and the test as over-exposed images. For SICE, following FECNet , we select the middle exposure subset as the ground truth, and define the second and the last second exposure subset as the under-exposed and over-exposed images, respectively. For LOLv1, LOLv2-real, LOLv2-synthetic datasets, we leverage their official training and testing data for model training and evaluation.

Implementation DetailsWe use the Adam optimizer with default parameters (\(_{1}=0.9\), \(_{2}=0.99\)) to implement our model by PyTorch. The initial learning rate is set to \(1 10^{-4}\) and then it is steadily decreased to \(1 10^{-6}\) by the cosine annealing scheme, respectively. We utilize random flipping and rotation for data augmentation. Image pairs are cropped as \(256 256\) and the batch size is set to \(4\). The total training iterations is set to 300K for ME dataset and 150K for other benchmarks, respectively. During the evaluation, we utilize Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM ) for numeric evaluation.

### Performances on Multi-Exposure and Under-Exposure Correction

Quantitative ResultsWe compare the performance of our ECMamba with current SOTA methods on multi-exposure correction datasets, and we report the quantitative results as Tab. 1. Notably, ECMamba significantly outperforms the current SOTA methods on both under-exposed and over-exposed images within ME dataset and SICE dataset. Specifically, our ECMamba excels in PSNR and SSIM, outperforming the second best method over 0.19 dB and 0.007 on ME dataset. Furthermore,

Figure 4: Visual comparison results on ME dataset. Compared to other exposure correction methods, our ECMamba excels in color preservation and structure recovery.

compared to the second best performance, our improvement has increased to 0.83 dB and 0.051 on SICE dataset. Tab. 2 summarizes the quantitative comparisons between our method with current SOTA methods on on under-exposure correction. Specifically, our ECMamba outperforms the second best performance (LLFlow-SKF) by an average 1.10 dB increase on PNSR with only \(4.4\%\) parameters, revealing the impressive effectiveness and high efficiency of our proposed ECMamba. These numbers demonstrate the superior quality of our enhancement and prove the effectiveness of our proposed ECMamba and two-branch Retinex-based pipeline.

Qualitative ComparisonsWe present the enhanced images of different methods in Fig. 4 (ME) and Fig. 5 (SICE). Our appealing and realistic enhancement results demonstrate our model can generate images with pleasant illumination, correct color retrieval, and enhanced texture details. For example, the rich structural details of cloud patterns (row \(2\)) mountain surface (row \(4\)) and in Fig. 4, the well preserved bridge and its edge contours (row \(1\)) and the vivid presentation of words in the

    &  &  &  &  \\  & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  Zero-DCE  cvpr\({}^{}}\)20 & 14.86 & 0.562 & 18.06 & 0.580 & - & - & 0.33 \\ RUAS  cvpr\({}^{}\)21 & 18.23 & 0.720 & 18.37 & 0.723 & 16.55 & 0.652 & 0.003 \\ URetinex-Net  cvpr\({}^{}\)22 & 21.33 & 0.835 & 21.16 & 0.840 & 24.14 & 0.928 & 1.32 \\ KinD  adv19 & 20.86 & 0.790 & 14.74 & 0.641 & 13.29 & 0.578 & 8.02 \\ LLFlow  adv172 & 25.19 & 0.870 & 26.53 & 0.892 & 26.08 & 0.940 & 37.68 \\ LLFlow-SKF  cvpr\({}^{}\)23 & 26.80 & 0.879 & 28.19 & 0.905 & 28.86 & 0.953 & 39.91 \\ DRBN  cvpr\({}^{}\)20 & 19.39 & 0.817 & 20.29 & 0.831 & 23.22 & 0.927 & 5.27 \\ DRBN+ERL  cvpr\({}^{}\)23 & 19.84 & 0.830 & - & - & - & - & - \\ FECNet  ECCV22 & 22.03 & 0.836 & 20.29 & 0.831 & 23.22 & 0.927 & 0.15 \\ FECNet+ERL  cvpr\({}^{}\)23 & 21.08 & 0.829 & - & - & - & - & - \\ Retiformer  ICCV23 & 25.16 & 0.845 & 22.80 & 0.840 & 25.67 & 0.930 & 1.61 \\ LACT\({}^{*}\) ICCV23 & 26.49 & 0.867 & 26.95 & 0.888 & 27.24 & 0.941 & 6.73 \\ 
**ECMamba (Ours)** & **27.69** & **0.885** & **29.24** & **0.908** & **29.94** & **0.959** & 1.75 \\   

Table 2: Quantitative comparisons of different methods for under-exposed correction. Notably, compared to SOTA methods, our ECMamba achieves enhanced performance on LOLv1 , LOLv2-real , and LOLv2-synthetic  datasets, demonstrating the effective of our proposed dual-branch Retinex-based framework and feature-aware SS2D layer.

Figure 5: Visual comparisons between ECMamba and other methods on SICE dataset. Our proposed ECMamba achieves compelling visual performance both on over-exposed and under-exposed images.

display board (row \(2\)) in Fig. 5. In contrast, previous methods struggle to preserve color fidelity and illumination harmonization.

### Ablation Study

To verify the effectiveness of our proposed ECMamba, we conduct extensive ablation experiments and report the average performance on SICE dataset.

The Contribution of Two-branch Retinex-based FrameworkWe first remove the branch (\(_{L}\)), which is utilized for accurate restoration of the illumination map. Therefore, the remaining network aims to optimize \(_{out}\) to the ground truth and the performance is reported in Tab. 3, which still presents competitive performance compared to the current SOTA in Tab. 1. However, compare to our complete ECMamba, only optimizing the reflectance inevitably leads to sub-optimal performance. Furthermore, we also adopt a more complicated \(_{R}\), whose parameters is comparable to the original two-branch framework. However, compared to our two-branch ECMamba, this network still demonstrate poor performance. Finally, similar to other Retinex-based methods , we then remove the Retinex estimator \(\) and directly adopt the remaining network for exposure correction. However, as shown in Tab. 3, this adaptation largely decrease the performance of our ECMamba, indicating the importance of our analysis regarding the intermediary space (\(^{}\) and \(^{}\)) in Sec. 4.1.

The Importance of Our Proposed ECMamba ModuleFirst, we replace our ECMamba module with Vision Transformer (ViT)  and Retiformer  architecture in our two-branch framework. As presented in Tab. 3, our complete ECMamba module offers impressive performance better than ViT. More importantly, ECMamba's efficiency is comparable to Retiformer, which is a famous efficient under-exposure correction approach. Furthermore, to study the significance of our proposed Retinex-SS2D layer and FA-SS2D, we replace the Retinex-SS2D layer with a cross-scan mechanism proposed in VMamba . The increased parameters and decreased performance demonstrate the superiority of our proposed Retinex-SS2D layer and FA-SS2D strategy.

## 6 Conclusions

We propose a new two-branch Retinex-based Mamba architecture for exposure correction. By carefully deriving Retinex theory, we propose an two-branch framework guided by Retinex information. To better balance the performance and efficiency, we introduce ECMamba as the primary restoration module with efficient Retinex-guided SS2D layer and Feature-aware scanning strategy. Extensive experiments demonstrate that our ECMamba significantly outperforms the current SOTA methods on both multi-exposure correction datasets and under-exposure correction datasets. We recognize that our work, while pioneering in certain aspects, also highlights avenues for future investigation. For example, similar to other methods, ECMamba struggles to deliver satisfactory results in scenarios involving extreme exposure cases (extremely dark or over-exposed environments) due to the extensive information loss inherent in degraded images. Essentially, directing recovery from severely degraded images is challenging, but recent advances in image restoration have utilized generative priors to infer the degraded details, and achieve favorable results. In the future, we plan to integrate Mamba with generative priors to effectively alleviate the performance drop on extreme exposure cases.

 
**Methods** & PSNR\(\) & SSIM\(\) & Param (M)\(\) & **Methods** & PSNR\(\) & SSIM\(\) & Param (M)\(\) \\  Removing \(_{L}\) & 21.55 & 0.721 & 1.0 & ViT & 21.88 & 0.724 & 14.46 \\ Removing \(_{L}^{*}\) & 21.63 & 0.723 & 1.93 & Retiformer & 21.35 & 0.702 & 1.6 \\ Removing \(\) & 21.12 & 0.695 & 1.5 & Cross-Scan Mechanism & 21.69 & 0.716 & 2.1 \\  

Table 3: Ablation studies on SICE dataset and the average PSNR and SSIM are reported. _Left_ is used to verify the effectiveness of the two-branch Retinex-based framework, _right_ is to present the importance of our proposed ECMamba module and FA-SS2D strategy. [Key: \({}^{*}\): When \(_{L}\) is removed, the hidden dimension of \(_{R}\) is increased to ensure its parameter number is comparable to ECMamba; ViT/Retiformer: utilized to replace our ECMamba module; Cross-Scan Mechanism: adopted to replace our FA-SS2D strategy.]