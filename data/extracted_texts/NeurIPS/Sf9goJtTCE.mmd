# Sampling from Gaussian Process Posteriors

using Stochastic Gradient Descent

Jihao Andreas Lin\({}^{*}\)\({}^{1}\)\({}^{,}\)\({}^{2}\) &Javier Antoran\({}^{*}\)\({}^{1}\) &Shreyas Padhy\({}^{*}\)\({}^{1}\)

**David Janz\({}^{3}\)** &Jose Miguel Hernandez-Lobato\({}^{1}\) &Alexander Terenin\({}^{1}\)\({}^{,}\)\({}^{4}\)

\({}^{1}\)University of Cambridge &\({}^{2}\)Max Planck Institute for Intelligent Systems

\({}^{3}\)University of Alberta &\({}^{4}\)Cornell University

Equal contribution, order chosen randomly.

Code available at: https://github.com/cambridge-mlg/SGD-GP.

###### Abstract

Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves state-of-the-art performance on sufficiently large-scale or ill-conditioned regression tasks. Its uncertainty estimates match the performance of significantly more expensive baselines on a large-scale Bayesian optimization task.

## 1 Introduction

Gaussian processes (GPs) provide a comprehensive framework for learning unknown functions in an uncertainty-aware manner. This often makes Gaussian processes the model of choice for sequential decision-making, achieving state-of-the-art performance in tasks such as optimizing molecules in computational chemistry settings  and automated hyperparameter tuning .

The main limitations of Gaussian processes is that their computational cost is cubic in the training dataset size. Significant research efforts have been directed at addressing this limitation resulting in two key classes of scalable inference methods: (i) _inducing point_ methods , which approximate the GP posterior, and (ii) _conjugate gradient_ methods , which approximate the computation needed to obtain the GP posterior. Note that in structured settings, such as geospatial learning in low dimensions, specialized techniques are available . Throughout this work, we focus on the generic setting, where scalability limitations are as of yet unresolved.

In recent years, stochastic gradient descent (SGD) has emerged as the leading technique for training machine learning models at scale , in both deep learning and related settings like kernel methods  and Bayesian modeling . While the principles behind the effectiveness of SGD are not yet fully understood, empirically, SGD often leads to good predictive performance--even when it does not fully converge. The latter is the default regime in deep learning, and has motivated researchers to study _implicit biases_ and related properties of SGD .

In the context of GPs, SGD is commonly used to learn kernel hyperparameters--by optimizing the marginal likelihood [18; 13; 19; 12; 11] or closely related variational objectives [43; 23]. In this work, we explore applying SGD to the complementary problem of approximating GP posterior samples given fixed kernel hyperparameters. In one of his seminal books on statistical learning theory, Vladimir Vapnik  famously said: _"When solving a given problem, try to avoid solving a more general problem as an intermediate step."_ Motivated by this viewpoint, as well as the aforementioned property of good performance often not requiring full convergence when using SGD, we ask: _Do the linear systems arising in GP computations necessarily need to be solved to a small error tolerance? If not, can SGD help accelerate these computations?_

We answer the latter question affirmatively, with specific contributions as follows: (i) We develop a scheme for drawing GP posterior samples by applying SGD to a quadratic problem. In particular, we re-cast the pathwise conditioning technique of Wilson et al. [50; 51] as an optimization problem to which we apply the low-variance SGD sampling estimator of Antoran et al. . We extend the proposed method to inducing point Gaussian processes. (ii) We characterize the implicit bias in SGD-approximated GP posteriors showing that despite optimization not fully converging, these match the true posterior in regions both near and far away from the data. (iii) Experimentally, we show that SGD produces strong results--compared to variational and conjugate gradient methods--on both large-scale and poorly-conditioned regression tasks, and on a parallel Thompson sampling task, where error bar calibration is paramount.

## 2 Optimization and Pathwise Conditioning in Gaussian Processes

A random function \(f:X\) over some set \(X\) is a _Gaussian process_ if, for every finite set of points \( X^{N}\), \(f()\) is multivariate Gaussian. A Gaussian process \(f(,k)\) is uniquely determined by a _mean function_\(()=(f())\) and a _covariance kernel_\(k(,^{})=(f(),f(^{}))\). We denote by \(_{}\) the _kernel matrix_\([k(x_{i},x_{j})]_{i,j=1,,N}\). We consider the Bayesian model \(=f()+\), where \((,)\) gives the likelihood, \(f(0,k)\) is the prior, and \(,\) are the training data. The posterior of this model is \(f(_{f},k_{f})\), with

\[_{f}()=_{()}(_{} +)^{-1} k_{f}(,^{})=_{(,^{})}-_{()}(_{} +)^{-1}_{(^{})}.\] (1)

Using pathwise conditioning [50; 51] one can also write the posterior directly as the random function

\[(f)()=f()+_{()}(_{ }+)^{-1}(-f()-)(,)\ f(0,k).\] (2)

We assume throughout that \(\) is diagonal. Due to the matrix inverses present, the cost to directly compute each of the above expressions is \((N^{3})\).

### Random Fourier Features and Efficient Sampling

Our techniques will rely on _random Fourier features_[34; 40]. Let \(X=^{d}\) and let \(k\) be stationary--that is, \(k(x,x^{})=k(x-x^{})\). Assume, in this section only and without loss of generality, that \(k(x,x)=1\). Random Fourier features are sets of \(L\) random functions \(:X^{L}\) whose components, indexed by \( L\), are \(_{}()=L^{-1/2}(2_{},)\) for \(\) odd, and \(_{}()=L^{-1/2}(2_{},)\) for \(\) even. By taking \(_{}\) to be samples distributed according to \(\), the normalized spectral measure of the kernel, for any \(x,x^{} X\) we have

\[k(x,x^{})=_{_{1},,_{L}}(x),(x^{}),\] (3)

which we will use to construct unbiased estimators of kernel matrices in the sequel. Fourier features can also be used to _efficiently sample_ functions from the GP prior. Naively, evaluating the random function \(f\) at \(N_{*}\) points requires computing a matrix square root of \(_{}\) at \((N_{*}^{3})\) computational cost. However, given a precomputed set of Fourier features \(\), the Monte Carlo estimator

\[f()()=^{T}()=_{=1}^ {L}_{}_{}() (,)\] (4)

approximately samples from \((0,k)\) at \((N_{*})\) cost. The approximation error in (4) decays as \(L\) goes to infinity. Following Wilson et al. [50; 51], our proposed algorithms will approximately sample from the posterior by replacing \(f\) with \(\) in the pathwise conditioning formula (2).

### Optimization-based Learning in Gaussian Processes

Following Matthews et al.  and Antoran et al. , both a Gaussian process' posterior mean and posterior samples can be expressed as solutions to quadratic optimization problems. Letting \(^{*}=(_{}+)^{-1}\) in (1), we can express the GP posterior mean in terms of the quadratic problem

\[_{f|}()=_{()}^{*}=_{i=1}^{N}v_{i}^ {*}k(x_{i},) 28.452756pt^{*}=*{arg\,min}_{ ^{N}}_{i=1}^{N}-_{x_{i}})^{2}}{ _{ii}}+\|\|_{_{}}^{2}.\] (5)

We say that \(k(x_{i},)\) are the _canonical basis functions_, \(v_{i}\) are the _representer weights_, and that \(\|\|_{_{}}^{2}=^{T}_{ }\) is the _regularizer_. The respective optimization problem for obtaining posterior samples is similar, but involves a stochastic objective, which will be given and analyzed in Section 3.2.

_Conjugate gradients_ (CG) are the most widely used algorithm to solve quadratic problems, both in the context of GPs [20; 19; 4] and more generally [33; 10]. CG reduces GP posterior inference to a series of matrix-vector products, each of \((N^{2})\) cost. Given the system \(^{-1}\), the number of matrix-vector products needed to guarantee convergence of CG to within a tolerance of \(\), is

\[()}()\|\|}{} 56.905512pt ()=()}{_{ }()},\] (6)

where \(_{}()\) and \(_{}()\) are the maximum and minimum eigenvalues of \(\). CG performs well in many GP use cases, for instance Gardner et al.  and Wang et al. . In general, however, the condition number \((_{}+)\) need not be bounded, and conjugate gradients may fail to converge quickly . Nonetheless, by exploiting the quadratic structure of the objective, CG obtains substantially better convergence rates than gradient descent [8; 52]. This presents a pessimistic outlook for gradient descent and related methods such as SGD which do not take explicit advantage of the quadratic nature of the objective. Nonetheless, the success of SGD in large-scale machine learning  motivates us to try it anyway.

Figure 1: Comparison of SGD, CG  and SVGP  for GP inference with a squared exponential kernel on \(10\)k datapoints from \((2x)+(5x)\) with observation noise \((0,0.5)\). We draw 2000 function samples with all methods by running them for 10 minutes on an RTX 2070 GPU. _Infill asymptotics_ considers \(x_{i}(0,1)\). A large number of points near zero result in a very ill-conditioned kernel matrix, preventing CG from converging. SGD converges in all of input space except at the edges of the data. SVGP can summarise the data with only 20 inducing points. Note that CG converges to the exact solution if one uses more compute, but produces significant errors if stopped too early, as occurs under the given compute budget. _Large domain asymptotics_ considers data on a regular grid with fixed spacing. This problem is better conditioned, allowing SGD and CG to recover the exact solution. However, 1024 inducing points are not enough for SVGP to summarize the data.

Gaussian Process Predictions via Stochastic Gradient Descent

We now develop and analyze techniques for drawing samples from GP posteriors using SGD. This is done by rewriting the pathwise conditioning formula (2) in terms of two stochastic optimization problems. As a preview of what this will produce, we showcase SGD's performance on a pair of toy problems, designed to capture complementary computational difficulties, in Figure 1.

### A Stochastic Objective for Computing the Posterior Mean

We begin by deriving a stochastic objective for the posterior mean. The optimization problem (5), with optimal representer weight solution \(^{*}\), requires \((N^{2})\) operations to compute both its square error and regularizer terms exactly. The square error loss term is amenable to minibatching, which gives an unbiased estimate in \((N)\) operations. Assuming that \(k\) is stationary, following Section 2.1, we can stochastically estimate the regularizer with Fourier features using the identity \(\|\|_{_{}}^{2}=_{_{1},, _{L}}^{T}^{T}()()\). Combining both estimators gives our SGD objective

\[_{i=1}^{D}-_{x_{i}})^{2}}{ _{ii}}+_{=1}^{L}(^{T}_{}())^{2}\] (7)

where \(D\) is the minibatch size and \(L\) the number of Fourier features. This regularizer estimate is unbiased even when drawing a single Fourier feature per step: the number of features controls the variance. Equation (7) presents \((N)\) complexity, in contrast with the \((N^{2})\) complexity of one CG step. We discuss sublinear inducing point techniques further on, but first turn to sampling.

### Computing Posterior Samples

We now frame GP posterior samples in a manner amenable to SGD computation similarly to (7). First, we re-write the pathwise conditioning expression given in (2) as

\[(f)()=_{}+}()}_{}-_{()}( _{}+)^{-1}(f()+)}_{ }(,) f(0,k).\] (8)

We approximate the prior function sample using the Fourier feature approach of (4) and the posterior mean, defined in Section 2.2, with the minimizer \(^{*}\) of (7) obtained by SGD. Each posterior sample's uncertainty reduction term is parametrized by a set of representer weights given by a linear solve against a noisy prior sample evaluated at the observed inputs, namely \(^{*}=(_{}+)^{-1}(f()+)\). We construct an optimization objective targeting a sample's optimal representer weights as

\[^{*}=*{arg\,min}_{^{N}}_ {i=1}^{N})+_{i}-_{x_{i}}) ^{2}}{_{ii}}+\|\|_{_{}}^{2} f()(,_{})\\ (,).\] (9)

Applying minibatch estimation to this objective results in high gradient variance, since the presence of \(_{i}\) makes the targets noisy. To avoid this while targeting the same objective, we modify (9) as

\[^{*}=*{arg\,min}_{^{N}}_ {i=1}^{N})-_{x_{i}})^{2}}{ _{ii}}+\|-\|_{_{}}^{2} f()(,_{})\\ (,^{-1}),\] (10)

moving the noise term into the regularizer. This modification _preserves the optimal representer weights_ since objective (10) equals (9) up to a constant: a proof is given in Appendix D. This generalizes the variance reduction technique of Antoran et al.  to the GP setting. Figure 2 illustrates minibatch gradient variance for these objectives. Applying the minibatch and random feature estimators of (7), we obtain a per-step cost of \((NS)\), for \(S\) the number of posterior samples.

### Inducing Points

So far, our sampling objectives have presented linear cost in dataset size. In the large-scale setting, algorithms with costs independent of the dataset size are often preferable. For GPs, this can be achieved through _inducing point posteriors_, to which we now extend SGD sampling.

Let \( X^{M}\) be a set of \(M\) inducing points. Applying pathwise conditioning to the Kullback-Leibler-optimal inducing point approximation of Titsias  gives the expression

\[(f^{()})()&=f( )+^{()}_{f}()-_{()}^{-1} _{}_{}(_{}^{-1 }_{}_{}+)^{-1}(f^{()}( )+)\\ &(,) f (0,k) f^{()}()=_{()}^{-1}_{}f().\] (11)

Following Wild et al. , Theorem 5, the optimal inducing point mean \(^{()}_{f}\) can therefore be written

\[^{()}_{f}()=_{()}^{*}= _{j=1}^{M}v^{*}_{i}k(z_{j},)^{*}=*{arg\,min}_{ ^{M}}_{i=1}^{N}-_{x,z})^{2 }}{_{ii}}+\|\|^{2}_{_{}}\] (12)

and we can again parameterize the uncertainty reduction term (8) as \(_{()}^{*}\) with

\[^{*}=*{arg\,min}_{^{M}}_{ i=1}^{N})}(x_{i})+_{i}-_{x_{i}} )^{2}}{_{ii}}+\|\|^{2}_{_{ {z}}} f^{()}() (,_{}^{-1}_{ }_{})\] (13)

A full derivation is given in Appendix C. Exact implementation of (13) is precluded by the need to sample from a Gaussian with covariance \(_{}^{-1}_{}_{}\). However, we identify this matrix as a Nystrom approximation to \(_{}\). Thus, we can approximate (13) by replacing \(f^{()}\) with \(f\), which can be sampled with Fourier features (4). The approximation error is small when \(M\) is large and the inducing points are close enough to the data. Our experiments in Appendix C show it to be negligible in practice. With this, we apply the stochastic estimator (7) to the inducing point sampling objective.

The inducing point objectives differ from those presented in previous sections in that there are \((M)\) and not \((N)\) learnable parameters, and we may choose the value of \(M\) and locations \(\) freely. The cost of inducing point representer weight updates is thus \((SM)\), where \(S\) is the number of samples. This contrasts with the \((M^{3})\) update cost of stochastic gradient variational Gaussian processes . Figure 2 shows that SGD with \(M 100\)k inducing points matches the performance of regular SGD on housekeeping(\(N 2\)M), but is an order of magnitude faster.

### The Implicit Bias and Posterior Geometry of Stochastic Gradient Descent

We have detailed an SGD-based scheme for obtaining approximate samples from a posterior Gaussian process. Despite SGD's significantly lower cost per-iteration than CG, its convergence to the true optima, shown in Figure 3, is much slower with respect to both Euclidean representer weight space, and the reproducing kernel Hilbert space (RKHS) induced by the kernel. Despite this, the predictions obtained by SGD are very close to those of the exact GP, and effectively achieve the same test RMSE. Moreover, Figure 4 shows the SGD posterior on a 1D toy task exhibits error bars of the correct width close to the data, and which revert smoothly to the prior far away from the data. Empirically, differences between the SGD and exact posteriors concentrate at the borders of data-dense regions.

Figure 2: Left: gradient variance throughout optimization for a single-sample minibatch estimator (\(D=1\)) of (9), labeled _Loss 1_, and the proposed sampling objective (10), labeled _Loss 2_, on the elevators dataset (\(N 16\)k). Middle plots: test RMSE and negative log-likelihood (NLL) obtained by SGD and its inducing point variants, for decreasing numbers of inducing points, given in the rightmost plot, as a function of time on an A100 GPU, on the housekeeping dataset (\(N 2\)M).

We now argue the behavior seen in Figure 4 is a general feature of SGD: one can expect it to obtain good performance even in situations where it does not converge to the exact solution. Consider posterior function samples in pathwise form, namely \((f)()=f()+_{()}\), where \(f(0,k)\) is a prior function sample and \(\) are the learnable representer weights. We characterize the behavior of SGD-computed approximate posteriors by splitting the input space \(X\) into 3 regions, which we call the _prior_, _interpolation_, and _extrapolation_ regions. This is done as follows.

_(I) The Prior Region._ This corresponds to points sufficiently distant from the observed data. Here, for kernels that decay over space, the canonical basis functions \(k(x_{i},)\) go to zero. Thus, both the true posterior and any approximations formulated pathwise revert to the prior. More precisely, let \(X=^{d}\), let \(k\) satisfy \(_{c}k(x^{},c x)=0\) for all \(x^{}\) and \(x\) in \(X\), and let \((f)()\) be given by \((f)()=f()+_{()}\), with \(^{N}\). Then, by passing the limit through the sum, for any fixed \(\), it follows immediately that \(_{c}(f)(c x)=f(c x)\). Therefore, SGD cannot incur error in regions which are sufficiently far away from the data. This effect is depicted in Figure 4.

_(II) The Interpolation Region._ This includes points close to the training data. We characterize this region through subspaces of the RKHS, where we show SGD incurs small error.

Let \(_{}=^{T}\) be the eigendecomposition of the kernel matrix. We index the eigenvalues \(=(_{1},,_{N})\) in descending order. Define the _spectral basis functions_ as eigenvector-weighed linear combinations of canonical basis functions

\[u^{(i)}()=_{j=1}^{N}}{}}k(x_{j},).\] (14)

These functions--which also appear in kernel principal component analysis--are orthonormal with respect to the RKHS inner product. To characterize them further, we lift the Courant-Fischer characterization of eigenvalues and eigenvectors to the RKHS \(H_{k}\) induced by \(k\), obtaining the expression

\[u^{(i)}()=*{arg\,max}_{u H_{k}}_{i=1}^{N}u (x_{i})^{2}:\|u\|_{H_{k}}=1, u,u^{(j)}_{H_{k}}=0,  j<i}.\] (15)

This means in particular that the top spectral basis function, \(u^{(1)}()\), is a function of fixed RKHS norm--that is, of fixed degree of smoothness, as defined by the kernel \(k\)--which takes maximal values at the observations \(x_{1},..,x_{n}\). Thus, \(u^{(1)}\) will be large near clusters of observations. The same will be true for the subsequent spectral basis functions, which also take maximal values at the observations, but are constrained to be RKHS-orthogonal to previous spectral basis functions. A proof of the preceding expression is given in Appendix E.3. Figure 4 confirms that the top spectral basis functions are indeed centered on the observed data.

Empirically, SGD matches the true posterior in this region. We formalize this observation by showing that SGD converges quickly in the directions spanned by spectral basis functions with large eigenvalues. Let \(_{u^{(i)}}()\) be the orthogonal projection onto the subspace spanned by \(u^{(i)}\).

Figure 3: Convergence of GP posterior mean with SGD and CG as a function of time (on an A100 GPU) on the elevators dataset (\(N 16\)k) while setting the noise scale to (i) maximize exact GP marginal likelihood and (ii) to \(10^{-3}\), labeled _low noise_. We plot, in left-to-right order, test RMSE, RMSE to the exact GP mean at the test inputs, representer weight error \(\|-^{*}\|_{2}\), and RKHS error \(\|_{f}-_{}\|_{H_{k}}\). In the latter two plots, the low-noise setting is shown on the bottom.

**Proposition 1**.: _Let \(>0\). Let \(=^{2}\) for \(^{2}>0\). Let \(_{}\) be the predictive mean obtained by Polyak-averaged SGD after \(t\) steps, starting from an initial set of representer weights equal to zero, and using a sufficiently small learning rate of \(0<<}{_{1}(_{1}+^{2})}\). Assume the stochastic estimate of the gradient is \(G\)-sub-Gaussian. Then, with probability \(1-\), we have for \(i=1,..,N\) that_

\[\|_{^{(i)}}\,_{f|}- _{^{(i)}}\,_{}\|_{H_{k}}}}\|_{2}}{^{2}t}+G|}{}}.\] (16)

The proof, as well as an additional pointwise convergence bound and a variant that handles projections onto general subspaces spanned by basis functions, are provided in Appendix E.1. In general, we expect \(G\) to be at most \((_{1}^{2}\|\|_{})\) with high probability.

The result extends immediately from the posterior mean to posterior samples. As consequence, _SGD converges to the posterior GP quickly in the data-dense region_, namely where the spectral basis functions corresponding to large eigenvalues are located. Since convergence speed on the span of each basis function is independent of the magnitude of the other basis functions' eigenvalues, SGD can perform well even when the kernel matrix is ill-conditioned. This is shown in Figure 3.

(III) The Extrapolation Region.This can be found by elimination from the input space of the prior and interpolation regions, in both of which SGD incurs low error. Consider the spectral basis functions \(u^{(i)}()\) with small eigenvalues. By orthogonality of \(u^{(1)},..,u^{(N)}\), such functions cannot be large near the observations while retaining a prescribed norm. Their mass is therefore placed away from the observations. SGD converges slowly in this region, resulting in a large error in its solution in both a Euclidean and RKHS sense, as seen in Figure 3. Fortunately, due to the lack of data in the extrapolation region, the excess test error incurred due to SGD nonconvergence may be low, resulting in _benign nonconvergence_. Similar phenomena have been observed in the inverse problems literature, where this is called _iterative regularization_. Figure 4 shows the Wasserstein distance to the exact GP predictions is high in this region, as when initialized at zero SGD tends to return small representer weights, thereby reverting to the prior.

Figure 4: SGD error and spectral basis functions. Top-left: SGD (blue) and exact GP (black, dashed) fit to a \(N=10\)k, \(d=1\) toy regression dataset. Top-right: 2-Wasserstein distance (W2) between both processes’ marginals. The W2 values are low near the data (interpolation region) and far away from the training data. The error concentrates at the edges of the data (extrapolation region). Bottom: The low-index spectral basis functions lie on the interpolation region, where the W2 error is low, while functions of index \(10\) and larger lie on the extrapolation region where the error is large.

## 4 Experiments

We now turn to empirical evaluation of SGD GPs, focusing on their predictive and decision-making properties. We compare SGD GPs with the two most popular scalable Gaussian process techniques: preconditioned conjugate gradient (CG) optimization [19; 46] and sparse stochastic variational inference (SVGP) [43; 23]. For CG, we use a pivoted Cholesky preconditioner of size 100, except in cases where this slows down convergence, where we instead report results without preconditioning. We employ the GPJax  SVGP implementation and use \(M=4096\) inducing points for all datasets, initializing their locations with the \(K\)-means algorithm. Full experimental details are in Appendix A.

### Regression Baselines

We first compare SGD-based predictions with baselines in terms of predictive performance, scaling of computational cost with problem size, and robustness to the ill-conditioning of linear systems. Following Wang et al. , we consider 9 datasets from the UCI repository  ranging in size from \(N=15\)k to \(N 2\)M datapoints and dimensionality from \(d=3\) to \(d=90\). We report mean and standard deviation over five \(90\%\)-train \(10\%\)-test splits for the small and medium datasets, and three splits for the largest dataset. For all methods, we use a Matern-\(}{{2}}\) kernel with a fixed set of hyperparameters obtained using maximum marginal likelihood , as described in Appendix A.1.

We run SGD for \(100\)k steps, with a fixed batch size of \(512\) for both the mean function and samples. For CG, we run a maximum of \(1000\) steps for datasets with \(N 500\)k, and a tolerance of \(0.01\). On the four largest datasets, CG's per-step cost is too large to run \(1000\) steps: instead, we run \(100\) steps, which takes roughly 9 hours per function sample on a TPUv2 core. For SVGP, we learn the variational parameters for \(M=4096\) inducing points by maximizing the ELBO with Adam until convergence. For all methods, we estimate predictive variances for log-likelihood computations from 64 function samples drawn using pathwise conditioning (2).

Predictive performance and scalability with the number of inputs.Our complete set of results is provided in Table 1, including test RMSE, test negative log-likelihood (NLL) and compute time needed to obtain the predictive mean on a single core of a TPUv2 device. Drawing multiple samples requires repeating this computation, which we perform in parallel. In the small setting (\(N 20\)k), taking \(100\)k steps of SGD presents a compute cost comparable to running CG to tolerance, which usually takes around 500-800 steps. Here, CG converges to the exact solution, while SGD tends to present increased test error due to non-convergence. In the large setting (\(N 100\)k), where neither method converges within the provided compute budget, SGD achieves better RMSE and NLL and results. SVGP always converges faster than CG and SGD, but it only performs best on the Buzz dataset, which can likely be summarised well by \(M=4096\) inducing points.

From Figure 5, we see that SGD makes the vast majority of its progress in prediction space in its first few iterations, improving roughly monotonically with the number of steps. Thus, early stopping after \(100\)k iterations incurs only moderate errors. In contrast, CG's initial steps actually increase test error, resulting in very poor performance if stopped too early. This interacts poorly with the number of CG steps needed, and the per-step cost, which generally grow with increased amounts of data .

Figure 5: Test RMSE and NLL as a function of compute time on a TPUv2 core for CG and SGD.

Robustness to kernel matrix ill-conditioning.GP models are known to be sensitive to kernel matrix conditioning. We explore how this affects the algorithms under consideration by fixing the noise variance to a low value of \(^{2}=10^{-6}\) and running them on our regression datasets. Table 1 shows the performance of CG severely degrades on all datasets and, for SVGP, optimization diverges for all datasets. SGD's results remain essentially-unchanged. This is because the noise only changes the smallest kernel matrix eigenvalues substantially and these do not affect convergence for the top spectral basis functions. This mirrors results presented previously for the elevators dataset in Figure 3.

Regression with large numbers of inducing points.We demonstrate the inducing point variant of our method, presented in Section 3.3, on houseelectric, our largest dataset (\(N=2\)M). We select varying numbers of inducing points with a \(K\)-nearest-neighbor algorithm, described in Appendix A.2. Figure 2 shows the time required for \(100\)k SGD steps scales roughly linearly with inducing points. It takes 68m for full SGD and 50m, 25m, and 17m for \(M=1099\)k, \(728\)k, and \(218\)k, respectively. Performance in terms of RMSE and NLL degrades less than \(10\%\) even when using \(218\)k points.

### Large-scale Parallel Thompson Sampling

A fundamental goal of scalable Gaussian processes is to produce uncertainty estimates useful for sequential decision making. Motivated by problems in large-scale recommender systems, where both the initial dataset and the total number of users queried are simultaneously large , we benchmark SGD on a large-scale Bayesian optimization task.

We draw a target function from a GP prior \(g(0,k)\) and optimize it on \(X=^{d}\) using parallel Thompson sampling . That is, we choose \(x_{}=*{arg\,max}_{x X}(f)()\) for a set of posterior function samples drawn in parallel. We compute these samples using pathwise conditioning with each respective scalable GP method. For each function sample maximum, we evaluate \(y_{}=g(x_{})+\) with \((0,10^{-6})\) and add the pair \((x_{},y_{})\) to the training data. We use an acquisition batch size of \(1000\) samples, and maximize them with a multi-start gradient descent-based approach described in Appendix A.3. We set the search space dimensionality to \(d=8\), the largest considered by Wilson et al. , and initialize all methods with a dataset of \(50\)k observations sampled uniformly at random from \(X\). To eliminate model misspecification confounding, we use a Matern-\(}{{2}}\) kernel and consider length scales of \((0.1,0.2,0.3,0.4,0.5)\) for both the target function and our models. For each length scale, we repeat the experiment for 10 seeds.

In large-scale Bayesian optimization, training and posterior function optimization costs can become significant, and predictions may be needed on demand. For this reason, we consider two variants of our experiment with different levels of compute. In the small-compute setting, SGD is run for \(15\)k steps, SVGP is given \(M=1024\) inducing points and \(20\)k steps to fit the variational parameters,

    & POL & elevators & bike & protein & kegodir & 3droad & song & buzz & houseleclec \\  & \(N\) & 15000 & 16599 & 17379 & 45730 & 48827 & 434874 & 515345 & 583250 & 2049280 \\    } & SGD & \(0.13 0.00\) & \(0.38 0.00\) & \(0.11 0.00\) & \(\) & \(0.12 0.00\) & \(\) & \(\) & \(0.42 0.01\) & \(\) \\  & CG & \(\) & \(\) & \(\) & \(\) & \(\) & \(0.15 0.01\) & \(0.85 0.03\) & \(1.41 0.08\) & \(0.87 0.14\) \\  & SVGP & \(0.10 0.00\) & \(0.37 0.00\) & \(0.07 0.00\) & \(0.57 0.00\) & \(0.09 0.00\) & \(0.49 0.01\) & \(0.81 0.00\) & \(\) & \(0.11 0.01\) \\    } & SGD & \(\) & \(\) & \(0.11 0.00\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & CG & \(0.16 0.01\) & \(0.08 0.00\) & \(\) & \(3.03 0.23\) & \(0.97 1.06\) & \(0.34 0.02\) & \(0.83 0.02\) & \(5.66 1.14\) & \(0.93 0.19\) \\  & SVGP & — & — & — & — & — & — & — & — & — \\    } & SGD & \(3.51 0.01\) & \(3.51 0.01\) & \(5.70 0.02\) & \(\) & \(15.2 0.02\) & \(27.6 11.4\) & \(220 14.5\) & \(347 61.5\) & \(162 54.3\) \\  & CG & \(\) & \(\) & \(\) & \(9.07 1.68\) & \(\) & \(85.2 36.0\) & \(195 2.31\) & \(351 48.3\) & \(157 0.41\) \\   & SVGP & \(21.2 0.27\) & \(21.3 0.12\) & \(20.5 0.02\) & \(20.8 0.04\) & \(20.8 0.05\) & \(and CG is run for \(10\) steps. In the large-compute setting, all methods are run for 5 times as many optimization steps.

Mean results and standard errors, across length scales and seeds, are presented in Figure 6. We plot the maximum function value achieved by each method. In the small-compute setting, the \(\)\(1.5\) (A100 GPU) hours required for 30 Thompson steps with SVGP and SGD are dominated by the algorithm used to maximise the models' posterior samples. In contrast, CG takes roughly twice the time, requiring almost 3h of wall-clock time. Despite this, SGD makes the largest progress per acquisition step, finding a target function value that improves upon the initial training set maximum twice as much as the other inference methods. VI and CG perform comparably, with the latter providing slightly more progress both per acquisition step and unit of time. Despite their limited compute budget, all methods outperform random search by a significant margin. In the large-compute setting, all inference methods achieve a similar maximum target value by the end of the experiment. CG and SGD make similar progress per acquisition step but SGD is faster per unit of time. SVGP is slightly slower than CG per both. In summary, our results suggest that SGD can be an appealing uncertainty quantification technique for large-scale GP-based sequential decision making.

## 5 Conclusion

In this work, we explored using stochastic gradient algorithms to approximately compute Gaussian process posterior means and function samples. We derived optimization objectives with linear and sublinear cost for both. We showed that SGD can produce accurate predictions--even in cases when it does not converge to an optimum. We developed a spectral characterization of the effects of non-convergence, showing that it manifests itself mainly through error in an extrapolation region located away--but not too far away--from the observations. We benchmarked SGD on regression tasks of various scales, achieving state-of-the-art performance for sufficiently large or ill-conditioned settings. On a Thompson sampling benchmark, where well-calibrated uncertainty is paramount, SGD matches the performance of more expensive baselines at a fraction of the computational cost.

### Hyperparameter Selection for Regression

We use a zero prior mean function and the Matern-\(}{{2}}\) kernel, and share hyperparameters across all methods, including baselines. For each dataset, we choose a homoscedastic Gaussian noise variance, kernel variance and a separate length scale per input. For datasets with less than \(50\)k observations, we tune these hyperparameters to maximize the exact GP marginal likelihood. The cubic cost of this procedure makes it intractable at a larger scale: instead, for datasets with more than \(50\)k observations, we obtain hyperparameters using the following procedure:

1. From the training data, select a _centroid_ data point uniformly at random.
2. Select a subset of \(10\)k data points with the smallest Euclidean distance to the centroid.
3. Find hyperparameters by maximizing the exact GP marginal likelihood using this subset.
4. Using \(10\) different centroids, repeat the preceding steps and average the hyperparameters.

This approach avoids aliasing bias [1; 5] due to data subsampling and is tractable for large datasets.

### Inducing Point Selection

For the inducing point SGD experiment shown in Figure 2, we choose inducing points as a subset of the training points. Due to the large number of datapoints in our dataset, we found \(k\)-means to converge very slowly. Instead, we develop an ad-hoc point elimination algorithm based on \(k\)-nearest-neighbors (KNN). We use the KNN implementation _ANNOY_, which we first run on the houseelectric dataset with num_trees set to \(50\). We then iterate through our dataset, retrieving \(100\) nearest neighbors for each original point. Of these \(100\) nearest neighbors, only the ones closer to the original point than some length scale parameter, in terms of \(_{2}\) distance, are selected. If the number of points selected is larger than one, we eliminate the original point from our dataset and we also eliminate other points which are within the length scale neighborhood of the original and selected points simultaneously. The selected points are kept. We vary the number of points eliminated by our algorithm through the modification of the length scale parameter.

### Maximizing Posterior Functions for Thompson Sampling

We maximize a random acquisition function sampled from the GP posterior in a three step process:

1. Evaluate the posterior function sample at a large number of nearby input locations. We find nearby locations using a combination of exploration and exploitation. For exploration, we sample locations uniformly at random from \(^{d}\). For exploitation, we first subsample the training data with probabilities proportional to the observed objective function values and then add Gaussian noise \(_{}(0,_{}^{2})\), where \(_{}=l/2\) and \(l\) is the kernel length scale. Throughout experiments, we find 10% of nearby locations using the uniform exploration strategy and 90% using the exploitation strategy.
2. Select the nearby locations which have the highest acquisition function values. To find the top nearby locations, we first try \(50\)k nearby locations and then identify the location with the highest acquisition function value. We repeat this process \(30\) times, finding and evaluating a total of \(1.5\)m nearby locations and obtaining a total of \(30\) top nearby locations.
3. Maximize the acquisition function with gradient-based optimization, using the top nearby locations as initialization. After optimization, the best location becomes \(x_{}\), the maximizer at which the true objective function will be evaluated in the next acquisition step. Initializing at the top nearby locations, we perform \(100\) steps of Adam on the sampled acquisition function, with a learning rate of \(0.001\) to find the maximizer.

In every Thompson step, we perform this three step process in parallel for \(1000\) random acquisition functions sampled from the GP posterior, resulting in a total of 1000 \(x_{}\), which will be added to the training data and evaluated at the objective function. Note that, although we share the initial nearby locations between sampled acquisition functions, each acquisition function will, in general, produce distinct top nearby locations and maximizers. Figure 7 illustrates a single Thompson step on a 1D problem using \(3\) acquisition functions, \(7\) nearby locations and \(3\) top nearby locations.

## Appendix B Additional Experimental Results

Figure 8 provides optimization traces for SGD, CG and their low noise variants on all datasets with under \(50\)k points. For these, we can compute the exact GP predictions, allowing us to evaluate error with respect to the exact GP in prediction space, representer weight space and in the RKHS. We observe the same trends reported in the main text. SGD decreases its prediction error monotonically, while CG does not. However, both take a similar amount of time to converge on smaller datasets and CG obtains lower error in the end. While SGD makes negligible progress in representer weight space, CG finds the optimal representer weights. CG's time to convergence is greatly increased in the low noise setting, but SGD is practically unaffected.

Table 2 provides quantitative results for inducing point SGD on the housekeeping dataset. SGD's time to convergence is shown to scale roughly linearly in the number of (inducing) points observed. However, for this dataset, keeping only \(10\%\) of observations and thus obtaining \(10\) faster

   Model &  & Standard SGD & CG & SVGP \\ \(M\) & 218 782 & 431 489 & 1 099 206 & 1 844 352 & 1 844 352 & 4 096 \\  RMSE & \(\) & \(\) & \(\) & \(0.09 0.00\) & \(0.87 0.14\) & \(0.11 0.01\) \\ Minutes & \(\) & \(24.6 5.40\) & \(49.6 10.2\) & \(162 54.3\) & \(157 0.41\) & \(20.0 0.03\) \\ NLL & -\(\) & -\(\) & -\(\) & -\(\) & \(2.07 0.58\) & -\(0.89 0.10\) \\   

Table 2: Time to convergence and predictive performance for all methods under consideration, including inducing point SGD, on the housekeeping dataset.

Figure 7: Illustration of a single Thompson sampling acquisition maximization step on a 1D problem.

convergence leaves performance unaffected. This suggests the dataset can be summarized well by a small number of points. Indeed, SVGP obtains almost as strong performance as SGD in terms of RMSE with only \(4096\) inducing points. SVGP's NLL is weaker however, which is consistent with known issues of uncertainty overestimation when using a too small amount of inducing points. On the other hand, the large and potentially redundant nature of this dataset makes the corresponding optimization problem ill-conditioned, hurting CG's performance. Tables 3 and 4 provide additional baseline results for the _subset of data_ approximation, which behaves similarly to SVGP.

Finally, Figure 9 shows all methods' Thompson sampling performance as a function of both compute time and acquisition steps for each individual true function length scale considered in our experiments. Differences among methods are more pronounced in the small compute budget setting. Here, SVGP performs well--on par with SGD--in the large length scale setting, where many observations can likely be summarized with 1024 inducing points. CG suffers from slow convergence due to ill-conditioning here. On the other hand, CG performs on par with SGD in the better-conditioned small length scale setting, while SVGP suffers. In the large compute setting, all methods perform similarly per acquisition step for all length scales except the small one, where SVGP suffers.

   Dataset & pol & elevators & bike & protein & kegglir \\ \(N\) & & 15000 & 16599 & 17379 & 45730 & 48827 \\   & 5\% & 0.20 \(\) 0.01 & 0.44 \(\) 0.00 & 0.18 \(\) 0.01 & 0.71 \(\) 0.01 & 0.11 \(\) 0.00 \\  & 10\% & 0.16 \(\) 0.00 & 0.41 \(\) 0.01 & 0.13 \(\) 0.00 & 0.66 \(\) 0.00 & 0.11 \(\) 0.00 \\   & 5\% & -0.43 \(\) 0.02 & 0.57 \(\) 0.01 & -0.61 \(\) 0.09 & 0.99 \(\) 0.01 & -0.76 \(\) 0.10 \\  & 10\% & -0.66 \(\) 0.02 & 0.51 \(\) 0.01 & -1.28 \(\) 0.07 & 0.91 \(\) 0.01 & -0.82 \(\) 0.09 \\   

Table 3: UCI benchmark with subset of data on the smaller datasets, using a randomly-sampled subset with size that varies as a percentage of the full data.

Figure 8: Convergence of GP posterior mean with SGD and CG as a function of time (on an A100 GPU) on the pol (\(N 15\)k), elevators (\(N 16\)k), bike (\(N 17\)k) and protein (\(N 46\)k) datasets while setting the noise scale to (i) maximize exact GP marginal likelihood and (ii) to \(10^{-3}\), labeled _low noise_. We plot, in left-to-right order, test RMSE, RMSE to the exact GP mean at the test inputs, representer weight error \(\|-^{*}\|_{2}\), and RKHS error \(\|_{f|}-_{}\|_{H_{k}}\).

   Dataset & 3droad & song & buzz & houselec \\ \(N\) & 434874 & 515345 & 583250 & 2049280 \\  RMSE & 25k & \(0.23 0.00\) & \(0.82 0.00\) & \(0.33 0.00\) & \(\) \\
50k & \(0.16 0.00\) & \(0.81 0.00\) & \(\) & \(\) \\  NLL & 25k & -\(0.42 0.01\) & \(1.22 0.00\) & \(0.30 0.06\) & -\(0.53 0.33\) \\
50k & **-\(\)** & \(\)** & \(\) & -\(0.49 0.39\) \\   

Table 4: UCI benchmark with subset of data on the larger datasets, using a randomly-sampled subsets of fixed size which are chosen to reflect typical GPU memory limitations. Numbers which are better than or competitive with respect to Table 1 are printed in boldface.

Figure 9: Maximum function values (mean and std. err.) obtained by Thompson sampling with our approximate inference methods as a function of acquisition steps and of compute time on an A100 GPU. The latter includes time spent drawing function samples and finding their maxima. All methods share a starting dataset of \(50\)k points and we take 30 Thompson steps, acquiring 1000 points in each. Different length scales \(l\) exhibit different behaviors: CG performs better in settings with smaller length scales and better conditioning, SVGP tends to perform better in settings with larger setting and increased smoothness, SGD performs well in both settings.

## Appendix C Inducing Points

Let \( X^{M}\) be a set of \(M>0\) inducing point locations. The variational inducing point framework of Titsias [43; 44] substitutes our observed targets \(\) with the inducing targets \(^{M}\). Our Gaussian process, conditional on a set of inducing locations and targets, is given by the mean and covariance functions

\[_{f|}() =_{()}_{}^{-1} k_{f|}(,^{}) =_{(,^{})}-_{()}_{ {zz}}^{-1}_{(^{})}.\] (17)

Titsias  provides a closed-form expression for the variational distribution \(q=(^{()},^{()})\) over inducing targets \(\) which minimizes the associated Kullback-Leibler divergence, namely

\[^{()} =_{}(_{}+_{ {x}}^{-1}_{})^{-1}_{}^{-1}\] (18) \[^{()} =_{}(_{}+_{ }^{-1}_{})^{-1}_{}.\] (19)

In turn, Matthews et al.  shows that these expressions lift to a respective infinite-dimensional Kullback-Leibler minimization problem between the variational and true posterior Gaussian processes. Using this, we obtain a variational posterior with mean and covariance functions

\[^{()}_{f|}() =_{()}(_{}+_{}^{-1}_{})^{-1}_{} ^{-1}\] (20) \[k^{()}_{f|}(,^{}) =_{(,^{})}+_{()}(( _{}+_{}^{-1}_{ {xz}})^{-1}-_{}^{-1})_{(^{})}.\] (21)

These expressions will be our starting point. We now write this Gaussian process posterior in a pathwise manner and derive stochastic estimators for the corresponding representer weights.

### Pathwise Representation of the Kullback-Leibler-optimal Variational Distribution

The pathwise expression of Section 3.3 can be derived by restricting the domain of the respective optimization problem which describes the exact posterior samples. This is done by replacing the reproducing kernel Hilbert space with a subset consisting of the span of a set of canonical basis functions centered at the inducing points, as described by Wild et al. , Theorem 5 for the posterior mean. Rather than deriving the respective result from scratch, we will verify its correctness by means of computing the relevant means and covariances. Define

\[(f^{()})() =f()+_{()}_{}^{-1} _{}(_{}_{}^{-1}_{}+)^{-1}(-f^{()}()-)\] (22) \[ (,) f (0,k) f^{()}() =_{()}_{}^{-1}f().\]

We now calculate the moments of this Gaussian process' marginal distributions and show them to match those of the KL-optimal variational Gaussian process given in (20). Write

\[[(f^{()})()] =_{()}_{}^{-1}_{ }(_{}_{}^{-1}_{ }+)^{-1}\] (23) \[=_{()}_{}^{-1}_{ }^{-1}(_{}_{}^{-1} _{}^{-1}+)^{-1}\] (24) \[=_{()}(_{}^ {-1}_{}+_{})^{-1}_{} ^{-1}\] (25) \[=^{()}_{f|}()\] (26)

and

\[((f^{()})()-^{()}_{f| }())\] (27) \[=((f^{()})()-^{()}_{f| }()),(f^{()})(^{})-^{()}_{f| }(^{}))\] (28) \[=_{(,^{})}-_{()} _{}^{-1}_{}(_{}_ {}^{-1}_{}+)^{-1}_{} _{}^{-1}_{(^{})}\] (29) \[=_{(,^{})}+_{()} _{}^{-1}(-+-_{} ^{-1}(_{}_{}^{-1}_{}^{-1}+)^{-1}_{}_{ }^{-1})_{(^{})}\] (30) \[=_{(,^{})}+_{()} _{}^{-1}(-+(_{}^{-1}_{}_{}^{-1}+)^{-1} )_{(^{})}\] (31) \[=_{(,^{})}+_{()} (-_{}^{-1}+(_{}^{-1} _{}+_{})^{-1})_{( ^{})}\] (32) \[=k^{()}_{f|}(,^{})\] (33)

which recovers (21), as claimed.

### Derivation of inducing point optimization objectives

We now derive the optimization objectives we use for inducing points.

The MAP objective (12).To find the optimization objective for the inducing point mean function's represent weights, we apply (23) and begin from

\[^{()}_{f|}()=_{()}(_{ }^{-1}_{}+_{})^{-1 }_{}^{-1}=_{()} ^{*}.\] (34)

Now, we recognize \((_{}^{-1}_{}+ _{})^{-1}_{}^{-1}=^{*}\) as the expression for the optimizer of a ridge-regularized linear regression problem--see Bishop , Chapter 3--with parameters \(\), features \(_{}\), Gaussian noise of covariance \(\), and regularizer \(_{}\). This means that its respective optimization problem is

\[*{arg\,min}_{^{M}}\|-_{ }\|_{^{-1}}^{2}+\|\|_{_{ {z}}}^{2}.\] (35)

The sampling objective (13).For the uncertainty reduction term's representer weights with inducing points, we also leverage (23), giving

\[_{()}_{}^{-1}_{}(_{}_{}^{-1} _{}+)^{-1}(f^{()}()+)\] (36) \[=_{()}(_{}^{-1}_{}+_{})^{-1}_{}^{-1}(f^{()}()+)\] (37) \[=_{()}^{*}.\] (38)

This is again therefore the solution to a ridge-regularized quadratic problem, but now the targets are given by the random variable \((f^{()}()+)\). We thus write the objective

\[*{arg\,min}_{^{M}}\|f^{()}( {x})+-_{}\|_{^{-1 }}^{2}+\|\|_{_{}}^{2}.\] (39)

On the Error in the Nystrom Approximation \(_{}_{}^{-1}_{} _{}\)

Figure 10 compares the KL-optimal inducing point posterior GP with that obtained when applying the approximation presented in Section 3.3. That is, taking the prior function samples which we fit with the representer weighed canonical basis functions to be \(f()\) with \(f(0,k)\) instead of \(f^{()}()=_{}_{}^{-1}f( )\). This amounts to approximating the Nystrom-type matrix \(_{}_{}^{-1}_{}\) with its exact-posterior counterpart \(_{}\). The difference between them is exactly equal to the posterior covariance: by posterior contraction, both of these matrices become very similar if there is an inducing point placed sufficiently close to every data point. In practice, this tends to occur when an inducing point is placed within roughly a half-length-scale of every observation. This is effectively what is needed for inducing point methods to provide a good approximation of the exact GP. This is reflected in Figure 10, where we see that our approximate inducing point posterior differs from the exact inducing point posterior only in situations where the latter fails to be a good approximation to the exact GP in the first place. This manifests as the approximate method providing larger error bars. When the number of inducing points increases, both methods become indistinguishable from each other and the exact GP. Fortunately, the linear cost of SGD in the number of inducing points allows us to use a very large number of these in practice.

## Appendix D Low-variance Sampling Objectives

Here, we show that the modified sampling objective (10) matches the standard sample-then-optimize objective (9) up to a constant. We then compare both objectives' minibatch gradient variance.

### Standard and Modified Sample-then-Optimize Objectives: Equivalent Gradients

Let \(^{T}=\) be the Cholesky factorization of the noise covariance, let \(f()(,_{})\), and let \((,)\). Consider the two objectives at hand, namely

\[\|f()+-_{}\|_{^{-1}}^{2}+\|\|_{_{}}^{2}\] (40)and

\[\|f()-_{}\|_{^{-1}}^{2}+\| -^{-T}\|_{_{}}^{2}.\] (41)

We show these are equal up to a constant by showing they have the same derivatives. Taking derivative with respect to \(\), we have

\[} \|f()+-_{} \|_{^{-1}}^{2}+\|\|_{_{ }}^{2}\] (42) \[=-2_{}^{-1}(f()+ -_{})+2_{ {x}}\] (43) \[=-2_{}(^{-1}f()-^{-1}_{}+^{-T}-),\] (44)

and

\[} \|f()-_{}\|_{^ {-1}}^{2}+\|-^{-T}\|_{_{}}^{2}\] (45) \[=-2_{}^{-1}(f()- _{})+2_{}(-^{-T})\] (46) \[=-2_{}(^{-1}f()-^{-1}_{}+^{-T}-)\] (47)

respectively. These expressions match, giving the claim.

### Minibatch Gradient Variance

Following Antoran et al. , we consider single sample minibatch gradient estimators for the data fit term in (7), that is \(D=1\). For each of the two sampling objectives under consideration, the gradient estimators are

\[ L_{} =-N}_{i U(1,,N)}_{ {x},x_{i}}(f(x_{i})+_{i}-_{x_{i},})\] (48) \[ L_{} =-N}_{i U(1,,N)}_{ {x},x_{i}}(f(x_{i})-_{x_{i},})\] (49)

Figure 10: Comparison of exact and approximate inducing point posteriors for a GP with squared exponential kernel and \(10\)k data points generated using the true regression function \((2x)+(5x)\) under two different data-generation schemes: _infill asymptotics_, which considers \(x_{i}(0,1)\), and _large-domain asymptotics_, which considers \(x_{i}\) on an evenly spaced grid with fixed spacing. We see that the approximation needed to apply inducing points is only inaccurate in situations where the inducing point posterior itself has significant error, which generally manifests itself as error bars that are larger than those of the exact posterior.

for (9) and (10), respectively. Since both objectives use the same Fourier feature estimator for the gradients of the regularizer, we omit these from our analysis. The variances for single sample gradient estimators of both expressions are given by

\[( L_{}) =N(_{}(f()+-_{,}))\] (50) \[( L_{}) =N(_{}(f()-_{ ,})),\] (51)

respectively. Expanding both expressions and subtracting them we arrive at

\[( L_{})-( L_{ }) =N(_{})-2N (_{}_{}, _{}).\] (52)

Whether our proposed sampling objective presents lower variance rests on the positivity of the above matrix. To analyze this, we must give concrete values to \(\). Following Antoran et al. , we consider two settings, initialization and convergence.

_(I) Initialization._ Here, \(=\) and therefore

\[( L_{})-( L_{ }) =N(_{})=N( _{}_{})\] (53)

is a positive definite matrix. Thus, our proposed objective has lower variance.

_(II) Convergence._ Here, \(=(_{}+)^{-1}(f()+)\), so we have

\[( L_{})-(  L_{}) =N(_{})-2N (_{}_{}, _{})\] (54) \[=N_{}_{}-2N_{}^{2}(_{}+)^{-1}_{ }.\] (55)

Letting \(=^{2}\) and \(^{T}=_{}\) be the eigendecomposition of the kernel matrix, we have

\[N_{}_{}-2N_{}^{2}(_{}+)^{-1}_{ }\] (56) \[=N^{2}(^{2})(-2 (+^{2})^{-1})^{T}.\] (57)

Thus, at convergence, whether our proposed objective reduces variance rests on whether or not the matrix \(-2(+^{2})^{-1}\) is positive definite.

Following Section 3.4, in practice, the representer weights only converge along the top few principal directions. For these, the corresponding eigenvalues will likely be much larger than the noise and thus \(1-2}<0\). However, in the vast majority of directions, the opposite is true: this yields lower variance to the minibatch estimator of our proposed objective, which is shown in Figure 2.

## Appendix E The Implicit Bias of SGD GPs

This section provides our main theoretical results.

### Convergence of Stochastic Gradient Descent

Before studying what happens in Gaussian processes, we first prove a fairly standard result on the convergence of SGD for a quadratic objective appearing in kernel ridge regression, which represents the posterior mean. For simplicity, we do not analyze Nesterov momentum, nor aspects such as gradient clipping. We will take the Gaussian observation noise covariance to be a constant multiple of the identity in this subsection. Recall that a random variable \(z\) is called _\(G\)-sub-Gaussian_ if \((((z-(z))))(G^{2})\) holds for all \(\). Then, a random vector is called \(G\)-sub-Gaussian if its dot product with any unit vector is \(G\)-sub-Gaussian. One can show this condition is equivalent to having tails that are no heavier than those of a Gaussian random vector, formulated appropriately. Let \(^{T}=_{}\) be the eigenvalue decomposition of the kernel matrix. The eigenvectors \(_{i}\) and eigenvalues \(_{i}\) are given in descending order: \(_{1}.._{N}>0\).

**Lemma 2**.: _Let \(>0\) and \(=^{2}\) for \(^{2}>0\). Let \(\) be a sufficiently small learning rate of \(0<<}{_{1}(_{1}+^{2})}\). Let \(^{*}^{N}\) be the solution of the respective linear system, and let \(}_{t}\) be the Polyak-averaged SGD iterate after \(t\) steps, starting from an initial condition of \(_{0}=\). Assume that the stochastic estimate of the gradient is unbiased and \(G\)-sub-Gaussian. Then, with probability \(1-\), we have for any \(\{1,..,N\}\) and all \(i\) that_

\[|_{i}^{T}(^{*}-}_{t})|}\|_{2}}{^{2}t}+G |}{}}.\] (58)Proof.: Consider the objective

\[L()=}_{i=1}^{N}(y_{i}-_{x_{i}, })^{2}+^{T}_{}\] (59)

and respective gradient

\[}()=}( _{}^{2}-_{})+ _{}=}(_{ }(_{}+^{2})-_ {}).\] (60)

Let us first look at non-stochastic gradient optimization of \(L\), without Polyak averaging. The iterate \(_{t}\) is given by

\[_{t}=_{t-1}-} (_{t-1})=-}_{}(_{}+^{2})_{t -1}+}_{}.\] (61)

Writing \(=-}_{} (_{}+^{2})\), and recalling that \(_{0}=\), we thus have that

\[_{t}=}_{j=0}^{t-1}^{j}_{}=}(-)^{-1}( -^{t})_{}=^{*}-( _{}+^{2})^{-1}^{t}\] (62)

where, for our choice of learning rate, (i) by direct calculation using simultaneous diagonalizability of \(\) and \(_{}\), the matrix \(\) has eigenvalues whose absolute values are strictly less than 1, (ii) thus, the geometric series converges, (iii) and \(-\) is positive definite. Examining the error in direction \(_{i}\), we use simultaneous diagonalizability to see that

\[|_{i}^{T}(^{*}-_{t})| =|_{i}^{T}(_{}+^{2})^ {-1}^{t}|=1-}{^{2}}( _{i}+^{2})^{t}}{_{i}+^{2}}|_{i}^{T}|\] (63) \[1-}{^{2}}(_{i} +^{2})^{t}}{_{i}+^{2}}\|\|_{2}\] (64)

which applies to ordinary gradient descent without stochastic gradients or Polyak averaging. Next, we consider stochastic gradient optimization. We will first consider the case where the gradient is independently perturbed by \(_{t}(,)\) for each step \(t>0\), and then relax this to sub-Gaussian noise in the sequel. Specifically, we consider

\[_{t}^{}=_{t-1}^{}-}(_{t-1}^{})+_{t}= {}_{t}-_{j=0}^{t-1}^{j}_{t-j}\] (65)

with \(_{0}^{}=_{0}=\). For these iterates, consider the respective Polyak-averaged iterates denoted by \(}_{t}=_{j=1}^{t}_{j}^{}\). We have

\[|_{i}^{T}(^{*}-}_{t})|=| _{j=1}^{t}_{i}^{T}(^{*}-_{j}^{})| _{j=1}^{t}_{i}^{T}(^{*}- _{j})|}_{A_{i,t}}+|_{j=1 }^{t}_{i}^{T}(_{j}-_{j}^{})|}_{B_{i,t}}|\] (66)

by expanding the Polyak averages and applying the triangle inequality, and where we have introduced the notation \(A_{i,t}\) and \(B_{i,t}\). Using (63), the triangle inequality and another geometric series, we can bound the first sum as

\[|A_{i,t}|=\|_{2}}{(_{i}+^{2})t}_{j=1 }^{t}1-}{^{2}}(_{i}+^{2}) ^{j}\|\|_{2}}{_{i}( _{i}+^{2})^{2}t}\] (67)

For the second sum, note that we can re-index the order of summation to to count each \(_{j}\) only once, rather than once for each Polyak average. This gives

\[B_{i,t} =_{j=1}^{t}_{q=0}^{j-1}_{i}^{T}^{q}_{j-q}=_{j=1}^{t}_{q=0}^{t-j}_{i}^{T} ^{q}_{j}\] (68) \[=_{j=1}^{t}_{q=0}^{t-j}1-}{^{2}}(_{i}+^{2})^{q}_{i}^{T}_{j}\] (69)where \(_{i}^{T}_{j}(0,1)\) are IID. The variance of \(B_{i}\) is bounded using another geometric series

\[(B_{i,t}) =_{j=1}^{t}_{q=0}^{t-j} \!(1-}{^{2}}(_{i}+^{2}))^{q }_{i}^{T}_{j}\] (70) \[=}{t^{2}}_{j=1}^{t}\!(_{q=0 }^{t-j}\!(1-}{^{2}}(_{i}+^{2}) )^{q}_{i}^{T}_{j})\] (71) \[=}{t^{2}}_{j=1}^{t}[(_{q=0}^{t-j} \!(1-}{^{2}}(_{i}+^{2}))^{ q})^{2}_{i}^{T}_{j}}_{=1} ]\] (72) \[}{t^{2}}_{j=1}^{t}\!( }{_{i}(_{i}+^{2})})^{2}=}{ _{i}^{2}(_{i}+^{2})^{2}t}.\] (73)

Thus, from standard tail inequalities for Gaussian random variables, for any fixed \(^{}>0\) we have

\[\!(|B_{i,t}|}{_{i}^{2}( _{i}+^{2})^{2}t}}}) 1- ^{}.\] (74)

We then take \(^{}=/||\), and apply the union bound for all indices in \(\). Finally, by comparing moment generating functions, we can relax the Gaussian assumption to \(G\)-sub-Gaussian random variables. To complete the claim, we combine the bounds for the two sums by writing

\[|_{i}^{T}(^{*}-}_{t})| |A_{i,t}|+|B_{i,t}|\|\|_{2}}{ _{i}(_{i}+^{2})^{2}t}+G}{_{ i}^{2}(_{i}+^{2})^{2}t}|}{}}\] (75) \[=}{_{i}(_{i}+^{2})} \|_{2}}{(_{i}+^{2})t}+G |}{}}\] (76) \[}\|_{2}}{ ^{2}t}+G|}{}}\] (77)

which gives the claim. 

### Convergence of SGD in RKHS Subspaces

The idea is to use the preceding result to show that SGD converges fast with respect to a certain seminorm. Let \(k\) be the kernel and let \(H_{k}\) be its associated reproducing kernel Hilbert space. We now define the spectral basis functions from Section 3.4.

**Definition 3**.: _Let \(_{}=^{T}\) be the respective eigendecomposition. Define the spectral basis functions_

\[u^{(i)}()=_{j=1}^{N}}{}}k(x_{j},).\] (78)

These are precisely the basis functions which arise in kernel principal component analysis. We also introduce two subspaces of the RKHS: the span of the representer weights, and the span of a subset of spectral basis functions. These are defined as follows.

**Definition 4**.: _Define the representer weight space_

\[R_{k,}=k(x_{i},):i=1,..,N} H_ {k}\] (79)

_equipped with the subspace inner product._

**Definition 5**.: _Let \(\{1,..,N\}\) be an arbitrary set of indices. Define the interpolation subspace_

\[R_{k,}^{}=u^{(i)}:i}  R_{k,}.\] (80)We can use these subspaces to define a seminorm on the RKHS, which we will use to measure convergence rates of SGD.

**Definition 6**.: _Define the interpolation seminorm_

\[|f|_{R^{T}_{k,}}=\|_{R^{T}_{k,}}f\|_{H_{k}}.\] (81)

Here, \(_{}f\) denotes orthogonal projection of a function \(f\) onto the subspace \(\) of a Hilbert space: to ease notation, in cases where we project onto the span of a single vector, we write the vector in place of \(\). The first order of business is to understand the relationship between the spectral basis functions and representer weight space.

**Lemma 7**.: _The functions \(u^{(i)}\), for \(i=1,..,N\), form an orthonormal basis of \(R_{k,}\)_

Proof.: Let \(i j\). Using the reproducing property, definition of an eigenvector, the assumption that \(\) is an eigenvector of \(_{}\), and orthonormality of eigenvectors, we have

\[ u^{(i)},u^{(j)}_{H_{k}}=_{i}^{T}_{ }_{j}}{_{j}}}=}{_{j}}}_{i}^{T}_{j}=0.\] (82)

The fact that \(u^{(i)}\) form a basis follows from the fact that they are linearly independent, that there are \(N\) of them in total, and that by definition this equals the dimension of \(R_{k,}\). Finally, we calculate the norm:

\[\|u^{(i)}\|_{H_{k}}^{2}=_{i}_{}_{i}^{T }}{_{i}}=1.\] (83)

Next, we compute a change-of-basis formula between the canonical basis functions and spectral basis functions.

**Lemma 8**.: _For some vector \(^{N}\), let_

\[()=_{i=1}^{N}_{i}k(x_{i},)=_{j=1}^{N}w_{j}u^{(j) }().\] (84)

_Then we have_

\[=^{1/2}^{T} \|\|_{H_{k}}^{2}=^{T}.\] (85)

Proof.: By expanding \(u^{(j)}\), we have

\[()=_{j=1}^{N}w_{j}u^{(j)}()=_{i=1}^{N}_{j=1}^{N}w_ {j}}{}}k(x_{i},)\] (86)

which since \(k(x_{i},)\) is a basis means \(_{i}=_{j=1}^{N}w_{j}}{}}\), or in matrix-vector notation \(=^{-1/2}\). The final claim about norms follows from orthonormality of \(u^{(j)}\) by writing

\[\|\|_{H_{k}}^{2}=,_{H_{k}}=_{i=1}^{N} _{j=1}^{N}w_{i}w_{j} u^{(i)},u^{(j)}_{H_{k}}=_{i=1}^{N}w_{i} ^{2}=^{T}.\] (87)

This allows us to get an explicit expression for the previously-introduced seminorm.

**Lemma 9**.: _For \( R_{k,}\), letting \(_{}=(_{1}_{1},..,_{1}_{i},..,_{N}_{N })\), we have_

\[||_{R^{T}_{k,}}^{2}=^{T}_{ }^{T}\] (88)Proof.: Decompose \(\) in the above orthonormal basis, giving

\[()=_{j=1}^{N}w_{j}u^{(j)}().\] (89)

By orthonormality, the definition of \(R_{k,}^{}\), and properties of projections, we have

\[(_{R_{k,}^{}})()=_{j }w_{j}u^{(j)}().\] (90)

Therefore, again using orthonormality, we obtain

\[||_{R_{k,}^{}}^{2}=_{j }w_{j}^{2}=^{T}_{} ^{T}\] (91)

which gives the claim. 

With this at hand, we claim that our variant of SGD converges quickly with respect to this seminorm.

**Proposition 10**.: _Under the assumptions of Lemma 2, for any \(\) we have_

\[|_{f|}-_{}|_{R_{k,}^{ }}(\|_{2}}{^{2}t}+G |}{}})}}}.\] (92)

Proof.: Using Lemma 9 and linearity, we have

\[|_{f|}-_{}|_{R_{k,}^{ }}=|_{i=1}^{N}(v_{i}-v_{i}^{*})k(x_{i},)|_{R_{k,}^{}}=-^{*})^{T}_{ }^{T}(-^{*})}\] (93) \[=}|_{i}^{T}(-^ {*})|^{2}_{i}}}|}(\|_{2}}{^{2}t}+G|}{}})|^{2} _{i}}\] (94) \[=(\|_{2}}{^{2}t}+G|}{}})}}}\] (95)

where the final inequality comes from substituting in the result of Lemma 2, yielding the claim. 

Our main claim follows directly from the established framework.

**Proposition 1**.: _Let \(=^{2}\) for \(^{2}>0\). Let \(_{}\) be the predictive mean obtained by Polyak-averaged SGD after \(t\) steps, starting from an initial set of representer weights equal to zero, and using a sufficiently small learning rate of \(0<<}{_{1}(_{1}+^{2})}\). Assume the stochastic estimate of the gradient is \(G\)-sub-Gaussian. Then, with probability \(1-\), we have for \(i=1,..,N\) that_

\[\|_{^{(i)}}\,_{f|}-_{ ^{(i)}}\,_{}\|_{H_{k}}}}\|_{2}}{^{2}t}+G|}{}}.\] (16)

Proof.: Choose \(\) to be a singleton, and apply Proposition 10, where by replacing \(||\) with \(N\) in the final bound the claim can be made to hold with probability \(1-\) for all \(i=1,..,N\) simultaneously. 

This concludes the first part of the argument for why SGD is a good idea: it converges fast with respect to this seminorm. In particular, taking \(\) to be the full index set, a position-dependent pointwise convergence bound follows.

**Corollary 11**.: _Under the assumptions of Lemma 2, we have_

\[|_{}()-_{f|}()| (\|_{2}}{^{2}t}+G |}{}})_{i=1}^{N}}}|u^{(i)}()|.\] (96)Proof.: Using Lemma 8, write

\[_{}()-_{f|}()=_{j=1}^{N}(v_{j}-v_{j}^{*})k( x_{i},)=_{i=1}^{N}(w_{i}-w_{i}^{*})u^{(i)}()\] (97)

where \(-^{*}=^{1/2}^{T}(-^{*})\). Applying Lemma 2 with \(=\{1,..,N\}\), we conclude

\[|_{}()-_{f|}()| _{i=1}^{N}|w_{i}-w_{i}^{*}\|u^{(i)}()|=_{i=1}^{N} }|_{i}^{T}(-^{*})||u^{(i)}()|\] (98) \[_{i=1}^{N}} }\|_{2}}{^{2}t}+G|}{}}|u^{(i)}()|\] (99) \[=(\|_{2}}{^{2}t}+G |}{}})_{i=1}^{N}}}|u^{(i)}()|.\] (100)

The claim follows. 

Examining the expression, the approximation error will be high at locations \(x X\) where \(u^{(i)}(x)\) corresponding to tail eigenfunctions is large. We proceed to analyze when this occurs.

### Courant-Fischer in the Reproducing Kernel Hilbert Space

Since the preceding seminorm is induced by a projection within the RKHS, the next step is to understand what kind of functions the corresponding subspace contains. To get a qualitative view, the basic idea is to lift the Courant-Fischer characterization of eigenvalues and eigenvectors to the RKHS.

We will need the following variant of the Min-Max Theorem, which is useful for describing multiple eigenvectors at once--as solutions to a sequence of Raleigh-quotient-type optimization problems, where the next optimization problem is performed on a subspace which ensures its solution is orthogonal to all previous solutions. Mirroring the rest of this work, we consider eigenvalues in decreasing order, namely \(_{1}.._{N} 0\).

**Result 12**.: _Let \(\) be a positive semi-definite matrix. Then_

\[_{i}=_{^{N}\{0\}\\ ^{T}_{j}=0, j<i}^{T} }{^{T}}\] (101)

_where the eigenvectors \(_{i}\) are the respective maximizers._

Proof.: Horn and Johnson , Theorem 4.2.2, modified appropriately to handle eigenvalues in decreasing order, and where we choose \(i_{1},..,i_{k}=1,..,N-i+1\). 

We will prove the main claim below in two stages: first, on representer weight space \(R_{k,}\), then on the full RKHS \(H_{k}\). We begin with the former.

**Lemma 13**.: _We have_

\[u^{(i)}()=*{arg\,max}_{u R_{k,}}_{i=1 }^{N}u(x_{i})^{2}:\|u\|_{H_{k}}=1, u,u^{(j)}_{H_{k}} =0, j<i}.\] (102)

Proof.: Define \(_{j}=}^{-1/2}_{j}\), where we recall that \(_{j}\) are the respective eigenvectors of the kernel matrix. From the Result 12 form of the Courant-Fischer Min-Max Theorem, the reproducing property, and the explicit form of the RKHS inner product on \(R_{k,}\) in the basis defined by the canonical basis functions, we can conclude that

\[_{i}=_{^{N}\{\}\\ ^{T}_{j}=0, j<i}^{T}} _{}}{^{T}}=_{ ^{N}\{0\}\\ ^{T}}_{}_{j}=0, j<i} ^{T}}_{}^{2}}{^{T}}_{}}\] (103)\[=_{^{N}\{ \}\\ ^{T}_{}_{j}=0, j<i}_{}\|_{2}^{2}}{^{T}_{ }}=_{^{N} \{\}\\ ^{T}_{}_{j}=0, j<i}^{N}w_{i}k(x_{i},)\|_{2}^{2}}{\|_{i=1}^{ N}w_{i}k(x_{i},)\|_{H_{k}}^{2}}\] (104) \[=_{u R_{k,}\{0\}\\  u,u^{(j)}_{H_{k}}=0, j<i})\|_{2}^{2}}{\|u\|_{H_{k}}^{2}}=_{u R_{k,}\\ \|u\|_{H_{k}}=1\\  u,u^{(j)}_{H_{k}}=0, j<i}_{i=1}^{N}u(x _{i})^{2}\] (105)

which gives the claim. 

Next, we use a Representer-Theorem-type orthogonality argument to extend the optimization problem to all of \(H_{k}\). The argument will rely heavily on the Projection Theorem for Hilbert spaces: for an overview of the setting, see Lang , Chapter 5.

**Proposition 14**.: _We have_

\[u^{(i)}()=*{arg\,max}_{u H_{k}}_{i=1}^{N}u (x_{i})^{2}:\|u\|_{H_{k}}=1, u,u^{(j)}_{H_{k}}=0,  j<i}.\] (106)

Proof.: Let \(H_{k}^{(i)}\) be the orthogonal complement of the finite-dimensional subspace \(\{u^{(j)},j=1,..,i-1\}\) in \(H_{k}\) and note by feasibility that \(u^{(i)} H_{k}^{(i)}\). Consider the decomposition of \(u^{(i)}\) into its projection onto an orthogonal complement with respect to the finite-dimensional subspace \(H_{k}^{(i)} R_{k,}\), namely

\[u^{(i)}()=u^{}()+u^{}()\] (107)

where this notation is defined as \(u^{}=_{H_{k}^{(i)} R_{k,}}u^{(i)}\) and \(u^{}=u^{(i)}-u^{}\). Observe that

\[u^{}(x_{i})= u^{},k(x_{i},)_{H_{k}}= _{H_{k}^{(i)}}u^{},k(x_{i},)_{H_{k}}=  u^{},_{H_{k}^{(i)}}k(x_{i},)_{H_{k }}=0\] (108)

where the first equality follows from the reproducing property, the second equality follows because \(u^{} H_{k}^{(i)}\), the third equality follows because the projection is orthogonal and therefore self-adjoint, and last equality follows since \(u^{}\) by definition lives in the orthogonal complement of \(H_{k}^{(i)} R_{k,}\) within \(H_{k}^{(i)}\), with the subspace inner product. Moreover, by the Projection Theorem and the fact that \(H_{k}^{(i)}\) inherits its norm from \(H_{k}\), note that

\[\|u^{(i)}\|_{H_{k}}^{2}=\|u^{}\|_{H_{k}}^{2}+\|u^{}\|_{H_{k}}^{2}.\] (109)

We now argue by contradiction: suppose that \(u^{} 0\). This means that \(\|u^{}\|_{H_{k}}>0\). From the above, we have

\[_{i=1}^{N}u^{(i)}(x_{i})^{2}=_{i=1}^{N}u^{}(x_{i})^{2} \|u^{}\|_{H_{k}}<\|u^{(i)}\|_{H_{k}}.\] (110)

Define the function

\[u^{}()=\|_{H_{k}}}{\|u^{}\|_{H_{k}}}u^{ }()\] (111)

and note that \(\|u^{}\|=1\), and \( u^{},u^{(j)}=0\) for all \(j<i\) since \(u^{} H_{k}^{(i)}\), which makes \(u^{}\) a feasible solution to the optimization problem considered. At the same time, it satisfies

\[_{i=1}^{N}u^{}(x_{i})^{2}=\|_{H_{k}}^{2}}{\|u^{ }\|_{H_{k}}^{2}}_{i=1}^{N}u^{}(x_{i})^{2}>_{i=1}^{ N}u^{}(x_{i})^{2}=_{i=1}^{N}u^{(i)}(x_{i})^{2}\] (112)

which shows that \(u^{(i)}\), contrary to its definition, is not actually optimal--contradiction. We conclude that \(u^{}=0\), which means that \(u^{(i)} H_{k}^{(i)} R_{k,}\), and the claim follows from Lemma 13. 

This means that the top eigenvalues can be understood as the largest possible squared sums of canonical basis functions evaluated at the data, subject to RKHS-norm constraints. In situations where \(k(x,)\) is continuous, non-negative, and decays as one moves away from the data, it is intuitively clear that such sums will be maximized by placing weight on canonical basis functions which cover as much of the data as possible. This mirrors the RKHS view of kernel principal component analysis.