# Streaming Detection of Queried Event Start

Cristobal Eyzaguirre

Stanford University

&Eric Tang

Stanford University

&Shyamal Buch

Stanford University

Adrien Gaidon

Stanford University

&Jiajun Wu

Stanford University

&Juan Carlos Niebles

Stanford University

###### Abstract

Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding--Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate four vision-language backbones and three adapter architectures in both short-clip and untrimmed video settings.

## 1 Introduction

The ubiquity of embodied vision applications, such as robotics , autonomous driving , and augmented reality , highlights the need for methods that can _detect the occurrence of events with low latency in untrimmed and egocentric video streams_. Despite significant strides made in video understanding, a review of the existing literature reveals a notable gap: most current methods are designed for batch processing or adopt windowed approaches that result in redundant computation when new frames are considered. These approaches are effective in addressing existing benchmark tasks, but they fall short in practical, real-time applications due to the high computational overhead required in processing additional new frames and their limited context.

Some prior work has attempted to bridge this gap by extending traditional offline action recognition  and detection  tasks to an online setting . In particular, online detection of action start (ODAS)  emphasizes low-latency1 detection of when an action from a predefined list of classes begins in a streaming video starts. ODAS captures the urgency of the detection task, but it cannot assess settings where the user wants to specify more complex event queries beyond the scope of the predefined classes, nor do existing benchmarks for ODAS focus on egocentric settings, which are important for embodied applications. Consequently, approaches trained on existing datasets for online action detection may be constrained by the limited range of events they are designed to recognize, reducing their applicability to more diverse or unforeseen scenarios.

Natural language, in contrast, enables users to flexibly specify complex events. However, traditional tasks for this kind of multimodal setting, such as temporal localization with language [11; 3], are typically offline, requiring full observation of the complex event (and potentially, of the video as a whole [12; 13]) before providing the output detection. Thus, models for this task are not suitable for deployment in online settings where time sensitivity is paramount, such as those shown in Figure 1. This gap underscores the need for new methods that enable low-latency, real-time detection of complex events specified through natural language in untrimmed and egocentric video streams, as well as datasets to train and benchmark on.

To this end, we propose a novel task at the unique intersection of online and multimodal video understanding: _Streaming Detection of Queried Event Start_ (SDQES). The goal of SDQES is to detect the start of a complex event, described by a natural language query or description, with high accuracy and low latency, with a particular focus on egocentric video streams in embodied applications. To support this, we present a new benchmark, EgoSDQES, leveraging annotations from the comprehensive Ego4D dataset . This task synthesizes three significant challenges. First, it operates under a streaming framework where models access only past video frames without future data, such that the model will need to use precursor visual cues to provide timely detection outputs. Second, SDQES demands a multimodal approach, incorporating language queries that require a profound understanding of the video's content, which means that models cannot rely on a small set of cues to distinguish a closed vocabulary of atomic actions. Lastly, the task also aims to enable progress on applications with egocentric video inputs, which often involve complex issues like variable camera angles and motion blur that effective streaming systems must learn to address.

We propose baseline methods that extend existing vision language foundation models by adding adapters to enable online applications with real-time outputs on untrimmed videos with constant time per additional frame. This approach leverages the pretraining of vision language foundation models for parameter-efficient adaptation and transfer to the task of event detection in video streams. By adapting foundation models into an efficient streaming video architecture, we combine the strengths of massively diverse vision pretraining sets with the specific requirements of real-time video processing.

In sum, we make three contributions. First, we formulate Streaming Detection of Queried Event Start (SDQES), a novel task for online multimodal video understanding representing a unique intersection of challenges for event detection models. Second, we construct a new benchmark based on the existing large-scale egocentric video dataset. We propose metrics suited for measuring progress on this streaming multimodal task. Third, we propose a mechanism to adapt existing pretrained vision foundation models to handle long streaming videos efficiently. We evaluate multiple combinations of vision backbones and adapter architectures on both short clips and extremely long videos.

Figure 1: **Overview of our proposed SDQES task**. The goal of streaming detection of queried event start (SDQES) is for a system to detect the _start_ of a complex event, described by _natural language_, with low latency from a _streaming_ video input. This task is a novel intersection of multimodal event and online/streaming video understanding benchmarks. It is intended to encourage the design of new streaming multimodal models for challenging _egocentric_ or embodied settings (_e.g._, assistive robotics, augmented reality) where time-sensitivity is a key concern for safety, accessibility, or convenience.

## 2 Related Work

Our proposed streaming detection of queried event start (SDQES) task is a unique intersection of video understanding areas that have not been explored by prior work, which we summarize below.

**Action Recognition and Detection in Videos.** The goal of action recognition systems is to output the action or activity present in an input video clip [14; 15]. While in recent years the focus has broadened to settings with untrimmed videos, these tasks are often designed with a fixed vocabulary of action classes [16; 17], and models have been primarily developed for offline usage where the model has access to the full video before outputting predictions [18; 19; 20; 21; 4]. This limits their immediate efficacy in online or streaming contexts, or in settings where a more flexible model is required to handle open-vocabulary complex event descriptions.

**Online Detection of Action Start (ODAS).** To address the limitations of offline tasks, Online Detection of Action Start was introduced , which built upon prior work in the field of early action recognition [22; 23; 24] and online action anticipation [7; 25]. Given an untrimmed video stream input, the goal of ODAS [9; 10] is to detect the _start_ of an action, from a set of pre-defined action classes [17; 16]. This is intended to also be useful in settings where having low total latency is paramount. On the other hand, in our proposed task of SDQES, the events are described by open-vocabulary natural language, an increased challenge for video understanding models that require the design of different technical approaches and appropriate evaluation protocols.

**Action Anticipation.** Like ODAS and SDQES, action anticipation [26; 27; 28; 29; 30; 31] involves the timely prediction of events in a video sequence from a predetermined set of action classes. However, unlike traditional ODAS, action anticipation predicts the action class for a frame in the future instead of focusing on the present frame. These models typically do not predict a "background class," which is a distinguishing feature of SDQES and ODAS. Most of them are traditionally offline, processing pre-recorded video data rather than streaming video, whereas SDQES is designed for online, real-time detection. As in ODAS, the output of action anticipation is a classification to a predetermined set of classes, rather than the start of an event specified in language. To our knowledge, no prior work has combined natural language event specification with online prediction.

**Video Understanding with Language.** There is significant work on video event understanding with language across datasets, models, and tasks [32; 33; 34; 35; 36; 37; 38; 39]. The most related to our work is the task of event localization with language [40; 34; 11; 13], where the goal is to take a language query and untrimmed video as input and to provide the localization of the queried event in the video. Others have considered anticipating which of two possible events is more likely given videos and additional dialogue transcriptions , with connections to commonsense reasoning . However, these are offline tasks or use auxiliary information to make predictions over a limited event space. Our work inherits the complexity of using natural language queries for events, while simultaneously extending to the challenging area of online/streaming video understanding.

**Egocentric Video Understanding.** Egocentric video data poses unique challenges relative to traditional video data often explored in video action understanding. Prior work [3; 1; 43; 44; 45], aiming to capture settings representative for assistive robotics, driving, and augmented reality, has

Figure 2: **Example videos and queries from our dataset EgoSDQES.**allowed access to rich collections of challenging egocentric visual data. Previous tasks in this domain have focused mainly on traditional event understanding, such as offline action recognition  or localization with language , so models developed for these tasks [12; 46; 47; 48; 49] are not directly suitable for online or streaming detection settings. Egocentric action anticipation [44; 30], has been constrained to specific atomic action vocabularies, which limits their effectiveness in settings where users may need to specify more complex events flexibly. We aim to address these limitations for the egocentric setting.

**Adaptation of Pretrained Vision Models for Video.** Due to the computational expense of fully fine-tuning video models, a recent line of work inspired by the use of adapter modules in NLP [50; 51] has focused on parameter-efficient fine-tuning of image models, in particular the CLIP vision encoder , for offline video tasks [53; 54; 55; 56]. Notably, ST-Adapter  and AIM  achieve strong results in action recognition by learning spatio-temporal adapter modules over a CLIP backbone. More recently, another line of work has focused on two-channel models that utilize intermediate CLIP features in order to avoid backpropagation through frozen ViT parameters [57; 58; 59]. Our proposed model builds upon these paradigms for efficiently transferring representations for video understanding, and further extends them for effectively handling the online video setting.

## 3 SDQES Task: Formulation and Metrics

### Formulation and Goal

Given an input video stream \(\) and an event query in natural language \(\), the goal of SDQES is to accurately provide the temporal location where the described event starts with low latency. Let \(^{(i)}_{}=\{_{1},_{2},_{i}\}\) be a streaming input video sequence of frames up to the current frame \(_{i}\) at time \(i\), and \(t_{s}\) the start time of the queried event in the video stream. A model \(M\) for SDQES is

\[M(_{},q) t_{},\] (1)

where the goal is to output a _high accuracy_ prediction of event start (_i.e._, output time \(t_{}=t_{s}\)) with low latency. Since the ground truth start time \(t_{s}\) is not known to the model in advance and inputs are processed sequentially, we do not restrict the model to a single prediction. Instead, the model may output a set of prior predictions \(t_{}<t_{s}\). Thus, an additional goal is one of _high precision_, where such false positive outputs by model \(M\) are minimal.

### Metrics: Accuracy with Low Latency

**Existing Protocols.** The seminal prior work in early detection, MMED , reported a comprehensive evaluation protocol including FPR, accuracy, and timeliness metrics. However, these do not provide a complete picture of the actual model performance for SDQES. Specifically, the FPR and accuracy metrics are measured at the frame level and may not be representative of the model's performance on the actual task. Additionally, the metric for timeliness assumes frame-perfect annotations, where, in practice, there can be reasonable disagreement about when an event starts. Later work in Online Detection of Action Start (ODAS) [19; 9] instead adopts a single evaluation metric: p-mAP. This metric addresses the noise in annotations by considering action starts as correct when they are contained within a temporal window. However, p-mAP disregards temporal order; thus, it is not strictly online. More discussion in the supplementary material.

**Streaming Recall.** Our key accuracy metric is _streaming recall_ of event start. SR builds on p-mAP and extends the definition to account for the greater ambiguity present in SDQES by considering the _first_\(k\) predictions. Following from Eq. 1, let \(P_{M}^{(k)}=\{t_{_{1}},,t_{_{k}}\}\) denote a set of the first (up to) \(k\) predictions \(t_{}\) generated by a model \(M(_{},q)\), and let \(t_{s}\) denote the groundtruth start time of the event described by query \(q\). A model output set \(P_{M}^{(k)}\) is then "correct" if and only if

\[ t_{}^{} P_{M}^{(k)}:-anticipation t_{s}-t_{ }^{} latency,\] (2)

where \(anticipation\) and \(latency\) define the asymmetric temporal tolerance window. Since \(k\) represents the _first_ predictions in the set, Eq. 2 penalizes models with a high false positive rate, as such models would exhaust their \(k\) guesses of the event start early in the stream. By considering different values of \(k\) and tolerances, we can provide fine-grained measurements of a model's capabilities.

**Streaming Minimum Distance.** In addition to Streaming Recall, we propose a second metric that focuses on timeliness: _Streaming Minimum Distance_ (SMD). This metric measures the average error of the closest prediction made by the model \(M\) to the groundtruth \(t_{s}\). Given the set of predictions \(P_{M}\), we define the minimum distance as \(d_{}=_{t_{} P_{M}}|t_{s}-t_{}|\).

We report this metric as SMD@\(k\), which measures the average minimum distance across all queries with groundtruth start times \(t_{s}\) and across a model's first \(k\) predictions \(t_{}\) This metric is complementary to the streaming recall metric, providing a measure of the temporal accuracy of the model's predictions. More details of both metrics are provided in the supplementary material.

**Model Efficiency.** In addition to the proposed metrics that focus on model task performance and _observational latency_, we evaluate the computational efficiency of our models to assess their suitability for real-time applications across a range of metrics that reflect both computational resource usage and response speed. Our computational efficiency metrics include Parameter Count for assessing memory footprint, Multiply-Accumulate Operations (MACs) and Floating Point Operations (FLOPs) to quantify computational load, and Computation Latency to measure model processing time per frame.

## 4 Data Collection and Annotation

As our task is novel, no dataset has been previously created for it. Instead, we repurpose publicly available datasets with temporally grounded language annotations. Specifically, we focus on Ego4D , a recent large-scale dataset of long videos from an egocentric perspective. This dataset is challenging because it contains diverse activities, viewpoints, and camera motion, making it ideal for evaluating the robustness of our method to challenging real-world scenarios. Furthermore, it is easily accessible under the Ego4D license. Additionally, we demonstrate how the dataset can be extended with other video sources by applying the same pipeline to the videos and annotations from the EgoExoLearn dataset . Details specific to this other dataset are provided in the supplementary material.

Our innovative data generation pipeline employs Large Language Models (LLMs) for generation and several key filtering steps, with stages illustrated in Fig. 2(a) and 2(b). Figure 2(a) illustrates the process of how the LLM contributes to modifying a single event, and generates relevant metadata for subsequent filtering. Figure 2(b) shows the flow of the existing temporal annotations through the successive filtering stages, some of which use outputs from the LLM. We describe each of these below.

**Generation Pipeline.** Specifically, we start with Ego4D's temporally grounded _Moments_ (Action Localization) and _NLQ_ (Natural Language Queries) annotations, as well as dense video captions (_Narrations_). For each annotation, the LLM extracts the event, as this is the key information needed. For instance, from the query "Where did I last leave the box?", we extract the event "leave box." It then confirms the event's reflection in narrations to ensure contextual accuracy and groundedness in the video content, a necessary step since the queries must refer to visible events.

Next, the LLM refers to the narrations to verify if the extracted event has previously occurred in the video. This prior check ensures that previous occurrences can be accurately identified, avoiding misclassification due to missing narrations. The LLM is then prompted to generate an original

Figure 3: **Dataset generation pipeline. Left: we show the generation pipeline steps for an example video with dense captions. Right: Sankey diagram illustrates the flow of data from Ego4D through the various filtering stages. _Asterisk_ (\(*\)) encodes a filter based on query specificity.**streaming query for the current instance, following the template of setting a reminder to do something when the event begins. We prompt the model to disambiguate so that the query cannot refer to another prior event if one was detected. Finally, we verify the specificity of the generated query to the annotated event instance using the LLM to differentiate between multiple instances of the same event.

The crucial filtering stage ensures data quality and relevance. We eliminate annotations based on fixed rules using metadata to quickly discard obviously irrelevant annotations. We discard annotations that cannot be verified within the LLM's context window (8k tokens). This is necessary to avoid truncation, which would compromise the LLM's capacity to identify if the event has occurred before.

Finally, there is a bifurcation in the filtering process. For events occurring for the first time, we use the generated annotation as is, ensuring that new events are accurately captured and added to the dataset. If the event has occurred before, we add a final filter stage to check if the generated query is unambiguous using the LLM. This specificity check is key because the dataset is intended for detecting events in a streaming video, where future video content is unknown. Thus, we ensure that queries are specific and identifiable without relying on context from future portions of the video.

**EgoSDQES.** We run the full pipeline using GPT-4  as the LLM, resulting in 12,767 annotations for over 740 hours of video. We split the videos and annotations following the original Ego4D train/val split, resulting in 1,331 training and 442 validation videos. These videos are untrimmed and contain at least one streaming query. Table 1 compares our dataset with other egocentric video datasets. Our dataset is larger than NLQ, Moments, and EgoSchema in terms of both the number of videos and annotations. Notably, our videos have a much longer duration (1,553 seconds on average) compared to other datasets, even those used for ODAS, which are limited to 180 seconds.

Note that because of the filter that discards annotations with scripts that do not fit in the context (and pre-existing biases in Ego4D annotations), our final dataset mostly contains queries that refer to events in the first 30 minutes of a video (see Figure 3(b)). Nevertheless, our dataset videos are substantially longer, so our annotations occur further into the videos compared to existing datasets.

Figure 3(c) shows the most common words in the generated queries, illustrating their diversity. Common words such as "next," "time," "remind," and "start" reflect the nature of the task, which involves setting reminders for future events or actions related to common objects and situations.

Datasheets, data cards, and visualizations of the annotations are in the supplementary material.

   Dataset & Task & Video Source & View & \#Videos & \#Annotations & Video duration (s) \\   Thumos 14 \\ ActivityNet \\  } & ODAS & YouTube & Allocentric & 413 & 6365 & 180 \\  & ODAS & YouTube & Allocentric & 15K & 22.6K & 180 \\  NLQ & Temporal localization & Ego4D & Egocentric & 1046 & 17052 & 492 \\ Moments & Temporal localization & Ego4D & Egocentric & 1189 & 19151 & 472 \\ EgoSchema & Video QA & Ego4D & Egocentric & 5063 & 5063 & 180 \\  EgoSDQES & SDQES & Ego4D & Egocentric & 1773 & 12767 & 1553 \\   

Table 1: **Comparison of various related datasets.**

Figure 4: **Dataset statistics. Left: Event duration in seconds. Center: Distribution of Event Start with respect to video start. Right: Word Cloud of the query generations.**

## 5 Baseline Approaches

We present adapter-based baseline models that enable efficient adaptation of vision-language models for streaming video input, allowing for online video modeling. We provide an overview of their model architectures and training objectives. The implementation details are in the supplementary material. Our models serve as initial explorations into this new problem setting, laying the groundwork for future work to build upon. Figure 5 shows an overview of our architecture.

### Streaming-Adapters

Given a pre-trained image model \(\) and a set of videos, our objective is to bridge the modality gap between the image-level model pretraining and the spatio-temporal video task. We adapt the image model into a spatio-temporal video model \(*\), while reusing as many parameters from \(\) as possible.

A common strategy for both adapting image models to video processing tasks [54; 56; 55; 53], as well as for developing video models from the ground up [62; 63], involves incorporating temporal aggregation layers into the pre-existing ViT architecture (Figure 4(a)). This enables the model to reason over successive video frames by adding new temporal-specific layers between the ViT's spatial layers for temporal aggregation across image patches.

A key consideration is the choice of architecture for the temporal aggregation layer, as this fundamentally governs how the model's computational requirements scale with the input sequence length (_i.e._, video duration). For streaming video-language tasks, it is desirable to use architectures that can efficiently process new frames with a constant computational cost, rather than requiring re-computation over the entire sequence. To this end, we explore different architectural choices suitable for streaming, such as recurrent models and 1D convolutional models, which can incrementally update their representations as new frames arrive. Specifically, we use them to aggregate temporal information across timesteps for each patch-wise tubelet with shape, as shown in Figure 4(b).

Importantly, the adapters operate in a reduced dimension \(d^{}<d\) to reduce computational cost. Also note that architectures that use convolutions require zero-padding to produce an equal-sized output. We pad the sequence on the left to ensure that no information from future frames is leaked into the past. Additional baseline architecture details are provided in the supplementary material.

### Training and Loss

**Data Sampling.** One key issue for detecting event starts, as also observed in prior work on ODAS , is the significant imbalance between positive and negative samples in the training data. As videos are long and events are infrequent, there are many more frames that are not event starts than those that are. To mitigate this impact, we reformulate the training approach by leveraging denser supervision

Figure 5: **Overview of the Streaming-Adapter. (a) Intervened Block: the lock icon denotes frozen parameters - only adapter parameters are trained. Temporal adapters operate on a reduced dimension for efficiency. (b) Adapter Internals: the adapter operates over the temporal dimension and consists of temporal aggregation layers. The final state of the model is stored for when the next frame arrives.**signals provided by a dense labeling task. In this task, we pair sampled windows of \(w_{s}\) frames \(f_{i-w_{s}+1},,f_{i}\) (along with query \(q\)) with ground truth labels \(y_{i-w_{s}+1},,y_{i}\), where each label \(y\) is true if the associated frame belongs to the region of the video corresponding to query \(q\). For further details, please refer to the supplementary material.

**Loss Formulation.** Our model employs a cross-entropy loss function, which operates over the cosine similarities between the frame embeddings \(_{f_{i}}\) obtained from the video encoder and the query embedding \(_{q}\) from the corresponding language encoder. The goal is to maximize the similarity between the frame embeddings that correspond to the specific event described by the query and the query embedding itself. For a set of frame embeddings \(\{_{f_{1}},,_{f_{N}}\}\) and the query embedding \(_{q}\), the cosine similarity \(s_{i}\) is given by \(s_{i}=_{f_{i}}_{q}}{\|_{f_{i}}\|\|_{q}\|}\). We then apply the sigmoid function to these similarities to model the probability \(p_{i}\) of each frame embedding corresponding to the query. The binary cross-entropy loss \(\) is computed as \(=-_{i=1}^{N}y_{i}(p_{i})\), where \(y_{i}\) is the ground truth label indicating whether frame \(f_{i}\) is relevant to the query \(q\). Finally, because events typically occur for only a fraction of the video (see Figure 3(a)), there are many more negative frames than positive ones. Therefore, we also apply a weighting scheme to the loss function during training.

## 6 Experiments

We evaluate a variety of combinations of Streaming Adapters and dual-encoder vision-language models, including the current state-of-the-art (SOTA) egocentric video encoder.

In no particular order, we consider adapters based on: 1) 1D Convolutions (which we refer to as ST-Adapter as they closely resemble the adapter in ); 2) Quasi-Recurrent Neural Networks , a more computationally efficient gated RNN (referred to as QR-Adapter); and 3) RetNet , a close analog to the standard Transformer architecture  that allows for low-cost inference by linearizing the attention mechanism (RN-Adapter in our experiments). Additionally, we also consider a standard non-temporal MLP adapter, refered to simply as Adapter in the experiments.

In this study, we evaluate several vision-language backbone models, including CLIP as detailed in the work by Radford et al. . Additionally, we extend our evaluation to incorporate vision-language models that have been pretrained using egocentric video data. It is important to highlight that our focus is primarily on dual-encoder models. This design choice is based on the efficiency consideration that modeling the query and the video with a dual-encoder does not require reprocessing the video for each new query. Among the models assessed, we include the well-known EgoVLP  and LaViLA , and the current state-of-the-art dual encoder model EgoVideo .

### Experimental Setup

We initialize backbone weights to the best-performing pretrained models available and then freeze them. For EgoVLP, LaViLa, and EgoVideo, we modify each architecture to process a single frame at a time, diverging from their original multi-frame input configurations. Unless otherwise specified, we use the _Base_ variants of all encoders. Adapters are added at the beginning of each block and before the MLP layers, with weights initialized to approximate an identity operation at the start of training. Dimensions are selected such that all adapters have roughly the same amount of hyperparameters.

All models are trained on 60-frame windows sampled at 1 frame per second (FPS), except for RetNet-based adapters, which use 30 frames to ensure stability, and models based on the EgoVideo backbone, which are limited to 30 frames due to memory constraints. We predict action starts by thresholding the cosine similarities between frame embeddings and the query embedding, with the Streaming Recall metric's anticipation and latency set to 5 and 10 seconds, respectively. Additional details are available in the supplementary material.

**Efficiency and Latency Measurement.** To assess model efficiency, we include metrics for both modified single-frame backbones (e.g., EgoVLP backbone) and unmodified versions using a sliding window of four frames. We measure computation latency by running each model on a full video and recording the total elapsed time, with results averaged over three runs to ensure consistency. Latency values reflect hardware and implementation specifics and may vary under different conditions, such as with different accelerators or environments.

### Task Performance Results

In our experimental evaluation, we present findings on the performance of various adapter models integrated with dual-encoder vision-language architectures, using video clips of 1-minute and 5-minute duration captured at 1 frame per sec. Table 2 summarizes these findings.

**Impact of New Dataset on Task Performance.** Our results demonstrate a clear improvement in model performance when trained with our data. We include a zero-shot single-frame baseline that uses CLIP  to illustrate the capabilities of the dual-encoder without additional training. Across all tested backbones, every adapter model outperformed the zero-shot CLIP baseline. This improvement underscores the effectiveness of training on our generated dataset.

**Temporal Adaptation with QR-Adapter.** We find that our QR-Adapter-based model consistently outperforms the zero-shot baseline and the standard non-temporal Adapter across all backbones. This supports our hypothesis that temporal adaptation, as introduced by QR-Adapter, is beneficial for SDQES.

**Alternative Streaming Temporal Adapters.** We further this analysis by including additional formulations for temporal streaming adapters based on alternative architectures. We find that, while the 1D convolution-based ST-Adapter showed limited success, the linear-attention-based RN-Adapter showed comparable performance to the QR-Adapter, supporting the claim that more complex temporal modeling capabilities are required for SDQES.

**Extension to Untrimmed Video Results.** We also assess model performance on full-length videos up to two hours long: Figure 5(a) shows the relationship between the number of predictions allowed (modulated by the K-value) and the mean Streaming Recall. This trade-off between recall and the volume of predictions is particularly relevant for longer videos, where the chance of capturing relevant events increases with more predictions. Figure 5(b) details the precision of predictions, showing that higher K-values are more likely to include predictions that are closer to the ground truth annotations.

    &  &  \\  Method & SR@1\(\) & SMD@1\(\) & SR@1\(\) & SR@2\(\) & SR@3\(\) & SMD@1\(\) & SMD@2\(\) & SMD@3\(\) \\  Zero-Shot CLIP & 16.9 & 24.3 & 7.9 & 11.6 & 14.0 & 151.3 & 140.3 & 132.6 \\  CLIP + Adapter & 19.5 & 23.5 & 8.9 & 13.7 & 17.2 & 135.7 & 121.7 & 113.3 \\ CLIP + QR-Adapter & 23.7 & 21.2 & 9.1 & 14.1 & 18.7 & 136.7 & 117.7 & 102.9 \\  LaViLa + Adapter & 19.5 & 23.4 & 8.7 & 13.0 & 16.2 & 163.4 & 151.7 & 144.0 \\ LaViLa + QR-Adapter & 29.1 & 18.1 & 9.3 & 12.8 & 16.5 & 132.1 & 115.9 & 104.1 \\  EgoVLP + Adapter & 18.1 & 24.0 & 8.4 & 13.0 & 16.7 & 160.8 & 148.7 & 141.5 \\ EgoVLP + QR-Adapter & 28.8 & 17.7 & 9.7 & 14.1 & 17.9 & 133.1 & 120.8 & 110.9 \\ EgoVLP + ST-Adapter & 17.4 & 30.5 & 8.6 & 13.4 & 17.0 & 170.7 & 161.4 & 155.6 \\ EgoVLP + RN-Adapter & 25.7 & 21.3 & 9.4 & 15.4 & 20.1 & 174.8 & 159.0 & 149.2 \\  EgoVideo + Adapter & 27.1 & 28.8 & 16.0 & 21.8 & 26.4 & 148.5 & 138.3 & 131.2 \\   

Table 2: **Baseline Results** for CLIP , EgoVLP , and LaViLa  fine tuned with a variety of adapters.

Figure 6: **Full length video results.**

### Latency and Model Efficiency

Latency is critical for real-time applications such as assistive technologies, human-computer interaction, and autonomous systems. Our models are designed for efficiency, featuring minimal parameters, low FLOPs, and reduced latency to meet these demands. Table 3 presents the memory and computation requirements of different adapter models. The context-less backbone architecture is the most memory-efficient and fastest option. However, all our adapters are highly efficient, introducing only about a 13% increase in operations compared to the backbone alone. Notably, our temporal adapters--ST-Adapter, QRNN-Adapter, and RetNet-Adapter--approach the efficiency of the non-temporal vanilla adapter.

In contrast, the Sliding Window variant consumes four times the computational resources and is limited to only four seconds of context. Regarding latency, all temporal adapters, except for the RetNet-Adapter, exhibit computation times comparable to the non-temporal adapter, adding approximately 5% to the total computation time. The higher latency of the RetNet-Adapter is due to the absence of an optimized CUDA implementation, whereas the QRNN-based adapters benefit from a dedicated CUDA kernel, and the ST-Adapter leverages PyTorch's efficient convolution operations. These results demonstrate that our proposed models effectively balance performance and efficiency, making them suitable for real-time applications requiring both rapid response and accurate temporal modeling.

## 7 Conclusion

We have introduced Streaming Detection of Queried Event Start (SDQES), a novel task designed to push the boundaries of online multimodal video understanding, with a specific focus on the challenges presented by egocentric video streams. The task synthesizes the unique complexities of detecting complex events in a streaming framework, requiring both high accuracy and low latency. Our contributions include the formulation of SDQES, which demands that the models operate without future data, relying instead on past video frames and language cues to predict events as they unfold. We have also developed a benchmark, leveraging existing egocentric videos and annotations, and proposed metrics tailored for evaluating progress in this streaming setting. We propose adapter-based baseline approaches to serve as a starting point. Our temporal adapter models highlight the benefits of incorporating temporal adaptation, as introduced by QR-Adapter, for this task. Importantly, these models achieve enhanced performance while maintaining low latency, making them suitable for real-time applications where rapid response is essential.

**Limitations.** Our dataset inherits any errors or omissions present in the Ego4D narrations that were used to generate it. Additionally, the narrations may lack important details, violating assumptions about their quality. As is the case for ODAS, the inherent ambiguity in defining precise start and end points of actions remains a challenge.

    &  &  & \\  Model & \# parameters & Multiply Adds & Floating Point Operations & Latency \\  EgoVLP backbone & 180.92 M & 7.85 TMACs & 15.7 Tflops & 1.68 s \\  EgoVLP + Adapter & +7.9\% & +12.7\% & +12.8\% & +15.5\% \\ EgoVLP + ST Adapter & +7.9\% & +12.7\% & +12.8\% & +18.5\% \\ EgoVLP + QRNN Adapter & +7.5\% & +12.0\% & +12.2\% & +21.5\% \\ EgoVLP + RetNet Adapter & +7.6\% & +15.2\% & +15.3\% & +99.5\% \\  EgoVLP Sliding Window & +0.1\% & +298.5\% & +298.8\% & +260.2\% \\   

Table 3: **Model Efficiency.** This table compares the number of model parameters along with the computational cost of processing a single frame for each listed architecture. For computational latency we report both the total number of operations (Multiply-Accumulates operations and floating point operations) along with the processing time taken on a single V100 graphics processor to run on a 5 minute and 50 seconds long video taken from the dataset: \(video\_uid=dd08bc58-b614-4ba7-b883-a213560621dd\).