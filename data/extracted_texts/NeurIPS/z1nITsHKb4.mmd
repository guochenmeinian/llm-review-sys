# MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations

Ruiyuan Lyu\({}^{1,2*}\), Jingli Lin\({}^{1,3,7*}\), Tai Wang\({}^{1*}\), Shuai Yang\({}^{1,4*}\), Xiaohan Mao\({}^{1,3}\), Yilun Chen\({}^{1}\),

**Runsen Xu\({}^{1,5}\), Haifeng Huang\({}^{1,4}\), Chenming Zhu\({}^{1,6}\), Dahua Lin\({}^{1,5,8}\), Jiangmiao Pang\({}^{1}\)**

\({}^{1}\)Shanghai AI Laboratory, \({}^{2}\)Tsinghua University, \({}^{3}\)Shanghai Jiao Tong University,

\({}^{4}\)Zhejiang University, \({}^{5}\)The Chinese University of Hong Kong,

\({}^{6}\)The University of Hong Kong,

\({}^{7}\)Zhiyuan College, Shanghai Jiao Tong University, \({}^{8}\)CPII under InnoHK

\({}^{*}\)Equal contribution \({}^{}\)Corresponding author

###### Abstract

With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.

Figure 1: MMScan provides the largest ever multi-modal 3D scene dataset with 6.9M hierarchical grounded language annotations, covering holistic aspects on both object- and region-level.

Introduction

Multi-modal 3D perception is a crucial capability needed by embodied agents and has been extensively studied [12; 6; 28; 15; 50]. As Large Language Models (LLMs) have had great success in recent years, integrating them to build 3D-LLMs is an inevitable trend. However, previous 3D-LLMs [24; 50; 23; 41] can only access object-level datasets  or existing limited multi-modal scene datasets [12; 6; 35; 7], thus being constrained to object-level understanding and the recognition of spatial relationships between objects. In contrast, our 3D world has complex hierarchies and rich contexts, suggesting that current 3D-LLMs do not meet our expectations. It urges us to build a comprehensive multi-modal 3D dataset and benchmark to improve the training and evaluation of these models.

To address this problem, this paper aims to build a holistic multi-modal 3D dataset and benchmark at the scene level. Prior works have focused on specific tasks such as 3D visual grounding [12; 6] and question-answering [7; 35] through rule-based or free-form human labeling, resulting in annotations that are either limited to inter-object spatial relationships [6; 47] or influenced by annotators' biases. Recent efforts [57; 29] to expand 3D-language annotations using VLMs have improved scalability but fall short in ensuring correctness and comprehensiveness. Moreover, current annotations lack hierarchical scene structures with fine-grained grounding information, leading to inefficient training for 3D-LLMs and suboptimal instruction following performance.

In contrast to previous works, we introduce a top-down 3D-language annotation logic and present the largest ever multi-modal 3D scene dataset (Fig. 1), _MMScan_, featuring hierarchical language annotations grounded in different granularities of scene context. Constructed using VLM-initialized and human-corrected meta-annotations, the process systematically decomposes complex 3D scenes into region- and object-level instances for comprehensive spatial and attribute annotation. These meta-annotations comprise 1.4M captions over 5k existing real-scanned scenes, 109k objects, and 7.7k regions, forming the basis to produce samples for benchmarks and training.

Given these meta-annotations, we establish two multi-modal 3D perception benchmarks: visual grounding and question-answering. All the samples are generated following two streams, a single target and inter-target relationships and 5 sub-classes (Fig. 3) for different aspects, resulting in 1.28M and 1.76M samples on each benchmark, respectively, to evaluate the model's capabilities from various aspects. In addition, the retained correspondence information of meta-annotations allows us to seamlessly integrate them into scene-level captions. All these caption data, as well as benchmark samples, can serve as a valuable resource for training 3D grounding and large language models.

We evaluate representative baselines on our benchmarks and discuss emerging challenges. Specifically, the performance of visual grounding models is much lower than existing benchmarks, indicating the difficulty of understanding complex prompts that entangle comprehensive spatial and attribute understanding. Including the image modality to enhance semantic understanding and improving the selection of candidate objects are promising directions. For the question-answering benchmark, we observe unsatisfactory results of current 3D-LLMs on our benchmark and a significant improvement, up to 25.6% accuracy, using our data for instruction tuning. Furthermore, we leverage MMScan's captions to train grounding and 3D-LLM models, resulting in a 7.17% AP increase and state-of-the-art performance on existing visual grounding and question-answering benchmarks, more importantly, enabling a much better instruction following performance in the wild.

## 2 Related Work

**Multi-Modal 3D Scene Datasets.** Despite the availability of various 3D scene datasets ranging from the early SUN RGB-D  to more recent large-scale ones [21; 9; 46; 11; 42], there remains a scarcity of datasets with multi-modal annotations that focus on language-grounded 3D scene understanding. Predominantly, earlier efforts like ScanRefer , ReferIt3D , and ScanQA  have been centered on ScanNet, pioneering the way of human annotations and template-based generation. SQA3D  further emphasizes the role of "situation" in the context. As subsequent efforts, RIORefer  and ARKitSceneRefer  are similar to ScanRefer but focus on 3RScan and ARKitScene. However, most of them are still limited in amount and scene diversity. Recent initiatives began to pursue scaling up such 3D-text data pairs to push multi-modal 3D learning towards the next stage. For example, 3D-VisTA  generates scene descriptions from existing 3D vision-language tasks, templates, and GPT-3 to obtain ScanScribe for pre-training. EmbodiedScan  and SceneVerse (real& synthetic) either collect more scenes or annotate more objects, scaling up annotations to millions. However, since there are only object annotations in previous scene datasets, all these works lack explicit hierarchical information in 3D scenes, _i.e._, different granularities of grounding entities.

Furthermore, most works scale up the annotation without humans in the loop, making them not suitable to serve as benchmarks for 3D-LLMs. This paper addresses these gaps and introduces the largest ever multi-modal 3D dataset with comprehensive annotations for both training and benchmarks (Tab. 1).

**Language-Grounded 3D Scene Understanding.** Accompanying these datasets, algorithms for language-grounded 3D scene understanding also make rapid progress. Earlier works focused on crafting specialized models for individual tasks, and recent research has ventured into consolidating these tasks or delving into universal frameworks, capitalizing on the powerful capability of LLMs.

_Specialists for Conventional Tasks._ In the domain of language-grounded 3D scene understanding, traditional tasks encompass: 1) 3D visual grounding , which involves identifying 3D objects through 3D bounding boxes or segmentation masks using language cues; 2) 3D question-answering , emphasizing the generation of language-based responses; and 3) 3D dense captioning , highlighting the synthesis of descriptive language for 3D scenes. Early research was dedicated to refining individual frameworks  or modules  for these tasks, with some recent efforts exploring task unification  and pre-training strategies . Despite these advancements, existing models remain constrained by their task-specific design, hindering broader applicability.

_3D Multi-modal LLMs._ The integration of 3D encoders with LLMs to foster versatile multi-modal 3D intelligence represents an emerging paradigm. Initial efforts  have predominantly addressed object-level 3D understanding, leveraging ample 3D-text pairs and simpler configurations. Pioneering work in scene-level comprehension includes 3D-LLM , which utilizes pre-trained 2D VLM features, and incorporates positional embeddings and location tokens. Subsequent approaches such as Chat-3D  and LL3DA  have enabled object-centric interactions through pre-selection techniques. Embodied Generalist  underscores the significance of 3D comprehension, delving into the amalgamation of reasoning and action within multi-modal LLMs. Despite the swift evolution of 3D-LLMs, there is an absence of a high-quality, extensive multi-modal 3D scene dataset with hierarchical language annotations. Such a dataset is essential for effectively training these models and for a holistic assessment of their capabilities. This paper endeavors to fill this void, offering a foundational resource for training and evaluating 3D-LLMs.

## 3 Dataset

In this section, we present our approach to building a large-scale 3D scene dataset with hierarchical grounded language annotations. This involves raw data preparation, top-down meta-annotation generation, and extraction of visual grounding and QA task samples. Finally, we make statistics on our annotations and analyze their superiority over existing datasets.

### Data Collection & Processing

For constructing a multi-modal 3D dataset, we prioritize selecting a foundational 3D scene dataset with extensive, real-scanned sensor data to minimize the sim-to-real gap and facilitate VLMs' automatic 2D image annotation. In addition, extensive object annotations are important to start the annotations from the object level. We opt for EmbodiedScan, chosen for its comprehensive collection of open-source, real-scanned datasets and its detailed object annotations aided by SAM-assisted labeling tools. Additionally, its Sr3D-based  language annotations provide a solid base for spatial relationship descriptions among objects.

   Dataset & \#Scans & \#Language & \#Tokens & Correspondence & Focus & Annotation \\  ScanRefer  & 0.7k & 11k & 1.18M & Sent.-Obj. & Natural & Human \\ N3D  & 0.7k & 42k & 0.62M & Sent.-Obj. & Natural & Human \\ Sr3D  & 0.7k & 115k & 1.48M & Sent.-Obj. & OO-Space & Template \\ ScanO  & 0.8k & 41k & - & Sent.-Obj. & QA & AutoGen+Human \\ SOA3D  & 0.7k & 53.8k & - & Sent.-Obj. & Situated QA & Human \\ ScanScribe  & 1.2k & 278K & 18.49M & Sent.-Obj. & Description & GPT \\ Multi3DRefer  & 0.7k & 62K & 1.2M & Sent.-Multi-Obj. & Multi-Obj. & GPT+Human \\ EmbodiedScan  & 5.2k & 970k & - & Sent.-Obj. & OO-Space & Template \\ R10Refer  & 1.4k & 63.6k & 0.94M & Sent.-Obj. & Natural & Human \\ ARIKSceneRefer  & 1.6k & 15.6k & 0.22M & Sent.-Obj. & Natural & Human \\ MMScan (Ours) & 5.2k & 6.9M & 114M & PinseObj./Reg. & Holistic & GPT+Temp.+Human \\   

Table 1: Comparison with other multi-modal 3D real-scanned scene datasets. “Sent.”, “Obj.”, “Reg.”, “OO-Space” and “Temp.” refer to “Sentence”, “Objects”, “Regions”, “Object-Object Space” and “Template”. MMScan has significant superiority in both the number and quality of annotations.

### Meta-annotations

We employ a top-down logic to generate comprehensive, hierarchical annotations for scenes, which include non-overlapping captions for scene elements like objects and regions. These _meta-annotations_ are designed for broad information coverage and will be further processed to generate specific data samples for benchmarking in Sec. 3.3. We outline our top-down annotation approach and detail the object and region-level language annotations subsequently.

**Overview of Top-Down Logic.** We employ a top-down approach to holistically annotate scenes by segmenting them into regions and objects, potentially including object parts in the future. This method captures information at various levels of granularity. At each level, we solicit human or VLM descriptions of key properties and inter-target relationships, focusing on _spatial_ and _attribute_ understanding.1 In this way, we can obtain the scene captions with a hierarchical structure and holistic descriptions for each level. Based on this logic, we will first demonstrate the use of VLMs for object-level language annotation. For the region level, due to the lack of region annotations, we need to annotate regions in each scene and then annotate the key information for each region.

**Object-Level Language Annotation.** We annotate object-level captions based on the bounding boxes from EmbodiedScan. For each object, we establish its main properties, including spatial (geometric shape, pose) and attribute (category, appearance, material, state, functional use) understanding. We then use VLMs with the best image view to initiate descriptions, followed by human annotators refining these with a tailored UI (Fig. 2). Key factors influencing annotation quality are 1) optimal view selection for objects and 2) VLMs selection with appropriate multi-modal prompts for effective caption initialization.

For view selection, we first evaluate image quality by calculating the Laplacian kernel and exclude frames with a variance below 100 to ensure clarity. Next, we project the center and evenly sampled surface points of each object's 3D bounding box onto the image plane. The optimal image is chosen based on the object's center being within the central 25% area and maximizing surface point visibility.

To initialize captions with VLMs, we meticulously craft language prompts and systematically evaluate various visual prompts and VLMs to ensure high-quality descriptions. We find that providing a cropped object patch as a visual prompt

  Models & Free & Acc. & Ann. Ratio \\  GPT-4v  (w/o crop) & ✗ & 84.55\% & 87.28\% \\ GPT-4v  & ✗ & 86.97\% & 88.96\% \\ Open-VL-Max  & ✗ & 87.63\% & 91.30\% \\  Interml-M-XComposer  & ✓ & 83.63\% & 92.86\% \\ Interml-Clat v1.2  & ✓ & 85.75\% & 91.06\% \\ CogVLM  & ✓ & **89.51\%** & 85.21\% \\  

Table 2: VLMs on object-level captioning. “Acc.” and “Ann. Ratio” refer to the “accuracy” and “notation ratio” (some can reject annotation due to security mechanisms). We prioritize “accuracy” here and provide complementary results to achieve a high annotation ratio.

Figure 2: Object-level (top) and region-level (down) meta-annotation UI, pipeline, and examples.

yields slightly better results than showing the entire image with 3D bounding boxes. After testing several VLMs on their ability to accurately describe the object properties, we manually checked a subset of 200 objects and determined that CogVLM  and InternVL-Chat-V2  perform the best in granularity and accuracy (Tab. 2). Leveraging their complementary strengths, we use the outputs from these two models as initial annotations, which annotators can then modify and refine to enhance the comprehensiveness of the information. See more details on language prompts and additional considerations in the supplementary materials.

**Region Segmentation Annotation.** We've introduced a new UI (Fig. 2) for annotating different regions in each scene, offering a set of predefined categories, including living, study, resting, dining, cooking, bathing, storage, toilet, corridor, open area, others. Annotators are prompted to use 2D polygons to define regions in scenes displayed in a bird's eye view. They can tap on the BEV to access related views of nearby objects, improving annotation accuracy. We eventually amassed 7692 valid annotated regions, excluding "others" and "open area".

**Region-Level Language Annotation.** Based on these region annotations, we further annotate their language descriptions by adapting the object-level language annotation pipeline. Initially, VLMs generate captions from selected views, followed by human revision. For region view selection, we identify target objects and assess their visibility across views based on sharpness and surface points, similar to object view selection. A greedy algorithm is then used to select a minimal set of views to cover all objects. Given the distinct properties of regions, including object-object and object-region relationships, we develop a customized annotation structure and employ varied visual prompts for multi-view images to anchor objects with their identity in captions.

Specifically, we annotate the region for intrinsic property and inter-entity relationships, respectively. Intrinsic properties include location and function, spatial layout and dimensions, architectural elements (doors, windows, walls, floors, ceilings), decorative and soft fitting details, lighting design, color coordination, and style themes, all of which collectively characterize the region. For inter-entity relationships, we annotate 1) object-object relationships, including spatial relationships (based on Sr3D) and attribute relationships, and 2) object-region relationships, such as the role and distinctiveness of the object in the region, to facilitate connections between objects and their respective regions. Finally, we ask annotators to write several advanced QA situated in the scene as a complement.

To anchor objects within captions and ensure consistent object identification across multiple views, we overlay 3D bounding boxes with unique IDs on images as visual cues (Fig. 2). These images are then input into GPT-4 to initiate the annotation process. GPT-4 is instructed to format object references using the template, such as <table 4>, within regional descriptions. This format enables precise object grounding for training and evaluating 3D-LLMs. Note that after preliminary testing with existing VLMs, only GPT-4 can produce reliable captioning with precise object identity information, so we chose it in this round of annotation.

**Efficiency Gains from the GPT-assisted Method.** We have conducted a detailed analysis comparing the annotation efficiency as well as the quality between fully manual annotation and human-in-the-loop annotation. We selected 20 objects and 2 regions from some representative scenes for annotation for this purpose and compared the annotation efficiency quantitatively with similar quality. The comparison includes 1) the annotation time and the number of tokens for each object, measured in seconds/object and tokens/object, 2) the annotation time and the number of sentences for each region, along with the number of tokens per annotation, measured in seconds/region, sentences/region and tokens/sentence. (Tab. 3 and Tab. 4, "fully manual" abbrex. as "FM", "human-in-the-loop" abbrex. as "HIL"). The comparison results show an overall increase in effective annotations with significantly less time spent using our annotation method.

### Post-processing

Given the comprehensive meta-annotations, we subsequently post-process them to obtain data samples for visual grounding and question-answering benchmarks. In addition, we can further generate grounded scene-level captions from meta-annotations and integrate them to train 3D-LLMs to understand the 3D scene with hierarchical grounding capability more efficiently. Next, we first detail the conversion of meta-annotations to benchmark data samples and then demonstrate the reformatting process for training purposes.

**Post-processing Annotations for Benchmarks.** Meta-annotations have covered most information in each scene, including each object, region, and simple relationships among these entities. Except for simple captioning tasks by directly using meta-annotations, visual grounding and question-answering are two fundamental tasks that necessitate the model to generate responses for more specific targets or questions. As shown in Fig. 3, we categorize the main questions into two cases: asking about a single target or inter-target relationships. For each of them, we similarly target two aspects, space and attribute understanding, on different granularities, objects and regions, to produce data samples for our benchmark. Next, we detail our approach to obtaining samples for different aspects. By default, the following mentioned data samples are converted by ChatGPT  and then checked and corrected by humans. Since the main work is information extraction and rephrasing, ChatGPT can complete it well, and humans only need very little effort afterward.

_Single-target._ Given descriptions for each entity, a natural way to convert them into specific questions is by directly asking about the spatial or attribute characteristic of a single target. As shown in the right part of Fig. 3, questions in QA benchmarks expect the model to identify the spatial features or attributes of the entity, while VG benchmarks directly require the model to locate specific entities within a scene. Two cases here are noteworthy. First, there is a kind of question concerning the existence and quantity of a specific target, which cannot be grounded and thus only exists in our QA benchmark. Second, the spatial or attribute characteristics can either be unique for a specific entity in the scene or common among multiple entities. To address this problem efficiently, we employ ChatGPT to extract descriptions for each spatial feature or attribute, _e.g._, placement: leaning against the wall at a small angle, devise a set of coarse yet orthogonal categories, _e.g._, placement: {standing upright, piled up, leaning, lying flat, hanging}, and categorize objects into the set to produce samples for understanding common characteristics. Additionally, we generate data samples to understand unique characteristics by combining the original detailed object-level descriptions with human annotations about the particularity of objects in each region.

_Inter-target._ Similarly, inter-target samples can be divided into understanding spatial and attribute relationships among objects and regions. We focus on object-object and object-region relationships for the current dataset due to scene simplicity, excluding region-region pairs. Specifically, the object-object spatial relationships, a well-studied problem, are refined based on EmbodiedScan annotations. Object-object attribute relationships and object-region relationships are derived from preliminary region-level meta-annotations. We utilize templates to create initial samples, refine them, and expand the dataset using ChatGPT. Note that there are two formats for each object-object relationship sub

Figure 3: Post-processed annotations for benchmarks. “O” and “R” means “objects” and “regions”. Apart from samples shown in the figure, there is a minor part of QA samples for advanced understanding and reasoning, such as situated QA related to everyday life, accounting for 2.18%.

class in the QA benchmark (Fig. 3): to directly inquire about the relationship between two objects or to identify a specific object based on its relationship to another object and inquire about its properties.

**Post-processing Annotations for Training.** Beyond post-processing meta-annotations for benchmarks, we also harness these data for training purposes. We present two primary approaches for leveraging this data: (1) generating grounded scene captions to facilitate efficient grounding training and (2) using all the captions for 3D-LLMs instruction tuning.

_Grounded Scene Captions Generation for Efficient Grounding Training._ Previous multi-modal 3D datasets typically generate holistic captions for objects or scenes without hierarchical grounding, lacking detailed correspondence for efficient model training. Drawing on Grounded 3D-LLM  and 2D multi-modal learning experiences [38; 43], we retain all correspondence information from meta-annotations. This allows us to seamlessly integrate object- and region-level captions into scene captions, complete with identity information. We log the indices of positive tokens to train the correspondence between 3D features and text phrases. Utilizing this data enhances the model's ability to comprehend complex statements and ground specific objects at the phrase level. Further implementation details are available in the supplementary materials.

_General Captions for Instruction Tuning._ Our meta-annotation offers the most extensive captions for 3D scenes, encompassing various granularities and perspectives for each entity. These captions, along with object- and region-level annotations enriched with grounding information, enable the creation of scene-level captions. They serve as valuable resources for 3D-LLMs' instruction tuning.

Except for these captions used for training, data samples of VG and QA benchmarks are also training resources for instruction tuning. Related attempts are presented in the supplementary materials.

### Analysis

**Statistics.** Our dataset comprises 6.9M language annotations and 114M tokens overall, encompassing a comprehensive range of correspondence granularities, as detailed in Tab. 1. It includes 1.4M meta-annotations, with 1.05M specific property captions and 380k complete captions, totaling 18.3M tokens across 109k objects from 285 categories and 7692 regions of 12 types. These meta-annotations facilitate the creation of 1.76M QA samples (4.06M captions), 1.28M VG samples, and 97k grounded scene captions (with 90 tokens per caption), providing various types of data resources for training and benchmarks. See more statistics and distribution figures in the supplemental.

**Comparison to Previous Datasets Annotated with VLMs.** Some recent works like 3D-VisTA, LEO, Multi3DRef, including ours, utilized powerful Visual Language Models for annotation. Among them, MMScan is unique and can bring new insights in the following aspects:

* Top-down annotation logic covering region/object level, single/inter-target descriptions, and spatial/attribute understanding.
* Systematic and customized annotation workflows for objects and regions, including carefully designed language/visual prompts to cover different aspects to obtain meta-annotations instead of direct benchmark samples and detailed ablation for the performance of different Visual Language Models choices.
* Adaptable methods for deriving different benchmark samples from meta-annotations.
* Human-in-the-loop design ensures quality and minimal biases, with UI tools prompting explicit error identification, achieving a sub-5% error rate.

## 4 Experiments

This section presents two main benchmarks based on MMScan: 3D visual grounding and 3D question answering, along with a preliminary 3D captioning benchmark as a potentially more challenging task in the future. We demonstrate new challenges of these tasks with different evaluations from GPT, human, and conventional metrics. Furthermore, we used the rich annotations from MMScan to obtain state-of-the-art grounding models and 3D-LLMs. Finally, we make an analysis of the scaling law of multi-modal 3D learning regarding language annotations. More qualitative and in-the-wild test results and analyses and detailed implementation details can be referred to the appendix.

### 3D Visual Grounding Benchmark

**Dataset & Evaluation Metrics.** As mentioned previously, we follow the original scene split of EmbodiedScan and obtain 848867/217002/209717 data samples for training/validation/testing on our visual grounding benchmark. All the data samples are categorized into a sub-class from the set {ST-attr, ST-space, OO-attr, OO-space, OR}, where _Single-target, attribute, Object-Object, Object-Region_ are abbreviated as _ST, attr, OO, OR_. We use 20% samples for training to reduce all the models' training time into 2 days and report the results on the validation set. For the evaluation metric, we adopt the conventional 3D IoU-based Average Precision (AP) used in 3D detection here, considering we can have multiple targets grounded instead of a single one in most previous grounding benchmarks. In addition, we also show the recall and performances for different sub-classes of samples for reference.

**Implementation Details.** We implement four popular baselines, ScanRefer , BUTD-DETR , ViL3DRef , and EmbodiedScan , to establish the initial benchmark, considering that they are representative of different types of methods. Note that ViL3DRef requires a prior segmentation of the point cloud as the grounding foundation. We employ the bounding boxes predicted by a trained EmbodiedScan  model for point cloud segmentation to ensure a fair comparison. In addition, we replace the RGB-D input of the EmbodiedScan grounding model with the reconstructed 3D point clouds and remove the image feature branch to keep the consistency with other baselines.

**Results.** As shown in Tab. 5, although we train these grounding models with our dataset, the performance is much lower than previous grounding benchmarks (_e.g._, state-of-the-art 48.1% accuracy on ScanRefer). The new challenges come from diverse and complex prompts that may need to involve LLMs and stronger 3D encoders, more difficult 9-DoF oriented box estimation in EmbodiedScan, and an uncertain number of grounding targets diverging from previous simple settings. In addition, we can find that the single-target performance is typically lower than that of inter-target, indicating that the model can understand inter-target relationships better. We conjecture that there are many small objects and appearance attributes that require involving the image modality to identify. It is also necessary to include image modality as the model design of EmbodiedScan to achieve better performance. We demonstrate several representative failure cases in the supplementary materials. Finally, we observe that ViL3DRef achieves high recall performance due to pretrained detection models on EmbodiedScan but encounters major problems when selecting the correct grounding targets. In contrast, the EmbodiedScan baseline achieves a better selection performance but can further improve the recall capability.

### 3D Question Answering Benchmark

**Dataset & Evaluation Metrics.** Similarly, following the scene split of EmbodiedScan, our 3D QA benchmark has 1167966/297014/295073 data samples for training/validation/testing correspondingly. Considering the intricacies of questions and answers, and in line with recent practices for assessing LLMs [50; 33; 20], we employ both human evaluators and GPT-4 to ascertain answer accuracy. This approach is complemented by data-driven and conventional metrics. Owing to the prohibitive costs of human evaluation, we primarily present GPT-4's evaluation results in Tab. 6 and demonstrate the concordance between human and GPT-4 assessments on a subset in the supplementary materials.

**Implementation Details.** Given that contemporary 3D-LLMs are anticipated to exhibit generalization in open-world scenarios, we first focus on assessing their zero-shot performance directly on our test set, avoiding the potential overlap of training scenes used by these methods. The baselines consist of 3D-LLM , Chat3D-v2 , LEO , LL3DA  and LLaVA-3D . LLaVA-3D is a modified version of PointLLM with RGB-D input to fit scene-level understanding (more details in the supplemental). We utilize the officially released versions of these models, tailor our data and questions to align with their input requirements and evaluate their performance accordingly. Additionally, we fine-tune the models from three recent works, LEO, LL3DA and LLaVA-3D, using our training set to offer reference results under the fine-tuning setting.

   &  &  &  \\   & AP\({}_{25}\) & AR\({}_{50}\) & AP\({}_{50}\) & AR\({}_{50}\) & ST-attr & ST-space & OO-attr & OO-space & OR \\  ScanRefer  & 3.83 & 42.40 & 1.37 & 20.96 & 1.44 & 2.84 & 5.22 & 4.32 & 1.12 \\ BUTD-DETR  & 2.29 & 65.61 & 0.84 & 33.11 & 4.79 & 2.04 & 1.49 & 1.75 & 11.87 \\ ViL3DRef  & 5.17 & 72.50 & 2.07 & 51.61 & 6.29 & 4.20 & 7.89 & 5.29 & 6.81 \\ ReGround3D  & 4.12 & 48.12 & 1.98 & 22.12 & 4.23 & 3.98 & 7.32 & 6.98 & 8.23 \\ MVT  & 3.65 & 72.38 & 1.02 & 51.50 & 1.74 & 2.34 & 3.58 & 4.45 & 1.49 \\
3D-VisTA  & 5.24 & **72.51** & 1.91 & **51.85** & 4.91 & 4.39 & 5.75 & 5.99 & 6.35 \\ EmbodiedScan  & **10.49** & 47.21 & **2.94** & 21.76 & 7.44 & 7.53 & 13.65 & 11.19 & 7.74 \\  

Table 5: 3D visual grounding benchmark on MMScan.

[MISSING_PAGE_FAIL:9]

grounding baseline on EmbodiedScan with MMScan, we achieve a substantial increase in benchmark performance (up to 7.17% AP), as shown in Tab. 7. Co-training slightly outperforms pre-training, and while human involvement improves data quality, its effect is less pronounced than expected.

**Captions for Instruction Tuning.** A key challenge for current 3D-LLMs to achieve stronger performance is the scarcity of high-quality multi-modal 3D datasets. Utilizing our annotated object and region captions, along with aggregated scene captions, for instruction tuning is a logical approach. We feed these data to our baseline, LLaVA-3D, and observe the significant improvement on traditional question-answering benchmarks (Tab. 8), achieving state-of-the-art performance. Furthermore, it also shows much better in-the-wild test performance, and we present the qualitative results in the supplementary materials.

**Scaling Law for Multi-modal 3D Learning.** Finally, to guide future research, we employ EmbodiedScan VG baseline and LL3DA with different amounts of data to study the scaling law for multi-modal 3D learning. As shown in the upper part of Fig. 4, the VG performance increases steadily while the QA performance exhibits an initial sharp increase followed by a gradual ascent, indicating the VG task still needs more data while our generated QA samples approach saturation. In summary, both tasks show significant improvement with the data increase, from 8.7% to 20.6% AP and 15.84% to 44.81% accuracy on the VG and QA benchmarks, respectively.

Furthermore, we conducted an investigation into the scaling law regarding different diversities, _i.e._, scene diversity vs. data sample diversity. The study evaluated the performance of the models when subjected to distinct sampling methodologies: uniform sampling of data samples and selective sampling based on scenes (with retention of only samples present within the chosen scenes). As depicted in the lower part of Fig. 4, the performance was found to be relatively lower under scenarios with restricted scene diversity, even with an equal distribution of samples. It indicates that both data sample and scene diversity matter when scaling up the training and more diverse scenes can result in more significant improvement.

## 5 Limitations and Conclusion

This paper establishes the largest ever multi-modal 3D scene dataset featuring hierarchical language annotations. We employ a top-down approach and harness both VLMs and human annotators to encompass holistic and precise annotations of 3D scene understanding. Based on meta-annotations, we further derive data samples and grounded scene captions for evaluating and training 3D grounding and language models comprehensively. Although this paper proposes a potentially scalable method to construct large-scale multi-modal 3D datasets, it still relies on human annotators and can be further improved regarding scene diversity. Exploring how to reduce human correction efforts and scale up the scene diversity are objectives for future work.

**Social Impact.** This paper proposes a multi-modal 3D scene dataset based on existing open-source real-scanned data and facilitates the training and evaluation of 3D-LLMs, potentially benefitting downstream 3D content generation and robotic applications. Meanwhile, the trained 3D-LLMs can still have occasional hallucination problems, leading to potential risks when being integrated into the entire system.

   &  &  &  \\   & Overall & Type & Code & Shape & Position & Function & Design & SurfSS & S-BERT & B-1 & B-4 & MET & R-L \\  LL3DA & 33.67 21.1 & 109.19 - 12.62 & 26.37 18.0 & 40.67 36.0 & 38.97 20.2 & 67.57 14.6 & 21.71 18.1 & 44.9 & 43.5 & 33.6 & 5.3 & 11.9 & 27.2 \\ LEO & 51.3 4.32 & 34.9 45.2 & 29.7 37.0 & 63.0 48.7 & 63.71 44.1 & 75.0 43.35 & 42.7 1.9 39.2 & 57.1 & 55.2 & 35.5 & 8.3 & 13.8 & 29.5 \\  

Table 10: Humans’ and models’ performance on the MMScan QA benchmark (human evaluation).

Figure 4: The performance of both tasks grows steadily with the increase of training data, and more diverse scenes can result in more significant improvement.

   &  &  &  \\   & Overall & Type & Code & Shape & Position & Function & Design & SurfSS & S-BERT & B-1 & B-4 & MET & R-L \\  LL3DA & 33.67 21.1 & 109.19 - 12.62 & 26.37 18.0 & 40.67 36.0 & 38.97 20.2 & 67.57 14.6 & 21.71 18.1 & 44.9 & 43.5 & 33.6 & 5.3 & 11.9 & 27.2 \\ LEO & 51.3 4.32 & 34.9 45.2 & 29.7 37.0 & 63.0 48.7 & 63.71 44.1 & 75.0 43.35 & 42.7 1.9 39.2 & 57.1 & 55.2 & 35.5 & 8.3 & 13.8 & 29.5 \\  

Table 9: 3D captioning benchmark on MMScan.