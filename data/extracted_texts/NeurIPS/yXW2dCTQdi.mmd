# Controlled maximal variability along with reliable performance in recurrent neural networks

Chiara Mastrogiuseppe

Center for Brain and Cognition, Department of Engineering

Universitat Pompeu Fabra

Barcelona, Spain

chiara.mastrogiuseppe@upf.edu

&Ruben Moreno-Bote

Center for Brain and Cognition, Department of Engineering,

Serra Hunter Fellow Programme

Universitat Pompeu Fabra

Barcelona, Spain

ruben.moreno@upf.edu

###### Abstract

Natural behaviors, even stereotyped ones, exhibit variability. Despite its role in exploring and learning, the function and neural basis of this variability is still not well understood. Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance. While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective. We investigate how to generate maximal neural variability while at the same time having high network performance. To do so, we extend to neural activity the maximum occupancy principle (MOP) developed for behavior, and refer to this new neural principle as NeuroMOP. NeuroMOP posits that the goal of the nervous system is to maximize future action-state entropy, a reward-free, intrinsic motivation that entails creating all possible activity patterns while avoiding terminal or dangerous ones. We show that this goal can be achieved through a neural network controller that injects currents (actions) into a recurrent neural network of fixed random weights to maximize future cumulative action-state entropy. High activity variability can be induced while adhering to an energy constraint or while avoiding terminal states defined by specific neurons' activities, also in a context-dependent manner. The network solves these tasks by flexibly switching between stochastic and deterministic modes as needed and projecting noise onto a null space. Based on future maximum entropy production, NeuroMOP contributes to a novel theory of neural variability that reconciles stochastic and deterministic behaviors within a single framework.

## 1 Introduction

From opening a door to crossing the street, everyday life hinges on reliably executing actions. Despite that, natural behaviors, including repetitive movements from expert athletes [1; 2; 3; 4], exhibit variability [5; 6; 7]. The mechanisms governing the emergence of this variability from the central and peripheral nervous systems remain unclear. Variability of neural activity in motor cortex [8; 9; 10] and subcortical structures [11; 12] controlling muscle movements, as well as state changes of the effectorsdue to, e.g., fatigue [13; 14], might contribute to the observed behavioral variability. However, as behavioral variability is also observed at longer time scales during perception [15; 16], decision making [17; 6; 5; 18] and planning , neural fluctuations in most parts of the brain [20; 21; 22] might be involved in the generation of variable behavioral repertoires as a whole.

Several mechanisms have been put forward for the generation of neural variability, including synaptic noise within neural circuits [23; 24] or non-linear network interactions leading to variable activity patterns [25; 26; 27; 28]. These proposed mechanisms are predominantly designed to describe variability during spontaneous activity - in the absence of sensory stimuli - [22; 20] or in response to simple stimuli [29; 30], most frequently outside a complex task. In other theoretical studies where the goal is to maximize network performance in a task, variability is typically suppressed after learning [31; 32; 33]. If anything, the initial noise or activity variability is used as means to regularize and promote exploration during learning [34; 35], but they are considered to be unnecessary thereafter. This approach is also the one taken in state-of-art reinforcement learning, where variability is added during learning, but during task execution policies are forced to be deterministic [36; 37].

Given these two coexisting sides of natural behavior - namely, high task performance in spite of large variability - we ask whether it is possible to have neural networks generating maximal variability while at the same time being able to flexibly switch to deterministic behavioral modes when needed. We surmise that generating neural variability is a fundamental goal of the nervous system, as it enables the exploration of its entire dynamical range. This idea parallels the one that neural activity should be variable to generate the vast behavioral repertoires [38; 8; 39; 40; 41] as the ones empirically observed [5; 18; 42; 11]. The sought neural variability needs to be highly structured in order to avoid non-adaptive behaviors. One relevant example comes from reaching tasks, where neural activity is indeed found to be confined in null spaces as to avoid undesired movements . To address the above question, we build on the maximum occupancy principle (MOP) developed for behavior [44; 45], which posits that agents ought to occupy action-state space by generating all sorts of action-state paths compatible with the dynamical and environmental constraints. Applied to neural activity, we introduce NeuroMOP, which puts forward the hypothesis that the brain should generate maximum entropy in the neural activity paths. Importantly, this entails avoiding the terminal states where further entropy cannot be generated. This principle aligns with a broad body of Reinforcement Learning (RL) literature on reward-free algorithms based on purely-entropic objectives [37; 46; 47]. By optimizing the cumulative sum of future action-state entropy, NeuroMOP emphasizes seeking future variability to the extent that does not compromise performance. By properly defining terminal states as absorbing states where no more entropy can be generated, this principle seeks variability while also generating behaviors that guarantee future'survival'.

In this paper, we employ random recurrent neural networks (RNNs) of fixed weights as a simplified representation of brain dynamics, and we let them interact with a stochastic input current generator following MOP (Fig. 1). The input current generator (the _agent_) is designed to maximize the entropy - hence, the variability - of the series of currents (a function of the _actions_) it injects in the RNN (the _environment_). As expected, variable currents lead to variable neural activities, but this variability becomes structured in order to avoid dangerous (terminal) states. To bridge neural variability with functionality, we test our architecture in a series of different problems. First, we show that the NeuroMOP network learns to satisfy energy constraints, while generating large neural variability. Second, a subset of RNN neuron activities can be confined within complex regions while remaining free within the region, thereby 'drawing' different symbols, also in a context dependent manner. Crucially, the NeuroMOP network not only learns to effectively solve tasks, but it maintains a high dimensionality of action signals whenever possible, allowing the visitation of a wide range of activity patterns. By flexibly reducing its dimensionality when close to terminal states, we show that low dimensionality is an emergent property under constrained tasks where higher dimensionality is the default mode.

## 2 Methods

### NeuroMOP architecture: controller and RNN

The NeuroMOP architecture consists of a controller (_agent_) injecting currents into an RNN of fixed random weights (_environment_) (Fig. 1). The state \(x^{N}\) of the RNN, where \(x_{i}\) is the activity of neuron \(i=1,,N\), follows the dynamics

\[x_{i}(t+1)=x_{i}(t)+ t(-}{}+(_{j=1}^{N}J_{ ij}x_{j}+I_{i}(t)))\,\] (1)

where \(()\) is a non-linear transfer function, \( t\) is the integration time step of the dynamics, and \(I_{i}(t)\) are currents injected by the controller in the RNN neurons. The recurrent connections of the RNN are fixed and sampled from a normal distribution, \(J_{ij}(0,g^{2}/N)\). We use a saturating transfer function \(()=()\), which leads to chaotic dynamics when the internal recurrent connections are strong enough  (see Fig. 2a). Results for RNNs with non-saturating (ReLU) transfer functions are shown in Appendix F.

At time \(t\), the controller samples a random _action_\(a\) from a state-dependent, stationary policy \((|x)\). The action is an \(M\)-dimensional discrete vector \(a(x)\); we will consider below the presence of terminal states \(x^{}\), where the number of available actions is drastically reduced. Based on the generated action \(a\), the controller injects into the \(i\)-th neuron the current

\[I_{i}(t)=_{k=1}^{M}K_{ik}a_{k}\,\] (2)

where \(K_{ik} U(0,1)\) are positive input weights sampled from a uniform distribution and \(\) is a parameter that scales the strength of the current. Thus, internal actions \(a\) are expanded via the random matrix \(K\) into \(N\)-dimensional currents. This expansion allows us to study the harder problem of controlling the RNN's dynamics using actions with reduced dimensionality, i.e., \(M N\), but our framework also works for projections, \(M>N\).

The controller follows MOP, that is, it aims at occupying action-state path space . Here we restrict ourselves to action paths, and show the general action-state framework in Appendix D. We assume that the controller gets an intrinsic reward of \(-(a|x)\) for generating action \(a\) when the network is in state \(x\) at time \(t\), being the largest when the generated action has low probability under the current policy \(\). The controller does not greedily maximize this immediate intrinsic reward at every time step. Instead, the policy \(\) is chosen to maximize the _value_ function, defined as the expected

Figure 1: Schematic of the NeuroMOP network. At each time step the controller reads the activity \(x\) (state) of the RNN and samples an action \(a\) from the policy \((|x)\). Using the optimal policy in Eq. 4 requires predicting the effect that each of the possible actions \(a^{(k)}(x)\) would have on the state of the RNN by computing the successor state \(x^{}(x,a^{(k)})\) and then evaluating the corresponding value function \(V(x^{}(x,a^{(k)}))\) in that state. The value function is approximated by a feedforward network (FFN). Once sampled, the low-dimensional action \(a\) is expanded and transformed into currents \(I\) via a matrix \(K\) and fed into the RNN. Next, the RNN state evolves one time step and the loop is repeated. The weights of the FFN are trained via gradient descent using as cost function the Bellman error stored along a batch of trajectories.

discounted sum of future intrinsic rewards

\[V_{}(x)=_{,p}[-_{t=0}^{}^{t}( a(t)|x(t))]=_{,p}[_{t}^{t} (|x(t))]\,\] (3)

with discount factor \(0<<1\). Note that this expression takes the form of a sum of future action entropies, where \((|x)=-_{a(x)}(a|x)(a|x)\). The expectation is over all paths \(=(x,a(0),x(1),a(1),)\) with initial condition \(x(0)=x\) generated by sampling actions from the policy \(\) and following the state transitions probability \(p=p(x(t+1)|x(t),a(t))\) defined by Eq. 1. By virtue of maximizing action path entropy, the MOP agent occupies current and future action space as broadly as possible [44; 45]. Discounted cumulative future action entropy stands as the only measure of occupancy that adheres to the intuitive notion that the occupancy over an action path is the sum of the occupancies over any of its subpaths, the so-called additive property .

The optimal policy \(^{*}\) (, Appendix C) maximizing the value function is

\[^{*}(a|x)=e^{_{x^{}}p(x^{}|x,a)V^{*}(x ^{})}\,\] (4)

where \(Z(x)=_{a(x)}e^{_{x^{}}p(x^{}|x,a)V^{*} (x^{})}\) is the partition function and \(V^{*}(x)\) is the optimal value following the optimal policy, defined as

\[V^{*}(x)= Z(x)=_{a(x)}e^{_{x^{}}p(x^{ }|x,a)V^{*}(x^{})}\.\] (5)

In our specific implementation with deterministic dynamics (Eq. 1), the transition probability \(p(x^{}|x,a)\) is a delta function, and so the successor state \(x^{}\) is uniquely determined by the current state \(x\) and the action \(a\), \(x^{}(x,a)\). Our algorithms work well also for RNNs with noisy dynamics (Appendix E).

We will define different problems by choosing specific state-dependent action sets, so that the available set of actions \(a\) depends on \(x\), \(a(x)\). Specifically, we define terminal states, denoted \(x^{}\), as absorbing states the network cannot escape from and where doing nothing is the only available action. These terminal states might represent detrimental state regions, e.g., too high neural activity, or other adverse activity patterns resulting in significant external penalties, such as the falling of an agent to the floor. With doing nothing being the only action, entering a terminal state is an irreversible process leading to an intrinsic reward of always zero from that point onwards, i.e., \(-(a=|x^{})= 1=0\), as no further action entropy can be generated. Therefore, by definition, \(V_{}(x^{})=0\) for any policy. Terminal states can be considered 'dead' states of the network, and they will be naturally avoided to keep maximizing future action path entropy. In our implementations, non-terminal states share the same action set of \(M-\)dimensional binary actions \(a_{k}\{-1,1\}\  k=1,,M\). This does not imply that all non-terminal states are equally desirable; via the computation of the value function, the network naturally exhibits less preference for 'bad' states that increase the likelihood of encountering terminal states in the future. We will show that the structure of terminal regions, along with the network dynamics, leads to complex, rich, variable behaviors without the need to specify an extrinsic reward function. In essence, MOP tells agents what _not_ to do, and thus it does not restrict behavior. In contrast, standard extrinsic reward maximization tells agents what to do, inevitably limiting behavior. Note that maximizing action path entropy entails striking a balance between maximizing immediate and future entropy, with behaviors that can become _locally_ very deterministic if this _globally_ opens up larger repertoires of possible action courses, i.e., larger future action entropy.

### Value function approximator by minimizing the Bellman error

As our problem involves a high-dimensional continuous state space (\(N=100\)), we use a feed-forward network (FFN) to approximate the optimal value function \(V^{*}(x)\) in Eq. 5 with \(V(x,w)\). The FFN with parameters \(w\) consists of one hidden layer of \(N_{hid}\) neurons, an input layer with input the activities \(x\) of the RNN, and one single output neuron with activity \(V(x,w)\) (Fig. 1).

In order to optimize the parameters of the FFN, we consider \(V_{}(x,w)\), the expected evolution of the approximated value function satisfying the Bellman consistency equation, defined as

\[V_{}(x,w)=_{a(x)}e^{_{x^{ }}p(x^{}|x,a)V(x^{},w)}\.\] (6)If the approximated value function \(V(x,w)\) were equal to the optimal value \(V^{*}(x)\), its expected evolution would coincide with the value function itself, i.e., \(V_{}()=V^{*}()\) (see Eq. 5). Thus, we optimize the weights \(w\) of the FFN by minimizing a loss function, defined as the summed squared errors between \(V_{}()\) and \(V(,w)\) over trajectories in each epoch \(l=1,,N_{}\),

\[_{l}(w)=}_{=1}^{N_{traj}}^ {()}}_{t=1}^{t_{end}^{()}}(V(x^{()}(t),w)-V_{}( x^{()}(t),w_{l}))^{2}\;,\] (7)

where the squared error is accumulated over a batch of \(N_{traj}\) paths \(=(x^{()}(0),x^{()}(1),,x^{()}(t),)\), and the path lifetime \(t_{end}^{()}\) is the minimum between the time when the network reaches a terminal state and the maximum episode duration \(T\). Each path \(\) is generated by sampling actions from the policy in Eq. 4 with the same initial condition \(x(0)=x\). The policy depends on the specific values of the FFN weights at epoch \(l\). The parameters of the network are updated at each epoch \(l\) using Adam as optimizer.

Our results have proven to be stable also for FFNs only receiving as an input \(N_{inp}<N\) activities randomly selected from the \(N-\)dimensional state, denoted \(\). If needed, the input can be extended to include any required extra-information or \(\) can be constrained to specific neurons. For instance, in the context-dependent constraints problem defined in Sec. 3.2.1, we added extra units to the FFN input layer to flag (via a one-hot vector) the context. Including in \(\) the readout neurons \((x_{1},x_{2})\) improved stability and performance.

### The reward-maximizing network

To provide a comparison for the NeuroMOP network, we introduce the R network, which aims at maximizing the discounted sum of future extrinsic reward. To ensure a fair comparison, we incorporate the notion of survivability present in MOP by assigning to the R network in state \(x\) and taking action \(a\) the extrinsic reward

\[r(x,a)\;=1&\;\;x^{}(x,a) x^{}\\ 0&\;\;x^{}(x,a)=x^{},\] (8)

where \(x^{}(x,a)\) is the state evolution of the RNN as defined in Eq. 1. To allow the generation of variable trajectories also by this network, stochasticity in the action selection is implemented by endowing the network with an \(\)-greedy policy defined as

\[a_{}(|x)\;=*{argmax}_{a}V_{ }(x^{}(x,a))&\;\;1-\\ &\;\;,\] (9)

where \(V_{}(x)\) is the expected future cumulative reward when following this policy, which can be written recursively as

\[V_{}(x)=_{a_{},x^{} p}[r(x,a) + V_{}(x^{})]\;.\] (10)

In this framework \(\) is a hyperparameter controlling the amount of (random) action variability generated by the network. To better compare the two networks behaviors, the choice of \(\) is such that the average lifetime of the two networks in each problem is comparable. The value function is approximated using a one-hidden layer FFN as in Sec. 2.2. Analogously to MOP, in order to train the network we generate a batch of \(N_{traj}\) paths \(\) starting with the same initial conditions \(x^{()}(0)=x\) and minimize the loss function defined as the summed squared error between the approximated value \(V_{}(x,w)\) and its expected evolution following the Bellman equation in Eq. 10.

## 3 Results

### Energy constraint

To test whether NeuroMOP can produce maximal variability under strict constraints, we first study a scenario where a terminal state is reached when the overall level of the RNN's activities is high. Specifically, \(x^{}\) are all the states where the energy, defined as a function of \(x\), exceeds a certain value \(E(x^{})>L\) (see Appendix B). In this way, we implement the idea that high activity is detrimental,either because it is costly , or because it leads to neural saturation, impeding sensory encoding . We consider a free network (the RNN without external current, Eq. 1 with \(I=0\)) in a chaotic state, where the repetitive saturation of the neurons activities leads to high energy consumption over time (Fig. 2a). The NeuroMOP network learns to control the input current in order to generate maximal input entropy while, at the same time, avoiding terminal states and thus surviving for the whole length of the stimulation (Fig. 2b, early training; Fig. 2c, late training). Importantly, we observe that the NeuroMOP network changes dynamical regimes depending on how far the energy consumption is to threshold (Fig. 2c, bottom panel, inset): when the energy is close to threshold, the action entropy reduces, and therefore the policy becomes more deterministic. In contrast, when the energy is far from threshold, action entropy rises again and the policy increases its stochasticity. Moreover, the policy dimensionality, as measured by the effective dimensionality of the currents (see Appendix A), is lower when close to the threshold compared to further away (Fig. 2f), projecting noise to a lower dimensional manifold such that actions (i.e., currents) that would push the network above threshold are suppressed. Overall, these results show that the NeuroMOP network flexibly changes from a highly stochastic to a more deterministic policy depending on the network state.

Comparing NeuroMOP with the R network, we find that the R network employs different solutions, showcasing a preference for risk-averse solutions (Fig. 2d). As a long lifetime is encouraged by the

Figure 2: Energy constraint. **(a)** In the free network, the RNN shows chaotic activity (top panel) with high energy consumption (bottom) above threshold (dashed line). In the top panel, each line represents the activity of a randomly sampled neuron of the RNN. A single trial is shown. **(b)** Early in training (\( 10\) epochs), the NeuroMOP network quickly reaches a terminal state by crossing the energy threshold (dashed line, bottom panel). Indeed, the large action entropy throughout the trajectory suggests no state-dependent action entropy adjustment. Inset provides a zoom of the energy close to the boundary. **(c)** After training, the NeuroMOP network is able to avoid terminal states for the whole duration of the trajectory (\(T=1000\)) by reducing its action entropy whenever closer to the energy threshold. Inset as in (b). **(d)** The R network employs completely different and risk-averse solutions. **(e)** Probability density function of the average state occupancy. **(f)** When far from the energy threshold (\(E(x) L\)), the NeuroMOP network exhibits maximum effective dimensionality (\(ED_{a} M=8\)), but loses one degree of freedom (\(ED_{a} M=7\)) when approaching the threshold corresponding to terminal states (\(E(x) L\), i.e. \(E(x)[L- L,L]\), with \( L=0.001\), arbitrary). The R network only lives far from the threshold injecting mainly inhibitory currents and it exhibits a low effective action dimensionality. **(g,h)** With training, both networks increase the average standard deviation of the individual trajectories \(\), with MOP displaying larger variability (g). Together, they learn to reach the end of the episode \(t_{end}=T=1000\) (h). Averages over \(N_{av}=10\) networks with batches of \(N_{traj}=10\) trajectories; errors are standard errors of the mean (\(SEM\)).

extrinsic reward, the R network learns to steer the RNN's energy very far from the terminal state, so that the spontaneous action fluctuations given by the stochasticity of the random policy would not harm the overall performance by keeping it far from threshold. The policy found by the R network consists in injecting mostly inhibitory currents, driving the RNN towards the point of minimum energy and effectively'silencing' the RNN.

Although both networks are able to avoid the terminal states, they lead to different behaviors and space occupancy: while the R network tends to suppress the RNN's activity, the NeuroMOP network exploits the overall range of activities permitted by the terminal states (Fig. 2e). Again, the NeuroMOP network is able to do so by adapting its action entropy in a state dependent manner. Proximity to the terminal state imposes a constraint on the network's activity. In contrast, the R network operates only far from the threshold and, by showing a clear preference for inhibitory actions, exhibits an effective dimensionality significantly lower than the maximum possible one, i.e., \(ED_{a}<M\) (Fig. 2f). As a consequence of the different action selection strategies, the RNN's neurons within the NeuroMOP network display greater variability than those within the R network, with a larger average standard deviation over the trajectories (Fig. 2 g). The choice of \(\), representing the level of stochasticity, of the R network is such that the two networks have comparable lifetimes (Fig. 2 h).

### Constrained neural space

Terminal states can be arbitrarily imposed on the activities of individual neurons or any subset of them, not only globally as in the previous scenario. Here, we test NeuroMOP in a new problem

Figure 3: Constrained neural space. **(a)** Terminal states are defined as the boundaries of a square in the activity space of two randomly selected RNN’s neurons \((x_{1},x_{2})\). **(b-c)** As a result, the NeuroMOP network confines the activities \(x_{1}\) and \(x_{2}\) within the square boundaries (panel b, magenta traces), while it ‘draws’ the square by filling its inside (panel c, colored line, representing one trajectory of the two readout neurons). In contrast, in the space of any other pair of neurons \((x_{i},x_{j})\)\(i,j>2\) activities spread in space (grey line, representing one trajectory). A zoom-in of the readout space shows that the NeuroMOP network adapts the action entropy based on the state proximity to the boundaries (colorbar, c, right panel). **(d)** The R network, following an \(-greedy\) policy, fails to avoid the terminal states except for extremely small values of \(\), effectively reducing its action stochasticity to zero. Lifetime computed after training the network for \(100\) epochs. **(e)** Matching lifetimes for both NeuroMOP and R network with an \(-\)greedy policy with exponential decay (see Appendix B). The R network learns to satisfy the boundaries constraints after the exponential decay has dropped the randomness of the action selection (\(\)) to zero. **(f)** Same as in (c) for the R network. The R network only ‘draws’ one side of the square with the two readout neurons \((x_{1},x_{2})\), while the other neurons, as well receiving the external currents, are driven towards the saturating states. Averages are over \(N_{av}=10\) networks with batches of \(N_{traj}=10\) trajectories. Errors are \(SEM\).

where terminal states are set directly on two randomly selected readout neurons \((x_{1},x_{2})\) of the RNN. Specifically, a terminal state is encountered any time \(|x_{1}|>L\) or \(|x_{2}|>L\).

The structure of the terminal states generates interesting behavior in the NeuroMOP network: the network 'draws' a square in the \((x_{1},x_{2})\) activity space by filling the available area while avoiding the square's boundaries (Fig. 3). In other words, the network occupies all the space allowed by the terminal states after learning. The NeuroMOP network can confine the activities of the two readout neurons even within very small regions of the activity space (Fig. 3b, magenta lines). Notably, as neurons are driven by actions that aim to maximize future cumulative entropy and terminal states are here exclusively set on \(x_{1}\) and \(x_{2}\), all other neurons (\(x_{i}\), \( i 1,2\)) occupy a much larger region of the activity space (grey lines, in their own spaces \((x_{i},x_{j})\)). In the \((x_{1},x_{2})\) space, the network reduces its action entropy when in proximity to terminal states, corresponding to the square boundaries (Fig. 3c). Due to the activity correlations induced by the shared input current, controlling the readout neurons along the anti-diagonal of the square presents challenges for the NeuroMOP network. In those regions, the NeuroMOP network learns the necessity of highly deterministic action selection to avoid terminal states (Fig. 3c, right panel).

While NeuroMOP adapts the stochasticity of its policy to occupy the maximum available space, we find that the R network, following \(-\)greedy policy, fails to do the same for most values of \(\) (Fig. 3d). The R network's lifetime is comparable to that of the NeuroMOP network for extremely low values of \(\) (notice decreasing scale), for which the consequent randomness of the action selection is effectively zero. To give more flexibility to the R agent, we allowed the R network to first explore phase space by using an epoch-dependent \(_{l}\) with an exponential decay. Starting from a larger \(_{0}\) at epoch \(l=0\), and slowly decreasing it, we can match the two lifetimes (Fig. 3e). Despite this, the inherent greediness of the action selection forbids the R network to occupy all the available activity region (Fig. 3f), resulting in a largely repetitive and stereotyped network behavior.

#### 3.2.1 Context-dependent neural space constraints

We next wondered about the versatility of the NeuroMOP network to confine neural activity within even more complex boundaries. We introduce context-dependent neural space constraints (see Appendix B), where in each context the set of terminal states in the readout space of two random

Figure 4: NeuroMOP can constrain a subset of neural activities within different regions of the neural space in a context-dependent manner. The network is informed of the shape it needs to draw via a one-hot vector fed into the value function. **(a)** Example of a single network drawing \(C=6\) different shapes by confining its readout activities \((x_{1},x_{2})\) within the corresponding activity regions (\(T=5000\)). Notably, action entropy is both state and context dependent. One trajectory per context is shown. **(b)** Mean accuracy, measured as the mean lifetime in each context \(t_{end}^{C}\), reflects varying shape difficulty, consistent across networks. **(c)** With training, the NeuroMOP network learns to approach the arbitrary training end of the simulation \(t_{end}=T=600\) (left panel) and to increase the average standard deviation of the individual trajectories \(\) (right panel). Averages over \(N=10\) networks, with batches of \(N_{traj}=10\) trajectories. Errors are \(SEM\).

activities \((x_{1},x_{2})\) define different shapes, with different sizes, orientations and border complexity. We augment the input layer of the value approximator described in Sec. 2.2 with \(C=6\) additional nodes, where \(C\) represents the number of different shapes where activity has to be confined. Thus, the feedforward value approximator now receives, alongside the network state \(x\), a \(C\) dimensional one-hot vector indicating the current context.

The same feedforward network, opportunely informed of the context, correctly approximates the value function. Consequently, the NeuroMOP network also adapts its action entropy in a context-dependent manner (Fig. 4a). Notably, while avoiding terminal states, the NeuroMOP network learns to occupy the available state region in each given context. As expected, some terminal states rise greater challenges for the NeuroMOP network to be avoided (Fig. 4b), a feature that is independent on the number of stored contexts \(C\). Overall, the NeuroMOP network successfully avoids terminal states while increasing the variability in the network with learning (Fig. 4c).

## 4 Discussion

We have introduced NeuroMOP, a novel theory that puts forward the idea that neural variability arises from the active generation of future neural activity entropy. We have explored this theory by introducing a mechanism for maximizing and controlling variability in a highly-dimensional RNN in the chaotic regime. Contrary to the common idea that excessive variability may impede performance, our model showcases that injecting maximal controlled variability into RNNs actually permits to solve different 'tasks', indirectly defined by the structure of terminal states. By allowing for a diverse array of actions according to the state, this variability enables the network to explore a wider solution space, potentially leading to more effective adaptations [50; 51; 52].

We tested our network in a series of scenarios. First, we introduced an energy threshold on the network activity. Energy constraints may have been likely selected by evolution, as brain activity is costly both during information processing and at rest [53; 48]. By observing that the network always keeps energy consumption close to the threshold without exceeding it, our results align with the idea that sustained, controlled energy consumption could actually be beneficial . In a second series of problems, we showed that the NeuroMOP network can avoid terminal states in the readout space while increasing the variability in the subspaces where no boundaries are set. Therefore, long lifetime is achieved by flexibly switching between deterministic and stochastic dynamical regimes when needed. Additionally, we show that our algorithm is capable of solving problems often tackled through extrinsic rewards, such as balancing a cartpole (Appendix G), or scenarios where more deterministic behavioral modes are required, like traversing a narrow corridor in neural space (Appendix H). In addition, we show that introducing extrinsic rewards in the MOP framework largely reduces the variability of the network behavior (Appendix I).

When comparing MOP with other systems that also generate variability, like a reward-maximizing (R) network with epsilon-greedy noise, we find that R networks can only avoid terminal states after extensive training and only by quenching the source of randomness. We remark that MOP agents face as well the drawback of stochasticity as their policy follows a Boltzmann distribution (Eq. 4), and thus a non-zero probability is assigned to all actions regardless of the state. Despite that, MOP agents overcome this tendency by adapting their randomness via the computation of the value function, which trades-off immediate with future variability. This is in contrast with R agents, where the stochasticity parameter \(\) is state independent. These results suggest that state adaptation of stochasticity is a relevant property we might expect in intelligent systems.

By keeping the weights of the RNN fixed, we depart from the common practice of training networks. Weights training usually leads to activities exploiting the saturation state of the neurons [35; 33]. We conjecture this dynamics to be unrealistic, as biological neurons largely display activities that are well below their maximum values [55; 56; 49]. Analogously, saturation is undesirable even in artificial neural networks due to the vanishing curvature of the loss landscape. By favoring states and actions with low probability, the NeuroMOP network leads to the more uniform occupation as possible, avoiding saturation and encouraging neurons to stay in a 'healthy' regime, i.e., a regime suitable for computation . As well, we have demonstrated that NeuroMOP can control high-dimensional chaotic RNNs. Future research should investigate how MOP-driven input currents affect the RNN's regime. We anticipate that MOP currents will stabilize neural trajectories, consistent with operating at the edge of chaos . Characterizing chaoticity as a function of the input properties (e.g., magnitude)  is a promising direction.

In our system, an external stochastic input current generator is designed to maximize its cumulative entropy impinging onto the neurons of an RNN. The idea of specialized circuits serving the role of stochastic input generator is not novel. In songbirds, for instance, the LMAN brain region, part of the neural circuit controlling songs' production, has been largely postulated to fulfill the function of injecting variability into the downstream motor pathways . Allegedly, increased variability in the motor neural activity favors behavioral exploration in the songs' production. Remarkably, it has also been shown that, during courtship, adult birds significantly reduce their vocal variability compared to their solitary singing . This switch of behavior from random to more deterministic modes aligns with our hypothesis of the existence of directed variability in the brain.

Finally, NeuroMOP offers several testable predictions regarding the nature of neural variability we should expect in the brain. Firstly, it predicts that neural variability will persist even after extensive training, which aligns with studies reporting large spiking variability even in well-trained non-human primates . Despite this persistence, our model also suggests that neural variability may decrease when terminal states are sufficiently close, as the network is expected to transition into a more deterministic mode to avoid those states . Finally, our model predicts that reward signaling systems in the brain will also signal intrinsic motivation rewards. This is partially supported by recent studies demonstrating that spontaneous movements elicit dopamine release . Further, we postulate that the visitation of all activity states may increase flexibility and help generalization. Consistent with that, certain activity states are observed to be replayed in the absence of any stimulation in the brain, and several mechanisms in RNNs have been proposed for this phenomenon . NeuroMOP predicts the deterministic reactivation of activity patterns and memories that are relevant for generating higher future behavioral entropy, but the more stochastic reactivation of less relevant memories.

LimitationsIn the proposed framework, we choose not to approximate the policy, but instead to rely on an 'oracle' to provide the best action following the derived exact analytical expression (Fig. 1, \(\) box). Our model could be extended to include a neural network to also approximate the policy \(\) using actor critic approaches . Therefore, we do not delve in the process of learning the policy itself. Exploring the policy learning process represents a significant direction for future work. Despite relying on the exact policy, the input current selection has still a high computational cost. In order to partially mitigate this, we introduced the random matrix \(K\), which transforms low \(M\)-dimensional binary actions into high \(N\)-dimensional currents. Via this matrix, we were able to reduce complexity from \((2^{N})\) to \((2^{M})\), thereby significantly speeding up the computation, without compromising the convergence of the algorithm. The extension of NeuroMOP to more realistic spiking and Poisson-like variability is another major possible direction. Finally, another interesting direction that we have not addressed here is how to learn the structure of terminal states, and how nearby 'bad' states surrounding terminal states can be learnt and used to speed up learning.

ConclusionOur results demonstrate that maximizing cumulative future action entropy while avoiding terminal states leads to interesting behaviors without the need of defining an extrinsic reward function. Our work shows that NeuroMOP networks can flexibly switch between stochastic and deterministic modes as needed to avoid terminal states. These results contribute to a novel theory of neural variability based on future entropy production, reconciling stochastic and deterministic behaviors within a single framework. Our work highlights a significant limitation in classical neuroscience studies, where limited behavioral repertoires are promoted by the task design and experimental trials terminate upon reaching the goal. In ecological settings, in contrast, agents continuously generate interim goals and elicit new behaviors. NeuroMOP offers a powerful model of neural activity underlying natural behavior.