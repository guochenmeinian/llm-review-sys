# Cross-Device Collaborative Test-Time Adaptation

Guohao Chen\({}^{1}\)\({}^{2}\)

 Shuaicheng Niu\({}^{3}\)

Deyu Chen\({}^{1}\)

Shuhai Zhang\({}^{1}\)

Changsheng Li\({}^{4}\)

Yuanqing Li\({}^{1}\)

Mingkui Tan\({}^{1}\)

\({}^{1}\)South China University of Technology, \({}^{2}\)Pazhou Laboratory,

\({}^{3}\)Nanyang Technological University, \({}^{4}\)Beijing Institute of Technology,

Equal contribution. Email: secasper@mail.scut.edu.cn, shuaicheng.niu@ntu.edu.sgCorresponding author. Email: mingkuitan@scut.edu.cn, lcs@bit.edu.cn

###### Abstract

In this paper, we propose test-time **C**ollaborative **L**ifelong **A**daptation (CoLA), which is a general paradigm that can be incorporated with existing advanced TTA methods to boost the adaptation performance and efficiency in a multi-device collaborative manner. Specifically, we maintain and store a set of device-shared _domain knowledge vectors_, which accumulates the knowledge learned from all devices during their lifelong adaptation process. Based on this, CoLA conducts two collaboration strategies for devices with different computational resources and latency demands. 1) Knowledge reprogramming learning strategy jointly learns new domain-specific model parameters and a reweighting term to reprogram existing shared domain knowledge vectors, termed adaptation on _principal agents_. 2) Similarity-based knowledge aggregation strategy solely aggregates the knowledge stored in shared domain vectors according to domain similarities in an optimization-free manner, termed adaptation on _follower agents_. Experiments verify that CoLA is simple but effective, which boosts the efficiency of TTA and demonstrates remarkable superiority in collaborative, lifelong, and single-domain TTA scenarios, _e.g._, on follower agents, we enhance accuracy by over 30% on ImageNet-C while maintaining nearly the same efficiency as standard inference. The source code is available at https://github.com/Cascol-Chen/COLA.

## 1 Introduction

The conventional pipeline of deep learning typically trains a model and deploys it across numerous devices with frozen parameters. This pipeline has demonstrated great success in various applications, such as autonomous driving cars , embodied robots , and many other smart devices . However, during deployment, the model on each device may encounter test samples drawn from a domain different from the training one. In some cases, the testing environment even changes continuously and periodically, such as changes in weather. Unfortunately, the deep model often struggles to generalize to unseen testing domains and its performance may degrade significantly.

To resolve domain shifts, test-time adaptation (TTA)  has emerged as a promising research field. TTA updates a given model w.r.t. a testing sample using self-/unsupervised objectives, such as rotation prediction , contrastive learning , entropy minimization , _etc_. Compared to conventional domain adaptation  or fine-tuning  methods that require performing offline model learning on the whole pre-collected target dataset, TTA distinguishes itself with minimal overhead by utilizing each test sample only once for immediate post-inference adaptation. This renders TTA more adaptable in real-world applications.

However, prior TTA methods mainly validate their effectiveness on a single device, _i.e._, re-adapting the model from scratch on each. In practice, models are often deployed across multiple devices.

As shown in Figure 1, in the multi-device adaptation scenario, single-device adaptation methods suffer from the following limitations. **First**, single-device TTA neglects useful knowledge learned from other devices and adapts independently. Since different devices may frequently encounter similar or even identical testing domains, ignoring this shared knowledge often leads to suboptimal adaptation performance, as demonstrated in Table 2. **Second**, due to limited resources or latency demands, some devices may not support the backpropagation operation required by learning-based TTA methods , rendering single-device TTA infeasible. **Third**, even on a single device, models may also encounter dynamically and periodically changing domain shifts. Although recent works have proposed continual TTA to mitigate the catastrophic forgetting issue, such as anti-forgetting regularizer  or restoration schemes , these methods still struggle to accumulate previously learned knowledge over a long-term adaptation process, as shown in Table 1.

In this paper, we propose a test-time **Co**llaborative **L**ifelong **A**daptation (CoLA) method to enable knowledge accumulation, sharing, and exploitation across devices. Our approach exploits both the previously learned knowledge from other devices and the device itself to achieve efficient and collaborative TTA. Specifically, we represent the knowledge learned on each domain of each device by a domain vector, and automatically detect domain changes on each device during its continual adaptation process. These domain vectors are stored and shared across devices upon domain changes for collaborative TTA and catastrophic forgetting mitigation. Based on the shared domain vectors, we first introduce a knowledge reprogramming learning method for _Principal Agents_, _e.g._, the resource-abundant devices, where we enhance TTA performance and efficiency by leveraging available shared knowledge while learning new domain-specific parameters in case existing knowledge is insufficient. The newly learned parameters/knowledge are subsequently stored for shared domain vector set updating. Furthermore, we devise an optimization-free collaborative TTA method, to reduce the computation consumption of TTA and thus enable TTA in latency-sensitive scenarios or resource-limited devices, which we term as _Follower Agents_ in CoLA. We achieve this by directly aggregating the domain knowledge shared by principal agents according to domain similarities.

**Main novelty and contributions.** 1) We introduce a novel and practical collaborative lifelong adaptation paradigm to TTA. This paradigm addresses a practical demand in real-world applications to perform effective adaptation on numerous devices with varying resources and latency requirements simultaneously, meanwhile keeping privacy preserved and communication efficient. 2) We devise domain vectors to explicitly store the domain knowledge and share them across devices for collaborative TTA. Based on this, we devise two collaborative strategies, _i.e._, knowledge reprogramming learning for resource-abundant principal agents and similarity-based knowledge aggregation for resource-limited or latency-sensitive follower agents. 3) Extensive experiments demonstrate the superiority of our CoLA regarding the scenarios of collaborative, lifelong, and single-domain TTA in a plug-and-play manner. By leveraging available shared knowledge, on principal agents, we achieve an up to 78.0\(\) speed up on ETA compared with the baseline without collaborative learning on ImageNet-C. On follower agents, we enhance the accuracy by over 30% while maintaining nearly the same computation and memory efficiency as standard inference on ImageNet-C.

Figure 1: Comparison w.r.t. (a) prior single-device TTA _vs._ (b) our collaborative TTA. Prior TTA operates on each device independently and may be infeasible in resource-limited devices. In contrast, our collaborative TTA allows devices to share knowledge. Based on this, on different devices, one can choose to solely aggregate the shared knowledge for TTA (_Follower Agents_), or further conduct backpropagation for knowledge aggregation and new domain knowledge learning (_Principal Agents_).

## 2 Problem statement and motivation

Let \(f_{}()\) be the model trained on a labeled dataset \(_{train}=\{(_{i},y_{i})\}\) and \(_{i} P()\). After training, \(f_{}()\) is often deployed on various devices, on each device \(f_{}()\) may encounter test samples drawn from a shifted and dynamically changing domain distribution \(Q()\), where \(Q() P()\). Under such domain shifts, deep models are often very sensitive and suffer from severe performance degradation. To address this, on each device, one can adapt \(f_{}()\) to \(\) by optimizing some self-/unsupervised learning objective at test time:

\[_{_{i}}(;_{f},_{l}),\ \ \  Q(),\] (1)

where \(_{f}\) and \(_{l}\) denote frozen and learnable model parameters, respectively.

**Motivation.** Eqn. (1) is known as a single-device test-time adaptation (TTA) method, which naively re-adapts \(f_{}()\) from scratch on each device. In our multi-device adaptation scenario, this independent adaptation manner neglects the valuable knowledge learned from other devices and often obtains limited performance, as in Figure 3(a). Therefore, there is an urgent demand to devise multi-device collaborative TTA methods, to enhance the adaptation performance and efficiency. To this end, the key technical challenge lies in devising a collaboration scheme that effectively exploits the knowledge from other devices while ensuring privacy preservation and communication efficiency.

**Domain knowledge vectors.** In real-world scenarios, a model is often deployed in environments that may change continuously and cyclically, _e.g._, day \(\) night \(\) day. Moreover, the model deployed on different devices may encounter similar environments, experiencing similar domain shifts. In such cases, when a device encounters test samples drawn from a domain previously seen by itself or by other devices, it is unnecessary to conduct adaptation from scratch. Instead, leveraging the previously acquired knowledge can achieve enhanced adaptation. Inspired by this, we seek to explicitly store the knowledge learned on each domain of each device, and then exploit this knowledge for collaborative TTA. We term this knowledge as domain vectors and introduce its definition below.

Formally, given \(m\) devices with each having \(n\) domains, we use a domain vector \(^{i,j}\)=\(^{i,j}_{l}-^{o}_{l}\) to denote the knowledge learned on the \(i\)-th domain of the \(j\)-th device. Here, \(^{i,j}_{l}\) denotes the learned parameters on the \(i\)-th domain of the \(j\)-th device, and \(^{o}_{l}\) denotes the corresponding original learnable parameters. For privacy and efficiency considerations, we select the **affine parameters of the norm layers** as learnable parameters \(_{l}\) and transmit domain vectors between devices for knowledge sharing, which consumes negligible extra cost as in Table 5. We store each knowledge vector \(^{i,j}\) in a set

Figure 2: An illustration of our proposed CoLA. We maintain a shared domain vector set \(}\) to explicitly store the knowledge learned by each principal agent during adaptation. Based on \(}\), for _Principal Agents_, we jointly learn the domain-specific parameters \(\) and the reweighting term \(\) via backward propagation, where the learned knowledge is then stored in \(}\). For _Follower Agents_, we adaptively aggregate the shared knowledge in \(}\) in a forward-only manner, based on the domain similarities, which prioritizes knowledge derived from domains that are similar to the testing domain.

\(}{=}\{^{i,j}\}_{i=1,j=1}^{n,m}\). During the continual adaptation process, we dynamically expand \(}\) by storing a new \(^{i,j}\) in \(}\) once \(^{i,j}\) is learned. For simplicity of presentation, we omit \(}\) as \(\{_{i}\}_{i=1}^{N}\) where \(N{=}mn\) and exploit \(}\) to devise collaborative TTA strategies in the following sections.

## 3 Cross-device collaborative test-time adaptation

In this paper, we propose a test-time **C**ollaborative **L**ifelong **A**daptation (CoLA) method. In CoLA, we seek to conduct collaborative TTA across multiple devices by exploiting a set of shared **domain vectors**, termed \(}\). We automatically detect the domain changes on each device during a continual adaptation process and explicitly store the current domain knowledge in \(}\) once the testing domain is changed (c.f. Sect. 3.3). Then, based on \(}\), we develop two distinct collaboration strategies. In practice, users can determine which strategy to use according to the computational resource of their device or their latency requirements. **First**, the collaborative knowledge reprogramming learning strategy (c.f. Sect. 3.1) is designed for "**principal agents**", _i.e._, the devices that will dominate the learning of new knowledge and have sufficient resources for backpropagation-based model updates. This strategy jointly learns new domain-specific model parameters and a reweighting term to reprogram the knowledge learned from previously encountered distributions from both the device itself and other devices, through backpropagation-based optimization. **Second**, the optimization-free collaborative TTA (c.f. Sect. 3.2) is designed for "**follower agents**", _i.e._, the devices that are resource-limited or latency-sensitive. This strategy mainly exploits the knowledge shared from principal agents, by aggregating the valuable shared knowledge according to distribution similarities. We summarize the pseudo-code in Algorithm 1 and illustrate the overall pipeline of CoLA in Figure 2.

```
0: Test samples \(\{^{m}\}_{m=1}^{M}\), where \(^{m}{=}\{_{t}^{m}\}_{t=1}^{T}\) denotes the batches of test samples on the \(m\)-th device, the model \(f_{}()\) and its stem (first) layer \(f_{_{s}}()\), threshold \(z\).
1: Initialize: shared domain vectors \(}{=}\{\}\), \(_{d}{=}\) for each device.
2:for\(t=1,2,,T\)do
3:for each device (in parallel)do
4: Calculate batch statistics \(_{t}\) over \(f_{_{s}}(_{t}^{m})\);
5: Update distribution statistics \(_{d}\) by Eqn. (6);
6: For Principal Agent: // knowledge reprogramming learning, Sect. 3.1
7:if\(D(_{d},_{t})>z\)then\(\) domain change detection, Eqn. (7)
8: Update domain vectors \(}\) by storing the newly learned \(\) and reset \(_{d}=\).
9:endif
10: Predict \(_{t}^{m}\) by \(f_{}(_{t}^{m})\) based on Eqn. (2);
11: Update \(\) and \(\) via Eqn. (2) with backpropagation;
12: For Follow Agent: // similarity-based aggregation, Sect. 3.2
13: Update \(_{i}\) for different domain knowledge via Eqn. (5);
14: Predict \(_{t}^{m}\) by \(f_{}(_{t}^{m})\) based on Eqn. (3);
15:endfor
16:endfor
17: Predictions \(\{_{t}^{m} t=1,...,T\) and \(m=1,...,M\}\). ```

**Algorithm 1** The overall pipeline of CoLA.

### Collaborative test-time adaptation via knowledge reprogramming learning

We conduct collaborative adaptation with both the knowledge learned from other devices and the device itself. This latter one is particularly advantageous for lifelong adaptation since, in practice, a deployed model may encounter diverse and evolving domains. By storing and reprogramming all knowledge learned from previously encountered domains, we naturally mitigate the issue of catastrophic forgetting during lifelong TTA (see results and analysis in Table 1). To this end, on each device, we detect domain changes and store the learned model parameters for each specific domain once the test domain changes. We then reprogram the knowledge stored in these model parameters by reweighting as below, facilitating seamless adaptation to changing environments.

As aforementioned, we assume there exist \(N\) sets of parameters, _i.e._, domain vectors, learned from previously encountered domains across all devices, denoted as \(}{=}\{_{i}\}_{i=1}^{N}\), where \(_{i}{=}_{i}-_{i}^{o}\). For flexibility, we denote null knowledge as \(_{0}{=}\) and include it in \(}\). We use \(}\) to illustrate our collaborative learning scheme here and put details for detecting and storing each \(_{i}\) in **Sect. 3.3**. Based on \(}\), we learn a reweighting term that adaptively aggregates shared knowledge via backpropagation, while learning new knowledge simultaneously if existing knowledge is insufficient. The overall optimization problem is given by:

\[_{,}(;\ _{f},_{l}),\ \ \ _{l}=_{l}^{o}+_{i=0}^{N}_{i}_{i}+\ \ \ _{i}}=\{_{i}\}_{i=0}^{N}.\] (2)

Here, \(\) denotes learnable parameters for current round of adaptation, and \(\) denotes the normalized weights for different domain knowledge, _i.e._, \(_{i}_{i}=1\). Thus, knowledge reprogramming and new knowledge learning are decoupled into the optimization of \(\) and \(\). Note that we introduce \(_{0}\) to ensure more flexibility in reprogramming, _e.g._, by setting \(_{0}=1\), one can entirely disregard previously learned knowledge when it is not beneficial for adapting to currently encountered domain. Moreover, when no knowledge has been accumulated, _i.e._, \(}=\{\}\), Eqn. (2) simplifies to the conventional TTA. We put details of the initialization for \(\) and \(}\) in Appendix B due to page limits.

**Adaptive temperature scaling for fast adaptation.** Promptly re-weighting the appropriate knowledge for aggregation is key to fast adaptation when the testing distribution suddenly changes. However, this process is hindered when the logits of \(\) are sharpened, making re-weighting difficult to favor another knowledge. To mitigate this, we further introduce an adaptive scaling temperature during the optimization phase, which helps adjust the sharpness of \(\) adaptively while maintaining the smoothness of the original logits. The calculation of \(\) can be thus expressed as \(=( T_{l})\), where \(\) is the logit vector and \(T_{l}\) is a learnable temperature.

### Collaborative test-time adaptation via similarity-based knowledge aggregation

The computational constraints of devices, combined with the real-time demands of various applications, often necessitate TTA to be as efficient as possible. To this end, we propose a forward-only collaborative TTA strategy for devices operating in the follower agent mode. Here, the follower agent adapts a given model by aggregating the previously learned knowledge and the knowledge shared from principal agents without learning new domain-specific parameters, aiming to maintain almost the same efficiency as pure inference. Formally, given domain vectors \(}{=}\{_{i}\}_{i=0}^{N}\) shared from \(m\) principal resource-abundant devices with each having \(n\) encountered domains and \(_{0}=\), the goal of adaptation is to find appropriate normalized weights \(^{*}{}^{N+1}\) such that:

\[^{*}=*{arg\,min}_{}(;\ _{f},_{l}),\ \ \ _{l}=_{l}^{o}+_{i=0}^{N}_{i} _{i}\ \ \ _{i}}=\{_{i}\}_{i=0}^{N}.\] (3)

Here, since backpropagation-based learning is not supported, we directly assign the specific value of each \(_{i}\) approximately according to distribution similarities. We estimate the distribution by calculating the feature statistics, _i.e._, the mean and standard deviation of the features from the first stem layer. Formally, let \(_{d}\) denote the statistics of the current distribution that is online estimated via Eqn. (6), and \(_{i}\) be the distribution statistics on which \(_{i}\) is adapted (_i.e._, the corresponding \(_{d}\) while learning \(_{i}\)), we re-weight the knowledge from different distributions by:

\[=(),\ \ \ \ \ _{i}=1\ /\ D(_{d},_{i})+.\] (4)

Here, \(\) is a logit vector, \(D(,)\) is a distance for which we adopt KL divergence as in Eqn. (7), and \(\) is a small constant for numerical stability. In this way, we can adaptively prioritize shared knowledge learned from distributions that are similar to the current distribution. Note that a principal agent with abundant resources may also leverage Eqn. (3) for efficient TTA in real-time scenarios.

**Exploiting diverse knowledge for aggregation.** Aggregating the advantages of different knowledge is the key to achieving satisfying robustness under various distribution shifts, as shown in Figure 3 (b). However, when distributions are highly similar, the re-weighting logit \(_{i}\) shall become sufficiently large (_e.g._, \(_{i}>10\)) and the softmax function tends to simplify to the max function, which hinders the potential of Eqn. (3) from aggregating a diverse set of knowledge. To alleviate this, we further introduce a pre-defined temperature scaling factor \(T_{f}\) to soften \(_{i}\), thereby encouraging the aggregation of more existing knowledge. Then, \(_{i}\) is re-defined as:

\[_{i}=1\;/\;T_{f} D(_{d},_{i})+.\] (5)

**Remark.** It is worth noting that our CoLA can be incorporated with existing TTA techniques as a plug-and-play module for a more effective solution (as in Table 1 and Table 3). Furthermore, unlike prior methods  that impose intensive transmission of both data and model weights, CoLA offers several merits for real-world implementation: 1) CoLA involves only the transmission of learned parameters \(_{i}\), which preserves user privacy and imposes much less communication burden (_e.g._, the affine parameters of the norm layers in ViT-Base , which are typically updated in TTA [54; 38], occupy only 0.15 MB). 2) the domain vectors are preserved and shared intermittently with a shift detector, which further reduces the communication burden by a considerable margin. 3) CoLA is decentralized and flexible, which allows all agents to join or leave the collaboration at any time.

### Automatic domain shift detection for constructing domain knowledge vectors \(}\)

In this section, we introduce the construction of the domain knowledge vectors \(}\) shared across multiple devices. As aforementioned, explicitly accumulating/storing the learned knowledge from each domain of each device in \(}\) is a key step in our CoLA for collaborative learning. However, in practice, during the lifelong adaptation process of each device, we do not have any prior information on the domain labels regarding a given test sample stream. To conquer this, we devise an efficient distribution shift detector to identify whether the test distribution changes, and then automatically store the currently learned model weights to the domain vector set \(}\) once the domain change is detected. We achieve this by measuring the discrepancy between the distribution's statistics \(_{d}\) and the statistics of the current test batch \(_{t}\). Formally, let \(_{_{s}}()\) be the stem layer of \(f_{}()\), _i.e._, the first layer. \(_{t}\) comprises the mean \(_{t}\) and standard deviation \(_{t}\) calculated over \(f_{_{s}}(_{t})\). Then, we estimate the statistics \(_{d}\) from the observed test samples \(\{_{t}\}_{t=1}^{n}\) via exponential moving average:

\[_{d}=_{t}+(1-)_{d},\] (6)

where \(\) is a moving average factor belongs to \(\). Inspired by existing distance-based detection methods , we capture the magnitude of distribution shifts by a distance function \(D(,)\) as follows.

\[D(_{d},_{t})=_{i=1}^{H}KL(_{d,i}||_ {t,i})+KL(_{t,i}||_{d,i}),\;\;KL(_{1}||_{2})=^{2}}(_{1}^{2}+(_{1}-_{2})^{2}),\] (7)

where \(H\) denotes the dimension of statistics, and \(KL(||)\) is the KL-divergence simplified from . Here, a distribution shift is detected when \(D(_{d},_{t})>z\), where \(z\) is a pre-defined threshold. This simple design offers several merits: 1) It imposes minimal computational and memory costs without necessitating data preservation. 2) By leveraging the features from the stem layer, we can promptly detect and respond to distribution shifts, rendering it well-suited for the online nature of TTA.

## 4 Experiments

**Datasets and models.** We conduct experiments on the ImageNet-1k , as well as five benchmarks for OOD generalization, _i.e._, ImageNet-C  (contains corrupted images in 15 types of 4 main categories and each type has 5 severity levels), ImageNet-R (various artistic renditions of 200 ImageNet classes) , ImageNet-Sketch , ImageNet-A , and ImageNet-V2 . We use ViT-Base  as the source model unless stated otherwise. The model is trained on the source ImageNet-1K  training set and the model weights are obtained from the timm repository .

**Compared methods and implementation details.** We compared our proposed CoLA with 1) Backpropagation-based methods: CoTTA , ETA , EATA , SAR , and DeYO . 2) Backpropagation-free methods: LAME  and T3A . For Eqn. (2), We directly leverage the learnable test-time objectives from the integrated TTA methods. \(\) is optimized by following the update rules of the integrated baseline as listed in Appendix C. \(\) is updated via the AdamW optimizer with a learning rate of 0.1. The shift detection threshold \(z\) is set to 0.1. For follower agents, we consistently set \(T_{f}\) in Eqn. (3) to 5 for all experiments. More details are put in Appendix A and C.

### Comparison with state-of-the-art methods

**Results under lifelong test-time adaptation.** We evaluate the long-term effectiveness of our CoLA on ImageNet-C  within a challenging lifelong TTA scenario where the model is online adapted to 15 corruptions over 10 rounds (total 150 corruptions), and the parameters will never be reset. We put more details of the experimental protocol in Appendix C due to page limits. From Table 1, we derive the following observations. 1) Our CoLA achieves new state-of-the-art results on the first round, last round, and the average of adaptation, suggesting our superiority. 2) Most methods, including CoTTA  and EATA  with an anti-forgetting strategy, experience performance degradation as the number of adaptation rounds increases (_e.g._, ETA's performance degrades from \(61.4\%\) to \(35.1\%\) on the average accuracy), indicating the difficulty of the evaluated scenario. 3) By integrating our CoLA with existing methods, we enhance the performance steadily with more adaptations, demonstrating our effectiveness in accumulating and exploiting learned knowledge for long-term adaptation. 4) Although EATA mitigates performance degradation by introducing an anti-forgetting regularization, it suffers from the _stability-plasticity_ trade-off, _i.e._, the average accuracy drops from 61.4% (ETA) to 60.4% (EATA) in the first round. In contrast, our CoLA enhances ETA's performance even at the first round of adaptation, _i.e._, 61.4% (ETA) _vs._ 62.0% (ETA+CoLA), indicating that CoLA does not limit the learning ability. The sensitivity analyses on threshold \(z\) are provided in Appendix E.

**Results under collaborative test-time adaptation.** To evaluate our CoLA under the collaborative TTA scenario, we first assess its performance across multiple principal resource-abundant devices. From Table 2, our CoLA outperforms the integrated baseline from the adaptation to the second group of corruptions, _e.g._, the accuracy of 58.0% (SAR+CoLA) _vs._ 54.4% (SAR) on 'Blur' in Devices 1. Moreover, this improvement becomes increasingly more pronounced as more knowledge is shared across devices, _e.g._, improving the accuracy from 47.7% (SAR) to 55.5% (SAR+CoLA) on 'Noise' in Device 3. This demonstrates our effectiveness in facilitating knowledge sharing and exploitation across principal devices via our knowledge reprogramming learning scheme, _i.e._, with Eqn. (2).

  Time: \\ Round \\  } &  &  &  \\  & Noise & Blur & Weat. & Digit. & Blur & Noise & Digit. & Weat. & Weat. & Digit. & Blur & Noise & Avg. \\  NoAdapt & 8.2 & 28.4 & 36.1 & 41.7 & 28.4 & 8.2 & 41.7 & 36.1 & 36.1 & 41.7 & 28.4 & 8.2 & 28.6 \\ CoTTA  & 28.9 & 41.3 & 50.2 & 47.6 & 36.3 & 37.1 & 50.4 & 52.7 & 50.4 & 55.0 & 42.5 & 38.7 & 44.2 \\ EATA  & 53.5 & 57.0 & 68.1 & 67.2 & 58.1 & 52.2 & 67.0 & 68.5 & 69.4 & 67.7 & 57.9 & 51.5 & 61.5 \\  SAR  & 50.4 & 54.4 & 66.3 & 64.5 & 55.1 & 48.3 & 64.0 & 66.4 & 66.5 & 64.3 & 55.4 & 47.7 & 58.6 \\ + CoLA (Ours) & 50.4 & 58.0 & 69.4 & 68.7 & 55.0 & 55.0 & 67.1 & 70.5 & 66.3 & 65.3 & 58.8 & 55.5 & 61.7 \\  ETA  & 55.2 & 56.9 & 67.5 & 66.0 & 59.8 & 51.7 & 65.0 & 67.4 & 70.3 & 67.8 & 58.0 & 49.4 & 61.2 \\ + CoLA (Ours) & 55.2 & 60.0 & 70.9 & 69.3 & 59.5 & 56.3 & 68.8 & 70.9 & 70.2 & 68.1 & 59.7 & 55.3 & **63.7** \\  DeYO  & 56.3 & 49.9 & 68.1 & 67.8 & 55.6 & 46.7 & 67.2 & 69.0 & 71.1 & 68.8 & 51.1 & 4.3 & 56.3 \\ + CoLA (Ours) & 56.2 & 55.1 & 71.2 & 70.2 & 54.8 & 54.5 & 70.0 & 71.5 & 71.0 & 69.0 & 53.7 & 54.3 & 62.6 \\ 

Table 2: Effectiveness under collaborative adaptation across **principal resource-abundant devices** w.r.t. **Accuracy (%)**. Results are evaluated on ImageNet-C (level 5, containing 15 corruption types of 4 groups). We share learned weights across devices post-adaptation to each group of corruptions.

  Time: \\ Round \\  } &  &  &  \\  & Noise & Blur & Weat. & Digit. & Blur & Noise & Digit. & Weat. & Weat. & Digit. & Blur & Noise & Avg. \\  NoAdapt & 8.2 & 28.4 & 36.1 & 41.7 & 28.4 & 8.2 & 41.7 & 36.1 & 36.1 & 41.7 & 28.4 & 8.2 & 28.6 \\ CoTTA  & 28.9 & 41.3 & 50.2 & 47.6 & 36.3 & 37.1 & 50.4 & 52.7 & 50.4 & 55.0 & 42.5 & 38.7 & 44.2 \\ EATA  & 53.5 & 57.0 & 68.1 & 67.2 & 58.1 & 52.2 & 67.0 & 68.5 & 69.4 & 67.7 & 57.9 & 51.5 & 61.5 \\  SAR  & 50.4 & 54.4 & 66.3 & 64.5 & 55.1 & 48.3 & 64.0 & 66.4 & 66.5 & 64.3 & 55.4 & 47.7 & 58.6 \\ + CoLA (Ours) & 50.4 & 58.0 & 69.4 & 68.7 & 55.0 & 55.0 & 67.1 & 70.5 & 66.3 & 65.3 & 58.8 & 55.5 & 61.7 \\  ETA  & 55.2 & 56.9 & 67.5 & 66.0 & 59.8 & 51.7 & 65.0 & 67.4 & 70.3 & 67.8 & 58.0 & 49.4 & 61.2 \\ + CoLA (Ours) & 55.2 & 60.0 & 70.9 & 69.3 & 59.5 & 56.3 & 68.8 & 70.9 & 70.2 & 68.1 & 59.7 & 55.3 & **63.7** \\  DeYO  & 56.3 & 49.9 & 68.1 & 67.8 & 55.6 & 46.7 & 67.2 & 69.0 & 71.1 & 68.8 & 51.1 & 4.3 & 56.3 \\ + CoLA (Ours) & 56.2 & 55.1 & 71.2 & 70.2 & 54.8 & 54.5 & 70.0 & 71.5 & 71.0 & 69.0 & 53.7 & 54.3 & 62.6 \\ 

Table 1: Comparison on ImageNet-C (level 5) regarding **Accuracy (%)** under lifelong adaptation for 10 rounds, in total of 150 corruptions, on **a single principal resource-abundant device**. We report the average accuracy of 15 corruptions in each round here and put more results in Appendix D.

Given learned knowledge from resource-abundant principal devices (totaling 34 weights occupying 5.0 MB), we further evaluate the effectiveness of CoLA on resource-limited follower devices. From Table 3, we observe that existing TTA methods struggle to improve the performance of the source model without model updates, highlighting the urgent need for a more effective solution. In contrast, by exploiting shared knowledge adaptively in a forward-only manner with Eqn. (3), CoLA achieves a substantial performance gain, _e.g._, enhancing the average accuracy from 29.9% to 64.1% in CoLA (ETA). Note that we also verify CoLA's sample efficiency as well as its computation and memory efficiency in Figure 3 (a) and Table 5. These results collectively underscore the importance of cross-device collaboration and our effectiveness regarding the scenario of collaborative TTA.

**Results under single-domain test-time adaptation.** Following DeYo , we validate our CoLA in both the wild scenario (_i.e._, imbalanced label distribution shifts and mixture of distribution shifts) and the mild scenario of single-domain TTA, where the model is reset post-adaptation to each corruption. Here, CoLA saves learned weights for every adaptation to 10 batches of samples while maintaining a maximum of 32 weights (totaling 4.7 MB) by discarding the unused ones according to \(_{i}\).

From Table 4, within all evaluated scenarios, incorporating CoLA consistently improves the performance by a considerable margin (_e.g._, +2.0% on SAR w.r.t. overall average accuracy). Interestingly, the enhancement from CoLA may even help surpass a stronger baseline, _e.g._, the average accuracy of 64.4% (ETA+CoLA) _vs._ 64.1% (DeYO) on the mild scenario, demonstrating our effectiveness. This improvement mainly stems from our ability to alleviate error accumulation. Given multiple saved weights, instead of naively selecting the newest weight that may have adapted to noise, CoLA dynamically favors the more optimal one via loss optimization. This renders CoLA more robust to scenarios where perturbations may occur. We also visualize \(_{i}\) in Appendix E to offer more insights.

### Ablation studies and more discussions

**Effectiveness of \(T_{l}\) on sample efficiency in Eqn. (2).** Sample efficiency is particularly important in scenarios where the availability of target data is limited or early adaptation performance is paramount. As shown in Figure 3 (a), by leveraging the learned knowledge from other devices, ETA+CoLA achieves an up to \(78.0\) speed up compared with ETA, _i.e._, 51.7% accuracy with 640 samples (ETA+CoLA) _vs._ 51.37% accuracy with 49,920 samples (ETA), demonstrating the importance

 Method &  Mild \\  &  L.S. \\  &  M.S \\  & 
 Avg. \\  \\  NoAdapt & 29.9 & 29.9 & 29.9 & 29.9 \\ SAR  & 54.5 & 56.7 & 57.1 & 56.1 \\ + CoLA (Ours) & 57.7 & 58.5 & 58.0 & 58.1 \\  ETA  & 63.3 & 47.6 & 57.4 & 56.1 \\ + CoLA (Ours) & 64.4 & 55.2 & 58.3 & 59.3 \\  DeYO  & 64.1 & 61.3 & 59.1 & 61.5 \\ + CoLA (Ours) & 64.7 & 63.5 & 59.3 & 62.5 \\  \\ 

Table 4: Comparison under single-domain TTA (on one principal device) w.r.t. **Acc (%)**. Results are averaged over 15 corruptions on ImageNet-C (level 5). **L.S** denotes label distribution shifts, **MS** denotes mixed domain shifts per SAR .

 Method &  Noise \\  &  Blur \\  &  Weather \\  &  Digital \\  & 
 Avg. \\  \\  NoAdapt & 9.5 & 6.7 & 8.2 & 29.0 & 23.4 & 33.9 & 27.1 & 15.9 & 26.5 & 47.2 & 54.7 & 44.1 & 30.5 & 44.5 & 47.8 & 29.9 \\ T3A  & 9.5 & 7.0 & 8.7 & 23.3 & 23.3 & 31.2 & 25.9 & 11.9 & 24.2 & 44.0 & 52.2 & 41.0 & 30.1 & 43.0 & 47.0 & 28.2 \\ T3A*  & 9.5 & 6.5 & 8.1 & 29.8 & 24.1 & 34.3 & 28.2 & 16.0 & 26.9 & 49.0 & 55.5 & 44.5 & 33.1 & 44.5 & 48.2 & 30.5 \\ LAME  & 9.3 & 6.5 & 8.0 & 28.6 & 23.0 & 33.3 & 26.6 & 15.2 & 26.0 & 45.9 & 54.1 & 43.6 & 29.3 & 44.0 & 47.4 & 29.4 \\  CoLA (SAR) & 55.2 & 56.0 & 56.8 & 57.3 & 49.1 & 59.9 & 58.5 & 65.8 & 65.8 & 72.2 & 77.1 & 66.2 & 65.9 & 72.2 & 69.4 & 63.2 \\ CoLA (ETA) & 55.7 & 57.3 & 56.9 & 58.5 & 46.2 & 59.4 & 63.4 & 69.1 & 66.5 & 73.1 & 77.6 & 66.3 & 69.2 & 73.1 & 69.9 & **64.1** \\ CoLA (DeYO) & 56.6 & 57.7 & 57.5 & 58.2 & 47.7 & 55.5 & 39.0 & 69.6 & 67.2 & 73.5 & 78.0 & 67.0 & 70.4 & 73.5 & 70.3 & 62.8 \\ 

Table 3: Comparison on ImageNet-C (level 5) regarding **Accuracy (%)** under lifelong adaptation on **resource-limited follower devices**. T3A* resets the model after adaptation on each corruption. CoLA exploits the learned weights from Table 2 (_e.g._, ETA + CoLA at the 7-th row) for Eqn. (3).

 Method & BP & C & R & \(\) (s) & Mem. (MB) \\  NoAdapt & ✗ & 9.5 & 43.1 & 50 & 816.6 \\ T3A  & ✗ & 9.5 & 42.1 & 158 & 909.9 \\ CoLA (Eqn.3) & ✗ & 55.7 & 51.5 & 51 & 821.9 \\  EATA  & ✗ & 49.5 & 56.8 & 113 & 7439.3 \\ SAR  & ✗ & 44.0 & 51.8 & 202 & 7429.9 \\  ETA  & ✗ & 51.9 & 57.5 & 109 & 7429.6 \\ + CoLA (Eqn.2) & ✗ & 54.3 & 59.0 & 112 & 7435.3 \\ 

Table 5: Comparison w.r.t. wall-clock time and memory on ImageNet-C (Gaussian, level 5) on an A100 GPU. C/R refers to accuracy on ImageNet-C/R. **BP** is short for back-propagation. CoLA utilizes weights of ETA+CoLA in Table 2.

[MISSING_PAGE_FAIL:9]

average accuracy. Such improvement can be attributed to the transferability of knowledge learned from similar domains . These findings collectively indicate that the effectiveness of CoLA is not limited to previously encountered distributions on both principal agents and follower agents.

**Prompt tuning with CoLA using multiple hard prompts.** Besides aggregating learned knowledge from other devices, we demonstrate that CoLA can also benefit from aggregating diverse knowledge from humans (_i.e._, manual-designed hard prompts). As shown in Table 7, compared with TPT which is limited to leveraging a single hard prompt, CoLA enhances the adaptation performance on 4 out of 5 datasets (_e.g._, +1.5% on ImageNet w.r.t. accuracy). These results collectively indicate that CoLA effectively exploits both the knowledge of humans and the knowledge from optimization, which may bring new perspectives to the design of learning algorithms when introducing diverse human prior knowledge is beneficial, _e.g._, chain-of-thoughts . All used prompts are listed in Appendix C.

**Differences and advantages over federated TTA .** The main difference is that our CoLA conducts collaborative learning at the testing phase, whereas federated TTA conducts collaborative learning during federated source training. For instance, FedTHE+  federatedly trains a global and a local personalized model for each client, then adaptively ensemble their outputs at test time. ATP  federatedly learns module-specific adaptation rates across clients during training for test-time adaptation. However, these methods still conduct TTA independently on each devices during testing, and thus inherits the limitation of the single-device TTA methods. Moreover, in federated TTA , the training phase and test-time adaptation phase are highly correlated, which means they can only use their own federated-trained models during TTA. This makes these methods restricted for real-world applications. In contrast, our CoLA enhances test-time model adaptation performance and efficiency by leveraging knowledge from multiple devices in the application environment, which essentially establishes a new unsupervised on-time TTA paradigm. Moreover, our CoLA paradigm can be applied to any pre-trained models, and thus offers much better flexibility in deployment. Additional comparisons with FedAvg  for collaborative adaptation are also provided in Appendix E.

## 5 Conclusion

In this paper, we propose a multi-device **Co**llaborative **L**ifelong **A**daptation (CoLA) paradigm for test-time adaptation (TTA), which addresses a practical scenario where multiple devices with different computational resources and latency requirements need to perform TTA simultaneously. In particular, we first accumulate a set of shared domain knowledge vectors with an efficient domain shift detector. Based on this, we develop a knowledge reprogramming learning strategy on principal agents, which leverages backpropagation-based optimization to aggregate existing knowledge while learning new domain-specific parameters simultaneously. To further improve adaptation efficiency, we introduce an optimization-free TTA strategy on follower agents, which solely aggregates the shared domain vectors based on domain similarity. In CoLA, all devices/agents work collaboratively while keeping privacy preserved and communication efficient. Experiments verify that CoLA boosts the performance and efficiency of existing TTA solutions in collaborative, lifelong, and single-domain TTA scenarios.