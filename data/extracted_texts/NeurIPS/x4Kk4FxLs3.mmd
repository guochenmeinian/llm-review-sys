# Pard: Permutation-invariant Autoregressive Diffusion

for Graph Generation

 Lingxiao Zhao

Carnegie Mellon University

lingxiao2lx@gmail.com &Xueying Ding

Carnegie Mellon University

xding2@andrew.cmu.edu &Leman Akoglu

Carnegie Mellon University

lakoglu@andrew.cmu.edu

###### Abstract

Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to node ordering. Diffusion models, on the other hand, have garnered increasing attention as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, however they require extra features and thousands of denoising steps to achieve optimal performance. We introduce Pard, a Permutation-invariant AutoRegressive Diffusion model that integrates diffusion models with autoregressive methods. Pard harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without order sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, Pard generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, Pard achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard

## 1 Introduction

Graphs provide a powerful abstraction for representing relational information in many domains, including social networks, biological and molecular structures, recommender systems, and networks of various infrastructures such as computers, roads, etc. Accordingly, generative models of graphs that learn the underlying graph distribution from data find applications in network science , drug discovery , protein design , and various use-cases for Internet of Things . Importantly, they serve as a prerequisite for building a generative foundation model  for graphs.

Despite significant progress in generative models for images and language, graph generation is uniquely challenged by its inherent combinatorial nature. Specifically: 1) Graphs are naturally high-dimensional and _discrete_ with _varying sizes_, contrasting with the continuous space and fixed-size advancements that cannot be directly applied here; 2) Being permutation-invariant objects, graphs require modeling an _exchangeable probability_ distribution, where permutations of nodes and edges do not alter a graph's probability; and 3) The rich substructures in graphs necessitate an expressive model capable of capturing _higher-order_ motifs and interactions. Several graph generative models have been proposed to address (part of) these challenges, based on various techniques like autoregression , VAEs , GANs , flow-based methods , and denoising diffusion . Among these, autoregressive models and diffusion models stand out with superior performance, thus significant popularity. However, current autoregressive models, whileefficient, are sensitive to node/edge order with non-exchangeable probabilities; whereas diffusion models, though promising, are less efficient, requiring thousands of denoising steps and extra node/edge/graph-level features (structural and/or domain-specific) to achieve high generation quality.

In this paper, we introduce (leopard in Ancient Greek), the _first_ Permutation-invariant AutoRegressive Diffusion model that combines the efficiency of autoregressive methods and the quality of diffusion models together, while retaining the property of exchangeable probability. Instead of generating an entire graph directly, we explore the direction of generating through _block-wise_ graph enlargement. Graph enlargement offers a fine-grained control over graph generation, which can be particularly advantageous for real-world applications that require local revisions to generate graphs. Moreover, it essentially decomposes the joint distribution of the graph into a series of simpler conditional distributions, thereby leveraging the data efficiency characteristic of autoregressive modeling. We also argue that graphs, unlike sets, inherently exhibit a _unique partial order_ among nodes, naturally facilitating the decomposition of the joint distribution. Thanks to this unique partial order, (block-wise autoregressive sequence is permutation-invariant, unlike any prior graph autoregressive methods in the literature.

To model the conditional distribution of nodes and edges within a block, we have, for the first time, identified a fundamental challenge in equivariant models for generation: it is impossible for _any_ equivariant model, no matter how powerful, to perform general graph transformations without symmetry breaking. However, through a diffusion process that injects noise, a permutation equivariant network can progressively denoise to realize targeted graph transformations. This approach is inspired by the annealing process where energy is initially heightened before achieving a stable state, akin to the process of tempering iron. Our analytical findings naturally lead to the design of our proposed (tm) that combines autoregressive approach with local block-wise discrete denoising diffusion. Using a diffusion model with equivariant networks ensures that each block's conditional distribution is exchangeable. Coupled with the permutation-invariant block sequence, this renders the entire process permutation-invariant and the joint distribution exchangeable. What is more, this inevitable combination of autoregression and diffusion successfully combines the strength of both approaches while getting rid of their shortcomings: By being permutation-invariant, a key advantage of diffusion, it generalizes better than autoregressive models while also being much more data-efficient. By decomposing the challenging joint probability into simpler conditional distributions, an advantage of autoregression, it requires significantly fewer diffusion steps, outperforming pure diffusion methods by a large margin. Additionally, each inference step in the diffusion process incurs lower computational cost by processing only the generated part of the graph, rather than the entire graph. And it can further leverage caching mechanisms (to be explored in future) to avoid redundant computations.

Within (), we further propose several architectural improvements. First, to achieve 2-FWL expressivity with improved memory efficiency, we propose a higher-order graph transformer that integrates transformer with PPGN , while utilizing a significantly reduced representation size for edges. Second, to ensure training efficiency without substantial overhead compared to the original diffusion model, we design a GPT-like causal mechanism to support parallel training of all blocks. These extensions are generalizable and can lay the groundwork for a higher-order GPT.

() achieves new SOTA performance on many molecular and non-molecular datasets _without any extra features_, significantly outperforming DiGress . Thanks to efficient architecture and parallel training, () scales to large datasets like MOSES  with 1.9M graphs. Finally, not only can serve as a generative foundation model for graphs in the future, its autoregressive parallel mechanism can further be combined with language models for language-graph generative pretraining.

## 2 Related Work

**Autoregressive (AR) Models for Graph Generation.** AR models create graphs step-by-step, adding nodes and edges sequentially. This method acknowledges graphs' discrete nature but faces a key challenge as there is no inherent order in graph generation. To address this, various strategies have been proposed to simplify orderings and approximate the marginalization over permutations; i.e. \(p(G)=_{(G)}p(G,)\). Li et al.  propose using random or deterministic empirical orderings. GraphRNN  aligns permutations with breadth-first-search (BFS) ordering, with a many-to-one mapping. GRAN  offers marginalization over a family of canonical node orderings, including node degree descending, DFS/BFS tree rooted at the largest degree node, and k-core ordering. GraphGEN  uses a single canonical node ordering, but does not guarantee the same canonical ordering during generation. Chen et al.  avoid defining ad-hoc orderings by modeling the conditional probability of orderings, \(p(|G)\), with a trainable AR model, estimating marginalized probabilities during training to enhance both the generative model and the ordering probability model.

**Diffusion Models for Graph Generation.** EDP-GNN  is the first work that adapts score matching  to graph generation, by viewing graphs as matrices with continuous values. GDSS  generalizes EDP-GNN by adapting SDE-based diffusion  and considers node and edge features. Yan et al.  argues that learning exchangeable probability with equivariant networks is hard, hence proposes permutation-sensitive SwinGNN with continuous-state score matching. Previous works apply continuous-state diffusion to graph generation, ignoring the natural discreteness of graphs. DiGress  is the first to apply discrete-state diffusion [3; 20] to graph generation and achieves significant improvement. However, DiGress relies on many additional structural and domain-specific features. GraphArm  applies Autoregressive Diffusion Model (ADM)  to graph generation, where exactly one node and its adjacent edges decay to the absorbing states at each forward step based on a _random_ node order. Similar to AR models, GraphArm is permutation sensitive.

We remark that although both are termed "autoregressive diffusion", it is important to distinguish that Pard is _not_ ADM. The term "autoregressive diffusion" in our context refers to the integration of autoregressive methods with diffusion models. In contrast, ADM represents a specific type of discrete denoising diffusion where exactly one dimension decays to an absorbing state at a time in the forward diffusion process. See Fan et al.  for a survey of recent diffusion models on graphs.

## 3 Permutation-invariant Autoregressive Denoising Diffusion

We first introduce setting and notations. We focus on graphs with categorical features. Let \(G=(,)\) be a _labeled_ graph with the number of distinct node and edge labels denoted \(K_{v}\) and \(K_{e}\), respectively. Let \(^{i}\{0,1\}^{K_{v}}, i\) be the one-hot encoding of node \(i\)'s label. Let \(^{i,j}\{0,1\}^{K_{e}}, i,j\) be the one-hot encoding of the label for the edge between node \(i\) and \(j\). We also represent "absence of edge" as a type of edge label, hence \(||=||||\). Let \(\{0,1\}^{|| K_{v}}\) and \(\{0,1\}^{|||| K_{e}}\) be the collection of one-hot encodings of all nodes and edges using the _default node order_, and let \(:=(,)\). To describe probability, let \(\) be a random variable with its sampled value \(\). Similarly, \(\) is a random graph with its sampled graph \(\). In diffusion process, noises are injected from \(t{=}0\) to \(t{=}T\) with \(T\) being the maximum time step. Let \(_{0} p_{}(_{0})\) be the random variable of observed data with underlying distribution \(p_{}(_{0})\), \(_{t} q(_{t})\) be the random variable at time \(t\), and let \(_{t|s} q(_{t}|_{s})\) denote the conditional random variable. Also, we interchangeably use \(q(_{t}|_{s})\), \(q(_{t}{=}_{t}|_{s}{=}_{s})\), and \(q_{t|s}(_{t}|_{s})\) when there is no ambiguity. We model the _forward diffusion_ process independently for each node and edge, while the _backward denoising_ process is modeled jointly for all nodes and edges. All vectors are column-wise vectors. Let \(,\) denote inner product.

### Discrete Denoising Diffusion on Graphs

Denoising Diffusion is first developed by Sohl-Dickstein et al.  and later improved by Ho et al. . It is further generalized to discrete-state case by Hoogeboom et al.  and Austin et al. . Taking a graph \(_{0}\) as example, diffusion model defines a forward diffusion process to gradually inject noise to all nodes and edges independently until all reach a non-informative state \(_{T}\). Then, a denoising network is trained to reconstruct \(_{0}\) from the noisy sample \(_{t}\) at each time step, by optimizing a Variational Lower Bound (VLB) for \( p_{}(_{0})\). Specifically, the forward process is defined as a Markov chain with \(q(_{t}|_{t-1}), t[1,T]\), and the backward denoising process is parameterized with another Markov chain \(p_{}(_{t-1}|_{t}), t[1,T]\). Note that while the forward process is independently applied to all elements, the backward process is coupled together with conditional independence assumption. Formally,

\[q(_{t}|_{t-1})=_{i}q(_{t}^ {i}|_{t-1}^{i})_{i,j}q(_{t}^{i,j}| _{t-1}^{i,j}), p_{}(_{t-1}|_{t})= _{i}p_{}(_{t-1}^{i}|_{t}) _{i,j}p_{}(_{t-1}^{i,j}|_{t})\.\] (1)

Then, the VLB of \( p_{}(_{0})\) can be written (see Apdx. SSA.1) as

\[ p_{}(_{0})_{q(_{1}| _{0})} p_{}(_{0}|_{1}) }_{-_{1}()}-}(q(_{T}| _{0})||p_{}(_{T}))}_{_{}}- _{t=2}^{T}_{q(_{1}|_{0})}D _{}(q(_{t-1}|_{t},_{0})||p_{ }(_{t-1}|_{t})]}_{_{i}()}\] (2)

where \(_{} 0\), since \(p_{}(_{T}) q(_{T}|_{0})\) is designed as a fixed noise distribution that is easy to sample from. To compute Eq. (2), we need to formalize the distributions \((i)\)\(q(_{t}|_{0})\) and \((ii)\)\(q(_{t-1}|_{t},_{0})\), as well as (\(iii\)) the parameterization of \(p_{}(_{t-1}|_{t})\). DiGress  applies D3PM  to define these three terms. Different from DiGress, we closely follow the approach in Zhao et al.  to define these three terms, as their formulation is simplified with improved memory usage and loss computation. For brevity, we refer readers to Appx. SSA.2 for the details. Notice that while a neural network can directly be used to parameterize \((iii)\)\(p_{}(_{t-1}|_{t})\), we follow  to parameterize \(p_{}(_{0}|_{t})\) instead, and compute (\(iii\)) from \(p_{}(_{0}|_{t})\).

With (\(i iii\)) known, one can compute the negative VLB loss in Eq. (2) exactly. In addition, at each time step \(t\), the cross entropy (CE) loss between (\(i\)) \(q(_{t}|_{0})\) and \(p_{}(_{0}|_{t})\) that quantifies reconstruction quality is often employed as an auxiliary loss, which is formulated as

\[_{t}^{CE}()=-_{q(_{t}|_{0})} _{i} p_{}(_{0}^{i}|_{t})+_{ i,j} p_{}(_{0}^{i,j}|_{t})\;.\]

In fact, DiGress solely uses \(_{t}^{CE}()\) to train their diffusion model. In this paper, we adopt a hybrid loss , that is \(_{t}()+_{t}^{CE}()\) with \(=0.1\) at each time \(t\), as we found it to help reduce overfitting. To generate a graph from \(p_{}(_{0})\), a pure noise graph is first sampled from \(p_{}(_{T})\) and gradually denoised using the learned \(p_{}(_{t-1}|_{t})\) from step \(T\) to 0.

A significant advantage of diffusion models is their ability to achieve exchangeable probability in combination with permutation equivariant networks under certain conditions . DiGress is the first work that applied discrete denoising diffusion to graph generation, achieving significant improvement over previous continuous-state based diffusion. However, given the inherently high-dimensional nature of graphs and their complex internal dependencies, modeling the joint distribution of all nodes and edges directly presents significant challenges. DiGress requires thousands of denoising steps to accurately capture the original dependencies. Moreover, DiGress relies on many extra supplementary node and graph-level features, such as cycle counts and eigenvectors, to effectively break symmetries among structural equivalences to achieve high performance.

### Autoregressive Graph Generation

Order is important for AR models. Unlike diffusion models that aim to capture the joint distribution directly, AR models decompose the joint probability into a product of simpler conditional probabilities based on an order. This makes AR models inherently suitable for ordinal data, where a natural order exists, such as in natural languages and images.

**Order Sensitivity**. Early works of graph generation contain many AR models like GraphRNN  and GRAN  based on non-deterministic heuristic node orders like BFS/DFS and k-core ordering. Despite being permutation sensitive, AR models achieve SOTA performance on small simulated structures like grid and lobster graphs. However, permutation invariance is necessary for estimating an accurate likelihood of a graph, and can benefit large-size datasets for better generalization.

Let \(\) denote an ordering of nodes. To make AR order-_insensitive_, there are two directions: (1) Modeling the joint probability \(p(,)\) and then marginalizing \(\), (2) Finding a unique canonical order \(^{*}()\) for any graph G such that \(p(|)\) = 1 if \(=^{*}()\) and 0 otherwise. In direction (1), directly integrating out \(\) is prohibitive as the number of permutations is factorial in the graph size. Several studies [27; 9; 28] have used subsets of either random or canonical orderings. This approach aims to simplify the process, but it results in approximated integrals with indeterminate errors. Moreover, it escalates computational expense due to the need for data augmentation involving these subsets of orderings. In direction (2), identifying a universal canonical order for all graphs is referred to as graph canonicalization. There exists no polynomial time solution for this task on general graphs, which is at least as challenging as the NP-intermediate Graph Isomorphism problem . Goyal et al.  explored using minimum DFS code to construct canonical labels for a specific dataset with non-polynomial time complexity. However, the canonicalization is specific to each training dataset with the randomness derived from DFS. This results in a generalization issue, due to the canonical order being \((|)\) instead of \(()\).

**The Existence of Partial Order**. While finding a unique order for all nodes of a graph is NP-intermediate, we argue that finding a unique _partial_ order, where certain nodes and edges are with the same rank, is easily achievable. For example, a trivial partial order is simply all nodes and edges having the same rank. Nevertheless, a graph is not the same as a set (a set is just a graph with empty \(\)), where all elements are essentially unordered with equivalent rank. That is because a non-empty graph contains edges between nodes, and these edges give different structural properties to nodes. Notice that some nodes or edges have the same structural property as they are structurally equivalent.

We can view each structural property as a color, and rank all unique colors within the graph to define the partial order over nodes, which we call a _structural partial order_. The structural partial order defines a sequence of _blocks_ such that all nodes within a block have the same rank (i.e. color).

Let \(:[1,...,K_{B}]\) be the function that assigns rank to nodes based on their structural properties, where \(K_{B}\) denotes the maximum number of blocks. We use \(G[]\) to denote the induced subgraph on the subset \(\). There are many ways to assign rank to structural colors, however we would like the resulting partial order to satisfy certain constraints. Most importantly, we want

\[ r[1,...,K_{B}],\ G[() r]\] (3)

The connectivity requirement is to ensure a more accurate representation of real-world graph generation processes, where it is typical of real-world dynamic graphs to enlarge with newcoming nodes being connected at any time. Then, _one can sequentially remove all nodes with the lowest degree to maintain this connectivity and establish a partial order_. However, degree only reflects limited information of the first-hop neighbors, and many nodes share the same degree--leading to only a few distinct blocks, not significantly different from a trivial, single-block approach.

To ensure connectivity while reducing rank collision, we consider larger hops to define a weighted degree. Consider a maximum of \(K_{h}\) hops. For any node \(\), the number of neighbors at each hop of \(\) can be easily obtained as \([d_{1}(),...,d_{K_{h}}()]\). We then define the weighted degree as

\[w_{K_{h}}()=_{k=1}^{K_{h}}d_{k}()||^{K_{h}-k}\] (4)

Equation Eq. (4) may appear as an ad-hoc design, but it fundamentally serves as a hash that maps the vector \([d_{1}(),d_{2}(),...,d_{K}()]\) to a unique scalar value. This mapping ensures a one-to-one correspondence between the vector and the scalar. Furthermore, it prioritizes lower-hop degrees over higher-hop degrees, akin to how e.g. the number "123" is represented as \(1 10^{2}+2 10^{1}+3 10^{0}\). Eq. (4) is efficient to compute and guarantees: 1) Nodes have the same rank if and only if they have the same number of neighbors up to \(K_{h}\) hops. 2) Lower-hop degrees are weighted more heavily. With \(w_{K_{h}}\) defined, we present our structural partial order in Algo. 1.

**Proposition 3.1**.: _For any \(G\), its structural partial order \(\) defined by Algo. 1 is permutation equivariant, such that \(()=()\) for any permutation operator \(\)._

It is easy to prove Prop. 3.1. Algo. 1 shows that \((i)\) for \( i\) is uniquely determined by node \(i\)'s structural higher-order degree. As nodes' higher-order degree is permutation equivariant, \(\) is also permutation equivariant. Notice that \(\) is unique and deterministic for any graph.

**Autoregressive Blockwise Generation.**\(\) in Algo. 1 with output in range \([1,K_{B}]\) divides the nodes \((G)\) into \(K_{B}\) blocks \([_{1},...,_{K_{B}}]\) in order, where \(_{j}=\{i(G)|(i)=j\}\). Let \(_{1:i}:=_{j=1}^{i}_{j}\) be the union of the first \(i\) blocks. \(}}\) decomposes the joint probability of a graph \(G\) into

\[p_{}()=_{i=1}^{K_{B}}p_{}[ _{1:i}][_{1:i-1}]\ \ [_{1:i-1}]\] (5)

where \([_{1:0}]\) is defined as the empty graph, and \([_{1:i}][_{1:i-1}]\) denotes the set of nodes and edges that are present in \([_{1:i}]\) but not in \([_{1:i-1}]\). All of them are represented in natural order of \(G\). As each conditional probability only contains a subset of edges and nodes, and having access to all previous blocks, this conditional probability is significantly easier to model than the whole joint probability. Given the property Prop. 3.1 of \(_{i}\), it is easy to verify that \(p_{}()\) is exchangeable with permutation-invariant probability for any \(G\) if and only if all conditional probabilities are exchangeable. See Appx. SSA.6 for details of permutation-invariant probability. Note that while GRAN  also generates graphs block-by-block, all nodes within GRAN have different generation

Figure 1: Example case where the equivariant graph transformation from \(G[_{1:i}]\) to \(G[_{1:i+1}]\) is impossible for _any_ permutation-equivariant network due to structural equivalence of nodes. See Proposition 3.3.

ordering, even within the same block (it breaks symmetry for the problem identified later). Essentially, GRAN is an autoregressive method and, as such, suffers from all the disadvantages inherent to AR.

### Impossibility of Equivariant Graph Transformation

In Eq. (5), we need to parameterize the conditional probability \(p_{}[_{1:i}][_{1:i-1 }]\ [_{1:i-1}]\) to be permutation-invariant. This can be achieved by letting the conditional probability be

\[p_{}_{i}\ \ [_{1:i-1 }]_{[_{1:i}][ _{1:i-1}]}p_{}\ \ [_{1:i-1}][ _{1:i}]\] (6)

where \(\) is any node and edge in \([_{1:i}][_{1:i-1}]\), \(\) denotes an empty graph, hence \([_{1:i-1}][_{1:i}]\) depicts augmenting \([_{1:i-1}]\) with empty (or virtual) nodes and edges to the same size as \([_{1:i}]\). With the augmented graph, we can parameterize \(p_{}\ \ [_{1:i-1}] [_{1:i}]\) for any node and edge \(\) with a permutation equivariant network to achieve the required permutation invariance. For simplicity, let \(_{1:i-1},\ |_{i}|:=[ _{1:i-1}][_{1:i}]\).

**The Flaw in Equivariant Modeling**. Although the parameterization in Eq. (6) along with an equivariant network makes the conditional probability in Eq. (5) become permutation-invariant, we have found that the equivariant graph transformation \(p_{}(\ |\ _{1:i-1},\ |_{i}|)\) cannot be achieved in general for _any_ permutation equivariant network, no matter how powerful it is (!) For definition of graph transformation, see Appx. SSA.7. The underlying cause is the symmetry of structural equivalence, which is also a problem in link prediction . Formally, let \((G)\) be the adjacency matrix of \(G\) (ignoring labels) based on \(G\)'s default node order, then an _automorphism_\(\) of \(G\) satisfies

\[(G)=( G)\] (7)

where \( G\) is a reordering of nodes based on the mapping \(\). Then the automorphism group is

\[(G)=\{_{||}\ |\ (G)= ( G)\}\] (8)

where \(_{n}\) denotes all permutation mappings for size \(n\). That is, \((G)\) contains all automorphisms of \(G\). For a node \(i\) of \(G\), the _orbit_ that contains node \(i\) is defined as

\[o(i)=\{(i)\ |\ (G)\}\.\] (9)

In words, the orbit \(o(i)\) contains all nodes that are _structurally equivalent_ to node \(i\) in \(G\). Two edges \((i,j)\) and \((u,v)\) are structurally equivalent if \((G)\), such that \((i)=u\) and \((j)=v\).

**Theorem 3.2**.: _Any structurally equivalent nodes and edges will have identical representations in any equivariant network, regardless of its power or expressiveness._

See proof in Apdx. SSA.5. Theorem 3.2 indicates that no matter how powerful the equivariant network is, any structurally equivalent elements have the same representation, which implies the following.

**Proposition 3.3**.: _General graph transformation is not achievable with any equivariant model._

Proof.: To prove it, we only need to show there are many "bottleneck" cases where the transformation cannot be achieved. Fig. 1 shows a case where \(G[_{1:}]\) is a 4-cycle, and the next target block contains two additional nodes, each with a single edge connecting to one of the nodes of \(G[_{1:}]\). It is easy to see that nodes \(1\)-\(4\) are all structurally equivalent, and so are nodes \(5,6\) in the augmented case (middle). Hence, edges in \(\{(5,i)| i\}\) are structurally equivalent (also \(\{(6,i)| i\}\)). Similarly, \( i\), edge \((5,i)\) and \((6,i)\) are structurally equivalent. Combining all cases, edges in \(\{(j,i)| i,j\{5,6\}\}\) are structurally equivalent. Theorem 3.2 states that all these edges would have the _same_ prediction, hence making the target \(G[_{1:i+1}]\) not achievable. 

### Pard: Autoregressive Denoising Diffusion

**The Magic of Annealing/Randomness**. In Fig. 1 we showed that a graph with many automorphisms cannot be transformed to a target graph with fewer automorphisms. We hypothesize that _a graph with lower "energy" is hard to be transformed to a graph with higher "energy" with equivariant networks._ There exist some definitions and discussion of graph energy  based on symmetry and eigen-information to measure graph complexity, where graphs with more symmetries have lower energy. The theoretical characterization of the conditions for successful graph transformation is a valuable direction, which we leave for future work to investigate.

Based on the above hypothesis, to achieve a successful transformation of a graph into a target graph, it is necessary to increase its energy. Since graphs with fewer symmetries exhibit higher energy levels, our approach involves adding random noise to nodes and edges. Our approach of elevating the energy level, followed by its reduction to attain desired target properties, mirrors the annealing process.

**Diffusion.** This further motivates us to use denoising diffusion to model \(p_{}(\![_{1:i-1},|_{ i}|])\): it naturally injects noise in the forward process, and its backward denoising process is the same as annealing. As we show below, this yields \(p_{}(G)\) in Eq. (5) to be permutation-invariant.

**Theorem 3.4**.: _Pard is permutation-invariant such that \(p_{}(\!.)=p_{}( \!)\)\(\) permutator \(\)._

The proof is given in Appx. SSA.6. With all constituent parts presented, we summarize our proposed Pard, the first permutation-invariant autoregressive diffusion model that integrates AR with denoising diffusion. Pard relies on a unique, permutation equivariant structural partial order \(\) (Algo. 1) to decompose the joint graph probability to the product of simpler conditional probabilities, based on Eq. (5). Each block's conditional probability is modeled with the product of a conditional block size probability and a conditional block enlargement probability as in Eq. (6), where the latter for every block is a shared discrete denoising diffusion model as described in SS3.1. Fig. 2 illustrates Pard's two parts : (top) block-wise AR and (bottom) local denoising diffusion at each AR step.

Notice that there are two tasks in Eq. (6); one for predicting the next block's size, and the other for predicting the next block's nodes and edges with diffusion. These two tasks can be trained together with a single network, although for better performance we use two different networks. For each block's diffusion model, we set the maximum time steps to 40 without much tuning.

**Training and Inference.** We provide the training and inference algorithms for Pard in Apdx. SSA.8. Specifically, Algo. 2 is used to train next block's size prediction model; Algo. 3 is used to train the shared diffusion for block conditional probabilities; and Algo. 4 presents the generation steps.

## 4 Architecture Improvement

Pard is a general framework that can be combined with any equivariant network. Nevertheless, we would like an equivariant network with enough expressiveness to process symmetries inside the generated blocks for modeling the next block's conditional probability. While there are many expressive GNNs like subgraph GNNs  and higher-order GNNs , PPGN  is still a natural choice that models edge (2-tuple) representations directly with 3-WL expressivity and \(O(n^{3})\) complexity in graph size. However, PPGN's memory cost is relatively high for many datasets.

### Efficient and Expressive Higher-order Transformer

To enhance the memory efficiency of PPGN while maintaining the expressiveness equivalent to the 3-Weisfeiler-Lehman (3-WL) test, we introduce a hybrid approach that integrates Graph Transformers with PPGN. Graph Transformers operate on nodes as the fundamental units of representation, offering better scalability and reduced memory consumption (\(O(n^{2})\)) compared to PPGN. PPGN utilizes edges as their primary representation units and therefore incurs significantly higher memory requirements (\(O(n^{3})\)). However, the expressiveness of Graph Transformers (without position encoding) is limited to the 1-WL test . By combining these two models, we can drastically decrease the size of edge representations while allocating larger hidden sizes to nodes. This synergistic approach not only substantially lowers the memory footprint but also enhances overall performance, leveraging the

Figure 2: Pard integrates the autoregressive method with diffusion modeling. (top) Pard decomposes the joint probability into a series of block-wise enlargements, where each blockâ€™s conditional distribution is captured with a shared discrete diffusion (bottom).

strengths of both architectures to achieve a balance between expressivity and efficiency. We provide the detailed design in Appx. SSA.9. Note that we use GRIT  as the graph transformer block.

### Parallel Training with Causal Transformer

As shown in Eq. (5), for a graph \(G\), there are \(K_{B}\) conditional probabilities being modeled by a shared diffusion model using a \(\)-parameterized network \(f_{}\). By default, these \(K_{B}\) number of inputs \(\{[_{1:i-1}]\}_{i=1}^{K_{B}}\) are viewed as separate graphs and the representations during network passing \(f_{}([_{1:i-1}])\) for different \(i[1,K_{B}]\) are not shared. This leads to a scalability issue; in effect enlarging the dataset by roughly \(K_{B}\) times and resulting in \(K_{B}\) times longer training.

To minimize computational overhead, it is crucial to enable parallel training of all the \(K_{B}\) conditional probabilities, and allow these processes to share representations, through which we can pass the full graph \(G\) to the network \(f_{}\) only once and obtain all \(K_{B}\) conditional probabilities. This is also a key advantage of transformers over RNNs. Transformers (GPTs) can train all next-token predictions simultaneously with representation sharing through causal masking, whereas RNNs must train sequentially. However, the default causal masking of GPTs is not applicable to our architecture, as Pard contains both Transformer and PPGN where the PPGN's causal masking is not designed.

To ensure representation sharing without risking information leakage, we first assign a "block ID" to every node and edge within graph \(G\). Specifically, for every node and edge in \(G[_{1:i}] G[_{1:i-1}]\), we assign the ID equal to \(i\). To prevent information leakage effectively, it is crucial that any node and edge labeled with ID \(i\) are restricted to communicate only with other nodes and edges whose ID is \( i\). Let \(,^{n n}\), and \(^{n}\). There are mainly two non-elementwise operations in Transformer and PPGN that have the risk of leakage: the attention-vector product operation \(\) of Transformer, and the matrix-matrix product operation \(\) of PPGN. (We ignore the feature dimension of \(\) and \(\) as it does not affect the information leakage.) Let \(\{0,1\}^{n n}\) be a mask matrix, such that \(_{i,j}=1\) if block_ID(node \(i\)) \(\) block_ID(node \(j\)) else 0. One can verify that

\[(),()+ (^{})-()(^{})\] (10)

generalize \(\) and \(\) respectively and safely bypass information leakage. We provide more details of parallel training and derivation of Eq. (10) in Appx. SSA.10. We use these operations in our network and enable representation sharing, along with parallel training of all \(K_{B}\) blocks for denoising diffusion as well as next block size prediction. In practice, these offer more than 10\(\) speed-up, and the parallel training allows Pard to scale to large datasets like MOSES .

## 5 Experiments

We evaluate Pard on 8 diverse benchmark datasets with varying sizes and structural properties, including both molecular (SS5.1) and non-molecular/generic (SS5.2) graph generation. A summary of the datasets and details are in Appx. SSA.11. Ablations and runtime measures are in Appx. SSA.12.

### Molecular Graph Generation

**Datasets.** We experiment with three different molecular datasets used across the graph generation literature: (1) QM9  (2) ZINC250k , and (3) MOSES  that contains more than 1.9 million graphs. We use a 80%-20% train and test split, and among the train data we split additional 20% as validation. For QM9 and ZINC250k, we generate 10,000 molecules for stand-alone evaluation, and on MOSES we generate 25,000 molecules.

**Baselines.** The literature has not been consistent in evaluating molecule generation on well-adopted benchmark datasets and metrics. Among baselines, DiGress  stands out as the most competitive. We also compare to many other baselines shown in tables, such as GDSS  and GraphARM .

**Metrics.** The literature has adopted a number of different evaluation metrics that are not consistent across datasets. Most common ones include Validity (\(\)), Uniqueness (\(\)) (frac. of valid molecules that are unique), and Novelty (\(\)) (frac. of valid molecules that are not included in the training set). For QM9, following earlier work , we report additional evaluations w.r.t. Atom Stability (\(\)) and Molecule Stability (\(\)), as defined by , whereas Novelty is not reported as explained in . On ZINC250k and MOSES, we also measure the Frechet ChemNet Distance (FCD) (\(\)) between the generated and the training samples, which is based on the embedding learned by ChemNet . For MOSES, there are three additional measures: Filter (\(\)) score is the fraction of molecules passing the same filters as the test set, SNN (\(\)) evaluates nearest neighbor similarity using Tanimoto Distance, and Scaffold similarity (\(\)) analyzes the occurrence of Bemis-Murcko scaffolds .

**Results.** Table 1 shows generation evaluation results on QM9, where the baseline results are sourced from . \(\) outperforms DiGress and variants that do _not_ use any auxiliary features, with slightly lower Uniqueness. What is notable is that \(\), without using any extra features, achieves a similar performance gap against DiGress that uses specialized extra features. Table 2 shows \(\)'s performance on \(\), with baseline results carried over from  and . \(\) achieves the best Uniqueness, stands out in FCD alongside SwinGNN , and is the runner-up w.r.t. Validity.

Finally, Table 3 shows generation quality on the largest dataset MOSES. We mainly compare with DiGress and its variants, which has been the only general-purpose generative model in the literature that is not based on molecular fragments or SMILES strings. Baselines are sourced from . While other specialized models, excluding \(\) and DiGress, have hard-coded rules to ensure high Validity, \(\) outperforms those on several other metrics including FCD and SNN, and achives competitive performance on others. Again, it is notable here that \(\), _without_ relying on any auxiliary features, achieves similarly competitive results as with DiGress which utilizes extra features.

### Generic Graph Generation

**Datasets.** We use five generic graph datasets with various structure and semantic: (1) \(\), (2) \(\), (3) \(\), (4) \(\), and (5) \(\). We split each dataset into 80%-20% train-test, and randomly sample 20% of training graphs for validation. We generate the same number of samples as the test set. Notice that \(\) contains graphs with 100\(\)400 nodes, which is relatively large for diffusion models. There are lots of symmetries inside, hence it is difficult to capture all dependencies with permutation-equivariant models.

**Baselines.** We mainly compare against the latest general-purpose GraphArm , which reported DiGress  and GDSS  as top two most competitive, along with several other baselines.

**Metrics.** We follow  to measure generation quality using the maximum mean discrepancy (MMD) as a distribution distance between the generated graphs and the test graphs (\(\)), as pertain to distributions of (\(i\)) Degree, (\(ii\)) Clustering coefficient, and (\(iii\)) occurrence count of all Orbits.

  
**Model** & **Valid.**\(\) & **Uni.**\(\) & \(\) & \(\) \\  Dataset (optimal) & 97.8 & 100 & 98.5 & 87.0 \\  ConGress & 86.7 & **98.4** & 97.2 & 69.5 \\ DiGress (uniform) & 89.8 & 97.8 & 97.3 & 70.5 \\ DiGress (marginal) & 92.3 & 97.9 & 97.3 & 66.8 \\ DiGress (marg. + _feat._) & 95.4 & 97.6 & 98.1 & 79.8 \\    
  
**Model** & **Validity**\(\) & **FCD**\(\) & **Uni.**\(\) & **Model Size** \\   \(\)-GNN & 82.97 & 16.74 & 99.79 & 0.09M \\  GraphEBM & 5.29 & 35.47 & 98.79 & - \\ SPECIRE & 90.20 & 18.44 & 67.05 & - \\ \(\) & **97.01** & 14.66 & 99.64 & 0.37M \\ GraphArm & 88.23 & 16.26 & 99.46 & - \\ DiGress & 91.02 & 23.06 & 81.23 & 18.43M \\ SwinGNN-L & 90.68 & 1.99 & 99.73 & 35.91M \\   \(\) & 95.23 & **1.98** & **99.99** & 4.1M \\   

Table 2: Generation quality on ZINC250k.

  
**Model** & **Val.**\(\) & **Uni.**\(\) & **Novel.**\(\) & **Filters**\(\) & **FCD**\(\) & **SNN**\(\) & **Sear.**\(\) \\  VAE & 97.7 & 99.8 & 69.5 & 99.7 & 0.57 & 0.58 & 5.9 \\ JT-VAE & 100 & 100 & 99.9 & 97.8 & 1.00 & 0.53 & 10.0 \\ GraphEvent & 96.4 & 99.8 & - & 95.0 & 1.22 & 0.54 & 12.7 \\  ConGress & 83.4 & 99.9 & **96.4** & 94.8 & 1.48 & 0.50 & **16.4** \\ DiGress & 85.7 & **100** & 95.0 & 97.1 & 1.19 & 0.52 & 14.8 \\   \(\) & **86.8** & **100** & 78.2 & **99.0** & **1.00** & **0.56** & 2.2 \\   

Table 3: Generation quality on MOSES. The top three methods use hard-coded rules (not highlight).

    & \)} & \)} & \)} & \)} & \)} \\ 
**Model** & **Deg.** & **Clus.** & **Orbit** & **Deg.** & **Clus.** & **Orbit** & **Deg.** & **Clus.** & **Orbit** & **Deg.** & **Clus.** & **Orbit** & **Deg.** & **Clus.** & **Orbit** \\  GraphRNN & 0.080 & 0.120 & 0.040 & 0.371 & 1.035 & 0.033 & 1.689 & 0.608 & 0.308 & 0.103 & 0.138 & 0.005 & 0.064 & 0.043 & 0.021 \\ GRAN & 0.060 & 0.110 & 0.050 & 0.043 & 0.130 & 0.018 & 0.125 & 0.272 & 0.127 & 0.073 & 0.413 & 0.010 & - & - & - \\ EDP-GNN & 0.053 & 0.144 & 0.026 & 0.032 & 0.168 & 0.030 & 0.093 & 0.269 & 0.062 & 0.131 & 0.038 & 0.019 & 0.455 & 0.238 & 0.328 \\ GDSS & 0.045 & 0.086 & 0.007 & 0.019 & 0.048 & 0.006 & 0.160 & 0.376 & 0.187 & 0.113 & **0.020** & 0.003 & 0.111 & 0.005 & 0.070 \\ GraphArm & 0.034 & 0.082 & **0.004** & 0.039 & **0.028** & 0.118 & 0.273 & 0.138 & 0.105 & **0.036** & 0.041 & 0.002 & - & - & - \\ DiGress & 0.047 & **0.041** & 0.026 & 0.019 & 0.040 & 0.003 & 0.044 & 0.0223 & 0.152 & 0.024 & 0.008 & - & - & - \\   \(\) & **0.023** & 0.071 & 0.012 & **0.002** & 0.047 & **0.0003** & **0.0003** & **0.003** & **0.0097** & 0.044 & 0.024 & **0.003** & **0

**Results.** Table 4 provides the generation results of Pard against the baselines as sourced from . Pard shows outstanding performance achieving SOTA or close runner-up results, while none of the baselines shows as consistent performance across datasets and metrics.

### Ablation Study

**Q1: do we really need AR in diffusion, given diffusion is already permutation-invariant?**

In DiGress , we observed that pure diffusion, while being permutation-invariant, requires (1) many sampling/denoising steps to break symmetries and (2) additional features like eigenvectors to further break symmetry. This shows that directly capturing the FULL joint distribution and solving the transformation difficulty (in Sec. 3.3) via diffusion is challenging. Additionally, AR methods still dominate LLMs, indicating their potential to benefit diffusion models. To quantitatively verify our analysis, we perform an ablation study on the maximum number of hops, \(K_{h}\), which controls the extent of autoregression (AR). When \(K_{h}=0\), all nodes have a fixed degree of 1, resulting in a single block per graph, equivalent to full diffusion without AR. As \(K_{h}\) increases, more blocks are generated with smaller average block sizes, indicating a greater number of AR steps.

Table 5 shows the result of the controlled experiment with 140 total diffusion steps across all trials, using the same model architecture, diffusion algorithm, and training settings. The significant improvement from \(K_{h}=0\) to \(K_{h}=1\) confirms that our enhancement stems from breaking the full joint distribution into several conditional distributions. Furthermore, adding more diffusion steps to full diffusion approach does not close the gap to AR enhanced diffusion, indicates the necessary of combining AR and diffusion.

**Q2: how does model architecture affect performance?**

Table 6 this indicates that PPGN component is essential for diffusion with autoregression. The design of combining PPGN and transformer in Sec. 4 further addresses the efficiency of PPGN.

**Other ablations:** other ablations and runtime measures are in Appx. SSA.12

## 6 Conclusion

We presented Pard, the _first_ permutation-invariant autoregressive diffusion model for graph generation. Pard decomposes the joint probability of a graph autoregressively into the product of several block conditional probabilities, by relying on a unique and permutation equivariant structural partial order. All conditional probabilities are then modeled with a shared discrete diffusion. Pard can be trained in parallel on all blocks, and efficiently scales to millions of graphs. Pard achieves SOTA performance on molecular and non-molecular datasets without using any extra features. Notably, we expect Pard to serve as a cornerstone toward generative foundation modeling for graphs.

  
**Backbone** & **Transformer** & **PPGN** & **PPGNTransformer** \\  Validity & 26.3 & 96.6 & 97.1 \\ Uniqueness & 94.5 & 96.3 & 96.0 \\ Mol Stability & 17.6 & 84.67 & 86.2 \\ Atom Stability & 81.4 & 98.2 & 98.4 \\   

Table 6: Results for QM9 dataset with different model architectures with \(K_{h}=3\) and 140 total steps.

  
**Setting** & **No AR** &  &  \\  Total diffusion steps & 140 &  &  & 490 \\ Maximum hops & 0 & 1 & 2 & 3 & 0 & 0 \\ Average number of blocks & 1 & 4.3 & 5.6 & 7.75 & 1 & 1 \\ Diffusion steps per block & 140 & 32 & 25 & 20 & 280 & 490 \\  Validity & 93.8 & **97.1** & 96.7 & 97.0 & 94.3 & 95.2 \\ Uniqueness & **96.9** & 96.5 & 96.2 & 96.1 & 96.5 & 96.9 \\ Mol stability & 76.4 & 86.1 & 85.4 & **86.3** & 79.3 & 79.2 \\ Atom Stability & 97.7 & 98.3 & 98.3 & **98.4** & 97.9 & 98.0 \\   

Table 5: Ablation study on QM9 with varying maximum hops while keeping the total diffusion steps fixed (first two parts). The last part examines the effect of increasing steps for the no AR case.