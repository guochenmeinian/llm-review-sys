# Differentially Private Equivalence Testing for

Continuous Distributions and Applications

 Daniel Omer

Math. Dept.

Bar-Ilan University

omerdan@biu.ac.il &Or Sheffet

Faculty of Engineering

Bar-Ilan University

or.sheffet@biu.ac.il

###### Abstract

We present the first algorithm for testing equivalence between two continuous distributions using differential privacy (DP). Our algorithm is a private version of the algorithm of Diakonikolas et al . The algorithm of  uses the data itself to repeatedly discretize the real line so that -- when the two distributions are far apart in \(_{k}\)-norm -- one of the discretized distributions exhibits large \(L_{2}\)-norm difference; and upon repeated sampling such large gap would be detected. Designing its private analogue poses two difficulties. First, our DP algorithm can _not_ resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line. In contrast, the (sorted) index of the discretization point changes only by \(1\) between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling. Second, our algorithm, which doesn't resample data, requires we also revisit the utility analysis of the original algorithm and prove its correctness w.r.t. the original sorted data; a problem we tackle using sampling a subset of Poisson-drawn size from each discretized bin. Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications.

## 1 Introduction

Differential privacy (DP), a mathematically rigorous notion that bounds the effect of any single datum on the output distribution, is the current (de facto) gold-standard of privacy preserving data analysis. By now we have a myriad of DP-algorithms for learning and for various tasks of _statistical inference_. Indeed, the design of DP-hypothesis testers is crucial for the dissipation of DP into other data-centric fields -- such as economics, education and health -- that analyze sensitive data in massive quantities. However, by and large the design of DP-hypothesis testing is confined to distributions over finite (and thus discrete) domains rather than continuous distributions.

Hypothesis testing over continuous distributions poses a special challenge due to infinitesimally small perturbations -- two continuous distributions can have Total-Variation distance of \(\) using, say, exponentially many intervals each with an exponentially small shift of probability mass, which clearly cannot be detected with polynomially-size sample. Luckily, this issue was resolved in the works of  who restricted the TV-distance to using \(k\)-intervals. Formally, these works measure distance in \(_{k}\)_-norm_: where for any two distributions \(\) and \(\) we have that \(\|-\|_{_{k}}=_{}_{j =1}^{k}|(I_{j})-(I_{j})|\) where \(\) is a partition of the real-line \(\) into \(k\) intervals \(I_{1},I_{2},...,I_{k}\).

But the fact remains that continuous distributions pose a special challenge for DP-algorithm designers. In fact, releasing even a simple statistics like the median is impossible over infinite domains .

And yet, as we show in this work, it is possible to compare (samples from) two continuous distributions while preserving differential privacy and discern whether the two are identical or far-apart in \(_{k}\)-distance. This suggests a sharp contrast between the task of _learning_ from a (single) continuous distribution and the task of _statistical inference_ based on _two_ continuous distributions.

Baseline.This sharp contrast may seem striking at first, yet on second thought, it is known that any statistical inference task that only has two possible outputs (in our case - "accept / identical" vs "reject / far-apart") can easily be made private using the Subsample-and-Aggregregate framework  -- simply run \(O(1/)\)-times the non-private equivalence tester of Diakonikolas et al  and return the most prevalent output using simple noisy count. This gives a simple \(\)-DP equivalence tester with \(O((k^{4/6}^{-6/5}+k^{1/2}^{-2})/)\) sample complexity with two clear drawbacks: (1) its large sample complexity bound and (2) its decision to accept or reject is explained as 'in the majority of the runs of the tester it decided so'. The algorithm we present in this work improves on both aspects: it has a sample complexity of \((\{}{^{6/5}},}{^ {2}},}{ c^{1/3}},}{^{4/3}c^{2/3}}, {}{}\})\) and it is capable of producing numerical estimators directly from the data that explain its accept/reject decision.

Our Algorithm.The difference that makes learning substantially more difficult from equivalence testing is that when having two distributions we are capable of "pitting one against the other": roughly speaking, we can partition the real line into intervals based on points from one distribution and see whether this partition acts as a random partition for the batch of examples that come from the second distribution. This involves only _sorting_ all \(s\) points from our sample on the real line and dealing with the order statistics. Our algorithm is based on privizing the equivalence tester of , which operates over the continuous real line. This algorithm works in two stages: In the first stage, it looks at autocorrelation statistics that involve all \((s-1)\) pairs of adjacent (post-sorting) data points. In the second stage, it equipartitions the data using into \(m\) bins using a random draw of \(m\) points and repeatedly runs the following operation: run a closeness \(L_{2}\)-norm based tester on the discretized \(m\)-bins distribution that draws \(N\) new points, and should it not reject - merge every pair of adjacent bins to create a \(m/2\)-bins discretization and continue.

Our DP-version of this tester also works in similar stages. The first stage is almost trivially privatized (the statistics estimated in the first stage exhibits small global sensitivity) but numerous difficulties arise in the privatization of the second stage. Most notably -- the fact we use datapoints to define a partition into bins. To that end, we replace it so that we use sorted-indices, implying that a bin is composed of all datapoints in sorted indices from index \(_{i}\) to index \(_{i+1}\) (not including). This asserts that any single bin changes by at most two datapoints between neighboring samples. Combining this with the fact that under the null-hypothesis the difference between the number of points from \(\) and \(\) in each bin is proportional to the square-root of the bin size, we can bound the sensitivity in a Propose-Test-Release fashion  for any single bin. However, the approach of fixing sorted indices raises two concerns. The first is that we have to revisit the utility analysis of  because we no longer use re-sample new points to estimate the \(L_{2}\)-norm difference of the two discretized distributions but rather re-visit the same dataset from which the bin-defining datapoints are taken. We bypass this difficulty by sampling ourselves Poisson-drawn subsample of each bin, and by focusing our analysis on a single iteration that should cause us to reject.

The second concern lies in the privacy analysis, and it is the observation that a change to a single bin isn't enough to bound the sensitivity of our statistical estimator. The algorithm computes an \(L_{2}\)-approximation of the norm-difference using all bins, and in the extreme case of two neighboring datasets that differ on the very first (sorted) datapoint that ends up as the very last datapoint in the neighboring dataset we have that _all_ bins shift. To that end we use the following novel idea: we add a \((}{{2}})\)-random variable to each of the bin-defining indices. On the one hand, this possible shift by 1 changes very little in terms of the analysis; on the other hand, it allows us to argue that within \(_{4}(}{{4}})\) random shifts we can correlate these Bernoulli random variables so that the bins identify. Full details of our algorithm and its analysis appear in Section 3.

Applications.Having designed our private equivalence tester for continuous distributions under \(_{k}\)-norm, we can now apply it to test equivalence between two distributions from a family \(\) of distributions which, under suitable partition, yield large enough \(_{k}\)-distance. The following fact is immediate.

**Fact 1**.: _Given a univariate distribution family \(\) and \(\), let \(k=k(,)\) be the smallest integer such that for any \(f_{1},f_{2}\) it holds that \(\|f_{1}-f_{2}\|_{1}\|f_{1}-f_{2}\|_{_{k}}+/2\). Then there exists a \((,)\)-DP equivalence testing algorithm for \(\) using \(O(\{k^{}{{}}}/}{{ }},}}{{}}/}}{{}}, }}{{}}/}}{{}}/}{{}}^{2/3},}}{{}}\})\) samples._

In Section 4 we detail, much like , a variety of such hypothesis-families and the sample complexities we obtain for their respective DP-equivalence testers.

### Related Work

Over the past twenty years, significant progress has been made in distribution property testing. Initially, research focused on the sample sizes required to assess properties of arbitrary distributions with a specific support size. Goldreich and Ron  introduced uniformity testing, proposing an algorithm using collision statistics with a sample complexity of \(}{^{4}}\). Batu et al  studied closeness testing, evaluating whether two distributions are close in total variation. Paninski  established the first lower bound for the sample complexity of uniformity testing at \((}{^{2}})\) and proposed a new uniformity test using unique element statistics. The works of [1; 10; 29] explored optimal bounds for identity testing, focusing on \(^{2}\)-based testers. They established a lower bound for closeness testing at \(O(}{^{4/3}}+}{^{2}})\), with  providing matching upper bounds for \(L_{2}\) closeness testing under certain conditions. . introduced a technique reducing distribution norms to \(O()\) by expanding the domain size, demonstrating an efficient \(L_{2}\) tester for closeness testing. Recent works [11; 15; 16] have leveraged structural assumptions for more efficient testers in various settings, including continuous cases. These studies used the \(_{k}\) metric, aligning with the Kolmogorov distance for \(k=2\) and total variation distance for \(k\) being the domain size. Goldreich  showed that identity tests could be reduced to uniformity testing. While earlier testers relied on proxy measures like \(L_{2}\) and \(^{2}\), and [12; 13] demonstrated efficient sample complexity using direct \(L_{1}\) metric testers due to low sensitivity. We refer the interested reader to Cannone's excellent survey .

Several recent papers have examined hypothesis testing problems under the framework of differential privacy. Cai et al  used \(^{2}\) statistic for identity testing, achieving a sample complexity of \((\{}{^{2}},}{ ^{3/2}},}{^{5/3}^{2/3}}\}(1/ ))\). Aliakbarpour et al  proposed three algorithms for differentially private uniformity and closeness testing. They privatized unique element algorithms, collision-based testers, and \(^{2}\) tests, each with specific sample complexities and limitations. Acharya et al  established a lower bound for private identity testing, suggesting that small expected Hamming distances might compromise privacy guarantees. They proposed a sample complexity concerning privacy and distance parameters by privatizing an \(L_{1}\) statistic-based algorithm. Aliakbarpour et al  also examined closeness testing between distributions with unequal sample sizes, introducing a new technique to privatize the 'flattening' method through data permutation. Zhang  derived an upper bound for closeness testing by privatizing an empirical total variation method, demonstrating small sensitivity and sample complexity of \(O(}{^{2}}+}{^{4/3}}+}{}+}{^{4/3}^{2/3}}+ {})\).

Comparison to Existing Lower Bounds.The well-known lower bound for \(_{k}\) closeness testing in the non-private regime is given in . It is equal to \(O(}{^{6/5}}+}{^{2}})\). As far as we know, there is no known lower bound for the private regime in that task. In , a lower bound for the private regime in identity testing is presented as \(O(}{^{2}}+}{^{4/3}}+}{}+}{^{4/3}^{2/3}}+ {})\) (where \(n=k\) is the domain size), which is a simpler task than closeness testing. Additionally,  provided an upper bound equal to the lower bound of the closeness testing also in the private regime when he crucially relies on the \(L_{1}\) tester that is known with low sensitivity. We have established an upper bound that is asymptotically close to the lower bound, given by \((\{k^{}{{}}}/}{{ }},}}{{}}/}{{}}^{1/3}, }}{{}}/}{{}}^{2},}}{{}}/}{{}}^{3},}}{{ }}/}{{}}\})\). The difference between our upper bound and the lower bound lies in two specific terms: \(}{}\) and \(}{^{1/3}}\) The first term is a result of the high sensitivity in our algorithm due to the use of \(L_{2}\) norm testing, which was selected to ensure the utility proof. Additionally, we did not focus on optimizing the term \(()\).

Preliminaries

Equivalence (Closeness) Testing.We assume oracle access to two distributions \(\), \(\) which gives an i.i.d. example from the resp. distribution. We also use \((S)\) (resp. \((S)\)) to denote the total probability mass assigned by \(\) (resp. \(\)) to a set \(S\). We assume the two distributions are continuous with no discrete point mass, which can always be obtained by concatenating each sample with a uniformly drawn number \(_{R}\). An equivalence tester between the two distributions should return NULL w.p. \( 2/3\) if it holds that \(=\) and return \(\) w.p. \( 2/3\) if it holds that \(\|-\|_{_{k}}\).

Differential Privacy.Two databases \(D\) and \(D^{}\) are considered neighboring databases if they differ by exactly one record, noted as \(d(D,D^{})=1\), where \(d(,)\) represents the Hamming distance. Given a domain \(\), two multi-sets \(S,S^{}\) are called _neighbors_ if they differ on a single entry. An algorithm (alternatively, mechanism) \(\) is said to be \((,)\)_-differentially private_ (DP) [21; 19] if for any two neighboring \(S,S^{}\) and any set \(O\) of possible outputs we have: \([(S) O] e^{}[(S^{}) O]+\). If \(=0\) then we say the algorithm \(\) is \(\)-DP.

The Global Sensitivity of a function \(f:^{d}\) is defined as the maximal difference \(_{S,S^{}}\|f(S)-f(S^{})\|_{1}\). It is known that adding \((GS(f)/)\) to each coordinate of \(f(S)\) is \(\)-DP; where \(()\) denotes the Laplace Distribution with parameter \(\), whose \(\) is \((x) e^{-|x|/}\). It is known  that if \(_{1}\) and \(_{2}\) are \((_{1},_{1})\)-DP and \((_{2},_{2})\)-DP resp., then their composition is \((_{1}+_{2},_{1}+_{2})\)-DP. It is also known  that the \(k\)-fold composition of \(k\) algorithms, each is \((,)\)-DP is \((^{*},k+^{})\)-DP for any \(^{}>0\) and \(^{*}=k(e^{}-1)+2)}\). Lastly, it is also known that if there exists an event \(\) such that under \(\) holding, algorithm \(\) satisfies that for any two neighboring \(S\) and \(S^{}\) and any set of outputs \(O\) we have that \([(S) O|\;] e^{}[(S^{ }) O|\;]+\) then algorithm \(\) is \((,+[-])\)-DP.

Poisson Distribution.The Poisson distribution \(()\) is a discrete distribution over the Naturals which satisfies \([k]=e^{-}^{k}/k!\). It has multiple properties that make it easy to work with.

**Proposition 2**.:
* _The sum of two ind. Poisson Poi\((_{1})\) and Poi\((_{2})\) is Poi\((_{1}+_{2})\)._
* _Let_ \(X_{1},X_{2},...\) _be i.i.d. Bernoulli r.v.s. with parameter_ \(p\)_; then drawing_ \(t\ ()\)_, it holds that_ \(_{i=1}^{t}X_{i}\) _is distributed like Poi_\(( p)\)_._
* _If_ \(X(_{x})\) _and_ \(Y(_{y})\) _are two ind. Poisson r.v.s, then the distribution of_ \(X\) _conditioned on the event that_ \(X+Y=n\) _is Binomial Bin(_\(n,}{_{x}+_{y}}\)_)._
* _If_ \(X_{i}()\) _then_ \([|X-|>k] 2(-}{2(+k)})\)_. So for any_ \(>0\)_, setting_ \(k=2}{{}})}\) _we get_ \([|X-|>2}{{}})}]\) _provided_ \(>4(}{{}})\)_._

**Misc.** We use \(\) (resp. \(\)) to denote big-\(O\) (resp. big-\(\)) up to poly-log factors. We made no effort to minimize constants or the degree of the poly-log.

## 3 Private Equivalence Testing for Continuous Distributions

In this section, we present our DP-tester for equivalence between two distributions with large \(_{k}\) distance and give both its formal privacy guarantee and its sample complexity bounds. The tester is detailed in Algorithm 1 (which in turn invokes Algorithm 2), where \(c_{}\) denotes a large enough constant detailed in .

The algorithm consists of two parts. In terms of the non-private algorithm, both parts function similarly to the algorithm presented by . Our main contribution is the privatization of this algorithm. In the first part the main objective is to compute the estimator \(Z\) (line I.5) which is privatized using a straightforward approach - since it has low sensitivity we merely add to is some Laplace noise. The second part is more complex, as it involves discretizing the domain based on the data itself, resulting in high sensitivity. We addressed this issue by creating the initial partition of the domain and adding a Bernoulli random variable to each index position (line II.5). Consequently, the algorithm can iteratively run \(j_{0}\) iterations of the \(L_{2}\)-tester TestCloseness on based on the randomized partition, where in each invocation of TestCloseness we sample a Poisson-size batch from each partition (line II.11). If the \(L_{2}\)-tester doesn't reject, then we merge adjacent partition cells (line II.15) and move on to the next iteration. If either invocations of TestCloseness (or the estimator \(Z\) has too high of a value) then we reject; but if all test pass we return NULL.

**Input:** 2 continuous distributions \(,\), distance parameter \(\), privacy parameter \(,\).

**Output:** "NULL" if \(=\); "\(\)" if \(\|-\|_{_{k}}\)

```
1: Part I:

1. Set \(m 100c_{}({k^{}}/{}^{}+{k^{}}/{^{}})\)
2. Draw \(s(m)\) points from \((+)\).
3. Label each \(x_{i}\) drawn from \(\) with \((x_{i})=1\) and label each \(x_{j}\) drawn from \(\) as \((x_{j})=-1\).
4. Sort the sample, denote the outcome as \(x_{(1)}<x_{(2)}<<x_{(s)}\).
5. \(Z=_{i=1}^{s-1}(x_{(i)})(x_{(i+1)})\)
6. Draw \(X(6/)\).
7. \(=Z+X\)
8. if\(>^{3}}{2k^{2}}\)then return\(\)
2: Part II:

1. Set \(N 10^{7}(}{^{4/3}^{2/3}}+}{}+}{^{2}})^{6}({k}/{ })\) and assert \(m\) divides \(N\).
2. Draw \(s_{p}()\) and \(s_{q}()\) ind. Set \(s s_{p}+s_{q}\).
3. \(S\) a sample of \(s_{p}\) i.i.d. examples from \(\) and \(s_{q}\) i.i.d. examples from \(\).
4. Sort \(S\). Denote the outcome as \(x_{(1)}<x_{(2)}<x_{(s)}\).
5. For each \(1 i m\) set \(_{i} i+B_{i}\) for ind. drawn \(B_{i}(}{{2}})\). Also set \(_{0} 0,_{m+1} s+1\).
6. Form the Partition \(^{0}=\{_{1}^{0},_{2}^{0},...,_{m}^{0}\}\) where \(_{i}^{0}=\{x_{(i^{})}:_{i}<i^{}<_{i+1}\}\) for every \(1 i m\).
7. Set \(j_{0} 1+\) and \(m^{0} m\).
8. for\(j=0,1,,j_{0}-1\)do:
9. for\(i=1,2,,m^{j}\)do:
10. Draw \(N_{i}^{J}(})\).
11. Pick a u.a.r. subset \(C_{i}^{J}\) of \(N_{i}^{j}\) points from \(_{i}^{j}\). (If \(N_{i}^{j}>|_{i}^{j}|\) then use special \(\) points.)
12. Set \(X_{i}^{j}\) (resp. \(Y_{i}^{j}\)) as #points from \(\) (resp. \(\)) in \(C_{i}^{j}\).
13. if\((_{i}N_{i}^{j},m^{j}, X_{i}^{j} , Y_{i}^{j},( }{{n}})},}(}{{n}})},},)=\)then
14. return\(\)
15. Merge cells: Set \(m^{j+1}}}{{2}}\), and for each \(1 i m^{j+1}\) set: \(_{i}^{j+1}_{2i-1}^{j}\{x_{(_{i^{}})}\}_{2i} ^{j}\)\(\)\(i^{}\)=the separating index of the two bins
3:return NULL ```

**Algorithm 1** Private Equivalence Tester

### Privacy Proof

In this subsection, our goal is to proof the following theorem.

**Theorem 3**.: _Algorithm 1 is \((2,2)\)-DP._

The proof of Theorem 3 shows that Part I of Algorithm 1 is \(\)-DP -- which is very straight forward, whereas Part II of the algorithm is \((,2)\)-DP -- which involves more intricate arguments. In fact, all that is required regarding the DP of Part I is the following claim. Its proof, as well as most proofs in this section, is deferred to Appendix A.

**Claim 4**.: _The estimator \(Z=_{i=1}^{s}(x_{(i)})(x_{(i+1)})\) in Part I of Algorithm 1 (Line 5) has global senstivity of \(6\)._We thus focus for now on Part II of Algorithm 1. In Part II our output is the \((2j_{0})\)-long tuple \(_{}^{j},Z^{j}_{j=0}^{j_{0}-1}\) which we may return from the \(j_{0}\) invocations of TestCloseness. Thus, we denote \(^{}=(2/)}}\), \(^{}=}\) and note that each invocation of TestCloseness is with these parameters. Throughout our analysis of Part II we assume that the Poisson random variables are known to us, but leave the Bernoulli and the Laplace random variables unknown.

Each invocation of TestCloseness releases two statistics: an approximation of \(n_{}\) and approximation of \(Z\). The next two claims bounds their sensitivity (w.h.p.).

**Claim 5**.: _The Global Sensitivity of \(n_{}\) is at most \(4\)._

**Lemma 6**.: _There exists an event \(\) where \([-]<3}{{2}}\), and under \(\) it holds that the sensitivity of \(Z\) at any iteration \(j\) is at most \(8_{4/3}(2/)((800)}+)}{^{}}+1)\)._

Proof.: Denote \(_{0}\) as the event that there exists an invocation of TestCloseness with \(m\) balls and \(n\) bins in which \(n_{}(800m)}+)}{^{}}\), yet \(_{}<(800m)}+)}{^{}}\). It follows that there must exists an invocation of TestCloseness in which the noise added to \(n_{}\) (drawn from \((8/^{})\)) must be smaller than \(-)}{^{}}\). This holds w.p. \(<^{}/2\), and from the Union Bound it follows that \([_{0}]=^{}}{2}=\).

We now turn to the Bernoulli random variables. Fix \(S\) and \(S^{}\) to be two neighboring inputs, and again, we assume that the changed point appears in place \((1)\) in \(S\) and in place \((s)\) in \(S^{}\). (Otherwise, our analysis is only simpler.) It follows that the changed point falls in \(_{1}^{0}\) in \(S\) and in \(_{m}^{0}\) in \(S^{}\).

We now specify the coupling of the Bernoulli random variables between \(S\) and \(S^{}\), which is the following. We draw \(B_{1}\), \(B_{1}^{}\). \(B_{2}\), \(B_{2}^{}\) and so on, u.a.r. and independently, and apply \(B_{i}\)-s to the invocation on \(S\) and \(B_{i}^{}\)-s to the invocation on \(S^{}\), _until the first occurrence of \(B_{i}=0,B_{i}^{}=1\)_ from which we give \(B_{i+1}\), \(B_{i+2}\) and so on to both invocation. This is depicted in Figure 1.

Denote \(_{j}\) as the event that the first occurrence of \(B_{i}=0,B_{i}^{}=1\) is at the \(j\)-th draw. Clearly \([_{j}]=(}{{4}})^{j-1}}{{4}}\). Thus, from the disjointness of events, it follows that \([_{j=1}^{_{4/3}(2/)}_{j}]=_{j=1}^{_{4/3}(2/)}(3/4)^{j-1}=}{{4}})^{_{4/3}(2/)}}{1-}{{4}}}=1-}{{2}}\). Symmetrically, we apply the same coupling

Figure 1: Two neighboring inputs that differ on one datapoint, appearing first in \(S\) and last in \(S^{}\). In this example, the index defining the first bin, \(_{1}\), is such that for \(S\) we go \(B_{1}=0\) and for \(S^{}\) we have \(B_{1}^{}=0\); but the index defining the 2nd bin, \(_{2}\) it does hold that \(B_{2}=0,B_{2}^{}=1\) so the indices starting from bin 2 onwards align.

to align the last bins, those affected by \(x_{(s)}\), in the last position. We use the coupling that provides \(B_{m},B^{}_{m},B_{m-1},B^{}_{m_{1}},B_{m-2},B^{}_{m-2}\) and so on until the first occurrence of \((1,0)\) then it switches to the same variable \(B_{j}\). Denoting \(^{}_{j}\) as the event that the first occurrence of \(B_{i}=1,B^{}_{i}=0\) is at the \(m-j\)-th draw, we have that \([_{j=1}^{_{4/3}(2/)}^{}_{j}]=1 -}{{2}}\).

We thus denote \(=_{0}_{j=1}^{_{4/3}(2/)}( _{j}^{}_{j})\) and have that \([]=3/2\). Note, under \(\) it follows that at most \(2_{4/3}(2/)\) bins have a change in their \(|X_{i}-Y_{i}|\)-value, which - due to Claim 5 - is at most \(4\). (Observe that in the case where \(S\) and \(S^{}\) are such that there are fewer than \(2_{4/3}(2/)\) bins between the locations of the changed point then this statement holds w.p. \(1\).)

It follows that under \(\) we have that the value of \(Z\) is affected by at most \(2_{4/3}(2/)\) bins, where for each bin \(|X_{i}-Y_{i}|\) changes by at most \(4\). It follows that \(Z\) can change by at most \(2_{4/3}(2/) 4(n_{}+1) 8_{4/3}(2/)( (800N)}+)}{ ^{}}+1)\). 

Completing the Proof of Theorem 3 is simple, and is deferred to Appendix A.

### Utility Proof

In this subsection, our goal is to proof the following theorems.

**Theorem 7**.: _W,p. \( 2/3\), Algorithm 1 returns_ NULL _when \(=\)._

**Theorem 8**.: _W,p. \( 2/3\), Algorithm 1 returns_ ALT _when \(\|-\|_{_{k}}\)._

The proof of Theorem 7 is fairly simple, but the proof of Theorem 8 requires some preliminaries. In fact, in order to argue the correctness of theorems we need to argue that both Parts I & II of the algorithm are correct w.p. \( 5/6\). Proving that Part I is correct is very straight-forward due to claims from . And so we focus on the correctness of Part II. Its correctness requires that we first assert a few rudimentary propositions and claims.

#### 3.2.1 Rudimentary Claims

Similarly to the analysis in  our goal is to compare the two continuous distributions to their resp. discretizations that were forms under the various \(^{j}\). However, we do not have the luxury of resampling the points from \(\) and \(\), and so our argument diverges from theirs as we fix one particular \(j^{*}\) and then argue from first principles that under large \(_{k}\)-difference the specific partition \(^{j^{*}}\) cause us to reject. (Arguing that when \(=\) we return null is also fairly straight-forward.) Our intermediate goal is to argue that the points from \(\) (the \(X\)-s) and the points from \(\) (the \(Y\)-s) are distributed like independent Poisson random variables. Thus, for the remainder of the discussion we fix some particular iteration \(j\) and examine _solely_ it, without considering the previous iterations.

Due to space constraint, we defer nearly all the claims in this section to Appendix B, but they all lead to towards the following main lemma.

**Lemma 9**.: _Fix index \(j\). For each index \(i\) denote by \(I_{i}\) as the interval \((x_{(^{j}_{i})},x_{(^{j}_{i+1})})\) which is the interval defining bin \(i\) in the partition \(^{j}\). Then the estimator \(Z\) computed by TestCloseness satisfies that \([Z]=_{i=1}^{m^{j}}(p_{i}-q_{i})^{2}\) where_

\[p_{i}=(I_{i})}{4m((I_{i})+(I_{i}))},  q_{i}=(I_{i})}{4m((I_{i})+(I_{i} ))}.\]

_and that \([Z]=_{i=1}^{m^{j}}4(p_{i}+q_{i})(p_{i}-q_{i})^{2}+2(p_{i}+q_{i})^ {2}\)._

In Lemma 9 we established the expectation and variance of our estimator using the notation \(p_{i}\) which in turn is defined as \((I)}{4m((I)+(I))}\) (and \(q_{i}\) similarly). The following claim is going to assist us in bounding the denominators of \(p_{i}\) and \(q_{i}\).

**Claim 10**.: _If \(N>3000m(2m)\), then with a probability of \(1-\), any \(I\) that forms a bin \(^{0}_{i}\) in \(^{0}\), has that \((I)+(I)}{2}\)._Unfortunately, due to space constraints, we defer proof of Claim 10 as well as _the entire proof of Theorem 7_ to Appendix B.

#### 3.2.2 Proof of Theorem 8

We now turn our attention to proving Theorem 8, however, we need one more technical lemma, whose proof - like all proofs in this section - is deferred to Appendix B. Then we can proof Theorem 8.

**Lemma 11**.: _Fix \(p(0,1)\). Suppose there exists \(n\) non-negative random variables \(X_{1},X_{2},,X_{n}\), such that for each \(i\) it holds that for some fixed number \(a_{i}\) we have \([X_{i} a_{i}] p\). Then, given a constant \(c>0\) there exists another constant \(C>0\) such that with a probability at least \(C\) it holds that \(_{i=1}^{n}X_{i} c_{i=1}^{n}a_{i}\)._

Proof of Theorem 8.: In the alternative case, where \(\|-\|_{_{k}}\), we know that there exists \(k\) intervals \(I\) such that \(_{I}|(I)-(I)|\). We denote for each interval \((I)=|(I)-(I)|\). For the sake of analyze we use two different kinds of interval, small and large.

**Definition 12**.: _Interval \(I\) is called small if there exists a subinterval \(J I\) such that \((J)+(J)<1/m\) and \(|(J)-(J)|(I)/10\), and large otherwise._

Note that \(\|-\|_{_{k}}=_{I }|(I)-(I)|=_{I,\ I} (I)+_{I,\ I}(I)\), so half of the discrepancy comes from either small or large intervals. We consider first the case where half of the discrepancy comes from small intervals. In this case, Lemma 9 in  states that the expectation of statistic \(Z\) in Line 5 of Part I is bounded by \([Z] c^{3}}{k^{2}}\) for some constant \(c>0\). Just like we did in the soundness case, Proposition 21 gives that the variance \([Z] 9m\). Therefore for some large constant \(c_{ dkn}\), setting \(m=100c_{ dkn}(}}{^{}}+}}{})\) then by Chebyshev's inequality we get that with probability \(\) it holds that \([Z]-10-|X|^{3}}{2k^{2}}\) (with \(X\) being the random noise sampled in Line 6 of Part I of the algorithm).

Now consider the case where \(_{I,\ I}|(I)-(I)| \). We prove that Part II of Algorithm 1 must return ALT on at least one invocation of TestCloseness. In order to do this, we first need to show that the discretization of the domain with \(m\) samples preserves most of the \(_{k}\)-distance. Take any interval \(I\) that gives the \(_{k}\) distance, and denote \(I=[a,b]\) as its boundaries.

Now, consider the total probability mass between \(a\) and \(_{i}\), then the first datapoint selected for the partition that is greater than \(a\). Since the total number of points is taken from a Poisson distribution, it is known that the mass from one point to the next has Exponential distribution \((N)\); and so the total mass from \(a\) to \(\) is distributed like the sum of Exponential random variables, namely, a Gamma-distribution with mean \(=\), and variance \(()^{2}=<(m)}\). It follows that w.p. \( 0.01\) we have that \(([a,_{i}]+[a,_{i}])>\). A similar analysis shows that for \(_{j}\), the last datapoint selected for the partition before \(b\), we also have \(([_{j},b])+([_{j},b])<\). Note that a large interval must have a total probability mass \((I)+(I)\) and so w.p. \( 0.98\) we have formed the subinterval \(I^{}=[_{i},_{j}] I\). Moreover, by the subinterval property of large intervals, we have that because \(([a,_{i}])+([a,_{i}])+([_{j},b])+ ([_{j},b])<\) then \((I^{})(I)-=(I)/2\).

Therefore, denote \(N_{I}\) as the indicator of the event that \(|(I^{})-(I^{})|\), and we denote \(^{^{0}}\) and \(^{^{0}}\) as the reduced discretized distribution formed by the partition point. We show it preserves most of the total variation \(\|^{^{0}}-^{^{0}}\|_{1}=_{I^{}}|(I^{})-(I^{})|_{I}N_{I}_{I,\ I}N_{I}\). We have established that if \(I\) is large interval then \([N_{I}] 0.98\), and so, for the random variables, \((N_{I})\)-s (one for each large interval) where for each variable with probability \(0.98\) it holds that \(N_{I}\), we apply Lemma 11 with \(c=0.36\) and have that there for the constant \(C=1-(1-0.36)^{2}}>0.95\) such that with probability \(C>0.95\)

\[_{I,\;I\;}N_{I} 0.36_{I ,\;I\;}>\]

Therefore, with probability \(>0.95\), \(\|^{^{0}}-^{^{0}}\|_{1}\).

Observe that the TV-distance follows from finding suitable subintervals \(I^{}\) inside large intervals with large discrepancy. Thus, each of the \( k\) large intervals now gives (at most) \(2\) points that form \(I^{}\), so these \( 2k\) points partition the real line to \( 2k+1\) intervals where the various \(I^{}\)-s are part of this partition. It follows that \(\|^{^{0}}-^{^{0}}\|_{_{2k+1}} {}{12}\).

Now, to complete the proof, we need to show that Part I of the algorithm detects the discrepancy. We deploy the following lemma from  where for a vector \(v\) the notation \(\|v\|_{1,k}\) refers to the sum of the largest \(k\) bins/coordinates of \(v\).

**Lemma 13**.: _For any two distributions \(\) and \(\) on \([m]\) such that \(\|-\|_{_{k}}>\), there iteration \(j[(}{{k}})]\) such that \(\|^{^{j}}-^{^{j}}\|_{1,k}>/(}{{k}})\)._

So since \(\|^{^{0}}-^{^{0}}\|_{_{2k+1}}\), we know that by Lemma 13, there exists some \(j^{*}[(m/k)]\) such that \(\|^{^{j^{*}}}-^{^{j^{*}}}\|_{1,2k+1} }{{k}})}\) and therefore by Cauchy-Schwarz inequality \(\|^{^{j^{*}}}-^{^{j^{*}}}\|_{2} }{{k}})}}\). We know that \(\|^{^{j^{*}}}-^{^{j^{*}}}\|_{1,2k+1} }{{k}})}\). Denote the set of indices of these \(2k+1\) intervals as \(\), we get from Lemma 9 that

\[[Z]=_{i=1}^{m^{j}}(p_{i}-q_{i})^{2}_{i^{ }}(p_{i^{}}-q_{i^{}})^{2}}}{{}}}|p_{i^{ }}-q_{i^{}}|)^{2}}{2k+1}(_{i^{ }}(I)-(I)|}{4m^{j}|(I)+(I)|})^{2}.\]

Also from Lemma 9 we can infer that

\[[|Z-[Z]|[Z]] [Z]}{[Z]^{2}}^{m^{j}}64(p_{i} +q_{i})(p_{i}-q_{i})^{2}+32(p_{i}+q_{i})^{2}}{(_{i=1}^{m^{j}}(p_{i}-q_{i}) ^{2})^{2}}\] \[=^{m^{j}}64((I)+( I)}{4m^{j}((I)+(I)})(p_{i}-q_{i})^{2}+32((I)+(I)}{4m^{j}((I)+(I)})^{2})^{2 }}{(_{i=1}^{m^{j}}(p_{i}-q_{i})^{2})^{2}}=/m^{j}}{(_{i=1} ^{m^{j}}(p_{i}-q_{i})^{2})^{2}}+}{_{i=1}^{m^{j}}(p_{i}-q_{i })^{2}}\] \[ k}}{{}}(2k+1)^{2 (}{{k}})}}{N^{2}^{4}}+}{{k}})}{N^{2}}<0.01\]

when \(N 10^{6}^{2}(m/k)}{^{2}}\). Now we assert that the noise we added is also proportional to at most \([Z]\), and indeed if we draw a random variable \(R(})\) with \((Z)\) as defined in Line 5 of TestCloseness, then we get \([R^{2}}{2500(2k+1)(m/k)}]( ^{2}^{}}{2500(2k+1)(Z)}) 0.01\). which holds if

\[N>10^{6}}{}} }{}}}{{3}}}(}{{ }})\{(10N)},)}{^{}}\}}\]

so we get \(N=(}{^{4/3}^{2/3}}+ }{})\). Combining it all together, we have that w.p. \( 5/6\) Part II of the algorithm also returns ALT. 

## 4 Applications

Our algorithm is designed for continuous distributions, but it can also be used for discrete distributions. The process is simple: for a given discrete distribution \(\), for each example \(x_{j}\) we draw a number \(i_{j}_{R}\), and then sort all the examples \((x_{j},i_{j})_{j=1}^{m}\) using lexicographic order. This process gives a simple privatization of the "Flattening method" proposed by Diakonikolas et al ,as it does so without looking at the example drawn. Our algorithm method is quite simple: draw \(m\) samples from \((+)\), then calculate the autocorrelation to identify discrepancies within small intervals. Next, use \(2(m,n)\) for flatting to test for total variation distance. It's important to remember that if \(n=k\), where \(n\) is the size of the domain, then \(_{k}\) distance is equal to \(L_{1}\). However, the sample complexity of the first part, which involves finding discrepancies in small intervals using the analysis of , requires \(}{^{6/5}}\). Fact 1 indicates however that the size of the domain is not always the most suitable parameter for distribution testing. Having knowledge about the structure of the distribution enables more efficient testing, as we illustrate below. In Table 1, we give a brief summary of the various statistical inference tasks that can be conducted using our algorithm.