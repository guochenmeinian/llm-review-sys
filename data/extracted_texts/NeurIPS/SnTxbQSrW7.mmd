# Adapting to Unknown Low-Dimensional Structures

in Score-Based Diffusion Models

Gen Li

The Chinese University of Hong Kong

genli@cuhk.edu.hk

&Yuling Yan

University of Wisconsin-Madison

yuling.yan@wisc.edu

Equal contribution. Corresponding author: Yuling Yan (Email: yuling.yan@wisc.edu).

###### Abstract

This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension \(d\) is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of \(O(k^{2}/)\) (up to log factors), where \(k\) is the intrinsic dimension of the target distribution and \(T\) is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.

## 1 Introduction

Score-based diffusion models are a class of generative models that have gained prominence in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions, such as images, audio, and text . These models operate by gradually transforming noise into samples from the target distribution through a denoising process guided by pre-trained neural networks that approximate the score functions. In practice, score-based diffusion models have demonstrated remarkable performance in generating realistic and diverse content across various domains , achieving state-of-the-art performance in generative AI.

### Diffusion models

The development of score-based diffusion models is deeply rooted in the theory of stochastic processes. At a high level, we consider a forward process:

\[X_{0}}}{{}}X_{1} }}{{}} }}{{}}X_{T},\] (1.1)

which draws a sample from the target data distribution (i.e., \(X_{0} p_{}\)), then progressively diffuses it to Gaussian noise over time. The key aspect of the diffusion model is to construct a reverse process:

\[Y_{T}}}{{}}Y_{T-1} }}{{}} }}{{}}Y_{0}\] (1.2)satisfying \(Y_{t}}}}{{}}X_{t}\) for all \(t\), which starts with pure Gaussian noise (i.e., \(Y_{T}(0,I_{d})\)) and gradually converts it back to a new sample \(Y_{0}\) sharing a similar distribution to \(p_{}\).

The classical results on time-reversal of SDEs [1; 10] provide the theoretical foundation for the above task. Consider a continuous time diffusion process:

\[X_{t}=-(t)X_{t}t+W_{t}(0 t T), X_{0} p_{}\] (1.3)

for some function \(:[0,T]^{+}\), where \((W_{t})_{0 t T}\) is a standard Brownian motion. For a wide range of functions \(\), this process converges exponentially fast to a Gaussian distribution. Let \(p_{X_{t}}()\) be the density of \(X_{t}\). One can construct a reverse-time SDE:

\[_{t}=-(t)_{t}+2  p_{X_{T-t}}(_{t})+Z_{t }(0 t T),_{0} p_{X_{T}},\] (1.4)

where \((Z_{t})_{0 t T}\) is another standard Brownian motion. Define \(Y_{t}=_{T-t}\). It is well-known that \(X_{t}}}}{{=}}Y_{t}\) for all \(0 t T\). Here, \( p_{X_{t}}\) is called the score function for the law of \(X_{t}\), which is not explicitly known.

The above result motivates the following paradigm: we can construct the forward process (1.1) by time-discretizing the diffusion process (1.3), and construct the reverse process (1.2) by discretizing the reverse-time SDE (1.4) and learning the score functions from the data. This approach leads to the popular DDPM sampler [11; 16]. Although the idea of the DDPM sampler is rooted in the theory of SDEs, the algorithm and analysis presented in this paper do not require any prior knowledge of SDEs.

This paper examines the accuracy of the DDPM sampler by establishing the proximity between the output distribution of the reverse process and the target data distribution. Since these two distributions are identical in the continuous time limit with perfect score estimation, the performance of the DDPM sampler is influenced by two sources of error: discretization error (due to a finite number of steps) and score estimation error (due to imperfect estimation of the scores). This paper views the score estimation step as a black box (often addressed by training a large neural network) and focuses on understanding how time discretization and imperfect score estimation affect the accuracy of the DDPM sampler.

### Inadequacy of existing results

The past few years have witnessed a significant interest in studying the convergence guarantees for the DDPM sampler [4; 6; 3; 14; 13]. To facilitate discussion, we consider an ideal setting with perfect score estimation. In this context, existing results can be interpreted as follows: to achieve \(\)-accuracy (i.e., the total variation distance between the target and the output distribution is smaller than \(\)), it suffices to take a number of steps exceeding the order of \((d)/^{2}\) (ignoring logarithm factors), where \(d\) is the problem dimension. Among these results, the state-of-the-art is given by , which achieved linear dependency on the dimension \(d\).

However, there seems to be a significant gap between the practical performance of the DDPM sampler and the existing theory. For example, for two widely used image datasets, CIFAR-10 (dimension \(d=32 32 3\)) and ImageNet (dimension \(d 64 64 3\)), it is known that 50 and 250 steps (also known as NFE, the number of function evaluations) are sufficient to generate good samples [16; 9]. This is in stark contrast with the existing theoretical guarantees discussed above, which suggest that the number of steps \(T\) should exceed the order of the dimension \(d\) to achieve good performance.

Empirical evidence suggests that the distributions of natural images are concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside [22; 18]. In view of this, a reasonable conjecture is that the convergence rate of the DDPM sampler actually depends on the intrinsic dimension rather than the ambient dimension. However, the theoretical understanding of diffusion models when the support of the target data distribution has a low-dimensional structure remains vastly under-explored. As some recent attempts,  established the first convergence guarantee under the Wasserstein-1 metric. However, their error bound has linear dependence on the ambient dimension \(d\) and exponential dependence on the diameter of the low-dimensional manifold. Another line of works [5; 26; 17] focused mainly on score estimation with properly chosen neural networks that exploit the low-dimensional structure, which is also different from our main focus.

### Our contributions

In light of the large theory-practice gap and the insufficiency of prior results, this paper takes a step towards understanding the performance of the DDPM sampler when the target data distribution has low-dimensional structure. Our main contributions can be summarized as follows:

* We show that, with a particular coefficient design, the error of the DDPM sampler, evaluated by the total variation distance between the laws of \(X_{1}\) and \(Y_{1}\), is upper bounded by \[}{}+_{t=1}^{T}\| s_{t}(X_{t})-s_{t}^{}(X_{t})\|_{2}^{2}},\] up to some logarithmic factors, where \(k\) is the intrinsic dimension of the target data distribution (which will be rigorously defined later), and \(s_{t}^{}\) (resp. \(s_{t}\)) is the true (resp. learned) score function at each step. The first term represents the discretization error (which vanishes as the number of steps \(T\) goes to infinity), while the second term should be interpreted as the score matching error. This bound is nearly dimension-free -- the ambient dimension \(d\) only appears in logarithmic terms.
* We also show that our choice of the coefficients is, in some sense, the unique schedule that does not incur discretization error proportional to the ambient dimension \(d\) at each step. This is in sharp contrast with the general setting without a low-dimensional structure, where a fairly wide range of coefficient designs can lead to convergence rates with polynomial dependence on \(d\). Additionally, this confirms the observation that the performance of the DDPM sampler can be improved through carefully designing coefficients [2; 16].

As far as we know, this paper provides the first theory demonstrating the capability of the DDPM sampler in adapting to unknown low-dimensional structures.

## 2 Problem set-up

In this section, we introduce some preliminaries and key ingredients for the diffusion model and the DDPM sampler.

Forward process.We consider the forward process (1.1) of the form

\[X_{t}=}X_{t-1}+}W_{t}(t=1,,T),  X_{0} p_{},\] (2.1)

where \(W_{1},,W_{T}}}{{}} (0,I_{d})\), and the learning rates \(_{t}(0,1)\) will be specified later. For each \(t 1\), \(X_{t}\) has a probability density function (PDF) supported on \(^{d}\), and we will use \(q_{t}\) to denote the law or PDF of \(X_{t}\). Let \(_{t} 1-_{t}\) and \(_{t}_{i=1}^{t}_{i}\). It is straightforward to check that

\[X_{t}=_{t}}X_{0}+_{t}}\,W_{t }_{t}(0,I_{d}).\] (2.2)

We will choose the learning rates \(_{t}\) to ensure that \(_{T}\) becomes vanishingly small, such that \(q_{T}(0,I_{d})\).

Score functions.The key ingredients for constructing the reverse process with the DDPM sampler are the score functions \(s_{t}^{}:^{d}^{d}\) associated with each \(q_{t}\), defined as

\[s_{t}^{}(x) q_{t}(x)(t=1,,T).\]

These score functions are not explicitly known. Here we assume access to an estimate \(s_{t}()\) for each \(s_{t}^{}()\), and we define the averaged \(_{2}\) score estimation error as

\[_{}^{2}_{t=1}^{T}_{ X q_{t}}[\|s_{t}(X)-s_{t}^{}(X)\|_{2}^{2}].\]

This quantity captures the effect of imperfect score estimation in our theory.

The DDPM sampler.To construct the reverse process (1.2), we use the DDPM sampler

\[Y_{t-1}=}}Y_{t}+_{t}s_{t}(Y_{t})+ _{t}Z_{t}(t=T,,1), Y_{T}(0,I_{d})\] (2.3)

where \(Z_{1},,Z_{T}}}{{}}(0, I_{d})\). Here \(_{t},_{t}>0\) are the hyperparameters that play an important role in the performance of the DDPM sampler, especially when the target data distribution has low-dimensional structure. As we will see, our theory suggests the following choice

\[_{t}^{}=1-_{t}_{t}^{ 2}=)(_{t}-_{t})}{1- _{t}}.\] (2.4)

For each \(1 t T\), we will use \(p_{t}\) to denote the law or PDF of \(Y_{t}\).

Target data distribution.Let \(^{d}\) be the support set of the target data distribution \(p_{}\), i.e., the smallest closed set \(C^{d}\) such that \(p_{}(C)=1\). To allow for the greatest generality, we use the notion of \(\)-net and covering number (see e.g., ) to characterize the intrinsic dimension of \(\). For any \(>0\), a set \(_{}\) is said to be an \(\)-net of \(\) if for any \(x\), there exists some \(x^{}\) in \(_{}\) such that \(\|x-x^{}\|_{2}\). The covering number \(N_{}()\) is defined as the smallest possible cardinality of an \(\)-net of \(\).

* **(Low-dimensionality)** Fix \(=T^{-c_{}}\), where \(c_{}>0\) is some sufficiently large universal constant. We define the intrinsic dimension of \(\) to be some quantity \(k>0\) such that \[ N_{}() C_{}k T\] for some constant \(C_{}>0\).
* **(Bounded support)** Suppose that there exists a universal constant \(c_{R}>0\) such that \[_{x}\|x\|_{2} R R  T^{c_{R}}.\] Namely we allow polynomial growth of the diameter of \(\) in the number of steps \(T\).

Our setting allows \(\) to be concentrated on or near low-dimensional manifolds, which is less stringent than assuming an exact low-dimensional structure. In fact, our definition of the intrinsic dimension \(k\) is the metric entropy of \(\) (see e.g., ), which is widely used in statistics and learning theory to characterize the complexity of a set or a class. The low-dimensionality is also a concept of complexity, therefore it is natural to use covering number, or metric entropy to characterize the intrinsic dimension. As a sanity check, when \(\) resides in an \(r\)-dimensional subspace of \(^{d}\), a standard volume argument (see e.g., [27, Section 4.2.1]) gives \( N_{}() r(R/) r T\), suggesting that the intrinsic dimension \(k\) is of order \(r\) in this case. In addition, in applications like image generation, the data is naturally bounded, as pixel values are typically normalized within the range \([-1,1]\). For example, the \(_{2}\) norm of an image from the CIFAR dataset is typically below \(60\).

Learning rate schedule.Following , we adopt the following learning rate schedule

\[_{1}=}},_{t+1}= T}{T}\{ _{1}(1+ T}{T})^{t},1\}(t=1,,T-1)\] (2.5)

for some sufficiently large constants \(c_{0},c_{1}>0\). This schedule is not unique - any other schedule of \(_{t}\) satisfying the properties in Lemma 8 can lead to the same result in this paper.

## 3 Main results

We are now positioned to present our main theoretical guarantees for the DDPM sampler.

### Convergence analysis

We first present the convergence theory for the DDPM sampler. The proof can be found in Section 4.

**Theorem 1**.: _Suppose that we take the coefficients for the DDPM sampler (2.3) to be \(_{t}=_{t}^{}\) and \(_{t}=_{t}^{}\) (cf. (2.4)), then there exists some universal constant \(C>0\) such that_

\[(q_{1},p_{1}) C^{ 3}T}{}+C_{} T.\] (3.1)

Several implications of Theorem 1 follow immediately. The two terms in (3.1) correspond to discretization error and score matching error, respectively. Assuming perfect score estimation (i.e., \(_{}=0\)) for the moment, our error bound (3.1) suggests an iteration complexity of order \(k^{4}/^{2}\) ignoring logarithmic factors) for achieving \(\)-accuracy, for any nontrivial target accuracy level \(<1\). In the absence of low-dimensional structure (i.e., \(k d\)), our result also recovers the iteration complexity in  of order \((d)/^{2}\).2 This suggests that our choice of coefficients (2.4) allows the DDPM sampler to adapt to any potential (unknown) low-dimensional structure in the target data distribution, and remains a valid criterion in the most general settings. The score matching error in (3.1) scales proportionally with \(_{}\), suggesting that the DDPM sampler is stable to imperfect score estimation.

### Uniqueness of coefficient design

In this section, we examine the importance of the coefficient design in the adaptivity of the DDPM sampler to intrinsic low-dimensional structure. Our goal is to show that, unless the coefficients \(_{t},_{t}\) of the DDPM sampler (2.3) are chosen according to (2.4), discretization errors proportional to the ambient dimension \(d\) will emerge in each denoising step.

In this paper, as well as in most previous DDPM literature, the analysis on the error \((q_{1},p_{1})\) usually starts with the following decomposition

\[^{2}(q_{1},p_{1})\] (3.2) \[}}{{=}}(p_{X_{T}}\|p_{Y_{T}})}_{}+_{t=2}^{T}_{x_{t} q_{t}} [(p_{X_{t-1}|X_{t}}(\,\,|\,x_{t}) \,\|\,p_{Y_{t-1}|Y_{t}}(\,\,|\,x_{t})) ]}_{(T+1-t)}.\]

Here step (i) follows from Pinsker's inequality, step (ii) utilizes from the data-processing inequality, while step (iii) uses the chain rule of KL divergence. We may interpret each term in the above decomposition as the error incurred in each denoising step. In fact, this decomposition is also closely related to the variational bound on the negative log-likelihood of the reverse process, which is the optimization target for training DDPM .

We consider a target distribution \(p_{}=(0,I_{k})\), where \(I_{k}^{d d}\) is a diagonal matrix with \(I_{i,i}=1\) for \(1 i k\) and \(I_{i,i}=0\) for \(k+1 i d\). This is a simple distribution over \(^{d}\) that is supported on a \(k\)-dimensional subspace.3 Our second theoretical result provides a lower bound for the error incurred in each denoising step for this target distribution. The proof can be found in Appendix B.

**Theorem 2**.: _Consider the target distribution \(p_{}=(0,I_{k})\) and assume that \(k d/2\). For the DDPM sampler (2.3) with perfect score estimation (i.e., \(s_{t}()=s_{t}^{}()\) for all t) and arbitrary coefficients \(_{t},_{t}>0\), we have_

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_{t}}(\, \,|\,x_{t})\,\|\,p_{Y_{t-1}|Y_{t}}(\,\, |\,x_{t}))](_{t}-_{t}^{ })^{2}+(^{ 2}}{_{t}^{2}}-1)^{2}\]

_for each \(2 t T\). See (2.4) for the definitions of \(_{t}^{}\) and \(_{t}^{}\)._Theorem 2 shows that, unless we choose \(_{t}\) and \(_{t}^{2}\) to be identical (or exceedingly close) to \(_{t}^{*}\) and \(_{t}^{*2}\), the corresponding denoising step will incur an undesired error that is linear in the ambient dimension \(d\). This highlights the critical importance of coefficient design for the DDPM sampler, especially when the target distribution exhibits a low-dimensional structure.

Finally, we would like to make note that the above argument only demonstrates the impact of coefficient design on an _upper bound_ (3.2) of the error \((q_{1},p_{1})\), rather than the error itself. It might be possible that a broader range of coefficients can lead to dimension-independent error bound like (3.1), while the upper bound (3.2) remains dimension-dependent. This calls for new analysis tools (since we cannot use the loose upper bound (3.1) in the analysis), which we leave for future works.

## 4 Analysis for the DDPM sampler (Proof of Theorem 1)

This section is devoted to establishing Theorem 1. The idea is to bound the error incurred in each denoising step as characterized in the decomposition (3.2), namely for each \(2 t T\), we need to bound

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_{t}}( \,\,|\,x_{t})\,\|\,p_{Y_{t-1}|Y_{t}}(\,\,|\,x_{t}) )].\]

This requires connecting the two conditional distributions \(p_{X_{t-1}|X_{t}}\) and \(p_{Y_{t-1}|Y_{t}}\). It would be convenient to decouple the errors from time discretization and imperfect score estimation by introducing auxiliary random variables

\[Y_{t-1}^{}}}(Y_{t}+_{t}^{*}s_ {t}^{}(Y_{t})+_{t}^{}Z_{t})(2 t T).\] (4.1)

On a high level, for each \(2 t T\), our proof consists of the following steps:

1. Identify a typical set \(_{t}^{d}^{d}\) such that \((X_{t},X_{t-1})_{t}\) with high probability.
2. Establish point-wise proximity \(p_{X_{t-1}|X_{t}}(x_{t-1}\,|\,x_{t}) p_{Y_{t-1}^{}|Y_{t}}(x_{t-1} \,|\,x_{t})\) for \((x_{t},x_{t-1})_{t}\).
3. Characterize the deviation of \(p_{Y_{t-1}^{}|Y_{t}}\) from \(p_{Y_{t-1}|Y_{t}}\) caused by imperfect score estimation.

### Step 1: identifying high-probability sets

For simplicity of presentation, we assume without loss of generality that \(k d\) throughout the proof.4 Let \(\{x_{i}^{}\}_{1 i N_{}}\) be an \(\)-net of \(\), and let \(\{_{i}\}_{1 i N_{}}\) be a disjoint \(\)-cover for \(\) such that \(x_{i}^{}_{i}\). Let

\[ \{1 i N_{}:(X_{0} _{i})(-C_{1}k T)\}\,,\] \[ ^{d}:\|\|_{2} 2 +k T},\] \[|(x_{i}^{}-x_{j}^{})^{}| k T}\|x_{i}^{}-x_{j}^{}\|_{2} 1 i,j N_{}},\]

where \(C_{1}>0\) is some sufficiently large universal constants. Then \(_{i}_{i}\) and \(\) can be interpreted as high probability sets for the variable \(X_{0}\) and a standard Gaussian random variable in \(^{d}\). For each \(t=1, T\), we define a typical set for each \(X_{t}\) as follows

\[_{t}_{t}}x_{0}+_{t}}:x_{0}_{i}_{i}, }\,,\]

and a typical set for \((X_{t},X_{t-1})\) jointly as follows

\[_{t}(x_{t},x_{t-1}):x_{t}_{t},-}x_{t-1}}{}} }.\]

The following lemma shows that \(_{t}\) is indeed a high-probability set for \((X_{t},X_{t-1})\).

**Lemma 1**.: _Suppose that \(C_{1} C_{}\). Then for each \(1 t T\) we have_

\[((X_{t},X_{t-1})_{t})- }{4}k T.\]

Proof.: See Appendix A.3.

[MISSING_PAGE_FAIL:7]

### Step 3: bounding the KL divergence between \(p_{X_{t-1}|X_{t}}\) and \(p_{Y^{*}_{t-1}|Y_{t}}\)

We first decompose the expected KL divergence between \(p_{X_{t-1}|X_{t}}\) and \(p_{Y^{*}_{t-1}|Y_{t}}\) into

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_{t }}(\,\,|\,x_{t})\, p_{Y^{*}_{t-1}|Y_{t}}(\, \,|\,x_{t}))]\] \[=(_{_{t}}+_{^{c}_{t}})p _{X_{t-1}|X_{t}}(x_{t-1}\,|\,x_{t})(|X_{t}} (x_{t-1}\,|\,x_{t})}{p_{Y^{*}_{t-1}|Y_{t}}(x_{t-1}\,|\,x_{t} )})p_{X_{t}}(x_{t})x_{t-1}x_{t}\] \[=:_{t,1}+_{t,2},\]

where \(_{t,1}\) and \(_{t,2}\) are the integrals over \(_{t}\) and \(^{c}_{t}\). It boils down to bounding these two terms.

By a direct application of Lemma 3 together with the first-order Taylor expansion of \((x)\) around \(x=1\), one can easily show that \(|_{t,1}| k^{2}^{3}(T)/T\). However this naive bound will lead to a vacuous final bound on \((q_{1},p_{1})\), which depends on the sum of \(_{t,1}\) over all \(2 t T\) according to (3.2). By a more careful analysis, we achieve a better bound for \(_{t,1}\), as shown in the following lemma.

**Lemma 5**.: _Suppose that \(T k^{2}^{3}T\). Then for each \(2 t T\), we have_

\[|_{t,1}| 2C_{5}^{2}^{6}T}{T^{2}}.\]

Proof.: See Appendix A.7. 

For \(_{t,2}\), we can employ the course bound in Lemma 4 to show that it is exponentially small.

**Lemma 6**.: _Suppose that \(T 1\). Then for each \(2 t T\), we have_

\[|_{t,2}|(-}{16}k T).\]

Proof.: See Appendix A.8. 

By putting together Lemma 5 and Lemma 6, we achieve

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_{t }}(\,\,|\,x_{t})\, p_{Y^{*}_{t-1}|Y_{t}}(\, \,|\,x_{t}))]=_{t,1}+_{t,2} 3C_{5}^{2}^{6}T}{T^{2}}\] (4.5)

provided that \(T\) is sufficiently large.

### Step 4: bounding the KL divergence between \(p_{X_{t-1}|X_{t}}\) and \(p_{Y_{t-1}|Y_{t}}\)

Since our goal is to bound the expected KL divergence between \(p_{X_{t-1}|X_{t}}\) and \(p_{Y_{t-1}|Y_{t}}\), we also need to upper bound the following difference

\[_{x_{t} q_{t}}p_{X_{t-1}|X _{t}}(\,\,|\,x_{t})\, p_{Y_{t-1}|Y_{t}}(\, \,|\,x_{t})-_{x_{t} q_{t}} p_{X_{t-1}|X_{t}}(\,\,|\,x_{t})\, p_{Y^{*}_{t -1}|Y_{t}}(\,\,|\,x_{t})\] \[= p_{X_{t-1}|X_{t}}(x_{t-1}\,|\,x_{t}) _{t-1}|Y_{t}}(x_{t-1}\,|\,x_{t})}{p_{Y^{*}_{t-1}| Y_{t}}(x_{t-1}\,|\,x_{t})}x_{t-1}q_{t}(x_{t} )x_{t}\] (4.6) \[= p_{X_{t-1},X_{t}}(x_{t-1},x_{t})(-\|x_{t-1}-_{t}^{*}(x_{t})\|_{2}^{2}}{2_{t}^{*2}} )\|x_{t-1}-_{t}(x_{t})\|_{2}^{2}}{2_ {t}^{*2}}\,x_{t-1}x_{t}\] \[=^{*2}}{2_{t}^{*2}}_{x_{t} q_{t }}[\|_{t}(x_{t})\|_{2}^{2}]+^{*}}}{{_{t}^{*2}}},X_ {t}}(x_{t-1},x_{t})(x_{t-1}-_{t}^{*}(x_{t}) )^{}_{t}(x_{t})x_{t-1}x_{t }}_{=:K_{t}},\]

where we define

\[_{t}(x_{t}) s_{t}^{}(x_{t})-s_{t} (x_{t}),_{t}^{}(x_{t}) +_{t}^{*}s_{t}^{}(x_{t})}{}},_{t} (x_{t})+_{t}^{}s_{t}(x_{t}) }{}}.\] (4.7)

It then boils down to bounding \(K_{t}\), which is presented in the following lemma.

**Lemma 7**.: _Suppose that \(T k^{2}^{3}T\). Then we have_

\[|K_{t}| 4C_{5}^{3}T}{T} T}{T}} _{x_{t} q_{t}}^{1/2}[\|_{t}(x_{t} )\|_{2}^{2}].\]

Proof.: See Appendix A.8. 

Hence we know that for \(2 t T\),

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_ {t}}(\,\,|\,x_{t}))\|p_{Y_{t-1}|Y_{t}}( \,\,|\,x_{t}))]-_{x_{t} q_{t}} [(p_{X_{t-1}|X_{t}}(\,\,|\,x_{t}) )\|p_{Y^{}_{t-1}|Y_{t}}(\,\,|\,x_{t}) )]\] \[ T}{T}_{x_{t} q_{t}}[ \|_{t}(x_{t})\|_{2}^{2}]+8C_{5}^{3}T}{T} T}{T}}_{x_{t} q_{t}}^{1/ 2}[\|_{t}(x_{t})\|_{2}^{2}].\] (4.8)

Here the first relation follows from Lemma 7 and (2.4); while the second relation follows from Lemma 8 and holds provided that \(T\) is sufficiently large.

### Step 5: putting everything together

By taking (4.5) and (4.8) collectively, we have

\[_{x_{t} q_{t}}[(p_{X_{t-1}|X_ {t}}(\,\,|\,x_{t})\,\|\,p_{Y_{t-1}|Y_{t}}( \,\,|\,x_{t}))]\] \[ 3C_{5}^{2}^{6}T}{T^{2}}+ T}{T} _{x_{t} q_{t}}[\|_{t}(x_{t}) \|_{2}^{2}]+8C_{5}^{3}T}{T} T} {T}}_{x_{t} q_{t}}^{1/2}[\|_{t}(x_{t} )\|_{2}^{2}]\] \[ 7C_{5}^{2}^{6}T}{T^{2}}+ T}{T} _{x_{t} q_{t}}[\|_{t}(x_{t}) \|_{2}^{2}].\] (4.9)

Here the last relation follows from an application of the AM-GM inequality

\[8C_{5}^{3}T}{T} T}{T}}_{x_{t}  q_{t}}^{1/2}[\|_{t}(x_{t})\|_{2}^{ 2}] T}{T}_{x_{t} q_{t}}[\| _{t}(x_{t})\|_{2}^{2}]+4C_{5}^{2} ^{6}T}{T^{2}}.\]

Finally we conclude that

\[^{2}(q_{1},p_{1}) (p_{X_{T}}\|p_{Y_{T}})+_{t=2}^{T} _{x_{t} q_{t}}[(p_{X_{t-1}|X_{t}}(\, \,|\,x_{t})\,\|\,p_{Y_{t-1}|Y_{t}}(\,\, |\,x_{t}))]\] \[ 8C_{5}^{2}^{6}T}{T}+ T}{T} _{t=2}^{T}_{x_{t} q_{t}}[\|_{t}(x_{ t})\|_{2}^{2}],\]

as claimed. Here the first relation follows from (3.2), while the second relation follows from the fact that \((p_{X_{T}}\|p_{Y_{T}}) T^{-100}\) provided that \(T\) is sufficiently large (see Lemma 10).

## 5 Simulation study

We conducted a simple simulation to compare our coefficient design (2.4) with another design

\[_{t}=_{t}^{2}=1-_{t} 1 t T,\] (5.1)

which has been widely adopted in theoretical analysis of diffusion model (see e.g., [14; 15]). We consider the degenerated Gaussian distribution \(p_{}=(0,I_{k})\) in Theorem 2 as a tractable example, and run the DDPM sampler with exact score functions (so that the error only comes from discretization). We fix the intrinsic dimension \(k=8\), and let the ambient dimension \(d\) grow from \(10\) to \(10^{3}\). We implement the experiment for four different number of steps \(T\{100,200,500,1000\}\). Instead of using the learning rate schedule (2.5), which is chosen mainly to facilitate analysis, we use the schedule in  that is commonly used in practice. Figure 1 displays the error, in terms of both the TV distance \((q_{1},p_{1})\) and KL divergence \((q_{1}\|p_{1})\), as the ambient dimension \(d\) varies. As we can see, our design (2.4) leads to dimension-independent error while the other design (5.1) incures an error that grows as \(d\) increases. This provides empirical evidence that (2.4) represents a unique coefficient design for DDPM in achieving dimension-independent error.

## 6 Discussion

The present paper investigates the DDPM sampler when the target distribution is concentrated on or near low-dimensional manifolds. We identify a particular coefficient design that enables the adaptivity of the DDPM sampler to unknown low-dimensional structures and establish a dimension-free convergence rate at the order of \(k^{2}/\) (up to logarithmic factors). We conclude this paper by pointing out several directions worthy of future investigation. To begin with, our theory yields an iteration complexity that scales quartically in the intrinsic dimension \(k\), which is likely sub-optimal. Improving this dependency calls for more refined analysis tools. Recent work  achieved a convergence rate of order \(O(d/T)\), suggesting the potential for enhancing the dependence on \(T\). Furthermore, as we have discussed in the end of Section 3.2, it is not clear whether our coefficient design (2.4) is unique in terms of achieving dimension-independent error \((q_{1},p_{1})\). Finally, the analysis ideas and tools developed for the DDPM sampler might be extended to study another popular DDIM sampler.