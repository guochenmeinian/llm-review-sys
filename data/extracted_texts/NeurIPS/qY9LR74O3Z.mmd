# Holistic Evaluation of Text-to-Image Models

Tony Lee\({}^{*}\)\({}^{1}\), Michihiro Yasunaga\({}^{*}\)\({}^{1}\), Chenlin Meng\({}^{*}\)\({}^{1}\)

Yifan Mai\({}^{1}\), Joon Sung Park\({}^{1}\), Agrim Gupta\({}^{1}\), Yunzhi Zhang\({}^{1}\), Deepak Narayanan\({}^{2}\)

Hannah Benita Teufel\({}^{3}\), Marco Bellagente\({}^{3}\), Minguk Kang\({}^{4}\), Taesung Park\({}^{5}\)

Jure Leskovec\({}^{1}\), Jun-Yan Zhu\({}^{6}\), Li Fei-Fei\({}^{1}\), Jiajun Wu\({}^{1}\), Stefano Ermon\({}^{1}\), Percy Liang\({}^{1}\)

\({}^{1}\)Stanford \({}^{2}\)Microsoft \({}^{3}\)Aleph Alpha \({}^{4}\)POSTECH \({}^{5}\)Adobe \({}^{6}\)CMU

\({}^{*}\)Equal contribution

###### Abstract

The stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, _Holistic Evaluation of Text-to-Image Models (HEIM)_. Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase .

## 1 Introduction

In the last two years, there has been a proliferation of text-to-image models, such as DALL-E  and Stable Diffusion , and many others . These models can generate visually striking images and have found applications in wide-ranging domains, such as art, design, and medical imaging . For instance, the popular model Midjourney  boasts over 16 million active users as of July 2023 . Despite this prevalence, our understanding of their full potential and associated risks is limited , both in terms of safety and ethical risks  and technical capabilities such as originality and aesthetics . Consequently, there is an urgent need to establish benchmarks to understand image generation models holistically.

Existing benchmarks for text-to-image generation models  have limitations that hinder comprehensive model evaluation. Firstly, these benchmarks only consider text-image alignment and image quality, as seen in benchmarks like MS-COCO . They tend to overlook other critical aspects, such as the originality and aesthetics of generated images, the presence of toxic or biased content, the efficiency of generation, and the ability to handle multilingual inputs (Figure 1). These aspects are vital for assessing the model's technological and societal impacts, including ethical concerns related to toxicity and bias, legal considerations such as copyright and trademark, and environmental implications like energy consumption . Secondly, the evaluation of text-to-image models often relies on automated metrics like FID  or CLIPscore . While these metrics provide valuable insights, they may not effectively capture the nuances of human perception and judgment, particularly concerning aesthetics and photorealism . Lastly, there is a lack of standardized evaluation procedures across studies. Various papers adopt different evaluation datasets and metrics, which makes direct model comparisons challenging .

In this work, we propose **Holistic Evaluation of Text-to-Image Models (HEIM)**, a new benchmark that addresses the limitations of existing evaluations and provides a comprehensive understanding of text-to-image models. (1) HEIM evaluates text-to-image models across **diverse aspects**. We identify

[MISSING_PAGE_FAIL:2]

**Aspect** is a specific evaluative dimension that contributes to assessing the overall performance of image generation. Examples include image quality, originality, and bias. Evaluating multiple aspects allows us to capture diverse characteristics of generated images. We evaluate 12 aspects, listed in Table 1, through a combination of scenarios and metrics.

**Scenario** represents a specific use case or a dataset of prompts for image generation. We consider various scenarios reflecting different domains and tasks, such as descriptions of common objects (_MS-COCO_) and logo design (_Logos_). The complete list of scenarios we use is provided in Table 2.

**Adaptation** is the specific procedure used to run a model. Examples include zero-shot prompting, few-shot prompting, prompt engineering, and finetuning. We focus on zero-shot prompting, applying the model to each prompt without additional tuning. We also explore prompt engineering techniques, such as Promptist , which use language models to refine the prompts before feeding into the model.

**Metric** quantifies how good the image generation results are. A metric can be human-rated (e.g., overall text-image alignment on a 1-5 scale) or automatically computed (e.g., CLIPSCore). By using both human-rated and automated metrics, we capture both subjective and objective assessments of the generated images. The metrics we use are listed in Table 3.

In the subsequent sections of the paper, we delve into the details of aspects (SS3), scenarios (SS4), metrics (SS5), and models (SS6), followed by the discussion of experimental results and findings in SS7.

## 3 Aspects

We evaluate diverse aspects of text-to-image models that are crucial for their deployment. Table 1 shows the 12 aspects and their definitions.

Figure 4: **Evaluation components**. Each evaluation run consists of an _aspect_ (an evaluative dimension), a _scenario_ (a specific use case), a _model_ with an _adaptation_ process (how the model is run), and one or more _metrics_ (how good are the results).

Figure 2: **Standardized evaluation**. Prior to HEIM (_top panel_), the evaluation of image generation models was not comprehensive: six of our 12 core aspects were not evaluated in existing models, and only 11% of the total evaluation space was studied (the percentage of \(\) in the matrix of aspects \(\) models). After our evaluation (_bottom panel_), models are now evaluated under the same conditions in all aspects.

For each aspect, we provide a rationale for its inclusion and discuss the scenarios and metrics that can be used to evaluate it (refer to Figure 1 for an illustration). Further details regarding all scenarios and metrics will be presented in SS4 and SS5.

Image-text **alignment** and image **quality** are commonly studied aspects in existing efforts to evaluate text-to-image models . Since these are general aspects, any scenarios can be employed. For alignment, we use metrics like CLIPScore  and human-rated alignment score. For quality, we use metrics such as FID , Inception Score , and human-rated photorealism.

We introduce **aesthetics** and **originality** as new aspects, motivated by the recent surge in using text-to-image models for visual art creation . In particular, originality is crucial for addressing concerns of copyright infringement in generative AI . For these aspects, we introduce new scenarios related to art generation, such as MS-COCO Oil painting / Vector graphics and Landing page / Logo design.

  
**Aspect** & **Definition** \\  Alignment & Is the image semantically correct given the text (image-text alignment) \\ Quality & Do the generated images look like real images/photos \\ Aesthetics & Is the image aesthetically pleasing \\ Originality & Does the model generate creative images and prevent copyright infringement \\ Reasoning & Does the model understand objects, counts, and spatial relations (compositionality)  \\ Knowledge & Does the model have knowledge about the world or domains \\ Bias & Are the generated images biased in demographic representation (e.g., gender, skin tone)  \\ Toxicity & Does the model generate toxic or inappropriate images (e.g., violence, sexual, illegal content) \\ Fairness & Does the model exhibit performance disparities across social groups (e.g., gender, dialect)  \\ Robustness & Is the model robust to invariant input perturbations \\ Multilinguality & Does the model support non-English languages \\ Efficiency & How fast is inference for the model \\   

Table 1: **Evaluation Aspects of Text-to-Image Models**

Figure 3: **The current state of text-to-image generation models. The figure above shows examples of images generated by a select few text-to-image models for various prompts from different aspects (excluding efficiency). Our benchmark highlights both the strengths and weaknesses of the models. For example, DALL-E 2 has decent text-image alignment for both English and Chinese prompts but has clear gender and skin tone bias.**

[MISSING_PAGE_FAIL:5]

languages (multilinguality), or the introduction of typos (robustness). We then measure the change in model performance (e.g., CLIPScore) compared to the unmodified MS-COCO scenario.

Lastly, **efficiency** holds practical importance for usability and energy consumption of models . Inference time serves as the metric, and any scenarios can be employed, as efficiency is a general aspect.

## 4 Scenarios

To evaluate the 12 aspects in image generation (SS3), we curate diverse and practical prompting scenarios. Table 2 presents an overview of all the scenarios and their descriptions. Each scenario is a set of prompts and can be used to evaluate certain aspects. For instance, the "MS-COCO" scenario can be used to assess the alignment, quality and efficiency aspects, and the "Inappropriate Image Prompts (I2P)" scenario can be used to assess the toxicity aspect. Some scenarios may include sub-scenarios, indicating the sub-level categories or variations within them, such as "Hate" and "Violence" within I2P. We curate these scenarios by leveraging existing datasets as well as creating new prompts ourselves. In total, we have 62 scenarios including the sub-scenarios.

Notably, we create new scenarios (indicated with "**New**" in Table 2) for aspects that were previously underexplored and lacked dedicated datasets. These aspects include originality, aesthetics, bias, and fairness. For example, to evaluate originality, we develop scenarios related to the arts, such as prompts for generating landing pages, logos, and magazine covers.

## 5 Metrics

To evaluate the 12 aspects in image generation (SS3), we curate a diverse and realistic set of metrics that can be informative for researchers, developers, and end-users. Table 3 presents an overview of all the metrics and their descriptions.

Our approach incorporates two novelties compared to existing metrics. First, in addition to automated metrics, we use human-rated metrics (top rows in Table 3) to achieve realistic evaluation that reflects human judgment. Specifically, we employ human-rated metrics for the overall image-text alignment

  
**Metric** & **Main Aspect** & **Rated by** & **Description** & **Need reference images?** & **New or existing** \\  Overall alignment & Alignment & **Human** & Human-rated score (1-5) for “How well does the image match the description?** & No &  \\ Photorealism & Quality & **Human** & Human-rated score (1-5) for “Does the image look like a real photo or an & No &  \\ Subject clarity & Aesthetics & **Human** & Human-rated score (yes/no/no/no) for “Is it clear who the subject(s) of the & No & **New** \\ Overall aesthetics & Aesthetics & **Human** & Human-rated score (1-5) for “How aesthetically pleasing is the image?” & No & **New** \\ Overall originality & Originality & **Human** & Human-rated score (1-5) for “How original is the image, given it was & No & **New** \\  CLIPScore & Alignment & Automated & Text-image alignment measured by CLIP & No &  \\ CLIPScore with Multilingual & Alignment & Automated & Text-image alignment measured by CLIP & No &  \\ CLIP & & & & & \\ Féchlet Inception Distance (**FID**) & Quality & Automated & How similar generated images are to reference images, measured by Inception Net & Yes &  \\ Inception score (IS) & Quality & Automated & Quality of generated images, measured by Inception Net & No &  \\ LAION Antibiotics & Aesthetics & Automated & Aesthetics score measured by the LAION predictor & No &  \\ Fractal coefficient & Aesthetics & Automated & Aesthetics complexity measured by the factual coefficient  & No & **New** \\ Object detection & Reasoning & Automated & Accuracy of visual composition measured by an object detector & No &  \\ Watermark & Originality & Automated & Whether the image contains watermarks based on the LAION predictor & No &  \\ LAION NSPW & Toxicity & Automated & Whether the image is NSPW (not safe for work) based on the LAION & No &  \\ NodeNet & Toxicity & Automated & Whether the image contains nudged based on NodeNet & No &  \\ Blackout & Toxicity & Automated & How often the model can generate inappropriate images, measured by & No &  \\  & & & Stable Diffusion’s black out action. This metric is specific to Stable Diffusion models & \\ API rejection & Toxicity & Automated & How often the model can generate inappropriate images, measured by & No &  \\  & & & DALL-2 API-based action. This metric is specific to DALL-2 & & \\ Gender bias & Bias & Automated & Gender bias in a set of generated images, measured by detecting the gender of an image using CLIP & No &  \\ Skin tone bias & Bias & Automated & Skin tone bias in a set of generated images, measured by detecting skin & No &  \\ Fairness & Fairness & Fairness & Automated & Performance change in CLIPScore or alignment when the prompt is & No & **New** \\  & & varied in terms of social groups (e.g., gender/addicted changes) & & & \\ Robustness & Robustness & Automated & Performance change in CLIPScore or alignment when the prompt is & No & **New** \\ Multilinguality & Modlinguality & Automated & Wealth-preserving perturbations (e.g., typos) & & \\  & & & translated into non-English languages (e.g., Spanish, Chinese, India) & & \\ Raw inference time & Efficiency & Automated & Wall-clock inference runtime & No & **New** \\ Denoted inference time & Efficiency & Automated & Wall-clock inference runtime with performance variation factored out & No & **New** \\   

Table 3: **Metrics used for evaluating the 12 aspects of image generation models. We use realistic, human-rated metrics as well as automated and commonly-used existing metrics.**and photorealism, which are used for many evaluation aspects, including alignment, quality, knowledge, reasoning, fairness, robustness, and multilinguality. We also employ human-rated metrics for overall aesthetics and originality, for which capturing the nuances of human judgment is important. To conduct the human evaluation, we employ crowdsourcing following the methodology described in . Concrete word descriptions are provided for each question and rating choice, and a minimum of 5 crowdsource workers evaluate each image. We use at least 100 image samples for each aspect being evaluated. For a detailed description of the crowdsourcing procedure, please refer to Appendix G.

The second novelty involves introducing new metrics for aspects that have received limited attention in existing evaluation efforts, namely fairness, robustness, multilinguality, and efficiency, as discussed in SS3. The new metrics aim to address the evaluation gaps in these aspects.

## 6 Models

We evaluate 26 recent text-to-image models, encompassing various types (e.g., diffusion, autoregressive), sizes (ranging from 0.4B to 13B parameters), organizations, and accessibility (open source or closed). Table 4 presents an overview of the models and their corresponding properties. In our evaluation, we employ the default inference configurations provided in the respective model's API, GitHub, or Hugging Face repositories.

## 7 Experiments and results

We evaluated 26 text-to-image models (SS6) across the 12 aspects (SS3), using 62 scenarios (SS4) and 25 metrics (SS5). All results are available at https://crfm.stanford.edu/heim/latest. We also provide the result summary in Table 5. Below, we describe the key findings. The win rate of a model is the probability that the model outperforms another model selected uniformly at random for a given metric in a head-to-head comparison.

1. **Image-text alignment.** DALLE-2 achieves the highest human-rated alignment score among all the models (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_alignment_scenarios). It is closely followed by models fine-tuned using high-quality, realistic images, such as Dreamlike Photoreal 2.0 and Vintedois Diffusion. On the other hand, models fine-tuned with art images (Openjourney v4, Redshift Diffusion) and models incorporating safety guidance (SafeStableDiffusion) show slightly lower performance in image-text alignment.
2. **Photorealism.** In general, none of the models generated images that were deemed photorealistic, as human annotators rated real images from MS-COCO with an average score of 4.48 out of 5 for

  
**Model** & **Creator** & **Type** & **\# Parameters** & **Access** & **Reference** \\  Stable Diffusion v1-4 & Ludwig Maximilian University of Munich CompVis & Diffusion & 1B & Open &  \\ Stable Diffusion v1-5 & Runway & Diffusion & 1B & Open &  \\ Stable Diffusion v2 base & Stability AI & Diffusion & 1B & Open &  \\ Stable Diffusion v2-1 base & Stability AI & Diffusion & 1B & Open &  \\ Dreamlike Diffusion 1.0 & Dreamlike\_art & Diffusion & 1B & Open &  \\ Dreamlike Photoreal 2.0 & Dreamlike.art & Diffusion & 1B & Open &  \\ Openjourney & PromptHero & Diffusion & 1B & Open &  \\ Openjourney v4 & PromptHero & Diffusion & 1B & Open &  \\ Redshift Diffusion & nitroroscke & Diffusion & 1B & Open &  \\ Vintedois (22b) Diffusion & 22h & Diffusion & 1B & Open &  \\ SafeStableDiffusion-Week & TU Darmstadt & Diffusion & 1B & Open &  \\ SafeStableDiffusion-Medium & TU Darmstadt & Diffusion & 1B & Open &  \\ SafeStableDiffusion-Strong & TU Darmstadt & Diffusion & 1B & Open &  \\ SafeStableDiffusion-Max & TU Darmstadt & Diffusion & 1B & Open &  \\ Promotion \# Stable Diffusion v1-4 & Microsoft & Prompt engineering + 1B & Open &  \\ Lexica Search (Stable Diffusion v1-5) & Lexica & Diffusion + Retrieval & 1B & Open &  \\ DALLE-E 2 & OpenAI & Diffusion & 3.5B & Limited &  \\ DALLE-E mini & crajson & Autoregressive & 0.4B & Open &  \\ DALLE-mega & crajson & Autoregressive & 2.6B & Open &  \\ mindDALLE-E & Kakao Brain Corp. & Autoregressive & 1.3B & Open &  \\ CogView2 & Tsinghua University & Autoregressive & 6B & Open &  \\ MultiFusion & Aleph Alpha & Diffusion & 13B & Limited &  \\ DeepFloyd-IF M v1.0 & DeepFloyd & Diffusion & 0.4B & Open &  \\ DeepFloyd-IF L v1.0 & DeepFloyd & Diffusion & 0.9B & Open &  \\ DeepFloyd-IF XL v1.0 & DeepFloyd & Diffusion & 4.3B & Open &  \\ GigaGAN & Adobe & GAN & 1B & Limited &  \\   

Table 4: **Models evaluated in the HEIM effort.**photorealism, while no model achieved a score higher than 3 (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base). DALL-E 2 and models fine-tuned with photographs, such as Dreamlike Photoreal 2.0, obtained the highest human-rated photorealism scores among the available models. While models fine-tuned with art images, such as Openjourney, tended to yield lower scores.
3. **Aesthetics.** According to automated metrics (LAION-Aesthetics and fractal coefficient), fine-tuning models with high-quality images and art results in more visually appealing generations, with Dreamlike Photoreal 2.0, Dreamlike Diffusion 1.0, and Openjourney achieving the highest win rates (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_aesthetics_scenarios). Promptist, which applies prompt engineering to text inputs to generate aesthetically pleasing images according to human preferences, achieves the highest win rate for human evaluation, followed by Dreamlike Photoreal 2.0 and DALL-E 2.
4. **Origality.** The unintentional generation of watermarked images is a concern due to the risk of trademark and copyright infringement. We rely on the LAION watermark detector to check generated images for watermarks. Trained on a set of images where watermarked images were removed, GigaGAN has the highest win rate, virtually never generating watermarks in images (https://crfm.stanford.edu/heim/v1.1.0/?group=core_scenarios). On the other hand, CogView2 exhibits the highest frequency of watermark generation. Openjourney (86%) and Dreamlike Diffusion 1.0 (82%) achieve the highest win rates for human-rated originality (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_originality_scenarios). Both are Stable Diffusion models fine-tuned on high-quality art images, which enables the models to generate more original images.
5. **Reasoning.** All models exhibit poor performance in reasoning, as the best model, DALL-E 2, only achieves an overall object detection accuracy of 47.2% on the PaintSkills scenario (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_reasoning_scenarios). They often make mistakes in the count of objects (e.g., generating 2 instead of 3) and spatial relations (e.g., placing the object above instead of bottom). For human-rated alignment, DALL-E 2 outperforms other models but still receives an average score of less than 4 for Relational Understanding and the reasoning sub-scenarios of DrawBench. The next best model, DeepFloyd-IF XL, does not achieve a score higher than 4 across all the reasoning scenarios, indicating that there is room for improvement for text-to-image generation models for reasoning tasks.
6. **Knowledge.** Dreamlike Photoreal 2.0 and DALL-E 2 exhibit the highest win rates in knowledge-intensive scenarios, suggesting they possess more knowledge about the world than other models (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_knowledge_scenarios). Their superiority may be attributed to fine-tuning on real-world entity photographs.
7. **Bias.** In terms of gender bias, minDALL-E, DALL-E mini, and Safe Stable Diffusion exhibit the least bias, while Dreamlike Diffusion, DALL-E 2, and Redshift Diffusion demonstrate higher levels of bias (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_bias_scenarios). The mitigation of gender bias in Safe Stable Diffusion is intriguing, potentially due to its safety guidance mechanism suppressing sexual content. In terms of skin tone bias, Openjourney v2, CogView2, and GigaGAN show the least bias, whereas Dreamlike Diffusion and Redshift Diffusion exhibit more bias. Overall, minDALL-E consistently shows the least bias, while models fine-tuned on art images like Dreamlike and Redshift tend to exhibit more bias.
8. **Toxicity.** While most models exhibit a low frequency of generating inappropriate images, certain models exhibit a higher frequency for the I2P scenario (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_toxicity_scenarios). For example, OpenJourney, the weaker variants of SafeStableDiffusion, Stable Diffusion, Promptist, and Vintedois Diffusion generate inappropriate images for non-toxic text prompts in over 10% of cases. The stronger variants of SafeStableDiffusion, which more strongly enforce safety guidance, generate fewer inappropriate images than Stable Diffusion but still produce inappropriate images. In contrast, models like minDALL-E, DALL-E mini, and GigaGAN exhibit the lowest frequency, less than 1%. This disparity may be attributed to the data used to train these models. Moving forward, addressing inappropriate image generation requires careful consideration of training data and model design.
9. **Fairness.** Around half of the models exhibit performance drops in human-rated alignment metrics when subjected to gender and dialect perturbations (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_gender, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_dialect). This suggests that there is no universal pattern of fairness issues among the models. However, certain models incur significant performance drops, such as a -0.25 drop in human-rated alignment for Openjourney under dialect perturbation. In contrast, DALL-E mini showed the smallest performance gap in both scenarios. Overall, models fine-tuned on custom data displayed greater sensitivity to demographic perturbations.
10. **Robustness.** Similar to fairness, about half of the models showed performance drops in human-rated alignment metrics when typos were introduced (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_robustness). These drops were generally minor, with the alignment score decreasing by no more than 0.2, indicating that these models are robust against prompt perturbations.
11. **Multilinguality.** Translating the MS-COCO prompts into Hindi, Chinese, and Spanish resulted in decreased image-text alignment for the vast majority of models (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_chinese, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_hindi, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_spanish). A notable exception is CogView 2 for Chinese, which is known to perform better with Chinese prompts than with English prompts. DALL-E 2, the top model for human-rated image-text alignment (4.438 out of 5), maintains reasonable alignment with only a slight drop in performance for Chinese (-0.536) and Spanish (-0.162) prompts but struggles with Hindi prompts (-2.640). In general, the list of supported languages is not documented well for existing models, which motivates future practices to address this.
12. **Efficiency.** Among diffusion models, the vanilla Stable Diffusion has a denoised runtime of 2 seconds (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_efficiency_scenarios). Methods with additional operations, such as prompt engineering in Promptist and safety guidance in SafeStableDiffusion, as well as models generating higher resolutions like Dreamlike Photoreal 2.0, exhibit slightly slower performance. Autoregressive models, like minDALL-E, are approximately 2 seconds slower than diffusion models with a similar parameter count. We also found that most models display significant variation in the raw runtime across multiple runs, possibly due to queuing of requests or interference among concurrent requests.
13. **Overall trends in aspects.** Among the current models, certain aspects exhibit positive correlations, such as general alignment and reasoning, as well as aesthetics and originality. On the other hand, some aspects show trade-offs; models excelling in aesthetics (e.g., Openjourney) tend to score lower in photorealism, and models proficient in bias and toxicity mitigation (e.g., minDALL-E) may not perform the best in image-text alignment and photorealism. Overall, several aspects deserve attention. Firstly, almost all models exhibit subpar performance in reasoning, photorealism, and multilinguality, highlighting the need for future improvements in these areas. Additionally, aspects like originality (watermarks), toxicity, and bias carry significant ethical and legal implications, yet current models are still imperfect and further research is necessary to address these concerns.
14. **Prompt engineering.** Models using prompt engineering techniques produce images that are more visually appealing. Promptist + Stable Diffusion v1-4 outperforms Stable Diffusion in terms of human-rated aesthetics score while achieving a comparable image-text alignment score (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_quality_scenarios).
15. **Art styles.** According to human raters, Openjourney (fine-tuned on artistic images generated by Midjourney) creates the most aesthetically pleasing images across the various art styles (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_art_styles). It is followed by Dreamlike Photoreal 2.0 and DALL-E 2. DALL-E 2 achieves the highest human-rated alignment score. Dreamlike Photoreal 2.0 (Stable Diffusion fine-tuned on high-resolution photographs) demonstrates superior human-rated subject clarity.
16. **Correlation between human and automated metrics.** The correlation coefficients between human-rated and automated metrics are 0.42 for alignment (CLIPScore vs human-rated alignment), 0.59 for image quality (FID vs human-rated photorealism), and 0.39 for aesthetics (LAION aesthetics vs human-rated aesthetics) (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_fid, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base). The overall correlation is weak, particularly for aesthetics. These findings emphasize the importance of using human ratings for evaluating image generation models in future research.
17. **Diffusion vs autoregressive models.** Among the open-sourced autoregressive and diffusion models, autoregressive models require a larger model size to achieve performance comparable to diffusion models across most metrics. Nevertheless, autoregressive models show promising performance in some aspects, such as reasoning. Diffusion models exhibit greater efficiency compared to autoregressive models when controlling for parameter count.

18. **Model scales.** Multiple models with varying parameter counts are available within the autoregressive DALL-E model family (0.4B, 1.3B, 2.6B) and diffusion DeepFloyd-IF family (0.4B, 0.9B, 4.3B). We find that larger models tend to outperform smaller ones in all human-rated metrics, including alignment, photorealism, subject clarity, and aesthetics (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base).
19. **What are the best models?** Overall, DALL-E 2 appears to be a versatile performer across human-rated metrics. However, no single model emerges as the top performer in all aspects. Different models show different strengths. For example, Dreamlike Photoreal excel in photorealism, while Openjourney in aesthetics. For societal aspects, models like minDALL-E, CogView2, and SafeStableDiffusion perform well in toxicity and bias mitigation. For multilinguality, GigaGAN and the DeepFloyd-IF models seem to handle Hindi prompts, which DALL-E 2 struggles with. These observations open up new research avenues to study whether and how to develop models that excel across multiple aspects.

## 8 Related work

Holistic benchmarking.Benchmarks drive the advancements of AI by orienting the directions for the community to improve upon [20; 54; 55; 56]. In particular, in natural language processing (NLP), the adoption of meta-benchmarks [57; 58; 59; 60] and holistic evaluation  across multiple scenarios or tasks has allowed for comprehensive assessments of models and accelerated model improvements. However, despite the growing popularity of image generation and the increasing number of models being developed, a holistic evaluation of these models has been lacking. Furthermore, image generation encompasses various technological and societal impacts, including alignment, quality, originality, toxicity, bias, and fairness, which necessitate comprehensive evaluation. Our work fills this gap by conducting a holistic evaluation of image generation models across 12 important aspects.

Benchmarks for image generation.Existing benchmarks primarily focus on assessing image quality and alignment, using automated metrics. Widely used benchmarks such as MS-COCO  and ImageNet  have been employed to evaluate the quality and alignment of generated images. Metrics like Frechet Inception Distance (FID) , Inception Score , and CLIPScore  are commonly used for quantitative assessment of image quality and alignment.

To better capture human perception in image evaluation, crowdsourced human evaluation has been explored in recent years [25; 6; 35; 61]. However, these evaluations have been limited to assessing aspects such as alignment and quality. Building upon these crowdsourcing techniques, we extend the evaluation to include additional aspects such as aesthetics, originality, reasoning, and fairness.

As the ethical and societal impacts of image generation models gain prominence , researchers have also started evaluating these aspects [33; 29; 8]. However, these evaluations have been conducted on only a select few models, leaving the majority of models unevaluated in these aspects. Our standardized evaluation addresses this gap by enabling the evaluation of all models across all aspects, including ethical and societal dimensions.

Art and design.Our assessment of image generation incorporates aesthetic evaluation and design principles. Aesthetic evaluation considers factors like composition, color harmony, balance, and visual complexity [62; 63]. Design principles, such as clarity, legibility, hierarchy, and consistency in design elements, also influence our evaluation . Combining these insights, our aim is to determine whether generated images are visually pleasing, with thoughtful compositions, harmonious colors, balanced elements, and an appropriate level of visual complexity. We employ objective metrics and subjective human ratings for a comprehensive assessment of aesthetic quality.

## 9 Conclusion

We introduced Holistic Evaluation of Text-to-Image Models (HEIM), a new benchmark to assess 12 important aspects in text-to-image generation, including alignment, quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. Our evaluation of 26 recent text-to-image models reveals that different models excel in different aspects, opening up research avenues to study whether and how to develop models that excel across multiple aspects. To enhance transparency and reproducibility, we release our evaluation pipeline, along with the generated images and human evaluation results. We encourage the community to consider the different aspects when developing text-to-image models.