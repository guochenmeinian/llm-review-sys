# Language-driven Scene Synthesis using Multi-conditional Diffusion Model

An Dinh Vuong

FSOFT AI Center

Vietnam

&Minh Nhat Vu

TU Wien, AIT GmbH

Austria

&Toan Tien Nguyen

FSOFT AI Center

Vietnam

&Baoru Huang

Imperial College London

UK

&Dzung Nguyen

FSOFT AI Center

Vietnam

&Thieu Vo

Ton Duc Thang University

Vietnam

&Anh Nguyen

University of Liverpool

UK

###### Abstract

Scene synthesis is a challenging problem with several industrial applications. Recently, substantial efforts have been directed to synthesize the scene using human motions, room layouts, or spatial graphs as the input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a language-driven scene synthesis task, which is a new task that integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results illustrate that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/.

## 1 Introduction

Scene synthesis has gained significant attention from the research community in the past few years (Ye et al., 2022). This area of research has numerous applications such as virtual simulation, video animation, and human-robot communication (Yi et al., 2023). Recently, many scene synthesis methods have been proposed (Ye et al., 2022; Yi et al., 2023; Wang et al., 2019). However, most of these methods generate objects based on furniture distribution (Paschalidou et al., 2021) rather than customized user preferences. On the contrary, arranging household appliances is a personalized task, in which the most suitable room layout is usually the one that fits our eyes best (Chang et al., 2017). In this work, we hypothesize that the scene can be synthesized from multiple conditions such as human motion, object location, and customized user preferences (_i.e._, text prompts). We show that the text prompt input plays an important role in generating physically realistic and semantically meaningful scenes while enabling real-world scene editing applications.

Existing literature on scene synthesis often makes assumptions about scene representation, such as knowing floor plan (Wang et al., 2018) or objects are represented using bounding boxes (Paschalidouet al., 2021). These assumptions limit the ability to represent the object in a finer manner or edit the scene. To address these limitations, we revisit the 3D point clouds representation (Zhao et al., 2021), a traditional approach that can capture both the spatial position and the shape of objects (Rusu et al., 2007). The use of 3D point clouds can represent objects that are not directly linked to human movement, which is a constraint in the representation of contact points in (Ye et al., 2022), and lead to a more comprehensive and meticulous representation of the scenes.

To synthesize the scene from multiple conditions, including existing objects of the scene represented by 3D point clouds, human motion, and their textual command, we propose a new multi-conditional diffusion model. Although several multi-conditional diffusion models have been presented recently (Nichol et al., 2021), most of them focus on images/videos and employ _implicit_ unification of all latent spaces (Ma et al., 2023; Nguyen et al., 2023; Qin et al., 2023; Zhang and Agrawala, 2023; Zhao et al., 2023; Ruan et al., 2022). On the other hand, we introduce the first multi-conditional diffusion model on 3D point clouds and an _explicit_ mechanism to combine multiple conditions. Specifically, we introduce a concept of guiding points for scene synthesis and theoretically show that the guiding points explicitly contribute to the denoising process. The intensive empirical evidence confirms our findings.

Taking advantage of using the text prompt input, we introduce a new family of editing operations for the scene synthesis task, including object replacement, shape alternation, and object displacement. Inspired by (Tseng et al., 2022; Tevet et al., 2022; Song et al., 2020; Saharia et al., 2022), our proposed method enables users to select an object they want to modify and specify their desired changes via textual command. Our experimental results show that the proposed methodology yields encouraging editing results, demonstrating the potential of our method to bridge the gap between research and practical real-world applications. Our contribution can be summarized as:

* We present the language-driven scene synthesis task, a new challenge that generates objects based on human motions and given objects while following user linguistic commands.
* We propose a new multi-conditional diffusion model to tackle the language-driven scene synthesis task from multiple conditions.
* We validate our method empirically and theoretically, and introduce several scene-editing applications. The results show remarkable improvements over state-of-the-art approaches.

## 2 Related Works

**Scene Representation.** Generating new objects in a scene requires a scene representation method. Prior works have proposed many approaches to address this problem. Fisher et al. (2012) projects objects onto the 2D surface and sample jittered points for each object. Similarly, 2D heatmaps are implemented in (Qi et al., 2018). However, the major limitation of both mentioned approaches is that boundaries of 2D projections often do not capture sufficient details of 3D objects (Yi et al., 2019). Recent approaches, such as ATISS (Paschalidou et al., 2021) and MIME (Yi et al., 2023) represent each object as a 3D bounding box using a quadruple of category, size, orientation, and position, which has limitation in estimating the corresponding shape of the object. Additionally, Ye et al. (2022) represent objects as contact points with a sequence of human movements, but this approach

Figure 1: We introduce _Language-driven Scene Synthesis_ task, which involves the leverage of human-input text prompts to generate physically plausible and semantically reasonable objects.

only works for objects attached to human motions and ignores detached objects. To overcome these shortcomings, we propose a representation method based on 3D point clouds (Qi et al., 2017) that estimates not only the positions of room objects but also their shapes (Yi et al., 2019).

**Scene Synthesis.** The literature about scene synthesis can be categorized into the following groups. _i) Scene synthesis from floor plan_(Paschalidou et al., 2021; Wang et al., 2019, 2021; Ritchie et al., 2019; Wang et al., 2018): generating objects that fit a given floor layout, which clearly lacks incorporating human interactions with objects. _ii) Scene synthesis from text_(Chang et al., 2015, 2014, 2017; Savva et al., 2017; Ma et al., 2018): completing a scene given a textual description of the room arrangement. The majority of these works do not consider human motions in the generated scenes. The prevalent solution, for instance, SceneSeer (Chang et al., 2017) implements a scene template capturing the relationship between room objects. However, this method necessitates considerable manual effort, resulting in inflexibility. _iii) Scene synthesis from human motions_(Yi et al., 2023; Ye et al., 2022; Nie et al., 2022; Yi et al., 2023): generating objects aligning with a human motions sequence. The assumption in this direction is more practical since humans are interactive, but the existing issue is that the generated object often complies with the dataset distribution than personalized intentions. _iv) Scene synthesis from spatial graph_(Jiang et al., 2018; Li et al., 2019; Dhamo et al., 2021; Wang et al., 2019; Qi et al., 2018): rendering scene from a spatial graph. Despite having a great potential to describe large 3D scenes (Wald et al., 2020), spatial graphs require regular updates to account for entities such as humans with multifaceted roles, which results in extensive computation (Gadre et al., 2022). Since humans usually arrange objects with their personalized intentions, we propose a task that comprises both text prompts and human motions as the input, which aims to overcome the domain gap from scene synthesis research to real-life practice.

**Diffusion Models for Scene Synthesis.** Diffusion probabilistic models are a group of latent variable models that leverage Markov chains to map the noise distribution to the original data distribution (Sohl-Dickstein et al., 2015; Song and Ermon, 2020; Ho et al., 2020; Luo and Hu, 2021). In Dhariwal and Nichol (2021), classifier-guided diffusion is introduced for conditional generation tasks and has since been improved by Nichol et al. (2021); Ho et al. (2022). Subsequently, inspired by these impressive results, numerous works based on conditional diffusion models have achieved state-of-the-art outcomes in different generative tasks (Liu et al., 2023; Tevet et al., 2022; Tseng et al., 2022; Le et al., 2023; Huang et al., 2023). However, in our problem settings, the condition is a complicated system that includes not only textual commands but also the existing objects of the scene and human motions. Therefore, a solution to address multi-conditional guidance is required. While multi-conditional diffusion models have been actively researched, most of them focus on images/videos and utilize implicit unification of all latent spaces (Ma et al., 2023; Ruan et al., 2022; Zhang and Agrawala, 2023; Zhao et al., 2023). On the other hand, we propose a novel approach to handle multi-conditional settings by predicting the guiding points as the mean of the original data distribution. We demonstrate that the guiding points explicitly contribute to the synthesis results.

## 3 Methodology

### Problem Formulation

We input a text prompt \(e\) and a partial of the scene \(\), which consists of human pose \(H\) and a set of objects \(\{O_{1},O_{2},,O_{M}\}\). Each object is represented as a point cloud: \(O_{i}=\{(x_{0}^{i},y_{0}^{i},z_{0}^{i}),,(x_{N}^{i},y_{N}^{i},z_{N}^{i})\}\). Let \(=\{e,H,O_{1},,O_{M}\}\) be the generative conditions. Our goal is to generate object \(O_{M+1}\) consistent with the room arrangement of \(\), human motion \(H\), and semantically related to \(e\):

\[(O_{M+1}|)\;.\] (1)

We design a guidance diffusion model to handle the task. The forward process is adapted as in (Ho et al., 2020), and our primary focus is on developing an effective backward (denoising) process. Our problem involves multiple conditions, so we propose fusing all conditions into a new concept called _Guiding Points_, which serves as a comprehensive feature representation. We provide theoretical evidence demonstrating that guiding points explicitly contribute to the backward process, unlike the previous implicit unification of multi-conditional diffusion models in the literature (Sohl-Dickstein et al., 2015; Song and Ermon, 2020; Ho et al., 2020; Luo and Hu, 2021).

### Multi-conditional Diffusion for Scene Synthesis

**Forward Process.** Given a point cloud \(_{0}\) drawn from the interior of \(O_{M+1}\). We add Gaussian noise each timestep to obtain a sequence of noisy point clouds \(_{1},_{2},,_{T}\) as follows:

\[q(_{t+1}|_{t})=(}_{t },_{t}); q(_{1:T}|_{0})=_{t=0}^{T -1}q(_{t+1}|_{t})\,\] (2)

where variance \(_{t}\) is determined by a known scheduler. Eventually when \(T\), \(_{T}\) is equivalent to an isotropic Gaussian distribution.

**Backward Process.** The goal of the conditional reverse noise process is to determine the backward probability \((_{t}|_{t+1},)\). We begin with the following proposition:

**Proposition 1**.: \[(_{t}|_{t+1},)=_{t}| _{t+1})(|_{t})}{(, _{t+1})}[q(_{t+1}|_{0})]\.\] (3)

Proof.: See Appendix. 

_Remark 1.1_.: The expression in Eq. (3) provides a more explicit formulation of \((_{t}|_{t+1},)\) in comparison to the conditional backward process presented in (Dhariwal and Nichol, 2021). This enables a further examination of the impact of the multi-condition \(\) on the prediction of \(_{0}\).

_Remark 1.2_.: Sohl-Dickstein et al. (2015) indicate that generating canonical samples from Eq. (3) is often infeasible. Assume \(_{0}\) is drawn from a uniform distribution over a domain of \(\) and \(q(|_{0})\) is non-zero uniform over \(\), we discretize the expression in Eq. (3) by

\[(_{t}|_{t+1},)_ {t}|_{t+1})(|_{t})}{(, _{t+1})}}|}_{_{0}}}q(_{t+1}|_{0})q(_{0})\,\] (4)

with \(}\) being the sampling set of \(_{0}\). Using Bayes' theorem, we obtain:

\[q(_{0})=_{0})q(_{0},)}{q( _{0},)}=_{0},)}{q(| _{0})}=_{0}|)q()}{q( |_{0})}\.\] (5)

As \(q()\) is independent with denoising process, \(q(_{0})\) is uniform, and \(q(|_{0})\) is non-zero uniform over \(\); we infer \(q(_{0}|)\) is also uniform over \(\). In addition, \(q(_{0}) 0 q(_{0}|) 0\); thus, \(}\) can also be considered as a sampling set of \(_{0}|\). We can now derive Eq. (4) using Eq. (5) as follows:

\[(_{t}|_{t+1},) q(_ {t}|_{t+1})(|_{t})}|}_{_{0}|}}q(_{t+1}| _{0})q(_{0}|)\,\] (6)

where \(=q()/(q(|_{0})(, _{t+1}))\). Denote \(_{0}\) as the mean of the initial probability distribution \(q(_{0})\). Consequently, \(_{0}|\) can be used to approximate \(_{0}|\). We further consider \(}\), the sampling set of estimated points of \(_{0}\) as an approximation of \(}\); therefore, Eq. (6) can be reduced to:

\[(_{t}|_{t+1},) q(_ {t}|_{t+1})(|_{t})}|}_{_{0}|}}q(_ {t+1}|_{0})q(_{0}|)\.\] (7)

In our problem settings, \(_{0}\) is drawn from the interior of an object uniformly; therefore, Remark 1.2 is applicable. Inspired by Tevet et al. (2022), we model the conditional reverse noising process in Eq. (7) using the following reduction form:

\[(_{t}|_{t+1},) f(_{t+1}, ,})\,\] (8)

where \(f\) is a neural network. The main difference between our proposal and related works is that we incorporate the term guiding points \(}\) into the backward probability.

### Guiding Points Network

**Observation.** The text-based modality is a reliable and solid source of information to synthesize unseen objects, given partial entities of the scene (Chang et al., 2014). Indeed, while many works contemplate floor plan as a significant input (Wang et al., 2019; Yi et al., 2023; Paschalidou et al., 2021), we find that the floor plan is redundant for generating the target object when the context is known. Consider the example in Fig. 1(a)- 1(c), which is also common in not only our problem but also in our daily life. In this scene, a human is sitting on a chair with another nearby; suppose the human gives a command: "_Place a desk to the right of me and next to the chair._". Regardless of the floor plan, there is only an optimal setting location, depicted in Fig. 1(b), for the desk. Therefore, the text-based modality provides enough information to achieve our task.

In this example, what is the most crucial factor that the modal command offers? The answer is the _spatial relation_ implied from the linguistic meanings, which is also indicated in Chang et al. (2014). We further argue this spatial relationship is entity-centric as it only prioritizes certain scene entities. Indeed, if there are two round tables added to the scene as in Fig. 1(c), the optimal location for the target object remains the same. Consequently, the targeted object primarily depends on the position of the human and the chair, _i.e._, a defined set of scene entities.

To align each given scene entity with the target object \(O_{M\!+\!1}\), we use 3D transformations approach (Besl and McKay, 1992). Specifically, for a given point cloud \(O_{i}=\{o_{0}^{i},o_{1}^{i},,o_{N}^{i}\}\), our objective is to compute a transformation matrix for each point \(o_{j}^{i}\) given by:

\[_{i;j}=_{x}^{i}&_{x}^{i}&_{x}^{i}&t _{x}^{i}\\ _{y}^{i}&_{y}^{i}&_{y}^{i}&t_{y}^{i}\\ _{z}^{i}&_{z}^{i}&_{z}^{i}&t_{z}^{i}\\ 0&0&0&1.\] (9)

We can predict each point \(_{i}\) of the target object by applying transformation matrix \(_{i;j}\) to each \(o_{j}^{i}\):

\[_{j}^{i}\\ _{j}^{i}\\ _{j}^{i}\\ 1=_{x}^{i}&_{x}^{i}&_{x}^{i}&t_{ x}^{i}\\ _{y}^{i}&_{y}^{i}&_{y}^{i}&t_{y}^{i}\\ _{z}^{i}&_{z}^{i}&_{z}^{i}&t_{z}^{i}\\ 0&0&0&1x_{j}^{i}\\ y_{j}^{i}\\ z_{j}^{i}\\ 1,\] (10)

where \(o_{j}^{i}=(x_{j}^{i},y_{j}^{i},z_{j}^{i})\) and \(_{j}^{i}=(_{j}^{i},_{j}^{i},_{j}^{i})\). We refer this concept of predicting \(_{j}^{i}\) as guiding points, which has been introduced in the main paper.

Next, as observed in Fig. 1(c), we find that each object \(O_{i}\) contributes differently in providing clues to locate the object \(O_{M\!+\!1}\). This implies that each object should possess its own attention score, represented by a scalar \(_{i}\), which resembles the famous attention technique (Vaswani et al., 2017). We employ this attention mechanism to determine attention matrix \(=[_{0},_{1},,_{M}]\) of the set of objects and utilize the 3D transformation in Eq. (10) to determine guiding points.

**Guiding Points Prediction.** Fig. 3 illustrates the method overview. We hypothesize that the target object can be determined from the scene entities using 3D transformation techniques (Park et al., 2017). Each scene entity contributes a distinct weight to the generation of the target object, which can be inferred from the spatial context of the text prompt as discussed in our Observation. Our

Figure 2: **Motivational example. In this example, we want to generate an object aligned with the following command: “_Place a desk to the right of me and next to the chair._”central component, the Guiding Points Network utilizes the theory in Section 3.2 to establish a translation vector between each scene entity and the target object and then utilizes the high-level translation to determine point-wise transformation matrices for each scene entity. Finally, guiding points are obtained by aggregating the contributions of all scene entities, which are weighted by their corresponding attention scores.

Initially, we extract point-level features from the input by feeding human motions \(H\) into a human pose backbone (Hassan et al., 2021) and objects \(O_{1},O_{2},,O_{M}\) into a point cloud backbone (Qi et al., 2017) to obtain point-level representations: \([Q_{0},Q_{1},Q_{2},,Q_{M}]\). We encode \(e\) using a text encoder (Radford et al., 2021) to obtain the text feature \(e^{}\). Following the point-level extraction for each scene entity, we extract spatial information from the text prompt by utilizing the off-the-shelf multi-head attention layer (Vaswani et al., 2017), where the input key is the text embedding \(e^{}\), the input queries are the given scene entities. The attention layer computes two outputs: \(\), which represents the weights that indicate the contribution of each scene entity to the generation of guiding points, and \(\), which denotes the translation vectors between the scene entities and the target object.

We then utilize the weight \(\) and translation vectors \(\) to calculate the guiding points \(}\) using 3D transformations. From the \(_{i}\) of each scene entity, we predict a set of transformation matrices \(_{i}\) for each point of that scene entity. This approach, apart from applying an identical translation vector to every point, diversifies the point set distribution, therefore, can capture the shape of the target object. The guiding points \(}_{i}\) derived from \(Q_{i}\) are then calculated by applying the corresponding point-wise transformation matrices \(_{i}\) to the points of the scene entity (Fig. 3). Finally, we composite all relative guiding points from each entity with its corresponding \(_{i}\) as follows: \(}=_{i=0}^{M}}_{i}_{i}\).

**Output Procedure.** Eventually, we want to sample \(_{t}\) given the input \(_{t+1}\) and conditions \(\). We compute the signal \(_{t}\) given by: \(_{t}=(_{t+1},t+1,})\). More information about the model and training procedure can be found in the Appendix.

## 4 Experiment

We first compare our method for the scene synthesis task with other recent work and demonstrate its editing applications. We then assess the effectiveness of the guiding points to the synthesis results. All experiments are trained on an NVIDIA GeForce 3090 Ti with 1000 epochs within two days.

### Language-driven Scene Synthesis

**Datasets.** We use the following datasets to evaluate the language-driven scene synthesis task: _i) PRO-teXt:_ We contribute PRO-teXt, an extension of PROXD (Hassan et al., 2019) and PROXE (Zhang et al., 2020). There are 180/20 interactions for training/testing in our PRO-teXt dataset. _ii) HUMANISE:_

Figure 3: **Methodology overview.** Our main contribution is the _Guiding Points Network_, where we integrate all information from the conditions \(\) to generate guiding points \(}\).

We utilize 143/17 interactions of HUMANISE (Wang et al., 2022) to train/test. Details of the datasets are specified in the Appendix.

**Baselines.** We compare our approach **L**anguage-driven **S**cene Synthesis using Multi-conditional **D**iffusion **M**odel (LSDM) with: _i) ATISS_(Paschalidou et al., 2021). An autoregressive model that outputs the next object given some objects of the scene. _ii) SUMMON_(Ye et al., 2022). A transformer-based approach that predicts the human's contact points with objects. _iii) MIME_(Yi et al., 2023). An autoregressive model based on transformer techniques to handle human motions. Since the language-driven scene synthesis is a new task and none of the baselines utilize the text prompt, we also set up our model without using the text input for a fair comparison. _iv) MIME + text embedding_. We extend MIME with a text encoder to handle the text prompts; the represented text features are directly concatenated to the transformer layers of the original MIME's architecture. _v) MCDM_. We implement a multi-conditional diffusion model (MCDM) that directly combines all features from the input (3D point clouds, text prompts).

**Metrics.** As in (Lyu et al., 2021), we employ three widely used metrics in 3D point cloud tasks, Chamfer distance (CD), Earth Mover's distance (EMD), and F1 score, to evaluate the degree of correspondence between the predicted point cloud and the ground truth of the target object.

**Quantitative Results.** Table 1 summarises the results of our method and other approaches. In the absence of text prompts, our method shows remarkable improvements in CD and EMD metrics, while MIME achieves the highest F1 score. When text prompts are employed, our LSDM yields dominant results compared to all baselines in all metrics. Notably, the F1 scores of our method are approximately \(1.45\) times and \(1.76\) times greater than the second-best in PRO-teXt and HUMANISE, respectively. This result confirms the importance of text prompts and our explicit strategy for the scene synthesis task.

**Qualitative Results.** We utilize the object recovery algorithm (Ye et al., 2022) and 3D-FUTURE (Fu et al., 2021) to synthesize 3D scenes. Fig. 4 demonstrates the qualitative results. It is clear that our proposed method successfully generates more suitable objects, while other benchmark models fail to capture the desired positions. More qualitative results can be found in our demonstration video.

**User Study.** We ask 40 users to evaluate the generated results of all methods from the PRO-teXt and HUMANISE datasets. 8 output videos are generated from each method and placed side-by-side. The participants evaluate based on three criteria: naturalness, non-collision, and intentional matching. Fig. 5 shows that our method is preferred over the compared baselines in most cases and is the closest to the ground truth.

### Scene Editing with Text Prompts

The use of text prompts provides a natural way to edit the scene. We introduce three operations based on users' text prompts: _i) Object Replacement_: given an object \(O_{M+1}\), the goal is to replace \(O_{M+1}\) with a new object \(O_{M+1}^{*}\) via a text prompt \(e^{*}\). _ii) Shape Alternation_: we alter the shape of an object \(O_{M+1}\) by a text prompt \(e^{*}\) to obtain a new shape built upon \(O_{M+1}\). We follow Tevet et al. (2022) by retaining a quarter of \(O_{M+1}\)'s point cloud, which contains the lowest \(z\)-coordinate points (i.e., those closest to the ground) while diffusing the remaining 75% through our pre-trained model. _iii) Object Displacement_: we want to move object \(O_{M+1}\) to a new position using a textual command \(e^{*}\).

    &  &  \\  Baseline & CD \(\) & EMD \(\) & F1 \(\) & CD \(\) & EMD \(\) & F1 \(\) \\  ATISS (Paschalidou et al., 2021) & 2.0756 & 1.4140 & 0.0663 & 5.3595 & 2.0843 & 0.0308 \\ SUMMON (Ye et al., 2022) & 2.1437 & 1.3994 & 0.0673 & 5.3260 & 2.0827 & 0.0305 \\ MIME (Yi et al., 2023) & 2.0493 & 1.3832 & 0.0990 & 5.4259 & 2.0837 & 0.0628 \\ MIME (Yi et al., 2023) + text embedding & 1.8424 & 1.2865 & 0.1032 & 4.7035 & 1.8201 & 0.0849 \\ MCDM & 0.6301 & 0.7269 & 0.3574 & 0.8586 & 0.8757 & 0.2515 \\  LSDM w.o. text (Ours) & 0.9134 & 1.0156 & 0.0506 & 1.1740 & 1.1128 & 0.0412 \\ LSDM (Ours) & **0.5365** & **0.5906** & **0.5160** & **0.7379** & **0.7505** & **0.4395** \\   

Table 1: **Scene synthesis results.** Bold and underline are the best and second-best, respectively.

**Qualitative Results.** Fig. 6 illustrates the proposed editing operations, showcasing the promising and semantically meaningful outcomes obtained by our method. The above settings are close to real life, which bridges the gap between the scene synthesis literature and practical applications.

**Quantitative Results.** Table 2 shows the quantitative results of scene editing operations. The experiment setups are described in our Appendix. Table 2 demonstrates that our approach yields decent performance for the shape alternation operation while there is still scope for improvement in the other operations. It is worth noting that shape alternation is relatively easier than object replacement and displacement as inherits \(25\%\) of the target object as the input.

   Operation & CD \(\) & EMD \(\) & F1 \(\) \\  Object replacement & 12.8825 & 2.1757 & 0.0574 \\ Object displacement & 7.4405 & 2.2897 & 0.0032 \\ Shape alternation & **3.4249** & **1.3754** & **0.5108** \\   

Table 2: **Scene editing results.**

Figure 4: **Scene visualization.** We present illustrative examples generated by all baseline methods on two test cases: generating a desk (first row) and a sofa (second row).

Figure 5: **User evaluation of all baselines.**

Figure 6: **Scene editing visualization.** We provide qualitative results of three introduced editing operations. The text prompts are indicated above each example.

### Ablation Study

This section presents an in-depth study of the proposed neural architecture. We show the impact of each modality on the overall results. We also provide a detailed analysis of the importance and effectiveness guiding points technique.

**How does each modality contribute to the performance?** To understand the contributions of each component utilized to predict guiding points as well as overall results, we conduct a comprehensive analysis in Table 3. In the first row, our model does not predict the high-level translation vector \(\), which results in the absence of guiding points. When our model does not predict \(\) (second row), it utilizes \(\) to generate an identical transformation matrix for each point of an entity. In the next two rows, only a partial representation of the scene (human/objects) is used. Table 3 shows that the utilization of \(,\), as well as the inclusion of text prompts, humans, and existing objects substantially impact the prediction of guiding points and the final results.

**Can guiding points represent the target object?** Recall that the guiding points are estimations of the mean \(_{0}\). To determine whether our method can reasonably predict \(_{0}\) from the conditions \(\), we provide both quantitative and qualitative assessments. We first compare \(s^{2}\) and \(d_{0}^{2}\) in Table 4. The relatively small errors suggest that our model has good generalization capability towards unseen objects, as the forecast guiding points are in close proximity to the ground truth centroids. In addition, we provide demonstrations of our proposed guiding points mechanism. Fig. 7 depicts guiding points \(}\) in three test scenarios. The visualization illustrates that our guiding points (red) extend seamlessly over the centroids of the target objects (blue). This outcome fulfills our original intent when synthesizing the new object based on the guiding points. Furthermore, referring to Table 3 and 4, we observe that as the MSE decreases, the model's performance improves, providing empirical confirmation of the usefulness of guiding points.

**How does each scene entity contribute to the generation of guiding points?** Fig. 7(a) demonstrates how the attention mechanism determines the most relevant objects providing information for the generation of guiding points. In this example, in response to the text prompt "Place a one-seater sofa under me and beside the sofa", the utilized mechanism assigns the highest attention weight to the human and subsequently to the sofa numbered 2 referenced in the prompt. This is consistent with the semantic meaning of the text prompt, which enables our model to correctly leverage the attention weights from scene entities to generate guiding points.

**How many guiding points are needed for the estimation process?** Fig. 7(b) demonstrates the influence of the number of guiding points used in the estimation process on the CD and F1 metrics.

   Baseline & MSE \(\) \\  LSDM w.o. predicting \(\) & 0.5992 \\ LSDM predicting \(}\) from only objects & 0.4618 \\ LSDM predicting \(}\) from only human & 0.3388 \\ LSDM (ours) & 0.2091 \\  Minimal squared distance \(d_{0}^{2}\) & **0.0914** \\   

Table 4: **Guiding points evaluation.** We calculate the MSE between predicted \(}\) and the centroids of target objects.

   Baseline & Input used & \(}\) & CD \(\) & EMD \(\) & F1 \(\) \\  LSDM w.o. predicting \(\) & \(\) & none & 4.6172 & 2.1086 & 0.0391 \\ LSDM w.o. predicting \(\) & text, human, objects & partial & 1.8933 & 1.1350 & 0.2400 \\ LSDM predicting \(}\) from only objects & text, objects & partial & 1.5050 & 1.0653 & 0.3185 \\ LSDM predicting \(}\) from only human & text, human & partial & 1.0119 & 0.8419 & 0.3855 \\ LSDM (ours) & text, human, objects & full & **0.5365** & **0.5906** & **0.5160** \\   

Table 3: **Components Analysis.** We assess the performance of our method under different settings.

Figure 7: **Guiding points visualization.** The MSE is indicated above each sample.

The result illustrates that the effectiveness of our method is directly proportional to the number of guiding points employed. However, the performance of our approach reaches a plateau as the number of guiding points surpasses 1024. Consequently, to optimize the results as well as computational complexity, we set the number of guiding points to 1024.

## 5 Discussion

**Limitations.** Although achieving satisfactory results and meaningful scene editing applications, our LSDM approach still holds some limitations. First, our theoretical findings have an assumption constrained to uniform data like point clouds. Second, we train the guiding point network jointly with the denoising process; therefore, the predicted guiding points are not always aligned with the target object. Some failure cases of guiding points are visualized in the Appendix. In addition, our scene editing demonstrates modest results and necessitates future improvements.

**Broader Impact.** We believe our paper has two key applications. First, our proposed language-driven scene synthesis task can be applied to metaverse and animation. For instance, a user enters an empty apartment and gives different commands (e.g., "_Putting a sofa in the corner_," "_Placing a table next to the sofa_") to arrange the room furniture (where physical contact is not mandatory). Second, our proposed guiding points concept is a general concept and can be applied beyond the scope of scene synthesis literature. Guiding points can be applied to visual grounding tasks such as language driven-grasping (Vuong et al., 2023), where we can predict _guiding pixels_ indicating possible boxes from the conditions to guide the denoising process.

## 6 Conclusion

We have introduced language-driven scene synthesis, a new task that incorporates human intention into the process of synthesizing objects. To address the challenge of multiple conditions, we have proposed a novel solution inspired by the conditional diffusion model and guiding points to effectively generate target objects. The intensive experimental results show that our method significantly improves over other state-of-the-art approaches. Furthermore, we have introduced three scene editing operations that can be useful in real-world applications. Our source code, dataset, and trained models will be released for further study.