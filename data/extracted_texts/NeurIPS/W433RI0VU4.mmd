# MILP-StuDio: MILP Instance Generation via Block Structure Decomposition

Haoyang Liu1, Jie Wang1, Wanbo Zhang1, Zijie Geng1, Yufei Kuang1, Xijun Li2,3, Yongdong Zhang1, Bin Li1, Feng Wu1

1MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,

University of Science and Technology of China

2 Shanghai Jiao Tong University

3 Noah's Ark Lab, Huawei Technologies

{dgyoung,zhang_wb,ustcgzj,yfkuang}@mail.ustc.edu.cn,

lixijun@sjtu.edu.cn,

{jiewangx,zhyd73,binli,fengwu}@ustc.edu.cn

###### Abstract

Mixed-integer linear programming (MILP) is one of the most popular mathematical formulations with numerous applications. In practice, improving the performance of MILP solvers often requires a large amount of high-quality data, which can be challenging to collect. Researchers thus turn to generation techniques to generate additional MILP instances. However, existing approaches do not take into account specific block structures--which are closely related to the problem formulations--in the constraint coefficient matrices (CCMs) of MILPs. Consequently, they are prone to generate computationally trivial or infeasible instances due to the disruptions of block structures and thus problem formulations. To address this challenge, we propose a novel MILP generation framework, called Block Structure Decomposition (MILP-StuDio), to generate high-quality instances by preserving the block structures. Specifically, MILP-StuDio begins by identifying the blocks in CCMs and decomposing the instances into block units, which serve as the building blocks of MILP instances. We then design three operators to construct new instances by removing, substituting, and appending block units in the original instances, enabling us to generate instances with flexible sizes. An appealing feature of MILP-StuDio is its strong ability to preserve the feasibility and computational hardness of the generated instances. Experiments on commonly-used benchmarks demonstrate that with instances generated by MILP-StuDio, the learning-based solvers are able to significantly reduce over 10% of the solving time.

## 1 Introduction

Mixed-integer linear programming (MILP) is a fundamental mathematical optimization problem that finds extensive applications in the real world, such as scheduling , planning , and chip design . In industrial scenarios, the solving efficiency of MILPs is associated with substantial economic value. To speed up the solving process, a great number of high-quality MILP instances are required to develop or test the solvers. Here we give the following two examples. First, both traditional solvers  and learning-based solvers  rely heavily on a lot of MILP instances for hyperparameter tuning or model training. Second, evaluating the robustness of solvers needs a comprehensive MILP benchmark consisting of numerous instances. However, acquiring many instances is often difficult due to high acquisition costs or privacy concerns . As a result, the limited data availability poses great challenges and acts as a bottleneck for solver performance.

This challenge motivates a wide range of MILP generation techniques. In the past, researchers relied on problem-specific techniques for generation . These methods assumed knowledge of problem types and generated instances based on the corresponding mathematical formulations, such as the satisfiability problem , set covering problem , and others. However, these techniques require much expert knowledge to design and are limited to specific MILP problems. They also face limitations in practical scenarios where problem formulations are unknown .

In recent years, there has been some progress in general MILP generation that does not require explicit knowledge of the problem formulations. These approaches can be broadly classified into statistics-based and learning-based methods. Statistics-based approaches utilize a few instance statistics to sample in the MILP space . More advanced learning-based approaches, exemplified by G2MILP , leverage deep learning models to capture global instance features and iteratively modify constraints in the original instances. Though in the early stage, learning-based techniques offer convenience and strong adaptability for MILP generation, making them applicable in a wider range of practical scenarios . However, they still suffer from significant challenges. (1) They fail to account for the inherent problem structures adequately and disrupt instances' mathematical properties. This leads to low-quality instances with degrading computational hardness or infeasible regions. (2) Existing methods fail to generate instances with different sizes from the original ones, limiting instance diversity. (3) The iterative style to modify constraints becomes time-consuming when dealing with large-scale instances.

Therefore, a natural question arises: can we analyze and exploit the problem structures during generation to address the above challenges? Consider a MILP instance with constraints \(\), where \(\) is the constraint coefficient matrix (CCM), \(\) is the decision variable and \(\) is a vector. As shown in Figure 0(a), we observe that a great number of real-world MILP problems exhibit structures with repeated patterns of block units in their CCMs. In operational research, researchers have long noticed the similar block structures of CCMs across instances from the same problem type, and they have been aware of the critical role of CCMs in determining problem formulation and mathematical properties . Although a wide suite of CCM-based techniques have been developed to solve MILPs , existing works on MILP generation rarely pay attention to CCMs. Consequently, these works fail to preserve the block structures during the generation process.

In light of this, we propose a novel MILP generation framework called Block Structure Decomposition (MILP-StuDio), which takes into account the block structures throughout the generation process and addresses Challenge (1)-(3) simultaneously. Specifically, MILP-StuDio consists of three key steps. We begin by identifying the block structures in CCMs and decomposing the instances into block units, which serve as the building blocks in the MILP instances. We then construct a library of the block units, enabling efficient storage, retrieval, and utilization of the comprehensive block characteristics during the subsequent process. Leveraging this library, we design three block operators on the original instances to generate new ones, including block reduction (eliminating certain blocks from the original instances), block mix-up (substituting some blocks with others sampled from the library), and block expansion (appending selected blocks from the library). These operators enable us to generate instances with flexible sizes, effectively improving the diversity of instances.

Experiments demonstrate that MILP-StuDio has the following advanced features. (1) Hardness preservation. MILP-StuDio can effectively preserve the computational hardness and feasibility in the

Figure 1: Figure 0(a) visualizes the CCMs of four instances from the FA problem, where the white points represent the nonzero entries in CCMs. As we can see, the CCMs exhibit similar block structures across instances, with the patterns in red boxes being the block units. Figure 0(b) illustrates the block decomposition process, advanced features, and applications of our proposed MILP-StuDio.

generated instances. (2) Scalable generation. MILP-StuDio can generate instances with flexible sizes. (3) High efficiency. MILP-StuDio can reduce over two-thirds of the generation time in real-world large datasets. We observe an over 10% reduction in the solving time for learning-based solvers using instances generated by MILP-StuDio.

## 2 Background

### MILP and MILP with Block Structure

A MILP instance takes the form of:

\[_{^{n}}^{}, ,,^{p} ^{n-p}.\] (1)

In Formula (1), \(\) denotes the decision variables, \(^{n}\) denotes the coefficients in the objective function, \(^{m n}\) is the constraint coefficient matrix (CCM) and \(^{m}\) denotes the terms on the right side of the constraints, respectively. The vectors \((\{-\})^{n}\) and \((\{+\})^{n}\) denote lower and upper bounds for the variables, respectively.

In real-world applications, a significant portion of MILPs exhibit block structures--consisting of many block units--in their constraint coefficient matrices (CCMs) \(\). These problems, referred as MILPs with block structures, include many commonly-used and widely-studied datasets in recent papers on learning-based solvers [8; 9; 10; 28], such as combinatorial auctions (CA), capacitated facility location (FA), item placement (IP), multiple knapsacks (MIK), and workload balancing (WA). In Figure 2, we visualize the CCMs of MILP instances using a black-and-white digital _image representation_. In this representation, the rows and columns of the digital images correspond to the constraints and variables in the MILPs, respectively. To construct the digital image, we assign a pixel value of 255 (white) to the entry \((i,j)\) if the corresponding entry in the CCM \([i,j]\) is nonzero. Conversely, if \([i,j]\) is zero, we set the pixel value to 0 (black). This mapping allows us to depict the sparsity patterns and structural characteristics of CCMs visually. For each problem, the CCMs of the instances present a similar block structure, characterizing specific mathematical formulations.

The importance of block-structured CCMs in the context of MILP solving has long been acknowledged by operational researchers, where instances with similar block structures share similar mathematical properties [29; 30; 31]. Furthermore, the block structures of CCMs are closely related to the problem formulations . Thus, the block matrices have shown great potential in accelerating the solution process for a family of MILP problems [21; 22; 23; 24]. One notable technique developed to exploit this structure is Dantzig-Wolfe decomposition  for solving large-scale MILP instances.

### Bipartite Graph Representation of MILPs

A MILP instance can be represented as a weighted bipartite graph \(=(,)\). The two sets of nodes \(=\{w_{1},,w_{m}\}\) and \(=\{v_{1},,v_{n}\}\) in the bipartite graph correspond to the MILP's constraints and variables, respectively. The edge set \(=\{e_{ij}\}\) comprises edges, each connecting a constraint node \(w_{i}\) with a variable node \(v_{j}\). The presence of an edge \(e_{ij}\) is determined by the coefficient matrix, with \(_{ij}=(e_{ij})\) as its edge feature, and an edge \(e_{ij}\) does not exist if \([i,j]=0\). Please refer to Appendix I.1 for more details on the graph features we use in this paper.

## 3 Motivated Experiments

Preserving the mathematical properties of the original instances is a fundamental concern in MILP generation . These properties encompass feasibility, computational hardness, and problem structures, with the latter being particularly crucial. The problem structure directly determines the problem formulation and, consequently, impacts other mathematical properties. However, it

Figure 2: Visualization of the CCMs of instances in four widely recognized benchmarks. The block structures can be commonly seen in MILP problems.

is important to note that the term "problem structure" can be ambiguous and confusing. There are different understandings and definitions of the problem structure--such as the bipartite graph structure  and the CCM's block structure --and we are supposed to identify the most relevant and useful ones that contribute to the mathematical properties of MILPs. Analyzing and exploiting these specific structure types become key factors in improving the quality of the generated instances.

### Challenges of Low-Quality Generation

G2MILP is the first learning-based approach for MILP instance generation. While it has shown promising performance, we observe that G2MILP still encounters difficulties in generating high-quality instances for MILPs with block structures. To evaluate its performance, we compare the graph structural distributional similarity and solving properties of the original and generated instances from the workload appointment (WA) benchmark  using Gurobi , a state-of-the-art traditional MILP solver. We set the masking ratio of G2MILP--which determines the proportion of constraints to be modified--to 0.01. The results are summarized in Table 1. In this table, the _Similarity_ metric refers to the graph structural distributional similarity score  (defined in Appendix I.3), _Time_ represents the average solving time (with a time limit 1,000s), and _Feasible ratio_ indicates the proportion of feasible instances out of the total instances. Results show that although the generated instances achieve a high similarity score, most of them are infeasible. Furthermore, the feasible instances exhibit a severe degradation in computational hardness.

### Visualization of CCMs

The aforementioned experiments provide evidence that by iterative modifications of sampled constraints, G2MILP can generate MILP with high graph structural distributional similarities to the original instances, but may lead to disruptions in mathematical properties. Consequently, it becomes necessary to explore alternative definitions of problem structures that offer stronger correlations to the mathematical properties of the instances and can be effectively preserved during the generation process. One such promising structure is the block structures within CCMs.

The concept of block structures within CCMs originates from traditional operational research and has proven to be effective for problem analysis [21; 22; 23; 24]. It is widely recognized that MILPs with similar block structures in CCMs often share similar formulations, resulting in similar mathematical properties . We visualize and compare the CCMs of the original and generated instances in Figure 3. In the middle figure, we observe that G2MILP breaks the block pattern in the left and introduces a noisy block in the bottom right. It becomes evident that the generation operation in G2MILP breaks the block structures presenting in the original instances. Thus, it motivates us that exploring and preserving the block structures in CCMs can hold the potential for generating high-quality instances.

## 4 Generation Framework Using Block Structure Decomposition

In this section, we introduce the proposed MILP-StuDio framework to generate high-quality instances. MILP-StuDio comprises three steps: block decomposition, construction of structure library, and block manipulations. We begin by presenting the concept of block decomposition for MILP in Section 4.1,

    & Similarity & Time & Feasible Ratio \\  Original & 1.000 & 1000.00 & 100.00\% \\ G2MILP & 0.854 & 12.01 & 10.00\% \\   

Table 1: The comparison of graph similarity and solving properties between the original instances and the generated instances using G2MILP , a popular MILP generation framework. Here we use 100 original instances to generate 1,000 instances.

Figure 3: Visualization of CCMs from original instances (left), instances generated by G2MILP (middle), and instances generated by MILP-StuDio (right).

as it forms the core of our method. The subsequent sections, from Section 4.2 to Section 4.4, provide a detailed explanation of each step. The overview of MILP-StuDio is depicted in Figure 4.

### Block Decomposition of MILP

In this part, we specify the block structures that we are interested in. CCMs are often reordered to achieve the well-studied block structures , including the block-diagonal (BD), bordered block-diagonal (BBD), and doubly bordered block-diagonal (DBBD) structures. We highlight the _block unit_ of the decomposition in blue in Equation (2). We can see that the former two structures are special cases of the latter one. Despite the simplicity, they are the building blocks for more complex block structures and are widely used in operational research .

\[_{1}&&&\\ &_{2}&&\\ &&&\\ &&&_{k}_{1}&&&\\ &_{2}&&\\ &&&\\ &&_{k}\\ _{1}&_{2}&&_{k}_{1}&&&_{1}\\ &_{2}&&_{2}\\ &&&&\\ &&&_{k}&_{k}\\ _{1}&_{2}&&_{k}&\] (2)

\((a)\) Block-diagonal \((b)\) Bordered block-diagonal \((c)\) Doubly bordered block-diagonal

Formally, a MILP with a DBBD structure can be written as

\[_{^{n}}&_{1}^{} _{1}+_{2}^{}_{2}++_{k}^{}_{k}+_{k+1}^{}_{k+1},\\ &_{i}_{i}+_{i}_{k+1}_{i},  1 i k,_{i}=$, otherwise DB-Cons)}\\ &_{i=1}^{k}_{i}_{i}+_{k+1}_ {k+1},\\ &,^{p} ^{n-p},\] (3)

where the partition \(=(_{1}^{},,_{k+1}^{})^{}\), \(=(_{1}^{},,_{k+1}^{})^{}\) and \(=(_{1}^{},,_{k+1}^{})^{}\). To process more complex block structures beyond the three basic ones, we specify different types of constraints and variables in a CCM (and the corresponding MILP). First, we classify variables as _block_ and _bordered_ variables (Bl-Vars and Bd-Vars). The block variables DBBD are \(_{}=(_{1}^{},,_{k}^{})^{}\), which are involved in the blocks \(_{i}\), \((1 i k)\). The bordered variables are

Figure 4: An overview of MILP-StuDio. (1) We detect the block structures in the original instances and decompose the CCMs into sub-matrices of block units. (2) The sub-matrices are transferred into the corresponding sub-graphs of instances’ bipartite graph representations. These sub-graphs are used to construct the structure library. (3) We sample instances and sub-graphs of block units and perform block manipulations, including block reduction, mix-up and expansion.

defined to be those in \(_{i}\), \((1 i k)\), i.e. the variables \(_{k+1}\). Notice that all the variables in BD and BBD are block variables. Then, we classify the constraints in an instance as _master_, _block_ and _doubly block constraints_ (M-Cons, B-Cons, and DB-Cons), which we have illustrated in Equation (2). As we can see, BD only contains B-Cons, BBD contains B-Cons and M-Cons, and DBBD contains DB-Cons and M-Cons. The classifications of constraints and variables make it possible for us to investigate more delicate structures in the instances--such as the combination of the three basic ones--in the subsequent process (please see Appendix H.2 and H.4).

### Block Decomposition

Reordering Rows and Columns in CCMsGiven a MILP instance, the raw orders of rows and columns for a CCM are determined by the orders of constraints and variables respectively, which is defined when we establish the instance. Generally, the block structures are not readily apparent in this raw form. To identify and exploit these block structures, we employ a structure detector implemented in the Generic Column Generation (GCG) solver  for CCM reordering. This detector identifies row and column permutations that effectively cluster the nonzero coefficients, thereby revealing distinct block structures within CCMs.

Block DecompositionWe employ an enhanced variable partition algorithm based on the image representations of CCMs for block decomposition, using the constraint-variable classification results mentioned in Section 4.1. Specifically, we extract the sub-matrices of the _block units_\(\) in CCMs, i.e., which take the form of \(_{i}\) in BD, \(_{i}\\ _{i}\) in BBD, and \(_{i}&_{i}\\ _{i}\) in DBBD. Finally, we partition and decompose the CCMs into sub-matrices of block units. The algorithm enables us to handle more complex structures beyond the basic three found by GCG, such as instances in WA with M-Cons, B-Cons and BD-Cons. In the case of WA, the sub-matrices are in the form of \(_{i}^{(1)}&_{i}\\ _{i}^{(2)}\\ \), where \(_{i}^{(1)}\) represents the diagonal block with DB-Cons, and \(_{i}^{(2)}\) represents the diagonal block with B-Cons. Please refer to Section H.3 for the detailed implementation of the decomposition process.

### Construction of Structure Library

As we can see in Figure 0(a), the block units across instances exhibit striking similarities in terms of the internal structures. These common characteristics indicate that the distribution of block units holds valuable information about the problem formulations, making it an ideal building block for reconstructing new instances. Given block-unit sub-matrices of CCMs obtained in Section 4.2, we proceed to extract the corresponding bipartite sub-graphs within the graph representations of the original instances. Compared to the image representation, graph representation offers more convenience for modifying MILP instances during block manipulation. Specifically, suppose that a sub-matrix contains constraints \(}=\{w_{i_{1}},,w_{i_{k}}\}\) and variables \(}=\{v_{i_{1}},,v_{i_{l}}\}\) in the instances, we then extract the sub-graph containing \(}\), \(}\) and the edges connecting \(}\) and \(}\) in the bipartite graph representation of the original instance. Subsequently, we collect these sub-graphs from all the training instances and utilize them to construct a comprehensive structure library denoted as \(\). This structure library serves as a repository for the collected sub-graphs, allowing efficient storage, retrieval, and utilization of the block information.

### Scalable Generation via Block Manipulations

With the structure library, we devise three types of generation operators that enable the generation of high-quality MILP instances with flexible sizes. These operators, namely block reduction, block mix-up, and block expansion, play a crucial role in the instance generation process.

* **Reduction.** This operator involves randomly sampling a block unit \(_{}\) from the original instances and then removing it. The reduction operator generates MILP instances with smaller sizes compared to the original ones, reducing the complexity of the problem.
* **Mix-up.** This operator involves randomly sampling one block unit \(_{}\) from the original instances and another block unit \(\) from the structure library \(\). We then replace \(_{}\) with \(\) to generate a new instance. The mix-up operator introduces structural variations through the incorporation of external block units.

* **Expansion.** This operator involves randomly sampling a block unit \(\) from the structure library \(\) and appending it to the original instances. This process generates new instances of larger sizes compared to the original ones, potentially introducing more complex structures.

To preserve the block structures, the operators should leverage the constraint-variable classification results. Taking the expansion operator as an example, the coefficients of M-Cons in the external block unit should be properly inserted into the M-Cons of the original instances. Meanwhile, we construct new constraints for the B- and DB-Cons of the block using the corresponding right-hand-side terms \(_{i}\). Finally, we design a coefficient refinement algorithm to align the coefficients of the external blocks during mix-up and expansion (please see Appendix H.4 for details).

## 5 Experiments

### Experiment Settings

BenchmarksWe consider four MILP problem benchmarks: combinatorial auctions (CA) , capacitated facility location (FA) , item placement (IP)  and workload appointment (WA) . The first two benchmarks, CA and FA, are commonly-used benchmarks proposed in . The last two benchmarks, IP and WA, come from two challenging real-world problem families used in NeurIPS ML4CO 2021 competition . The numbers of training, validation, and testing instances are 100, 20, and 50. More details on the benchmarks are in Appendix I.2.

MetricsWe leverage three metrics to evaluate the similarity between the original and generated instances. (1) Graph statistics are composed of 11 classical statistics of the bipartite graph . Following , we compute the Jensen-Shannon divergence for each statistic between the generated and original instances. We then standardize the metrics into similarity scores ranging from 0 to 1. (2) Computational hardness is measured by the average solving time of the instances using the Gurobi solver . (3) Feasible ratio is the proportion of feasible instances out of the total ones.

BaselinesWe consider two baselines for MILP generation. The first baseline is the statistics-based MILP generation approach Bowly , which generates MILP instances by controlling specific statistical features, including the coefficient density and coefficient mean. We set these features to match the corresponding statistics of the original instances so as to generate instances with high statistical similarity with the original ones. The second baseline is the learning-based approach G2MILP . By leveraging masked VAE, G2MILP iteratively masks one constraint node in the bipartite graph and replaces it with a generated one.

Downstream TasksWe consider three downstream tasks to demonstrate the effectiveness of the generated instances in practical applications. (1) Improving the performance of learning-based solvers, including predict-and-search (PS)  in Section 5.3 and the GNN approach for learning-to-branch  in Appendix F.1. (2) Hyperparameter tuning for the traditional solver (please see Appendix F.3). In the above two tasks, we use MILP-StuDio and the baselines to generate new instances to enrich the training data. We also consider the task of (3) hard instances generation in Section 5.4, which reflects the ability to construct hard benchmarks.

Variants of MILP-StuDioDuring the generation process, we use the three operators to generate one-third of the instances, respectively. We can choose different modification ratios, which implies that we will remove (substitute or append) block units when performing the reduction (mix-up or expansion) operator until the proportions of variables modified in the instance reaches \(\).

### Similarity between the Generated and the Original Instances

For each generation technique, we use 100 original instances to generate 1,000 instances and evaluate the similarity between them. As shown in Table 2, we present the graph structural distributional similarity scores between the original and generated instances. We do not consider Bowly in WA since the instance sizes in WA are so large that the generation time is over 200 hours. The results suggest that MILP-StuDio shows a high graph structural similarity compared to the baselines. In FA, instances generated from G2MILP present a low similarity, while our proposed MILP-StuDio can still achieve a high similarity score. As the modification ratio increases, the similarity scores of G2MILP and MILP-StuDio decrease, whereas G2MILP suffers from a more severe degradation.

We also evaluate the computational hardness and feasibility of the generated instances in Table 3. We report the average solving time and feasibility ratio for each dataset. Results demonstrate that the instances generated by the baselines represent a severe degradation in computational hardness.

Moreover, most of the instances generated by G2MILP in FA and WA datasets are infeasible. Encouragingly, MILP-StuDio is able to preserve the computational hardness and feasibility of the original instances, making it a strong method for maintaining instances' mathematical properties.

We visualize the CCMs of the original and generated instances to demonstrate the effectiveness of MILP-StuDio in preserving block structures in Appendix G. MILP-StuDio is able to preserve the block structures while all the baselines fail to do so.

### Improving the Performance of Learning-based Solvers

GNN branching policy  and predict-and-search  are two representative approaches of learning-based solvers. Here we report the results of PS built on Gurobi and leave the other in Appendix F.1. In alignment with , we consider two additional baselines Gurobi and BKS. For each instance, we run Gurobi on a single-thread mode for 1,000 seconds. We also run Gurobi  for 3,600 seconds and denote the obtained objective values as the best-known solution (BKS). For more details on the implementation of PS, please refer to Appendix H.1. Table 4 demonstrates the performance of MILP-StuDio-enhanced PS and the baselines, where we set the modification ratio \(=0.05\). We report three metrics to measure the solving performance. (1) Obj represents the objective values achieved by different methods. (2) \(_{}\) is the absolute primal gap defined as \(_{}:=|-|\), where smaller gaps indicate superior primal solutions and, consequently, a better performance. (3) Time denotes the average time used to find the solutions.

In CA and FA, all the approaches can solve the instances to the optimal with \(_{}=0\), thus we compare the Time metric. In IP and WA, all the approaches reach the time limit of 1,000s, thus we mainly focus on the \(_{}\) metric. Methods built on PS does not perform well in CA compared to Gurobi, since CA is easy for Gurobi to solve and PS needs to spend time for network inference. Even so, PS+MILP-StuDio achieves a comparable performance to Gurobi. In the other three datasets, MILP-StuDio-enhanced PS outperforms other baselines, achieving the best solving time or \(_{}\).

### Hard Benchmark Generation

The hard instance generation task is important as it provides valuable resources for evaluating solvers and thus potentially motivates more efficient algorithms. The objective of this experiment is to test the ability to generate harder instances within a given number of iterations. We use 30 original instances to construct the pool. In each iteration, for each instance in the pool, we employ mix-up or expansion operators to generate two new ones, and We then select the instance among and with the longest solving time to replace in the pool. This setting is to preserve the diversity of the pool. We observe that there exist slight differences in the hardness of the original instances, and the generated instances

    &  &  &  &  \\   & & G2MILP & MILP-StuDio & G2MILP & MILP-StuDio & G2MILP & MILP-StuDio \\  CA & 0.567 & 0.997 & 0.997 & 0.995 & 0.981 & 0.990 & 0.946 \\ FA & 0.077 & 0.358 & 0.663 & 0.092 & 0.646 & 0.091 & 0.618 \\ IP & 0.484 & 0.717 & 0.661 & 0.352 & 0.528 & 0.336 & 0.493 \\ WA & Timeout & 0.854 & 0.980 & 0.484 & 0.853 & 0.249 & 0.783 \\   

Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. _Timeout_ implies the generation time is over 200h.

    &  & FA & IP & WA \\  Original Instances & 0.50 (100.0\%) & 4.78 (100.0\%) & 1000 (100.0\%) & 1000 (100.0\%) \\  Bowl & 0.02 (100.0\%) & 0.07 (100.0\%) & 56.45 (100.0\%) & Timeout \\  G2MILP \(=0.01\) & 0.58 (100.0\%) & 0.02 (1.7\%) & 802.3 (100.0\%) & 12.01 (10.0\%) \\ MILP-StuDio \(=0.01\) & **0.50 (100.0\%)** & **4.94 (100.0\%)** & **1000 (100.0\%)** & **1000 (100.0\%)** \\  G2MILP \(=0.05\) & 0.69 (100.0\%) & 0.01 (1.8\%) & 0.14 (100.0\%) & 0.01 (0.0\%) \\ MILP-StuDio \(=0.05\) & **0.48 (100.0\%)** & **4.92 (100.0\%)** & **691.66 (100.0\%)** & **1000 (100.0\%)** \\  G2MILP \(=0.10\) & 0.72 (100.0\%) & 0.01 (2.0\%) & 0.03 (100.0\%) & 0.02 (0.0\%) \\ MILP-StuDio \(=0.10\) & **0.40 (100.0\%)** & **4.64 (100.0\%)** & **550.33 (100.0\%)** & **1000 (94.5\%)** \\   

Table 3: Average solving time (s) and feasible ratio (in parentheses) of the instances. We set the solving time limit to be 1,000s. We mark the **values closest to those in original instances** in bold.

derived from the harder original instances are also harder than those from easier ones. If we had simply generated 60 instances and selected the hardest 30, the proportion of instances generated from the hard original instances would have continuously increased, reducing the diversity of the pool. In Figure 5, we depict the growth curve of the average solving time of instances in the pool during 10 iterations. The solving time of the final set is two times larger than that of the initial set, suggesting MILP-StuDio's ability to generate increasingly harder instances during iterations.

### Extensive Studies

Generation EfficiencyWA is a challenging benchmark with large instance sizes. The instances in WA have over 60,000 constraints and variables, making generation in WA especially time-consuming. We compare the generation time of the generation techniques to generate 1,000 instances. We set the time limit to 200 hours. The results in Table 6 show that MILP-StuDio significantly archives \(3\) acceleration compared to G2MILP, demonstrating high generation efficiency.

More ResultsWe conduct ablation studies on three operators and modification ratios in Appendix F.2, and we also try to extend MILP-StuDio to MILPs without block structures in Appendix F.4 and MILPs in the real-world industrial dataset in Appendix F.5.

## 6 Related Work on MILP Generation

The field of MILP generation encompasses two main categories: problem-specific generation and general MILP generation. Problem-specific generation methods rely on expert knowledge of the mathematical formulation of problems to generate instances. Examples include set covering , combinatorial auctions , and satisfiability . While these methods can generate instances tailored to specific problem types, they are limited in their applicability and require much modeling expert knowledge. On the other hand, general MILP generation techniques aim to generate MILPs using statistical information  or by leveraging neural networks to capture instance distributions . G2MILP  is the first learning-based generation framework designed for generating general MILPs. This approach represents MILPs as bipartite graphs and utilizes a masked variational auto-encoder  to iteratively corrupt and replace parts of the original graphs to generate new ones.

## 7 Limitations and Future Avenue

Our method originates from the field of operational research and is designed for instances with block structures. The performance of the detector in GCG influences the overall generation quality. Although the detector can identify a wide range of useful structures in the real world, it is still limited when facing instances with extremely complex structures. The exploration of enhancing the performance of the detector, such as those involving prior knowledge of the instances, represents a promising avenue for future research.

## 8 Conclusion

In this paper, we propose a novel MILP generation framework (MILP-StuDio) to generate high-quality MILP instances. Inspired by the studies of CCMs in operational research, MILP-StuDio manipulates the block structures in CCMs to preserve the mathematical properties--including the computational hardness and feasibility--of the instances. Furthermore, MILP-StuDio has a strong ability of scalable generation and high generation efficiency. Experiments demonstrate the effectiveness of MILP-StuDio in improving the performance of learning-based solvers.

## 9 Acknowledgement

The authors would like to thank all the anonymous reviewers for their insightful comments and valuable suggestions. This work was supported by the National Key R&D Program of China under contract 2022ZD0119801 and the National Nature Science Foundations of China grants U23A20388 and 62021001.