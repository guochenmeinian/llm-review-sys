# Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms

Qian Yu

Princeton University

qy4628@princeton.edu

&Yining Wang

University of Texas at Dallas

yining.wang@utdallas.edu

Baihe Huang

University of California, Berkeley

baihe_huang@berkeley.edu

&Qi Lei

New York University

q1518@nyu.edu

&Jason D. Lee

Princeton University

Jasondl@princeton.edu

###### Abstract

In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called _energy allocation_, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributions, which are enabled by a truncation method.

## 1 Introduction

Stochastic optimization in the zeroth order (gradient-free) setting has attracted significant attention in recent decades. It naturally arises in various applications such as autonomous driving , AI gaming , robotics , healthcare , and education . This setting is particularly important when the gradient of the objective function cannot be directly evaluated or is expensive.

An important case of interest in the zeroth order setting is the bandit optimization of smooth and strongly-convex functions. Although there are ample results regarding the minimax rates of this problem , little is known about how its complexity depends on the geometry of the objective function \(f\) near the global optimum \(x^{*}\), as specified by the quadratic approximation \((x-x^{*})^{}^{2}f(x^{*})(x-x^{*})\). As an initial step, we investigate the following natural questions:

* For zeroth-order bandit optimization problems of quadratic functions of the form \((x-x_{0})^{}A(x-x_{0})\), what is the optimal instance-dependent upper bound with respect to \(A\)?
* Is there an algorithm that universally achieves the optimal instance-dependent bounds for all quadratic functions, but without the knowledge of Hessian?

As our main contributions, we fully addressed the above questions as follows. First, we established the tight Hessian-dependent upper and lower bounds of the simple regret. Our bounds indicate asymptotic sample complexity bounds of \(^{2}(A^{-})/(2)\) to achieve \(\) accuracy. This covers theminimax lower bound of \((d^{2}/)\) in . Second, we prove the existence of a Hessian-independent algorithm with a matching upper bound of \(O(^{2}(A^{-})/)\). Thus, we complete the theory of zeroth-order bandit optimization on quadratic functions.

Related works beyond linear or convex bandit optimization.Compared to their linear/convex counterparts, much less is known for finding optimal bandit under nonlinear/non-convex reward function. A natural next step beyond linear bandits is to look at quadratic reward functions, as studied in this paper and some prior work (see reference on bandit PCA [15; 4], its rank-1 special cases [18; 11], bilinear , low-rank linear  and some other settings [9; 5; 16]).

When getting beyond quadratic losses, prior work on non-convex settings mainly focuses on finding achieving \(\)-stationary points instead of \(\)-optimal reward , except for certain specific settings (see, e.g., [7; 25; 26; 20; 26; 20]).

## 2 Problem Formulation and Main Results

We first present a rigorous formulation for the stochastic zeroth-order optimization problem studied in this paper. Given a fixed dimension parameter \(d\), let \(f:\) be an unknown objective function defined on the closed unit \(L_{2}\)-ball \(\) of the Euclidean space \(^{d}\), which takes the following form for some positive-semidefinite (PSD) matrix \(A\).

\[f()=(-_{0})^{}A(-_{0}).\] (1)

For each time \(t[T]\), an optimization algorithm \(\) produces a query point \(_{t}\), and receives

\[y_{t}=f(_{t})+w_{t}.\] (2)

The algorithm can be _adaptive_, so that \(\) is described by a sequence of conditional distributions for choosing each \(_{t}\) based on all prior observations \(\{_{},y_{}\}_{<t}\). Then we only assume that the noises \(\{w_{t}\}_{t=1}^{T-1}\) are independent random variables with zero mean and unit variance, i.e., \([w_{t}|_{t}]=0\) and \([w_{t}^{2}|_{t}] 1\).

For brevity, we use \((A)\) to denote all functions \(f\) satisfying equation (1) for some \(_{0}\). We are interested in the following minimax _simple_ regret for any PSD matrix \(A\).

\[(T;A):=_{}_{f(A)}[ f(_{T})].\] (3)

The above quantity characterizes the simple regrets achievable by algorithms with perfect Hessian information. We also aim to identify the existence of algorithms that universally achieves the minimax regret for all \(A\), without having access to that knowledge.

Our first result provides a tight characterization for the asymptotics of the minimax regret.

**Theorem 2.1**.: _For any PSD matrix \(A\), we have_

\[_{T}(T;A) T( (A^{-}))^{2},\] (4)

_where \(A^{-}\) denotes the pseudo inverse of \(A^{}\). Moreover, if the distributions of \(w_{t}\) are i.i.d. standard Gaussian, the above bound provides a tight characterization, i.e., there is a matching lower bound that the following equality is implied._

\[_{T}(T;A) T=( (A^{-}))^{2}.\]

We prove Theorem 2.1 in Section 3. More generally, we also provide a full characterization of \((T;A)\) for the non-asymptotic regime, stated as follows and proved in Appendix C.

**Theorem 2.2**.: _For any PSD matrix \(A\) and \(T>3\)\(A\), where \(A\) denotes the rank of \(A\), we have_

\[(T;A)=(^{k^{*}} _{k}^{-})^{2}}{T}+_{k^{*}+1})k^{*}<A\\ ((A^{-}))^{2}}{T}) \]_where \(_{j}\) is the \(j\)th smallest eigenvalue of \(A\) and \(k^{*}\) is the largest integer in \(\{0,1,...,A\}\) satisfying \(T(_{k=1}^{k^{*}}_{k}^{-})(_{k=1}^{ k^{*}}_{k}^{-})\)._

**Remark 2.3**.: _While our formulation requires the algorithm to return estimates from the bounded domain, i.e., \(_{T}\), our lower bound applies to algorithms without this constraint as well. This can be proved by showing that the worst-case simple regret over all \(f(A)\) is always achievable by estimators satisfying \(_{T}\). Their construction can be obtained using the projection step in Algorithm 1, and proof details can be found in Appendix A._

Finally, we provide a positive answer to the existence of universally optimal algorithms, stated in the following Theorem. We present the algorithm and prove its achievability guarantee in Section 4.

**Theorem 2.4**.: _There exists an algorithm \(\), which does not depend on the Hessian parameter \(A\), such that for \(A\) being any PSD matrix, the achieved minimax simple regret satisfies_

\[_{T}\;_{f(A)}[f( {x}_{T})] T=O(((A^{-}))^{2}).\]

## 3 Proof of Theorem 2.1

To motivate the main construction ideas, we start by proving a weaker version of the lower bound in Theorem 2.1. We use the provided intuition to construct a matching upper bound. Then we complete the proof by strengthening the lower bound through a Bayes analysis and the uncertainty principle.

### Proof of a Weakened Lower Bound

Assuming \(A\) being positive definite, we prove the following inequality when the additive noises \(w_{t}\)'s are i.i.d. standard Gaussian.

\[_{T}(T;A) T(((A^{-}))^{2}).\] (5)

Throughout this section, we fix any basis such that \(A=(_{1},...,_{d})\). We construct a class of hard instances by letting

\[_{0}_{}\{(x_{1},x_{2},...,x_{d}) \;|\;x_{k}=^{-}(_{j=1}^{d }_{j}^{-})}{2T}}, k[d]\}.\]

We investigate a Bayes setting where the objective function is defined by equation (1) and \(_{0}\) is uniformly random on the above set. For convenience, let \(_{t}=(x_{1,t},...,x_{d,t})\) be the action of the algorithm at time \(t\). We define the _energy spectrum_ to be a vector \(=(R_{1},...,R_{d})\) with each entry given by

\[R_{k}=[_{t=1}^{T}x_{k,t}^{2}].\]

Intuitively, our proof is to show that an allocation of at least \(R_{k}(_{k}^{-2}x_{k}^{-2})\) energy is required to correctly estimate each \(x_{k}\) with \((1)\) probability. Note that for any entry that is incorrectly estimated, a penalty of \((_{k}x_{k}^{2})\) is applied to the simple regret. This penalty is proportional to the required energy, which is due to the design of each \(x_{k}\). Meanwhile, the total expected energy is upper bounded by \(T\), which is no greater than the summation of the individual requirements. Therefore, an \((1)\) fraction of the penalty is guaranteed, resulting in an overall effect of \((_{k}_{k}x_{k}^{2})=(((A^{- }))^{2}/T)\) on the simple regret.

Rigorously, consider any fixed algorithm, we define the estimation cost of each entry as follows.

\[E_{k}=[_{k}(x_{k,T}-x_{k})^{2}].\]For brevity, let \(s_{k}=(x_{k})\). The above function is minimized by the minimum mean square error (MMSE) estimator (e.g., see [2, Sec. 4.6]), which depends on the following log-likelihood ratio (LLR) function.

\[L_{k}[s_{k}=1|\{_{},y_{}\}_{ <T}]}{[s_{k}=-1|\{_{},y_{}\}_{<T} ]}.\]

Specifically, the MMSE estimator is given by \(_{k,T}|x_{k}|}{2}\), and the resulting error conditioned on any fixed \(\{_{},y_{}\}_{<T}\) has an expectation of \(x_{k}^{2}^{2}}{2}\). Hence, we have the following lower bound for each cost entry.

\[E_{k}[_{k}(_{k,T}-x_{k})^{2} ]=_{k}x_{k}^{2}[^{2} }{2}].\] (6)

By the Gaussian noise assumption, the conditional expectation of LLR can be written as follows.

\[[L_{k}|s_{k}] =[_{t}--_{ k}(x_{k,t}-x_{k})^{2})^{2}}{2}+-_{k}(x_{k,t }+x_{k})^{2})^{2}}{2}s_{k}]\] \[=2s_{k}(_{k}x_{k})^{2}[_{t} x_{k,t}^{2}s_{k}].\]

The above quantity equals the KL divergence between the distributions of action-reward sequences generated by \(s_{k}= 1\). Clearly, it has the following connection to the energy spectrum.

\[[L_{k}s_{k}]=2(_{k}x_{k})^{2}[_{t}x_{k,t}^{2}]=2(_{k}x_{k})^{2}R_{k}.\] (7)

Recall inequality (6), it also implies the following lower bound of the mean-squared error.

\[[L_{k}s_{k}]=[L_{k}[s_{k} |L_{k}]]=[L_{k}}{2}]  2-2[^{2}}{2}] 2-}{ _{k}x_{k}^{2}}.\] (8)

Combine inequality (7) and (8), the overall simple regret can be bounded as follows.

\[[f(_{T})]=_{k}E_{k}_{k} (1-_{k}^{2}x_{k}^{2}R_{k})_{k}x_{k}^{2}.\] (9)

Note that the definition of \(_{H}\) implies that the value of each \(x_{k}^{2}\) is fixed. Hence,

\[[f(_{T})]_{j}^{- })^{2}}{4T}-_{j}^{-} )^{2}}{8T^{2}}_{k}R_{k}_{j}^{-})^{2}}{8T}=(A^{-}))^{2}}{8T},\]

where the last inequality uses the fact that the total energy is upper bounded by the number of samples.

Finally, when \(T\) is sufficiently large, we have \(||_{0}||_{2} 1\) almost surely, which implies that our hard-instance functions belong to the set of objective functions \((A)\). Therefore, \([f(_{T})]\) provides a lower bound of the asymptotic minimax regret, and we can conclude that

\[_{T}(T;A) T((A^ {-}))^{2}.\]

**Remark 3.1**.: _The validity of the hard-instance functions requires \(T=((_{k}_{k}^{-})(_{k} _{k}^{-}))\), which is consistent with the transition threshold in Theorem 2.2 to the non-asymptotic regimes._

### Proof of the Upper Bound

Now we provide a proof for equation (4), which is implied by the following result.

**Proposition 3.2**.: _For any PSD matrix \(A\), let \(k^{*}\) be the rank, \(_{1},...,_{k^{*}}\) be the non-zero eigenvalues, and \(_{1},_{2},...,_{k^{*}}\) be the associated orthonormal eigenvectors. Then the expected simple regret achieved by algorithm 1 satisfies_

\[_{T}_{f(A)}[f(_{T})] T (A^{-})^{2}.\] (10)Proof.: Consider an eigenbasis of \(A\) with \(_{1},...,_{k^{*}}\) being the first \(k^{*}\) vectors. For each \(k[k^{*}]\), the algorithm allocates \(R_{k}\) energy to estimate the \(k\)th entry of \(_{0}\). The value of \(R_{k}\) is chosen such that the hard instances in the earlier subsection maximizes (modulo a constant factor) the lower bound in inequality (9) (i.e., \(R_{k}\) satisfies \(x_{k}^{2} 1/(_{k}^{2}R_{k})\)) while ensuring the total number of samples does not exceed \(T-1\). By the zero-mean and unit-variance assumptions of the noise distribution, we have

\[[(_{k}-_{0}_{k})^{2}]=^{2}t_{k}}^{-}_{j=1}^{k^{* }}_{j}^{-}}{T-2d-1}.\]

This essentially provides an unbounded estimator \(}_{k=1}^{k^{*}}_{k}_{k}\) that satisfies

\[_{T}_{f(A)}[f( })] T =_{T}_{_{0}} _{k=1}^{k^{*}}_{k}[(_{k}-_{0}_{k}) ^{2}] T\] \[^{k^{*}}_{j}^{-} )^{2}}{2}_{T}\] \[=(A^{-})^{2}.\] (11)

Then, to obtain \(_{T}\) within the bounded constraint set, \(}\) is projected to \(\) under a pseudometric defined by matrix \(A\). Due to the convexity of set \(\), we have \(f(}) f(_{T})\) with probability 1 (see Appendix A for a proof). Therefore, inequality (10) is implied by inequality (11). 

### Proof of the Lower Bound with Tight Constant Factors

To complete the proof of Theorem 2.1, it remains to show that for standard Gaussian noise, we have

\[_{T}(T;A) T (A^{-})^{2}.\] (12)

Notice that the object function and reward feedbacks depend only on the projections of \(_{0}\) and query points onto the column (or row) space of \(A\). Hence, it suffices to focus on algorithms with actions that are constrained on this subspace. This reduces the original problem to an equivalent instance that is defined on a (possibly) lower dimensional space and by a non-singular \(A\). Therefore, we only need to prove inequality (12) for those reduced cases (i.e., when \(A\) is full-rank).

We lower bound the minimax regret by comparing them with a Bayes estimation error, where \(_{0}\) has a prior distribution that violates the bounded-norm constraint. Formally, for any fixed algorithm \(\), we can consider its expected simple regret \([f(_{T})]\) over an extended class of objective functions where \(f\) is defined by equation (1) but \(_{0}\) is chosen from the entire Euclidian space. Then for any distribution of \(_{0}\), the overall expectation is upper bounded by the following inequality.

\[_{_{0}}[[f(_{T})]][ _{0}]_{f(A)}[f(_{T} )]+[(_{0})_{ }f()],\] (13)where the first term on the RHS above is obtained by taking the supremum over all objective functions that satisfies \(_{0}\), then the second term is obtained from the adversarial choice over all estimators for \(_{0}\). Compare the RHS of inequality (13) with equation (3), a lower bound of the minimax simple regret \((T;A)\) can be obtained by taking the infimum over algorithm \(\) on both sides. Hence, it remains to characterize the optimal Bayes estimation error on the LHS.

To provide a concrete analysis, let \(_{0}\) be a Gaussian vector with zero mean and a covariance of \(T^{-} I_{d}\), where \(I_{d}\) denotes the identity matrix.1 Under this setting, conditioned on any realization of queries \(_{1},...,_{T-1}\) and feedbacks \(y_{1},...,y_{T-1}\), the posterior distribution of \(_{0}\) is proportional to

\[(-_{0}||_{2}^{2} T^{}}{2}-_{t=1}^{ T-1}-(_{t}-_{0})^{}A(_{t}- _{0}))^{2}}{2}).\]

Clearly, the Bayesian error can be lower bounded by the expectation of the conditional covariance, which is further characterized by the following principle (see Appendix B for a proof).

**Proposition 3.3** (Uncertainty Principle).: _Let \(\) be a random variable on any measurable space and \(\) be a real-valued random variable dependent of \(\). If the conditional distribution of \(\) given \(\) has a density function \(f_{}()\), and \( f_{}()\) has a second derivative that is integrable over \(f_{}\), then_

\[[[|]][-}{^{2}} f_{}()]}.\]

Hence, by taking the second derivative, the squared estimation error for the \(k\)th entry of \(_{0}\) is lower bounded by the inverse of the following expectation.

\[[T^{}+(_{t=1}^{T-1}A(_{t }-_{0})(_{t}-_{0})^{}A^{}-Aw_{t})_{ kk}]\] \[=T^{}+[(_{t=1}^{T-1}A( _{t}-_{0})(_{t}-_{0})^{}A^{})_ {kk}],\]

where \(()_{kk}\) denotes the \(k\)th diagonal entry of the given matrix. Recall that we chose a basis where \(A\) is diagonal. The overall Bayes error is bounded as follows

\[_{_{0}}[[f(_{T})]] _{k}}{2}/(T^{}+_{k}^{2} [(_{t=1}^{T-1}(_{t}-_{0})(_{t}-_{0})^{ })_{kk}]),\]

where \(_{k}=(A)_{kk}\) is the \(k\)th eigenvalue of \(A\). By Cauchy's inequality, the RHS above can be further bounded by

\[(_{k}_{k}^{-})^{2}}{_{k }(T^{}_{k}^{-2}+[(_{t=1}^{T-1}( _{t}-_{0})(_{t}-_{0})^{})_{kk}] )}=((A^{-}))^{2}}{T^{ }(A^{-2})+_{t=1}^{T-1}[||_{t }-_{0}||_{2}^{2}]]}.\]

Note that by triangle inequality

\[[||_{t}-_{0}||_{2}^{2}][ (1+||_{0}||_{2})^{2}](1+[||_{0}||_{2}^{2}]^{})^{2}=(1+d^{}T^{- {1}{3}})^{2},\]

where \(d\) is the dimension of the action space. We have obtained a lower bound of \(_{_{0}}[[f(_{T})]]\) that is independent of the algorithm. Formally,

\[_{}_{_{0}}[[f(_{T}) ]]((A^{-}))^{2}\! \!(T(1+d^{}T^{-})^{2}+T^{}(A^{-2}))\!.\]

We apply the above estimate to inequality (13). By taking the infimum over all algorithms,

\[_{T}(T;A) T_{T}}_{_{0}}[[f(_{T})]]- [(_{0})_{ }f()]}{[_{0}]} T\]Observe that for our Gaussian prior,

\[_{T}[_{0}]=1,\] \[_{T}[(_{0} )_{}f()] T=0.\]

Hence, all terms above can be replace by closed-form functions. and we have

\[_{T}(T;A) T _{T}_{}_{_{0 }}[[f(_{T})]] T\] \[_{T}((A^{-}))^{2}1+d^{}T^{-}^{2 }+T^{-}(A^{-2})\] \[=(A^{-})^{2}.\]

## 4 Proof of Theorem 2.4

To prove the universal achievability result, we need to develop a new algorithm to learn and incorporate the needed Hessian information. Particularly, the procedure required for achieving the optimal rates is beyond simply adding an initial stage with an arbitrary Hessian estimator, for there are two challenges. First, any Hessian estimation algorithm would result in a mean squared error of \((1/T)\) in the worst case, which translates to a cost of \((1/T)\) in the minimax simple regret. This induced cost often introduces a multiplicative factor that is order-wise larger than the desired \(O(((A^{-}))^{2})\). Second, to utilize any estimated Hessian, the analysis for the subsequent stages often requires the estimation error to have a light-tail distribution, which is not guaranteed for general linear estimators when the additive noise in the observation model has a heavy-tail distribution.

To overcome these challenges, we present two main algorithmic building blocks in the following subsections. The first uses \(o(T)\) samples to obtain a rough estimate of the Hessian, and achieves low-error guarantees with high probability through the introduction of a truncation method. The second estimates the global minimum with carefully designed sample points to minimize the error contributed from the Hessian estimation. We show that this sample phase can be written in the form of a two-step descent. Finally, we show the combination of two stages provides an algorithm that proves Theorem 2.4.

### Initial Hessian Estimate and Truncation Method

Consider any sufficiently large \(T\), we first use \(T_{0}= T^{0.8}\) samples to find a rough estimate of \(A\). Particularly, we rely on the following result.

**Proposition 4.1**.: _For any fixed dimension \(d\), there is an algorithm that samples on \(T_{0}\) predetermined points and returns an estimation \(\), such that for any fixed \((0,0.5)\), \((0,+)\), and PSD matrix \(A\),_

\[_{T_{0}}_{f_{A}}[|| -A||_{} T_{0}^{-}] T_{0}^{}=0,\] (14)

_where \(||||_{}\) denotes the Frobenius norm._

Proof.: We first consider the 1D case. Let \(y_{+}\), \(y_{-}\), and \(y\) each be samples of \(f\) at \(1\), \(-1\), and \(0\), respectively. When the additive noises are subgaussian, we have that \((y_{+}+y_{-}-2y)\) is an unbiased estimator of \(A\) with an error that is subgaussian. By repeating and averaging over this process \( T_{0}/3\) times, the squared estimation error is reduced to \(O(1/T_{0})\), and the normalized error is subgaussian, which satisfies the statement in the proposition.

However, recall that in Section 2 we did not assume the subgaussianity of \(w_{t}\). A modification of the above estimator is required to achieve the same guanrantee in equation (14). Here we propose a truncation method, which projects each measurement \((y_{+}+y_{-}-2y)\) to a bounded interval \([-T_{0}^{0.5},T_{0}^{0.5}]\). Specifically, the returned \(\) is the average of \( T_{0}/3\) samples of \(\{\{y_{+}+y_{-}-2y,T_{0}^{0.5}\},-T_{0}^{0.5}\}\), which provides a guaranteed superpolynomial tail bound. A more detailed discussion and analysis for the truncation method is provided in Appendix D.1.

For general \(d\), one can return an estimator that satisfies the same light-tail requirement, for example, by applying the above 1D estimator repetitively \((d)\) times to obtain estimates of all entries of \(A\). Then the overall error probability is controlled by the union bound. 

### Two-Step Descent Stage

Let \(\) be any estimator that satisfies the condition in Proposition 4.1 for \(=0.4\) and \(=1.6\). Asymptotically, this implies the eigenvalues and eigenvectors of \(\) are close to that of \(A\). We choose an eigenbasis of \(\) and remove vectors with vanishingly small eigenvalues to approximate the row space of \(A\). Then, we estimate the entries of \(_{0}\) in these remaining directions following an energy allocation defined based on \(\).

Specifically, consider any fixed realization of \(\), let \(_{1},_{2},...\) be its eigenvalues in the non-increasing order, \(}_{1},}_{2},...\) be the corresponding eigenvectors, and \(k^{*}\) be the largest integer such that \(_{k^{*}} T^{-0.2}\). We present the detailed steps in Algorithm 2, where \(T_{1}\) denotes the remaining available samples, i.e., \(T-T_{0}\).

``` procedureQuadraticSearch(\(_{1},_{2},...,_{k^{*}}\), \(}_{1},...,}_{k^{*}}\), \(T_{1}\)) for\(k 1\) to \(k^{*}\)do  Let \(p_{k}=_{k}^{*}-}{4_{j=1}^{k^{*}} _{j}^{-}}\), \(t_{k}= p_{k}(T_{1}-4d-1)\).  Let \(_{k}=-}\)Truncated Diff\((}_{k},-}_{k},t_{k})\). endfor  Let \(}=_{k=1}^{k^{*}}_{k}}_{k}\), \(}=}\{1,1.5/|| }||_{2}\}\) Projecting to a \(\)-centered hyperball for\(k 1\) to \(k^{*}\)do  Let \(_{k}=-}\)Truncated Diff\((}_{k}+2}}{4},}_{k}+2 }}{4},t_{k})\). \(\) Obtain an unbounded estimator endfor return\(_{T}=_{=_{k=1}^{k^{*}}_{k}}_{k} }_{k=1}^{k^{*}}_{k}(_{k}-_ {k})^{2}\) Projection to \(\) endprocedureprocedureTruncatedDiff(\(_{0}\), \(_{1}\), \(t\)) for\(k 1\) to \(t\)do  Let \(y_{+},y_{-}\) be a sample of \(f\) at \(_{0}\), \(_{1}\), respectively  Compute the projection of the difference \(y_{+}-y_{-}\) to the interval \([-t^{0.5},t^{0.5}]\), i.e., let \(z_{k}=\{\{y_{+}-y_{-},t^{0.5}\},-t^{0.5}\}\) endfor return\(_{k=1}^{t}z_{k}\) endprocedure ```

**Algorithm 2**

We use the eigenbasis of \(\) and let \(_{0}\) be the diagonal matrix given by \((_{1},_{2},...,_{k^{*}},0,...,0)\). In the first for-loop, we essentially estimated \(A_{0}\), and then computed the first \(k^{*}\) entries of its product with the pseudo-inverse of \(_{0}\) (note that the rest of the entries are all zero). Since \(_{0}\) is a good estimate of \(A\) with high probability, we use it to compute the optimal energy spectrum similar to the instance-dependent case, and allocate the measurements accordingly.

Recall the proof in Section 3. By the analysis of the truncation method in Appendix D.1, the variable \(}\) could have served as an estimator of \(_{0}\) if \(_{0}=A\), where the estimation process reduces to the Hessian-dependent case. However, \(_{0}\) relies only on \(O(T^{0.8})\) samples, which generally leads to an expected penalty of \((T^{-0.8})=(1/T)\) in simple regret if used in place of \(A\). We reserve a fraction of samples for a second for-loop to fine-tune the estimation in order to achieve the optimal Hessian-dependent simple regrets.

### Regret Analysis

Recall our assumption on the Hessian Estimator and \(T_{0}=O(T^{0.8})\). The probability for \(||-A||_{} T^{-0.32}\) is \(o(1/T)\). As our algorithm always returns \(_{T}\) with bounded norms, the simple regret contributed by this exceptional case is negligible. Hence, we can focus on instances where \(||-A||_{} T^{-0.32}\).

Under such scenario, any non-zero \(_{k}\) is close to a non-zero eigenvalue of \(A\). Specifically, we have \(|_{k}-_{k}| T^{-0.32}\) for all \(k\), where each \(_{k}\) is the \(k\)th largest eigenvalue of \(A\). Recall the definition of \(_{0}\). It implies that for sufficiently large \(T\), \(k^{*}\) equals the rank of \(A\), and all non-zero eigenvalues of \(_{0}\) are bounded away from zero. Consequently, \(||_{0}^{-1}||_{}=\|A^{-1}\|_{}(1+o(1))\), which does not scale w.r.t. \(T\), where \(M^{-1}\) denotes the pseudo-inverse for any symmetric \(M\). We also have \(((_{0}^{-}))^{2}=O(((A^{- }))^{2}).\)

The closeness between \(\) and \(A\) also implies information on the eigenvectors of \(_{0}\). Recall that \(_{0}\) is obtained by removing the diagonal entries of \(\) (in their eigenbasis) that are below a threshold \(T^{-0.2}\). For sufficiently large \(T\), the removed entries are associated with the \(d-k^{*}\) smallest eigenvalues, which are no greater than \(T^{-0.32}\). Hence, as a rough estimate, we have \(||_{0}-A||_{}=o(T^{-0.3})\).

These conditions can be used to characterize the distribution of \(}\). As mentioned earlier, from the analysis of the truncation method, each \(_{k}\) concentrates around \(}_{k}_{0}^{-1}A_{0}\). Hence, \(}\) concentrates near \(_{0}^{-1}A_{0}\), and the truncation method ensures that

\[[(}-_{0}^{-1}A _{0})^{}_{0}(}- {A}_{0}^{-1}A_{0})]=O(((_{0}^{- }))^{2})/T=O(((A^{-}))^{2})/T,\] (15) \[[||}-_{0}^{- 1}A_{0}||_{2} T^{-0.4}]=o(1/T).\] (16)

Note that the \(L_{2}\)-norm of \(_{0}^{-1}A_{0}\) is no greater than \(1+o(1)\) under the condition of \(||_{0}-A||_{}=o(T^{-0.3})\). Formally, by triangle inequality

\[||_{0}^{-1}A_{0}||_{2}||_{0}^{-1} _{0}_{0}||_{2}+||_{0}^{-1}(_{0}-A)_{0}||_{2}  1+||_{0}^{-1}||_{}||_{0}-A||_{ }=1+o(1).\]

We can apply inequality (16) to show that the \(L_{2}\)-norm of \(}\) is no greater than \(1+o(1)\) with \(1-o(1/T)\) probability. Under such high probability cases, the projection of \(}\) to the hyperball of radius \(1.5\) remains identical. Hence, by the PSD property of \(_{0}\), which is due to the convergence of its eigenvalues, we can replace all \(}\) in inequality (15) with \(}\) and obtain that

\[[(}-_{0}^{-1}A_{0})^ {}_{0}(}-_{0}^{-1}A_ {0})]=O(((A^{-}))^{2})/T.\] (17)

The same analysis can also be performed for each \(_{k}\), which concentrates near \(}_{k}_{0}^{-1}A(2_{0}-)\). Formally, let \(}_{T}_{k}_{k}}_{k}\), we have

\[[(}_{T}-_{0}^{-1}A(2_{0}- ))^{}_{0}(}_{T}- _{0}^{-1}A(2_{0}-))]=O(((A^{-}))^{2})/T.\] (18)

The above results for the two descent steps can be combined, using triangle inequality and Proposition 4.2 below, to obtain the following inequality (See Appendix D for their proofs).

\[[(}_{T}-)^{}_{0}(}_{T}-)]=O(((A^{- }))^{2})/T.\] (19)

where we denote \((2_{0}^{-1}A-(_{0}^{-1}A )^{2})_{0}\) for brevity.

**Proposition 4.2**.: _Let \(_{0},Z,\) be variables dependent on a parameter \(T\), where \(_{0},Z\) are PSD matrices and \(\) belongs to the column space of \(_{0}\). If \(_{T}||_{0}^{-1}||_{}<\) and \(_{T}||Z-_{0}||_{}=0\), then_

\[^{}Z(1+||Z-_{0}||_{}|| _{0}^{-1}||_{})(^{}_{0})=(1+o(1))( {y}^{}_{0}).\]

[MISSING_PAGE_FAIL:10]