# Learning Low-Rank Feature for Thorax Disease Classification

Yancheng Wang\({}^{1}\) Rajeev Goel\({}^{1}\) Utkarsh Nath\({}^{1}\) Alvin C. Silva\({}^{2}\) Teresa Wu\({}^{1}\)

**Yingzhen Yang\({}^{1}\)**

\({}^{1}\) School of Computing and Augmented Intelligence, Arizona State University

{ywan1053, rgoel15, unath, teresa.wu, yingzhen.yang}@asu.edu

\({}^{2}\) Mayo Clinic Arizona

silva.alvin@mayo.edu

###### Abstract

Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in the medical image domain. We study thorax disease classification in this paper. Effective extraction of features for the disease areas is crucial for disease classification on radiographic images. While various neural architectures and training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there are no principled methods that can effectively reduce the adverse effect of noise and background or non-disease areas on the radiographic images for disease classification. To address this challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks. The LRFL method is both empirically motivated by a Low Frequency Property (LFP) and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. LFP not only widely exists in deep neural networks for generic machine learning but also exists in all the thorax medical datasets studied in this paper. In the empirical study, using a neural network such as a ViT or a CNN pre-trained on unlabeled chest X-rays by Masked Autoencoders (MAE), our novel LRFL method is applied on the pre-trained neural network and demonstrates better classification results in terms of both multi-class area under the receiver operating curve (mAUC) and classification accuracy than the current state-of-the-art. The code of LRFL is available at https://github.com/Statistical-Deep-Learning/LRFL.

## 1 Introduction

Following the huge success of deep learning, recent studies have developed deep neural networks (DNNs) for various tasks in medical imaging, such as disease classification and abnormalities detection in anatomy in chest X-rays . Accurate clinical decision-making with DNNs heavily relies on learning informative medical feature representation. Early works adopt convolutional neural networks (CNNs) such as U-Net  for representation learning on radiography images. Recently, Visual Transformers (ViTs)  are also adopted to learn informative medical representations from radiography images , utilizing their capabilities in capturing long-range feature dependencies. Albeit the success of CNNs and ViTs in analyzing radiography images, their accuracy heavily relies on the quality and quantity of data and annotations . However, the collection of large amounts of training data and high-quality annotations in the medical imaging domain are extremely hard . Totackle this problem, self-supervised learning (SSL) has been employed as a solution for acquiring representations from unlabeled data. Given the greater availability of unlabeled medical images , SSL proves to be an efficient approach for obtaining discriminative representations. SSL employs a range of pretext tasks to acquire transferable representations without manual annotations. Over recent years, numerous variations of self-supervised learning have surfaced using contrastive learning  and restorative learning .

**Challenges in the Current Literature for Disease Classification.** We study thorax disease classification in this paper. Clinical studies show that the disease areas on radiographic images are subtle and exhibit localized variations. Such conditions are further complicated by the inevitable noise that is ubiquitous in radiographic images, as detailed in Section 2.1. Effective and robust extraction of features for the disease areas is crucial for disease classification on radiographic images. Although various neural architectures, such as CNNs and ViTs, and different training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there have been no principled methods that can effectively reduce the adverse effect of noise and background, or non-disease areas, for disease classification on radiographic images.

**Our Contributions.** The contributions of this paper are presented as follows. First, in order to address the aforementioned challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks with the application for thorax disease classification. Our LRFL method employs low-rank features for disease classification. The usage of low-rank features is empirically motivated by a Low Frequency Property (LFP) illustrated in Figure 1. That is, the low-rank projection of the ground truth training class labels possesses the majority of the information of the training class labels. In fact, LFP widely holds for a broad range of classification problems using deep neural networks, such as [1; 8; 9]. Inspired by LFP, our LRFL method adds the truncated nuclear norm as a low-rank regularization term to the training loss of a neural network so as to perform classification using low-rank features. Because the actual features used for classification are approximately low-rank and the high-rank features are significantly truncated, all the noise and the information about the background or the non-disease areas on radiographic images in the high-rank features are largely discarded and not learned in a neural network. _Importantly and significantly different from existing low-rank learning methods reviewed in Section 2.3, we introduce a novel separable approximation for the TNN, enabling the optimization of the LRFL training loss using standard SGD._ The appropriate feature ranks retained in the LRFL method across various datasets are determined through an efficient cross-validation process, and the optimal ranks are detailed in Table 8. Extensive experimental results demonstrate that our LRFL method renders new record mAUC on three standard thorax disease datasets, NIH-ChestX-ray , COVIDX , and CheXpert , surpassing the current state-of-the-art  with the same pre-training setup.

Second, we provide a theoretical analysis showing a sharp generalization bound for the LRFL method, underscoring the substantial benefits of employing low-rank regularization within LRFL. Given these theoretical insights and the versatility of LRFL across various neural networks, we anticipate broader applications of LRFL in the classification of other diseases beyond thoracic ones, potentially enhancing classification tasks across different radiographic imaging contexts. It is worthwhile to mention that the literature has studied low-rank learning using TNN resembling LRFL, as to be reviewed in Section 2.3. Our LRFL method builds upon these foundational principles by incorporating low-rank regularization into the training of neural networks, aiming to improve thorax disease classification by reducing the adverse effects of noise and irrelevant background information. **Different from the conventional low-rank learning methods, our approach introduces a separable approximation to the TNN, facilitating the optimization process and enhancing the generalization ability of the model**. Such improved generalization is evidenced by the improved prediction accuracy of LRFL compared to the current state-of-the-art (SOTA) methods in medical image analysis.

Moreover, we have employed a conditional diffusion model trained on COVIDx and CheXpert datasets to generate synthetic images. These synthetic images are then added to their respective training sets to form the augmented training data on which our LRFL models are trained. This approach has further elevated the state-of-the-art mAUC scores achieved by LRFL on both COVIDx and CheXpert datasets.

**Motivation for using synthetic images to boost the accuracy for thorax disease classification.** The computer vision literature [13; 14; 15] has extensively studied the usage of the generated synthetic images which augment the training data and improve the prediction accuracy of image classification. Inspired and motivated by this observation, we propose to generate synthetic images and use them to form the augmented training data and improve the performance of thorax disease classification. The augmented training data comprise the original training images and the synthetic images. However, too many synthetic images tend to introduce more noise to the augmented training data so excessive synthetic images can hurt the prediction accuracy of DNNs trained on the augmented training data . As evidenced in the ablation study in Section C.3, our proposed LRFL method, coupled with the selection of the number of synthetic images, effectively mitigates this issue. The proposed low-rank learning method only learns the low-rank part of the features learned by a deep learning model so that noise in the high-rank part would not affect the learned model. Also, cross-validation is used to select a proper number of synthetic images, which will boost the prediction accuracy while not introducing too much noise to the augmented training data.

We also present ablation study results evidencing our contributions. We compare the eigenvalues of the kernels and the kernel complexity associated with the LRFL models and the corresponding base models in Section B.4.1 of the appendix, and the lower kernel complexity of the LRFL models suggests their lower generalization error [17; 18; 19].

**Notations.** We use bold letters to denote matrices or vectors. \([]_{i}\) stands for the \(i\)-th row of a matrix \(\). \(_{p}\) denotes the \(p\)-norm of a vector or a matrix. \(_{}\) is the Frobenius norm of a matrix. We use \([m n]\) to indicate numbers between \(m\) and \(n\) inclusively, and \([n]\) denotes the natural numbers between \(1\) and \(n\) inclusively.

## 2 Related Works

### Radiographic Imaging

Radiographic imaging  is a cornerstone in medical image analysis. Unlike photographic images , radiography images have consistent backgrounds due to fixed imaging protocols [22; 23; 24; 2]. Clinical details are spread across the images, while areas indicating illness show localized variations [2; 25; 26], making analysis challenging. Noise is unavoidable in ra

Figure 1: Eigen-projection (first row) and signal concentration ratio (second row) of Vit-Base on NiH-ChestXray-14, COVIDx, and CheXpert. To compute the eigen-projection, we first calculate the eigenvectors \(\) of the kernel gram matrix \(^{n n}\) computed by a feature matrix \(^{n d}\), then the projection value is computed by \(=_{}^{C}^{} ^{(c)}_{2}^{2}/^{(c)}_{ 2}^{2}^{n}\), where \(C\) is the number of classes, and \(\{0,1\}^{n C}\) is the one-hot labels of all the training data, \(^{(c)}\) is the \(c\)-th column of \(\). The eigen-projection \(_{r}\) for \(r[(n,d)]\) reflects the amount of the signal projected onto the \(r\)-th eigenvector of \(\), and the signal concentration ratio of a rank \(r\) reflects the proportion of signal projected onto the top \(r\) eigenvectors of \(\). The signal concentration ratio for rank \(r\) is computed by \(^{(1:r)}_{2}\), where \(^{(1:r)}\) contains the first \(r\) elements of \(\). For example, by the rank \(r=38\), the signal concentration ratio of \(\) on NIH ChestX-ray14, COVIDx, and CheXpert are \(0.959\), \(0.964\), and \(0.962\) respectively.

diography images, stemming from quantum fluctuations, electronic interference, scatter radiation, motion blur, and overlapping structures [27; 28; 29; 30]. Quantum noise, originating from statistical fluctuations in detected X-ray photons [31; 32; 25; 30], is often the primary source. Quantum noise introduces graininess, obscuring details and diminishing contrast . Modeled as a Poisson process [25; 30], it can be approximated by a Gaussian distribution under high photon flux [33; 34], enabling noise reduction techniques .

### Medical Image Analysis with Deep Learning

Deep learning has made remarkable progress in photographic image analysis [35; 36; 37], sparking interest in applying it to medical imaging due to the ability to learn complex representations. Convolutional neural networks (CNNs) like U-Net [3; 38; 39] pioneered this field, achieving state-of-the-art performance across various tasks such as image classification [40; 41; 42], object detection [43; 39; 44], and semantic segmentation [44; 45; 39; 46; 47]. More recently, visual transformers, inspired by the success of transformers in natural language processing , have outperformed state-of-the-art CNNs on various computer vision benchmarks [49; 50; 4; 51; 52; 53]. Despite debates around transformers vs CNNs in terms of generalization [54; 55; 56; 57; 58], data requirements [4; 59; 60], and computational costs , transformers have shown great potential in medical image analysis [2; 62; 63]. Given the scarcity of high-quality annotations, self-supervised contrastive learning techniques [7; 64; 65; 66; 2] have gained traction for pre-training networks in this domain [22; 2; 62]. However, the high similarity between radiographic images due to standardized protocols [67; 68] poses challenges compared to photographic images [69; 7]. To address this, recent works utilize restorative strategies like masked autoencoders (MAE) [70; 71; 72; 73; 74; 2; 75] for pre-training . Similarly, we adopt MAE  to pre-train our networks before learning low-rank features.

### Low-Rank Learning

Low-rank learning has garnered significant attention across various fields for its capacity to reduce dimensionality, suppress noise, and enhance feature extraction. Robust Principal Component Analysis (RPCA)  serves as a cornerstone in this realm, efficiently separating data matrices into low-rank and sparse components. This technique proves invaluable for vision-related tasks such as image denoising and background subtraction. Building on this foundation,  introduced singular value pruning, a method to impose low-rank constraints on neural network layers, thereby boosting both computational efficiency and performance. The concept of TNN regularization (TNNR) has been further refined by researchers like , who noted that TNNR more accurately approximates the rank function by selectively minimizing singular values, essential for precise low-rank matrix recovery in noisy conditions. Following that, some existing works [79; 80; 81] propose to perform low-rank feature learning by minimizing the TNN of the feature matrix. Additionally, the use of TNNR in tensor completion has markedly improved the restoration of incomplete visual data, utilizing tensor singular value decomposition (t-SVD) [82; 83]. More contemporary learning-based methods, such as those developed by , have optimized low-rank approximations through targeted training, enhancing practical application outcomes. Some works [85; 86; 87] also demonstrate that learning low-rank features can significantly enhance the robustness of deep neural networks against noise in input images. In addition, recent works [88; 89; 90] find that the good generalization capabilities of deep neural networks are attributed to the fact that deeper networks are inductively biased to find solutions with lower effective rank embeddings.

## 3 Formulation

### Pipeline for Thorax Disease Classification

We utilize the masked MAE technique  for the initial pre-training of both CNNs and ViTs following, and subsequently fine-tune the pre-trained networks with our Low-Rank Feature Learning (LRFL). The full training pipeline of learning low-rank features for disease classification can be described in three steps. In the first step, which is the **pre-training** step, we pre-train the networks using the self-supervised restorative learning method, masked MAE , on a diverse pre-training dataset that includes ImageNet-1k  and a collection of X-rays (0.5M) . During this phase, we randomly mask patches on input images and drive the networks to optimize pixel-wise image reconstruction for the obscured patches. In the second step, which is the **regular fine-tuning** step, we fine-tune the pre-trained networks employing cross-entropy loss aimed at image classification on specific target datasets, namely NIH-ChestX-ray , COVIDx , and CheXpert . In the last step, which is the **low-rank feature learning** step, we fix the backbones of the networks and fine-tune the linear classifier utilizing our novel LRFL method.

### Problem Setup for LRFL

We now introduce the problem setup for LRFL with training details. Suppose the training data are given as \(\{_{i},_{i}\}_{i=1}^{n}\) where \(_{i}\) and \(_{i}^{C}\) are the \(i\)-th training data point and its corresponding class label vector respectively, and \(C\) is the number of classes. Each element \(_{i}\) is binary with \(_{i}=1\) indicating the \(i\)-th disease is present in \(_{i}\), otherwise \(_{i}=0\). Suppose that the neural network trained by step two of our pipeline in Section 3.1 generates a feature vector \(f_{_{1}(0)}()^{d}\) (the output of the layer preceding the final linear/softmax layer of the network) for any input \(\), and \(f_{^{}}()\) is the feature extraction function with \(^{}\) being the weights of the feature extraction backbone of the network. \(_{1}(0)\) denotes the denotes the weights of feature extraction backbone by step two of the pipeline. We can train a neural network by optimizing

\[_{}L()=_{i=1}^{n}( _{i},(_{2}f_{_{1}(0)}() )),\] (1)

where \(_{1}\) is initialized by \(_{1}(0)\), \(_{2}^{C d}\), and \(=(_{1},_{2})\). Here \(\) is an element-wise sigmoid function, \(()^{C}\) with \([()]_{c}=1/(1+(-_{c}))\) for \(^{C}\) and \(c[C]\). KL stands for the element-wise binary cross-entropy function. Given two nonnegative vectors \(=[u_{1},,u_{d}]^{d},=[v_{1},,v_{d }]^{d}\) where \(u_{i}\{0,1\}\) for all \(i[d]\) and \(\|\|_{} 1\), \((,)_{j=1}^{d}-u_{i} v_{i}-(1-u_{ i})(1-v_{i})\). We use \(=[_{1}^{};_{2}^{};;_{ n}^{}]^{n C}\) to denote the training label matrix by stacking the label vectors of all the training data. Let the mapping function of the neural network used in the loss function \(L()\) be \(_{}()=_{2}f_{_{1}}()\).

**Motivation for Low-Rank Regularization** The Low Frequency Property is illustrated in Figure 1, that is, the low-rank projection of the ground truth class labels possesses the majority of the information of the class labels. Inspired by this observation, our LRFL encourages the low-rank part of the feature to participate in the classification process. In this way, the noise and non-disease areas in the high-rank part of the feature are mostly not learned by LRFL so as to improve the classification accuracy. Using notations in Section 3.2, the truncated nuclear norm of \(\) is \(\|\|_{T}_{i=T+1}^{d}_{i}\) where \(T[0,d]\). It can be observed by the generalization error bound discussed in Section 3.2 that a smaller \(\|\|_{T}\) renders a tighter upper bound for the generalization error of the linear neural network used for LRFL. This observation gives a strong theoretical motivation for us to add the truncated nuclear norm \(\|\|_{T}\) to the training loss \(L()\).

### Generalization Bound for Low-Rank Feature Learning

We define the loss function \((_{}(),)\| _{}()-\|_{2}^{2}\), and the generalization error of the network NN is the expected risk of the loss \(\), which is denoted by \(L_{}(_{})_{(, )}[(_{}(), )]\), with \(\) being the distribution of the data \(\) and its class label \(\). The network \(_{}\) generates a feature \(^{n d}\) of all the training data with \(_{i}=f_{_{i}}^{}(_{i})\) for \(i[n]\). The kernel gram matrix for the feature \(\) is \(_{n}=^{}\). We let \(_{1}_{2}_ {r}>0\) where \(\{n,d\}\) is the rank of \(_{n}\). Suppose the Singular Value Decomposition of \(\) is \(=^{}\), where \(^{n d}\) has orthogonal columns, \(^{d d}\) is a diagonal matrix with diagonal elements being the singular values of \(\), and \(^{d d}\) is an orthogonal matrix. The columns of \(\) and \(\) are also called the left eigenvectors and the right eigenvectors of \(\), respectively. Let \(_{1}_{2}_{d}\) be the singular values of \(\), and \(}=^{()}^{()}{}^{}\) be the projection of the training label matrix \(\) onto the subspace spanned by the top-\(\) left eigenvectors of \(\), where \(^{()}^{n}\) is formed by the top \(\) eigenvectors in \(\). Then, we have the following theorem giving the sharp generalization error bound for the linear neural network in (1).

**Theorem 3.1**.: For every \(x>0\), with probability at least \(1-(-x)\), after the \(t\)-th iteration of gradient descent for all \(t 1\), we have

\[L_{}(_{})\|-} \|_{}+c_{1}(1-_{r})^{2t}\| \|_{}^{2}+c_{2}_{h[0,r]}(+ _{i=h+1}^{r}_{i}})+x}{n},\] (2)

where \(c_{1},c_{2},c_{3}\) are positive constants.

**Remark 3.2**.: The RHS of (2) is the generalization error bound for the linear neural network used in LRFL as step three of the pipeline in Section 3.1. Moreover, let \(_{1}_{2}_{d}\) be the singular values of \(\). Due to the fact that \(_{i=h+1}^{r}_{i}} _{i=h+1}^{r}_{i}\), it follows by (2) that

\[L_{}(_{}) c_{1}(1-_{r})^{2t}\|\|_{}^{2}+c_{2}(+_{i=T+1}^{d}_{i})+x}{n},\] (3)

which holds for all \(T[0,d]\). (3) motivates the reduction of the truncated nuclear norm of the feature \(\), as detailed in the next subsection.

### Optimization of the Truncated Nuclear Norm in SGD

The truncated nuclear norm \(\|\|_{T}\) is not separable, so the training loss with \(\|\|_{T}\) cannot be directly optimized by the standard SGD. To address this problem, we propose an approximation \(\|_{T}}\) to \(\|\|_{T}\) which is separable so that \(\|_{T}}\) can be optimized by standard SGD.

First, we note that if \(,\) are known, then \(=^{}\). If we have an approximation \(}\) to \(\) and an approximation \(}\) to \(\), then \(\) can be approximated by \(}=}^{}}\). As a result, the approximation \(\|_{T}}\) to the truncated nuclear norm is \(\|_{T}}=_{i=1}^{n}( _{s=T+1}^{d}_{k=1}^{d}}_{si}^{} _{ik}}_{ks})\). Due to the above discussions, the loss function of LRFL with the approximate truncated nuclear norm \(\|_{T}}\) is \(_{}()=_{s_{i}_{s }}(_{i},[(^{ {dim}})]_{s})+\|_{T}}\), which is separable, so that it can be trained by the standard SGD. \(>0\) is the weighting parameter for the truncated nuclear norm. Because \(_{}()\) is to be optimized by the standard SGD, we have the loss function of LRFL for the \(j\)-th minibatch \(_{j}[n]\) as

\[_{j}()=_{j}|}_{i _{j}}(_{i},[( ^{})]_{i})+_{j}|}_{i_{j}}(_{s=T+1}^{d} _{k=1}^{d}}_{si}^{}_{ik}}_{ks}).\] (4)

The approximation \(}\) and \(}\) can be computed as the left and right eigenvectors of the feature \(\) computed at earlier epochs. In order to save computation and avoiding performing SVD for \(\) at every epoch, we propose to update \(}\) and \(}\) only after certain epochs. Algorithm 1 describes the training algorithm for the neural network trained with LRFL, which uses the standard SGD to optimize the loss function \(_{}()\), as step three of our pipeline in Section 3.1. Before the first epoch, we compute \(}\) and \(}\) as the left and right eigenvectors of the feature \(\) at the initialization of the neural network. After every \(t_{0}\) epoch with \(t_{0}\) being a constant integer, we update \(}\) and \(}\) as the left and right eigenvectors of the feature \(\) produced by the neural network right after \(t_{0}\)-th epoch, with \(t_{0}\) being a constant integer.

```
1:Initialize the weights \(_{1}\) by \(_{1}=_{1}(0)\), and initialize \(_{2}\) randomly
2:Compute feature \(\) by the neural network, and its SVD as \(=\)
3:Update \(}=,}=\)
4:for\(t=1,2,,t_{}\)do
5:if\(t 0}\)then
6: Compute feature \(\) of the neural network, and its SVD \(=\).
7: Update \(}=,}=\)
8:endif
9:for\(b=1,2,,B\)do
10: Update \(\) by applying gradient descent on batch \(_{j}[n]\) using the gradient of the loss \(_{j}\) in Eq.(4)
11:endfor
12:endfor
13:return The trained weights \(\) of the network ```

**Algorithm 1** Training Algorithm with the Approximate Truncated Nuclear Norm by SGD

## 4 Experimental Results

In this section, we conduct experiments on medical datasets to demonstrate the effectiveness of the proposed LRFL method. The experiments section is organized as follows: In Section 4.1, wediscuss our experimental setup and implementation details. In Sections 4.2 and 4.3, we evaluate the LRFL models for thorax disease classification on CheXpert and COVIDTx. Evaluation results on NIH ChestX-ray14 are deferred to Section B.1 of the appendix. In Section 4.4, we evaluate synthetic data augmentation on LRFL models, with additional details and results deferred to Section C of the appendix. Comprehensive ablation studies on LRFL are performed in Section 4.5. In Section 4.5.1, we study the effectiveness of the LRFL models in reducing the adverse effect of the background for disease classification. In Section 4.5.2, we study the performance of LRFL models for disease localization. Grad-CAM visualization results of LRFL models and baseline models are illustrated in Section 4.5.3. Additional ablation studies are deferred to Section B.4 of the appendix. In Section B.4.1, we compare the kernel eigenvalues and kernel complexity between the LRFL models and their corresponding base models to show that LRFL improves the generalization capability of the base models by reducing their kernel complexity. In Section B.4.2, we evaluate the performance of the LRFL models with limited data availability. In Section B.4.3, we compare the performance of the LRFL with other fine-tuning strategies. In Section B.4.5, we present the training time of the LRFL models compared with the corresponding base models.

### Implementation Details

In this section, we evaluate the proposed LRFL for thorax disease classification. We utilize networks pre-trained on ImageNet  or chest X-rays in  with MAE, a self-supervised learning strategy that reconstructs missing pixels from patches of input images. We fine-tune these pre-trained networks with low-rank regularization on three public X-ray datasets: (1) NIH ChestX-ray14 , (2) Stanford CheXpert , and (3) COVIDTx . The ADAM optimizer is used with a batch size of 1024 for all datasets. Initially, we fine-tune the entire networks for 75 epochs following the settings in , then fine-tune with low-rank regularization for another 75 epochs. We use a cosine learning rate schedule, and the initial learning rate, which is denoted as \(\), is selected by cross-validation for each model and each dataset. The default values for momentum and weight decay are set to 0.9 and 0, respectively. We use standard data augmentation techniques, including random-resize cropping, random rotation, and random horizontal flipping. For a fair comparison, all baselines are also fine-tuned for an additional 150 epochs, showing almost no improvement. An exhaustive analysis of this additional fine-tuning is in Section B.4.3. We evaluate our LRFL method on both CNN and visual transformer architectures, including ResNet-50, DenseNet, ViT-S, and ViT-B. Our model is referred to as 'X-LR', where X is the base model (e.g., ResNet-50-LR for ResNet-50 with low-rank features).

**Tuning the \(T\), \(\), and \(\) by Cross-Validation.** We search for the optimal values of feature rank \(T\), the weighting parameter for the truncated nuclear norm \(\), and the learning rate \(\) on each dataset. Let \(T=(n,d)\), where \(\) is the rank ratio. We select the values of \(\) and \(\) by performing 5-fold cross-validation on 20% of the training data in each dataset. The value of \(\) is selected from \(\{0.01,0.02,0.03,0.04,0.05,0.1,0.15,0.2\}\). The value of \(\) is selected from \(\{5 10^{-4},1 10^{-3},2.5 10^{-3},5 10^{-3},1  10^{-2}\}\). The value of \(\) is selected form \(\{5 10^{-4},2.5 10^{-4},1 10^{-4},5 10^{-5},2.5  10^{-5},1 10^{-5}\}\). To determine the optimal values of the parameters \(\), \(\), and \(\), we employ a sequential greedy search strategy. We first fix \(\) and \(\) and find the optimal value of \(\) by cross-validation. Subsequently, using this optimized \(\), we proceed to search for the optimal \(\) while keeping \(\) constant. Finally, with optimal \(\) and \(\), we search for the optimal \(\) by cross-validation. The optimal values of \(\), \(\), and \(\) selected by cross-validation are shown in Table 8 in Section B.3 of the appendix. The time spent for the entire cross-validation process is presented in Table 9 Section B.3 of the appendix, which demonstrates that the cross-validation process is efficient and does not significantly increase the computational overhead of the training process.

### Stanford CheXpert

**Experimental setup.** CheXpert  consists of 224,316 chest X-rays collected from 65,240 patients, where 191,028 chest X-rays are used for training. Each X-ray in the dataset has radiology reports indicating the presence of 14 diseases. Following the protocol in , all images are resized into \(224 224\). We also report the mean AUC (Area Under the Curve) for the 5 distinct classes and conduct a comprehensive comparison with state-of-the-art baseline methods.

**Results and analysis.** Table 1 presents the performance comparisons between the baseline models and the LRFL models on the CheXpert dataset. Throughout this section, we use the postfix "-LR" to indicate a neural network trained with our LRFL. For example, we use the ViT-B model pre-trained on \(489,090\) and the ViT-S model pre-trained on \(266,340\) chest X-rays with Masked Autoencoders (MAE) . The pre-trained ViT-B network is fine-tuned on the CheXpert dataset and achieves a mean AUC of \(89.3\). It is observed that ViT-B-LR achieves state-of-the-art performance of 89.8% in mAUC and improves the performance of ViT-B by 0.5% in mAUC. ViT-S-LR also improves the performance of ViT-S by 0.4% in mAUC, which demonstrates the power of LRFL. We also show the classification accuracy of the five diseases in Table 1, where our method exhibits much better performance than baseline methods. For example, ViT-S-LR achieves an mAUC of 86.3% on Cardiomegaly, with a 4.5% improvement over ViT-S trained with MAE. Such improvements demonstrate the power of LRFL in detecting distinct diseases. The comparison between LRFL models and a more comprehensive list of baseline models are deferred to Table 7 of the appendix.

### COVIDx

**Experimental setup.** COVIDx (Version 9A)  consists of 30,386 chest X-rays collected from 17,026 unique patients. We follow the previous works [11; 2] in splitting the dataset into 29,986 training images with four different classes and 400 testing images with three classes. For fair comparisons with the previous methods, we report Top-1 accuracy on the test set (3 classes).

**Results and Analysis.** Table 2 compares the performance of SOTA transformer-based models and the LRFL models on the COVIDx dataset. Similar to Section 4.2, the base ViTs are first pre-trained on chest X-rays using Masked Autoencoders (MAE), and then the pre-trained model is fine-tuned on the COVIDx dataset. It can be observed from Table 2 that both ViT-S-LR and ViT-B-LR outperform the corresponding base models ViT-S and ViT-B, achieving an increase in accuracy of 1.6% and 1.7%, respectively. Table 2 also compares the performance of our LRFL models against the state-of-the-art models on the COVIDx dataset. LRFL models achieve much higher accuracy compared to CNN-based models such as DenseNet-121. ViT-B-LR achieves the new SOTA performance of 97% top-1 accuracy with input resolution set to 224\(\)224, which exceeds the previous SOTA performance  by 1.7% in top-1 accuracy.

### Improved Results using Diffusion Model

**Experimental Setup.** In this section, we aim to further improve the performance of LRFL models by adding labeled synthetic radiographic images of thorax diseases to the training sets of COVIDx and CheXpert. The synthetic radiographic images are generated by a conditional diffusion model, Diffusion Transformer (DiT) , trained on the training set of the corresponding dataset. Details on the training of DiT are deferred to Section C.2 of the appendix. To maintain the same disease co-occurrence, synthetic radiographic images are generated based on the labels from the label set of

  Method & Architecture & Rank & Atelectasis & Cardiomegaly & Consolidation & Edema & Effusion & mAUC (\%) \\  Irvin et al. & - & 81.8 & 82.8 & 93.8 & 93.4 & 92.8 & 88.9 \\ Pham et al. & DN121 & - & 82.5 & 85.5 & 93.7 & 93.0 & 92.3 & 89.4 \\ Kang et al. & DN121 & - & 82.1 & 85.9 & **94.4** & 89.2 & 93.6 & 89.0 \\ MoCo v2  & DN121 & - & 78.5 & 77.9 & 92.5 & 92.8 & 92.7 & 88.7 \\  ViT-S-2 & ViT-S/16 & - & 83.5 & 81.8 & 93.5 & 94.0 & 93.2 & 89.2 \\ ViT-S-LR (Ours) & ViT-S/16 & 0.05r & **83.7** & 86.3 & 90.9 & 93.7 & 93.1 & 89.6 \\  ViT-B  & ViT-B/16 & - & 82.7 & 83.5 & 92.5 & 93.8 & **94.1** & 89.3 \\ ViT-B-LR (Ours) & ViT-B/16 & 0.05r & 81.6 & 85.4 & 93.4 & **94.6** & 93.9 & **89.8** \\  

Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.

  Method & Architecture & Rank & Covid-19 Sensitivity & Accuracy \\  COVIDNet-CXR Small  & - & - & 87.1 & 92.6 \\ COVIDNet-CXR Large  & - & - & 96.8 & 94.4 \\ MoCo v2  & DN121 & - & 94.5 & 94.0 \\ DN121  & DN121 & - & 97.0 & 93.5 \\  ViT-S  & ViT-S/16 & - & 94.5 & 95.2 \\ ViT-S-LR (Ours) & ViT-S/16 & 0.01r & 97.5 & 96.8 \\  ViT-B  & ViT-B/16 & - & 95.5 & 95.3 \\ ViT-B-LR (Ours) & ViT-B/16 & 0.003r & **98.5** & **97.0** \\  

Table 2: Performance comparisons between LRFL models and SOTA baselines on COVIDx (in accuracy). DN represents DenseNet.

each dataset. The number of synthetic images added to the training set of each dataset is determined via cross-validation. We first generate synthetic images of the same size as the training set. The optimal percentage of synthetic images is selected using 5-fold cross-validation on the training data, which is detailed in Section C.2 of the appendix. Synthetic images are combined with the original dataset for further fine-tuning with low-rank regularization. Ablation studies on the number of synthetic images incorporated are performed in SectionC.3.

**Results.** The results of LRFL models trained after adding synthetic images on CheXpert and COVIDx are shown in Table 3. It is observed from the results that adding synthetic data into the training set of LRFL models can further increase their performance. For example, ViT-B-LR with synthetic images added in training outperforms the corresponding base model ViT-B by \(2.2\%\) on COVIDx.

### Ablation Study

#### 4.5.1 Study of LRFL in Reducing the Adverse Effects of Background

To demonstrate that the LRFL models are more robust to the background than the baselines, we perform an ablation study on the LRFL to reduce the adverse effects of the background. In this study, we create a mask for the disease area for each original image, then decompose the original image, which has a bounding box for the disease, into a disease image and a background image. Both the disease image and the background image are of the same size as the original image. The background image has grayscale \(0\) in the masked disease area, and the disease image has grayscale \(0\) in the non-disease area. We feed the three images, which are the original image, the disease image, and the background image, to an LRFL model and obtain the original features, disease features, and background features for the LRFL model, respectively. We also feed these three images to a baseline model and obtain the original features, disease features, and background features for the baseline model. For each original image, we measure the distance between the disease features and original features using KL-divergence on the softmaxed features for the LRFL model and the baseline model. We then compute the average feature distance for each model, which is the average distance between the disease features and original features over the images with a ground-truth bounding box for the disease in the NIH ChestX-ray 14. The results in Table 4 indicate that the original features are closer to the disease features by the LRFL models compared to the baseline models, evidencing the effectiveness of the LRFL models in reducing the adverse effect of the background area. We also remark that since only the low-rank part of the original features participates in the classification process, the noise and non-disease areas in the high-rank part of the features are mostly not learned by LRFL, and in this manner, LRFL is robust to both noise and background.

#### 4.5.2 Disease Localization

To study which part of the X-ray image is responsible for the model prediction by the LRFL models, we perform the disease localization experiment following the settings in . We first obtain the Grad

  Methods & mAUC \((\%)\) & Average Feature Distance \\  ViT-S & 82.3 & 0.7030 \\ ViT-S-LR & **82.7** & **0.6352** \\  ViT-B & 83.0 & 0.5642 \\ ViT-B-LR & **83.4** & **0.6628** \\  

Table 4: Average feature distance between original features and disease features of images with a ground-truth bounding box for the disease in the NIH ChestX-ray 14.

   &  &  &  \\   & & Rank & \# Synthetic Images & mAUC (\%) & Rank & \# Synthetic Images & Accuracy (\%) \\  ViT-S  & ViT-S/16 & - & - & 89.2 & - & - & 95.2 \\ ViT-S-LR (Ours) & ViT-S/16 & 0.05r & - & 89.6 & 0.01r & - & 96.8 \\ ViT-S (Ours) & ViT-S/16 & - & 0.2\(n\) & 89.3 & - & 1.0\(n\) & 97.0 \\ ViT-S-LR (Ours) & ViT-S/16 & 0.05r & 0.2\(n\) & 89.7 & 0.01r & 1.0\(n\) & 97.3 \\  ViT-B  & ViT-B/16 & - & - & 89.3 & - & - & 95.3 \\ ViT-B-LR (Ours) & ViT-B/16 & 0.025r & - & 89.8 & 0.003r & - & 97.0 \\ ViT-B-LR (Ours) & ViT-B/16 & - & 0.25\(n\) & 89.9 & - & 1.0\(n\) & 97.0 \\ ViT-B-LR (Ours) & ViT-B/16 & 0.025r & 0.25\(n\) & **90.4** & 0.003r & 1.0\(n\) & **97.5** \\  

Table 3: Performance comparison of baseline models and LRFL models on the CheXpert and COVIDx datasets, with and without synthetic data. \(n\) denotes the number of training images in the respective dataset.

CAM visualization results with the last transformer block of ViT-S. The experiments are performed with all the images with a ground-truth bounding box for disease in ChestX-ray14. The predicted bounding box is generated with the thresholded Grad-CAM heatmap, largest connected component, and box regression. We evaluate the performance of disease localization by Intersection over Union (IoU) between the ground-truth bounding box and the predicated bounding box used for evaluation. The Average Precision (AP) on \(25\%\) and \(50\%\) IoUs, which are denoted as AP\({}_{25}\) and AP\({}_{50}\), for ViT-S and ViT-S-LR are shown in Table 5. It is observed from the results that our LRFL model significantly outperforms the base model in detecting the bounding box of thorax disease. For example, ViT-S-LR outperforms ViT-S by \(26.9\%\) in AP\({}_{25}\) for detecting the bounding box of Mass. In addition, ViT-S-LR outperforms ViT-S by \(21.2\%\) in AP\({}_{25}\) for detecting the bounding box of Effusion.

#### 4.5.3 Grad-CAM Visualization

To study how LRFL improves the performance of base models in disease detection, we use the Grad-CAM  to visualize the parts in the input images that are responsible for the predictions of the base models and low-rank models. Robust Grad-CAM  visualization results of Low-Rank ViT-Base are illustrated in Figure 2. All Grad-CAM visualization results illustrate that our LRFL models usually focus more on the areas inside the bounding box associated with the labeled disease. In contrast, the base models also focus on the areas outside the bounding box or even areas in the background. Robust Grad-CAM visualization results of Low-Rank ResNet-50 and additional Grad-CAM visualization results of Low-Rank ViT-Base are deferred to Figure 4 and Figure 5 in Section B.4.4 of the appendix.

## 5 Conclusion

In this paper, we propose a novel Low-Rank Feature Learning (LRFL) method for thorax disease classification, which can effectively reduce the adverse effect of noise and background, or non-disease areas, on the radiographic images for disease classification. Being universally applicable to the training of all neural networks, LRFL is both empirically motivated by the low frequency property and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. Extensive experimental results on thorax disease datasets, including NIH-ChestX-ray, COVIDx, and CheXpert, demonstrate the superior performance of LRFL in terms of mAUC and classification accuracy. In addition, the performance of LRFL models is further improved by adding synthetic radiographic images into the training set for data augmentation.

   &  & _{25}\)} & _{50}\)} \\   & & ViT-S & ViT-S-LR & ViT-S & ViT-S-LR \\  Mass & 756 & 27.0 & **53.9** & **11.1** & 8.0 \\ Atelectasis & 924 & 31.5 & **49.3** & **8.1** & **11.3** \\ Pneumothorax & 1899 & 4.7 & **18.3** & 0.0 & **1.5** \\ infiltrate & 2754 & 11.4 & **22.7** & 1.3 & **2.1** \\ Effusion & 2925 & 8.8 & **30.0** & 1.0 & **3.1** \\ Pneumonia & 2944 & 27.8 & **44.1** & 9.3 & **12.5** \\  All & 2300 & 18.0 & **28.5** & 4.7 & **5.2** \\  

Table 5: AP\({}_{25}\) and AP\({}_{50}\) scores for different diseases using ViT-S and ViT-S-LR models.

Figure 2: Robust Grad-CAM  visualization results on NIH ChestX-ray 14. The figures in the first row are the visualization results of ViT-Base, and the figures in the second row are the visualization results of Low-Rank ViT-Base.