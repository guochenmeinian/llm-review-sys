# Foundations of Multivariate Distributional Reinforcement Learning

Harley Wiltzer

Mila -- Quebec AI Institute

McGill University

harley.wiltzer@mail.mcgill.ca &Jesse Farebrother

Mila -- Quebec AI Institute

McGill University

jfarebro@cs.mcgill.ca &Arthur Gretton

Google DeepMind

Gatsby Unit, University College London

gretton@google.com &Mark Rowland

Google DeepMind

markrowland@google.com

###### Abstract

In reinforcement learning (RL), the consideration of multivariate reward signals has led to fundamental advancements in multi-objective decision-making, transfer learning, and representation learning. This work introduces the first oracle-free and computationally-tractable algorithms for provably convergent multivariate _distributional_ dynamic programming and temporal difference learning. Our convergence rates match the familiar rates in the scalar reward setting, and additionally provide new insights into the fidelity of approximate return distribution representations as a function of the reward dimension. Surprisingly, when the reward dimension is larger than \(1\), we show that standard analysis of categorical TD learning fails, which we resolve with a novel projection onto the space of mass-\(1\) signed measures. Finally, with the aid of our technical results and simulations, we identify tradeoffs between distribution representations that influence the performance of multivariate distributional RL in practice.

## 1 Introduction

Distributional reinforcement learning [DRL; MSK\({}^{+}\)10, BDM17b, BDR23] focuses on the idea of learning probability distributions of an agent's random return, rather than the classical approach of learning only its mean. This has been highly effective in combination with deep reinforcement learning [YZL\({}^{+}\)19, BCC\({}^{+}\)20, WBK\({}^{+}\)22], and DRL has found applications in risk-sensitive decision making [LM22, KEF23], neuroscience [DKNU\({}^{+}\)20], and multi-agent settings [ROH\({}^{+}\)21, SLL21].

In general, research in distributional reinforcement learning has focused on the classical setting of a scalar reward function. However, prior non-distributional approaches to multi-objective RL [RVWD13, HRB\({}^{+}\)22] and transfer learning [BDM\({}^{+}\)17a, BHB\({}^{+}\)20] model value functions of multivariate cumulants,1 rather than a scalar reward. Having learnt such a multivariate value function, it is then possible to perform zero-shot evaluation and policy improvement for any scalar reward signal contained in the span of the coordinates of the multivariate cumulants, opening up a variety of applications in transfer learning, and multi-objective and constrained RL.

_Multivariate distributional RL_ combines these two ideas, and aims to learn the full probability distribution of returns given a multivariate cumulant function. Successfully learning the multivariatereward distribution opens up a variety of unique possibilities, such as zero-shot return distribution estimation  and risk-sensitive policy improvement .

Pioneering works have already proposed algorithms for multivariate distributional RL. While these works all demonstrate benefits from the proposed algorithmic approaches, each suffers from separate drawbacks, such as not modelling the full joint distribution , lacking theoretical guarantees , or requiring a maximum-likelihood optimisation oracle for implementation . Concurrently, the work of  analyzed algorithms for DRL with Banach-space-valued rewards, and provided convergence guarantees for dynamic programming with non-parametric (intractable) distribution models.

Our central contribution in this paper is to propose algorithms for dynamic programming and temporal-difference learning in multivariate DRL which are computationally tractable and theoretically justified, with convergence guarantees. We show that reward dimensions strictly larger than \(1\) introduce new computational and statistical challenges. To resolve these challenges, we introduce multiple novel algorithmic techniques, including a randomized dynamic programming operator for efficiently approximating projected updates with high probability, and a novel TD-learning algorithm operating on mass-\(1\)_signed_ measures. These new techniques recover existing bounds even in the scalar reward case, and provide new insights into the behavior of distributional RL algorithms as a function of the reward dimension.

## 2 Background

We consider a Markov decision process with Polish state space \(\), action space \(\), transition kernel \(P:()\), and discount factor \([0,1)\). Unlike the standard RL setting, we consider a vector-valued reward function \(r:[0,R_{}]^{d}\), as in the literature on successor features . Given a policy \(:()\), we write the policy-conditioned transition kernel \(P^{}( x)= P( x,a)(a x)\).

**Multi-variate return distributions.** We write \((X_{t})_{t 0}\) for a trajectory generated by setting \(X_{0}=x\), and for each \(t 0\), \(X_{t+1} P^{}(|X_{t})\). The return obtained along this trajectory is then defined by \(G^{}(x)=_{t=0}^{}^{t}r(X_{t})\), and the _(multi-)return distribution function_ is \(^{}(x)=(G^{}(x))\).

**Zero-shot evaluation.** An intriguing prospect of estimating multivariate return distributions is the ability to predict (scalar) return distributions for any reward function in the span of the cumulants. Indeed,  show that for any reward function \(:x r(x),w\) for some \(w^{d}\), \( G^{}(x),w=_{}_{t 0}^{t}(X_{t})\) for \(X_{0}=x\). Likewise, one might consider \(r(x)=_{x}\), in which case \(G^{}(x)\) corresponds to the per-trajectory discounted state visitation measure, and  demonstrated methods for learning the distribution of \(G^{}\) to infer the return distribution for any bounded deterministic reward function.

**Multivariate distributional Bellman equation.** It was shown in  that multi-return distributions obey a distributional Bellman equation, similar to the scalar case , and defines the multivariate distributional Bellman operator \(^{}:(^{d})^{}(^{d})^{}\) by

\[(^{})(x)= P^{}( x)}{ }[(_{r(x),})_{}(X^{})],\] (1)

where \(_{y,}(z)=y+ z\) and \(f_{}= f^{-1}\) is the _pushforward_ of a measure \(\) through a measurable function \(f\). In particular,  showed that \(^{}\) satisfies the _multi-variate distributional Bellman equation_\(^{}^{}=^{}\), and that \(^{}\) is a \(\)-contraction in \(_{p}\), where \(_{p}(_{1},_{2})=_{x}W_{p}(_{1}(x), _{2}(x))\) and \(W_{p}\) is the \(p\)-Wasserstein metric . This suggests a convergent scheme for approximating \(^{}\) in \(_{p}\) by _distributional dynamic programming_, that is, computing the iterates \(_{k+1}=^{}_{k}\), following Banach's fixed point theorem.

**Approximating multivariate return distributions.** In practice, however, the iterates \(_{k+1}=^{}_{k}\) cannot be computed efficiently, because the size of the support of \(_{k}\) may increase exponentially with \(k\). A variety of alternative approaches that aim to circumvent this computational difficulty have been considered . Many of these approaches have proven effective in combination with deep reinforcement learning, though as tabular algorithms, either lack theoretical guarantees, or rely on oracles for solving possibly intractable optimisation problems. A more complete account of multivariate DRL is given in Appendix A. A central motivation of our work is the development of computationally-tractable algorithms for multivariate distributional RL with theoretically guarantees.

**Maximum mean discrepancies.** A core tool in the development of our proposed algorithms, as well as some prior work , is the notion of distance over probability distributions given by maximum mean discrepancies . A maximum mean discrepancy \(_{}:()() _{+}\) assigns a notion of distance to pairs of probability distributions, and is parametrised via a choice of kernel \(:\), defined by

\[_{}(p,q)=_{(Y_{1},Y_{2}) p p}[(Y_ {1},Y_{2})]-2_{(Y,Z) p q}[(Y,Z)]+_{(Z_{1}, Z_{2}) q q}[(Z_{1},Z_{2})]\,.\]

A useful alternative perspective on MMD is that the choice of kernel \(\) induces a reproducing kernel Hilbert space (RKHS) of functions \(\), namely the closure of the span of functions of the form \(z(y,z)\) for each \(y\), with respect to the norm \(\|\|_{}\) induced by the inner product \((y_{1},),(y_{2},)=(y_{1},y_{2})\). With this interpretation, \(_{}(p,q)\) is equal to \(\|_{p}-_{q}\|_{}\), where \(_{p}=_{}(,y)p(y)\) is the _mean embedding_ of \(p\) (similarly for \(_{q}\)). When \(p_{p}\) is injective, the kernel \(\) is called _characteristic_, and \(_{}\) is then a proper metric on \(()\). In the remainder of this work, we will assume that all spaces of measures will be over compact sets \(\); thus with continuous kernels, we are ensured that distances between probability measures are bounded. When comparing return distributions, this is achieved by asserting that rewards are bounded.

We conclude this section by recalling a particular family of kernels, introduced in , that will be particularly useful for our analysis. These are the kernels induced by semimetrics.

**Definition 1**.: _Let \(\) be a nonempty set, and consider a function \(:_{+}\). Then \(\) is called a semimetric if it is symmetric and \((y_{1},y_{2})=0 y_{1}=y_{2}\). Additionally, \(\) is said to have strong negative type if \(([p-q][p-q])<0\) whenever \(p,q()\) with \(p q\)._

Notably, certain semimetrics naturally induce characteristic kernels and probability metrics.

**Theorem 1** ([21, Proposition 29]).: _Let \(\) be a semimetric on a space \(\) have strong negative type, in the sense that \(([p-q][p-q])<0\) whenever \(p q\) are probability measures on a compact set \(\). Moreover, let \(:\) denote the kernel induced by \(\), that is_

\[(y_{1},y_{2})=((y_{1},y_{0})+(y_{2},y_{0})-(y_{1},y_{2}))\]

_for some \(y_{0}\). Then \(\) is characteristic, so \(_{}\) is a metric._

**Remark 1**.: _An important class of semimetrics are the functions \(_{}:^{d}^{d}_{+}\) given by \(_{}(y_{1},y_{2})=\|y_{1}-y_{2}\|_{2}^{2}\) for \((0,2)\). It is known that these semimetrics have strong negative type, and thus the kernels \(_{}\) induced by \(_{}\) are characteristic . The resulting metric \(_{_{}}\) is known as the energy distance._

## 3 Multivariate Distributional Dynamic Programming

To warm up, we begin by demonstrating that indeed the (multivariate) distributional Bellman operator is contractive in a supremal form \(_{}}\) of MMD, given by \(_{}}(_{1},_{2})=_{x} _{}(_{1}(x),_{2}(x))\), for a particular class of kernels \(\). Our first theorem generalizes the analogous results of  in the scalar case to multivariate cumulants. The proof of Theorem 2, as well as proofs of all remaining results, are deferred to Appendix B.

**Theorem 2** (Convergent MMD dynamic programming for the multi-return distribution function).: _Let \(\) be a kernel induced by a semimetric \(\) on \([0,(1-)^{-1}R_{}]^{d}\) with strong negative type, satisfying_

1. _[leftmargin=0.5cm]_
2. _Shift-invariance__. For any_ \(z^{d}\)_,_ \((z+y_{1},z+y_{2})=(y_{1},y_{2})\)_._
3. _Homogeneity__. For any_ \([0,1)\)_, there exists_ \(c>0\) _for which_ \(( y_{1}, y_{2})=^{c}(y_{1},y_{2})\)_._

_Consider the sequence \(\{_{k}\}_{k=1}^{}\) given by \(_{k+1}=^{}_{k}\). Then \(_{k}^{}\) at a geometric rate of \(^{c/2}\) in \(_{}}\), as long as \(_{}}(_{0},^{}) C<\)._

Notably, the energy distance kernels \(_{}\) satisfy the conditions of Theorem 2, and \(_{}( y_{1}, y_{2})^{}(y_{1},y_{2})\) by the homogeneity of the Euclidean norm, so \(^{}\) is a \(^{/2}\)-contraction in the energy distances. This generalizes the analogous result of  in the one-dimensional case.

While Theorem 2 illustrates a method for approximating \(^{}\) in MMD, it leaves a lot to be desired. Firstly, even in tabular MDPs, just as in the case of scalar distributional RL, return distribution functions have infinitely many degrees of freedom, precluding a tractable exact representation. As such, it will be necessary to study approximate, finite parameterizations of the return distribution functions, requiring more careful convergence analysis. Moreover, in RL it is generally assumed that the transition kernel and reward function are not known analytically--we only have access to sampled state transitions and cumulants. Thus, \(^{}\) cannot be represented or computed exactly, and instead we must study algorithms for approximating \(^{}\) from samples. We provide algorithms for resolving both of these concerns--the former in Section 5 and the latter in Section 6--where we illustrate the difficulties that arise once the cumulant dimension exceeds unity.

## 4 Particle-Based Multivariate Distributional Dynamic Programming

Our first algorithmic contribution is inspired by the empirically successful _equally-weighted particle_ (EWP) representations of multivariate return distributions employed by .

**Temporal-difference learning with EWP representations.** EWP representations, expressed by the class \(_{,m}\), are defined by

\[_{,m}=(_{,m}^{})^{},_{,m}^{}=\{_{i=1}^{m} _{_{i}}\;:\;_{i}^{d}\}.\] (2)

For simplicity, we consider the case here where at each state \(x\), the multi-return distribution is approximated by \(N(x)=m\) atoms. We can represent \(_{,m}\) by \((x)=_{i=1}^{m}_{_{i}(x)}\) for \(_{i}:^{d}\). The work of  introduced a TD-learning algorithm for learning a \(_{,m}\) representation of \(^{}\), computing iterates of the particles \((_{i}^{(k)})_{i=1}^{m}\) according to

\[_{i}^{(k+1)}(x)=_{i}^{(k)}(x)-_{k}_{_{i}(x)} _{}^{2}(_{i=j}^{m}_{_{j}^{(k )}(x)},_{j=1}^{m}_{r(x)+_{j}^{(k)} (X^{})})\] (3)

for step sizes \((_{k})_{k 0}\) and sampled next states \(X^{} P^{}( x)\), where \(=(^{(k)})\) is a copy of \(^{(k)}\) that does not propagate gradients. Despite the empirical success of this method in combination with deep learning, no convergence analysis has been established, owing to the nonconvexity of the MMD objective with respect to the particle locations. In this section we aim to understand to what extent analysis is possible for dynamic programming and temporal-difference learning algorithms based on the EWP representations in Equation (2).

**Dynamic programming with EWP representations.** As is often the case in approximate distributional dynamic programming , we have \(^{}_{,m}_{,m}\); in words, the distributional Bellman operator does not map EWP representations to themselves. Concretely, as long as there exists a state \(x\) at which the support of \(P^{}( x)\) is not a singleton, \((^{})(x)\) will consist of more than \(m\) atoms even when \(_{,m}\); and secondly, if \(P( x)\) is not uniform, \((^{})(x)\) will not consist of equally-weighted particles.

Consequently, to obtain a DP algorithm over EWP representations, we must consider a _projected_ operator of the form \(_{}^{}\), for a projection \(_{}:(^{d})^{X}_{,m}\). A natural choice for this projection is the operator that minimizes the MMD of each multi-return distribution in \(_{,m}\),

\[(_{,}^{m})(x)*{argmin}_{p _{,m}^{}}_{}(p,(x)).\] (4)

Unfortunately, even in the scalar-reward (\(d=1\)) case, the operator \(_{,}^{m}\) is problematic; \((_{,}^{m})(x)\) is not uniquely defined, and \(_{,}^{m}\) is not a non-expansion in \(_{}}\). These pathologies present significant complications when analyzing even the convergence of dynamic programming routines for learning an EWP representation of the multi-return distribution -- in particular, it is not even clear that \(_{,}^{m}^{}\) has a fixed point (let alone a unique one). Another complication arises due to the computational difficulty of computing the projection (4): even in the case where \((x)\) has finite support for each state \(x\), the projection \((_{,}^{m})(x)\) is very similar to clustering, which can be intractable to compute exactly for large \(m\). Thus, the argmin projection in Equation (4) cannot be used directly to obtain a tractable DP algorithm.

**Randomised dynamic programming.** Towards this end, we introduce a tractable _randomized dynamic programming_ algorithm for the EWP representation, by using a randomized proxy \(_{,m}^{}\) for \(_{,m}^{}\), that produces accurate return distribution estimates with high probability. The method produces the following iterates,

\[_{k+1}(x)=_{,m}^{}_{k}(x):=_{i =1}^{m}_{r(x)+ Z_{i}}, Z_{i}_{k}(X_{i}),\ X_{i} }}{{}}\ P^{}( x)\] (5)

A similar algorithm for categorical representations was discussed in concurrent work  without convergence analysis.

The intuition is that, particularly for large \(m\), the Monte Carlo error associated with the sample-based approximation to \((^{})(x)\) is small, and we can therefore expect the DP process, though randomised, to be accurate with high probability. This is summarised by a key theoretical result of this section; our proof of this result in the appendix provides a general approach to proving convergence for algorithms using arbitrary accurate approximations to (4) that we expect to be useful in future work.

**Theorem 3**.: _Consider a kernel \(\) induced by the semimetric \((x,y)=\|x-y\|_{2}^{}\) with \((0,2)\), and suppose rewards are bounded in each dimension within \([0,R_{}]\). For any \(_{0}\) such that \(_{}}(_{0},^{}) D<\), and any \(>0\), for the sequence \((_{k})_{k 0}\) defined in Equation (5), with probability at least \(1-\) we have_

\[_{}}(_{K},^{})( R_{}^{}}{(1-^{/2})(1-)^{} }(|^{-1}}{^{-}}) ).\]

_where \(_{k+1}=_{,m}^{}_{k}\) and \(K=}\), and where \(\) omits logarithmic factors in \(m\)._

This shows that our novel randomised DP algorithm with EWP representations can tractably compute accurate approximations to the true multivariate return distributions, with only polynomial dependence on the dimension \(d\). Appendix C illustrates explicitly how this procedure is more memory efficient than unprojected EWP dynamic programming. However, the guarantees associated with this algorithm hold only in high probability, and are weaker than the pointwise convergence guarantees of one-dimensional distributional DP algorithms . Consequently, these guarantees do not provide a clear understanding of the EWP-TD method described at the beginning of this section. However, in the sequel, we introduce DP and TD algorithms based on _categorical representations_, for which we derive dynamic programming and TD-learning convergence bounds.

The proof of Theorem 3 is hinges on the following proposition, which demonstrates that convergence of projected EWP dynamic programming is controlled by how far return distributions are transported under the projection map.

**Proposition 1** (Convergence of EWP Dynamic Programming).: _Consider a kernel satisfying the hypotheses of Theorem 2, suppose rewards are globally bounded in each dimension in \([0,R_{}]\), and let \(\{_{,m}^{(k)}\}_{k 0}\) be a sequence of maps \(:([0,(1-)^{-1}R_{}]^{d})^{}_ {,m}\) satisfying_

\[_{}((_{,m}^{(k)})(x),(x)) f(d,m)<  k 0.\] (6)

_Then the iterates \((_{k})_{k 0}\) given by \(_{k+1}=_{,m}^{(k)}^{}_{k}\) with \(_{}}(_{0},^{})=D<\) converge to a set \(_{,}^{m}(^{},(1- ^{c/2})^{-1}f(d,m))\) in \(_{}}\), where \(\) denotes the closed ball in \(_{}}\),_

\[(,R)\{^{}(^{d })^{}\,:\,_{}}(,^{}) R \}.\]

As an immediate corollary of Proposition 1 and Theorem 3, we can derive an error rate for projected dynamic programming with \(_{,}^{m}\) as well.

**Corollary 1**.: _For any kernel \(\) satisfying the hypotheses of Theorem 3, and for any \(_{0}_{,m}\) for which \(_{}}(_{0},^{}) D<\), the iterates \(_{k+1}=_{,}^{m}^{}_{k}\) converge to a set \(_{,}^{m}_{,m}\), where_

\[_{,}^{m}(^{ },R_{}^{}}{(1-^{/2})(1-)^{ }}).\]Categorical Multivariate Distributional Dynamic Programming

Our next contribution is the introduction of a convergent multivariate distributional dynamic programming algorithm based on a _categorical_ representation of return distribution functions, generalizing the algorithms and analysis of  to the multivariate setting.

**Categorical representations.** As outlined above, to model the multi-return distribution function in practice, it is necessary to restrict each multi-return distribution to a finitely-parameterized class. In this work, we take inspiration from successful distributional RL algorithms  and employ a _categorical representation_. The work of  proposed a categorical representation for multivariate DRL, but their categorical projection was not justified theoretically, and it required a particular choice of fixed support. We propose a novel categorical representation with a finite (possibly state-dependent) support \((x)=\{(x)_{i}\}_{i=1}^{N(x)}^{d}\), that models the multi-return distribution function \(\) such that \((x)_{(x)}\) for each \(x\). The notation \((x)_{i}\) simply refers to the \(i\)th support point at state \(x\) specified by \(\), and \(_{A}\) denotes the probability simplex on the finite set \(A\). We refer to the mapping \(\) as the _support map_2 and we denote the class of multi-return distribution functions under the corresponding categorical representation as \(_{}_{x}_{(x)}\).

**Categorical projection.** Once again, the distributional Bellman operator is not generally closed over \(_{}\), that is, \(^{}_{}_{}\). As such, we cannot actually employ the procedure described in Theorem 2 - rather, we need to project applications of \(^{}\) back onto \(_{}\). Roughly, we need an operator \(:(^{d})^{}_{}\) for which \(|_{_{}}=\). Given such an operator, as in the literature on categorical distributional RL , we will study the convergence of iterates \(_{k+1}=^{}_{k}\).

Projection operators used in the scalar categorical distributional RL literature are specific to distributions over \(\), so we must introduce a new projection. We propose a projection similar to (4),

\[(^{}_{,})(x)=*{arginf}_{p _{(x)}}_{}(p,(x)).\] (7)

We will now verify that \(^{}_{,}\) is well-defined, and that it satisfies the aforementioned properties.

**Lemma 1**.: _Let \(\) be a kernel induced by a semimetric \(\) on \([0,(1-)^{-1}R_{}]^{d}\) with strong negative type (cf. Theorem 1). Then \(^{}_{,}\) is well-defined, \((^{}_{,})_{ }\), and \(^{}_{,}|_{_{}}=\)._

It is worth noting that beyond simply ensuring the well-posedness of the projection \(^{}_{,}\), Lemma 1 also certifies an efficient algorithm for computing the projection -- namely, by solving the appropriate quadratic program (QP), as observed by . We demonstrate pseudocode for computing the projected Bellman operator \(^{}_{,}^{}\) with a QP solver \(\) in Algorithm 1.

```
0: Support map \(\), kernel \(\), transition kernel \(P^{}\), reward function \(r\), discount \(\)
0: Return distribution function \(_{}\) for\(x\)do \((^{})_{}_{x^{}}_{ (x^{})}P^{}(x^{} x)_{x^{}}()_ {r(x)+}\) \(K^{}_{i,j}(_{i},_{j})\) for each \((_{i},_{j})(x)^{2}\) \(q^{x}_{j}_{*{supp}{(^{})_{}} _{x}}(^{})_{x}()(_{j},)\) for each \(_{j}(x)\) \(p(_{p^{|(x)|}}[ p^{}K^{x}p-2p^{}q]p 0,\ _{i}p_{i}=1)\) \((^{}_{,}^{})_{x}_ {_{i}(x)}p_{i}_{_{i}}\) endfor return\(^{}_{,}^{}\) ```

**Algorithm 1** Projected Categorical Dynamic Programming

**Lemma 2**.: _Under the conditions of Lemma 1, \(^{}_{,}\) is a nonexpansion in \(_{}}\). That is, for any \(_{1},_{2}([0,(1-)^{-1}R_{}]^{d})^{ }\), we have \(_{}}(^{}_{,} _{1},^{}_{,}_{2})_{}}(_{1},_{2})\)._

**Categorical multivariate distributional dynamic programming.** As an immediate consequence of Lemma 2, it follows that projected dynamic programming under the projection \(^{}_{,}\) is convergent;this is because \(^{}\) is a contraction in \(_{}}\) and \(^{}_{,}\) is a nonexpansion in \(_{}}\), so the projected operator \(^{}_{,}^{}\) is a contraction in \(_{}}\); a standard invokation of the Banach fixed point theorem appealing to the completeness of \(_{}}\) certifies that repeated application of \(^{}_{,}^{}\) will result in convergence to a unique fixed point.

**Corollary 2**.: _Let \(\) be a kernel satisfying the conditions of Theorem 2. Then for any \(_{0}_{}\), the iterates \(\{_{k}\}_{k=1}^{}\) given by \(_{k+1}=^{}_{,}^{}_{k}\) converge geometrically to a unique fixed point._

Beyond the result of Theorem 2, Corollary 2 illustrates an algorithm for estimating \(^{}\) in \(_{}}\) provided knowledge of the transition kernel and the reward function, which is _computationally tractable_ in tabular MDPs. Indeed, the iterates \((_{k})_{k 0}\) all lie in \(_{}\), having finitely-many degrees of freedom. Algorithm 1 outlines a computationally tractable procedure for learning \(^{}_{,}\) in this setting.

We note that the work of  provided an alternative multivariate categorical algorithm, which was not analyzed theoretically. Moreover, our method provides the additional ability to support state-dependent arbitrary support maps, while theirs requires support maps to be uniform grids.

**Accurate approximations.** We now provide bounds showing that the fixed point \(^{}_{,}\) from Corollary 2 can be made arbitrarily accurate by increasing the number of atoms.

To derive a bound on the quality of the fixed point, we provide a reduction via partitioning the space of returns to the covering number of this space. Proceeding, we define a class of partitions \(_{(x)}\), where each \(P_{(x)}\) satisfies

1. \(|P|=N(x)\);
2. For any \(_{1},_{2} P\), either \(_{1}_{2}=\) or \(_{1}=_{2}\);
3. \(_{ P}=([0,(1-)^{-1}R_{}]^{d})\);
4. Each element \(_{i} P\) contains exactly one element \(z_{i}(x)\).

For any partition \(P\), we define a notion of _mesh size_ relative to a kernel \(\) induced by a semimetric \(\),

\[(P;)=_{ P}_{y_{1},y_{2}}(y_{ 1},y_{2}).\] (8)

The following result confirms that \(^{}_{,}\) recovers \(^{}\) as the mesh decreases.

**Theorem 4**.: _Let \(\) be a kernel induced by a \(c\)-homogeneous and shift-invariant semimetric \(\) conforming to the conditions of Theorem 2. Then the fixed point \(^{}_{,}\) of \(^{}_{,}^{}\) satisfies_

\[_{}}(^{}_{,},^{}) }_{x}_{P_{ (x)}}(P;)}.\] (9)

Thus, for any sequence of supports \(\{(x)_{m}\}_{m 1}\) for which the maximal space (in \(\)) between any two points in \((x)_{m}\) tends to \(0\) as \(m\), the fixed point \(^{}_{,}\) approximates \(^{}\) to arbitrary precision for large enough \(m\). The next corollary illustrates this in a familiar setting.

**Corollary 3**.: _Let \((x)=U_{m}\), where \(U_{m}\) is a set of \(m\) uniformly-spaced support points on \([0,(1-)^{-1}R_{}]\). For \(\) induced by the semimetric \((x,y)=\|x-y\|_{2}^{}\) for \((0,2)\),_

\[_{}}(^{}_{,},^{}) )(1-)^{/2}}R_{ }^{/2}}{(m^{1/d}-2)^{/2}}.\]

With \(=1\) and \(d=1\), the MMD in Corollary 3 is equivalent to the Cramer metric , the setting in which categorical (scalar) distributional dynamic programming is well understood. Our rate matches the known \((m^{-1/2})\) rate shown by  in this setting. Thus, our results offer a new perspective on categorical DRL, and naturally generalizes the theory to the multivariate setting.

Theorem 4 relies on the following lemma about the approximation quality of the categorical MMD projection, which may be of independent interest.

**Lemma 3**.: _Let \(\) be kernel satisfying the conditions of Lemma 1, and for any finite \(^{d}\), define \(:(^{d})_{}\) via \( p=*{arginf}_{q_{}}_{} (p,q)\). Then \(_{}^{2}( p,p)_{P_{}} (P;)\)._

At this stage, we have shown definitively that categorical dynamic programming converges in the multivariate case. In the sequel, we build on these results to provide a convergent multivariate categorical TD-learning algorithm.

### Simulation: The Distributional Successor Measure

As a preliminary example, we consider 3-state MDPs with the cumulants \(r(i)=(1-)e_{i}\), \(i\) for \(e_{i}\) the \(i\)th basis vector. In this setting, \(^{}\) encodes the distribution over trajectory-wise discounted state occupancies, which was discussed in the recent work of  and called the _distributional successor measure_ (DSM). Particularly,  showed that \(x(G_{x}^{})\) for \(G_{x}^{}(x)\) is the return distribution function for any scalar reward function \(\). Figure 1 shows that the projected categorical dynamic programming algorithm accurately approximates the distribution over discounted state occupancies as well as distributions over returns on held-out reward functions.

## 6 Multivariate Distributional TD-Learning

Next, we devise an algorithm for approximating the multi-return distribution function when the transition kernel and reward function are not known, and are observed only through samples. Indeed, this is a strong motivation for TD-learning algorithms , wherein state transition data alone is used to solve the Bellman equation. In this section, we devise a TD-learning algorithm for multivariate DRL, leveraging our results on categorical dynamic programming in \(_{}}\).

**Relaxation to signed measures.** In the \(d=1\) setting, the categorical projection presented above is known to be affine , making scalar categorical TD-learning amenable to common techniques in stochastic approximation theory. However, the projection is _not_ affine when \(d 2\); we give an explicit example in Appendix D. Thus, we relax the categorical representation to include _signed_ measures, which will provide us with an affine projection --this is crucial for proving our main result, Theorem 6. We denote by \(^{1}()\) the set of all signed measures \(\) over \(\) with \(()=1\). We begin by noting that the MMD endows \(^{1}()\) with a metric structure.

**Lemma 4**.: _Let \(:\) be a characteristic kernel over some space \(\). Then \(_{}:^{1}()^{1}( )_{+}\) given by \((p,q)\|_{p}-_{q}\|_{}\) defines a metric on \(^{1}()\), where \(_{p}\) denotes the usual mean embedding of \(p\) and \(\) is the RKHS with kernel \(\)._

We define the relaxed projection \(_{,}^{}:^{1}([0,(1-)^{-1}R_{ }]^{d})^{}_{x}^{1}((x))=:_{}\),

\[(_{,}^{})(x)*{ arginf}_{p^{1}((x))}_{}(p,(x)).\] (10)

Note that (10) is very similar to the definition of the categorical MMD projection in (7)--the only difference is that the optimization occurs over the larger class of signed mass-1 measures. It is also worth noting that the distributional Bellman operator can be applied directly to signed measures, which yields the following convenient result.

**Lemma 5**.: _Under the conditions of Corollary 2, the projected operator \(_{,}^{}^{}:_{ }_{}\) is affine, is contractive with contraction factor \(^{c/2}\), and has a unique fixed point \(_{,}^{}\)._

While we have "relaxed" the projection, the fixed point \(_{,}^{}\) is a good approximation of \(^{}\).

**Theorem 5**.: _Under the conditions of Lemma 5, we have that_

Figure 1: Distributional SMs and associated predicted return distributions with the categorical (left) and EWP (right) representations. Simplex plots denote the distributional SM. Histograms denote the associated return distributions, predicted from a pair of held-out reward functions.

1. \(_{}}(^{}_{,},^{}) }_{x}_{P_{ (x)}}(P;)}\)_; and_
2. \(_{}}(^{}_{,} ^{}_{,},^{})(1+} )_{x}_{P_{(x)}}(P;)}\)_._

Notably, the second statement of Theorem 5 states that projecting \(^{}_{,}\) back onto the space of multi-return distribution functions yields approximately the same error as \(^{}_{,}\) when \(\) is near \(1\).

In the remainder of the section, we assume access to a stream of MDP transitions \(\{T_{t}\}_{t=1}^{}^{ d}\) consisting of elements \(T_{t}=(X_{t},A_{t},R_{t},X_{t}^{})\) with the following structure,

\[X_{t}(_{t-1}) A_{t}( X _{t}) R_{t}=r(X_{t}) X_{t}^{} P( X_{t},A_{t})\] (11)

where \(\) is some probability measure and \(\{_{t}\}_{t=1}^{}\) is the canonical filtration \(_{t}=(_{t=1}^{t}T_{t})\). Based on these transitions, we can define stochastic distributional Bellman backups by

\[}_{t}^{}(x)=(_{R_{t}, })_{}(X_{t}^{})&x=X_{t}\\ (x)&,\] (12)

which notably can be computed exactly without knowledge of \(P,r\). Due to the stronger convergence guarantees shown for projected multivariate distributional dynamic programming, we introduce an asynchronous incremental algorithm leveraging the categorical representation, which produces iterates \(\{_{t}\}_{t=1}^{}\) according to

\[_{t+1}=(1-_{t})_{t}+_{t}^{}_{,}}_{t}^{}_{t}\] (13)

for \(_{0}_{}\), where \(\{_{t}\}_{t=1}^{}\) is any sequence of (possibly) random step sizes adapted to the filtration \(\{_{t}\}_{t=1}^{}\). The iterates of (13) closely resemble those of classic stochastic approximation algorithms  and particularly asynchronous TD learning algorithms , but with iterates taking values in the space of state-indexed signed measures. Indeed, our next result draws on the techniques from these works to establish convergence of TD-learning on \(_{}\) representations.

**Theorem 6**.: _For a kernel \(\) induced by a semimetric \(\) of strong negative type, the sequence \(\{_{t}\}_{t=1}^{}\) given by (11)-(13) converges to \(^{}_{,}\) with probability 1._

### Simulations: Distributional Successor Features

To illustrate the behavior of our categorical TD algorithm, we employ it to learn the multi-return distributions for several tabular MDPs with random cumulants. We focus on the case of \(2\)- and \(3\)-dimensional cumulants, which is the setting studied in recent works regarding multivariate distributional RL . Interpreting the multi-return distributions as joint distributions over successor features , SFs, we additionally evaluate the return distributions for random reward functions in the span of the cumulants. We compare our categorical TD approach with a tabular implementation of the EWP TD algorithm of , for which no convergence bounds are known.

Figure 1(a) compares TD learning approaches based on their ability to accurately infer (scalar) return distributions on held out reward functions, averaged over 100 random MDPs, with transitions drawn

Figure 2: Error of zero-shot return distribution predictions over random MDPs, measured by Cramér distance, and showing 95% confidence intervals.

from Dirichlet priors and \(2\)-dimensional cumulants drawn from uniform priors. The performance of the categorical algorithms sharply increases as the number of atoms increases. On the other hand, the EWP TD algorithm performs well with few atoms, but does not improve very much with higher-resolution representations. We posit this is due to the algorithm getting stuck in local minima, given the non-convexity of the EWP MMD objective. This hypothesis is supported as well by Figure 3, which depicts the learned distributional SFs and return distribution predictions.

Particularly, we observe that the learned particle locations in the EWP TD approach tend to cluster in some areas or get stuck in low-density regimes, which suggests the presence of a local optimum. On the other hand, our provably-convergent categorical TD method learns a high fidelity quantization of the true multi-return distributions.

Naturally, however, the benefits of the \((d)\) bounds for EWP suggested by Theorem 3 become more present as we increase the cumulant dimension. Figure 1(b) repeats the experiment of Figure 1(a) with \(d=3\), using randomized support points for the categorical algorithm to avoid a cubic growth in the cardinality of the supports. Notably, our method is the first capable of supporting such unstructured supports. While the categorical TD approach can still outperform EWP, a much larger number of atoms is required. This is unsurprising in light of our theoretical results.

## 7 Conclusion

We have provided the first provably convergent and computationally tractable algorithms for learning multivariate return distributions in tabular MDPs. Our theoretical results include convergence guarantees that indicate how accuracy scales with the number of particles \(m\) used in distribution representations, and interestingly motivate the use of signed measures to develop provably convergent TD algorithms.

While it is difficult to scale categorical representations to high-dimensional cumulants, our algorithm is highly performant in the low \(d\) setting, which has been the focus of recent work in multivariate distributional RL. Notably, even the \(d=2\) setting has important applications--indeed, efforts in safe RL depend on distinguishing a cost signal from a reward signal (see, e.g., ), which can be modeled by bivariate distributional RL. In this setting, our method can easily be scaled to large state spaces by approximating the categorical signed measures with neural networks; an illustrated example is given in Appendix F.

On the other hand, the prospect of learning multi-return distributions for high-dimensional cumulants also has many important applications, such as modeling close approximations to distributional successor measures  for zero-shot risk-sensitive policy evaluation. In this setting, we believe EWP-based multivariate DRL will be highly impactful. Our results concerning EWP dynamic programming provide promising evidence that the accuracy of EWP representations scales gracefully with \(d\) for a fixed number of atoms. Thus, we believe that understanding convergence of EWP TD-learning algorithms is a very interesting and important open problem for future investigation.

Figure 3: Distributional SFs and predicted return distributions with \(m=400\) atoms, in a random MDP with known rectangular bound on cumulants. **Left**: Categorical TD. **Right**: EWP TD.