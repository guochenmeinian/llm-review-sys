# FairLISA: Fair User Modeling with Limited Sensitive Attributes Information

Zheng Zhang\({}^{1,2}\) Qi Liu\({}^{1,2}\) Hao Jiang\({}^{1,2}\) Fei Wang\({}^{1,2}\) Yan Zhuang\({}^{1,2}\)

Le Wu\({}^{3}\) Weibo Gao\({}^{1,2}\) Enhong Chen\({}^{1,2}\)

1: Anhui Province Key Laboratory of Big Data Analysis and Application

University of Science and Technology of China

2: State Key Laboratory of Cognitive Intelligence

3: Hefei University of Technology

{zhangzheng,jianghao0728,wf314159,zykb,weibogao}@mail.ustc.edu.cn; {qiliuql,cheneh}@ustc.edu.cn;

lewu.ustc@gmail.com

Corresponding Author

###### Abstract

User modeling techniques profile users' latent characteristics (e.g., preference) from their observed behaviors, and play a crucial role in decision-making. Unfortunately, traditional user models may unconsciously capture biases related to sensitive attributes (e.g., gender) from behavior data, even when this sensitive information is not explicitly provided. This can lead to unfair issues and discrimination against certain groups based on these sensitive attributes. Recent studies have been proposed to improve fairness by explicitly decorrelating user modeling results and sensitive attributes. However, most existing approaches assume that fully sensitive attribute labels are available in the training set, which is unrealistic due to collection limitations like privacy concerns, and hence bear the limitation of performance. In this paper, we focus on a practical situation with limited sensitive data and propose a novel FairLISA framework, which can efficiently utilize data with known and unknown sensitive attributes to facilitate fair model training. We first propose a novel theoretical perspective to build the relationship between data with both known and unknown sensitive attributes with the fairness objective. Then, based on this, we provide a general adversarial framework to effectively leverage the whole user data for fair user modeling. We conduct experiments on representative user modeling tasks including recommender system and cognitive diagnosis. The results demonstrate that our FairLISA can effectively improve fairness while retaining high accuracy in scenarios with different ratios of missing sensitive attributes.

## 1 Introduction

User modeling is a fundamental task in many applications (e.g., recommender system), which aims to infer latent characteristics through analyzing users' behavioral information, such as capability and preference . The results from user modeling serve as an important basis for decision-making, which might affect users' lives to some extent . However, recent studies have demonstrated that biases related to sensitive attributes (e.g., gender, race) can be unconsciously captured from behavior data, even when this sensitive information is not explicitly provided. This can lead to unfair user modeling results . For example, college admissions based on user modeling may underestimate underrepresented demographic groups . Career recommendation shows apparent gender discrimination even for equally qualified men and women . As such, it is crucial to consider fairness issues and ensure that users with different sensitive attributes are treated similarly.

Following previous studies [3; 23; 45], the fairness requirement for user modeling in this paper is that the results should not expose any sensitive information. This requirement can be formalized as minimizing the mutual information between user modeling results and sensitive attributes . Along this way, many approaches have been proposed, such as constraint optimization , adversarial learning [3; 23; 45; 20] and regularization methods [49; 42], among which adversarial methods show their theoretical elegance and receive widespread attention.

While many methods have shown promising results, most of them require explicit sensitive attributes to guide fair model training. However, in real-world scenarios, users are not always willing to share sensitive information due to privacy concerns. For example, only 14% of teen users expose their complete profiles on Meta . The limited sensitive attributes situation means that only a limited amount of data with known sensitive attributes can participate in model training, resulting in insufficient training data problem for existing methods. The most practical solution is to utilize data with unknown sensitive attributes to facilitate model training. Nevertheless, several obstacles must be addressed, including: 1) **Efficient data utilization**: Currently, some researchers have utilized data with unknown sensitive attributes by predicting missing sensitive labels based on data with known sensitive attributes . However, the accuracy of these predicted labels cannot be guaranteed, especially in extremely limited sensitive situations, which results in suboptimal data utilization. 2) **Theoretical guarantee**: The fair goal in our paper is to minimize the mutual information between the sensitive attributes and the user modeling results. When we utilize data with unknown sensitive attributes to improve fair user modeling, how can we establish a good theoretical guarantee? 3) **Framework generalization**: Several fair adversarial methods have been introduced to handle data with known sensitive attributes [3; 23; 45; 20]. Each method has its advantages and disadvantages in different scenarios. In this case, we believe that a general framework should be proposed that can incorporate data with unknown sensitive attributes into existing fair adversarial methods.

To address these obstacles, in this paper, we propose FairLISA, a general framework for _Fair user modeling with LUnited Sensitive Attributes_, which can efficiently utilize data with known and unknown sensitive attributes to facilitate fair model training. We first propose a novel theoretical perspective to build the relationship between two types of data and the fair mutual information objective. Specifically, we establish a connection between data with unknown sensitive attributes and the fairness goal through shannon entropy, while data with known sensitive attributes and the fairness goal are connected through cross-entropy. By doing so, we can directly leverage user data with unknown sensitive attributes without the need for predicting missing attributes, reducing information loss caused by predictions and ultimately increasing the utilization rate of data with unknown sensitive attributes. Then building on this foundation, we propose an adversarial learning approach to effectively leverage the entire user data for fair model training. Since existing fair adversarial approaches requiring fully known sensitive data can be interpreted from a mutual information perspective [3; 23; 45], FairLISA can easily generalize most existing adversarial methods and expand them to limited sensitive attribute situations. Finally, to validate the effectiveness of our proposed FairLISA framework, we conduct extensive experiments on two representative user modeling tasks including the recommender system and cognitive diagnosis. Empirical results show that our framework can enhance fairness with satisfactory accuracy performance, even in scenarios with different ratios of missing sensitive attributes. The main contributions of this work are as follows:

* _Efficient Data Utilization and Theoretical Guarantee._ We provide a novel theoretical perspective to build the relationship between data with known and unknown sensitive attributes with fairness objective. This allows for the effective leveraging of data with unknown sensitive attributes, reducing information loss caused by predicting missing attributes.
* _Framework Design and Generalization._ We propose a general framework, FairLISA, which can expand most existing fair adversarial methods to limited sensitive situations.
* _Experimental Evaluations._ We conduct experiments in two representative user modeling tasks (i.e., recommender system, cognitive diagnosis) with six representative models and two datasets, the results demonstrate the effectiveness and robustness of FairLISA.

## 2 Related work

User Modeling.User modeling is a fundamental task that captures users' useful potential characteristics, such as capability, preference, and so on [44; 56; 28; 50]. It has been applied in various applications. For example, based on user capability fitting, researchers employed cognitive diagnosis [30; 9] to model student proficiency [4; 30]; based on user preference mining, researchers applied collaborative filtering  for mining user interest preferences to different tasks including news recommendation , social network . Most of the existing user modeling methods focused on getting more precise results [4; 30; 16; 15; 27; 29]. However, as the results directly affect the opportunity of users [32; 54; 55], it can easily lead to unfair outcomes. In this paper, we explore fairness-aware user modeling, which aims to ensure fairness during the user modeling process.

Fairness in Machine Learning.Fairness in machine learning can be divided into three categories: (1) individual fairness, which requires similar individuals to be treated similarly [7; 6]; (2) group fairness, which requires different groups divided by sensitive attributes should be treated similarly [35; 7; 13]; (3) max-min fairness, which aims to maximize the utility of worst-off group . In this work, we focus on group fairness since it can measure how the underrepresented group is treated.

In group fairness research, many fairness metrics and unfairness mitigation techniques had been proposed [22; 36; 45; 38; 47; 3; 23; 43; 10; 21]. For example, Avishek et al.  discovered the learned user representations would capture biases of sensitive information and proposed to minimize the mutual information between representations and sensitive attributes through an adversarial framework. Li et al.  also proposed this goal from the causal perspective and provided two similar adversarial strategies. Wu et al.  argued the user-centric structure would expose users' sensitive attributes and proposed an adversarial framework from a graph perspective to obtain more efficient results.

Despite their ability to improve fairness, all of these methods are based on enough user sensitive attribute information. While in many real-world applications, it is difficult to collect such data since users may care about privacy and are reluctant to divulge their sensitive information . The lacking of sensitive information challenges most existing solutions. Exploring fair models with missing sensitive attributes is an important and challenging issue, and it is still under-explored. There are only a few works in this direction. One branch of approaches investigates fairness without sensitive attributes. For example, some researchers achieved this via solving a max-min problem [25; 14; 18]. However, they were only effective for max-min fairness , which was different from our group fairness goal. Some researchers tried to solve this problem with the help of related features [51; 53]. However, obtaining such related features is not always feasible in practical applications, thereby limiting the utilization of these methods. The other branch explores improving fairness in limited sensitive attribute situations, Dai et al.  proposed FairGNN, the main idea was to predict missing sensitive attribute labels. However, their reliance on the accuracy of the predicted pseudo labels resulted in suboptimal data utilization, particularly in extremely limited sensitive situations. Different from previous papers, we explore group fairness in limited sensitive attribute situations and can efficiently utilize data with known sensitive attributes without predicting missing sensitive attributes.

## 3 Problem Definition

In user modeling, we denote \(U=\{u_{i}\}_{i=1}^{N_{u}}\) and \(V=\{v_{i}\}_{i=1}^{N_{v}}\) as the user set and the item set, respectively. The interactions between users and items generate interaction records \(R\), whose element is denoted as triplet \((u,v,r)\) where \(u U\), \(v V\), and \(r\) is the corresponding interaction decided by the corresponding task. As we have assumed that sensitive attributes are only available for a limited number of users, we split the user set \(U\) into two disjoint sets as \(U_{L}=\{u_{i}\}_{i=1}^{l}\) and \(U_{N}=\{u_{i}\}_{i=i+1}^{l+n}\) so that the sensitive label \(s\) is only accessible for each user in \(U_{L}\) instead of \(U_{N}\). Let \(S\) be the sensitive attribute that takes value in \(\), whose values for users in \(U_{L}\) are available, i.e. \(\{s_{i}|s_{i}\}_{i=1}^{l}\). Our goal is to learn a user model \(f\) that generates a fair representative characteristic \(\) for each user \(u\). Following previous studies [3; 21; 42], the fairness requirement for user modeling is that the generated outcome should not expose any sensitive information, which can be depicted as the result of zero mutual information between the user's generated characteristic \(\) and sensitive attribute \(S\), i.e. \(I(,S)=0\). In this way, we have the following problem definition:

**Problem 1**.: _Given interaction records \(R\), users \(U\), items \(V\) and the sensitive label set \(\{s_{i}\}_{i=1}^{l}\) for \(U_{L} U\), the goal is to learn a user model \(f:U\) that generates a **fair** representative characteristic \(\) for each user \(u U\), such that \(I(,S)=0\) where \(S\) is the corresponding sensitive attribute._FairLISA

### Overview

FairLISA is an adversarial learning-based framework designed to promote fair model training by efficiently utilizing data with known and unknown sensitive attributes. The architecture is shown in Figure 1. It consists of two modules including a filter \(\) and a discriminator \(\). Specifically, for a given user with the original representation \(\) generated by any user models, we use the filter \(\) trained on data with known sensitive attribute \(U_{L}\) and data with unknown sensitive attribute \(U_{N}\) to filter out the information of sensitive attributes. The filtered user modeling result of a user \(u\) is denoted as \(=()\). To guide the filter \(\) training, we use the idea of adversary learning to train a discriminator \(\). The goal of the discriminator \(\) is to fail the filter \(\), which is achieved by predicting the corresponding sensitive attribute from the filtered user representations \(\). Through adversarial training between \(\) and \(\), we can get a filter to remove the effect of sensitive attributes. Unlike traditional adversarial methods [3; 23; 45] that only rely on \(U_{L}\), FairLISA can train a filter with \(U_{L}\) and \(U_{N}\), i.e., \(=_{U_{L}+U_{N}}()\). Following the previous study , we assume the distribution of users' filtered representations in \(U_{L}\) and \(U_{N}\) to be the same and use \(p()\) to represent them. We will introduce how FairLISA achieves the fairness goal (i.e., \(I(,S)=0\)) through \(U_{L},U_{N}\) respectively and how to utilize these two types of data to form the final architecture. For simplicity of theoretical analysis, we assume there is one sensitive attribute \(S\) (e.g., gender) and \(s\) (e.g., female) is an attribute value of \(S\).

### Fairness with Known Sensitive Attributes

We will begin by explaining how FairLISA achieves \(I(,S)=0\) with data with known sensitive attributes \(U_{L}\), i.e., \(=_{U_{L}}()\). More specifically, we will describe how to establish a connection between \(U_{L}\) and \(I(,S)\), as well as how to leverage data with known sensitive attributes to minimize \(I(,S)\) based on this connection. In this regard, we define \(_{s}()\) as the probability that the discriminator \(\) recognizes the value of the sensitive attribute \(S\) as \(s\) based on \(\), \(H\) denotes the shannon entropy . According to the following lemma, mutual information \(I(,S)\) can be expressed as the optimization function shown below, when the optimal discriminator can achieve \(_{s}^{*}()=p(s|)\).

**Lemma 4.1**.: _Given the optimal discriminator \(_{s}^{*}()\) and let \(p(|s)\) be the distribution of users' filtered representations whose sensitive attribute value is \(s\), then \(I(,S)\) can be derived as,_

\[I(,S)=_{s}p(s)_{ p(|s) }[_{s}^{*}()]+H(S).\] (1)

The detailed proof is provided in Appendix A.1. We have established a connection between \(U_{L}\) and \(I(,S)\) through Lemma A.1. Consequently, fairness can be achieved by minimizing Eq. (11) with a fixed optimal discriminator \(^{*}\). It is important to note that estimating \(p(|s)\) in Eq. (11) requires access to sensitive information. This implies that leveraging \(U_{N}\) (data with unknown sensitive attributes) to enhance fairness becomes impossible. However, in real-world applications, users are often concerned about privacy and may be unwilling to disclose their sensitive information. The limited availability of sensitive attributes means that only a restricted amount of data with known

Figure 1: The architecture of FairLISA. For a given user with the original representation \(\) generated by any user models, we first use the filter \(\) trained on \(U_{L}\) and \(U_{N}\) to remove the sensitive information and get the filtered representation \(\). Then we use the discriminator \(\) trained on \(U_{L}\) to predict the sensitive feature of \(\). Through adversarial training and the better utilization of \(U_{N}\), we can get a better filter to remove sensitive information.

sensitive attributes can participate in model training, leading to an insufficient training data problem for existing methods. In the following sections, we will introduce how to alleviate this problem by effectively utilizing data with unknown sensitive attributes.

### Fairness with Unknown Sensitive Attributes

An intuitive approach to leverage data with unknown sensitive attributes \(U_{N}\) is to predict the missing sensitive labels based on \(U_{L}\). However, the accuracy of these predicted pseudo-labels cannot be guaranteed, which results in suboptimal data utilization. Therefore, it is crucial to explore new methods that fully harness the potential of \(U_{L}\) to enhance fairness. Given that the suboptimal data utilization stems from predicting missing attributes, a natural idea is to directly employ \(U_{N}\) without predicting missing attributes, which can avoid information loss caused by prediction and maximize the utilization of \(U_{N}\) to a greater extent. To achieve this objective, it is necessary to directly establish a relationship between \(I(,S)\) and \(U_{N}\). Drawing inspiration from Lemma A.1 and structural insights related to the KL divergence , we conclude that shannon entropy \(H\) can build this relationship.

**Lemma 4.2**.: _Given the optimal discriminator \(^{*}_{s}()\) and let \(p()\) be the distribution of users' filtered representation, we have,_

\[ I(,S)&=_{ p()}_{s}[^{*}_{s}() ^{*}_{s}()]+H(S)\\ &=-H(^{*}_{s}())+H(S).\] (2)

Proof.: According to the relationship between mutual information and JS divergence, the relationship between JS divergence and KL divergence , we have

\[I(,S)=JS_{p(s)}(p(|s))=_{s}p(s)KL(p(|s) ||p()).\] (3)

By calculating KL divergence, we get,

\[_{s}p(s)KL(p(|s)||p()=_{s}p(s)p(|s)|s)}{p()}d.\] (4)

Then we expand the formula and use bayes' theorem,

\[&_{s}p(s)p(|s)|s)}{p()}d\\ &= p()_{s}p(s|) p(s|)d+H(S).\] (5)

Noted that \(p(s|)\) is exactly \(^{*}\), we replace it in the formula, and get the result,

\[& p()_{s}p(s|) p(s| )d+H(S)\\ &=-H(^{*}())+H(S).\] (6)

Here we give the key steps of proof, more details are provided in Appendix A.2. In Lemma A.2, the term \(p(|s)\) disappears, removing the obstacle that hindered the direct utilization of \(U_{N}\) as discussed in Lemma A.1. Thus, we have successfully established the relationship between \(I(,S)\) and \(U_{N}\) through Lemma A.2. We also provide an intuitive illustration. By maximizing the entropy of the discriminator, we can ensure that the prediction probability of sensitive attributes becomes uniform. For example, in the case of gender prediction, both male and female probabilities are set to 0.5. This shows that the discriminator becomes unable to predict the sensitive features from the learned user embedding. Consequently, we can minimize our fairness criterion \(I(,S)\) directly by minimizing Eq. (15) using \(U_{N}\). This allows for the effective utilization of \(U_{N}\) to enhance fairness without the need to predict unknown sensitive attribute labels.

### Architecture

In this section, we now introduce how to combine \(U_{L}\) and \(U_{N}\) together to form the final architecture (shown in Figure 1) in order to obtain a higher improvement of fairness, i.e., \(=_{U_{L}+U_{N}}()\). By leveraging the insights from Lemma A.1 and Lemma A.2, we have established the connection between \(U_{L}\), \(U_{N}\), and the fairness goal \(I(,S)\). Thus, fairness can be achieved through minimizing Eq. (11) with \(U_{L}\) and minimizing Eq. (15) with \(U_{N}\) with a fixed optimal discriminator \(^{*}\). To achieve this goal, we first optimize the \(\) to the best, then fix \(\) to optimize \(\). FairLISA consists of a filter and a discriminator. We will introduce them respectively as well as the training algorithm.

Filter.We use the whole data to optimize the \(\). For data with known sensitive attributes, we optimize \(\) by minimizing the right side of Eq. (11). The empirical estimate can be computed as:

\[_{L}=_{s}_{1 i l:s_{i}=s} _{s}(_{i}),\] (7)

where \(_{i}\) is the filtered representation of \(u_{i}\). Here we omit \(H(S)\) in Eq. (11) since it is a constant.

For data with unknown sensitive attributes, we optimize \(\) by minimizing the right side of Eq. (15). The empirical estimate can be computed as:

\[_{N}=_{l+1 i l+n}_{s}_{s}( _{i})(_{s}(_{i})),\] (8)

where \(_{i}\) is the filtered representation of \(u_{i}\). Here we also omit \(H(S)\) since it is a constant.

Furthermore, in addition to the fairness requirement, we need to maintain users' useful potential characteristics for different user modeling tasks (e.g., recommender system, cognitive diagnosis). To achieve this, we use a task-specific loss function to mine useful information of users. With the combination of Eq. (7) and Eq. (8), we get the following loss function:

\[_{}_{}=_{1}_{task}+ _{2}_{L}+_{3}_{N},\] (9)

where \(_{task}\) represents the loss of specific user modeling task, \(_{1}\), \(_{2}\) and \(_{3}\) are hyperparameters. We study the influence of these hyperparameters in the experiment section (RQ3).

Discriminator.To guide the filter \(\) training, the goal of the discriminators is to predict the corresponding sensitive attribute from the filtered user representations \(\). Therefore, for the loss function of \(\), we optimize the discriminator with data \(U_{L}\) using the following cross-entropy:

\[_{}_{}=-_{s}_{1 i  l:s_{i}=s}_{s}(_{i}),\] (10)

where \(_{i}\) is the filtered representation of \(u_{i}\). Here we omit \(H(S)\) in Eq. (11) since it is a constant..

Training Algorithm.Since the discriminator can not be optimized to the best directly, following the traditional adversary learning training methods , we adopt mini-batch training in implementation. Specifically, 1) \(T\) mini-batch updates minimizing \(_{F}\) with the \(\) fixed; 2) \(T\) mini-batch updates minimizing \(_{D}\) with the \(\) fixed. Here \(T\) = 10 in our implementation. The whole optimization algorithm for FairLISA is shown in Appendix B.

### Relation to Existing Adversarial Methods

In this paper, we provide a novel mutual information perspective to interpret the strategies for achieving fairness in existing adversarial methods, and unify the treatment of data with known and unknown sensitive attributes. As a result, FairLISA can easily generalize most existing fairness-aware adversarial methods with data with unknown sensitive attributes. For example, if we set \(_{3}\) in Eq. (9) to 0, which means ignoring the data in \(U_{N}\), FairLISA degenerates to ComFair , a classical fairness-aware adversarial method. Moreover, our fairness mutual information objective is compatible with existing adversarial methods and can theoretically expand them to the limited sensitive attribute situation. For example, if we combine Eq. (8) from FairLISA with FairGo , a state-of-the-art fair model which considers graph structure, we could enhance its fairness performance. We will show the performance in experiments (denoted as FairGo+FairLISA).

Experiments

In this section, we first introduce the dataset and experimental setup. Then, we conduct extensive experiments to answer the following questions:

* **RQ1**: Does FairLISA outperform the fairness-aware baselines in limited sensitive situations?
* **RQ2**: Can FairLISA achieve more robust fairness results on different missing ratios?
* **RQ3**: How do the data with known and unknown sensitive attributes impact the results?
* **RQ4**: How is the performance of FairLISA on several classical group fairness metrics?

The code is released at https://github.com/zhengz99/FairLISA.

### User Modeling Tasks

To explore fairness in user modeling with limited sensitive information, we choose two representative issues in real-world user modeling scenarios. The first is user capability modeling in areas such as education which is called cognitive diagnosis . The user \(u\), item \(v\) in our general user modeling refer to student and exercise correspondingly, and interaction result \(r\) is the score that the student got on exercise. The goal is to model the capability of students. To achieve this, we train models on student performance prediction task. We choose several representative models: IRT , MIRT , NCD . The second is user preference modeling in recommender system . The user \(u\), item \(v\) here refer to the customer and product respectively. The interaction result in \(r\) is the user evaluation of the product, such as rating behavior. By user rating prediction task, we can mine user preference. We choose several representative models: PMF , NCF , LightGCN . Detailed basic user model descriptions will be shown in Appendix C.1.

### Dataset Description

We use two real-world datasets to verify the effectiveness and robustness of FairLISA. Specifically, we use PISA20152 for cognitive diagnosis and MovieLens-1M3 for a recommender system. Detailed descriptions can be found in Appendix C.2. Both of these datasets are publicly available and do not contain any personally identifiable information. In our experiments, we split each dataset into training, validation, and testing sets, with a ratio of 7:1:2. All baseline models were trained and evaluated using the same datasets.

### Experimental Setup and Baselines

Evaluation.The evaluation of our methods can be divided into accuracy evaluation and fairness evaluation. For accuracy evaluation, we select different accuracy metrics for different tasks. For cognitive diagnosis, following previous works from cognitive diagnosis [41; 8], we adopt different metrics from the perspectives of both regression (MAE, RMSE) and classification (AUC, ACC). For recommender system task, we adopt RMSE.

For fairness evaluation, our fairness goal refers to user modeling results do not leak any user's sensitive information. To measure the fairness performance, following the settings in fair user modeling such as [3; 23; 45], we train an attacker who has the same structure as the discriminator. Specifically, after training our framework, we use the filtered user representations as input and the corresponding sensitive attributes as labels to train the attacker. If the attacker can distinguish sensitive features from the user representations, it indicates that the sensitive information is exposed by user modeling results. To train and evaluate attackers, we split the data into train (80%) and test sets (20%) and calculate the AUC score and apply their macro-average to make the result insensitive to imbalanced data. The smaller values of AUC denote better fairness performance with less sensitive information leakage, which be used as the unfairness metric in the following experiments.

Implementation detail.For task-specific loss \(_{task}\) in Eq.(9), we use cross-entropy loss for cognitive diagnosis tasks and MSE loss for recommendation tasks. For all datasets and models, we set the learning rate to 0.001 and the dropout rate to 0.2. We select the best model based on its original performance on the valid set and use it to conduct the fairness test. We set adversarial coefficient \(_{1}=1\), \(_{2}=2\), \(_{3}=1\) for cognitive diagnosis, and \(_{1}=1\), \(_{2}=20\), \(_{3}=10\) for recommendation tasks. For the adversarial architecture, the filter module is two-layer perceptrons and we use ReLU as the activation function. The discriminators and attackers are three-layer perceptrons with the activation function of LeakyReLU. We set the dropout rate to 0.1 and the slope of the negative section for LeakyReLU to 0.2 for them. We implement all models with PyTorch and conduct all experiments on four 2.0GHz Intel Xeon E5-2620 CPUs and a Tesla K20m GPU.

Baseline approaches.To validate the effectiveness of FairLISA, we compare FairLISA with the following baselines. Origin (i.e., basic user models without fairness consideration), ComFair , FairGo , FairGNN . Further, to validate the generalizability of FairLISA, we expand FairGo to FairGo+FairLISA (introduced in section 4.5). A detailed description will be shown in Appendix C.3.

### Experimental Results

Overall results (RQ1).The fairness and accuracy results of the recommendation task are shown in Table 1. The results of cognitive diagnosis are shown in Appendix C.4. We have several observations.

* From the perspective of model fairness, the AUC of attackers on origin user models is significantly higher than 0.5, which means that the sensitive information is exposed by user modeling results even if such information is not explicitly used. Further, models which can simultaneously apply data with known and unknown sensitive information (i.e., FairGNN, FairLISA, FairG+FairLISA) get better fairness performance. It shows that data with unknown sensitive attributes benefit fair user modeling. More importantly, we find our methods (i.e., FairLISA, FairGo+FairLISA) outperform others, suggesting the effectiveness of our work.
* From the perspective of model accuracy, all fairness-improving methods lose some accuracy compared with origin user models. The reason is that the fairness-aware methods are aiming at filtering out the information of certain sensitive features from user modeling results, which will to some extent reduce the information contained, thus decreasing accuracy . Among these fairness-aware methods, we could find FairLISA, FairGo+FairLISA have the best accuracy performance, and they could even maintain comparable accuracy with origin models in some cases.

Performance on different ratios of missing sensitive attributes (RQ2).In this part, we study the impacts of sensitive attributes missing ratios and investigate the robustness of FairLISA in extremely limited sensitive situations. We choose the representative models which achieve the best performance result in RQ1 (i.e., NCD, LightGCN) and vary the ratio of missing sensitive attributes as {20%, 40%, 60%, 80%, 95%} in the training set. From the results in Figure 2, we have the following findings.

* The sensitive attributes missing ratios have a huge impact on the fairness results. As the missing ratios increase, the fairness-aware methods are not effective enough. Especially, when the missing

   &  &  &  \\   & AUC-G & AUC-A & AUC-O & RMSE & AUC-G & AUC-A & AUC-O & RMSE & AUC-G & AUC-A & AUC-O & RMSE \\  Origin & 0.6862 & 0.7235 & 0.6656 & **0.8670** & 0.6915 & 0.7153 & 0.6625 & **0.8635** & 0.7112 & 0.7053 & 0.6939 & **0.8301** \\  ComFair & 0.5389 & 0.5554 & 0.5398 & 0.9027 & 0.5379 & 0.5414 & 0.5374 & 0.8994 & 0.5498 & 0.5411 & 0.8704 \\ FairGo & 0.5992 & 0.5592 & 0.5371 & 0.8933 & 0.5351 & 0.5412 & 0.5319 & 0.8941 & 0.5457 & 0.5435 & 0.5408 & 0.8658 \\ FairGNN & 0.5288 & 0.5370 & 0.5212 & 0.9025 & 0.5294 & 0.5305 & 0.5256 & 0.9009 & 0.5289 & 0.5315 & 0.5225 & 0.8648 \\  FairLISA & 0.5193 & **0.5240** & 0.5210 & 0.8935 & 0.5281 & 0.5220 & 0.5222 & 0.8853 & 0.5273 & 0.5291 & 0.5214 & 0.8564 \\ FairGo+FairLISA & **0.5132** & 0.5270 & **0.5112** & 0.8922 & **0.5273** & **0.5204** & **0.5214** & 0.8867 & **0.5241** & **0.5260** & **0.5208** & 0.8628 \\  

Table 1: The fairness and accuracy performance on recommender system task. AUC represents AUC scores of all attackers. The smaller values of AUC denote better fairness performance with less sensitive information leakage (the fairer). G, A, O represent gender, age, and occupation. RMSE represents accuracy performance. The best fairness and accuracy results methods are highlighted in bold. The runner-up accuracy results are represented by underline.

ratio reaches 95%, several fairness-aware baselines converge to the Origin baseline. This observation indicates that in the absence of extremely sensitive attributes, other baseline methods have essentially lost their effectiveness in achieving fairness and perform similarly to the case where fairness was not considered at all. This highlights the point that fair user modeling in limited sensitive situations is worth studying.
* The models that can deal with unknown sensitive attributes, i.e., FairGNN, FairLISA, FairGo+FairLISA, perform better in most cases. It shows that making use of data with unknown sensitive attributes can alleviate the limited known sensitive information problem to some extent.
* Our methods, i.e., FairLISA and FairGo+FairLISA, consistently achieve the best performance. As the missing ratio increases, the improvements of our methods over FairGNN become more prominent. When the missing ratio reaches 95%, FairLISA can achieve an 8% improvement in fairness compared to FairGNN. This is due to FairGNN's reliance on the accuracy of the estimator. When numerous sensitive attributes are present, the accuracy of the sensitive estimator can be ensured. Nevertheless, FairGNN encounters notable constraints when access to sensitive information is limited. Our methods, based on information theory, can directly apply data with unknown sensitive attributes to improve fairness and are not greatly affected by the missing ratios, demonstrating the robustness of our methods.

Influence of \(U_{L}\), \(U_{N}\) (RQ3).

FairLISA can directly leverage \(U_{L}\) and \(U_{N}\) to improve fairness. In this part, we investigate their influence by adjusting the hyperparameter \(_{2}\) (\(U_{L}\)), \(_{3}\) (\(U_{N}\)) as defined in Eq. (9) on two user models (i.e., NCD, LightGCN). Additional results are included in Appendix C.5. Specifically, we vary the values of \(_{2}\), \(_{3}\) on NCD as {0, 0.5, 1, 1.5} and the values of \(_{2}\), \(_{3}\) on LightGCN as {0, 5, 10, 15}. The results are shown in Figure 5 (the lower, the fairer). We can find that \(_{2}\) has a greater impact than \(_{3}\), indicating that \(U_{L}\) has a more significant role in promoting fairness. This is reasonable because sensitive labels in \(U_{L}\) can guide the filtering process. Moreover, as the values of hyperparameters increase, the results become fairer, suggesting that both \(U_{L}\) and \(U_{N}\) contribute to fairness.

Relation to classical group fairness metrics (RQ4).Previous experiments have demonstrated the effectiveness of FairLISA in terms of preventing the leakage of sensitive information from representations. In this section, we present the results of our proposed model on two classical group fairness metrics, namely Demographic Parity  and Equal Opportunity  (the lower, the better). The specific formulas for these metrics and additional results are included in Appendix C.6. In short, the results shown in Figure 7 indicate that our goal of removing the effect of sensitive attributes also benefits the classical group fairness metrics, and our models achieve the best performance.

Figure 4: Performance of different group fairness metrics (the lower, the better).

Figure 3: Effects of \(_{2}\), \(_{3}\) on user models.

Figure 2: Fairness performance on different missing sensitive attributes ratios ( the lower, the fairer).

Conclusion

In this paper, we focused on the fairness problem of user modeling and proposed a general framework FairLISA as a solution in limited sensitive information situations. We first provide a novel theoretical perspective to unify data with known and unknown sensitive attributes with the fair mutual information objective. Building on this unification, we propose an adversarial learning-based framework that enables us to effectively leverage the entire user dataset for fair user modeling. We further showed that FairLISA could be seen as a generalized framework from traditional adversarial methods and take better advantage of data with unknown sensitive attributes. Experimental results on different user modeling tasks showed the effectiveness of FairLISA.

## 7 Border Impact and Future Work

User modeling is a crucial task in various applications and has a significant impact on various aspects of our daily lives. In this paper, we discuss the social responsibilities of user modeling and consider a situation where limited sensitive attributes are available. Our model could better help to design fairness-aware user modeling tasks and also protect users' privacy in the user modeling process. However, we notice that despite considering a limited sensitive situation, there is still a need to collect part of users' private information as model input, which may increase the burden on users as well as incurs possible privacy leakage in the data storage and transfer process. In the future, we will try to combine the privacy and fairness concerns so as to explore fairness and privacy aware user modeling.