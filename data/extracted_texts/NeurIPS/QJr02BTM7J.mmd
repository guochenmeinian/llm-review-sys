# Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective

Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective

 Takeshi Koshizuka

Department of Computer Science

The University of Tokyo

koshizuka-takeshi938444@g.ecc.u-tokyo.ac.jp

&Masahiro Fujisawa

RIKEN AIP

masahiro.fujisawa@riken.jp

&Yusuke Tanaka

NTT Communication Science Laboratories

ysk.tanaka@ntt.com

&Issei Sato

Department of Computer Science

The University of Tokyo

sato@g.ecc.u-tokyo.ac.jp

###### Abstract

In this paper, we explores the expressivity and trainability of the Fourier Neural Operator (FNO). We establish a mean-field theory for the FNO, analyzing the behavior of the random FNO from an _edge of chaos_ perspective. Our investigation into the expressivity of a random FNO involves examining the ordered-chaos phase transition of the network based on the weight distribution. This phase transition demonstrates characteristics unique to the FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Furthermore, we identify a connection between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradients, respectively. This finding provides a practical prerequisite for the stable training of the FNO. Our experimental results corroborate our theoretical findings.

## 1 Introduction

The recent surge in interest in solving partial differential equations (PDEs) has led to the use of neural network (NN)-based surrogate models. One promising line of work is the neural operator (NO), which learns the solution operator of PDEs, thereby bypassing the need for mesh dependency. Among the variants of NO, the Fourier neural operator (FNO) (Li et al., 2020) has gained popularity because of its advantageous cost/accuracy trade-off. The FNO can capture long-distance spatial interactions using the Fourier transform, whereas convolutional neural networks (CNNs) (Wen et al., 2019; Jiang et al., 2021) and message-passing graph neural networks (GNNs) (Li et al., 2020, 2020) are limited to operating solely on local variables. From a computational cost perspective, the Fourier transform is performed in quasi-linear time by the fast Fourier transform (FFT), making it significantly faster than the Transformer (Li et al., 2023).

Despite the widespread use of FNO as an architecture, there is a lack of comprehensive theoretical analysis on its expressivity and trainability. The universal approximation property (Kovachki et al., 2021), recognized as the basic expressivity of the FNO, is well-known; however, the exponential expressivity depending on the weight distribution, which are known for the densely connected network (DCN) (Schoenholz et al., 2016), a.k.a. fully connected network, and CNN (Xiao et al., 2018), remains unexplored. Regarding the trainability of FNO, the training instability in deep FNO has been experimentally reported by Tran et al. (2022), but the causes and conditions of the difficulty have not been clarified either theoretically or experimentally.

We analyze the exponential expressivity (how far can two different input vectors be pulled apart) and trainability (how much gradient explosion on average) of the random FNO from the perspective of whether the network is _ordered_ or _chaotic_. This viewpoint is grounded in mean-field theory, an analytical framework for NN established by Poole et al. (2016); Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018). A network is considered ordered when it brings all representations of two different spatial positions closer together, and chaotic when it drives them apart during forward propagation. Furthermore, a network can only be stably trained when initialized close to the _edge of chaos_, which is the transition point between the ordered phase and the chaotic phase. In fact, He initialization (He et al., 2015) is an example of a commonly used edge of chaos initialization for the DCN with ReLU activation (Burkholz & Dubatovka, 2019).

In this study, we establish a mean-field theory to analyze the expressivity and trainability of the FNO. Our investigation reveals the expressivity of random FNO at initialization by examining the transition point between ordered and chaos phases. The phase transition exhibits FNO-specific characteristics induced by mode truncation, as well as similarities with the characteristics of DCN and CNN. We also find a link between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradient, respectively. This discovery offers a practical initialization prerequisite for the stable training of the FNO.

## 2 Background

### Fourier Neural Operators

The FNO (Li et al., 2020) is one of the well-established methods for solving PDEs across many scientific problems (Yang et al., 2021; Wen et al., 2022; Hwang et al., 2022; Jiang et al., 2021; Pathak et al., 2022). An \(M\)-dimensional FNO with the number of hidden features \(D\) and the spatial size \(N\) for learning the operators between scalar-valued functions is defined as follows.

\[^{(+1)}=(^{()}(^{()} )+^{()}(^{()})),\] (1)

where \(^{()}^{^{M} D}\) is the \(\)-th hidden representation, \(M\) is the number of spatial dimensions, and \(\) is activation. The hidden representations \(^{(0)}\) and \(^{(L)}\) are the output of the lifting operator and the input of the projection operator, respectively. The architecture of these operators does not affect our analysis as long as the network stays shallow, as implemented in (Li et al., 2020). The \(\)-th densely connected (DC) module \(^{()}\) is an affine point-wise map in the physical space and the \(\)-th

Figure 1: Illustration of ordered-chaos phase transition for the weight initialization parameter \(^{2}\). In the ordered phase, the spatial hidden representations \(^{()}\) on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes.

Fourier convolution module \(^{()}\) is a parameterized kernel integral operator using Fourier transform. The bias term is considered to be included in either or both modules \(^{()}\) and \(^{()}\).

Several FNO variants have been developed to address specific challenges, such as geo-FNO (Li et al., 2022) for irregular regions and group equivariant FNO (G-FNO) (Helwig et al., 2023), which maintains equivariance to rotation, reflection, and translation. U-NO (Rahman et al., 2022) and U-FNO (Wen et al., 2022a) integrate FNO with U-Net for multiscale modeling. Additionally, WNO (Tripura and Chakraborty, 2022) utilizes wavelet bases, while CFNO (Brandstetter et al., 2023) enhances the use of geometric relations between different fields and field components through Clifford algebras. Adaptive FNO (Zhao et al., 2022; Guibas et al., 2021) and F-FNO (Tran et al., 2022) have improved computational and memory efficiency through incremental learning and architectural modifications. Other approaches for improving performance include methods with increasing physical inductive bias (Li et al., 2024), data augmentation (Brandstetter et al., 2022), and a variance-preserving weight initialization scheme (Poli et al., 2022).

While numerous new models and learning methods have been proposed, relatively little research has been conducted to understand the intrinsic nature of these methods. Issues such as spectral bias (Zhao et al., 2022) and training instability (Tran et al., 2022) have been reported. Tran et al. (2022) observed that training did not converge even at \(24\) layers. They successfully addressed the stability and accuracy degradation issues associated with an increase in the number of layers by implementing skip connections behind activation and introducing various training techniques. However, it is still unknown that the theoretical basis for why the original architecture of the FNO has problems with training instability and accuracy degradation.

### Mean-field Theory for Neural Networks

The mean-field theory has been used to provide a mathematical framework for understanding the expressivity and trainability of neural networks (Poole et al., 2016; Schoenholz et al., 2016; Yang and Schoenholz, 2017; Hayou et al., 2018; Xiao et al., 2018). A series of papers (Poole et al., 2016; Schoenholz et al., 2016) delved into the average behavior of infinite-width random deep DCN, with weights and biases initialized by a zero-mean Gaussian distribution. The formulation is given below.

\[&^{()}=(^{()}), \;^{()}=^{()}^{(-1)}+^{()},\\ & W_{i,j}^{()}(0,}{D}),\;b_{i}^{()}( 0,_{b}^{2}),\] (2)

where \(^{()}^{D}\) is the \(\)-th hidden representation, \(^{()}^{D D},\;^{()}^{D}\) are the \(\)-th learnable parameters, and the width is assumed to be sufficiently large \(D 1\).

Poole et al. (2016) and Schoenholz et al. (2016) explored the exponential expressivity of random DCN determined by two phases depending on the initial variance parameters \(^{2}\) and \(_{b}^{2}\), as shown in Fig. 5a. Poole et al. (2016) first examined the forward propagation of a random DCN with Tanh activation. They demonstrated that the covariance \(^{()}\) of the \(\)-th pre-activation representations \(^{()}\) and \(}^{()}\) corresponding to two different inputs \(^{(0)}\) and \(}^{(0)}\) are obtained by

\[ d[D],\;^{()}=^{2}[(h_ {d}^{(-1)})(_{d}^{(-1)})]+_{ b}^{2},\]

where the expectation is taken over the pre-activations \([h_{d}^{(-1)},_{d}^{(-1)}](,^{( -1)})\). The covariance converges exponentially to a fixed point \(^{*}\) determined by parameters \(^{2}\) and \(_{b}^{2}\).

A network is considered _ordered_ when it brings two distinct representations closer together, which implies a state of small expressivity. Conversely, a network is _chaotic_ when it drives them apart during forward propagation, implying a state of large expressivity. Networks with either excessively small or large expressivity can disrupt the structure of the input: the difference between two distinct inputs quickly becomes indistinguishable in networks with small expressivity, while similarities between inputs are no longer recognized in networks with large expressivity. The network is _ordered_ if the initial variance of the weights is small. For larger values, and beyond a certain threshold, the phase shifts, and the network behaves chaotically. This phase shift point is termed _the edge of chaos_.

Subsequently, Schoenholz et al. (2016) discovered the connection between expressivity and trainability of DCN through analysis of the backpropagation behavior. In an ordered network, the expectedvalue of the gradient norm becomes exponentially small during backpropagation, while it becomes exponentially large in a chaotic network. This implies that the gradient vanishes/explodes in ordered or chaotic networks, respectively. These findings suggest that deep DCN can be stably trained only near the edge of chaos. Schoenholz et al. (2016) also provided an estimate of the maximum depth at which a network can be trained when initialized away from the edge of chaos. These insights are not limited to DCN and similar results have been observed for residual networks (Yang & Schoenholz, 2017) and CNN (Xiao et al., 2018).

## 3 A mean-field theory for FNO

In this section, we establish a mean-field theory of FNO. We demonstrate the exponential expressivity of random FNO by examining the ordered-chaos phase transition during the forward propagation. Furthermore, we identify the connection between expressivity and trainability by concentrating on backward propagation behaviors. Our analysis is an advanced version of the approach developed by Poole et al. (2016); Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018). In Section 3.1, we outline the problem setup. In Section 3.2, we analyze the forward and backward propagation behavior of random FNO at initialization. In Section 3.3, we discuss the practical prerequisites for initialization to stabilize the training of FNO, leveraging the similarities between FNO and DCN. The proofs for all the lemmas and theorems are provided in Appendices A and B.

### Problem setting

Here, we consider a simplified one-dimensional (1D) FNO. Note that our theory is extensively applicable to the original FNO, as discussed in Section 3.3. The simplified 1D FNO, with a depth of \(L\), is defined by the number of hidden features \(D\), a spatial size \(N=2^{m}\) (where \(m\) is an integer), the number of Fourier modes \(K+1\), two learnable weights \(^{(,k)}^{D D}\) and \(^{(,k)}^{D D}\), and a bias \(^{()}^{D}\). Denote \(\) by the non-decreasing activation function. Let \(^{()}^{N D}\) and \(^{()}^{N D}\) be the post and pre-activation representations defined by

\[^{()}&=( ^{()}),\;^{()}=_{k=0}^{K-1}}{2}}(^{(,k)}+}^{(,k)})+ ^{()}_{N}^{},\\ &^{(,k)}^{} ^{(k)}^{(-1)}(^{(,k)}+^{(,k)}),\] (3)

where \(_{a,b}\) is the Kronecker-delta, \(c_{k}=2-_{k,0}-_{k,N/2}\) is a constant, \(_{N}\) is all-ones column vector with the size \(N\), \(}^{(,k)}\) is the conjugate of \(^{(,k)}\) corresponding to the \((N-k)\)-th frequency components, \(\) is the transpose conjugate, \(^{N N}\) is the Discrete Fourier Transform (DFT) matrix defined by \(F_{k,n}=(-n)\), and \(^{(k)}\) is a diagonal matrix with a 1 at position \(D_{k,k}^{(k)}\).

There are two differences from the original FNO proposed by Li et al. (2020a): (1) the DC module is dropped for the simplicity, and (2) \(^{(,k)}\) is multiplied by \(\) with respect to \(k=0,\) for appropriate normalization. We assume that the weights of FNO are initialized by i.i.d. samples from Gaussian distribution, _i.e._\(_{i,j}^{(,k)}}{{}}(0, }{2D})\), \(_{i,j}^{(,k)}}{{}}(0, }{2D})\), \(b_{i}^{()}}{{}}(0,_ {b}^{2})\). For \(k=0,\), the parameter \(^{(,k)}\) is set to zero exceptionally. For all \(d[D]=\{0,\;,\;D-1\}\), the pre-activations \(_{:,d}^{()}^{N}\) are i.i.d. random variables. When \(D 1\), by the central limit theorem, the variables \(_{:,d}^{()}\) follow Gaussian distribution with mean \(0\) and covariance matrix \(_{,^{}}^{()}_{^{1:}, ^{1:}}[H_{,d}^{()}H_{^{},d}^{( )}]\), where the expectation is taken over all random variables \([^{1:},^{1:}]\{ ^{(^{},k^{})},^{(^{ },k^{})}\}_{^{}[],k^{}[K]}\). Our theory can be easily extended to 2D and 3D FNOs.

### Expressivity and trainability of FNO

Firstly, the forward propagation of a single input signal with spatial features is described as follows.

**Lemma 3.1** (Iterated map).: _For all \(d[D]\), the covariance \(^{()}_{^{1,},^{1,}}[ {}_{:,d}^{()}\,}_{:,d}^{()}]\) is obtained recursively by the iterated map \(\) defined by_

\[_{,^{}}^{()}= _{k=0}^{K-1}c_{k}[[[(}_{:,d})]_{k}]^{2}](_{,^{ }}^{(k)})+_{b}^{2}}_{=:(^{(-1)})_{ ,^{}}}\] (4)

_where the expectation is taken over the pre-activations \(}_{:,d}(0,^{(-1)})\), \(_{,^{}}^{(k)}(-^ {})\) represents the scaled positional difference._

The indices \(\) and \(^{}\) correspond to different spatial locations as with the mean-field theory for CNN (Xiao et al., 2018). Note that \([(}_{:,d})]_{k}\) is the \(k\)-th Fourier modes of the post-activation representation. When applying DCN (Poole et al., 2016; Schoenholz et al., 2016) or CNN (Xiao et al., 2018) to the spatial signal, the iterated map depends only on local spatial locations, while in the case of FNO, the iterated map depends on all spatial locations because of the global Fourier convolution. In addition, only periodic spatial correlations with shift-invariant are propagated, and high-frequency components exceeding mode \(K\) are eliminated.

Next, we explore the fixed point \(^{*}\) of the iterated map \(\) satisfying \(^{*}=(^{*})\). By linearizing the dynamics of signal propagation around this fixed point and analyzing the stability and rate of convergence to the fixed point, we can determine the depth to which each component of the input can propagate. Schoenholz et al. (2016) showed that the iterated map of DCN defined in Eq. (2) has a fixed point of the form:

\[^{*}=q^{*}_{N}+q^{*}c^{*}(_{N}_{N}^{}- _{N}),\] (5)

where \(q^{*},c^{*}\) are the fixed points of variance and correlation, and \(_{N}\) is the identity matrix. Meanwhile, Xiao et al. (2018) showed that any fixed point for the iterated map of the DCN is also a fixed point for that of CNN. We show that random FNO has the same fixed points of the form of Eq. (5) with \(c^{*}=1\) in the following lemma.

**Lemma 3.2** (Exsistance of fixed points).: _When a random DCN defined by Eq. (2) has the fixed point \((q^{*},c^{*}=1)\) for the initial parameters \((^{2},_{b}^{2})\), then a random simplified FNO defined by Eq. (3) has a fixed point \(^{*}\) of the form_

\[^{*}=q^{*}_{N}+q^{*}c^{*}(_{N}_{N}^{}- _{N})=q^{*}_{N}_{N}^{}.\]

Lemma 3.2 indicates that the fixed point for the iterated map \(^{*}\) of the DCN serves as a fixed point for the iterated map of the simplified FNO (as well as CNN). To analyze the stability and convergence rate, we linearly approximate the C-map around the fixed point \(^{*}\), i.e., \(()^{*}+J_{^{*}}(-^{*})\), where \(J_{^{*}}\) is the Jacobian linear map of the iterated map defined in Eq. (15). We then derive the eigenvalues and eigenvectors for the Jacobian linear map \(J_{^{*}}\) as follows.

**Definition 3.3**.: \[_{q^{*}}^{2}[^{ 2}(H_{ ,d})+^{}(H_{,d})(H_{,d})],\] (6) \[_{c^{*}}^{2}[^{}(H_{ ,d})^{}(H_{^{},d})],\] (7) \[_{}}{2}[^{ }(H_{,d})(H_{^{},d})+(H_{,d}) ^{}(H_{^{},d})]+^{2}[c^ {*}^{}(H_{,d})^{}(H_{^{},d})],\] (8)

where the expectation is taken over the pre-activations \(}_{:,d}(0,^{*})\), and \(^{},\ ^{}\) are the first- and second-order derivatives of the activation \(\).

The bases \(,\ ^{(1)},\ ,\ ^{(K-1)}^{N N}\) using the above quantities are defined below.

\[_{,^{}} 1-(+_{c^{*}}- _{q^{*}}}{_{}})_{s=0}^{K-1}c_{s}(_{ ,^{}}^{(s)}),\] (9) \[_{,^{}}^{(k)}(_{, ^{}}^{(k)})-^{K-1}c_{s}}_{s=0}^{K-1}c_ {s}(_{,^{}}^{(s)}).\]From Lemma A.4, \(K-1\) matrices in \(\{^{(k)}\}_{k[K]\{0\}}\) are eigenbases with the eigenvalue \(_{c^{*}}\) of the Jacobian linear map. From Lemma A.5, the matrix \(\) is the eigenbases with the eigenvalue \(\) of the Jacobian linear map. Since the rank of the Jacobian linear map is at most K (Lemma A.3), the deviation from the fixed point \(^{()}-^{*}\) is spanned by K-dimensional eigenspace \(\{^{(k)}\}_{k[K]\{0\}}\{\}\). Then, the fixed point stability and the convergence rate are shown in the following theorem.

**Theorem 3.4** (Exponential expressivity).: _Let \(^{()}^{()}-^{*}\) be the deviation from the fixed point at the \(\)-th layer. Suppose that the deviation at the first layer is decomposed as \(^{(0)}=+_{k=1}^{K-1}_{k}^{(k)}+ \). The scalars \(,\ _{k}\) represent the scale of the perturbation for each eigemcomponent of the linearly approximated map \(^{()}^{(+1)}\). The component \(^{N N}\) belongs to the orthogonal complements of the space \(\{,^{(1)},\ ,\ ^{(K-1)}\}\)._

_Then, the deviation at the \(\)-th layer is obtained by_

\[^{()}=^{}+_{k=1}^{K-1} ^{}_{c^{*}}_{k}^{(k)},\] (10) \[_{s=0}^{K-1}c_{s}_{q^{*}}+( 1-_{s=0}^{K-1}c_{s})(_{}+_{c^{*}}).\]

_In particular, when the Fourier mode is \(K=+1\), Eqs. (9) and (10) reduce to the following._

\[^{()}=^{}_{q^{*}}+_{k=1}^{ K-1}^{}_{c^{*}}_{k}^{(k)},\] (11) \[,^{}[N],\ _{,^{}}=1,\ ^{(k)}_{ ,^{}}=(^{(k)}_{,^{}})- _{,^{}}.\]

Theorem 3.4 shows the expressivity of the FNO, which is characterized by the ordered-chaos phases and varies exponentially with respect to the number of layers. Theorem 3.4 indicates that the asymptotic behavior of the zero-frequency deviation is mostly determined by \(\) and the periodic deviation is determined by \(_{c^{*}}\). If \(<1\) and \(_{c^{*}}<1\), the fixed point is stable as the deviation from the fixed point converges exponentially to zero. When the fixed point remains stable at \(c^{*}=1\), a random network exists in an ordered phase, where all spatial representations are correlated in an asymptotic manner. Conversely, when the fixed point with \(c^{*}=1\) becomes unstable, the network transitions into a chaotic phase, exhibiting behavior dependent on the activation function \(\). The boundary between these two phases is referred to as _the edge of chaos_.

The convergence rates \(_{q^{*}}\) and \(_{c^{*}}\) are the same as the convergence rates of the variance and correlation to the fixed point for DCN (Schoenholz et al., 2016) and CNN (Xiao et al., 2018). However, only periodic spatial correlations are propagated in the FNO, resulting in a different eigenspace of the map \(^{()}^{(+1)}\) compared to the DCN and CNN. In DCN, the deviation belongs to a vector space with dimension \(\) in DCN, whereas in FNO, the dimension is \(K\), or at most \(+1\). CNN possess diagonal eigenspaces associated with eigenvalues \(_{q^{*}}\) and non-diagonal eigenspaces associated with eigenvalues \(_{c^{*}}\). In contrast, FNOs without mode truncation exhibit a similarity, possessing eigenspaces \(_{q^{*}}\) for zero-frequency and eigenspaces \(_{c^{*}}\) for k-frequencies with diagonal components removed. Furthermore, mode truncation increases the convergence rate of zero-frequency deviation from \(_{q^{*}}\) to \(\) and affects all eigenbases as well. For further discussions on the similarities between CNN and FNO, please refer to Appendix C. A visualization of the covariance of the FNO with Tanh and ReLU activations is shown in Appendix F.

Finally, we demonstrate the connection between expressivity and trainability. By examining the covariance of the gradient in each layer during backpropagation, we investigate the conditions under which training is stable without gradient vanishing or exploding.

**Theorem 3.5** (Trainability).: _Let \(}^{()}^{N N}\) be the gradient covariance with respect to some loss \(\), e.g. mean squared error, at the \(\)-th layer. Suppose that the gradient covariance at the L-th layer is decomposed as \(^{(L)}_{,^{}}=_{k=0}^{K-1}_ {k}(^{(k)}_{,^{}})+}\), where \(_{k}\) is the coefficient of each basis and \(}\)belongs to the orthogonal complements of \((\{(_{,^{}}^{(k)})\}_{k=0}^{K-1})\). Then, the gradient covariance at the \(\)-th layer is obtained by_

\[_{,^{}}^{()}=_{k=0}^{K-1}_{c^{*}}^{ L-}_{k}(_{,^{}}^{(k)}).\]

Theorem 3.5 shows that gradient vanishing occurs when \(_{c^{*}}<1\) (ordered phase) and gradient explosion occurs when \(_{c^{*}}>1\) (chaos phase). Thus, stable training of the FNO can be achieved close to _the edge of chaos_ by setting the initial parameter \(^{2}\) to satisfy \(_{c^{*}} 1\). We specifically present the initial parameter choices that achieve _the edge of chaos_ for several FNOs in Section 3.3.

When \(c^{*}=1\), there is no change in the dynamics during backpropagation due to mode truncation. When using the full mode \(K=+1\), the condition \(_{c^{*}}=1\) always achieves _the edge of chaos_, which is consistent with the results for the DCN and CNN. Despite the architectural and iterative map differences among FNO, DCN, and CNN, Theorem 3.4 and Theorem 3.5 demonstrate the similarities in the random behavior of FNO, DCN, and CNN. This allows existing results based on mean-field theory to be applied to the FNO.

### Initialization requirements for stable training

For stable training, Theorem 3.5 suggests the necessity of initializing FNO near _the edge of chaos_, _i.e._, initializing FNO so that \(_{c^{*}} 1\). In this section, we present the initial parameter choices that achieve _the edge of chaos_ for several FNOs, each with slightly different architectures such as activation functions. Furthermore, the behavior of the gradient norm \((}^{()})/D\) as a function of layer \(\) are visualized in Fig. 2 for several variants of random FNO with different initialization parameters \(^{2}\). We used FNO with a width of \(D=32\) and a number of layers \(L=64\), and for simplicity, we computed the absolute value of the output as the loss with respect to the input sampled from the standard normal distribution.

**Simplified FNO with Tanh activation**.

The behavior of \(_{c^{*}}\) for the parameters \(^{2}\) and \(^{2}_{b}\) of the Tanh network has been extensively studied by Poole et al. (2016); Schoenholz et al. (2016). The phase diagram drawn by Pennington et al. (2017) is shown in Fig. 4(a). By using parameters \((^{2},^{2}_{b})\) around the two phase boundaries of ordered and chaotic that achieve \(_{c^{*}}=1\), the training of the simplified FNO with Tanh activation can be stabilized. Figure 1(a) depicts the behavior of the gradient backpropagation in the simplified FNO with Tanh activation and the bias parameter being \(^{2}_{b}=0.1\). Figure 1(a) shows that when \(^{2} 2\), the gradient diminishes exponentially; otherwise, it explodes exponentially.

Figure 2: Average gradient norm \((}^{()})/D\) during the backpropagation of several FNOs plotted as a function of layer \(\). Each line corresponds to the result of different initial values of \(^{2}\) from \(0.5\) to \(4.0\) in increments of \(0.5\). The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \(^{2}\), the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.

**Simplified FNO with ReLU activation**.

The iterated map \(\) of the DCN with ReLU activation is given by Cho and Saul (2009) as follows.

\[q^{(+1)} =^{2}q^{()}+_{b}^{2},\] (12) \[c^{(+1)}q^{(+1)} =^{2}q^{()}_{1}(q^{()}}{q^{()}})+_{b}^{2},\] (13) \[_{1}(c) =(}+(-)c),\]

The edge of chaos initialization for the DCN with ReLU activation is known as He initialization (He et al., 2015), which sets the initial variance parameter as \(^{2}=2\) and initial bias as \(b_{i}^{()}=0\) for Eq. (2). From the similarity of the DCN and the FNO, we can derive the FNO version of the He initialization that achieves \(_{c^{*}}=1\) by setting \(^{2}=2,\ b_{i}^{()}=0\) for Eq. (3). The He initialization scheme for the simplified FNO with the activation \(=\) is derived as follows.

\[_{i,j}^{(,k)}(0,D^{-1}),\ _{i,j}^{(,k)}(0,D^{-1}).\]

Figure 2b demonstrates that the choice of \(^{2}=2\) preserves the magnitude of the gradient norm during backpropagation of deep simplified FNO with ReLU activation.

**Original FNO**.

In the original architecture of the FNO proposed by Li et al. (2020c), the DC module is used together with the Fourier convolution module as shown in Eq. (1). We initialize the weights of both layers consistently as follows. For all \([L]\), \(k[K]\), and \(i,j[D]\),

\[_{i,j}^{(,k)}(0,}{4D}),\ _{i,j}^{(,k)}(0,}{4D}),\]

\[W_{i,j}^{()}(0,}{2D }),\ b_{i}^{()}(0,_{b}^{2}).\]

From the similarity of the initial network behavior of the FNO and the DCN, the fixed point with \(c^{*}=1\) of the simplified FNO is also a fixed point of the original FNO. In the neighborhood of the fixed point \(^{*}\), the eigencomponents spanned by \(\{^{(k)}\}_{k=1}^{K}\) will decay or increase at the rate of \(_{c^{*}}\). Following the derivation of Theorem 3.5, the eigenvalues of the \(\) function eigencomponents of the gradient covariance are also \(_{c^{*}}\). These results show that the edge of chaos initialization scheme can be used for the original FNO with each activation function. Figure 2c shows that the original FNO with ReLU activation and the parameter fixed as \(b_{i}^{()}=0\) exhibits similar backpropagation behavior as the simplified FNO, _i.e._, \(^{2} 2\) is an appropriate choice.

## 4 Experiments

In this section, we experimentally demonstrate that deep FNO training requires appropriate initialization settings on a variety of datasets, consistent with the theory discussed in Section 3.

### Datasets

We evaluated three models on commonly used PDEs: the advection equation, Burgers' equation, Darcy Flow equation, and incompressible Navier-Stokes (NS) equation. All datasets were generated by numerical simulations used in (Takamoto et al., 2022; Li et al., 2020c) and are publicly available. A summary of the dataset is provided in Table 1, and more details are given in Appendix D.

**Advection equation and Burgers' equation**. The linear advection equation for the function \(u(x,t)\) is given by

\[_{t}u(x,t)+_{x}(u(x,t)/2)=0,\ u(x,0)=u_{0}(x),\]

where \(u_{0}\) is the initial condition and \(=2.0\) is an advection speed. The non-linear Burgers' equation for the function \(u(x,t)\) is given by

\[_{t}u(x,t)+_{x}(u^{2}(x,t)/2)=_{xx}u(x,t ),\ u(x,0)=u_{0}(x),\]

[MISSING_PAGE_FAIL:9]

unique FNO-specific behaviors caused by mode truncation, as well as common behaviors akin to those of DCN. With our analysis as a basis, we identified the necessity of initializing FNO near _the edge of chaos_ for stable training of the FNO. Experimental results supported our theoretical results.

A limitation of our analysis is that it is limited to the network at initialization and does not address the stability of the entire optimization process. While we do not provide sufficient conditions for stable training, we do offer one necessary condition for achieving stable training. Future work may consider a mean-field analysis of the FNO when using skip-connection (Tran et al., 2022), Dropout and batch normalization, as well as initialization methods that ensure the input-output Jacobian of the FNO satisfies dynamical isometry (Pennington et al., 2017, 2018).

Figure 4: nMSE of FNOs on test datasets for four distinct PDEs. **(a, b):** the advection equation **(c, d):** the Burgers’ equation, **(e):** Darcy Flow, **(f-h):** the NS equation. The heatmaps for each nMSE correspond to the results of each heatmap of training loss in Fig. 3. The lighter colors, the better the test performance. The presented results are the mean nMSE calculated over three different seeds.

Figure 3: Training loss of FNOs at last epoch for four distinct PDEs. **(a, b):** the advection equation, **(c, d):** the Burgers’ equation, **(e):** Darcy Flow, **(f-h):** the NS equation. The heatmaps represents the training loss values for varying depth \(L\{4,8,16,32\}\) and initial weight parameter \(^{2}\{0.1,0.5,1.0,2.0,3.0,4.0\}\), with lighter colors signifying lower training loss. The presented results are the mean training loss at the last epoch over three different seeds.