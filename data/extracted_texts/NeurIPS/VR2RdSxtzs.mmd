# MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems

Bin Lei\({}^{1,2}\) Yi Zhang\({}^{2}\) Shan Zuo\({}^{2}\) Ali Payani\({}^{3}\) Caiwen Ding\({}^{1}\)

\({}^{1}\)University of Minnesota \({}^{2}\)University of Connecticut \({}^{3}\)CiscoResearch

{lei00126, dingc}@umn.edu

{bin.lei, yi.2.zhang, shan.zuo}@uconn.edu

{apayani}@cisco.com

###### Abstract

Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in **advanced mathematical problems requiring complex, multi-step logical reasoning**. To enhance their inferential capabilities, current research has delved into _prompting engineering_, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the _Multi-Agent System for conditional Mining_ (**MACM**) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from \(\) to \(\). The code is available in https://github.com/bin123apple/MACM.

## 1 Introduction

Large language models (LLMs) like GPT-4  excel in various problem-solving tasks but still struggle with complex logical deduction, especially in mathematics involving abstract concepts and multi-step reasoning . This limitation hinders their accuracy and reliability in fields requiring precise mathematical reasoning, such as academic research, engineering, and theoretical physics.

A contemporary and efficacious method for tackling this issue is the _prompting engineering_. It enhances accuracy in complex problem-solving without necessitating further training of the model. By strategically crafting prompts, prompting engineering optimizes the utilization of large language models, guiding their processing pathways more efficiently and effectively .

Previous prompting methods mainly include the Chain of Thought (CoT) , Self-consistency Chain of Thought (SC-CoT) , Tree of Thought (ToT) , and Graph of Thought (GoT) . CoT and SC-CoT show limited capabilities in complex logical reasoning, achieving only 4.0% and 9.0% accuracy in simple tasks like the 24-point game using GPT-4 . While ToT and GoT have improved LLMs' problem-solving abilities, they lack generalizability, requiring specific prompts for each problem, as detailed in Appendix A.

**To address two key issues:**

1. _The insufficient reasoning capability of LLMs for complex mathematical problems._
2. _The inadequate generalizability of current prompting methods._We propose the _Multi-Agent System for Condition Mining (MACM)_ prompting method. MACM has moved beyond being restricted by the specific contents of a problem. Instead, it first abstracts the conditions and the objective of the problem. Subsequently, through a Multi-Agent interactive system, it progressively mines new conditions conducive to achieving the objective, thereby ultimately fulfilling the goal of problem-solving.

The comparison between MACM and current mainstream prompting methods in problem-solving is shown in Figure 1. MACM extracts conditions and objectives from each math problem, iteratively adding new insights until enough information is gathered to find a solution. Performance-wise, MACM improves accuracy by over 10 percentage points on datasets like the 24-point game, matching the effectiveness of ToT and GoT. Moreover, MACM is versatile; it can apply the same set of prompts to various mathematical reasoning problems without manual modifications, unlike the tailored prompts needed for ToT and GoT.

Through several experiments on the math related datasets, we verified MACM's generalizability and superior error correction compared to original prompting methods. With MACM, the GPT-4 turbo model's accuracy on the MATH dataset increased by \(\), and by \(\) compared to SC-CoT. In the 24-point game, MACM achieved an accuracy rate \(\) higher than ToT.

## 2 Related Work

In this section, we summarize several major current prompting methods.

_I-O_ **Prompting:** Input-Output (I-O) prompting is the most common method for interacting with large language models, where users specify the problem conditions directly to the model, which generates responses through a token-level, sequential decision-making process .

_CoT_ **Prompting:**: Chain of Thought (CoT) prompting refines the model's output into more structured and logically coherent text by methodically constructing and elaborating upon chains of reasoning. This approach enhances the model's ability to produce outputs rooted in logical deduction. There are several variants of CoT, including Zero-Shot CoT , Few-Shot CoT , and Auto-CoT , each tailored to different prompting scenarios and requirements to improve logical reasoning in diverse contexts.

_SC-CoT_ **Prompting:**: Self-Consistency Chain of Thought (SC-CoT) prompting improves upon CoT by introducing a voting mechanism, which emphasizes internal consistency and semantic interconnectedness. In this method, models evaluate (vote on) their own outputs to select the most coherent response, thereby reducing logical fallacies and inconsistencies.

_ToT_ **Prompting:**: Tree of Thought (ToT) prompting uses a hierarchical, tree-like structure to organize and guide the model's text generation. This method improves precision and structure in responses, incorporating a voting mechanism to refine outcomes and reduce computational demands.

Figure 1: The comparison between the current mainstream prompting methods and MACM.

_GoT_**Promping :**[4; 9]: Graph of Thought (GoT) prompting enhances ToT by allowing interconnections between thoughts on different branches. It decomposes complex tasks into simpler sub-tasks, solves them independently, and merges the results, thus reducing computational costs.

## 3 Method

### MACM Overall Structure

The overall structure of MACM is shown in Figure 2.

We have designed an interactive system comprising three agents: _Thinker_, _Judge_, and _Executor_ to solve complex mathematical problems.

* _Thinker_: Responsible for generating new thoughts or ideas. This role involves creative thinking and the generation of novel solutions or approaches to problems.
* _Judge_: Evaluate the thoughts generated by the _Thinker_. It assesses the viability and correctness of new ideas, ensuring that only the most logical and beneficial ones are pursued.
* _Executor_: Performs calculations or actions according to predefined steps. It is focused on the implementation of the ideas approved by the _Judge_, turning steps into tangible outcomes.

When a mathematical problem is input into our system, the _Thinker_ initially sets up the _Condition List_ and defines the final _Objective_ based on the given problem. After initialization, the _Thinker_ mines new conditions conducive to the objective from the current _Condition List_, i.e., the _Known Conditions_. The _Judge_ then assesses these newly mined conditions. If deemed correct, the _Judge_ incorporates the new condition into the _Condition List_. Otherwise, the new condition is discarded.

Once all new Conditions have been reviewed, we obtain a revised _Condition List_. At this point, the _Judge_ evaluates whether the current conditions are sufficient to achieve the objective. If the answer is False, the process reverts to step 2 for further mining of new conditions. In our experiments, we set a limit of five iterations; if the objective is not met after five rounds of mining, we consider the problem unsolvable. This prevents the program from entering an infinite loop. If the answer is True, the _Thinker_ designs steps based on the _Known Conditions_ to achieve the _Objective_. Finally, the _Executor_ performs calculations following these steps to produce the final result.

MACM achieves a high level of generalizability by abstracting conditions and objectives from each specific mathematical problem. Through a multi-agent interactive system, where the _Thinker_ is responsible for ideation and design, the _Judge_ for inspection and decision-making, and the _Executor_ for computation, most potential errors in reasoning and calculation are eliminated. By repeatedly mining for conditions and adding the correct ones to the _Condition List_, MACM ensures depth in thinking, making it suitable for analyzing complex mathematical problems.

### Theoretical Analysis

MACM moves away from the hierarchical dependencies of previous methods by introducing _Conditions_ and _Objectives_. It continuously expands _Known conditions_ to derive the final answer, eliminating

Figure 2: The overall structure of MACM. Original Math problem; \([width=12.9pt]{fig:mall}\): Condition list; \([width=12.9pt]{fig:mall}\): True; \([width=12.9pt]{fig:mall}\): False; \([width=12.9pt]{fig:mall}\): Discard; \([width=12.9pt]{fig:mall}\): Known Conditions; \([width=12.9pt]{fig:mall}\): New Conditions; \([width=12.9pt]{fig:mall}\): Objective; \([width=12.9pt]{fig:mall}\): Thinker; \([width=12.9pt]{fig:mall}\): _Judge_; \([width=12.9pt]{fig:mall}\): _Executor_; \([width=12.9pt]{fig:mall}\): Initialize the initial condition list and the objective; \([width=12.9pt]{fig:mall}\): Explore new Conditions based on current condition list; \([width=12.9pt]{fig:mall}\): Check if the new condition is correct; \([width=12.9pt]{fig:mall}\): Check if the objective can be achieved based on the current Conditions in the Condition list; \([width=12.9pt]{fig:mall}\): Designing steps for achieving the objective based on current Conditions; \([width=12.9pt]{fig:mall}\): Achieve the objective.

the need for manual, problem-specific prompts. This method compresses information from various _Thoughts_ into existing _Condition List_, capturing more connections than traditional prompting methods that rely on navigating a hierarchical structure.

The _Thinker_ initiates a thought set \(_{1}=\{T_{1}^{1},T_{1}^{2},,T_{2}^{m}\}\) contains \(m\) new thoughts from question \(Q\) and generates subsequent thought sets \(_{i}=\{T_{i}^{1},T_{i}^{2},,T_{i}^{m}\}\) based on the current condition list \(_{i}=\{C_{1},C_{2},,C_{i-1}\}\). Each condition \(C_{i-1}\) is derived from the most accurate thought \(T_{i-1}^{*}\) in \(_{i-1}\). At each step \(i\), the _Judge_ selects the correct thought \(T_{i}^{*}\) in thought set \(_{i}\) such that \(T_{i}^{*}=*{arg\,max}_{s}P_{i}^{}(T_{i}^{*}|T_{i}^{s} _{i},s\{1,,m\})\), where \(P^{}\) is the probability that the _Judge_ confirms the thought as correct. By using this method, we map the whole thoughts space \(\) to the _Condition List_\(\). In an ideal situation where \(P^{} 1\), we have \(\). The probability of arriving at the correct answer \(A_{}\) based on the final _Condition list_\(\) is equal to that based on the entire thoughts space \(\). Thus we have: \(P_{}(A_{}|)=P_{}(A_{}|)>P_{}(A_{}|\)\(\{T_{ij} i=1,2,,m\) and \(j=1,2,,n\})\). In practice, where \(P^{} 1\), we performed the experiments to test its performance, the results are shown in Section 4.

### Using Cases

Our prompts and use cases are shown in Figure 3. It demonstrates the specific process of MACM analyzing algebra and geometry problems. In these two examples, we have employed OpenAI's GPT-4 Turbo  as the intelligent agent, which is capable of performing calculations using code. It is endowed with three roles: _Thinker_, _Judge_, and _Executor_ by using the following instructions:

**For** _Thinker_**: _You take the role of a **Thinker**. I need you to help me gradually ponder over some problems following my instructions. You need to answer the question by using the following format: Based on Condition A and Condition B, we can get: C._

**For** _Judge_**: _You take the role of a **Judge**. I need you to make judgments on some statements. You are only allowed to use the True or False as the final answer._

**For** _Executor_**: _You take the role of a **Executor**. I need you to calculate the final result based on the given conditions and steps._

**In the first algebra problem:**

_Let \(S\) be the set of all real numbers \(\) such that the function \(+5x+}{x^{2}+7x-44}\)can be expressed as a quotient of two linear functions. What is the sum of the elements of \(S\)?_

GPT-4 Turbo's raw response reached an incorrect conclusion: \(+5x+=k(x+11)(x-4)}\) which then led to issues in the subsequent code design, ultimately resulting in an incorrect output.

In the MACM analysis process, the _Thinker_ initially identifies conditions and objectives from the problem statement and then uncovers new conditions. Although the _Thinker_ initially identifies the same incorrect condition as GPT-4 Turbo, the _Judge_ detects and rejects this error, preventing its addition to the _Known Conditions_. In the second round, the _Thinker_ identifies two new conditions: \(+5(-11)+_{1}=0}+5(4)+ _{2}=0}\) which the _Judge_ verifies and adds to the _Known Conditions_ list. The _Judge_ then confirms that the _Known Conditions_ are sufficient to achieve the objective. The _Thinker_ designs steps to reach the objectives based on these conditions, and finally, the _Executor_ performs the necessary calculations to produce the result.

**In the second geometry problem:**

_Square \(ABCD\) has side lengths of 13 units. Point \(E\) lies in the interior of the square such that \(AE=5\) units and \(BE=12\) units. What is the distance from \(E\) to side \(AD\)?_

While GPT-4 Turbo's response had the correct theoretical approach, it failed to identify relationships between points in the problem, leading to incorrect expressions and an incorrect final result.

During the MACM analysis, the _Thinker_ first clarifies the conditions and objectives of the geometry problem and then uncovers new conditions to achieve the goal. Initially, it discovers that: \( ABE\)_is a right triangle_. After verification by the _Judge_, this condition is added to the known conditions. The _Judge_ then checks if the known conditions are sufficient. A _False_ result means more conditions are needed, so the _Thinker_ continues searching. In the second round, the _Thinker_ deduces \(\), which the _Judge_ verifies and adds to the _Condition List_.

Figure 3: MACMâ€™s detailed analysis process for complex mathematical problems with specific prompts, illustrated with an **algebra problem (on the left)** and a **geometry problem (on the right)**. We use one set of prompts that can target different types of problems, with prompts 0-6 displayed in the \( ABE\) below the dialogue box. In these examples, MACM involves three steps: 1. Extracting conditions and the objective. 2. Iteratively identifying new conditions. 3. Solve the problem based on known conditions.

Upon confirming sufficiency, the _Thinker_ plans the steps to solve the problem, and the _Executor_ performs the calculations to find the result.

In analyzing these two problems, MACM first extracts the specific conditions and objectives from the questions. This allows MACM to directly use these conditions and objectives for prompt design in subsequent processes, enhancing our approach's generalizability. Previous methods like ToT and GoT lack this setup, resulting in poorer generalizability. For example, in the 24-point game experiment with ToT, the lack of this setting necessitated the manual configuration of the following prompt:

_Evaluate if given numbers can reach 24 \(() \)_

In MACM, the _if given numbers can reach 24_ is obtained by the first step and the evaluation prompt is generalized to _Evaluate {objective} {input}_, ensuring higher generalizability.

## 4 Experiment

### Performance on MATH benchmark

The MATH dataset  includes a variety of mathematical problems. It offers seven types of mathematical problems, including geometry, algebra, probability theory, etc., with difficulty levels ranging from 1 to 5. We first tested the overall performance of MACM on the MATH dataset without distinguishing difficulty levels. Afterward, we specifically selected the most difficult mathematical problems from the MATH dataset for testing. The detailed experimental setup is presented in the Appendix B.

In Table 1, we compared the accuracy of GPT-4 Turbo on the MATH dataset with various prompting methods. We found that compared to the original GPT-4 Turbo, MACM increased its accuracy by 20%. Compared to CoT, the increase was 13.56%, and compared to SC-CoT, it was 7.8%. Among these, MACM led to the greatest improvement in accuracy for the original GPT-4 Turbo model on number theory problems, at 23.53%. In geometry problems, although MACM has increased the accuracy of GPT-4 Turbo by 17.63%, the final accuracy rate is still only 62.74%. Upon analyzing the causes of errors, we found that many mistakes were due to GPT-4 Turbo's difficulty in accurately understanding the relationships between various geometric figures, thereby failing to design corresponding code to solve the problems. However, in algebra and number theory problems, MACM, by correcting the erroneous analysis of GPT-4 Turbo and helping it explore potential approaches, achieved accuracy rates of 96.07% and 98.04%, respectively. Moreover, compared to the previous SOTA method, CSV prompting, on the MATH dataset, MACM achieves a 3.6 percentage points higher accuracy rate on the same dataset. This demonstrates the effectiveness of MACM in solving mathematical problems.

In Figure 4, we tested the performance of MACM on the **open-source LLaMA series models** on the MATH dataset and compared it with other prompting methods. Since LLaMA models do not have a code interpreter like GPT-4 Turbo, we disabled the code-checking function of MACM in this group of tests. The rest of the experimental setup was consistent with GPT-Turbo. In zero-shot scenarios, the accuracy rates of LLaMA 7B and LLaMA 13B were both below 5% . Majority voting could enhance their accuracies to 6.9% and 8.8% respectively , while MACM further increased them to 9.5% and 10.2%. On LLaMA 2  and LLaMA 3 , compared to 4-shots, MACM could further improve the accuracy on the MATH dataset by 3-5 percentage points. Overall, We found that MACM can also be applied to LLaMA models, although the performance improvement was not as significant as with GPT-4 Turbo. This is because GPT-4 Turbo has a better understanding of MACM's intrinsic directive prompts, enabling it to find the correct results more effectively.

   & Algebra &  Counting and \\ Probability \\  & Geometry &  Intermediate \\ Algebra \\  &  Number \\ Tregg \\  &  Prealgebra \\  & 
 Prealculus \\  & Overall \\  I-O & 88.24 & 81.63 & 45.11 & 66.67 & 74.51 & 81.82 & 71.15 & 72.78 \\ CoT & 92.99 & 83.67 & 42.02 & 68.07 & 77.31 & 82.07 & 74.18 & 74.36 \\ SC-CoT & 94.96 & 87.17 & 50.14 & 71.99 & 89.91 & 86.75 & 79.67 & 80.12 \\ CSV  & 86.9 & 77.3 & 54.0 & 56.6 & 85.6 & 86.5 & 53.9 & 73.54 \\ CSV + voting  & 95.6 & 89.0 & 64.9 & 74.4 & 94.1 & 91.6 & 67.8 & 84.32 \\ 
**MACM** & 96.07 & 97.95 & 62.74 & 78.43 & 98.04 & 94.11 & 88.46 & 87.92 \\   

Table 1: Accuracy (%) of GPT-4 Turbo on MATH dataset with different prompting methods.

In Figure 5, We focused on the ability of MACM to solve the **Level 5** mathematics problems in MATH. As shown in the figure, MACM improved the accuracy of GPT-4 Turbo in all seven categories of level 5 problems. The two types of problems that saw the most significant improvement with MACM were the very categories where the original GPT-4 Turbo performed the worst: Geometry and Intermediate Algebra. The original GPT-4 Turbo had an accuracy rate of only 18.18% on Geometry problems and 34.04% on Intermediate Algebra problems. With the support of MACM, it's accuracy rate in Geometry problems increased to 50.0%, and in Intermediate Algebra problems, it increased to 65.96%. This demonstrates MACM's effectiveness in solving difficult mathematical problems.

### Comparison with ToT and GoT

Due to the lack of generalization of ToT and GoT prompting methods (See Appendix A for the reason), we were unable to test them on the MATH benchmark. To compare MACM with them, we selected two mathematical problems where their methods are applicable: the 24-point game and sequence sorting. Among these, ToT tested the 24-point game, while GoT studied the sequence sorting problem. The detailed experimental setup is presented in the Appendix B.

In Table 2, We compared MACM with IO, CoT, SC-CoT, and ToT models on the 24-point game. When the model is GPT-4, MACM is 17% higher than ToT (\(b=5\)). Note that here, to ensure a fair comparison, we used the standard GPT-4 without any code capabilities. Additionally, with the support of MACM, GPT-3.5 also achieved an accuracy of 67% in the 24-point game, which is higher than the GPT-4 model with ToT (\(b=1\)) support. Upon analyzing the reasons for the improvement in accuracy, we found that MACM's Judge corrected many thoughts that were mistakenly evaluated in ToT, leading to GPT-4 choosing incorrect approaches. This correction process significantly contributed to the increase in accuracy. In addition, We compared the GPT-3.5 model's ability to sort 64 numbers using GoT and MACM. MACM outperformed GoT by 2.94%. Note that some results marked with * were estimated from graphs without specific data. Additionally, GPT-4 Turbo achieved 100% accuracy on the Sequence Sorting task due to its problem-based code construction capability.

### Performance on other datasets

This section primarily tests two capabilities of MACM:

_1. The ability to solve more challenging mathematical problems._ We applied MACM to two datasets, SciBench  and TheoremQA , which claim their difficulty surpasses the middle school level of the MATH dataset, reaching the Undergraduate Level.

_2. Transferability to General Logic Reasoning Tasks._ Although MACM focuses on solving mathematical problems, to test its applicability, we applied it to the Reclor logic reasoning dataset .

Table 3 displays the testing results of MACM on the SciBench dataset. The SciBench dataset includes problems in chemistry, physics, and mathematics. We only selected the math-related subset for testing, which includes: the _diff_, _stat_ and _calc_ subset. The experimental setup was consistent with the testing on the MATH dataset (as shown in Appendix B). The results demonstrate that, in contrast to the Chain of Thought (CoT) method, which resulted in decreased accuracy for both GPT-4 and GPT-4 Turbo on this dataset, MACM led to an approximate 20 percentage points.

Table 4 presents the performance of GPT-4 Turbo on the TheoremQA dataset using various prompting methods. The TheoremQA dataset encompasses problems from multiple domains including mathematics, physics, finance, and computer science. Notably, its mathematics subset contains numerous conceptual and definitional questions that do not involve logic reasoning processes; thus, these were not considered in our experiments. We solely tested questions within the TheoremQA mathematics subset that involve logic reasoning and have definite answers (e.g., a specific number, rather than an interpretation of a theorem). The experimental setup was consistent with the testing on the MATH dataset (as detailed in Appendix B). The results indicate that MACM enabled a roughly 30 percentage points increase in accuracy for GPT-4 Turbo on this subset.

Table 5 shows the test results of GPT-4o  on the Reclor dataset using various prompting methods. For general logical reasoning problems, we adjusted the computation-related prompts in the original MACM to make them suitable for non-mathematical logical reasoning problems, while maintaining the overall structure consistent. Results show that MACM can also improve the accuracy of general logic reasoning tasks, but the increase is smaller compared to math tasks.

### Ablation Study

In this section, we primarily investigate two issues. 1 Explore the relationship between MACM Accuracy and LLM Queries, comparing it with other methods. 2 Analyze the proportional impact of each component within the MACM on the overall accuracy improvement. We performed these two experiments on 200 randomly selected questions from the MATH dataset that **the original GPT-4 Turbo model answered incorrectly**.

**Trade-off Between Accuracy and LLM Queries:** In general, increasing the number of responses generated by LLMs leads to an improvement in accuracy. Each prompting method has parameters that can increase it, such as the length of the chain \(l\) in CoT, the number of voters \(v\) in SC-CoT, and the Tree breadth \(b\) in ToT. To measure the search efficiency of each method, we compared the relationship between the accuracy and the number of responses generated by GPT-4 Turbo.

We increased the number of answers generated by various prompting methods. For I-O prompting, we directly adjusted the model's response generation parameter \(n\), which enables the model to \(n\) responses. For CoT, we adjusted not only the parameter \(n\) but also the length \(l\) of the Chain. For SC-CoT, we built on the first two methods by adding an adjustment to the number of voters \(v\).

As shown in Figure 6, although I-O, CoT, and SC-CoT only require simple queries to correct the original errors made by GPT-4 Turbo, their upper limits are not high. Even if we continue to increase the number of queries, they can only correct about 20% of the original errors made by GPT-4 Turbo. In contrast, MACM can correct nearly 90% of the original errors of GPT-4 Turbo when the number of queries is high. This is actually quite reasonable because the MACM structure is more complex, including multiple processes of mining conditions and checking. These processes allow the large model to gradually think and identify errors, thus significantly improving accuracy.

**Proportional Impact of Various Components:** To analyze the functions of each component in MACM, we randomly combined the four components within MACM: Condition Mining, Multi-Agent System, Self-Checking, and Voting, and tested their performance. During the experiment, we maintained a maximum of 2 Condition Mining iterations and used 3 Voters.

As shown in Figure 7, the combination of all four components yields the best performance. Among the individual components, Multi-Agents and Condition Mining have comparable error correction capabilities. In the combinations of two components, the pairing of Self-Checking and Condition Mining shows the best performance. Among the three-component combinations, the combination of Multi-Agents, Condition Mining, and either Voting or Self-Checking achieves better results.

## 5 Conclusion

We introduce MACM, a new and generalizable prompting technique that significantly enhances the inferential capabilities of large language models on mathematical problems. MACM can be applied to different types of mathematical questions. Through comparisons in several experiments on the math related datasets, we have verified the superiority of our method over the original prompting methods. With the aid of MACM, the accuracy of the GPT-4 Turbo model on the MATH dataset has increased by 15.14%. Compared to SC-CoT, its accuracy has increased by 7.8%. For the most challenging level 5 mathematical problems in the MATH dataset, its accuracy increased from 54.68% to 76.73%. In the game of 24 points, using the same GPT-4 model, MACM's accuracy is 17% higher than that of ToT. At the same time, by comparing accuracy with the number of times the large model responds, we find that MACM has a higher limit; increasing the number of responses from the large model can significantly improve accuracy. These experiments demonstrate MACM's generalizability and its powerful error-correction capability for complex mathematical problems in original LLMs.