# Tackling Unconditional Generation for Highly Multimodal Distributions with Hat Diffusion EBM

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This work introduces the Hat Diffusion Energy-Based Model (HDEBM), a hybrid of EBMs and diffusion models that can perform high-quality unconditional generation for multimodal image distributions. Our method is motivated by the observation that a partial forward and reverse diffusion defines an MCMC process whose steady-state is the data distribution when the diffusion is perfectly trained. The components of HDEBM are a generator network that proposes initial model samples, a truncated diffusion model that adds and removes noise to generator samples as an approximate MCMC step that pushes towards realistic images, and an energy network that further refines diffusion outputs with Langevin MCMC. All networks are incorporated into a single unnormalized density. MCMC with the energy network is crucial for driving multimodal generation, while the truncated diffusion can generate fine details needed for high-quality images. Experiments show HDEBM is effective for unconditional generation with sampling costs significantly lower than diffusion models. We achieve an FID score of 21.82 on unconditional ImageNet at 128x128 resolution, which to our knowledge is state-of-the-art among unconditional models which do not use separate retrieval data.

## 1 Introduction

Image generation with deep learning has made tremendous progress in the past decade as models become more sophisticated and available compute increases. Conditional modeling, where auxiliary information such as a label or text is used to guide model synthesis, has led to impressive results for applications such as class-conditioned [3; 7] and text-conditioned [44; 45] generation. While unconditional modeling has also seen great progress, unconditional models often significantly under-perform conditional counterparts. This is especially true for highly multimodal1 and high-resolution datasets such as ImageNet . Improvements in unconditional modeling techniques have the potential to increase our understanding of non-convex probability densities, enable better generation when supervised information is unavailable, and increase the diversity of conditional generations.

Generative adversarial networks (GANs)  and diffusion models  are the most popular current methods for high-resolution image synthesis. Both have drawbacks when it comes to highly multimodal unconditional modeling. The adversarial objective of GAN learning often leads to the mode collapse phenomenon  where the generator model only learns to generate a small subset of the entire data distribution. Diffusion models face the challenge of assigning samples to separate modes early in the reverse diffusion process when image features first start to emerge from noise. The initial phase of the reverse diffusion process can be very sensitive to network changes. Using an exponential moving average (EMA) of the weights can help alleviate this sensitivity by stabilizing outputs across model updates and it is typically a vital part of diffusion learning.

We explore the potential of the energy-based model (EBM) as a method for highly multimodal unconditional learning. While existing EBMs often do not match the performance of GANs and diffusion models for low-resolution data, the recently introduced Hat EBM  showed surprisingly strong performance on high-resolution unconditional ImageNet generation. Nonetheless, Hat EBM does not achieve state-of-the-art results. In this work, we build upon Hat EBM to develop a new model that achieves state-of-the-art unconditional synthesis for ImageNet 128x128.

A fundamental obstacle of EBM learning is the computational burden of the MCMC inner loop used for each model update. Computational restrictions allow only shortrun MCMC trajectories during training, limiting the fine image details the EBM generates. Diffusion models decouple learning from MCMC sampling and thereby can use many steps during test-time generation to create fine image details. Incorporating diffusion sampling into the EBM sampling process has the potential to greatly improve generation quality while preserving the relatively fast sampling speed and wide mode coverage of EBMs.

Our key insight is that adding noise to an image and removing noise with a perfectly trained diffusion model defines an MCMC trajectory whose steady-state is the data distribution (see Section 3.2). Building on this, we propose to add and remove noise from generator samples using a truncated diffusion model as an initial approximate MCMC step, followed by further Langevin MCMC refinement from the EBM. To enable the truncated diffusion to be incorporated into EBM learning, we train it separately and distill it to a single step using progressive distillation . Similar to the approach in , we only train the truncated part of the diffusion model near the data distribution and ignore the higher noise levels. This bypasses the most challenging parts of diffusion learning and greatly reduces the size of the diffusion model without sacrificing denoising performance. The difficulty of the training and distillation process is greatly reduced for truncated diffusion compared to full diffusion. Once trained, the truncated and distilled diffusion is incorporated into Hat EBM between the generator and energy network. It can be viewed both as an extension of the generator that refines the base generator output and as an extension of the MCMC sampling phase using an approximate sampling step. We call this model the Hat Diffusion EBM (HDEBM). Experiments show HDEBM significantly improves sample quality compared to Hat EBM without significant increase in computation cost beyond training the truncated diffusion. Curated samples from HDEBM along with model energy structure are shown in Figure 1. In summary, our main contributions are listed below.

* We introduce the novel perspective that a partial forward and reverse process for a perfectly trained diffusion model defines an MCMC trajectory whose steady-state is the data distribution.
* We develop the Hat Diffusion EBM (HDEBM) modeling framework that incorporates a truncated and distilled diffusion into Hat EBM to help train the generator and energy network. All networks are incorporated into a single unnormalized density.
* Experiments on CIFAR-10  and Celeb-A 64x64  show that HDEBM has state-of-the-art synthesis performance among explicit EBMs. Experiments on unconditional ImageNet 128x128 show that, to our knowledge, Hat EBM has state-of-the-art synthesis performance among models that do not use retrieval data during test time.

Figure 1: _Left:_ Unconditional ImageNet 128x128 samples generated by HDEBM. _Right:_ Visualization of energy function structure in HDEBM. \(G_{1}\) creates an initial image from noise \(Z_{1}\) which is passed through a forward/reverse truncated diffusion in \(G_{2}\). \(G_{2}\) then performs approximate MCMC on the data distribution. The output is adjusted with residual image \(Y\) to create a visible image \(X\) for forward pass energy calculation with \(H\). Sampling uses Langevin MCMC via backpropagation.

Related Work

EBMs.Energy Based Models (EBMs) are a class of generative models that aim to represent a data distribution with an unnormalized density. Early work includes Boltzmann machines [1; 46], and the Filters Random field And Maximum Entropy (FRAME) model . Recent advancements in deep learning have led to investigations in using Convolutional Network based EBM models  increasing the image synthesis [38; 10] abilities. The community has also trained EBMs with auxiliary models. One approach trains the EBM using direct outputs from the generator without MCMC sampling , which is further explored by methods such as EGAN  and the VERA model . Cooperative learning  uses a generator to initialize MCMC samples used to train the EBM and uses a reconstruction loss between the EBM samples and the generator output to update the generator. EBMs defined in latent space [39; 40] have also been explored as the energy landscape in latent space can provide better movement along the complex image manifold. The closest work to our approach is Hat EBM  which builds upon  to incorporate the generator latent space into the unnormalized density. We provide a comparison between Hat EBM and HDEBM in Appendix C.1.

Diffusion Models.Diffusion models [49; 21; 50] are based on the notion of adding and removing noise in order to learn underlying patterns of a dataset. The slow sampling speed of early models has been significantly expedited with acceleration techniques [50; 48; 59; 31; 60; 32], several of which are related to our approach. DDIM  employs a class of non-Markovian diffusion processes to define a faster deterministic sampling method. Truncated diffusion trajectories, wherein only fragments of the forward and reverse processes are performed, have been appended to other kinds of generative models to improve sample quality [60; 32]. Truncated diffusion models have also found applications in image editing  and adversarial defense . We build upon these works by noting that an ideal truncated diffusion defines an approximate MCMC process with the data distribution as its steady-state, which can serve as a tool for instructing other networks. A comparison between HDEBM and other methods [60; 32] that employ truncated diffusion for generation is provided in Appendix C.2. Progressive distillation  trains a series of student networks to match the DDIM paths of teacher networks. The distilled model obtains high quality samples with only a few steps.

## 3 Method

This section first presents background theory for learning EBMs and for learning and distilling diffusion models. Next, we discuss how truncated and distilled diffusion can be viewed as an efficient approximate MCMC update. Finally, HDEBM model formulation and training methods are presented.

### Background

EBM Learning.Our EBM learning follows the methods from [20; 57; 61]. An EBM is defined as

\[p(x;)=()}\{-U(x;)\}\] (1)

where \(U(x;)\) is a deep neural network with parameters \(\), \(x\) is an image state, and \(Z()\) is an intractable normalizing function. A maximum likelihood objective is used to minimize the Kullback-Leibler divergence \(_{}D_{KL}(q_{0}(x) p(x;))\), where \(q_{0}(x)\) is the true and unknown data distribution, by using stochastic gradient descent

\[()_{i=1}^{n}_{}U(X_{ i}^{+};)-_{i=1}^{n}_{}U(X_{i}^{-};)\] (2)

where \(X_{i}^{+}\) are samples from the data distribution and \(X_{i}^{-}\) are samples from the model \(p(x;)\). To obtain samples from the model, MCMC sampling with \(K\) steps of the Langevin equation is used:

\[X^{(k+1)}=X^{k}-}{2}_{X^{(k)}}U(X^{(k)};)+  V_{k}\] (3)

where \(X^{(k)}\) is the sample at step \(k\), \(\) is the step size, and \(V_{k} N(0,I)\). Generating negative samples also requires an initialization strategy to obtain the initial states \(\{X_{i,0}^{-}\}_{i=1}^{n}\).

Diffusion Learning and Distillation.This section provides a concise review of diffusion models, truncated diffusion, and distilled diffusion. We denote data distribution as \(X q_{0}\) and consider \(q_{t}\) for \(t[0,T]\) as the forward process which produces noisy samples by adding Gaussian noise to the data. Specifically, the noisy samples \(x_{t}\) can be parameterized given \(_{t}\) and \(_{t}\), such that the log signal-to-noise-ratio, \(_{t}=(_{t}^{2}/_{t}^{2})\), decreases monotonically over time \(t\). The forward process can be defined by a Gaussian process constituting a Markov chain:

\[q_{t}(x_{t}|x)=N(x_{t};_{t}x,_{t}^{2}I), q_{t}(x_{t}|x_{s})=N( x_{t};(_{t}/_{s})x_{s},_{t|s}^{2}I),\] (4)

where \(0 s<t T\) and \(_{t|s}^{2}=(1-e^{_{t}-_{s}})_{t}^{2}\). To sample from data distribution \(q_{0}\), we first sample from \(q_{T}\) then sample reverse steps until we reach \(x\). As suggested by  and following works, we can construct a neural denoiser \(_{}(x_{t})\) to predict an estimate of \(x\), and learn a model using a weighted mean-squared-error loss:

\[()=E_{X q_{0},t U[0,T],x_{t} X_{t}(|x)}[w( _{t})\|_{}(X_{t})-X\|_{2}^{2}].\] (5)

In this work, we train truncated diffusions which only use part of the forward/reverse process as in . This simply involves changing the sampling distribution of \(t\) from \(U[0,T]\) to \(U[0,T^{}]\) for \(T^{}<T\). We limit our training to either the final \(T^{}=256\) or \(T^{}=512\) steps of a discrete cosine schedule with \(T=1000\) steps.

A DDIM sampler  can achieve fast, high-quality, and deterministic generation. Our works utilizes progressive distillation of DDIM to further accelerate sampling . Student models are trained so that one DDIM step of the student model matches two DDIM steps of the teacher model. The initial teacher is the full diffusion. After convergence the student becomes the new teacher, halving the number of steps until the entire denoiser consists of a single step.

### Truncated Diffusion as MCMC Sampling

In this section we develop a theoretical understanding of the truncated diffusion process used as part of the HDEBM. We begin with a straightforward proposition that demonstrates the central claim.

**Proposition 3.2.1**.: _Suppose \(D(x,t)\) is the DDIM reverse process starting at timestep \(t\) for a perfectly trained diffusion model, meaning that if \(X^{} q_{t}\) then \(D(X^{},t) q_{0}\). Further suppose that the support of \(q_{0}\) is contained within a compact set. Then the stochastic update_

\[X D(_{t}X+_{t}Z,t) Z N(0,I)\] (6)

_defines an ergodic MCMC process with a unique steady-state distribution \(q_{0}\) for any timestep \(t\)._

Proof.: It is clear that (6) is a Markov transition since the updated state only depends on the starting state \(X\) and noise. The forward process by definition has the property that \(X^{}=_{t}X+_{t}Z q_{t}\) if \(X q_{0}\) and \(Z N(0,I)\). Therefore if \(X q_{0}\) then \(Y=D(_{t}X+_{t}Z,t) q_{0}\) under the assumption that \(D\) is perfectly trained. This shows that the Markov update (6) preserves the data distribution \(q_{0}\). We note the process is aperiodic and irreducible because adding Gaussian noise to \(X\) can map to any image state with non-zero probability, and the assumption that \(D\) is perfect means there is always some image in \(q_{t}\) that will map to a given image in the support of \(q_{0}\). Since the support of \(q_{0}\) is contained in a compact set, the chain must be recurrent so that the process is ergodic and \(q_{0}\) is the unique steady-state. 

We view the proposition as a useful insight that allows a more principled framework for using truncated diffusion than the empirical perspective presented in prior works. SDEdit  empirically observes that a truncated diffusion process can add realism to naive edits or rough user-defined templates. Understanding the truncated diffusion as an approximate MCMC step on the data distribution gives a clearer picture of why this occurs since we expect MCMC to move out-of-distribution states towards the data distribution while still retaining some features of the original state due to MCMC autocorrelation. The same observation applies to the DiffPure defense  which uses truncated diffusion to remove adversarial signals while preserving most of the original image appearance.

The proposition applies to any timestep \(t>0\). The value of the \(t\) determines how far the MCMC step travels across the data distribution. In the limiting case \(t=T\) the MCMC step samples independent images from the full diffusion each time. Large values of \(t\) greatly changes image appearance in each step (lower MCMC autocorrelation) while small values of \(t\) retains more of the original image appearance (higher MCMC autocorrelation). This is analogous to the discussion of the role of noise in SDEdit . Additionally, we expect that truncated diffusions are much easier to learn when \(t\) is small and much harder to learn when \(t\) is large because smaller \(t\) define an easier denoising problem while larger \(t\) require more coordination between timesteps to drive noisy samples to different modes.

We further develop truncated diffusion as a modeling tool by observing a synergy between learning a truncated diffusion and learning a distilled diffusion. As noted in , a challenging aspect of learning a distilled diffusion is that the diffusion network output for a noise image at \(t=T\) provides essentially no information about the final state before distillation, while once the model is distilled to a single step the final image must be fully predicted from noise. When distilling a truncated diffusion, noisy images at \(t=T^{}\) have many features of the original image and diffusion network outputs can retain many of these features throughout distillation while refining overall appearance. In contrast to full diffusiontillation  we find only a minor performance gap between the undistilled truncated diffusion and the truncated diffusion distilled to a single step.

In summary, we view truncated and distilled diffusion as an efficient tool that can perform approximate MCMC sampling with updates that can travel much further along with image manifold than conventional methods like Langevin MCMC. After distillation, it becomes computationally feasible to perform forward/backward passes through the MCMC step to train other networks. There is a significant need for MCMC tools with better movement along complex manifolds and truncated diffusion MCMC has the potential to fill this gap. We note there are several challenges before truncated diffusion MCMC can be a general purpose tool. The process is approximate in practice, non-reversible, and lacks an explicit transition density function so that Metropolis-Hastings correction is not immediately applicable. In this work, we show that the unadjusted process is a useful tool for teaching generator and energy networks. We hope that this tool can be adapted into a rigorous and general purpose MCMC transition in future works.

### Hat Diffusion EBM

This section describes the HDEBM model formulation and training process. We assume that a truncated and distilled diffusion network \(D(x)\) that approximately maps the noisy distribution \(q_{T^{}}\) to the data distribution \(q_{0}\) in a single forward pass has been trained and frozen. The choice of \(T^{}\) depends on the dataset. We first adapt the synthesis-oriented training of Hat EBM to jointly train the energy network and generator network. In this stage of training, samples are generated by drawing latent Gaussian random variables and performing MCMC on a residual image conditioned on frozen latents. We then perform a second stage of training that adapts the energy function from a form where latents must be frozen after initialization to a form where both latents and image residuals can be updated with MCMC sampling. The ability to perform MCMC refinement in both the image and latent space can greatly improve sample quality over the first stage model. The second stage model has more appealing properties as an explicit energy model since the intractable normalizer does not depend on the latent state.

Following the Hat EBM formulation, we make the assumption that data samples \(X q_{0}\) can be decomposed as \(X=G(Z)+Y\) where \(G\) is a generator network, \(Z\) is a latent random variable, and \(Y\) is a random variable which functions as a residual image to bridge the gap between the generator output manifold and the data manifold. In the first stage of learning we assume \(Z N(0,I)\) and we learn the distribution \(Y|Z\). In the second stage we learn the joint distribution of \(Z\) and \(Y\).

Figure 2: Visualization of Stage 1 (left) and Stage 2 (right) training methods for HDEBM. Red arrows indicate that the initial random variable is updated using Langevin MCMC according to the given density. Langevin with \(p(Y|G(Z_{1},Z_{2};);)\) and \(p(Y,Z_{1},Z_{2};)\) uses the same equations as .

Our central modification is to define \(G\) as

\[G(z)=G_{2}(G_{1}(z_{1}),z_{2}), G_{2}(x,z)=D(_{T^{ }}x+_{T^{}}z)\] (7)

where \(z=(z_{1},z_{2})\). \(G_{1}\) creates initial image proposals from noise which are refined by the forward and reverse process of \(G_{2}\). Depending on the context we use the notation \(G_{1}(z_{1})\) to show a fixed generator and \(G_{1}(z_{1};)\) to show a learnable generator with weights \(\). When \(G_{1}\) is learnable we denote the entire generator as \(G(z_{1},z_{2};)\). The noising and denoising process applied to \(G_{2}\) can be interpreted as an MCMC step that pushes the initial generator output closer to the data distribution. In the second stage, we refine both \(z_{1}\) and \(z_{2}\) using MCMC initialized from \(N(0,I)\). A diagram of each training stage is show in Figure 2.

#### 3.3.1 First Stage: Residual Distribution Conditioned on Fixed Latent

This section describes how to train a model that can create samples by drawing a Gaussian random variable \(Z N(0,I)\), passing \(Z\) through a generator network to create initial images, and then refining these image samples using MCMC with an energy network while leaving latent variables fixed. The methodology takes inspiration from cooperative learning  and Hat EBM . It is difficult to formulate a single maximum likelihood learning framework to train both the generators and energy. Therefore we follow prior work and use two maximum likelihood objectives: one to train the energy network assuming the generator is fixed and another to train the generator assuming the energy is fixed. Intuitively the energy objective will teach the EBM the best way to refine a fixed generator and the generator objective will teach the generator how to best close the gap between its current samples and refined EBM samples. The generator is trained using purely synthetic data from its own outputs and EBM refinement. Following prior work, in practice we alternate between updates of the EBM and generator.

To train the energy function, we assume that \(G_{1}\) and \(G_{2}\) are both fixed and use \(G(Z)\) to denote the entire generator process (7). The model density is given by

\[p(y,z;)=_{z}()}\;p_{0}(z)\{-H(G(z)+y;)\}\] (8)

where \(H(x;)\) gives the energy output from the sum of the generator and residual image and \(p_{0}\) is the \(N(0,I)\) density. Learning the weights \(\) is identical to Hat EBM learning with a different generator structure. To obtain negative samples, we initialize \(Z^{-} N(0,I)\) and \(Y^{-}=0\) and then use shortrun Langevin sampling (about 50 steps) to obtain \(Y^{-}|Z^{-}\). We assume the data distribution has the form \(X=G(Z)+Y\) where \(Z N(0,I)\) and \(Y|Z q_{0}(y|z)\) for some unknown distribution. Learning uses the standard EBM update (2) with an energy form \(U(y|z;)=H(G(z)+y;)\) where data samples \(X^{+} q_{0}\) are sufficient statistics for updating \(\) and \((Y^{+},Z^{+})\) do not need to be inferred.

To update the generator \(G_{1}(z_{1};)\), we assume that we have a fixed energy network \(H(x)\) and a fixed generator \(G(z)\) from the current model. We treat the shortrun MCMC process with the density (8) used to generate negative samples as the ground truth distribution. Specifically, while updating the generator we assume that the data distribution is the joint distribution \((X,Z)\) where \(Z N(0,I)\) and \(X=G(Z)+Y\) where \(Y\) is generated from short-run MCMC using the energy \(U(y|z)=H(G(z)+y)\) initialized from \(Y=0\). We denote the shortrun MCMC distribution as \(s(x|z)\). We aim to train the generator \(G(z_{1},z_{2};)=G_{2}(G_{1}(z_{1};),z_{2})\) to match this distribution. No real data is used to train the generator. Even with perfect learning, the samples from the updated generator can be no better than samples from the current HDEBM model. The goal is instead to match the current HDEBM samples with only the updated generator to provide a better MCMC initialization for future HDEBM learning iterations.

The form of the learnable generator distribution is a key design choice of HDEBM. The latent distribution is set to be \(Z N(0,I)\) and we learn the conditional density \(p(x|z;)\). We propose to use two energy terms: one which encourages the output of \(G(z_{1},z_{2};)\) to be close to refined EBM samples, and one which encourages the output of \(G_{1}(z_{1};)\) to be close to refined EBM samples. The density is given by

\[p(x|z;)=}\{-_{1}\|x-G(z_{1},z_{2};) \|^{2}\}\{-_{2}\|x-G_{1}(z_{1};)\|^{2}\}\] (9)

which is the product of the Gaussians \(N(G(z_{1},z_{2};),_{1}^{-}{{2}}})\) and \(N(G_{1}(z_{1};),_{2}^{-}{{2}}})\) for constants \(_{1},_{2}\). The constants \(_{1},_{2}\) allow a trade-off between the importance of the energy terms. Since the product of two Gaussians is a Gaussian whose standard deviation does not depend on the Gaussian means of the product terms, the normalizer \(\) does not depend on \(\) and the maximum likelihood objective can be written in closed form:

\[^{*}=_{}E_{p_{0}(z)s(x|z)}[_{1}\|X-G(Z_{1},Z_{2}; )\|^{2}+_{2}\|X-G_{1}(Z_{1};)\|^{2}].\] (10)

The first term will adjust the output of \(G_{1}(z_{1};)\) so that the entire generator \(G(z_{1},z_{2};)\) produces a sample that resembles an EBM sample after the output of \(G_{1}\) goes through the noising and denoising step from \(G_{2}\). This greatly eases the burden of training the generator compared to the approach in Hat EBM. Assuming that EBM samples are close to the data distribution, the forward/reverse diffusion from \(G_{2}\) will naturally push samples from \(G_{1}\) towards the target distribution. \(G_{1}\) can learn to produce any distribution whose samples are mapped to samples close to the data distribution when \(G_{2}\) is applied. The data distribution itself is one such possibility, but there are others which are easier to learn including images which resembled smoothed data or even images with artifacts that exploit imperfections in the diffusion \(G_{2}\). The function of this loss term can be interpreted as training \(G_{1}\) to invert \(G_{2}\) given forward noise \(Z_{2}\) and target image \(X\).

The second term will encourage the output of \(G_{1}\) to match EBM without considering \(G_{2}\). We view this term as a regularizer. Since \(G_{1}\) can learn many possible distributions that match noisy data after forward noise is applied, this term can encourage \(G_{1}\) to find a solution that resembles the data. In practice, we find that the first term is essential for good synthesis results while including the second term with a small \(_{2}\) can in some cases yield slight improvements. In other cases we set \(_{2}=0\) and only use the first term.

In practice we alternate between one update of \(H(x;)\) and one update of \(G_{1}(z_{1};)\). Additionally, we maintain a bank of pairs \(\{(X^{(i)},Z^{(i)})\}\) and draw batches from this bank to update the generator, after which the batch states are overwritten with pairs from the most recent EBM update. As in Hat EBM, we find this is more effective than using only the most recent EBM samples to update \(G_{1}\) because the most recent EBM samples can sometimes lack diversity while historical samples tend to have good diversity. Training the generator on samples with limited diversity can cause instability as the EBM tries to compensate by strongly adjusting the MCMC paths. We experimented with performing 10 steps to update the EBM followed by 10 steps to update the generator without the historical bank and saw good initial results. However this training strategy requires twice the amount of MCMC since fresh samples are needed to update both the generator and EBM. It also requires a second copy of the weights of \(G\) in GPU memory since sampling from \(s(x|z)\) requires a frozen copy if more than one generator update is used. To save memory and compute we use the historical bank approach. See Appendix D.3.1 for training pseudocode.

#### 3.3.2 Second Stage: Joint Residual and Latent Distribution

The second stage of training will finetune a model \(H(x;)\) which is pretrained as a density of the form (8) to become a density of the form

\[p(y,z;)=()}\{-H(G(z)+y;)\}.\] (11)

The primary difference between (11) and (8) is that the normalizer of the former depends only on \(\) while the normalizer of the latter depends on both \(\) and \(z\). This means that we can perform MCMC on \(z\) for the density (11) but not for (8). We use the second stage as a way to refine the initial generator appearance which might have blurs or artifacts that can be corrected by local movement. We leave both \(G_{1}\) and \(G_{2}\) frozen during the second stage and we initialize \(\) from the weights of the first phase. Negative samples are obtained from alternating Langevin steps of \(Y\) and \(Z\) initialized from \(Y=0\) and \(Z N(0,I)\). The first stage is critical for aligning the output of \(G(z)\) to produce realistic images near \(N(0,I)\), which provides high-quality MCMC initialization for the second phase. The EBM update uses the same equation as the first stage. We experimented with including the Gaussian prior \(p_{0}(z)\) in (11) but found negligible effect. See Appendix D.3.2 for training pseudocode.

## 4 Experiments

We now present our HDEBM experiments for unconditional generation. All networks used in our experiments are trained from scratch using only unconditional data from a single dataset. Each experiment involves three rounds of training. First, we train the truncated diffusion and distill it to a single step. As described in  the entire distillation takes about the same time as training the initial truncated diffusion. Since we are only training the truncated diffusion, compute is significantly less than required for full diffusion training. The diffusion network is frozen after training. We then train the first stage HDEBM using Algorithm 1. This is the most compute intensive part of training. Finally, we freeze the generator \(G_{1}\) and initialize the energy network weights \(\) from the first stage weights to perform second stage training using Algorithm 2. This training converges rapidly and the cost is minor. See Appendix D for a thorough discussion of experimental details.

Datasets.We experiment with CIFAR-10 , Celeb-A  at resolution 64x64, and ImageNet  at resolution 128x128. Following standard procedure, we train and evaluate our models using only the training sets. For CIFAR-10 we trained the truncated diffusion using the first \(T^{}=256\) timesteps of a \(T=1000\) step diffusion with the cosine schedule from  and for Celeb-A and ImageNet we used the first \(T^{}=512\) timesteps of the same schedule. We use 4 A100 GPUs to train CIFAR-10 models and 8 A100 GPUs to train Celeb-A and ImageNet models.

### Unconditional Generation

Our main experiments are unconditional generation on CIFAR-10, Celeb-A 64x64, and ImageNet 128x128. Table 1 presents FID scores for the first and second stages of our model, along with a comparison to a representative selection of existing models. Overall, our results show that HDEBM achieves state-of-the-art (SOTA) synthesis results among explicit EBMs for CIFAR-10 and Celeb-A. Furthermore, HDEBM achieves an FID score of 21.82 for unconditional ImageNet at 128x128 resolution which, to our knowledge, is SOTA for unconditional image generation without separate retrieval data.

To our knowledge, the generative modeling literature does not include a clear SOTA diffusion baseline for unconditional ImageNet at 128x128 resolution. At 256x256 resolution, unconditional ADM  achieves an FID score of 26.21 and RDM  achieves an FID of 12.21 with external retrieval data. RDM uses CLIP  encodings and therefore implicitly relies on the large-scale (text, image) dataset used to train CLIP. This complicates the unconditional modeling scenario. Modeling ImageNet at resolution 256x256 is beyond the 8 GPU budget used in this work and we hope to scale in future works for direct comparison to higher-resolution SOTA models.

For semi-unconditional ImageNet diffusion at 128x128 resolution, we include results from the recent work  which trains an unconditional diffusion model at 128x128 resolution and uses classifier guidance with standard reverse sampling (FID score of 30.46) and UHMC sampling (FID score of 26.89). It is likely that these models are not as highly optimized as ADM since FID scores at a lower resolution and with classifier guidance do not match unconditional ADM at a higher resolution. This highlights the difficulty of training highly optimized unconditional diffusion models. Overall, there is strong evidence that HDEBM can be competitive with or surpass highly optimized unconditional diffusion at 128x128 resolution. HDEBM will likely not match highly optimized retrieval augmented diffusion. We view the retrieval strategy as orthogonal to our approach and believe retrieval augmented HDEBM could yield further improvement in future work.

  \\  Model & FID \\  VERA  & 27.5 \\ Improved C and EBM  & 25.1 \\ Hat EBM  & 19.30 \\ CopyFlow  & 15.80 \\ VAEBH  & 12.19 \\ Diff. Rove. EDM  & 9.58 \\ CLEL  & 8.61 \\ HDEBM (Stage 1) _(ours)_ & 8.40 \\ HDEBM (Stage 2) _(ours)_ & **8.06** \\  NCSNv2 & 10.9 \\ DDPM  & 3.2 \\ StyleGAN2-ADA  & 2.92 \\ NCSN+  & **2.20** \\     \\  Model & FID \\  Divergence Triangle  & 31.9 \\ Hat EBM (small) * & 43.89 \\ Diff. Recov. EDM & 5.98 \\ HDEMM (Stage 1) _(ours)_ & 5.55 \\ HDEMM (Stage 2) _(ours)_ & **4.13** \\  NVAE  & 14.7 \\ NCSNv2  & 10.2 \\ QA-GAN  & 6.42 \\ Diffusion Autoencoder  & 5.30 \\ COCO-GAN  & 4.0 \\ DDMM  & 3.5 \\ PNDM  & **2.71** \\     \\  Model & FID \\  InfoMax GAN  & 58.9 \\ Hat EBM (small) * & 43.89 \\ Hat EBM (large) * & 31.89 \\ SS-GAN (small)  & 43.9 \\ SS-GAN (large)  & 23.4 \\ HDEMM (Stage 1) _(ours)_ & 28.08 \\ HDEBM (Stage 2) _(ours)_ & **21.82** \\  Cond. EBM  & 43.7 \\ Diffusion + c.g.  & 30.46 \\ UHMC Diffusion + c.g.  & 26.89 \\ UHDM  & 3.5 \\ PNDM  & **2.71** \\     \\  Model & FID \\  InfoMax GAN  & 58.9 \\ Hat EBM (small) * & 43.89 \\ Hat EBM (large) * & 31.89 \\ SS-GAN (small)  & 43.9 \\ SS-GAN (large)  & 23.4 \\ HDEMM (Stage 1) _(ours)_ & 28.08 \\ HDEBM (Stage 2) _(ours)_ & **21.82** \\  Cond. EBM  & 43.7 \\ Diffusion + c.g.  & 30.46 \\ UHMC Diffusion + c.g.  & 26.89 \\ UHDM  & 3.5 \\ PNDM  & **2.71** \\     \\  Model & FID \\  InfoMax GAN  & 58.9 \\ Hat EBM (small) * & 43.89 \\ Hat EBM (large) * & 31.89 \\ SS-GAN (small)  & 43.9 \\ SS-GAN (large)  & 23.4 \\ HDEMM (Stage 1) _(ours)_ & 28.08 \\ HDEBM (Stage 2) _(ours)_ & **21.82** \\  Cond. EBM  & 43.7 \\ Diffusion + c.g.  & 30.46 \\ UHMC Diffusion + c.g.  & 26.89 \\ UHDM  & 3.5 \\ PNDM  & **2.71** \\     \\  Model & FID \\  InfoMax GAN  & 58.9 \\ Hat EBM (small) * & 43.89 \\ Hat EBM (large) * & 31.89 \\ SS-GAN (small)  & 43.9 \\ SS-GAN (large)  & 23.4 \\ HDEMM (Stage 1) _(ours)_ & 28.08 \\ HDEBM (Stage 2) _(ours)_ & **21.82** \\  Cond. EBM  & 43.7 \\ Diffusion + c.g.  & 30.46 \\ UHMC Diffusion + c.g.  & 26.89 \\ UHDM  & 3.5 \\ PNDM  & **2.71** \\   
  \\  Model & FID \\  InfoMax GAN  & 58.9 \\ Hat EBM (small) (small) * & 43.89 \\ Hit EBM (large) * & 31.89 \\ SS-GAN (small)  & 43.9 \\ SS-GAN (large)  & 23.4 \\ HDEMM (Stage 1) _(ours)_ & 28.08 \\ HDEBM (Stage 2) _(ours)_ & **21.82** \\  Cond. EBM  & 43.7 \\ Diffusion + c.g.  & 30.46 \\ UHMC Diffusion + c.g.  & 26.89 \\ UHDM  & 3.5 \\ PNDM  & **2.71** \\  

Table 1: Comparison of FID scores among representative generative models. For CIFAR-10 and Celeb-A, all FID reports are for unconditional models. EBMs are above the dividing line and other models are below. For ImageNet, models above the dividing line are unconditional and models below use label information. (*=re-evaluated using evaluation code from , c.g.=classifier guidance)

[MISSING_PAGE_FAIL:9]