# Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer

Shuang Wu\({}^{1,2}\) Youtian Lin\({}^{2}\) Feihu Zhang\({}^{1}\) Yifei Zeng\({}^{1,2}\) Jingxi Xu\({}^{1}\)

**Philip Torr\({}^{3}\)\({}^{}\) Xun Cao\({}^{2}\) Yao Yao\({}^{2}\)\({}^{}\)**

\({}^{1}\)DreamTech \({}^{2}\)Nanjing University \({}^{3}\)University of Oxford

{wushuang,lingyoutian}@smail.nju.edu.cn yaoyao@nju.edu.cn

Equal contribution.

###### Abstract

Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions. In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multi-view diffusion model or SDS optimization. Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (**D3D-VAE**) and a Direct 3D Diffusion Transformer (**D3D-DiT**). D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space. Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods that rely on rendered images as supervision signals. D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets. Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic-level and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input. Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation. Project page: https://www.neural4d.com/research/direct3d.

## 1 Introduction

In recent years, substantial advancements have been made in 3D shape generation through the utilization of diffusion models [13; 51]. Inspired by the efficacy demonstrated in text-to-2D image generation, these methods seek to extend the capabilities of diffusion models to the realm of 3D shape generation through extensive training on diverse 3D datasets. Various approaches have explored diverse 3D representations, including point clouds [37; 38], voxels , and SDFs , aiming not only to faithfully capture object appearance but also to preserve intricate geometric details. However, existing large-scale 3D datasets, such as ObjverseXL , are constrained both in the quantity anddiversity of shapes compared to their 2D counterparts like Laion5B , which contains 5 billion images, while ObjverseXL only comprises 10 million 3D shapes.

To address this limitation, many existing methods [5; 4; 9; 26; 27; 28; 29; 31; 34; 43; 65] employ a pipeline where multi-view images of an object are initially generated from a single image using a multi-view diffusion model. Subsequently, techniques such as sparse view reconstruction methods [23; 32; 55; 62] or score distillation sampling (SDS) optimization [42; 43; 49; 54] are applied to fuse these multi-view images into 3D shapes. While this pipeline can result in high-quality 3D shape creation, the indirect generation from multi-view images raises efficiency concerns. Additionally, the quality of the resulting shape is heavily dependent on the fidelity of the multi-view images, often leading to detail loss or reconstruction failures.

In this paper, we eschew the conventional approach of indirectly generating multi-view images and instead advocate the direct generation of 3D shapes from single-view images, leveraging a native 3D diffusion model. Inspired by the success of latent diffusion models in 2D image generation, we propose the utilization of a 3D variational auto-encoder (VAE)  to encode 3D shapes into a latent space, followed by a diffusion transformer model (DiT)  to generate 3D shapes from this latent space, conditioned on an image input. However, efficiently encoding a 3D shape into a latent space conducive to diffusion model training is challenging, as is decoding the latent representation back into 3D geometry. Previous approaches have employed multi-view images as indirect supervision [19; 22; 35; 61] through differentiable rendering, but still encounter accuracy and efficiency issues. To

Figure 1: Direct3D is a novel image-to-3D generation method that directly trains on larger-scale 3D datasets and performs state-of-the-art generation quality and generalizability. We achieve this by designing a novel 3D latent diffusion model to take an image as the prompt and generate high-quality 3D shapes that highly consistent with input images. As shown above, our method can generate 3D shapes from existing text-to-image diffusion models, which indicates that our method generalizes to in-the-wild images, while it only trains on 3D data.

address these challenges, we employ a transformer model to encode high-resolution point clouds into an explicit triplane latent, which has been widely used in 3D reconstruction methods  for its efficiency. While the latent triplane is intentionally set with a low resolution, we introduce a convolutional neural network to upsample the latent resolution and decode it into a high-resolution 3D occupancy grid. Furthermore, to ensure precise supervision of the 3D occupancy grid, we adopt a semi-continuous surface sampling strategy, enabling the sampling and supervision of surface points in both continuous and discrete manners. This approach facilitates the encoding and reconstruction of 3D shapes within a compact and continuous explicit latent space.

For image-to-3D generation, we further leverage an image input as a condition to the 3D diffusion transformer. This involves arranging the 3D latent space as a combination of three orthogonal views of a 3D shape and incorporating pixel-level image information into each DiT block to enhance conditional consistency. Furthermore, we introduce cross-attention layers into each DiT block to incorporate semantic-level image information, thereby facilitating the generation of high-quality 3D shapes consistent with input images.

We demonstrate the high-quality 3D generation and strong generalization abilities of the proposed Direct3D approach through extensive experiments. Figure 1 illustrates the 3D generation results of our method on the in-the-wild images generated from text-to-image model. To summarize, the major contributions of this work include:

* We introduce Direct3D, to our best knowledge, the first native 3D generative model scalable to in-the-wild input images (e.g., from Flux , Hunyuan-DiT  or SDXL ). This enables high-fidelity image-to-3D generation without the need for multi-view diffusion models or SDS optimization.
* We propose D3D-VAE, a novel 3D variational auto-encoder effectively encoding a 3D point cloud into a triplane latent. Instead of using rendered images as supervision signals, we supervise the decoded geometry directly using a semi-continuous surface sampling strategy to preserve detailed 3D information in the latent triplane.
* We present D3D-DiT, a scalable image-conditioned 3D diffusion transformer capable of generating 3D asserts consistent with input images. The D3D-DiT is specially designed to better fuse the positional information from the latent triplane and effectively integrates pixel-level and semantic-level information from the input image.
* We demonstrate through extensive experiments that our large-scale pre-trained Direct3D model surpasses previous image-to-3D approaches in terms of generation quality and generalization ability, setting a new state-of-the-art for the task of 3D content creation.

## 2 Related Work

### Neural 3D Representations for 3D Generation

Neural 3D representations are essential for 3D generation tasks. The introduction of Neural Radiance Fields (NeRF)  has significantly advanced 3D generation. Building on NeRF, DreamFusion  introduced a Score Distillation Sampling (SDS) method to generate 3D shapes using an off-the-shelf 2D diffusion model from any text prompt. Many subsequent methods have explored various representations to enhance the speed and quality of 3D generation. For instance, Magic3D  improves generation quality by introducing a second stage using the DMet  representation, which combines Signed Distance Function (SDF) with a tetrahedral grid to represent the 3D shape.

Beyond SDS-based methods, some approaches use directly trained networks to generate different representations [16; 17; 59]. For example, LRM  uses triplane NeRF representations as network outputs, significantly speeding up the generation process, albeit with some loss in quality. Another approach, One-2-3-45++ , proposes to use a 3D occupancy grid as the output representation to enhance geometric quality.

### Multi-view Diffusion

Following the success of novel view prediction methods using diffusion models, such as Zero123 , which generates different unknown views of an object from a single image and text guidance.

MVDream  extends novel view diffusion to generate multiple views of an object at once, improving consistency across views. Imagedream  further enhances generation quality by introducing a novel image conditional module. Some methods adopt this approach to first generate multi-view images of an object and then reconstruct the 3D shape from these views using sparse reconstruction [23; 31; 55; 62]. Instant3D  proposes a reconstruction model that takes four multi-view images as input and reconstructs a NeRF representation of the 3D shape. Many subsequent methods have improved on this by enhancing multi-view or reconstruction models [53; 57; 58].

### Direct 3D Diffusion

Despite the challenges of directly training a 3D diffusion model, such as the lack of a diffusible 3D representation, various strategies have been explored. One line of work fits multiple NeRFs to obtain a neural representation of 3D datasets and then applies a diffusion model to generate NeRFs from this learned representation . However, separate training of NeRFs can hinder the diffusion model's ability to generalize to more diverse 3D shapes. 3DGenNeural  proposes joint training of triplane fitting of the 3D shape with occupancy as direct supervision to train the triplane reconstruction model.

Another line of work leverages VAEs to encode 3D shapes into a latent space and trains a diffusion model on this latent space to generate 3D shapes [15; 19; 22; 61]. For instance, Shap-E  uses a pure transformer VAE to encode a point cloud and image of a 3D shape into an implicit latent space, which is then recovered into a NeRF and SDF field. 3DGen  encodes only the point cloud of a 3D shape into an explicit triplane latent space, enhancing generation efficiency. Similar to previous works that fit multiple NeRFs, 3DTopia  fits multiple triplane NeRFs and encodes the triplane into a latent space for which a diffusion model is trained to generate 3D shapes. 3DShape2VecSet  and Michelangelo  employ 3D occupancy as the output representation for the VAE but use multiple 1D vectors as implicit latent space instead of a triplane.

However, these methods often rely on rendering loss to supervise the VAE reconstruction, resulting in suboptimal reconstruction and generation quality. Additionally, using implicit latent representations not designed for efficient encoding and lacking compact explicit 3D representations for diffusion further limits their performance. Our 3D VAE combines the advantages of explicit 3D latent representation and direct 3D supervision to achieve high-quality VAE reconstruction, ensuring robust 3D shape generation. Furthermore, our design for the diffusion architecture specifically addresses conditional 3D latent generation. Our 3D DiT facilitates pixel-level and semantic-level 3D-specific image conditioning, allowing the diffusion process to generate highly detailed 3D shapes consistent with the condition images.

## 3 Methods

Inspired by LDM , we train a latent diffusion model for 3D generation within a 3D latent space. Unlike previous methods [19; 64] that typically rely on a 1D implicit latent space for generative models, our approach addresses two crucial limitations: 1) the struggle of the implicit latent repr presentation to capture structured information inherent in 3D space, leading to sub-optimal quality of decoded 3D shapes; 2) the challenge of training and sampling from the latent distribution, given that the implicit latent space is unstructured and under-constrained.

To mitigate these issues, we adopt an explicit triplane latent representation, utilizing a triplane of three feature maps to represent the 3D geometry latent. The design draws inspiration from LDM, which applies feature maps to represent the 2D image latent. Figure 2 illustrates the overall framework of our proposed method, which comprises a two-step training process: 1) the D3D-VAE is first trained to convert 3D shapes into 3D latents, which is described in Sec. 3.1; 2) the image-conditioned D3D-DiT is then trained to generate high-quality 3D assets, which is detailed in Sec. 3.2.

### Direct 3D Variational Auto-Encoder

The proposed D3D-VAE consists of three components: a point-to-latent encoder, a latent-to-triplane decoder, and a geometry mapping network. Meanwhile, we design a semi-continuous surface sampling strategy that utilizes both continuous and discrete supervision to ensure the high-frequency geometric details of the decoded 3D shape.

**Point-to-latent encoder.** In order to obtain robust representations in the latent space that can effectively capture intricate geometry, we uniformly sample high-resolution point clouds from the surface of 3D objects, which is then encoded to an explicit latent representation \(^{(3 r r) d_{}}\), where \(r\) and \(}}\) denotes the resolution and channel dimensional of the latent representation, respectively. To be specific, given a set of point clouds \(P^{N_{P}(3+3)}\) sampled from 3D models, where \(N_{P}\) denotes the number of points, the channel dimension \((3+3)\) comprises of the normalized position and normal of each point, we first use Fourier features  to represent the position structure of point clouds. Then we introduce a series of learnable tokens \(^{(3 r r) d_{}}\) to query the point cloud features using a cross-attention layer, where \(d_{}\) denotes the channel dimensional of \(\). This enables the injection of 3D information from the point clouds into the latent tokens. Subsequently, multiple self-attention layers are employed to enhance the representation of these tokens, ultimately yielding the latent representation \(^{(3 r r) d_{}}\), where \(d_{}\) represents the channel dimensional of \(\).

**Latent-to-triplane decoder.** After obtaining the latent representation \(\), we reshape it to the triplane representation. Inspired by RODIN , we concatenate the three planes vertically along the height dimension, yielding \(^{r(3 r) d_{}}\), to prevent incorrect blend of the planes across the channel dimension. Afterwards, the latent-triplane decoder upsamples \(\) to high-resolution triplane feature maps with upsampling factors \(f\). In contrast to the transformer architecture used in the encoder, our decoder model employs convolutional networks to progressively upsample the explicit latent representation and obtain the final triplane \(=(},},})\).

**Semi-continuous surface sampling.** We employ a Multi-Layer Perceptron (MLP) as the geometric mapping network to predict the occupancy of queried points via features interpolated from the triplane. The MLP contains multiple linear layers with ReLU activation. Typical occupancy is represented by a discrete binary value of 0 and 1 to indicate whether a point is inside an object. However, when the query point is very close to the object surface, it can result in abrupt gradient changes that affect model optimization. In this work, we adopt semi-continuous occupancy, using both continuous and discrete supervision to ensure smooth gradient. Specifically, given a query point \(\) in 3D space, when its distance to the surface is greater than a small threshold value \(s=\), the occupancy value

Figure 2: The framework of our Direct3D. (a) We utilize transformer to encode point cloud sampled from 3D model, along with a set of learnable tokens, into an explicit triplane latent space. Subsequently, a CNN-based decoder is employed to upsample these latent representations into high-resolution triplane feature maps. The occupancy values of queried points can be decoded through a geometric mapping network. (b) Then we train the image conditional latent diffusion transformer in the 3D latent space obtained by VAE. Pixel-level information and semantic-level information from images are extracted using DINO-v2 and CLIP, respectively, and then injected into each DiT block.

remains either 0 or 1. When the distance is less than s, a continuous value ranging from 0 to 1 is assigned to it. The formula for the semi-continuous occupancy \(o()\) is as follows:

\[o()=1,&sdf()<-s\\ 0.5-)}{s},&-s sdf() s \\ 0,&sdf()>s,\] (1)

where \(sdf()\) denotes the Signed Distance Function (SDF) value of **x**.

**End-to-end optimization.** During the training process, we uniformly sample points from the 3D space and sample points proximate to the object surface to predict their semi-continuous occupancy. We utilize Binary Cross-Entropy (BCE) loss \(L_{}\) to supervise the predictions. Additionally, we employ KL loss \(L_{}\) to prevent excessive variance in the latent space. Thus, our D3D-VAE is optimized by minimizing:

\[L_{}=L_{}+_{}L_{},\] (2)

where \(_{}\) denotes the weight of KL regularization.

### Image-conditioned Direct3D Diffusion Transformer

After training the D3D-VAE, we have access to a continuous and compact latent space, upon which we train the latent diffusion model. Since the obtained latent embedding is an explicit triplane representation, a naive approach would be to directly use a well-designed 2D U-Net as the diffusion model. However, this would result in a lack of communication between the three planes, thus failing to capture the structured and intrinsic properties required for 3D generation. Therefore, we build the generation model based on the architecture of the Diffusion Transformer (DiT), utilizing the transformer to better extract spatial positional information among the planes. Meanwhile, we propose to incorporate pixel-level and semantic-level information of the image in each DiT block, thereby aligning the image feature space and latent space to generate 3D assets consistent with the conditional image content. The framework of our latent diffusion model is shown in Figure 2 (**b**) and the architecture of each DiT block is illustrated in Figure 3.

**Pixel-level alignment module.** To ensure the high-frequency details of 3D assets generated by the diffusion model are aligned with the conditional images, we design a pixel-level alignment module to inject pixel-level information from the images into the latent space. We employ the pre-trained DINO-v2  (ViT-L/14) as the pixel-level image encoder, which has been revealed in previous work  to outperform other pre-trained vision models in extracting structural information beneficial for 3D tasks. Specifically, we first use two linear layers with GeLU  activation to project the image tokens \(_{}\) extracted by DINO-v2 to match the channel dimension of the noisy latent tokens \(_{t}\). Then in each DiT block, we concatenate them with the flattened \(_{t}\) and feed them into a self-attention layer to model the intrinsic relationship between \(_{}\) and \(_{t}\). Subsequently, we eliminate the part of image tokens and only reserve the part of noisy tokens for input to the next module.

Figure 3: The architecture of our 3D DiT. We employ the pre-trained DINO-v2 and CLIP vision model to extract tokens from conditional images respectively, then incorporate the pixel-level and semantic-level information into each DiT block.

**Semantic-level alignment module.** We devise a semantic-level alignment module to ensure semantic consistency between the generated 3D models and the conditional images. We employ the pre-trained CLIP  visual model (ViT-L/14) to extract semantic image tokens \(_{}\) from the conditional images, and then utilize a cross-attention layer within each DiT block to facilitate the interaction between \(_{}\) and noisy latent token \(_{}\). Meanwhile, unlike the original class conditional DiT, our image-conditioned diffusion model no longer utilizes class embedding. Instead, we use the classification token from the semantic image tokens \(_{}\) after projection and add it to the time embedding to enhance semantic features. In addition, to reduce the number of parameters and computational cost, we employ adaLN-single, as proposed in PixArt , which predicts a set of global shift and scale parameters \(P=[_{1},_{1},_{1},_{2},_{2},_{2}]\) using time embeddings, then sets a trainable embedding and adds it to \(P\) for adjustment in each block.

**Training.** Following LDM , our 3D latent diffusion transformer model predicts the noise \(\) of the noisy latent representation \(_{}\) at time t, conditioned on image \(C\). When training the diffusion model, we randomly zero the conditional input \(_{}\) and \(_{}\) with a probability of \(10\%\) to use classifier-free guidance  during inference, thereby improving the quality of conditional generation.

## 4 Experiments

### Dataset

Our Direct3D is trained on a filtered subset of the Objavverse  dataset which consists of 160K high-quality 3D assets. To evaluate the scalability of our Direct3D, we also employ additional internal data for training. Each 3D model is normalized to a unit sphere centered at the world origin. To construct conditional images for training the 3D latent diffusion transformer, we randomly render 24 views at a resolution of \(512 512\) using Blender for each 3D model. Additionally, we employ depth-conditioned ControlNet  to generate 16 diverse images to ensure the generalization of the diffusion model. To evaluate the performance of our Direct3D, we randomly select 30 3D models from the Google Scanned Objects (GSO)  dataset for image-to-3D experiments. For the text-to-3D task, we utilize existing text-to-image models like Hunyuan-DiT  to generate images with several classic text prompts as conditional inputs for qualitative comparisons with other methods. The ablation studies for each component are presented in the Appendix.

### Implementation Details

**D3D-VAE.** Our D3D-VAE takes as input 81,920 point clouds with normal uniformly sampled from the 3D model, along with a learnable latent token of a resolution \(r=32\) and a channel dimension \(d_{}=768\). The encoder network consists of 1 cross-attention layer and 8 self-attention layers, with each attention layer comprising 12 heads of a dimension 64. The channel dimension of the latent representation is \(d_{}=16\). The decoder network comprises of 1 self-attention layer and 5 ResNet  blocks to upsample the latent representation into triplane feature maps with resolution of \(256 256\) and channel dimension of 32. The geometric mapping network consists of 5 linear layers with hidden dimension 64. During training, we sample 20,480 uniform points and 20,480 near-surface points for supervision. The KL regularization weight is set to \(_{}=1e-6\). We use the AdamW  optimizer with a learning rate \(1e-4\) and a batch size of 16 per GPU.

**D3D-DiT.** Our diffusion model adopts the network configuration of DiT-XL/2 , which consists of 28 layers of DiT blocks. Each attention layer includes 16 heads with a dimension of 72. We train the

   Methods & Chamfer Distance \(\) & Volume IoU \(\) & F-Score \(\) \\  Shap-E  & 0.0585 & 0.2347 & 0.3474 \\ Michelangelo  & 0.0441 & 0.1260 & 0.4371 \\ One-2-3-45  & 0.0513 & 0.2868 & 0.3149 \\ InstantMesh  & 0.0327 & 0.4105 & 0.5058 \\  Ours (trained on Objavverse) & 0.0296 & 0.4307 & 0.5356 \\ Ours (trained on Objavverse + internal data) & **0.0271** & **0.4323** & **0.5624** \\   

Table 1: Quantitative results on Google Scanned Objects dataset.

diffusion model with 1000 denoising steps using a linear variance scheduler ranging from \(1e-4\) to \(2e-2\). We employ the AdamW optimizer with a batch size of 32 per GPU and train for 800K steps. During inference, we apply 50 steps of DDIM  with the guidance scale set to 7.5.

### Image and Text to 3D Generation

**Image-to-3D.** We conduct qualitative and quantitative comparisons of our Direct3D with other baseline methods on the GSO dataset for the image-to-3D task, as illustrated in Figure 4 and Table 1, respectively. Shap-E , a 3D diffusion model trained on millions of 3D assets, is capable of producing plausible geometry, but it suffers from artifacts and holes in the meshes. Michelangelo  performs diffusion process on a 1D implicit latent space, and fails to align the generated mesh with the semantic content of the conditional images. Multi-view based approaches such as One-2-3-45  and InstantMesh  heavily rely on the performance of multi-view 2D diffusion model. One-2-3-45 directly employs SparseNeuS  for reconstruction, resulting in coarse geometry. The meshes generated by InstantMesh perform decent quality, but lack consistency with the input images in certain details like the water spout on the sink and the windows of the school bus. It also produces some failure cases such as merging the hind legs of the horse together, due to the limitation of multi-view diffusion model. In contrast, our Direct3D consistently generates high-quality meshes that align with the conditional images in most cases. In Table 1, we report the Chamfer Distance, Volume IoU and F-Score to compare the quality of the generated meshes with other methods. It can be observed that our Direct3D achieves state-of-the-art performance across all metrics when trained on Objaverse dataset. Integrating our internal data for training further enhances the model's performance, validating the scalability of our approach.

**Text-to-3D.** Our Direct3D can produce 3D assets from text prompts by incorporating text-to-image models, such as Flux  and Hunyuan-DiT . Figure 5 illustrates the qualitative comparisons of our Direct3D and other baseline methods on the text-to-3D task. To ensure a fair comparison, all methods utilize the same generated image as input. It can be observed that these baseline methods fail in almost all cases, while our Direct3D is still able to generate high-quality meshes, demonstrating

    & Shap-E  & One-2-3-45  & Michelangelo  & InstantMesh  & Ours \\  Quality & 1.18 & 1.24 & 2.51 & 2.53 & **4.41** \\ Consistency & 1.19 & 1.28 & 2.32 & 2.66 & **4.35** \\   

Table 2: User study on the quality of meshes. The higher the score, ranging from 1 to 5, the better.

Figure 4: Qualitative comparisons with different baseline methods on GSO dataset.

the generalizability of our approach. We also conducted a user study to quantitatively compare our D3D-DiT with other methods. We render videos of meshes generated by each method rotating 360 degrees, and ask 46 volunteers to rate each mesh based on its quality and consistency with the input images. The results in Table 2 indicate that our D3D-DiT perform superior mesh quality and consistency compared to other baseline methods.

**Generation of textured mesh.** Benefited from the smooth and detailed geometry produced by our Direct3D, we can easily dress up the mesh using existing texture synthesis methods. As shown in Figure 6, we utilize SyncMVD  to obtain exquisite textured meshes.

## 5 Conclusion

In conclusion, our paper introduces a novel approach for direct 3D shape generation from a single image, bypassing the need for multi-view reconstruction. Leveraging a hybrid architecture, our proposed D3D-VAE efficiently encode 3D shapes into a compact latent space, enhancing the fidelity of the generated shapes. Our image-conditioned 3D diffusion transformer (D3D-DiT) further improves the generation quality by integrating image information at both pixel and semantic levels, ensuring

Figure 5: Qualitative comparisons of the meshes generated from text. We employ the existing text-to-image models (e.g. Hunyuan-DiT) to produce highly detailed images as the inputs of each method.

high consistency between generated 3D shapes and conditional images. Extensive experiments on the image-to-3D and text-to-3D tasks demonstrate the superior performance of our Direct3D in 3D generation, surpassing existing methods in quality and generalizability.

**Limitations.** Despite the capability of our Direct3D to produce high-fidelity 3D assets, it is currently limited to the generation of individual or multiple objects and cannot generate large-scale scenes. We will focus on it in future research.

**Acknowledgment.** This work was mainly supported by DreamTech, and in part by the National Natural Science Foundation of China (62441204, 62472213).