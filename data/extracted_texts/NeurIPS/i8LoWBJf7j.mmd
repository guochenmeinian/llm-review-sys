# Interpretable Lightweight Transformer via Unrolling

of Learned Graph Smoothness Priors

 Tam Thuc Do

York University

Toronto, Canada

dtamthuc@yorku.ca

&Parham Eftekhar

York University

Toronto, Canada

eftekhar@yorku.ca

&Seyed Alireza Hosseini

York University

Toronto, Canada

ahoseini@yorku.ca

&Gene Cheung

York University

Toronto, Canada

genec@yorku.ca

&Philip A. Chou

packet.media

Seattle, USA

pachou@ieee.org

###### Abstract

We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors--the quadratic graph Laplacian regularizer (GLR) and the \(_{1}\)-norm graph total variation (GTV)--subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike "black-box" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers.

## 1 Introduction

Focusing on the _self-attention_ mechanism  as the basic building block--where the _affinity_ between two input tokens is computed as a transformed dot product--_transformers_ learn large parameter sets to achieve state-of-the-art (SOTA) performance in a wide range of signal prediction/classification problems , outperforming convolutional neural nets (CNNs) and recurrent neural nets (RNNs). However, there are shortcomings: i) lack of mathematical interpretability to characterize general performance1, ii) requiring substantial training datasets to train sizable parameters , and iii) fragility to _covariate shift_--when training and testing data have different distributions .

Orthogonally, _algorithm unrolling_ implements iterations of a model-based algorithm as a sequence of neural layers to build a feed-forward network, whose parameters can be learned end-to-end via back-propagation from data. A classic example is the unrolling of the _iterative soft_thresholding algorithm_ (ISTA) in sparse coding into _Learned_ ISTA (LISTA) . Recently,  showed that by unrolling an iterative algorithm minimizing a sparse rate reduction (SRR) objective, it can lead to a family of "white-box" transformer-like deep neural nets that are 100% mathematically interpretable. Inspired by , in this work we also seek to build white-box transformers via algorithm unrolling, but from a unique _graph signal processing_ (GSP) perspective [17; 18; 19].

Over the last decade and a half, GSP studies spectral analysis and processing of discrete signals on structured data kernels described by finite graphs. Specifically, by assuming that the sought signal is smooth (low-pass) with respect to (w.r.t.) a particular graph, a plethora of graph-based restoration algorithms can be designed for practical applications, including image denoising , JPEG dequantization , interpolation , 3D point cloud denoising [23; 24] and super-resolution . At the heart of GSP is the construction of a _similarity graph_ that captures pairwise similarities between signal samples on two connected nodes. We first demonstrate that _signal-dependent similarity graph learning with normalization is akin to affinity computation in the self-attention mechanism_. Thus, _our first contribution is to show that unrolling of a graph-based iterative algorithm2 with normalized graph learning results in an interpretable transformer-like feed-forward network._

Second, computation of a positive weight \(w_{i,j}=(-d(i,j))\) of an edge \((i,j)\) connecting two graph nodes \(i\) and \(j\) often employs _Mahalanobis distance_\(d(i,j)=(_{i}-_{j})^{}(_{i}- _{j})\) between representative (_e.g._, CNN-computed) _feature vectors_\(_{i}\) and \(_{j}\), where \(_{i},_{j}^{D}\) reside in low-dimensional space [27; 28]. Hence, unlike a conventional transformer that requires large key and query matrices, \(\) and \(\), to compute transformed dot products, the similarity graph learning module can be more parameter-efficient. Moreover, by adding a graph smoothness prior such as _graph Laplacian regularizer_ (GLR) [18; 20] or _graph total variation_ (GTV) [29; 30; 31] in the optimization objective, once a graph \(\) is learned, the target signal is simply computed as the low-pass filtered output derived from the minimization of the assumed graph smoothness prior. Thus, a large value matrix \(\) to compute output embeddings typical in a transformer is also not needed. _Our second contribution is to demonstrate that a **lightweight** transformer with fewer parameters can be built via unrolling of a graph-based restoration algorithm with a chosen graph signal smoothness prior._

Specifically, focusing on the signal interpolation problem, we first derive linear-time graph-based algorithms by minimizing GLR or GTV, via _conjugate gradient_ (CG)  or a modern adaptation of _alternative method of multipliers_ (ADMM) for _sparse linear programming_ (SLP) , respectively. In each iteration, given a learned graph, each algorithm deploys a low-pass graph filter to interpolate the up-sampled observation vector into the target signal. We intersperse unrolled algorithm iterations with graph learning modules into a compact and interpretable neural network. We demonstrate its restoration performance, parameter efficiency (\(3\%\) of SOTA's parameters in one case), and robustness to covariate shift for two practical applications: image demosaicking, and image interpolation.

**Notation:** Vectors and matrices are written in bold lowercase and uppercase letters, respectively. The \((i,j)\) element and the \(j\)-th column of a matrix \(\) are denoted by \(A_{i,j}\) and \(_{j}\), respectively. The \(i\)-th element in the vector \(\) is denoted by \(a_{i}\). The square identity matrix of rank \(N\) is denoted by \(_{N}\), the \(M\)-by-\(N\) zero matrix is denoted by \(_{M,N}\), and the vector of all ones / zeros of length \(N\) is denoted by \(_{N}\) / \(_{N}\), respectively. Operator \(\|\|_{p}\) denotes the \(\)-\(p\) norm.

## 2 Preliminaries

### GSP Definitions

A graph \((,,)\) is defined by a node set \(=\{1,,N\}\) and an edge set \(\) of size \(||=M\), where \((i,j)\) means nodes \(i,j\) are connected with weight \(w_{i,j}=W_{i,j}\). In this paper, we consider only _positive_ graphs \(\) with no self-loops, _i.e._, \(w_{i,j} 0, i,j\), and \(w_{i,i}=0, i\). We assume edges are undirected, and thus _adjacency matrix_\(^{N N}\) is symmetric. The _combinatorial graph Laplacian matrix_ is defined as \(-^{N N}\), where \((_{N})\) is the _degree matrix_, and \(()\) returns a diagonal matrix with \(\) along its diagonal. \(\) for a positive graph \(\) is provably _positive semi-definite_ (PSD), _i.e._, all its eigenvalues \(_{i}\)'s are non-negative .

We define also the _incidence matrix_\(=^{M N}\): each \(k\)-th row of \(\) corresponds to the \(k\)-th edge \((i,j)\), where \(C_{k,i}=w_{i,j}\), \(C_{k,j}=-w_{i,j}\), and \(C_{k,l}=0,\  l i,j\). Since our assumed graph \(\) is undirected, the polarities of \(C_{k,i}\) and \(C_{k,j}\) are arbitrary, as long as they are opposite.

### Graph Laplacian Regularizer

Given a positive _connected_ graph \(\) with \(N\) nodes and \(M\) edges, we first define smoothness of a signal \(^{N}\) w.r.t. \(\) using the _graph Laplacian regularizer_ (GLR) [18; 20] as

\[\|\|_{,2}=^{}=_{( i,j)}w_{i,j}(x_{i}-x_{j})^{2}\] (1)

where \(\) is a combinatorial Laplacian matrix specifying graph \(\). GLR (1) is non-negative for a positive graph, and thus is suitable as a signal prior for minimization problems [20; 21].

### Graph Total Variation

Instead of GLR (1), we can alternatively define graph signal smoothness using _graph total variation_ (GTV) [29; 30; 31]\(\|\|_{,1}\) for signal \(^{N}\) as

\[\|\|_{,1}=\|\|_{1} _{(i,j)}w_{i,j}|x_{i}-x_{j}|\] (2)

where \((a)\) is true since \(\) is positive. GTV is also non-negative for positive \(\), and has been used as a signal prior for restoration problems such as image deblurring .

## 3 Problem Formulation & Optimization using GLR

### Problem Formulation

We first assume a positive, _sparse_ and _connected_ graph \(\) with \(N\) nodes and \(M\) edges specified by graph Laplacian matrix \(\). By sparse, we mean that \(M\) is \((N)\) and not \((N^{2})\). By connected, we mean that any node \(j\) can be traversed from any other node \(i\). Given \(\), we first derive a _linear-time_ iterative algorithm to interpolate signal \(\) by minimizing GLR given observed samples \(\). In Section 4, we derive an algorithm by minimizing GTV instead given \(\). In Section 5, we unroll iterations of one of two derived algorithms into neural layers, together with strategically inserted graph learning modules, to construct graph-based lightweight transformer-like neural nets.

We first employ GLR  as the objective to reconstruct \(^{N}\) given partial observation \(^{K}\), where \(K<N\). Denote by \(\{0,1\}^{K N}\) a _sampling matrix_ defined as

\[H_{i,j}=\{1&\\ 0&.\] (3)

that picks out \(K\) samples from signal \(\). The optimization is thus

\[_{}\ ^{},\ \ \ \ \ \ \ =\] (4)

where \(^{N N}\) is a graph Laplacian matrix corresponding to a positive graph \(\). PSD \(\) implies that \(^{} 0,\), and thus (4) has a convex objective with a linear interpolation constraint.

### Optimization

We solve (4) via a standard Lagrangian approach  and write its corresponding unconstrained Lagrangian function \(f(,)\) as

\[f(,)=^{}+ ^{}(-)\] (5)

where \(^{K}\) is the Lagrange multiplier vector. To minimize \(f(,)\) in (5), we take the derivative w.r.t. \(\) and \(\) separately and set them to zero, resulting in the following linear system:

\[2&^{}\\ &_{K,K}]}_{}[[] {c}\\ ]=[_{N,N}\\ ].\] (6)Given that the underlying graph \(\) is positive and connected, coefficient matrix \(\) is provably full-rank and thus invertible (see Appendix A.1 for a proof). Hence, (6) has a unique solution \(^{*}\).

Suppose we index the sampled nodes \(\) in \(\) before the non-sampled nodes \(}\), _i.e._, \(=[_{};_{}}]\). Then \(=[_{K}\;\;_{K,N-K}]\), and the second block row in (6) implies \(_{}=\). Suppose we write \(=[_{,}\;\;_{, };_{,}\;\;_{, }]\) in blocks also. For the first block row in (6), consider only the non-sampled rows:

\[(2\;\;^{}[\; \\ ])_{}}=2(_{ ,}\,_{}+_{, }\,_{})=_{N-K}\] (7)

where \((^{})_{}}= _{N-K}\) since the non-sampled rows of \(^{}\) (the non-sampled columns of \(\)) are zeros. Thus, \(_{}\) can be computed via the following system of linear equations:

\[_{,}\,_{}=-_{ ,}\,\] (8)

where \(_{},}}\) is a symmetric, sparse, and provably _positive definite_ (PD) matrix (see Appendix A.2 for a proof). Thus, there exists a unique solution \(_{}\) in (8).

**Complexity**: For notation simplicity, let \(=_{,}}\). Linear system (8) can be solved efficiently using _conjugate gradient_ (CG), an iterative descent algorithm with complexity \(((})})}/())\), where \((})\) is the number of non-zero entries in matrix \(}\), \(()=}(})}{ _{}(})}\) is the _condition number_ of \(}\), \(_{}(})\) and \(_{}(})\) are the respective largest and smallest eigenvalues of \(}\), and \(\) is the convergence threshold of the gradient search . Because \(\) is sparse by graph construction (\((N)\) edges), \(=_{},}}\) is also sparse, _i.e._, \(()=(N)\). Assuming \(()\) can be reasonably lower-bounded for PD \(\) and \(\) is reasonably chosen, the complexity of solving (8) using CG is \((N)\).

**Interpretation**: To elicit a signal filtering interpretation from (6), we assume for now that \(\) is PD3 and thus invertible. Recall that the block matrix inversion formula  is

\[^{-1}=([&\\ &])^{-1}=[ ^{-1}+^{-1}(/) ^{-1}&-^{-1}(/)\\ -(/)^{-1}&(/) ]\] (9)

where \(/=(-^{-1})^{-1}\) is the _Schur complement_ of block \(\) of matrix \(\). Solution \(^{*}\) can thus be computed as:

\[^{*} =^{-1}^{}(^{-1} ^{})^{-1}\] \[=^{-1}^{}((^{-1})_{ })^{-1}=^{-1}^{}_{}^{\#}\] (10)

where \(_{}^{\#}((^{-1})_{} )^{-1}\) and \((^{-1})_{}\) denotes the rows and columns of \(^{-1}\) corresponding to the sampled nodes. \(_{}^{\#}\) is a high-pass filter similar to \(_{}\). Thus, we can interpret \(^{*}\) as a _low-pass filtered output of up-sampled \(^{}_{}^{\#}\)_--with low-pass filter response \(r()=^{-1}\) where \(=^{}\), \(=([_{1},,_{N}])\), is eigen-decomposible with frequencies \(_{k}\)'s and Fourier modes \(_{k}\)'s.

## 4 Problem Formulation & Optimization using GTV

### Problem Formulation

Given a positive connected graph \(\) specified by incidence matrix \(^{M N}\) and partial observation \(\), we now employ instead GTV as the objective to interpolate target signal \(\), resulting in

\[_{}\;\;\|\|_{1},\;\;\;\;\;\;\; =,\] (11)

(11) is a _linear program_ (LP), since both the objective and the lone constraint are linear. Thus, while minimizing GLR leads to a _linear system_ (6), minimizing GTV leads to a _linear program_ (11).

[MISSING_PAGE_FAIL:5]

Linear system (18) is solvable if the coefficient matrix \(}+^{}\), where \(^{}\) is a PSD graph Laplacian for a positive graph, is invertible. See Appendix A.4 for a proof that \(\) is PD and thus invertible.

**Complexity**: Linear system (18) again can be solved efficiently using CG with complexity \(((})})}/( ))\). Because \(\) is sparse by graph construction (\((N)\) edges) and \(\) is sparse by definition, \(\) is also sparse, _i.e._, \(()=(N)\). Thus, assuming \(()\) is also upper-bounded and \(\) is reasonably chosen, the complexity of solving (18) using CG is \((N)\).

**Interpretation**: (18) can be interpreted as follows. \(^{}\) is the _up-sampled version_ of observation \(\). \(=^{}\), \(=([_{1},,_{N}])\), is an eigen-decomposible _generalized Laplacian matrix_--Laplacian matrix \(^{}\) plus self-loops of weight \(1\) at sampled nodes due to diagonal matrix \(^{}\), and like a graph Laplacian \(\) without self-loops, can be interpreted as a high-pass spectral filter . \(^{-1}=^{-1}^{}\) is thus a _low-pass_ spectral filter with frequency response \(r()=^{-1}\) to interpolate output \(^{-1}^{}\). We interpret the remaining terms on the right-hand side of (18) as bias.

#### 4.2.2 Optimizing Auxiliary Variable

Fixing \(^{t+1}\), \(^{t+1}\) and \(^{t+1}\), the optimization for \(}^{t+1}\) for (15) simplifies to

\[_{}}g(})+(_{d}^{t})^{} (^{t+1}-})+\| ^{t+1}-}\|_{2}^{2}.\] (20)

The solution for optimal \(}^{t+1}\) is term-by-term thresholding:

\[_{i}^{t+1}=\{q_{i}^{t+1}+ _{d,i}^{t}&q_{i}^{t+1}+_{d,i}^{t} 0\\ 0&., i.\] (21)

See Appendix A.5 for a derivation.

#### 4.2.3 Updating Lagrange Multiplier

The Lagrange multiplier \(^{t+1}\) can be updated in the usual manner in an ADMM framework :

\[^{t+1}=^{t}+([ \\ \\ ]-[\\ }]).\] (22)

**Algorithm Complexity**: Given that the number of iterations until ADMM convergence is not a function of input size, it is \((1)\). The most time-consuming step in each ADMM iteration is the solving of linear system (18) via CG in \((N)\). Thus, we conclude that solving SLP (12) using the aforementioned ADMM algorithm is \((N)\).

**Algorithm Comparison**: Comparing the CG algorithm used to solve linear system (8) and the ADMM algorithm developed to solve SLP (12), we first observe that, given a similarity graph \(\) specified by Laplacian or incidence matrix, \(\) or \(\), both algorithms compute the interpolated signal \(^{*}\) as a low-pass filtered output of the up-sampled input \(^{}\) in (10) and (18), respectively. This is intuitive, given the assumed graph smoothness priors, GLR and GTV. We see also that the ADMM algorithm is more intricate: in each iteration, the main variables are computed using CG, while the auxiliary variable is updated via ReLU-like thresholding. As a result, the ADMM algorithm is more amenable to deep algorithm unrolling with better performance in general (see Section 6 for details).

## 5 Graph Learning & Algorithm Unrolling

We now discuss how a similarity graph \(\) can be learned from data, specified by graph Laplacian \(\) for GLR minimization (4) or incidence matrix \(\) for GTV minimization (11), so that the two proposed graph-based interpolations can take place. Moreover, we show how a normalized graph learning module4 performs comparable operations to the self-attention mechanism in conventional transformers. Thus, unrolling sequential pairs of graph-based iterative algorithm and graph learning module back-to-back leads to an interpretable "white-box" transformer-like neural net.

### Self-Attention Operator in Transformer

We first review the self-attention operator in a conventional transformer architecture, defined using a transformed dot product and a softmax operation . Specifically, first denote by \(_{i}^{E}\) an _embedding_ for token \(i\) of \(N\) tokens. _Affinity_\(e(i,j)\) between tokens \(i\) and \(j\) is defined as the dot product between linear-transformed embeddings \(_{i}\) and \(_{j}\), where \(,^{E E}\) are the _query_ and _key_ matrices, respectively. Using softmax, a non-linear function that maps a vector of real numbers to a vector of positive numbers that sum to \(1\), _attention weight_\(a_{i,j}\) is computed as

\[a_{i,j}=^{N}(e(i,l))},\ \ \ \ \ e(i,j)=( _{j})^{}(_{i}).\] (23)

Given self-attention weights \(a_{i,j}\), output embedding \(_{i}\) for token \(i\) is computed as

\[_{i}=_{l=1}^{N}a_{i,l}_{l}\] (24)

where \(^{E E}\) is a _value_ matrix. "Self-attention" here means that input embeddings are weighted to compute output embeddings. A transformer is thus a sequence of embedding-to-embedding mappings via different learned self-attention operations defined by \(\), \(\) and \(\) matrices. _Multi-head_ attention is possible when multiple query and key matrices \(^{(m)}\) and \(^{(m)}\) are used to compute different attention weights \(a_{i,j}^{(m)}\)'s for the same input embeddings \(_{i}\) and \(_{j}\), and the output embedding \(_{i}\) is computed using an average of these multi-head attention weights \(a_{i,l}^{(m)}\)'s.

### Computation of Graph Edge Weights

Consider now how edge weights \(w_{i,j}\)'s can be computed from data to specify a finite graph \(\)[27; 28]. A low-dimensional _feature vector_\(_{i}^{D}\) can be computed for each node \(i\) from embedding \(_{i}^{E}\) via some (possibly non-linear) function \(_{i}=F(_{i})\), where typically \(D E\). Edge weight \(w_{i,j}\) between nodes \(i\) and \(j\) in a graph \(\) can then be computed as

\[w_{i,j}=(-d(i,j)),\ \ \ \ \ d(i,j)=(_{i}-_{j})^{ }(_{i}-_{j})\] (25)

where \(d(i,j)\) is the squared _Mahalanobis distance_ given PSD _metric matrix_\(\) that quantifies the difference between nodes \(i\) and \(j\). \(M\) edge weights \(\{w_{i,j}\}\) compose a graph \(\), specified by the Laplacian matrix \(\) for GLR minimization (4) and the incidence matrix \(^{M N}\) for GTV minimization (11). Because \(w_{i,j} 0, i,j\), constructed graph \(\) is positive.

As a concrete example, consider _bilateral filter_ (BF) weights commonly used in image filtering , where feature \(_{i}\) contains the 2D grid location \(_{i}\) and color intensity \(p_{i}\) of pixel \(i\), and metric \(=([1/_{d}^{2};1/_{r}^{2}])\) is a diagonal matrix with weights to specify the relative strength of the _domain_ and _range_ filters in BF. Because BF uses input pixel intensities \(p_{i}\)'s to compute weighted output pixel intensities \(p_{i}\)'s, BF is _signal-dependent_, similar to self-attention weights in transformers.

Edge weights are often first _normalized_ before being used for filtering.

**Normalization**: For normalization, the symmetric _normalized graph Laplacian_\(_{n}\) is defined as \(_{n}^{-1/2}^{-1/2}\), so that the diagonal entries of \(_{n}\) are all ones (assuming \(\) is connected and positive) . We assume normalized \(_{n}\) is used for Laplacian \(\) in GLR minimization in (4).

Alternatively, the asymmetric _random walk graph Laplacian_\(_{rw}\) is defined as \(_{rw}^{-1}\), so that the sum of each row of \(_{rw}\) equals to zero . Interpreting \(_{rw}\) as a Laplacian matrix to a _directed_ graph, the weight sum of edges leaving each node \(i\) is one, _i.e._, \(_{l|(i,l)}_{i,l}=1, i\). To accomplish this, undirected edges weights \(\{w_{i,j}\}\) are normalized to \(\{_{i,j}\}\) via

\[_{i,j}=}(-d(i,l))}.\] (26)

For GTV minimization in (11), we normalize edge weights in incidence matrix \(\) instead using (26). This results in normalized \(}^{2M N}\) for a _directed_ graph with \(2M\) directed edges. Subsequently,we define symmetric graph Laplacian \(}=}^{}}\) and generalized graph Laplacian \(}=}+^{}\). Note that \(\|}\|_{1}=_{l=1}^{N}_{i,l}|1-1|=0\) after normalization, as expected for a total variation term on a constant signal \(\). Further, note that while \(}\) is an incidence matrix for a directed graph with \(2M\) edges, \(}\) is a graph Laplacian for an undirected graph with \(M\) edges. See Fig. 3 in Appendix A.6 for an example of incidence matrix \(\), normalized incidence matrix \(}\), and graph Laplacian matrix \(}\).

**Comparison to Self-Attention Operator**: We see how the definitions of edge weights (25) and normalization (26) are similar to attention weights in (23). Specifically, _interpreting the negative squared Mahalanobis distance \(-d(i,j)\) as affinity \(e(i,j)\), normalized edge weights \(_{i,j}\) in (25) are essentially the same as attention weights \(a_{i,j}\) in (23)_. There are subtle but important differences: i) how non-negative Mahalanobis distance \(d(i,j)\) is computed in (25) using features \(_{i}=F(_{i})\) and metric \(\) versus how real-valued affinity is computed via a transformed dot product in (23), and ii) how the normalization term is computed in a one-hop neighborhood from node \(i\) in (26) versus how it is computed using all \(N\) tokens in (23). The first difference conceptually means that edge weight based on Mahalanobis distance \(d(i,j)\) is symmetric (_i.e._, \(_{i,j}=_{j,i}\)), while attention weight \(a_{i,j}\) is not. Both differences have crucial complexity implications, which we will revisit in the sequel.

Further, we note that, given a graph \(\), the interpolated signal \(^{*}\) is computed simply as a low-pass filtered output of the up-sampled input observation \(^{}\) via (10) or (18), depending on the assumed graph smoothness prior, GLR or GTV, while the output embedding \(_{i}\) in a conventional transformer requires value matrix \(\) in (24). This also has a complexity implication.

### Deep Algorithm Unrolling

We unroll \(T\) sequential pairs of an iterative interpolation algorithm (GLR- or GTV-based) with a graph learning module into an interpretable neural net. See Fig. 1a for an illustration of the GTV-based algorithm unrolling, where the \(t\)-th pair of ADMM block and the graph learning module have respective parameters \(_{t}\) and \(_{t}\) that are learned from back-propagation via a defined loss function. \(_{t}\) include parameters used to define feature function \(F()\) and metric matrix \(\) in (25), so the module can construct a graph \(\) specified by incidence matrix \(^{t+1}\) given signal \(^{t}\). In our implementation, we employ a shallow CNN to map a neighborhood of pixels centered at pixel \(i\) to a low-dimensional feature \(_{i}\), with a parameter size smaller than query and key matrices, \(\) and \(\), in a conventional transformer. See Section 6.1 for details.

An ADMM block contains multiple ADMM layers that are unrolled iterations of the iterative ADMM algorithm described in Section 4.2. Each ADMM layer updates the main variables \(,,\), auxiliary variable \(}\), and Lagrange multiplier \(\) in turn using (17) to (22). ADMM weight parameter \(\), as well as parameters in CG used to compute linear system (18), are learned via back-propagation. Specifically, two CG parameters \(\) and \(\) that represent step size and momentum during the conjugate gradient descent step are learned. See Appendix A.7 for details.

Figure 1: Unrolling of GTV-based signal interpolation algorithm.

## 6 Experiments

### Experimental Setup

All models were developed using Python 3.11. We leveraged PyTorch to implement all models and trained them using NVIDIA GeForce RTX 2080 Ti. To train each learned model, we used the DIV2K dataset, which contains 800 and 100 high-resolution (HR) training and validation images, respectively. Since the images are HR, we patchified the images into small images and used only about \(1\) to \(4\%\) of the patches for training and validation sets. We randomly sampled patches of \(64 64\) pixels to train the model. To test a model, we used the McM , Kodak , and Urban100  datasets, running each model on the whole images. See Appendix A.8 for more implementation details.

We tested model performance in two imaging applications: demosaicking and image interpolation. Demosaicking reconstructs a full-color image (each pixel contains RGB colors) from a Bayer-patterned image, where each pixel location has only one of Red, Green, or Blue. Interpolation reconstructs empty pixels missing all three colors in an image. To create input images, for the first application, we started from a full-color image and then removed color components per pixel according to the Bayer pattern. For the second application, we directly down-sampled horizontally and vertically a HR image by a factor of \(2\) to get the corresponding low-resolution (LR) image without any anti-aliasing filtering. This is equivalent to keeping every four pixels in the HR image.

### Experimental Results

For the first application, we evaluated our graph-based models against two variants of RSTCANet , RSTCANet-B and RSTCANet-S (RST-B and RST-S for short), a SOTA framework that employs a swin transformer architecture, Menon , Malvar  and bicubic interpolation. Menon  employs a directional approach combined with an _a posteriori_ decision, followed by an additional refinement step. Malvar  uses a linear filtering technique that incorporates inter-channel information across all channels for demosaicking.

The baselines for our second application are MAIN , a multi-scale deep learning framework for image interpolation, SwinIR , and bicubic interpolation. SwinIR consists of three stages: shallow feature extraction, deep feature extraction, and a final reconstruction stage; see  for details. We use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM)  as our evaluation metrics, common in image quality assessment.

   &  \\  &  \\  & \(=10\) & \(=20\) & \(=30\) & \(=50\) \\  RST-B & 28.01 & 22.7 & 19.34 & 15.03 \\ uGLR & 28.24 & 22.84 & 19.49 & 15.203 \\ uGTV & **28.31** & **22.89** & **19.56** & **15.38** \\  

Table 1: Demosaicking performance in noisy scenario, where models are trained on noiseless dataset.

   &  &  &  \\  & Params\# & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  Bicubic & - & 29.01 & 0.8922 & 26.75 & 0.8299 & 22.95 & 0.7911 \\ MAIN  & 10942977 & 32.72 & 0.822 & 28.23 & 0.728 & 25.46 & 0.806 \\ SwinIR-lightweight  & 904744 & 32.24 & 0.9354 & 28.62 & 0.8794 & 25.08 & 0.8553 \\ iGLR & - & 28.53 & 0.8537 & 26.71 & 0.8005 & 22.87 & 0.7549 \\ iGTV & - & 30.41 & 0.887 & 28.05 & 0.832 & 24.26 & 0.7855 \\ uGLR & 319090 & 33.31 & 0.9431 & **29.10** & 0.8870 & 25.94 & 0.8777 \\ uGTV & 319115 & **33.36** & **0.9445** & 29.08 & **0.8888** & **26.12** & **0.8801** \\  

Table 3: Interpolation performance for different models, trained on 10k sample dataset.

Figure 2: Demosaicking performance vs. training size for different models.

Table 2 shows the demosaicking performance for different models, where all models were trained on the same dataset and the same number of epochs (\(30\)), using a subset of DIV2K dataset containing \(10K\) of \(64 64\) patches. We observe that our unrolled GTV model (uGTV) achieved the best overall performance, while the unrolled GLR model (uGLR) and RST-S performed similarly. Both our models (uGTV and uGLR) performed better than RST-B while employing significantly fewer parameters. Crucially, we observe that although our normalized edge weight \(_{i,j}\) based on Mahalanobis distance is symmetric while the self-attention weight \(a_{i,j}\) is not (due to query and key matrices \(\) and \(\) not being the same in general), the directionality in the self-attention mechanism does not appear to help improve performance of the conventional transformer further, at least for image interpolation tasks. The iterative GTV algorithm (iGTV) without parameter optimization performed the worst, demonstrating the importance of parameter learning.

In Fig. 2, we see the demosaicking performance of different models versus training data size. We see that for small data size, our models (uGTV and uGLR) performed significantly better than RST-B. This is intuitive, since a model with more parameters requires more training data in general. See Appendix A.9 for example visual results.

Next, we test robustness to covariate shift by testing models trained on noiseless data using a dataset artificially injected with Gaussian noise. Table 1 shows the demosaicking performance versus different noise variances. We observe that our models outperformed RST-B in all noisy scenarios.

For image interpolation, we interploated a LR image to a corresponding HR image. We trained all models on the same dataset as the first application with the same number of epochs (\(15\)). Table 3 shows that under the same training conditions, our proposed models (uGTV and uGLR) outperformed MAIN in interpolation performance in all three benchmark datasets by about \(0.7\) dB. Note that for this application we only interpolated Y-channel from YCbCr color space for PSNR computation. Similar to the first application, our models achieved slight performance gain while employing drastically fewer parameters; specifically, uGTV employed only about \(3\%\) of the parameters in MAIN.

## 7 Conclusion

By unrolling iterative algorithms that minimize one of two graph smoothness priors--\(_{2}\)-norm graph Laplacian regularizer (GLR) or \(_{1}\)-norm graph total variation (GTV)--we build interpretable and light-weight transformer-like neural nets for the signal interpolation problem. The key insight is that the normalized graph learning module is akin to the self-attention mechanism in a conventional transformer architecture. Moreover, the interpolated signal in each layer is simply the low-pass filtered output derived from the assumed graph smoothness prior, eliminating the need for the value matrix. Experiments in two imaging applications show that interpolation results on par with SOTA can be achieved with a fraction of the parameters used in conventional transformers.