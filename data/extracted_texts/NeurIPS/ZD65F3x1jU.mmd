# On Learning Latent Models with Multi-Instance Weak Supervision

Kaifu Wang

University of Pennsylvania

kaifu@sas.upenn.edu

&Efthymia Tsamoura

Samsung AI

efi.tsamoura@samsung.com

&Dan Roth

University of Pennsylvania

danroth@seas.upenn.edu

###### Abstract

We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function \(\) of labels associated with multiple input instances. We formulate this problem as _multi-instance Partial Label Learning (multi-instance PLL)_. Our problem is an extension to the standard PLL problem and is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition \(\). Our main contributions are as follows. First, we propose a necessary and sufficient condition for the learnability of the problem. This condition nontrivially generalizes and relaxes the existing _small ambiguity degree_ in PLL literature since we allow the transition to be deterministic. Second, we derive Rademacher-style error bounds based on a top-\(k\) surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning under unknown transitions. The empirical results align with our theoretical findings, exposing also the issue of scalability in the weak supervision literature.

## 1 Introduction

Consider the scenario in Figure 1, where a learner aims to learn one or more classifiers \(f_{1},,f_{n}\), each of which maps an instance \(x\) to its corresponding label \(y\). The learner is given training examples, each of which consists of a _vector_ of \(M>1\) instances \(=(x_{1},,x_{M})\), and each \(x_{i}\) is processed by one of the \(f_{j}\)'s. Differently from supervised learning, the gold labels \(=(y_{1},,y_{M})\) of \(\) are _hidden_; instead, the learner is provided with a _weak_ label \(s\) which is produced by applying a _transition function_1\(\) to the gold labels \(\). The transition \(\) itself may be unknown to the learner.

The above weak supervision setting has been a topic of active research in NLP . Recently, it has received renewed attention as it is met in _neuro-symbolic_ frameworks , i.e., frameworks that integrate inference and learning over neural models with inference and learning over symbolic models, such as logical theories . The benefits of this setting over architectures that approximate the \(f_{i}\)'s and \(\) via an end-to-end neural model  are (i) the ability to reuse the latent models, something particularly useful in NLP , (ii) higher accuracy in multiple tasks, e.g., NLP  and visual question answering , (iii) greater interpretability and (iv) the ability to encode prior knowledge via \(\). Below, we present an example of our learning setting in neuro-symbolic learning.

**Example 1** (Sum2).: _We aim to learn an MNIST classifier \(f\). Differently from supervised learning, we use training examples of the form \((x_{1},x_{2},s)\), where \(x_{1}\) and \(x_{2}\) are MNIST digits and \(s\) istheir sum. The above implies a transition function \(:\{0,,9\}\{0,,9\}\{0,,18\}\), s.t. \((y,y^{})=y+y^{}\). The target sum \(s\) restricts the space of labels that can be assigned to the input pair of digits \((x_{1},x_{2})\), e.g., if \(s=2\), then the only combinations of labels that abide by the constraint that the sum is 2 are (2,0), (1,1) and (0,2). This problem is referred to as SUM2 in literature [24; 30]._

The _transition function_\(\) is not necessarily one-to-one. For instance, multiple combinations of digits can lead to the same target sum. The above leaves us with two questions to answer: _can we provide any learning guarantees for the classifiers \(f_{1},,f_{n}\)?_ and _what if \(\) is unknown?_ Despite that multiple techniques for neuro-symbolic and latent structural learning have been recently proposed, those theoretical questions remain open. Practically, for the single input case (\(M=1\)), our setting can reduce to that of _partial label learning_ (PLL), where each input instance is accompanied by a set of labels, including the gold one [25; 11; 5; 29; 39; 54; 57; 60]. However, a key PLL learnability assumption - that the probability a wrong label co-occurs with the gold one is always less than one - is violated in our setting, rendering existing learnability results inapplicable. This is because \(\) is a deterministic but _not_ necessarily bijective function, as opposed to \(\) in PLL which is randomized.

Due to its relevance to PLL, we refer to our learning setting as _multi-instance PLL_. Differently from prior art, e.g., [42; 35], we do not restrict \(\) to specific types of functions, e.g., linear functions. Hence, we can express constraints in several formal languages, including systems of Boolean equations and Datalog , a language widely used in neuro-symbolic techniques . Notice that many neuro-symbolic learning techniques are oblivious to the implementation of the symbolic component: abstracting the symbolic component as a transition \(\) has been actually proposed as the means to compositionally integrate neural with symbolic models . Furthermore, although we primarily consider deterministic transitions motivated by the neuro-symbolic and NLP literature, our results can be extended to support randomized transitions when \(\) is known.

We aim to make _minimal_ assumptions on the data distributions by showing learnability even under the "toughest" distributions. In addition, we provide learning guarantees under the semantic loss , a widely-used surrogate loss for training classifiers subject to logical theories [30; 43; 24]. To our knowledge, we are the first to provide this theoretical analysis, closing a gap in the neuro-symbolic and latent structural learning literature.

**Contributions.** Our contributions can be summarized into the following:

* We propose necessary and sufficient learnability conditions of the multi-instance PLL problem assuming all the instances are classified by a single classifier \(f\) and the transition is known. We further provide a Rademacher-style error bound using a top-\(k\) approximation to the _semantic loss_, see Section 3. Our analysis confirms the intuition that a larger \(k\) decreases the empirical risk, but increases the number of samples required for learning.
* We extend the above results for the more general case where one learns multiple classifiers \(=(f_{1},,f_{n})\) that classify instances from different domains, see Section 4.
* We prove learnability with a single classifier and an unknown transition function, see Section 5, and assess the validity of our theoretical results using SOTA neuro-symbolic frameworks, see Section 6.

Proofs, additional backgrounds and details on our empirical analysis are in the appendix.

## 2 Preliminaries

We use \(()\) to denote probability mass, \([]\) to denote expectation and \(\{\}\) to denote an indicator function. For a positive integer \(M\), we use \([M]\) as a short for \(\{1,,M\}\). For a vector of \(M\) elements \((x_{1},,x_{M})\), a _position_\(j\) is a number in \([M]\) used to identify the element \(x_{j}\). A vector is said to be _diagonal_ if its elements in all positions are equal. Suppose \(f\) is a function on a domain \(\) and \(\) is a vector of \(M\) elements in \(\), we use the symbol \(f()\) to denote the vector \((f(x_{1}),,f(x_{M}))\).

Figure 1: Multi-Instance PLL. We aim to learn the \(f_{i}\)’s given the \(x_{i}\)’s and \(s\). \(M\) may be different from \(n\) and \(\) may be unknown.

**Classifiers.** Let \(\) denote an instance space and \(\) denote an output space with \(||=c\). Let also \(\) be the joint distribution of two random variables \((X,Y)\) and \(_{X}\) be the marginal of \(X\). Throughout the text, we use \(\) for \((x_{1},,x_{M})\) and \(\) for the corresponding vector of gold labels. We consider _seroring functions_ of the form \(f:_{c}\), where \(_{c}\) is the space of probability distributions on \(\) (e.g., \(f\) outputs the softmax probabilities of a neural network). We use \(f^{j}(x)\) to denote the \(j\)-th output of \(f(x)\). A scoring function \(f\) induces a _classifier_ whose _prediction_ on \(x\)\([f](x)\) is defined by \([f](x):=*{argmax}_{j[c]}f^{j}(x)\). We use \(\) and \([]\) to denote the space of scoring functions and the space of classifiers induced by \(\), respectively. We use the VC-dimension, the Natarajan dimension and Rademacher complexities to characterize the complexity of \(\).

**Loss functions.** We define the problem of _learning_ a scoring function using a fixed set of training samples as the one of _choosing_ the scoring function from \(\) that better _fits_ the training samples. We give semantics to the term _fits_ by using the _risk_\((f;)\) of \(f\) subject to a sampling distribution and a given _loss function_\(\): the lower \((f;)\) becomes, the better \(f\) fits the data. When there exists an \(f^{*}\), such that \((f^{*};)=0\), we say that the space \(\) is _realizable_ under \(\). In _supervised learning_, we are provided with samples of the form \((x,y)\) drawn from \(\). We use \(}\) to denote the _empirical risk_, i.e., the average risk computed over the training samples only. The _risk_2 of \(f\) subject to a loss function \(:^{+}\) is given by \((f;):=_{(X,Y)}[([f](X),Y)]\).

A commonly used loss function is the _zero-one_ loss defined by \(^{01}(y,y^{}):=\{y^{} y\}\) for any pair \(y,y^{}\). We use \(^{01}(f)\) as a short for \((f;^{01})\) and refer to it as the _zero-one risk of \(f\)_. We will also consider _Semantic Loss_ (SL) , which has been adopted to train classifiers subject to Boolean formulas . Let \(\) be a Boolean formula where each variable in \(\) is associated with a class from \(\). Then, the SL of \(\) subject to \(f(x)\) is the negative logarithm of the _weighted model counting_ (WMC)  of \(\) under \(f(x)\), namely \((,f(x)):=-((,f(x)))\), where \((,f(x))\) takes values in \((0,1)\) and denotes the probability that \(\) is logically satisfied when its variables become true with probabilities specified by \(f(x)\).

## 3 Learning a Single Classifier Under a Known Transition

We begin with the setting where the goal is to learn a single classifier \(f\) under a known transition \(\). Firstly, we show ERM-learnability (Theorem 1). Then, inspired by neuro-symbolic learning, we provide a Rademacher-style error bound with a top-\(k\) surrogate loss based on WMC (Theorem 2). Below, we formally introduce the learning setting.

**Problem setting.** Let \(:^{M}\) be a transition function, where \(=\{1,,d\}\) with \(||=d 1\). Let also \(_{}\) be a set of \(m_{}\)_partially labeled_ samples of the form \((,s)=(x_{1},,x_{M},s)\). Each training sample is formed by, firstly drawing \(M\) i.i.d. samples \((x_{i},y_{i})\) from \(\) and then setting \(s=(y_{1},,y_{M})\). We use \(_{}\) to denote the distribution followed by the training samples \((,s)\). In analogy to the zero-one classification loss \(^{01}\), we define the _zero-one partial loss_ subject to \(\) as \(^{01}_{}(,s):=\{() s\}\), for any \(^{M}\) and \(s\). The learner aims to find the classifier \(f\) with the minimal _zero-one risk_\(^{01}(f)\). As the gold labels are hidden, the learner uses the dataset \(_{}\) to estimate and minimize the _zero-one partial risk_ of \(f\) subject to \(\) defined as \(^{01}_{}(f;):=_{(X_{1},,X_{M},S) _{}}[^{01}_{}(([f](X_{1}),,[f](X_{M})),S)]\).

We demonstrate these notions via Example 1. There, \(f\) is an MNIST classifier, \(\) is the space of MNIST images and \(=\{0,,9\}\). The training samples are of the form \((x_{1},x_{2},s)\) where \(s=(y_{1},y_{2})=y_{1}+y_{2}\). The partial risk of \(f\) is then the probability of predicting the wrong sum.

**Learnability.** A _partial learning algorithm_\(\) takes a partially labeled dataset \(_{}\) with \(|_{}|=m_{}\) as input and outputs a function \((_{})\). Similar to standard PAC learning (see, for example, ), we say a multi-instance problem is _learnable_, if there exists a partial learning algorithm \(\), such that for any data distribution \(\) over \(\) and any \(,(0,1)\), there is an integer \(m_{,}\), such that \(m_{} m_{,}\) implies \(^{01}((_{}))\) with probability at least \(1-\) with respect to \(_{}\). We consider \(\) to be an Empirical Risk Minimizer (ERM) algorithm that finds the classifier \(f\) which minimizes the empirical zero-one partial risk \(}^{01}_{}(f;;_{}):= _{(,s)_{}}^{01}_{}(([f](x_{1}), ,[f](x_{M})),s)/m_{}\).

### ERM Learnability

To prove learnability under the partial zero-one loss, we aim to bound \(^{01}(f)\) with the partial risk \(^{01}_{}(f;)\). Firstly, we propose a sufficient and necessary condition called _\(M\)-unambiguity_.

Recall that learnability requires handling all possible distributions of \((,)\). Therefore, to propose a _necessary_ condition to guarantee learnability, we consider a special case: the one in which \(\) concentrates its mass on a single item \(x_{0}\), i.e., \(_{_{X}}(x_{0})=1\). In that case, the only sample observed during learning includes only the instance \(x_{0}\). Now, let \(l_{0}\) be the gold label of \(x_{0}\). If \(f\) mispredicts the class of \(x_{0}\), i.e., \(f(x_{0}) l_{0}\), and this misprediction leads to the same observed partial label, i.e., \((f(x_{0}),,f(x_{0}))=(l_{0},,l_{0})\), then the problem will be not learnable, as we will never be able to correct this error. Motivated by this special case, we propose the following condition:

**Definition 1** (\(M\)-unambiguity).: _Transition \(\) is \(M\)-unambiguous if for any two diagonal label vectors \(\) and \(^{}^{M}\) such that \(^{}\), we have that \((^{})()\)._

Recall that a vector is diagonal if all its elements are equal. Below, we provide examples of transitions that satisfy (or not) the \(M\)-unambiguity condition.

**Example 2**.: _Consider the following learning problems._

* _(SUM-\(M\)): Consider a variant of Example_ 1 _where_ \(s\) _is the sum of_ \(M\) _digits, i.e.,_ \((y_{1},,y_{M})=_{i=1}^{M}y_{i}\)_. The transition is_ \(M\)_-unambigous, since_ \((y,,y)(y^{},,y^{})\) _holds for any_ \(y y^{}\)_._
* _(PRODUCT-\(M\)): Consider a variant of Example_ 1 _where_ \(s\) _is given by_ \(^{*}(y_{1},,y_{M})=_{i=1}^{M}y_{i}\)_. Then,_ \(^{*}\) _is_ \(M\)_-unambiguous, since_ \(^{*}(y,,y)=y^{M}(y^{})^{M}\) _holds for any_ \(y y^{}\)_._
* _(XOR): Consider Boolean labels. We take the XOR of the two digits as the partial label, i.e.,_ \(^{}(y_{1},y_{2})=\{y_{1} y_{2}\}\)_. Then_ \(^{}\) _is not_ \(M\)_-unambiguous, since_ \(^{}(1,1)=^{}(0,0)\)_._

Although \(M\)-unambiguity is proposed as a necessary condition under special types of data distributions, we now show that it is also _sufficient_ for proving ERM-learnability. Firstly, we prove that the classification risk can be bounded by the partial risk.

**Lemma 1**.: _If \(\) is \(M\)-unambigous, then we have:_

\[^{01}(f)(^{01}_{}(f;)^{1 /M})^{01}_{}(f;) 0\] (1)

_Moreover, if \(\) is not \(M\)-unambiguous, then learning from partial labels is arbitrarily difficult, in the sense that a classifier \(f\) with partial risk \(^{01}_{}(f;)=0\) can have a risk of \(^{01}(f)=1\)._

To show learnability, we bound the partial risk with its empirical counterpart in the realizable case.

**Theorem 1** (ERM learnability under \(M\)-unambiguity).: _Suppose \(\) is realizable under \(^{01}_{}\) and \([]\) has a finite Natarajan dimension \(d_{[]}\). Then for any \(,(0,1)\), there exists a universal3 constant \(C_{0}>0\), such that with probability at least \(1-\), the empirical partial risk minimizer with \(}^{01}_{}(f;;_{})=0\) has a classification risk \(^{01}(f)<\), if_

\[m_{} C_{0}}{^{M}}(d_{[]} (6cMd_{[]})(}{^{M}})+ ())\] (2)

Beyond Lemma 1, Theorem 1 builds upon several non-trivial intermediate results: (i) A bound of the VC dimension for the partial label predictor (Lemma 3 in the Appendix) and (ii) Construction of counter-examples for arguing the necessity of \(M\)-unambiguity (see the proof of Theorem 1).

**Toward a faster convergence rate.** Theorem 1 presents a rather slow convergence rate of \((c^{2}/)^{M}\). In the following, we show that a better convergence rate is achievable by forcing stricter ambiguity conditions. In the following, we introduce the concept of _1-unambiguity_, which requires the transition to be sensitive to 1-position perturbations:

**Definition 2** (1-unambiguity).: _Transition \(\) is 1-unambiguous, if there exists an index \(1 i M\), such that flipping the \(i\)-th label of any label vector \(^{M}\), results in a vector \(^{}\) with \((^{})()\)._

In the following, we provide examples of transitions that satisfy (or not) the \(1\)-unambiguity condition.

**Example 3**.: _Let us continue with Example 2. The transition \(\) from SUM-\(M\) is 1-unambiguous since \(()=_{i=1}^{M}y_{i}\) always change when replacing any of the labels \(y_{i}\). However, the transition \(^{*}\) from PRODUCT-\(M\) is not 1-unambiguous, since for the label vector \(=(0,1,1)\) (with product zero), the flipped vector \(^{}=(0,1,2)\) also leads to product zero._

We show that a better convergence rate can be achieved if \(\) is both 1- and \(M\)-unambigous.

**Proposition 1** (ERM learnability under 1- and \(M\)-unambiguity).: _If \(\) is both 1- and \(M\)-unambigous, then we have:_

\[^{01}(f)(^{01}_{}(f;)) ^{01}_{}(f;) 0\] (3)

_Furthermore, if \(\) is realizable under \(^{01}_{}\) and \([]\) has a finite Natarajan dimension \(d_{[]}\), then for any \((0,1)\) and \((0,1)\) that is sufficiently close to 0, there exists a universal constant \(C_{1}\), such that with probability at least \(1-\), the empirical partial risk minimizer with \(}^{01}_{}(f;)=0\) has a classification risk \(^{01}(f)<\), if_

\[m_{} C_{1}(d_{[]}(6cMd_{[ ]})()+( ))\] (4)

**Remark 1**.: The supervision power of a partial label \(s\) depends on the "instability" of \(\): if \(\) returns different results under certain perturbations in \(I\) positions for an \(I[M]\), then the classification risk is bounded by \(^{01}(f)(^{01}_{}(f;)^{1/ I})\). To formalize this intuition, in the appendix, we provide a generalized definition of both 1- and \(M\)-unambiguity, called \(I\)-unambiguity, and show learnability with intermediate convergence rates. Notice that \(M\)-unambiguity is closely related to the small ambiguity degree condition in the PLL literature . Appendix E.3 provides a relevant discussion and an extension of our results for randomized transitions.

**Remark 2**.: We use the concentrated distribution described at the beginning of Section 3.1 as a pivot to design a necessary and sufficient learnability condition. However, the results in Lemma 1, Theorem 1 and Proposition 1 do not apply only to this distribution, but to any distribution \(_{}\) of partial data. The same applies to all learnability results that follow.

### Error Bounds with the Top-\(k\) Semantic Loss

We now study the error bounds with the semantic loss (see Section 2 and Appendix A) under top-\(k\) approximations (Theorem 2). We are interested in this loss for two reasons. Firstly, risk minimization under the zero-one loss is intractable even for linear classifiers  and hence in practice, learning is performed by minimizing differentiable surrogate losses such as the semantic loss. Secondly, computing a surrogate loss typically introduces another source of intractability: that of enumerating the entries of \(\). For example, in SUM-\(M\), we have to enumerate \(10^{M}\) digit combinations to populate \(\), which is not scalable.

A popular way to reduce the second source of inefficiency is to consider only the \(k\) most probable label combinations during training, where the probability of a label vector \(\) given \(\) and classifier \(f\) is defined by \(P_{f()}():=_{i=1}^{M}f^{y_{i}}(x_{i})\). Then, training can proceed by taking the semantic loss over the top-\(k\) label vectors . The above is possible, as the top-\(k\) label vectors can be equivalently viewed as a formula that is true iff one or more of the top-\(k\) label vectors is the gold one.

**Example 4**.: _Let us return back to Example 1. Instead of considering all three combinations \((2,0)\), \((1,1)\) and \((0,2)\) assuming a target \(s=2\), we can consider only the first and the last combination, if the probabilities (predicted by \(f\)) of the two images being 1 are way lower than those of being 0 or 2. Then, the top-2 label vectors \((2,0)\), \((0,2)\) can be seen as the formula \((A_{1,2} A_{2,0})(A_{1,0} A_{2,2})\), where \(A_{i,j}\) is a Boolean variable that is true iff the \(i\)-th input digit is assigned label \(j\)._

We use \(_{i=1}^{k}_{j}\) to denote the Boolean formula computed out of the \(k\) label vectors \(_{1},,_{k}\), where each \(_{i}\) is the conjunction of Boolean variables of the form \(A_{i,y}\), as in Example 4. Given a softmax score \(f()\), variable \(A_{i,y}\) is assigned probability \(f^{y}(x_{i})\). We now define the _top-\(k\) partial loss_:

**Definition 3** (Top-\(k\) partial loss).: _The top-\(k\) partial loss for an integer \(k 1\) subject to a classifier \(f\), transition \(\) and sample \((,s)\) is defined as_

\[^{k}_{}(f(),s):=(_{i=1}^{k} ^{(i)},f())\] (5)_where \(^{(1)},,^{(k)}^{M}\) are the top-\(k\) maximizers of \(P_{f()}\) in the preimage of \(s\), i.e., \(\{^{M}:()=s\}\). The top-\(k\) partial classification risk of \(f\) subject to \(\) is then given by_

\[^{k}_{}(f;):=_{(X_{1},,X_{M},S) _{}}[^{k}_{}(f(X_{1},,X_{M}),S)]\] (6)

**Remark**.: The WMC is upper bounded by the sum of probabilities, so we can approximate the SL as \(^{k}_{}(f(),s)-(_{i=1}^{k}P_{f()}(^{(i )}))\). Furthermore, the special case where \(k=1\) reduces to the infimum loss  and minimal loss  in the PLL literature as we show in Appendix B.2.

Let \(_{m}()\) denote the Rademacher complexity  of \(\) with \(m\) samples as defined in Appendix A. We are now ready to bound the zero-one classification risk with the empirical top-\(k\) partial risk.

**Theorem 2** (Error bound under unambiguity).: _Let an integer \(k 1\) and \((0,1)\). If \(\) is both 1- and \(M\)-unambiguous, then with probability at least \(1-\), we have:_

\[^{01}(f)((k+1)(}^{k}_{ }(f;;_{})+2M^{3/2}_{ Mm_{}}()+}}} ))\] (7)

_where \(}^{k}_{}(f;;_{})= _{(,s)_{}}^{k}_{}(f(),s)/ m_{}\) is the empirical counterpart of (6) and \(\) is an increasing function that satisfies \(_{t 0}(t)/t=1\)._

Proof sketch.: Theorem 2 builds upon several results. Firstly, we derived an inequality that bounds the top-\(k\) loss with the zero-one loss (Lemma 5), which requires the construction of an intermediate \(^{1}\) loss (Definition 11). Secondly, we show the Lipschtness of the semantic loss (Lemma 7). This result is further combined with a contraction lemma that is proposed in  (Lemma 6) to bound the Rademacher complexity of the model. 

**Remark**.: Similarly to Theorem 1 and Proposition 1, Theorem 2 suggests that the learning difficulty increases as \(M\) increases. This is intuitive, since it gets harder to disambiguate the gold labels when \(M\) increases, e.g., for 2-SUM, there are \(10^{2}\) possible label vectors need to be considered, while for 4-SUM, there are \(10^{4}\) ones. Our bound also suggests a tradeoff with the choice of \(k\): a larger \(k\) decreases the risk, but tends to increase the complexity term.

## 4 Learning Multiple Classifiers Under a Known Transition

In this section, we extend the learning problem from Section 3 to jointly learn \(n 1\) different classifiers. Similarly to Section 3, we aim to propose minimal assumptions for the data distribution to show both results on ERM-learnability (Theorem 3) and a Rademacher-style error bound with the top-\(k\) loss (Theorem 4). We formally define the learning setting below.

**Problem setting.** We aim to learn \(n 1\) classifiers, each of which maps instances from a space \(_{i}\) to the corresponding label space \(_{i}\) with \(|_{i}|=c_{i}\). For each \(i[n]\), we define a scoring space \(_{i}\), which contains mappings of the form \(f_{i}:_{i}_{c_{i}}\). Each \(f_{i}\) is used to classify \(M_{i} 1\) instances \((x_{i1},,x_{iM_{i}})=_{i}_{i}^{M_{i}}\) with (hidden) gold labels \(_{i}=(y_{i1},,y_{iM_{i}})\). We denote the vector of scoring functions as \(=(f_{1},,f_{n})\). Let \(_{i}\) be the joint distribution over elements from \(_{i}_{i}\). Each training sample given to the learner \((_{1},,_{n},s)\) is formed by (i) drawing \(M_{i}\) i.i.d. samples \((_{i},_{i})\) from \(_{i}\), for each \(i[n]\) and then (ii) obtaining its partial label as \(s=(_{1},,_{n})\). We override the notation \(_{}\) to denote the distribution of the training samples in this learning setting. The partially labeled dataset \(_{}\) then contains \(m_{}\) i.i.d. samples drawn from \(_{}\).

Similarly to Section 3, the learner aims to find the classifiers \(f_{1},,f_{n}\) with the minimal _zero-one risk_ defined as \(^{01}():=_{i=1}^{n}^{01}(f_{i})\). As the gold labels are hidden, the learner uses the dataset \(_{}\) to estimate and minimize the _zero-one partial risk_ of \(f_{1},,f_{n}\), which is defined as \(_{}^{01}(f_{1},,f_{n};):=_{( _{1},,_{n},S)_{}}[^{01}_{}(([f_{ 1}](_{1}),,[f_{n}](_{n})),S)]\), where \([f_{i}](_{i})\) is a short for \(([f_{i}](X_{i1}),,[f_{i}](X_{iM_{i}}))\), for \(1 i M_{i}\).

**Example 5** (Learning binary operators).: _Consider a setting where we aim to train a classifier \(f_{1}\) for recognizing MNIST digits and a classifier for recognizing images of addition and multiplication taken from some space \(_{2}\). The training samples are of the form \((x_{11},x_{12},x_{21},s)\), where \(x_{11}\) and \(x_{12}\) are non-zero MNIST digits with \(_{1}=\{1,,9\}\), \(x_{21}\) is an image of an operator with \(_{2}=\{+,\}\), and \(s\) is the result of applying the operator in \(x_{2}\) on the digits in \(x_{11}\) and \(x_{12}\)._

### ERM Learnability

Similarly to Section 3, to propose a necessary condition for learnability, we consider the most challenging distribution where each \(_{i}\) is concentrated on a single instance \(x_{i}^{*}_{i}\), for \(1 i n\). Then, the only sample that will be observed during learning is \((_{1}^{*},,_{n}^{*})\) where \(_{i}=(x_{i}^{*},,x_{i}^{*})\), for \(1 i n\). Assuming that \(l_{i}^{*}\) is the gold label of \(x_{i}^{*}\), detecting a classification error is possible only if a misclassification of any \(x_{i}^{*}\), i.e., \([f_{i}](x_{i}^{*}) l_{i}^{*}\), will lead to a prediction vector \(^{}\) with a different image under \(\) from the gold label \(\), i.e., \((^{})()\). The above intuition is captured via the notion of _multi-unambiguity_, which generalizes the notion of \(M\)-unambiguity. Below, we use \(_{i}^{M_{i}}\), for \(i[n]\), to denote the \(M_{i}\)-ary diagonal vector including the \(y_{i}\) element only.

**Definition 4** (Multi-unambiguity).: _Transition \(\) is multi-unambiguous if for any vector \(=(_{1}^{M_{1}},,_{n}^{M_{n}})\), and any position \(i[n]\), such that the vector \(^{}\) that results after flipping the labels in \(_{i}^{M_{i}}\) to some diagonal vector \((_{i}^{})^{M_{i}}_{i}^{M_{i}}\), has a different image under \(\), i.e., \(()(^{})\)._

Multi-unambiguity reduces to \(M\)-unambiguity for \(n=1\). An example of multi-unambiguity is below.

**Example 6** (Learning binary operator, cont'd).: _In Example 5, the multi-unambiguity condition is violated since \(2+2=2 2\). Namely, if a distribution assigns all its weight to the digit \(2\), then it is difficult for the model to distinguish the two operators. On the other hand, if instead, the label space is \(_{1}=\{3,,9\}\), then the multi-unambiguity condition is satisfied._

**Bounded risk assumption.** Differently from Section 3, multi-unambiguity alone is not sufficient to ensure learnability. To see this, consider a variant of SUM2 where the digits are classified by two independent classifiers, \(f_{1}\) and \(f_{2}\). If the distributions of the first and the second image are concentrated on 1 and 7 respectively, but \(f_{1}(x_{1})=7\) and \(f_{2}(x_{2})=1\), then these errors cannot be detected. Therefore, we assume there is a constant \(R<1\), such that \(^{01}(f_{i}) R\) holds for any \(i[n]\) and any \(f_{i}\). We refer to such \(f_{i}\)'s as _zero-one risk \(R\)-bounded_. This assumption is mild since it only requires the classifiers to be slightly better than being totally wrong. Also, when a small directly labeled dataset is available, one can bound the classification risks away from 1 with high probability in a uniform manner using standard learning theory. See Appendix C for more details. Now, we are ready to state the ERM-learnability result assuming \(_{i=1}^{n}_{i}\) is realizable under \(_{}^{01}\).

**Theorem 3** (ERM learnability under multi-unambiguity).: _Assume that there is a constant \(R<1\), such that for each \(i[n]\), each \(f_{i}\) is zero-one risk \(R\)-bounded. Assume also that there exist positive integers \(M^{*}\) and \(c^{*}\), such that \(M_{i} M^{*}\) and \(c_{i} c_{0}\) hold for any \(i[n]\). Then, if \(\) is multi-unambiguous, we have:_

\[^{01}()((_{}^{01}( ;))^{1/M^{*}})_{}^{01}(; ) 0\] (8)

_Furthermore, for any \(,(0,1)\), there is a universal constant \(C_{3}\), such that with probability at least \(1-\), the empirical partial risk minimizer with \(}_{}^{01}(f;)=0\) has a classification risk \(^{01}(f)<\) if_

\[m_{} C_{3}^{2M^{*}-2}}{^{M^{*}}(1-R)^{M}} (_{i=1}^{n}d_{[_{i}]}(nc_{i}M_{i}d_{[_{i}] })(^{2M^{*}-2}}{^{M^{*}}(1-R)^{M}})+ ())\] (9)

**Remark.** Theorem 3 suggests that the rate of convergence is controlled by \(M^{*}\). A smaller \(M^{*}\) leads to a more strict unambiguity condition. For example, the case \(M^{*}=1\) requires \(\) to be unstable to _any_ 1-position perturbations, while the case \(M^{*}=M\) reduces to \(M\)-unambiguity. This observation confirms the intuition that the supervision power of partial labels depends on the "instability" of \(\).

The top-\(k\) partial loss \(_{}^{k}((),s)\) for multiple classifiers subject to \(^{n}\), transition \(\) and \(s\) straightforwardly extends the top-\(k\) partial loss for the single classifier case, see Definition 3. Similarly, the _(empirical) top-\(k\) partial classification risk_ of \(\) subject to \(\) straightforwardly extends the one from Section 3.2. Below, we provide a Rademacher-style error bound with the top-\(k\) loss.

**Theorem 4** (Error bound under multi-unambiguity with multiple classifiers).: _Suppose \(\) is multi-unambiguous and each \(f_{i}\) is zero-one risk \(R\)-bounded, for \(R(0,1)\). Then, for any integer \(k 1\) and any \((0,1)\), with probability at least \(1-\), we have:_

\[^{01}()(^{2M^{*}-2}(k+1)}{(1-R)^{M}} (}_{}^{k}(;;_{})+_{i=1}^{n}M_{i}_{m_{}M_{i}}( _{i})+}}}))^{1 /M^{*}}\]Learning Under an Unknown Transition

We now explore another direction by dropping the assumption that \(\) is known. Instead, we assume the learner has access to a _transition space_\(\) that contains mappings of the form \(^{M}\) including the "true" transition \(\). Notice that learning now becomes particularly challenging, since \(\) can be expressive enough to lead to correct predictions for \(s\) from many wrong classifications.

**Problem setting.** To illustrate the core idea, we consider a simple setting where we only learn a single scoring function \(f\) as in Section 3. However, \(\) is an unknown mapping in \(\). Given partially labeled samples, the learner aims to minimize the _zero-one partial risk of \(f\) subject to \(\)_\(^{01}_{}(f;)\) that is defined as the _minimal_ partial risk to predict \(s\) with a candidate transition in \(\), i.e., \(^{01}_{}(f;):=_{^{*}} ^{01}_{}(f;^{*})\). The risk is empirically estimated with a partially labeled dataset \(_{}\) as \(_{^{*}}}^{01}_{}(f;^{ *};_{})\). We demonstrate this learning setting with the following example:

**Example 7**.: _Let \(=\{(y_{1},y_{2}) y_{1}+ y_{2}|(,) ^{2}-\{(0,0)\}\}\), namely all the weighted sums with at least one non-zero weight. We aim to learn an MNIST classifier \(f\), given training samples of the form \((x_{1},x_{2},s)\), where the \(x_{i}\)'s are MNIST digits and \(s\) is the result of applying some \(\) from \(\) on \((y_{1},y_{2})\), where \(y_{i}\) is the prediction of \(f\) on \(x_{i}\). The gold \(\), i.e., the exact \(\) and \(\), is unknown._

### ERM Learnability

Similarly to the known transition case, learnability requires us to learn under any data distribution \(\). Slightly differently from Sections 3 and 4, we start with the adversarial distribution where the mass in \(\) is concentrated to a single label, i.e., all instances \(x_{i}\) have the same gold label \(l_{i}\), and hence, the training samples are all associated with the gold label vector \(=(l_{i},,l_{i})\). Suppose there is a certain probability that \(f\) misclassifies \(l_{i}\) as \(l_{j}\). Then, the predicted label vectors will be in \(\{l_{i},l_{j}\}^{M}\). In this case, it will be impossible to detect the classification errors if there is a candidate transition \(^{}\) which maps all the vectors in \(\{l_{i},l_{j}\}^{M}\) to the gold observation of \(s\). Since the actual \(\) is unknown, we require this does not happen for any pair of \(,^{}\) to ensure learnability. Below, we formalize our learnability condition:

**Definition 5** (Unambiguous transition space).: _Transition space \(\) is unambigous, if for each \(^{}\), each diagonal label vector \(=(l_{i},,l_{i})\), where \(l_{i}\), and each \(l_{j} l_{i}\), where \(l_{j}\), there exists a vector \(^{}\{l_{i},l_{j}\}^{M}\), such that \(^{}(^{})()\)._

In pratic, one can examine whether a transition space is unambiguous by checking if for each \(^{}\) and any two different labels \(l_{i} l_{j}\), set \(\{^{}()|\{l_{i},l_{j}\}^{M}\}\) is not a singleton. The above ensures that when given a fixed diagonal label vector, and when the classifier makes mistakes, the predicted partial labels are not unique, and hence cannot all agree with the ground truth label.

Class \(\) in Example 7 is unambiguous: for each transition \((y_{1},y_{2})^{}y_{1}+^{}y_{2}\) and each two labels \(l^{} l\), the set \(\{^{}y_{1}+^{}y_{2}|(y_{1},y_{2})\{l,l^{}\}^{2}\}\) is not a singleton, so there must exist a label vector with a different weighted sum that is different than the true sum. A counterexample is below:

**Example 8**.: _Let \(=\{(y_{1},y_{2}) w_{1}y_{1}+w_{2}y_{2}^{2}+w_{3}y_{2}^{2}+w_ {4}y_{2}|w_{i} 0\  i\}\). Consider the true transition \(:(y_{1},y_{2}) y_{1}-y_{1}^{2}+y_{2}-y_{2}^{2}\) from \(\) and a candidate transition \(^{}:(y_{1},y_{2}) y_{1}-y_{1}^{2}-y_{2}+y_{2}^{2}\). Then, \(\) is not unambiguous, since \(^{}()=()=0\) for any \(\{0,1\}^{2}\). Namely, the partially labeled data are not informative enough to distinguish the label \(0\) from \(1\)._

**Bounded risk assumption.** Similarly to Section 4, this unambiguity condition is necessary but not sufficient to show learnability. To show learnability, we additionally assume there is a constant \(r>0\) such that \(([f](X)=y|Y=y) r\) for each \(y\) and \(f\). We refer to such an \(f\) as \(r\)_-bounded_. This assumption is slightly stronger than that of Section 4 as it additionally requires the classifiers to be not fully wrong for _every_ possible label. The main learnability result of this section is below:

**Theorem 5**.: _If \(\) is unambiguous and any \(f\) is \(r\)-bounded, then we have:_

\[^{01}(f) O(^{01}_{}(f;)^{1/M}) ^{01}_{}(f;) 0\] (10)

_Furthermore, suppose \([]\) has a finite Natarajan dimension \(d_{[]}\) and the function class \(\{(,s) 1\{^{}() s\}|^{ }\}\) has a finite VC-dimension \(d_{}\). Then, for any \(,(0,1)\), there is a universal constant \(C_{4}\) such that with probability at least \(1-\), the empirical partial risk minimizer with \(}_{}^{01}(f;)=0\) has a classification risk \(^{01}(f)<\), if_

\[m_{} C_{4}}{r^{M}^{M}}(((d_{[ ]}+d_{})(6M(d_{[]}+d_{}))+d_{[ ]} c)(}{r^{M}^{M}})+ ())\]

## 6 Experiments

We further explore the learning scenario in Section 5 with empirical evaluations. We aim to learn an MNIST classifier using the weighted sum of 2, 3 and 4 MNIST digits and assuming that the weights are unknown, as in Example 7. The weighted sum function is very expressive as it can model Boolean formulas including conjunction and disjunction . Notice that we do not aim to exhaustively assess the weak supervision literature ([24; 43; 59; 13]), but to validate the results of our theoretical analysis.

**Baselines.** We considered state-of-the-art neuro-symbolic frameworks, namely DeepProbLog (DLog) , DeepProbLog with approximations (DLog-A) , NeuroLog (NLog) , NeurASP (NASP) , ABL , ENT  and Scallop . Only the target sum is used during training under those frameworks. DLog and NLog employ the semantic loss without any approximations (see Section 2). NLog(\(k\)) denotes an NLog variant where only the top-\(k\) predictions are considered during training, while Scallop(\(k\)) denotes Scallop using the top-\(k\) semantic loss, see Section 3.2. The approximations in DLog-A are different from the ones in NLog and Scallop; however, both DLog-A and NLog employ the semantic loss on the chosen subset of proofs. As the above frameworks learn classifiers under fixed theories only, we considered weights in \(\{1,2,3,4,5\}\), and used an additional neural classifier to learn the unknown weights. Finally, we considered standard supervised learning (SL). To contrast learning under unknown to learning under known transitions, we also ran experiments assuming that the weights are known. Our results show that the classification accuracy exceeds the 98% even for \(M=10\). More details on this experiment are in the arXiv version of this paper .

**Results.** Tables 1 and 2 report the accuracy of the learned MNIST classifiers over ten runs. #0, #8 and #16 denote the number of directly labeled samples used for pretraining the MNIST classifiers: #0 means no pretraining; for #8 and #16, the accuracy of the pre-trained classifiers is 37% and 58%. Table 1 presents results for \(M=2\), while Table 2 presents results for \(M\{2,3,4\}\) for Scallop and the two best performing frameworks in Table 1, DLog and NLog- NASP could not scale for \(M 3\). We used DLog-A for \(M\{3,4\}\), due to scalability reasons. Similarly, NLog without approximations does not scale for \(M=4\) (N/A indication). In Table 1, the number of weak samples is in parentheses, e.g., DLog(#4K). For ABL and ENT, we used 10K samples, as they rely on sampling, and used the hyperparameters suggested by the authors.

**Discussion.** The results confirm our theory: (i) we can learn classifiers under unknown transitions as per Theorem 5 (see the accuracy for DLog and NLog in Table 1); and (ii) learning gets harder when \(M\) increases as per Eq. (5) (see the accuracy for different \(M\)'s in Table 2). For \(M=2\), weak supervision leads to better accuracy than SL even without pre-training and with fewer training samples: DLog(#4K) and NLog(#4K) lead to mean accuracy 96% and 97%, while SL(#4K) leads to mean accuracy 93%, see Table 1. This result suggests that the \(r\)-bounded assumption in Theorem 5 could be potentially relaxed. Another observation is that the approximations in DLog-A are less effective than the top-\(k\) one of NLog. We leave the theoretical analysis of the latter approximation as part of future research. Table 2 also suggests that the top-\(k\) semantic loss constitutes the most scalable and effective approach to training: in contrast to DLog-A and NLog, the accuracy of the digit classifier reaches 78% for \(M=4\), when \(k=4\) for Scallop.

Our empirical analysis shows that a key issue is _scalability_. This is a known problem in neuro-symbolic learning and it is partially because the relevant problems in logic are intractable. For instance, semantic loss requires computing the models of Boolean formulas, which is #P-complete. Reasoning over logical theories can be also intractable , if not undecidable. Scalability straightforwardly affects accuracy, e.g., NLog obtains better accuracy over DLog-A for \(M=3\) (see Table 2) due to its ability to explore the whole search space. It is also worth stressing that Scallop could not scale beyond \(M=4\) for \(k>1\). The above challenges bring up new research directions [44; 41]. Closing this discussion, it is worth exploring why ABL and ENT led to low accuracy4.

## 7 Related Work

We provide an overview of the literature that closely relates to our work. A more comprehensive review of PLL, latent structural learning, and neuro-symbolic learning, is presented in Appendix F.

**Partial label learning.** Learnability of PLL is typically shown under the _small ambiguity_ assumption [11; 28; 5]. While it is technically possible to cast our problem to standard PLL by viewing a target \(s\) as a partial label for the hidden labels \(\), the small ambiguity condition is violated in our case since a distracting label can occur because \(\) is deterministic. Therefore, our proposed conditions relax and generalize the small ambiguity by allowing deterministic transitions, see Appendix E.3. Another line of work in weak supervision exploits the invertibility of _transition matrices_ to compute posterior class probabilities [8; 9; 61]. Our learnability condition, \(M\)-unambiguity, and the invertibility condition do not imply each other, see Appendix E.1 and E.2. There, we also show that small ambiguity and the invertibility condition do not imply each other, which might be of independent interest. Another related topic is _learning with label constraints_[21; 6; 49], where the label of each instance \(x\) (possibly structured) in the dataset is constrained in a subset \(C\). The difference is that the constraint mapping itself is known to the learner and hence can be encoded in the inference algorithm directly, for example, via the CCM .

**Neuro-symbolic and structured learning.** We were motivated by frameworks that employ logic for training neural models [30; 53; 13; 59; 43; 31; 24; 27; 23]. We prove learnability under (unknown) transitions that capture different languages and error bounds under the SL  and top-\(k\) approximations. Notice that  also proves the consistency of a top-\(k\) loss. However, they use a zero-one-style loss for standard supervised learning. Our work is closely related to , which studies a similar problem of weakly supervised learning with multiple instances. Our results are stronger in the sense that we propose sufficient and necessary conditions to recover the hidden labels, while  concerns the likelihood of the observed labels rather than the hidden ones. Our work is also relevant to _latent structural prediction_[42; 35; 34; 32]. To our knowledge, no formal learning guarantees have been given for that problem. Complementary to ours is the work in [19; 47; 33] that integrates combinatorial solvers into deep models.

## 8 Conclusions

We formulated multi-instance PLL and showed its connections with latent structural and neuro-symbolic learning. Our work exhibits a greater level of flexibility compared to (single-instance) PLL, as it allows for multiple instances and deterministic transitions. We introduced minimal assumptions that enabled us to establish ERM-learnability and derive error bounds with the top-\(k\) semantics loss. Our findings suggest that the interaction of multiple instances in forming the observed labels can help relax the learnability assumptions in weakly supervised learning. For future work, we will further relax the learning conditions by assuming a certain degree of smoothness for the data distributions. Another direction is to explore the setting where the hidden labels are non-i.i.d. or structured.

   &  &  &  \\   & \#0 & \#16 & \#0 & \#16 & \#0 & \#16 \\ 
**DLog-A** & \(34\% 0.1\) & \(80\% 0.06\) & \(24\% 0.09\) & \(27\% 0.13\) & \(18\% 0.02\) & \(26\% 0.25\) \\
**NLog** & \(97\% 0.05\) & \(97\% 0.01\) & \(61\% 0.07\) & \(97\% 0.01\) & N/A & N/A \\
**NLog(5)** & \(48\% 0.003\) & \(87\% 0.007\) & \(27\% 0.06\) & \(84\% 0.06\) & \(29\% 0.1\) & \(46\% 0.05\) \\
**Scallop(1)** & \(97\% 0.05\) & \(97\% 0.01\) & \(21\% 3.5\) & \(97\% 0.01\) & \(36\% 0.2\) & \(54\% 0.02\) \\
**Scallop(4)** & \(97\% 0.05\) & \(97\% 0.05\) & \(53\% 0.15\) & \(97\% 0.07\) & \(51\% 0.09\) & \(78\% 0.05\) \\  

Table 2: Classifier accuracy for WEIGHTED-SUM for \(M=2\). #0, #8 and #16 are # of directly labeled samples used for pre-training. Inside the parentheses are the # of (weak) training samples.

   & **SL(4K)** & **SL(8K)** & **DLog(\#4K)** & **NLog(\#4K)** & **NASP(\#4K)** & **ABL(\#10K)** & **ENT(\#10K)** \\ 
40 & \(93\% 0.01\) & \(96\% 0.008\) & \(96\% 0.004\) & \(97\% 0.05\) & \(98\% 0.01\) & \(9\% 0.05\) & \(40\% 15.2\) \\
**\#8** & \(93\% 0.01\) & \(96\% 0.008\) & \(96\% 0.005\) & \(97\% 0.005\) & \(98\% 0.01\) & \(10\% 0.01\) & \(43\% 17.11\) \\
**\#16** & \(93\% 0.01\) & \(96\% 0.008\) & \(96\% 0.001\) & \(97\% 0.01\) & \(98\% 0.01\) & \(11\% 0.1\) & \(49\% 16.4\) \\  

Table 1: Classifier accuracy for WEIGHTED-SUM for \(M=2\). #0, #8 and #16 are # of directly labeled samples used for pre-training. Inside the parentheses are the # of (weak) training samples.