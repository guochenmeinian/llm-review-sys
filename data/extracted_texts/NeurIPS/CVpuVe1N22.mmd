# Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs

Zhiyuan Hu\({}^{1}\)1  Chunin Liu\({}^{2}\)  Xidong Feng\({}^{3}\)  Yilun Zhao\({}^{4}\)

**See-Kiong Ng\({}^{1}\)  Anh Tuan Luu\({}^{2}\)  Junxian He\({}^{5}\)  Pang Wei Koh\({}^{6}\) Bryan Hooi\({}^{1}\)**

\({}^{1}\) National University of Singapore \({}^{2}\) Nanyang Technological University

\({}^{3}\) University College London \({}^{4}\) Yale University

\({}^{5}\) The Hong Kong University of Science and Technology \({}^{6}\) University of Washington

###### Abstract

In the face of uncertainty, the ability to _seek information_ is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an _uncertainty-aware simulation approach_ which enables the model to simulate possible future scenarios and how likely they are to occur, 2) _uncertainty-based rewards_ motivated by information gain which incentivizes the model to seek information, and 3) a _reward propagation scheme_ to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 38.1% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task). Our code are released3.

## 1 Introduction

As the capabilities of large language models (LLMs) grow, they are being increasingly deployed in challenging real-world settings involving uncertainty and ambiguity. In particular, recent work aims to develop LLM agents or assistants  that effectively complete tasks in interactive environments, leading to a growing need for LLMs that can _actively seek the information they need_ to solve a task by asking questions in conversational settings. For example, in medical diagnosis, patients often do not initially report their symptoms in full detail. In such situations, a doctor's ability to ask effective questions is crucial, as a successful diagnosis often depends on revealing important details that the patient did not initially provide (Figure 1).

Figure 1: The importance of information seeking in medical diagnosis. The patient initially only complains of a headache, but by asking the right questions, the doctor uncovers the critical information needed for a correct diagnosis.

Recent techniques aim to improve LLMs' reasoning or planning abilities based on the given information rather than enabling LLMs to seek information efficiently. For example, Chain-of-Thought (CoT)  and Tree-of-Thoughts (ToT)  allow LLMs to express intermediate 'thoughts' and reason over them. Unlike these methods, our focus is on enabling the LLM to ask questions effectively by explicitly guiding the model toward reducing uncertainty, which these do not consider. Thus, they lack effective signals for questions that better reduce uncertainty by revealing critical information.

To enhance LLMs in actively seeking information, we introduce Uncertainty of Thoughts (UoT), a plug-and-play approach that improves LLMs' abilities to ask useful questions by modeling their own uncertainty. UoT is a principled approach relying on _uncertainty-based rewards motivated by information gain_, which incentivizes a model to seek information in a way that maximally reduces the amount of information it does not know. To utilize these rewards, we develop an _uncertainty-aware simulation framework_, enabling the model to simulate possible future scenarios along with how likely they are to occur. Given these scenarios, we utilize a _reward propagation scheme_ to select the optimal question to ask in a way that maximizes the expected reward.

Additionally, most standard benchmarks for LLMs, particularly in question answering, assume that all necessary information to solve a task is provided at the outset, and thus do not evaluate the model's active information-seeking capabilities. To close this gap, we first introduce a benchmark comprising 5 datasets3 on 3 tasks: 20 Questions, a simplified medical diagnosis task, and a basic troubleshooting task. These tasks are designed to measure the model's ability to ask questions effectively to gather the information they need. For example, the 20 Questions game, also studied by Noever et al., requires the model to ask 'yes' or 'no' questions to determine an unknown object or entity. This scenario serves as a clear and easily analyzed test case, isolating the model's ability to recognize its own uncertainty, and to ask questions that guide it to the correct answer.

Our work is a step toward LLMs that can effectively operate in settings with high uncertainty and ambiguity, beyond conventional QA settings where all the information needed to solve the task is provided to the model at the outset. To the best of our knowledge, UoT is the first approach for enabling LLMs to ask effective questions by explicitly modeling and seeking to reduce their uncertainty. Our key contributions are as follows:

1. We introduce Uncertainty of Thoughts (UoT), a plug-and-play approach enabling LLMs to explicitly model and seek to reduce their uncertainty. UoT utilizes a principled approach based on an uncertainty-aware framework for simulating possible futures, rewards motivated by information gain, and a reward propagation scheme to select the optimal question to ask.
2. We introduce a benchmark of 3 tasks and 5 datasets, designed to evaluate the ability of LLMs to seek the information they need by asking questions.
3. Experiments show that UoT improves the success rate of multiple LLMs by 38.1% on average compared with direct prompting, achieving top performance on both task success and efficiency. Our benchmark and code are publicly available.

## 2 Methodology

### Problem Formulation

The problem setting involves two roles: the Questioner and the Answerer, performed by the LLM and a human, respectively. The goal of the Questioner is to deduce an unknown piece of information. We formulate this using a _possibility space_\(\), which is the set of all possible options, of which a single element \(\), is the _true option_ in each given scenario4. For example, in a medical diagnosis setting, \(\) is the set of all possible diseases relevant in the context, e.g., \(=\{,,\}\), and for each patient, \(\) is the actual disease of the patient.

The interaction between the Questioner and the Answerer occurs over multiple turns. For instance, the Questioner may ask, "Do you have a fever?", to which the Answerer responds, "Yes, I've had a high fever for the past two days." The Questioner then asks another question such as "Have you vomited?" This exchange continues either until the Questioner correctly determines the final answer, or the conversation reaches a maximum number of turns. At this point, the interaction ends, and the Questioner is _successful_ if it has correctly determined the true option \(\).

Most of the description of our approach focuses on the _closed set_ scenario, in which we assume that the Questioner starts with knowledge of the possibility space \(\), e.g., the set of all possible diseases in medical diagnosis. In our extension section 2.7, we adapt our approach to the _open set_ scenario, in which this knowledge is absent. Moreover, as the questioning progresses, we use an LLM to gradually refine this set of possibilities to those that are consistent with the current answers given so far by the Answerer. Define the _current possibility set_\(_{i}\) as the subset of \(\) that is consistent with all answers given by the Answerer before the start of the \(i\)th interaction step.

As we discuss more later, we focus on applications where answers can be grouped into a small number of semantically distinct categories (in our case, affirmative and negative responses), as this allows us to compute meaningful uncertainty metrics in a simpler way. Conceptually, our framework can straightforwardly be extended to allow for a wider selection of answers.

### Uncertainty of Thoughts: Overview

As Figure 2 shows, to effectively reduce uncertainty, our UoT method first **generates multiple questions** as candidates to ask, and **simulates possible futures** for each one in the form of a tree structure. Next, **uncertainty-based rewards**, motivated by information gain, are used to assess the questions within the simulation. Finally, a **reward propagation scheme** is used to compute the expected reward from asking each candidate question, allowing us to select the one with highest expected reward, to ask the Answerer.

### Question Generation and Simulation

UoT starts by using an LLM to generate several candidate questions, then simulates future scenarios for each one. This simulation process allows us to measure how much information we can expect to gain in the next few steps from each question, and thus to choose the most suitable question.

Question GenerationRecall that our setting involves sequential interactions between a Questioner (e.g., a chatbot) and an Answerer (e.g., a human patient). During the \(i\)th interaction step, the Questioner generates candidate questions, then selects one of these to ask, denoted as \(q_{i}\).

To generate candidate questions to ask, UoT uses two inputs: (1) the _history of past interactions_\(h_{i}=\{q_{1},a_{1},q_{2},a_{2},,q_{i-1},a_{i-1}\}\), comprising the sequence of past questions and answers; and (2) the _current possibility set_\(_{i}\). These two inputs are combined to form a prompt that includes instructions explaining the nature of the task (e.g., how the 20 Questions game works), provides the current history \(h_{i}\) and the current possibility set \(_{i}\), and asks an LLM to generate \(m\) candidate next questions, conditioned on the previous context. This prompt, denoted as \(}(h_{i},_{i})\), is fed to

Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards \(R_{a}\) over past questions, and expected rewards \(R_{e}\) capturing expected future gains. The process ends by choosing questions with the highest expected reward.

our generator \(_{}\), which then generates \(m\) candidate questions, denoted \(q_{i}^{1},q_{i}^{2},,q_{i}^{m}\):

\[q_{i}^{1},q_{i}^{2},,q_{i}^{m}=_{}(_{}(h_{i},_{i}))\] (1)

Multistep SimulationAs shown in Figure 2 (a), the Question Generation stage generates candidate questions such as \(q_{i}^{1}\) = "Did you vomit?" Next, during Simulation stage, for each such generated candidate question, we simulate possible futures for a few steps, forming a tree of possibilities. This process enables us to compute rewards for each question, helping us to decide which question to ask.

Each node of the tree can be one of two types: _Answerer Nodes_ where it is the Answerer's turn to answer a question, and _Questioner Nodes_ where it is the Questioner's turn to ask a question. At the root, a question has just been asked (e.g., \(q_{i}^{1}\)), so the root is an Answerer Node. Next, we explain how to construct tree by recursively expanding (or 'branching') each node to construct its children, i.e., starting from the root, then proceeding to its children, and so on.

* At each **Answerer Node**, a question has just been asked. Next, we need to further 'branch' the tree based on the possible answers to the current question. Rather than allowing completely open-ended answers, we instead focus on affirmative and negative responses5, as this allows us to compute meaningful uncertainty metrics, as we discuss later. Hence, we branch the node into two children, corresponding to affirmative and negative answers. * At each **Questioner Node**, we prompt an LLM to generate \(m\) questions using the current history and possibility set, in the same way as in the Question Generation step. Note that while the generation procedure is similar, the purpose is different: the Question Generation step generates _candidate_ questions to select from, while here we are generating _simulated_ questions to form a tree for the purpose of evaluating the current question. The resulting \(m\) generated questions are added to the tree as children of the current node.

In this way, we recursively generate tree nodes, stopping at a fixed number of levels (i.e., depth).

While generating this tree, we also recursively compute the _current possibility set_\(_{v}\) at each node \(v\). Specifically, let \(h_{v}\) be the current conversation history up to node \(v\), combining both the actual conversation history \(h_{i}\) and the simulated conversation up to node \(v\). Then the current possibility set at this node, denoted \(_{v}\), is the subset of the possibility space consistent with \(h_{v}\). At the root, the current possibility set is only limited by the actual conversation history, i.e., \(_{i}\). Then, as we proceed over the simulated tree, note that the current possibility set only changes at Answerer nodes, when an answer is added to the current history. Hence, at each Answerer node \(v\), we prompt a new LLM (an 'Answerer Simulator' \(_{}\)), to determine the further subset \(_{v}^{A}_{v}\) for which the answer to the current question is affirmative, and the corresponding \(_{v}^{N}=_{v}_{v}^{A}\) for which the answer is negative.6 This allows us to recursively compute the possibility sets of the children of \(v\) (which themselves correspond to the affirmative and negative answers).

\[_{v}^{A},_{v}^{N}=_{}(_{ }(h_{v},_{v}))\] (2)

In this way, we can recursively compute the possibility set on each node of the tree.

### Uncertainty-Based Reward Calculation

To develop suitable information-seeking approaches, a critical question is _how to evaluate the effectiveness of a question, i.e., its contribution to reducing uncertainty_. To address this, we turn to information theory, specifically the concept of _information gain_, which measures the amount by which uncertainty decreases after a particular observation. To reward information-seeking behavior, we assign rewards to questions based on how much they reduce the model's uncertainty about the unknown random variable. These reward signals are used by our UoT framework to determine which question to select, to maximize the reduction of uncertainty.

Entropy.Entropy and information gain are well-known concepts in information theory . In our work, we use these concepts to measure how much information is gained (or equivalently, how much uncertainty is reduced) by asking a question, to formulate our rewards. Entropy measures the level of uncertainty in a random variable: higher entropy indicates greater uncertainty. The entropy of a discrete random variable \(X\) taking values \(x_{1},...,x_{n}\) is:

\[H(X)=-_{i=1}^{n}p(x_{i}) p(x_{i})\] (3)

Since our goal is to reduce the uncertainty in the unknown \(\), we use entropy to measure this uncertainty. Formally, let \(=\{_{1},,_{n}\}\), and we define an additional set of arbitrary real numbers \(=\{x_{1},,x_{n}\}\) which we will associate with each of these possibilities. Define a random variable \(X:\) such that \(X(_{i})=x_{i}\). Intuitively, \(X\) is a discrete random variable that takes the value \(x_{i}\) if the \(i\)th possibility is true, i.e., if \(=_{i}\). \(X\) serves to capture our uncertainty about \(\), since observing \(X\) is equivalent to observing the true option \(\). As a simple example, suppose our possibility space is \(=\{_{1},_{2},_{3}\}\); we accompany these with real numbers \(x_{1},x_{2},x_{3},\) and have a distribution for our random variable \(X\) reflecting prior beliefs over these possibilities: e.g., \(p(x_{1})=0.2,\ p(x_{2})=0.3,\ p(x_{3})=0.5\). Conceptually, our framework allows for any prior probability distribution over the possibilities (i.e., \(p(x_{i})\)), but in our experiments, we assume a uniform distribution over them due to the lack of an informative prior.

Before asking any questions, our uncertainty about the unknown \(\) is given by \(H(X)\), as in Eq. (3). At any node \(v\) of the trees described in the previous section, recall that we have a conversation history \(h_{v}\) which contains some answers given by the Answerer. This history limits the current possibility set to those in \(_{v}\), thereby reducing our uncertainty. We model this using the standard notion of _conditional probability on an event_: since \(_{v}\), thus \(_{v}\) is an event which we can condition on:

\[p(x_{i}|_{v})=p(x_{i})/p(_{v})\ \ \ i\ \ _{i}_{v}\] (4)

where \(p(_{v})\) is the sum of probabilities of the elements in \(_{v}\). To illustrate, we continue from the earlier example, where \(p(x_{1})=0.2,\ p(x_{2})=0.3,\ p(x_{3})=0.5\). If the conversation history \(h_{v}\) at node \(v\) is only consistent with \(x_{1}\) and \(x_{2}\), i.e., \(_{v}=\{_{1},_{2}\}\), we can adjust probability distribution by conditioning: e.g., the adjusted probability of \(x_{1}\) is \(p(x_{1})/p(_{v})=0.2/(0.2+0.3)=0.4\).

Next, to quantify the uncertainty at node \(v\), note that since \(X\) is conditionally distributed based on \(p(|_{v})\), the entropy of this distribution is:

\[H_{v}(X):=_{i:_{i}_{v}}p(x_{i}|_{v}) p( x_{i}|_{v})\] (5)

Intuitively, \(H_{v}(X)\) is the remaining uncertainty in \(X\) at node \(v\) (i.e., after observing the history \(h_{v}\)).

Information Gain at a NodeWe now quantify the uncertainty reduction when receiving answers at an Answerer node \(v\). Recall that the answer given at \(v\) partitions \(_{v}\) into two disjoint subsets: \(_{v}=_{v}^{A}_{v}^{N}\), where \(_{v}^{A}\) and \(_{v}^{N}\) are the subsets of possibilities resulting in affirmative and negative answers to last asked question. Given an affirmative answer, the remaining entropy becomes:

\[H_{v}^{A}(X):=_{i:_{i}_{v}^{A}}p(x_{i}|_{v} ^{A}) p(x_{i}|_{v}^{A})\] (6)

We define \(H_{v}^{N}(X)\) analogously for negative answers. Let \(p_{v}^{A}=p(_{v}^{A})/p(_{v})\) and \(p_{v}^{N}=p(_{v}^{N})/p(_{v})\) be the conditional probabilities of affirmative and negative answers at node \(v\). To compute the expected entropy after receiving the answer at node \(v\), since we have a \(p_{v}^{A}\) probability of receiving an affirmative answer and \(p_{v}^{N}\) of a negative answer, the expected entropy is:

\[p_{v}^{A} H_{v}^{A}(X)+p_{v}^{N} H_{v}^{N}(X)\] (7)

As such, the expected information gain at node \(v\) is the difference in entropies before and after receiving the answer:

\[IG_{v}(X):=H_{v}(X)-p_{v}^{A} H_{v}^{A}(X)-p_{v}^{N} H_{v}^{N}(X)\] (8)

We can simplify this: as proven in Appendix A, the above equation reduces to:

\[IG_{v}(X)=-p_{v}^{A} p_{v}^{A}-p_{v}^{N} p_{v}^{N}\] (9)

This represents the expected reduction of uncertainty in \(X\) when receiving an answer at node \(v\). Note that it has an entropy-like expression, and is therefore nonnegative.

Reward FormulationA natural approach would be to define the reward function \(R_{u}(v)\) at node \(v\) as the information gain \(IG_{v}(X)\): that is, the reward from the question at node \(v\) is the expected information gain \(IG_{v}(X)\) from receiving its answer. In practice, we find that a slightly modified function \(_{v}(X)\) is preferable. In particular, we find that \(IG_{v}(X)\) does not result in sufficiently sharp differences in reward over the typical ranges we encounter. Hence, we introduce an additional hyperparameter \( 0\) which helps to sharpen the rewards using a scaling approach. We compare other scaling methods and determine the current design is optimal in performance and their corresponding benefits. Details are in the Appendix B.

\[R_{u}(v)=_{v}(X):=(-p_{v}^{A} p_{v}^{A}-p_{v}^{N} p_{v}^ {N})/(1+^{-1}|p_{v}^{A}-p_{v}^{N}|)\] (10)

This definition ensures that \(R_{u}(v)\) falls within the range \(\), providing a normalized and consistent reward to measure uncertainty reduction. The reward function reaches its maximum when the subsets \(_{v}^{A}\) and \(_{v}^{N}\) have equal probability, reflecting the maximum reduction in uncertainty. It reaches its minimum when one of the subsets has zero probability, indicating no reduction in uncertainty. Appendix G plots the reward function curve across values of \(p_{v}^{A}\) and \(p_{v}^{N}\).

### Question Selection Via Reward Propagation

Single-step rewards often fall short in dynamic settings as they only consider immediate impact, overlooking long-term effects. To overcome this, our method uses a reward propagation scheme across simulation trees by defining 'accumulated rewards' that gather rewards over multiple simulation steps to reflect the effectiveness of past decisions. These accumulated rewards help compute 'expected rewards', indicating the likely benefits of the questions and guide the selection of candidate questions.

Accumulated RewardWe first define the accumulated reward at each node \(v\), which accumulates the rewards at \(v\) and all its ancestors on the tree, defined recursively as:

\[R_{a}(v):=R_{u}(v)+\{0&\\ R_{a}((v))&.\]

Here \(R_{u}(v)\) is the uncertainty-based reward at node \(v\) defined in Eq. (10), and \(R_{a}((v))\) is the accumulated reward of the parent of \(v\). We compute these accumulated rewards by starting at the root and propagating down to the leaves. Intuitively, the accumulated reward at each leaf node represents the total reward we end up with at the end of the conversation at that node.

Expected RewardNext, we compute the expected reward for each node \(R_{e}(v)\), which represents the expected total value of rewards received on expectation on a node and all its descendants on tree.

\[R_{e}(v):=R_{a}(v)&\\ p_{v}^{u}R_{e}(v^{A})+p_{v}^{N}R_{e}(v^{N})&\\ _{w(v)}^{m}R_{e}(w)&\]

For the case where \(v\) is an Answerer Node, recall that \(p_{v}^{A}\) and \(p_{v}^{N}\) are the conditional probabilities of affirmative and negative answers at node \(v\), defined in section 2.4. \(v^{A}\) and \(v^{N}\) are its children, corresponding to the affirmative and negative answers. For the case where \(v\) is a Questioner Node, we assign equal probability to the \(m\) questions asked from this node. In this way, we propagate the expected rewards from the leaves up to the root, allowing us to compute the expected gain at the root. We compare different reward propagation schemes and find that using cumulative rewards from all paths enhances long-term decision-making benefits. See Appendix C for details.

Determining the Optimal QuestionFinally, to decide the question to ask, we select the question with highest expected reward (and therefore, the highest expected information gain, considering both immediate and future information gains):

\[q_{i}=*{arg\,max}_{n=1}R_{e}(q_{i}^{n})\] (11)

### UoT Summary

UoT first generates candidate questions \(q_{i}^{1},q_{i}^{2},,q_{i}^{m}\) based on the history \(h_{i}\) and current possibility set \(_{i}\). Then, we conduct multistep simulation to generate a tree for each candidate question \(q_{i}^{n}\). Next, we compute the uncertainty-based rewards \(R_{u}(v)\), and propagate over the trees to compute accumulated reward \(R_{u}(v)\) and expected reward \(R_{e}(v)\). Lastly, the optimal question \(q_{i}^{n}\) with highest expected reward will be selected as \(q_{i}\) to interact with the Answerer. UoT generates candidate questions \(q_{i}^{1},q_{i}^{2},,q_{i}^{m}\) based on history and the current possibility set \(_{i}\). It simulates a tree for each question, calculates uncertainty-based rewards \(R_{u}(v)\), and computes expected rewards \(R_{e}(v)\). The question \(q_{i}^{n}\) with the highest expected reward is chosen for interaction.

### Extensions and Discussion

**Open Set UoT.** Recall that in the closed set scenario, the Questioner starts with knowledge of the possibility space \(\). In practice, the possibility space is often unknown, resulting in the open set setting. To adapt UoT to this case, we prompt Questioner to initialize the possibility space \(\) and then reinitialize the possibility set \(_{i}\) according to current history \(h_{i}\). Then, the rest of UoT is unchanged.

**The generalization in open-end answers.** The UoT framework enables LLMs to update possibilities after each interaction, including affirmative/negative or open-ended responses. Thus, it can be applied to open-ended answers scenarios. **Pruned UoT.** To enhance efficiency during simulation, pruning akin to Beam Search can be employed when constructing the simulation trees, which limits the number of paths to explore over the tree to a predetermined size.

## 3 Experiments

### Experimental Setup

ModelsWe test various LLMs to evaluate the generality of our method, including **Llama-3-70B-Instruct**, **Mistral-Large**, **Gemini-1.5-Pro**, **Claude-3-Opus** and **GPT-4**. We also validate the performance of earlier released LLMs (Refer to Appendix D) including **Llama 2-70B-Chat**, **Cohere**, **PaLM 2**, **Claude 2** and **GPT-3.5-turbo**.

Baselines**Direct Prompting (DP)** prompts an LLM directly to generate the next response. **Planning Prompting (PP)** is motivated by Wang et al.. We leverage another LLM to plan the future and, consequently, determine the question to ask. **Chain-of-Thought (CoT)** improves reasoning in LLMs by detailing reasoning steps. **CoT-SC (Self-Consistency)** an is an ensemble method, explores multiple reasoning paths. We standardize sampling counts for fair computational cost comparison with other methods. **Reflexion** lets agents propose actions and self-assess to foster new ideas. **Tree-of-Thoughts (ToT)** enables LLMs to make decisions by exploring and evaluating multiple reasoning paths over a tree structure. We examine ToT under two setups: **Original-ToT**, which uses the standard approach of generating and evaluating questions, and **Adapted-ToT (Ad-ToT)**, where we integrate heuristic experience into prompt for question generation and evaluation, focusing on questions that halve the search space. We matched the tree depth to the simulation steps in our UoT method for a fair comparison. We evaluate methods and LLMs in both open set (**OS**) and closed set (**CS**) settings. In open set, models are tested without prior knowledge of outcomes; in closed set, they are given complete information about all possible outcomes. For details, see Appendix I.1 for experimental settings and Appendix L for prompts.

Scenarios and Datasets20 Questions is a game where the _answer_ thinks of an item and the _questioner_ asks up to 20 yes-or-no questions to guess it. We use two datasets, Common (collected by us, refer to Appendix I.2 for more details) and Things , including 111 and 1854 items separately. In this scenario, the maximal turns is set to 20. In **Medical Diagnosis**, the doctor needs to ask questions to patients about their symptoms, to determine an accurate diagnosis. We use two datasets: DX , with 104 doctor-patient dialogues and 5 diseases in test set, and MedDG  with over 17K conversations across 15 disease types. We manually selected 500 high-quality samples for evaluation (see Appendix I.3 for selection process). _Importantly, Open-ended responses from patient are allowed in MedDG to validate UoT's generalization in open-ended scenarios._ Both datasets are limited to 5 turns. **Troubleshooting** is a scenario where a customer support technician interacts with customers to identify and resolve faults or issues within computer systems, electronic devices, machinery, orother complex systems. Raghu et al. introduce FloDial with 894 dialogues, containing 153 faults and we also conduct the data preprocessing of FloDial (See Appendix I.4 for details). We evaluate using a maximum of 20 turns. The answerer, simulated by GPT-4, is prompted with the patient's actual disease and conversation details for each case. For more details, refer to Appendix I.2 and see examples of these scenarios in Appendix K.

UoT (Open Set) SetupWe iteratively update LLMs' perceived possibilities based on conversational history, rather than defining them all upfront. In medical diagnosis and troubleshooting, initial descriptions from symptoms or issues help set up initial possibilities. In the 20-question game, we start with broad inquiries using the Direct Prompting method for the first three rounds to gather more information. The ToT tree structure method employs a similar strategy. Setup details in Appendix I.5.

Evaluation MetricsTo measure efficacy and efficiency, we use: **Success Rate (%)**: \(=S/T\), where \(S\) is the number of successful cases, and \(T\) is the total number of cases; **Mean Conversation Length in Successful Cases**: \(=R_{s}/S\), where \(R_{s}\) is the total rounds in successful cases; **Mean Conversation Length**: \(=R/T\), where \(R\) is the total rounds in all cases. **MCL** measures efficiency based on the resources used in both successes and failures.

### Performance

20 QuestionsAs illustrated in Table 5, for all types of LLMs, those equipped with UoT outperform the baselines in both open set and close settings. Among the methods used on GPT-4 to enhance planning and reasoning, CoT (CS) and PP (CS) show inferior performance even compared to GPT-4 alone. UoT (OS) demonstrates superior performance, with with an average 8.7% improvement than Adapted-ToT (OS) in success rate. Moreover, UoT (CS) achieves the highest success rate, surpassing the second-best Reflexion by an average of 4.3%.

Medical DiagnosisUoT (CS) outperforms baselines in simplified medical diagnostics, achieving a 97.0% success rate on the DX dataset with GPT-4. On the MedDG dataset, UoT (CS) on Gemini-1.5-Pro and GPT-4 achieve success rates of 81.4% and 88.0%. It also reduces conversation lengths to an average MSC of 2.0 on GPT-4 for DX, lower than 3.5 and 3.0 for DP methods. _These results demonstrate the versatility of our UoT in handling both binary and open-ended interactions effectively._

    &  &  &  &  \\   & &  &  &  &  &  \\   & & SRI & MSCL & MCI & SRI & MSCL & MCI & SRI & MSCL & SRI & MSCL & SCI & SRI & MSCL & MCI \\   & DP (OS) & 3.42 & 119 & 17.9 & 15.5 & 14.9 & 19.2 & 36.0 & 3.6 & 4.6 & 25.7 & 3.6 & 4.6 & 11.1 & 15.9 \\  & UoT(OS) & **36.9** & **12.4** & **17.3** & **23.0** & **13.6** & **18.7** & **35.6** & 2.6 & **4.1** & **96.6** & **2.5** & **3.6** & **26.4** & **5.4** & **17.2** \\  & UoT(CS) & 51.4 & 14.6 & 17.2 & 15.0 & 13.8 & 19.1 & 83.7 & 5.5 & 3.7 & 6.2 & 3.5 & 4.1 & 25.8 & 15.7 & 18.8 \\  & UoT(CS) & **55.9** & **12.6** & **15.9** & **25.0** & **13.0** & **18.3** & **90.4** & **1.6** & **4.3** & **1.4** & **2.7** & **47.8** & **16.2** \\   & DP(OS) & 20.7 & **13.1** & **18.6** & 12.5 & 13.6 & 13.2 & 18.3 & 3.4 & 4.7 & 28.3 & 3.2 & 4.5 & 11.1 & 15.8 & 19.5 \\  & UoT(OS) & **27.9** & 18.7** & **15.0** & **13.1** & **19.0** & **24.0** & **2.5** & **4.4** & **90.0** & **9.0** & **4.9** & **19.6** & **1.3** & **19.3** \\   & UoT(CS) & 56.1 & 13.4 & 18.5 & 13.6 & **12.6** & 90.1 & 83.5 & 3.3 & 4.3 & 4.7 & 3.3 & 4.2 & 4.2 & 4.2 & 16.0 & 19.4 \\   & UoT(CS) & **31.5** & **9.8** & **16.8** & **18.5** & **13.2** & **18.7** & **48.1** & **2.2** & **3.6** & **60.0** & **1.9** & **3.2** & **30.1** & **19.0** & **17.3** \\   & DP (OS) & 3.60 & 18.6 & 18.8 & 17.5 & 14.4 & 19.0 & 26.9 & 3.5 & 4.6 & 23.7 & 4.0 & 4.8 & 15.5 & 16.5 & 19.6 \\  & UoT(CS) & **39.7** & **14.6** & **17.9** & **22.0** & **13.4** & **18.5** & **39.4** & **2.4** & **3.8** & **3.6** & **2.9** & **2.3** & **19.0** & **12.1** & **18.5** \\   & D\(\)(CS) & 47.7 & 19.0 & 18.6 & 25.5 & 15.6 & 18.6 & 6.2 & 2.2 & -3.5 & 31.4 & 3.2 & 4.1 & 3.1 & 30.1 & 10.0 & 18.2 \\   & UoT(CS) & **60.4** & **13.9** & **16.3** & **32.0** & **14.0** & **18.1** & **81.7** & **2.1** & **2.1** & **84.4** & **2.1** & **2.4** & **53.6** & **11.5** & **15.4** \\   & DP(OS) & 45.0 & **14.2** & 17.4 & 16.5 & 13.8 & 19.0 & 33.7 & 3.4 & 4.5 & 3.2 & 4.0 & 31.4 & 15.7 & 18.6 \\  & UoT(CS) & **63.1** & **43.1** & **16.8** & **23.5** & **13.8** & **18.0** & **45.6** & **4.9** & **3.9** & **4.5** & **4.5** & **3.2** & **3.3** & **35.9** & **19.0** & **16.5** \\   & UoT(CS) & 52.2 & 13.8 & 16.6 & 33.5 & 14.1 & 19.0 &TroubleshootingUoT (CS) with GPT-4 similarly achieves the highest SR of 67.3%, and the lowest MSC of 7.8. It also shows a remarkable improvement from 43.7% to 67.3% in Success Rate.

Overall PerformanceOn average, UoT enhances the success rate by 38.1% compared to DP across 5 datasets and 5 different LLMs, including open source and commercial models. Notably, Success Rate increases 46.6% for Llama3-70B. Furthermore, UoT outperforms CoT-SC by 33.8% and Reflexion by 29.9%. Even compared to tree structure methods like Original-ToT and Adapted-ToT, UoT still shows superior performance with gains of 28.3% and 12.4% respectively. Additionally, Pruned UoT, our pruning method to improve efficiency, outperforms Adapted-ToT by 7.36%. Additionally, our study shows that UoT's one-step planning is effective due to effective reward design and question selection. We limit simulations to three steps for budgetary reasons, balancing efficiency and effectiveness (see Appendix E for further details on simulation depth). To determine whether the differences in success rates between the two methods were statistically significant, we performed a t-test. The results and details are in Appendix H.

Case Studies and Reliability of GPT-4 as answererFigure 3 shows UoT, compared to direct prompting, more effectively reduce uncertainty and narrow down candidates, avoiding overly specific queries. After gaining initial information (e.g., stomach pain), it generates targeted questions about related issues rather than general inquiries. Additionally, GPT-4's accuracy as answerer is evaluated by analyzing 10% of interactions from each dataset, consistently showing reliable responses. For quantitative details, see Appendix F.

### Analysis

#### 3.3.1 Comparing Model Performance at Equal Computational Efficiency

We compare the performance of approaches with similar computational costs in a closed set setting, in terms of token consumption. To do so, we first prune our UoT as described in section 2.7. Secondly, we expand exploration depth of Adapted-ToT method to bring its token cost in line with that of UoT.

As shown in the top half of Table 2, the Pruned UoT method, despite its reduced efficacy compared to UoT, still outperforms ToT and other methods. Also, the bottom part of Table 2 shows that even when increasing the depth of Adapted ToT (Adapted-ToT (\(D=4\))) to match the token cost of UoT (\(D=3\)), it still underperforms compared to UoT.

#### 3.3.2 Effectiveness of Uncertainty Rewards

To further demonstrate the effectiveness of our uncertainty-based reward, we compare it with the self-evaluation reward used in the original ToT based on GPT-4 model. We implement the uncertainty-based reward in place of the self-evaluation reward in ToT, creating a variant we call ToT (+UR). The results, as shown in left side of Figure 4, indicate that our reward significantly enhances planning efficacy by an average of 5.9%. Additionally, we use the heuristic self-evaluation reward in Adapted-ToT to replace our current uncertainty-based reward in UoT, a variant we refer to as UoT (-UR). This change results in a performance decrease shown in the right part of Figure 4, further validating the effectiveness of our uncertainty-based reward. Moreover, the performance of UoT (-UR) still surpasses that of Adapted-ToT illustrated in Table 5,

Figure 3: Case studies from the 20 Questions game (left) and simplified medical diagnosis (right).

## 4 Related Work

Planning and Reasoning of LLMsLLMs show prowess in planning and reasoning. Wei et al. introduced CoT prompting for intermediate reasoning; Yao et al. proposed ToT prompting using DFS/BFS. Besta et al. present GoT to solve elaborate problems. Feng et al. illustrated TS-LLM's tree-search guided decoding. ReAct  offers acting-based prompting, while Reflexion  enhances this with feedback reflection. Zhou et al. unify reasoning and planning.

Decision-making and Information-seeking by LLMsLLMs have evolved as decision-making tools, with models like LLM+P  and LLM-DP  combining external planners and LLMs for natural language-based programming. RAP  goes beyond structured language, using LLMs with Monte Carlo Tree Search (MCTS)  for dynamic decision-making. This approach is also seen in the work of Zhao et al., applying MCTS and LLM knowledge for complex tasks like robot control. However, MCTS struggles in uncertain scenarios due to its reliance on terminal states and specific modules for rewards and action selection. Additionally, to enhance LLMs' questioning abilities, Deng et al. introduce the Rephrase and Respond method. AVIS  represents an autonomous visual question answering system that uses external tools. Pan et al. introduce KwaiAgents for processing queries, following guidelines, and accessing external documents. Frameworks such as MEDIQ  and MDAgents  improve the reliability of LLMs in clinical settings by strengthening information-seeking capabilities and agent systems, thereby supporting more realistic diagnostic processes.  also explore Chatgpt's information seeking strategy in 20-questions game.

## 5 Limitation and Future Work

In practice, \(_{v}^{A}\) and \(_{v}^{N}\) might overlap, as different answers (such as "yes" or "no") may lead to the exclusion of different sets of possibilities. Another similar limitation is that some questions or answers may not fully eliminate certain possibilities (e.g.,"I don't have a fever" does not 100% eliminate the possibility of having COVID-19). Furthermore, compared to completely open-ended interaction in medical diagnosis or troubleshooting, our current benchmark represents a simplified scenario. In theory, such cases could be handled using the method of converting interactions into probability estimations and applying some kind of Bayesian update to the probabilities of each possibility, rather than just eliminating some subset.

## 6 Conclusion and Discussion

This paper presents the Uncertainty of Thoughts (UoT) algorithm, significantly improving LLMs in tasks requiring active information seeking through tree-based simulation, uncertainty-based rewards and a reward propagation scheme. On five datasets UoT increases success rate by 38.1% on average, establishing a new benchmark for evaluating LLMs in active information-seeking tasks. We evaluate UoT on simplified scenarios; more realistic scenarios raise challenges like allowing incomplete elimination of possibilities by answers, and others which we leave for future work.

  
**Method** & Tokens & 20Q & MD & TB \\  CoT-SC(\(k=33\)) & 4.6k & 32.6 & 37.6 & 42.5 \\ Orig-ToT(\(D=3\)) & 4.5k & 23.7 & 65.3 & 40.4 \\ Adapt-ToT(\(D=3\)) & 4.5k & 33.8 & 85.1 & 60.3 \\ Pruned UoT(\(D=3\)) & 4.7k & **48.1** & **88.4** & **63.2** \\  Adapt-ToT(\(D=4\)) & 9.3k & 40.9 & 86.7 & 63.7 \\ UoT(\(D=3\)) & 9.2k & **54.4** & **92.5** & **66.0** \\   

Table 2: Average success rates for 20Q, MD, and TB at comparable efficiency, measured by GPT-4 token use. \(k\) is sampling count, \(D\) is tree depth.

Figure 4: Success rate comparison between Adapted-ToT and Adapted-ToT using uncertainty reward, and between UoT and UoT without uncertainty reward.

Acknowledgment

Pang Wei Koh is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Innovation under the AI Visiting Professorship Programme (award number AIVP-2024-001).